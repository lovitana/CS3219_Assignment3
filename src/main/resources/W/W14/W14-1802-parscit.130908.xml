<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000346">
<title confidence="0.995616">
Automatic Assessment of the Speech of Young English Learners
</title>
<author confidence="0.971199">
Jian Cheng1, Yuan Zhao D’Antilio1, Xin Chen1, Jared Bernstein2
</author>
<affiliation confidence="0.6937125">
1Knowledge Technologies, Pearson, Menlo Park, California, USA
2Tasso Partners LLC, Palo Alto, California, USA
</affiliation>
<email confidence="0.994585">
jian.cheng@pearson.com
</email>
<sectionHeader confidence="0.993812" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986348">
This paper introduces some of the research
behind automatic scoring of the speak-
ing part of the Arizona English Language
Learner Assessment, a large-scale test now
operational for students in Arizona. Ap-
proximately 70% of the students tested are
in the range 4-11 years old. We cover the
methods used to assess spoken responses
automatically, considering both what the
student says and the way in which the stu-
dent speaks. We also provide evidence
for the validity of machine scores. The
assessments include 10 open-ended item
types. For 9 of the 10 open item types,
machine scoring performed at a similar
level or better than human scoring at the
item-type level. At the participant level,
correlation coefficients between machine
overall scores and average human overall
scores were: Kindergarten: 0.88; Grades
1-2: 0.90; Grades 3-5: 0.94; Grades 6-8:
0.95; Grades 9-12: 0.93. The average cor-
relation coefficient was 0.92. We include
a note on implementing a detector to catch
problematic test performances.
</bodyText>
<sectionHeader confidence="0.999101" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998762125">
Arizona English Language Learner Assessment
(AZELLA) (Arizona Department of Education,
2014) is a test administered in the state of Arizona
to all students from kindergarten up to grade 12
(K-12) who had been previously identified as En-
glish learners (ELs). AZELLA is used to place EL
students into an appropriate level of instructional
and to reassess EL students on an annual basis to
monitor their progress. AZELLA was originally
a fully human-delivered paper-pencil test cover-
ing four domains: listening, speaking, reading and
writing. The Arizona Department of Education
chose to automate the delivery and scoring of the
speaking parts of the test, and further decided that
test delivery via speakerphone would be the most
efficient and universally accessible mode of ad-
ministration. During the first field test (Nov. 7 -
Dec. 2, 2011) over 31,000 tests were administered
to 1st to 12th graders on speakerphones in Arizona
schools. A second field test in April 2012 deliv-
ered over 13,000 AZELLA tests to kindergarten
students. This paper reports research results based
on analysis of data sets from the 44,000 students
tested in these two administrations.
</bodyText>
<sectionHeader confidence="0.943045" genericHeader="method">
2 AZELLA speaking tests
</sectionHeader>
<bodyText confidence="0.9994594">
AZELLA speaking tests are published in five
stages (Table 1), one for each of five grade ranges
or student levels. Each stage has four fixed test
forms. Table 1 presents the total number of field
tests delivered for each stage, or level.
</bodyText>
<tableCaption confidence="0.993239">
Table 1: Stages, grades, and number of field tests
</tableCaption>
<bodyText confidence="0.980593611111111">
Stage I II III IV V
Grade K 1-2 3-5 6-8 9-12
N 13184 10646 9369 6439 5231
Fourteen different speaking exercises (item-
types) were included in the various level-specific
forms of the test. Some item-types were accom-
panied by images; some only had audio prompts.
Note, however, that before the change to automatic
administration and scoring, test forms had only in-
cluded speaking item-types from a set of thirteen
different types, of which ten were not designed to
constrain the spoken responses. On the contrary,
these ten item-types were designed to elicit rela-
tively open-ended displays of speaking ability, and
most test forms included one or two items of most
types. A Repeat Sentence item type was added
to the test designs (10 Repeat items per test form
at every level), yielding test forms with around
</bodyText>
<page confidence="0.984686">
12
</page>
<note confidence="0.713488">
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications, pages 12–21,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.996507444444445">
27 items total, including Repeats. Table 2 lists
all the speaking item types that are presented in
one AZELLA test form for Stage III (Grades 3-
5). Some items such as Questions on Image, Sim-
ilarities &amp; Differences, Ask Qs about a Statement,
and Detailed Response to Topic are presented as a
sequence of two related questions and the two re-
sponses are human-rated together to produce one
holistic score.
</bodyText>
<tableCaption confidence="0.995193">
Table 2: Stage III (Grades 3-5) items.
</tableCaption>
<table confidence="0.999058454545454">
Descriptions items/test Score-Points
Repeat Sentence 10 0-4
Read-by-Syllables 3 0-1
Read-Three-Words 3 0-1
Questions on Image 3 0-4
Similarities &amp; Differences 2 0-4
Give Directions from Map 1 0-4
Ask Qs about a Statement 1 0-4
Give Instructions 1 0-4
Open Question on Topic 1 0-4
Detailed Response to Topic 1 0-4
</table>
<tableCaption confidence="0.978076">
Table 3: Item types used in AZELLA speaking
field tests.
</tableCaption>
<table confidence="0.994725066666666">
Description (restriction) Score-Points
Naming (Stage I) 0-1
Short Response (Stage I) 0-2
Open Question (Stage I) 0-2
Read-by-Syllables 0-1
Read-Three-Words 0-1 or 0-3
Repeat Sentence 0-4
Questions on Image 0-4
Similarities &amp; Differences (III) 0-4
Give Directions from Map 0-4
Ask Qs about a Thing (II) 0-2
Ask Qs about a Statement (III) 0-4
Give Instructions 0-4
Open Questions on Topic 0-4
Detailed Response to Topic 0-4
</table>
<bodyText confidence="0.988175181818182">
All the speaking item-types used at any level
in the AZELLA field tests are listed in Table
3. Item-types used at only one stage (level) are
noted. From Table 3 we can see that, except for
Naming, Repeat Sentence, Read-by-Syllables, and
Read-Three-Words, all the items are fairly uncon-
strained questions. Engineering considerations did
not guide the design of these items to make them
be more suitable for machine learning and auto-
matic scoring, and they were, indeed, a challenge
to score.
By tradition and by design, human scoring of
AZELLA responses is limited to a single holistic
score, guided by sets of Score-Point rubrics defin-
ing scores at 2, 3, 4, or 5 levels. The column Score-
Points specifies the number of categories used in
holistic scoring. One set of five abbreviated holis-
tic rubrics for assigning points by human rating is
presented below in Table 4. For the Repeat Sen-
tence items only, separate human ratings were col-
lected under a pronunciation rubric and a fluency
rubric.
</bodyText>
<tableCaption confidence="0.95766">
Table 4: Example AZELLA abbreviated holistic
rubric (5 Score-Points).
</tableCaption>
<figure confidence="0.806217">
Points Descriptors
4 Correct understandable English us-
</figure>
<figureCaption confidence="0.87386632">
ing two or more sentences. 1. Com-
plete declarative or interrogative sen-
tences. 2. Grammar (or syntax) er-
rors are not evident and do not im-
pede communication. 3. Clear and
correct pronunciation. 4. Correct
syntax.
3 Understandable English using two or
more sentences. 1. Complete declar-
ative or interrogative sentences. 2.
Minor grammatical (or syntax) er-
rors. 3. Clear and correct pronuncia-
tion.
2 An intelligible English response. 1.
Less than two complete declarative
or interrogative sentences. 2. Errors
in grammar (or syntax). 3. Attempt
to respond with clear and correct pro-
nunciation.
1 Erroneous responses. 1. Not com-
plete declarative or interrogative sen-
tences. 2. Significant errors in gram-
mar (or syntax). 3. Not clear and cor-
rect pronunciation.
0 Non-English or silence.
</figureCaption>
<sectionHeader confidence="0.971238" genericHeader="method">
3 Development and validation data
</sectionHeader>
<bodyText confidence="0.999916142857143">
From the data in the first field test (Stages II, III,
IV, V), for each AZELLA Stage, we randomly
sampled 300 tests (75 tests/form x 4 forms) as a
validation set and 1, 200 tests as a development
set. For the data in the second field test (Stage
I), we randomly sampled 167 tests from the four
forms as the validation set and 1, 200 tests as the
</bodyText>
<page confidence="0.998346">
13
</page>
<bodyText confidence="0.9787895">
development set. No validation data was used for
model training.
</bodyText>
<subsectionHeader confidence="0.99992">
3.1 Human transcriptions and scoring
</subsectionHeader>
<bodyText confidence="0.999987555555556">
In the development sets, we needed from 100 to
300 responses per item to be transcribed, depend-
ing on the complexity of the item type. In the val-
idation sets, all responses were fully transcribed.
Depending on the item type, we got single or dou-
ble transcriptions, as necessary.
All responses from the tests were scored by
trained professional raters according to predefined
rubrics (Arizona Department of Education, 2012),
such as those in Table 4. Departing from usual
practice in production settings, we used the aver-
age score from different raters as the final score
during machine learning. The responses in each
validation set were double rated (producing two
final scores) for use in validation. Note that five of
the 1,367 tests in the validation sets had no human
transcriptions and ratings, and so were excluded
from the final validation results.
</bodyText>
<sectionHeader confidence="0.99557" genericHeader="method">
4 Machine scoring methods
</sectionHeader>
<bodyText confidence="0.999869923076923">
Previous research on automatic assessment of
spoken responses can be found in Bernstein et
al. (2000; 2010), Cheng (2011) and Higgins
et al. (2011). Past work on automatic assess-
ment of children’s oral reading fluency has been
reported at the passage-level (Cheng and Shen,
2010; Downey et al., 2011) and at the word-level
(Tepperman et al., 2007). A comprehensive review
of spoken language technologies for education can
be found in Eskanazi (2009). The following sub-
sections summarize the methods we have used for
scoring AZELLA tests. Those methods with cita-
tions have been previously discussed in research
papers. Other methods described are novel modi-
fications or extensions of known methods.
Both the linguistic content and the manner of
speaking are scored. Our machine scoring meth-
ods include a combination of automatic speech
recognition (ASR), speech processing, statistical
modeling, linguistics, word vectors, and machine
learning. The speech processing technology was
built to handle the different rhythms and varied
pronunciations used by a range of natives and
learners. In addition to recognizing the words
spoken, the system also aligns the speech signal,
i.e., it locates the part of the signal containing
relevant segments, syllables, and words, allowing
the system to assign independent scores based on
the content of what is spoken and the manner in
which it is said. Thus, we derive scores based on
the words used, as well as the pace, fluency, and
pronunciation of those words in phrases and sen-
tences. For each response, base measures are then
derived from the linguistic units (segments, sylla-
bles, words), with reference to statistical models
built from the spoken performances of natives and
learners. Except for the Repeat items, the system
produces only one holistic score per item from a
combination of base measures.
</bodyText>
<subsectionHeader confidence="0.997157">
4.1 Acoustic models
</subsectionHeader>
<bodyText confidence="0.999980324324324">
We tried various sets of recorded responses to train
GMM-HMM acoustic models as implemented in
HTK (Young et al., 2000). Performance im-
proved by training acoustic models on larger sets
of recordings, including material from students
out of the age range being tested. For exam-
ple, training acoustic models using only the Stage
II transcriptions to recognize other Stage II re-
sponses was significantly improved by using more
data from outside the Stage II data set, such as
other AZELLA field data. We observed that the
more child speech data, the better the automatic
scoring. The final acoustic models used for recog-
nition were trained on all transcribed AZELLA
field data, except the data in the validation sets,
plus data from an unrelated set of children’s oral
reading of passages (Cheng and Shen, 2010), and
the data collected during the construction of the
Versant Junior English tests for use by young chil-
dren in Asia (Bernstein and Cheng, 2007). Thus,
the acoustic models were built using any and all
relevant data available: totaling about 380 hours
of data (or around 176, 000 responses). The word
error rate (WER) over all the validation sets using
the final acoustic models is around 35%.
For machine scoring (after recognition and
alignment), native acoustic models are used to
compute native likelihoods of producing the ob-
served base measures. Human listeners classified
student recordings from Stage II (grades 1-2) as
native or non-native. For example, in Stage II
data, 287 subjects were identified as native and the
recordings from these 287 subjects plus the native
recordings from the Versant Junior English tests
were used to build native acoustic models for grad-
ing. (approximately 66 hours of speech data, or
39, 000 responses).
</bodyText>
<page confidence="0.993811">
14
</page>
<subsectionHeader confidence="0.989185">
4.2 Language models
</subsectionHeader>
<bodyText confidence="0.999973333333333">
Item-specific bigram language models were built
using the human transcription of the development-
set as described in Section 3.1.
</bodyText>
<subsectionHeader confidence="0.999581">
4.3 Content modeling
</subsectionHeader>
<bodyText confidence="0.999993263157895">
&amp;quot;Content&amp;quot; refers to the linguistic material (words,
phrases, and semantic elements) in the spoken re-
sponse. Appropriate response content reflects the
speaker’s productive control of English vocabu-
lary and also indicates how well the test-taker un-
derstood the prompt. Previous work on scoring
linguistic content in the speech domain includes
Bernstein et al. (2010) and Xie et al. (2012).
Except for the four relatively closed-response-
form items (Naming, Repeat, Read-by-Syllables
and Read-Three-Words), we produced a
word_vector score for each response (Bern-
stein et al., 2010). The value of the word_vector
score is calculated by scaling the weighted sum of
the occurrence of a large set of expected words
and word sequences available in an item-specific
response scoring model. An automatic process
assigned weights to the expected words and word
sequences according to their semantic relation to
known good responses using a method similar to
latent semantic analysis (Landauer et al., 1998).
The word_vector score is generally the most
powerful feature used to predict the final human
scores.
Note that a recent competition to develop accu-
rate scoring algorithms for student-written short-
answer responses (Kaggle, 2012) focused on a
similar problem to the content scoring task for
AZELLA open-ended responses. We assume that
the methods used by the prize-winning teams,
for example Tandalla (2012) and Zbontar (2012),
should work well for the AZELLA open-ended
material too, although we did not try these meth-
ods.
For the responses to Naming, Read-by-
Syllables, and Read-Three-Words items, the ma-
chine scoring makes binary decisions based on the
occurrence of a correct sequence of syllables or
words (keywords). In Stage II forms, for first
and second grade students, the responses to Read-
Three-Words items were human-rated in four cat-
egories. For this stage, the machine counted the
number of words read correctly.
For the responses to Repeat items, the recog-
nized string is compared to the word string re-
cited in the prompt, and the number of word er-
rors (word_errors) is calculated as the minimum
number of substitutions, deletions, and/or inser-
tions required to find a best string match in the
response. This matching algorithm ignores hes-
itations and filled or unfilled pauses, as well as
any leading or trailing material in the response
(Bernstein et al., 2010). A verbatim repetition
would have zero word errors. For Repeat re-
sponses, the percentage of words repeated cor-
rectly (percent_correct) was used as an addi-
tional feature.
</bodyText>
<subsectionHeader confidence="0.999154">
4.4 Duration modeling
</subsectionHeader>
<bodyText confidence="0.9998469375">
Phone-level duration statistics contribute to ma-
chine scores of test-takers’ pronunciation and flu-
ency. Native-speakers segment duration statis-
tics from Versant Junior English tests (Bernstein
and Cheng, 2007) were used to compute the
log-likelihood of phone durations produced by
test-takers. No data from AZELLA tests con-
tributed to the duration models. We calculated the
phoneme duration log-likelihood: log_seg_prob
and the inter-word silence duration log-likelihood:
iw_log_seg_prob (Cheng, 2011).
Assume in a recognized response that the se-
quence of phonemes and their corresponding du-
rations are pi and Di, i = 1...N, then the
log likelihood segmental probability for phonemes
(log_seg_prob) was computed as:
</bodyText>
<equation confidence="0.964636">
1 N−1log_seg prob = N − 2 Y-
i=2
</equation>
<bodyText confidence="0.9998564">
where Pr(Di) was the probability that a native
would produce phoneme pi with the observed du-
ration Di in the context found. The first and last
phonemes in the response were not used for the
calculation of the log_seg_prob because durations
of these phonemes as determined by the ASR were
more likely to be incorrect. The log likelihood
segmental probability for inter-word silence dura-
tions, iw_log_seg_prob, was calculated the same
way (Cheng, 2011).
</bodyText>
<subsectionHeader confidence="0.990096">
4.5 Spectral modeling
</subsectionHeader>
<bodyText confidence="0.999991333333333">
To construct scoring models for pronunciation
and fluency, we computed several spectral likeli-
hood features with reference to native and learner
segment-specific models applied to the recogni-
tion alignment, computing the phone-level poste-
rior probabilities given the acoustic observation X
</bodyText>
<equation confidence="0.890281">
log(Pr(Di)), (1)
15
that is recognized as pi:
P (X|pi)P (pi)
P(pi|X) = Emk=1 P(X|pk)P(pk) (2)
</equation>
<bodyText confidence="0.999847413793103">
where k runs over all the potential phonemes. In
a real-world ASR system, it is extremely difficult
to estimate Emk=1 P(X|pk)P(pk) precisely. So
approximations are used, such as substituting a
maximum for the summation, etc. Formula 2 is
the general framework for pronunciation diagno-
sis (Witt and Young, 1997; Franco et al., 1999;
Witt and Young, 2000) and pronunciation assess-
ment (Witt and Young, 2000; Franco et al., 1997;
Neumeyer et al., 1999; Bernstein et al., 2010).
Various authors use different approximations to
suit the particulars of their data and their applica-
tions.
In the AZELLA spectral scoring, we approx-
imated Formula 2 with the following procedure.
After the learner acoustic models produce a recog-
nition result, we force-align the utterance on the
recognized word string, but using the native mono-
phone acoustic models, producing acoustic log-
likelihood, duration and time boundaries for ev-
ery phone. For each such phone, again using the
native monophone time alignment, we perform
an all-phone recognition using the native mono-
phone acoustic models. The recognizer calculates
a log-likelihood for every phone and picks the
best match from all possible phones over that time
frame. For each phone-of-interest in a response,
we calculated the average spectral score difference
as:
</bodyText>
<equation confidence="0.96314775">
lpfa
i − lpap
i (3)
di
</equation>
<bodyText confidence="0.999358">
where the variables are:
</bodyText>
<listItem confidence="0.8421899">
• lpfa
i is the log-likelihood corresponding to
the i-th phoneme by using the forced align-
ment method;
• lpap
i is the log-likelihood by using the all-
phone recognition method;
• di is its duration;
• N is the number of phonemes of interest in a
response.
</listItem>
<bodyText confidence="0.983660181818182">
In calculating spectral_1, all possible
phonemes are included. We define another
variable, spectral_2, that only accumulates
the log-likelihood for a target set of phonemes
that learners often have difficulty with. We call
the percentage of phones from the all-phone
recognition that match the phones from the forced
alignment the percent phone match, or ppm.
We take Formula 3 as the average log of the
approximate posterior probabilities that phones
were produced by a native.
</bodyText>
<subsectionHeader confidence="0.996773">
4.6 Confidence modeling
</subsectionHeader>
<bodyText confidence="0.999950428571428">
After finishing speech recognition, we can assign
speech confidence scores to words and phonemes
(Cheng and Shen, 2011). Then for every response,
we can compute the average confidence, the per-
centage of words or phonemes whose confidences
are lower than a threshold value as features to pre-
dict test-takers’ performance.
</bodyText>
<subsectionHeader confidence="0.952397">
4.7 Final models
</subsectionHeader>
<bodyText confidence="0.999938153846154">
AZELLA holistic score rubrics (Arizona Depart-
ment of Education, 2012), such as those shown
in Table 4, consider both the answer content and
the manner of speaking used in the response. The
automatic scoring should consider both too. Fea-
tures word_vector, keywords, word_errors,
percent_correct can represent content scores
based on what is spoken. Features log_seg_prob,
iw_log_seg_prob, spectral_1, spectral_2, ppm
can represent both the rhythmic and segmental as-
pects of the performance as native likelihoods of
producing the observed base measures. By feed-
ing these features to models, we can effectively
predict human holistic scores, as well as human
pronunciation and fluency ratings, although we did
not model grammar errors in the way they are
specifically described in the rubrics, e.g. in Table
4.
For each item, a specific combination of base
scores was selected. So, on an item-by-item basis,
we tried two methods of combination: (i) multiple
linear regression and (ii) neural networks with one
hidden layer trained by back propagation. Then
we selected the one that was more accurate for
that item. For almost all items, the neural network
model worked better.
</bodyText>
<subsectionHeader confidence="0.987783">
4.8 Unscorable test detection
</subsectionHeader>
<bodyText confidence="0.999886833333333">
Many factors can render a test unscorable: poor
sound quality (recording noise, mouth too close
to the microphone, too soft, etc.), gibberish (non-
sense words, noise, or a foreign language), off-
topic (off topic, but intelligible English), unintelli-
gible English (e.g. a good-faith attempt to respond
</bodyText>
<equation confidence="0.99225275">
N
1
spectral_1 = N
i=1
</equation>
<page confidence="0.977045">
16
</page>
<bodyText confidence="0.999904228571428">
in English, but is so unintelligible and/or disfluent
that it cannot be understood confidently).
There have been several approaches to dealing
with this issue (Cheng and Shen, 2011; Chen and
Mostow, 2011; Yoon et al., 2011). Some un-
scorable tests can be identified easily by a hu-
man listener, and we reported research on a speci-
fied unscorable category (off-topic) before (Cheng
and Shen, 2011). Dealing with a specified cat-
egory could be significantly easier than dealing
with wide-open items as in AZELLA. Also, be-
cause we did not collect human “unscorable&amp;quot; rat-
ings for this data, we worked on predicting the ab-
solute overall difference between human and ma-
chine scores; which is like predicting outliers. If
the difference is expected to exceed a threshold,
the test should be sent for human grading.
Many problems were due to low volume record-
ings made by shy kids, so we identified features to
deal with low-volume tests. These included max-
imum energy, the number of frames with funda-
mental frequency, etc., using many features men-
tioned in Cheng and Shen (2011). The method
used to detect off-topic responses did not work
well here, but features based on lattice confidence
seemed to work fairly well. If we define an un-
scorable test as one with an overall difference be-
tween human and machine scores greater than or
equal to 3 (within the score range 0-14), our final
unscorable test detector achieves an equal-error
rate of 16.5% in validation sets; or when fixing the
false rejection rate at 6%, the false acceptance rate
is 44%. We are actively investigating better meth-
ods to achieve acceptable performance for use in
real tests.
</bodyText>
<sectionHeader confidence="0.996111" genericHeader="method">
5 Experimental results
</sectionHeader>
<bodyText confidence="0.965871428571429">
All results presented in this section used the vali-
dation data sets, while the recognition and scoring
models were built from completely separate mate-
rial. The participant-level speaking scores were
designed not to consider the scores from Read-
by-Syllables and Read-Three-Words. For each
test, the system produced holistic scores for Re-
peat items and for non-Repeat items. For every
Repeat item, the machine generated pronuncia-
tion, fluency and accuracy scores mapped into the
0 to 4 score-point range. Both human and machine
holistic scores for a Repeat response are equal
to: 50% · Accuracy + 25% · Pronunciation +
25% · Fluency. Accuracy scores were scaled
as percent_correct times four. Human accuracy
scores were based on human transcriptions instead
of ASR transcriptions. Holistic scores for Repeat
items at the participant level were the simple aver-
age of the corresponding item-level scores.
For every non-Repeat item, we generated one
holistic score that considered pronunciation, flu-
ency and content together. The non-Repeat holis-
tic scores at the participant level were the sim-
ple average of the corresponding item level scores
after normalizing them to the same scale. The
final generated holistic scores for Repeats were
scaled to a 0 − 4 range and non-Repeat holis-
tic scores were scaled to a 0 − 10 range to sat-
isfy an AZELLA design requirement that Repeat
items count for 4 points and non-Repeats count
for 10 points. The overall participant level scores
are the sum of the Repeat holistic scores and the
non-Repeat holistic scores (maximum 14). All
machine-generated scores are continuous values.
In the following tables, H-H r stands for the
human-human correlation and M-H r stands for
the correlation between machine-generated scores
and average human scores.
Table 5: Human rating reliabilities and Machine-
human correlations by item type. Third column
gives mean and standard deviation of words per
response.
</bodyText>
<table confidence="0.99991125">
S Item types Words/response H-H r M-H r
µ f σ
I Naming 2.5 f 2.5 0.83 0.67
I Short Response 5.7 f 3.8 0.71 0.73
I Open Question 8.7 f 7.9 0.70 0.76
I Repeat Sentence 5.0 f 2.5 0.91 0.83
II Questions on Image 14.0 f 10.8 0.87 0.86
II Give Directions from Map 10.9 f 9.7 0.82 0.84
II Ask Qs about a Thing 6.8 f 5.9 0.83 0.64
II Open Question on Topic 11.6 f 10.6 0.75 0.72
II Give Instructions 11.5 f 10.0 0.83 0.80
II Repeat Sentence 6.1 f 2.9 0.95 0.85
III Questions on Image 14.5 f 10.2 0.87 0.77
III Similarities &amp; Differences 19.5 f 11.6 0.75 0.75
III Give Directions from Map 16.3 f 11.2 0.74 0.85
III Ask Qs about a Statement 16.7 f 13.4 0.79 0.82
III Give Instructions 17.0 f 12.8 0.77 0.81
III Open Question on Topic 13.9 f 11.1 0.85 0.85
III Detailed Response to Topic 13.8 f 10.5 0.81 0.80
III Repeat Sentence 6.4 f 3.2 0.97 0.88
IV Questions on Image 13.9 f 11.8 0.84 0.84
IV Give Directions from Map 13.7 f 13.3 0.84 0.90
IV Open Question on Topic 17.2 f 15.2 0.82 0.82
IV Detailed Response to Topic 13.9 f 11.4 0.85 0.87
IV Give Instructions 16.5 f 15.7 0.87 0.90
IV Repeat Sentence 6.9 f 3.2 0.96 0.89
V Questions on Image 17.3 f 12.0 0.80 0.76
V Open Question on Topic 18.7 f 14.9 0.84 0.82
V Detailed Response to Topic 17.7 f 15.2 0.88 0.87
V Give Instructions 17.2 f 16.6 0.90 0.90
V Give Directions from Map 22.4 f 16.8 0.86 0.85
V Repeat Sentence 6.4 f 3.5 0.95 0.89
</table>
<page confidence="0.998814">
17
</page>
<bodyText confidence="0.999917647058824">
We summarize the psychometric properties of
different item types that contribute to the final
scores in Table 5. For each item-type and each
stage, the third column in Table 5 presents the
mean and standard deviation of the words-per-
response produced by students, showing that older
students generally produce more spoken material.
We found that the number of words spoken is a
better measure than speech signal duration to rep-
resent the amount of material produced, because
young English learners often emit long silences
while speaking. The difference between the two
measures in columns 4 and 5 is statistically signif-
icant (two-tailed, p &lt; 0.05) for item types Nam-
ing (Stage I), Ask Qs about a Thing (Stage II),
Questions on Image (Stage III), and Repeat Sen-
tence (all Stages), in which machine scoring does
not match human; and for item types Give Direc-
tions from Map (Stage III, IV), in which machine
is better than a single human score. For almost
all open-ended items, machine scoring is similar
to or better than human scoring. We noticed that
machine scoring of one open-ended item type, Ask
Qs about a Thing used in Stage II test forms, was
significantly worse than human scoring, leading us
to identify problems specific to the item type itself,
both in the human rating rubric and in the machine
grading approach. Arizona is not using this item
type in operational tests.
Figures 1, 2, 3, 4, 5 present scatter plots of over-
all scores at the participant level comparing hu-
man and machine scores for test in each AZELLA
stage. Figure 6 shows the averaged human holistic
score distribution for participants in the validation
set for Stage V. The human holistic score distribu-
tions for participants in other AZELLA stages are
similar to those in Figure 6, except the means shift
somewhat.
We identified several participants for whom the
difference between human and machine scores is
bigger than 4 in Figures 1, 2, 3, 4, 5. Listen-
ing to the recordings of these tests, we concluded
that the most important factor was low Signal-to-
Noise Ratio (SNR). Either the background noise
was very high (in 6 of 1,362 tests in the validation
set), or speech volume was low (in 3 of 1,362 tests
in the validation set). Either condition can make
recognition difficult. With very low voice ampli-
tude and high background noise levels, the SNR of
some outlier response recordings is so low that hu-
man raters refuse to affirm that they understand the
</bodyText>
<figureCaption confidence="0.981906125">
Figure 1: Overall human vs. machine scores at the
participant level for Stage I (Grade K). Mean and
standard deviation for human scores: (8.74, 3.1).
Figure 2: Overall human vs. machine scores at
the participant level for Stage II (Grades 1-2).
Mean and standard deviation for human scores:
(7.1,2.5).
Figure 3: Overall human vs. machine scores at
</figureCaption>
<bodyText confidence="0.520013">
the participant level for Stage III (Grades 3-5).
Mean and standard deviation for human scores:
(9.6,2.3).
</bodyText>
<page confidence="0.996788">
18
</page>
<figureCaption confidence="0.9790272">
Figure 4: Overall human vs. machine scores at
the participant level for Stage IV (Grades 6-8).
Mean and standard deviation for human scores:
(8.3,2.9).
Figure 5: Overall human vs. machine scores at
</figureCaption>
<bodyText confidence="0.931278166666667">
the participant level for Stage V (Grades 9-12).
Mean and standard deviation for human scores:
(8.9,2.9).
content of the response or rate its pronunciation.
Since many young children in kindergarten and
early elementary school speak softly, the youngest
children’s speech is substantially harder to recog-
nize (Li and Russell, 2002; Lee et al., 1999). This
probably contributes to the lower reliabilities in
Stage I and II. When setting the total rejection rate
at 6%, our unscorable test detector identifies only
7 of the 13 outlier tests.
</bodyText>
<tableCaption confidence="0.954627">
Table 6: Reliability of human scores and Human-
Machine correlations of overall test scores by
stage.
</tableCaption>
<table confidence="0.999334142857143">
Stage H-H r M-H r
I 0.91 0.88
II 0.96 0.90
III 0.97 0.94
IV 0.98 0.95
V 0.98 0.93
Average 0.96 0.92
</table>
<bodyText confidence="0.99611225">
Table 6 summarizes the reliabilities of the tests
in different stages. At the participant level, the av-
erage inter-rater reliability coefficient across the
five stages was 0.96, suggesting that the well-
trained human raters agree with each other with
high consistency when ratings are combined over
all the material in all the responses in a whole
test; the average correlation coefficient between
machine-generated overall scores and average hu-
man overall scores was 0.92. This suggests that
the machine grading may be sufficiently reliable
for most purposes.
</bodyText>
<tableCaption confidence="0.6541375">
Table 7: Test reliability by stage, separating non-
Repeat holistic scores and Repeat holistic scores.
</tableCaption>
<table confidence="0.99959775">
Stage H-H r M-H r H-H r M-H r
NonRptH NonRptH RptH RptH
I 0.85 0.83 0.99 0.94
II 0.93 0.89 0.99 0.90
III 0.95 0.92 0.99 0.92
IV 0.96 0.95 0.99 0.94
V 0.96 0.91 0.99 0.93
Average 0.93 0.90 0.99 0.93
</table>
<figureCaption confidence="0.988628">
Figure 6: Distribution of average human holistic
</figureCaption>
<bodyText confidence="0.745001857142857">
score for participants in the validation set for Stage
V (Grades 9-12).
Table 7 summarizes the reliabilities of test
scores in the different stages considering the non-
Repeat holistic scores and Repeat holistic scores
separately to check the effect of adding the Re-
peat items. Repeat items improve the machine re-
</bodyText>
<page confidence="0.997175">
19
</page>
<bodyText confidence="0.99902875">
liability in Stage I significantly, but not so much
for other stages. This difference may relate to the
difficulty in eliciting sufficient speech samples in
non-Repeat items from the young EL students in
Stage I. Eliciting spoken materials in Repeat items
is more straightforward. Consideration of Table
7 suggests that using only open-ended item-types
can also achieve sufficiently reliable results.
</bodyText>
<sectionHeader confidence="0.996314" genericHeader="evaluation">
6 Discussion and future work
</sectionHeader>
<bodyText confidence="0.999978954545455">
We believe that we can improve this system fur-
ther by scoring Repeat items using a partial credit
Rasch model (Masters, 1982) instead of the av-
erage of percent_correct, which should improve
the reliability of the Repeat item type. We may
also be able to train a better native acoustic model
by using a larger sample of native data from
AZELLA, if we are given access to the test-taker
demographic information.
The original item selection and assignment of
items to forms was quite simple and had room for
improvement. Currently in the AZELLA testing
program, test forms go through a post-pilot re-
vision, so that the operational tests only include
good items in the final test forms. This post-pilot
selection and arrangement of items into forms
should improve human-machine correlations be-
yond the values reported here. If we effectively
address the problem of shy-kids-talking-softly, the
scoring performance will definitely improve even
more. Getting young students to talk louder is
probably something that can be best done at the
testing site (by instruction or by example); and
it may solve several problems. We are happy
to report that the first operational AZELLA test
with automatic speech scoring took place between
January 14 and February 26, 2013, with approxi-
mately 140, 700 tests delivered.
Recent progress in machine learning has ap-
plied deep neural networks (DNNs) to many
long-standing pattern recognition and classifica-
tion problems. Many groups have now applied
DNNs to the task of building better acoustic mod-
els for speech recognition (Hinton et al., 2012).
DNNs have repeatedly been shown to work better
than Gaussian mixture models (GMMs) for ASR
acoustic modeling (Hinton et al., 2012; Dahl et al.,
2012). We are actively exploring the use of DNNs
for use in recognition of children’s speech. We
expect that DNN acoustic models can overcome
some of the recognition difficulties mentioned in
this paper (e.g. low SNR in responses and short
response item types like Naming) and boost the fi-
nal assessment accuracy significantly.
</bodyText>
<sectionHeader confidence="0.998729" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999991333333334">
We have reported an evaluation of the automatic
methods that are currently used to assess spo-
ken responses to test tasks that occur in Ari-
zona’s AZELLA test for young English learners.
The methods score both the content of the re-
sponses and the quality of the speech produced
in the responses. Although most of the speak-
ing item types in the AZELLA tests are uncon-
strained and open-ended, machine scoring accu-
racy is similar to or better than human scoring for
most item types. We presented basic validity evi-
dence for machine-generated scores, including an
average correlation coefficient between machine-
generated overall scores and human overall scores
derived from subscores that are based on multi-
ple human ratings. Further, we described the de-
sign, implementation and evaluation of a detec-
tor to catch problematic, unscorable tests. We be-
lieve that near-term re-optimization of some scor-
ing process elements may further improve ma-
chine scoring accuracy.
</bodyText>
<sectionHeader confidence="0.99833" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99760456">
Arizona Department of Education. 2012.
AZELLA update. http://www.azed.gov/
standards-development-assessment/
files/2012/12/12-12-12-update-
v5.pdf. [Accessed 19-March-2014].
Arizona Department of Education. 2014. Arizona
English Language Learner Assessment (AZELLA).
http://www.azed.gov/standards-
development-assessment/arizona-
english-language-learner-
assessment-azella. [Accessed 19-March-
2014].
J. Bernstein and J. Cheng. 2007. Logic and valida-
tion of a fully automatic spoken English test. In
V. M. Holland and F. P. Fisher, editors, The Path
of Speech Technologies in Computer Assisted Lan-
guage Learning, pages 174–194. Routledge, New
York.
J. Bernstein, J. De Jong, D. Pisoni, and B. Townshend.
2000. Two experiments on automatic scoring of spo-
ken language proficiency. In Proc. of STIL (Integrat-
ing Speech Technology in Learning), pages 57–61.
J. Bernstein, A. Van Moere, and J. Cheng. 2010. Vali-
dating automated speaking tests. Language Testing,
27(3):355–377.
</reference>
<page confidence="0.903055">
20
</page>
<reference confidence="0.997197115384615">
W. Chen and J. Mostow. 2011. A tale of two tasks: De-
tecting children’s off-task speech in a reading tutor.
In Interspeech 2011, pages 1621–1624.
J. Cheng and J. Shen. 2010. Towards accurate recogni-
tion for children’s oral reading fluency. In IEEE-SLT
2010, pages 91–96.
J. Cheng and J. Shen. 2011. Off-topic detection in
automated speech assessment applications. In Inter-
speech 2011, pages 1597–1600.
J. Cheng. 2011. Automatic assessment of prosody
in high-stakes English tests. In Interspeech 2011,
pages 1589–1592.
G. Dahl, D. Yu, L. Deng, and A. Acero. 2012.
Context-dependent pretrained deep neural networks
for large vocabulary speech recognition. IEEE
Transactions on Speech and Audio Processing, Spe-
cial Issue on Deep Learning for Speech and Lang.
Processing, 20(1):30–42.
R. Downey, D. Rubin, J. Cheng, and J. Bernstein.
2011. Performance of automated scoring for chil-
dren’s oral reading. In Proceedings of the Sixth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 46–55.
M. Eskanazi. 2009. An overview of spoken language
technology for education. Speech Communication,
51:832–844.
H. Franco, L. Neumeyer, Y. Kim, and O. Ronen. 1997.
Automatic pronunciation scoring for language in-
struction. In ICASSP 1997, pages 1471–1474.
H. Franco, L. Neumeyer, M. Ramos, and H. Bratt.
1999. Automatic detection of phone-level mispro-
nunciation for language learning. In Eurospeech
1999, pages 851–854.
D. Higgins, X. Xi, K. Zechner, and D. Williamson.
2011. A three-stage approach to the automated scor-
ing of spontaneous spoken responses. Computer
Speech and Language, 25:282–306.
G. Hinton, L. Deng, Y. Dong, G. Dahl, A. Mohamed,
N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. Sainath, and B. Kingsbury. 2012. Deep neu-
ral networks for acoustic modeling in speech recog-
nition: The shared views of four research groups.
IEEE Signal Processing Magazine, 29(6):82–97.
Kaggle. 2012. The Hewlett Foundation: Short
answer scoring. http://www.kaggle.com/
c/asap-sas;http://www.kaggle.com/
c/asap-sas/details/winners. [Accessed
20-April-2014].
T. K. Landauer, P. W. Foltz, and D. Laham. 1998.
Introduction to latent semantic analysis. Discourse
Processes, 25:259–284.
S. Lee, A. Potamianos, and S. Narayanan. 1999.
Acoustics of children’s speech: developmental
changes of temporal and spectral parameters. Jour-
nal of Acoustics Society of American, 105:1455–
1468.
Q. Li and M. Russell. 2002. An analysis of the causes
of increased error rates in children’s speech recogni-
tion. In ICSLP 2002, pages 2337–2340.
G. N. Masters. 1982. A Rasch model for partial credit
scoring. Psychometrika, 47(2):149–174.
L. Neumeyer, H. Franco, V. Digalakis, and M. Wein-
traub. 1999. Automatic scoring of pronunciation
quality. Speech Communication, 30:83–93.
L. Tandalla. 2012. ASAP Short Answer
Scoring Competition System Description:
Scoring short answer essays. https:
//kaggle2.blob.core.windows.net/
competitions/kaggle/2959/media/
TechnicalMethodsPaper.pdf. [Accessed
20-April-2014].
J. Tepperman, M. Black, P. Price, S. Lee,
A. Kazemzadeh, M. Gerosa, M. Heritage, A. Al-
wan, and S. Narayanan. 2007. A Bayesian network
classifier for word-level reading assessment. In
Interspeech 2007, pages 2185–2188.
S. M. Witt and S. J. Young. 1997. Language learn-
ing based on non-native speech recognition. In Eu-
rospeech 1997, pages 633–636.
S. M. Witt and S. J. Young. 2000. Phone-level pro-
nunciation scoring and assessment for interactive
language learning. Speech Communication, 30:95–
108.
S. Xie, K. Evanini, and K. Zechner. 2012. Explor-
ing content features for automated speech scoring.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 103–111.
S.-Y. Yoon, K. Evanini, and K. Zechner. 2011. Non-
scorable response detection for automated speaking
proficiency assessment. In Proceedings of the Sixth
Workshop on Innovative Use of NLP for Building
Educational Applications, pages 152–160.
S. J. Young, D. Kershaw, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland. 2000. The HTK
Book Version 3.0. Cambridge University, Cam-
bridge, England.
J. Zbontar. 2012. ASAP Short Answer Scoring Com-
petition System Description: Short answer scoring
by stacking. https://kaggle2.blob.core.
windows.net/competitions/kaggle/
2959/media/jzbontar.pdf. [Accessed
20-April-2014].
</reference>
<page confidence="0.999381">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.564293">
<title confidence="0.999682">Automatic Assessment of the Speech of Young English Learners</title>
<author confidence="0.95732">Zhao</author>
<affiliation confidence="0.636266">Technologies, Pearson, Menlo Park, California,</affiliation>
<address confidence="0.979612">Partners LLC, Palo Alto, California, USA</address>
<email confidence="0.99988">jian.cheng@pearson.com</email>
<abstract confidence="0.991882807692308">This paper introduces some of the research behind automatic scoring of the speakpart of the English Language a large-scale test now operational for students in Arizona. Approximately 70% of the students tested are in the range 4-11 years old. We cover the methods used to assess spoken responses automatically, considering both what the student says and the way in which the student speaks. We also provide evidence for the validity of machine scores. The assessments include 10 open-ended item types. For 9 of the 10 open item types, machine scoring performed at a similar level or better than human scoring at the item-type level. At the participant level, correlation coefficients between machine overall scores and average human overall were: Kindergarten: Grades Grades 3-5: Grades 6-8: Grades 9-12: average corcoefficient was include a note on implementing a detector to catch problematic test performances.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<date>2012</date>
<institution>Arizona Department of Education.</institution>
<note>AZELLA update. http://www.azed.gov/ standards-development-assessment/ files/2012/12/12-12-12-updatev5.pdf. [Accessed 19-March-2014].</note>
<contexts>
<context position="12460" citStr="(2012)" startWordPosition="2016" endWordPosition="2016">6 hours of speech data, or 39, 000 responses). 14 4.2 Language models Item-specific bigram language models were built using the human transcription of the developmentset as described in Section 3.1. 4.3 Content modeling &amp;quot;Content&amp;quot; refers to the linguistic material (words, phrases, and semantic elements) in the spoken response. Appropriate response content reflects the speaker’s productive control of English vocabulary and also indicates how well the test-taker understood the prompt. Previous work on scoring linguistic content in the speech domain includes Bernstein et al. (2010) and Xie et al. (2012). Except for the four relatively closed-responseform items (Naming, Repeat, Read-by-Syllables and Read-Three-Words), we produced a word_vector score for each response (Bernstein et al., 2010). The value of the word_vector score is calculated by scaling the weighted sum of the occurrence of a large set of expected words and word sequences available in an item-specific response scoring model. An automatic process assigned weights to the expected words and word sequences according to their semantic relation to known good responses using a method similar to latent semantic analysis (Landauer et al</context>
</contexts>
<marker>2012</marker>
<rawString>Arizona Department of Education. 2012. AZELLA update. http://www.azed.gov/ standards-development-assessment/ files/2012/12/12-12-12-updatev5.pdf. [Accessed 19-March-2014].</rawString>
</citation>
<citation valid="true">
<title>Arizona English Language Learner Assessment (AZELLA).</title>
<date>2014</date>
<institution>Arizona Department of Education.</institution>
<note>http://www.azed.gov/standardsdevelopment-assessment/arizonaenglish-language-learnerassessment-azella. [Accessed 19-March2014].</note>
<marker>2014</marker>
<rawString>Arizona Department of Education. 2014. Arizona English Language Learner Assessment (AZELLA). http://www.azed.gov/standardsdevelopment-assessment/arizonaenglish-language-learnerassessment-azella. [Accessed 19-March2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bernstein</author>
<author>J Cheng</author>
</authors>
<title>Logic and validation of a fully automatic spoken English test.</title>
<date>2007</date>
<booktitle>The Path of Speech Technologies in Computer Assisted Language Learning,</booktitle>
<pages>174--194</pages>
<editor>In V. M. Holland and F. P. Fisher, editors,</editor>
<publisher>Routledge,</publisher>
<location>New York.</location>
<contexts>
<context position="11100" citStr="Bernstein and Cheng, 2007" startWordPosition="1800" endWordPosition="1803">I transcriptions to recognize other Stage II responses was significantly improved by using more data from outside the Stage II data set, such as other AZELLA field data. We observed that the more child speech data, the better the automatic scoring. The final acoustic models used for recognition were trained on all transcribed AZELLA field data, except the data in the validation sets, plus data from an unrelated set of children’s oral reading of passages (Cheng and Shen, 2010), and the data collected during the construction of the Versant Junior English tests for use by young children in Asia (Bernstein and Cheng, 2007). Thus, the acoustic models were built using any and all relevant data available: totaling about 380 hours of data (or around 176, 000 responses). The word error rate (WER) over all the validation sets using the final acoustic models is around 35%. For machine scoring (after recognition and alignment), native acoustic models are used to compute native likelihoods of producing the observed base measures. Human listeners classified student recordings from Stage II (grades 1-2) as native or non-native. For example, in Stage II data, 287 subjects were identified as native and the recordings from t</context>
<context position="14847" citStr="Bernstein and Cheng, 2007" startWordPosition="2386" endWordPosition="2389">insertions required to find a best string match in the response. This matching algorithm ignores hesitations and filled or unfilled pauses, as well as any leading or trailing material in the response (Bernstein et al., 2010). A verbatim repetition would have zero word errors. For Repeat responses, the percentage of words repeated correctly (percent_correct) was used as an additional feature. 4.4 Duration modeling Phone-level duration statistics contribute to machine scores of test-takers’ pronunciation and fluency. Native-speakers segment duration statistics from Versant Junior English tests (Bernstein and Cheng, 2007) were used to compute the log-likelihood of phone durations produced by test-takers. No data from AZELLA tests contributed to the duration models. We calculated the phoneme duration log-likelihood: log_seg_prob and the inter-word silence duration log-likelihood: iw_log_seg_prob (Cheng, 2011). Assume in a recognized response that the sequence of phonemes and their corresponding durations are pi and Di, i = 1...N, then the log likelihood segmental probability for phonemes (log_seg_prob) was computed as: 1 N−1log_seg prob = N − 2 Yi=2 where Pr(Di) was the probability that a native would produce p</context>
</contexts>
<marker>Bernstein, Cheng, 2007</marker>
<rawString>J. Bernstein and J. Cheng. 2007. Logic and validation of a fully automatic spoken English test. In V. M. Holland and F. P. Fisher, editors, The Path of Speech Technologies in Computer Assisted Language Learning, pages 174–194. Routledge, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bernstein</author>
<author>J De Jong</author>
<author>D Pisoni</author>
<author>B Townshend</author>
</authors>
<title>Two experiments on automatic scoring of spoken language proficiency.</title>
<date>2000</date>
<booktitle>In Proc. of STIL (Integrating Speech Technology in Learning),</booktitle>
<pages>57--61</pages>
<marker>Bernstein, De Jong, Pisoni, Townshend, 2000</marker>
<rawString>J. Bernstein, J. De Jong, D. Pisoni, and B. Townshend. 2000. Two experiments on automatic scoring of spoken language proficiency. In Proc. of STIL (Integrating Speech Technology in Learning), pages 57–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bernstein</author>
<author>A Van Moere</author>
<author>J Cheng</author>
</authors>
<title>Validating automated speaking tests.</title>
<date>2010</date>
<journal>Language Testing,</journal>
<volume>27</volume>
<issue>3</issue>
<marker>Bernstein, Van Moere, Cheng, 2010</marker>
<rawString>J. Bernstein, A. Van Moere, and J. Cheng. 2010. Validating automated speaking tests. Language Testing, 27(3):355–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Chen</author>
<author>J Mostow</author>
</authors>
<title>A tale of two tasks: Detecting children’s off-task speech in a reading tutor. In Interspeech</title>
<date>2011</date>
<pages>1621--1624</pages>
<contexts>
<context position="20420" citStr="Chen and Mostow, 2011" startWordPosition="3266" endWordPosition="3269"> For almost all items, the neural network model worked better. 4.8 Unscorable test detection Many factors can render a test unscorable: poor sound quality (recording noise, mouth too close to the microphone, too soft, etc.), gibberish (nonsense words, noise, or a foreign language), offtopic (off topic, but intelligible English), unintelligible English (e.g. a good-faith attempt to respond N 1 spectral_1 = N i=1 16 in English, but is so unintelligible and/or disfluent that it cannot be understood confidently). There have been several approaches to dealing with this issue (Cheng and Shen, 2011; Chen and Mostow, 2011; Yoon et al., 2011). Some unscorable tests can be identified easily by a human listener, and we reported research on a specified unscorable category (off-topic) before (Cheng and Shen, 2011). Dealing with a specified category could be significantly easier than dealing with wide-open items as in AZELLA. Also, because we did not collect human “unscorable&amp;quot; ratings for this data, we worked on predicting the absolute overall difference between human and machine scores; which is like predicting outliers. If the difference is expected to exceed a threshold, the test should be sent for human grading.</context>
</contexts>
<marker>Chen, Mostow, 2011</marker>
<rawString>W. Chen and J. Mostow. 2011. A tale of two tasks: Detecting children’s off-task speech in a reading tutor. In Interspeech 2011, pages 1621–1624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cheng</author>
<author>J Shen</author>
</authors>
<title>Towards accurate recognition for children’s oral reading fluency.</title>
<date>2010</date>
<booktitle>In IEEE-SLT 2010,</booktitle>
<pages>91--96</pages>
<contexts>
<context position="8556" citStr="Cheng and Shen, 2010" startWordPosition="1391" endWordPosition="1394">ent raters as the final score during machine learning. The responses in each validation set were double rated (producing two final scores) for use in validation. Note that five of the 1,367 tests in the validation sets had no human transcriptions and ratings, and so were excluded from the final validation results. 4 Machine scoring methods Previous research on automatic assessment of spoken responses can be found in Bernstein et al. (2000; 2010), Cheng (2011) and Higgins et al. (2011). Past work on automatic assessment of children’s oral reading fluency has been reported at the passage-level (Cheng and Shen, 2010; Downey et al., 2011) and at the word-level (Tepperman et al., 2007). A comprehensive review of spoken language technologies for education can be found in Eskanazi (2009). The following subsections summarize the methods we have used for scoring AZELLA tests. Those methods with citations have been previously discussed in research papers. Other methods described are novel modifications or extensions of known methods. Both the linguistic content and the manner of speaking are scored. Our machine scoring methods include a combination of automatic speech recognition (ASR), speech processing, stati</context>
<context position="10954" citStr="Cheng and Shen, 2010" startWordPosition="1775" endWordPosition="1778"> recordings, including material from students out of the age range being tested. For example, training acoustic models using only the Stage II transcriptions to recognize other Stage II responses was significantly improved by using more data from outside the Stage II data set, such as other AZELLA field data. We observed that the more child speech data, the better the automatic scoring. The final acoustic models used for recognition were trained on all transcribed AZELLA field data, except the data in the validation sets, plus data from an unrelated set of children’s oral reading of passages (Cheng and Shen, 2010), and the data collected during the construction of the Versant Junior English tests for use by young children in Asia (Bernstein and Cheng, 2007). Thus, the acoustic models were built using any and all relevant data available: totaling about 380 hours of data (or around 176, 000 responses). The word error rate (WER) over all the validation sets using the final acoustic models is around 35%. For machine scoring (after recognition and alignment), native acoustic models are used to compute native likelihoods of producing the observed base measures. Human listeners classified student recordings f</context>
</contexts>
<marker>Cheng, Shen, 2010</marker>
<rawString>J. Cheng and J. Shen. 2010. Towards accurate recognition for children’s oral reading fluency. In IEEE-SLT 2010, pages 91–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cheng</author>
<author>J Shen</author>
</authors>
<title>Off-topic detection in automated speech assessment applications. In Interspeech</title>
<date>2011</date>
<pages>1597--1600</pages>
<contexts>
<context position="18469" citStr="Cheng and Shen, 2011" startWordPosition="2959" endWordPosition="2962">n calculating spectral_1, all possible phonemes are included. We define another variable, spectral_2, that only accumulates the log-likelihood for a target set of phonemes that learners often have difficulty with. We call the percentage of phones from the all-phone recognition that match the phones from the forced alignment the percent phone match, or ppm. We take Formula 3 as the average log of the approximate posterior probabilities that phones were produced by a native. 4.6 Confidence modeling After finishing speech recognition, we can assign speech confidence scores to words and phonemes (Cheng and Shen, 2011). Then for every response, we can compute the average confidence, the percentage of words or phonemes whose confidences are lower than a threshold value as features to predict test-takers’ performance. 4.7 Final models AZELLA holistic score rubrics (Arizona Department of Education, 2012), such as those shown in Table 4, consider both the answer content and the manner of speaking used in the response. The automatic scoring should consider both too. Features word_vector, keywords, word_errors, percent_correct can represent content scores based on what is spoken. Features log_seg_prob, iw_log_seg</context>
<context position="20397" citStr="Cheng and Shen, 2011" startWordPosition="3262" endWordPosition="3265">ccurate for that item. For almost all items, the neural network model worked better. 4.8 Unscorable test detection Many factors can render a test unscorable: poor sound quality (recording noise, mouth too close to the microphone, too soft, etc.), gibberish (nonsense words, noise, or a foreign language), offtopic (off topic, but intelligible English), unintelligible English (e.g. a good-faith attempt to respond N 1 spectral_1 = N i=1 16 in English, but is so unintelligible and/or disfluent that it cannot be understood confidently). There have been several approaches to dealing with this issue (Cheng and Shen, 2011; Chen and Mostow, 2011; Yoon et al., 2011). Some unscorable tests can be identified easily by a human listener, and we reported research on a specified unscorable category (off-topic) before (Cheng and Shen, 2011). Dealing with a specified category could be significantly easier than dealing with wide-open items as in AZELLA. Also, because we did not collect human “unscorable&amp;quot; ratings for this data, we worked on predicting the absolute overall difference between human and machine scores; which is like predicting outliers. If the difference is expected to exceed a threshold, the test should be </context>
</contexts>
<marker>Cheng, Shen, 2011</marker>
<rawString>J. Cheng and J. Shen. 2011. Off-topic detection in automated speech assessment applications. In Interspeech 2011, pages 1597–1600.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cheng</author>
</authors>
<title>Automatic assessment of prosody in high-stakes English tests. In Interspeech</title>
<date>2011</date>
<pages>1589--1592</pages>
<contexts>
<context position="8399" citStr="Cheng (2011)" startWordPosition="1367" endWordPosition="1368">Department of Education, 2012), such as those in Table 4. Departing from usual practice in production settings, we used the average score from different raters as the final score during machine learning. The responses in each validation set were double rated (producing two final scores) for use in validation. Note that five of the 1,367 tests in the validation sets had no human transcriptions and ratings, and so were excluded from the final validation results. 4 Machine scoring methods Previous research on automatic assessment of spoken responses can be found in Bernstein et al. (2000; 2010), Cheng (2011) and Higgins et al. (2011). Past work on automatic assessment of children’s oral reading fluency has been reported at the passage-level (Cheng and Shen, 2010; Downey et al., 2011) and at the word-level (Tepperman et al., 2007). A comprehensive review of spoken language technologies for education can be found in Eskanazi (2009). The following subsections summarize the methods we have used for scoring AZELLA tests. Those methods with citations have been previously discussed in research papers. Other methods described are novel modifications or extensions of known methods. Both the linguistic con</context>
<context position="15139" citStr="Cheng, 2011" startWordPosition="2427" endWordPosition="2428">centage of words repeated correctly (percent_correct) was used as an additional feature. 4.4 Duration modeling Phone-level duration statistics contribute to machine scores of test-takers’ pronunciation and fluency. Native-speakers segment duration statistics from Versant Junior English tests (Bernstein and Cheng, 2007) were used to compute the log-likelihood of phone durations produced by test-takers. No data from AZELLA tests contributed to the duration models. We calculated the phoneme duration log-likelihood: log_seg_prob and the inter-word silence duration log-likelihood: iw_log_seg_prob (Cheng, 2011). Assume in a recognized response that the sequence of phonemes and their corresponding durations are pi and Di, i = 1...N, then the log likelihood segmental probability for phonemes (log_seg_prob) was computed as: 1 N−1log_seg prob = N − 2 Yi=2 where Pr(Di) was the probability that a native would produce phoneme pi with the observed duration Di in the context found. The first and last phonemes in the response were not used for the calculation of the log_seg_prob because durations of these phonemes as determined by the ASR were more likely to be incorrect. The log likelihood segmental probabil</context>
</contexts>
<marker>Cheng, 2011</marker>
<rawString>J. Cheng. 2011. Automatic assessment of prosody in high-stakes English tests. In Interspeech 2011, pages 1589–1592.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dahl</author>
<author>D Yu</author>
<author>L Deng</author>
<author>A Acero</author>
</authors>
<title>Context-dependent pretrained deep neural networks for large vocabulary speech recognition.</title>
<date>2012</date>
<booktitle>IEEE Transactions on Speech and Audio Processing, Special Issue on Deep Learning for Speech and Lang. Processing,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="32415" citStr="Dahl et al., 2012" startWordPosition="5304" endWordPosition="5307">ems. We are happy to report that the first operational AZELLA test with automatic speech scoring took place between January 14 and February 26, 2013, with approximately 140, 700 tests delivered. Recent progress in machine learning has applied deep neural networks (DNNs) to many long-standing pattern recognition and classification problems. Many groups have now applied DNNs to the task of building better acoustic models for speech recognition (Hinton et al., 2012). DNNs have repeatedly been shown to work better than Gaussian mixture models (GMMs) for ASR acoustic modeling (Hinton et al., 2012; Dahl et al., 2012). We are actively exploring the use of DNNs for use in recognition of children’s speech. We expect that DNN acoustic models can overcome some of the recognition difficulties mentioned in this paper (e.g. low SNR in responses and short response item types like Naming) and boost the final assessment accuracy significantly. 7 Conclusions We have reported an evaluation of the automatic methods that are currently used to assess spoken responses to test tasks that occur in Arizona’s AZELLA test for young English learners. The methods score both the content of the responses and the quality of the spe</context>
</contexts>
<marker>Dahl, Yu, Deng, Acero, 2012</marker>
<rawString>G. Dahl, D. Yu, L. Deng, and A. Acero. 2012. Context-dependent pretrained deep neural networks for large vocabulary speech recognition. IEEE Transactions on Speech and Audio Processing, Special Issue on Deep Learning for Speech and Lang. Processing, 20(1):30–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Downey</author>
<author>D Rubin</author>
<author>J Cheng</author>
<author>J Bernstein</author>
</authors>
<title>Performance of automated scoring for children’s oral reading.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>46--55</pages>
<contexts>
<context position="8578" citStr="Downey et al., 2011" startWordPosition="1395" endWordPosition="1398">l score during machine learning. The responses in each validation set were double rated (producing two final scores) for use in validation. Note that five of the 1,367 tests in the validation sets had no human transcriptions and ratings, and so were excluded from the final validation results. 4 Machine scoring methods Previous research on automatic assessment of spoken responses can be found in Bernstein et al. (2000; 2010), Cheng (2011) and Higgins et al. (2011). Past work on automatic assessment of children’s oral reading fluency has been reported at the passage-level (Cheng and Shen, 2010; Downey et al., 2011) and at the word-level (Tepperman et al., 2007). A comprehensive review of spoken language technologies for education can be found in Eskanazi (2009). The following subsections summarize the methods we have used for scoring AZELLA tests. Those methods with citations have been previously discussed in research papers. Other methods described are novel modifications or extensions of known methods. Both the linguistic content and the manner of speaking are scored. Our machine scoring methods include a combination of automatic speech recognition (ASR), speech processing, statistical modeling, lingu</context>
</contexts>
<marker>Downey, Rubin, Cheng, Bernstein, 2011</marker>
<rawString>R. Downey, D. Rubin, J. Cheng, and J. Bernstein. 2011. Performance of automated scoring for children’s oral reading. In Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 46–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Eskanazi</author>
</authors>
<title>An overview of spoken language technology for education.</title>
<date>2009</date>
<journal>Speech Communication,</journal>
<pages>51--832</pages>
<contexts>
<context position="8727" citStr="Eskanazi (2009)" startWordPosition="1420" endWordPosition="1421"> of the 1,367 tests in the validation sets had no human transcriptions and ratings, and so were excluded from the final validation results. 4 Machine scoring methods Previous research on automatic assessment of spoken responses can be found in Bernstein et al. (2000; 2010), Cheng (2011) and Higgins et al. (2011). Past work on automatic assessment of children’s oral reading fluency has been reported at the passage-level (Cheng and Shen, 2010; Downey et al., 2011) and at the word-level (Tepperman et al., 2007). A comprehensive review of spoken language technologies for education can be found in Eskanazi (2009). The following subsections summarize the methods we have used for scoring AZELLA tests. Those methods with citations have been previously discussed in research papers. Other methods described are novel modifications or extensions of known methods. Both the linguistic content and the manner of speaking are scored. Our machine scoring methods include a combination of automatic speech recognition (ASR), speech processing, statistical modeling, linguistics, word vectors, and machine learning. The speech processing technology was built to handle the different rhythms and varied pronunciations used</context>
</contexts>
<marker>Eskanazi, 2009</marker>
<rawString>M. Eskanazi. 2009. An overview of spoken language technology for education. Speech Communication, 51:832–844.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Franco</author>
<author>L Neumeyer</author>
<author>Y Kim</author>
<author>O Ronen</author>
</authors>
<title>Automatic pronunciation scoring for language instruction. In ICASSP</title>
<date>1997</date>
<pages>1471--1474</pages>
<contexts>
<context position="16668" citStr="Franco et al., 1997" startWordPosition="2670" endWordPosition="2673">ition alignment, computing the phone-level posterior probabilities given the acoustic observation X log(Pr(Di)), (1) 15 that is recognized as pi: P (X|pi)P (pi) P(pi|X) = Emk=1 P(X|pk)P(pk) (2) where k runs over all the potential phonemes. In a real-world ASR system, it is extremely difficult to estimate Emk=1 P(X|pk)P(pk) precisely. So approximations are used, such as substituting a maximum for the summation, etc. Formula 2 is the general framework for pronunciation diagnosis (Witt and Young, 1997; Franco et al., 1999; Witt and Young, 2000) and pronunciation assessment (Witt and Young, 2000; Franco et al., 1997; Neumeyer et al., 1999; Bernstein et al., 2010). Various authors use different approximations to suit the particulars of their data and their applications. In the AZELLA spectral scoring, we approximated Formula 2 with the following procedure. After the learner acoustic models produce a recognition result, we force-align the utterance on the recognized word string, but using the native monophone acoustic models, producing acoustic loglikelihood, duration and time boundaries for every phone. For each such phone, again using the native monophone time alignment, we perform an all-phone recogniti</context>
</contexts>
<marker>Franco, Neumeyer, Kim, Ronen, 1997</marker>
<rawString>H. Franco, L. Neumeyer, Y. Kim, and O. Ronen. 1997. Automatic pronunciation scoring for language instruction. In ICASSP 1997, pages 1471–1474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Franco</author>
<author>L Neumeyer</author>
<author>M Ramos</author>
<author>H Bratt</author>
</authors>
<title>Automatic detection of phone-level mispronunciation for language learning. In Eurospeech</title>
<date>1999</date>
<pages>851--854</pages>
<contexts>
<context position="16573" citStr="Franco et al., 1999" startWordPosition="2654" endWordPosition="2657">ood features with reference to native and learner segment-specific models applied to the recognition alignment, computing the phone-level posterior probabilities given the acoustic observation X log(Pr(Di)), (1) 15 that is recognized as pi: P (X|pi)P (pi) P(pi|X) = Emk=1 P(X|pk)P(pk) (2) where k runs over all the potential phonemes. In a real-world ASR system, it is extremely difficult to estimate Emk=1 P(X|pk)P(pk) precisely. So approximations are used, such as substituting a maximum for the summation, etc. Formula 2 is the general framework for pronunciation diagnosis (Witt and Young, 1997; Franco et al., 1999; Witt and Young, 2000) and pronunciation assessment (Witt and Young, 2000; Franco et al., 1997; Neumeyer et al., 1999; Bernstein et al., 2010). Various authors use different approximations to suit the particulars of their data and their applications. In the AZELLA spectral scoring, we approximated Formula 2 with the following procedure. After the learner acoustic models produce a recognition result, we force-align the utterance on the recognized word string, but using the native monophone acoustic models, producing acoustic loglikelihood, duration and time boundaries for every phone. For each</context>
</contexts>
<marker>Franco, Neumeyer, Ramos, Bratt, 1999</marker>
<rawString>H. Franco, L. Neumeyer, M. Ramos, and H. Bratt. 1999. Automatic detection of phone-level mispronunciation for language learning. In Eurospeech 1999, pages 851–854.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Higgins</author>
<author>X Xi</author>
<author>K Zechner</author>
<author>D Williamson</author>
</authors>
<title>A three-stage approach to the automated scoring of spontaneous spoken responses. Computer Speech and Language,</title>
<date>2011</date>
<pages>25--282</pages>
<contexts>
<context position="8425" citStr="Higgins et al. (2011)" startWordPosition="1370" endWordPosition="1373">cation, 2012), such as those in Table 4. Departing from usual practice in production settings, we used the average score from different raters as the final score during machine learning. The responses in each validation set were double rated (producing two final scores) for use in validation. Note that five of the 1,367 tests in the validation sets had no human transcriptions and ratings, and so were excluded from the final validation results. 4 Machine scoring methods Previous research on automatic assessment of spoken responses can be found in Bernstein et al. (2000; 2010), Cheng (2011) and Higgins et al. (2011). Past work on automatic assessment of children’s oral reading fluency has been reported at the passage-level (Cheng and Shen, 2010; Downey et al., 2011) and at the word-level (Tepperman et al., 2007). A comprehensive review of spoken language technologies for education can be found in Eskanazi (2009). The following subsections summarize the methods we have used for scoring AZELLA tests. Those methods with citations have been previously discussed in research papers. Other methods described are novel modifications or extensions of known methods. Both the linguistic content and the manner of spe</context>
</contexts>
<marker>Higgins, Xi, Zechner, Williamson, 2011</marker>
<rawString>D. Higgins, X. Xi, K. Zechner, and D. Williamson. 2011. A three-stage approach to the automated scoring of spontaneous spoken responses. Computer Speech and Language, 25:282–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
<author>L Deng</author>
<author>Y Dong</author>
<author>G Dahl</author>
<author>A Mohamed</author>
<author>N Jaitly</author>
<author>A Senior</author>
<author>V Vanhoucke</author>
<author>P Nguyen</author>
<author>T Sainath</author>
<author>B Kingsbury</author>
</authors>
<title>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups.</title>
<date>2012</date>
<journal>IEEE Signal Processing Magazine,</journal>
<volume>29</volume>
<issue>6</issue>
<contexts>
<context position="32264" citStr="Hinton et al., 2012" startWordPosition="5279" endWordPosition="5282">ng students to talk louder is probably something that can be best done at the testing site (by instruction or by example); and it may solve several problems. We are happy to report that the first operational AZELLA test with automatic speech scoring took place between January 14 and February 26, 2013, with approximately 140, 700 tests delivered. Recent progress in machine learning has applied deep neural networks (DNNs) to many long-standing pattern recognition and classification problems. Many groups have now applied DNNs to the task of building better acoustic models for speech recognition (Hinton et al., 2012). DNNs have repeatedly been shown to work better than Gaussian mixture models (GMMs) for ASR acoustic modeling (Hinton et al., 2012; Dahl et al., 2012). We are actively exploring the use of DNNs for use in recognition of children’s speech. We expect that DNN acoustic models can overcome some of the recognition difficulties mentioned in this paper (e.g. low SNR in responses and short response item types like Naming) and boost the final assessment accuracy significantly. 7 Conclusions We have reported an evaluation of the automatic methods that are currently used to assess spoken responses to te</context>
</contexts>
<marker>Hinton, Deng, Dong, Dahl, Mohamed, Jaitly, Senior, Vanhoucke, Nguyen, Sainath, Kingsbury, 2012</marker>
<rawString>G. Hinton, L. Deng, Y. Dong, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. 2012. Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaggle</author>
</authors>
<title>The Hewlett Foundation: Short answer scoring. http://www.kaggle.com/ c/asap-sas;http://www.kaggle.com/ c/asap-sas/details/winners.</title>
<date>2012</date>
<note>[Accessed 20-April-2014].</note>
<contexts>
<context position="13297" citStr="Kaggle, 2012" startWordPosition="2141" endWordPosition="2142">ore is calculated by scaling the weighted sum of the occurrence of a large set of expected words and word sequences available in an item-specific response scoring model. An automatic process assigned weights to the expected words and word sequences according to their semantic relation to known good responses using a method similar to latent semantic analysis (Landauer et al., 1998). The word_vector score is generally the most powerful feature used to predict the final human scores. Note that a recent competition to develop accurate scoring algorithms for student-written shortanswer responses (Kaggle, 2012) focused on a similar problem to the content scoring task for AZELLA open-ended responses. We assume that the methods used by the prize-winning teams, for example Tandalla (2012) and Zbontar (2012), should work well for the AZELLA open-ended material too, although we did not try these methods. For the responses to Naming, Read-bySyllables, and Read-Three-Words items, the machine scoring makes binary decisions based on the occurrence of a correct sequence of syllables or words (keywords). In Stage II forms, for first and second grade students, the responses to ReadThree-Words items were human-r</context>
</contexts>
<marker>Kaggle, 2012</marker>
<rawString>Kaggle. 2012. The Hewlett Foundation: Short answer scoring. http://www.kaggle.com/ c/asap-sas;http://www.kaggle.com/ c/asap-sas/details/winners. [Accessed 20-April-2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>Introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--259</pages>
<contexts>
<context position="13068" citStr="Landauer et al., 1998" startWordPosition="2105" endWordPosition="2108"> et al. (2012). Except for the four relatively closed-responseform items (Naming, Repeat, Read-by-Syllables and Read-Three-Words), we produced a word_vector score for each response (Bernstein et al., 2010). The value of the word_vector score is calculated by scaling the weighted sum of the occurrence of a large set of expected words and word sequences available in an item-specific response scoring model. An automatic process assigned weights to the expected words and word sequences according to their semantic relation to known good responses using a method similar to latent semantic analysis (Landauer et al., 1998). The word_vector score is generally the most powerful feature used to predict the final human scores. Note that a recent competition to develop accurate scoring algorithms for student-written shortanswer responses (Kaggle, 2012) focused on a similar problem to the content scoring task for AZELLA open-ended responses. We assume that the methods used by the prize-winning teams, for example Tandalla (2012) and Zbontar (2012), should work well for the AZELLA open-ended material too, although we did not try these methods. For the responses to Naming, Read-bySyllables, and Read-Three-Words items, t</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T. K. Landauer, P. W. Foltz, and D. Laham. 1998. Introduction to latent semantic analysis. Discourse Processes, 25:259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lee</author>
<author>A Potamianos</author>
<author>S Narayanan</author>
</authors>
<title>Acoustics of children’s speech: developmental changes of temporal and spectral parameters.</title>
<date>1999</date>
<journal>Journal of Acoustics Society of American,</journal>
<volume>105</volume>
<pages>1468</pages>
<contexts>
<context position="28645" citStr="Lee et al., 1999" startWordPosition="4685" endWordPosition="4688">des 3-5). Mean and standard deviation for human scores: (9.6,2.3). 18 Figure 4: Overall human vs. machine scores at the participant level for Stage IV (Grades 6-8). Mean and standard deviation for human scores: (8.3,2.9). Figure 5: Overall human vs. machine scores at the participant level for Stage V (Grades 9-12). Mean and standard deviation for human scores: (8.9,2.9). content of the response or rate its pronunciation. Since many young children in kindergarten and early elementary school speak softly, the youngest children’s speech is substantially harder to recognize (Li and Russell, 2002; Lee et al., 1999). This probably contributes to the lower reliabilities in Stage I and II. When setting the total rejection rate at 6%, our unscorable test detector identifies only 7 of the 13 outlier tests. Table 6: Reliability of human scores and HumanMachine correlations of overall test scores by stage. Stage H-H r M-H r I 0.91 0.88 II 0.96 0.90 III 0.97 0.94 IV 0.98 0.95 V 0.98 0.93 Average 0.96 0.92 Table 6 summarizes the reliabilities of the tests in different stages. At the participant level, the average inter-rater reliability coefficient across the five stages was 0.96, suggesting that the welltrained</context>
</contexts>
<marker>Lee, Potamianos, Narayanan, 1999</marker>
<rawString>S. Lee, A. Potamianos, and S. Narayanan. 1999. Acoustics of children’s speech: developmental changes of temporal and spectral parameters. Journal of Acoustics Society of American, 105:1455– 1468.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Li</author>
<author>M Russell</author>
</authors>
<title>An analysis of the causes of increased error rates in children’s speech recognition.</title>
<date>2002</date>
<booktitle>In ICSLP</booktitle>
<pages>2337--2340</pages>
<contexts>
<context position="28626" citStr="Li and Russell, 2002" startWordPosition="4681" endWordPosition="4684">vel for Stage III (Grades 3-5). Mean and standard deviation for human scores: (9.6,2.3). 18 Figure 4: Overall human vs. machine scores at the participant level for Stage IV (Grades 6-8). Mean and standard deviation for human scores: (8.3,2.9). Figure 5: Overall human vs. machine scores at the participant level for Stage V (Grades 9-12). Mean and standard deviation for human scores: (8.9,2.9). content of the response or rate its pronunciation. Since many young children in kindergarten and early elementary school speak softly, the youngest children’s speech is substantially harder to recognize (Li and Russell, 2002; Lee et al., 1999). This probably contributes to the lower reliabilities in Stage I and II. When setting the total rejection rate at 6%, our unscorable test detector identifies only 7 of the 13 outlier tests. Table 6: Reliability of human scores and HumanMachine correlations of overall test scores by stage. Stage H-H r M-H r I 0.91 0.88 II 0.96 0.90 III 0.97 0.94 IV 0.98 0.95 V 0.98 0.93 Average 0.96 0.92 Table 6 summarizes the reliabilities of the tests in different stages. At the participant level, the average inter-rater reliability coefficient across the five stages was 0.96, suggesting t</context>
</contexts>
<marker>Li, Russell, 2002</marker>
<rawString>Q. Li and M. Russell. 2002. An analysis of the causes of increased error rates in children’s speech recognition. In ICSLP 2002, pages 2337–2340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G N Masters</author>
</authors>
<title>A Rasch model for partial credit scoring.</title>
<date>1982</date>
<journal>Psychometrika,</journal>
<volume>47</volume>
<issue>2</issue>
<contexts>
<context position="30813" citStr="Masters, 1982" startWordPosition="5048" endWordPosition="5049">ng the Repeat items. Repeat items improve the machine re19 liability in Stage I significantly, but not so much for other stages. This difference may relate to the difficulty in eliciting sufficient speech samples in non-Repeat items from the young EL students in Stage I. Eliciting spoken materials in Repeat items is more straightforward. Consideration of Table 7 suggests that using only open-ended item-types can also achieve sufficiently reliable results. 6 Discussion and future work We believe that we can improve this system further by scoring Repeat items using a partial credit Rasch model (Masters, 1982) instead of the average of percent_correct, which should improve the reliability of the Repeat item type. We may also be able to train a better native acoustic model by using a larger sample of native data from AZELLA, if we are given access to the test-taker demographic information. The original item selection and assignment of items to forms was quite simple and had room for improvement. Currently in the AZELLA testing program, test forms go through a post-pilot revision, so that the operational tests only include good items in the final test forms. This post-pilot selection and arrangement </context>
</contexts>
<marker>Masters, 1982</marker>
<rawString>G. N. Masters. 1982. A Rasch model for partial credit scoring. Psychometrika, 47(2):149–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Neumeyer</author>
<author>H Franco</author>
<author>V Digalakis</author>
<author>M Weintraub</author>
</authors>
<title>Automatic scoring of pronunciation quality.</title>
<date>1999</date>
<journal>Speech Communication,</journal>
<pages>30--83</pages>
<contexts>
<context position="16691" citStr="Neumeyer et al., 1999" startWordPosition="2674" endWordPosition="2677">uting the phone-level posterior probabilities given the acoustic observation X log(Pr(Di)), (1) 15 that is recognized as pi: P (X|pi)P (pi) P(pi|X) = Emk=1 P(X|pk)P(pk) (2) where k runs over all the potential phonemes. In a real-world ASR system, it is extremely difficult to estimate Emk=1 P(X|pk)P(pk) precisely. So approximations are used, such as substituting a maximum for the summation, etc. Formula 2 is the general framework for pronunciation diagnosis (Witt and Young, 1997; Franco et al., 1999; Witt and Young, 2000) and pronunciation assessment (Witt and Young, 2000; Franco et al., 1997; Neumeyer et al., 1999; Bernstein et al., 2010). Various authors use different approximations to suit the particulars of their data and their applications. In the AZELLA spectral scoring, we approximated Formula 2 with the following procedure. After the learner acoustic models produce a recognition result, we force-align the utterance on the recognized word string, but using the native monophone acoustic models, producing acoustic loglikelihood, duration and time boundaries for every phone. For each such phone, again using the native monophone time alignment, we perform an all-phone recognition using the native mon</context>
</contexts>
<marker>Neumeyer, Franco, Digalakis, Weintraub, 1999</marker>
<rawString>L. Neumeyer, H. Franco, V. Digalakis, and M. Weintraub. 1999. Automatic scoring of pronunciation quality. Speech Communication, 30:83–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Tandalla</author>
</authors>
<title>ASAP Short Answer Scoring Competition System Description: Scoring short answer essays. https: //kaggle2.blob.core.windows.net/ competitions/kaggle/2959/media/ TechnicalMethodsPaper.pdf.</title>
<date>2012</date>
<note>[Accessed 20-April-2014].</note>
<contexts>
<context position="13475" citStr="Tandalla (2012)" startWordPosition="2169" endWordPosition="2170">omatic process assigned weights to the expected words and word sequences according to their semantic relation to known good responses using a method similar to latent semantic analysis (Landauer et al., 1998). The word_vector score is generally the most powerful feature used to predict the final human scores. Note that a recent competition to develop accurate scoring algorithms for student-written shortanswer responses (Kaggle, 2012) focused on a similar problem to the content scoring task for AZELLA open-ended responses. We assume that the methods used by the prize-winning teams, for example Tandalla (2012) and Zbontar (2012), should work well for the AZELLA open-ended material too, although we did not try these methods. For the responses to Naming, Read-bySyllables, and Read-Three-Words items, the machine scoring makes binary decisions based on the occurrence of a correct sequence of syllables or words (keywords). In Stage II forms, for first and second grade students, the responses to ReadThree-Words items were human-rated in four categories. For this stage, the machine counted the number of words read correctly. For the responses to Repeat items, the recognized string is compared to the word </context>
</contexts>
<marker>Tandalla, 2012</marker>
<rawString>L. Tandalla. 2012. ASAP Short Answer Scoring Competition System Description: Scoring short answer essays. https: //kaggle2.blob.core.windows.net/ competitions/kaggle/2959/media/ TechnicalMethodsPaper.pdf. [Accessed 20-April-2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tepperman</author>
<author>M Black</author>
<author>P Price</author>
<author>S Lee</author>
<author>A Kazemzadeh</author>
<author>M Gerosa</author>
<author>M Heritage</author>
<author>A Alwan</author>
<author>S Narayanan</author>
</authors>
<title>A Bayesian network classifier for word-level reading assessment. In Interspeech</title>
<date>2007</date>
<pages>2185--2188</pages>
<contexts>
<context position="8625" citStr="Tepperman et al., 2007" startWordPosition="1403" endWordPosition="1406">es in each validation set were double rated (producing two final scores) for use in validation. Note that five of the 1,367 tests in the validation sets had no human transcriptions and ratings, and so were excluded from the final validation results. 4 Machine scoring methods Previous research on automatic assessment of spoken responses can be found in Bernstein et al. (2000; 2010), Cheng (2011) and Higgins et al. (2011). Past work on automatic assessment of children’s oral reading fluency has been reported at the passage-level (Cheng and Shen, 2010; Downey et al., 2011) and at the word-level (Tepperman et al., 2007). A comprehensive review of spoken language technologies for education can be found in Eskanazi (2009). The following subsections summarize the methods we have used for scoring AZELLA tests. Those methods with citations have been previously discussed in research papers. Other methods described are novel modifications or extensions of known methods. Both the linguistic content and the manner of speaking are scored. Our machine scoring methods include a combination of automatic speech recognition (ASR), speech processing, statistical modeling, linguistics, word vectors, and machine learning. The</context>
</contexts>
<marker>Tepperman, Black, Price, Lee, Kazemzadeh, Gerosa, Heritage, Alwan, Narayanan, 2007</marker>
<rawString>J. Tepperman, M. Black, P. Price, S. Lee, A. Kazemzadeh, M. Gerosa, M. Heritage, A. Alwan, and S. Narayanan. 2007. A Bayesian network classifier for word-level reading assessment. In Interspeech 2007, pages 2185–2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Witt</author>
<author>S J Young</author>
</authors>
<title>Language learning based on non-native speech recognition. In Eurospeech</title>
<date>1997</date>
<pages>633--636</pages>
<contexts>
<context position="16552" citStr="Witt and Young, 1997" startWordPosition="2650" endWordPosition="2653">veral spectral likelihood features with reference to native and learner segment-specific models applied to the recognition alignment, computing the phone-level posterior probabilities given the acoustic observation X log(Pr(Di)), (1) 15 that is recognized as pi: P (X|pi)P (pi) P(pi|X) = Emk=1 P(X|pk)P(pk) (2) where k runs over all the potential phonemes. In a real-world ASR system, it is extremely difficult to estimate Emk=1 P(X|pk)P(pk) precisely. So approximations are used, such as substituting a maximum for the summation, etc. Formula 2 is the general framework for pronunciation diagnosis (Witt and Young, 1997; Franco et al., 1999; Witt and Young, 2000) and pronunciation assessment (Witt and Young, 2000; Franco et al., 1997; Neumeyer et al., 1999; Bernstein et al., 2010). Various authors use different approximations to suit the particulars of their data and their applications. In the AZELLA spectral scoring, we approximated Formula 2 with the following procedure. After the learner acoustic models produce a recognition result, we force-align the utterance on the recognized word string, but using the native monophone acoustic models, producing acoustic loglikelihood, duration and time boundaries for </context>
</contexts>
<marker>Witt, Young, 1997</marker>
<rawString>S. M. Witt and S. J. Young. 1997. Language learning based on non-native speech recognition. In Eurospeech 1997, pages 633–636.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S M Witt</author>
<author>S J Young</author>
</authors>
<title>Phone-level pronunciation scoring and assessment for interactive language learning.</title>
<date>2000</date>
<journal>Speech Communication,</journal>
<volume>30</volume>
<pages>108</pages>
<contexts>
<context position="16596" citStr="Witt and Young, 2000" startWordPosition="2658" endWordPosition="2661">erence to native and learner segment-specific models applied to the recognition alignment, computing the phone-level posterior probabilities given the acoustic observation X log(Pr(Di)), (1) 15 that is recognized as pi: P (X|pi)P (pi) P(pi|X) = Emk=1 P(X|pk)P(pk) (2) where k runs over all the potential phonemes. In a real-world ASR system, it is extremely difficult to estimate Emk=1 P(X|pk)P(pk) precisely. So approximations are used, such as substituting a maximum for the summation, etc. Formula 2 is the general framework for pronunciation diagnosis (Witt and Young, 1997; Franco et al., 1999; Witt and Young, 2000) and pronunciation assessment (Witt and Young, 2000; Franco et al., 1997; Neumeyer et al., 1999; Bernstein et al., 2010). Various authors use different approximations to suit the particulars of their data and their applications. In the AZELLA spectral scoring, we approximated Formula 2 with the following procedure. After the learner acoustic models produce a recognition result, we force-align the utterance on the recognized word string, but using the native monophone acoustic models, producing acoustic loglikelihood, duration and time boundaries for every phone. For each such phone, again usin</context>
</contexts>
<marker>Witt, Young, 2000</marker>
<rawString>S. M. Witt and S. J. Young. 2000. Phone-level pronunciation scoring and assessment for interactive language learning. Speech Communication, 30:95– 108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Xie</author>
<author>K Evanini</author>
<author>K Zechner</author>
</authors>
<title>Exploring content features for automated speech scoring.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>103--111</pages>
<contexts>
<context position="12460" citStr="Xie et al. (2012)" startWordPosition="2013" endWordPosition="2016">oximately 66 hours of speech data, or 39, 000 responses). 14 4.2 Language models Item-specific bigram language models were built using the human transcription of the developmentset as described in Section 3.1. 4.3 Content modeling &amp;quot;Content&amp;quot; refers to the linguistic material (words, phrases, and semantic elements) in the spoken response. Appropriate response content reflects the speaker’s productive control of English vocabulary and also indicates how well the test-taker understood the prompt. Previous work on scoring linguistic content in the speech domain includes Bernstein et al. (2010) and Xie et al. (2012). Except for the four relatively closed-responseform items (Naming, Repeat, Read-by-Syllables and Read-Three-Words), we produced a word_vector score for each response (Bernstein et al., 2010). The value of the word_vector score is calculated by scaling the weighted sum of the occurrence of a large set of expected words and word sequences available in an item-specific response scoring model. An automatic process assigned weights to the expected words and word sequences according to their semantic relation to known good responses using a method similar to latent semantic analysis (Landauer et al</context>
</contexts>
<marker>Xie, Evanini, Zechner, 2012</marker>
<rawString>S. Xie, K. Evanini, and K. Zechner. 2012. Exploring content features for automated speech scoring. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 103–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-Y Yoon</author>
<author>K Evanini</author>
<author>K Zechner</author>
</authors>
<title>Nonscorable response detection for automated speaking proficiency assessment.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>152--160</pages>
<contexts>
<context position="20440" citStr="Yoon et al., 2011" startWordPosition="3270" endWordPosition="3273">the neural network model worked better. 4.8 Unscorable test detection Many factors can render a test unscorable: poor sound quality (recording noise, mouth too close to the microphone, too soft, etc.), gibberish (nonsense words, noise, or a foreign language), offtopic (off topic, but intelligible English), unintelligible English (e.g. a good-faith attempt to respond N 1 spectral_1 = N i=1 16 in English, but is so unintelligible and/or disfluent that it cannot be understood confidently). There have been several approaches to dealing with this issue (Cheng and Shen, 2011; Chen and Mostow, 2011; Yoon et al., 2011). Some unscorable tests can be identified easily by a human listener, and we reported research on a specified unscorable category (off-topic) before (Cheng and Shen, 2011). Dealing with a specified category could be significantly easier than dealing with wide-open items as in AZELLA. Also, because we did not collect human “unscorable&amp;quot; ratings for this data, we worked on predicting the absolute overall difference between human and machine scores; which is like predicting outliers. If the difference is expected to exceed a threshold, the test should be sent for human grading. Many problems were </context>
</contexts>
<marker>Yoon, Evanini, Zechner, 2011</marker>
<rawString>S.-Y. Yoon, K. Evanini, and K. Zechner. 2011. Nonscorable response detection for automated speaking proficiency assessment. In Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 152–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Young</author>
<author>D Kershaw</author>
<author>J Odell</author>
<author>D Ollason</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<date>2000</date>
<booktitle>The HTK Book Version 3.0.</booktitle>
<institution>Cambridge University,</institution>
<location>Cambridge, England.</location>
<contexts>
<context position="10265" citStr="Young et al., 2000" startWordPosition="1661" endWordPosition="1664">in which it is said. Thus, we derive scores based on the words used, as well as the pace, fluency, and pronunciation of those words in phrases and sentences. For each response, base measures are then derived from the linguistic units (segments, syllables, words), with reference to statistical models built from the spoken performances of natives and learners. Except for the Repeat items, the system produces only one holistic score per item from a combination of base measures. 4.1 Acoustic models We tried various sets of recorded responses to train GMM-HMM acoustic models as implemented in HTK (Young et al., 2000). Performance improved by training acoustic models on larger sets of recordings, including material from students out of the age range being tested. For example, training acoustic models using only the Stage II transcriptions to recognize other Stage II responses was significantly improved by using more data from outside the Stage II data set, such as other AZELLA field data. We observed that the more child speech data, the better the automatic scoring. The final acoustic models used for recognition were trained on all transcribed AZELLA field data, except the data in the validation sets, plus</context>
</contexts>
<marker>Young, Kershaw, Odell, Ollason, Valtchev, Woodland, 2000</marker>
<rawString>S. J. Young, D. Kershaw, J. Odell, D. Ollason, V. Valtchev, and P. Woodland. 2000. The HTK Book Version 3.0. Cambridge University, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Zbontar</author>
</authors>
<title>ASAP Short Answer Scoring Competition System Description: Short answer scoring by stacking.</title>
<date>2012</date>
<note>https://kaggle2.blob.core. windows.net/competitions/kaggle/</note>
<contexts>
<context position="13494" citStr="Zbontar (2012)" startWordPosition="2172" endWordPosition="2173">ned weights to the expected words and word sequences according to their semantic relation to known good responses using a method similar to latent semantic analysis (Landauer et al., 1998). The word_vector score is generally the most powerful feature used to predict the final human scores. Note that a recent competition to develop accurate scoring algorithms for student-written shortanswer responses (Kaggle, 2012) focused on a similar problem to the content scoring task for AZELLA open-ended responses. We assume that the methods used by the prize-winning teams, for example Tandalla (2012) and Zbontar (2012), should work well for the AZELLA open-ended material too, although we did not try these methods. For the responses to Naming, Read-bySyllables, and Read-Three-Words items, the machine scoring makes binary decisions based on the occurrence of a correct sequence of syllables or words (keywords). In Stage II forms, for first and second grade students, the responses to ReadThree-Words items were human-rated in four categories. For this stage, the machine counted the number of words read correctly. For the responses to Repeat items, the recognized string is compared to the word string recited in t</context>
</contexts>
<marker>Zbontar, 2012</marker>
<rawString>J. Zbontar. 2012. ASAP Short Answer Scoring Competition System Description: Short answer scoring by stacking. https://kaggle2.blob.core. windows.net/competitions/kaggle/</rawString>
</citation>
<citation valid="false">
<date></date>
<note>[Accessed 20-April-2014].</note>
<marker></marker>
<rawString>2959/media/jzbontar.pdf. [Accessed 20-April-2014].</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
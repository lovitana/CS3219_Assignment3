<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000168">
<title confidence="0.987336">
Using statistical parsing to detect agrammatic aphasia
</title>
<author confidence="0.954161">
Kathleen C. Fraser1, Graeme Hirst1, Jed A. Meltzer2,
</author>
<affiliation confidence="0.924224666666667">
Jennifer E. Mack3, and Cynthia K. Thompson3,4,5
1Dept. of Computer Science, University of Toronto
2Rotman Research Institute, Baycrest Centre, Toronto
3Dept. of Communication Sciences and Disorders, Northwestern University
4Dept. of Neurology, Northwestern University
4Cognitive Neurology and Alzheimer’s Disease Center, Northwestern University
</affiliation>
<email confidence="0.985668">
{kfraser,gh}@cs.toronto.edu, jmeltzer@research.baycrest.org
{jennifer-mack-0,ckthom}@northwestern.edu
</email>
<sectionHeader confidence="0.993628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969619047619">
Agrammatic aphasia is a serious language
impairment which can occur after a stroke
or traumatic brain injury. We present
an automatic method for analyzing apha-
sic speech using surface level parse fea-
tures and context-free grammar produc-
tion rules. Examining these features in-
dividually, we show that we can uncover
many of the same characteristics of agram-
matic language that have been reported
in studies using manual analysis. When
taken together, these parse features can
be used to train a classifier to accurately
predict whether or not an individual has
aphasia. Furthermore, we find that the
parse features can lead to higher classifica-
tion accuracies than traditional measures
of syntactic complexity. Finally, we find
that a minimal amount of pre-processing
can lead to better results than using either
the raw data or highly processed data.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969732142857">
After a stroke or head injury, individuals may
experience aphasia, an impairment in the ability
to comprehend or produce language. The type
of aphasia depends on the location of the lesion.
However, even two patients with the same type
of aphasia may experience different symptoms. A
careful analysis of narrative speech can reveal spe-
cific patterns of impairment, and help a clinician
determine whether an individual has aphasia, what
type of aphasia it is, and how the symptoms are
changing over time.
In this paper, we present an automatic method
for the analysis of one type of aphasia, agram-
matic aphasia. characterized by the omission of
function words, the omission or substitution of
morphological markers for person and number, the
absence of verb inflection, and a relative increase
in the number of nouns and decrease in the number
of verbs (Bastiaanse and Thompson, 2012). There
is often a reduction in the variety of different syn-
tactic structures used, as well as a reduction in the
complexity of those structures (Progovac, 2006).
There may also be a strong tendency to use the
canonical word order of a language, for example
subject-verb-object in English (Progovac, 2006).
Most studies of narrative speech in agrammatic
aphasia are based on manually annotated speech
transcripts. This type of analysis can provide de-
tailed and accurate information about the speech
patterns that are observed. However, it is also very
time consuming and requires trained transcribers
and annotators. Studies are necessarily limited to
a manageable size, and the level of agreement be-
tween annotators can vary.
We propose an automatic approach that uses in-
formation from statistical parsers to examine prop-
erties of narrative speech. We extract context-
free grammar (CFG) production rules as well as
phrase-level features from syntactic parses of the
speech transcripts. We show that this approach can
detect many features which have been previously
reported in the aphasia literature, and that classifi-
cation of agrammatic patients and controls can be
achieved with high accuracy.
We also examine the effects of including speech
dysfluencies in the transcripts. Dysfluencies and
non-narrative words are usually removed from the
transcripts as a pre-processing step, but we show
that by retaining some of these items, we can ac-
tually achieve a higher classification accuracy than
by using the completely clean transcripts.
Finally, we investigate whether there is any ben-
efit to using the parse features instead of more tra-
ditional measures of syntactic complexity, such as
Yngve depth or mean sentence length. We find
that the parse features convey more information
</bodyText>
<page confidence="0.979751">
134
</page>
<note confidence="0.7881005">
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 134–142,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.997021">
about the specific syntactic structures being pro-
duced (or avoided) by the agrammatic speakers,
and lead to better classification accuracies.
</bodyText>
<sectionHeader confidence="0.999661" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.88079">
2.1 Syntactic analysis of agrammatic
narrative speech
</subsectionHeader>
<bodyText confidence="0.999985521739131">
Much of the previous work analyzing narrative
speech in agrammatic aphasia has been performed
manually. One widely used protocol is called
Quantitative Production Analysis (QPA), devel-
oped by Saffran et al. (1989). QPA can be used to
measure morphological content, such as whether
determiners and verb inflections are produced in
obligatory contexts, as well as structural complex-
ity, such as the number of embedded clauses per
sentence. Subsequent studies have found a num-
ber of differences between normal and agrammatic
speech using QPA (Rochon et al., 2000). Another
popular protocol called the Northwestern Narra-
tive Language Analysis (NNLA) was introduced
by Thompson et al. (1995). This protocol analyzes
each utterance at five different levels, and focuses
in particular on the production of verbs and verb
argument structure.
Perhaps more analogous to our work here,
Goodglass et al. (1994) conducted a detailed ex-
amination of the syntactic constituents used by
aphasic patients and controls. In that study, utter-
ances were grouped according to how many syn-
tactic constituents they contained. They found
that agrammatic participants were more likely to
produce single-constituent utterances, especially
noun phrases, and less likely to produce subor-
dinate clauses. They also found that agrammatic
speakers sometimes produced two-constituent ut-
terances consisting of only a subject and object,
with no verb. This pattern was never observed in
control speech.
A much smaller body of work explores the use
of computational techniques to analyze agramma-
tism. Holmes and Singh (1996) analyzed conver-
sational speech from aphasic speakers and con-
trols. Their features mostly included measures
of vocabulary richness and frequency counts of
various parts-of-speech (e.g. nouns, verbs); how-
ever they also measured “clause-like semantic unit
rate”. This feature was intended to measure the
speaker’s ability to cluster words together, al-
though it is not clear what the criteria for segment-
ing clause-like units were or whether it was done
manually or automatically. Nonetheless, it was
found to be one of the most important variables
for distinguishing between patients and controls.
MacWhinney et al. (2011) presented several ex-
amples of how researchers can use the Aphasia-
Bank1 database and associated software tools to
conduct automatic analyses (although the tran-
scripts are first hand-coded for errors by experi-
enced speech-language pathologists). Specifically
with regards to syntax, they calculated several fre-
quency counts and ratios for different parts-of-
speech and bound morphemes. There was one
extension beyond treating each word individually:
this involved searching for pre-defined colloca-
tions such as once upon a time or happily ever af-
ter, which were found to occur more rarely in the
patient transcripts than in the control transcripts.
We present an alternative, automated method of
analysis. We do not attempt to fully replicate the
results of the manual studies, but rather provide
a complementary set of features which can indi-
cate grammatic abnormalities. Unlike previous
computational studies, we attempt to move beyond
single-word analysis and examine which patterns
of syntax might indicate agrammatism.
</bodyText>
<subsectionHeader confidence="0.923139">
2.2 Using parse features to assess
grammaticality
</subsectionHeader>
<bodyText confidence="0.999838727272727">
Syntactic complexity metrics derived from parse
trees have been used by various researchers in
studies of mild cognitive impairment (Roark et al.,
2011), autism (Prud’hommeaux et al., 2011), and
child language development (Sagae et al., 2005;
Hassanali et al., 2013). Here we focus specifically
on the use of CFG production rules as features.
Using the CFG production rules from statistical
parsers as features was first proposed by Baayen
et al. (1996), who applied the features to an au-
thorship attribution task. More recently, similar
features have been widely used in native language
identification (Wong and Dras, 2011; Brooke and
Hirst, 2012; Swanson and Charniak, 2012). Per-
haps most relevant to the task at hand, CFG pro-
ductions as well as other parse outputs have proved
useful for judging the grammaticality and fluency
of sentences. For example, Wong and Dras (2010)
used CFG productions to classify sentences from
an artificial error corpus as being either grammat-
ical or ungrammatical.
Taking a different approach, Chae and Nenkova
</bodyText>
<footnote confidence="0.991459">
1http://talkbank.org/AphasiaBank/
</footnote>
<page confidence="0.991431">
135
</page>
<table confidence="0.7578096">
Agrammatic Control
(N = 24) (N = 15)
Male/Female 15/9 8/7
Age (years) 58.1 (10.6) 63.3 (6.4)
Education (years) 16.3 (2.5) 16.4 (2.4)
</table>
<tableCaption confidence="0.999223">
Table 1: Demographic information. Numbers are
</tableCaption>
<bodyText confidence="0.9608058">
given in the form: mean (standard deviation).
(2009) calculated several surface features based on
the output of a parser, such as the length and rel-
ative proportion of different phrase types. They
used these features to distinguish between human
and machine translations, and to determine which
of a pair of translations was the more fluent. How-
ever, to our knowledge there has been no work us-
ing parser outputs to assess the grammaticality of
speech from individuals with post-stroke aphasia.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="method">
3 Data
</sectionHeader>
<subsectionHeader confidence="0.999413">
3.1 Participants
</subsectionHeader>
<bodyText confidence="0.999938777777778">
This was a retrospective analysis of data col-
lected by the the Aphasia and Neurolinguistics Re-
search Laboratory at Northwestern University. All
agrammatic participants had experienced a stroke
at least 1 year prior to the narrative sample col-
lection. Demographic information for the partic-
ipants is given in Table 1. There is no significant
(p &lt; 0.05) difference between the patient and con-
trol groups on age or level of education.
</bodyText>
<subsectionHeader confidence="0.999137">
3.2 Narrative task
</subsectionHeader>
<bodyText confidence="0.999991333333333">
To obtain a narrative sample, the participants were
asked to relate the well-known fairy tale Cin-
derella. Each participant was first given a word-
less picture book of the story to look through. The
book was then removed, and the participant was
asked to tell the story in his or her own words. The
examiner did not interrupt or ask questions.
The narratives were recorded and later tran-
scribed following the NNLA protocol. The data
was segmented into utterances based on syntac-
tic and prosodic cues. Filled pauses, repetitions,
false starts, and revisional phrases (e.g. I mean)
were all placed inside parentheses. The average
length of the raw transcripts was 332 words for
agrammatic participants and 387 words for con-
trols; when the non-narrative words were excluded
the average length was 194 words for the agram-
matic group and 330 for controls.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="method">
4 Methods
</sectionHeader>
<subsectionHeader confidence="0.999649">
4.1 Parser Features
</subsectionHeader>
<bodyText confidence="0.998460130434783">
We consider two types of features: CFG pro-
duction rules and phrase-level statistics. For the
CFG production rules, we use the Charniak parser
(Charniak, 2000) trained on Wall Street Journal
data to parse each utterance in the transcript and
then extract the set of non-lexical productions.
The total number of types of productions is large,
many of them occurring very infrequently, so we
compile a list of the 50 most frequently occurring
productions in each of the two groups (agrammatic
and controls) and use the combined set as the set
of features. The feature values can be binary (does
a particular production rule appear in the narrative
or not?) or integer (how many times does a rule oc-
cur?). The CFG non-terminal symbols follow the
Penn Treebank naming conventions.
For our phrase-level statistics, we use a subset
of the features described by Chae and Nenkova
(2009), which are related to the incidence of dif-
ferent phrase types. We consider three different
phrase types: noun phrases, verb phrases, and
prepositional phrases. These features are defined
as follows:
</bodyText>
<listItem confidence="0.940294625">
• Phrase type proportion: Length of each
phrase type (including embedded phrases),
divided by total narrative length.
• Average phrase length: Total number of
words in a phrase type, divided by number
of phrases of that type.
• Phrase type rate: Number of phrases of a
given type, divided by total narrative length.
</listItem>
<bodyText confidence="0.999979529411765">
Because we are judging the grammaticality of
the entire narrative, we normalize by narrative
length (rather than sentence length, as in Chae and
Nenkova’s study). These features are real-valued.
We first perform the analysis on the transcribed
data with the dysfluencies removed, labeled the
“clean” dataset. This is the version of the tran-
script that would be used in the manual NNLA
analysis. However, it is the result of human ef-
fort and expertise. To test the robustness of the
system on data that has not been annotated in this
way, we also use the “raw” dataset, with no dys-
fluencies removed (i.e. including everything inside
the parentheses), and an “auto-cleaned” dataset,
in which filled pauses are automatically removed
from the raw transcripts. We also use a simple al-
gorithm to remove “stutters” and false starts, by
</bodyText>
<page confidence="0.994193">
136
</page>
<bodyText confidence="0.999641666666667">
removing non-word tokens of length one or two
(e.g. C- C- Cinderella would become simply Cin-
derella). This provides a more realistic view of the
performance of our system on real data. We also
hypothesize that there may be important informa-
tion to be found in the dysfluent speech segments.
</bodyText>
<subsectionHeader confidence="0.999586">
4.2 Feature weighting and selection
</subsectionHeader>
<bodyText confidence="0.997769142857143">
We assume that some production rules will be
more relevant to the classification than others,
and so we want to weight the features accord-
ingly. Using term frequency–inverse document
frequency (tf-idf) would be one possibility; how-
ever, the tf-idf weights do not take into account
any class information. Supervised term weight-
ing (STW), has been proposed by Debole and Se-
bastiani (2004) as an alternative to tf-idf for text
classification tasks. In this weighting scheme, fea-
ture weights are assigned using the same algo-
rithm that is used for feature selection. For ex-
ample, one way to select features is to rank them
by their information gain (InfoGain). In STW,
the InfoGain value for each feature is also used
to replace the idf term. This can be expressed as
W(i,d) = df(i,d) × InfoGain(i), where W(i,d) is
the weight assigned to feature i in document d,
df(i,d) is the frequency of occurrence of feature i
in document d, and InfoGain(i) is the information
gain of feature i across all the training documents.
We considered two different methods of STW:
weighting by InfoGain and weighting by gain ratio
(GainRatio). The methods were also used as fea-
ture selection, since any feature that was assigned
a weight of zero was removed from the classifi-
cation. We also consider tf-idf weights and un-
weighted features for comparison.
</bodyText>
<subsectionHeader confidence="0.998904">
4.3 Syntactic complexity metrics
</subsectionHeader>
<bodyText confidence="0.999894454545455">
To compare the performance of the parse features
with more-traditional syntactic complexity met-
rics (SC metrics), we calculate the mean length of
utterance (MLU), mean length of T-unit2 (MLT),
mean length of clause (MLC), and parse tree
height. We also calculate the mean, maximum,
and total Yngve depth, which measures the pro-
portion of left-branching to right-branching in
each parse tree (Yngve, 1960). These measures
are commonly used in studies of impaired lan-
guage (e.g. Roark et al. (2011), Prud’hommeaux et
</bodyText>
<footnote confidence="0.9658815">
2A T-unit consists of a main clause and its attached de-
pendent clauses.
</footnote>
<bodyText confidence="0.947881833333333">
al. (2011), Fraser et al. (2013b)). We hypothesize
that the parse features will capture more informa-
tion about the specific impairments seen in agram-
matic aphasia; however, using the general mea-
sures of syntactic complexity may be sufficient for
the classifiers to distinguish between the groups.
</bodyText>
<subsectionHeader confidence="0.982296">
4.4 Classification
</subsectionHeader>
<bodyText confidence="0.999996304347826">
To test whether the features can effectively distin-
guish between the agrammatic group and controls,
we use them to train and test a machine learn-
ing classifier. We test three different classifica-
tion algorithms: naive Bayes (NB), support vec-
tor machine (SVM), and random forests (RF). We
use a leave-one-out cross-validation framework, in
which one transcript is held out as a test set, and
the other transcripts form the training data. The
feature weights are calculated on the training set
and then applied to the test set (as a result, each
fold of training/testing may use different features
and feature weights). The SVM and RF algo-
rithms are tuned in a nested cross-validation loop.
The classifier is then tested on the held-out point.
This procedure is repeated across all data points,
and the average accuracy is reported.
A baseline classifier which assigns all data to
the largest class would achieve an accuracy of .62
on this classification task. For a more realistic
measure of performance, we also compare our re-
sults to the baseline accuracy that can be achieved
using only the length of the narrative as input.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.994869">
5.1 Features using clean transcripts
</subsectionHeader>
<bodyText confidence="0.999947764705882">
We first present the results for the clean tran-
scripts. Although different features may be se-
lected in each fold of the cross-validation, for sim-
plicity we show only the feature rankings on the
whole data set. Table 2 shows the top features as
ranked by GainRatio. The frequencies are given to
indicate the direction of the trend; they represent
the average frequency per narrative for each class
(agrammatic = AG and control = CT). Boldface
indicates the group with the higher frequency. As-
terisks are used to indicate the significance of the
difference between the groups.
When working with clinical data, careful exam-
ination of the features can be beneficial. By com-
paring features with previous findings in the liter-
ature on agrammatism, we can be confident that
we are measuring real effects and not just artifacts
</bodyText>
<page confidence="0.977515">
137
</page>
<table confidence="0.999942636363636">
Rule AG CT p
freq freq
1 PP → IN NP 10.3 24.9 ∗∗∗
2 ROOT → NP 2.9 0.2 ∗∗∗
3 NP → DT NN POS 0.0 0.7 ∗
4 NP → PRP$ JJ NN 0.5 0.7 ∗
5 VP → TO VP 4.2 7.5 ∗
6 NP → NNP 5.9 6.6
7 VP → VB PP 1.1 2.9 ∗∗
8 VP → VP CC VP 1.1 3.1 ∗∗
9 NP → DT NN NN 1.0 2.7 ∗∗
10 VP → VBD VP 0.1 0.5 ∗
11 WHADVP → WRB 0.5 1.4 ∗
12 FRAG → NP . 0.7 0.0 ∗∗
13 NP → JJ NN 0.7 0.0 ∗∗
14 SBAR → WHNP S 1.7 3.1 ∗
15 NP → NP SBAR 1.6 2.5
16 S → NP VP 7.8 16.1 ∗∗
17 NP → PRP$ JJ NNS 0.0 0.5 ∗
18 NP → PRP$ NN NNS 0.0 0.6 ∗
19 SBAR → WHADVP S 0.4 1.2 ∗
20 VP → VBN PP 0.4 2.0 ∗
</table>
<tableCaption confidence="0.990093">
Table 2: Top 20 features ranked by GainRatio us-
</tableCaption>
<bodyText confidence="0.9692672">
ing the clean transcripts. (∗p &lt; 0.05, ∗∗p &lt; 0.005,
∗∗∗p &lt; 0.0005).
of the parsing algorithm. This can also poten-
tially provide an opportunity to observe features of
agrammatic speech that have not been examined in
manual analyses. We examine the top-ranked fea-
tures in Table 2 in some detail, especially as they
relate to previous work on agrammatism. In par-
ticular, the top features suggest some of the fol-
lowing features of agrammatic speech:
</bodyText>
<listItem confidence="0.964991545454545">
• Reduced number of prepositional phrases.
This is suggested by feature 1, PP → IN NP.
It is also reflected in features 7 and 20.
• Impairment in using verbs. We can see in fea-
ture 2 (ROOT → NP) that there is a greater
number of utterances consisting of only a
noun phrase. Feature 12 is also consistent
with this pattern (FRAG → NP .). We also
observe a reduced number of coordinated
verb phrases (VP → VP CC VP).
• Omission of grammatical morphemes and
</listItem>
<bodyText confidence="0.745136">
function words. The agrammatic speakers
use fewer possessives (NP → DT NN POS).
Feature 9 indicates that the control partic-
ipants more frequently produce compound
</bodyText>
<table confidence="0.999759923076923">
NB SVM RF
Narrative length .62 .56 .64
Binary, no weights .87 .87 .77
Binary, tf-idf .87 .90 .85
Binary, InfoGain .82 .90 .74
Binary, GainRatio .90 .82 .79
Frequency, no weights .90 .85 .85
Frequency, tf-idf .85 .82 .77
Frequency, InfoGain .90 .90 .82
Frequncy, GainRatio .90 .92 .74
SC metrics, no weights .85 .77 .82
SC metrics, InfoGain .85 .77 .79
SC metrics, GainRatio .85 .77 .82
</table>
<tableCaption confidence="0.9992">
Table 3: Average classification accuracy using the
</tableCaption>
<bodyText confidence="0.937175875">
clean transcripts. The highest classification accu-
racy for each feature set is indicated with boldface.
nouns with a determiner (often the glass
slipper or the fairy godmother). Feature 4
also suggests some difficulty with determin-
ers, as the agrammatic participants produce
fewer nouns modified by a possessive pro-
noun and an adjective. Contrast this with fea-
ture 13, which shows agrammatic speech is
more likely to contain noun phrases contain-
ing just an adjective and a noun. For example,
in the control narratives we are more likely to
see phrases such as her godmother ... waves
her magic wand, while in the agrammatic
narratives phrases like Cinderella had wicked
stepmother are more common.
</bodyText>
<listItem confidence="0.8971582">
• Reduced number of embedded clauses and
phrases. Evidence for this can be found in
the reduced number of wh-adverb phrases
(WHADVP → WRB), as well as features 14,
15, and 19.
</listItem>
<bodyText confidence="0.999987333333333">
The results of our classification experiment on
the clean data are shown in Table 3. The results
are similar for the binary and frequency features,
with the best result of .92 achieved using an SVM
classifier and frequency features, with GainRatio
weights. The best results using parse features
(.85–.92) are the same or slightly better than the
best results using SC features (.85), and both fea-
ture sets perform above baseline.
</bodyText>
<subsectionHeader confidence="0.9996">
5.2 Effect of non-narrative speech
</subsectionHeader>
<bodyText confidence="0.9996735">
In this section we perform two additional experi-
ments, using the raw and auto-cleaned transcripts.
</bodyText>
<page confidence="0.996854">
138
</page>
<table confidence="0.998997181818182">
Rule AG CT p
freq. freq.
1 NP → DT NN POS 0.0 0.5 ∗
2 PP → IN NP 12.2 26.1 ∗∗∗
3 SBAR → WHADVP S 0.4 1.5 ∗
4 VP → VBD 0.75 1.1
5 VP → TO VP 4.3 7.3 ∗
6 S → CC PP NP VP . 0.04 0.5 ∗
7 NP → PRP$ JJ NNS 0.04 0.5 ∗
8 VP → AUX VP 3.7 6.0
9 ROOT → FRAG 4.5 0.7 ∗∗
10 ADVP → RB 9.8 12.3
11 NP → NNP 4.4 6.2 ∗
12 NP → DT NN 15.0 24.1 ∗∗
13 VP → VB PP 1.2 2.8 ∗
14 VP → VP CC VP 1.0 2.9 ∗
15 WHADVP → WRB 0.6 1.5 ∗
16 VP → VBN PP 0.4 2.0 ∗
17 INTJ → UH UH 3.5 0.3 ∗
18 VP → VBP NP 0.5 0.0 ∗
19 NP → NNP NNP 1.5 0.5 ∗∗
20 S → CC ADVP NP VP . 1.3 2.3
</table>
<tableCaption confidence="0.982083">
Table 4: Top 20 features ranked by GainRatio
</tableCaption>
<bodyText confidence="0.988855769230769">
using the raw transcripts. Bold feature numbers
indicate rules which did not appear in Table 2.
(∗p &lt; 0.05, ∗∗p &lt; 0.005, ∗∗∗p &lt; 0.0005).
We discuss the differences between the selected
features in each case, and the resulting classifica-
tion accuracies.
Using the raw transcripts, we find that the rank-
ing of features is markedly different than with the
human-annotated transcripts (Table 4, bold feature
numbers). Examining these production rules more
closely, we observe some characteristics of agram-
matic speech which were not detectable in the an-
notated transcripts:
</bodyText>
<listItem confidence="0.903382583333333">
• Increased number of dysfluencies. We ob-
serve a higher number of consecutive fillers
(INTJ → UH UH) in the agrammatic data,
as well as a higher number of consecutive
proper nouns (NP → NNP NNP), usually
two attempts at Cinderella’s name. Feature
18 (VP → VBP NP) also appears to support
this trend, although it is not immediately ob-
vious. Most of the control participants tell
the story in the past tense, and if they do
use the present tense then the verbs are of-
ten in the third-person singular (Cinderella
</listItem>
<bodyText confidence="0.999359142857143">
finds her fairy godmother). Looking at the
data, we found that feature 18 can indicate a
verb agreement error, as in he attend the ball.
However, in almost twice as many cases it in-
dicates use of the discourse markers I mean
and you know, followed by a repaired or tar-
get noun phrase.
</bodyText>
<listItem confidence="0.738899">
• Decreased connection between sentences.
</listItem>
<bodyText confidence="0.996249906976744">
Feature 6 shows a canonical NP VP sentence,
preceded by a coordinate conjunction and a
prepositional phrase. Some examples of this
from the control transcripts include, And at
the stroke of midnight... and And in the pro-
cess .... The conjunction creates a connec-
tion from one utterance to the next, and the
prepositional phrase indicates the temporal
relationship between events in the story, cre-
ating a sense of cohesion. See also the similar
pattern in feature 20, representing sentence
beginnings such as And then ....
However, there are some features which were
highly ranked in the clean transcripts but do not
appear in Table 4. What information are we losing
by using the raw data? One issue with using the
raw transcripts is that the inclusion of filled pauses
“splits” the counts for some features. For example,
the feature FRAG → NP . is ranked 12th using the
clean transcripts but does not appear in the top 20
when using the raw transcripts. When we examine
the transcripts, we find that the phrases that are
counted in this feature in the clean transcripts are
actually split into three features in the raw tran-
scripts: FRAG → NP ., FRAG → INTJ NP ., and
FRAG → NP INTJ ..
The classification results for the raw transcripts
are given in Table 5. The results are similar to
those for the clean transcripts, although in this
case the best accuracy (.92) is achieved in three
different configurations (all using the SVM clas-
sifier). The phrase-level features out-perform the
traditional SC measures in only half the cases.
Using the auto-cleaned transcripts, we see some
similarities with the previous cases (Table 6).
However, some of the highly ranked features
which disappeared when using the raw transcripts
are now significant again (e.g. ROOT → NP,
FRAG → NP .). There are also three remain-
ing features which are significant and have not yet
been discussed. Feature 9 shows an increased use
of determiners with proper nouns (e.g. the Cin-
derella), a frank grammatical error. Feature 20
</bodyText>
<page confidence="0.995625">
139
</page>
<table confidence="0.999762384615385">
NB SVM RF
Narrative length .51 .62 .69
Binary, no weights .87 .92 .82
Binary, tf-idf .87 .92 .72
Binary, InfoGain .85 .87 .82
Binary, GainRatio .82 .87 .85
Frequency, no weights .85 .90 .69
Frequency, tf-idf .82 .92 .90
Frequency, InfoGain .85 .74 .85
Frequncy, GainRatio .85 .74 .82
SC metrics, no weights .74 .79 .82
SC metrics, InfoGain .77 .85 .85
SC metrics, GainRatio .77 .85 .87
</table>
<tableCaption confidence="0.994442">
Table 5: Average classification accuracy using
</tableCaption>
<bodyText confidence="0.958823470588235">
raw transcripts. The highest classification accu-
racy for each feature set is indicated with boldface.
provides another example of a sentence fragment
with no verb. Finally, feature 19 represents an in-
creased number of sentences or clauses consist-
ing of a noun phrase followed by adjective phrase.
Looking at the transcripts, this is not generally in-
dicative of an error, but rather use of the word
okay, as in she dropped her shoe okay.
The classification results for the auto-cleaned
data, shown in Table 7, show a somewhat differ-
ent pattern from the previous experiments. The
accuracies using the parse features are generally
higher, and the best result of .97 is achieved using
the binary features and the naive Bayes classifier.
Interestingly, this data set also results in the lowest
accuracy for the syntactic complexity metrics.
</bodyText>
<subsectionHeader confidence="0.999859">
5.3 Phrase-level parse features
</subsectionHeader>
<bodyText confidence="0.999417875">
The classifiers in Tables 3, 5, and 7 used the
phrase-level parse features as well as the CFG
productions. Although these features were cal-
culated for NPs, VPs, and PPs, the NP features
were never selected by the GainRatio ranking al-
gorithm, and did not differ significantly between
groups. The significance levels of the VP and PP
features are reported in Table 8. PP rate and pro-
portion are significantly different in all three sets
of transcripts, which is consistent with the high
ranking of PP → IN NP in each case. VP rate
and proportion are often significant, although less
so. Notably, PP and VP length are both significant
in the clean transcripts, but not significant in the
raw transcripts and only barely significant in the
auto-cleaned transcripts.
</bodyText>
<table confidence="0.999585545454546">
Rule AG CT p
freq. freq.
1 PP → IN NP 12.0 26.0 ∗∗∗
2 NP → DT NN POS 0.0 0.7 ∗
3 VP → VP CC VP 0.8 2.9 ∗∗
4 S → CC SBAR NP VP . 0.0 0.5
5 SBAR → WHADVP S 0.4 1.5 ∗
6 NP → NNP 5.6 6.7
7 VP → VBD 0.8 1.1
8 S → CC PP NP VP . 0.04 0.6 ∗
9 NP → DT NNP 0.6 0.0 ∗∗
10 VP → TO VP 4.6 7.5 ∗
11 ROOT → FRAG 3.0 0.5 ∗∗∗
12 ROOT → NP 2.1 0.1 ∗
13 VP → VBP NP 1.7 3.6
14 NP → PRP$ JJ NNS 0.04 0.5 ∗
15 VP → VB PP 1.1 2.8 ∗∗
16 VP → VBN PP 0.4 1.9 ∗
17 FRAG → NP . 0.4 0.0 ∗
18 NP → NNP. 2.1 0.1
19 S → NP ADJP 0.4 0.0 ∗
20 FRAG → CC NP . 0.7 0.07 ∗∗
</table>
<tableCaption confidence="0.99614325">
Table 6: Top 10 features ranked by GainRatio
using the auto-cleaned transcripts. Bold feature
numbers indicate rules which did not appear in Ta-
ble 2. (∗p &lt; 0.05, ∗∗p &lt; 0.005, ∗∗∗p &lt; 0.0005).
</tableCaption>
<subsectionHeader confidence="0.999753">
5.4 Analysis of variance
</subsectionHeader>
<bodyText confidence="0.999976785714286">
With a multi-way ANOVA we found significant
main effects of classifier (F(2,63) = 11.6, p &lt;
0.001) and data set (F(2,63) = 11.2, p &lt; 0.001)
on accuracy. A Tukey post-hoc test revealed sig-
nificant differences between SVM and RF (p &lt;
0.001) and NB and RF (p &lt; 0.001) but not be-
tween SVM and NB. As well, we see a sig-
nificant difference between the clean and auto-
cleaned data (p &lt; 0.001) and the raw and auto-
cleaned data (p &lt; 0.001) but not between the raw
and clean data. There was no significant main ef-
fect of weighting scheme or feature type (binary or
frequency) on accuracy. We did not examine any
possible interactions between these variables.
</bodyText>
<sectionHeader confidence="0.999783" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.8674">
6.1 Transcripts
</subsectionHeader>
<bodyText confidence="0.99955">
We achieved the highest classification accuracies
using the auto-cleaned transcripts. The raw tran-
scripts, while containing more information about
dysfluent events, also seemed to cause more dif-
</bodyText>
<page confidence="0.987741">
140
</page>
<table confidence="0.999730692307692">
NB SVM RF
Narrative length .51 .62 .64
Binary, no weights .92 .95 .90
Binary, If-idf .92 .95 .87
Binary, InfoGain .97 .90 .85
Binary, GainRatio .97 .90 .95
Frequency, no weights .90 .95 .77
Frequency, If-idf .87 .95 .79
Frequency, InfoGain .92 .85 .82
Frequncy, GainRatio .92 .87 .95
SC metrics, no weights .79 .77 .74
SC metrics, InfoGain .79 .74 .72
SC metrics, GainRatio .79 .74 .67
</table>
<tableCaption confidence="0.98263225">
Table 7: Average classification accuracy using
auto-cleaned transcripts. The highest classifica-
tion accuracy for each feature set is indicated with
boldface.
</tableCaption>
<table confidence="0.999804714285714">
Clean Raw Auto
PP rate ∗∗∗ ∗∗∗ ∗∗∗
PP proportion ∗∗∗ ∗∗∗ ∗∗
PP length ∗∗
VP rate ∗∗∗ ∗∗ ∗
VP proportion ∗∗∗ ∗ ∗
VP length ∗
</table>
<tableCaption confidence="0.981112">
Table 8: Significance of the phrase-level features
</tableCaption>
<bodyText confidence="0.992876875">
in each of the three data sets (∗p &lt; 0.05, ∗∗p &lt;
0.005, ∗∗∗p &lt; 0.0005).
ficulty for the parser, which mis-labelled filled
pauses and false starts in some cases. We also
found that the insertion of filled pauses resulted
in the creation of multiple features for a single un-
derlying grammatical structure. The auto-cleaned
transcripts appeared to avoid some of those prob-
lems, while still retaining information about many
of the non-narrative speech productions that were
removed from the clean transcripts.
Some of the features from the auto-cleaned tran-
scripts appear to be associated with the discourse
level of language, such as connectives and dis-
course markers. A researcher solely interested in
studying the syntax of language might resist the
inclusion of such features, and prefer to use only
features from the human-annotated clean tran-
scripts. However, we feel that such productions
are part of the grammar of spoken language, and
merit inclusion. From a practical standpoint, our
findings are reassuring: data preparation that can
be done automatically is much more feasible in
many situations than human annotation.
</bodyText>
<subsectionHeader confidence="0.956115">
6.2 Features
</subsectionHeader>
<bodyText confidence="0.999989684210526">
CFG production rules can offer a more detailed
look at specific language impairments. We were
able to observe a number of important characteris-
tics of agrammatic language as reported in previ-
ous studies: fragmented speech with a higher in-
cidence of solitary noun phrases, difficulty with
determiners and possessives, reduced number of
prepositional phrases and embedded clauses, and
(in the raw transcripts), increased use of filled
pauses and repair phrases. For this reason, we be-
lieve that they are more useful for the analysis of
disordered or otherwise atypical language than tra-
ditional measures of syntactic complexity.
In some cases an in-depth analysis may not be
required, and in such cases it may be tempting to
simply use one of the more-general syntactic com-
plexity measures. Nevertheless, even in our simple
binary classification task, we found that using the
more-specific features gave us a higher accuracy.
</bodyText>
<subsectionHeader confidence="0.837382">
6.3 Future work
</subsectionHeader>
<bodyText confidence="0.999989181818182">
Because of the limited data, we consider these re-
sults to be preliminary. We hope to replicate this
study as more data become available in the fu-
ture. We also plan to examine the effect, if any,
of the specific narrative task. Furthermore, we
have shown that these methods are effective for
the analysis of agrammatic aphasia, but there are
other types of aphasia in which semantic, rather
than syntactic, processing is the primary impair-
ment. We would like to extend this work to find
features which distinguish between different types
of aphasia.
Although we included manually transcribed
data in this study, these methods will be most use-
ful if they are also effective on automatically rec-
ognized speech. Previous work on speech recog-
nition for aphasic speech reported high error rates
(Fraser et al., 2013a). Our finding that the auto-
cleaned transcripts led to the highest classification
accuracy is encouraging, but we will have to test
the robustness to recognition errors and the depen-
dence on sentence boundary annotations.
</bodyText>
<sectionHeader confidence="0.998456" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.588774666666667">
This research was supported by the Natural Sciences and En-
gineering Research Council of Canada and National Institutes
of Health R01DC01948 and R01DC008552.
</bodyText>
<page confidence="0.998056">
141
</page>
<sectionHeader confidence="0.995852" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999928480769231">
Harald Baayen, Hans Van Halteren, and Fiona
Tweedie. 1996. Outside the cave of shadows:
Using syntactic annotation to enhance authorship
attribution. Literary and Linguistic Computing,
11(3):121–132.
Roelien Bastiaanse and Cynthia K. Thompson. 2012.
Perspectives on Agrammatism. Psychology Press.
Julian Brooke and Graeme Hirst. 2012. Robust, lex-
icalized native language identification. In Proceed-
ings of the 24th International Conference on Com-
putational Linguistics, pages 391–408.
Jieun Chae and Ani Nenkova. 2009. Predicting the
fluency of text with shallow structural features: case
studies of machine translation and human-written
text. In Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 139–147.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 1st Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 132–139.
Franca Debole and Fabrizio Sebastiani. 2004. Super-
vised term weighting for automated text categoriza-
tion. In Text mining and its applications, pages 81–
97. Springer.
Kathleen Fraser, Frank Rudzicz, Naida Graham, and
Elizabeth Rochon. 2013a. Automatic speech recog-
nition in the diagnosis of primary progressive apha-
sia. In Proceedings of the Fourth Workshop on
Speech and Language Processing for Assistive Tech-
nologies, pages 47–54.
Kathleen C. Fraser, Jed A. Meltzer, Naida L. Graham,
Carol Leonard, Graeme Hirst, Sandra E. Black, and
Elizabeth Rochon. 2013b. Automated classification
of primary progressive aphasia subtypes from narra-
tive speech transcripts. Cortex.
Harold Goodglass, Julie Ann Christiansen, and
Roberta E. Gallagher. 1994. Syntatic construc-
tions used by agrammatic speakers: Comparison
with conduction aphasics and normals. Neuropsy-
chology, 8(4):598.
Khairun-nisa Hassanali, Yang Liu, Aquiles Iglesias,
Thamar Solorio, and Christine Dollaghan. 2013.
Automatic generation of the index of productive
syntax for child language transcripts. Behavior re-
search methods, pages 1–9.
David I. Holmes and Sameer Singh. 1996. A stylo-
metric analysis of conversational speech of apha-
sic patients. Literary and Linguistic Computing,
11(3):133–140.
Brian MacWhinney, Davida Fromm, Margaret Forbes,
and Audrey Holland. 2011. Aphasiabank: Methods
for studying discourse. Aphasiology, 25(11):1286–
1307.
Ljiljana Progovac. 2006. The Syntax of Nonsen-
tentials: Multidisciplinary Perspectives, volume 93.
John Benjamins.
Emily T. Prud’hommeaux, Brian Roark, Lois M.
Black, and Jan van Santen. 2011. Classification of
atypical language in autism. In Proceedings of the
2nd Workshop on Cognitive Modeling and Compu-
tational Linguistics, CMCL ’11, pages 88–96.
Brian Roark, Margaret Mitchell, John-Paul Hosom,
Kristy Hollingshead, and Jeffery Kaye. 2011. Spo-
ken language derived measures for detecting mild
cognitive impairment. IEEE Transactions on Au-
dio, Speech, and Language Processing, 19(7):2081–
2090.
Elizabeth Rochon, Eleanor M. Saffran, Rita Sloan
Berndt, and Myrna F. Schwartz. 2000. Quantita-
tive analysis of aphasic sentence production: Further
development and new data. Brain and Language,
72(3):193–218.
Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F.
Schwartz. 1989. The quantitative analysis of
agrammatic production: Procedure and data. Brain
and Language, 37(3):440–479.
Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005. Automatic measurement of syntactic develop-
ment in child language. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 197–204.
Ben Swanson and Eugene Charniak. 2012. Native lan-
guage detection with tree substitution grammars. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, pages 193–
197.
Cynthia K. Thompson, Lewis P. Shapiro, Ligang Li,
and Lee Schendel. 1995. Analysis of verbs and
verb-argument structure: A method for quantifica-
tion of aphasic language production. Clinical Apha-
siology, 23:121–140.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classification.
In Proceedings of the Australasian Language Tech-
nology Association Workshop, pages 67–75.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identifica-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600–1610.
Victor Yngve. 1960. A model and hypothesis for lan-
guage structure. Proceedings of the American Phys-
ical Society, 104:444–466.
</reference>
<page confidence="0.997728">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.138948">
<title confidence="0.989394">Using statistical parsing to detect agrammatic aphasia</title>
<author confidence="0.713954">C Graeme Jed A E</author>
<author confidence="0.713954">K Cynthia</author>
<affiliation confidence="0.9786605">of Computer Science, University of Research Institute, Baycrest Centre,</affiliation>
<address confidence="0.5127815">of Communication Sciences and Disorders, Northwestern of Neurology, Northwestern</address>
<affiliation confidence="0.757186">Neurology and Alzheimer’s Disease Center, Northwestern University</affiliation>
<abstract confidence="0.999456409090909">Agrammatic aphasia is a serious language impairment which can occur after a stroke or traumatic brain injury. We present an automatic method for analyzing aphasic speech using surface level parse features and context-free grammar production rules. Examining these features individually, we show that we can uncover many of the same characteristics of agrammatic language that have been reported in studies using manual analysis. When taken together, these parse features can be used to train a classifier to accurately predict whether or not an individual has aphasia. Furthermore, we find that the parse features can lead to higher classification accuracies than traditional measures of syntactic complexity. Finally, we find that a minimal amount of pre-processing can lead to better results than using either the raw data or highly processed data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
<author>Hans Van Halteren</author>
<author>Fiona Tweedie</author>
</authors>
<title>Outside the cave of shadows: Using syntactic annotation to enhance authorship attribution.</title>
<date>1996</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>11--3</pages>
<marker>Baayen, Van Halteren, Tweedie, 1996</marker>
<rawString>Harald Baayen, Hans Van Halteren, and Fiona Tweedie. 1996. Outside the cave of shadows: Using syntactic annotation to enhance authorship attribution. Literary and Linguistic Computing, 11(3):121–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roelien Bastiaanse</author>
<author>Cynthia K Thompson</author>
</authors>
<title>Perspectives on Agrammatism.</title>
<date>2012</date>
<publisher>Psychology Press.</publisher>
<contexts>
<context position="2313" citStr="Bastiaanse and Thompson, 2012" startWordPosition="337" endWordPosition="340">perience different symptoms. A careful analysis of narrative speech can reveal specific patterns of impairment, and help a clinician determine whether an individual has aphasia, what type of aphasia it is, and how the symptoms are changing over time. In this paper, we present an automatic method for the analysis of one type of aphasia, agrammatic aphasia. characterized by the omission of function words, the omission or substitution of morphological markers for person and number, the absence of verb inflection, and a relative increase in the number of nouns and decrease in the number of verbs (Bastiaanse and Thompson, 2012). There is often a reduction in the variety of different syntactic structures used, as well as a reduction in the complexity of those structures (Progovac, 2006). There may also be a strong tendency to use the canonical word order of a language, for example subject-verb-object in English (Progovac, 2006). Most studies of narrative speech in agrammatic aphasia are based on manually annotated speech transcripts. This type of analysis can provide detailed and accurate information about the speech patterns that are observed. However, it is also very time consuming and requires trained transcribers</context>
</contexts>
<marker>Bastiaanse, Thompson, 2012</marker>
<rawString>Roelien Bastiaanse and Cynthia K. Thompson. 2012. Perspectives on Agrammatism. Psychology Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Brooke</author>
<author>Graeme Hirst</author>
</authors>
<title>Robust, lexicalized native language identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics,</booktitle>
<pages>391--408</pages>
<contexts>
<context position="8431" citStr="Brooke and Hirst, 2012" startWordPosition="1267" endWordPosition="1270">ics derived from parse trees have been used by various researchers in studies of mild cognitive impairment (Roark et al., 2011), autism (Prud’hommeaux et al., 2011), and child language development (Sagae et al., 2005; Hassanali et al., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentences. For example, Wong and Dras (2010) used CFG productions to classify sentences from an artificial error corpus as being either grammatical or ungrammatical. Taking a different approach, Chae and Nenkova 1http://talkbank.org/AphasiaBank/ 135 Agrammatic Control (N = 24) (N = 15) Male/Female 15/9 8/7 Age (years) 58.1 (10.6) 63.3 (6.4) Education (years) 16.3 (2.5) 16.4 (2.4) Table 1: Demographic informatio</context>
</contexts>
<marker>Brooke, Hirst, 2012</marker>
<rawString>Julian Brooke and Graeme Hirst. 2012. Robust, lexicalized native language identification. In Proceedings of the 24th International Conference on Computational Linguistics, pages 391–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jieun Chae</author>
<author>Ani Nenkova</author>
</authors>
<title>Predicting the fluency of text with shallow structural features: case studies of machine translation and human-written text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>139--147</pages>
<contexts>
<context position="11771" citStr="Chae and Nenkova (2009)" startWordPosition="1813" endWordPosition="1816">the set of non-lexical productions. The total number of types of productions is large, many of them occurring very infrequently, so we compile a list of the 50 most frequently occurring productions in each of the two groups (agrammatic and controls) and use the combined set as the set of features. The feature values can be binary (does a particular production rule appear in the narrative or not?) or integer (how many times does a rule occur?). The CFG non-terminal symbols follow the Penn Treebank naming conventions. For our phrase-level statistics, we use a subset of the features described by Chae and Nenkova (2009), which are related to the incidence of different phrase types. We consider three different phrase types: noun phrases, verb phrases, and prepositional phrases. These features are defined as follows: • Phrase type proportion: Length of each phrase type (including embedded phrases), divided by total narrative length. • Average phrase length: Total number of words in a phrase type, divided by number of phrases of that type. • Phrase type rate: Number of phrases of a given type, divided by total narrative length. Because we are judging the grammaticality of the entire narrative, we normalize by n</context>
</contexts>
<marker>Chae, Nenkova, 2009</marker>
<rawString>Jieun Chae and Ani Nenkova. 2009. Predicting the fluency of text with shallow structural features: case studies of machine translation and human-written text. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 139–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropyinspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="11052" citStr="Charniak, 2000" startWordPosition="1694" endWordPosition="1695">The data was segmented into utterances based on syntactic and prosodic cues. Filled pauses, repetitions, false starts, and revisional phrases (e.g. I mean) were all placed inside parentheses. The average length of the raw transcripts was 332 words for agrammatic participants and 387 words for controls; when the non-narrative words were excluded the average length was 194 words for the agrammatic group and 330 for controls. 4 Methods 4.1 Parser Features We consider two types of features: CFG production rules and phrase-level statistics. For the CFG production rules, we use the Charniak parser (Charniak, 2000) trained on Wall Street Journal data to parse each utterance in the transcript and then extract the set of non-lexical productions. The total number of types of productions is large, many of them occurring very infrequently, so we compile a list of the 50 most frequently occurring productions in each of the two groups (agrammatic and controls) and use the combined set as the set of features. The feature values can be binary (does a particular production rule appear in the narrative or not?) or integer (how many times does a rule occur?). The CFG non-terminal symbols follow the Penn Treebank na</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropyinspired parser. In Proceedings of the 1st Conference of the North American Chapter of the Association for Computational Linguistics, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franca Debole</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>Supervised term weighting for automated text categorization. In Text mining and its applications,</title>
<date>2004</date>
<pages>81--97</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13833" citStr="Debole and Sebastiani (2004)" startWordPosition="2150" endWordPosition="2154">ply Cinderella). This provides a more realistic view of the performance of our system on real data. We also hypothesize that there may be important information to be found in the dysfluent speech segments. 4.2 Feature weighting and selection We assume that some production rules will be more relevant to the classification than others, and so we want to weight the features accordingly. Using term frequency–inverse document frequency (tf-idf) would be one possibility; however, the tf-idf weights do not take into account any class information. Supervised term weighting (STW), has been proposed by Debole and Sebastiani (2004) as an alternative to tf-idf for text classification tasks. In this weighting scheme, feature weights are assigned using the same algorithm that is used for feature selection. For example, one way to select features is to rank them by their information gain (InfoGain). In STW, the InfoGain value for each feature is also used to replace the idf term. This can be expressed as W(i,d) = df(i,d) × InfoGain(i), where W(i,d) is the weight assigned to feature i in document d, df(i,d) is the frequency of occurrence of feature i in document d, and InfoGain(i) is the information gain of feature i across </context>
</contexts>
<marker>Debole, Sebastiani, 2004</marker>
<rawString>Franca Debole and Fabrizio Sebastiani. 2004. Supervised term weighting for automated text categorization. In Text mining and its applications, pages 81– 97. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen Fraser</author>
<author>Frank Rudzicz</author>
<author>Naida Graham</author>
<author>Elizabeth Rochon</author>
</authors>
<title>Automatic speech recognition in the diagnosis of primary progressive aphasia.</title>
<date>2013</date>
<booktitle>In Proceedings of the Fourth Workshop on Speech and Language Processing for Assistive Technologies,</booktitle>
<pages>47--54</pages>
<contexts>
<context position="15430" citStr="Fraser et al. (2013" startWordPosition="2417" endWordPosition="2420"> To compare the performance of the parse features with more-traditional syntactic complexity metrics (SC metrics), we calculate the mean length of utterance (MLU), mean length of T-unit2 (MLT), mean length of clause (MLC), and parse tree height. We also calculate the mean, maximum, and total Yngve depth, which measures the proportion of left-branching to right-branching in each parse tree (Yngve, 1960). These measures are commonly used in studies of impaired language (e.g. Roark et al. (2011), Prud’hommeaux et 2A T-unit consists of a main clause and its attached dependent clauses. al. (2011), Fraser et al. (2013b)). We hypothesize that the parse features will capture more information about the specific impairments seen in agrammatic aphasia; however, using the general measures of syntactic complexity may be sufficient for the classifiers to distinguish between the groups. 4.4 Classification To test whether the features can effectively distinguish between the agrammatic group and controls, we use them to train and test a machine learning classifier. We test three different classification algorithms: naive Bayes (NB), support vector machine (SVM), and random forests (RF). We use a leave-one-out cross-v</context>
</contexts>
<marker>Fraser, Rudzicz, Graham, Rochon, 2013</marker>
<rawString>Kathleen Fraser, Frank Rudzicz, Naida Graham, and Elizabeth Rochon. 2013a. Automatic speech recognition in the diagnosis of primary progressive aphasia. In Proceedings of the Fourth Workshop on Speech and Language Processing for Assistive Technologies, pages 47–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen C Fraser</author>
<author>Jed A Meltzer</author>
<author>Naida L Graham</author>
<author>Carol Leonard</author>
<author>Graeme Hirst</author>
<author>Sandra E Black</author>
<author>Elizabeth Rochon</author>
</authors>
<title>Automated classification of primary progressive aphasia subtypes from narrative speech transcripts.</title>
<date>2013</date>
<journal>Cortex.</journal>
<contexts>
<context position="15430" citStr="Fraser et al. (2013" startWordPosition="2417" endWordPosition="2420"> To compare the performance of the parse features with more-traditional syntactic complexity metrics (SC metrics), we calculate the mean length of utterance (MLU), mean length of T-unit2 (MLT), mean length of clause (MLC), and parse tree height. We also calculate the mean, maximum, and total Yngve depth, which measures the proportion of left-branching to right-branching in each parse tree (Yngve, 1960). These measures are commonly used in studies of impaired language (e.g. Roark et al. (2011), Prud’hommeaux et 2A T-unit consists of a main clause and its attached dependent clauses. al. (2011), Fraser et al. (2013b)). We hypothesize that the parse features will capture more information about the specific impairments seen in agrammatic aphasia; however, using the general measures of syntactic complexity may be sufficient for the classifiers to distinguish between the groups. 4.4 Classification To test whether the features can effectively distinguish between the agrammatic group and controls, we use them to train and test a machine learning classifier. We test three different classification algorithms: naive Bayes (NB), support vector machine (SVM), and random forests (RF). We use a leave-one-out cross-v</context>
</contexts>
<marker>Fraser, Meltzer, Graham, Leonard, Hirst, Black, Rochon, 2013</marker>
<rawString>Kathleen C. Fraser, Jed A. Meltzer, Naida L. Graham, Carol Leonard, Graeme Hirst, Sandra E. Black, and Elizabeth Rochon. 2013b. Automated classification of primary progressive aphasia subtypes from narrative speech transcripts. Cortex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Goodglass</author>
<author>Julie Ann Christiansen</author>
<author>Roberta E Gallagher</author>
</authors>
<title>Syntatic constructions used by agrammatic speakers: Comparison with conduction aphasics and normals.</title>
<date>1994</date>
<journal>Neuropsychology,</journal>
<volume>8</volume>
<issue>4</issue>
<contexts>
<context position="5404" citStr="Goodglass et al. (1994)" startWordPosition="809" endWordPosition="812">ether determiners and verb inflections are produced in obligatory contexts, as well as structural complexity, such as the number of embedded clauses per sentence. Subsequent studies have found a number of differences between normal and agrammatic speech using QPA (Rochon et al., 2000). Another popular protocol called the Northwestern Narrative Language Analysis (NNLA) was introduced by Thompson et al. (1995). This protocol analyzes each utterance at five different levels, and focuses in particular on the production of verbs and verb argument structure. Perhaps more analogous to our work here, Goodglass et al. (1994) conducted a detailed examination of the syntactic constituents used by aphasic patients and controls. In that study, utterances were grouped according to how many syntactic constituents they contained. They found that agrammatic participants were more likely to produce single-constituent utterances, especially noun phrases, and less likely to produce subordinate clauses. They also found that agrammatic speakers sometimes produced two-constituent utterances consisting of only a subject and object, with no verb. This pattern was never observed in control speech. A much smaller body of work expl</context>
</contexts>
<marker>Goodglass, Christiansen, Gallagher, 1994</marker>
<rawString>Harold Goodglass, Julie Ann Christiansen, and Roberta E. Gallagher. 1994. Syntatic constructions used by agrammatic speakers: Comparison with conduction aphasics and normals. Neuropsychology, 8(4):598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khairun-nisa Hassanali</author>
<author>Yang Liu</author>
<author>Aquiles Iglesias</author>
<author>Thamar Solorio</author>
<author>Christine Dollaghan</author>
</authors>
<title>Automatic generation of the index of productive syntax for child language transcripts. Behavior research methods,</title>
<date>2013</date>
<pages>1--9</pages>
<contexts>
<context position="8050" citStr="Hassanali et al., 2013" startWordPosition="1206" endWordPosition="1209">ttempt to fully replicate the results of the manual studies, but rather provide a complementary set of features which can indicate grammatic abnormalities. Unlike previous computational studies, we attempt to move beyond single-word analysis and examine which patterns of syntax might indicate agrammatism. 2.2 Using parse features to assess grammaticality Syntactic complexity metrics derived from parse trees have been used by various researchers in studies of mild cognitive impairment (Roark et al., 2011), autism (Prud’hommeaux et al., 2011), and child language development (Sagae et al., 2005; Hassanali et al., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentences. For example, Wong and </context>
</contexts>
<marker>Hassanali, Liu, Iglesias, Solorio, Dollaghan, 2013</marker>
<rawString>Khairun-nisa Hassanali, Yang Liu, Aquiles Iglesias, Thamar Solorio, and Christine Dollaghan. 2013. Automatic generation of the index of productive syntax for child language transcripts. Behavior research methods, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David I Holmes</author>
<author>Sameer Singh</author>
</authors>
<title>A stylometric analysis of conversational speech of aphasic patients.</title>
<date>1996</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>11--3</pages>
<contexts>
<context position="6092" citStr="Holmes and Singh (1996)" startWordPosition="912" endWordPosition="915">sed by aphasic patients and controls. In that study, utterances were grouped according to how many syntactic constituents they contained. They found that agrammatic participants were more likely to produce single-constituent utterances, especially noun phrases, and less likely to produce subordinate clauses. They also found that agrammatic speakers sometimes produced two-constituent utterances consisting of only a subject and object, with no verb. This pattern was never observed in control speech. A much smaller body of work explores the use of computational techniques to analyze agrammatism. Holmes and Singh (1996) analyzed conversational speech from aphasic speakers and controls. Their features mostly included measures of vocabulary richness and frequency counts of various parts-of-speech (e.g. nouns, verbs); however they also measured “clause-like semantic unit rate”. This feature was intended to measure the speaker’s ability to cluster words together, although it is not clear what the criteria for segmenting clause-like units were or whether it was done manually or automatically. Nonetheless, it was found to be one of the most important variables for distinguishing between patients and controls. MacW</context>
</contexts>
<marker>Holmes, Singh, 1996</marker>
<rawString>David I. Holmes and Sameer Singh. 1996. A stylometric analysis of conversational speech of aphasic patients. Literary and Linguistic Computing, 11(3):133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian MacWhinney</author>
<author>Davida Fromm</author>
<author>Margaret Forbes</author>
<author>Audrey Holland</author>
</authors>
<title>Aphasiabank: Methods for studying discourse.</title>
<date>2011</date>
<journal>Aphasiology,</journal>
<volume>25</volume>
<issue>11</issue>
<pages>1307</pages>
<contexts>
<context position="6712" citStr="MacWhinney et al. (2011)" startWordPosition="1006" endWordPosition="1009">996) analyzed conversational speech from aphasic speakers and controls. Their features mostly included measures of vocabulary richness and frequency counts of various parts-of-speech (e.g. nouns, verbs); however they also measured “clause-like semantic unit rate”. This feature was intended to measure the speaker’s ability to cluster words together, although it is not clear what the criteria for segmenting clause-like units were or whether it was done manually or automatically. Nonetheless, it was found to be one of the most important variables for distinguishing between patients and controls. MacWhinney et al. (2011) presented several examples of how researchers can use the AphasiaBank1 database and associated software tools to conduct automatic analyses (although the transcripts are first hand-coded for errors by experienced speech-language pathologists). Specifically with regards to syntax, they calculated several frequency counts and ratios for different parts-ofspeech and bound morphemes. There was one extension beyond treating each word individually: this involved searching for pre-defined collocations such as once upon a time or happily ever after, which were found to occur more rarely in the patien</context>
</contexts>
<marker>MacWhinney, Fromm, Forbes, Holland, 2011</marker>
<rawString>Brian MacWhinney, Davida Fromm, Margaret Forbes, and Audrey Holland. 2011. Aphasiabank: Methods for studying discourse. Aphasiology, 25(11):1286– 1307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ljiljana Progovac</author>
</authors>
<title>The Syntax of Nonsententials: Multidisciplinary Perspectives, volume 93. John Benjamins.</title>
<date>2006</date>
<contexts>
<context position="2474" citStr="Progovac, 2006" startWordPosition="366" endWordPosition="367">, what type of aphasia it is, and how the symptoms are changing over time. In this paper, we present an automatic method for the analysis of one type of aphasia, agrammatic aphasia. characterized by the omission of function words, the omission or substitution of morphological markers for person and number, the absence of verb inflection, and a relative increase in the number of nouns and decrease in the number of verbs (Bastiaanse and Thompson, 2012). There is often a reduction in the variety of different syntactic structures used, as well as a reduction in the complexity of those structures (Progovac, 2006). There may also be a strong tendency to use the canonical word order of a language, for example subject-verb-object in English (Progovac, 2006). Most studies of narrative speech in agrammatic aphasia are based on manually annotated speech transcripts. This type of analysis can provide detailed and accurate information about the speech patterns that are observed. However, it is also very time consuming and requires trained transcribers and annotators. Studies are necessarily limited to a manageable size, and the level of agreement between annotators can vary. We propose an automatic approach t</context>
</contexts>
<marker>Progovac, 2006</marker>
<rawString>Ljiljana Progovac. 2006. The Syntax of Nonsententials: Multidisciplinary Perspectives, volume 93. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily T Prud’hommeaux</author>
<author>Brian Roark</author>
<author>Lois M Black</author>
<author>Jan van Santen</author>
</authors>
<title>Classification of atypical language in autism.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, CMCL ’11,</booktitle>
<pages>88--96</pages>
<marker>Prud’hommeaux, Roark, Black, van Santen, 2011</marker>
<rawString>Emily T. Prud’hommeaux, Brian Roark, Lois M. Black, and Jan van Santen. 2011. Classification of atypical language in autism. In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, CMCL ’11, pages 88–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Margaret Mitchell</author>
<author>John-Paul Hosom</author>
<author>Kristy Hollingshead</author>
<author>Jeffery Kaye</author>
</authors>
<title>Spoken language derived measures for detecting mild cognitive impairment.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>19</volume>
<issue>7</issue>
<contexts>
<context position="7936" citStr="Roark et al., 2011" startWordPosition="1189" endWordPosition="1192">nscripts than in the control transcripts. We present an alternative, automated method of analysis. We do not attempt to fully replicate the results of the manual studies, but rather provide a complementary set of features which can indicate grammatic abnormalities. Unlike previous computational studies, we attempt to move beyond single-word analysis and examine which patterns of syntax might indicate agrammatism. 2.2 Using parse features to assess grammaticality Syntactic complexity metrics derived from parse trees have been used by various researchers in studies of mild cognitive impairment (Roark et al., 2011), autism (Prud’hommeaux et al., 2011), and child language development (Sagae et al., 2005; Hassanali et al., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as othe</context>
<context position="15308" citStr="Roark et al. (2011)" startWordPosition="2396" endWordPosition="2399">e classification. We also consider tf-idf weights and unweighted features for comparison. 4.3 Syntactic complexity metrics To compare the performance of the parse features with more-traditional syntactic complexity metrics (SC metrics), we calculate the mean length of utterance (MLU), mean length of T-unit2 (MLT), mean length of clause (MLC), and parse tree height. We also calculate the mean, maximum, and total Yngve depth, which measures the proportion of left-branching to right-branching in each parse tree (Yngve, 1960). These measures are commonly used in studies of impaired language (e.g. Roark et al. (2011), Prud’hommeaux et 2A T-unit consists of a main clause and its attached dependent clauses. al. (2011), Fraser et al. (2013b)). We hypothesize that the parse features will capture more information about the specific impairments seen in agrammatic aphasia; however, using the general measures of syntactic complexity may be sufficient for the classifiers to distinguish between the groups. 4.4 Classification To test whether the features can effectively distinguish between the agrammatic group and controls, we use them to train and test a machine learning classifier. We test three different classifi</context>
</contexts>
<marker>Roark, Mitchell, Hosom, Hollingshead, Kaye, 2011</marker>
<rawString>Brian Roark, Margaret Mitchell, John-Paul Hosom, Kristy Hollingshead, and Jeffery Kaye. 2011. Spoken language derived measures for detecting mild cognitive impairment. IEEE Transactions on Audio, Speech, and Language Processing, 19(7):2081– 2090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Rochon</author>
<author>Eleanor M Saffran</author>
<author>Rita Sloan Berndt</author>
<author>Myrna F Schwartz</author>
</authors>
<title>Quantitative analysis of aphasic sentence production: Further development and new data.</title>
<date>2000</date>
<journal>Brain and Language,</journal>
<volume>72</volume>
<issue>3</issue>
<contexts>
<context position="5066" citStr="Rochon et al., 2000" startWordPosition="758" endWordPosition="761"> Work 2.1 Syntactic analysis of agrammatic narrative speech Much of the previous work analyzing narrative speech in agrammatic aphasia has been performed manually. One widely used protocol is called Quantitative Production Analysis (QPA), developed by Saffran et al. (1989). QPA can be used to measure morphological content, such as whether determiners and verb inflections are produced in obligatory contexts, as well as structural complexity, such as the number of embedded clauses per sentence. Subsequent studies have found a number of differences between normal and agrammatic speech using QPA (Rochon et al., 2000). Another popular protocol called the Northwestern Narrative Language Analysis (NNLA) was introduced by Thompson et al. (1995). This protocol analyzes each utterance at five different levels, and focuses in particular on the production of verbs and verb argument structure. Perhaps more analogous to our work here, Goodglass et al. (1994) conducted a detailed examination of the syntactic constituents used by aphasic patients and controls. In that study, utterances were grouped according to how many syntactic constituents they contained. They found that agrammatic participants were more likely to</context>
</contexts>
<marker>Rochon, Saffran, Berndt, Schwartz, 2000</marker>
<rawString>Elizabeth Rochon, Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F. Schwartz. 2000. Quantitative analysis of aphasic sentence production: Further development and new data. Brain and Language, 72(3):193–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor M Saffran</author>
<author>Rita Sloan Berndt</author>
<author>Myrna F Schwartz</author>
</authors>
<title>The quantitative analysis of agrammatic production:</title>
<date>1989</date>
<booktitle>Procedure and data. Brain and Language,</booktitle>
<volume>37</volume>
<issue>3</issue>
<contexts>
<context position="4719" citStr="Saffran et al. (1989)" startWordPosition="703" endWordPosition="706"> Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 134–142, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics about the specific syntactic structures being produced (or avoided) by the agrammatic speakers, and lead to better classification accuracies. 2 Related Work 2.1 Syntactic analysis of agrammatic narrative speech Much of the previous work analyzing narrative speech in agrammatic aphasia has been performed manually. One widely used protocol is called Quantitative Production Analysis (QPA), developed by Saffran et al. (1989). QPA can be used to measure morphological content, such as whether determiners and verb inflections are produced in obligatory contexts, as well as structural complexity, such as the number of embedded clauses per sentence. Subsequent studies have found a number of differences between normal and agrammatic speech using QPA (Rochon et al., 2000). Another popular protocol called the Northwestern Narrative Language Analysis (NNLA) was introduced by Thompson et al. (1995). This protocol analyzes each utterance at five different levels, and focuses in particular on the production of verbs and verb</context>
</contexts>
<marker>Saffran, Berndt, Schwartz, 1989</marker>
<rawString>Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F. Schwartz. 1989. The quantitative analysis of agrammatic production: Procedure and data. Brain and Language, 37(3):440–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
<author>Brian MacWhinney</author>
</authors>
<title>Automatic measurement of syntactic development in child language.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>197--204</pages>
<contexts>
<context position="8025" citStr="Sagae et al., 2005" startWordPosition="1202" endWordPosition="1205">nalysis. We do not attempt to fully replicate the results of the manual studies, but rather provide a complementary set of features which can indicate grammatic abnormalities. Unlike previous computational studies, we attempt to move beyond single-word analysis and examine which patterns of syntax might indicate agrammatism. 2.2 Using parse features to assess grammaticality Syntactic complexity metrics derived from parse trees have been used by various researchers in studies of mild cognitive impairment (Roark et al., 2011), autism (Prud’hommeaux et al., 2011), and child language development (Sagae et al., 2005; Hassanali et al., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentence</context>
</contexts>
<marker>Sagae, Lavie, MacWhinney, 2005</marker>
<rawString>Kenji Sagae, Alon Lavie, and Brian MacWhinney. 2005. Automatic measurement of syntactic development in child language. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 197–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Swanson</author>
<author>Eugene Charniak</author>
</authors>
<title>Native language detection with tree substitution grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>193--197</pages>
<contexts>
<context position="8460" citStr="Swanson and Charniak, 2012" startWordPosition="1271" endWordPosition="1274">rees have been used by various researchers in studies of mild cognitive impairment (Roark et al., 2011), autism (Prud’hommeaux et al., 2011), and child language development (Sagae et al., 2005; Hassanali et al., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentences. For example, Wong and Dras (2010) used CFG productions to classify sentences from an artificial error corpus as being either grammatical or ungrammatical. Taking a different approach, Chae and Nenkova 1http://talkbank.org/AphasiaBank/ 135 Agrammatic Control (N = 24) (N = 15) Male/Female 15/9 8/7 Age (years) 58.1 (10.6) 63.3 (6.4) Education (years) 16.3 (2.5) 16.4 (2.4) Table 1: Demographic information. Numbers are given in the f</context>
</contexts>
<marker>Swanson, Charniak, 2012</marker>
<rawString>Ben Swanson and Eugene Charniak. 2012. Native language detection with tree substitution grammars. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 193– 197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia K Thompson</author>
<author>Lewis P Shapiro</author>
<author>Ligang Li</author>
<author>Lee Schendel</author>
</authors>
<title>Analysis of verbs and verb-argument structure: A method for quantification of aphasic language production. Clinical Aphasiology,</title>
<date>1995</date>
<pages>23--121</pages>
<contexts>
<context position="5192" citStr="Thompson et al. (1995)" startWordPosition="776" endWordPosition="779">ic aphasia has been performed manually. One widely used protocol is called Quantitative Production Analysis (QPA), developed by Saffran et al. (1989). QPA can be used to measure morphological content, such as whether determiners and verb inflections are produced in obligatory contexts, as well as structural complexity, such as the number of embedded clauses per sentence. Subsequent studies have found a number of differences between normal and agrammatic speech using QPA (Rochon et al., 2000). Another popular protocol called the Northwestern Narrative Language Analysis (NNLA) was introduced by Thompson et al. (1995). This protocol analyzes each utterance at five different levels, and focuses in particular on the production of verbs and verb argument structure. Perhaps more analogous to our work here, Goodglass et al. (1994) conducted a detailed examination of the syntactic constituents used by aphasic patients and controls. In that study, utterances were grouped according to how many syntactic constituents they contained. They found that agrammatic participants were more likely to produce single-constituent utterances, especially noun phrases, and less likely to produce subordinate clauses. They also fou</context>
</contexts>
<marker>Thompson, Shapiro, Li, Schendel, 1995</marker>
<rawString>Cynthia K. Thompson, Lewis P. Shapiro, Ligang Li, and Lee Schendel. 1995. Analysis of verbs and verb-argument structure: A method for quantification of aphasic language production. Clinical Aphasiology, 23:121–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Parser features for sentence grammaticality classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop,</booktitle>
<pages>67--75</pages>
<contexts>
<context position="8661" citStr="Wong and Dras (2010)" startWordPosition="1306" endWordPosition="1309">l., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentences. For example, Wong and Dras (2010) used CFG productions to classify sentences from an artificial error corpus as being either grammatical or ungrammatical. Taking a different approach, Chae and Nenkova 1http://talkbank.org/AphasiaBank/ 135 Agrammatic Control (N = 24) (N = 15) Male/Female 15/9 8/7 Age (years) 58.1 (10.6) 63.3 (6.4) Education (years) 16.3 (2.5) 16.4 (2.4) Table 1: Demographic information. Numbers are given in the form: mean (standard deviation). (2009) calculated several surface features based on the output of a parser, such as the length and relative proportion of different phrase types. They used these feature</context>
</contexts>
<marker>Wong, Dras, 2010</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2010. Parser features for sentence grammaticality classification. In Proceedings of the Australasian Language Technology Association Workshop, pages 67–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Exploiting parse structures for native language identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1600--1610</pages>
<contexts>
<context position="8407" citStr="Wong and Dras, 2011" startWordPosition="1263" endWordPosition="1266">actic complexity metrics derived from parse trees have been used by various researchers in studies of mild cognitive impairment (Roark et al., 2011), autism (Prud’hommeaux et al., 2011), and child language development (Sagae et al., 2005; Hassanali et al., 2013). Here we focus specifically on the use of CFG production rules as features. Using the CFG production rules from statistical parsers as features was first proposed by Baayen et al. (1996), who applied the features to an authorship attribution task. More recently, similar features have been widely used in native language identification (Wong and Dras, 2011; Brooke and Hirst, 2012; Swanson and Charniak, 2012). Perhaps most relevant to the task at hand, CFG productions as well as other parse outputs have proved useful for judging the grammaticality and fluency of sentences. For example, Wong and Dras (2010) used CFG productions to classify sentences from an artificial error corpus as being either grammatical or ungrammatical. Taking a different approach, Chae and Nenkova 1http://talkbank.org/AphasiaBank/ 135 Agrammatic Control (N = 24) (N = 15) Male/Female 15/9 8/7 Age (years) 58.1 (10.6) 63.3 (6.4) Education (years) 16.3 (2.5) 16.4 (2.4) Table 1</context>
</contexts>
<marker>Wong, Dras, 2011</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting parse structures for native language identification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1600–1610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Yngve</author>
</authors>
<title>A model and hypothesis for language structure.</title>
<date>1960</date>
<booktitle>Proceedings of the American Physical Society,</booktitle>
<pages>104--444</pages>
<contexts>
<context position="15216" citStr="Yngve, 1960" startWordPosition="2382" endWordPosition="2383">e selection, since any feature that was assigned a weight of zero was removed from the classification. We also consider tf-idf weights and unweighted features for comparison. 4.3 Syntactic complexity metrics To compare the performance of the parse features with more-traditional syntactic complexity metrics (SC metrics), we calculate the mean length of utterance (MLU), mean length of T-unit2 (MLT), mean length of clause (MLC), and parse tree height. We also calculate the mean, maximum, and total Yngve depth, which measures the proportion of left-branching to right-branching in each parse tree (Yngve, 1960). These measures are commonly used in studies of impaired language (e.g. Roark et al. (2011), Prud’hommeaux et 2A T-unit consists of a main clause and its attached dependent clauses. al. (2011), Fraser et al. (2013b)). We hypothesize that the parse features will capture more information about the specific impairments seen in agrammatic aphasia; however, using the general measures of syntactic complexity may be sufficient for the classifiers to distinguish between the groups. 4.4 Classification To test whether the features can effectively distinguish between the agrammatic group and controls, w</context>
</contexts>
<marker>Yngve, 1960</marker>
<rawString>Victor Yngve. 1960. A model and hypothesis for language structure. Proceedings of the American Physical Society, 104:444–466.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
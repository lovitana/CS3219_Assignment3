<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005368">
<title confidence="0.986102">
Randomized Significance Tests in Machine Translation
</title>
<author confidence="0.986169">
Yvette Graham Nitika Mathur Timothy Baldwin
</author>
<affiliation confidence="0.9960515">
Department of Computing and Information Systems
The University of Melbourne
</affiliation>
<email confidence="0.941276">
ygraham@unimelb.edu.au, nmathur@student.unimelb.edu.au, tb@ldwin.net
</email>
<sectionHeader confidence="0.997228" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999128789473684">
Randomized methods of significance test-
ing enable estimation of the probability
that an increase in score has occurred sim-
ply by chance. In this paper, we examine
the accuracy of three randomized meth-
ods of significance testing in the context
of machine translation: paired bootstrap
resampling, bootstrap resampling and ap-
proximate randomization. We carry out
a large-scale human evaluation of shared
task systems for two language pairs to
provide a gold standard for tests. Re-
sults show very little difference in accu-
racy across the three methods of signif-
icance testing. Notably, accuracy of all
test/metric combinations for evaluation of
English-to-Spanish are so low that there is
not enough evidence to conclude they are
any better than a random coin toss.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999731355932204">
Automatic metrics, such as BLEU (Papineni et
al., 2002), are widely used in machine translation
(MT) as a substitute for human evaluation. Such
metrics commonly take the form of an automatic
comparison of MT output text with one or more
human reference translations. Small differences
in automatic metric scores can be difficult to inter-
pret, however, and statistical significance testing
provides a way of estimating the likelihood that a
score difference has occurred simply by chance.
For several metrics, such as BLEU, standard sig-
nificance tests cannot be applied due to scores
not comprising the mean of individual sentence
scores, justifying the use of randomized methods.
Bootstrap resampling was one of the early ran-
domized methods proposed for statistical signifi-
cance testing of MT (Germann, 2003; Och, 2003;
Kumar and Byrne, 2004; Koehn, 2004), to assess
for a pair of systems how likely a difference in
BLEU scores occurred by chance. Empirical tests
detailed in Koehn (2004) show that even for test
sets as small as 300 translations, BLEU confidence
intervals can be computed as accurately as if they
had been computed on a test set 100 times as large.
Approximate randomization was subsequently
proposed as an alternate to bootstrap resam-
pling (Riezler and Maxwell, 2005). Theoretically
speaking, approximate randomization has an ad-
vantage over bootstrap resampling, in that it does
not make the assumption that samples are repre-
sentative of the populations from which they are
drawn. Both methods require some adaptation in
order to be used for the purpose of MT evalua-
tion, such as combination with an automatic met-
ric, and therefore it cannot be taken for granted
that approximate randomization will be more ac-
curate in practice. Within MT, approximate ran-
domization for the purpose of statistical testing is
also less common.
Riezler and Maxwell (2005) provide a compar-
ison of approximate randomization with bootstrap
resampling (distinct from paired bootstrap resam-
pling), and conclude that since approximate ran-
domization produces higher p-values for a set of
apparently equally-performing systems, it more
conservatively concludes statistically significant
differences, and recommend preference of approx-
imate randomization over bootstrap resampling
for MT evaluation. Conclusions drawn from ex-
periments provided in Riezler and Maxwell (2005)
are oft-cited, with experiments interpreted as ev-
idence that bootstrap resampling is overly opti-
mistic in reporting significant differences (Riezler
and Maxwell, 2006; Koehn and Monz, 2006; Gal-
ley and Manning, 2008; Green et al., 2010; Monz,
2011; Clark et al., 2011).
Our contribution in this paper is to revisit sta-
tistical significance tests in MT — namely, boot-
strap resampling, paired bootstrap resampling and
</bodyText>
<page confidence="0.974592">
266
</page>
<note confidence="0.714706">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99937675">
approximate randomization — and find problems
with the published formulations. We redress these
issues, and apply the tests in statistical testing of
two language pairs. Using human judgments of
translation quality, we find only very minor differ-
ences in significance levels across the three tests,
challenging claims made in the literature about rel-
ative merits of tests.
</bodyText>
<subsectionHeader confidence="0.559074">
2 Revisiting Statistical Significance Tests
for MT Evaluation
</subsectionHeader>
<bodyText confidence="0.999946136986302">
First, we revisit the formulations of bootstrap
resampling and approximate randomization al-
gorithms as presented in Riezler and Maxwell
(2005). At first glance, both methods appear to
be two-tailed tests, with the null hypothesis that
the two systems perform equally well. To facili-
tate a two-tailed test, absolute values of pseudo-
statistics are computed before locating the abso-
lute value of the actual statistic (original differ-
ence in scores). Using absolute values of pseudo-
statistics is not problematic in the approximate
randomization algorithm, and results in a reason-
able two-tailed significance test. However, the
bootstrap algorithm they provide uses an addi-
tional shift-to-zero method of simulating the null
hypothesis. The way in which this shift-to-zero
and absolute values of pseudo-statistics are ap-
plied is non-standard. Combining shift-to-zero
and absolute values of pseudo-statistics results
in all pseudo-statistics that fall below the mean
pseudo-statistic to be omitted from computation of
counts later used to compute p-values. The ver-
sion of the bootstrap algorithm, as provided in the
pseudo-code, is effectively a one-tailed test, and
since this does not happen in the approximate ran-
domization algorithm, experiments appear to com-
pare p-values from a one-tailed bootstrap test di-
rectly with those of a two-tailed approximate ran-
domization test. This inconsistency is not recog-
nized, however, and p-values are compared as if
both tests are two-tailed.
A better comparison of p-values would first re-
quire doubling the values of the one-sided boot-
strap, leaving those of the two-sided approximate
randomization algorithm as-is. The results of the
two tests on this basis are extremely close, and
in fact, in two out of the five comparisons, those
of the bootstrap would have marginally higher p-
values than those of approximate randomization.
As such, it is conceivable to conclude that the ex-
periments actually show no substantial difference
in Type I error between the two tests, which is con-
sistent with results published in other fields of re-
search (Smucker et al., 2007). We also note that
the pseudo-code contains an unconventional com-
putation of mean pseudo-statistics, rB, for shift-
to-zero.
Rather than speculate over whether these is-
sues with the original paper were simply presen-
tational glitches or the actual basis of the experi-
ments reported on in the paper, we present a nor-
malized version of the two-sided bootstrap algo-
rithm in Figure 1, and report on the results of our
own experiments in Section 4. We compare this
method with approximate randomization and also
paired bootstrap resampling (Koehn, 2004), which
is widely used in MT evaluation. We carry out
evaluation over a range of MT systems, not only
including pairs of systems that perform equally
well, but also pairs of systems for which one
system performs marginally better than the other.
This enables evaluation of not only Type I error,
but the overall accuracy of the tests. We carry out
a large-scale human evaluation of all WMT 2012
shared task participating systems for two language
pairs, and collect sufficient human judgments to
facilitate statistical significance tests. This hu-
man evaluation data then provides a gold-standard
against which to compare randomized tests. Since
all randomized tests only function in combina-
tion with an automatic MT evaluation metric, we
present results of each randomized test across four
different MT metrics.
</bodyText>
<sectionHeader confidence="0.990343" genericHeader="method">
3 Randomized Significance Tests
</sectionHeader>
<subsectionHeader confidence="0.999685">
3.1 Bootstrap Resampling
</subsectionHeader>
<bodyText confidence="0.999819266666667">
Bootstrap resampling provides a way of estimat-
ing the population distribution by sampling with
replacement from a representative sample (Efron
and Tibshirani, 1993). The test statistic is taken
as the difference in scores of the two systems,
SX − SY , which has an expected value of 0 under
the null hypothesis that the two systems perform
equally well. A bootstrap pseudo-sample consists
of the translations by the two systems (Xb, Yb) of
a bootstrapped test set (Koehn, 2004), constructed
by sampling with replacement from the original
test set translations. The bootstrap distribution
Sboot of the test statistic is estimated by calculat-
ing the value of the pseudo-statistic SXb − SYb for
each pseudo-sample.
</bodyText>
<page confidence="0.986717">
267
</page>
<figure confidence="0.925011066666667">
Set c = 0
Compute actual statistic of score differences SX − SY
on test data
B
Calculate sample mean TB = B1 E SXb − SYb over
b=1
bootstrap samples b = 1, ..., B
For bootstrap samples b = 1, ..., B
Sample with replacement from variable tuples test
sentences for systems X and Y
Compute pseudo-statistic SXb − SYb on bootstrap data
If |SXb − SYb − TB |1 |SX − SY |
c = c + 1
If c/B &lt; α
Reject the null hypothesis
</figure>
<figureCaption confidence="0.992634">
Figure 1: Two-sided bootstrap resampling statisti-
cal significance test for automatic MT evaluation
</figureCaption>
<figure confidence="0.867730916666667">
Set c = 0
Compute actual statistic of score differences SX − SY
on test data
For random shuffles r = 1, ..., R
For sentences in test set
Shuffle variable tuples between systems X and Y
with probability 0.5
Compute pseudo-statistic SXr − SYr on shuffled data
If SXr − SYr 1 SX − SY
c = c + 1
If c/R &lt; α
Reject the null hypothesis
</figure>
<figureCaption confidence="0.819237">
Figure 2: Approximate randomization statistical
significance test for automatic MT evaluation
</figureCaption>
<bodyText confidence="0.9574865">
The null hypothesis distribution SH0 can be es-
timated from Sboot by applying the shift method
(Noreen, 1989), which assumes that SH0 has the
same shape but a different mean than Sboot. Thus,
Sboot is transformed into SH0 by subtracting the
mean bootstrap statistic from every value in Sboot.
Once this shift-to-zero has taken place, the null
hypothesis is rejected if the probability of observ-
ing a more extreme value than the actual statistic
is lower than a predetermined p-value α, which is
typically set to 0.05. In other words, the score dif-
ference is significant at level 1 − α.
Figure 3 provides a one-sided implementation
of bootstrap resampling, where Ho is that the score
of System X is less than or equal to the score of
Set c = 0
Compute actual statistic of score differences SX − SY
on test data
</bodyText>
<figure confidence="0.99598525">
B
Calculate sample mean TB = B1 E SXb − SYb over
b=1
bootstrap samples b = 1, ..., B
For bootstrap samples b = 1, ..., B
Sample with replacement from variable tuples test
sentences for systems X and Y
Compute pseudo-statistic SXb − SYb on bootstrap data
If SXb − SYb − TB 1 SX − SY
c = c + 1
If c/B &lt; α
Reject the null hypothesis
</figure>
<figureCaption confidence="0.987791">
Figure 3: One-sided Bootstrap resampling statisti-
cal significance test for automatic MT evaluation
</figureCaption>
<figure confidence="0.990467333333333">
Set c = 0
For bootstrap samples b = 1, ..., B
If SXb &lt; SYb
c = c + 1
If c/B &lt; α
Reject the null hypothesis
</figure>
<figureCaption confidence="0.960865">
Figure 4: Paired bootstrap resampling randomized
significance test
</figureCaption>
<bodyText confidence="0.999722833333333">
System Y . Figure 5 includes a typical example of
bootstrap resampling applied to BLEU, for a pair
of systems for which differences in scores are sig-
nificant, while Figure 6 shows the same for ME-
TEOR but for a pair of systems with no significant
difference in scores.
</bodyText>
<subsectionHeader confidence="0.999692">
3.2 Approximate Randomization
</subsectionHeader>
<bodyText confidence="0.999950384615385">
Unlike bootstrap, approximate randomization
does not make any assumptions about the popula-
tion distribution. To simulate a distribution for the
null hypothesis that the scores of the two systems
are the same, translations are shuffled between the
two systems so that 50% of each pseudo-sample
is drawn from each system. In the context of ma-
chine translation, this can be interpreted as each
translation being equally likely to have been pro-
duced by one system as the other (Riezler and
Maxwell, 2005).
The test statistic is taken as the difference in
scores of the two systems, SX − SY . If there is
</bodyText>
<page confidence="0.981289">
268
</page>
<figure confidence="0.982799307692308">
Paired Bootstrap Res. BLEU Bootstrap Resampling BLEU Approximate Randomization BLEU
0 100 200 300 400
c = 13
origin
0 100 200 300 400
c = 14
actual statistic
0 100 200 300 400
c = 11
actual statistic
−0.015 −0.005 0.005 0.015
−0.015 −0.005 0.005 0.015
−0.015 −0.005 0.005 0.015
</figure>
<figureCaption confidence="0.98447">
Figure 5: Pseudo-statistic distributions for a typical pair of systems with close BLEU scores for each
randomized test (System F vs. System G).
</figureCaption>
<figure confidence="0.99163975">
Paired Bootstrap Res. METEOR Bootstrap Resampling METEOR Approximate Randomization METEOR
0 100 200 300 400
c = 269 origin
0 100 200 300 400
c = 275
actual statistic
0 100 200 300 400
c = 260
actual statistic
−0.015 −0.005 0.005 0.015
−0.015 −0.005 0.005 0.015
−0.015 −0.005 0.005 0.015
</figure>
<figureCaption confidence="0.999906">
Figure 6: Pseudo-statistic distributions of METEOR with randomized tests (System D vs. System A).
</figureCaption>
<bodyText confidence="0.999799363636364">
a total of S sentences, then a total of 2S shuffles is
possible. If S is large, instead of generating all 2S
possible combinations, we instead generate sam-
ples by randomly permuting translations between
the two systems with equal probability. The distri-
bution of the test statistic under the null hypoth-
esis is approximated by calculating the pseudo-
statistic, SX, − SY,., for each sample. As before,
the null hypothesis is rejected if the probability of
observing a more extreme value than the actual
test statistic is lower than α.
Figure 2 provides a one-sided implementation
of approximate randomization for MT evaluation,
where the null hypothesis is that the score of Sys-
tem X is less than or equal to the score of System
Y . Figure 5 shows a typical example of pseudo-
statistic distributions for approximate randomiza-
tion for a pair of systems with a small but signifi-
cant score difference according to BLEU, and Fig-
ure 6 shows the same for METEOR applied to a
pair of systems where no significant difference is
concluded.
</bodyText>
<subsectionHeader confidence="0.996637">
3.3 Paired Bootstrap Resampling
</subsectionHeader>
<bodyText confidence="0.999681">
Paired bootstrap resampling (Koehn, 2004) is
shown in Figure 4. Unlike the other two random-
ized tests, this method makes no attempt to simu-
late the null hypothesis distribution. Instead, boot-
strap samples are used to estimate confidence in-
tervals of score differences, with confidence inter-
vals not containing 0 implying a statistically sig-
nificant difference.
We compare what takes place with the two other
tests, by plotting differences in scores for boot-
strapped samples, SX, − SYb, as shown in Fig-
ure 5 for BLEU and Figure 6 for METEOR. Instead
of computing counts with reference to the actual
statistic, the line through the origin provides the
cut-off for counts.
</bodyText>
<page confidence="0.980987">
269
</page>
<figure confidence="0.999877260000001">
System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
p-value
Adequacy Fluency Combined
System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
</figure>
<figureCaption confidence="0.9886485">
Figure 7: Human evaluation pairwise significance tests for Spanish-to-English systems (colored cells
denote scores for System row being significantly greater than System column.
</figureCaption>
<sectionHeader confidence="0.998833" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999941610169492">
In order to evaluate the accuracy of the three ran-
domized significance significance tests, we com-
pare conclusions reached in a human evaluation
of shared task participant systems. We carry out
a large-scale human evaluation of all participating
systems from WMT 2012 (Callison-Burch et al.,
2012) for the Spanish-to-English and English-to-
Spanish translation tasks. Large numbers of hu-
man assessments of translations were collected us-
ing Amazon’s Mechanical Turk, with strict qual-
ity control filtering (Graham et al., 2013). A to-
tal of 82,100 human adequacy assessments and
62,400 human fluency assessments were collected.
After the removal of quality control items and
filtering of judgments from low-quality workers,
this resulted in an average of 1,280 adequacy and
1,013 fluency assessments per system for Spanish-
to-English (12 systems), and 1,483 adequacy and
1,534 fluency assessments per system for English-
to-Spanish (11 systems). To remove bias with re-
spect to individual human judge preference scor-
ing severity/leniency, scores provided by each hu-
man assessor were standardized according to the
mean and standard deviation of all scores provided
by that individual.
Significance tests were carried out over the
scores for each pair of systems separately for
adequacy and fluency assessments using the
Wilcoxon rank-sum test. Figure 7 shows pairwise
significance test results for fluency, adequacy and
the combination of the two tests, for all pairs of
Spanish-to-English systems. Combined fluency
and adequacy significance test results are con-
structed as follows: if a system’s adequacy score is
significantly greater than that of another, the com-
bined conclusion is that it is significantly better,
at that significance level. Only when a tie in ad-
equacy scores occurs are fluency judgments used
to break the tie. In this case, p-values from signifi-
cance tests applied to fluency scores of that system
pair are used. For example, in Figure 7, adequacy
scores of System B are not significantly greater
than those of Systems C, D and E, while fluency
scores for System B are significantly greater than
those of the three other systems. The combined re-
sult for each pair of systems is therefore taken as
the p-value from the corresponding fluency signif-
icance test.
We use the combined human evaluation pair-
wise significant tests as a gold standard against
which to evaluate the randomized methods of sta-
tistical significance testing. We evaluate paired
bootstrap resampling (Koehn, 2004) and bootstrap
resampling as shown in Figure 3 and approxi-
mate randomization as shown in Figure 2, each
in combination with four automatic MT metrics:
BLEU (Papineni et al., 2002), NIST (NIST, 2002),
METEOR (Banerjee and Lavie, 2005) and TER
(Snover et al., 2006).
</bodyText>
<subsectionHeader confidence="0.730677">
4.1 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.989538875">
Figure 8 shows the outcome of pairwise random-
ized significance tests for each metric for Spanish-
to-English systems, and Table 1 shows numbers of
correct conclusions and accuracy of each test.
When we compare conclusions made by the
three randomized tests for Spanish-to-English sys-
tems, there is very little difference in p-values for
all pairs of systems. For both BLEU and NIST,
</bodyText>
<page confidence="0.990762">
270
</page>
<table confidence="0.999877529411765">
α Paired Bootst. Resamp. Bootst. Resamp. Approx. Rand.
Conc. Acc.(%) Conc. Acc. (%) Conc. Acc. (%)
BLEU 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1]
NIST 54 81.8 [70.4, 90.2] 54 81.8 [70.4, 90.2] 54 81.8 [70.4, 90.2]
0.05
METEOR 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9]
TER 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9] 52 78.8 [67.0, 87.9]
BLEU 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]
NIST 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]
0.01
METEOR 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1]
TER 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7] 51 77.3 [65.3, 86.7]
BLEU 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0]
NIST 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0] 48 72.7 [60.4, 83.0]
0.001
METEOR 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 52 78.8 [67.0, 87.9]
TER 50 75.8 [63.6, 85.5] 51 77.3 [65.3, 86.7] 52 78.8 [67.0, 87.9]
</table>
<tableCaption confidence="0.9958925">
Table 1: Accuracy of randomized significance tests for Spanish-to-English MT with four automatic
metrics, based on the WMT 2012 participant systems.
</tableCaption>
<table confidence="0.999740571428572">
α Paired Bootst. Resamp. Bootst. Resamp. Approx. Rand.
Conc. Acc.(%) Conc. Acc. (%) Conc. Acc. (%)
0.05 BLEU 34 61.8 [47.7, 74.6] 34 61.8 [47.7, 74.6] 34 61.8 [47.7, 74.6]
0.01 NIST 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
0.001 METEOR 31 56.4 [42.3, 69.7] 31 56.4 [42.3, 69.7] 31 56.4 [42.3, 69.7]
TER 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
BLEU 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0]
NIST 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
METEOR 31 56.4 [42.3, 69.7] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
TER 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0]
BLEU 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0] 33 60.0 [45.9, 73.0]
NIST 33 60.0 [45.9, 73.0] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
METEOR 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3] 32 58.2 [44.1, 71.3]
TER 30 54.5 [40.6, 68.0] 30 54.5 [40.6, 68.0] 31 56.4 [42.3, 69.7]
</table>
<tableCaption confidence="0.953445">
Table 2: Accuracy of randomized significance tests for English-to-Spanish MT with four automatic
metrics, based on the WMT 2012 participant systems.
</tableCaption>
<bodyText confidence="0.999566166666667">
all three randomized methods produce p-values
so similar that when α thresholds are applied, all
three tests produce precisely the same set of pair-
wise conclusions for each metric. When tests are
combined with METEOR and TER, similar results
are observed: at the α thresholds of 0.05 and 0.01,
precisely the same conclusions are drawn for both
metrics combined with each of the three tests, and
at most a difference of two conclusions at the low-
est α level.
Table 2 shows the accuracy of each test on the
English-to-Spanish data, showing much the same
set of conclusions at all α levels. For BLEU and
NIST, all three tests again produce precisely the
same conclusions, at p &lt; 0.01 there is at most a
single different conclusion for METEOR, and only
at the lowest p-value level is there a single differ-
ence for TER.
</bodyText>
<page confidence="0.991547">
271
</page>
<figure confidence="0.997106790697675">
Paired Bootstrap Bootstrap Approximate
Resampling Resampling Randomization
METEOR TER NIST BLEU System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
System.A
System.B
System.C
System.D
System.E
System.F
System.G
System.H
System.J
System.K
System.L
System.M
</figure>
<figureCaption confidence="0.8970145">
Figure 8: Automatic metric pairwise randomized significance test results for Spanish-to-English systems
(colored cells denote scores for System row significantly greater than System column).
</figureCaption>
<bodyText confidence="0.9992385">
Finally, we examine which combination of met-
ric and test is most accurate for each language
pair at the conventional significance level of p &lt;
0.05. For Spanish-to-English evaluation, NIST
combined with any of the three randomized tests
is most accurate, making 54 out of 66 (82%) cor-
rect conclusions. For English-to-Spanish, BLEU
in combination with any of the three randomized
tests, is most accurate at 62%. For both language
pairs, however, differences in accuracy for metrics
</bodyText>
<page confidence="0.987315">
272
</page>
<bodyText confidence="0.999546583333334">
are not significant (Chi-square test).
For English-to-Spanish evaluation, an accuracy
as low as 62% should be a concern. This level
of accuracy for significance testing – only making
the correct conclusion in 6 out of 10 tests – acts
as a reminder that no matter how sophisticated the
significance test, it will never make up for flaws in
an underlying metric. When we take into account
the fact that lower confidence limits all fall below
50%, significance tests based on these metrics for
English-to-Spanish are effectively no better than a
random coin toss.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999996176470588">
We provided a comparison of bootstrap resam-
pling and approximate randomization significance
tests for a range of automatic machine trans-
lation evaluation metrics. To provide a gold-
standard against which to evaluate randomized
tests, we carried out a large-scale human evalua-
tion of all shared task participating systems for the
Spanish-to-English and English-to-Spanish trans-
lation tasks from WMT 2012. Results showed for
many metrics and significance levels that all three
tests produce precisely the same set of conclu-
sions, and when conclusions do differ, it is com-
monly only by a single contrasting conclusion,
which is not significant. For English-to-Spanish
MT, the results of the different MT evaluation met-
ric/significance test combinations are not signifi-
cantly higher than a random baseline.
</bodyText>
<sectionHeader confidence="0.993987" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.903068666666667">
We wish to thank the anonymous reviewers for their valuable
comments. This research was supported by funding from the
Australian Research Council.
</bodyText>
<sectionHeader confidence="0.998056" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999652970588236">
S. Banerjee and A. Lavie. 2005. METEOR: An au-
tomatic metric for mt evaluation with improved cor-
relation with human judgements. In Proc. Wkshp.
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, pages 65–
73, Ann Arbor, MI. ACL.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proc. 7th Wkshp. Statistical Machine Translation,
pages 10–51, Montreal, Canada. ACL.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith.
2011. Better hypothesis testing for statistical ma-
chine translation: Controlling for optimizer instabil-
ity. In Proc. of the 49th Annual Meeting of the As-
soc. Computational Linguistics: Human Language
Technologies: short papers-Volume 2, pages 176–
181, Portland, OR. ACL.
B. Efron and R. J. Tibshirani. 1993. An Introduction
to the Bootstrap. Chapman &amp; Hall, New York City,
NY.
M. Galley and C. D. Manning. 2008. A simple and
effective hierarchical phrase reordering model. In
Proc. of the Conference on Empirical Methods in
Natural Language Processing, pages 848–856, Ed-
inburgh, Scotland. ACL.
U. Germann. 2003. Greedy decoding for statisti-
cal machine translation in almost linear time. In
Proc. of the 2003 Conference of the North American
Chapter of the Assoc. Computational Linguistics on
Human Language Technology-Volume 1, pages 1–8,
Edmonton, Canada. ACL.
Y. Graham, T. Baldwin, A. Moffat, and J. Zobel. 2013.
Continuous measurement scales in human evalua-
tion of machine translation. In Proc. 7th Linguis-
tic Annotation Wkshp. &amp; Interoperability with Dis-
course, pages 33–41, Sofia, Bulgaria. ACL.
S. Green, M. Galley, and C. D. Manning. 2010. Im-
proved models of distortion cost for statistical ma-
chine translation. In Human Language Technolo-
gies: The 2010 Annual Conference of the North
American Chapter of the Assoc. Computational Lin-
guistics, pages 867–875, Los Angeles, CA. ACL.
P. Koehn and C. Monz. 2006. Manual and automatic
evaluation of machine translation between European
languages. In Proceedings of the Workshop on Sta-
tistical Machine Translation, pages 102–121, New
York City, NY. ACL.
P. Koehn. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proc. of Empiri-
cal Methods in Natural Language Processing, pages
388–395, Barcelona, Spain. ACL.
S. Kumar and W. J. Byrne. 2004. Minimum Bayes-
risk decoding for statistical machine translation. In
HLT-NAACL, pages 169–176, Boston, MA. ACL.
C. Monz. 2011. Statistical machine translation with lo-
cal language models. In Proc. of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 869–879, Edniburgh, Scotland. ACL.
NIST. 2002. Automatic Evaluation of Machine Trans-
lation Quality Using N-gram Co-Occurrence Statis-
tics. Technical report.
E. W. Noreen. 1989. Computer intensive methods for
testing hypotheses. Wiley, New York City, NY.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. 41st Ann. Meet-
ing of the Assoc. Computational Linguistics, pages
160–167, Sapporo, Japan. ACL.
</reference>
<page confidence="0.980661">
273
</page>
<reference confidence="0.999753214285714">
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002. A method for automatic evaluation of ma-
chine translation. In Proc. 40th Ann. Meeting of the
Assoc. Computational Linguistics, pages 311–318,
Philadelphia, PA. ACL.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls
in automatic evaluation and significance testing for
mt. In Proc. of the ACL Workshop on Intrinsic and
Extrinsic Evaluation Measures for Machine Trans-
lation and/or Summarization, pages 57–64, Ann Ar-
bor, MI. ACL.
S. Riezler and J. T. Maxwell. 2006. Grammatical
machine translation. In Proc. of the Main Confer-
ence on Human Language Technology Conference of
the North American Chapter of the Assoc. Computa-
tional Linguistics, pages 248–255, New York City,
NY. ACL.
M. Smucker, J. Allan, and B. Carterette. 2007. A com-
parison of statistical significance tests for informa-
tion retrieval evaluation. In Proc. of the Sixteenth
ACM Conference on Information and Knowledge
Management (CIKM 2007), pages 623–632, Lisbon,
Portugal. ACM.
M. Snover, B. Dorr, R. Scwartz, J. Makhoul, and
L. Micciula. 2006. A study of translation error rate
with targeted human annotation. In Proc. 7th Bien-
nial Conf. of the Assoc. Machine Translaiton in the
Americas, pages 223–231, Boston, MA. ACL.
</reference>
<page confidence="0.998296">
274
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.988732">
<title confidence="0.999288">Randomized Significance Tests in Machine Translation</title>
<author confidence="0.999897">Yvette Graham Nitika Mathur Timothy Baldwin</author>
<affiliation confidence="0.997885">Department of Computing and Information The University of Melbourne</affiliation>
<abstract confidence="0.99967545">Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shared task systems for two language pairs to provide a gold standard for tests. Results show very little difference in accuracy across the three methods of significance testing. Notably, accuracy of all test/metric combinations for evaluation of English-to-Spanish are so low that there is not enough evidence to conclude they are any better than a random coin toss.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>A Lavie</author>
</authors>
<title>METEOR: An automatic metric for mt evaluation with improved correlation with human judgements.</title>
<date>2005</date>
<booktitle>In Proc. Wkshp. Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--73</pages>
<publisher>ACL.</publisher>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="17825" citStr="Banerjee and Lavie, 2005" startWordPosition="2870" endWordPosition="2873">r than those of the three other systems. The combined result for each pair of systems is therefore taken as the p-value from the corresponding fluency significance test. We use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing. We evaluate paired bootstrap resampling (Koehn, 2004) and bootstrap resampling as shown in Figure 3 and approximate randomization as shown in Figure 2, each in combination with four automatic MT metrics: BLEU (Papineni et al., 2002), NIST (NIST, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). 4.1 Results and Discussion Figure 8 shows the outcome of pairwise randomized significance tests for each metric for Spanishto-English systems, and Table 1 shows numbers of correct conclusions and accuracy of each test. When we compare conclusions made by the three randomized tests for Spanish-to-English systems, there is very little difference in p-values for all pairs of systems. For both BLEU and NIST, 270 α Paired Bootst. Resamp. Bootst. Resamp. Approx. Rand. Conc. Acc.(%) Conc. Acc. (%) Conc. Acc. (%) BLEU 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 53 80.3 [6</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>S. Banerjee and A. Lavie. 2005. METEOR: An automatic metric for mt evaluation with improved correlation with human judgements. In Proc. Wkshp. Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65– 73, Ann Arbor, MI. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proc. 7th Wkshp. Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>ACL.</publisher>
<location>Montreal, Canada.</location>
<contexts>
<context position="15394" citStr="Callison-Burch et al., 2012" startWordPosition="2487" endWordPosition="2490">tem.J System.K System.L System.M System.A System.B System.C System.D System.E System.F System.G System.H System.J System.K System.L System.M Figure 7: Human evaluation pairwise significance tests for Spanish-to-English systems (colored cells denote scores for System row being significantly greater than System column. 4 Evaluation In order to evaluate the accuracy of the three randomized significance significance tests, we compare conclusions reached in a human evaluation of shared task participant systems. We carry out a large-scale human evaluation of all participating systems from WMT 2012 (Callison-Burch et al., 2012) for the Spanish-to-English and English-toSpanish translation tasks. Large numbers of human assessments of translations were collected using Amazon’s Mechanical Turk, with strict quality control filtering (Graham et al., 2013). A total of 82,100 human adequacy assessments and 62,400 human fluency assessments were collected. After the removal of quality control items and filtering of judgments from low-quality workers, this resulted in an average of 1,280 adequacy and 1,013 fluency assessments per system for Spanishto-English (12 systems), and 1,483 adequacy and 1,534 fluency assessments per sy</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proc. 7th Wkshp. Statistical Machine Translation, pages 10–51, Montreal, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Clark</author>
<author>C Dyer</author>
<author>A Lavie</author>
<author>N A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proc. of the 49th Annual Meeting of the Assoc. Computational Linguistics: Human Language Technologies: short</booktitle>
<volume>2</volume>
<pages>176--181</pages>
<publisher>ACL.</publisher>
<location>Portland, OR.</location>
<contexts>
<context position="3661" citStr="Clark et al., 2011" startWordPosition="551" endWordPosition="554">nce approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics approximate randomization — and find problems with the published formulations. We redress these issues, and apply the tests in statistical testing of two language pairs. Using human judgments of translation quality, we find only very minor differences in significance lev</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proc. of the 49th Annual Meeting of the Assoc. Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 176– 181, Portland, OR. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Efron</author>
<author>R J Tibshirani</author>
</authors>
<title>An Introduction to the Bootstrap.</title>
<date>1993</date>
<publisher>Chapman &amp; Hall,</publisher>
<location>New York City, NY.</location>
<contexts>
<context position="8100" citStr="Efron and Tibshirani, 1993" startWordPosition="1236" endWordPosition="1239">k participating systems for two language pairs, and collect sufficient human judgments to facilitate statistical significance tests. This human evaluation data then provides a gold-standard against which to compare randomized tests. Since all randomized tests only function in combination with an automatic MT evaluation metric, we present results of each randomized test across four different MT metrics. 3 Randomized Significance Tests 3.1 Bootstrap Resampling Bootstrap resampling provides a way of estimating the population distribution by sampling with replacement from a representative sample (Efron and Tibshirani, 1993). The test statistic is taken as the difference in scores of the two systems, SX − SY , which has an expected value of 0 under the null hypothesis that the two systems perform equally well. A bootstrap pseudo-sample consists of the translations by the two systems (Xb, Yb) of a bootstrapped test set (Koehn, 2004), constructed by sampling with replacement from the original test set translations. The bootstrap distribution Sboot of the test statistic is estimated by calculating the value of the pseudo-statistic SXb − SYb for each pseudo-sample. 267 Set c = 0 Compute actual statistic of score diff</context>
</contexts>
<marker>Efron, Tibshirani, 1993</marker>
<rawString>B. Efron and R. J. Tibshirani. 1993. An Introduction to the Bootstrap. Chapman &amp; Hall, New York City, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>848--856</pages>
<publisher>ACL.</publisher>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="3608" citStr="Galley and Manning, 2008" startWordPosition="540" endWordPosition="544">ct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics approximate randomization — and find problems with the published formulations. We redress these issues, and apply the tests in statistical testing of two language pairs. Using human judgments of translation quality, we</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>M. Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proc. of the Conference on Empirical Methods in Natural Language Processing, pages 848–856, Edinburgh, Scotland. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Germann</author>
</authors>
<title>Greedy decoding for statistical machine translation in almost linear time.</title>
<date>2003</date>
<booktitle>In Proc. of the 2003 Conference of the North American Chapter of the Assoc. Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>1--8</pages>
<publisher>ACL.</publisher>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1836" citStr="Germann, 2003" startWordPosition="273" endWordPosition="274">parison of MT output text with one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as BLEU, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in Koehn (2004) show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005). Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make </context>
</contexts>
<marker>Germann, 2003</marker>
<rawString>U. Germann. 2003. Greedy decoding for statistical machine translation in almost linear time. In Proc. of the 2003 Conference of the North American Chapter of the Assoc. Computational Linguistics on Human Language Technology-Volume 1, pages 1–8, Edmonton, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Graham</author>
<author>T Baldwin</author>
<author>A Moffat</author>
<author>J Zobel</author>
</authors>
<title>Continuous measurement scales in human evaluation of machine translation.</title>
<date>2013</date>
<booktitle>In Proc. 7th Linguistic Annotation Wkshp. &amp; Interoperability with Discourse,</booktitle>
<pages>33--41</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="15620" citStr="Graham et al., 2013" startWordPosition="2520" endWordPosition="2523">d cells denote scores for System row being significantly greater than System column. 4 Evaluation In order to evaluate the accuracy of the three randomized significance significance tests, we compare conclusions reached in a human evaluation of shared task participant systems. We carry out a large-scale human evaluation of all participating systems from WMT 2012 (Callison-Burch et al., 2012) for the Spanish-to-English and English-toSpanish translation tasks. Large numbers of human assessments of translations were collected using Amazon’s Mechanical Turk, with strict quality control filtering (Graham et al., 2013). A total of 82,100 human adequacy assessments and 62,400 human fluency assessments were collected. After the removal of quality control items and filtering of judgments from low-quality workers, this resulted in an average of 1,280 adequacy and 1,013 fluency assessments per system for Spanishto-English (12 systems), and 1,483 adequacy and 1,534 fluency assessments per system for Englishto-Spanish (11 systems). To remove bias with respect to individual human judge preference scoring severity/leniency, scores provided by each human assessor were standardized according to the mean and standard d</context>
</contexts>
<marker>Graham, Baldwin, Moffat, Zobel, 2013</marker>
<rawString>Y. Graham, T. Baldwin, A. Moffat, and J. Zobel. 2013. Continuous measurement scales in human evaluation of machine translation. In Proc. 7th Linguistic Annotation Wkshp. &amp; Interoperability with Discourse, pages 33–41, Sofia, Bulgaria. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>Improved models of distortion cost for statistical machine translation.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Assoc. Computational Linguistics,</booktitle>
<pages>867--875</pages>
<publisher>ACL.</publisher>
<location>Los Angeles, CA.</location>
<contexts>
<context position="3628" citStr="Green et al., 2010" startWordPosition="545" endWordPosition="548">esampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics approximate randomization — and find problems with the published formulations. We redress these issues, and apply the tests in statistical testing of two language pairs. Using human judgments of translation quality, we find only very mino</context>
</contexts>
<marker>Green, Galley, Manning, 2010</marker>
<rawString>S. Green, M. Galley, and C. D. Manning. 2010. Improved models of distortion cost for statistical machine translation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Assoc. Computational Linguistics, pages 867–875, Los Angeles, CA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between European languages.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<publisher>ACL.</publisher>
<location>New York City, NY.</location>
<contexts>
<context position="3582" citStr="Koehn and Monz, 2006" startWordPosition="536" endWordPosition="539">rap resampling (distinct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics approximate randomization — and find problems with the published formulations. We redress these issues, and apply the tests in statistical testing of two language pairs. Using human judgments </context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>P. Koehn and C. Monz. 2006. Manual and automatic evaluation of machine translation between European languages. In Proceedings of the Workshop on Statistical Machine Translation, pages 102–121, New York City, NY. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<publisher>ACL.</publisher>
<location>Barcelona,</location>
<contexts>
<context position="1884" citStr="Koehn, 2004" startWordPosition="281" endWordPosition="282">reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as BLEU, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in Koehn (2004) show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005). Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative o</context>
<context position="7068" citStr="Koehn, 2004" startWordPosition="1079" endWordPosition="1080">sults published in other fields of research (Smucker et al., 2007). We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, rB, for shiftto-zero. Rather than speculate over whether these issues with the original paper were simply presentational glitches or the actual basis of the experiments reported on in the paper, we present a normalized version of the two-sided bootstrap algorithm in Figure 1, and report on the results of our own experiments in Section 4. We compare this method with approximate randomization and also paired bootstrap resampling (Koehn, 2004), which is widely used in MT evaluation. We carry out evaluation over a range of MT systems, not only including pairs of systems that perform equally well, but also pairs of systems for which one system performs marginally better than the other. This enables evaluation of not only Type I error, but the overall accuracy of the tests. We carry out a large-scale human evaluation of all WMT 2012 shared task participating systems for two language pairs, and collect sufficient human judgments to facilitate statistical significance tests. This human evaluation data then provides a gold-standard again</context>
<context position="8413" citStr="Koehn, 2004" startWordPosition="1294" endWordPosition="1295"> we present results of each randomized test across four different MT metrics. 3 Randomized Significance Tests 3.1 Bootstrap Resampling Bootstrap resampling provides a way of estimating the population distribution by sampling with replacement from a representative sample (Efron and Tibshirani, 1993). The test statistic is taken as the difference in scores of the two systems, SX − SY , which has an expected value of 0 under the null hypothesis that the two systems perform equally well. A bootstrap pseudo-sample consists of the translations by the two systems (Xb, Yb) of a bootstrapped test set (Koehn, 2004), constructed by sampling with replacement from the original test set translations. The bootstrap distribution Sboot of the test statistic is estimated by calculating the value of the pseudo-statistic SXb − SYb for each pseudo-sample. 267 Set c = 0 Compute actual statistic of score differences SX − SY on test data B Calculate sample mean TB = B1 E SXb − SYb over b=1 bootstrap samples b = 1, ..., B For bootstrap samples b = 1, ..., B Sample with replacement from variable tuples test sentences for systems X and Y Compute pseudo-statistic SXb − SYb on bootstrap data If |SXb − SYb − TB |1 |SX − SY</context>
<context position="13808" citStr="Koehn, 2004" startWordPosition="2259" endWordPosition="2260"> the actual test statistic is lower than α. Figure 2 provides a one-sided implementation of approximate randomization for MT evaluation, where the null hypothesis is that the score of System X is less than or equal to the score of System Y . Figure 5 shows a typical example of pseudostatistic distributions for approximate randomization for a pair of systems with a small but significant score difference according to BLEU, and Figure 6 shows the same for METEOR applied to a pair of systems where no significant difference is concluded. 3.3 Paired Bootstrap Resampling Paired bootstrap resampling (Koehn, 2004) is shown in Figure 4. Unlike the other two randomized tests, this method makes no attempt to simulate the null hypothesis distribution. Instead, bootstrap samples are used to estimate confidence intervals of score differences, with confidence intervals not containing 0 implying a statistically significant difference. We compare what takes place with the two other tests, by plotting differences in scores for bootstrapped samples, SX, − SYb, as shown in Figure 5 for BLEU and Figure 6 for METEOR. Instead of computing counts with reference to the actual statistic, the line through the origin prov</context>
<context position="17592" citStr="Koehn, 2004" startWordPosition="2834" endWordPosition="2835">luency scores of that system pair are used. For example, in Figure 7, adequacy scores of System B are not significantly greater than those of Systems C, D and E, while fluency scores for System B are significantly greater than those of the three other systems. The combined result for each pair of systems is therefore taken as the p-value from the corresponding fluency significance test. We use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing. We evaluate paired bootstrap resampling (Koehn, 2004) and bootstrap resampling as shown in Figure 3 and approximate randomization as shown in Figure 2, each in combination with four automatic MT metrics: BLEU (Papineni et al., 2002), NIST (NIST, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). 4.1 Results and Discussion Figure 8 shows the outcome of pairwise randomized significance tests for each metric for Spanishto-English systems, and Table 1 shows numbers of correct conclusions and accuracy of each test. When we compare conclusions made by the three randomized tests for Spanish-to-English systems, there is very little </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>P. Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of Empirical Methods in Natural Language Processing, pages 388–395, Barcelona, Spain. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kumar</author>
<author>W J Byrne</author>
</authors>
<title>Minimum Bayesrisk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>169--176</pages>
<publisher>ACL.</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="1870" citStr="Kumar and Byrne, 2004" startWordPosition="277" endWordPosition="280">with one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as BLEU, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in Koehn (2004) show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005). Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are re</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>S. Kumar and W. J. Byrne. 2004. Minimum Bayesrisk decoding for statistical machine translation. In HLT-NAACL, pages 169–176, Boston, MA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Monz</author>
</authors>
<title>Statistical machine translation with local language models.</title>
<date>2011</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>869--879</pages>
<publisher>ACL.</publisher>
<location>Edniburgh, Scotland.</location>
<contexts>
<context position="3640" citStr="Monz, 2011" startWordPosition="549" endWordPosition="550">lude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics approximate randomization — and find problems with the published formulations. We redress these issues, and apply the tests in statistical testing of two language pairs. Using human judgments of translation quality, we find only very minor difference</context>
</contexts>
<marker>Monz, 2011</marker>
<rawString>C. Monz. 2011. Statistical machine translation with local language models. In Proc. of the Conference on Empirical Methods in Natural Language Processing, pages 869–879, Edniburgh, Scotland. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics.</title>
<date>2002</date>
<tech>Technical report.</tech>
<contexts>
<context position="17790" citStr="NIST, 2002" startWordPosition="2867" endWordPosition="2868"> significantly greater than those of the three other systems. The combined result for each pair of systems is therefore taken as the p-value from the corresponding fluency significance test. We use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing. We evaluate paired bootstrap resampling (Koehn, 2004) and bootstrap resampling as shown in Figure 3 and approximate randomization as shown in Figure 2, each in combination with four automatic MT metrics: BLEU (Papineni et al., 2002), NIST (NIST, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). 4.1 Results and Discussion Figure 8 shows the outcome of pairwise randomized significance tests for each metric for Spanishto-English systems, and Table 1 shows numbers of correct conclusions and accuracy of each test. When we compare conclusions made by the three randomized tests for Spanish-to-English systems, there is very little difference in p-values for all pairs of systems. For both BLEU and NIST, 270 α Paired Bootst. Resamp. Bootst. Resamp. Approx. Rand. Conc. Acc.(%) Conc. Acc. (%) Conc. Acc. (%) BLEU 53 80.3 [68.7, 89</context>
</contexts>
<marker>NIST, 2002</marker>
<rawString>NIST. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Noreen</author>
</authors>
<title>Computer intensive methods for testing hypotheses.</title>
<date>1989</date>
<publisher>Wiley,</publisher>
<location>New York City, NY.</location>
<contexts>
<context position="9694" citStr="Noreen, 1989" startWordPosition="1530" endWordPosition="1531">ed bootstrap resampling statistical significance test for automatic MT evaluation Set c = 0 Compute actual statistic of score differences SX − SY on test data For random shuffles r = 1, ..., R For sentences in test set Shuffle variable tuples between systems X and Y with probability 0.5 Compute pseudo-statistic SXr − SYr on shuffled data If SXr − SYr 1 SX − SY c = c + 1 If c/R &lt; α Reject the null hypothesis Figure 2: Approximate randomization statistical significance test for automatic MT evaluation The null hypothesis distribution SH0 can be estimated from Sboot by applying the shift method (Noreen, 1989), which assumes that SH0 has the same shape but a different mean than Sboot. Thus, Sboot is transformed into SH0 by subtracting the mean bootstrap statistic from every value in Sboot. Once this shift-to-zero has taken place, the null hypothesis is rejected if the probability of observing a more extreme value than the actual statistic is lower than a predetermined p-value α, which is typically set to 0.05. In other words, the score difference is significant at level 1 − α. Figure 3 provides a one-sided implementation of bootstrap resampling, where Ho is that the score of System X is less than o</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>E. W. Noreen. 1989. Computer intensive methods for testing hypotheses. Wiley, New York City, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. 41st Ann. Meeting of the Assoc. Computational Linguistics,</booktitle>
<pages>160--167</pages>
<publisher>ACL.</publisher>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1847" citStr="Och, 2003" startWordPosition="275" endWordPosition="276">utput text with one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as BLEU, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in Koehn (2004) show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005). Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumpt</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F. J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. 41st Ann. Meeting of the Assoc. Computational Linguistics, pages 160–167, Sapporo, Japan. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. 40th Ann. Meeting of the Assoc. Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>ACL.</publisher>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1084" citStr="Papineni et al., 2002" startWordPosition="154" endWordPosition="157">ificance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shared task systems for two language pairs to provide a gold standard for tests. Results show very little difference in accuracy across the three methods of significance testing. Notably, accuracy of all test/metric combinations for evaluation of English-to-Spanish are so low that there is not enough evidence to conclude they are any better than a random coin toss. 1 Introduction Automatic metrics, such as BLEU (Papineni et al., 2002), are widely used in machine translation (MT) as a substitute for human evaluation. Such metrics commonly take the form of an automatic comparison of MT output text with one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as BLEU, standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the us</context>
<context position="17771" citStr="Papineni et al., 2002" startWordPosition="2862" endWordPosition="2865">luency scores for System B are significantly greater than those of the three other systems. The combined result for each pair of systems is therefore taken as the p-value from the corresponding fluency significance test. We use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing. We evaluate paired bootstrap resampling (Koehn, 2004) and bootstrap resampling as shown in Figure 3 and approximate randomization as shown in Figure 2, each in combination with four automatic MT metrics: BLEU (Papineni et al., 2002), NIST (NIST, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). 4.1 Results and Discussion Figure 8 shows the outcome of pairwise randomized significance tests for each metric for Spanishto-English systems, and Table 1 shows numbers of correct conclusions and accuracy of each test. When we compare conclusions made by the three randomized tests for Spanish-to-English systems, there is very little difference in p-values for all pairs of systems. For both BLEU and NIST, 270 α Paired Bootst. Resamp. Bootst. Resamp. Approx. Rand. Conc. Acc.(%) Conc. Acc. (%) Conc. Acc. (%) BLE</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. A method for automatic evaluation of machine translation. In Proc. 40th Ann. Meeting of the Assoc. Computational Linguistics, pages 311–318, Philadelphia, PA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>J T Maxwell</author>
</authors>
<title>On some pitfalls in automatic evaluation and significance testing for mt.</title>
<date>2005</date>
<booktitle>In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>57--64</pages>
<publisher>ACL.</publisher>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2315" citStr="Riezler and Maxwell, 2005" startWordPosition="351" endWordPosition="354">randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess for a pair of systems how likely a difference in BLEU scores occurred by chance. Empirical tests detailed in Koehn (2004) show that even for test sets as small as 300 translations, BLEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005). Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn. Both methods require some adaptation in order to be used for the purpose of MT evaluation, such as combination with an automatic metric, and therefore it cannot be taken for granted that approximate randomization will be more accurate in practice. Within MT, approximate randomization for the purpose of statistical testing is also less common. Riezler and Maxwell (2005) provide a comp</context>
<context position="4567" citStr="Riezler and Maxwell (2005)" startWordPosition="682" endWordPosition="685">c�2014 Association for Computational Linguistics approximate randomization — and find problems with the published formulations. We redress these issues, and apply the tests in statistical testing of two language pairs. Using human judgments of translation quality, we find only very minor differences in significance levels across the three tests, challenging claims made in the literature about relative merits of tests. 2 Revisiting Statistical Significance Tests for MT Evaluation First, we revisit the formulations of bootstrap resampling and approximate randomization algorithms as presented in Riezler and Maxwell (2005). At first glance, both methods appear to be two-tailed tests, with the null hypothesis that the two systems perform equally well. To facilitate a two-tailed test, absolute values of pseudostatistics are computed before locating the absolute value of the actual statistic (original difference in scores). Using absolute values of pseudostatistics is not problematic in the approximate randomization algorithm, and results in a reasonable two-tailed significance test. However, the bootstrap algorithm they provide uses an additional shift-to-zero method of simulating the null hypothesis. The way in </context>
<context position="11796" citStr="Riezler and Maxwell, 2005" startWordPosition="1910" endWordPosition="1913">6 shows the same for METEOR but for a pair of systems with no significant difference in scores. 3.2 Approximate Randomization Unlike bootstrap, approximate randomization does not make any assumptions about the population distribution. To simulate a distribution for the null hypothesis that the scores of the two systems are the same, translations are shuffled between the two systems so that 50% of each pseudo-sample is drawn from each system. In the context of machine translation, this can be interpreted as each translation being equally likely to have been produced by one system as the other (Riezler and Maxwell, 2005). The test statistic is taken as the difference in scores of the two systems, SX − SY . If there is 268 Paired Bootstrap Res. BLEU Bootstrap Resampling BLEU Approximate Randomization BLEU 0 100 200 300 400 c = 13 origin 0 100 200 300 400 c = 14 actual statistic 0 100 200 300 400 c = 11 actual statistic −0.015 −0.005 0.005 0.015 −0.015 −0.005 0.005 0.015 −0.015 −0.005 0.005 0.015 Figure 5: Pseudo-statistic distributions for a typical pair of systems with close BLEU scores for each randomized test (System F vs. System G). Paired Bootstrap Res. METEOR Bootstrap Resampling METEOR Approximate Rando</context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>S. Riezler and J. T. Maxwell. 2005. On some pitfalls in automatic evaluation and significance testing for mt. In Proc. of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 57–64, Ann Arbor, MI. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Riezler</author>
<author>J T Maxwell</author>
</authors>
<title>Grammatical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of the Main Conference on Human Language Technology Conference of the North American Chapter of the Assoc. Computational Linguistics,</booktitle>
<pages>248--255</pages>
<publisher>ACL.</publisher>
<location>New York City, NY.</location>
<contexts>
<context position="3560" citStr="Riezler and Maxwell, 2006" startWordPosition="532" endWordPosition="535">e randomization with bootstrap resampling (distinct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics approximate randomization — and find problems with the published formulations. We redress these issues, and apply the tests in statistical testing of two language pairs. </context>
</contexts>
<marker>Riezler, Maxwell, 2006</marker>
<rawString>S. Riezler and J. T. Maxwell. 2006. Grammatical machine translation. In Proc. of the Main Conference on Human Language Technology Conference of the North American Chapter of the Assoc. Computational Linguistics, pages 248–255, New York City, NY. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Smucker</author>
<author>J Allan</author>
<author>B Carterette</author>
</authors>
<title>A comparison of statistical significance tests for information retrieval evaluation.</title>
<date>2007</date>
<booktitle>In Proc. of the Sixteenth ACM Conference on Information and Knowledge Management (CIKM</booktitle>
<pages>623--632</pages>
<publisher>ACM.</publisher>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="6522" citStr="Smucker et al., 2007" startWordPosition="988" endWordPosition="991"> A better comparison of p-values would first require doubling the values of the one-sided bootstrap, leaving those of the two-sided approximate randomization algorithm as-is. The results of the two tests on this basis are extremely close, and in fact, in two out of the five comparisons, those of the bootstrap would have marginally higher pvalues than those of approximate randomization. As such, it is conceivable to conclude that the experiments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007). We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, rB, for shiftto-zero. Rather than speculate over whether these issues with the original paper were simply presentational glitches or the actual basis of the experiments reported on in the paper, we present a normalized version of the two-sided bootstrap algorithm in Figure 1, and report on the results of our own experiments in Section 4. We compare this method with approximate randomization and also paired bootstrap resampling (Koehn, 2004), which is widely used in MT evaluation. We carry out </context>
</contexts>
<marker>Smucker, Allan, Carterette, 2007</marker>
<rawString>M. Smucker, J. Allan, and B. Carterette. 2007. A comparison of statistical significance tests for information retrieval evaluation. In Proc. of the Sixteenth ACM Conference on Information and Knowledge Management (CIKM 2007), pages 623–632, Lisbon, Portugal. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Scwartz</author>
<author>J Makhoul</author>
<author>L Micciula</author>
</authors>
<title>A study of translation error rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proc. 7th Biennial Conf. of the Assoc. Machine Translaiton in the Americas,</booktitle>
<pages>223--231</pages>
<publisher>ACL.</publisher>
<location>Boston, MA.</location>
<contexts>
<context position="17855" citStr="Snover et al., 2006" startWordPosition="2876" endWordPosition="2879">tems. The combined result for each pair of systems is therefore taken as the p-value from the corresponding fluency significance test. We use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing. We evaluate paired bootstrap resampling (Koehn, 2004) and bootstrap resampling as shown in Figure 3 and approximate randomization as shown in Figure 2, each in combination with four automatic MT metrics: BLEU (Papineni et al., 2002), NIST (NIST, 2002), METEOR (Banerjee and Lavie, 2005) and TER (Snover et al., 2006). 4.1 Results and Discussion Figure 8 shows the outcome of pairwise randomized significance tests for each metric for Spanishto-English systems, and Table 1 shows numbers of correct conclusions and accuracy of each test. When we compare conclusions made by the three randomized tests for Spanish-to-English systems, there is very little difference in p-values for all pairs of systems. For both BLEU and NIST, 270 α Paired Bootst. Resamp. Bootst. Resamp. Approx. Rand. Conc. Acc.(%) Conc. Acc. (%) Conc. Acc. (%) BLEU 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] 53 80.3 [68.7, 89.1] NIST 54 81.8 [70.4,</context>
</contexts>
<marker>Snover, Dorr, Scwartz, Makhoul, Micciula, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Scwartz, J. Makhoul, and L. Micciula. 2006. A study of translation error rate with targeted human annotation. In Proc. 7th Biennial Conf. of the Assoc. Machine Translaiton in the Americas, pages 223–231, Boston, MA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
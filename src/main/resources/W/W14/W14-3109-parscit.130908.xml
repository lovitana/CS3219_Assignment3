<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.048016">
<title confidence="0.9967845">
Design of an Active Learning System with Human Correction for
Content Analysis
</title>
<author confidence="0.998139">
Nancy McCracken
</author>
<affiliation confidence="0.9303395">
School of Information Studies
Syracuse University, USA
</affiliation>
<email confidence="0.979193">
njmccrac@syr.edu
</email>
<author confidence="0.989408">
Jasy Liew Suet Yan
</author>
<affiliation confidence="0.9258255">
School of Information Studies
Syracuse University, USA
</affiliation>
<email confidence="0.980832">
jliewsue@syr.edu
</email>
<author confidence="0.99512">
Kevin Crowston
</author>
<affiliation confidence="0.9272295">
National Science Foundation
Syracuse University, USA
</affiliation>
<email confidence="0.982672">
crowston@syr.edu
</email>
<sectionHeader confidence="0.993462" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999533647058824">
Our research investigation focuses on the role of
humans in supplying corrected examples in active
learning cycles, an important aspect of deploying
active learning in practice. In this paper, we dis-
cuss sampling strategies and sampling sizes in set-
ting up an active learning system for human ex-
periments in the task of content analysis, which
involves labeling concepts in large volumes of
text. The cost of conducting comprehensive hu-
man subject studies to experimentally determine
the effects of sampling sizes and sampling sizes is
high. To reduce those costs, we first applied an
active learning simulation approach to test the ef-
fect of different sampling strategies and sampling
sizes on machine learning (ML) performance in
order to select a smaller set of parameters to be
evaluated in human subject studies.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999884784313726">
Social scientists often use content analysis to
understand the practices of groups by analyzing
texts such as transcripts of interpersonal commu-
nication. Content analysis is the process of iden-
tifying and labeling conceptually significant fea-
tures in text, referred to as “coding” (Miles and
Huberman, 1994). For example, researchers
studying leadership might look for evidence of
behaviors such as “suggesting or recommending”
or “inclusive reference” expressed in email mes-
sages. However, analyzing text is very labor-
intensive, as the text must be read and under-
stood by a human. Consequently, important re-
search questions in the qualitative social sciences
may not be addressed because there is too much
data for humans to analyze in a reasonable time.
A few researchers have tried automatic tech-
niques on content analysis problems. For exam-
ple, Crowston et al. (2012) manually developed a
classifier to identify codes related to group
maintenance behavior in free/libre open source
software (FLOSS) teams. Others have applied
machine-learning (ML) techniques. For example,
Ishita et al. (2010) used ML to automatically
classify sections of text within documents on ten
human values taken from the Schwartz’s Value
Inventory. Broadwell et al. (2012) developed
models to classify sociolinguistic behaviors to
infer social roles (e.g., leadership). On the best
performing codes, these approaches achieve ac-
curacies from 60–80%, showing the potential of
automatic qualitative content analysis. However,
these studies all limited their reports to a subset
of codes used by the social scientists, due in part
to the need for a large volume of training data.
The state-of-the-art ML approaches for con-
tent analysis require researchers to obtain a large
amount of annotated data upfront, which is often
costly or impractical. An active learning ap-
proach which uses human correction during the
steps of active learning could potentially help
produce a large amount of annotated data while
minimizing the cost of human annotation effort.
Unlike other text annotation tasks, the code an-
notation for content analysis requires significant
cognitive effort, which may limit, or even nulli-
fy, the benefits of active learning.
We are building an active machine learning
system to semi-automate the process of content
analysis, and are planning to study the human
role in such machine learning systems.
</bodyText>
<figureCaption confidence="0.989934">
Figure 1. Active learning for semi-automatic
content analysis.
</figureCaption>
<bodyText confidence="0.996340666666667">
As illustrated in Figure 1, the system design in-
corporates building a classifier from an initial set
of hand-coded examples and iteratively improv-
</bodyText>
<figure confidence="0.995814375">
Human Annotation Machine Annotation
Export gold
standard data into
XML
Manually code
documents in
ATLAS.ti
Apply model to
additional
documents
Learn model
Save corrected
coding as silver data
Human Correction
Manually correct
model coding
</figure>
<page confidence="0.940343">
59
</page>
<bodyText confidence="0.95783978125">
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 59–62,
Baltimore, Maryland, USA, June 27, 2014. @2014 Association for Computational Linguistics
ing the model by having human annotators cor-
rect new examples identified by the system
Little is yet known about the optimal number
of machine annotations to be presented to human
annotators for correction, and how the sample
sizes of machine annotations affect ML perfor-
mance. Also, existing active learning sampling
strategies to pick out the most “beneficial” ex-
amples for human correction to be used in the
next round of ML training have not been tested
in the context of social science data, where con-
cept codes may be multi-dimensional or hierar-
chical, and the problem may be multi-label (one
phrase or sentence in the annotated text has mul-
tiple labels). Also, concept codes tend to be very
sparse in the text, resulting in a classification
problem that has both imbalance—the non-
annotated pieces of text (negative examples) tend
to be far more frequent that annotated text—and
rarity, where there may not be enough examples
of some codes to achieve a good classifier.
The cost of conducting comprehensive human
subject studies to experimentally determine the
effects of sampling sizes and sampling sizes is
high. Therefore, we first applied an active learn-
ing simulation approach to test the effect of dif-
ferent sampling strategies and sampling sizes on
machine learning (ML) performance. This allows
the human subject studies to involve a smaller set
of parameters to be evaluated.
</bodyText>
<sectionHeader confidence="0.99923" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99997447826087">
For active learning in our system, we are using
what is sometimes called pool-based active
learning, where a large number of unlabeled ex-
amples are available to be the pool of the next
samples. This type of active learning has been
well explored for text categorization tasks (Lewis
and Gale, 1994; Tong and Koller 2000; Schohn
and Cohn 2000). This approach often uses the
method of uncertainty sampling to pick new
samples from the pool, both with probability
models to give the “uncertainty” (Lewis and
Gale, 1994) and with SVM models, where the
margin numbers give the “uncertainty” (Tong
and Koller 2000; Schohn and Cohn 2000). While
much of the research focus has been on the sam-
pling method, some has also focused on the size
of the sample, e.g. in Schohn and Cohn (2000),
sample sizes of 4, 8, 16, and 32 were used, where
the result was that smaller sizes gave a steeper
learning curve with a greater classification cost,
and the authors settled on a sample size of 8. For
additional active learning references, see the Set-
tles (2009) survey of active learning literature.
This type of active learning has also been
used in the context of human correction. One
such system is described in Mandel et al. (2006),
using active learning for music retrieval, where
users were presented with up to 6 examples of
songs to label. Another is the DUALLIST system
described in Settles (2011) and Settles and Zhu
(2012) where human experiments were carried
out for text classification and other tasks. While
most active learning experiments focus on reduc-
ing the number of examples to achieve an accu-
rate model, there has been some effort to model
the reduction of the cost of human time in anno-
tation, where the human time is non-uniform per
example. Both the systems in Culotta and
McCallum (2005) and in Clancy et al. (2012) for
the task of named entity extraction, modeled hu-
man cost in the context of sequential information
extraction tasks. However, one difference be-
tween these systems and ours is that all of the
tasks studied in these systems did not require
annotators to have extensive training to annotate
complex concept codes.
</bodyText>
<sectionHeader confidence="0.997982" genericHeader="method">
3 Problem
</sectionHeader>
<bodyText confidence="0.99998862962963">
We worked with a pilot project in which
researchers are studying leadership in open
source software groups by analyzing open source
developer emails. After a year of part-time
annotation by two annotators, the researchers
developed a codebook that provides a definition
and examples for 35 codes. The coders achieved
an inter-annotator agreement (kappa) of about
80%, and annotated about 400 email threads,
consisting of about 3700 sentences. We used
these coded messages as the “gold standard” data
for our study. However, only 15 codes had more
than 25 instances in the gold standard set. The
most common code (“Explanation/Rationale/
Background”) occurred only 319 times.
In our pilot correction experiments, annota-
tors tried correcting samples of sizes ranging
from about 50 to about 400. Anecdotal evidence
indicates that annotators liked to annotate sample
sizes of about 100 in order to achieve good focus
on a particular code definition at one time, but
without getting stressed with too many examples.
Part of the required focus is that annotators need
to refresh their memory on any particular code at
the start of annotation, so switching frequently
between different codes is cognitively taxing.
This desired sample size contrasts with prior ac-
</bodyText>
<page confidence="0.996492">
60
</page>
<bodyText confidence="0.999930266666667">
tive learning systems that employ much smaller
sample sizes, in the range of 1 to 20.
We are currently in the process of setting up
the human experiments to test our main research
question of achieving an accurate model for con-
tent analysis using a minimum of human effort.
In this paper, we discuss two questions for
active learning in order to have annotators cor-
rect an acceptable number of machine annota-
tions that are most likely to increase the perfor-
mance of the ML model in each iteration. These
are: how do different sample sizes and different
sampling strategies of machine annotations pre-
sented to human annotators for correction in each
round affect ML performance?
</bodyText>
<sectionHeader confidence="0.995638" genericHeader="method">
4 Active Learning Simulation Setup
</sectionHeader>
<bodyText confidence="0.999978765957447">
In a similar strategy to that of Clancy et al.
(2012), we carried out a preliminary investiga-
tion by conducting an active learning simulation
on our gold standard data. The simulation starts
with a small initial sample, and uses active learn-
ing where we “correct” the sample labels by tak-
ing labels from the gold standard corpus. For our
simulation experiments, we separated the gold
standard data randomly into a training set of 90%
of the examples, 3298 sentences, and a test set of
10%, 366 sentences.
In the experimental setup, we used a version
of libSVM that was modified to produce num-
bers of distance to the margin of the SVM classi-
fication. We implemented the multi-label classi-
fication by classifying each label separately
where some sentences have the selected label
and all others were counted as “negative” labels.
We used svm weights to handle the problem of
imbalance in the negative examples. After exper-
imentation with different combinations of fea-
tures, we used a set of features that was best
overall for the codes: unigram tokens lowercased
and filtered by stop words, bigrams, orthographic
features from capitalization, the token count, and
the role of the sender of the email.
For an initial sample, we randomly chose 3
positive and 3 negative examples from the de-
velopment set to be the initial training set used
for all experimental runs. We carried out experi-
ments with a number of sample sizes, b, ranging
over 5, 10, 20, 40, 50, 60, 80 and 100 instances.
For experiments on methods used to select
correction examples, we have chosen to experi-
ment with sampling methods similar to those
found in Lewis and Gale (1994) and Lewis
(1995) using a random sampling method, where
a new sample is chosen randomly from the re-
maining examples in the development set, a rele-
vance sampling method, where a new sample is
chosen as the b number of most likely labeled
candidates in the development set with the larg-
est distance from the margin of the SVM classi-
fication, and an uncertainty sampling method,
where a new sample is chosen as the b number of
candidates in the region of uncertainty on either
side of the margin of the SVM classification.
</bodyText>
<sectionHeader confidence="0.952506" genericHeader="method">
5 Preliminary Results
</sectionHeader>
<bodyText confidence="0.99936804">
In this simulation experiment, the pool size is
quite small (3664 examples) compared to the
large amount of unlabeled data that is normally
available for active learning, and would be avail-
able for our system under actual use. We tested
the active learning simulation on 8 codes. There
was no clear winning sampling strategy out of
the 3 we used in the simulation experiment but
random sampling (5 out of 8 codes) appeared to
be the one that most often produced the highest
F§2 score in the shortest number of iterations.
Figure 2 shows the F§2 score for each sampling
strategy based on code “Opinion/Preference”
using sample sizes 5 and 100 respectively.
As for sampling sizes, we did not observe a
large difference in the evolution of the F§2 score
between the various sample sizes, and the learn-
ing curves in Figure 2, shown for the sample siz-
es of 5 and 100, are typical. This means that we
should be able to use larger sample sizes for hu-
man subject studies to achieve the same im-
provements in performance as with the smaller
sample sizes, and can carry out the experiments
to relate the cost of human annotation with in-
creases in performance.
</bodyText>
<figureCaption confidence="0.844165">
Figure 2. Active ML performance for code
Opinion/Preference.
</figureCaption>
<page confidence="0.998766">
61
</page>
<sectionHeader confidence="0.96707" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.993114020833334">
Our findings are inconclusive as we have yet to
run the active learning simulations on all the
codes. However, preliminary results are directing
us towards using larger sample sizes and then
experimenting with random and uncertainty
sampling in the human subject studies.
From our experiments with the different
codes, we found the performance on less fre-
quent codes to be problematic as it is difficult for
the active learning system to identify potential
positive examples to improve the models. While
the system performance may improve to handle
such sparse cases, it may be better to modify the
codebook instead. We plan to give the user feed-
back on the performance of the codes at each
iteration of the active learning and support modi-
fications to the codebook, for example, the user
may wish to drop some codes or collapse them
according to some hierarchy. After all, if a code
is not found in the text, it is hard to argue for its
theoretical importance.
We are currently completing the design of
the parameters of the active learning process for
the human correction experiments on our pilot
project with the codes about leadership in open
source software groups. We will also be testing
and undergoing further development of the user
interface for the annotators.
Our next step will be to test the system on
other projects with other researchers. We hope to
gain more insight into what types of coding
schemes and codes are easier to learn than oth-
ers, and to be able to guide social scientists into
developing coding schemes that are not only
based on the social science theory but also useful
in practice to develop an accurate classifier for
very large amounts of digital text.
Acknowledgements:
This material is based upon work supported
by the National Science Foundation under Grant
No. IIS-1111107. Kevin Crowston is supported
by the National Science Foundation. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the
author(s) and do not necessarily reflect the views
of the National Science Foundation. The authors
gratefully acknowledge helpful suggestions by
the reviewers.
</bodyText>
<sectionHeader confidence="0.893533" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999792181818182">
Broadwell, G. A., Stromer-Galley, J., Strzalkowski,
T., Shaikh, S., Taylor, S., Liu, T., Boz, U., Elia, A.,
Jiao, L., &amp; Webb, N. (2013). Modeling sociocul-
tural phenomena in discourse. Natural Language
Engineering, 19(02), 213–257.
Clancy, S., Bayer, S. and Kozierok, R. (2012) “Ac-
tive Learning with a Human In The Loop,” Mitre
Corporation.
Crowston, K., Allen, E. E., &amp; Heckman, R. (2012).
Using natural language processing technology for
qualitative data analysis. International Journal of
Social Research Methodology, 15(6), 523–543.
Culotta, A. and McCallum, A. (2005) “Reducing La-
beling Effort for Structured Prediction Tasks.”
Ishita, E., Oard, D. W., Fleischmann, K. R., Cheng,
A.-S., &amp; Templeton, T. C. (2010). Investigating
multi-label classification for human values. Pro-
ceedings of the American Society for Information
Science and Technology, 47(1), 1–4.
Miles, M. B., &amp; Huberman, A. M. (1994). Qualitative
data analysis: An expanded sourcebook. Sage Pub-
lications.
Lewis, D. D., &amp; Gale, W. A. (1994). A sequential
algorithm for training text classifiers. In Proceed-
ings of the 17th annual international ACM SIGIR
conference on Research and development in infor-
mation retrieval (pp. 3-12).
Lewis, D. D. (1995). A sequential algorithm for train-
ing text classifiers: Corrigendum and additional da-
ta. In ACM SIGIR Forum (Vol. 29, No. 2, pp. 13-
19).
Mandel, M. I., Poliner, G. E., &amp; Ellis, D. P. (2006).
Support vector machine active learning for music
retrieval. Multimedia systems, 12(1), 3-13.
Schohn, G., &amp; Cohn, D. (2000). Less is more: Active
learning with support vector machines. In Interna-
tional Conference on Machine Learning (pp. 839-
846).
Settles, B. (2010). Active learning literature survey.
University of Wisconsin, Madison, 52, 55-66.
Settles, B. (2011). Closing the loop: Fast, interactive
semi-supervised annotation with queries on fea-
tures and instances. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (pp. 1467-1478).
Settles, B., &amp; Zhu, X. (2012). Behavioral factors in
interactive training of text classifiers. In Proceed-
ings of the 2012 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies (pp. 563-
567).
Tong, S., &amp; Koller, D. (2002). Support vector ma-
chine active learning with applications to text clas-
sification. The Journal of Machine Learning Re-
search, 2, 45-66.
</reference>
<page confidence="0.999187">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.872862">
<title confidence="0.998374">Design of an Active Learning System with Human Correction Content Analysis</title>
<author confidence="0.986958">Nancy</author>
<affiliation confidence="0.999893">School of Information Syracuse University,</affiliation>
<email confidence="0.999408">njmccrac@syr.edu</email>
<author confidence="0.999186">Jasy Liew Suet</author>
<affiliation confidence="0.99997">School of Information Syracuse University,</affiliation>
<email confidence="0.999547">jliewsue@syr.edu</email>
<author confidence="0.904045">Kevin</author>
<affiliation confidence="0.999933">National Science Syracuse University,</affiliation>
<email confidence="0.999771">crowston@syr.edu</email>
<abstract confidence="0.998851944444445">Our research investigation focuses on the role of humans in supplying corrected examples in active learning cycles, an important aspect of deploying active learning in practice. In this paper, we discuss sampling strategies and sampling sizes in setting up an active learning system for human experiments in the task of content analysis, which involves labeling concepts in large volumes of text. The cost of conducting comprehensive human subject studies to experimentally determine the effects of sampling sizes and sampling sizes is high. To reduce those costs, we first applied an active learning simulation approach to test the effect of different sampling strategies and sampling sizes on machine learning (ML) performance in order to select a smaller set of parameters to be evaluated in human subject studies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G A Broadwell</author>
<author>J Stromer-Galley</author>
<author>T Strzalkowski</author>
<author>S Shaikh</author>
<author>S Taylor</author>
<author>T Liu</author>
<author>U Boz</author>
<author>A Elia</author>
<author>L Jiao</author>
<author>N Webb</author>
</authors>
<title>Modeling sociocultural phenomena in discourse. Natural Language Engineering,</title>
<date>2013</date>
<pages>213--257</pages>
<marker>Broadwell, Stromer-Galley, Strzalkowski, Shaikh, Taylor, Liu, Boz, Elia, Jiao, Webb, 2013</marker>
<rawString>Broadwell, G. A., Stromer-Galley, J., Strzalkowski, T., Shaikh, S., Taylor, S., Liu, T., Boz, U., Elia, A., Jiao, L., &amp; Webb, N. (2013). Modeling sociocultural phenomena in discourse. Natural Language Engineering, 19(02), 213–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clancy</author>
<author>S Bayer</author>
<author>R Kozierok</author>
</authors>
<title>Active Learning with a Human In The Loop,” Mitre Corporation.</title>
<date>2012</date>
<contexts>
<context position="7449" citStr="Clancy et al. (2012)" startWordPosition="1177" endWordPosition="1180"> et al. (2006), using active learning for music retrieval, where users were presented with up to 6 examples of songs to label. Another is the DUALLIST system described in Settles (2011) and Settles and Zhu (2012) where human experiments were carried out for text classification and other tasks. While most active learning experiments focus on reducing the number of examples to achieve an accurate model, there has been some effort to model the reduction of the cost of human time in annotation, where the human time is non-uniform per example. Both the systems in Culotta and McCallum (2005) and in Clancy et al. (2012) for the task of named entity extraction, modeled human cost in the context of sequential information extraction tasks. However, one difference between these systems and ours is that all of the tasks studied in these systems did not require annotators to have extensive training to annotate complex concept codes. 3 Problem We worked with a pilot project in which researchers are studying leadership in open source software groups by analyzing open source developer emails. After a year of part-time annotation by two annotators, the researchers developed a codebook that provides a definition and ex</context>
<context position="9798" citStr="Clancy et al. (2012)" startWordPosition="1560" endWordPosition="1563">riments to test our main research question of achieving an accurate model for content analysis using a minimum of human effort. In this paper, we discuss two questions for active learning in order to have annotators correct an acceptable number of machine annotations that are most likely to increase the performance of the ML model in each iteration. These are: how do different sample sizes and different sampling strategies of machine annotations presented to human annotators for correction in each round affect ML performance? 4 Active Learning Simulation Setup In a similar strategy to that of Clancy et al. (2012), we carried out a preliminary investigation by conducting an active learning simulation on our gold standard data. The simulation starts with a small initial sample, and uses active learning where we “correct” the sample labels by taking labels from the gold standard corpus. For our simulation experiments, we separated the gold standard data randomly into a training set of 90% of the examples, 3298 sentences, and a test set of 10%, 366 sentences. In the experimental setup, we used a version of libSVM that was modified to produce numbers of distance to the margin of the SVM classification. We </context>
</contexts>
<marker>Clancy, Bayer, Kozierok, 2012</marker>
<rawString>Clancy, S., Bayer, S. and Kozierok, R. (2012) “Active Learning with a Human In The Loop,” Mitre Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Crowston</author>
<author>E E Allen</author>
<author>R Heckman</author>
</authors>
<title>Using natural language processing technology for qualitative data analysis.</title>
<date>2012</date>
<journal>International Journal of Social Research Methodology,</journal>
<volume>15</volume>
<issue>6</issue>
<pages>523--543</pages>
<contexts>
<context position="2056" citStr="Crowston et al. (2012)" startWordPosition="309" endWordPosition="312">n text, referred to as “coding” (Miles and Huberman, 1994). For example, researchers studying leadership might look for evidence of behaviors such as “suggesting or recommending” or “inclusive reference” expressed in email messages. However, analyzing text is very laborintensive, as the text must be read and understood by a human. Consequently, important research questions in the qualitative social sciences may not be addressed because there is too much data for humans to analyze in a reasonable time. A few researchers have tried automatic techniques on content analysis problems. For example, Crowston et al. (2012) manually developed a classifier to identify codes related to group maintenance behavior in free/libre open source software (FLOSS) teams. Others have applied machine-learning (ML) techniques. For example, Ishita et al. (2010) used ML to automatically classify sections of text within documents on ten human values taken from the Schwartz’s Value Inventory. Broadwell et al. (2012) developed models to classify sociolinguistic behaviors to infer social roles (e.g., leadership). On the best performing codes, these approaches achieve accuracies from 60–80%, showing the potential of automatic qualita</context>
</contexts>
<marker>Crowston, Allen, Heckman, 2012</marker>
<rawString>Crowston, K., Allen, E. E., &amp; Heckman, R. (2012). Using natural language processing technology for qualitative data analysis. International Journal of Social Research Methodology, 15(6), 523–543.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>A McCallum</author>
</authors>
<title>Reducing Labeling Effort for Structured Prediction Tasks.”</title>
<date>2005</date>
<booktitle>Proceedings of the American Society for Information Science and Technology,</booktitle>
<volume>47</volume>
<issue>1</issue>
<pages>1--4</pages>
<contexts>
<context position="7421" citStr="Culotta and McCallum (2005)" startWordPosition="1171" endWordPosition="1174"> such system is described in Mandel et al. (2006), using active learning for music retrieval, where users were presented with up to 6 examples of songs to label. Another is the DUALLIST system described in Settles (2011) and Settles and Zhu (2012) where human experiments were carried out for text classification and other tasks. While most active learning experiments focus on reducing the number of examples to achieve an accurate model, there has been some effort to model the reduction of the cost of human time in annotation, where the human time is non-uniform per example. Both the systems in Culotta and McCallum (2005) and in Clancy et al. (2012) for the task of named entity extraction, modeled human cost in the context of sequential information extraction tasks. However, one difference between these systems and ours is that all of the tasks studied in these systems did not require annotators to have extensive training to annotate complex concept codes. 3 Problem We worked with a pilot project in which researchers are studying leadership in open source software groups by analyzing open source developer emails. After a year of part-time annotation by two annotators, the researchers developed a codebook that </context>
</contexts>
<marker>Culotta, McCallum, 2005</marker>
<rawString>Culotta, A. and McCallum, A. (2005) “Reducing Labeling Effort for Structured Prediction Tasks.” Ishita, E., Oard, D. W., Fleischmann, K. R., Cheng, A.-S., &amp; Templeton, T. C. (2010). Investigating multi-label classification for human values. Proceedings of the American Society for Information Science and Technology, 47(1), 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M B Miles</author>
<author>A M Huberman</author>
</authors>
<title>Qualitative data analysis: An expanded sourcebook.</title>
<date>1994</date>
<publisher>Sage Publications.</publisher>
<contexts>
<context position="1492" citStr="Miles and Huberman, 1994" startWordPosition="219" endWordPosition="222">nd sampling sizes is high. To reduce those costs, we first applied an active learning simulation approach to test the effect of different sampling strategies and sampling sizes on machine learning (ML) performance in order to select a smaller set of parameters to be evaluated in human subject studies. 1 Introduction Social scientists often use content analysis to understand the practices of groups by analyzing texts such as transcripts of interpersonal communication. Content analysis is the process of identifying and labeling conceptually significant features in text, referred to as “coding” (Miles and Huberman, 1994). For example, researchers studying leadership might look for evidence of behaviors such as “suggesting or recommending” or “inclusive reference” expressed in email messages. However, analyzing text is very laborintensive, as the text must be read and understood by a human. Consequently, important research questions in the qualitative social sciences may not be addressed because there is too much data for humans to analyze in a reasonable time. A few researchers have tried automatic techniques on content analysis problems. For example, Crowston et al. (2012) manually developed a classifier to </context>
</contexts>
<marker>Miles, Huberman, 1994</marker>
<rawString>Miles, M. B., &amp; Huberman, A. M. (1994). Qualitative data analysis: An expanded sourcebook. Sage Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
<author>W A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval</booktitle>
<pages>3--12</pages>
<contexts>
<context position="5924" citStr="Lewis and Gale, 1994" startWordPosition="912" endWordPosition="915">ing sizes and sampling sizes is high. Therefore, we first applied an active learning simulation approach to test the effect of different sampling strategies and sampling sizes on machine learning (ML) performance. This allows the human subject studies to involve a smaller set of parameters to be evaluated. 2 Related Work For active learning in our system, we are using what is sometimes called pool-based active learning, where a large number of unlabeled examples are available to be the pool of the next samples. This type of active learning has been well explored for text categorization tasks (Lewis and Gale, 1994; Tong and Koller 2000; Schohn and Cohn 2000). This approach often uses the method of uncertainty sampling to pick new samples from the pool, both with probability models to give the “uncertainty” (Lewis and Gale, 1994) and with SVM models, where the margin numbers give the “uncertainty” (Tong and Koller 2000; Schohn and Cohn 2000). While much of the research focus has been on the sampling method, some has also focused on the size of the sample, e.g. in Schohn and Cohn (2000), sample sizes of 4, 8, 16, and 32 were used, where the result was that smaller sizes gave a steeper learning curve with</context>
<context position="11388" citStr="Lewis and Gale (1994)" startWordPosition="1830" endWordPosition="1833">r the codes: unigram tokens lowercased and filtered by stop words, bigrams, orthographic features from capitalization, the token count, and the role of the sender of the email. For an initial sample, we randomly chose 3 positive and 3 negative examples from the development set to be the initial training set used for all experimental runs. We carried out experiments with a number of sample sizes, b, ranging over 5, 10, 20, 40, 50, 60, 80 and 100 instances. For experiments on methods used to select correction examples, we have chosen to experiment with sampling methods similar to those found in Lewis and Gale (1994) and Lewis (1995) using a random sampling method, where a new sample is chosen randomly from the remaining examples in the development set, a relevance sampling method, where a new sample is chosen as the b number of most likely labeled candidates in the development set with the largest distance from the margin of the SVM classification, and an uncertainty sampling method, where a new sample is chosen as the b number of candidates in the region of uncertainty on either side of the margin of the SVM classification. 5 Preliminary Results In this simulation experiment, the pool size is quite smal</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>Lewis, D. D., &amp; Gale, W. A. (1994). A sequential algorithm for training text classifiers. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval (pp. 3-12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
</authors>
<title>A sequential algorithm for training text classifiers: Corrigendum and additional data.</title>
<date>1995</date>
<journal>In ACM SIGIR Forum</journal>
<volume>29</volume>
<pages>13--19</pages>
<contexts>
<context position="11405" citStr="Lewis (1995)" startWordPosition="1835" endWordPosition="1836">s lowercased and filtered by stop words, bigrams, orthographic features from capitalization, the token count, and the role of the sender of the email. For an initial sample, we randomly chose 3 positive and 3 negative examples from the development set to be the initial training set used for all experimental runs. We carried out experiments with a number of sample sizes, b, ranging over 5, 10, 20, 40, 50, 60, 80 and 100 instances. For experiments on methods used to select correction examples, we have chosen to experiment with sampling methods similar to those found in Lewis and Gale (1994) and Lewis (1995) using a random sampling method, where a new sample is chosen randomly from the remaining examples in the development set, a relevance sampling method, where a new sample is chosen as the b number of most likely labeled candidates in the development set with the largest distance from the margin of the SVM classification, and an uncertainty sampling method, where a new sample is chosen as the b number of candidates in the region of uncertainty on either side of the margin of the SVM classification. 5 Preliminary Results In this simulation experiment, the pool size is quite small (3664 examples)</context>
</contexts>
<marker>Lewis, 1995</marker>
<rawString>Lewis, D. D. (1995). A sequential algorithm for training text classifiers: Corrigendum and additional data. In ACM SIGIR Forum (Vol. 29, No. 2, pp. 13-19).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M I Mandel</author>
<author>G E Poliner</author>
<author>D P Ellis</author>
</authors>
<title>Support vector machine active learning for music retrieval. Multimedia systems,</title>
<date>2006</date>
<volume>12</volume>
<issue>1</issue>
<pages>3--13</pages>
<contexts>
<context position="6843" citStr="Mandel et al. (2006)" startWordPosition="1073" endWordPosition="1076">Koller 2000; Schohn and Cohn 2000). While much of the research focus has been on the sampling method, some has also focused on the size of the sample, e.g. in Schohn and Cohn (2000), sample sizes of 4, 8, 16, and 32 were used, where the result was that smaller sizes gave a steeper learning curve with a greater classification cost, and the authors settled on a sample size of 8. For additional active learning references, see the Settles (2009) survey of active learning literature. This type of active learning has also been used in the context of human correction. One such system is described in Mandel et al. (2006), using active learning for music retrieval, where users were presented with up to 6 examples of songs to label. Another is the DUALLIST system described in Settles (2011) and Settles and Zhu (2012) where human experiments were carried out for text classification and other tasks. While most active learning experiments focus on reducing the number of examples to achieve an accurate model, there has been some effort to model the reduction of the cost of human time in annotation, where the human time is non-uniform per example. Both the systems in Culotta and McCallum (2005) and in Clancy et al. </context>
</contexts>
<marker>Mandel, Poliner, Ellis, 2006</marker>
<rawString>Mandel, M. I., Poliner, G. E., &amp; Ellis, D. P. (2006). Support vector machine active learning for music retrieval. Multimedia systems, 12(1), 3-13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Schohn</author>
<author>D Cohn</author>
</authors>
<title>Less is more: Active learning with support vector machines.</title>
<date>2000</date>
<booktitle>In International Conference on Machine Learning</booktitle>
<pages>839--846</pages>
<contexts>
<context position="5969" citStr="Schohn and Cohn 2000" startWordPosition="920" endWordPosition="923">ore, we first applied an active learning simulation approach to test the effect of different sampling strategies and sampling sizes on machine learning (ML) performance. This allows the human subject studies to involve a smaller set of parameters to be evaluated. 2 Related Work For active learning in our system, we are using what is sometimes called pool-based active learning, where a large number of unlabeled examples are available to be the pool of the next samples. This type of active learning has been well explored for text categorization tasks (Lewis and Gale, 1994; Tong and Koller 2000; Schohn and Cohn 2000). This approach often uses the method of uncertainty sampling to pick new samples from the pool, both with probability models to give the “uncertainty” (Lewis and Gale, 1994) and with SVM models, where the margin numbers give the “uncertainty” (Tong and Koller 2000; Schohn and Cohn 2000). While much of the research focus has been on the sampling method, some has also focused on the size of the sample, e.g. in Schohn and Cohn (2000), sample sizes of 4, 8, 16, and 32 were used, where the result was that smaller sizes gave a steeper learning curve with a greater classification cost, and the autho</context>
</contexts>
<marker>Schohn, Cohn, 2000</marker>
<rawString>Schohn, G., &amp; Cohn, D. (2000). Less is more: Active learning with support vector machines. In International Conference on Machine Learning (pp. 839-846).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
</authors>
<title>Active learning literature survey.</title>
<date>2010</date>
<journal>University of Wisconsin, Madison,</journal>
<volume>52</volume>
<pages>55--66</pages>
<marker>Settles, 2010</marker>
<rawString>Settles, B. (2010). Active learning literature survey. University of Wisconsin, Madison, 52, 55-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
</authors>
<title>Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing</booktitle>
<pages>1467--1478</pages>
<contexts>
<context position="7014" citStr="Settles (2011)" startWordPosition="1103" endWordPosition="1104">000), sample sizes of 4, 8, 16, and 32 were used, where the result was that smaller sizes gave a steeper learning curve with a greater classification cost, and the authors settled on a sample size of 8. For additional active learning references, see the Settles (2009) survey of active learning literature. This type of active learning has also been used in the context of human correction. One such system is described in Mandel et al. (2006), using active learning for music retrieval, where users were presented with up to 6 examples of songs to label. Another is the DUALLIST system described in Settles (2011) and Settles and Zhu (2012) where human experiments were carried out for text classification and other tasks. While most active learning experiments focus on reducing the number of examples to achieve an accurate model, there has been some effort to model the reduction of the cost of human time in annotation, where the human time is non-uniform per example. Both the systems in Culotta and McCallum (2005) and in Clancy et al. (2012) for the task of named entity extraction, modeled human cost in the context of sequential information extraction tasks. However, one difference between these systems</context>
</contexts>
<marker>Settles, 2011</marker>
<rawString>Settles, B. (2011). Closing the loop: Fast, interactive semi-supervised annotation with queries on features and instances. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (pp. 1467-1478).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Settles</author>
<author>X Zhu</author>
</authors>
<title>Behavioral factors in interactive training of text classifiers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</booktitle>
<pages>563--567</pages>
<contexts>
<context position="7041" citStr="Settles and Zhu (2012)" startWordPosition="1106" endWordPosition="1109">of 4, 8, 16, and 32 were used, where the result was that smaller sizes gave a steeper learning curve with a greater classification cost, and the authors settled on a sample size of 8. For additional active learning references, see the Settles (2009) survey of active learning literature. This type of active learning has also been used in the context of human correction. One such system is described in Mandel et al. (2006), using active learning for music retrieval, where users were presented with up to 6 examples of songs to label. Another is the DUALLIST system described in Settles (2011) and Settles and Zhu (2012) where human experiments were carried out for text classification and other tasks. While most active learning experiments focus on reducing the number of examples to achieve an accurate model, there has been some effort to model the reduction of the cost of human time in annotation, where the human time is non-uniform per example. Both the systems in Culotta and McCallum (2005) and in Clancy et al. (2012) for the task of named entity extraction, modeled human cost in the context of sequential information extraction tasks. However, one difference between these systems and ours is that all of th</context>
</contexts>
<marker>Settles, Zhu, 2012</marker>
<rawString>Settles, B., &amp; Zhu, X. (2012). Behavioral factors in interactive training of text classifiers. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 563-567).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tong</author>
<author>D Koller</author>
</authors>
<title>Support vector machine active learning with applications to text classification.</title>
<date>2002</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>2</volume>
<pages>45--66</pages>
<marker>Tong, Koller, 2002</marker>
<rawString>Tong, S., &amp; Koller, D. (2002). Support vector machine active learning with applications to text classification. The Journal of Machine Learning Research, 2, 45-66.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003811">
<title confidence="0.99091">
Learning Grammar Specifications from IGT: A Case Study of Chintang
</title>
<author confidence="0.999342">
Emily M. Bender Joshua Crowgey Michael Wayne Goodman Fei Xia
</author>
<affiliation confidence="0.9985">
Department of Linguistics
University of Washington
</affiliation>
<address confidence="0.766514">
Seattle, WA 98195-4340 USA
</address>
<email confidence="0.999715">
{ebender,jcrowgey,goodmami,fxia}@uw.edu
</email>
<sectionHeader confidence="0.997401" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989642857143">
We present a case study of the methodol-
ogy of using information extracted from
interlinear glossed text (IGT) to create of
actual working HPSG grammar fragments
using the Grammar Matrix focusing on
one language: Chintang. Though the re-
sults are barely measurable in terms of
coverage over running text, they nonethe-
less provide a proof of concept. Our expe-
rience report reflects on the ways in which
this task is non-trivial and on mismatches
between the assumptions of the methodol-
ogy and the realities of IGT as produced in
a large-scale field project.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996765">
We explore the possibility of learning precision
grammar fragments from existing products of doc-
umentary linguistic work. A precision grammar is
a grammar which encodes a sharp notion of gram-
maticality and furthermore relates strings to elabo-
rate semantic representations. Such objects are of
interest in the context of documentary linguistics
because: (1) they are valuable tools in the explo-
ration of linguistic hypotheses (especially regard-
ing the interaction of various phenomena); (2) they
facilitate the search for examples in corpora which
are not yet understood; and (3) they can support
the development of treebanks (see Bender et al.,
2012a). However, they are expensive to build.
The present work is carried out in the context of
the AGGREGATION project,1 which is exploring
whether such grammars can be learned on the ba-
sis of data already collected and enriched through
the work of descriptive linguists, specifically, col-
lections of IGT (interlinear glossed text).
The grammars themselves are not likely targets
for machine learning, especially in the absence of
</bodyText>
<footnote confidence="0.930161">
1http://depts.washington.edu/uwcl/aggregation/
</footnote>
<bodyText confidence="0.999954073170732">
treebanks, which are not generally available for
languages that are the focus of descriptive and
documentary linguistics. Instead, we take advan-
tage of the LinGO Grammar Matrix customiza-
tion system (Bender et al., 2002; Bender et al.,
2010) which maps from collections of statements
of linguistic properties (encoded in choices files)
to HPSG (Pollard and Sag, 1994) grammar frag-
ments which in turn can be used to parse strings
into semantic representations in the format of Min-
imal Recursion Semantics (MRS; Copestake et al.,
2005) and conversely, to generate strings from
MRS representations. The choices files are a
much simpler representation than the grammars
derived from them and therefore a more approach-
able learning target. Furthermore, using the Gram-
mar Matrix customization system to produce the
grammars results in much less noise in the auto-
matically derived grammar code than would arise
in a system learning grammars directly.
Here, we focus on a case study of Chintang, a
Kiranti language of Nepal, described by the Chin-
tang Language Research Project (CLRP) (Bickel
et al., 2009). Where Lewis and Xia (2008) and
Bender et al. (2013) apply similar methodologies
to extract large scale properties for many lan-
guages, we focus on a case study of a single lan-
guage, looking at both the large scale properties
and the lexical details. This is important for two
reasons: First, it gives us a chance to look in-
depth at the possible sources of difficulty in ex-
tracting the large scale properties. Second, while
large-scale properties are undoubtedly important,
the bulk of the information specified in a preci-
sion grammar is far more fine-grained. In this
case study we apply the methodology of Bender et
al. (2013) to extract general word order and case
properties and examine the sources of error affect-
ing those results. We also explore extensions of
those methodologies and that of Wax (2014) to ex-
tract lexical entries and specifications for morpho-
</bodyText>
<page confidence="0.997672">
43
</page>
<note confidence="0.848517">
Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 43–53,
Baltimore, Maryland, USA, 26 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999836214285714">
logical rules. Together with a few default spec-
ifications, this information is enough to allow us
to define grammars through the Grammar Matrix
customization system and thus evaluate the results
in terms of parsing coverage, accuracy and am-
biguity over running text. Chintang is particu-
larly well-suited for this case study because it is
an actual endangered language subject to active
descriptive research, making the evaluation of our
techniques realistic. Furthermore, the descriptive
research on Chintang is fairly advanced, having
produced both large corpora of high-quality IGT
and sophisticated linguistic descriptions, making
the evaluation and error analysis possible.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.980529371428571">
This work can be understood as a task related to
both grammar induction and grammar extraction,
though it is distinct from both. It also connects
with and extends previous work using interlinear
glossed text to extract grammatical properties.
Grammar induction (Clark, 2001; Klein and
Manning, 2002; Klein and Manning, 2004;
Haghighi and Klein, 2006; Smith and Eisner,
2006; Snyder et al., 2009, inter alios) involves the
learning of grammars from unlabeled sentences.
Here, unlabeled means that the sentences are of-
ten POS tagged, but no syntactic structures for
the sentences are available. Most of those stud-
ies choose probabilistic context-free grammars
(PCFGs) or dependency grammars as the gram-
mar framework, and estimate the probability of
the context-free rules or dependency arcs from the
data. These studies improve parsing performance
significantly over some baselines such as the EM
algorithm, but the induced grammars are very dif-
ferent from precision grammars with respect to
content, quality, and grammar framework.
Grammar extraction, on the other hand, learns
grammars (sets of rules) from treebanks. Here the
idea is to use heuristics to convert the syntactic
structures in a treebank into derivation trees con-
forming to a particular framework, and then ex-
tract grammars from those trees. This has been
done in a wide range of grammar frameworks, in-
cluding PCFG (e.g. Krotov et al., 1998), LTAG
(e.g. Xia, 1999; Chen and Vijay-Shanker, 2000),
LFG (e.g. Cahill et al., 2004), CCG (e.g. Hock-
enmaier and Steedman, 2002, 2007), and HPSG
(e.g. Miyao et al., 2004; Cramer and Zhang, 2009).
However, this approach is not applicable to work
</bodyText>
<figure confidence="0.99266852631579">
word-order=v-final
has-dets=yes
noun-det-order=det-noun
...
case-marking=erg-abs
erg-abs-erg-case-name=erg
erg-abs-abs-case-name=abs
...
verb4_valence=erg-abs
verb4_stem1_orth=sams-i-ne
verb4_stem1_pred=_sams-i-ne_v_re
...
verb-pc3_inputs=verb-pc2
verb-pc3_lrt1_name=2nd-person-subj
verb-pc3_lrt1_feat1_name=pernum
verb-pc3_lrt1_feat1_value=2nd
verb-pc3_lrt1_feat1_head=subj
verb-pc3_lrt1_lri1_inflecting=yes
verb-pc3_lrt1_lri1_orth=a-
</figure>
<figureCaption confidence="0.99997">
Figure 1: Excerpts from a choices file
</figureCaption>
<bodyText confidence="0.999828736842105">
on endangered language documentation, as tree-
banks are not available for such languages.
A third line of research attempts to bootstrap
NLP tools for resource-poor languages by taking
advantage of IGT data and resources for resource-
rich languages. The canonical form of an IGT in-
stance includes a language line, a word-to-word
or morpheme-to-morpheme gloss line, and a trans-
lation line (typically in a resource-rich language).
The bootstrapping process starts with word align-
ment of the language line and translation line with
the help of the gloss line. Then the translation line
is parsed and the parse tree is projected to the lan-
guage line using the alignments (Xia and Lewis,
2007). The projected trees can be used to answer
linguistic questions such as word order (Lewis
and Xia, 2008) or bootstrap parsers (Georgi et al.,
2013). Our work extends this methodology to the
construction of precision grammars.
</bodyText>
<sectionHeader confidence="0.99807" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999703538461538">
Our goal in this work is to automatically create
choices files on the basis of IGT data. The choices
files encode both general properties about the lan-
guage we are trying to model as well as more spe-
cific information including lexical classes, lexical
items within lexical classes and definitions of lexi-
cal rules. Lexical rule definitions can include both
morphotactic information (ordering of affixes) as
well as morphosyntactic information, though here
our focus is on the former. Sample excerpts from
a choices file are given in Fig 1. These choices
files are then input into the Grammar Matrix cus-
tomization system2 which produces HPSG gram-
</bodyText>
<footnote confidence="0.943347">
2SVN revision (for reproducibility): 27678.
</footnote>
<page confidence="0.999478">
44
</page>
<bodyText confidence="0.99989875">
mar fragments that meet the specifications in the
choices files. The Grammar Matrix customization
system provides analyses of a range of linguistic
phenomena. Here, we focus on a few that we con-
sider the most basic: major constituent word or-
der, the general case system, case frames for spe-
cific verbs, case marking on nouns, and morpho-
tactics for verbs. In §3.1 we describe the dataset
we are working with. §3.2 describes the different
approaches we take to building choices files on the
basis of this dataset. §3.3 explains the metrics we
will use to evaluate the resulting grammars in §4.
</bodyText>
<subsectionHeader confidence="0.999476">
3.1 The Chintang Dataset
</subsectionHeader>
<bodyText confidence="0.999987759036145">
Chintang (ISO639-3: ctn) is a language spoken by
about 5000 people in Nepal and believed to be-
long to the Eastern subgroup of the Kiranti lan-
guages, which in turn are argued to belong to the
larger Tibeto-Burman family (Bickel et al., 2007;
Schikowski et al., in press). Here we briefly sum-
marize properties of the language that relate to
the information we are attempting to automatically
detect in the IGT, and in many cases make the
problem interestingly difficult.
Schikowski et al. (in press) describe Chintang
as exhibiting information-structurally constrained
word order: All permutations of the major senten-
tial constituents are expected to be valid, with the
different orders subject to different felicity condi-
tions. They state, however, that no detailed analy-
sis of word order has yet been carried out, and so
this description should be taken as preliminary.
In contrast, much detailed work has been done
on the marking of arguments, both via agree-
ment on the verb and via case marking of depen-
dents (Bickel et al., 2010; Stoll and Bickel, 2012;
Schikowski et al., in press). The case marking sys-
tem can be understood as following an ergative-
absolutive pattern, but with several variations from
that theme. In an ergative-absolutive pattern, the
sole argument of an intransitive verb (here called
S) is marked the same as the most patient-like ar-
gument of a transitive verb (here called O) and
differentiated from the most agent-like argument
of a transitive verb (here called A). Most A ar-
guments are marked with an overt case marker
called ergative, while S and O arguments appear
without a case marker. In most writing about the
language, this unmarked case is called nomina-
tive; here we will use the term absolutive. Simi-
larly, verbs agree with up to two arguments, and
the agreement markers for S and O are generally
shared and distinguished from those for A.
Divergences from the ergative-absolutive pat-
tern include variable marking of ergative case on
first and second person pronouns as well as va-
lence alternations such as one that licenses oc-
currences of transitive verbs with two absolutive
arguments (and S-style agreement with the A ar-
gument) when the O argument is of an indefinite
quantity (Schikowski et al., in press). Further-
more, the language allows dropping of arguments
(A, S, and O). Finally, there are of course valences
beyond simple intransitive and transitive, as well
as case frames even for two-argument verbs other
than { ERG, ABS }. As a result of the combination of
these facts, the actual occurrence of ergative-case-
marked arguments in speech is relatively low: Ex-
amining a corpus of speech spoken to and around
children, Stoll and Bickel (2012) find that only
11% of (semantically) transitive verb tokens have
an overt, ergative-marked NP A argument. As dis-
cussed below, these properties make it difficult for
automated methods to detect both the overall case
system of the language and accurate information
regarding the case frames of individual verbs.
The dataset we are using contains 9793 (8863
train, 930 test) IGT instances which come from
the corpus of narratives and other speech col-
lected, transcribed, translated and glossed by the
CLRP.3 An example is shown in Fig. 2. As can
be seen in Fig. 2, the glossing in this dataset is ex-
tremely thorough. It is also supported by a detailed
Toolbox lexicon that encodes not only alternative
forms for each lemma as well as glosses in English
and Nepali, but also valence frames for most verb
entries which list the expected case marking on
the arguments. Finally, note that morphosyntactic
properties without a morphological reflex are sys-
tematically unglossed in the data, so that ABS never
appears (nor does SG for singular nouns, etc.).
In our experiments, we abstract away from the
problem of morphophonological analysis in order
to focus on morphosyntax and lexical acquisition.
Accordingly, our grammars target the second line
of the IGT, which represents each form as a se-
quence of phonologically regularized morphemes.
</bodyText>
<subsectionHeader confidence="0.998802">
3.2 Grammars
</subsectionHeader>
<bodyText confidence="0.9983565">
In this section, we describe the different means we
use for extracting the different kinds of informa-
</bodyText>
<footnote confidence="0.935069">
3http://www.spw.uzh.ch/clrp
</footnote>
<page confidence="0.996124">
45
</page>
<figure confidence="0.91723">
unisaNa khatte mo kosi moba
u-nisa-Na khatt-e mo kosi-i mo-pe
3sPOSS-younger.brother-ERG.A take-IND.PST DEM.DOWN river-LOC DEM.DOWN-LOC
‘The younger brother took it to the river.’ [ctn] (Bickel et al., 2013c)
</figure>
<figureCaption confidence="0.999608">
Figure 2: Sample IGT
</figureCaption>
<bodyText confidence="0.999957291666667">
tion required to build the choices files (see Fig 1
above). We first describe our points of comparison
(oracle, §3.2.1 and baseline, §3.2.2), and then con-
sider different ways of detecting the large-scale
properties (word order, §3.2.3; overall case sys-
tem, §3.2.4). Next we turn to different ways of ex-
tracting two kinds of lexical information: the con-
straints on case (i.e. case frames of verbs and the
case marking on nouns, §3.2.5) and verbal mor-
photactics (§3.2.6). Finally, we describe a small
set of hand-coded ‘choices’ which are added to all
choices files (except the oracle one) in order to cre-
ate working grammars (§3.2.7).
The alternative approaches to extracting the var-
ious kinds of information can be cross-classified
with each other, giving the set of choices files de-
scribed in Table 1. The first column gives iden-
tifiers for the choices files. The second specifies
how the lexicon was created, the third how the
value for major constituent word order was deter-
mined, and the fourth how the values for case were
determined, including the overall case system, the
case frames, and the case values for nouns. These
options are all described in more detail below.
</bodyText>
<subsectionHeader confidence="0.825582">
3.2.1 Oracle choices file
</subsectionHeader>
<bodyText confidence="0.999988607142857">
As an upper-bound, we use the choices file de-
veloped in Bender et al., 2012b. This file in-
cludes hand-specified definitions of lexical rules
for nouns and verbs as well as lexical entries cre-
ated by importing lexical entries from the Tool-
box lexicon developed by the CLRP. This lex-
icon, as noted above, lists valence frames for
most verbal entries. As the Grammar Matrix
customization system currently only provides for
simple transitive and intransitive verbs, only two
verb classes were defined: intransitives with the
case frame { ABS } and transitives with the case
frame { ERG, ABS }. In addition, there is one class
of nouns. Finally, the choices file includes hand-
coded lexical entries for pronouns. As an upper-
bound, this choices file can be expected to repre-
sent high precision and moderate recall: verbs that
don’t fit the two classes defined aren’t imported.
Note that the Grammar Matrix customization
system does not currently support the definition of
adjectives, adverbs, or other parts of speech out-
side of verb, noun, determiner, (certain) adposi-
tions, conjunctions and auxiliaries. Thus while we
expect each grammar to be able to parse at least
some sentences in the corpus, to the extent that
sentences tend to include words outside the classes
noun, verb and determiner, we expect relatively
low coverage, even from our upper-bound.
</bodyText>
<subsectionHeader confidence="0.653894">
3.2.2 Baseline choices file
</subsectionHeader>
<bodyText confidence="0.999942588235294">
Our baseline choices file is designed to create a
working grammar, without particular high-level
information about Chintang, that focuses on cov-
erage at the expense of precision. We hand-
specified the (counter-factual) assertion that there
is no case marking in Chintang, and in addi-
tion that Chintang allows free word order (on the
grounds that this is the least constrained word or-
der possibility). It also defines bare-bones classes
of nouns, determiners and transitive verbs, and
then populates the lexicon by using a variant of the
methodology in Xia and Lewis 2007. In particu-
lar, we parse the translation line using the Char-
niak parser (Charniak, 1997) and then use the cor-
respondences inherent in IGT to create a projected
tree structure for the language line, following Xia
and Lewis. An example of the result for Chintang
is shown in Fig 3. The projected trees include part
of speech tags for each word that can be aligned.
For each such word tagged as noun, verb, or deter-
miner, we create an instance in the corresponding
lexical type. In this baseline grammar, all verbs
are assumed to be transitive, but since all argu-
ments can (optionally) be dropped, the grammar is
expected to be able to cover intransitive sentences,
even if the semantic representation is wrong.
Since this baseline choices file models Chintang
as if it had no case marking, we expect it the re-
sulting grammar to have relatively high recall in
terms of the combination of nominal and verbal
constituents. On the other hand, since it is build-
ing a full-form lexicon and Chintang is a morpho-
logically complex language, we expect it to have
relatively low lexical coverage on held-out data.
</bodyText>
<page confidence="0.998723">
46
</page>
<table confidence="0.999709857142857">
Choices file Lexicon Word order Case
ORACLE Manual Manual Manual
BASELINE Fullform Default None
FF-AUTO-NONE Fullform Auto None
FF-DEFAULT-GRAM Fullform Default Auto (GRAM)
FF-AUTO-GRAM Fullform Auto Auto (GRAM)
FF-DEFAULT-SAO* Fullform Default Auto (SAO)
FF-AUTO-SAO* Fullform Auto Auto (SAO)
MOM-DEFAULT-NONE MOM Default None
MOM-AUTO-NONE MOM Auto None
MOM-DEFAULT-GRAM* MOM Default Auto (GRAM)
MOM-AUTO-GRAM* MOM Auto Auto (GRAM)
MOM-DEFAULT-SAO* MOM Default Auto (SAO)
MOM-AUTO-SAO* MOM Auto Auto (SAO)
</table>
<tableCaption confidence="0.999399">
Table 1: Choices files generated
</tableCaption>
<figureCaption confidence="0.987123">
Figure 3: Projected tree structure (ex. from (Bickel
et al., 2013d))
</figureCaption>
<subsectionHeader confidence="0.968137">
3.2.3 Word order
</subsectionHeader>
<bodyText confidence="0.99999175">
We applied the methodology of Bender et al.
(2013) for determining major constituent order.
For our dataset, the algorithm chose ‘v-final’,
which matches what is in the ORACLE choices file,
but is not necessarily correct. We created two ver-
sions of each of the other choices files, one with
the default (baseline) answer of ‘free word order’
and one with this automatically supplied answer.
</bodyText>
<subsectionHeader confidence="0.9554">
3.2.4 Case system
</subsectionHeader>
<bodyText confidence="0.99999225">
Similarly, we applied extended versions of the two
methods for automatically discovering case sys-
tems from Bender et al. 2013: GRAM which looks
for known case grams in glosses (not using pro-
jected trees) and SAO which extends the structure-
projection methodology of Xia and Lewis (2007)
to detect S, A and O arguments and then looks
for the most frequent gram associated with each
of these.4 The GRAM method determines the
case system of Chintang to be ergative-absolutive,
while the SAO method indicates ‘none’ (no case).
Specifying a case system in a choices file has no
effect on the coverage or precision of the resulting
grammar if the lexical items don’t constrain case.
Thus the case system choices only make sense in
combination with the case frames choices (§3.2.5).
</bodyText>
<subsectionHeader confidence="0.979431">
3.2.5 Case frames and case values
</subsectionHeader>
<bodyText confidence="0.99991252631579">
The HPSG analysis of case involves a feature CASE
which is constrained by both verbs and nouns:
Nouns constrain their own CASE value, while verbs
constrain the CASE value of the arguments they se-
lect for.5 In order to constrain verbs and nouns
appropriately, we first need a range of possible
case values. For choices files built based on the
GRAM system, we consider case markers to be any
of those included in the set of grams defined by
the Leipzig Glossing Rules (Bickel et al., 2008):
ABL, ABS, ACC, ALL, COM, DAT, ERG, GEN, INS, LOC,
and OBL. For choices files built based on the SAO
system, we consider as case markers only those
grams (automatically) identified as marking S, A,
or O. In the present study, that should only be erga-
tive; as there is no marked case for absolutive, all
other nouns were treated as absolutive (regardless
of their actual case marking, since the SAO system
has no way to detect other case grams).
</bodyText>
<footnote confidence="0.987641">
4Our extensions involved making the system able to han-
dle the situation where one or more of S, A and O are morpho-
logically unmarked and therefore unreflected in the glosses.
5For the details of the analyses of case systems provided
by the Grammar Matrix, see Drellishak 2009.
</footnote>
<figure confidence="0.992979666666667">
shoe buy-PST-1sS/P-IND.PST
I bought a pair of shoe .
PRP VBD DT NN IN NN
NP-SUBJ-PRP NP NP
PP .
NP-OBJ
VP
S
NP-OBJ
PP
NP
NN VBD
jutta khet-a-ij-e
S
VP
</figure>
<page confidence="0.996883">
47
</page>
<bodyText confidence="0.9784525">
In choices files which specify case systems, we
constrain the case value for nouns by creating one
noun class for every case value, and then assigning
the lexical entries for nouns to those lexical classes
based on the grams in the gloss of the noun.6
Similarly, we create lexical classes for each
case frame identified for transitive and intransitive
verbs: We look for case grams on each argument
of the verb, as determined by the function tags in
the projected tree (e.g. NP-SUBJ-PRP in Fig 3).7 For
each case frame we identify, we create a lexical
class, and we create lexical entries for verbs based
on the case frames we extract for them. When
the system identifies both an overt subject and an
overt object, it considers the verb to be transitive
and constrains the case of its two arguments based
on the observed case values. If either argument
is overt but not marked for case, the verb is con-
strained to select for the default case on that argu-
ment, according to the detected case marking sys-
tem (i.e. ergative for transitive subjects and absolu-
tive for transitive objects, in this instance). When
there is an overt subject but no overt object, the
verb is treated as intransitive and is constrained to
select for a subject of the observed case (or the
default case, here absolutive, if the overt subject
bears no case marker). When there is an overt ob-
ject but no subject, the verb is assumed to be tran-
sitive and the object’s case assigned as with other
transitives but the subject’s case is constrained to
the default (i.e. ergative, in this instance). Verbs
with no overt arguments are not matched.
3.2.6 MOM choices file: Automatically
extracted lemmas and lexical rules
The final refinement we try on our baseline is
to apply the ‘Matrix-ODIN Morphology’ (MOM)
methodology of Wax 2014. This methodology at-
tempts to automatically identify affixes and cre-
ate appropriate descriptions of lexical rules in a
choices file to model those affixes. As a result,
it also identifies stems. Thus we use the same ba-
sic choices as in the baseline choices file, but now
populate the lexicon with stems rather than full-
forms. Compared to BASELINE, this one should re-
sult in a grammar with better lexical coverage on
held-out data, to the extent that the MOM system
</bodyText>
<footnote confidence="0.994915333333333">
6In future work, we plan to extend the MOM approach
(§3.2.6) from verbs to nouns, but for now, the nouns are
treated as full-form lexical entries across all choices files.
7While the GRAM method doesn’t require the projected
trees to determine the overall case system, we do need them
here to find case frames for particular verbs.
</footnote>
<bodyText confidence="0.9943235">
is able to correctly extract both stems and inflec-
tional rules. We note that while the MOM system
uses the same conceptual approach to alignment as
that in the BASELINE, GRAM and SAO approaches, the
implementation is separate, and so does not find
exactly the same set of verbs.
</bodyText>
<subsectionHeader confidence="0.698754">
3.2.7 Shared choices
</subsectionHeader>
<bodyText confidence="0.99991325">
The ORACLE choices file ran as-is. For the re-
maining choices files, we also needed to answer
the questions about determiners (whether there are
any, position with respect to the noun). Based on
initial experiments, we chose ‘yes’ for the pres-
ence of determiners and ‘det-noun’ order. In an
attempt to boost coverage generally, we also coded
the choices that allow any argument to be dropped.
While the determiner-related choices are specific
to Chintang, the latter set of choices could be ex-
pected to boost coverage (at the cost of some pre-
cision) for any language.
</bodyText>
<subsectionHeader confidence="0.665884">
3.2.8 Summary
</subsectionHeader>
<bodyText confidence="0.999809260869565">
Table 1 shows the 10 logical possibilities that arise
from combining the methods discussed in this sec-
tion, in addition to the ORACLE grammar and the
BASELINE grammar. However, we test only a subset
of these possibilities for the following reasons:8
The SAO system chose no case as the case system
for Chintang. As a result, this makes FF-DEFAULT-
SAO and FF-AUTO-SAO the same as BASELINE and FF-
AUTO-NONE, respectively. In future work, we aim
to improve the SAO system but until it is effec-
tive enough to pick some case system for Chin-
tang, these options do not require further testing.
Secondly, while it is possible in principle to com-
bine the output of the MOM system (which classi-
fies verbs based on their morphological combina-
toric potential) with the output of the system be-
hind the GRAM choices files (which classifies verbs
based on their case frames), doing so is non-trivial
because these classifications are orthogonal, yet
each verb must inherit from each dimension. We
thus leave the exploration of MOM-DEFAULT-GRAM
and MOM-AUTO-GRAM (and likewise MOM-DEFAULT-
SAO and MOM-AUTO-SAO) for future work.
</bodyText>
<subsectionHeader confidence="0.992765">
3.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.99985575">
We evaluate the grammars generated by the
choices files over both the data used to develop
them (‘training’; 8863 items) as well as data not
included in the development process (held-out
</bodyText>
<footnote confidence="0.922264">
8Untested choices files are marked with an * in the table.
</footnote>
<page confidence="0.999298">
48
</page>
<bodyText confidence="0.9999787">
‘test’ data; 930 items). We run both of these eval-
uations because we are actually testing two sepa-
rate questions. The first is whether the grammars
generated in this way can provide useful analyti-
cal tools to linguists. In this primary use-case, we
expect a linguist to provide the system with all of
their IGT and then use the generated grammars in
order to gain insights into that same data. This
does not amount to a case of testing on the train-
ing data because the annotations provided to the
system (IGT) are not the same as those produced
by the system (full parses, including semantic rep-
resentations). However, we are still interested in
also testing on held-out data in order to answer the
second question: whether grammars generated in
this way can also generalize to further texts.
We evaluate the grammars generated by the
choices files we create in terms of lexical cov-
erage, parse coverage, parse accuracy and am-
biguity. Lexical coverage measures how many
items consist only of word forms recognized by
the grammar. Any item with unknown lexical
items won’t parse.9 Parse coverage is the num-
ber of items that receive any analysis at all, where
ambiguity is the number of different analyses each
item receives. To measure parse accuracy, we
examined the items that parse and determined
which parses had semantic representations whose
predicate-argument structures plausibly matched
what was indicated in the gloss.
</bodyText>
<sectionHeader confidence="0.999942" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999954375">
Table 2 compares the lexical information encoded
in each of the choices files in a quantitative fash-
ion. The first thing to note is that the grammars
vary widely in the size of their lexicons. The BASE-
LINE/FF lexicons are expected to be larger than the
others because they take each fully inflected form
encountered as a separate lexical entry. On the
other hand, the ORACLE choices file was built on the
basis of the Toolbox lexicon (dictionary) from the
CLRP and thus is effectively created on the basis
of a much larger dataset. The GRAM choices files
only contain verbs for which a case frame could
be identified. If the projected tree was not inter-
pretable by our extraction heuristics or if the ex-
ample had no overt arguments, then the verb will
not be extracted. The MOM choices files, on the
</bodyText>
<footnote confidence="0.892785">
9There are methods for handling unknown lexical items
(e.g. Adolphs et al., 2008) in more mature grammars of this
type, but these are not applicable at this stage.
</footnote>
<bodyText confidence="0.999098265306123">
other hand, only need to identify verbs in the string
to be able to extract them, and should be able to
generalize across different inflected forms of the
same verb. This gives a number of verb entries
intermediate between that for BASELINE/FF and the
GRAM files. For nouns, there is less variation: the
MOM files use the same data as the BASELINE, while
the GRAM method faces as simpler problem than
for verbs: it only needs to identify the case gram
(if any) in a noun’s gloss. The slightly larger num-
bers of nouns in the GRAM files v. the others can be
explained by the same form being glossed in two
different ways in the training data.
The remaining differences can be briefly ex-
plained as follows: The ORACLE choices file does
not contain any entries for determiners. The oth-
ers all contain the same 240 entries; one for any
word aligned by the algorithm to a determiner in
the English translation. Only the ORACLE and MOM
choices files attempt to handle morphology, and so
far MOM only does verbal morphology.
Table 3 presents the results of parsing training
and test data with the various grammars, in abso-
lute numbers and in percentages of the entire data
set. The ‘lexical coverage’ columns indicate for
how many items the grammars were able to rec-
ognize each constituent word form. The ‘items
parsed’ columns show the number of items that
received any analysis at all, while ‘items correct’
show the number of items that were judged (by
one of the authors) to have a predicate-argument
structure that plausibly reflects the gloss given in
the IGT. The final column shows the average num-
ber of distinct analyses the grammars find for the
items they parse at all.
The results are in fact barely measurable with
these metrics (especially on the test data), but
nonetheless speak to the differences between the
grammars. Regarding lexical coverage, the ORA-
CLE grammar does best on the test data set. This
is because it is the only choices file not derived
from the training data. Not surprisingly, the BASE-
LINE grammar has the highest number of readings
per item parsed, followed closely by FF-AUTO-NONE
which adds only a minor constraint on word or-
der.10 On the other hand, comparing the number
of items parsed to the number judged correct, ex-
cept for the MOM choices files, the ‘survival rate’
was over 50% for all other tests.11 This suggests
</bodyText>
<footnote confidence="0.998955">
10It is in this relative lack of constraint that BASELINE
mostly clearly forms a baseline to improve upon.
11The vast majority of the incorrect parses for the MOM
</footnote>
<page confidence="0.997784">
49
</page>
<table confidence="0.9998765">
Choices file # verb entries # noun entries # det entries # verb affixes # noun affixes
ORACLE 900 4751 0 160 24
BASELINE 3005 1719 240 0 0
FF-AUTO-NONE 3005 1719 240 0 0
FF-DEFAULT-GRAM 739 1724 240 0 0
FF-AUTO-GRAM 739 1724 240 0 0
MOM-DEFAULT-NONE 1177 1719 240 262 0
MOM-AUTO-NONE 1177 1719 240 262 0
</table>
<tableCaption confidence="0.969171">
Table 2: Amount of lexical information in each choices file
</tableCaption>
<table confidence="0.9995997">
choices file Training Data (N = 8863) Test Data (N = 930)
lexical items items average lexical items items average
coverage (%) parsed (%) correct (%) readings coverage (%) parsed (%) correct (%) readings
ORACLE 1165 174 (3.5) 132 (1.5) 2.17 116 (12.5) 20 (2.2) 10 (1.1) 1.35
BASELINE 1276 398 (7.9) 216 (2.4) 8.30 41 (4.4) 15 (1.6) 8 (0.9) 28.87
FF-AUTO-NONE 1276 (14) 354 (4.0) 196 (2.2) 7.12 41 (4.4) 13 (1.4) 7 (0.8) 13.92
FF-DEFAULT-GRAM 911 (10) 126 (1.4) 84 (0.9) 4.08 18 (1.9) 4 (0.4) 2 (0.2) 5.00
FF-AUTO-GRAM 911 (10) 120 (1.4) 82 (0.9) 3.84 18 (1.9) 4 (0.4) 2 (0.2) 5.00
MOM-DEFAULT-NONE 1102 (12) 814 (9.2) 52 (0.6) 6.04 39 (4.2) 16 (1.7) 3 (0.3) 10.81
MOM-AUTO-NONE 1102 (12) 753 (8.5) 49 (0.6) 4.20 39 (4.2) 10 (1.1) 3 (0.3) 9.20
</table>
<tableCaption confidence="0.999869">
Table 3: Results
</tableCaption>
<bodyText confidence="0.999689833333333">
that, despite the noise introduced by the automatic
methods of lexical extraction, the precision gram-
mar backbone provided by the Grammar Matrix
can still provide high-quality parses.
For example, the BASELINE grammar produces
six parses of the string in (1):
</bodyText>
<listItem confidence="0.981539666666667">
(1) din khiptukum
din khipt-u-kV-m
day count-3P-IND.NPST-1/2nsA
</listItem>
<bodyText confidence="0.949886857142857">
‘(We) count days.’ [ctn] (Bickel et al., 2013b)
Among these six is one which produces the se-
mantic representation in (2). While this grammar
does not yet capture any of the agreement mor-
phology that indicates that the subject is first per-
son plural, it does correctly link the ‘day’ to the
semantic ARG2 of ‘count’.
</bodyText>
<equation confidence="0.7336548">
( h1,
h3: din n day(x4 ),
(2) h5: exist q rel(x4, hs, h7),
hs: khipt-u-kv-m v count(e2, x9, x4)
{ hs = qh3 } )
</equation>
<bodyText confidence="0.9985662">
Finally, we note that the longest items we are
able to parse consist of one verb and two NPs, each
of which can have only up to two words (a deter-
miner and a noun). Most of the examples that do
parse consist of only one or two words, while the
full data set ranges from items of length 1 to items
of length 25 (average 4.5 words/item in training,
choices files involved analyses of words for ‘yes’, ‘well’,
‘what’ and the like as verbs. Note that one form of ‘yes’ is the
copula, and such examples were accepted. Another source of
incorrect parses for many grammars involves homophony be-
tween the focus particle and a verb meaning ‘come’.
5 words/item in test). The Grammar Matrix al-
ready supports some longer sentences in the form
of coordination, so one avenue for future work is
to explore the automatic detection of coordination
strategies. Otherwise, branching out to longer sen-
tences will require additions to the Grammar Ma-
trix allowing the specification of modifiers and a
wider range of valence types for verbs.
</bodyText>
<sectionHeader confidence="0.998515" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.999890227272727">
The opportunity to work closely with one lan-
guage has allowed us to observe several ways
in which the assumptions of the systems we are
building on do not match what we find in the data.
Here we briefly review some of those mismatches
and reflects on what could be done to handle them.
The first observation concerns the non-glossing
of zero-marked morphosyntactic features, such as
absolutive case in Chintang. From the point of
view of a consumer of IGT it is certainly desirable
to have as much information as possible made ex-
plicit in the glossing. From the point of view of
a project creating IGT in the context of on-going
fieldwork, however, it is likely often difficult to
reliably gloss zero morphemes and thus the de-
cision to leave them systematically unglossed is
quite sensible. Both the GRAM method and espe-
cially the SAO method for detecting case systems,
which we extended to extracting case frames for
particular verbs, are not yet fully robust to the
possibility that certain case values are unmarked
morphologically and thus not glossed in the data.
</bodyText>
<page confidence="0.990068">
50
</page>
<bodyText confidence="0.99946375">
While we extended them to a certain extent in this
work, there is still more to be done on this front.
A second observation concerns the glossing of
proper names, as in (3):
</bodyText>
<table confidence="0.785443">
(3) pailego ubhiyauti paphuma
paile-ko u-bhiya paphu-ma
first-GEN 3A-marriage a.clan.of.Rai.people-F
‘His first marriage was with a Phuphu woman.’
[ctn] (Bickel et al., 2013a)
</table>
<bodyText confidence="0.999966390625">
We use statistical alignment between the trans-
lation line and the gloss line and between the
gloss line and the language line in order to project
information from the analysis of the translation
line onto the language line. Glosses such as
‘a.clan.of.Rai.people’ tend to confuse this align-
ment process, though they are very informative to
a human reader of the IGT. Error analysis of sen-
tences for which we were unable to extract subject
and object arguments at all suggested that many
of the errors were caused by misalignments likely
due to the aligner not being able to cope with this
kind of glossing. Future work will explore how to
train the aligner to function better in such cases.
In addition to properties of the glossing conven-
tions, there are also properties of the language that
proved challenging for our system. The first is the
intricate nature of the case-marking system as dis-
cussed in §3.1. In particular, our system does not
model any distinction between 1st and 2nd per-
son pronouns and other nouns, such that when the
pronouns appear without a case marker, they are
taken to be in the unmarked case (i.e. absolutive),
though this is not necessarily so. The second prop-
erty of the language that our system found diffi-
cult is the optionality of arguments. We were able
to adapt our case frame extraction strategy to han-
dle dropped subjects, but dropped objects are more
confounding: our system is unable so far to distin-
guish such verbs from intransitives. One possible
way forward in this case is to draw more informa-
tion from the English translation in the IGT: En-
glish tends not to drop arguments, and so when we
find an object (especially a pronominal object) in
the English translation that is not aligned to any-
thing in the language line, we would have evidence
that the verb in question may be transitive.
Finally, we looked closely at the items in the test
data for which we had complete lexical analysis,
but which still failed to parse. We did this both for
the fullform and MOM-based lexicons. The goal
here was to evaluate whether (a) our assignment of
items to lexical categories was correct (and there
was some other issue standing in the way of an-
alyzing the item) or (b) we should have parsed a
given item, but our system had misidentified the
words in question in such a way that no syntactic
analysis could be found. For the baseline system,
we found that although some items had misidenti-
fied categories (specifically, pronouns and adverbs
were sometimes misidentified as determiners), the
two major obstacles to parsing came from multi-
verb constructions or sentential fragments. Of the
26 unparsed items with lexical coverage, 10 con-
tained multiple verbs and 12 were NP or interjec-
tory fragments (eg: ‘Yes, yes, yes.’). We observed
a similar pattern among 23 unparsed items from
the MOM-based lexicon. We can take two lessons
from this assessment: (1) since much of our data
comes from naturally occurring speech, it may be
useful to rerun our tests with an NP fragment as
a valid root symbol in our grammars; (2) proper
identification of auxiliary verbs is an important
next step for improving our system.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999497733333333">
In this paper we have taken the first steps towards
creating actual precision grammars by creating
Grammar Matrix customization system choices
files on the basis of automated analysis of IGT.
Measured in terms of coverage over held-out data,
the results are hardly impressive and might seem
discouraging. However, we see in these initial for-
ays rather a proof-of-concept. Moreover, the pro-
cess of digging into the details of getting an IGT-
to-grammar system working for one particular lan-
guage has been a very rich source of information
on the mismatches between the assumptions of
systems built to handle high-level properties and
the linguistic facts and glossing conventions of the
kind of data they are meant to handle.
</bodyText>
<sectionHeader confidence="0.998963" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997776">
This material is based upon work supported by
the National Science Foundation under Grant No.
BCS-1160274. Any opinions, findings, and con-
clusions or recommendations expressed in this
material are those of the author(s) and do not nec-
essarily reflect the views of the NSF.
We would like to thank David Wax for his as-
sistance in setting up the MOM system, Olga Za-
maraeva for general discussion, and especially the
CRLP for providing access to the Chintang data.
</bodyText>
<page confidence="0.997997">
51
</page>
<sectionHeader confidence="0.992973" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983557">
Peter Adolphs, Stephan Oepen, Ulrich Callmeier,
Berthold Crysmann, Dan Flickinger, and Bernd
Kiefer. 2008. Some fine points of hybrid natural
language parsing. Marrakech, Morocco, May.
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
John Carroll, Nelleke Oostdijk, and Richard Sut-
cliffe, editors, Proceedings of the Workshop on
Grammar Engineering and Evaluation at the 19th
International Conference on Computational Lin-
guistics, pages 8–14, Taipei, Taiwan.
Emily M. Bender, Scott Drellishak, Antske Fokkens,
Laurie Poulson, and Safiyyah Saleem. 2010. Gram-
mar customization. Research on Language &amp; Com-
putation, pages 1–50. 10.1007/s11168-010-9070-1.
Emily M. Bender, Sumukh Ghodke, Timothy Baldwin,
and Rebecca Dridan. 2012a. From database to tree-
bank: Enhancing hypertext grammars with grammar
engineering and treebank search. In Sebastian Nord-
hoff and Karl-Ludwig G. Poggeman, editors, Elec-
tronic Grammaticography, pages 179–206. Univer-
sity of Hawaii Press, Honolulu.
Emily M. Bender, Robert Schikowski, and Balthasar
Bickel. 2012b. Deriving a lexicon for a precision
grammar from language documentation resources:
A case study of Chintang. In Proceedings of COL-
ING 2012, pages 247–262, Mumbai, India, Decem-
ber. The COLING 2012 Organizing Committee.
Emily M. Bender, Michael Wayne Goodman, Joshua
Crowgey, and Fei Xia. 2013. Towards creating pre-
cision grammars from interlinear glossed text: Infer-
ring large-scale typological properties. In Proceed-
ings of the 7th Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Human-
ities, pages 74–83, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Balthasar Bickel, Goma Banjade, Martin Gaenszle,
Elena Lieven, Netra Paudyal, Ichchha Rai, Manoj
Rai, Novel Kishor Rai, and Sabine Stoll. 2007. Free
prefix ordering in Chintang. Language, 83(1):43–
73.
Balthasar Bickel, Bernard Comrie, and Martin Haspel-
math. 2008. The Leipzig glossing rules: Con-
ventions for interlinear morpheme-by-morpheme
glosses. Max Planck Institute for Evolutionary An-
thropology and Department of Linguistics, Univer-
sity of Leipzig.
Balthasar Bickel, Martin Gaenszle, Novel Kishore Rai,
Elena Lieven, Goma Banjade, Toya Nath Bhatta,
Netra Paudyal, Judith Pettigrew, Ichchha P. Rai,
Manoj Rai, Robert Schikowski, and Sabine Stoll.
2009. Audiovisual corpus of the chintang lan-
guage, including a longitudinal corpus of language
acquisition by six children, plus a trilingual dic-
tionary, paradigm sets, grammar sketches, ethno-
graphic descriptions, and photographs. DOBES
Archive, http://www.mpi.nl/DOBES.
Balthasar Bickel, Manoj Rai, Netra P. Paudyal, Goma
Banjade, Toya N. Bhatta, Martin Gaenszle, Elena
Lieven, Ichchha Purna Rai, Novel Kishore Rai, and
Sabine Stoll. 2010. The syntax of three-argument
verbs in Chintang and Belhare (Southeastern Ki-
ranti). In Studies in Ditransitive Constructions: A
Comparative Handbook, pages 382–408. Mouton de
Gruyter, Berlin.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013a. Hatuwa. Accessed:
15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013b. Khadak’s daily life.
Accessed: 15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013c. Tale of a poor guy.
Accessed: 15 January 2013.
Balthasar Bickel, Martin Gaenszle, Novel Kishore
Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll,
G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew,
and M Rai, I. P.and Rai. 2013d. Talk of kazi’s trip.
Accessed: 15 January 2013.
Aoife Cahill, Michael Burke, Ruth O’Donovan, Josef
Van Genabith, and Andy Way. 2004. Long-distance
dependency resolution in automatically acquired
wide-coverage pcfg-based lfg approximations. In
Proceedings of the 42nd Meeting of the Association
for Computational Linguistics (ACL’04), Main Vol-
ume, pages 319–326, Barcelona, Spain, July.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of AAAI-1997.
John Chen and K. Vijay-Shanker. 2000. Automated
Extraction of TAGs from the Penn Treebank. In
Proc. of the 6th International Workshop on Parsing
Technologies (IWPT-2000), Italy.
Alexander Clark. 2001. Unsupervised induction
of stochastic context-free grammars using distri-
butional clustering. In Proc. of the 5th Confer-
ence on Computational Natural Language Learning
(CoNLL-2001).
Ann Copestake, Dan Flickinger, Carl Pollard, and
Ivan A. Sag. 2005. Minimal recursion semantics:
An introduction. Research on Language &amp; Compu-
tation, 3(4):281–332.
</reference>
<page confidence="0.97416">
52
</page>
<reference confidence="0.99973794">
Bart Cramer and Yi Zhang. 2009. Construction of a
german hpsg grammar from a detailed treebank. In
Proceedings of the 2009 Workshop on Grammar En-
gineering Across Frameworks (GEAF 2009), pages
37–45, Suntec, Singapore.
Scott Drellishak. 2009. Widespread But Not Uni-
versal: Improving the Typological Coverage of the
Grammar Matrix. Ph.D. thesis, University of Wash-
ington.
Ryan Georgi, Fei Xia, and William D. Lewis. 2013.
Enhanced and portable dependency projection algo-
rithms using interlinear glossed text. In Proceedings
of ACL 2013 (Volume 2: Short Papers), pages 306–
311, Sofia, Bulgaria, August.
Aria Haghighi and Dan Klein. 2006. Prototype-
driven grammar induction. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics (COLING/ACL
2006), pages 881–888, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2002.
Acquiring compact lexicalized grammars from a
cleaner treebank. In Proc. of LREC-2002, pages
1974–1981.
Julia Hockenmaier and Mark Steedman. 2007. Ccg-
bank: A corpus of ccg derivations and dependency
structures extracted from the penn treebank. Com-
putational Linguistics, 33(3):355–396.
Dan Klein and Christopher Manning. 2002. A gen-
eral constituent context model for improved gram-
mar induction. In Proceedings of the 40th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-2002), Philadelphia, PA.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting of the Association for Compu-
tational Linguistics (ACL-2004), Barcelona, Spain.
Alexander Krotov, Mark Hepple, Robert Gaizauskas,
and Yorick Wilks. 1998. Compacting the Penn
Treebank Grammar. In Proc. of the 36th Annual
Meeting of the Association for Computational Lin-
guistics (ACL-1998), Montreal, Quebec, Canada.
William D. Lewis and Fei Xia. 2008. Automati-
cally identifying computationally relevant typolog-
ical features. In Proceedings of the Third Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 685–690, Hyderabad, India.
Yusuke Miyao, Takashi Ninomiya, and Junichi Tsu-
jii. 2004. Corpus-oriented grammar development
for acquiring a head-driven phrase structure gram-
mar from the penn treebank. In Proc. of the First In-
ternational Joint Conference on Natural Language
Processing (IJCNLP-2004), Hainan, China.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Studies in Contempo-
rary Linguistics. The University of Chicago Press
and CSLI Publications, Chicago, IL and Stanford,
CA.
Robert Schikowski, Balthasar Bickel, and Netra
Paudyal. in press. Flexible valency in Chintang.
In B. Comrie and A. Malchukov, editors, Valency
Classes: A Comparative Handbook. Mouton de
Gruyter, Berlin.
Noah A. Smith and Jason Eisner. 2006. Annealing
structural bias in multilingual weighted grammar in-
duction. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL/COLING 2006), pages 569–
576, Sydney, Australia, July. Association for Com-
putational Linguistics.
Benjamin Snyder, Tahira Naseem, and Regina Barzi-
lay. 2009. Unsupervised multilingual grammar in-
duction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 73–81, August.
Sabine Stoll and Balthasar Bickel. 2012. How to
measure frequency? Different ways of counting
ergatives in Chintang (Tibeto-Burman, Nepal) and
their implications. In Frank Seifart, Geoffrey Haig,
Nikolaus P. Himmelmann, Dagmar Jung, Anna Mar-
getts, and Paul Trilsbeek, editors, Potentials of Lan-
guage Documentation: Methods, Analyses, and Uti-
lization, pages 83–89. University of Hawai‘i Press,
Manoa.
David Wax. 2014. Automated grammar engineering
for verbal morphology. Master’s thesis, University
of Washington.
Fei Xia and William D. Lewis. 2007. Multilin-
gual structural projection across interlinear text.
In Proc. of the Conference on Human Language
Technologies (HLT/NAACL 2007), pages 452–459,
Rochester, New York.
Fei Xia. 1999. Extracting Tree Adjoining Gram-
mars from Bracketed Corpora. In Proc. of 5th Nat-
ural Language Processing Pacific Rim Symposium
(NLPRS-1999), Beijing, China.
</reference>
<page confidence="0.99935">
53
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.887492">
<title confidence="0.999902">Learning Grammar Specifications from IGT: A Case Study of Chintang</title>
<author confidence="0.999995">Emily M Bender Joshua Crowgey Michael Wayne Goodman Fei</author>
<affiliation confidence="0.9997475">Department of University of</affiliation>
<address confidence="0.999624">Seattle, WA 98195-4340</address>
<abstract confidence="0.992045066666667">We present a case study of the methodology of using information extracted from interlinear glossed text (IGT) to create of actual working HPSG grammar fragments using the Grammar Matrix focusing on one language: Chintang. Though the results are barely measurable in terms of coverage over running text, they nonetheless provide a proof of concept. Our experience report reflects on the ways in which this task is non-trivial and on mismatches between the assumptions of the methodology and the realities of IGT as produced in a large-scale field project.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter Adolphs</author>
<author>Stephan Oepen</author>
<author>Ulrich Callmeier</author>
<author>Berthold Crysmann</author>
<author>Dan Flickinger</author>
<author>Bernd Kiefer</author>
</authors>
<title>Some fine points of hybrid natural language parsing.</title>
<date>2008</date>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="28213" citStr="Adolphs et al., 2008" startWordPosition="4595" endWordPosition="4598">because they take each fully inflected form encountered as a separate lexical entry. On the other hand, the ORACLE choices file was built on the basis of the Toolbox lexicon (dictionary) from the CLRP and thus is effectively created on the basis of a much larger dataset. The GRAM choices files only contain verbs for which a case frame could be identified. If the projected tree was not interpretable by our extraction heuristics or if the example had no overt arguments, then the verb will not be extracted. The MOM choices files, on the 9There are methods for handling unknown lexical items (e.g. Adolphs et al., 2008) in more mature grammars of this type, but these are not applicable at this stage. other hand, only need to identify verbs in the string to be able to extract them, and should be able to generalize across different inflected forms of the same verb. This gives a number of verb entries intermediate between that for BASELINE/FF and the GRAM files. For nouns, there is less variation: the MOM files use the same data as the BASELINE, while the GRAM method faces as simpler problem than for verbs: it only needs to identify the case gram (if any) in a noun’s gloss. The slightly larger numbers of nouns </context>
</contexts>
<marker>Adolphs, Oepen, Callmeier, Crysmann, Flickinger, Kiefer, 2008</marker>
<rawString>Peter Adolphs, Stephan Oepen, Ulrich Callmeier, Berthold Crysmann, Dan Flickinger, and Bernd Kiefer. 2008. Some fine points of hybrid natural language parsing. Marrakech, Morocco, May.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>The grammar matrix: An open-source starterkit for the rapid development of cross-linguistically consistent broad-coverage precision grammars. In</title>
<date>2002</date>
<booktitle>Proceedings of the Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics,</booktitle>
<pages>8--14</pages>
<editor>John Carroll, Nelleke Oostdijk, and Richard Sutcliffe, editors,</editor>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="2167" citStr="Bender et al., 2002" startWordPosition="325" endWordPosition="328">in the context of the AGGREGATION project,1 which is exploring whether such grammars can be learned on the basis of data already collected and enriched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text). The grammars themselves are not likely targets for machine learning, especially in the absence of 1http://depts.washington.edu/uwcl/aggregation/ treebanks, which are not generally available for languages that are the focus of descriptive and documentary linguistics. Instead, we take advantage of the LinGO Grammar Matrix customization system (Bender et al., 2002; Bender et al., 2010) which maps from collections of statements of linguistic properties (encoded in choices files) to HPSG (Pollard and Sag, 1994) grammar fragments which in turn can be used to parse strings into semantic representations in the format of Minimal Recursion Semantics (MRS; Copestake et al., 2005) and conversely, to generate strings from MRS representations. The choices files are a much simpler representation than the grammars derived from them and therefore a more approachable learning target. Furthermore, using the Grammar Matrix customization system to produce the grammars r</context>
</contexts>
<marker>Bender, Flickinger, Oepen, 2002</marker>
<rawString>Emily M. Bender, Dan Flickinger, and Stephan Oepen. 2002. The grammar matrix: An open-source starterkit for the rapid development of cross-linguistically consistent broad-coverage precision grammars. In John Carroll, Nelleke Oostdijk, and Richard Sutcliffe, editors, Proceedings of the Workshop on Grammar Engineering and Evaluation at the 19th International Conference on Computational Linguistics, pages 8–14, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Scott Drellishak</author>
<author>Antske Fokkens</author>
<author>Laurie Poulson</author>
<author>Safiyyah Saleem</author>
</authors>
<title>Grammar customization.</title>
<date>2010</date>
<journal>Research on Language &amp; Computation,</journal>
<pages>1--50</pages>
<contexts>
<context position="2189" citStr="Bender et al., 2010" startWordPosition="329" endWordPosition="332"> AGGREGATION project,1 which is exploring whether such grammars can be learned on the basis of data already collected and enriched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text). The grammars themselves are not likely targets for machine learning, especially in the absence of 1http://depts.washington.edu/uwcl/aggregation/ treebanks, which are not generally available for languages that are the focus of descriptive and documentary linguistics. Instead, we take advantage of the LinGO Grammar Matrix customization system (Bender et al., 2002; Bender et al., 2010) which maps from collections of statements of linguistic properties (encoded in choices files) to HPSG (Pollard and Sag, 1994) grammar fragments which in turn can be used to parse strings into semantic representations in the format of Minimal Recursion Semantics (MRS; Copestake et al., 2005) and conversely, to generate strings from MRS representations. The choices files are a much simpler representation than the grammars derived from them and therefore a more approachable learning target. Furthermore, using the Grammar Matrix customization system to produce the grammars results in much less no</context>
</contexts>
<marker>Bender, Drellishak, Fokkens, Poulson, Saleem, 2010</marker>
<rawString>Emily M. Bender, Scott Drellishak, Antske Fokkens, Laurie Poulson, and Safiyyah Saleem. 2010. Grammar customization. Research on Language &amp; Computation, pages 1–50. 10.1007/s11168-010-9070-1.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Emily M Bender</author>
</authors>
<title>Sumukh Ghodke, Timothy Baldwin, and Rebecca Dridan. 2012a. From database to treebank: Enhancing hypertext grammars with grammar engineering and treebank search.</title>
<booktitle>Electronic Grammaticography,</booktitle>
<pages>179--206</pages>
<editor>In Sebastian Nordhoff and Karl-Ludwig G. Poggeman, editors,</editor>
<publisher>University of Hawaii Press,</publisher>
<location>Honolulu.</location>
<marker>Bender, </marker>
<rawString>Emily M. Bender, Sumukh Ghodke, Timothy Baldwin, and Rebecca Dridan. 2012a. From database to treebank: Enhancing hypertext grammars with grammar engineering and treebank search. In Sebastian Nordhoff and Karl-Ludwig G. Poggeman, editors, Electronic Grammaticography, pages 179–206. University of Hawaii Press, Honolulu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Robert Schikowski</author>
<author>Balthasar Bickel</author>
</authors>
<title>Deriving a lexicon for a precision grammar from language documentation resources: A case study of Chintang.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>247--262</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="1474" citStr="Bender et al., 2012" startWordPosition="223" endWordPosition="226">learning precision grammar fragments from existing products of documentary linguistic work. A precision grammar is a grammar which encodes a sharp notion of grammaticality and furthermore relates strings to elaborate semantic representations. Such objects are of interest in the context of documentary linguistics because: (1) they are valuable tools in the exploration of linguistic hypotheses (especially regarding the interaction of various phenomena); (2) they facilitate the search for examples in corpora which are not yet understood; and (3) they can support the development of treebanks (see Bender et al., 2012a). However, they are expensive to build. The present work is carried out in the context of the AGGREGATION project,1 which is exploring whether such grammars can be learned on the basis of data already collected and enriched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text). The grammars themselves are not likely targets for machine learning, especially in the absence of 1http://depts.washington.edu/uwcl/aggregation/ treebanks, which are not generally available for languages that are the focus of descriptive and documentary linguistics. Ins</context>
<context position="14853" citStr="Bender et al., 2012" startWordPosition="2337" endWordPosition="2340">s to extracting the various kinds of information can be cross-classified with each other, giving the set of choices files described in Table 1. The first column gives identifiers for the choices files. The second specifies how the lexicon was created, the third how the value for major constituent word order was determined, and the fourth how the values for case were determined, including the overall case system, the case frames, and the case values for nouns. These options are all described in more detail below. 3.2.1 Oracle choices file As an upper-bound, we use the choices file developed in Bender et al., 2012b. This file includes hand-specified definitions of lexical rules for nouns and verbs as well as lexical entries created by importing lexical entries from the Toolbox lexicon developed by the CLRP. This lexicon, as noted above, lists valence frames for most verbal entries. As the Grammar Matrix customization system currently only provides for simple transitive and intransitive verbs, only two verb classes were defined: intransitives with the case frame { ABS } and transitives with the case frame { ERG, ABS }. In addition, there is one class of nouns. Finally, the choices file includes handcode</context>
</contexts>
<marker>Bender, Schikowski, Bickel, 2012</marker>
<rawString>Emily M. Bender, Robert Schikowski, and Balthasar Bickel. 2012b. Deriving a lexicon for a precision grammar from language documentation resources: A case study of Chintang. In Proceedings of COLING 2012, pages 247–262, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Michael Wayne Goodman</author>
<author>Joshua Crowgey</author>
<author>Fei Xia</author>
</authors>
<title>Towards creating precision grammars from interlinear glossed text: Inferring large-scale typological properties.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities,</booktitle>
<pages>74--83</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3098" citStr="Bender et al. (2013)" startWordPosition="476" endWordPosition="479">05) and conversely, to generate strings from MRS representations. The choices files are a much simpler representation than the grammars derived from them and therefore a more approachable learning target. Furthermore, using the Grammar Matrix customization system to produce the grammars results in much less noise in the automatically derived grammar code than would arise in a system learning grammars directly. Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (Bickel et al., 2009). Where Lewis and Xia (2008) and Bender et al. (2013) apply similar methodologies to extract large scale properties for many languages, we focus on a case study of a single language, looking at both the large scale properties and the lexical details. This is important for two reasons: First, it gives us a chance to look indepth at the possible sources of difficulty in extracting the large scale properties. Second, while large-scale properties are undoubtedly important, the bulk of the information specified in a precision grammar is far more fine-grained. In this case study we apply the methodology of Bender et al. (2013) to extract general word </context>
<context position="18509" citStr="Bender et al. (2013)" startWordPosition="2927" endWordPosition="2930"> Manual BASELINE Fullform Default None FF-AUTO-NONE Fullform Auto None FF-DEFAULT-GRAM Fullform Default Auto (GRAM) FF-AUTO-GRAM Fullform Auto Auto (GRAM) FF-DEFAULT-SAO* Fullform Default Auto (SAO) FF-AUTO-SAO* Fullform Auto Auto (SAO) MOM-DEFAULT-NONE MOM Default None MOM-AUTO-NONE MOM Auto None MOM-DEFAULT-GRAM* MOM Default Auto (GRAM) MOM-AUTO-GRAM* MOM Auto Auto (GRAM) MOM-DEFAULT-SAO* MOM Default Auto (SAO) MOM-AUTO-SAO* MOM Auto Auto (SAO) Table 1: Choices files generated Figure 3: Projected tree structure (ex. from (Bickel et al., 2013d)) 3.2.3 Word order We applied the methodology of Bender et al. (2013) for determining major constituent order. For our dataset, the algorithm chose ‘v-final’, which matches what is in the ORACLE choices file, but is not necessarily correct. We created two versions of each of the other choices files, one with the default (baseline) answer of ‘free word order’ and one with this automatically supplied answer. 3.2.4 Case system Similarly, we applied extended versions of the two methods for automatically discovering case systems from Bender et al. 2013: GRAM which looks for known case grams in glosses (not using projected trees) and SAO which extends the structurepr</context>
</contexts>
<marker>Bender, Goodman, Crowgey, Xia, 2013</marker>
<rawString>Emily M. Bender, Michael Wayne Goodman, Joshua Crowgey, and Fei Xia. 2013. Towards creating precision grammars from interlinear glossed text: Inferring large-scale typological properties. In Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 74–83, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balthasar Bickel</author>
</authors>
<title>Goma Banjade, Martin Gaenszle, Elena Lieven, Netra Paudyal, Ichchha Rai, Manoj Rai, Novel Kishor Rai,</title>
<date>2007</date>
<journal>Language,</journal>
<volume>83</volume>
<issue>1</issue>
<pages>73</pages>
<location>and</location>
<marker>Bickel, 2007</marker>
<rawString>Balthasar Bickel, Goma Banjade, Martin Gaenszle, Elena Lieven, Netra Paudyal, Ichchha Rai, Manoj Rai, Novel Kishor Rai, and Sabine Stoll. 2007. Free prefix ordering in Chintang. Language, 83(1):43– 73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balthasar Bickel</author>
<author>Bernard Comrie</author>
<author>Martin Haspelmath</author>
</authors>
<title>The Leipzig glossing rules: Conventions for interlinear morpheme-by-morpheme glosses.</title>
<date>2008</date>
<institution>Max Planck Institute for Evolutionary Anthropology and Department of Linguistics, University of Leipzig.</institution>
<contexts>
<context position="20165" citStr="Bickel et al., 2008" startWordPosition="3208" endWordPosition="3211">hus the case system choices only make sense in combination with the case frames choices (§3.2.5). 3.2.5 Case frames and case values The HPSG analysis of case involves a feature CASE which is constrained by both verbs and nouns: Nouns constrain their own CASE value, while verbs constrain the CASE value of the arguments they select for.5 In order to constrain verbs and nouns appropriately, we first need a range of possible case values. For choices files built based on the GRAM system, we consider case markers to be any of those included in the set of grams defined by the Leipzig Glossing Rules (Bickel et al., 2008): ABL, ABS, ACC, ALL, COM, DAT, ERG, GEN, INS, LOC, and OBL. For choices files built based on the SAO system, we consider as case markers only those grams (automatically) identified as marking S, A, or O. In the present study, that should only be ergative; as there is no marked case for absolutive, all other nouns were treated as absolutive (regardless of their actual case marking, since the SAO system has no way to detect other case grams). 4Our extensions involved making the system able to handle the situation where one or more of S, A and O are morphologically unmarked and therefore unrefle</context>
</contexts>
<marker>Bickel, Comrie, Haspelmath, 2008</marker>
<rawString>Balthasar Bickel, Bernard Comrie, and Martin Haspelmath. 2008. The Leipzig glossing rules: Conventions for interlinear morpheme-by-morpheme glosses. Max Planck Institute for Evolutionary Anthropology and Department of Linguistics, University of Leipzig.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Balthasar Bickel</author>
<author>Martin Gaenszle</author>
<author>Novel Kishore Rai</author>
<author>Elena Lieven</author>
<author>Goma Banjade</author>
<author>Toya Nath Bhatta</author>
<author>Netra Paudyal</author>
<author>Judith Pettigrew</author>
<author>Ichchha P Rai</author>
<author>Manoj Rai</author>
<author>Robert Schikowski</author>
<author>Sabine Stoll</author>
</authors>
<title>Audiovisual corpus of the chintang language, including a longitudinal corpus of language acquisition by six children, plus a trilingual dictionary, paradigm sets, grammar sketches, ethnographic descriptions, and photographs.</title>
<date>2009</date>
<journal>DOBES Archive, http://www.mpi.nl/DOBES.</journal>
<contexts>
<context position="3045" citStr="Bickel et al., 2009" startWordPosition="466" endWordPosition="469">inimal Recursion Semantics (MRS; Copestake et al., 2005) and conversely, to generate strings from MRS representations. The choices files are a much simpler representation than the grammars derived from them and therefore a more approachable learning target. Furthermore, using the Grammar Matrix customization system to produce the grammars results in much less noise in the automatically derived grammar code than would arise in a system learning grammars directly. Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (Bickel et al., 2009). Where Lewis and Xia (2008) and Bender et al. (2013) apply similar methodologies to extract large scale properties for many languages, we focus on a case study of a single language, looking at both the large scale properties and the lexical details. This is important for two reasons: First, it gives us a chance to look indepth at the possible sources of difficulty in extracting the large scale properties. Second, while large-scale properties are undoubtedly important, the bulk of the information specified in a precision grammar is far more fine-grained. In this case study we apply the methodo</context>
</contexts>
<marker>Bickel, Gaenszle, Rai, Lieven, Banjade, Bhatta, Paudyal, Pettigrew, Rai, Rai, Schikowski, Stoll, 2009</marker>
<rawString>Balthasar Bickel, Martin Gaenszle, Novel Kishore Rai, Elena Lieven, Goma Banjade, Toya Nath Bhatta, Netra Paudyal, Judith Pettigrew, Ichchha P. Rai, Manoj Rai, Robert Schikowski, and Sabine Stoll. 2009. Audiovisual corpus of the chintang language, including a longitudinal corpus of language acquisition by six children, plus a trilingual dictionary, paradigm sets, grammar sketches, ethnographic descriptions, and photographs. DOBES Archive, http://www.mpi.nl/DOBES.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balthasar Bickel</author>
<author>Manoj Rai</author>
<author>Netra P Paudyal</author>
<author>Goma Banjade</author>
<author>Toya N Bhatta</author>
<author>Martin Gaenszle</author>
</authors>
<title>Elena Lieven, Ichchha Purna Rai, Novel Kishore Rai, and Sabine Stoll.</title>
<date>2010</date>
<booktitle>In Studies in Ditransitive Constructions: A Comparative Handbook,</booktitle>
<pages>382--408</pages>
<location>Berlin.</location>
<contexts>
<context position="10207" citStr="Bickel et al., 2010" startWordPosition="1572" endWordPosition="1575"> in many cases make the problem interestingly difficult. Schikowski et al. (in press) describe Chintang as exhibiting information-structurally constrained word order: All permutations of the major sentential constituents are expected to be valid, with the different orders subject to different felicity conditions. They state, however, that no detailed analysis of word order has yet been carried out, and so this description should be taken as preliminary. In contrast, much detailed work has been done on the marking of arguments, both via agreement on the verb and via case marking of dependents (Bickel et al., 2010; Stoll and Bickel, 2012; Schikowski et al., in press). The case marking system can be understood as following an ergativeabsolutive pattern, but with several variations from that theme. In an ergative-absolutive pattern, the sole argument of an intransitive verb (here called S) is marked the same as the most patient-like argument of a transitive verb (here called O) and differentiated from the most agent-like argument of a transitive verb (here called A). Most A arguments are marked with an overt case marker called ergative, while S and O arguments appear without a case marker. In most writin</context>
</contexts>
<marker>Bickel, Rai, Paudyal, Banjade, Bhatta, Gaenszle, 2010</marker>
<rawString>Balthasar Bickel, Manoj Rai, Netra P. Paudyal, Goma Banjade, Toya N. Bhatta, Martin Gaenszle, Elena Lieven, Ichchha Purna Rai, Novel Kishore Rai, and Sabine Stoll. 2010. The syntax of three-argument verbs in Chintang and Belhare (Southeastern Kiranti). In Studies in Ditransitive Constructions: A Comparative Handbook, pages 382–408. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balthasar Bickel</author>
<author>Martin Gaenszle</author>
</authors>
<title>Novel Kishore Rai, Vishnu Singh Rai,</title>
<date>2013</date>
<journal>Hatuwa. Accessed:</journal>
<volume>15</volume>
<location>Elena</location>
<marker>Bickel, Gaenszle, 2013</marker>
<rawString>Balthasar Bickel, Martin Gaenszle, Novel Kishore Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll, G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew, and M Rai, I. P.and Rai. 2013a. Hatuwa. Accessed: 15 January 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balthasar Bickel</author>
<author>Martin Gaenszle</author>
</authors>
<title>Novel Kishore Rai, Vishnu Singh Rai,</title>
<date>2013</date>
<booktitle>P.and Rai. 2013b. Khadak’s daily life. Accessed: 15</booktitle>
<location>Elena</location>
<marker>Bickel, Gaenszle, 2013</marker>
<rawString>Balthasar Bickel, Martin Gaenszle, Novel Kishore Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll, G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew, and M Rai, I. P.and Rai. 2013b. Khadak’s daily life. Accessed: 15 January 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balthasar Bickel</author>
<author>Martin Gaenszle</author>
</authors>
<title>Novel Kishore Rai, Vishnu Singh Rai,</title>
<date>2013</date>
<journal>Accessed:</journal>
<volume>15</volume>
<location>Elena</location>
<marker>Bickel, Gaenszle, 2013</marker>
<rawString>Balthasar Bickel, Martin Gaenszle, Novel Kishore Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll, G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew, and M Rai, I. P.and Rai. 2013c. Tale of a poor guy. Accessed: 15 January 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Balthasar Bickel</author>
<author>Martin Gaenszle</author>
</authors>
<title>Novel Kishore Rai, Vishnu Singh Rai,</title>
<date>2013</date>
<booktitle>P.and Rai. 2013d. Talk of kazi’s trip. Accessed: 15</booktitle>
<location>Elena</location>
<marker>Bickel, Gaenszle, 2013</marker>
<rawString>Balthasar Bickel, Martin Gaenszle, Novel Kishore Rai, Vishnu Singh Rai, Elena Lieven, Sabine Stoll, G. Banjade, T. N. Bhatta, N Paudyal, J Pettigrew, and M Rai, I. P.and Rai. 2013d. Talk of kazi’s trip. Accessed: 15 January 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Michael Burke</author>
<author>Ruth O’Donovan</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Long-distance dependency resolution in automatically acquired wide-coverage pcfg-based lfg approximations.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>319--326</pages>
<location>Barcelona, Spain,</location>
<marker>Cahill, Burke, O’Donovan, Van Genabith, Way, 2004</marker>
<rawString>Aoife Cahill, Michael Burke, Ruth O’Donovan, Josef Van Genabith, and Andy Way. 2004. Long-distance dependency resolution in automatically acquired wide-coverage pcfg-based lfg approximations. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 319–326, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of AAAI-1997.</booktitle>
<contexts>
<context position="16816" citStr="Charniak, 1997" startWordPosition="2658" endWordPosition="2659">eate a working grammar, without particular high-level information about Chintang, that focuses on coverage at the expense of precision. We handspecified the (counter-factual) assertion that there is no case marking in Chintang, and in addition that Chintang allows free word order (on the grounds that this is the least constrained word order possibility). It also defines bare-bones classes of nouns, determiners and transitive verbs, and then populates the lexicon by using a variant of the methodology in Xia and Lewis 2007. In particular, we parse the translation line using the Charniak parser (Charniak, 1997) and then use the correspondences inherent in IGT to create a projected tree structure for the language line, following Xia and Lewis. An example of the result for Chintang is shown in Fig 3. The projected trees include part of speech tags for each word that can be aligned. For each such word tagged as noun, verb, or determiner, we create an instance in the corresponding lexical type. In this baseline grammar, all verbs are assumed to be transitive, but since all arguments can (optionally) be dropped, the grammar is expected to be able to cover intransitive sentences, even if the semantic repr</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of AAAI-1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>K Vijay-Shanker</author>
</authors>
<title>Automated Extraction of TAGs from the Penn Treebank.</title>
<date>2000</date>
<booktitle>In Proc. of the 6th International Workshop on Parsing Technologies (IWPT-2000),</booktitle>
<contexts>
<context position="6275" citStr="Chen and Vijay-Shanker, 2000" startWordPosition="974" endWordPosition="977">ing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars with respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work word-order=v-final has-dets=yes noun-det-order=det-noun ... case-marking=erg-abs erg-abs-erg-case-name=erg erg-abs-abs-case-name=abs ... verb4_valence=erg-abs verb4_stem1_orth=sams-i-ne verb4_stem1_pred=_sams-i-ne_v_re ... verb-pc3_inputs=verb-pc2 verb-pc3_lrt1_name=2nd-person-subj verb-pc3_lrt1_feat1_name=pernum verb-pc3_lrt1_feat1_value=2nd verb-pc3_lrt1_feat1_head=subj verb-pc3_lrt1_lri1_inflecting=yes</context>
</contexts>
<marker>Chen, Vijay-Shanker, 2000</marker>
<rawString>John Chen and K. Vijay-Shanker. 2000. Automated Extraction of TAGs from the Penn Treebank. In Proc. of the 6th International Workshop on Parsing Technologies (IWPT-2000), Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Unsupervised induction of stochastic context-free grammars using distributional clustering.</title>
<date>2001</date>
<booktitle>In Proc. of the 5th Conference on Computational Natural Language Learning (CoNLL-2001).</booktitle>
<contexts>
<context position="5089" citStr="Clark, 2001" startWordPosition="789" endWordPosition="790">red language subject to active descriptive research, making the evaluation of our techniques realistic. Furthermore, the descriptive research on Chintang is fairly advanced, having produced both large corpora of high-quality IGT and sophisticated linguistic descriptions, making the evaluation and error analysis possible. 2 Related Work This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some bas</context>
</contexts>
<marker>Clark, 2001</marker>
<rawString>Alexander Clark. 2001. Unsupervised induction of stochastic context-free grammars using distributional clustering. In Proc. of the 5th Conference on Computational Natural Language Learning (CoNLL-2001).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="2481" citStr="Copestake et al., 2005" startWordPosition="376" endWordPosition="379">hine learning, especially in the absence of 1http://depts.washington.edu/uwcl/aggregation/ treebanks, which are not generally available for languages that are the focus of descriptive and documentary linguistics. Instead, we take advantage of the LinGO Grammar Matrix customization system (Bender et al., 2002; Bender et al., 2010) which maps from collections of statements of linguistic properties (encoded in choices files) to HPSG (Pollard and Sag, 1994) grammar fragments which in turn can be used to parse strings into semantic representations in the format of Minimal Recursion Semantics (MRS; Copestake et al., 2005) and conversely, to generate strings from MRS representations. The choices files are a much simpler representation than the grammars derived from them and therefore a more approachable learning target. Furthermore, using the Grammar Matrix customization system to produce the grammars results in much less noise in the automatically derived grammar code than would arise in a system learning grammars directly. Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (Bickel et al., 2009). Where Lewis and Xia (2008) and Ben</context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag. 2005. Minimal recursion semantics: An introduction. Research on Language &amp; Computation, 3(4):281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Cramer</author>
<author>Yi Zhang</author>
</authors>
<title>Construction of a german hpsg grammar from a detailed treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks (GEAF</booktitle>
<pages>37--45</pages>
<location>Suntec, Singapore.</location>
<contexts>
<context position="6416" citStr="Cramer and Zhang, 2009" startWordPosition="999" endWordPosition="1002">h respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work word-order=v-final has-dets=yes noun-det-order=det-noun ... case-marking=erg-abs erg-abs-erg-case-name=erg erg-abs-abs-case-name=abs ... verb4_valence=erg-abs verb4_stem1_orth=sams-i-ne verb4_stem1_pred=_sams-i-ne_v_re ... verb-pc3_inputs=verb-pc2 verb-pc3_lrt1_name=2nd-person-subj verb-pc3_lrt1_feat1_name=pernum verb-pc3_lrt1_feat1_value=2nd verb-pc3_lrt1_feat1_head=subj verb-pc3_lrt1_lri1_inflecting=yes verb-pc3_lrt1_lri1_orth=aFigure 1: Excerpts from a choices file on endangered language documentation, as treebanks are not available for suc</context>
</contexts>
<marker>Cramer, Zhang, 2009</marker>
<rawString>Bart Cramer and Yi Zhang. 2009. Construction of a german hpsg grammar from a detailed treebank. In Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks (GEAF 2009), pages 37–45, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Drellishak</author>
</authors>
<title>Widespread But Not Universal: Improving the Typological Coverage of the Grammar Matrix.</title>
<date>2009</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Washington.</institution>
<contexts>
<context position="20886" citStr="Drellishak 2009" startWordPosition="3338" endWordPosition="3339">em, we consider as case markers only those grams (automatically) identified as marking S, A, or O. In the present study, that should only be ergative; as there is no marked case for absolutive, all other nouns were treated as absolutive (regardless of their actual case marking, since the SAO system has no way to detect other case grams). 4Our extensions involved making the system able to handle the situation where one or more of S, A and O are morphologically unmarked and therefore unreflected in the glosses. 5For the details of the analyses of case systems provided by the Grammar Matrix, see Drellishak 2009. shoe buy-PST-1sS/P-IND.PST I bought a pair of shoe . PRP VBD DT NN IN NN NP-SUBJ-PRP NP NP PP . NP-OBJ VP S NP-OBJ PP NP NN VBD jutta khet-a-ij-e S VP 47 In choices files which specify case systems, we constrain the case value for nouns by creating one noun class for every case value, and then assigning the lexical entries for nouns to those lexical classes based on the grams in the gloss of the noun.6 Similarly, we create lexical classes for each case frame identified for transitive and intransitive verbs: We look for case grams on each argument of the verb, as determined by the function ta</context>
</contexts>
<marker>Drellishak, 2009</marker>
<rawString>Scott Drellishak. 2009. Widespread But Not Universal: Improving the Typological Coverage of the Grammar Matrix. Ph.D. thesis, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Georgi</author>
<author>Fei Xia</author>
<author>William D Lewis</author>
</authors>
<title>Enhanced and portable dependency projection algorithms using interlinear glossed text.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013 (Volume 2: Short Papers),</booktitle>
<pages>306--311</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="7774" citStr="Georgi et al., 2013" startWordPosition="1170" endWordPosition="1173">urces for resourcerich languages. The canonical form of an IGT instance includes a language line, a word-to-word or morpheme-to-morpheme gloss line, and a translation line (typically in a resource-rich language). The bootstrapping process starts with word alignment of the language line and translation line with the help of the gloss line. Then the translation line is parsed and the parse tree is projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. 3 Methodology Our goal in this work is to automatically create choices files on the basis of IGT data. The choices files encode both general properties about the language we are trying to model as well as more specific information including lexical classes, lexical items within lexical classes and definitions of lexical rules. Lexical rule definitions can include both morphotactic information (ordering of affixes) as well as morphosyntactic information, though here our focus is on the former. Sample excerpts from a </context>
</contexts>
<marker>Georgi, Xia, Lewis, 2013</marker>
<rawString>Ryan Georgi, Fei Xia, and William D. Lewis. 2013. Enhanced and portable dependency projection algorithms using interlinear glossed text. In Proceedings of ACL 2013 (Volume 2: Short Papers), pages 306– 311, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Prototypedriven grammar induction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL</booktitle>
<pages>881--888</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="5165" citStr="Haghighi and Klein, 2006" startWordPosition="799" endWordPosition="802"> evaluation of our techniques realistic. Furthermore, the descriptive research on Chintang is fairly advanced, having produced both large corpora of high-quality IGT and sophisticated linguistic descriptions, making the evaluation and error analysis possible. 2 Related Work This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different</context>
</contexts>
<marker>Haghighi, Klein, 2006</marker>
<rawString>Aria Haghighi and Dan Klein. 2006. Prototypedriven grammar induction. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL 2006), pages 881–888, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Acquiring compact lexicalized grammars from a cleaner treebank.</title>
<date>2002</date>
<booktitle>In Proc. of LREC-2002,</booktitle>
<pages>1974--1981</pages>
<contexts>
<context position="6349" citStr="Hockenmaier and Steedman, 2002" startWordPosition="986" endWordPosition="990">m, but the induced grammars are very different from precision grammars with respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work word-order=v-final has-dets=yes noun-det-order=det-noun ... case-marking=erg-abs erg-abs-erg-case-name=erg erg-abs-abs-case-name=abs ... verb4_valence=erg-abs verb4_stem1_orth=sams-i-ne verb4_stem1_pred=_sams-i-ne_v_re ... verb-pc3_inputs=verb-pc2 verb-pc3_lrt1_name=2nd-person-subj verb-pc3_lrt1_feat1_name=pernum verb-pc3_lrt1_feat1_value=2nd verb-pc3_lrt1_feat1_head=subj verb-pc3_lrt1_lri1_inflecting=yes verb-pc3_lrt1_lri1_orth=aFigure 1: Excerpts from a choices file on endang</context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Acquiring compact lexicalized grammars from a cleaner treebank. In Proc. of LREC-2002, pages 1974–1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Ccgbank: A corpus of ccg derivations and dependency structures extracted from the penn treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. Ccgbank: A corpus of ccg derivations and dependency structures extracted from the penn treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>A general constituent context model for improved grammar induction.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002),</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="5114" citStr="Klein and Manning, 2002" startWordPosition="791" endWordPosition="794">subject to active descriptive research, making the evaluation of our techniques realistic. Furthermore, the descriptive research on Chintang is fairly advanced, having produced both large corpora of high-quality IGT and sophisticated linguistic descriptions, making the evaluation and error analysis possible. 2 Related Work This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM alg</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher Manning. 2002. A general constituent context model for improved grammar induction. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-2002), Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-2004),</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="5139" citStr="Klein and Manning, 2004" startWordPosition="795" endWordPosition="798">tive research, making the evaluation of our techniques realistic. Furthermore, the descriptive research on Chintang is fairly advanced, having produced both large corpora of high-quality IGT and sophisticated linguistic descriptions, making the evaluation and error analysis possible. 2 Related Work This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced g</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher Manning. 2004. Corpusbased induction of syntactic structure: models of dependency and constituency. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-2004), Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Krotov</author>
<author>Mark Hepple</author>
<author>Robert Gaizauskas</author>
<author>Yorick Wilks</author>
</authors>
<title>Compacting the Penn Treebank Grammar.</title>
<date>1998</date>
<booktitle>In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics (ACL-1998),</booktitle>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="6222" citStr="Krotov et al., 1998" startWordPosition="966" endWordPosition="969">cs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars with respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work word-order=v-final has-dets=yes noun-det-order=det-noun ... case-marking=erg-abs erg-abs-erg-case-name=erg erg-abs-abs-case-name=abs ... verb4_valence=erg-abs verb4_stem1_orth=sams-i-ne verb4_stem1_pred=_sams-i-ne_v_re ... verb-pc3_inputs=verb-pc2 verb-pc3_lrt1_name=2nd-person-subj verb-pc3_lrt1_feat1_name=pernum verb-pc3_lrt1_feat1_value=2nd verb-pc3_l</context>
</contexts>
<marker>Krotov, Hepple, Gaizauskas, Wilks, 1998</marker>
<rawString>Alexander Krotov, Mark Hepple, Robert Gaizauskas, and Yorick Wilks. 1998. Compacting the Penn Treebank Grammar. In Proc. of the 36th Annual Meeting of the Association for Computational Linguistics (ACL-1998), Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William D Lewis</author>
<author>Fei Xia</author>
</authors>
<title>Automatically identifying computationally relevant typological features.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>685--690</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="3073" citStr="Lewis and Xia (2008)" startWordPosition="471" endWordPosition="474">MRS; Copestake et al., 2005) and conversely, to generate strings from MRS representations. The choices files are a much simpler representation than the grammars derived from them and therefore a more approachable learning target. Furthermore, using the Grammar Matrix customization system to produce the grammars results in much less noise in the automatically derived grammar code than would arise in a system learning grammars directly. Here, we focus on a case study of Chintang, a Kiranti language of Nepal, described by the Chintang Language Research Project (CLRP) (Bickel et al., 2009). Where Lewis and Xia (2008) and Bender et al. (2013) apply similar methodologies to extract large scale properties for many languages, we focus on a case study of a single language, looking at both the large scale properties and the lexical details. This is important for two reasons: First, it gives us a chance to look indepth at the possible sources of difficulty in extracting the large scale properties. Second, while large-scale properties are undoubtedly important, the bulk of the information specified in a precision grammar is far more fine-grained. In this case study we apply the methodology of Bender et al. (2013)</context>
<context position="7731" citStr="Lewis and Xia, 2008" startWordPosition="1163" endWordPosition="1166">es by taking advantage of IGT data and resources for resourcerich languages. The canonical form of an IGT instance includes a language line, a word-to-word or morpheme-to-morpheme gloss line, and a translation line (typically in a resource-rich language). The bootstrapping process starts with word alignment of the language line and translation line with the help of the gloss line. Then the translation line is parsed and the parse tree is projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. 3 Methodology Our goal in this work is to automatically create choices files on the basis of IGT data. The choices files encode both general properties about the language we are trying to model as well as more specific information including lexical classes, lexical items within lexical classes and definitions of lexical rules. Lexical rule definitions can include both morphotactic information (ordering of affixes) as well as morphosyntactic information, though here our focu</context>
</contexts>
<marker>Lewis, Xia, 2008</marker>
<rawString>William D. Lewis and Fei Xia. 2008. Automatically identifying computationally relevant typological features. In Proceedings of the Third International Joint Conference on Natural Language Processing, pages 685–690, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Miyao</author>
<author>Takashi Ninomiya</author>
<author>Junichi Tsujii</author>
</authors>
<title>Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank.</title>
<date>2004</date>
<booktitle>In Proc. of the First International Joint Conference on Natural Language Processing (IJCNLP-2004),</booktitle>
<location>Hainan, China.</location>
<contexts>
<context position="6391" citStr="Miyao et al., 2004" startWordPosition="995" endWordPosition="998">ecision grammars with respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work word-order=v-final has-dets=yes noun-det-order=det-noun ... case-marking=erg-abs erg-abs-erg-case-name=erg erg-abs-abs-case-name=abs ... verb4_valence=erg-abs verb4_stem1_orth=sams-i-ne verb4_stem1_pred=_sams-i-ne_v_re ... verb-pc3_inputs=verb-pc2 verb-pc3_lrt1_name=2nd-person-subj verb-pc3_lrt1_feat1_name=pernum verb-pc3_lrt1_feat1_value=2nd verb-pc3_lrt1_feat1_head=subj verb-pc3_lrt1_lri1_inflecting=yes verb-pc3_lrt1_lri1_orth=aFigure 1: Excerpts from a choices file on endangered language documentation, as treebanks </context>
</contexts>
<marker>Miyao, Ninomiya, Tsujii, 2004</marker>
<rawString>Yusuke Miyao, Takashi Ninomiya, and Junichi Tsujii. 2004. Corpus-oriented grammar development for acquiring a head-driven phrase structure grammar from the penn treebank. In Proc. of the First International Joint Conference on Natural Language Processing (IJCNLP-2004), Hainan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar. Studies in Contemporary Linguistics.</title>
<date>1994</date>
<publisher>The University of Chicago Press and CSLI Publications,</publisher>
<location>Chicago, IL and Stanford, CA.</location>
<contexts>
<context position="2315" citStr="Pollard and Sag, 1994" startWordPosition="348" endWordPosition="351">riched through the work of descriptive linguists, specifically, collections of IGT (interlinear glossed text). The grammars themselves are not likely targets for machine learning, especially in the absence of 1http://depts.washington.edu/uwcl/aggregation/ treebanks, which are not generally available for languages that are the focus of descriptive and documentary linguistics. Instead, we take advantage of the LinGO Grammar Matrix customization system (Bender et al., 2002; Bender et al., 2010) which maps from collections of statements of linguistic properties (encoded in choices files) to HPSG (Pollard and Sag, 1994) grammar fragments which in turn can be used to parse strings into semantic representations in the format of Minimal Recursion Semantics (MRS; Copestake et al., 2005) and conversely, to generate strings from MRS representations. The choices files are a much simpler representation than the grammars derived from them and therefore a more approachable learning target. Furthermore, using the Grammar Matrix customization system to produce the grammars results in much less noise in the automatically derived grammar code than would arise in a system learning grammars directly. Here, we focus on a cas</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. Studies in Contemporary Linguistics. The University of Chicago Press and CSLI Publications, Chicago, IL and Stanford, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Robert Schikowski</author>
</authors>
<title>Balthasar Bickel, and Netra Paudyal. in press. Flexible valency in Chintang.</title>
<booktitle>Valency Classes: A Comparative Handbook. Mouton de Gruyter,</booktitle>
<editor>In B. Comrie and A. Malchukov, editors,</editor>
<location>Berlin.</location>
<marker>Schikowski, </marker>
<rawString>Robert Schikowski, Balthasar Bickel, and Netra Paudyal. in press. Flexible valency in Chintang. In B. Comrie and A. Malchukov, editors, Valency Classes: A Comparative Handbook. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Annealing structural bias in multilingual weighted grammar induction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL/COLING</booktitle>
<pages>569--576</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="5189" citStr="Smith and Eisner, 2006" startWordPosition="803" endWordPosition="806">ues realistic. Furthermore, the descriptive research on Chintang is fairly advanced, having produced both large corpora of high-quality IGT and sophisticated linguistic descriptions, making the evaluation and error analysis possible. 2 Related Work This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars</context>
</contexts>
<marker>Smith, Eisner, 2006</marker>
<rawString>Noah A. Smith and Jason Eisner. 2006. Annealing structural bias in multilingual weighted grammar induction. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (ACL/COLING 2006), pages 569– 576, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Unsupervised multilingual grammar induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>73--81</pages>
<contexts>
<context position="5210" citStr="Snyder et al., 2009" startWordPosition="807" endWordPosition="810">re, the descriptive research on Chintang is fairly advanced, having produced both large corpora of high-quality IGT and sophisticated linguistic descriptions, making the evaluation and error analysis possible. 2 Related Work This work can be understood as a task related to both grammar induction and grammar extraction, though it is distinct from both. It also connects with and extends previous work using interlinear glossed text to extract grammatical properties. Grammar induction (Clark, 2001; Klein and Manning, 2002; Klein and Manning, 2004; Haghighi and Klein, 2006; Smith and Eisner, 2006; Snyder et al., 2009, inter alios) involves the learning of grammars from unlabeled sentences. Here, unlabeled means that the sentences are often POS tagged, but no syntactic structures for the sentences are available. Most of those studies choose probabilistic context-free grammars (PCFGs) or dependency grammars as the grammar framework, and estimate the probability of the context-free rules or dependency arcs from the data. These studies improve parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars with respect to cont</context>
</contexts>
<marker>Snyder, Naseem, Barzilay, 2009</marker>
<rawString>Benjamin Snyder, Tahira Naseem, and Regina Barzilay. 2009. Unsupervised multilingual grammar induction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 73–81, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Stoll</author>
<author>Balthasar Bickel</author>
</authors>
<title>How to measure frequency? Different ways of counting ergatives in Chintang (Tibeto-Burman, Nepal) and their implications.</title>
<date>2012</date>
<booktitle>Potentials of Language Documentation: Methods, Analyses, and Utilization,</booktitle>
<pages>83--89</pages>
<editor>In Frank Seifart, Geoffrey Haig, Nikolaus P. Himmelmann, Dagmar Jung, Anna Margetts, and Paul Trilsbeek, editors,</editor>
<publisher>University of Hawai‘i Press, Manoa.</publisher>
<contexts>
<context position="10231" citStr="Stoll and Bickel, 2012" startWordPosition="1576" endWordPosition="1579">he problem interestingly difficult. Schikowski et al. (in press) describe Chintang as exhibiting information-structurally constrained word order: All permutations of the major sentential constituents are expected to be valid, with the different orders subject to different felicity conditions. They state, however, that no detailed analysis of word order has yet been carried out, and so this description should be taken as preliminary. In contrast, much detailed work has been done on the marking of arguments, both via agreement on the verb and via case marking of dependents (Bickel et al., 2010; Stoll and Bickel, 2012; Schikowski et al., in press). The case marking system can be understood as following an ergativeabsolutive pattern, but with several variations from that theme. In an ergative-absolutive pattern, the sole argument of an intransitive verb (here called S) is marked the same as the most patient-like argument of a transitive verb (here called O) and differentiated from the most agent-like argument of a transitive verb (here called A). Most A arguments are marked with an overt case marker called ergative, while S and O arguments appear without a case marker. In most writing about the language, th</context>
<context position="11864" citStr="Stoll and Bickel (2012)" startWordPosition="1850" endWordPosition="1853">rrences of transitive verbs with two absolutive arguments (and S-style agreement with the A argument) when the O argument is of an indefinite quantity (Schikowski et al., in press). Furthermore, the language allows dropping of arguments (A, S, and O). Finally, there are of course valences beyond simple intransitive and transitive, as well as case frames even for two-argument verbs other than { ERG, ABS }. As a result of the combination of these facts, the actual occurrence of ergative-casemarked arguments in speech is relatively low: Examining a corpus of speech spoken to and around children, Stoll and Bickel (2012) find that only 11% of (semantically) transitive verb tokens have an overt, ergative-marked NP A argument. As discussed below, these properties make it difficult for automated methods to detect both the overall case system of the language and accurate information regarding the case frames of individual verbs. The dataset we are using contains 9793 (8863 train, 930 test) IGT instances which come from the corpus of narratives and other speech collected, transcribed, translated and glossed by the CLRP.3 An example is shown in Fig. 2. As can be seen in Fig. 2, the glossing in this dataset is extre</context>
</contexts>
<marker>Stoll, Bickel, 2012</marker>
<rawString>Sabine Stoll and Balthasar Bickel. 2012. How to measure frequency? Different ways of counting ergatives in Chintang (Tibeto-Burman, Nepal) and their implications. In Frank Seifart, Geoffrey Haig, Nikolaus P. Himmelmann, Dagmar Jung, Anna Margetts, and Paul Trilsbeek, editors, Potentials of Language Documentation: Methods, Analyses, and Utilization, pages 83–89. University of Hawai‘i Press, Manoa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Wax</author>
</authors>
<title>Automated grammar engineering for verbal morphology. Master’s thesis,</title>
<date>2014</date>
<institution>University of Washington.</institution>
<contexts>
<context position="3854" citStr="Wax (2014)" startWordPosition="606" endWordPosition="607">the large scale properties and the lexical details. This is important for two reasons: First, it gives us a chance to look indepth at the possible sources of difficulty in extracting the large scale properties. Second, while large-scale properties are undoubtedly important, the bulk of the information specified in a precision grammar is far more fine-grained. In this case study we apply the methodology of Bender et al. (2013) to extract general word order and case properties and examine the sources of error affecting those results. We also explore extensions of those methodologies and that of Wax (2014) to extract lexical entries and specifications for morpho43 Proceedings of the 2014 Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 43–53, Baltimore, Maryland, USA, 26 June 2014. c�2014 Association for Computational Linguistics logical rules. Together with a few default specifications, this information is enough to allow us to define grammars through the Grammar Matrix customization system and thus evaluate the results in terms of parsing coverage, accuracy and ambiguity over running text. Chintang is particularly well-suited for this case study because</context>
<context position="22842" citStr="Wax 2014" startWordPosition="3684" endWordPosition="3685">nd is constrained to select for a subject of the observed case (or the default case, here absolutive, if the overt subject bears no case marker). When there is an overt object but no subject, the verb is assumed to be transitive and the object’s case assigned as with other transitives but the subject’s case is constrained to the default (i.e. ergative, in this instance). Verbs with no overt arguments are not matched. 3.2.6 MOM choices file: Automatically extracted lemmas and lexical rules The final refinement we try on our baseline is to apply the ‘Matrix-ODIN Morphology’ (MOM) methodology of Wax 2014. This methodology attempts to automatically identify affixes and create appropriate descriptions of lexical rules in a choices file to model those affixes. As a result, it also identifies stems. Thus we use the same basic choices as in the baseline choices file, but now populate the lexicon with stems rather than fullforms. Compared to BASELINE, this one should result in a grammar with better lexical coverage on held-out data, to the extent that the MOM system 6In future work, we plan to extend the MOM approach (§3.2.6) from verbs to nouns, but for now, the nouns are treated as full-form lexi</context>
</contexts>
<marker>Wax, 2014</marker>
<rawString>David Wax. 2014. Automated grammar engineering for verbal morphology. Master’s thesis, University of Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>William D Lewis</author>
</authors>
<title>Multilingual structural projection across interlinear text.</title>
<date>2007</date>
<booktitle>In Proc. of the Conference on Human Language Technologies (HLT/NAACL</booktitle>
<pages>452--459</pages>
<location>Rochester, New York.</location>
<contexts>
<context position="7626" citStr="Xia and Lewis, 2007" startWordPosition="1145" endWordPosition="1148">le for such languages. A third line of research attempts to bootstrap NLP tools for resource-poor languages by taking advantage of IGT data and resources for resourcerich languages. The canonical form of an IGT instance includes a language line, a word-to-word or morpheme-to-morpheme gloss line, and a translation line (typically in a resource-rich language). The bootstrapping process starts with word alignment of the language line and translation line with the help of the gloss line. Then the translation line is parsed and the parse tree is projected to the language line using the alignments (Xia and Lewis, 2007). The projected trees can be used to answer linguistic questions such as word order (Lewis and Xia, 2008) or bootstrap parsers (Georgi et al., 2013). Our work extends this methodology to the construction of precision grammars. 3 Methodology Our goal in this work is to automatically create choices files on the basis of IGT data. The choices files encode both general properties about the language we are trying to model as well as more specific information including lexical classes, lexical items within lexical classes and definitions of lexical rules. Lexical rule definitions can include both mo</context>
<context position="16727" citStr="Xia and Lewis 2007" startWordPosition="2641" endWordPosition="2644">rom our upper-bound. 3.2.2 Baseline choices file Our baseline choices file is designed to create a working grammar, without particular high-level information about Chintang, that focuses on coverage at the expense of precision. We handspecified the (counter-factual) assertion that there is no case marking in Chintang, and in addition that Chintang allows free word order (on the grounds that this is the least constrained word order possibility). It also defines bare-bones classes of nouns, determiners and transitive verbs, and then populates the lexicon by using a variant of the methodology in Xia and Lewis 2007. In particular, we parse the translation line using the Charniak parser (Charniak, 1997) and then use the correspondences inherent in IGT to create a projected tree structure for the language line, following Xia and Lewis. An example of the result for Chintang is shown in Fig 3. The projected trees include part of speech tags for each word that can be aligned. For each such word tagged as noun, verb, or determiner, we create an instance in the corresponding lexical type. In this baseline grammar, all verbs are assumed to be transitive, but since all arguments can (optionally) be dropped, the </context>
<context position="19153" citStr="Xia and Lewis (2007)" startWordPosition="3032" endWordPosition="3035">onstituent order. For our dataset, the algorithm chose ‘v-final’, which matches what is in the ORACLE choices file, but is not necessarily correct. We created two versions of each of the other choices files, one with the default (baseline) answer of ‘free word order’ and one with this automatically supplied answer. 3.2.4 Case system Similarly, we applied extended versions of the two methods for automatically discovering case systems from Bender et al. 2013: GRAM which looks for known case grams in glosses (not using projected trees) and SAO which extends the structureprojection methodology of Xia and Lewis (2007) to detect S, A and O arguments and then looks for the most frequent gram associated with each of these.4 The GRAM method determines the case system of Chintang to be ergative-absolutive, while the SAO method indicates ‘none’ (no case). Specifying a case system in a choices file has no effect on the coverage or precision of the resulting grammar if the lexical items don’t constrain case. Thus the case system choices only make sense in combination with the case frames choices (§3.2.5). 3.2.5 Case frames and case values The HPSG analysis of case involves a feature CASE which is constrained by bo</context>
</contexts>
<marker>Xia, Lewis, 2007</marker>
<rawString>Fei Xia and William D. Lewis. 2007. Multilingual structural projection across interlinear text. In Proc. of the Conference on Human Language Technologies (HLT/NAACL 2007), pages 452–459, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
</authors>
<title>Extracting Tree Adjoining Grammars from Bracketed Corpora.</title>
<date>1999</date>
<booktitle>In Proc. of 5th Natural Language Processing Pacific Rim Symposium (NLPRS-1999),</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="6244" citStr="Xia, 1999" startWordPosition="972" endWordPosition="973">mprove parsing performance significantly over some baselines such as the EM algorithm, but the induced grammars are very different from precision grammars with respect to content, quality, and grammar framework. Grammar extraction, on the other hand, learns grammars (sets of rules) from treebanks. Here the idea is to use heuristics to convert the syntactic structures in a treebank into derivation trees conforming to a particular framework, and then extract grammars from those trees. This has been done in a wide range of grammar frameworks, including PCFG (e.g. Krotov et al., 1998), LTAG (e.g. Xia, 1999; Chen and Vijay-Shanker, 2000), LFG (e.g. Cahill et al., 2004), CCG (e.g. Hockenmaier and Steedman, 2002, 2007), and HPSG (e.g. Miyao et al., 2004; Cramer and Zhang, 2009). However, this approach is not applicable to work word-order=v-final has-dets=yes noun-det-order=det-noun ... case-marking=erg-abs erg-abs-erg-case-name=erg erg-abs-abs-case-name=abs ... verb4_valence=erg-abs verb4_stem1_orth=sams-i-ne verb4_stem1_pred=_sams-i-ne_v_re ... verb-pc3_inputs=verb-pc2 verb-pc3_lrt1_name=2nd-person-subj verb-pc3_lrt1_feat1_name=pernum verb-pc3_lrt1_feat1_value=2nd verb-pc3_lrt1_feat1_head=subj ve</context>
</contexts>
<marker>Xia, 1999</marker>
<rawString>Fei Xia. 1999. Extracting Tree Adjoining Grammars from Bracketed Corpora. In Proc. of 5th Natural Language Processing Pacific Rim Symposium (NLPRS-1999), Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
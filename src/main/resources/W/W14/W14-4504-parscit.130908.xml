<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.251975">
<title confidence="0.994007">
Proposition Knowledge Graphs
</title>
<author confidence="0.995051">
Gabriel Stanovsky Omer Levy Ido Dagan
</author>
<affiliation confidence="0.99264">
Computer Science Department, Bar-Ilan University
</affiliation>
<email confidence="0.9367825">
{gabriel.satanovsky, omerlevy}@gmail.com
dagan@cs.biu.ac.il
</email>
<sectionHeader confidence="0.992023" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997161909090909">
Open Information Extraction (Open IE) is a promising approach for unrestricted Information
Discovery (ID). While Open IE is a highly scalable approach, allowing unsupervised relation
extraction from open domains, it currently has some limitations. First, it lacks the expressiveness
needed to properly represent and extract complex assertions that are abundant in text. Second, it
does not consolidate the extracted propositions, which causes simple queries above Open IE as-
sertions to return insufficient or redundant information. To address these limitations, we propose
in this position paper a novel representation for ID – Propositional Knowledge Graphs (PKG).
PKGs extend the Open IE paradigm by representing semantic inter-proposition relations in a
traversable graph. We outline an approach for constructing PKGs from single and multiple texts,
and highlight a variety of high-level applications that may leverage PKGs as their underlying
information discovery and representation framework.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919791666666">
Information discovery from text (ID) aims to provide a consolidated and explorable data representation of
an input document or a collection of documents addressing a common topic. Ideally, this representation
would separate the input into logically discrete units, omit redundancies in the original text, and provide
semantic relations between the basic units of the representation. This representation can then be used
by human readers as a convenient and succinct format, or by subsequent NLP tasks (such as question
answering and multidocument summarization) as a structured input representation.
A common approach to ID is to extract propositions conveyed in the text by applying either supervised
Information Extraction (IE) techniques (Cowie and Lehnert, 1996), to recover propositions covering a
predefined set of relations (Auer et al., 2007; Suchanek et al., 2008), or more recently, Open Information
Extraction (Open IE) (Etzioni et al., 2008), which discovers open-domain relations (Zhu et al., 2009;
Wu et al., 2008). In Open IE, natural language propositions are extracted from text, based on surface
or syntactic patterns, and are then represented as predicate-argument tuples, where each element is a
natural language string. While Open IE presents a promising direction for ID, thanks to its robustness
and scalability across domains, we argue that it currently lacks representation power in two major aspects:
representing complex propositions extracted from discourse, such as interdependent propositions or
implicitly conveyed propositions, and consolidating propositions extracted across multiple sources,
which leads to either insufficient or redundant information when exploring a set of Open IE extractions.
In this position paper we outline Propositional Knowledge Graphs (PKG), a representation which
addresses both of Open IE’s mentioned drawbacks. The graph’s nodes are discrete propositions extracted
from text, and edges are drawn where semantic relations between propositions exists. Such relations can
be inferred from a single discourse, or from multiple text fragments along with background knowledge –
by applying methods such as textual entailment recognition (Dagan et al., 2013) – which consolidates the
information within the graph. We discuss this representation as a useful input for semantic applications,
and describe work we have been doing towards implementing such a framework.
</bodyText>
<footnote confidence="0.505979">
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</footnote>
<page confidence="0.988395">
19
</page>
<note confidence="0.8499925">
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 19–24,
Dublin, Ireland, August 23 2014.
</note>
<figureCaption confidence="0.99837225">
Figure 1: An excerpt from a PKG, containing a few propositions extracted from news reports about Curiosity (the Mars rover)
and their relations. The dashed boundaries in the figure denote paraphrase cliques, meaning that all propositions within them
are mutually entailing. Some of these propositions are complex, and the bottom-right corner illustrates how one of them can be
represented by inter-connected sub-propositions.
</figureCaption>
<sectionHeader confidence="0.901499" genericHeader="method">
2 Approach: Discover Inter-Proposition Relations
</sectionHeader>
<bodyText confidence="0.999986714285714">
We propose a novel approach for textual information discovery and representation that enhances the
expressiveness of Open IE with structural power similar to traditional knowledge graphs. Our represen-
tation aims to extract all the information conveyed by text to a traversable graph format – a Propositional
Knowledge Graph (PKG). The graph’s nodes are natural language propositions and its labeled edges are
semantic relations between these propositions. Figure 1 illustrates an excerpt of a PKG.
We separate the construction of such graphs into two phases, each of which addresses one of the afore-
mentioned limitations of current Open IE. The first phase (described in section 2.1) is the extraction of
complex propositions from a single discourse. This phase extends upon the definition of Open IE ex-
tractions to gain a more expressive paradigm and improve the recall of extracted propositions. In this
extension, a single assertion is represented by a set of interconnected propositions. An example can be
seen in the bottom right of Figure 1. The second phase (described in section 2.2) deals with the consolida-
tion of propositions extracted in the first phase. This is done by drawing relations such as entailment and
temporal succession between these propositions, which can be inferred utilizing background knowledge
applied on multiple text fragments.
</bodyText>
<subsectionHeader confidence="0.981893">
2.1 Relations Implied by Discourse
</subsectionHeader>
<bodyText confidence="0.999875727272727">
Current Open IE representation schemes lack the expressibility to represent certain quite common propo-
sitions implied by syntax, hindering Open IE’s potential as an information discovery framework. We dis-
cuss several cases in which this limitation is evident, and describe possible solutions within our proposed
framework.
Embedded and Interrelated Propositions Common Open IE systems retrieve only propositions in
which both predicates and arguments are instantiated in succession in the surface form. For such propo-
sitions, these systems produce independent tuples (typically a (subject, verb, object) triplet) consisting of
a predicate and a list of its arguments, all expressed in natural language, in the same way they originally
appeared in the sentence. This methodology lacks the ability to represent cases in which propositions are
inherently embedded, such as conditionals and propositional arguments (e.g. “Senator Kennedy asked
congress to pass the bill”). Mausam et al. (2012) introduced a context analysis layer, extending this
</bodyText>
<page confidence="0.967668">
20
</page>
<bodyText confidence="0.998880344827586">
representation with an additional field per tuple, which intends to represent the factuality of the extrac-
tion, accounting specifically for cases of conditionals and attribution. For instance, the assertion “If he
wins five key states, Romney will be elected President” will be represented as ((Romney; will be elected;
President) ClausalModifier if; he wins five key states).
While these methods capture some of the propositions conveyed by text, they fail to retrieve other
propositions expressed by more sophisticated syntactic constructs. Consider the sentence from Figure 1
“Curiosity will look for evidence that Mars might have had conditions for supporting life”. It exhibits a
construction which the independent tuples format seems to fall short from representing. Our proposed
representation for this sentence is depicted in the bottom right of Figure 1. We represent the complexity
of the sentence through a nested structure of interlinked propositions, each composed of a single pred-
icate and its syntactic arguments and modifiers. In addition, we model certain syntactic variabilities as
features, such as tense, negation, passive voice, etc. Thus, a single assertion is represented through the
discrete propositions it conveys, along with their inter-relations. In addition to the expressibility that this
representation offers, an immediate gain is the often recurring case in which a part of a proposition (for
example, one of the arguments) immediately implies another proposition. For instance, “The Mars rover
Curiosity is a mobile science lab” implies that “Curiosity is a rover”, and does so syntactically.
Implicit propositions Certain propositions which are conveyed by the text are not explicitly expressed
in the surface form. Consider, for instance, the sentence “Facebook’s acquisition of WhatsApp occurred
yesterday”. It introduces the proposition (Facebook, acquired, WhatsApp) through nominalization. Cur-
rent Open IE formalisms are unable to extract such triplets, since the necessary predicate (namely “ac-
quired”) does not appear in the surface form. Implicit propositions might be introduced in many other
linguistic constructs, such as: appositions (“The company, Random House, doesn’t report its earnings.”
implies that Random House is a company), adjectives (“Tall John walked home” implies that John is tall),
and possessives (“John’s book is on the table” implies that John has a book). We intend to syntactically
identify these implicit propositions, and make them explicit in our representation.
For further analysis of syntax-driven proposition representation, see our recent work (Stanovsky et al.,
2014). We believe that this extension of Open IE representation is feasibly extractable from syntactic
parse trees, and are currently working on automatic conversion from Stanford dependencies (de Marneffe
and Manning, 2008) to interconnected propositions as described.
</bodyText>
<subsectionHeader confidence="0.996945">
2.2 Consolidating Information across Propositions
</subsectionHeader>
<bodyText confidence="0.99986695">
While Open IE is indeed much more scalable than supervised approaches, it does not consolidate natu-
ral language expressions, which leads to either insufficient or redundant information when accessing a
repository of Open IE extractions. As an illustrating example, querying the University of Washington’s
Open IE demo (openie.cs.washington.edu) for the generally equivalent relieves headache or
treats headache returns two different lists of entities; out of the top few results, the only answers these
queries seem to agree on are caffeine and sex. Desirably, an information discovery platform should re-
turn identical results (or at least very similar ones) to these queries. This is a major drawback relative
to supervised knowledge representations, such as Freebase (Bollacker et al., 2008), which map natural
language expressions to canonical formal representations (e.g. the treatments relation in Freebase).
While much relational information can be salvaged from the original text, many inter-propositional
relations stem from background knowledge and our understanding of language. Perhaps the most promi-
nent of these is the entailment relation, as demonstrated in Figure 1. We rely on the definition of textual
entailment as defined by Dagan et al. (2013): proposition T entails proposition H if humans reading T
would typically infer that H is most likely true. Entailment provides an effective structure for aggregat-
ing natural-language based information; it merges semantically equivalent propositions into cliques, and
induces specification-generalization edges between them (if T entails H, then H is more general).
Figure 1 demonstrates the usefulness of entailment in organizing the propositions within a PKG. For
example, the two statements describing Curiosity as a mobile science lab (middle right) originated from
two different texts. However, in a PKG, they are marked as paraphrases (mutually entailing), and both
entail an additional proposition from a third source: “Curiosity is a lab”. If one were to query all the
</bodyText>
<page confidence="0.998062">
21
</page>
<bodyText confidence="0.99998">
propositions that entail “Curiosity is a lab” – e.g. in response to the query “What is Curiosity?” – all
three propositions would be retrieved, even though their surface forms may have “functions as” instead
of “is” or “laboratory” instead of “lab”.
We have recently taken some first steps in this direction, investigating algorithms for constructing
entailment edges over sets of related propositions (Levy et al., 2014). Even between simple propositions,
recognizing entailment is challenging. We are currently working on new methods that will leverage
structured and unstructured data to recognize entailment for Open IE propositions. There are additional
relations, besides entailment, that should desirably be represented in PKGs as well. Two such examples
are temporal relations (depicted in Figure 1) and causality. Investigating and adapting methods for
recognizing and utilizing these relations is intended for future work.
</bodyText>
<sectionHeader confidence="0.996829" genericHeader="method">
3 Applications
</sectionHeader>
<bodyText confidence="0.999901958333333">
An appealing application of knowledge graphs is question answering (QA). In this section we demon-
strate how our representation may facilitate more sophisticated information access scenarios.
Structured Queries Queries over structured data give the user the power to receive targeted answers
for her queries. Consider for example the query “electric cars on sale in Canada”. PKGs can give the
power of queries over structured data to the domain of unstructured information. To answer our query,
we can search the PKG for all of the propositions that entail these two propositions: (1) “X is an electric
car”, (2) “X is on sale in Canada”, where X is a variable. The list of X instantiations is the answer
to our structured query. Our knowledge structure enables even more sophisticated queries that involve
more than one variable. For example, “Japanese corporations that bought Australian start-ups” retrieves
a collection of pairs (X, Y ) where X is the Japanese corporation that bought Y , an Australian start-up.
Summarization Multi-document summarization gives the user the ability to compactly assimilate in-
formation from multiple documents on the same topic. PKGs can be a natural platform leveraged by
summarization because: (1) they would contain the information from those documents as fine-grained
propositions (2) they represent the semantic relations between those propositions. These semantic re-
lations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual
entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order
in which each proposition is presented. A recent method of summarizing text with entailment graphs
(Gupta et al., 2014) demonstrates the appeal and feasibility of this application.
Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012)
demonstrate this concept on a limited proposition graph. When searching for “headache” in their demo,
the user can drill-down to find possible causes or remedies, and even focus on subcategories of those;
for example, finding the foods which relieve headaches. As opposed to the structured query application,
retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new
information they might not have considered a-priori.
</bodyText>
<sectionHeader confidence="0.998706" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999772181818182">
In this position paper we outlined a framework for information discovery that leverages and extends Open
IE, while addressing two of its current major drawbacks. The proposed framework enriches Open IE by
representing natural language in a traversable graph, composed of propositions and their semantic inter-
relations – A Propositional Knowledge Graph (PKG). The resulting structure provides a representation
in two levels: locally, at sentence level, by representing the syntactic proposition structure embedded in a
single sentence, and globally, at inter-proposition level, where relations are drawn between propositions
from discourse, or from various sources.
At the sentence level, PKG can be compared to Abstract Meaning Representation (AMR) (Banarescu
et al., 2013), which maps a sentence onto a hierarchical structure of propositions (predicate-argument
relations) - a “meaning representation”. AMR uses Propbank (Kingsbury and Palmer, 2003) for pred-
icates’ meaning representation, where possible, and ungrounded natural language, where no respective
</bodyText>
<page confidence="0.986681">
22
</page>
<bodyText confidence="0.999923">
Propbank lexicon entry exists. While AMR relies on a deep semantic interpretation, our sentence level
representation is more conservative (and thus, hopefully, more feasible) and can be obtained by syntactic
interpretation.
At inter-proposition level, PKG can be compared with traditional Knowledge Graphs (such as Freebase
and Google’s Knowledge Graph). These Knowledge Graphs, in contrast with PKGs, require manual
intervention and aim to cover a rich set of relations using formal language and a pre-specified schema,
thus many relations are inevitably left out (e.g. the relation cracked, as in (Alan Turing, cracked, the
Enigma) does not exist in Freebase).
We believe that PKGs are a promising extension of Open IE’s unsupervised traits, for combining as-
pects of information representation - on a local scale, providing a rich schema for representing sentences,
and on a global scale providing an automated and consolidated method for structuring knowledge.
</bodyText>
<sectionHeader confidence="0.998509" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999421">
Meni Adler, Jonathan Berant, and Ido Dagan. 2012. Entailment-based text exploration with application to the
health-care domain. In Proceedings of the System Demonstrations of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL 2012), pages 79–84.
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpe-
dia: A nucleus for a web of open data. In The semantic web, pages 722–735. Springer.
Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp
Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively
created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD interna-
tional conference on Management of data, pages 1247–1250. ACM.
Jim Cowie and Wendy Lehnert. 1996. Information extraction. Communications of the ACM, 39(1):80–91.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment:
Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1–220.
Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representa-
tion. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,
pages 1–8, Manchester, UK, August. Coling 2008 Organizing Committee.
Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from
the Web. Communications of the ACM, 51(12):68–74.
Anand Gupta, Manpreet Kathuria, Shachar Mirkin, Adarsh Singh, and Aseem Goyal. 2014. Text summarization
through entailment-based minimum vertex cover. In *SEM.
Paul Kingsbury and Martha Palmer. 2003. Propbank: the next level of treebank. In Proceedings of Treebanks and
lexical Theories, volume 3.
Omer Levy, Ido Dagan, and Jacob Goldberger. 2014. Focused entailment graphs for open ie propositions. In
Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland,
USA, June. Association for Computational Linguistics.
Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning
for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning, pages 523–534, Jeju Island, Korea, July.
Association for Computational Linguistics.
Gabriel Stanovsky, Jessica Ficler, Ido Dagan, and Yoav Goldberg. 2014. Intermediary semantic representation
through proposition structures. In Workshop on Semantic Parsing, Baltimore, Maryland, USA, June. Associa-
tion for Computational Linguistics.
Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from wikipedia and
wordnet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203–217.
</reference>
<page confidence="0.971378">
23
</page>
<reference confidence="0.992820833333333">
Fei Wu, Raphael Hoffmann, and Daniel S Weld. 2008. Information extraction from wikipedia: Moving down the
long tail. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data
mining, pages 731–739. ACM.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. Statsnowball: a statistical approach to
extracting entity relationships. In Proceedings of the 18th international conference on World wide web, pages
101–110. ACM.
</reference>
<page confidence="0.999179">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979512">
<title confidence="0.999962">Proposition Knowledge Graphs</title>
<author confidence="0.999643">Gabriel Stanovsky Omer Levy Ido Dagan</author>
<affiliation confidence="0.999913">Computer Science Department, Bar-Ilan University</affiliation>
<email confidence="0.992119">dagan@cs.biu.ac.il</email>
<abstract confidence="0.998930583333333">Open Information Extraction (Open IE) is a promising approach for unrestricted Information Discovery (ID). While Open IE is a highly scalable approach, allowing unsupervised relation extraction from open domains, it currently has some limitations. First, it lacks the expressiveness needed to properly represent and extract complex assertions that are abundant in text. Second, it does not consolidate the extracted propositions, which causes simple queries above Open IE assertions to return insufficient or redundant information. To address these limitations, we propose in this position paper a novel representation for ID – Propositional Knowledge Graphs (PKG). PKGs extend the Open IE paradigm by representing semantic inter-proposition relations in a traversable graph. We outline an approach for constructing PKGs from single and multiple texts, and highlight a variety of high-level applications that may leverage PKGs as their underlying information discovery and representation framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Meni Adler</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
</authors>
<title>Entailment-based text exploration with application to the health-care domain.</title>
<date>2012</date>
<booktitle>In Proceedings of the System Demonstrations of the 50th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>79--84</pages>
<contexts>
<context position="14707" citStr="Adler et al. (2012)" startWordPosition="2147" endWordPosition="2150">rom those documents as fine-grained propositions (2) they represent the semantic relations between those propositions. These semantic relations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order in which each proposition is presented. A recent method of summarizing text with entailment graphs (Gupta et al., 2014) demonstrates the appeal and feasibility of this application. Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012) demonstrate this concept on a limited proposition graph. When searching for “headache” in their demo, the user can drill-down to find possible causes or remedies, and even focus on subcategories of those; for example, finding the foods which relieve headaches. As opposed to the structured query application, retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new information they might not have considered a-priori. 4 Discussion In this position paper we outlined a framework for information discovery that leverages and extends Open IE, while ad</context>
</contexts>
<marker>Adler, Berant, Dagan, 2012</marker>
<rawString>Meni Adler, Jonathan Berant, and Ido Dagan. 2012. Entailment-based text exploration with application to the health-care domain. In Proceedings of the System Demonstrations of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), pages 79–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ives</author>
</authors>
<title>Dbpedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In The semantic web,</booktitle>
<pages>722--735</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2048" citStr="Auer et al., 2007" startWordPosition="282" endWordPosition="285">t into logically discrete units, omit redundancies in the original text, and provide semantic relations between the basic units of the representation. This representation can then be used by human readers as a convenient and succinct format, or by subsequent NLP tasks (such as question answering and multidocument summarization) as a structured input representation. A common approach to ID is to extract propositions conveyed in the text by applying either supervised Information Extraction (IE) techniques (Cowie and Lehnert, 1996), to recover propositions covering a predefined set of relations (Auer et al., 2007; Suchanek et al., 2008), or more recently, Open Information Extraction (Open IE) (Etzioni et al., 2008), which discovers open-domain relations (Zhu et al., 2009; Wu et al., 2008). In Open IE, natural language propositions are extracted from text, based on surface or syntactic patterns, and are then represented as predicate-argument tuples, where each element is a natural language string. While Open IE presents a promising direction for ID, thanks to its robustness and scalability across domains, we argue that it currently lacks representation power in two major aspects: representing complex p</context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ives, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In The semantic web, pages 722–735. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Banarescu</author>
<author>Claire Bonial</author>
<author>Shu Cai</author>
<author>Madalina Georgescu</author>
<author>Kira Griffitt</author>
<author>Ulf Hermjakob</author>
<author>Kevin Knight</author>
<author>Philipp Koehn</author>
<author>Martha Palmer</author>
<author>Nathan Schneider</author>
</authors>
<title>Abstract meaning representation for sembanking.</title>
<date>2013</date>
<contexts>
<context position="15963" citStr="Banarescu et al., 2013" startWordPosition="2331" endWordPosition="2334"> drawbacks. The proposed framework enriches Open IE by representing natural language in a traversable graph, composed of propositions and their semantic interrelations – A Propositional Knowledge Graph (PKG). The resulting structure provides a representation in two levels: locally, at sentence level, by representing the syntactic proposition structure embedded in a single sentence, and globally, at inter-proposition level, where relations are drawn between propositions from discourse, or from various sources. At the sentence level, PKG can be compared to Abstract Meaning Representation (AMR) (Banarescu et al., 2013), which maps a sentence onto a hierarchical structure of propositions (predicate-argument relations) - a “meaning representation”. AMR uses Propbank (Kingsbury and Palmer, 2003) for predicates’ meaning representation, where possible, and ungrounded natural language, where no respective 22 Propbank lexicon entry exists. While AMR relies on a deep semantic interpretation, our sentence level representation is more conservative (and thus, hopefully, more feasible) and can be obtained by syntactic interpretation. At inter-proposition level, PKG can be compared with traditional Knowledge Graphs (suc</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract meaning representation for sembanking.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="10618" citStr="Bollacker et al., 2008" startWordPosition="1533" endWordPosition="1536">nt information when accessing a repository of Open IE extractions. As an illustrating example, querying the University of Washington’s Open IE demo (openie.cs.washington.edu) for the generally equivalent relieves headache or treats headache returns two different lists of entities; out of the top few results, the only answers these queries seem to agree on are caffeine and sex. Desirably, an information discovery platform should return identical results (or at least very similar ones) to these queries. This is a major drawback relative to supervised knowledge representations, such as Freebase (Bollacker et al., 2008), which map natural language expressions to canonical formal representations (e.g. the treatments relation in Freebase). While much relational information can be salvaged from the original text, many inter-propositional relations stem from background knowledge and our understanding of language. Perhaps the most prominent of these is the entailment relation, as demonstrated in Figure 1. We rely on the definition of textual entailment as defined by Dagan et al. (2013): proposition T entails proposition H if humans reading T would typically infer that H is most likely true. Entailment provides an</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Cowie</author>
<author>Wendy Lehnert</author>
</authors>
<title>Information extraction.</title>
<date>1996</date>
<journal>Communications of the ACM,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="1965" citStr="Cowie and Lehnert, 1996" startWordPosition="269" endWordPosition="272"> documents addressing a common topic. Ideally, this representation would separate the input into logically discrete units, omit redundancies in the original text, and provide semantic relations between the basic units of the representation. This representation can then be used by human readers as a convenient and succinct format, or by subsequent NLP tasks (such as question answering and multidocument summarization) as a structured input representation. A common approach to ID is to extract propositions conveyed in the text by applying either supervised Information Extraction (IE) techniques (Cowie and Lehnert, 1996), to recover propositions covering a predefined set of relations (Auer et al., 2007; Suchanek et al., 2008), or more recently, Open Information Extraction (Open IE) (Etzioni et al., 2008), which discovers open-domain relations (Zhu et al., 2009; Wu et al., 2008). In Open IE, natural language propositions are extracted from text, based on surface or syntactic patterns, and are then represented as predicate-argument tuples, where each element is a natural language string. While Open IE presents a promising direction for ID, thanks to its robustness and scalability across domains, we argue that i</context>
</contexts>
<marker>Cowie, Lehnert, 1996</marker>
<rawString>Jim Cowie and Wendy Lehnert. 1996. Information extraction. Communications of the ACM, 39(1):80–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Dan Roth</author>
<author>Mark Sammons</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies,</title>
<date>2013</date>
<pages>6--4</pages>
<contexts>
<context position="3414" citStr="Dagan et al., 2013" startWordPosition="477" endWordPosition="480">cted across multiple sources, which leads to either insufficient or redundant information when exploring a set of Open IE extractions. In this position paper we outline Propositional Knowledge Graphs (PKG), a representation which addresses both of Open IE’s mentioned drawbacks. The graph’s nodes are discrete propositions extracted from text, and edges are drawn where semantic relations between propositions exists. Such relations can be inferred from a single discourse, or from multiple text fragments along with background knowledge – by applying methods such as textual entailment recognition (Dagan et al., 2013) – which consolidates the information within the graph. We discuss this representation as a useful input for semantic applications, and describe work we have been doing towards implementing such a framework. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 19 Proceedings of the AHA! Workshop on Information Discovery in Text, pages 19–24, Dublin, Ireland, August 23 2014. Figure 1: An excerpt from a PKG, containing a few propositions e</context>
<context position="11088" citStr="Dagan et al. (2013)" startWordPosition="1602" endWordPosition="1605"> similar ones) to these queries. This is a major drawback relative to supervised knowledge representations, such as Freebase (Bollacker et al., 2008), which map natural language expressions to canonical formal representations (e.g. the treatments relation in Freebase). While much relational information can be salvaged from the original text, many inter-propositional relations stem from background knowledge and our understanding of language. Perhaps the most prominent of these is the entailment relation, as demonstrated in Figure 1. We rely on the definition of textual entailment as defined by Dagan et al. (2013): proposition T entails proposition H if humans reading T would typically infer that H is most likely true. Entailment provides an effective structure for aggregating natural-language based information; it merges semantically equivalent propositions into cliques, and induces specification-generalization edges between them (if T entails H, then H is more general). Figure 1 demonstrates the usefulness of entailment in organizing the propositions within a PKG. For example, the two statements describing Curiosity as a mobile science lab (middle right) originated from two different texts. However, </context>
</contexts>
<marker>Dagan, Roth, Sammons, Zanzotto, 2013</marker>
<rawString>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<location>Manchester, UK,</location>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michele Banko</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<title>Open information extraction from the Web.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<issue>12</issue>
<contexts>
<context position="2152" citStr="Etzioni et al., 2008" startWordPosition="298" endWordPosition="301">ons between the basic units of the representation. This representation can then be used by human readers as a convenient and succinct format, or by subsequent NLP tasks (such as question answering and multidocument summarization) as a structured input representation. A common approach to ID is to extract propositions conveyed in the text by applying either supervised Information Extraction (IE) techniques (Cowie and Lehnert, 1996), to recover propositions covering a predefined set of relations (Auer et al., 2007; Suchanek et al., 2008), or more recently, Open Information Extraction (Open IE) (Etzioni et al., 2008), which discovers open-domain relations (Zhu et al., 2009; Wu et al., 2008). In Open IE, natural language propositions are extracted from text, based on surface or syntactic patterns, and are then represented as predicate-argument tuples, where each element is a natural language string. While Open IE presents a promising direction for ID, thanks to its robustness and scalability across domains, we argue that it currently lacks representation power in two major aspects: representing complex propositions extracted from discourse, such as interdependent propositions or implicitly conveyed proposi</context>
</contexts>
<marker>Etzioni, Banko, Soderland, Weld, 2008</marker>
<rawString>Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from the Web. Communications of the ACM, 51(12):68–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Gupta</author>
<author>Manpreet Kathuria</author>
<author>Shachar Mirkin</author>
<author>Adarsh Singh</author>
<author>Aseem Goyal</author>
</authors>
<title>Text summarization through entailment-based minimum vertex cover.</title>
<date>2014</date>
<booktitle>In *SEM.</booktitle>
<contexts>
<context position="14549" citStr="Gupta et al., 2014" startWordPosition="2123" endWordPosition="2126">rmation from multiple documents on the same topic. PKGs can be a natural platform leveraged by summarization because: (1) they would contain the information from those documents as fine-grained propositions (2) they represent the semantic relations between those propositions. These semantic relations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order in which each proposition is presented. A recent method of summarizing text with entailment graphs (Gupta et al., 2014) demonstrates the appeal and feasibility of this application. Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012) demonstrate this concept on a limited proposition graph. When searching for “headache” in their demo, the user can drill-down to find possible causes or remedies, and even focus on subcategories of those; for example, finding the foods which relieve headaches. As opposed to the structured query application, retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new information they might no</context>
</contexts>
<marker>Gupta, Kathuria, Mirkin, Singh, Goyal, 2014</marker>
<rawString>Anand Gupta, Manpreet Kathuria, Shachar Mirkin, Adarsh Singh, and Aseem Goyal. 2014. Text summarization through entailment-based minimum vertex cover. In *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kingsbury</author>
<author>Martha Palmer</author>
</authors>
<title>Propbank: the next level of treebank.</title>
<date>2003</date>
<booktitle>In Proceedings of Treebanks and lexical Theories,</booktitle>
<volume>3</volume>
<contexts>
<context position="16140" citStr="Kingsbury and Palmer, 2003" startWordPosition="2354" endWordPosition="2357">ropositional Knowledge Graph (PKG). The resulting structure provides a representation in two levels: locally, at sentence level, by representing the syntactic proposition structure embedded in a single sentence, and globally, at inter-proposition level, where relations are drawn between propositions from discourse, or from various sources. At the sentence level, PKG can be compared to Abstract Meaning Representation (AMR) (Banarescu et al., 2013), which maps a sentence onto a hierarchical structure of propositions (predicate-argument relations) - a “meaning representation”. AMR uses Propbank (Kingsbury and Palmer, 2003) for predicates’ meaning representation, where possible, and ungrounded natural language, where no respective 22 Propbank lexicon entry exists. While AMR relies on a deep semantic interpretation, our sentence level representation is more conservative (and thus, hopefully, more feasible) and can be obtained by syntactic interpretation. At inter-proposition level, PKG can be compared with traditional Knowledge Graphs (such as Freebase and Google’s Knowledge Graph). These Knowledge Graphs, in contrast with PKGs, require manual intervention and aim to cover a rich set of relations using formal lan</context>
</contexts>
<marker>Kingsbury, Palmer, 2003</marker>
<rawString>Paul Kingsbury and Martha Palmer. 2003. Propbank: the next level of treebank. In Proceedings of Treebanks and lexical Theories, volume 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
</authors>
<title>Focused entailment graphs for open ie propositions.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="12289" citStr="Levy et al., 2014" startWordPosition="1785" endWordPosition="1788"> texts. However, in a PKG, they are marked as paraphrases (mutually entailing), and both entail an additional proposition from a third source: “Curiosity is a lab”. If one were to query all the 21 propositions that entail “Curiosity is a lab” – e.g. in response to the query “What is Curiosity?” – all three propositions would be retrieved, even though their surface forms may have “functions as” instead of “is” or “laboratory” instead of “lab”. We have recently taken some first steps in this direction, investigating algorithms for constructing entailment edges over sets of related propositions (Levy et al., 2014). Even between simple propositions, recognizing entailment is challenging. We are currently working on new methods that will leverage structured and unstructured data to recognize entailment for Open IE propositions. There are additional relations, besides entailment, that should desirably be represented in PKGs as well. Two such examples are temporal relations (depicted in Figure 1) and causality. Investigating and adapting methods for recognizing and utilizing these relations is intended for future work. 3 Applications An appealing application of knowledge graphs is question answering (QA). </context>
</contexts>
<marker>Levy, Dagan, Goldberger, 2014</marker>
<rawString>Omer Levy, Ido Dagan, and Jacob Goldberger. 2014. Focused entailment graphs for open ie propositions. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schmitz Mausam</author>
<author>Stephen Soderland</author>
<author>Robert Bart</author>
<author>Oren Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>523--534</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="6810" citStr="Mausam et al. (2012)" startWordPosition="978" endWordPosition="981">Open IE systems retrieve only propositions in which both predicates and arguments are instantiated in succession in the surface form. For such propositions, these systems produce independent tuples (typically a (subject, verb, object) triplet) consisting of a predicate and a list of its arguments, all expressed in natural language, in the same way they originally appeared in the sentence. This methodology lacks the ability to represent cases in which propositions are inherently embedded, such as conditionals and propositional arguments (e.g. “Senator Kennedy asked congress to pass the bill”). Mausam et al. (2012) introduced a context analysis layer, extending this 20 representation with an additional field per tuple, which intends to represent the factuality of the extraction, accounting specifically for cases of conditionals and attribution. For instance, the assertion “If he wins five key states, Romney will be elected President” will be represented as ((Romney; will be elected; President) ClausalModifier if; he wins five key states). While these methods capture some of the propositions conveyed by text, they fail to retrieve other propositions expressed by more sophisticated syntactic constructs. C</context>
</contexts>
<marker>Mausam, Soderland, Bart, Etzioni, 2012</marker>
<rawString>Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 523–534, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Stanovsky</author>
<author>Jessica Ficler</author>
<author>Ido Dagan</author>
<author>Yoav Goldberg</author>
</authors>
<title>Intermediary semantic representation through proposition structures.</title>
<date>2014</date>
<booktitle>In Workshop on Semantic Parsing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="9510" citStr="Stanovsky et al., 2014" startWordPosition="1374" endWordPosition="1377">te (namely “acquired”) does not appear in the surface form. Implicit propositions might be introduced in many other linguistic constructs, such as: appositions (“The company, Random House, doesn’t report its earnings.” implies that Random House is a company), adjectives (“Tall John walked home” implies that John is tall), and possessives (“John’s book is on the table” implies that John has a book). We intend to syntactically identify these implicit propositions, and make them explicit in our representation. For further analysis of syntax-driven proposition representation, see our recent work (Stanovsky et al., 2014). We believe that this extension of Open IE representation is feasibly extractable from syntactic parse trees, and are currently working on automatic conversion from Stanford dependencies (de Marneffe and Manning, 2008) to interconnected propositions as described. 2.2 Consolidating Information across Propositions While Open IE is indeed much more scalable than supervised approaches, it does not consolidate natural language expressions, which leads to either insufficient or redundant information when accessing a repository of Open IE extractions. As an illustrating example, querying the Univers</context>
</contexts>
<marker>Stanovsky, Ficler, Dagan, Goldberg, 2014</marker>
<rawString>Gabriel Stanovsky, Jessica Ficler, Ido Dagan, and Yoav Goldberg. 2014. Intermediary semantic representation through proposition structures. In Workshop on Semantic Parsing, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago: A large ontology from wikipedia and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web,</title>
<date>2008</date>
<pages>6--3</pages>
<contexts>
<context position="2072" citStr="Suchanek et al., 2008" startWordPosition="286" endWordPosition="289">screte units, omit redundancies in the original text, and provide semantic relations between the basic units of the representation. This representation can then be used by human readers as a convenient and succinct format, or by subsequent NLP tasks (such as question answering and multidocument summarization) as a structured input representation. A common approach to ID is to extract propositions conveyed in the text by applying either supervised Information Extraction (IE) techniques (Cowie and Lehnert, 1996), to recover propositions covering a predefined set of relations (Auer et al., 2007; Suchanek et al., 2008), or more recently, Open Information Extraction (Open IE) (Etzioni et al., 2008), which discovers open-domain relations (Zhu et al., 2009; Wu et al., 2008). In Open IE, natural language propositions are extracted from text, based on surface or syntactic patterns, and are then represented as predicate-argument tuples, where each element is a natural language string. While Open IE presents a promising direction for ID, thanks to its robustness and scalability across domains, we argue that it currently lacks representation power in two major aspects: representing complex propositions extracted fr</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2008</marker>
<rawString>Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. Yago: A large ontology from wikipedia and wordnet. Web Semantics: Science, Services and Agents on the World Wide Web, 6(3):203–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Wu</author>
<author>Raphael Hoffmann</author>
<author>Daniel S Weld</author>
</authors>
<title>Information extraction from wikipedia: Moving down the long tail.</title>
<date>2008</date>
<booktitle>In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>731--739</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2227" citStr="Wu et al., 2008" startWordPosition="310" endWordPosition="313">be used by human readers as a convenient and succinct format, or by subsequent NLP tasks (such as question answering and multidocument summarization) as a structured input representation. A common approach to ID is to extract propositions conveyed in the text by applying either supervised Information Extraction (IE) techniques (Cowie and Lehnert, 1996), to recover propositions covering a predefined set of relations (Auer et al., 2007; Suchanek et al., 2008), or more recently, Open Information Extraction (Open IE) (Etzioni et al., 2008), which discovers open-domain relations (Zhu et al., 2009; Wu et al., 2008). In Open IE, natural language propositions are extracted from text, based on surface or syntactic patterns, and are then represented as predicate-argument tuples, where each element is a natural language string. While Open IE presents a promising direction for ID, thanks to its robustness and scalability across domains, we argue that it currently lacks representation power in two major aspects: representing complex propositions extracted from discourse, such as interdependent propositions or implicitly conveyed propositions, and consolidating propositions extracted across multiple sources, wh</context>
</contexts>
<marker>Wu, Hoffmann, Weld, 2008</marker>
<rawString>Fei Wu, Raphael Hoffmann, and Daniel S Weld. 2008. Information extraction from wikipedia: Moving down the long tail. In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 731–739. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Zhu</author>
<author>Zaiqing Nie</author>
<author>Xiaojiang Liu</author>
<author>Bo Zhang</author>
<author>Ji-Rong Wen</author>
</authors>
<title>Statsnowball: a statistical approach to extracting entity relationships.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th international conference on World wide web,</booktitle>
<pages>101--110</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="2209" citStr="Zhu et al., 2009" startWordPosition="306" endWordPosition="309">entation can then be used by human readers as a convenient and succinct format, or by subsequent NLP tasks (such as question answering and multidocument summarization) as a structured input representation. A common approach to ID is to extract propositions conveyed in the text by applying either supervised Information Extraction (IE) techniques (Cowie and Lehnert, 1996), to recover propositions covering a predefined set of relations (Auer et al., 2007; Suchanek et al., 2008), or more recently, Open Information Extraction (Open IE) (Etzioni et al., 2008), which discovers open-domain relations (Zhu et al., 2009; Wu et al., 2008). In Open IE, natural language propositions are extracted from text, based on surface or syntactic patterns, and are then represented as predicate-argument tuples, where each element is a natural language string. While Open IE presents a promising direction for ID, thanks to its robustness and scalability across domains, we argue that it currently lacks representation power in two major aspects: representing complex propositions extracted from discourse, such as interdependent propositions or implicitly conveyed propositions, and consolidating propositions extracted across mu</context>
</contexts>
<marker>Zhu, Nie, Liu, Zhang, Wen, 2009</marker>
<rawString>Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. Statsnowball: a statistical approach to extracting entity relationships. In Proceedings of the 18th international conference on World wide web, pages 101–110. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
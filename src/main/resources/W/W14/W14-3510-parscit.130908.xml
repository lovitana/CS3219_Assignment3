<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.967785">
You get what you annotate:
a pedagogically annotated corpus of coursebooks
for Swedish as a Second Language
</title>
<author confidence="0.986298">
Elena Volodina1, Ildikó Pilán1, Stian Rødven Eide2, Hannes Heidarsson3
</author>
<affiliation confidence="0.692597666666667">
(1) Swedish Language Bank, Department of Swedish, University of Gothenburg, Sweden
(2) Department of Philosophy, Linguistics and Theory of Science, University of Gothenburg, Sweden
(3) Department of Swedish, University of Gothenburg, Sweden
</affiliation>
<email confidence="0.913399">
elena.volodina@svenska.gu.se, ildiko.pilan@svenska.gu.se,
stian@fripost.org, hannes.heidarsson@live.se
</email>
<sectionHeader confidence="0.994577" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.998660130434783">
We present the COCTAILL corpus, containing over 700.000 tokens of Swedish texts from 12
coursebooks aimed at second/foreign language (L2) learning. Each text in the corpus is labelled
with a proficiency level according to the CEFR proficiency scale. Genres, topics, associated
activities, vocabulary lists and other types of information are annotated in the coursebooks to
facilitate Second Language Acquisition (SLA)-aware studies and experiments aimed at
Intelligent Computer-Assisted Language Learning (ICALL). Linguistic annotation in the form of
parts-of-speech (POS; e.g. nouns, verbs), base forms (lemmas) and syntactic relations (e.g.
subject, object) has been also added to the corpus.
In the article we describe our annotation scheme and the editor we have developed for the content
mark-up of the coursebooks, including the taxonomy of pedagogical activities and linguistic
skills. Inter-annotator agreement has been computed and reported on a subset of the corpus.
Surprisingly, we have not found any other examples of pedagogically marked-up corpora based
on L2 coursebooks to draw on existing experiences. Hence, our work may be viewed as “groping
in the darkness” and eventually a starting point for others.
The paper also presents our first quantitative exploration of the corpus where we focus on
textually and pedagogically annotated features of the coursebooks to exemplify what types of
studies can be performed using the presented annotation scheme. We explore trends shown in use
of topics and genres over proficiency levels and compare pedagogical focus of exercises across
levels.
The final section of the paper summarises the potential this corpus holds for research within SLA
and various ICALL tasks.
KEYWORDS: L2 coursebook corpus, annotation scheme, CEFR proficiency levels, SLA-aware
ICALL, inter-annotator agreement
</bodyText>
<note confidence="0.857936333333333">
Elena Volodina, Ildikó Pilán, Stian Rødven Eide and Hannes Heidarsson 2014. You get what you annotate: a pedagogically
annotated corpus of coursebooks for Swedish as a Second Language. Proceedings of the third workshop on NLP for computer-
assisted language learning. NEALT Proceedings Series 22 / Linköping Electronic Conference Proceedings 107: 128–144.
</note>
<page confidence="0.996439">
128
</page>
<sectionHeader confidence="0.958998" genericHeader="keywords">
1 Background
</sectionHeader>
<subsectionHeader confidence="0.961061">
1.1 Corpora in CALL and ICALL
</subsectionHeader>
<bodyText confidence="0.9999315">
Corpora have become a useful and often central component in Computer-Assisted Language
Learning (CALL) applications and especially in Intelligent CALL, i.e. CALL based on Natural
Language Processing and Speech Technologies. Primarily, corpora of two types are being
employed in such applications: native speaker (NS) corpora (e.g. Vajjala &amp; Meurers, 2013) and
corpora consisting of L2 learner production, such as essays (e.g. Hancke &amp; Meurers, 2013). In
both cases variation can be observed in the mode of language, i.e. written vs spoken language.
NS corpora are primarily used for automatic selection and generation of learning materials (e.g.
Volodina et al., 2014), while L2 learner corpora are used for development of different types of
grammar and writing support (e.g. Attali &amp; Burstein, 2006).
However, a number of tasks that need to be modelled for the automatic generation of L2
materials, such as text readability classification for the automatic selection of appropriate texts,
depend on access to a special type of language which cannot be classified as typical NS or L2
learner language in the full sense of this word. NS corpora are unable to provide a reliable basis
for modelling for instance text difficulty at the beginner or lower intermediate levels, since NS
corpora exhibit a mixture of easy and complex linguistic phenomena, such as vocabulary,
grammar, sentences, texts. L2 corpora, on the other hand, contain errors and hence cannot be
used to model the language that L2 learners should be exposed to. However, reading and
coursebook materials used for L2 courses can – hypothetically – be used as a subset of NS
language that is appropriate for modelling L2 learner levels, for example for identifying texts
understandable at each of the proficiency levels.
Corpora of coursebook (CB) texts is no novelty in itself, see Meunier &amp; Gouverneur (2009) for
an overview. A number of recent projects dealing with collection and annotation of coursebooks
indicate a rise in interest in textbook analysis for various applied and theoretical studies (e.g.
Gamson et al., 2013). However, CB corpora research has dominated the area of Second
Language Acquisition (mainly English as a Foreign Language, EFL) to a larger extent than
ICALL-driven research. L2 researchers usually pursue a narrowly defined aim, e.g. teaching of
grammar/vocabulary in EFL coursebooks (Anping, 2005) or teaching phraseology at advanced
EFL levels (Meunier &amp; Gouverneur, 2007). To our knowledge, there are very few electronic CB
corpora that have been compiled (e.g. Römer, 2006), with numerous studies carried out using
paper copies of CBs (e.g. Reda, 2003). Systematic studies of textbooks from different angles
(textual, pedagogic, didactic, linguistic) have so far been outside of research focus, which partly
depends upon the lack of richly annotated electronic CB corpora.
</bodyText>
<subsectionHeader confidence="0.73094">
1.2 CEFR and L2 coursebook corpora
</subsectionHeader>
<bodyText confidence="0.994920333333333">
The corpus described in this article is an electronic collection of textbooks used for teaching of
L2 Swedish at CEFR-based courses. CEFR – Common European Framework of Reference for
Languages (COE, 2001) – is an influential cross-national initiative that aims at providing
language course syllabuses and assessment according to the same model of proficiency levels.
CEFR contains 6 levels - A1, A2, B1, B2, C1, C2 – where A1 is the beginner level and C2 is the
full proficiency level.
</bodyText>
<page confidence="0.997885">
129
</page>
<bodyText confidence="0.9899718">
Our interest towards studying CEFR descriptors has resulted from the lack of systematic
description of the CEFR levels for Swedish in concrete linguistic terms that could be useful for
ICALL applications. The CEFR descriptors, that are intentionally very general to cover different
languages, provide very vague guidelines on e.g. text complexity, vocabulary and grammar
scope, as can be seen from Figure 1. Subject to interpretation would be: how short should “short
pieces of information” and “short written passages” be? What does “collate” mean? What is
meant by “in a simple fashion”?
Can collate short pieces of information from several sources and summarise them for
somebody else. Can paraphrase short written passages in a simple fashion, using the original
text wording and ordering.
</bodyText>
<figureCaption confidence="0.618952">
FIGURE 1. CEFR descriptor for B1, for ability to process text. (COE, 2001:96).
</figureCaption>
<bodyText confidence="0.9998959">
Our assumption is that the necessary basis for interpretation of (a part of) the CEFR descriptors
can be obtained from texts used for practical teaching, e.g. coursebooks. A corpus of CB texts
linked to the CEFR levels can, firstly, facilitate pedagogical text studies which would help (1)
establish a relationship between how texts selected for reading influence productive writing
skills, and thus facilitate SLA research; (2) break down CEFR descriptors into concrete linguistic
constituents based on the evidence of the corpus of “input” (i.e. normative) texts - thus
attempting at the standardization of CEFR descriptors. Secondly, from the ICALL perspective,
CEFR-linked CB corpus can provide basis for comprehensive analysis of normative language
that students at CEFR courses are being exposed to. This would, among other things, entail
studies of vocabulary and grammar scopes per level; text and sentence readability experiments.
Depending on the type of annotation, other studies might also be possible, for instance
investigation of development in genre features and use of topics; change in type and format of
exercises across levels; shifts in the focus on language skills across levels. Besides, experiments
on topic modelling, automatic genre identification, analysis of text questions and text question
generation, etc. could also become feasible.
However evident the value of such data for ICALL and SLA might seem, there are very few
attempts undertaken to compile corpora of (CEFR-based) coursebooks. François (2011) describes
the only known to us CB corpus of CEFR-based texts stretching over all levels of proficiency.
The main aim with François&apos; corpus is to use it for NLP-based CALL applications for L2 French.
The corpus consists of 21 coursebooks distributed over the 6 proficiency levels, see Table 1:
</bodyText>
<table confidence="0.986376333333333">
A1 A2 B1 B2 C1 C2 Total
Nr textbooks 10 8 8 4 3 3 36
Nr texts 452 478 681 198 184 49 2042
</table>
<tableCaption confidence="0.98941">
TABLE 1. Overview over the French CB corpus (François, 2011)
</tableCaption>
<bodyText confidence="0.9989265">
All CBs have been published after 2001, have an explicit link to the CEFR levels of proficiency
and are aimed at general L2 French (as opposed to French for specific purposes). After scanning,
only reading materials (i.e. texts properly) have been extracted, leaving aside exercises, lists,
instructions, etc. found in the coursebooks. Texts have been labelled with the proficiency level of
the (chapter of the) book where texts came from, and assigned a genre (e.g. dialogue, recipe,
poem) and linguistic annotation (POS, lemmas). The corpus compiled by François has up to date
</bodyText>
<page confidence="0.994533">
130
</page>
<bodyText confidence="0.977317">
been used for readability studies of L2 French texts and for extraction of a graded lexicon aimed
at L2 learners of French (François, 2011; François et al., 2014).
</bodyText>
<sectionHeader confidence="0.468747" genericHeader="introduction">
2 COCTAILL: collection and annotation
</sectionHeader>
<bodyText confidence="0.9924946">
Work on COCTAILL (Corpus of CEFR-based Textbooks as Input for Learner Levels&apos;
modelling) was initiated in 2013 and has been funded partly by the Department of Swedish at the
University of Gothenburg (UGOT), and partly by the Center for Language Technology, UGOT.
The process of corpus compilation consisted of several stages, shortly presented in Figure 2
below:
</bodyText>
<figureCaption confidence="0.491079">
FIGURE 2. Overview of the CEFR-corpus creation
</figureCaption>
<listItem confidence="0.786633571428571">
• Interviews with L2 teachers. To identify candidate coursebooks, we have carried out
interviews with teachers engaged in CEFR-based courses as well as studied course plans for such
courses. Altogether, 7 teachers at different levels, schools and institutions have agreed to have an
interview. A number of CBs have been named as being used at more than one level. In such
cases, to decide the border between levels, we organized a CB workshop where trained teachers
discussed such coursebooks with each other and suggested division.
• Corpus structuring &amp; purchase of coursebooks. Books that have been suggested by at
</listItem>
<bodyText confidence="0.92955575">
least two teachers have been selected as core material. We have aimed at a balanced
representation at each level with respect to the number of coursebooks per level. However, very
few courses are offered at C1 and none at C2 levels that we know of, so the number of
coursebooks at these levels differ from the others: 2 titles at C1 and none at C2, see section 2.1
for an overview of the corpus structure. Before books were purchased, we explored the
possibility of getting electronic versions from the publishers, but only the publishing house Liber
was willing to cooperate. However, the titles that Liber could provide have been named by only
one teacher, and consequently have not been included into the final corpus.
</bodyText>
<listItem confidence="0.9974012">
• Optical scanning &amp; extraction of raw text. Once the books were purchased, optical
scanning was ordered from an outside contractor. PDF alongside XML files were delivered as
resulting output data. Raw text extracted from the XML files was used as the input for the next
stages.
• Implementation of a coursebook editor. At this stage we defined a taxonomy of textual
</listItem>
<bodyText confidence="0.69281325">
and pedagogical features for annotation, as well as the format of the output data. Previously, no
richly (pedagogically) annotated L2 coursebook corpora have been compiled. Therefore, there
were no available editing tools to reuse. After experimenting with XML editors and DTD
schemas, we have opted to develop our own editor as described in subsection 2.3.
</bodyText>
<page confidence="0.981182">
131
</page>
<listItem confidence="0.553618">
• Annotation for pedagogical and textual features involved manual work. Altogether, four
</listItem>
<bodyText confidence="0.993865666666667">
people have been involved in the content annotation. Initial annotation of the first two CBs was
performed to test the editor and to establish an acceptable taxonomy of textual and pedagogical
variables, see section 2.2. In the next round, one more annotator was trained, and as a result, a
number of revisions were suggested to improve the taxonomy of pedagogical and textual
features. The introduced changes led to a necessity to revise the two initially annotated
coursebooks. By the end of this round, annotation guidelines have been produced. Finally, two
more annotators have been trained. This stage was concluded by an inter-annotator agreement
experiments, which entailed revisions to the annotation guidelines and highlighted the need of
another round of revision of the already annotated books, as described in section 2.4.
</bodyText>
<listItem confidence="0.862215583333333">
• Linguistic annotation in the form of parts-of-speech, syntactic relations and lemmas has
been automatically added using Korp web services (Borin et al., 2012). Whereas annotation of
text passages and activity instructions holds good quality, we would need to assess annotation
quality of all other types of information. The reason for that is the fact that tasks, lists, and
language examples have an unpredictable structure – often incomplete sentences, or lists of mixed
linguistic units, which tends to get a very low-level accuracy when it comes to e.g. parts of speech
and dependency annotation.
• Release of the corpus. Unfortunately, the corpus as a whole cannot be made freely
available for download for copyright reasons, however, it is browsable for research purposes via
Korp (Borin et al., 2012) with password protection. Besides, parts of the corpus in the form of a
bag of sentences (as opposed to connected texts) for each proficiency level are released as
downloadable data1.
</listItem>
<subsectionHeader confidence="0.994837">
2.1 Corpus overview
</subsectionHeader>
<bodyText confidence="0.9999415">
The COCTAILL consists of 12 coursebooks, 5 of which are used at more than one level. The
corpus is balanced in the number of coursebooks per level (4 titles/level), except level C1 (2
titles/level). C2 level is not included in this corpus since it represents full language proficiency
when learners “can understand with ease virtually everything heard or read” (COE, 2001:24),
hence, from the point of view of linguistic modelling it corresponds to regular NS language. The
summary of the corpus is presented in Table 2.
</bodyText>
<tableCaption confidence="0.771296">
TABLE 2. Overview of the Swedish CEFR corpus
</tableCaption>
<footnote confidence="0.885858">
1 Contact Elena Volodina &lt;elena.volodina@svenska.gu.se&gt; or Ildikó Pilán &lt;ildiko.pilan@svenska.gu.se&gt; to get access to
the files.
</footnote>
<page confidence="0.99599">
132
</page>
<bodyText confidence="0.9999726">
The COCTAILL comprises a total of 708 589 tokens, about half of which belong to texts, the rest
to activity instructions, tasks, lists and language examples. The columns “Nr. of sentences
(texts)” and “Nr. of tokens (texts)” refer to sentences in texts only, other elements were excluded
from these counts since they often contain smaller linguistic units than a full sentence. The
amount of tasks in the corpus (a total of 1494) outnumbers the number of texts (1106). The
largest amount of material in terms of texts and tasks is available for B1 and B2 levels.
The values in Table 2 are meant primarily to give an idea of the size of the corpus, rather than
present data from which generalizations about the CEFR levels can be made, since authors&apos;
choice varied to a great extent as far as the division into lessons and the number of texts and tasks
included per level are concerned.
</bodyText>
<subsectionHeader confidence="0.999186">
2.2 Coursebook content annotation
</subsectionHeader>
<bodyText confidence="0.9870938">
An overview over the taxonomy of textual and pedagogical annotation is provided in Figure 3.
XML elements are shown on the left with their corresponding attributes on the right:
FIGURE 3. Overview over the textual and pedagogical annotation:
XML elements and their attributes
Structurally, each coursebook is divided into extras (contents, foreword, copyright note, etc) and
lessons (chapters). The running text in each lesson has been manually split into texts aimed at
reading comprehension and other types of information typical of coursebooks, such as activity
instructions, tasks, lists and language examples, whereby reading comprehension materials have
been annotated for textual features (section 2.2.1), and the rest of information for pedagogically
relevant features (section 2.2.2).
</bodyText>
<subsubsectionHeader confidence="0.867277">
2.2.1 Textual annotation
</subsubsectionHeader>
<bodyText confidence="0.989086625">
By textual annotation we understand mark-up of text passages for topics and genres.
We have listed 28 text topics (Table 3) which follow the CEFR guidelines (COE, 2001) in the
first place, with modifications introduced as a result of our practical work on the first
coursebooks (Volodina &amp; Johansson Kokkinakis, 2013).
In general, we followed the recommendation to opt for a broader topic, e.g. if a text is about a
political crisis in some country, including military actions, Politics and power would probably be
the best choice. In most cases, more than one topic has been applicable, in which case two or
more topics have been assigned. In case there were no topics that corresponded to the text, we
</bodyText>
<page confidence="0.996238">
133
</page>
<bodyText confidence="0.9395685">
considered adding new ones, see Table 3 for the alphabetic list of the topics we have been using
so far.
</bodyText>
<tableCaption confidence="0.378797">
TABLE 3. List of topics
</tableCaption>
<bodyText confidence="0.960089">
The taxonomy of genre families is comprised of four elements: Narration, Facts, Evaluation and
Other, following the taxonomy described in Johansson and Sandell Ring (2010) with slight
modifications as a result of the work on the first annotated coursebooks (Volodina and Johansson
Kokkinakis, 2013). Such a modification is the addition of the genre family Other which contains
text genres (e.g. puzzle) that were difficult to place into the other three Narration, Facts or
Evaluation families. Further subdivision of genre families into macrogenres is shown in Table 4.
</bodyText>
<table confidence="0.999796333333333">
Narration Facts Evaluation Other
Description Autobiography Advertisement Anecdote, joke
Fiction Biography Argumentation Dialogue
News article Demonstration Discussion Language tip
Personal story Explanation Exposition Letter
Facts Interpretation, Lyrics
Geographical facts exegesis Notice, short message
Historical facts Personal reflection Puzzle
Instruction Persuasion Questionnaire
Procedures Review Quotation
Report Recipe
Rules Rhyme
</table>
<tableCaption confidence="0.976934">
TABLE 4. List of genre families and macrogenres
</tableCaption>
<bodyText confidence="0.99639475">
It can be discussed whether some of the Other macrogenres can be moved to any of the other three
genre families (e.g. Anecdotes to the Narration family).
In a lot of cases, where there were no clear-cut genres, a combination of genres became an
optimal solution, see Figure 4.
</bodyText>
<listItem confidence="0.998119566666667">
• Animals
• Arts
• Clothes &amp; appearances
• Crime &amp; punishment
• Culture &amp; traditions
• Daily life
• Economy
• Education
• Family &amp; relatives
• Famous people
• Food &amp; drink
• Free time, entertainment
• Greetings/introductions
• Health &amp; body care
• House &amp; home, environment
• Jobs &amp; professions
• Languages
• Personal identification
• Places
• Politics &amp; power
• Relations with other
people
• Religion, myths &amp;
legends
• Science &amp; technology
• Services
• Shopping
• Sports
• Travel
• Weather &amp; nature
</listItem>
<page confidence="0.961247">
134
</page>
<figure confidence="0.795484">
C
</figure>
<figureCaption confidence="0.844989">
FIGURE 4. An example of textual annotation, text at level A1
</figureCaption>
<subsubsectionHeader confidence="0.575655">
2.2.2 Pedagogical annotation of coursebooks
</subsubsectionHeader>
<bodyText confidence="0.999541166666667">
Pedagogical annotation in this corpus is understood as mark-up assigned to all types of
information found in coursebook lessons except texts used for reading comprehension. All books
are structured by lessons (i.e. chapters in coursebooks), which are assigned a proficiency level,
which then applies to all texts and activities in the lesson. The taxonomy of the pedagogical
mark-up within each lesson is presented by lists, language examples, tasks and activity
instructions.
</bodyText>
<table confidence="0.999576636363636">
Activity Activity Activity Language examples,
instructions, instructions, Tasks instructions, Tasks Lists
Tasks, Language
examples, Lists
Target skills: Activity types: Activity formats: Linguistic units:
Listening Brainstorming Category Characters
Reading Composition/essay identification Dialogues
Writing writing Category substitution Full sentences
Speaking Dialogue/interview Free/short answers Incomplete sentences
Dictation Free writing Numbers
Target competences: Discussion Gaps Phrases
Grammar Error correction Matching Question-answer
Pronunciation Form manipulation Multiple choice Single words
Spelling Information search Narration, retelling, Texts/examples of text
Vocabulary Monologue presentation writing
Pre-reading Reordering/
Question answering Restructuring
Reading aloud Sorting
Role-playing True-false/Yes-no
Summary Wordbank
Text questions
Translation
</table>
<tableCaption confidence="0.98149">
TABLE 5. Overview over the taxonomy of the pedagogical mark-up
</tableCaption>
<page confidence="0.998951">
135
</page>
<bodyText confidence="0.999717333333333">
Further, each of the pedagogically-relevant elements is associated with the target
skills/competences (e.g. reading) they are aimed at. Lists and language examples are assigned
linguistic units (e.g. single words), and all tasks and activity instructions are associated with
format and type of exercises (e.g. gaps), see Table 5 for an overview. In the terms of the output
XML data, the table headings represent XML elements, the text in bold corresponds to XML
attributes, and the running text stands for a set of attribute values.
</bodyText>
<subsectionHeader confidence="0.343424">
An example of pedagogical annotation follows below (Figure 5)
</subsectionHeader>
<bodyText confidence="0.534187">
FIGURE 5. An example of pedagogical annotation, level A1
</bodyText>
<subsectionHeader confidence="0.952515">
2.3 Online coursebok editor
</subsectionHeader>
<bodyText confidence="0.993690333333333">
To simplify the process of inserting XML-annotation into the OCR-ed raw texts, an online
coursebook editor has been developed early in the project (Volodina &amp; Johansson Kokkinakis,
2013).2
</bodyText>
<figureCaption confidence="0.523127">
FIGURE 6. The online corpus editor.
</figureCaption>
<figure confidence="0.668341333333333">
2http://spraakbanken.gu.se/larka/larka_cefr_editor.html
c
c
</figure>
<page confidence="0.995302">
136
</page>
<bodyText confidence="0.999672266666667">
The annotation scheme for content annotation described in section 2.2 has been implemented in
the form of user-friendly menus (Figure 6, on the left). In the centre (Figure 6) is an editable text
area where text for annotation is pasted, and on the right is a field for an overview of all inserted
IDs. Link to annotation guidelines and an option of downloading the annotated text as a file are
also offered.
Each menu element is accompanied with a pop-up dialogue, which prompts what information
should be added, for example IDs, or references to previously used IDs, or titles. For categories
where lists of options exist, such as topics, genres or skills, options are offered as multi-select
drop-down menus. Besides, there are sub-menus for inserting subheadings and extra information,
such as text author, source of information, etc. Each new inserted XML element closes the
previously opened one, except in cases of lessons, extras, genres and subheadings.
Meta-information about each coursebook is collected once before the annotation of the rest of the
book starts, and includes title, author, publication year, publisher, ISBN.
The editor is language independent, freely accessible over internet and can be easily reusable in
other L2 coursebook annotation projects.
</bodyText>
<subsectionHeader confidence="0.992468">
2.4 Text-level annotation: inter-annotator agreement
</subsectionHeader>
<bodyText confidence="0.9887261">
Inter-annotator agreement is the degree of agreement among annotators about assigning
categories to the same objects (Artstein and Poesio, 2008). Our intention with the inter-annotator
agreement experiment was to estimate the quality of the text-level (textual) annotation on the one
hand, and to detect categories causing large number of disagreements and inconsistencies, on the
other.
We have investigated randomly chosen parts of the CEFR corpus, targeting at least one chapter
(lesson) per level. The controlled subset of the corpus comprised 21630 tokens at the five
proficiency levels, divided between 32 texts and a number of accompanying coursebook
activities. Our focus has been on texts: text topics, genre families and macrogenres. Three
annotators have been involved in this experiment with knowledge of linguistics, language
teaching and computational linguistics.
TABLE 6. Results of the inter-annotator agreement for topics, genre families and macrogenres
We report inter-annotator agreement in terms of Fleiss&apos; multi-kappa (Davies and Fleiss, 1982)
and Krippendorff&apos;s alpha (Krippendorff, 1980) being that the task involved multiple (i.e. three or
more) annotators. Both measures take into account chance agreement (Artstein and Poesio,
2008). Each annotator could assign more than one category to each text object, i.e. multiple
topics out of 28 possible ones, multiple genre families out of 4 choices and multiple macrogenres
out of 34 options, therefore, we used distance measures that would calculate the dissimilarity
between sets of multiple values. We considered both Jaccard&apos;s distance metric (Jaccard, 1908)
and MASI (Measuring Agreement on Set-valued Items; Passonneau, 2006) when calculating
</bodyText>
<page confidence="0.995836">
137
</page>
<bodyText confidence="0.99984975">
agreement with the previously mentioned measures. Both metrics are based on the union and the
intersection between sets, MASI including also an additional term, M, which equals 1 if the sets
are identical, 2/3 in case of subsumption, and 1/3 if there is at least one element in common
between the two sets (Passonneau, 2006). For both the distance3 and the agreement4 measures the
NLTK Python module has been used (Bird, 2006). Results are shown in Table 6.
Fleiss&apos; kappa within the range between 0.61-0.80 means substantial agreement, which given our
type of annotation is a very encouraging result. However, the original results for Fleiss&apos; kappa
were lower than the ones reported in Table 6 (e.g. Fleiss&apos; kappa for topics 0.52 with Jaccard
distance and 0.37 with MASI). The reason for that proved to lie in the fact that some of the texts
had substantial difference in the number of assigned values, with the intersection being a good
common ground. This has led us to the conclusion that we should set a maximum number of
values that may be assigned to each text object. To simulate that, we have calculated inter-
annotator agreement based on the intersection of values (i.e. considering only values that were
common between at least two of the three annotators, leaving out the ones that have been
assigned only once, except when only one label was provided), as reported in the table above.
The results have improved substantially. Following this experiment, in the near future, a revision
of the corpus annotation is planned where we will consider reducing the number of assigned
topics to a maximum of 3 and macrogenres – to a maximum of 2.
To exemplify cases with different interpretations, look at Figure 7 where a text with a horoscope
is given in the original language and translated into English in Figure 8.
</bodyText>
<figureCaption confidence="0.459377571428571">
FIGURE 7. Text on horoscope, level B2.
Freja looks into Jonas&apos;s horoscope: You are playful, and if you can choose, you&apos;d spend the day getting
to know better somebody you are acquainted with. The evening will be romantic.
And then into her own: The love life is a mess, but otherwise, the day will be funny, sensual and
entertaining. Don&apos;t work yourself up. You will receive compliments from somebody in your surrounding.
FIGURE 8. Translation of the text into English
Figure 9 provides the three annotations that have been provided to this text.
</figureCaption>
<bodyText confidence="0.999219666666667">
The first annotator assigned 4 topics: (1) culture and traditions, (2) daily life, (3) relations with
other people, (4) religion; myth and legends. The second annotator assigned topic (4), whereas
the third annotation assigned topics (2) and (3).
</bodyText>
<footnote confidence="0.9994705">
3http://www.nltk.org/_modules/nltk/metrics/distance.html
4http://www.nltk.org/_modules/nltk/metrics/agreement.html
</footnote>
<page confidence="0.990011">
138
</page>
<figureCaption confidence="0.987705">
FIGURE 9. Annotation of the text for topics and genres
</figureCaption>
<bodyText confidence="0.992406666666667">
For the experiments we used triples of values (annotator-code, text-code, list of assigned values),
in Table 7 shown with the original set-up in the first column, and with an intersection set-up in
the second column.
</bodyText>
<table confidence="0.9953325">
Original experiment Intersection-based experiment
• (ann1, text_5_8, [1,2,3,4]) • (ann1, text_5_8, [2,3,4])
• (ann2, text_5_8, [4]) • (ann2, text_5_8, [4])
• (ann3, text_5_8, [2,3]) • (ann3, text_5_8, [2,3])
</table>
<tableCaption confidence="0.905363">
TABLE 7. Original versus “intersection”-based triples
</tableCaption>
<bodyText confidence="0.999515181818182">
As can be seen, the value “1” has been removed from the list of assigned values from annotator
1, since this value has not been used by any other annotator. We can see here that annotator 1 has
agreed with both annotators 2 and 3, whereas there was no agreement between annotator 2 and 3.
Summarizing the results of the experiment on inter-annotator agreement, we can say that
categories causing a lot of disagreement proved to be the difference in number of assigned
values, rather that the values themselves, which is the reason for planned revisions in the
annotation guidelines and in the annotated files. However, the experiment has also shown that the
annotation is reliable and can be used for experiments as it is, in the sense that among the
multiple values there has always been a central overlap between different annotators. Non-
overlapping topics and genres can be considered peripheral adding an extra value to text
characteristics.
</bodyText>
<sectionHeader confidence="0.982271" genericHeader="method">
3 Initial quantitative explorations of the COCTAILL
</sectionHeader>
<bodyText confidence="0.9999108">
We carried out an initial quantitative analysis of the corpus observing variables such as text
genres, topics as well as skills and competences targeted by tasks at each CEFR level.
Texts showed a substantial variation both in genre and in topics across proficiency levels. About
half of the texts were dialogues at A1 level, but this amount steadily decreased at each CEFR
level, C1 level coursebooks containing barely any. Factual texts were presented at all levels, but
</bodyText>
<page confidence="0.997683">
139
</page>
<bodyText confidence="0.9224543125">
at higher proficiency levels they were almost twice as common. The percentage of dialogues and
factual texts at each level is presented in Figure 10.
FIGURE 10. Percentage of dialogues and factual texts per CEFR level
Not only genres, but also certain topics showed large difference in distribution at different CEFR
levels, as Figure 11 below shows.
FIGURE 11. Percentage of text topics per CEFR level
The topics “culture and tradition” and “politics and power” are either not present or appear to a
very limited extent at A1 level, but at higher proficiency levels their proportion increases
substantially. The topic of “daily life”, although appears at all CEFR levels, seems to be less
common at C1 level. Interestingly, the percentage of texts focusing on “family and relatives”
remains the same across all levels. Such topics would be particularly suitable for the analysis of
how linguistic complexity changes at different proficiency levels within the same topic.
Further, we retrieved some quantitative data from a more pedagogical perspective aiming at
tracing how the proportion of skills and competences targeted by tasks change at various levels.
This information is presented in Figure 12.
FIGURE 12. Target skills and competences per CEFR level
</bodyText>
<page confidence="0.990629">
140
</page>
<bodyText confidence="0.999820214285714">
At A1 and A2 levels, the focus is primarily on the productive skills of speaking and writing, each
of which accounted for about one fourth of the exercises at this level. Tasks involving the
receptive skills of reading and listening are about 10% less frequent at this initial stage. The
corpus also shows a shift in focus from oral language use to the written one at B1 and B2 levels.
More than half of the tasks are writing exercises at B1 level, and the highest percentage of
reading tasks (35%) appears at B2 level. The proportion of grammar exercises increases until B1
level, then it keeps its rather dominant presence (about 40%) at all further stages. Vocabulary
teaching is a primary target skill of tasks at A1 level, but less so at A2 level, whilst from
intermediate (B1) level on, vocabulary exercises dominate the items proposed for students, which
is especially obvious at C1, which supports Singleton&apos;s (1995) hypothesis that vocabulary doesn&apos;t
have a critical period at which it should be taught or learnt.
Another interesting piece of statistics we have looked at is average sentence length per CEFR
level (Figure 13). Numbers have been calculated upon sentences retrieved from texts aimed at
reading comprehension.
</bodyText>
<figureCaption confidence="0.370126">
FIGURE 13. Average sentence length per CEFR level.
</figureCaption>
<bodyText confidence="0.977896181818182">
The graph shows that sentence length grows steadily from lower levels to more advanced ones,
the largest increase being observed between A2 and B1 with no difference between B1 and B2.
The most feasible explanation for the less drastic increase in sentence length starting from B1 is
that texts at the higher levels contain a broader mixture of sentence types – both typical of the
level itself, and of all the lower levels, e.g. B2 texts hypothetically contain sentences typical of
levels A1, A2, B1 and B2. The sentence length typical of the lower levels would in that case
influence the calculations of the average length at B2. Another potential explanation might be
connected to the number of texts of certain genres: to take one example, dialogues that tend to
contain very short sentences, dominate at A1 and A2 levels and decrease in number from B1.
These numbers show some similarity in the tendency of increase to the reported average sentence
length in the L2 Fren
</bodyText>
<table confidence="0.766307">
A1 A2 B1 B2 C1 C2
9,1 14,54 16,85 18,6 19,36 21,43
</table>
<tableCaption confidence="0.87324">
TABLE 8. Average sentence length in L2 French corpus ( François, 2011:359)
</tableCaption>
<bodyText confidence="0.99987325">
There is a steady increase in the average sentence length over the levels in both languages.
However, there is a larger increase between A1 and A2 in L2 French coursebooks, and more
moderate growth between the rest of the levels. Differences in the average values between the
two languages can be accounted for by linguistic characteristics of the two language, by
</bodyText>
<page confidence="0.995149">
141
</page>
<bodyText confidence="0.998882">
differences in tokenization and segmentation tools, as well as by the variety of text genres present
in the two corpora. In general, this piece of statistics raises interesting questions about linguistic
complexity of each proficiency level and asks for deeper investigations of the problem.
</bodyText>
<sectionHeader confidence="0.990157" genericHeader="method">
4 Concluding remarks
</sectionHeader>
<bodyText confidence="0.997327756756757">
We have presented our work on COCTAILL, a corpus of L2 coursebooks, richly annotated for
textual, pedagogical and linguistic variables. The corpus is innovative in a number of ways: there
are no other existing electronic corpora that have pedagogical annotation alongside proficiency
level-labelling, textual annotation, and linguistic annotation covering all the spectrum of
proficiency levels interesting for linguistic modelling of learner levels. We pioneered in the
development of a taxonomy of pedagogical variables for L2 coursebook annotation, which up-to-
date remains the only one we are aware of. Besides, unlike a number of other coursebook
projects, where only reading materials are selected or only a subset of CB language is analysed,
we present a possibility to study coursebooks in their entirety with important implications for
correlating proficiency levels, L2 input as well as various pedagogical and textual variables, such
as target skills and competences. COCTAILL is available for browsing with password protection
and is downloadable as a bag of sentences labelled with coursebook levels.
In the future, we plan several iterations on the improvement of COCTAILL content annotation.
This will include the revision and a potential decrease in the number of assigned topics and
macrogenres. Besides, the topic and genre taxonomy may need to be revised to contain fewer, but
more general categories, i.e. going from a more detailed taxonomy to one with broader
categories.
Certain parameters have yet been outside the inter-annotator agreement experiment. In future we
plan to focus on
(1) activity instructions and tasks, where we will calculate agreement in assigning target
skills and exercise formats; and
(2) lists and language examples, where the main focus will be on the annotation of target
skills and linguistic units
We can foresee that results of the inter-annotator experiments will yield another round of
annotation revision.
Availability of the corpus opens prospects to engage in numerous SLA-aware ICALL-relevant
studies, such as CEFR profiling, vocabulary and grammar profiling, studies on sentence and text
readability, question generation, automatic genre identification, automatic topic modelling – to
name just a few potential directions of research.
The taxonomy of textual and pedagogical variables present in COCTAILL provides the key to
various empirical studies of coursebooks, which can help critically assess and reflect on the
relation between coursebook design and SLA research. Pedagogically annotated coursebook
corpora such as COCTAILL, have a potential to become a crystallized form of what should be
taught, at which level and in which format, which is crucial for various ICALL tasks, such as
material generation. We expect that these insights, implemented into ICALL applications, will
facilitate generation of pedagogically appropriate learning materials. To put it simply, you get
what you annotate.
</bodyText>
<page confidence="0.997291">
142
</page>
<sectionHeader confidence="0.995331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999346342105263">
Anping He. (2005). Corpus-Based Evaluation of ELT textbooks. Paper presented at the joint
conference of the American Association of Applied Corpus Linguistics and the International
Computer Archive of Modern and Medieval English, 12-15 May 2005, University of Michigan.
Artstein Ron &amp; Massimo Poesio. (2008). Inter-coder agreement for computational linguistics.
Computational Linguistics, 34(4): 555-596.
Attali Yigal &amp; Jill Burstein. (2006). Automated essay scoring with e-rater v.2. The Journal of
Technology, Learning and Assessment, 4(3).
Bird Steven. (2006). NLTK: the natural language toolkit. In Proceedings of the COLING/ACL
on Interactive presentation sessions, pp. 69-72.
Borin Lars, Markus Forsberg &amp; Johan Roxendal. (2012). Korp – the corpus infrastructure of
Språkbanken. Proceedings of LREC 2012. Istanbul: ELRA. 474–478.
Council of Europe (COE). (2001). The Common European Framework of Reference for
Languages: Learning, Teaching, Assessment. Cambridge University Press.
Davies Mark &amp; Joseph L. Fleiss. (1982). Measuring agreement for multinomial data.
Biometrics, 38(4): 1047–1051.
François Thomas. (2011). Les apports du traitement automatique du langage à la lisibilité du
français langue étrangère, Ph.D. Thesis, Université Catholique de Louvain. Thesis
Supervisors : Cédrick Fairon and Anne Catherine Simon.
François Thomas, Nuria Gala, Patrick Watrin &amp; Cédrick Fairon. (2014). FLELex: a graded
lexical resource for French foreign learners. In the 9th International Conference on Language
Resources and Evaluation (LREC 2014). Reykjavik, Iceland, 26-31 May.
Gamson David A., Lu Xiaofei, &amp; Eckert Sarah Anne. (2013). Challenging the research base of
the common core state standards: A historical reanalysis of text complexity. Educational
Researcher, 42(7):381-391.
Jaccard Paul. (1908). Nouvelles recherches sur la distribution florale. Bulletin de la Societe
Vaudoise des Sciences Naturelles, 44: 223-270.
Hancke Julia &amp; Detmar Meurers. (2013). Exploring CEFR classification for German based on
rich linguistic modeling. Learner Corpus Research 2013, Book of Abstracts. pp. 54-56. Bergen,
Norway.
Johansson Britt &amp; Anniqa Sandell Ring. (2010). Låt språket bära: genrepedagogiken i
praktiken. Hallgren och Fallgren, Stockholm.
Krippendorff Klaus. (1980). Content Analysis: An Introduction to Its Methodology, chapter 12.
Sage, Beverly Hills, CA.
Meunier Fanny &amp; Gouverneur Céline. (2007). The treatment of phraseology in ELT textbooks,
In: Corpora in the Foreign Language Classroom. Selected papers from the Sixth International
Conference on Teaching and Language Corpora (TaLC6), University of Granada, 4-7 July
2004, Encarnación H., Quereda L. and Santana J. ed(s), Amsterdamm &amp; New York, Rodopi,
Language and Computers Series 61, p. 119-139.
</reference>
<page confidence="0.98983">
143
</page>
<reference confidence="0.999291913043478">
Meunier Fanny &amp; Gouverneur Céline. (2009). New types of corpora for new educational
challenges: collecting, annotating and exploiting a corpus of textbook material, In: Corpora and
Language Teaching, Aijmer, K. ed(s), Amsterdam &amp; Philadelphia, Benjamins, p. 179-201
Passonneau Rebecca J. (2006). Measuring agreement on set-valued items (MASI) for semantic
and pragmatic annotation. In Proceedings of LREC, Genoa, pp. 831–836.
Reda Ghsoon. (2003). English Coursebooks: Prototype Textsts and Basic Vocabulary Norms.
ELT Journal 57(3): 260-268.
Römer Ute. (2006). Looking at Looking: Functions and Contexts of Progressives in Spoken
English and &apos;School&apos; English. In: Renouf, Antoinette &amp; Andrew Kehoe (eds.). The Changing
Face of Corpus Linguistics. Papers from the 24th International Conference on English
Language Research on Computerized Corpora (ICAME 24). Amsterdam: Rodopi. p.231-242.
Singleton David. (1995). Introduction: A Critical Look at the Critical Period in Second
Language Acquisition Research, In Singleton D. &amp; Lengyel, Z. (Eds.), The Age Factor in
Second Language Acquisition (1-29). Avon: Multilingual Matters, Ltd.
Vajjala Sowmya &amp; Detmar Meurers. (2013). On The Applicability of Readability Models to
Web Texts. Proceedings of the Workshop on Predicting and Improving Text Readability for
Target Reader Populations (PITR), ACL 2013
Volodina Elena, Ildikó Pilán, Lars Borin, &amp; Therese Lindström Tiedemann. (2014). A flexible
language learning platform based on language resources and web services. Proceedings of
LREC 2014, Reykjavik, Iceland.
Volodina Elena &amp; Sofie Johansson Kokkinakis. (2013). Compiling a corpus of CEFR-related
texts. Proceedings of the Language Testing and CEFR conference, Antwerpen, Belgium, May
27-29, 2013.
</reference>
<page confidence="0.998594">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001705">
<title confidence="0.971264">You get what you annotate: a pedagogically annotated corpus of for Swedish as a Second Language</title>
<author confidence="0.831933">Ildikó Stian Rødven Hannes</author>
<affiliation confidence="0.6768885">(1) Swedish Language Bank, Department of Swedish, University of Gothenburg, Sweden (2) Department of Philosophy, Linguistics and Theory of Science, University of Gothenburg,</affiliation>
<address confidence="0.738659">(3) Department of Swedish, University of Gothenburg, Sweden</address>
<email confidence="0.732607">elena.volodina@svenska.gu.se,ildiko.pilan@svenska.gu.se,</email>
<abstract confidence="0.961475333333333">stian@fripost.org, hannes.heidarsson@live.se We present the COCTAILL corpus, containing over 700.000 tokens of Swedish texts from 12 coursebooks aimed at second/foreign language (L2) learning. Each text in the corpus is labelled with a proficiency level according to the CEFR proficiency scale. Genres, topics, associated activities, vocabulary lists and other types of information are annotated in the coursebooks to facilitate Second Language Acquisition (SLA)-aware studies and experiments aimed at Intelligent Computer-Assisted Language Learning (ICALL). Linguistic annotation in the form of parts-of-speech (POS; e.g. nouns, verbs), base forms (lemmas) and syntactic relations (e.g. subject, object) has been also added to the corpus. In the article we describe our annotation scheme and the editor we have developed for the content mark-up of the coursebooks, including the taxonomy of pedagogical activities and linguistic skills. Inter-annotator agreement has been computed and reported on a subset of the corpus. Surprisingly, we have not found any other examples of pedagogically marked-up corpora based on L2 coursebooks to draw on existing experiences. Hence, our work may be viewed as “groping in the darkness” and eventually a starting point for others. The paper also presents our first quantitative exploration of the corpus where we focus on textually and pedagogically annotated features of the coursebooks to exemplify what types of studies can be performed using the presented annotation scheme. We explore trends shown in use of topics and genres over proficiency levels and compare pedagogical focus of exercises across levels. The final section of the paper summarises the potential this corpus holds for research within SLA and various ICALL tasks. L2 coursebook corpus, annotation scheme, CEFR proficiency levels, SLA-aware ICALL, inter-annotator agreement Elena Volodina, Ildikó Pilán, Stian Rødven Eide and Hannes Heidarsson 2014. You get what you annotate: a pedagogically corpus of coursebooks for Swedish as a Second Language. of the third workshop on NLP for computerlanguage learning. Proceedings Series 22 / Linköping Electronic Conference Proceedings 107: 128–144. 128 1 Background 1.1 Corpora in CALL and ICALL Corpora have become a useful and often central component in Computer-Assisted Language Learning (CALL) applications and especially in Intelligent CALL, i.e. CALL based on Natural Language Processing and Speech Technologies. Primarily, corpora of two types are being employed in such applications: native speaker (NS) corpora (e.g. Vajjala &amp; Meurers, 2013) and corpora consisting of L2 learner production, such as essays (e.g. Hancke &amp; Meurers, 2013). In both cases variation can be observed in the mode of language, i.e. written vs spoken language. NS corpora are primarily used for automatic selection and generation of learning materials (e.g. Volodina et al., 2014), while L2 learner corpora are used for development of different types of grammar and writing support (e.g. Attali &amp; Burstein, 2006). However, a number of tasks that need to be modelled for the automatic generation of L2 materials, such as text readability classification for the automatic selection of appropriate texts, on access to a special type of language which cannot be classified as or L2 learner language in the full sense of this word. NS corpora are unable to provide a reliable basis for modelling for instance text difficulty at the beginner or lower intermediate levels, since NS corpora exhibit a mixture of easy and complex linguistic phenomena, such as vocabulary, grammar, sentences, texts. L2 corpora, on the other hand, contain errors and hence cannot be used to model the language that L2 learners should be exposed to. However, reading and coursebook materials used for L2 courses can – hypothetically – be used as a subset of NS language that is appropriate for modelling L2 learner levels, for example for identifying texts understandable at each of the proficiency levels. Corpora of coursebook (CB) texts is no novelty in itself, see Meunier &amp; Gouverneur (2009) for an overview. A number of recent projects dealing with collection and annotation of coursebooks indicate a rise in interest in textbook analysis for various applied and theoretical studies (e.g. Gamson et al., 2013). However, CB corpora research has dominated the area of Second Language Acquisition (mainly English as a Foreign Language, EFL) to a larger extent than ICALL-driven research. L2 researchers usually pursue a narrowly defined aim, e.g. teaching of grammar/vocabulary in EFL coursebooks (Anping, 2005) or teaching phraseology at advanced EFL levels (Meunier &amp; Gouverneur, 2007). To our knowledge, there are very few electronic CB corpora that have been compiled (e.g. Römer, 2006), with numerous studies carried out using paper copies of CBs (e.g. Reda, 2003). Systematic studies of textbooks from different angles (textual, pedagogic, didactic, linguistic) have so far been outside of research focus, which partly depends upon the lack of richly annotated electronic CB corpora. 1.2 CEFR and L2 coursebook corpora The corpus described in this article is an electronic collection of textbooks used for teaching of L2 Swedish at CEFR-based courses. CEFR – Common European Framework of Reference for Languages (COE, 2001) – is an influential cross-national initiative that aims at providing language course syllabuses and assessment according to the same model of proficiency levels. CEFR contains 6 levels - A1, A2, B1, B2, C1, C2 – where A1 is the beginner level and C2 is the full proficiency level. 129 Our interest towards studying CEFR descriptors has resulted from the lack of systematic description of the CEFR levels for Swedish in concrete linguistic terms that could be useful for ICALL applications. The CEFR descriptors, that are intentionally very general to cover different languages, provide very vague guidelines on e.g. text complexity, vocabulary and grammar scope, as can be seen from Figure 1. Subject to interpretation would be: how short should “short pieces of information” and “short written passages” be? What does “collate” mean? What is meant by “in a simple fashion”? Can collate short pieces of information from several sources and summarise them for somebody else. Can paraphrase short written passages in a simple fashion, using the original text wording and ordering. CEFR descriptor for B1, for ability to process text. (COE, 2001:96). Our assumption is that the necessary basis for interpretation of (a part of) the CEFR descriptors can be obtained from texts used for practical teaching, e.g. coursebooks. A corpus of CB texts linked to the CEFR levels can, firstly, facilitate pedagogical text studies which would help (1) establish a relationship between how texts selected for reading influence productive writing skills, and thus facilitate SLA research; (2) break down CEFR descriptors into concrete linguistic constituents based on the evidence of the corpus of “input” (i.e. normative) texts thus attempting at the standardization of CEFR descriptors. Secondly, from the ICALL perspective, CEFR-linked CB corpus can provide basis for comprehensive analysis of normative language that students at CEFR courses are being exposed to. This would, among other things, entail studies of vocabulary and grammar scopes per level; text and sentence readability experiments. Depending on the type of annotation, other studies might also be possible, for instance investigation of development in genre features and use of topics; change in type and format of exercises across levels; shifts in the focus on language skills across levels. Besides, experiments on topic modelling, automatic genre identification, analysis of text questions and text question generation, etc. could also become feasible. However evident the value of such data for ICALL and SLA might seem, there are very few attempts undertaken to compile corpora of (CEFR-based) coursebooks. François (2011) describes the only known to us CB corpus of CEFR-based texts stretching over all levels of proficiency.</abstract>
<note confidence="0.972213833333333">The main aim with François&apos; corpus is to use it for NLP-based CALL applications for L2 French. The corpus consists of 21 coursebooks distributed over the 6 proficiency levels, see Table 1: A1 A2 B1 B2 C1 C2 Total Nr textbooks 10 8 8 4 3 3 36 Nr texts 452 478 681 198 184 49 2042 Overview over the French CB corpus (François, 2011)</note>
<abstract confidence="0.993944017699115">All CBs have been published after 2001, have an explicit link to the CEFR levels of proficiency and are aimed at general L2 French (as opposed to French for specific purposes). After scanning, only reading materials (i.e. texts properly) have been extracted, leaving aside exercises, lists, instructions, etc. found in the coursebooks. Texts have been labelled with the proficiency level of the (chapter of the) book where texts came from, and assigned a genre (e.g. dialogue, recipe, poem) and linguistic annotation (POS, lemmas). The corpus compiled by François has up to date 130 been used for readability studies of L2 French texts and for extraction of a graded lexicon aimed at L2 learners of French (François, 2011; François et al., 2014). 2 COCTAILL: collection and annotation on COCTAILL for modelling) was initiated in 2013 and has been funded partly by the Department of Swedish at the University of Gothenburg (UGOT), and partly by the Center for Language Technology, UGOT. The process of corpus compilation consisted of several stages, shortly presented in Figure 2 below: Overview of the CEFR-corpus creation Interviews with L2 teachers. identify candidate coursebooks, we have carried out interviews with teachers engaged in CEFR-based courses as well as studied course plans for such courses. Altogether, 7 teachers at different levels, schools and institutions have agreed to have an interview. A number of CBs have been named as being used at more than one level. In such cases, to decide the border between levels, we organized a CB workshop where trained teachers discussed such coursebooks with each other and suggested division. Corpus structuring &amp; purchase of coursebooks. that have been suggested by at least two teachers have been selected as core material. We have aimed at a balanced representation at each level with respect to the number of coursebooks per level. However, very few courses are offered at C1 and none at C2 levels that we know of, so the number of coursebooks at these levels differ from the others: 2 titles at C1 and none at C2, see section 2.1 for an overview of the corpus structure. Before books were purchased, we explored the of getting electronic versions from the publishers, but only the publishing house was willing to cooperate. However, the titles that Liber could provide have been named by only one teacher, and consequently have not been included into the final corpus. Optical scanning &amp; extraction of raw Once the books were purchased, optical scanning was ordered from an outside contractor. PDF alongside XML files were delivered as resulting output data. Raw text extracted from the XML files was used as the input for the next stages. Implementation of a coursebook At this stage we defined a taxonomy of textual and pedagogical features for annotation, as well as the format of the output data. Previously, no richly (pedagogically) annotated L2 coursebook corpora have been compiled. Therefore, there were no available editing tools to reuse. After experimenting with XML editors and DTD schemas, we have opted to develop our own editor as described in subsection 2.3. 131 Annotation for pedagogical and textual features manual work. Altogether, four people have been involved in the content annotation. Initial annotation of the first two CBs was performed to test the editor and to establish an acceptable taxonomy of textual and pedagogical variables, see section 2.2. In the next round, one more annotator was trained, and as a result, a number of revisions were suggested to improve the taxonomy of pedagogical and textual features. The introduced changes led to a necessity to revise the two initially annotated coursebooks. By the end of this round, annotation guidelines have been produced. Finally, two more annotators have been trained. This stage was concluded by an inter-annotator agreement experiments, which entailed revisions to the annotation guidelines and highlighted the need of another round of revision of the already annotated books, as described in section 2.4. Linguistic annotation the form of parts-of-speech, syntactic relations and lemmas has been automatically added using Korp web services (Borin et al., 2012). Whereas annotation of text passages and activity instructions holds good quality, we would need to assess annotation quality of all other types of information. The reason for that is the fact that tasks, lists, and language examples have an unpredictable structure – often incomplete sentences, or lists of mixed linguistic units, which tends to get a very low-level accuracy when it comes to e.g. parts of speech and dependency annotation. Release of the corpus. the corpus as a whole cannot be made freely available for download for copyright reasons, however, it is browsable for research purposes via Korp (Borin et al., 2012) with password protection. Besides, parts of the corpus in the form of a bag of sentences (as opposed to connected texts) for each proficiency level are released as 2.1 Corpus overview The COCTAILL consists of 12 coursebooks, 5 of which are used at more than one level. The corpus is balanced in the number of coursebooks per level (4 titles/level), except level C1 (2 titles/level). C2 level is not included in this corpus since it represents full language proficiency when learners “can understand with ease virtually everything heard or read” (COE, 2001:24), hence, from the point of view of linguistic modelling it corresponds to regular NS language. The summary of the corpus is presented in Table 2. Overview of the Swedish CEFR corpus 1Contact Elena Volodina &lt;elena.volodina@svenska.gu.se&gt; or Ildikó Pilán &lt;ildiko.pilan@svenska.gu.se&gt; to get access to the files. 132 The COCTAILL comprises a total of 708 589 tokens, about half of which belong to texts, the rest to activity instructions, tasks, lists and language examples. The columns “Nr. of sentences (texts)” and “Nr. of tokens (texts)” refer to sentences in texts only, other elements were excluded from these counts since they often contain smaller linguistic units than a full sentence. The amount of tasks in the corpus (a total of 1494) outnumbers the number of texts (1106). The largest amount of material in terms of texts and tasks is available for B1 and B2 levels. The values in Table 2 are meant primarily to give an idea of the size of the corpus, rather than present data from which generalizations about the CEFR levels can be made, since authors&apos; choice varied to a great extent as far as the division into lessons and the number of texts and tasks included per level are concerned. 2.2 Coursebook content annotation An overview over the taxonomy of textual and pedagogical annotation is provided in Figure 3. XML elements are shown on the left with their corresponding attributes on the right: Overview over the textual and pedagogical XML elements and their attributes each divided into foreword, copyright note, etc) and The running text in each been manually split into at comprehension and other types of information typical of coursebooks, such as whereby reading comprehension materials have annotated for features 2.2.1), and the rest of information for features 2.2.2). 2.2.1 Textual annotation we understand mark-up of text passages for have listed 28 text 3) which follow the CEFR guidelines (COE, 2001) in the first place, with modifications introduced as a result of our practical work on the first coursebooks (Volodina &amp; Johansson Kokkinakis, 2013). In general, we followed the recommendation to opt for a broader topic, e.g. if a text is about a crisis in some country, including military actions, and power probably be the best choice. In most cases, more than one topic has been applicable, in which case two or more topics have been assigned. In case there were no topics that corresponded to the text, we 133 considered adding new ones, see Table 3 for the alphabetic list of the topics we have been using so far. List of topics taxonomy of families comprised of four elements: following the taxonomy described in Johansson and Sandell Ring (2010) with slight modifications as a result of the work on the first annotated coursebooks (Volodina and Johansson 2013). Such a modification is the addition of the genre family contains genres (e.g. that were difficult to place into the other three Further subdivision of genre families into macrogenres is shown in Table 4.</abstract>
<title confidence="0.989908545454545">Narration Facts Evaluation Other Description Autobiography Advertisement Anecdote, joke Fiction Biography Argumentation Dialogue News article Demonstration Discussion Language tip Personal story Explanation Exposition Letter Facts Interpretation, exegesis Lyrics Geographical facts Notice, short message Historical facts Personal reflection Puzzle Instruction Persuasion Questionnaire Procedures Review Quotation Report Recipe</title>
<author confidence="0.594085">Rules Rhyme</author>
<abstract confidence="0.9507858">List of genre families and macrogenres can be discussed whether some of the can be moved to any of the other three families (e.g. the In a lot of cases, where there were no clear-cut genres, a combination of genres became an optimal solution, see Figure 4.</abstract>
<title confidence="0.90166">Animals • Arts • Clothes &amp; appearances • Crime &amp; punishment • Culture &amp; traditions</title>
<author confidence="0.894485">Daily life</author>
<affiliation confidence="0.6449825">Economy • Education</affiliation>
<title confidence="0.826603">Family &amp; relatives</title>
<author confidence="0.6111585">Famous people</author>
<intro confidence="0.648181">Free time, entertainment</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anping He</author>
</authors>
<title>Corpus-Based Evaluation of ELT textbooks. Paper presented at the joint conference of the American Association</title>
<date>2005</date>
<booktitle>of Applied Corpus Linguistics and the International Computer Archive of Modern and Medieval English,</booktitle>
<pages>12--15</pages>
<institution>University of Michigan.</institution>
<marker>He, 2005</marker>
<rawString>Anping He. (2005). Corpus-Based Evaluation of ELT textbooks. Paper presented at the joint conference of the American Association of Applied Corpus Linguistics and the International Computer Archive of Modern and Medieval English, 12-15 May 2005, University of Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Artstein Ron</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<pages>555--596</pages>
<marker>Ron, Poesio, 2008</marker>
<rawString>Artstein Ron &amp; Massimo Poesio. (2008). Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4): 555-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Attali Yigal</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e-rater v.2.</title>
<date>2006</date>
<journal>The Journal of Technology, Learning and Assessment,</journal>
<volume>4</volume>
<issue>3</issue>
<marker>Yigal, Burstein, 2006</marker>
<rawString>Attali Yigal &amp; Jill Burstein. (2006). Automated essay scoring with e-rater v.2. The Journal of Technology, Learning and Assessment, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bird Steven</author>
</authors>
<title>NLTK: the natural language toolkit.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions,</booktitle>
<pages>69--72</pages>
<marker>Steven, 2006</marker>
<rawString>Bird Steven. (2006). NLTK: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive presentation sessions, pp. 69-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Borin Lars</author>
<author>Markus Forsberg</author>
<author>Johan Roxendal</author>
</authors>
<title>Korp – the corpus infrastructure of Språkbanken.</title>
<date>2012</date>
<booktitle>Proceedings of LREC 2012. Istanbul: ELRA.</booktitle>
<pages>474--478</pages>
<marker>Lars, Forsberg, Roxendal, 2012</marker>
<rawString>Borin Lars, Markus Forsberg &amp; Johan Roxendal. (2012). Korp – the corpus infrastructure of Språkbanken. Proceedings of LREC 2012. Istanbul: ELRA. 474–478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Council of Europe</author>
</authors>
<title>The Common European Framework of Reference for Languages: Learning, Teaching, Assessment.</title>
<date>2001</date>
<publisher>Cambridge University Press.</publisher>
<marker>Europe, 2001</marker>
<rawString>Council of Europe (COE). (2001). The Common European Framework of Reference for Languages: Learning, Teaching, Assessment. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davies Mark</author>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring agreement for multinomial data.</title>
<date>1982</date>
<journal>Biometrics,</journal>
<volume>38</volume>
<issue>4</issue>
<pages>1047--1051</pages>
<marker>Mark, Fleiss, 1982</marker>
<rawString>Davies Mark &amp; Joseph L. Fleiss. (1982). Measuring agreement for multinomial data. Biometrics, 38(4): 1047–1051.</rawString>
</citation>
<citation valid="true">
<authors>
<author>François Thomas</author>
</authors>
<title>Les apports du traitement automatique du langage à la lisibilité du français langue étrangère, Ph.D. Thesis, Université Catholique de Louvain. Thesis Supervisors : Cédrick Fairon and Anne Catherine Simon.</title>
<date>2011</date>
<marker>Thomas, 2011</marker>
<rawString>François Thomas. (2011). Les apports du traitement automatique du langage à la lisibilité du français langue étrangère, Ph.D. Thesis, Université Catholique de Louvain. Thesis Supervisors : Cédrick Fairon and Anne Catherine Simon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>François Thomas</author>
<author>Nuria Gala</author>
</authors>
<title>Patrick Watrin &amp; Cédrick Fairon.</title>
<date>2014</date>
<booktitle>In the 9th International Conference on Language Resources and Evaluation (LREC 2014). Reykjavik,</booktitle>
<marker>Thomas, Gala, 2014</marker>
<rawString>François Thomas, Nuria Gala, Patrick Watrin &amp; Cédrick Fairon. (2014). FLELex: a graded lexical resource for French foreign learners. In the 9th International Conference on Language Resources and Evaluation (LREC 2014). Reykjavik, Iceland, 26-31 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gamson David A</author>
<author>Lu Xiaofei</author>
<author>Eckert Sarah Anne</author>
</authors>
<title>Challenging the research base of the common core state standards: A historical reanalysis of text complexity.</title>
<date>2013</date>
<journal>Educational Researcher,</journal>
<pages>42--7</pages>
<marker>A, Xiaofei, Anne, 2013</marker>
<rawString>Gamson David A., Lu Xiaofei, &amp; Eckert Sarah Anne. (2013). Challenging the research base of the common core state standards: A historical reanalysis of text complexity. Educational Researcher, 42(7):381-391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaccard Paul</author>
</authors>
<title>Nouvelles recherches sur la distribution florale.</title>
<date>1908</date>
<booktitle>Bulletin de la Societe Vaudoise des Sciences Naturelles,</booktitle>
<volume>44</volume>
<pages>223--270</pages>
<marker>Paul, 1908</marker>
<rawString>Jaccard Paul. (1908). Nouvelles recherches sur la distribution florale. Bulletin de la Societe Vaudoise des Sciences Naturelles, 44: 223-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hancke Julia</author>
<author>Detmar Meurers</author>
</authors>
<title>Exploring CEFR classification for German based on rich linguistic modeling.</title>
<date>2013</date>
<booktitle>Learner Corpus Research 2013, Book of Abstracts.</booktitle>
<pages>54--56</pages>
<location>Bergen,</location>
<marker>Julia, Meurers, 2013</marker>
<rawString>Hancke Julia &amp; Detmar Meurers. (2013). Exploring CEFR classification for German based on rich linguistic modeling. Learner Corpus Research 2013, Book of Abstracts. pp. 54-56. Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johansson Britt</author>
<author>Anniqa Sandell Ring</author>
</authors>
<title>Låt språket bära: genrepedagogiken i praktiken. Hallgren och Fallgren,</title>
<date>2010</date>
<location>Stockholm.</location>
<marker>Britt, Ring, 2010</marker>
<rawString>Johansson Britt &amp; Anniqa Sandell Ring. (2010). Låt språket bära: genrepedagogiken i praktiken. Hallgren och Fallgren, Stockholm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krippendorff Klaus</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology, chapter 12. Sage,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<marker>Klaus, 1980</marker>
<rawString>Krippendorff Klaus. (1980). Content Analysis: An Introduction to Its Methodology, chapter 12. Sage, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meunier Fanny</author>
<author>Gouverneur Céline</author>
</authors>
<title>The treatment of phraseology in ELT textbooks, In: Corpora in the Foreign Language Classroom. Selected papers from the</title>
<date>2007</date>
<journal>Language and Computers Series</journal>
<booktitle>Sixth International Conference on Teaching and Language Corpora (TaLC6), University of Granada,</booktitle>
<volume>61</volume>
<pages>4--7</pages>
<location>Encarnación</location>
<marker>Fanny, Céline, 2007</marker>
<rawString>Meunier Fanny &amp; Gouverneur Céline. (2007). The treatment of phraseology in ELT textbooks, In: Corpora in the Foreign Language Classroom. Selected papers from the Sixth International Conference on Teaching and Language Corpora (TaLC6), University of Granada, 4-7 July 2004, Encarnación H., Quereda L. and Santana J. ed(s), Amsterdamm &amp; New York, Rodopi, Language and Computers Series 61, p. 119-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meunier Fanny</author>
<author>Gouverneur Céline</author>
</authors>
<title>New types of corpora for new educational challenges: collecting, annotating and exploiting a corpus of textbook material, In: Corpora and Language Teaching, Aijmer, K. ed(s),</title>
<date>2009</date>
<pages>179--201</pages>
<location>Amsterdam &amp; Philadelphia, Benjamins,</location>
<marker>Fanny, Céline, 2009</marker>
<rawString>Meunier Fanny &amp; Gouverneur Céline. (2009). New types of corpora for new educational challenges: collecting, annotating and exploiting a corpus of textbook material, In: Corpora and Language Teaching, Aijmer, K. ed(s), Amsterdam &amp; Philadelphia, Benjamins, p. 179-201</rawString>
</citation>
<citation valid="true">
<authors>
<author>Passonneau Rebecca J</author>
</authors>
<title>Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC, Genoa,</booktitle>
<pages>831--836</pages>
<marker>J, 2006</marker>
<rawString>Passonneau Rebecca J. (2006). Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation. In Proceedings of LREC, Genoa, pp. 831–836.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reda Ghsoon</author>
</authors>
<title>English Coursebooks: Prototype Textsts and Basic Vocabulary Norms.</title>
<date>2003</date>
<journal>ELT Journal</journal>
<volume>57</volume>
<issue>3</issue>
<pages>260--268</pages>
<marker>Ghsoon, 2003</marker>
<rawString>Reda Ghsoon. (2003). English Coursebooks: Prototype Textsts and Basic Vocabulary Norms. ELT Journal 57(3): 260-268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Römer Ute</author>
</authors>
<title>Looking at Looking: Functions and Contexts of Progressives</title>
<date>2006</date>
<booktitle>The Changing Face of Corpus Linguistics. Papers from the 24th International Conference on English Language Research on Computerized Corpora (ICAME 24).</booktitle>
<pages>231--242</pages>
<editor>in Spoken English and &apos;School&apos; English. In: Renouf, Antoinette &amp; Andrew Kehoe (eds.).</editor>
<location>Amsterdam: Rodopi.</location>
<marker>Ute, 2006</marker>
<rawString>Römer Ute. (2006). Looking at Looking: Functions and Contexts of Progressives in Spoken English and &apos;School&apos; English. In: Renouf, Antoinette &amp; Andrew Kehoe (eds.). The Changing Face of Corpus Linguistics. Papers from the 24th International Conference on English Language Research on Computerized Corpora (ICAME 24). Amsterdam: Rodopi. p.231-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Singleton David</author>
</authors>
<title>Introduction: A Critical Look at the Critical Period in Second Language Acquisition Research, In</title>
<date>1995</date>
<publisher>Multilingual Matters, Ltd.</publisher>
<location>Avon:</location>
<marker>David, 1995</marker>
<rawString>Singleton David. (1995). Introduction: A Critical Look at the Critical Period in Second Language Acquisition Research, In Singleton D. &amp; Lengyel, Z. (Eds.), The Age Factor in Second Language Acquisition (1-29). Avon: Multilingual Matters, Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vajjala Sowmya</author>
<author>Detmar Meurers</author>
</authors>
<title>On The Applicability of Readability Models to Web Texts.</title>
<date>2013</date>
<booktitle>Proceedings of the Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR), ACL</booktitle>
<marker>Sowmya, Meurers, 2013</marker>
<rawString>Vajjala Sowmya &amp; Detmar Meurers. (2013). On The Applicability of Readability Models to Web Texts. Proceedings of the Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR), ACL 2013</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volodina Elena</author>
<author>Ildikó Pilán</author>
<author>Lars Borin</author>
<author>Therese Lindström Tiedemann</author>
</authors>
<title>A flexible language learning platform based on language resources and web services.</title>
<date>2014</date>
<booktitle>Proceedings of LREC 2014,</booktitle>
<location>Reykjavik, Iceland.</location>
<marker>Elena, Pilán, Borin, Tiedemann, 2014</marker>
<rawString>Volodina Elena, Ildikó Pilán, Lars Borin, &amp; Therese Lindström Tiedemann. (2014). A flexible language learning platform based on language resources and web services. Proceedings of LREC 2014, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volodina Elena</author>
<author>Sofie Johansson Kokkinakis</author>
</authors>
<title>Compiling a corpus of CEFR-related texts.</title>
<date>2013</date>
<booktitle>Proceedings of the Language Testing and CEFR conference,</booktitle>
<location>Antwerpen, Belgium,</location>
<marker>Elena, Kokkinakis, 2013</marker>
<rawString>Volodina Elena &amp; Sofie Johansson Kokkinakis. (2013). Compiling a corpus of CEFR-related texts. Proceedings of the Language Testing and CEFR conference, Antwerpen, Belgium, May 27-29, 2013.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
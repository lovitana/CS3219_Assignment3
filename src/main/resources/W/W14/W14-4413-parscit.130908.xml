<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.114016">
<title confidence="0.987476">
Generating Summaries of Line Graphs
</title>
<author confidence="0.999651">
Priscilla Moraes, Gabriel Sina, Kathleen McCoy and Sandra Carberry
</author>
<affiliation confidence="0.9988595">
Department of Computer and Information Sciences
University of Delaware, Newark, Delaware, USA
</affiliation>
<email confidence="0.798867">
[pmoraes  |gsina  |mccoy  |carberry]@udel.edu
</email>
<sectionHeader confidence="0.989717" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922333333334">
This demo presents a Natural Language Gener-
ation (NLG) system that generates summaries
of informational graphics, specifically simple
line graphs, present in popular media. The sys-
tem is intended to capture the high-level
knowledge conveyed by the graphic and its out-
standing visual features. It comprises a content
selection phase that extracts the most important
content of the graphic, an organization phase,
which orders the propositions in a coherent
manner, and a realization phase that uses the
text surrounding the article to make decisions
on the choice of lexical items and amount of ag-
gregation applied to the propositions to gener-
ate the summary of the graphic.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999479458333333">
Multimodal documents from online popular me-
dia often contain information graphics that aug-
ment the information found in the text. These
graphics, however, are inaccessible for visually
impaired users or in environments where the im-
age cannot be processed/displayed. Our system
captures the high-level content of the graphic and
produces a textual summary that conveys it. Fig-
ure 1 shows the system architecture.
The first step is the identification of the pres-
ence of a graphical image in the web page by a
Browser Helper Object (BHO) (Elzer et al., 2007).
If a graphic is present on the web page, the Graph-
ical Information Extraction Module (VEM)
(Chester &amp; Elzer, 2005) is triggered by the BHO
in order to extract the data from the image. The
VEM then produces an XML representation of the
graphic that is used by the Intention Recognition
Module (IRM) for simple bar charts (Elzer,
Green, Carberry, &amp; Hoffman, 2006), simple line
graphs (Wu, Carberry, Elzer, &amp; Chester, 2010)
and grouped bar charts (R. Burns, Carberry, &amp;
Elzer, 2010; R. Burns, Carberry, &amp; Schwartz,
2013; R. J. Burns, 2013). The XML representation
</bodyText>
<footnote confidence="0.874703">
1 http://ir.cis.udel.edu/~moraes/udgraphs
</footnote>
<bodyText confidence="0.999939866666667">
of the graphic, along with the intended message
identified by the IRM, is sent to the Generation
Module (GM), which produces a textual summary
of the most important content presented in the
graphic. The system produces an initial summary
and follow-up responses for simple bar charts
(Demir, Carberry, &amp; Elzer, 2009; Demir,
Carberry, &amp; McCoy, 2008) and this demo pre-
sents the GM for simple line graphs.
This demo focuses on presenting the generation
phase of the system. For that, we will demonstrate
the generation of summaries in the context of a
digital library that is available online1 and that
contains information graphics collected from
online popular media, along with the articles con-
taining the graphics. In addition, we have included
hand-generated XML representations for the
graphics (the current VEM is not fully robust). For
each article that contains a graph, the user can
choose to have access to the generated summary
by clicking on the “Generate summary” button
(highlighted in Figure 2). Figure 2 shows a screen-
shot on which the graph shown in Figure 3 has its
article featured.
For accessibility projects that may use our sys-
tem (applications developed for visually impaired
users, for example), the application might use a
combination of key strokes to allow user interac-
tion. The module of the system that is the focus of
this demo is the Generation Module.
</bodyText>
<figureCaption confidence="0.998143">
Figure 1: System Architecture
</figureCaption>
<page confidence="0.982294">
95
</page>
<note confidence="0.372743">
Proceedings of the 8th International Natural Language Generation Conference, pages 95–98,
Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.992158">
Figure 2: Digital library screenshot where we have added summary generation functionality.
</figureCaption>
<sectionHeader confidence="0.950028" genericHeader="method">
2 Generation Module
</sectionHeader>
<bodyText confidence="0.999932793103448">
For generating summaries of line graphs, the first
step is the selection of content. In order to select
the most important features of the line graph that
should be conveyed in the summary, the system
represents the intended message and the visual
features identified by a human subject experiment
(Greenbacker, Carberry, &amp; McCoy, 2011) using a
graph. A centrality-based algorithm, which is an
adapted version of PageRank (Page, Brin,
Motwani, &amp; Winograd, 1999), is then imple-
mented to select the most important information
(represented as nodes in the graph). This imple-
mentation allows semantic relationships between
propositions to be represented on the edges of the
graph. The core of the content selection frame-
work is to detect present outstanding visual fea-
tures in the graphic, along with its intended mes-
sage, in order to select nodes. Details in the con-
tent selection phase are available in the work pre-
sented at (P. S. Moraes, Carberry, &amp; McCoy,
2013).
The next phase is the organization of the se-
lected content. The organization phase works by
ordering the selected propositions such that the
delivered summary is fluent and coherent. The
summaries are organized having an introduction
section, a detailed section and a conclusion. The
introduction consists of overall information about
the line graph (the type of the graph, the entity be-
ing measured, the volatility of the graph and its
intended message). The identified trends are de-
scribed in the detail section. For this part of the
summary, pieces of the graphic that outstand due
to its visual features may be described first, being
followed by other trends. Finally, the conclusion
section of the summary presents computational
information about the graphic (overall value and
rate change, time span of the graphic, maximum
and minimum points and dates when they occur).
The strategies on organizing the summaries are
described in (P. Moraes, McCoy, &amp; Carberry,
2014).
The last step of the Generation Module is the
aggregation of propositions into more complex
sentences. This decision is usually left to the de-
signer’s choice on how much aggregation to per-
form when generating text. Some systems are de-
signed to generate simple text for people with low
reading abilities (Williams &amp; Reiter, 2005a). As
stated by (Williams &amp; Reiter, 2005b), most NLG
systems available generate text for high-skilled
users. Our system generates line graph summaries
that fit the reading level of the article in which the
line graph appears. We contend that users gener-
ally read articles from venues they feel comforta-
ble with reading. In this manner, we intrinsically
assess the user’s reading level without needing to
actively survey it.
</bodyText>
<figureCaption confidence="0.995949">
Figure 3: A line graph present in popular media.
</figureCaption>
<page confidence="0.948539">
96
</page>
<bodyText confidence="0.994982108695653">
The first step of the aggregation phase is to as-
sess the reading level of the article’s text. There is
a myriad of techniques to measure the reading
level of text. Much of them use machine learning
techniques in order to learn text constructions and
lexicalization used in different grade levels. As
presented in (P. Moraes et al., 2014), simpler and
well established reading level measurement tech-
niques suffice for our scenario. The work shows
that Flesh-Kincaid (Kincaid, Fishburne, Rogers,
&amp; Chissom, 1975) and SMOG (Laughlin, 1969)
provide the set of information needed by the sys-
tem in order to make decisions of syntactical text
complexity.
After assessing the reading level of the article,
the system then uses the text plan that applies to
the identified reading level. Text plans define
rules on Noun Phrase (NP) density and lexical
choice. When describing an entity, attributes of
this entity can be added to the NP as modifiers us-
ing either adjectives e.g. “a highly volatile rising
trend”, conjunctions e.g., “the rising trend is vol-
atile and steep” or relative clauses e.g. “a rising
trend, which is highly volatile”. When the modi-
fier of an NP is a Verb Phrase (VP), it is combined
using a relative clause e.g., “the line graph, which
presents the number of jackets sold in 2013...”
VPs can be modified by adverbs e.g., “the falling
trend is very steep”. The text plans apply rules
within sets of propositions that are grouped hier-
archically. The system then uses the appropriate
lexical items (highly volatile vs ups and downs;
conveys vs shows) and applies the appropriate
amount of aggregation in order to realize sen-
tences.
Figure 4: Pop up window with the resulting sum-
mary generated by the system.
Figure 4 and Figure 5 display the summaries
generated for a user whose reading level is 11th-
13th grade and 5th-7th grade respectively. From
these one can see the different aggregation and
lexical choice decisions made for the different
reading levels. The system also includes appropri-
ate pronominalization in order to avoid repetition
of the referring expressions (P. Moraes et al.,
2014).
</bodyText>
<figureCaption confidence="0.929352">
Figure 5: Example of a summary adapted to the
reading level of grades 5 to 7.
</figureCaption>
<bodyText confidence="0.9310652">
For the surface realization phase we use
FUF/SURGE (Elhadad &amp; Robin, 1999) to create
the templates for realization. The template are cre-
ated based on the text plans defined for a given
reading level, as described above.
</bodyText>
<sectionHeader confidence="0.998607" genericHeader="method">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.999982">
This paper presents the demonstration of the gen-
eration module of SIGHT. For the demo, the gen-
eration module works on a digital library that ar-
chives informational graphics collected from pop-
ular media available online. The aggregation
phase of the generation module tailors the syntac-
tical complexity of the generated text to that of the
article’s text in which the graphic appears.
An evaluation of the text summaries generated
at different reading level is presented at (P.
Moraes et al., 2014). It shows that, indeed, differ-
ent users have different preferences regarding dif-
ferent text designs.
</bodyText>
<sectionHeader confidence="0.999153" genericHeader="method">
4 Future Work
</sectionHeader>
<bodyText confidence="0.9997784">
A more automated way of defining a text plan for
a given reading level is under investigation. We
will explore techniques for learning how different
text constructions can affect reading measures and
then using these learned models when choosing an
</bodyText>
<page confidence="0.999034">
97
</page>
<bodyText confidence="0.999744142857143">
adjective over a relative clause for increasing the
NP density and use of passive voice, for example.
Choosing lexical items that are classified by
age is another possibility. We plan on investigat-
ing how the usage of word frequency by age/grade
level (Carroll, 1972) might influence the overall
generated summaries.
</bodyText>
<sectionHeader confidence="0.985377" genericHeader="conclusions">
5 Acknowledgement
</sectionHeader>
<bodyText confidence="0.983461333333333">
Gabriel Sina was supported by the Coor-
denação de Aperfeiçoamento de Pessoal de Nível
Superior from Brazil CAPES – in Portuguese.
</bodyText>
<sectionHeader confidence="0.984536" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994186375">
Burns, R., Carberry, S., &amp; Elzer, S. (2010). Visual and
spatial factors in a bayesian reasoning
framework for the recognition of intended
messages in grouped bar charts. Paper
presented at the Proceedings of the AAAI
Workshop on Visual Representations and
Reasoning.
Burns, R., Carberry, S., &amp; Schwartz, S. E. (2013).
Modeling a Graph Viewer&apos;s Effort in
Recognizing Messages Conveyed by Grouped
Bar Charts. Paper presented at the UMAP.
Burns, R. J. (2013). Automated intention recognition of
grouped bar charts in multimodal documents.
University of Delaware, Ann Arbor.
Retrieved from
http://search.proquest.com/docview/1318643
227?accountid=10457
Carroll, J. B. (1972). A New Word Frequency Book.
Elementary English, 49(7), pp. 1070-1074.
Chester, D., &amp; Elzer, S. (2005). Getting computers to
see information graphics so users do not have
to. Paper presented at the the Proceedings of
the 15th International Symposium on
Methodologies for Intelligent Systems.
Demir, S., Carberry, S., &amp; Elzer, S. (2009). Issues in
Realizing the Overall Message of a Bar Chart.
In N. Nicolov, G. Angelova &amp; R. Mitkov
(Eds.), Recent Advances in Natural Language
Processing V (pp. 311-320): John Benjamins.
Demir, S., Carberry, S., &amp; McCoy, K. F. (2008).
Generating textual summaries of bar charts.
Paper presented at the Proceedings of the
Fifth International Natural Language
Generation Conference, Stroudsburg, PA,
USA.
Elhadad, M., &amp; Robin, J. (1999). SURGE: a
comprehensive plug-in syntactic realization
component for text generation.
Computational Linguistics.
Elzer, S., Green, N., Carberry, S., &amp; Hoffman, J.
(2006). A Model of Perceptual Task Effort for
Bar Charts and its Role in Recognizing
Intention. International Journal on User
Modeling and User-Adapted Interaction,
16(1), 1-30.
Elzer, S., Schwartz, E., Carberry, S., Chester, D.,
Demir, S., &amp; Wu, P. (2007). A Browser
Extension For Providing Visually Impaired
Users Access To The Content Of Bar Charts
On The Web. Paper presented at the the
Proceedings of the International Conference
on Web Information Systems and
Technologies.
Greenbacker, C., Carberry, S., &amp; McCoy, K. (2011,
July). A Corpus of Human-written Summaries
of Line Graphs. Paper presented at the
Proceedings of the UCNLG+Eval: Language
Generation and Evaluation Workshop,
Edinburgh, Scotland.
Kincaid, J. P., Fishburne, R. P., Rogers, R. L., &amp;
Chissom, B. S. (1975). Derivation of New
Readability Formulas (Automated
Readability Index, Fog Count and Flesch
Reading Ease Formula) for Navy Enlisted
Personnel.
Laughlin, G. H. M. (1969). SMOG Grading-a New
Readability Formula. Journal of Reading,
12(8), pp. 639-646.
Moraes, P., McCoy, K., &amp; Carberry, S. (2014).
Adapting Graph Summaries to the Users’
Reading Levels. Paper presented at the
Proceedings of the 8th International Natural
Language Generation Conference.
Moraes, P. S., Carberry, S., &amp; McCoy, K. (2013).
Providing access to the high-level content of
line graphs from online popular media. Paper
presented at the Proceedings of the 10th
International Cross-Disciplinary Conference
on Web Accessibility, Rio de Janeiro, Brazil.
Page, L., Brin, S., Motwani, R., &amp; Winograd, T.
(1999). The PageRank Citation Ranking:
Bringing Order to the Web: Stanford InfoLab.
Williams, S., &amp; Reiter, E. (2005a). Appropriate
Microplanning Choices for Low-Skilled
Readers. Paper presented at the IJCAI.
Williams, S., &amp; Reiter, E. (2005b). Generating
readable texts for readers with low basic
skills. Paper presented at the Proceedings of
the 10th European Workshop on Natural
Language Generation (EWNLG 2005).
Wu, P., Carberry, S., Elzer, S., &amp; Chester, D. (2010).
Recognizing the intended message of line
graphs. Paper presented at the Proceedings of
the 6th international conference on
Diagrammatic representation and inference,
Berlin, Heidelberg.
</reference>
<page confidence="0.996224">
98
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.603860">
<title confidence="0.999952">Generating Summaries of Line Graphs</title>
<author confidence="0.996495">Priscilla Moraes</author>
<author confidence="0.996495">Gabriel Sina</author>
<author confidence="0.996495">Kathleen McCoy</author>
<author confidence="0.996495">Sandra</author>
<affiliation confidence="0.8115825">Department of Computer and Information University of Delaware, Newark, Delaware, USA</affiliation>
<email confidence="0.992069">[pmoraes|gsina|mccoy|carberry]@udel.edu</email>
<abstract confidence="0.998788">This demo presents a Natural Language Generation (NLG) system that generates summaries of informational graphics, specifically simple line graphs, present in popular media. The system is intended to capture the high-level knowledge conveyed by the graphic and its outstanding visual features. It comprises a content selection phase that extracts the most important content of the graphic, an organization phase, which orders the propositions in a coherent manner, and a realization phase that uses the text surrounding the article to make decisions on the choice of lexical items and amount of aggregation applied to the propositions to generate the summary of the graphic.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Burns</author>
<author>S Carberry</author>
<author>S Elzer</author>
</authors>
<title>Visual and spatial factors in a bayesian reasoning framework for the recognition of intended messages in grouped bar charts.</title>
<date>2010</date>
<booktitle>Paper presented at the Proceedings of the AAAI Workshop on Visual Representations and Reasoning.</booktitle>
<contexts>
<context position="1973" citStr="Burns, Carberry, &amp; Elzer, 2010" startWordPosition="308" endWordPosition="312">cture. The first step is the identification of the presence of a graphical image in the web page by a Browser Helper Object (BHO) (Elzer et al., 2007). If a graphic is present on the web page, the Graphical Information Extraction Module (VEM) (Chester &amp; Elzer, 2005) is triggered by the BHO in order to extract the data from the image. The VEM then produces an XML representation of the graphic that is used by the Intention Recognition Module (IRM) for simple bar charts (Elzer, Green, Carberry, &amp; Hoffman, 2006), simple line graphs (Wu, Carberry, Elzer, &amp; Chester, 2010) and grouped bar charts (R. Burns, Carberry, &amp; Elzer, 2010; R. Burns, Carberry, &amp; Schwartz, 2013; R. J. Burns, 2013). The XML representation 1 http://ir.cis.udel.edu/~moraes/udgraphs of the graphic, along with the intended message identified by the IRM, is sent to the Generation Module (GM), which produces a textual summary of the most important content presented in the graphic. The system produces an initial summary and follow-up responses for simple bar charts (Demir, Carberry, &amp; Elzer, 2009; Demir, Carberry, &amp; McCoy, 2008) and this demo presents the GM for simple line graphs. This demo focuses on presenting the generation phase of the system. For </context>
</contexts>
<marker>Burns, Carberry, Elzer, 2010</marker>
<rawString>Burns, R., Carberry, S., &amp; Elzer, S. (2010). Visual and spatial factors in a bayesian reasoning framework for the recognition of intended messages in grouped bar charts. Paper presented at the Proceedings of the AAAI Workshop on Visual Representations and Reasoning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Burns</author>
<author>S Carberry</author>
<author>S E Schwartz</author>
</authors>
<title>Modeling a Graph Viewer&apos;s Effort in Recognizing Messages Conveyed by Grouped Bar Charts. Paper presented at the UMAP.</title>
<date>2013</date>
<contexts>
<context position="2011" citStr="Burns, Carberry, &amp; Schwartz, 2013" startWordPosition="314" endWordPosition="318">fication of the presence of a graphical image in the web page by a Browser Helper Object (BHO) (Elzer et al., 2007). If a graphic is present on the web page, the Graphical Information Extraction Module (VEM) (Chester &amp; Elzer, 2005) is triggered by the BHO in order to extract the data from the image. The VEM then produces an XML representation of the graphic that is used by the Intention Recognition Module (IRM) for simple bar charts (Elzer, Green, Carberry, &amp; Hoffman, 2006), simple line graphs (Wu, Carberry, Elzer, &amp; Chester, 2010) and grouped bar charts (R. Burns, Carberry, &amp; Elzer, 2010; R. Burns, Carberry, &amp; Schwartz, 2013; R. J. Burns, 2013). The XML representation 1 http://ir.cis.udel.edu/~moraes/udgraphs of the graphic, along with the intended message identified by the IRM, is sent to the Generation Module (GM), which produces a textual summary of the most important content presented in the graphic. The system produces an initial summary and follow-up responses for simple bar charts (Demir, Carberry, &amp; Elzer, 2009; Demir, Carberry, &amp; McCoy, 2008) and this demo presents the GM for simple line graphs. This demo focuses on presenting the generation phase of the system. For that, we will demonstrate the generati</context>
</contexts>
<marker>Burns, Carberry, Schwartz, 2013</marker>
<rawString>Burns, R., Carberry, S., &amp; Schwartz, S. E. (2013). Modeling a Graph Viewer&apos;s Effort in Recognizing Messages Conveyed by Grouped Bar Charts. Paper presented at the UMAP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Burns</author>
</authors>
<title>Automated intention recognition of grouped bar charts in multimodal documents.</title>
<date>2013</date>
<institution>University of Delaware,</institution>
<location>Ann Arbor.</location>
<contexts>
<context position="2031" citStr="Burns, 2013" startWordPosition="321" endWordPosition="322">mage in the web page by a Browser Helper Object (BHO) (Elzer et al., 2007). If a graphic is present on the web page, the Graphical Information Extraction Module (VEM) (Chester &amp; Elzer, 2005) is triggered by the BHO in order to extract the data from the image. The VEM then produces an XML representation of the graphic that is used by the Intention Recognition Module (IRM) for simple bar charts (Elzer, Green, Carberry, &amp; Hoffman, 2006), simple line graphs (Wu, Carberry, Elzer, &amp; Chester, 2010) and grouped bar charts (R. Burns, Carberry, &amp; Elzer, 2010; R. Burns, Carberry, &amp; Schwartz, 2013; R. J. Burns, 2013). The XML representation 1 http://ir.cis.udel.edu/~moraes/udgraphs of the graphic, along with the intended message identified by the IRM, is sent to the Generation Module (GM), which produces a textual summary of the most important content presented in the graphic. The system produces an initial summary and follow-up responses for simple bar charts (Demir, Carberry, &amp; Elzer, 2009; Demir, Carberry, &amp; McCoy, 2008) and this demo presents the GM for simple line graphs. This demo focuses on presenting the generation phase of the system. For that, we will demonstrate the generation of summaries in t</context>
</contexts>
<marker>Burns, 2013</marker>
<rawString>Burns, R. J. (2013). Automated intention recognition of grouped bar charts in multimodal documents. University of Delaware, Ann Arbor.</rawString>
</citation>
<citation valid="false">
<note>Retrieved from http://search.proquest.com/docview/1318643 227?accountid=10457</note>
<marker></marker>
<rawString>Retrieved from http://search.proquest.com/docview/1318643 227?accountid=10457</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Carroll</author>
</authors>
<title>A New Word Frequency Book.</title>
<date>1972</date>
<journal>Elementary English,</journal>
<volume>49</volume>
<issue>7</issue>
<pages>1070--1074</pages>
<marker>Carroll, 1972</marker>
<rawString>Carroll, J. B. (1972). A New Word Frequency Book. Elementary English, 49(7), pp. 1070-1074.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chester</author>
<author>S Elzer</author>
</authors>
<title>Getting computers to see information graphics so users do not have to.</title>
<date>2005</date>
<booktitle>Paper presented at the the Proceedings of the 15th International Symposium on Methodologies for Intelligent Systems.</booktitle>
<contexts>
<context position="1609" citStr="Chester &amp; Elzer, 2005" startWordPosition="247" endWordPosition="250">ten contain information graphics that augment the information found in the text. These graphics, however, are inaccessible for visually impaired users or in environments where the image cannot be processed/displayed. Our system captures the high-level content of the graphic and produces a textual summary that conveys it. Figure 1 shows the system architecture. The first step is the identification of the presence of a graphical image in the web page by a Browser Helper Object (BHO) (Elzer et al., 2007). If a graphic is present on the web page, the Graphical Information Extraction Module (VEM) (Chester &amp; Elzer, 2005) is triggered by the BHO in order to extract the data from the image. The VEM then produces an XML representation of the graphic that is used by the Intention Recognition Module (IRM) for simple bar charts (Elzer, Green, Carberry, &amp; Hoffman, 2006), simple line graphs (Wu, Carberry, Elzer, &amp; Chester, 2010) and grouped bar charts (R. Burns, Carberry, &amp; Elzer, 2010; R. Burns, Carberry, &amp; Schwartz, 2013; R. J. Burns, 2013). The XML representation 1 http://ir.cis.udel.edu/~moraes/udgraphs of the graphic, along with the intended message identified by the IRM, is sent to the Generation Module (GM), w</context>
</contexts>
<marker>Chester, Elzer, 2005</marker>
<rawString>Chester, D., &amp; Elzer, S. (2005). Getting computers to see information graphics so users do not have to. Paper presented at the the Proceedings of the 15th International Symposium on Methodologies for Intelligent Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Demir</author>
<author>S Carberry</author>
<author>S Elzer</author>
</authors>
<title>Issues in Realizing the Overall Message of a Bar Chart. In</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing V</booktitle>
<pages>311--320</pages>
<institution>John Benjamins.</institution>
<contexts>
<context position="2413" citStr="Demir, Carberry, &amp; Elzer, 2009" startWordPosition="374" endWordPosition="378">ule (IRM) for simple bar charts (Elzer, Green, Carberry, &amp; Hoffman, 2006), simple line graphs (Wu, Carberry, Elzer, &amp; Chester, 2010) and grouped bar charts (R. Burns, Carberry, &amp; Elzer, 2010; R. Burns, Carberry, &amp; Schwartz, 2013; R. J. Burns, 2013). The XML representation 1 http://ir.cis.udel.edu/~moraes/udgraphs of the graphic, along with the intended message identified by the IRM, is sent to the Generation Module (GM), which produces a textual summary of the most important content presented in the graphic. The system produces an initial summary and follow-up responses for simple bar charts (Demir, Carberry, &amp; Elzer, 2009; Demir, Carberry, &amp; McCoy, 2008) and this demo presents the GM for simple line graphs. This demo focuses on presenting the generation phase of the system. For that, we will demonstrate the generation of summaries in the context of a digital library that is available online1 and that contains information graphics collected from online popular media, along with the articles containing the graphics. In addition, we have included hand-generated XML representations for the graphics (the current VEM is not fully robust). For each article that contains a graph, the user can choose to have access to </context>
</contexts>
<marker>Demir, Carberry, Elzer, 2009</marker>
<rawString>Demir, S., Carberry, S., &amp; Elzer, S. (2009). Issues in Realizing the Overall Message of a Bar Chart. In N. Nicolov, G. Angelova &amp; R. Mitkov (Eds.), Recent Advances in Natural Language Processing V (pp. 311-320): John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Demir</author>
<author>S Carberry</author>
<author>K F McCoy</author>
</authors>
<title>Generating textual summaries of bar charts.</title>
<date>2008</date>
<booktitle>Paper presented at the Proceedings of the Fifth International Natural Language Generation Conference,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2445" citStr="Demir, Carberry, &amp; McCoy, 2008" startWordPosition="379" endWordPosition="383">(Elzer, Green, Carberry, &amp; Hoffman, 2006), simple line graphs (Wu, Carberry, Elzer, &amp; Chester, 2010) and grouped bar charts (R. Burns, Carberry, &amp; Elzer, 2010; R. Burns, Carberry, &amp; Schwartz, 2013; R. J. Burns, 2013). The XML representation 1 http://ir.cis.udel.edu/~moraes/udgraphs of the graphic, along with the intended message identified by the IRM, is sent to the Generation Module (GM), which produces a textual summary of the most important content presented in the graphic. The system produces an initial summary and follow-up responses for simple bar charts (Demir, Carberry, &amp; Elzer, 2009; Demir, Carberry, &amp; McCoy, 2008) and this demo presents the GM for simple line graphs. This demo focuses on presenting the generation phase of the system. For that, we will demonstrate the generation of summaries in the context of a digital library that is available online1 and that contains information graphics collected from online popular media, along with the articles containing the graphics. In addition, we have included hand-generated XML representations for the graphics (the current VEM is not fully robust). For each article that contains a graph, the user can choose to have access to the generated summary by clickin</context>
</contexts>
<marker>Demir, Carberry, McCoy, 2008</marker>
<rawString>Demir, S., Carberry, S., &amp; McCoy, K. F. (2008). Generating textual summaries of bar charts. Paper presented at the Proceedings of the Fifth International Natural Language Generation Conference, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elhadad</author>
<author>J Robin</author>
</authors>
<title>SURGE: a comprehensive plug-in syntactic realization component for text generation. Computational Linguistics.</title>
<date>1999</date>
<contexts>
<context position="8813" citStr="Elhadad &amp; Robin, 1999" startWordPosition="1419" endWordPosition="1422">s. Figure 4: Pop up window with the resulting summary generated by the system. Figure 4 and Figure 5 display the summaries generated for a user whose reading level is 11th13th grade and 5th-7th grade respectively. From these one can see the different aggregation and lexical choice decisions made for the different reading levels. The system also includes appropriate pronominalization in order to avoid repetition of the referring expressions (P. Moraes et al., 2014). Figure 5: Example of a summary adapted to the reading level of grades 5 to 7. For the surface realization phase we use FUF/SURGE (Elhadad &amp; Robin, 1999) to create the templates for realization. The template are created based on the text plans defined for a given reading level, as described above. 3 Conclusion This paper presents the demonstration of the generation module of SIGHT. For the demo, the generation module works on a digital library that archives informational graphics collected from popular media available online. The aggregation phase of the generation module tailors the syntactical complexity of the generated text to that of the article’s text in which the graphic appears. An evaluation of the text summaries generated at differen</context>
</contexts>
<marker>Elhadad, Robin, 1999</marker>
<rawString>Elhadad, M., &amp; Robin, J. (1999). SURGE: a comprehensive plug-in syntactic realization component for text generation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Elzer</author>
<author>N Green</author>
<author>S Carberry</author>
<author>J Hoffman</author>
</authors>
<title>A Model of Perceptual Task Effort for Bar Charts and its Role in Recognizing Intention.</title>
<date>2006</date>
<booktitle>International Journal on User Modeling and User-Adapted Interaction,</booktitle>
<volume>16</volume>
<issue>1</issue>
<pages>1--30</pages>
<contexts>
<context position="1855" citStr="Elzer, Green, Carberry, &amp; Hoffman, 2006" startWordPosition="288" endWordPosition="293">aptures the high-level content of the graphic and produces a textual summary that conveys it. Figure 1 shows the system architecture. The first step is the identification of the presence of a graphical image in the web page by a Browser Helper Object (BHO) (Elzer et al., 2007). If a graphic is present on the web page, the Graphical Information Extraction Module (VEM) (Chester &amp; Elzer, 2005) is triggered by the BHO in order to extract the data from the image. The VEM then produces an XML representation of the graphic that is used by the Intention Recognition Module (IRM) for simple bar charts (Elzer, Green, Carberry, &amp; Hoffman, 2006), simple line graphs (Wu, Carberry, Elzer, &amp; Chester, 2010) and grouped bar charts (R. Burns, Carberry, &amp; Elzer, 2010; R. Burns, Carberry, &amp; Schwartz, 2013; R. J. Burns, 2013). The XML representation 1 http://ir.cis.udel.edu/~moraes/udgraphs of the graphic, along with the intended message identified by the IRM, is sent to the Generation Module (GM), which produces a textual summary of the most important content presented in the graphic. The system produces an initial summary and follow-up responses for simple bar charts (Demir, Carberry, &amp; Elzer, 2009; Demir, Carberry, &amp; McCoy, 2008) and this</context>
</contexts>
<marker>Elzer, Green, Carberry, Hoffman, 2006</marker>
<rawString>Elzer, S., Green, N., Carberry, S., &amp; Hoffman, J. (2006). A Model of Perceptual Task Effort for Bar Charts and its Role in Recognizing Intention. International Journal on User Modeling and User-Adapted Interaction, 16(1), 1-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Elzer</author>
<author>E Schwartz</author>
<author>S Carberry</author>
<author>D Chester</author>
<author>S Demir</author>
<author>P Wu</author>
</authors>
<title>A Browser Extension For Providing Visually Impaired Users Access To The Content Of Bar Charts On The Web. Paper presented at the the</title>
<date>2007</date>
<booktitle>Proceedings of the International Conference on Web Information Systems and Technologies.</booktitle>
<contexts>
<context position="1493" citStr="Elzer et al., 2007" startWordPosition="227" endWordPosition="230">ositions to generate the summary of the graphic. 1 Introduction Multimodal documents from online popular media often contain information graphics that augment the information found in the text. These graphics, however, are inaccessible for visually impaired users or in environments where the image cannot be processed/displayed. Our system captures the high-level content of the graphic and produces a textual summary that conveys it. Figure 1 shows the system architecture. The first step is the identification of the presence of a graphical image in the web page by a Browser Helper Object (BHO) (Elzer et al., 2007). If a graphic is present on the web page, the Graphical Information Extraction Module (VEM) (Chester &amp; Elzer, 2005) is triggered by the BHO in order to extract the data from the image. The VEM then produces an XML representation of the graphic that is used by the Intention Recognition Module (IRM) for simple bar charts (Elzer, Green, Carberry, &amp; Hoffman, 2006), simple line graphs (Wu, Carberry, Elzer, &amp; Chester, 2010) and grouped bar charts (R. Burns, Carberry, &amp; Elzer, 2010; R. Burns, Carberry, &amp; Schwartz, 2013; R. J. Burns, 2013). The XML representation 1 http://ir.cis.udel.edu/~moraes/udgr</context>
</contexts>
<marker>Elzer, Schwartz, Carberry, Chester, Demir, Wu, 2007</marker>
<rawString>Elzer, S., Schwartz, E., Carberry, S., Chester, D., Demir, S., &amp; Wu, P. (2007). A Browser Extension For Providing Visually Impaired Users Access To The Content Of Bar Charts On The Web. Paper presented at the the Proceedings of the International Conference on Web Information Systems and Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Greenbacker</author>
<author>S Carberry</author>
<author>K McCoy</author>
</authors>
<title>A Corpus of Human-written Summaries of Line Graphs. Paper presented at the Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop,</title>
<date>2011</date>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="4143" citStr="Greenbacker, Carberry, &amp; McCoy, 2011" startWordPosition="646" endWordPosition="650">re 95 Proceedings of the 8th International Natural Language Generation Conference, pages 95–98, Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics Figure 2: Digital library screenshot where we have added summary generation functionality. 2 Generation Module For generating summaries of line graphs, the first step is the selection of content. In order to select the most important features of the line graph that should be conveyed in the summary, the system represents the intended message and the visual features identified by a human subject experiment (Greenbacker, Carberry, &amp; McCoy, 2011) using a graph. A centrality-based algorithm, which is an adapted version of PageRank (Page, Brin, Motwani, &amp; Winograd, 1999), is then implemented to select the most important information (represented as nodes in the graph). This implementation allows semantic relationships between propositions to be represented on the edges of the graph. The core of the content selection framework is to detect present outstanding visual features in the graphic, along with its intended message, in order to select nodes. Details in the content selection phase are available in the work presented at (P. S. Morae</context>
</contexts>
<marker>Greenbacker, Carberry, McCoy, 2011</marker>
<rawString>Greenbacker, C., Carberry, S., &amp; McCoy, K. (2011, July). A Corpus of Human-written Summaries of Line Graphs. Paper presented at the Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Kincaid</author>
<author>R P Fishburne</author>
<author>R L Rogers</author>
<author>B S Chissom</author>
</authors>
<title>Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel.</title>
<date>1975</date>
<contexts>
<context position="7060" citStr="Kincaid, Fishburne, Rogers, &amp; Chissom, 1975" startWordPosition="1120" endWordPosition="1125">rinsically assess the user’s reading level without needing to actively survey it. Figure 3: A line graph present in popular media. 96 The first step of the aggregation phase is to assess the reading level of the article’s text. There is a myriad of techniques to measure the reading level of text. Much of them use machine learning techniques in order to learn text constructions and lexicalization used in different grade levels. As presented in (P. Moraes et al., 2014), simpler and well established reading level measurement techniques suffice for our scenario. The work shows that Flesh-Kincaid (Kincaid, Fishburne, Rogers, &amp; Chissom, 1975) and SMOG (Laughlin, 1969) provide the set of information needed by the system in order to make decisions of syntactical text complexity. After assessing the reading level of the article, the system then uses the text plan that applies to the identified reading level. Text plans define rules on Noun Phrase (NP) density and lexical choice. When describing an entity, attributes of this entity can be added to the NP as modifiers using either adjectives e.g. “a highly volatile rising trend”, conjunctions e.g., “the rising trend is volatile and steep” or relative clauses e.g. “a rising trend, whic</context>
</contexts>
<marker>Kincaid, Fishburne, Rogers, Chissom, 1975</marker>
<rawString>Kincaid, J. P., Fishburne, R. P., Rogers, R. L., &amp; Chissom, B. S. (1975). Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H M Laughlin</author>
</authors>
<title>SMOG Grading-a New Readability Formula.</title>
<date>1969</date>
<journal>Journal of Reading,</journal>
<volume>12</volume>
<issue>8</issue>
<pages>639--646</pages>
<contexts>
<context position="7087" citStr="Laughlin, 1969" startWordPosition="1128" endWordPosition="1129">ing to actively survey it. Figure 3: A line graph present in popular media. 96 The first step of the aggregation phase is to assess the reading level of the article’s text. There is a myriad of techniques to measure the reading level of text. Much of them use machine learning techniques in order to learn text constructions and lexicalization used in different grade levels. As presented in (P. Moraes et al., 2014), simpler and well established reading level measurement techniques suffice for our scenario. The work shows that Flesh-Kincaid (Kincaid, Fishburne, Rogers, &amp; Chissom, 1975) and SMOG (Laughlin, 1969) provide the set of information needed by the system in order to make decisions of syntactical text complexity. After assessing the reading level of the article, the system then uses the text plan that applies to the identified reading level. Text plans define rules on Noun Phrase (NP) density and lexical choice. When describing an entity, attributes of this entity can be added to the NP as modifiers using either adjectives e.g. “a highly volatile rising trend”, conjunctions e.g., “the rising trend is volatile and steep” or relative clauses e.g. “a rising trend, which is highly volatile”. When</context>
</contexts>
<marker>Laughlin, 1969</marker>
<rawString>Laughlin, G. H. M. (1969). SMOG Grading-a New Readability Formula. Journal of Reading, 12(8), pp. 639-646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Moraes</author>
<author>K McCoy</author>
<author>S Carberry</author>
</authors>
<title>Adapting Graph Summaries to the Users’ Reading Levels.</title>
<date>2014</date>
<booktitle>Paper presented at the Proceedings of the 8th International Natural Language Generation Conference.</booktitle>
<contexts>
<context position="5744" citStr="Moraes, McCoy, &amp; Carberry, 2014" startWordPosition="905" endWordPosition="909">ne graph (the type of the graph, the entity being measured, the volatility of the graph and its intended message). The identified trends are described in the detail section. For this part of the summary, pieces of the graphic that outstand due to its visual features may be described first, being followed by other trends. Finally, the conclusion section of the summary presents computational information about the graphic (overall value and rate change, time span of the graphic, maximum and minimum points and dates when they occur). The strategies on organizing the summaries are described in (P. Moraes, McCoy, &amp; Carberry, 2014). The last step of the Generation Module is the aggregation of propositions into more complex sentences. This decision is usually left to the designer’s choice on how much aggregation to perform when generating text. Some systems are designed to generate simple text for people with low reading abilities (Williams &amp; Reiter, 2005a). As stated by (Williams &amp; Reiter, 2005b), most NLG systems available generate text for high-skilled users. Our system generates line graph summaries that fit the reading level of the article in which the line graph appears. We contend that users generally read articl</context>
<context position="6888" citStr="Moraes et al., 2014" startWordPosition="1098" endWordPosition="1101">in which the line graph appears. We contend that users generally read articles from venues they feel comfortable with reading. In this manner, we intrinsically assess the user’s reading level without needing to actively survey it. Figure 3: A line graph present in popular media. 96 The first step of the aggregation phase is to assess the reading level of the article’s text. There is a myriad of techniques to measure the reading level of text. Much of them use machine learning techniques in order to learn text constructions and lexicalization used in different grade levels. As presented in (P. Moraes et al., 2014), simpler and well established reading level measurement techniques suffice for our scenario. The work shows that Flesh-Kincaid (Kincaid, Fishburne, Rogers, &amp; Chissom, 1975) and SMOG (Laughlin, 1969) provide the set of information needed by the system in order to make decisions of syntactical text complexity. After assessing the reading level of the article, the system then uses the text plan that applies to the identified reading level. Text plans define rules on Noun Phrase (NP) density and lexical choice. When describing an entity, attributes of this entity can be added to the NP as modifie</context>
<context position="8659" citStr="Moraes et al., 2014" startWordPosition="1391" endWordPosition="1394">priate lexical items (highly volatile vs ups and downs; conveys vs shows) and applies the appropriate amount of aggregation in order to realize sentences. Figure 4: Pop up window with the resulting summary generated by the system. Figure 4 and Figure 5 display the summaries generated for a user whose reading level is 11th13th grade and 5th-7th grade respectively. From these one can see the different aggregation and lexical choice decisions made for the different reading levels. The system also includes appropriate pronominalization in order to avoid repetition of the referring expressions (P. Moraes et al., 2014). Figure 5: Example of a summary adapted to the reading level of grades 5 to 7. For the surface realization phase we use FUF/SURGE (Elhadad &amp; Robin, 1999) to create the templates for realization. The template are created based on the text plans defined for a given reading level, as described above. 3 Conclusion This paper presents the demonstration of the generation module of SIGHT. For the demo, the generation module works on a digital library that archives informational graphics collected from popular media available online. The aggregation phase of the generation module tailors the syntacti</context>
</contexts>
<marker>Moraes, McCoy, Carberry, 2014</marker>
<rawString>Moraes, P., McCoy, K., &amp; Carberry, S. (2014). Adapting Graph Summaries to the Users’ Reading Levels. Paper presented at the Proceedings of the 8th International Natural Language Generation Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P S Moraes</author>
<author>S Carberry</author>
<author>K McCoy</author>
</authors>
<title>Providing access to the high-level content of line graphs from online popular media.</title>
<date>2013</date>
<booktitle>Paper presented at the Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility, Rio de Janeiro,</booktitle>
<location>Brazil.</location>
<contexts>
<context position="4769" citStr="Moraes, Carberry, &amp; McCoy, 2013" startWordPosition="750" endWordPosition="754"> 2011) using a graph. A centrality-based algorithm, which is an adapted version of PageRank (Page, Brin, Motwani, &amp; Winograd, 1999), is then implemented to select the most important information (represented as nodes in the graph). This implementation allows semantic relationships between propositions to be represented on the edges of the graph. The core of the content selection framework is to detect present outstanding visual features in the graphic, along with its intended message, in order to select nodes. Details in the content selection phase are available in the work presented at (P. S. Moraes, Carberry, &amp; McCoy, 2013). The next phase is the organization of the selected content. The organization phase works by ordering the selected propositions such that the delivered summary is fluent and coherent. The summaries are organized having an introduction section, a detailed section and a conclusion. The introduction consists of overall information about the line graph (the type of the graph, the entity being measured, the volatility of the graph and its intended message). The identified trends are described in the detail section. For this part of the summary, pieces of the graphic that outstand due to its visua</context>
</contexts>
<marker>Moraes, Carberry, McCoy, 2013</marker>
<rawString>Moraes, P. S., Carberry, S., &amp; McCoy, K. (2013). Providing access to the high-level content of line graphs from online popular media. Paper presented at the Proceedings of the 10th International Cross-Disciplinary Conference on Web Accessibility, Rio de Janeiro, Brazil.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Page</author>
<author>S Brin</author>
<author>R Motwani</author>
<author>T Winograd</author>
</authors>
<title>The PageRank Citation Ranking: Bringing Order to the Web: Stanford InfoLab.</title>
<date>1999</date>
<contexts>
<context position="4268" citStr="Page, Brin, Motwani, &amp; Winograd, 1999" startWordPosition="664" endWordPosition="669">19-21 June 2014. c�2014 Association for Computational Linguistics Figure 2: Digital library screenshot where we have added summary generation functionality. 2 Generation Module For generating summaries of line graphs, the first step is the selection of content. In order to select the most important features of the line graph that should be conveyed in the summary, the system represents the intended message and the visual features identified by a human subject experiment (Greenbacker, Carberry, &amp; McCoy, 2011) using a graph. A centrality-based algorithm, which is an adapted version of PageRank (Page, Brin, Motwani, &amp; Winograd, 1999), is then implemented to select the most important information (represented as nodes in the graph). This implementation allows semantic relationships between propositions to be represented on the edges of the graph. The core of the content selection framework is to detect present outstanding visual features in the graphic, along with its intended message, in order to select nodes. Details in the content selection phase are available in the work presented at (P. S. Moraes, Carberry, &amp; McCoy, 2013). The next phase is the organization of the selected content. The organization phase works by orde</context>
</contexts>
<marker>Page, Brin, Motwani, Winograd, 1999</marker>
<rawString>Page, L., Brin, S., Motwani, R., &amp; Winograd, T. (1999). The PageRank Citation Ranking: Bringing Order to the Web: Stanford InfoLab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williams</author>
<author>E Reiter</author>
</authors>
<title>Appropriate Microplanning Choices for Low-Skilled Readers. Paper presented at the IJCAI.</title>
<date>2005</date>
<contexts>
<context position="6074" citStr="Williams &amp; Reiter, 2005" startWordPosition="961" endWordPosition="964">nclusion section of the summary presents computational information about the graphic (overall value and rate change, time span of the graphic, maximum and minimum points and dates when they occur). The strategies on organizing the summaries are described in (P. Moraes, McCoy, &amp; Carberry, 2014). The last step of the Generation Module is the aggregation of propositions into more complex sentences. This decision is usually left to the designer’s choice on how much aggregation to perform when generating text. Some systems are designed to generate simple text for people with low reading abilities (Williams &amp; Reiter, 2005a). As stated by (Williams &amp; Reiter, 2005b), most NLG systems available generate text for high-skilled users. Our system generates line graph summaries that fit the reading level of the article in which the line graph appears. We contend that users generally read articles from venues they feel comfortable with reading. In this manner, we intrinsically assess the user’s reading level without needing to actively survey it. Figure 3: A line graph present in popular media. 96 The first step of the aggregation phase is to assess the reading level of the article’s text. There is a myriad of techniqu</context>
</contexts>
<marker>Williams, Reiter, 2005</marker>
<rawString>Williams, S., &amp; Reiter, E. (2005a). Appropriate Microplanning Choices for Low-Skilled Readers. Paper presented at the IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Williams</author>
<author>E Reiter</author>
</authors>
<title>Generating readable texts for readers with low basic skills.</title>
<date>2005</date>
<booktitle>Paper presented at the Proceedings of the 10th European Workshop on Natural Language Generation (EWNLG</booktitle>
<contexts>
<context position="6074" citStr="Williams &amp; Reiter, 2005" startWordPosition="961" endWordPosition="964">nclusion section of the summary presents computational information about the graphic (overall value and rate change, time span of the graphic, maximum and minimum points and dates when they occur). The strategies on organizing the summaries are described in (P. Moraes, McCoy, &amp; Carberry, 2014). The last step of the Generation Module is the aggregation of propositions into more complex sentences. This decision is usually left to the designer’s choice on how much aggregation to perform when generating text. Some systems are designed to generate simple text for people with low reading abilities (Williams &amp; Reiter, 2005a). As stated by (Williams &amp; Reiter, 2005b), most NLG systems available generate text for high-skilled users. Our system generates line graph summaries that fit the reading level of the article in which the line graph appears. We contend that users generally read articles from venues they feel comfortable with reading. In this manner, we intrinsically assess the user’s reading level without needing to actively survey it. Figure 3: A line graph present in popular media. 96 The first step of the aggregation phase is to assess the reading level of the article’s text. There is a myriad of techniqu</context>
</contexts>
<marker>Williams, Reiter, 2005</marker>
<rawString>Williams, S., &amp; Reiter, E. (2005b). Generating readable texts for readers with low basic skills. Paper presented at the Proceedings of the 10th European Workshop on Natural Language Generation (EWNLG 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Wu</author>
<author>S Carberry</author>
<author>S Elzer</author>
<author>D Chester</author>
</authors>
<title>Recognizing the intended message of line graphs.</title>
<date>2010</date>
<booktitle>Paper presented at the Proceedings of the 6th international conference on Diagrammatic representation and inference,</booktitle>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="1914" citStr="Wu, Carberry, Elzer, &amp; Chester, 2010" startWordPosition="297" endWordPosition="302">extual summary that conveys it. Figure 1 shows the system architecture. The first step is the identification of the presence of a graphical image in the web page by a Browser Helper Object (BHO) (Elzer et al., 2007). If a graphic is present on the web page, the Graphical Information Extraction Module (VEM) (Chester &amp; Elzer, 2005) is triggered by the BHO in order to extract the data from the image. The VEM then produces an XML representation of the graphic that is used by the Intention Recognition Module (IRM) for simple bar charts (Elzer, Green, Carberry, &amp; Hoffman, 2006), simple line graphs (Wu, Carberry, Elzer, &amp; Chester, 2010) and grouped bar charts (R. Burns, Carberry, &amp; Elzer, 2010; R. Burns, Carberry, &amp; Schwartz, 2013; R. J. Burns, 2013). The XML representation 1 http://ir.cis.udel.edu/~moraes/udgraphs of the graphic, along with the intended message identified by the IRM, is sent to the Generation Module (GM), which produces a textual summary of the most important content presented in the graphic. The system produces an initial summary and follow-up responses for simple bar charts (Demir, Carberry, &amp; Elzer, 2009; Demir, Carberry, &amp; McCoy, 2008) and this demo presents the GM for simple line graphs. This demo foc</context>
</contexts>
<marker>Wu, Carberry, Elzer, Chester, 2010</marker>
<rawString>Wu, P., Carberry, S., Elzer, S., &amp; Chester, D. (2010). Recognizing the intended message of line graphs. Paper presented at the Proceedings of the 6th international conference on Diagrammatic representation and inference, Berlin, Heidelberg.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
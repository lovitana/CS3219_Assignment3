<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000346">
<title confidence="0.9962435">
Wordsyoudontknow: Evaluation of lexicon-based decompounding
with unknown handling
</title>
<author confidence="0.9687">
Karolina Owczarzak Ferdinand de Haan George Krupka Don Hindle
</author>
<affiliation confidence="0.96345">
Oracle Language Technology
</affiliation>
<address confidence="0.947343">
1111 19th Street NW #600, Washington, DC 20036, USA
</address>
<email confidence="0.8370435">
{karolina.owczarzak,ferdinand.de.haan,george.krupka,
don.hindle}@oracle.com
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999164142857143">
In this paper we present a cross-linguistic evaluation of a lexicon-based decomposition method
for decompounding, augmented with a “guesser” for unknown components. Using a gold
standard test set, for which the correct decompositions are known, we optimize the method’s
parameters and show correlations between each parameter and the resulting scores. The results
show that even with optimal parameter settings, the performance on compounds with unknown
elements is low in terms of matching the expected lemma components, but much higher in
terms of correct string segmentation.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986166666667">
Compounding is a productive process that creates new words by combining existing words together in
a single string. It is predominant in Germanic and Scandinavian languages, but is also present in other
languages, e.g. Finnish, Korean, or Farsi. Many languages that are not usually thought of as
“compounding” nevertheless display marginal presence of compounds, restricted, for instance, to
numerical expressions (e.g. Polish czterogodzinny ‘four-hour’). Depending on a language,
compounding can be a very frequent and productive process, in effect making it impossible to list all
the compound words in the dictionary. This creates serious challenges for Natural Language
Processing in many areas, including search, Machine Translation, information retrieval and related
disciplines that rely on matching multiple occurrences of words to the same underlying representation.
In this paper, we present a cross-linguistic evaluation of a lexicon-based decomposition method
augmented with a “guesser” for handling unknown components. We use existing lexicons developed
at Oracle Language Technology in combination with a string scanner parametrized with language-
specific input/output settings. Our focus is on the evaluation that tries to tease apart string
segmentation (i.e. finding boundaries between components) and morphological analysis (i.e. matching
component parts to known lemmas).
The paper is organized as follows: Section 2 gives an overview of related research; Section 3
describes the compound analyzer used in our experiments; Section 4 presents experimental results;
Section 5 contains error analysis and discussion. Section 6 concludes and suggests future research.
</bodyText>
<sectionHeader confidence="0.995064" genericHeader="introduction">
2 Related research
</sectionHeader>
<bodyText confidence="0.916786375">
Current research on compound splitting is predominantly lexicon-based, with a range of selection
methods to choose the most likely decomposition. The lexicons used to identify components are
usually collected from large monolingual corpora (Larson et al., 2000; Monz and de Rijke, 2001;
Alfonseca et al, 2008; Holz and Biemann, 2008; von Huyssteen and von Zaanen, 2004).
The problem with pure lexicon-based approach without any constraints is that it will produce many
spurious decompositions, matching small substrings that happen to be legitimate words in the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.990568">
63
</page>
<note confidence="0.98526">
Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 63–71,
Dublin, Ireland, August 24 2014.
</note>
<bodyText confidence="0.999890363636364">
language. Therefore, some approaches introduce maximizing component length (or, conversely,
minimizing the number of components) as one of the selection factors (von Huyssteen and von
Zaanen, 2004; Holz and Biemann, 2008; Macherey et al., 2011; Larson et al., 2000); others use part of
speech to eliminate short components which tend to be function words (Koehn and Knight, 2003;
Monz and de Rijke, 2001). In other cases, Named Entity Recognition is used to filter out proper names
that should not be decomposed but that can contain frequent short components like “-berg” or “-dorf”
(Alfonseca et al., 2008).
Even after removing unlikely small component candidates, there is enough ambiguity in
decomposition to warrant further filtering methods. And so, approaches related to Machine Translation
use bilingual parallel corpora to find the most likely components by checking whether their
translations match elements of the whole compound translation (Koehn and Knight, 2003; Macherey
et al., 2011). Other filtering methods are based on combined frequency of the components (Koehn and
Knight, 2003; Holz and Biemann, 2008), point-wise mutual information of components, or occurrence
of components in related locations, such as anchor text (Alfonseca et al., 2008). A very interesting
lexicon-free approach is presented in Aussems et al. (2013), which uses point-wise mutual information
to detect likely boundaries between characters that would identify a compound.
A major issue with the current research is the absence of common training and testing data,
particularly across multiple languages, which then translates into limited evaluations of presented
methods. Using pre-annotated frequency lists, we create gold standard test sets for 10 languages:
Norwegian, Danish, Dutch, Estonian, Finnish, German, Hungarian, Korean, Farsi, Swedish, which
range from around 600 to 15,000 compounds. This allows a more thorough comparison of the analyser
performance across different languages.
</bodyText>
<sectionHeader confidence="0.956019" genericHeader="method">
3 Lexicon-based analyzer
</sectionHeader>
<bodyText confidence="0.999471307692308">
Our approach follows the main line of research in that it uses lexicons to identify potential components
in a compound; however, our lexicons contain lemmas rather than word forms, in contrast to lexicons
harvested from monolingual corpora. However, the lexicons we use contain as well as partial lemmas
whose occurrences are restricted to compounds (e.g. German verb forms without the final –en; for
example schließ-). In addition, we use morphological rules to map recognized inflected forms to base
(lexicon) lemmas. Both the lexicons and the morphological rules have been previously created by
computational linguists and native speakers for use in a variety of NLP applications at Oracle
Language Technology.
On the most basic level, a compound can be explicitly added to the lexicon, with a specific
decomposition and appropriate part of speech and grammatical features; this option is used when the
decomposition is irregular or non-obvious, for instance when the component appears in a form that is
not directly analyzable to its lemma, as in the example below, which shows irregular plurals and
deletion of consonant:
</bodyText>
<listItem confidence="0.929568333333333">
(1) a. Danish: barn ‘child’ plural: børn
børnebog barn+e+bog [child-connector-book] ‘children’s book’
b. Norwegian Bokmål: deletion of repeated consonant
</listItem>
<bodyText confidence="0.956288714285714">
musikkorps musikk+korps [music-band] ‘music band’
Lexicalized compounds are treated like any other words, and their inflected forms will be
recognized. Explicitly adding the compound to the lexicon is also useful when the compound can have
multiple decompositions, and we want to restrict the output only to the semantically correct analysis.
In Dutch, for instance, the compound part stem can refer to the noun stem ‘voice’ or to the root of the
verb stemmen ‘vote’. These readings are distinguished in the lexicon by listing explicit decompositions
for compounds that contain the part:
</bodyText>
<listItem confidence="0.967761333333333">
(2) Dutch stem N vs. V
a. stemband stem#band [voice-cord] ‘vocal cord’ (N-N)
b. stembureau stemmen#bureau [vote-station] ‘polling station (V-N)
</listItem>
<page confidence="0.998832">
64
</page>
<bodyText confidence="0.999990769230769">
However, adding all compounds to the lexicon is simply unfeasible for many languages where the
compounding process is highly productive. For this reason, we also use a compound analyser to
identify components in a dynamic manner, based on available component lemmas in the lexicon.
Components are found by removing any recognizable inflections from the candidate string, scanning it
left-to-right, and looking for all matching lemmas, subject to constraints based on part of speech,
length, number of components, and available features. For speed reasons, we apply greedy matching,
and prefer decompositions with the longest prefix and the smallest number of components.
Since our goal is developing language processing systems that are as universal as possible, leaving
context-dependent decisions to higher-level applications, we are not particularly concerned with
always selecting the single best decomposition for a compound, since in many cases is will be
dependent on the domain and application. However, it is useful to filter out decompositions that would
be highly unlikely in any context, for instance those containing small function words mentioned in
previous section. For this purpose, we apply constraints described below.
</bodyText>
<subsectionHeader confidence="0.989051">
3.1 Rules for compound sequences
</subsectionHeader>
<bodyText confidence="0.9996235">
For each language, we list the possible part of speech sequences that can appear in compounds. These
rules serve not only to prevent the decompositions that would not appear in the language (for instance,
noun-verb-particle), but also to restrict sequences that are fairly infrequent, but that would lead to
considerable over-generation if they were added. For example, in German, there are relatively few
compounds that end with a verb, unless it is a combination of movable prefix particle (aus, an, ab, ein,
etc.) and the verb (aus+gehen, auf+stehen, um+steigen, etc.). These verbs are functionally analyzed as
compounds, i.e. a concatenation of two lemmas. However, since sequences noun/adjective/verb + verb
are much less productive (spazieren+gehen, auto+fahren), it is more efficient to restrict the verb-final
compounds to particle-verb only, and add the exceptions to the lexicon. A few examples of compound
part of speech sequences for different languages are shown in (3).
</bodyText>
<listItem confidence="0.984532428571429">
(3) a. Dutch:
cardinal_number + verb e.g., vier+en+delen ‘quarter’
b. Estonian:
noun+adjective e.g. silmi+pimestav ‘eye-dazzling’
c. German:
ordinal_number + adjective e.g. zweit+größt ‘second largest’
d. Swedish:
</listItem>
<bodyText confidence="0.922687">
noun + noun e.g citron+saft ‘lemon juice’
Another issue is compounds of cardinal or ordinal numbers, which can also occur in some
languages like Italian (cinquecento+sessanta+nove ‘five hundred sixty nine’) or Greek (oxraxomoa,
oxrw + axomoa ‘eight hundred’). These number compounds can be very productive and are also
included in the lists of allowed compound sequences.
</bodyText>
<subsectionHeader confidence="0.999092">
3.2 Connectors
</subsectionHeader>
<bodyText confidence="0.999999142857143">
In many compounding languages, the subparts of a compound can be connected with extra material, a
connector (or linking element). These are semantically empty elements that have a mainly
phonological role in connecting the compound parts (Bauer, 2009). In many Germanic languages
connectors are derived from plural or genitive morphemes (such as –er or –s in German), but do not
have this role any more, as evidenced, among others, by the fact that in certain cases the connector is
optional and compounds with and without a connector co-exist (4a) or by the fact that there are cases
where two different connectors co-occur (4b) (Krott et al., 2007):
</bodyText>
<listItem confidence="0.931333666666667">
(4) a. Norwegian Bokmål:
rettssak rett + s + sak ‘court case’
rettsak rett + fly + sak
b. Dutch:
paddestoel pad + e + stoel ‘toadstool’
paddenstoel pad + en + stoel
</listItem>
<page confidence="0.996205">
65
</page>
<bodyText confidence="0.751323625">
For each language, we create a set of allowed connectors, a few examples of which can be seen in
(5).1 Note that it might be useful to restrict certain connectors to appear only in certain sequences (e.g.
between noun and noun, but not adjective and verb); we plan to implement this restriction in future
work.
(5) Connector examples
Dutch s e.g. water+s+nood ‘flood’
German zu e.g. to match auf+stehen and auf+zu+stehen ‘stand up’
Swedish o e.g. veck+o+slut ‘weekend’
</bodyText>
<subsectionHeader confidence="0.998902">
3.3 Decompounding settings
</subsectionHeader>
<bodyText confidence="0.997360647058824">
Another factor in successful dynamic decompounding is restrictions on possible number of
components, and on length of candidate strings and candidate components. Choosing to allow fewer
components of longer length helps to prevent spurious over-analysis, where several short words can
accidentally match the string which is being analyzed. However, setting the limits too high might also
prevent legitimate decomposition, so this trade-off needs to be carefully balanced. There are four basic
length settings, as shown in Table 1 below; the values are dependent on language.
Maximum number of elements: Limits the number of components in a compound. Low values help
prevent spurious decompositions into many small elements.
Minimum length of compound: The minimum length of string that should be subject to
decompounding; short strings are unlikely to be compounds, so for efficiency reasons, they are not
decompounded.
Minimum length of component: Specifies the minimum length of potential compound elements;
shorter substrings are excluded to avoid accidental matching of very short words.
Minimum length of component with connector: A version of the above setting, it specifies the
minimum length of potential element when this element is next to a connector; to avoid spurious
matches of the short word + connector combination (e.g. Dutch paspoort should be decomposed as
pas+poort, not pa+s+poort).
</bodyText>
<figureCaption confidence="0.819779">
setting value
maximum number of elements 2-4
minimum length of compound 4-11
minimum length of component 2-4
minimum length of component with connector 2-4
</figureCaption>
<tableCaption confidence="0.993373">
Table 1. Length settings for dynamic decompounding.
</tableCaption>
<bodyText confidence="0.998890333333333">
The values for these settings are established manually and separately for each language, based on
review of top N most frequent compounds in the lexicon and the general knowledge of that language’s
grammar and patterns.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="method">
4 Experimental results
</sectionHeader>
<bodyText confidence="0.99937575">
Despite all the constraints and settings described above, decompounding is still an imperfect process:
there can be multiple competing (i.e. overlapping) decompositions, and many decompositions that are
technically possible are incorrect due to semantic reasons. This problem becomes even more
challenging when some of the components are not present in the lexicon. Since lexicons are limited,
and real world text can contain misspellings, proper names, or obscure words, we need to address the
issue of decompounding with unknown elements. Therefore, we set out to evaluate the performance of
our lexicon-based method on a gold standard set of known compounds, and compare it to an
augmented version that also tries to construct potential components from unknown substrings.
</bodyText>
<footnote confidence="0.997207">
1 Note that for our purposes, particle zu in German is also treated as a connector, to match the movable particle verbs that can
appear with and without zu: auf + zu + stehen and auf + stehen ‘get up’.
</footnote>
<page confidence="0.971226">
66
</page>
<subsectionHeader confidence="0.997136">
4.1 Test set
</subsectionHeader>
<bodyText confidence="0.9997948">
For our experiments, we collected compounds from the top 90% frequency lists based on large
news and Wikipedia corpora. Each compound was annotated with the correct decomposition(s) by a
linguist who was also a native speaker of the target language according to simple instructions: if the
meaning of the word is compositional (i.e. can be fully described by the component elements), treat it
as a compound and provide component lemmas.
Approximate sizes of source corpora per language are given in Table 2; column “compounds”
shows the count of compounds; column “lexical” shows how many of these are lexicalized compounds
(i.e. compounds that have been added to the lexicon for reasons of irregularity). While two-part
compounds are by far the most frequent in all the languages we examined, there is also some
percentage of compounds with more than two parts; the distribution is shown in the last four columns.
</bodyText>
<table confidence="0.998637538461538">
language news wiki compounds lexical 2-part 3-part 4-part 5-part
corpus corpus
MB MB
Danish 335 154 1,982 1,326 1,856 122 4 0
Dutch 512 103 3,439 1,909 3,186 245 8 0
Estonian 204 41 2,343 562 2,166 169 8 0
Farsi 512 244 648 340 635 13 0 0
Finnish 512 78 1,868 1,665 1,703 154 11 0
German 520 227 15,490 5,087 14,544 915 31 0
Hungarian 512 257 1,841 1,537 1,794 45 2 0
Korean 826 190 11,398 4,774 10,919 425 39 5
Norwegian 512 88 3,582 1,106 3,405 175 2 0
Swedish 512 204 9,677 5,608 8,901 744 31 5
</table>
<tableCaption confidence="0.999798">
Table 2. Size of corpora per language, count of compounds, distribution of parts.
</tableCaption>
<subsectionHeader confidence="0.520877">
4.2 Dynamic decompounding with available lemmas
</subsectionHeader>
<bodyText confidence="0.999935571428571">
As mentioned before, it is not feasible to add all (or even the majority) of possible compounds, so we
need to examine our performance using only dynamic decompounding. For this purpose, we removed
all lexicalized compounds from the lexicon, and then ran the analyzer on the compound test set
described above. This means that all the compound analysis was done dynamically, using only the
available simple lemmas and compound rules and length restrictions. Table 3 shows the results. The
scores for lexicalized + dynamic decompounding are given only for reference; they are high but less
interesting, since they reflect the fact that the lexicalized compounds were largely collected from the
same corpora (among other sources). Our focus is on the dynamic scores, which show performance on
unknown compounds assuming a nearly “perfect” lexicon that contains almost all the component
lemmas. As such, these scores will serve as the upper bound for our next experiment, in which we
remove at least one of the component lemmas from the lexicon and test the resulting performance.
As can be seen in Table 3, for most languages recall decreases considerably – this suggests that
lexicalized compounds are of the kind that are not covered by the decompounding rules or whose
correct analysis is blocked by another decomposition.
</bodyText>
<subsectionHeader confidence="0.917229">
4.3 Dynamic decompounding with missing lemmas
</subsectionHeader>
<bodyText confidence="0.999906666666667">
While dynamic decompounding can handle the productive nature of compounds, it is still limited to
finding components that are already present in the lexicon. However, in the real world compounds will
contain elements unknown to a lexicon-based analyzer, whether it is because they are domain-specific
vocabulary, proper names, foreign borrowings, or misspellings. In those cases, it is still useful to
attempt analysis and return the known parts, with the option of returning the unknown substring as the
missing lemma.
</bodyText>
<page confidence="0.998788">
67
</page>
<table confidence="0.99983675">
prec lexicalized + dynamic prec dynamic only f-score
rec f-score rec
Danish 98.18 99.6 98.88 87.99 66.9 76.01
Dutch 98.84 100 99.42 84.46 80.49 82.43
Estonian 98.25 99.83 99.03 95.69 90.27 92.9
Farsi 92.9 100 96.32 65.75 72.84 69.11
Finnish 98.74 100 99.37 84.55 68.63 75.76
German 96.11 99.98 98.01 88.01 89.03 88.52
Hungarian 90.44 99.84 94.91 77.42 72.19 74.71
Korean 99.72 100 99.86 95.23 59.49 73.23
Norwegian 99.6 100 99.8 93.25 86.32 89.65
Swedish 96.35 99.88 98.08 86.67 75.75 80.84
</table>
<tableCaption confidence="0.99964">
Table 3. Precision, recall, and f-measure for dynamic decompounding.
</tableCaption>
<bodyText confidence="0.995324">
To evaluate the performance of our analyzer in case where some component lemmas are unknown,
we applied a “compound guesser” function that tries to find known elements of unknown compounds,
even if a complete decomposition to only known elements is impossible. The guesser has its own
constraints, independent of the main compound analyzer, which are shown in Table 4.
</bodyText>
<figureCaption confidence="0.547911">
setting value
maximum number of elements 2-20
minimum length of compound 3-20
minimum length of component 2-5
minimum length of unknown element 1-5
minimum percent of string covered 0-100%
</figureCaption>
<tableCaption confidence="0.692539">
Table 4. Settings for dynamic decompounding with unknown elements.
</tableCaption>
<bodyText confidence="0.9987278">
The first three settings are parallel to the settings for regular dynamic decompounding; however, we
also add restrictions on length for unknown elements (minimum length of unknown element) and total
string coverage (minimum percent of string covered). Restriction on length of unknown element mean
that any unknown string shorter than the minimum length will be treated as a potential
connector/suffix/prefix and will not be returned as a lemma:
</bodyText>
<listItem confidence="0.946333">
(6) German: assuming freundlicher ‘friendlier’ is unknown:
</listItem>
<equation confidence="0.378889">
umweltfreundlicher -&gt; umwelt + freundlich (! + er) [environment + friendly]
</equation>
<bodyText confidence="0.999893909090909">
The last setting allows a more fine-grained control over the proportion of known to unknown parts;
however, since any value less than 100% will restrict the number of produced candidate
decompositions, resulting in no output if the unknown substring is too long, we do not test the impact
of this setting.
For this experiment, we collected all component lemmas from the test compounds, and removed
from lexicon at least one component lemma per compound. This renders the whole string
unanalyzable by regular means. Then we ran the compound guesser with each combination of settings
from Table 4, to find the optimal set of values.
Table 5 shows results obtained with the optimal guesser settings per language, compared to scores
from Table 3: a fully functional decomposition that has access to both dynamic decomposition and
lexicalized compounds, and dynamic decomposition with near-perfect component lexicon. It is clear
</bodyText>
<page confidence="0.998577">
68
</page>
<bodyText confidence="0.9994995">
that even with optimal settings, the guesser performance falls well below the level of full functionality,
even when we compare to a system that has no access to lexicalized compounds. The highest score
achieved by the guesser is 34 for the Hungarian test set, which includes mostly simple two-part
compounds, and where the lexicon does not provide too many spurious sub-matches.
</bodyText>
<table confidence="0.9998325">
language lexical + dynamic dynamic dynamic dynamic guesser -
only guesser string segmentation
Danish 98.88 76.01 25.93 51.25
Dutch 99.42 82.43 27.13 64.01
Estonian 99.03 92.9 9.56 53.89
Farsi 96.32 69.11 27.16 78.68
Finnish 99.37 75.76 19.49 51.6
German 98.01 88.52 25.1 52.29
Hungarian 94.91 74.71 34 53.5
Korean 99.86 73.23 16.81 76.54
Norwegian 99.8 89.65 22.56 49.74
Swedish 98.08 80.84 25.56 54.18
</table>
<tableCaption confidence="0.999259">
Table 5. Dynamic decomposition with missing lemmas, optimal settings; string segmentation shows
</tableCaption>
<bodyText confidence="0.960720529411765">
accuracy score; remaining values are harmonic f-score of precision and recall.
However, a major problem with this evaluation is that output of the regular decompounding process
produces lemmas in their dictionary form, without inflection, whereas the guesser can only return
surface strings for the unknown elements which might carry grammatical inflection or stem
alternations. Therefore, it would be more fair to compare the guesser to dynamic decompounding in
terms of pure string segmentation – whether it finds the same boundaries between components,
without concern for the form of the returned component. This lets us tease apart the impact of finding
component elements from the impact of morphology. The last column in Table 5 shows accuracy of
guesser string segmentation as compared to string segmentation performed by regular dynamic
decompounding; in this respect the guesser’s performance is indeed much better. These results are
encouraging, showing that we can recover correct components in up to 79% of cases, which is a very
useful improvement for the purposes of information retrieval and search. While some recall is lost by
returning strings instead of lemmas, we are planning to add a second step that would employ a lemma
“guesser”, in order to produce the most likely dictionary form from the recovered unknown string.
language max corr. min corr. min corr. min corr.
elements with length of with length with length of with
score compound score of score unknown score
</bodyText>
<table confidence="0.976233583333333">
element element
Danish 2 -0.19 3-7 -0.46 4 0.43 3 -0.09
Dutch 2 -0.21 8 -0.47 5 0.51 3 -0.06
Estonian 2 -0.15 3-7 -0.57 4 0.33 3 -0.08
Farsi 2 -0.17 3-5 -0.51 3 0.11 2 -0.24
Finnish 2-10 -0.07 3-8 -0.49 5 0.41 3 0
German 2 -0.26 8 -0.43 5 0.5 3 -0.06
Hungarian 2-16 0 1-6 -0.58 4 0.39 2 -0.33
Korean 2-10 -0.07 3 -0.14 2 -0.07 1 -0.11
Norwegian 2-10 -0.18 3-8 -0.61 5 0.56 3 0.01
Swedish 2 -0.23 7 -0.45 4 0.49 3 -0.04
Average -0.15 -0.47 0.37 -0.1
</table>
<tableCaption confidence="0.970341">
Table 6. Optimal guesser settings and their correlations of settings with the guesser score.
</tableCaption>
<page confidence="0.998884">
69
</page>
<bodyText confidence="0.999965">
Finally, Table 6 shows the correlation (Pearson’s r) of guesser settings (or their ranges) and the
resulting scores. As can be seen, the strongest correlation holds for the minimum length of compound
(average -0.47) and minimum length of element (0.37). In the former case, the correlation is inverse,
which means the higher the value, the lower the final score; this is caused by the fact that our test set
contains only compounds, so returning the whole unsplit string will never be the right result. The
second correlation reflects the fact that it is safer to exclude very short elements from appearing as
components, a finding that confirms earlier research.
</bodyText>
<sectionHeader confidence="0.997573" genericHeader="method">
5 Error analysis
</sectionHeader>
<bodyText confidence="0.9996364">
A considerable percentage of mismatch errors when guessing the unknown components of
compounds is caused by the connectors. Our current guesser settings return the whole unknown string,
without attempting to identify any potential connectors on its edges. This seems like an obvious area
for improvement, as it would let us return more correct decompositions for cases shown in Table 7
(unknown strings are enclosed in square brackets and are currently returned whole).
</bodyText>
<table confidence="0.9956422">
language token dynamic guesser translation
Norwegian kjærlighetsbrev kjærlighet#brev kjærlighet#[s + brev] love letter
Danish ungdomshus ungdom#hus ungdom#[s + hus] youth
German sklavenmoral sklave#moral sklave#[n + moral] slave morality
Swedish kvinnoförbund kvinna#förbund kvinn#[o + förbund] women&apos;s alliance
</table>
<tableCaption confidence="0.998537">
Table 7. Examples of connector mismatches between dynamic decompounding and the guesser.
</tableCaption>
<bodyText confidence="0.9794425">
As could be expected, most errors are nevertheless caused by the guesser splitting unknown strings
into smaller known chunks; several typical examples are shown in Table 8.
</bodyText>
<table confidence="0.9797678">
language token dynamic guesser translation
Danish populærkulturen populær#kultur populær#kult#uren popular culture
Dutch kunstschilders kunst#schilder kunst#schil#ders painters
Finnish rockmuusikot rock#muusiko rock#muusi#kot rock music
Swedish radioversion radio#version radio#vers#ion radio version
</table>
<tableCaption confidence="0.998144">
Table 8. Examples of incorrect splitting of unknown strings.
</tableCaption>
<sectionHeader confidence="0.98133" genericHeader="conclusions">
6 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999965166666667">
In this paper, we have shown a dictionary-based compound analyzer, augmented with the function to
handle unknown substrings. A cross-linguistic evaluation against the gold standard containing
component lemmas shows that the correct handling of unknown compound elements is a difficult issue
especially if we try to match dictionary lemmas; however, a more detailed evaluation of the string
segmentation and boundary detection shows fairly good results. Being able to decompose unknown
compounds and match the components to known lemmas to increase recall is crucial to many NLP
applications, such as information retrieval or Machine Translation. A correct segmentation is of
fundamental importance, but the question remains how we can match the unknown, possibly inflected,
substring to known lemmas. In the future, we plan to address this question by (1) adding the option to
separate out connectors from unknown strings, and (2) build a lemma “guesser” that would try to
construct a probable dictionary representation for the unknown string, in effect building a pipeline that
would more fully mirror the process of regular dynamic decompounding.
</bodyText>
<sectionHeader confidence="0.99692" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9960205">
We would like to thank the rest of the Oracle Language Technology team, in particular Elena Spivak
and Rattima Nitisaroj, for their help with compound examples.
</bodyText>
<page confidence="0.997105">
70
</page>
<sectionHeader confidence="0.995883" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999862580645161">
Alfonseca, Enrique, Slaven Bilac and Stefan Pharies. 2008. German Decompounding in a Difficult Corpus. In
Computational Linguistics and Intelligent Text Processing, A. Gelbukh (ed.). Springer Verlag, Berlin and
Heidelberg, 128-139.
Aussems, Suzanne., Bas Goris., Vincent Lichtenberg, Nanne van Noord, Rick Smetser, and Menno van Zaanen.
2013. Unsupervised identification of compounds. In Proceedings of the 22nd Belgian-Dutch conference on
machine learning, A. van den Bosch, T. Heskes, &amp; D. van Leeuwen (Eds.), Nijmegen, 18-25.
Bauer, Laurie. 2009. Typology of Compounds. In The Oxford Handbook of Compounding, Rochelle Lieber and
Pavol Štekauer (eds.). Oxford University Press, Oxford.343-356.
Holz, Florian and Chris Biemann. 2008. Unsupervised and Knowledge-Free Learning of Compound Splits and
Periphrases. CICLing&apos;08 Proceedings of the 9th international conference on Computational linguistics and
intelligent text processing, A. Gelbukh (ed.). Springer Verlag, Berlin and Heidelberg, 117-127.
Koehn, Philipp and Kevin Knight. 2003. Empirical Methods for Compound Splitting. Proceedings of the 10th
conference of the European Chapter of the Association for Computational Linguistics, Vol. 1, 187-193.
Krott, Andrea, Robert Schreuder, R. Harald Baayen and Wolfgang U. Dressler. 2007 Analogical effects on
linking elements in German compounds. Language and Cognitive Processes, 22(1):25-57.
Larson, Martha, Daniel Willett, Joachin Köhler and Gerhard Rigoll. 2000. Compound splitting and lexical unit
recombination for improved performance of a speech recognition system for German parliamentary speeches
In INTERSPEECH, 945-948.
Macherey, Klaus, Andrew M. Dai, David Talbot, Ashok C. Popat and Franz Och. 2011. Language-independent
compound splitting with Morphological Operations. Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics, 1395-1404.
Monz, Christof and Maarten de Rijke. 2002. Shallow Morphological Analysis in Monolingual Information
Retrieval for Dutch, German and Italian. In Evaluation of Cross-Language Information Retrieval Systems.
Carol Peters, Martin Braschler, Julio Gonzalo and Michael Kluck (eds.). Springer Verlag, Berlin and
Heidelberg, 262-277.
van Huyssteen, Gerhard and Menno van Zaanen. 2004. Learning Compound Boundaries for Afrikaans Spelling
Checking. In Pre-Proceedings of the Workshop on International Proofing Tools and Language Technologies;
Patras, Greece. 101–108.
van Zaanen, Menno, Gerhard van Huyssteen, Suzanne Aussems, Chris Emmery, and Roald Eiselen. 2014. The
Development of Dutch and Afrikaans Language Resources for Compound Boundary Analysis. In Proceeding
of LREC 2014.
</reference>
<page confidence="0.999141">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981196">
<title confidence="0.9980515">Wordsyoudontknow: Evaluation of lexicon-based with unknown handling</title>
<author confidence="0.99991">Karolina Owczarzak Ferdinand de_Haan George Krupka Don Hindle</author>
<affiliation confidence="0.99272">Oracle Language Technology</affiliation>
<address confidence="0.999728">Street NW #600, Washington, DC 20036, USA</address>
<email confidence="0.999904">don.hindle}@oracle.com</email>
<abstract confidence="0.99904">In this paper we present a cross-linguistic evaluation of a lexicon-based decomposition method decompounding, with a “guesser” for unknown components. Using a gold test set, for which the correct are known, we optimize the method’s parameters and show correlations between each parameter and the resulting scores. The results show that even with optimal parameter settings, the performance on compounds with unknown elements is low in terms of matching the expected lemma components, but much higher in terms of correct string segmentation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Alfonseca</author>
</authors>
<title>Slaven Bilac and Stefan Pharies.</title>
<date>2008</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>128--139</pages>
<editor>A. Gelbukh (ed.).</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin and Heidelberg,</location>
<marker>Alfonseca, 2008</marker>
<rawString>Alfonseca, Enrique, Slaven Bilac and Stefan Pharies. 2008. German Decompounding in a Difficult Corpus. In Computational Linguistics and Intelligent Text Processing, A. Gelbukh (ed.). Springer Verlag, Berlin and Heidelberg, 128-139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne Aussems</author>
<author>Bas Goris</author>
<author>Vincent Lichtenberg</author>
<author>Nanne van Noord</author>
<author>Rick Smetser</author>
<author>Menno van Zaanen</author>
</authors>
<title>Unsupervised identification of compounds.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd Belgian-Dutch conference on machine learning, A. van</booktitle>
<pages>18--25</pages>
<location>Nijmegen,</location>
<marker>Aussems, Goris, Lichtenberg, van Noord, Smetser, van Zaanen, 2013</marker>
<rawString>Aussems, Suzanne., Bas Goris., Vincent Lichtenberg, Nanne van Noord, Rick Smetser, and Menno van Zaanen. 2013. Unsupervised identification of compounds. In Proceedings of the 22nd Belgian-Dutch conference on machine learning, A. van den Bosch, T. Heskes, &amp; D. van Leeuwen (Eds.), Nijmegen, 18-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurie Bauer</author>
</authors>
<title>Typology of Compounds.</title>
<date>2009</date>
<booktitle>In The Oxford Handbook of Compounding, Rochelle Lieber and Pavol Štekauer</booktitle>
<pages>343--356</pages>
<editor>(eds.).</editor>
<publisher>Oxford University Press,</publisher>
<contexts>
<context position="10645" citStr="Bauer, 2009" startWordPosition="1546" endWordPosition="1547">t ‘lemon juice’ Another issue is compounds of cardinal or ordinal numbers, which can also occur in some languages like Italian (cinquecento+sessanta+nove ‘five hundred sixty nine’) or Greek (oxraxomoa, oxrw + axomoa ‘eight hundred’). These number compounds can be very productive and are also included in the lists of allowed compound sequences. 3.2 Connectors In many compounding languages, the subparts of a compound can be connected with extra material, a connector (or linking element). These are semantically empty elements that have a mainly phonological role in connecting the compound parts (Bauer, 2009). In many Germanic languages connectors are derived from plural or genitive morphemes (such as –er or –s in German), but do not have this role any more, as evidenced, among others, by the fact that in certain cases the connector is optional and compounds with and without a connector co-exist (4a) or by the fact that there are cases where two different connectors co-occur (4b) (Krott et al., 2007): (4) a. Norwegian Bokmål: rettssak rett + s + sak ‘court case’ rettsak rett + fly + sak b. Dutch: paddestoel pad + e + stoel ‘toadstool’ paddenstoel pad + en + stoel 65 For each language, we create a </context>
</contexts>
<marker>Bauer, 2009</marker>
<rawString>Bauer, Laurie. 2009. Typology of Compounds. In The Oxford Handbook of Compounding, Rochelle Lieber and Pavol Štekauer (eds.). Oxford University Press, Oxford.343-356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Holz</author>
<author>Chris Biemann</author>
</authors>
<title>Unsupervised and Knowledge-Free Learning of Compound Splits and Periphrases.</title>
<date>2008</date>
<booktitle>CICLing&apos;08 Proceedings of the 9th international conference on Computational linguistics and intelligent text processing,</booktitle>
<pages>117--127</pages>
<editor>A. Gelbukh (ed.).</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin and Heidelberg,</location>
<contexts>
<context position="2935" citStr="Holz and Biemann, 2008" startWordPosition="405" endWordPosition="408">d as follows: Section 2 gives an overview of related research; Section 3 describes the compound analyzer used in our experiments; Section 4 presents experimental results; Section 5 contains error analysis and discussion. Section 6 concludes and suggests future research. 2 Related research Current research on compound splitting is predominantly lexicon-based, with a range of selection methods to choose the most likely decomposition. The lexicons used to identify components are usually collected from large monolingual corpora (Larson et al., 2000; Monz and de Rijke, 2001; Alfonseca et al, 2008; Holz and Biemann, 2008; von Huyssteen and von Zaanen, 2004). The problem with pure lexicon-based approach without any constraints is that it will produce many spurious decompositions, matching small substrings that happen to be legitimate words in the This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 63 Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 63–71, Dublin, Ireland, August 24 2014. language. Therefore, some</context>
<context position="4632" citStr="Holz and Biemann, 2008" startWordPosition="653" endWordPosition="656">e decomposed but that can contain frequent short components like “-berg” or “-dorf” (Alfonseca et al., 2008). Even after removing unlikely small component candidates, there is enough ambiguity in decomposition to warrant further filtering methods. And so, approaches related to Machine Translation use bilingual parallel corpora to find the most likely components by checking whether their translations match elements of the whole compound translation (Koehn and Knight, 2003; Macherey et al., 2011). Other filtering methods are based on combined frequency of the components (Koehn and Knight, 2003; Holz and Biemann, 2008), point-wise mutual information of components, or occurrence of components in related locations, such as anchor text (Alfonseca et al., 2008). A very interesting lexicon-free approach is presented in Aussems et al. (2013), which uses point-wise mutual information to detect likely boundaries between characters that would identify a compound. A major issue with the current research is the absence of common training and testing data, particularly across multiple languages, which then translates into limited evaluations of presented methods. Using pre-annotated frequency lists, we create gold stan</context>
</contexts>
<marker>Holz, Biemann, 2008</marker>
<rawString>Holz, Florian and Chris Biemann. 2008. Unsupervised and Knowledge-Free Learning of Compound Splits and Periphrases. CICLing&apos;08 Proceedings of the 9th international conference on Computational linguistics and intelligent text processing, A. Gelbukh (ed.). Springer Verlag, Berlin and Heidelberg, 117-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical Methods for Compound Splitting.</title>
<date>2003</date>
<booktitle>Proceedings of the 10th conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>187--193</pages>
<contexts>
<context position="3888" citStr="Koehn and Knight, 2003" startWordPosition="541" endWordPosition="544">d proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 63 Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 63–71, Dublin, Ireland, August 24 2014. language. Therefore, some approaches introduce maximizing component length (or, conversely, minimizing the number of components) as one of the selection factors (von Huyssteen and von Zaanen, 2004; Holz and Biemann, 2008; Macherey et al., 2011; Larson et al., 2000); others use part of speech to eliminate short components which tend to be function words (Koehn and Knight, 2003; Monz and de Rijke, 2001). In other cases, Named Entity Recognition is used to filter out proper names that should not be decomposed but that can contain frequent short components like “-berg” or “-dorf” (Alfonseca et al., 2008). Even after removing unlikely small component candidates, there is enough ambiguity in decomposition to warrant further filtering methods. And so, approaches related to Machine Translation use bilingual parallel corpora to find the most likely components by checking whether their translations match elements of the whole compound translation (Koehn and Knight, 2003; Ma</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Koehn, Philipp and Kevin Knight. 2003. Empirical Methods for Compound Splitting. Proceedings of the 10th conference of the European Chapter of the Association for Computational Linguistics, Vol. 1, 187-193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Krott</author>
<author>Robert Schreuder</author>
<author>R Harald Baayen</author>
<author>Wolfgang U Dressler</author>
</authors>
<title>Analogical effects on linking elements in German compounds.</title>
<date>2007</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>22--1</pages>
<contexts>
<context position="11044" citStr="Krott et al., 2007" startWordPosition="1613" endWordPosition="1616">the subparts of a compound can be connected with extra material, a connector (or linking element). These are semantically empty elements that have a mainly phonological role in connecting the compound parts (Bauer, 2009). In many Germanic languages connectors are derived from plural or genitive morphemes (such as –er or –s in German), but do not have this role any more, as evidenced, among others, by the fact that in certain cases the connector is optional and compounds with and without a connector co-exist (4a) or by the fact that there are cases where two different connectors co-occur (4b) (Krott et al., 2007): (4) a. Norwegian Bokmål: rettssak rett + s + sak ‘court case’ rettsak rett + fly + sak b. Dutch: paddestoel pad + e + stoel ‘toadstool’ paddenstoel pad + en + stoel 65 For each language, we create a set of allowed connectors, a few examples of which can be seen in (5).1 Note that it might be useful to restrict certain connectors to appear only in certain sequences (e.g. between noun and noun, but not adjective and verb); we plan to implement this restriction in future work. (5) Connector examples Dutch s e.g. water+s+nood ‘flood’ German zu e.g. to match auf+stehen and auf+zu+stehen ‘stand up</context>
</contexts>
<marker>Krott, Schreuder, Baayen, Dressler, 2007</marker>
<rawString>Krott, Andrea, Robert Schreuder, R. Harald Baayen and Wolfgang U. Dressler. 2007 Analogical effects on linking elements in German compounds. Language and Cognitive Processes, 22(1):25-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Larson</author>
<author>Daniel Willett</author>
<author>Joachin Köhler</author>
<author>Gerhard Rigoll</author>
</authors>
<title>Compound splitting and lexical unit recombination for improved performance of a speech recognition system for German parliamentary speeches In</title>
<date>2000</date>
<booktitle>INTERSPEECH,</booktitle>
<pages>945--948</pages>
<contexts>
<context position="2863" citStr="Larson et al., 2000" startWordPosition="392" endWordPosition="395">i.e. matching component parts to known lemmas). The paper is organized as follows: Section 2 gives an overview of related research; Section 3 describes the compound analyzer used in our experiments; Section 4 presents experimental results; Section 5 contains error analysis and discussion. Section 6 concludes and suggests future research. 2 Related research Current research on compound splitting is predominantly lexicon-based, with a range of selection methods to choose the most likely decomposition. The lexicons used to identify components are usually collected from large monolingual corpora (Larson et al., 2000; Monz and de Rijke, 2001; Alfonseca et al, 2008; Holz and Biemann, 2008; von Huyssteen and von Zaanen, 2004). The problem with pure lexicon-based approach without any constraints is that it will produce many spurious decompositions, matching small substrings that happen to be legitimate words in the This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 63 Proceedings of the First Workshop on Computational Approaches to Compound Analysis,</context>
</contexts>
<marker>Larson, Willett, Köhler, Rigoll, 2000</marker>
<rawString>Larson, Martha, Daniel Willett, Joachin Köhler and Gerhard Rigoll. 2000. Compound splitting and lexical unit recombination for improved performance of a speech recognition system for German parliamentary speeches In INTERSPEECH, 945-948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Macherey</author>
<author>Andrew M Dai</author>
<author>David Talbot</author>
<author>Ashok C Popat</author>
<author>Franz Och</author>
</authors>
<title>Language-independent compound splitting with Morphological Operations.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1395--1404</pages>
<contexts>
<context position="3753" citStr="Macherey et al., 2011" startWordPosition="518" endWordPosition="521">en to be legitimate words in the This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 63 Proceedings of the First Workshop on Computational Approaches to Compound Analysis, pages 63–71, Dublin, Ireland, August 24 2014. language. Therefore, some approaches introduce maximizing component length (or, conversely, minimizing the number of components) as one of the selection factors (von Huyssteen and von Zaanen, 2004; Holz and Biemann, 2008; Macherey et al., 2011; Larson et al., 2000); others use part of speech to eliminate short components which tend to be function words (Koehn and Knight, 2003; Monz and de Rijke, 2001). In other cases, Named Entity Recognition is used to filter out proper names that should not be decomposed but that can contain frequent short components like “-berg” or “-dorf” (Alfonseca et al., 2008). Even after removing unlikely small component candidates, there is enough ambiguity in decomposition to warrant further filtering methods. And so, approaches related to Machine Translation use bilingual parallel corpora to find the mos</context>
</contexts>
<marker>Macherey, Dai, Talbot, Popat, Och, 2011</marker>
<rawString>Macherey, Klaus, Andrew M. Dai, David Talbot, Ashok C. Popat and Franz Och. 2011. Language-independent compound splitting with Morphological Operations. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, 1395-1404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof Monz</author>
<author>Maarten de Rijke</author>
</authors>
<title>Shallow Morphological Analysis in Monolingual Information Retrieval for Dutch, German and Italian.</title>
<date>2002</date>
<booktitle>In Evaluation of Cross-Language Information Retrieval</booktitle>
<pages>262--277</pages>
<editor>Systems. Carol Peters, Martin Braschler, Julio Gonzalo and Michael Kluck (eds.).</editor>
<publisher>Springer Verlag,</publisher>
<location>Berlin and Heidelberg,</location>
<marker>Monz, de Rijke, 2002</marker>
<rawString>Monz, Christof and Maarten de Rijke. 2002. Shallow Morphological Analysis in Monolingual Information Retrieval for Dutch, German and Italian. In Evaluation of Cross-Language Information Retrieval Systems. Carol Peters, Martin Braschler, Julio Gonzalo and Michael Kluck (eds.). Springer Verlag, Berlin and Heidelberg, 262-277.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerhard van Huyssteen</author>
<author>Menno van Zaanen</author>
</authors>
<title>Learning Compound Boundaries for Afrikaans Spelling Checking.</title>
<date>2004</date>
<booktitle>In Pre-Proceedings of the Workshop on International Proofing Tools and Language Technologies; Patras,</booktitle>
<marker>van Huyssteen, van Zaanen, 2004</marker>
<rawString>van Huyssteen, Gerhard and Menno van Zaanen. 2004. Learning Compound Boundaries for Afrikaans Spelling Checking. In Pre-Proceedings of the Workshop on International Proofing Tools and Language Technologies; Patras, Greece. 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Menno van Zaanen</author>
<author>Gerhard van Huyssteen</author>
<author>Suzanne Aussems</author>
<author>Chris Emmery</author>
<author>Roald Eiselen</author>
</authors>
<title>The Development of Dutch and Afrikaans Language Resources for Compound Boundary Analysis.</title>
<date>2014</date>
<booktitle>In Proceeding of LREC</booktitle>
<marker>van Zaanen, van Huyssteen, Aussems, Emmery, Eiselen, 2014</marker>
<rawString>van Zaanen, Menno, Gerhard van Huyssteen, Suzanne Aussems, Chris Emmery, and Roald Eiselen. 2014. The Development of Dutch and Afrikaans Language Resources for Compound Boundary Analysis. In Proceeding of LREC 2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
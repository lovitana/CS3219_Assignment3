<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.072228">
<title confidence="0.998307">
Parsing Screenplays for Extracting Social Networks from Movies
</title>
<author confidence="0.990005">
Apoorv Agarwal†, Sriramkumar Balasubramanian†, Jiehan Zheng$, Sarthak Dash†
</author>
<affiliation confidence="0.983292">
†Dept. of Computer Science $Peddie School
Columbia University Hightstown, NJ, USA
</affiliation>
<address confidence="0.91349">
New York, NY, USA
</address>
<email confidence="0.998997">
apoorv@cs.columbia.edu jzheng-14@peddie.org
</email>
<sectionHeader confidence="0.993884" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999725">
In this paper, we present a formalization
of the task of parsing movie screenplays.
While researchers have previously moti-
vated the need for parsing movie screen-
plays, to the best of our knowledge, there
is no work that has presented an evalua-
tion for the task. Moreover, all the ap-
proaches in the literature thus far have
been regular expression based. In this pa-
per, we present an NLP and ML based
approach to the task, and show that this
approach outperforms the regular expres-
sion based approach by a large and statis-
tically significant margin. One of the main
challenges we faced early on was the ab-
sence of training and test data. We pro-
pose a methodology for using well struc-
tured screenplays to create training data
for anticipated anomalies in the structure
of screenplays.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999919192307693">
Social network extraction from unstructured text
has recently gained much attention (Agarwal and
Rambow, 2010; Elson et al., 2010; Agarwal et al.,
2013a; Agarwal et al., 2013b; He et al., 2013). Us-
ing Natural Language Processing (NLP) and Ma-
chine Learning (ML) techniques, researchers are
now able to gain access to networks that are not
associated with any meta-data (such as email links
and self-declared friendship links). Movies, which
can be seen as visual approximations of unstruc-
tured literary works, contain rich social networks
formed by interactions between characters. There
has been some effort in the past to extract social
networks from movies (Weng et al., 2006; Weng
et al., 2007; Weng et al., 2009; Gil et al., 2011).
However, these approaches are primarily regular
expression based with no evaluation of how well
they work.
In this paper we introduce a formalization of the
task of parsing screenplays and present an NLP
and ML based approach to the task. By parsing
a screenplay, we mean assigning each line of the
screenplay one of the following five tags: “S” for
scene boundary, “N” for scene description, “C”
for character name, “D” for dialogue, and “M” for
meta-data. We expect screenplays to conform to
a strict grammar but they often do not (Gil et al.,
2011). This disconnect gives rise to the need for
developing a methodology that is able to handle
anomalies in the structure of screenplays. Though
the methodology proposed in this paper is in the
context of movie screenplays, we believe, it is gen-
eral and applicable to parse other kinds of noisy
documents.
One of the earliest challenges we faced was the
absence of training and test data. Screenplays, on
average, have 7,000 lines of text, which limits the
amount of annotated data we can obtain from hu-
mans. We propose a methodology for using well
structured screenplays to create training data for
anticipated anomalies in the structure of screen-
plays. For different types of anomalies, we train
separate classifiers, and combine them using en-
semble learning. We show that our ensemble out-
performs a regular-expression baseline by a large
and statistically significant margin on an unseen
test set (0.69 versus 0.96 macro-F1 measure for the
five classes). Apart from performing an intrinsic
evaluation, we also present an extrinsic evaluation.
We show that the social network extracted from
the screenplay tagged by our ensemble is closer
to the network extracted from a screenplay tagged
</bodyText>
<page confidence="0.964979">
50
</page>
<note confidence="0.990731">
Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 50–58,
Gothenburg, Sweden, April 27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996806">
by a human, as compared to the network extracted
from a screenplay tagged by the baseline.
The rest of the paper is structured as follows: in
section 2, we present common terminology used
to describe screenplays. We survey existing liter-
ature in section 3. Section 4 presents details of
our data collection methodology, along with the
data distribution. Section 5 gives details of our
regular-expression based system, which we use as
a baseline for evaluation purposes. In section 6,
we present our machine learning approach. In sec-
tion 7, we give details of the features we use for
machine learning. In section 8, we present our ex-
periments and results. We conclude and give fu-
ture directions of research in section 9.
</bodyText>
<sectionHeader confidence="0.998013" genericHeader="introduction">
2 Terminology
</sectionHeader>
<bodyText confidence="0.9999919">
Turetsky and Dimitrova (2004) describe the
structure of a movie screenplay as follows: a
screenplay describes a story, characters, action,
setting and dialogue of a film. Additionally,
they report that the structure of a screenplay
follows a (semi) regular format. Figure 1 shows
a snippet of a screenplay from the film – The
Silence of the Lambs. A scene (tag “S”) starts
with what is called the slug line (or scene bound-
ary). The slug line indicates whether the scene
is to take place inside or outside (INT, EXT),
the name of the location (“FBI ACADEMY
GROUNDS, QUANTICO, VIRGINIA”), and
can potentially specify the time of day (e.g.
DAY or NIGHT). Following the scene boundary
is a scene description. A scene description
is followed by a character name (tag “C”),
which is followed by dialogues (tag “D”).
Character names are capitalized, with an optional
(V.O.) for “Voice Over” or (O.S.) for “Off-screen.”
Dialogues, like scene descriptions, are not asso-
ciated with any explicit indicators (such as INT,
V.O.), but are indented at a unique level (i.e.
nothing else in the screenplay is indented at this
level). Screenplays may also have other elements,
such as “CUT TO:”, which are directions for the
camera, and text describing the intended mood
of the speaker, which is found within parentheses
in the dialogue. For lack of a name for these
elements, we call them “Meta-data” (tag “M”).
</bodyText>
<sectionHeader confidence="0.967723" genericHeader="method">
3 Literature Survey
</sectionHeader>
<bodyText confidence="0.9999503">
One of the earliest works motivating the need for
screenplay parsing is that of Turetsky and Dim-
itrova (2004). Turetsky and Dimitrova (2004)
proposed a system to automatically align written
screenplays with their videos. One of the crucial
steps, they noted, is to parse a screenplay into its
different elements: scene boundaries, scene de-
scriptions, character names, and dialogues. They
proposed a grammar to parse screenplays and
show results for aligning one screenplay with its
video. Weng et al. (2009) motivated the need for
screenplay parsing from a social network analy-
sis perspective. They proposed a set of opera-
tions on social networks extracted from movies
and television shows in order to find what they
called hidden semantic information. They pro-
posed techniques for identifying lead roles in bi-
lateral movies (movies with two main characters),
for performing community analysis, and for au-
tomating the task of story segmentation. Gil et
al. (2011) extracted character interaction networks
from plays and movies. They were interested in
automatically classifying plays and movies into
different genres by making use of social network
analysis metrics. They acknowledged that the
scripts found on the internet are not in consistent
formats, and proposed a regular expression based
system to identify scene boundaries and character
names.
While there is motivation in the literature to
parse screenplays, none of the aforementioned
work addresses the task formally. In this paper, we
formalize the task and propose a machine learning
based approach that is significantly more effec-
tive and tolerant of anomalous structure than the
baseline. We evaluate our models on their ability
to identify scene boundaries and character names,
but also on their ability to identify other important
elements of a screenplay, such as scene descrip-
tions and dialogues.
</bodyText>
<sectionHeader confidence="0.996774" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999947">
We crawled the Internet Movie Script Database
(IMSDB) website1 to collect movie screenplays.
We crawled a total of 674 movies. Movies
that are well structured have the property that
scene boundaries and scene descriptions, charac-
ter names, and dialogues are all at different but
fixed levels of indentation.2 For example, in the
movie in Figure 1, all scene boundaries and scene
</bodyText>
<footnote confidence="0.999467666666667">
1http://www.imsdb.com
2By level of indentation we mean the number of spaces
from the start of the line to the first non-space character.
</footnote>
<page confidence="0.998832">
51
</page>
<figureCaption confidence="0.99641">
Figure 1: Example screenplay: first column shows the tags we assign to each line in the screenplay. M
</figureCaption>
<bodyText confidence="0.999015090909091">
stands for “Meta-data”, S stands for “Scene boundary”, N stands for “Scene description”, C stands for
“Character name”, and D stands for “Dialogue.” We also show the lines that are at context -2 and +3 for
the line “CRAWFORD.”
descriptions are at the same level of indentation,
equal to five spaces. All character names are at
a different but fixed level of indentation, equal to
20 spaces. Dialogues are at an indentation level
of eight spaces. These indentation levels may vary
from one screenplay to the other, but are consis-
tent within a well formatted screenplay. Moreover,
the indentation level of character names is strictly
greater than the indentation level of dialogues,
which is strictly greater than the indentation level
of scene boundaries and scene descriptions. For
each crawled screenplay, we found the frequency
of unique indentation levels in that screenplay. If
the top three unique frequencies constituted 90%
of the total lines of a screenplay, we flagged that
the movie was well-structured, and assigned tags
based on indentation levels. Since scene bound-
aries and scene descriptions are at the same level
of indentation, we disambiguate between them by
utilizing the fact that scene boundaries in well-
formatted screenplays start with tags such as INT.
and EXT. We programmatically checked the sanity
of these automatically tagged screenplays by using
the following procedure: 1) check if scene descrip-
tions are between scene boundaries and character
names, 2) check if dialogues are between charac-
ter names, and 3) check if all character names are
within two scene boundaries. Using this method-
ology, we were able to tag 222 movies that pass
the sanity check.
</bodyText>
<table confidence="0.9973894">
Data # S # N # C # D # M
TRAIN 2,445 21,619 11,464 23,814 3,339
DEV1 714 7,495 4,431 9,378 467
DEV2 413 5,431 2,126 4,755 762
TEST 164 845 1,582 3,221 308
</table>
<tableCaption confidence="0.999428">
Table 1: Data distribution
</tableCaption>
<bodyText confidence="0.988182944444444">
Table 1 gives the distribution of our training, de-
velopment and test sets. We use a random sub-
set of the aforementioned set of 222 movies for
training purposes, and another random subset for
development. We chose 14 movies for the train-
ing set and 9 for the development set. Since hu-
man annotation for the task is expensive, instead of
getting all 23 movies checked for correctness, we
asked an annotator to only look at the development
set (9 movies). The annotator reported that one
out of 9 movies was not correctly tagged. We re-
moved this movie from the development set. From
the remaining 8 movies, we chose 5 as the first de-
velopment set and the remaining 3 as the second
development set. For the test set, we asked our
annotator to annotate a randomly chosen screen-
play (Silver Linings Playbook) from scratch. We
chose this screenplay from the set of movies that
</bodyText>
<page confidence="0.990789">
52
</page>
<bodyText confidence="0.9999462">
sure for the five tags as 0.96. This confirms that
our baseline is well suited to parse screenplays that
are well structured.
we were unable to tag automatically, i.e. not from
the set of 222 movies.
</bodyText>
<sectionHeader confidence="0.967395" genericHeader="method">
5 Baseline System
</sectionHeader>
<bodyText confidence="0.999980217391304">
Gil et al. (2011) mention the use of regular expres-
sions for tagging screenplays. However, they do
not specify the regular expressions or their exact
methodology. We use common knowledge about
the structure of the screenplay (underlined text in
section 2) to build a baseline system, that uses reg-
ular expressions and takes into account the gram-
mar of screenplays.
Since scene descriptions, characters and dia-
logues are relative to the scene boundary, we do
a first pass on the screenplay to tag scene bound-
aries. We created a dictionary of words that are
expected to indicate scene boundaries. We use
this dictionary for tagging lines in the screenplay
with the tag “S”. We tag all the lines that con-
tain tags indicating a character (V.O., O.S.) with
“C”. We built a dictionary of meta-data tags that
contains patterns such as “CUT TO:, DISSOLVE
TO.” We tag all the remaining untagged lines con-
taining these patterns with the tag “M.” This ex-
hausts the list of regular expression matches that
indicate a certain tag.
In the next pass, we incorporate prior knowl-
edge that scene boundaries and character names
are capitalized. For this, we tag all the untagged
lines that are capitalized, and that have more than
three words as scene boundaries (tag “S”). We tag
all the untagged lines that are capitalized, and that
have less than four words as character (tag “C”).
The choice of the number four is not arbitrary;
we examined the set of 222 screenplays that was
tagged using indentation information and found
that less than two percent of the character names
were of length greater than three.
Finally, we incorporate prior knowledge about
relative positions of dialogues and scene descrip-
tions to tag the remaining untagged lines with one
of two tags: “D” or “N”. We tag all the untagged
lines between a scene boundary and the first char-
acter occurrence as “N”. We tag all the lines be-
tween consecutive character occurrences, the last
character occurrence and the scene boundary as
We use this baseline system, which incorporates
all of the prior knowledge about the structure of
screenplays, to tag movies in our first development
set DEV1 (section 8). We report a macro-F1 mea-
</bodyText>
<sectionHeader confidence="0.994419" genericHeader="method">
6 Machine Learning Approach
</sectionHeader>
<bodyText confidence="0.999955071428572">
Note that our baseline system is not dependent on
the level of indentation (it achieves a high macro-
F1 measure without using indentation informa-
tion). Therefore, we have already dealt with one
common problem with screenplays found on the
web: bad indentation. However, there are other
problems, some of which we noticed in the lim-
ited data we manually examined, and others that
we anticipate: (1) missing scene boundary spe-
cific patterns (such as INT./EXT.) from the scene
boundary lines, (2) uncapitalized scene boundaries
and (3) uncapitalized character names. These are
problems that a regular expression based system
is not well equipped to deal with. In this sec-
tion, we discuss a strategy for dealing with screen-
plays, which might have anomalies in their struc-
ture, without requiring additional annotations.
We synthesize training and development data
to learn to handle the aforementioned three types
of anomalies. We create eight copies of our
TRAIN set: one with no anomalies, represented as
TRAIN_000, 3 one in which character names are
uncapitalized, represented as TRAIN_001, one in
which both scene boundaries and character names
are uncapitalized, represented as TRAIN_011,
and so on. Similarly, we create eight copies
of our DEV1 set: {DEV1_000, DEV1_001, ...,
DEV1_111}. Now we have eight training and
eight development sets. We train eight models,
and choose the parameters for each model by tun-
ing on the respective development set. However,
at test time, we require one model. Moreover, our
model should be able to handle all types of anoma-
lies (all of which could be present in a random or-
der). We experiment with three ensemble learning
techniques and choose the one that performs the
best on the second development set, DEV2. We
add all three types of anomalies, randomly, to our
DEV2 set.
For training individual models, we use Support
Vector Machines (SVMs), and represent data as
feature vectors, discussed in the next section.
</bodyText>
<footnote confidence="0.96865125">
3Each bit refers to the one type of anomaly described
in the previous paragraph. If the least significant bit is 1,
this means, the type of anomaly is uncapitalized characters
names.
</footnote>
<page confidence="0.998553">
53
</page>
<sectionHeader confidence="0.998854" genericHeader="method">
7 Features
</sectionHeader>
<bodyText confidence="0.999959986842105">
We have six sets of features: bag-of-words fea-
tures (BOW), bag-of-punctuation-marks features
(BOP), bag-of-terminology features (BOT), bag-
of-frames features (BOF), bag-of-parts-of-speech
features (POS), and hand-crafted features (HAND).
We convert each line of a screenplay (input ex-
ample) into a feature vector of length 5,497: 3,946
for BOW, 22 for BOP, 2*58 for BOT, 2*45 for
POS, 2*651 for BOF, and 21 for HAND.
BOW, BOP, and BOT are binary features; we
record the presence or absence of elements of each
bag in the input example. The number of ter-
minology features is multiplied by two because
we have one binary vector for “contains term”,
and another binary vector for “is term.” We have
two sets of features for POS and BOF. One set
is binary and similar to other binary features that
record the presence or absence of parts-of-speech
and frames in the input example. The other set
is numeric. We record the normalized counts of
each part-of-speech and frame respectively. The
impetus to design this second set of features for
parts-of-speech and frames is the following: we
expect some classes to have a characteristic dis-
tribution of parts-of-speech and frames. For ex-
ample, scene boundaries contain the location and
time of scene. Therefore, we expect them to have
a majority of nouns, and frames that are related to
location and time. For the scene boundary in Fig-
ure 1 (EXT. FBI ACADEMY... - DAY), we find
the following distribution of parts of speech and
frames: 100% nouns, 50% frame LOCALE (with
frame evoking element grounds), and 50% frame
CALENDRIC_UNIT (with frame evoking element
DAY). Similarly, we expect the character names to
have 100% nouns, and no frames.
We use Stanford part-of-speech tagger
(Toutanova et al., 2003) for obtaining the
part-of-speech tags and Semafor (Chen et al.,
2010) for obtaining the FrameNet (Baker et
al., 1998) frames present in each line of the
screenplay.
We devise 21 hand-crafted features. Six-
teen of these features are binary (0/1). We
list these features here (the feature names
are self-explanatory): has-non-alphabetical-
chars, has-digits-majority, has-alpha-majority,
is-quoted, capitalization (has-all-caps, is-all-
caps), scene boundary (has-INT, has-EXT),
date (has-date, is-date), number (has-number,
is-number), and parentheses (is-parenthesized,
starts-with-parenthesis, ends-with-parenthesis,
contains-parenthesis). We bin the preceding
number of blank lines into four bins: 0 for no
preceding blank lines, 1 for one preceding blank
line, 2 for two preceding blank lines, and so on.
We also bin the percentage of capitalized words
into four bins: 0 for the percentage of capitalized
words lying between 0-25%, 1 for 25-50%, and
so on. We use three numeric features: number of
non-space characters (normalized by the maxi-
mum number of non-space characters in any line
in a screenplay), number of words (normalized
by the maximum number of words in any line in
a screenplay), and number of characters (normal-
ized by the maximum number of characters in any
line in a screenplay).
For each line, say linei, we incorporate con-
text up to x lines. Figure 1 shows the lines
at context -2 and +3 for the line contain-
ing the text CRAWFORD. To do so, we ap-
pend the feature vector for linei by the fea-
ture vectors of linei_1, linei_2, ... linei_x and
linei+1, linei+2, ... linei+x. x is one of the pa-
rameters we tune at the time of training. We refer
to this parameter as CONTEXT.
</bodyText>
<sectionHeader confidence="0.989529" genericHeader="evaluation">
8 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999940857142857">
In this section, we present experiments and results
for the task of tagging the lines of a screenplay
with one of five tags: {S, N, C, D, M}. Table 1
shows the data distribution. For parameter tun-
ing, we use DEV1 (section 8.1). We train sepa-
rate models on different types of known and antici-
pated anomalies (as discussed in section 6). In sec-
tion 8.2, we present strategies for combining these
models. We select the right combination of mod-
els and features by tuning on DEV2. Finally, we
show results on the test set, TEST. For all our ex-
periments, we use the default parameters of SVM
as implemented by the SMO algorithm of Weka
(Hall et al., 2009). We use a linear kernel.4
</bodyText>
<subsectionHeader confidence="0.998105">
8.1 Tuning learning parameters
</subsectionHeader>
<bodyText confidence="0.9970686">
We tune two parameters: the amount of train-
ing data and the amount of CONTEXT (section 7)
required for learning. We do this for each of
the eight models (TRAIN_000/DEV1_000, ...,
TRAIN_111/DEV1_111). We merge training
</bodyText>
<footnote confidence="0.9153765">
4We tried the polynomial kernel up to a degree of four and
the RBF kernel. They performed worse than the linear kernel.
</footnote>
<page confidence="0.995989">
54
</page>
<figure confidence="0.998606764705882">
% TRAINING DATA USED
MACRO F−MEASURE
TRAIN−DEV1 (000)
1
0.95
0.9
context−0
context−1
context−2
context−3
context−4
context−5
0.7
10 20 30 40 50 60 70 80 90 100
0.85
0.8
0.75
</figure>
<figureCaption confidence="0.957516">
Figure 2: Learning curve for training on
TRAIN_000 and testing on DEV1_000. X-axis
is the % of training data, in steps of 10%. Y-axis
is the macro-F1 measure for the five classes.
</figureCaption>
<bodyText confidence="0.99976659375">
data from all 14 movies into one (TRAIN). We
then randomize the data and split it into 10 pieces
(maintaining the relative proportions of the five
classes). We plot a learning curve by adding 10%
of training data at each step.
Figure 2 shows the learning curve for train-
ing a model on TRAIN_000 and testing on
DEV1_000.5 The learning curve shows that the
performance of our classifier without any context
is significantly worse than the classifiers trained
on context. Moreover, the learning saturates early,
and stabilizes at about 50% of the training data.
From the learning curves, we pick CONTEXT
equal to 1, and the amount of training data equal
to 50% of the entire training set.
Table 2 shows a comparison of our rule based
baseline with the models trained using machine
learning. For the 000 setting, when there is no
anomaly in the screenplay, our rule based base-
line performs well, achieving a macro-F1 measure
of 0.96. However, our machine learning model
outperforms the baseline by a statistically signif-
icant margin, achieving a macro-F1 measure of
0.99. We calculate statistical significance using
McNemar’s significance test, with significance de-
fined as p &lt; 0.05.6 Results in Table 2 also show
that while a deterministic regular-expression based
system is not well equipped to handle anomalies,
there is enough value in our feature set, that our
machine learning based models learn to adapt to
any combination of the three types of anomalies,
achieving a high F1-measure of 0.98 on average.
</bodyText>
<footnote confidence="0.999363333333333">
5Learning curves for all our other models were similar.
6We use the same test for reporting other statistically sig-
nificance results in the paper.
</footnote>
<subsectionHeader confidence="0.983926">
8.2 Finding the right ensemble and feature
selection
</subsectionHeader>
<bodyText confidence="0.99996075">
We have trained eight separate models, which
need to be combined into one model that we will
make predictions at the test time. We explore the
following ways of combining these models:
</bodyText>
<listItem confidence="0.998077444444444">
1. MA7: Given a test example, we get a vote
from each of our eight models, and take a ma-
jority vote. At times of a clash, we pick one
randomly.
2. MAX: We pick the class predicted by the
model that has the highest confidence in its
prediction. Since the confidence values are
real numbers, we do not see any clashes.
3. MA7-MAX: We use MA7 but at times of a
</listItem>
<bodyText confidence="0.996314029411765">
clash, we pick the class predicted by the clas-
sifier that has the highest confidence (among
the classifiers that clash).
Table 3 shows macro-F1 measures for the three
movies in our DEV2 set. Note, we added the three
types of anomalies (section 6) randomly to the
DEV2 set for tuning the type of ensemble. We
compare the performance of the three ensemble
techniques with the individual classifiers (trained
on TRAIN_000, ... TRAIN_111).
The results show that all our ensembles (ex-
cept MAX for the movie The Last Temptation of
Christ) perform better than the individual models.
Moreover, the MA7-MAX ensemble outperforms
the other two by a statistically significant margin.
We thus choose MA7-MAX as our final classifier.
Table 4 shows results for removing one of all
feature sets, one at a time. These results are for our
final model, MA7-MAX. The row “All” shows the
results when we use all our features for training.
The consecutive rows show the result when we re-
move the mentioned feature set. For example, the
row “- BOW” shows the result for our classifier
that was trained without the bag of words feature
set.
Table 4 shows that the performance drops the
most for bag of words (BOW) and for our hand-
crafted features (HAND). The next highest drop
is for the bag of frames feature set (BOF). Er-
ror analysis revealed that the major drop in per-
formance because of the removal of the BOF fea-
tures was not due the drop in the performance
of scene boundaries, counter to our initial intu-
ition. The drop was because the recall of dia-
</bodyText>
<page confidence="0.997372">
55
</page>
<table confidence="0.999765">
000 001 010 011 100 101 110 111
Rule based 0.96 0.49 0.70 0.23 0.93 0.46 0.70 0.24
ML model 0.99 0.99 0.98 0.99 0.97 0.98 0.98 0.98
</table>
<tableCaption confidence="0.981864666666667">
Table 2: Comparison of performance (macro-F1 measure) of our rule based baseline with our machine
learning based models on development sets DEV1_000, DEV1_001, ..., DEV1_111. All models are
trained on 50% of the training set, with the feature space including CONTEXT equal to 1.
</tableCaption>
<table confidence="0.9999756">
Movie 000 001 010 011 100 101 110 111 MAJ MAX MAJ-MAX
LTC 0.87 0.83 0.79 0.94 0.91 0.86 0.79 0.96 0.97 0.95 0.98
X-files 0.87 0.84 0.79 0.93 0.86 0.84 0.79 0.92 0.94 0.94 0.96
Titanic 0.87 0.87 0.81 0.94 0.86 0.83 0.82 0.93 0.94 0.95 0.97
Average 0.87 0.85 0.80 0.94 0.88 0.84 0.80 0.94 0.95 0.95 0.97
</table>
<tableCaption confidence="0.98574025">
Table 3: Macro-F1 measure for the five classes for testing on DEV2 set. 000 refers to the model trained
on data TRAIN_000, 001 refers to the model trained on data TRAIN_001, and so on. MAJ, MAX, and
MAJ-MAX are the three ensembles. The first column is the movie name. LTC refers to the movie “The
Last Temptation of Christ.”
</tableCaption>
<table confidence="0.999974625">
Feature set LTC X-files Titanic
All 0.98 0.96 0.97
- BOW 0.94 0.92 0.94
- BOP 0.98 0.97 0.97
- BOT 0.97 0.95 0.96
- BOF 0.96 0.93 0.96
- POS 0.98 0.96 0.95
- HAND 0.94 0.93 0.93
</table>
<tableCaption confidence="0.957115333333333">
Table 4: Performance of MAJ-MAX classifier with
feature removal. Statistically significant differ-
ences are in bold.
</tableCaption>
<bodyText confidence="0.999116631578947">
logues decreases significantly. The BOF features
were helping in disambiguating between the meta-
data, which usually have no frames associated
with them, and dialogues. Removing bag of punc-
tuation (BOP) results in a significant increase in
the performance for the movie Xffiles, with a small
increase for other two movies. We remove this
feature from our final classifier. Removing parts
of speech (POS) results in a significant drop in the
overall performance for the movie Titanic. Error
analysis revealed that the drop in performance here
was in fact due the drop in performance of scene
boundaries. Scene boundaries almost always have
100% nouns and the POS features help in cap-
turing this characteristic distribution indicative of
scene boundaries. Removing bag of terminology
(BOT) results in a significant drop in the overall
performance of all movies. Our results also show
that though the drop in performance for some fea-
</bodyText>
<table confidence="0.99854125">
Baseline MAJ-MAX
Tag P R F1 P R F1
S 0.27 1.00 0.43 0.99 1.00 0.99
N 0.21 0.06 0.09 0.88 0.95 0.91
C 0.89 1.00 0.94 1 0.92 0.96
D 0.99 0.94 0.96 0.98 0.998 0.99
M 0.68 0.94 0.79 0.94 0.997 0.97
Avg 0.61 0.79 0.69 0.96 0.97 0.96
</table>
<tableCaption confidence="0.761119">
Table 5: Performance comparison of our rule
based baseline with our best machine learning
model on the five classes.
</tableCaption>
<table confidence="0.99805875">
NB NMAJ-MAX NG
# Nodes 202 37 41
# Links 1252 331 377
Density 0.036 0.276 0.255
</table>
<tableCaption confidence="0.779608666666667">
Table 6: A comparison of network statistics for
the three networks extracted from the movie Silver
Linings Playbook.
</tableCaption>
<bodyText confidence="0.999758">
ture sets is larger than the others, it is the conjunc-
tion of all features that is responsible for a high
F1-measure.
</bodyText>
<subsectionHeader confidence="0.992461">
8.3 Performance on the test set
</subsectionHeader>
<bodyText confidence="0.9999304">
Table 5 shows a comparison of the performance
of our rule based baseline with our best machine
learning based model on our test set, TEST. The
results show that our machine learning based mod-
els outperform the baseline with a large and sig-
</bodyText>
<page confidence="0.994287">
56
</page>
<table confidence="0.997003666666667">
Model Degree Weighted Degree Closeness Betweenness PageRank Eigen
NB 0.919 0.986 0.913 0.964 0.953 0.806
NMAJ-MAX 0.997 0.997 0.997 0.997 0.998 0.992
</table>
<tableCaption confidence="0.875274">
Table 7: A comparison of Pearson’s correlation coefficients of various centrality measures for NB and
NMAJ-MAX with NG.
</tableCaption>
<bodyText confidence="0.999556928571429">
nificant margin on all five classes (0.96 versus
0.69 macro-F1 measure respectively). Note, as ex-
pected, the recall of the baseline is generally high,
while the precision is low. Moreover, for this test
set, the baseline performs relatively well on tag-
ging character names and dialogues. However, we
believe that the performance of the baseline is un-
predictable. It may get lucky on screenplays that
are well-structured (in one way or the other), but
it is hard to comment on the robustness of its per-
formance. On the contrary, our ensemble is ro-
bust, hedging its bets on eight models, which are
trained to handle different types and combinations
of anomalies.
In tables 6 and 7, we present an extrinsic evalua-
tion on the test set. We extract a network from our
test movie screenplay (Silver Linings Playbook)
by using the tags of the screenplay as follows
(Weng et al., 2009): we connect all characters hav-
ing a dialogue with each other in a scene with
links. Nodes in this network are characters, and
links between two characters signal their partici-
pation in the same scene. We form three such net-
works: 1) based on the gold tags (NG), 2) based on
the tags predicted by MAJ-MAX (NMAJ-MAX),
and 3) based on the tags predicted by our base-
line (NB). Table 6 compares the number of nodes,
number of links, and graph density of the three
networks. It is clear from the table that the net-
work extracted by using the tags predicted by
MAJ-MAX is closer to the gold network.
Centrality measures are one of the most funda-
mental social network analysis metrics used by so-
cial scientists (Wasserman and Faust, 1994). Ta-
ble 7 presents a comparison of Pearson’s correla-
tion coefficient for various centrality measures for
{NB, NG}, and {NMAJ-MAX, NG} for the top
ten characters in the movie. The table shows that
across all these measures, the statistics obtained
using the network NMAJ-MAX are significantly
more correlated to the gold network (NG), as com-
pared the the baseline network (NB).
</bodyText>
<sectionHeader confidence="0.98267" genericHeader="conclusions">
9 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999957222222222">
In this paper, we presented a formalization of the
task of parsing movie screenplays. We presented
an NLP and ML based approach to the task, and
showed that this approach outperforms the regular
expression based approach by a large and signifi-
cant margin. One of the main challenges we faced
early on was the absence of training and test data.
We proposed a methodology for learning to han-
dle anomalies in the structure of screenplays with-
out requiring additional annotations. We believe
that the machine learning approach proposed in
this paper is general, and may be used for parsing
noisy documents outside of the context of movie
screenplays.
In the future, we will apply our approach to
parse other semi-structured sources of social net-
works such as television show series and theatrical
plays.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999846222222222">
We would like to thank anonymous reviewers for
their useful comments. We would also like to
thank Caronae Howell for her insightful com-
ments. Agarwal was funded by IBM Ph.D. fellow-
ship 2013-2014. This paper is based upon work
supported in part by the DARPA DEFT Program.
The views expressed are those of the authors and
do not reflect the official policy or position of the
Department of Defense or the U.S. Government.
</bodyText>
<page confidence="0.997868">
57
</page>
<sectionHeader confidence="0.995873" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999898681159421">
Apoorv Agarwal and Owen Rambow. 2010. Auto-
matic detection and classification of social events.
In Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1024–1034, Cambridge, MA, October. Association
for Computational Linguistics.
Apoorv Agarwal, Anup Kotalwar, and Owen Ram-
bow. 2013a. Automatic extraction of social net-
works from literary text: A case study on alice in
wonderland. In the Proceedings of the 6th Interna-
tional Joint Conference on Natural Language Pro-
cessing (IJCNLP 2013).
Apoorv Agarwal, Anup Kotalwar, Jiehan Zheng, and
Owen Rambow. 2013b. Sinnet: Social interaction
network extractor from text. In Sixth International
Joint Conference on Natural Language Processing,
page 33.
C. Baker, C. Fillmore, and J. Lowe. 1998. The berke-
ley framenet project. Proceedings of the 17th inter-
national conference on Computational linguistics, 1.
Desai Chen, Nathan Schneider, Dipanjan Das, and
Noah A. Smith. 2010. Semafor: Frame argument
resolution with log-linear models. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, pages 264–267, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
David K. Elson, Nicholas Dames, and Kathleen R.
McKeown. 2010. Extracting social networks from
literary fiction. Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 138–147.
Sebastian Gil, Laney Kuenzel, and Suen Caroline.
2011. Extraction and analysis of character interac-
tion networks from plays and movies. Technical re-
port, Stanford University.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11.
Hua He, Denilson Barbosa, and Grzegorz Kondrak.
2013. Identification of speakers in novels. The
51st Annual Meeting of the Association for Compu-
tational Linguistics (ACL 2013).
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of HLT-NAACL.
Robert Turetsky and Nevenka Dimitrova. 2004.
Screenplay alignment for closed-system speaker
identification and analysis of feature films. In Mul-
timedia and Expo, 2004. ICME’04. 2004 IEEE In-
ternational Conference on, volume 3, pages 1659–
1662. IEEE.
Stanley Wasserman and Katherine Faust. 1994. Social
Network Analysis: Methods and Applications. New
York: Cambridge University Press.
Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu. 2006.
Movie analysis based on roles’ social network. In
Proceedings of IEEE Int. Conference Multimedia
and Expo., pages 1403–1406.
Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu. 2007.
Rolenet: treat a movie as a small society. In Pro-
ceedings of the international workshop on Workshop
on multimedia information retrieval, pages 51–60.
ACM.
Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu. 2009.
Rolenet: Movie analysis from the perspective of so-
cial networks. Multimedia, IEEE Transactions on,
11(2):256–271.
</reference>
<page confidence="0.999259">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.940712">
<title confidence="0.999975">Parsing Screenplays for Extracting Social Networks from Movies</title>
<author confidence="0.999503">Sriramkumar Jiehan Sarthak</author>
<affiliation confidence="0.989531">of Computer Science School Columbia University Hightstown, NJ,</affiliation>
<address confidence="0.998779">New York, NY, USA</address>
<email confidence="0.970583">apoorv@cs.columbia.edujzheng-14@peddie.org</email>
<abstract confidence="0.999575047619047">In this paper, we present a formalization of the task of parsing movie screenplays. While researchers have previously motivated the need for parsing movie screenplays, to the best of our knowledge, there is no work that has presented an evaluation for the task. Moreover, all the approaches in the literature thus far have been regular expression based. In this paper, we present an NLP and ML based approach to the task, and show that this approach outperforms the regular expression based approach by a large and statistically significant margin. One of the main challenges we faced early on was the absence of training and test data. We propose a methodology for using well structured screenplays to create training data anticipated the structure of screenplays.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Owen Rambow</author>
</authors>
<title>Automatic detection and classification of social events.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1024--1034</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="1199" citStr="Agarwal and Rambow, 2010" startWordPosition="184" endWordPosition="187">roaches in the literature thus far have been regular expression based. In this paper, we present an NLP and ML based approach to the task, and show that this approach outperforms the regular expression based approach by a large and statistically significant margin. One of the main challenges we faced early on was the absence of training and test data. We propose a methodology for using well structured screenplays to create training data for anticipated anomalies in the structure of screenplays. 1 Introduction Social network extraction from unstructured text has recently gained much attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). Using Natural Language Processing (NLP) and Machine Learning (ML) techniques, researchers are now able to gain access to networks that are not associated with any meta-data (such as email links and self-declared friendship links). Movies, which can be seen as visual approximations of unstructured literary works, contain rich social networks formed by interactions between characters. There has been some effort in the past to extract social networks from movies (Weng et al., 2006; Weng et al., 2007; Weng et al.</context>
</contexts>
<marker>Agarwal, Rambow, 2010</marker>
<rawString>Apoorv Agarwal and Owen Rambow. 2010. Automatic detection and classification of social events. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1024–1034, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Anup Kotalwar</author>
<author>Owen Rambow</author>
</authors>
<title>Automatic extraction of social networks from literary text: A case study on alice in wonderland.</title>
<date>2013</date>
<booktitle>In the Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<contexts>
<context position="1241" citStr="Agarwal et al., 2013" startWordPosition="192" endWordPosition="195">egular expression based. In this paper, we present an NLP and ML based approach to the task, and show that this approach outperforms the regular expression based approach by a large and statistically significant margin. One of the main challenges we faced early on was the absence of training and test data. We propose a methodology for using well structured screenplays to create training data for anticipated anomalies in the structure of screenplays. 1 Introduction Social network extraction from unstructured text has recently gained much attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). Using Natural Language Processing (NLP) and Machine Learning (ML) techniques, researchers are now able to gain access to networks that are not associated with any meta-data (such as email links and self-declared friendship links). Movies, which can be seen as visual approximations of unstructured literary works, contain rich social networks formed by interactions between characters. There has been some effort in the past to extract social networks from movies (Weng et al., 2006; Weng et al., 2007; Weng et al., 2009; Gil et al., 2011). However, these </context>
</contexts>
<marker>Agarwal, Kotalwar, Rambow, 2013</marker>
<rawString>Apoorv Agarwal, Anup Kotalwar, and Owen Rambow. 2013a. Automatic extraction of social networks from literary text: A case study on alice in wonderland. In the Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Anup Kotalwar</author>
<author>Jiehan Zheng</author>
<author>Owen Rambow</author>
</authors>
<title>Sinnet: Social interaction network extractor from text.</title>
<date>2013</date>
<booktitle>In Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>33</pages>
<contexts>
<context position="1241" citStr="Agarwal et al., 2013" startWordPosition="192" endWordPosition="195">egular expression based. In this paper, we present an NLP and ML based approach to the task, and show that this approach outperforms the regular expression based approach by a large and statistically significant margin. One of the main challenges we faced early on was the absence of training and test data. We propose a methodology for using well structured screenplays to create training data for anticipated anomalies in the structure of screenplays. 1 Introduction Social network extraction from unstructured text has recently gained much attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). Using Natural Language Processing (NLP) and Machine Learning (ML) techniques, researchers are now able to gain access to networks that are not associated with any meta-data (such as email links and self-declared friendship links). Movies, which can be seen as visual approximations of unstructured literary works, contain rich social networks formed by interactions between characters. There has been some effort in the past to extract social networks from movies (Weng et al., 2006; Weng et al., 2007; Weng et al., 2009; Gil et al., 2011). However, these </context>
</contexts>
<marker>Agarwal, Kotalwar, Zheng, Rambow, 2013</marker>
<rawString>Apoorv Agarwal, Anup Kotalwar, Jiehan Zheng, and Owen Rambow. 2013b. Sinnet: Social interaction network extractor from text. In Sixth International Joint Conference on Natural Language Processing, page 33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>C Fillmore</author>
<author>J Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>1</pages>
<contexts>
<context position="17583" citStr="Baker et al., 1998" startWordPosition="2894" endWordPosition="2897">erefore, we expect them to have a majority of nouns, and frames that are related to location and time. For the scene boundary in Figure 1 (EXT. FBI ACADEMY... - DAY), we find the following distribution of parts of speech and frames: 100% nouns, 50% frame LOCALE (with frame evoking element grounds), and 50% frame CALENDRIC_UNIT (with frame evoking element DAY). Similarly, we expect the character names to have 100% nouns, and no frames. We use Stanford part-of-speech tagger (Toutanova et al., 2003) for obtaining the part-of-speech tags and Semafor (Chen et al., 2010) for obtaining the FrameNet (Baker et al., 1998) frames present in each line of the screenplay. We devise 21 hand-crafted features. Sixteen of these features are binary (0/1). We list these features here (the feature names are self-explanatory): has-non-alphabeticalchars, has-digits-majority, has-alpha-majority, is-quoted, capitalization (has-all-caps, is-allcaps), scene boundary (has-INT, has-EXT), date (has-date, is-date), number (has-number, is-number), and parentheses (is-parenthesized, starts-with-parenthesis, ends-with-parenthesis, contains-parenthesis). We bin the preceding number of blank lines into four bins: 0 for no preceding bla</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C. Baker, C. Fillmore, and J. Lowe. 1998. The berkeley framenet project. Proceedings of the 17th international conference on Computational linguistics, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Desai Chen</author>
<author>Nathan Schneider</author>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Semafor: Frame argument resolution with log-linear models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>264--267</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="17535" citStr="Chen et al., 2010" startWordPosition="2886" endWordPosition="2889">ries contain the location and time of scene. Therefore, we expect them to have a majority of nouns, and frames that are related to location and time. For the scene boundary in Figure 1 (EXT. FBI ACADEMY... - DAY), we find the following distribution of parts of speech and frames: 100% nouns, 50% frame LOCALE (with frame evoking element grounds), and 50% frame CALENDRIC_UNIT (with frame evoking element DAY). Similarly, we expect the character names to have 100% nouns, and no frames. We use Stanford part-of-speech tagger (Toutanova et al., 2003) for obtaining the part-of-speech tags and Semafor (Chen et al., 2010) for obtaining the FrameNet (Baker et al., 1998) frames present in each line of the screenplay. We devise 21 hand-crafted features. Sixteen of these features are binary (0/1). We list these features here (the feature names are self-explanatory): has-non-alphabeticalchars, has-digits-majority, has-alpha-majority, is-quoted, capitalization (has-all-caps, is-allcaps), scene boundary (has-INT, has-EXT), date (has-date, is-date), number (has-number, is-number), and parentheses (is-parenthesized, starts-with-parenthesis, ends-with-parenthesis, contains-parenthesis). We bin the preceding number of bl</context>
</contexts>
<marker>Chen, Schneider, Das, Smith, 2010</marker>
<rawString>Desai Chen, Nathan Schneider, Dipanjan Das, and Noah A. Smith. 2010. Semafor: Frame argument resolution with log-linear models. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 264–267, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David K Elson</author>
<author>Nicholas Dames</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting social networks from literary fiction.</title>
<date>2010</date>
<booktitle>Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>138--147</pages>
<contexts>
<context position="1219" citStr="Elson et al., 2010" startWordPosition="188" endWordPosition="191">thus far have been regular expression based. In this paper, we present an NLP and ML based approach to the task, and show that this approach outperforms the regular expression based approach by a large and statistically significant margin. One of the main challenges we faced early on was the absence of training and test data. We propose a methodology for using well structured screenplays to create training data for anticipated anomalies in the structure of screenplays. 1 Introduction Social network extraction from unstructured text has recently gained much attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). Using Natural Language Processing (NLP) and Machine Learning (ML) techniques, researchers are now able to gain access to networks that are not associated with any meta-data (such as email links and self-declared friendship links). Movies, which can be seen as visual approximations of unstructured literary works, contain rich social networks formed by interactions between characters. There has been some effort in the past to extract social networks from movies (Weng et al., 2006; Weng et al., 2007; Weng et al., 2009; Gil et al., </context>
</contexts>
<marker>Elson, Dames, McKeown, 2010</marker>
<rawString>David K. Elson, Nicholas Dames, and Kathleen R. McKeown. 2010. Extracting social networks from literary fiction. Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 138–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Gil</author>
<author>Laney Kuenzel</author>
<author>Suen Caroline</author>
</authors>
<title>Extraction and analysis of character interaction networks from plays and movies.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>Stanford University.</institution>
<contexts>
<context position="1824" citStr="Gil et al., 2011" startWordPosition="288" endWordPosition="291">et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). Using Natural Language Processing (NLP) and Machine Learning (ML) techniques, researchers are now able to gain access to networks that are not associated with any meta-data (such as email links and self-declared friendship links). Movies, which can be seen as visual approximations of unstructured literary works, contain rich social networks formed by interactions between characters. There has been some effort in the past to extract social networks from movies (Weng et al., 2006; Weng et al., 2007; Weng et al., 2009; Gil et al., 2011). However, these approaches are primarily regular expression based with no evaluation of how well they work. In this paper we introduce a formalization of the task of parsing screenplays and present an NLP and ML based approach to the task. By parsing a screenplay, we mean assigning each line of the screenplay one of the following five tags: “S” for scene boundary, “N” for scene description, “C” for character name, “D” for dialogue, and “M” for meta-data. We expect screenplays to conform to a strict grammar but they often do not (Gil et al., 2011). This disconnect gives rise to the need for de</context>
<context position="6875" citStr="Gil et al. (2011)" startWordPosition="1115" endWordPosition="1118">ons, character names, and dialogues. They proposed a grammar to parse screenplays and show results for aligning one screenplay with its video. Weng et al. (2009) motivated the need for screenplay parsing from a social network analysis perspective. They proposed a set of operations on social networks extracted from movies and television shows in order to find what they called hidden semantic information. They proposed techniques for identifying lead roles in bilateral movies (movies with two main characters), for performing community analysis, and for automating the task of story segmentation. Gil et al. (2011) extracted character interaction networks from plays and movies. They were interested in automatically classifying plays and movies into different genres by making use of social network analysis metrics. They acknowledged that the scripts found on the internet are not in consistent formats, and proposed a regular expression based system to identify scene boundaries and character names. While there is motivation in the literature to parse screenplays, none of the aforementioned work addresses the task formally. In this paper, we formalize the task and propose a machine learning based approach t</context>
<context position="11380" citStr="Gil et al. (2011)" startWordPosition="1867" endWordPosition="1870"> 9 movies was not correctly tagged. We removed this movie from the development set. From the remaining 8 movies, we chose 5 as the first development set and the remaining 3 as the second development set. For the test set, we asked our annotator to annotate a randomly chosen screenplay (Silver Linings Playbook) from scratch. We chose this screenplay from the set of movies that 52 sure for the five tags as 0.96. This confirms that our baseline is well suited to parse screenplays that are well structured. we were unable to tag automatically, i.e. not from the set of 222 movies. 5 Baseline System Gil et al. (2011) mention the use of regular expressions for tagging screenplays. However, they do not specify the regular expressions or their exact methodology. We use common knowledge about the structure of the screenplay (underlined text in section 2) to build a baseline system, that uses regular expressions and takes into account the grammar of screenplays. Since scene descriptions, characters and dialogues are relative to the scene boundary, we do a first pass on the screenplay to tag scene boundaries. We created a dictionary of words that are expected to indicate scene boundaries. We use this dictionary</context>
</contexts>
<marker>Gil, Kuenzel, Caroline, 2011</marker>
<rawString>Sebastian Gil, Laney Kuenzel, and Suen Caroline. 2011. Extraction and analysis of character interaction networks from plays and movies. Technical report, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<contexts>
<context position="19830" citStr="Hall et al., 2009" startWordPosition="3269" endWordPosition="3272">eriments and results for the task of tagging the lines of a screenplay with one of five tags: {S, N, C, D, M}. Table 1 shows the data distribution. For parameter tuning, we use DEV1 (section 8.1). We train separate models on different types of known and anticipated anomalies (as discussed in section 6). In section 8.2, we present strategies for combining these models. We select the right combination of models and features by tuning on DEV2. Finally, we show results on the test set, TEST. For all our experiments, we use the default parameters of SVM as implemented by the SMO algorithm of Weka (Hall et al., 2009). We use a linear kernel.4 8.1 Tuning learning parameters We tune two parameters: the amount of training data and the amount of CONTEXT (section 7) required for learning. We do this for each of the eight models (TRAIN_000/DEV1_000, ..., TRAIN_111/DEV1_111). We merge training 4We tried the polynomial kernel up to a degree of four and the RBF kernel. They performed worse than the linear kernel. 54 % TRAINING DATA USED MACRO F−MEASURE TRAIN−DEV1 (000) 1 0.95 0.9 context−0 context−1 context−2 context−3 context−4 context−5 0.7 10 20 30 40 50 60 70 80 90 100 0.85 0.8 0.75 Figure 2: Learning curve fo</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDD Explorations, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua He</author>
<author>Denilson Barbosa</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Identification of speakers in novels.</title>
<date>2013</date>
<booktitle>The 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="1283" citStr="He et al., 2013" startWordPosition="200" endWordPosition="203">sent an NLP and ML based approach to the task, and show that this approach outperforms the regular expression based approach by a large and statistically significant margin. One of the main challenges we faced early on was the absence of training and test data. We propose a methodology for using well structured screenplays to create training data for anticipated anomalies in the structure of screenplays. 1 Introduction Social network extraction from unstructured text has recently gained much attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). Using Natural Language Processing (NLP) and Machine Learning (ML) techniques, researchers are now able to gain access to networks that are not associated with any meta-data (such as email links and self-declared friendship links). Movies, which can be seen as visual approximations of unstructured literary works, contain rich social networks formed by interactions between characters. There has been some effort in the past to extract social networks from movies (Weng et al., 2006; Weng et al., 2007; Weng et al., 2009; Gil et al., 2011). However, these approaches are primarily regular expressio</context>
</contexts>
<marker>He, Barbosa, Kondrak, 2013</marker>
<rawString>Hua He, Denilson Barbosa, and Grzegorz Kondrak. 2013. Identification of speakers in novels. The 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL.</booktitle>
<contexts>
<context position="17465" citStr="Toutanova et al., 2003" startWordPosition="2875" endWordPosition="2878">istic distribution of parts-of-speech and frames. For example, scene boundaries contain the location and time of scene. Therefore, we expect them to have a majority of nouns, and frames that are related to location and time. For the scene boundary in Figure 1 (EXT. FBI ACADEMY... - DAY), we find the following distribution of parts of speech and frames: 100% nouns, 50% frame LOCALE (with frame evoking element grounds), and 50% frame CALENDRIC_UNIT (with frame evoking element DAY). Similarly, we expect the character names to have 100% nouns, and no frames. We use Stanford part-of-speech tagger (Toutanova et al., 2003) for obtaining the part-of-speech tags and Semafor (Chen et al., 2010) for obtaining the FrameNet (Baker et al., 1998) frames present in each line of the screenplay. We devise 21 hand-crafted features. Sixteen of these features are binary (0/1). We list these features here (the feature names are self-explanatory): has-non-alphabeticalchars, has-digits-majority, has-alpha-majority, is-quoted, capitalization (has-all-caps, is-allcaps), scene boundary (has-INT, has-EXT), date (has-date, is-date), number (has-number, is-number), and parentheses (is-parenthesized, starts-with-parenthesis, ends-with</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Turetsky</author>
<author>Nevenka Dimitrova</author>
</authors>
<title>Screenplay alignment for closed-system speaker identification and analysis of feature films.</title>
<date>2004</date>
<booktitle>In Multimedia and Expo, 2004. ICME’04. 2004 IEEE International Conference on,</booktitle>
<volume>3</volume>
<pages>1659--1662</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="4526" citStr="Turetsky and Dimitrova (2004)" startWordPosition="730" endWordPosition="733">ows: in section 2, we present common terminology used to describe screenplays. We survey existing literature in section 3. Section 4 presents details of our data collection methodology, along with the data distribution. Section 5 gives details of our regular-expression based system, which we use as a baseline for evaluation purposes. In section 6, we present our machine learning approach. In section 7, we give details of the features we use for machine learning. In section 8, we present our experiments and results. We conclude and give future directions of research in section 9. 2 Terminology Turetsky and Dimitrova (2004) describe the structure of a movie screenplay as follows: a screenplay describes a story, characters, action, setting and dialogue of a film. Additionally, they report that the structure of a screenplay follows a (semi) regular format. Figure 1 shows a snippet of a screenplay from the film – The Silence of the Lambs. A scene (tag “S”) starts with what is called the slug line (or scene boundary). The slug line indicates whether the scene is to take place inside or outside (INT, EXT), the name of the location (“FBI ACADEMY GROUNDS, QUANTICO, VIRGINIA”), and can potentially specify the time of da</context>
<context position="6021" citStr="Turetsky and Dimitrova (2004)" startWordPosition="980" endWordPosition="984">or “Off-screen.” Dialogues, like scene descriptions, are not associated with any explicit indicators (such as INT, V.O.), but are indented at a unique level (i.e. nothing else in the screenplay is indented at this level). Screenplays may also have other elements, such as “CUT TO:”, which are directions for the camera, and text describing the intended mood of the speaker, which is found within parentheses in the dialogue. For lack of a name for these elements, we call them “Meta-data” (tag “M”). 3 Literature Survey One of the earliest works motivating the need for screenplay parsing is that of Turetsky and Dimitrova (2004). Turetsky and Dimitrova (2004) proposed a system to automatically align written screenplays with their videos. One of the crucial steps, they noted, is to parse a screenplay into its different elements: scene boundaries, scene descriptions, character names, and dialogues. They proposed a grammar to parse screenplays and show results for aligning one screenplay with its video. Weng et al. (2009) motivated the need for screenplay parsing from a social network analysis perspective. They proposed a set of operations on social networks extracted from movies and television shows in order to find wh</context>
</contexts>
<marker>Turetsky, Dimitrova, 2004</marker>
<rawString>Robert Turetsky and Nevenka Dimitrova. 2004. Screenplay alignment for closed-system speaker identification and analysis of feature films. In Multimedia and Expo, 2004. ICME’04. 2004 IEEE International Conference on, volume 3, pages 1659– 1662. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley Wasserman</author>
<author>Katherine Faust</author>
</authors>
<title>Social Network Analysis: Methods and Applications. New York:</title>
<date>1994</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="29440" citStr="Wasserman and Faust, 1994" startWordPosition="4949" endWordPosition="4952">k are characters, and links between two characters signal their participation in the same scene. We form three such networks: 1) based on the gold tags (NG), 2) based on the tags predicted by MAJ-MAX (NMAJ-MAX), and 3) based on the tags predicted by our baseline (NB). Table 6 compares the number of nodes, number of links, and graph density of the three networks. It is clear from the table that the network extracted by using the tags predicted by MAJ-MAX is closer to the gold network. Centrality measures are one of the most fundamental social network analysis metrics used by social scientists (Wasserman and Faust, 1994). Table 7 presents a comparison of Pearson’s correlation coefficient for various centrality measures for {NB, NG}, and {NMAJ-MAX, NG} for the top ten characters in the movie. The table shows that across all these measures, the statistics obtained using the network NMAJ-MAX are significantly more correlated to the gold network (NG), as compared the the baseline network (NB). 9 Conclusion and Future Work In this paper, we presented a formalization of the task of parsing movie screenplays. We presented an NLP and ML based approach to the task, and showed that this approach outperforms the regular</context>
</contexts>
<marker>Wasserman, Faust, 1994</marker>
<rawString>Stanley Wasserman and Katherine Faust. 1994. Social Network Analysis: Methods and Applications. New York: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-Yi Weng</author>
<author>Wei-Ta Chu</author>
<author>Ja-Ling Wu</author>
</authors>
<title>Movie analysis based on roles’ social network. In</title>
<date>2006</date>
<booktitle>Proceedings of IEEE Int. Conference Multimedia and Expo.,</booktitle>
<pages>1403--1406</pages>
<contexts>
<context position="1767" citStr="Weng et al., 2006" startWordPosition="276" endWordPosition="279">y gained much attention (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). Using Natural Language Processing (NLP) and Machine Learning (ML) techniques, researchers are now able to gain access to networks that are not associated with any meta-data (such as email links and self-declared friendship links). Movies, which can be seen as visual approximations of unstructured literary works, contain rich social networks formed by interactions between characters. There has been some effort in the past to extract social networks from movies (Weng et al., 2006; Weng et al., 2007; Weng et al., 2009; Gil et al., 2011). However, these approaches are primarily regular expression based with no evaluation of how well they work. In this paper we introduce a formalization of the task of parsing screenplays and present an NLP and ML based approach to the task. By parsing a screenplay, we mean assigning each line of the screenplay one of the following five tags: “S” for scene boundary, “N” for scene description, “C” for character name, “D” for dialogue, and “M” for meta-data. We expect screenplays to conform to a strict grammar but they often do not (Gil et </context>
</contexts>
<marker>Weng, Chu, Wu, 2006</marker>
<rawString>Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu. 2006. Movie analysis based on roles’ social network. In Proceedings of IEEE Int. Conference Multimedia and Expo., pages 1403–1406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-Yi Weng</author>
<author>Wei-Ta Chu</author>
<author>Ja-Ling Wu</author>
</authors>
<title>Rolenet: treat a movie as a small society.</title>
<date>2007</date>
<booktitle>In Proceedings of the international workshop on Workshop on multimedia information retrieval,</booktitle>
<pages>51--60</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1786" citStr="Weng et al., 2007" startWordPosition="280" endWordPosition="283">tion (Agarwal and Rambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). Using Natural Language Processing (NLP) and Machine Learning (ML) techniques, researchers are now able to gain access to networks that are not associated with any meta-data (such as email links and self-declared friendship links). Movies, which can be seen as visual approximations of unstructured literary works, contain rich social networks formed by interactions between characters. There has been some effort in the past to extract social networks from movies (Weng et al., 2006; Weng et al., 2007; Weng et al., 2009; Gil et al., 2011). However, these approaches are primarily regular expression based with no evaluation of how well they work. In this paper we introduce a formalization of the task of parsing screenplays and present an NLP and ML based approach to the task. By parsing a screenplay, we mean assigning each line of the screenplay one of the following five tags: “S” for scene boundary, “N” for scene description, “C” for character name, “D” for dialogue, and “M” for meta-data. We expect screenplays to conform to a strict grammar but they often do not (Gil et al., 2011). This di</context>
</contexts>
<marker>Weng, Chu, Wu, 2007</marker>
<rawString>Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu. 2007. Rolenet: treat a movie as a small society. In Proceedings of the international workshop on Workshop on multimedia information retrieval, pages 51–60. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung-Yi Weng</author>
<author>Wei-Ta Chu</author>
<author>Ja-Ling Wu</author>
</authors>
<title>Rolenet: Movie analysis from the perspective of social networks.</title>
<date>2009</date>
<journal>Multimedia, IEEE Transactions on,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="1805" citStr="Weng et al., 2009" startWordPosition="284" endWordPosition="287">ambow, 2010; Elson et al., 2010; Agarwal et al., 2013a; Agarwal et al., 2013b; He et al., 2013). Using Natural Language Processing (NLP) and Machine Learning (ML) techniques, researchers are now able to gain access to networks that are not associated with any meta-data (such as email links and self-declared friendship links). Movies, which can be seen as visual approximations of unstructured literary works, contain rich social networks formed by interactions between characters. There has been some effort in the past to extract social networks from movies (Weng et al., 2006; Weng et al., 2007; Weng et al., 2009; Gil et al., 2011). However, these approaches are primarily regular expression based with no evaluation of how well they work. In this paper we introduce a formalization of the task of parsing screenplays and present an NLP and ML based approach to the task. By parsing a screenplay, we mean assigning each line of the screenplay one of the following five tags: “S” for scene boundary, “N” for scene description, “C” for character name, “D” for dialogue, and “M” for meta-data. We expect screenplays to conform to a strict grammar but they often do not (Gil et al., 2011). This disconnect gives rise</context>
<context position="6419" citStr="Weng et al. (2009)" startWordPosition="1042" endWordPosition="1045">n the dialogue. For lack of a name for these elements, we call them “Meta-data” (tag “M”). 3 Literature Survey One of the earliest works motivating the need for screenplay parsing is that of Turetsky and Dimitrova (2004). Turetsky and Dimitrova (2004) proposed a system to automatically align written screenplays with their videos. One of the crucial steps, they noted, is to parse a screenplay into its different elements: scene boundaries, scene descriptions, character names, and dialogues. They proposed a grammar to parse screenplays and show results for aligning one screenplay with its video. Weng et al. (2009) motivated the need for screenplay parsing from a social network analysis perspective. They proposed a set of operations on social networks extracted from movies and television shows in order to find what they called hidden semantic information. They proposed techniques for identifying lead roles in bilateral movies (movies with two main characters), for performing community analysis, and for automating the task of story segmentation. Gil et al. (2011) extracted character interaction networks from plays and movies. They were interested in automatically classifying plays and movies into differe</context>
<context position="28709" citStr="Weng et al., 2009" startWordPosition="4817" endWordPosition="4820">haracter names and dialogues. However, we believe that the performance of the baseline is unpredictable. It may get lucky on screenplays that are well-structured (in one way or the other), but it is hard to comment on the robustness of its performance. On the contrary, our ensemble is robust, hedging its bets on eight models, which are trained to handle different types and combinations of anomalies. In tables 6 and 7, we present an extrinsic evaluation on the test set. We extract a network from our test movie screenplay (Silver Linings Playbook) by using the tags of the screenplay as follows (Weng et al., 2009): we connect all characters having a dialogue with each other in a scene with links. Nodes in this network are characters, and links between two characters signal their participation in the same scene. We form three such networks: 1) based on the gold tags (NG), 2) based on the tags predicted by MAJ-MAX (NMAJ-MAX), and 3) based on the tags predicted by our baseline (NB). Table 6 compares the number of nodes, number of links, and graph density of the three networks. It is clear from the table that the network extracted by using the tags predicted by MAJ-MAX is closer to the gold network. Centra</context>
</contexts>
<marker>Weng, Chu, Wu, 2009</marker>
<rawString>Chung-Yi Weng, Wei-Ta Chu, and Ja-Ling Wu. 2009. Rolenet: Movie analysis from the perspective of social networks. Multimedia, IEEE Transactions on, 11(2):256–271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
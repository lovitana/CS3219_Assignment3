<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004112">
<title confidence="0.88672">
Word Clustering Based on Un-LP Algorithm
</title>
<author confidence="0.989394">
Jiguang Liang1, Xiaofei Zhou1, Yue Hu1, Li Guo1∗, Shuo Bai1,2
</author>
<affiliation confidence="0.9129225">
1National Engineering Laboratory for Information Security Technologies,
Institute of Information Engineering, Chinese Academy of Sciences,
Beijing 100190, China
2Shanghai Stock Exchange, Shanghai 200120, China
</affiliation>
<email confidence="0.992047">
{liangjiguang, zhouxiaofei, huyue, guoli, baishuo}@iie.ac.cn
</email>
<sectionHeader confidence="0.993842" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999242285714286">
Word clustering which generalizes specific features cluster words in the same syntactic or seman-
tic categories into a group. It is an effective approach to reduce feature dimensionality and feature
sparseness which are clearly useful for many NLP applications. This paper proposes an unsu-
pervised label propagation algorithm (Un-LP) for word clustering which uses multi-exemplars
to represent a cluster. Experiments on a synthetic 2D dataset show the strong ability of self-
correcting of the proposed algorithm. Besides, the experimental results on 20NG demonstrate
that our algorithm outperforms the conventional cluster algorithms.
</bodyText>
<sectionHeader confidence="0.999195" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99875">
Word clustering is the task of the division of words into a certain number of clusters (groups or cat-
egories). Each cluster is required to consist of words that are similar to one another in syntactic or
semantic construct and dissimilar to words in distinctive groups. Word clustering generalizes specific
features by considering the common characteristics and ignoring the specific characteristics among the
individual features. It is an effective approach to reduce feature dimensionality and feature sparseness
(Han et al., 2005).
Recently, word clustering offers great potential for various useful NLP applications. Several studies
have addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009). Momtazi and Klakow
(2009) propose a word clustering approach to improve the performance of sentence retrieval in Question
Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information
into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for
a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et
al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah,
2010) and machine translation (Uszkoreit and Brants, 2008; Jeff et al., 2011).
This paper presents an unsupervised algorithm for word clustering based on a probabilistic transition
matrix. Given a text document dataset, a list of words is generated by removing stop words and very
unfrequent words. Each word is required to be represented by the documents in the dataset, which results
in a co-occurrence matrix. By calculating the similarity of words, a word similarity graph with transition
(propagation) probabilities as weight edges is created. Then, a new kind word clustering algorithm, based
on label propagation, is applied.
The remaining parts of this paper are organized as follows: Section 2 formulates word clustering
problem in the context of unsupervised learning. Then we describe the word clustering algorithm in
Section 3 and present our experiments in Section 4. Finally we conclude our work in Section 5.
</bodyText>
<sectionHeader confidence="0.981184" genericHeader="method">
2 Problem setup
</sectionHeader>
<bodyText confidence="0.8631324">
Assume that we have a corpus with N documents denoted by D = {d1, d2, · · · , dN}; each document in
the corpus consists of a list of words denoted by di = {w1, w2, · · · , wNd} where each wi is an item from
a vocabulary index with V distinct terms denoted by W = {v1, v2, · · · , vV } and Nd is the document
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/.
</bodyText>
<page confidence="0.981628">
25
</page>
<note confidence="0.742665">
Proceedings of the AHA! Workshop on Information Discovery in Text, pages 25–30,
Dublin, Ireland, August 23 2014.
</note>
<table confidence="0.709343">
Algorithm 1 Semi-supervised LP Algorithm Algorithm 2 Unsupervised LP Word Clustering
Input: Input:
Wl = {vi}l i=1 labeled data W = {vi}ui=1 (u = V ) unlabeled words
Wu = {vi}Vi=u unlabeled data Tuu = {Tij} 1 ≤ i, j ≤ V transition matrix
T = {Tij} 1 ≤ i, j ≤ V transition matrix Output:
Output: YU A = {(A1, A2, · · · , AL} word-clusters
</table>
<listItem confidence="0.967840538461539">
1: Begin 1: Begin
2: Row-normalize T by T ij = Tij/ EVk=1 Tik 2: {V0L, YL, T0ul} = initialization(W)
3: While not converged do 3: While not converged do
4: Propagate the labels by Y t+1 = TYt 4: Y t+1
5: Row-normalize Yt+1 U = Semi − LP(Vt L,Yt L,T0ul,Tuu)
6: Clamp the labeled data 5: At+1 = partition cluster(Y t+1
7: End while U )
8: End 6: {V t+1
9: Return YU L , T t+1
ul } = update(At+1)
7: End while
8: End
9: Return At+1
</listItem>
<bodyText confidence="0.991700625">
length. We define the vector of word vi in the vocabulary to be vi =&lt; vid1, vid2, · · · , vidN &gt;. This
allows us to define a V x N word-document matrix WD for the vocabularies. WDij is equal to 1 if
vi E dj and equal to 0 otherwise. Then we take these words as the vertices of a connected graph. In
this paper, we define the edge weight wij as the co-occurrence frequency between vi and vj. Obviously,
we expect that larger edge weights allow labels to travel through more easily. So we define a V x V
probabilistic transition matrix T where Tij = P(vj —* vi) = wij/ EVk=1 wkj.
The L value which is used to represent the number of word clusters is specified. We define a V x L
label matrix Y . Clearly, yi E Y represents the label probability distributions of word vi and Y ∗
</bodyText>
<equation confidence="0.891275333333333">
i =
argmax Yik(0 &lt; k &lt; L) is its cluster label. For example, suppose L = 3 and a word v has a label
distribution y =&lt; 0.1, 0.8, 0.1 &gt;, it implies that v belongs to the second class.
</equation>
<sectionHeader confidence="0.984629" genericHeader="method">
3 Unsupervised LP Word Clustering
</sectionHeader>
<bodyText confidence="0.99890725">
Label propagation (Zhu and Ghahramani, 2002) is a semi-supervised algorithm (Semi-LP) which needs
labeled data. Let {(v1, y1), · · · , (vl, yl)} be labeled data, {(vl+1, yl+1), · · · , (vl+u, yl+u)} be unlabeled
ones where l + u = V , YL = [y1, , · · · , yl]T and YU = [yl+1, · · · , yl+u]T. YU is un-known and l &lt;&lt; u.
The label propagation algorithm is summarized in Algorithm 1.
In clustering problems, the goal is to select a set of exemplars from a dataset that are representative
of the dataset and each cluster is represented by one and only one exemplar (Krause and Gomes, 2010).
However, these exemplars are just all Semi-LP needs for clustering. LP lacks labeled data when is used
for unsupervised learning. In this paper, we are interested in partitioning words into several clusters
without any label priori using unsupervised LP (Un-LP) algorithm. Firstly we randomly select K (K &gt;
L, usually K is a multiple of L) words to construct an exemplar set E = {Ei}Ki=1 which is different
from the conventional exemplar-based cluster algorithms, assign class labels to them and construct the
corresponding probabilistic transition matrix T ul 0 (initialization). These exemplars are considered as
labeled words and the rest U = W − E are unlabeled words. Tul is the probability of transition from
unlabeled words to labeled ones. At this step, it needs the assurance that each class could be represented
by at least one exemplar and each exemplar could only be assigned one class label.
Now the connected weighted graph consists of two parts: G = (E U U, Tul U Tuu) where Tuu is
the transition probability between unlabeled words. Next, our algorithm iterates between the following
three steps: given a set of LP parameters, we first propagate labels to unlabeled words with the initial
label distributions and get the corresponding labels (Semi−LP). Then, these derived label distributions
are used to guide the partitioning of unlabeled data (partition cluster) to L clusters. We use residual
sum of squares (RSS) to choose the most centrally located words and replace the old exemplars that
represent the cluster. Specifically, for a word cluster ci = {v1, · · · vn}, RSSi = Enj=1 wij. Then we sort
RSSi (0 &lt; i &lt; n) and update exemplars by the words with bigger RSS for this cluster (update). All
of the above steps, summarized in Algorithm 2, are iterated until convergence.
</bodyText>
<page confidence="0.991016">
26
</page>
<sectionHeader confidence="0.999069" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998604">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.99999225">
To demonstrate properties of our proposed algorithm we investigate both a synthetic dataset and a real-
world dataset. Figure 1(a) shows the synthetic dataset. For a real world example we test Un-LP on a
subset of 20 Newsgroups (20NG) dataset which is preprocessed by removing common stop-words and
stemming. We use the classes atheism, hardware, hockey and space for test and randomly select
300 samples from each class as the test dataset in this section. However, 20NG is not suited for word
clustering evaluation. So, firstly, we reconstruct it by pair-wise testing which is a specification-based
testing criterion. Then we can obtain six (C24 = 6) pairwise subsets represented by {D1, · · · , D61. In
order to facilitate the evaluation, we use those words that only occur in one class for clustering.
</bodyText>
<subsectionHeader confidence="0.932101">
4.2 Exemplar Self-correction
</subsectionHeader>
<bodyText confidence="0.999859571428572">
This multi-step iterative method is simple to implement and surprisingly effective even with wrong initial
labeled data. To illustrate the point, we describe a simulated dataset with two-moon pattern. Obviously,
the points in one moon should be more similar to each other than the points across the moons as shown
in Figure 1(b). During the initialization phase, four points in the lower moon are selected and assigned
with different labels. The exemplars of the upper moon are mis-labeled as shown in Figure 1(c). In the
next five iteration steps, exemplars have been gradually moved to the center of the upper moon. Finally,
when t &gt; 5 Un-LP converges to a fixed assignment, which achieves an ideal cluster result.
</bodyText>
<figure confidence="0.961520795454545">
(1) Synthetic Dataset
2
1
0
−1
−2
−2 −1 0 1 2 3
(2) Ideal Cluster
2
1
0
−1
−2
−2 −1 0 1 2 3
(3) t=1
2
1
0
−1
−2
−2 −1 0 1 2 3
(4) t=2
2
1
0
−1
−2
−2 −1 0 1 2 3
(5) t=3 (6) t=4 (7) t=5 (8) t=6
2
2
2
1
1
1
0
0
0
−1
−1
−1
−2
−2
−2
</figure>
<equation confidence="0.949080222222222">
−2 −1 0 1 2 3
−2 −1 0 1 2 3
2
1
0
−1
−2
−2 −1 0 1 2 3
−2 −1 0 1 2 3
</equation>
<figureCaption confidence="0.9455215">
Figure 1: Clustering result of unsupervised LP clustering algorithm on two-moon pattern dataset. (a)
Two-moon pattern dataset without any labeled points, (b) ideal clustering results. The convergence pro-
cess of unsupervised LP with t from 1 to 6 is shown from (c) to (h). Solid points are labeled data that are
selected to represent the clusters.
</figureCaption>
<subsectionHeader confidence="0.998367">
4.3 Word Clustering Performance
</subsectionHeader>
<bodyText confidence="0.999727">
This section provides empirical evidence that the proposed algorithm performs well in the problem of
word clustering. Figure 2 shows the mean precisions and recalls over 10 runs of the baseline algorithms
as well as Un-LP.
From Figure 2, it can be clearly observed that Un-LP (K/L = 5) yields the best performance, followed
by Semi-LP with 20 labeled words. In general, the recalls with k-means and k-medoids are higher,
while the precisions are much lower. Figure 2 also shows the results of other two semi-supervised word
</bodyText>
<page confidence="0.996121">
27
</page>
<table confidence="0.989417888888889">
Cluster1 Cluster2 Cluster3 Cluster4
Atheism Hardware Hockey Space
geode religiously bene- driver soundblaster card- goalies bug hfd johansson hub atom aug larson sts
factor meng stacker s isbn manufacturer portal breton scorers carpenter orbital skydive parity
mcl mormon madden prize mastering connectors stevens smythe janney accelerations desire an-
mythology timmons cb- floppies dock adapter mul- fleury vancouver stl niversary projects digital
newsj agnostics fanatism timedia installing bowman cheveldae selanne win- protection atari temper-
engr chade tan falsifiable configure physchem jumpers nipeg canadiens bure nyr atures voyagers zoology
existed ucsb sentence motherboardsfdisk seagate capitals updated teflon
</table>
<tableCaption confidence="0.999809">
Table 1: Top-20 words extracted by unsupervised LP word cluster algorithm.
</tableCaption>
<bodyText confidence="0.8214584">
clustering algorithms, PCK-means (Basu et al., 2004) and MPCK-means (Bilenko et al., 2004) with 200
must-link and cannot-link constraints. Also when comparing these unsupervised and semi-supervised
approaches previously mentioned, we can find that our unsupervised algorithm consistently achieves
significantly better results. Therefore, unsupervised LP seems to be a more reasonable algorithm design
in terms of word clustering.
</bodyText>
<figureCaption confidence="0.952545333333333">
Figure 2: Precision vs. recall of clustering results on 20NG where D1 = {atheism vs. hardware}, D2 =
{atheism vs. hockey}, D3 = {atheism vs. space}, D4 = {hardware vs. hcokey}, D5 = {hardware
vs. space} and D6 = {hockey vs. space}.
</figureCaption>
<subsectionHeader confidence="0.999276">
4.4 Effect of exemplar number e
</subsectionHeader>
<bodyText confidence="0.999989888888889">
We now investigate how the number of exemplar (e) for each cluster affects the clustering. In particular,
we are interested in performance under conditions when the number of exemplar grows - which is the
motivation for using more than one exemplars to represent a cluster. From Figure 3, we can observe that
when more words are labeled, Semi-LP shows further improvement in F-value. However, the changes
for PCK-means and MPCK-means are not obvious. Interestingly, even with the number of labeled data
growing, Semi-LP still performs worse than Un-LP. As is shown in Figure 3, Un-LP benefits much from
multi-exemplars (e ≥ 2). For D4, Un-LP is capable of achieving 99.58% in F-value when e = 7,
obtaining 21.32% improvement over the baseline (e = 1). This indicates that our algorithm leverages
the additional exemplars effectively.
</bodyText>
<subsectionHeader confidence="0.998619">
4.5 Case Study
</subsectionHeader>
<bodyText confidence="0.999986">
We conduct an experiment to illustrate the characteristics of the proposed algorithm in this subsection.
We cluster the words in all the four domain datasets and select the most representative words for each
cluster by sorting yi. In the experiment, we set L = 4 in order to match the class number of the
dataset. Table 1 shows top-20 representative words for each cluster, where the bold words are the ones
</bodyText>
<figure confidence="0.999672162790697">
(a) D1
(b) D2
(c) D3
Precision
(d) D4
Precision
(f) D6
(e) D5
Precision
Precision
Precision
Un−LP Semi−LP K−means
K−medoids PCK MPCK
Recall
0.8
0.6
0.4 0.7 1
1
Recall
0.9
0.8
1
0.4 0.7 1
Recall
0.9
0.8
1
0.4 0.7 1
Recall
0.9
0.8
1
0.4 0.7 1
Recall
0.9
0.8
1
0.4 0.7 1
Recall
0.9
0.8
1
0.4 0.7 1
</figure>
<page confidence="0.936473">
28
</page>
<figureCaption confidence="0.999065">
Figure 3: Results on 20NG where X-axis is e, Y-axis is F-value.
</figureCaption>
<bodyText confidence="0.9049188">
domain meng configure johansson aug geode isbn bug parity
Atheism 100.00% 0 0 0 0 91.67% 89.47% 0
Hardware 0 90.91% 0 0 0 0 10.53% 66.67%
Hockey 0 9.09% 100.00% 0 0 8.33% 0 0
Space 0 0 0 100.00% 100.00% 0 0 33.33%
</bodyText>
<tableCaption confidence="0.873252">
Table 2: Distributions of the incorrect words partitioned by the literal meaning.
</tableCaption>
<bodyText confidence="0.999974111111111">
with correct cluster label inferencing from the literal meaning. We observe that the accuracy of word
clustering on 20NG is very low (28.75%), which is at variance with the preceding conclusion. One
reason is that words in 20NG are stemmed, so, from Table 1 it can be clearly seen that there are some
non-English words (e.g., ”mcl”, ”hfd”, ”stl”, etc.) that don’t have actual meanings.
In order to gain further insights into the reasons, the distributions of these incorrect words have been
made in statistics. Partial results are shown in Table 2. From the distributions, we can find that many
words marked in italics in Table 1 have been correctly clustered, although they have nothing to do with
corresponding class in the literal meaning. Taking these words into account, the accuracy can reach
81.25% which demonstrates once again the effectiveness of Un-LP word clustering algorithm.
</bodyText>
<sectionHeader confidence="0.998954" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99971575">
In this paper, we propose an unsupervised label propagation algorithm to tackle the problem of word
clustering. The proposed algorithm uses a similarity graph based on co-occurrence information to en-
courage similar words to have similar cluster labels. One of the advantages of this algorithm is that it
uses multi-exemplars to represent a cluster, which can significantly improve the clustering results.
</bodyText>
<sectionHeader confidence="0.996629" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.788586666666667">
This work was supported by Strategic Priority Research Program of Chinese Academy of Sciences
(XDA06030602), National Nature Science Foundation of China (No. 61202226), National 863 Program
(No. 2011AA010703), IIE Program (No.Y3Z0062201).
</bodyText>
<figure confidence="0.999864806451613">
(a) Results on D1
(b) Results on D2
(c) Results on D3
(d) Results on D4
(e) Results on D5
(0 Results on D6
0.9
0.7
0.9
0.9
0.7
0.7
0.5
1 3 5 7 9
0.5
1 3 5 7 9
0.5
1 3 5 7 9
0.9
0.7
0.9
0.9
0.7
0.7
0.5
1 3 5 7 9
0.5
1 3 5 7 9
0.5
1 3 5 7 9
Semi−LP Un−LP PCK−Means MPCK−Means
</figure>
<page confidence="0.994583">
29
</page>
<sectionHeader confidence="0.995431" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999779914285714">
Basu S, Bilenko M, Mooney R J. 2004. A probabilistic framework for semi-supervised clustering. In Proceedings
of SIGKDD, pages 59-68.
Bilenko M, Basu S, Mooney R J. 2004. Integrating constraints and metric learning in semi-supervised clustering.
In Proceedings ofICML.
Blei D M, Ng A Y, Jordan M I. 2003. Latent dirichlet allocation. The Journal of machine Learning research,
pages 993-1022.
Candito M, Seddah D. 2010. Parsing word clusters. In Proceedings of the NAACL HLT 2010 First Workshop on
Statistical Parsing ofMorphologically-Rich Languages, pages 76-84.
Han H, Manavoglu E, Zha H, et al. 2005. Rule-based word clustering for document metadata extraction. In
Proceedings of the 2005 ACMsymposium on Applied computing, pages 1049-1053.
Jeff M A, Matsoukas S, Schwartz R. 2011. Improving Low-Resource Statistical Machine Translation with a Novel
Semantic Word Clustering Algorithm. In Proceedings of the MT Summit XIII.
Jin P, Sun X, Wu Y, et al. 2007. Word clustering for collocation-based word sense disambiguation. Computational
Linguistics and Intelligent Text Processing, pages 267-274.
Koo T, Carreras X, Collins M. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL-HLT,
pages 595-603.
Krause A, Gomes R G. 2010. Budgeted nonparametric learning from data streams. In Proceedings of ICML,
pages 391-398.
Momtazi S, Klakow D. 2009. A word clustering approach for language model-based sentence retrieval in question
answering systems. In Proceedings of CIKM, pages 1911-1914.
Sagae K, Gordon A S. 2009. Clustering words by syntactic similarity improves dependency parsing of predicate-
argument structures. In Proceedings of the 11th International Conference on Parsing Technologies, pages 192-
201.
Sun A, Grishman R, Sekine S. 2011. Semi-supervised relation extraction with large-scale word clustering. In
Proceedings ofACL, pages 521-529.
Turian J, Ratinov L, Bengio Y. 2010. Word representations: a simple and general method for semi-supervised
learning. In Proceedings ofACL, pages 384-394.
Uszkoreit J, Brants T. 2008. Distributed Word Clustering for Large Scale Class-Based Language Modeling in
Machine Translation. In Proceedings ofACL, pages 755-762.
Wu Q, Ye Y, Ng M, et al. 2010. Exploiting word cluster information for unsupervised feature selection Trends in
Artificial Intelligence, pages 292-303.
Zhu X, Ghahramani Z. 2002. Learning from labeled and unlabeled data with label propagation. Technical Report
CMU-CALD-02-107, Carnegie Mellon University.
Zhu X, Ghahramani Z, Lafferty J. 2003. Semi-supervised learning using gaussian fields and harmonic functions.
In Proceedings ofICML, pages 912-919.
</reference>
<page confidence="0.998799">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.735276">
<title confidence="0.999325">Word Clustering Based on Un-LP Algorithm</title>
<author confidence="0.99777">Xiaofei Yue Li Shuo</author>
<affiliation confidence="0.9961305">Engineering Laboratory for Information Security Institute of Information Engineering, Chinese Academy of</affiliation>
<address confidence="0.8882175">Beijing 100190, Stock Exchange, Shanghai 200120,</address>
<email confidence="0.973784">zhouxiaofei,huyue,guoli,</email>
<abstract confidence="0.99574025">Word clustering which generalizes specific features cluster words in the same syntactic or semantic categories into a group. It is an effective approach to reduce feature dimensionality and feature sparseness which are clearly useful for many NLP applications. This paper proposes an unsupervised label propagation algorithm (Un-LP) for word clustering which uses multi-exemplars to represent a cluster. Experiments on a synthetic 2D dataset show the strong ability of selfcorrecting of the proposed algorithm. Besides, the experimental results on 20NG demonstrate that our algorithm outperforms the conventional cluster algorithms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Basu</author>
<author>M Bilenko</author>
<author>R J Mooney</author>
</authors>
<title>A probabilistic framework for semi-supervised clustering.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGKDD,</booktitle>
<pages>59--68</pages>
<contexts>
<context position="11640" citStr="Basu et al., 2004" startWordPosition="1986" endWordPosition="1989">arpenter orbital skydive parity mcl mormon madden prize mastering connectors stevens smythe janney accelerations desire anmythology timmons cb- floppies dock adapter mul- fleury vancouver stl niversary projects digital newsj agnostics fanatism timedia installing bowman cheveldae selanne win- protection atari temperengr chade tan falsifiable configure physchem jumpers nipeg canadiens bure nyr atures voyagers zoology existed ucsb sentence motherboardsfdisk seagate capitals updated teflon Table 1: Top-20 words extracted by unsupervised LP word cluster algorithm. clustering algorithms, PCK-means (Basu et al., 2004) and MPCK-means (Bilenko et al., 2004) with 200 must-link and cannot-link constraints. Also when comparing these unsupervised and semi-supervised approaches previously mentioned, we can find that our unsupervised algorithm consistently achieves significantly better results. Therefore, unsupervised LP seems to be a more reasonable algorithm design in terms of word clustering. Figure 2: Precision vs. recall of clustering results on 20NG where D1 = {atheism vs. hardware}, D2 = {atheism vs. hockey}, D3 = {atheism vs. space}, D4 = {hardware vs. hcokey}, D5 = {hardware vs. space} and D6 = {hockey vs</context>
</contexts>
<marker>Basu, Bilenko, Mooney, 2004</marker>
<rawString>Basu S, Bilenko M, Mooney R J. 2004. A probabilistic framework for semi-supervised clustering. In Proceedings of SIGKDD, pages 59-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bilenko</author>
<author>S Basu</author>
<author>R J Mooney</author>
</authors>
<title>Integrating constraints and metric learning in semi-supervised clustering.</title>
<date>2004</date>
<booktitle>In Proceedings ofICML.</booktitle>
<contexts>
<context position="11678" citStr="Bilenko et al., 2004" startWordPosition="1992" endWordPosition="1995"> mormon madden prize mastering connectors stevens smythe janney accelerations desire anmythology timmons cb- floppies dock adapter mul- fleury vancouver stl niversary projects digital newsj agnostics fanatism timedia installing bowman cheveldae selanne win- protection atari temperengr chade tan falsifiable configure physchem jumpers nipeg canadiens bure nyr atures voyagers zoology existed ucsb sentence motherboardsfdisk seagate capitals updated teflon Table 1: Top-20 words extracted by unsupervised LP word cluster algorithm. clustering algorithms, PCK-means (Basu et al., 2004) and MPCK-means (Bilenko et al., 2004) with 200 must-link and cannot-link constraints. Also when comparing these unsupervised and semi-supervised approaches previously mentioned, we can find that our unsupervised algorithm consistently achieves significantly better results. Therefore, unsupervised LP seems to be a more reasonable algorithm design in terms of word clustering. Figure 2: Precision vs. recall of clustering results on 20NG where D1 = {atheism vs. hardware}, D2 = {atheism vs. hockey}, D3 = {atheism vs. space}, D4 = {hardware vs. hcokey}, D5 = {hardware vs. space} and D6 = {hockey vs. space}. 4.4 Effect of exemplar numbe</context>
</contexts>
<marker>Bilenko, Basu, Mooney, 2004</marker>
<rawString>Bilenko M, Basu S, Mooney R J. 2004. Integrating constraints and metric learning in semi-supervised clustering. In Proceedings ofICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent dirichlet allocation. The Journal of machine Learning research,</title>
<date>2003</date>
<pages>993--1022</pages>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei D M, Ng A Y, Jordan M I. 2003. Latent dirichlet allocation. The Journal of machine Learning research, pages 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Candito</author>
<author>D Seddah</author>
</authors>
<title>Parsing word clusters.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing ofMorphologically-Rich Languages,</booktitle>
<pages>76--84</pages>
<contexts>
<context position="2288" citStr="Candito and Seddah, 2010" startWordPosition="319" endWordPosition="322">dies have addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah, 2010) and machine translation (Uszkoreit and Brants, 2008; Jeff et al., 2011). This paper presents an unsupervised algorithm for word clustering based on a probabilistic transition matrix. Given a text document dataset, a list of words is generated by removing stop words and very unfrequent words. Each word is required to be represented by the documents in the dataset, which results in a co-occurrence matrix. By calculating the similarity of words, a word similarity graph with transition (propagation) probabilities as weight edges is created. Then, a new kind word clustering algorithm, based on lab</context>
</contexts>
<marker>Candito, Seddah, 2010</marker>
<rawString>Candito M, Seddah D. 2010. Parsing word clusters. In Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 76-84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Han</author>
<author>E Manavoglu</author>
<author>H Zha</author>
</authors>
<title>Rule-based word clustering for document metadata extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2005 ACMsymposium on Applied computing,</booktitle>
<pages>1049--1053</pages>
<contexts>
<context position="1564" citStr="Han et al., 2005" startWordPosition="216" endWordPosition="219"> that our algorithm outperforms the conventional cluster algorithms. 1 Introduction Word clustering is the task of the division of words into a certain number of clusters (groups or categories). Each cluster is required to consist of words that are similar to one another in syntactic or semantic construct and dissimilar to words in distinctive groups. Word clustering generalizes specific features by considering the common characteristics and ignoring the specific characteristics among the individual features. It is an effective approach to reduce feature dimensionality and feature sparseness (Han et al., 2005). Recently, word clustering offers great potential for various useful NLP applications. Several studies have addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambig</context>
</contexts>
<marker>Han, Manavoglu, Zha, 2005</marker>
<rawString>Han H, Manavoglu E, Zha H, et al. 2005. Rule-based word clustering for document metadata extraction. In Proceedings of the 2005 ACMsymposium on Applied computing, pages 1049-1053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Jeff</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
</authors>
<title>Improving Low-Resource Statistical Machine Translation with a Novel Semantic Word Clustering Algorithm.</title>
<date>2011</date>
<booktitle>In Proceedings of the MT Summit XIII.</booktitle>
<contexts>
<context position="2360" citStr="Jeff et al., 2011" startWordPosition="330" endWordPosition="333">9). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah, 2010) and machine translation (Uszkoreit and Brants, 2008; Jeff et al., 2011). This paper presents an unsupervised algorithm for word clustering based on a probabilistic transition matrix. Given a text document dataset, a list of words is generated by removing stop words and very unfrequent words. Each word is required to be represented by the documents in the dataset, which results in a co-occurrence matrix. By calculating the similarity of words, a word similarity graph with transition (propagation) probabilities as weight edges is created. Then, a new kind word clustering algorithm, based on label propagation, is applied. The remaining parts of this paper are organi</context>
</contexts>
<marker>Jeff, Matsoukas, Schwartz, 2011</marker>
<rawString>Jeff M A, Matsoukas S, Schwartz R. 2011. Improving Low-Resource Statistical Machine Translation with a Novel Semantic Word Clustering Algorithm. In Proceedings of the MT Summit XIII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jin</author>
<author>X Sun</author>
<author>Y Wu</author>
</authors>
<title>Word clustering for collocation-based word sense disambiguation.</title>
<date>2007</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>267--274</pages>
<contexts>
<context position="2189" citStr="Jin et al., 2007" startWordPosition="306" endWordPosition="309">ly, word clustering offers great potential for various useful NLP applications. Several studies have addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah, 2010) and machine translation (Uszkoreit and Brants, 2008; Jeff et al., 2011). This paper presents an unsupervised algorithm for word clustering based on a probabilistic transition matrix. Given a text document dataset, a list of words is generated by removing stop words and very unfrequent words. Each word is required to be represented by the documents in the dataset, which results in a co-occurrence matrix. By calculating the similarity of words, a word similarity graph with transition (propagation)</context>
</contexts>
<marker>Jin, Sun, Wu, 2007</marker>
<rawString>Jin P, Sun X, Wu Y, et al. 2007. Word clustering for collocation-based word sense disambiguation. Computational Linguistics and Intelligent Text Processing, pages 267-274.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Koo</author>
<author>X Carreras</author>
<author>M Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-HLT,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="1719" citStr="Koo et al., 2008" startWordPosition="237" endWordPosition="240">er of clusters (groups or categories). Each cluster is required to consist of words that are similar to one another in syntactic or semantic construct and dissimilar to words in distinctive groups. Word clustering generalizes specific features by considering the common characteristics and ignoring the specific characteristics among the individual features. It is an effective approach to reduce feature dimensionality and feature sparseness (Han et al., 2005). Recently, word clustering offers great potential for various useful NLP applications. Several studies have addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah, 2010) and machine translation (Uszko</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Koo T, Carreras X, Collins M. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL-HLT, pages 595-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Krause</author>
<author>R G Gomes</author>
</authors>
<title>Budgeted nonparametric learning from data streams.</title>
<date>2010</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>391--398</pages>
<contexts>
<context position="6203" citStr="Krause and Gomes, 2010" startWordPosition="1058" endWordPosition="1061">ss. 3 Unsupervised LP Word Clustering Label propagation (Zhu and Ghahramani, 2002) is a semi-supervised algorithm (Semi-LP) which needs labeled data. Let {(v1, y1), · · · , (vl, yl)} be labeled data, {(vl+1, yl+1), · · · , (vl+u, yl+u)} be unlabeled ones where l + u = V , YL = [y1, , · · · , yl]T and YU = [yl+1, · · · , yl+u]T. YU is un-known and l &lt;&lt; u. The label propagation algorithm is summarized in Algorithm 1. In clustering problems, the goal is to select a set of exemplars from a dataset that are representative of the dataset and each cluster is represented by one and only one exemplar (Krause and Gomes, 2010). However, these exemplars are just all Semi-LP needs for clustering. LP lacks labeled data when is used for unsupervised learning. In this paper, we are interested in partitioning words into several clusters without any label priori using unsupervised LP (Un-LP) algorithm. Firstly we randomly select K (K &gt; L, usually K is a multiple of L) words to construct an exemplar set E = {Ei}Ki=1 which is different from the conventional exemplar-based cluster algorithms, assign class labels to them and construct the corresponding probabilistic transition matrix T ul 0 (initialization). These exemplars a</context>
</contexts>
<marker>Krause, Gomes, 2010</marker>
<rawString>Krause A, Gomes R G. 2010. Budgeted nonparametric learning from data streams. In Proceedings of ICML, pages 391-398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Momtazi</author>
<author>D Klakow</author>
</authors>
<title>A word clustering approach for language model-based sentence retrieval in question answering systems.</title>
<date>2009</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>1911--1914</pages>
<contexts>
<context position="1771" citStr="Momtazi and Klakow (2009)" startWordPosition="245" endWordPosition="248"> cluster is required to consist of words that are similar to one another in syntactic or semantic construct and dissimilar to words in distinctive groups. Word clustering generalizes specific features by considering the common characteristics and ignoring the specific characteristics among the individual features. It is an effective approach to reduce feature dimensionality and feature sparseness (Han et al., 2005). Recently, word clustering offers great potential for various useful NLP applications. Several studies have addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah, 2010) and machine translation (Uszkoreit and Brants, 2008; Jeff et al., 2011). This pape</context>
</contexts>
<marker>Momtazi, Klakow, 2009</marker>
<rawString>Momtazi S, Klakow D. 2009. A word clustering approach for language model-based sentence retrieval in question answering systems. In Proceedings of CIKM, pages 1911-1914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A S Gordon</author>
</authors>
<title>Clustering words by syntactic similarity improves dependency parsing of predicateargument structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 11th International Conference on Parsing Technologies,</booktitle>
<pages>192--201</pages>
<contexts>
<context position="1744" citStr="Sagae and Gordon, 2009" startWordPosition="241" endWordPosition="244">oups or categories). Each cluster is required to consist of words that are similar to one another in syntactic or semantic construct and dissimilar to words in distinctive groups. Word clustering generalizes specific features by considering the common characteristics and ignoring the specific characteristics among the individual features. It is an effective approach to reduce feature dimensionality and feature sparseness (Han et al., 2005). Recently, word clustering offers great potential for various useful NLP applications. Several studies have addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah, 2010) and machine translation (Uszkoreit and Brants, 2008; Je</context>
</contexts>
<marker>Sagae, Gordon, 2009</marker>
<rawString>Sagae K, Gordon A S. 2009. Clustering words by syntactic similarity improves dependency parsing of predicateargument structures. In Proceedings of the 11th International Conference on Parsing Technologies, pages 192-201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Sun</author>
<author>R Grishman</author>
<author>S Sekine</author>
</authors>
<title>Semi-supervised relation extraction with large-scale word clustering.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>521--529</pages>
<contexts>
<context position="2039" citStr="Sun et al. (2011)" startWordPosition="285" endWordPosition="288">eristics among the individual features. It is an effective approach to reduce feature dimensionality and feature sparseness (Han et al., 2005). Recently, word clustering offers great potential for various useful NLP applications. Several studies have addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah, 2010) and machine translation (Uszkoreit and Brants, 2008; Jeff et al., 2011). This paper presents an unsupervised algorithm for word clustering based on a probabilistic transition matrix. Given a text document dataset, a list of words is generated by removing stop words and very unfrequent words. Each word is required to be represented by the documents </context>
</contexts>
<marker>Sun, Grishman, Sekine, 2011</marker>
<rawString>Sun A, Grishman R, Sekine S. 2011. Semi-supervised relation extraction with large-scale word clustering. In Proceedings ofACL, pages 521-529.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Turian</author>
<author>L Ratinov</author>
<author>Y Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="2237" citStr="Turian et al., 2010" startWordPosition="313" endWordPosition="316">r various useful NLP applications. Several studies have addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah, 2010) and machine translation (Uszkoreit and Brants, 2008; Jeff et al., 2011). This paper presents an unsupervised algorithm for word clustering based on a probabilistic transition matrix. Given a text document dataset, a list of words is generated by removing stop words and very unfrequent words. Each word is required to be represented by the documents in the dataset, which results in a co-occurrence matrix. By calculating the similarity of words, a word similarity graph with transition (propagation) probabilities as weight edges is created. Then,</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Turian J, Ratinov L, Bengio Y. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings ofACL, pages 384-394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Uszkoreit</author>
<author>T Brants</author>
</authors>
<title>Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>755--762</pages>
<contexts>
<context position="2340" citStr="Uszkoreit and Brants, 2008" startWordPosition="326" endWordPosition="329"> 2008; Sagae and Gordon, 2009). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah, 2010) and machine translation (Uszkoreit and Brants, 2008; Jeff et al., 2011). This paper presents an unsupervised algorithm for word clustering based on a probabilistic transition matrix. Given a text document dataset, a list of words is generated by removing stop words and very unfrequent words. Each word is required to be represented by the documents in the dataset, which results in a co-occurrence matrix. By calculating the similarity of words, a word similarity graph with transition (propagation) probabilities as weight edges is created. Then, a new kind word clustering algorithm, based on label propagation, is applied. The remaining parts of t</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Uszkoreit J, Brants T. 2008. Distributed Word Clustering for Large Scale Class-Based Language Modeling in Machine Translation. In Proceedings ofACL, pages 755-762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Wu</author>
<author>Y Ye</author>
<author>M Ng</author>
</authors>
<title>Exploiting word cluster information for unsupervised feature selection Trends in Artificial Intelligence,</title>
<date>2010</date>
<pages>292--303</pages>
<contexts>
<context position="1908" citStr="Wu et al. (2010)" startWordPosition="266" endWordPosition="269"> groups. Word clustering generalizes specific features by considering the common characteristics and ignoring the specific characteristics among the individual features. It is an effective approach to reduce feature dimensionality and feature sparseness (Han et al., 2005). Recently, word clustering offers great potential for various useful NLP applications. Several studies have addressed dependency parsing (Koo et al., 2008; Sagae and Gordon, 2009). Momtazi and Klakow (2009) propose a word clustering approach to improve the performance of sentence retrieval in Question Answering (QA) systems. Wu et al. (2010) present an approach to integrate word clustering information into the process of unsupervised feature selection. Sun et al. (2011) use large-scale word clustering for a semi-supervised relation extraction system. It also contributes to word sense disambiguation (Jin et al., 2007), named entity recognition (Turian et al., 2010), part-of-speech tagging (Candito and Seddah, 2010) and machine translation (Uszkoreit and Brants, 2008; Jeff et al., 2011). This paper presents an unsupervised algorithm for word clustering based on a probabilistic transition matrix. Given a text document dataset, a lis</context>
</contexts>
<marker>Wu, Ye, Ng, 2010</marker>
<rawString>Wu Q, Ye Y, Ng M, et al. 2010. Exploiting word cluster information for unsupervised feature selection Trends in Artificial Intelligence, pages 292-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
</authors>
<title>Learning from labeled and unlabeled data with label propagation.</title>
<date>2002</date>
<tech>Technical Report CMU-CALD-02-107,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="5662" citStr="Zhu and Ghahramani, 2002" startWordPosition="949" endWordPosition="952"> that larger edge weights allow labels to travel through more easily. So we define a V x V probabilistic transition matrix T where Tij = P(vj —* vi) = wij/ EVk=1 wkj. The L value which is used to represent the number of word clusters is specified. We define a V x L label matrix Y . Clearly, yi E Y represents the label probability distributions of word vi and Y ∗ i = argmax Yik(0 &lt; k &lt; L) is its cluster label. For example, suppose L = 3 and a word v has a label distribution y =&lt; 0.1, 0.8, 0.1 &gt;, it implies that v belongs to the second class. 3 Unsupervised LP Word Clustering Label propagation (Zhu and Ghahramani, 2002) is a semi-supervised algorithm (Semi-LP) which needs labeled data. Let {(v1, y1), · · · , (vl, yl)} be labeled data, {(vl+1, yl+1), · · · , (vl+u, yl+u)} be unlabeled ones where l + u = V , YL = [y1, , · · · , yl]T and YU = [yl+1, · · · , yl+u]T. YU is un-known and l &lt;&lt; u. The label propagation algorithm is summarized in Algorithm 1. In clustering problems, the goal is to select a set of exemplars from a dataset that are representative of the dataset and each cluster is represented by one and only one exemplar (Krause and Gomes, 2010). However, these exemplars are just all Semi-LP needs for c</context>
</contexts>
<marker>Zhu, Ghahramani, 2002</marker>
<rawString>Zhu X, Ghahramani Z. 2002. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02-107, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
<author>Z Ghahramani</author>
<author>J Lafferty</author>
</authors>
<title>Semi-supervised learning using gaussian fields and harmonic functions.</title>
<date>2003</date>
<booktitle>In Proceedings ofICML,</booktitle>
<pages>912--919</pages>
<marker>Zhu, Ghahramani, Lafferty, 2003</marker>
<rawString>Zhu X, Ghahramani Z, Lafferty J. 2003. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings ofICML, pages 912-919.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
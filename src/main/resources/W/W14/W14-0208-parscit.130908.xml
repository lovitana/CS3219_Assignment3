<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004030">
<title confidence="0.9986865">
Multi-threaded Interaction Management for
Dynamic Spatial Applications
</title>
<author confidence="0.881672">
Srinivasan Janarthanam
</author>
<affiliation confidence="0.882394">
Interaction Lab
Heriot-Watt University
Edinburgh
</affiliation>
<email confidence="0.994321">
sc445@hw.ac.uk
</email>
<sectionHeader confidence="0.993777" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995407">
We present a multi-threaded Interaction
Manager (IM) that is used to track differ-
ent dimensions of user-system conversa-
tions that are required to interleave with
each other in a coherent and timely man-
ner. This is explained in the context of
a spoken dialogue system for pedestrian
navigation and city question-answering,
with information push about nearby or vis-
ible points-of-interest (PoI).
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999994">
We present a multi-threaded Interaction Manager
(IM) that is used to track different dimensions of
user-system conversations and interleave the dif-
ferent converational threads coherently. The IM
that we present interacts with the user in a spa-
tial domain and interleaves navigation informa-
tion along with historical and cultural information
about the entities that users can see around them.
In addition, it aims to answer questions that users
might have about those entities. This presents
a complex conversational situation where several
conversational threads have to be interleaved in
such a way that the system utterances are pre-
sented to the user at the right time but in a pri-
oritised order, and with bridging utterances when
threads are interrupted and resumed. For instance,
a navigation instruction may be important (since
the user is walking up to a junction at which they
need to turn) and therefore it needs to be spoken
before continuing information presentation about
an entity or answering other ongoing questions.
</bodyText>
<sectionHeader confidence="0.999715" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.988542782608696">
Previously, multi-threaded interaction was used
to handle multiple simultaneous tasks in human-
robot interaction (HRI) scenarios (Lemon and
Gruenstein, 2004). This idea also turns out to be
Oliver Lemon
Interaction Lab
Heriot-Watt University
Edinburgh
o.lemon@hw.ac.uk
important for cases where humans are interacting
with a variety of different web-services in paral-
lel. Human multitasking in dialogue is discussed
in (Yang et al., 2008).
(Lemon and Gruenstein, 2004) presented a
multi-threaded dialogue management approach
for managing several concurrent tasks in an HRI
scenario. The robot could, for example be flying
to a location while simultaneously searching for
a vehicle, and utterances about both tasks could
be interleaved. Here, conversational threads were
managed using a representation called the “Dia-
logue Move Tree”, which represented conversa-
tional threads as branches of the tree, linked to an
“Activity Tree” which represented the states of on-
going robot tasks (deliver medical supplies, fly to a
waypoint, search for a truck), which could be ac-
tive simultaneously. The situation for our pedes-
trian navigation and information system is simi-
lar - concurrent tasks need to be managed coher-
ently via conversation. The approach adopted in
this paper is similar to (Lemon and Gruenstein,
2004). However, in this work we separate out
a domain-general thread called ‘dialogue control’
which handles generic issues like clarification of
reference across all tasks. This increasing modu-
larisation of the dialogue threads makes it possible
to learn individual dialogue policies for each one,
in future work.
(Nakano et al., 2008) presented an approach
where one of the several expert modules handling
different tasks is activated based on the user input,
but only one verbal expert is active at any one time.
In contrast to this, we present an approach where
several thread managers each handling a different
task can be activated in parallel and their outputs
stored and retrieved based on priority.
</bodyText>
<sectionHeader confidence="0.999306" genericHeader="method">
3 Multi-threaded IM
</sectionHeader>
<bodyText confidence="0.912118">
The Interaction Manager (IM) is the central com-
ponent of any spoken dialogue system architec-
</bodyText>
<page confidence="0.993492">
48
</page>
<note confidence="0.8655065">
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 48–52,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999875">
Figure 1: Interaction Manager Architecture
</figureCaption>
<bodyText confidence="0.999901777777778">
ture. Generally, it takes as input the user’s utter-
ances in the form of dialogue acts from the parser
and identifies the next dialogue action to present to
the user. Dialogue about a domain task is managed
using a dialogue strategy or policy (e.g. (Young,
2000; Lemon and Pietquin, 2007)). A dialogue
policy is a mapping between dialogue states and
dialogue actions, which are semantic representa-
tions of what the system should say next.
In order to handle multiple tasks simul-
taneously, we present an architecture for a
multi-threaded interaction manager that treats
conversation about each domain task as a thread.
These conversational threads are interleaved and
managed using techniques such as multi-queuing,
priority based pushing, and queue revision. We
describe these techniques below. The architecture
of the Interaction Manager is shown in figure 1.
</bodyText>
<subsectionHeader confidence="0.825998">
Multi-threading and queuing
</subsectionHeader>
<bodyText confidence="0.997036583333333">
In order to manage complex interactions involving
several conversational tasks/topics, we propose
that the each task be handled by a thread manager
within the interaction management framework.
Each such manager will handle a conversational
thread using a dialogue policy. Each thread
manager will be fed with the input from the user
and the dialogue actions generated will be stored
in separate queues. This approach allows the
interaction manager to produce several dialogue
actions at the same time although for different
conversational tasks.
</bodyText>
<subsectionHeader confidence="0.895759">
Prioritised Queue Management
</subsectionHeader>
<bodyText confidence="0.999984636363636">
Dialogue actions from the several threads are
stored in separate queues. The queues can be
assigned priorities that decide the order in which
items from the queues will be popped. The
dialogue actions in the queues are pushed to the
user based on an order of priority (see below).
This priority can either be fixed or dynamic based
on context. The system and user engagement
should also be checked so that system utterances
are pushed only when the system and user are not
speaking already.
</bodyText>
<subsectionHeader confidence="0.971544">
Queue Revision: resuming and bridging
</subsectionHeader>
<bodyText confidence="0.999906066666667">
The dialogue actions are generated and stored in
queues. Therefore, there is a difference between
the time they are generated and time that they are
pushed. Therefore dialogue actions in the queues
are revised periodically to reflect changes in con-
text. Obsolete dialogue actions will have to re-
moved for two reasons. Firstly, pushing them to
the user may make the conversation incoherent be-
cause the system may be speaking about an entity
that is no longer relevant and secondly, these obso-
lete dialogue actions may delay other other impor-
tant dialogue actions from being pushed on time.
In addition, it may also be useful to edit the dia-
logue actions to include discourse markers to sig-
nify topic change (Yang et al., 2008) and bridge
</bodyText>
<page confidence="0.99736">
49
</page>
<bodyText confidence="0.997458">
phrases to reintroduce a previous topic. We dis-
cuss some examples later in section 4.3.
</bodyText>
<sectionHeader confidence="0.99334" genericHeader="method">
4 SPACEBOOK Interaction Manager
</sectionHeader>
<bodyText confidence="0.999994">
As a part of the SpaceBook EU FP7 project,
we implemented the above design for a multi-
threaded interaction manager that presents the user
with navigational instructions, pushes PoI infor-
mation, and manages QA questions (Janarthanam
et al., 2013). It receives the user’s input in the
form of a dialogue act (DA) from the ASR mod-
ule and the user’s location (latitude and longitude),
orientation, and speed from the Pedestrian Tracker
module. Based on these inputs and the dialogue
context, the IM responds with a system output di-
alogue act. It should be noted that the location
coordinates of the user are sent to the IM every 2
seconds. This allows the IM to generate location
aware information at a high frequency. In addition,
the IM has to deal with incoming requests and re-
sponses from the user’s spoken inputs. With the
possibility of system utterances being generated
at a frequency of one every two seconds, there is
a need for an efficient mechanism to manage the
conversation and reduce the risk of overloading
the user with information. These tasks are treated
as separate conversational threads.
</bodyText>
<subsectionHeader confidence="0.991044">
4.1 Conversational Threads
</subsectionHeader>
<bodyText confidence="0.999990928571429">
The SpaceBook IM manages the conversation
using five conversational threads using dedicated
task managers. Three threads: ‘navigation’,
‘question answering’ and ‘PoI pushing’, represent
the core tasks of our system. In addition, for
handling the issues in dialogue management,
we introduce two threads: ‘dialogue control’
and ‘request response’. These different threads
represent the state of different dimensions of the
user-system conversation that need to interleave
with each other coherently. Each of the threads
is managed by a thread manager using a dialogue
policy. Each thread can generate a dialogue ac-
tion depending on the context, as described below:
</bodyText>
<subsectionHeader confidence="0.767263">
Dialogue Control
</subsectionHeader>
<bodyText confidence="0.9999815">
During the course of the conversation, the IM uses
this thread to manage user requests for repetition,
issues with unparsed (i.e. not understood) user
utterances, utterances that have low ASR confi-
dence, and so on. The dialogue control thread is
also used to manage reference resolution in cases
where referring expressions are underspecified.
The IM resolves anaphoric references by keeping
a record of entities mentioned in the dialogue
context. It stores the name and type information
for each entity (such as landmark, building, etc)
mentioned in previous utterances by either user
or system. Subsequent user references to these
entities using expressions such as “the museum”,
“the cafe”, and so on, are resolved by searching
for the latest entity of the given type. In cases
where the IM cannot resolve the referent, it asks
the user to clarify.
</bodyText>
<subsectionHeader confidence="0.515144">
Request Response
</subsectionHeader>
<bodyText confidence="0.985636815789474">
The user can also initiate tasks that interest
him/her at anytime during the conversation.
These tasks include searching for an entity (e.g.
a museum or a restaurant), requesting navigation
instructions to a destination, and asking questions
about the entities in the city database such as their
location (“Where is X?”, “How far is X?”). Dur-
ing navigation, users might want to ask questions
about the destination, ask for next instructions,
etc. All these user requests are handled using
the request response thread. For instance, when
the user asks for directions, the IM resolves the
destination entity (perhaps using clarification)
in the city model and acknowledges the user
request. The task is then further handled using the
Navigation thread.
Navigation
The IM identifies the location of the destination
entity and queries a city database (Bartie and
Mackaness, 2013) for a route plan. Using the route
plan, the navigation thread presents step-by-step
instructions to the user based on the current loca-
tion and orientation of the user. The IM contin-
uously monitors users to determine if at any time
they are deviating from the planned route and pro-
vides corrective instructions. As users get near
to the next node on the route plan, the next in-
struction is given. The IM uses highly salient vis-
ible landmarks (Bartie et al., 2013) and popular
landmarks near the nodes to instruct the user (e.g.
“When you reach Clydesdale Bank, turn left on
to Nicolson Square”). The IM also informs users
when they pass by recognisable landmarks, just to
reassure them that they are on the right track (e.g.
“You will pass by Tesco on the right”). When the
user is close to his/her destination, the IM deter-
mines whether the destination is visible to the user,
informs the user, and closes the task.
</bodyText>
<page confidence="0.990049">
50
</page>
<bodyText confidence="0.8638939375">
Usr: I want to go to the National Gallery.
Sys: Ok. I am now looking for directions to
the National Gallery.
Sys: The National Gallery is on Queens Street
at about 300 meters away.
....
Sys: Can you see the Bank of Scotland about
300 meters away on your left? It has a big
green dome.
Usr: Yes. I can see it.
Sys: Walk towards it.
Sys: Can you see the National Gallery in front
of you at about 100 meters? It is a building
with Roman architecture.
Usr: Yes.
Sys: You have now reached your destination.
</bodyText>
<subsectionHeader confidence="0.995505">
Question Answering
</subsectionHeader>
<bodyText confidence="0.9999432">
The system also answers ad hoc questions from
the user (e.g. “Who is David Hume?”, “What is
the Old College?”, “Who was William Wallace”,
etc). These are sent to the QA server and answered
based on responses from the Question-Answering
(QA) server (Janarthanam et al., 2013). The
dialogue policy here is to answer the user’s
question with the first snippet available and ask
the user to request for more if more snippets are
available and he or she is interested.
</bodyText>
<subsectionHeader confidence="0.835589">
Pushing PoI Information
</subsectionHeader>
<bodyText confidence="0.999973857142857">
When the user is mobile, the IM identifies pop-
ular points of interest (PoI) on the route based on
two factors: proximity and visibility. The dialogue
policy is to introduce the PoI, query the QA server
for snippets and push the first snippet to the user.
The user is encouraged to ask for more informa-
tion if he/she is interested.
</bodyText>
<subsectionHeader confidence="0.992411">
4.2 Priority assignment in SpaceBook
</subsectionHeader>
<bodyText confidence="0.940147">
Priority is assigned to the above dialogue threads
as follows:
</bodyText>
<figureCaption confidence="0.590157">
Priority 1. Dialogue control (repeat request, clari-
fications etc)
Priority 2. Responding to user requests
Priority 3. System initiated navigation task actions
Priority 4. Responses to User-initiated QA actions
Priority 5. PoI Push actions
</figureCaption>
<bodyText confidence="0.9957">
For instance, informing the user of a PoI could be
delayed if the user needs to be given an instruction
to turn at the junction he is approaching.
</bodyText>
<subsectionHeader confidence="0.999476">
4.3 Queue revision and bridging utterances
</subsectionHeader>
<bodyText confidence="0.979073045454545">
The queues need to be revised at regular intervals
in order to keep the information in them relevant
to context. For instance, the dialogue action of in-
forming the user of his/her location is deleted after
5 seconds, as this tends to become obsolete. Sim-
ilarly, dialogue actions corresponding to informa-
tion segments in PoI and QA queues are edited to
inform the utterance generator of other interven-
ing dialogue actions so that it can use appropri-
ate bridge phrases to reintroduce the focus of the
conversational thread. For instance, as shown in
the example below, the utterance generator inserts
a bridge phrase (i.e. “More on Old College”) to
reintroduce the focus of the PoI push task because
of the intervening user request and the subsequent
system response.
Sys: In front of you, about 200 meters away is
the Old College. It has a grey dome on top.
User: Where am I?
Sys: You are on Chambers street.
Sys: More on Old College. Situated on South
Bridge, the Old College is
</bodyText>
<sectionHeader confidence="0.998872" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999433">
We presented an architecture for a multi-threaded
Interaction Manager that can handle multiple con-
versational tasks. We also described an implemen-
tation of the architecture in a dynamic spatial en-
vironment. The SpaceBook IM is a multi-tasking
IM that aims to interleave navigation information
along with historical information about the enti-
ties users can see around them. In addition, it aims
to answer questions users might have about those
entities.
</bodyText>
<sectionHeader confidence="0.996924" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.959926222222222">
The research leading to these results has received
funding from the European Community’s Seventh
Framework Programme (FP7/2007-2013) under
grant agreement no. 270019 (SPACEBOOK project
www.spacebook-project.org).
Sys: In front of you, about 200 meters away is
Old College. It has a grey dome on top.
Sys: Situated on South Bridge, Old College is
Sys: Ask for more information if interested.
</bodyText>
<page confidence="0.998105">
51
</page>
<sectionHeader confidence="0.995891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999906911764706">
P. Bartie and W. Mackaness. 2013. D3.1.2: The SpaceBook
City Model. Technical report, The SPACEBOOK Project
(FP7/2011-2014 grant agreement no. 270019).
P. Bartie, W. Mackaness, M. Fredriksson, and J. Konigsmann.
2013. D2.1.2 Final Viewshed Component. Technical
report, The SPACEBOOK Project (FP7/2011-2014 grant
agreement no. 270019).
S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas, A. Dick-
inson, X. Liu, W. Mackaness, and B. Webber. 2013.
Evaluating a city exploration dialogue system combining
question-answering and pedestrian navigation. In Proc.
ACL 2013.
Oliver Lemon and Alexander Gruenstein. 2004. Mul-
tithreaded context for robust conversational interfaces:
context-sensitive speech recognition and interpretation of
corrective fragments. ACM Transactions on Computer-
Human Interaction (ACM TOCHI), 11(3):241– 267.
Oliver Lemon and Olivier Pietquin. 2007. Machine learning
for spoken dialogue systems. In Interspeech.
Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and Hi-
roshi Tsujino. 2008. A framework for building conversa-
tional agents based on a multi-expert model. In Proceed-
ings of the 9th SIGdial Workshop on Discourse and Dia-
logue, SIGdial ’08, pages 88–91, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Fan Yang, Peter A. Heeman, and Andrew Kun. 2008.
Switching to real-time tasks in multi-tasking dialogue.
In Proceedings of the 22Nd International Conference on
Computational Linguistics - Volume 1, COLING ’08,
pages 1025–1032, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Steve Young. 2000. Probabilistic methods in spoken dia-
logue systems. Philosophical Transactions of the Royal
Society (Series A), 358(1769):1389–1402.
</reference>
<page confidence="0.998855">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.447957">
<title confidence="0.923577">Multi-threaded Interaction Management Dynamic Spatial Applications Srinivasan Interaction</title>
<author confidence="0.546537">Heriot-Watt</author>
<email confidence="0.960317">sc445@hw.ac.uk</email>
<abstract confidence="0.993254909090909">We present a multi-threaded Interaction Manager (IM) that is used to track different dimensions of user-system conversations that are required to interleave with each other in a coherent and timely manner. This is explained in the context of a spoken dialogue system for pedestrian navigation and city question-answering, with information push about nearby or visible points-of-interest (PoI).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Bartie</author>
<author>W Mackaness</author>
</authors>
<title>D3.1.2: The SpaceBook City Model.</title>
<date>2013</date>
<booktitle>The SPACEBOOK Project (FP7/2011-2014 grant agreement no.</booktitle>
<tech>Technical report,</tech>
<pages>270019</pages>
<contexts>
<context position="10372" citStr="Bartie and Mackaness, 2013" startWordPosition="1622" endWordPosition="1625">the entities in the city database such as their location (“Where is X?”, “How far is X?”). During navigation, users might want to ask questions about the destination, ask for next instructions, etc. All these user requests are handled using the request response thread. For instance, when the user asks for directions, the IM resolves the destination entity (perhaps using clarification) in the city model and acknowledges the user request. The task is then further handled using the Navigation thread. Navigation The IM identifies the location of the destination entity and queries a city database (Bartie and Mackaness, 2013) for a route plan. Using the route plan, the navigation thread presents step-by-step instructions to the user based on the current location and orientation of the user. The IM continuously monitors users to determine if at any time they are deviating from the planned route and provides corrective instructions. As users get near to the next node on the route plan, the next instruction is given. The IM uses highly salient visible landmarks (Bartie et al., 2013) and popular landmarks near the nodes to instruct the user (e.g. “When you reach Clydesdale Bank, turn left on to Nicolson Square”). The </context>
</contexts>
<marker>Bartie, Mackaness, 2013</marker>
<rawString>P. Bartie and W. Mackaness. 2013. D3.1.2: The SpaceBook City Model. Technical report, The SPACEBOOK Project (FP7/2011-2014 grant agreement no. 270019).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Bartie</author>
<author>W Mackaness</author>
<author>M Fredriksson</author>
<author>J Konigsmann</author>
</authors>
<date>2013</date>
<booktitle>D2.1.2 Final Viewshed Component. Technical report, The SPACEBOOK Project (FP7/2011-2014</booktitle>
<contexts>
<context position="10835" citStr="Bartie et al., 2013" startWordPosition="1704" endWordPosition="1707">led using the Navigation thread. Navigation The IM identifies the location of the destination entity and queries a city database (Bartie and Mackaness, 2013) for a route plan. Using the route plan, the navigation thread presents step-by-step instructions to the user based on the current location and orientation of the user. The IM continuously monitors users to determine if at any time they are deviating from the planned route and provides corrective instructions. As users get near to the next node on the route plan, the next instruction is given. The IM uses highly salient visible landmarks (Bartie et al., 2013) and popular landmarks near the nodes to instruct the user (e.g. “When you reach Clydesdale Bank, turn left on to Nicolson Square”). The IM also informs users when they pass by recognisable landmarks, just to reassure them that they are on the right track (e.g. “You will pass by Tesco on the right”). When the user is close to his/her destination, the IM determines whether the destination is visible to the user, informs the user, and closes the task. 50 Usr: I want to go to the National Gallery. Sys: Ok. I am now looking for directions to the National Gallery. Sys: The National Gallery is on Qu</context>
</contexts>
<marker>Bartie, Mackaness, Fredriksson, Konigsmann, 2013</marker>
<rawString>P. Bartie, W. Mackaness, M. Fredriksson, and J. Konigsmann. 2013. D2.1.2 Final Viewshed Component. Technical report, The SPACEBOOK Project (FP7/2011-2014 grant agreement no. 270019).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Janarthanam</author>
<author>O Lemon</author>
<author>P Bartie</author>
<author>T Dalmas</author>
<author>A Dickinson</author>
<author>X Liu</author>
<author>W Mackaness</author>
<author>B Webber</author>
</authors>
<title>Evaluating a city exploration dialogue system combining question-answering and pedestrian navigation.</title>
<date>2013</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="7060" citStr="Janarthanam et al., 2013" startWordPosition="1097" endWordPosition="1100"> these obsolete dialogue actions may delay other other important dialogue actions from being pushed on time. In addition, it may also be useful to edit the dialogue actions to include discourse markers to signify topic change (Yang et al., 2008) and bridge 49 phrases to reintroduce a previous topic. We discuss some examples later in section 4.3. 4 SPACEBOOK Interaction Manager As a part of the SpaceBook EU FP7 project, we implemented the above design for a multithreaded interaction manager that presents the user with navigational instructions, pushes PoI information, and manages QA questions (Janarthanam et al., 2013). It receives the user’s input in the form of a dialogue act (DA) from the ASR module and the user’s location (latitude and longitude), orientation, and speed from the Pedestrian Tracker module. Based on these inputs and the dialogue context, the IM responds with a system output dialogue act. It should be noted that the location coordinates of the user are sent to the IM every 2 seconds. This allows the IM to generate location aware information at a high frequency. In addition, the IM has to deal with incoming requests and responses from the user’s spoken inputs. With the possibility of system</context>
<context position="12087" citStr="Janarthanam et al., 2013" startWordPosition="1930" endWordPosition="1933">ters away. .... Sys: Can you see the Bank of Scotland about 300 meters away on your left? It has a big green dome. Usr: Yes. I can see it. Sys: Walk towards it. Sys: Can you see the National Gallery in front of you at about 100 meters? It is a building with Roman architecture. Usr: Yes. Sys: You have now reached your destination. Question Answering The system also answers ad hoc questions from the user (e.g. “Who is David Hume?”, “What is the Old College?”, “Who was William Wallace”, etc). These are sent to the QA server and answered based on responses from the Question-Answering (QA) server (Janarthanam et al., 2013). The dialogue policy here is to answer the user’s question with the first snippet available and ask the user to request for more if more snippets are available and he or she is interested. Pushing PoI Information When the user is mobile, the IM identifies popular points of interest (PoI) on the route based on two factors: proximity and visibility. The dialogue policy is to introduce the PoI, query the QA server for snippets and push the first snippet to the user. The user is encouraged to ask for more information if he/she is interested. 4.2 Priority assignment in SpaceBook Priority is assign</context>
</contexts>
<marker>Janarthanam, Lemon, Bartie, Dalmas, Dickinson, Liu, Mackaness, Webber, 2013</marker>
<rawString>S. Janarthanam, O. Lemon, P. Bartie, T. Dalmas, A. Dickinson, X. Liu, W. Mackaness, and B. Webber. 2013. Evaluating a city exploration dialogue system combining question-answering and pedestrian navigation. In Proc. ACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Alexander Gruenstein</author>
</authors>
<title>Multithreaded context for robust conversational interfaces: context-sensitive speech recognition and interpretation of corrective fragments.</title>
<date>2004</date>
<journal>ACM Transactions on ComputerHuman Interaction (ACM TOCHI),</journal>
<volume>11</volume>
<issue>3</issue>
<pages>267</pages>
<contexts>
<context position="1778" citStr="Lemon and Gruenstein, 2004" startWordPosition="261" endWordPosition="264">nterleaved in such a way that the system utterances are presented to the user at the right time but in a prioritised order, and with bridging utterances when threads are interrupted and resumed. For instance, a navigation instruction may be important (since the user is walking up to a junction at which they need to turn) and therefore it needs to be spoken before continuing information presentation about an entity or answering other ongoing questions. 2 Related work Previously, multi-threaded interaction was used to handle multiple simultaneous tasks in humanrobot interaction (HRI) scenarios (Lemon and Gruenstein, 2004). This idea also turns out to be Oliver Lemon Interaction Lab Heriot-Watt University Edinburgh o.lemon@hw.ac.uk important for cases where humans are interacting with a variety of different web-services in parallel. Human multitasking in dialogue is discussed in (Yang et al., 2008). (Lemon and Gruenstein, 2004) presented a multi-threaded dialogue management approach for managing several concurrent tasks in an HRI scenario. The robot could, for example be flying to a location while simultaneously searching for a vehicle, and utterances about both tasks could be interleaved. Here, conversational </context>
</contexts>
<marker>Lemon, Gruenstein, 2004</marker>
<rawString>Oliver Lemon and Alexander Gruenstein. 2004. Multithreaded context for robust conversational interfaces: context-sensitive speech recognition and interpretation of corrective fragments. ACM Transactions on ComputerHuman Interaction (ACM TOCHI), 11(3):241– 267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Olivier Pietquin</author>
</authors>
<title>Machine learning for spoken dialogue systems.</title>
<date>2007</date>
<journal>In Interspeech.</journal>
<contexts>
<context position="4250" citStr="Lemon and Pietquin, 2007" startWordPosition="649" endWordPosition="652">ority. 3 Multi-threaded IM The Interaction Manager (IM) is the central component of any spoken dialogue system architec48 Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 48–52, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Figure 1: Interaction Manager Architecture ture. Generally, it takes as input the user’s utterances in the form of dialogue acts from the parser and identifies the next dialogue action to present to the user. Dialogue about a domain task is managed using a dialogue strategy or policy (e.g. (Young, 2000; Lemon and Pietquin, 2007)). A dialogue policy is a mapping between dialogue states and dialogue actions, which are semantic representations of what the system should say next. In order to handle multiple tasks simultaneously, we present an architecture for a multi-threaded interaction manager that treats conversation about each domain task as a thread. These conversational threads are interleaved and managed using techniques such as multi-queuing, priority based pushing, and queue revision. We describe these techniques below. The architecture of the Interaction Manager is shown in figure 1. Multi-threading and queuing</context>
</contexts>
<marker>Lemon, Pietquin, 2007</marker>
<rawString>Oliver Lemon and Olivier Pietquin. 2007. Machine learning for spoken dialogue systems. In Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikio Nakano</author>
<author>Kotaro Funakoshi</author>
<author>Yuji Hasegawa</author>
<author>Hiroshi Tsujino</author>
</authors>
<title>A framework for building conversational agents based on a multi-expert model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, SIGdial ’08,</booktitle>
<pages>88--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3260" citStr="Nakano et al., 2008" startWordPosition="488" endWordPosition="491">ypoint, search for a truck), which could be active simultaneously. The situation for our pedestrian navigation and information system is similar - concurrent tasks need to be managed coherently via conversation. The approach adopted in this paper is similar to (Lemon and Gruenstein, 2004). However, in this work we separate out a domain-general thread called ‘dialogue control’ which handles generic issues like clarification of reference across all tasks. This increasing modularisation of the dialogue threads makes it possible to learn individual dialogue policies for each one, in future work. (Nakano et al., 2008) presented an approach where one of the several expert modules handling different tasks is activated based on the user input, but only one verbal expert is active at any one time. In contrast to this, we present an approach where several thread managers each handling a different task can be activated in parallel and their outputs stored and retrieved based on priority. 3 Multi-threaded IM The Interaction Manager (IM) is the central component of any spoken dialogue system architec48 Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 48–52, Gothenburg, Sweden, April 2</context>
</contexts>
<marker>Nakano, Funakoshi, Hasegawa, Tsujino, 2008</marker>
<rawString>Mikio Nakano, Kotaro Funakoshi, Yuji Hasegawa, and Hiroshi Tsujino. 2008. A framework for building conversational agents based on a multi-expert model. In Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, SIGdial ’08, pages 88–91, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fan Yang</author>
<author>Peter A Heeman</author>
<author>Andrew Kun</author>
</authors>
<title>Switching to real-time tasks in multi-tasking dialogue.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ’08,</booktitle>
<pages>1025--1032</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2059" citStr="Yang et al., 2008" startWordPosition="303" endWordPosition="306">tion at which they need to turn) and therefore it needs to be spoken before continuing information presentation about an entity or answering other ongoing questions. 2 Related work Previously, multi-threaded interaction was used to handle multiple simultaneous tasks in humanrobot interaction (HRI) scenarios (Lemon and Gruenstein, 2004). This idea also turns out to be Oliver Lemon Interaction Lab Heriot-Watt University Edinburgh o.lemon@hw.ac.uk important for cases where humans are interacting with a variety of different web-services in parallel. Human multitasking in dialogue is discussed in (Yang et al., 2008). (Lemon and Gruenstein, 2004) presented a multi-threaded dialogue management approach for managing several concurrent tasks in an HRI scenario. The robot could, for example be flying to a location while simultaneously searching for a vehicle, and utterances about both tasks could be interleaved. Here, conversational threads were managed using a representation called the “Dialogue Move Tree”, which represented conversational threads as branches of the tree, linked to an “Activity Tree” which represented the states of ongoing robot tasks (deliver medical supplies, fly to a waypoint, search for </context>
<context position="6680" citStr="Yang et al., 2008" startWordPosition="1036" endWordPosition="1039">they are generated and time that they are pushed. Therefore dialogue actions in the queues are revised periodically to reflect changes in context. Obsolete dialogue actions will have to removed for two reasons. Firstly, pushing them to the user may make the conversation incoherent because the system may be speaking about an entity that is no longer relevant and secondly, these obsolete dialogue actions may delay other other important dialogue actions from being pushed on time. In addition, it may also be useful to edit the dialogue actions to include discourse markers to signify topic change (Yang et al., 2008) and bridge 49 phrases to reintroduce a previous topic. We discuss some examples later in section 4.3. 4 SPACEBOOK Interaction Manager As a part of the SpaceBook EU FP7 project, we implemented the above design for a multithreaded interaction manager that presents the user with navigational instructions, pushes PoI information, and manages QA questions (Janarthanam et al., 2013). It receives the user’s input in the form of a dialogue act (DA) from the ASR module and the user’s location (latitude and longitude), orientation, and speed from the Pedestrian Tracker module. Based on these inputs and</context>
</contexts>
<marker>Yang, Heeman, Kun, 2008</marker>
<rawString>Fan Yang, Peter A. Heeman, and Andrew Kun. 2008. Switching to real-time tasks in multi-tasking dialogue. In Proceedings of the 22Nd International Conference on Computational Linguistics - Volume 1, COLING ’08, pages 1025–1032, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
</authors>
<title>Probabilistic methods in spoken dialogue systems.</title>
<date>2000</date>
<journal>Philosophical Transactions of the Royal Society (Series A),</journal>
<volume>358</volume>
<issue>1769</issue>
<contexts>
<context position="4223" citStr="Young, 2000" startWordPosition="647" endWordPosition="648"> based on priority. 3 Multi-threaded IM The Interaction Manager (IM) is the central component of any spoken dialogue system architec48 Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 48–52, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics Figure 1: Interaction Manager Architecture ture. Generally, it takes as input the user’s utterances in the form of dialogue acts from the parser and identifies the next dialogue action to present to the user. Dialogue about a domain task is managed using a dialogue strategy or policy (e.g. (Young, 2000; Lemon and Pietquin, 2007)). A dialogue policy is a mapping between dialogue states and dialogue actions, which are semantic representations of what the system should say next. In order to handle multiple tasks simultaneously, we present an architecture for a multi-threaded interaction manager that treats conversation about each domain task as a thread. These conversational threads are interleaved and managed using techniques such as multi-queuing, priority based pushing, and queue revision. We describe these techniques below. The architecture of the Interaction Manager is shown in figure 1. </context>
</contexts>
<marker>Young, 2000</marker>
<rawString>Steve Young. 2000. Probabilistic methods in spoken dialogue systems. Philosophical Transactions of the Royal Society (Series A), 358(1769):1389–1402.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004065">
<title confidence="0.971449">
Overview of SIGHAN 2014 Bake-off for Chinese Spelling Check
</title>
<author confidence="0.99952">
Liang-Chih Yu1,2, Lung-Hao Lee3,4, Yuen-Hsien Tseng3, Hsin-Hsi Chen4
</author>
<affiliation confidence="0.9988425">
1Dept. of Information Management, Yuen-Ze University
2Innovation Center for Big Data and Digital Convergence, Yuen-Ze University
3Information Technology Center, National Taiwan Normal University
4Dept. of Computer Science and Information Engineering, National Taiwan University
</affiliation>
<email confidence="0.9837335">
lcyu@saturn.yzu.edu.tw, lhlee@ntnu.edu.tw,
samtseng@ntnu.edu.tw, hhchen@ntu.edu.tw
</email>
<sectionHeader confidence="0.995561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997645">
This paper introduces a Chinese Spelling
Check campaign organized for the
SIGHAN 2014 bake-off, including task
description, data preparation, perfor-
mance metrics, and evaluation results
based on essays written by Chinese as a
foreign language learners. The hope is
that such evaluations can produce more
advanced Chinese spelling check tech-
niques.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999814875">
Chinese spelling errors frequently arise from
confusion between multiple Chinese characters
which are phonologically and visually similar,
but semantically distinct (Liu et al., 2011). The
SIGHAN 2013 Chinese Spelling Check Bake-
off was the first campaign to provide data sets as
benchmarks for the objective performance evalu-
ation of Chinese spelling checkers (Wu et al.
2013). The collected data set is publicly available
at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm.
The competition resulted in the integration of
effective NLP techniques in the development of
Chinese spelling checkers. Language modeling
was used to glean extra semantic clues and col-
lect web resources together to identify and cor-
rect spelling errors (Chen et al., 2013). A hybrid
model was proposed to combine language mod-
els and statistical machine translation for spelling
error correction (Liu et al. 2013). A linear regres-
sion model was trained using phonological and
orthographic similarities to correct misspelled
characters (Chang et al. 2013). Web-based
measures were adopted to score candidates for
Chinese spelling error correction (Yu et al.,
2013). A graph model was used to represent the
sentence, using the single source shortest path
algorithm for correcting spelling errors (Jia et al.
2013)
SIGHAN 2014 Bake-off, again features a Chi-
nese Spelling Check task, providing an evalua-
tion platform for the development and implemen-
tation of automatic Chinese spelling checkers.
Given a passage composed of several sentences,
the checker should identify all possible spelling
errors, highlight their locations and suggest pos-
sible corrections. While previous tasks were
based on essays written by native Chinese speak-
ers, the current task is based on essays written by
learners of Chinese as a Foreign Language (CFL),
which should provide a greater challenge
The rest of this article is organized as follows.
Section 2 provides an overview of the SIGHAN
2014 Bake-off Chinese Spelling Check task. Sec-
tion 3 introduces the data sets used for evaluation.
Section 4 proposes evaluation metrics. Section 5
compares results for the various contestants. Fi-
nally, we conclude this with findings and future
research directions in Section 6.
</bodyText>
<sectionHeader confidence="0.974923" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.999908733333334">
This task evaluates Chinese spelling checker per-
formance based on Chinese text passages con-
sisting of several sentences with and without
spelling errors. The checker should identify in-
correct characters in the passage and suggest cor-
rections. Each character or punctuation mark oc-
cupies 1 spot for counting location. The input
instance is given a unique passage number PID.
If the sentence contains no spelling errors, the
checker should return “PID, 0”. If an input pas-
sage contains at least one spelling error, the out-
put format is “PID [, location, correction]+”,
where the symbol “+” indicates there is one or
more instance of the predicting element “[, loca-
tion, correction]”. “Location” and “correction”
</bodyText>
<page confidence="0.980033">
126
</page>
<note confidence="0.6532325">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 126–132,
Wuhan, China, 20-21 October 2014
</note>
<bodyText confidence="0.999735">
respectively denote the location of incorrect
character and its correct version. Table 1 presents
some examples. In Ex. 1, the 15th character “無”
is wrong, and should be “舞”. There are 3 wrong
characters in Ex. 2, and correct characters “生,”
“直,” and “關” should be used in locations 3, 26,
and 35, respectively. Location “0” denotes that
there is no spelling error in Ex. 3
</bodyText>
<table confidence="0.996313">
Example 1
Input (pid= A2-1051-1) 後天是小明的生
日,我要開一個無會。
Output A2-1051-1, 15, 舞
Example 2
Input (pid=B1-0201-1) 我一身中的貴人就是我姨媽,從我回來台灣的時候,
她一只都很照顧我,也很觀心我。
Output B1-0201-1, 3, 生, 26, 直, 35, 關
Example 3
Input (pid=C1-1849-1) 聯合國報告指出,
至二零五零年,全球人口將達九十
二億,新增人口幾乎全來自開發中
國家。
Output C1-1849-1, 0
</table>
<tableCaption confidence="0.999841">
Table 1. Some examples used in our task
</tableCaption>
<sectionHeader confidence="0.966529" genericHeader="method">
3 Data Preparation
</sectionHeader>
<bodyText confidence="0.99977">
The learner corpus used in our task was collected
from the essay section of the computer-based
Test of Chinese as a Foreign Language (TOCFL),
administered in Taiwan. The writing test is de-
signed according to the six proficiency levels of
the Common European Framework of Reference
(CEFR). A total of 1714 essays were typed
online (i.e., not hand-written), and then spelling
errors were manually annotated by trained native
Chinese speakers who also provided corrections
corresponding to each error. The essays were
then split into three sets as follows
</bodyText>
<listItem confidence="0.970877">
• Training Set
</listItem>
<bodyText confidence="0.9984836">
This set included 1,301 selected essays with a
total of 5,284 spelling errors. Each essay is rep-
resented in SGML format shown in Fig. 1. The
title attribute is used to describe the essay topic.
Each passage is composed of several sentences,
and each passage contains at least one spelling
error, and the data indicates both the error’s loca-
tion and corresponding correction. All essays in
this set are used to train the developed spelling
checker.
</bodyText>
<figure confidence="0.993982952380952">
&lt;ESSAY title= &amp;quot;寫給即將初次見面的筆友的
一封信&amp;quot;&gt;
&lt;TEXT&gt;
&lt;PASSAGE id=&amp;quot;B1-0112-1&amp;quot;&gt;那一天我會穿牛
仔褲和紅色的外套;頭會帶著藍色的帽子。
如果你找不到我,可以打我的手機。
&lt;/PASSAGE&gt;
&lt;PASSAGE id=&amp;quot;B1-0112-2&amp;quot;&gt;我記得你說你想
試試看越南菜是有什麼味覺,午餐我會帶你
去吃。我也想試試看那一家的越南菜;網路
站說很多人喜歡那一家餐廳。&lt;/PASSAGE&gt;
&lt;/TEXT&gt;
&lt;MISTAKE id=&amp;quot;B1-0112-1&amp;quot; location=&amp;quot;19&amp;quot;&gt;
&lt;WRONG&gt;帶著&lt;/WRONG&gt;
&lt;CORRECTION&gt;戴著&lt;/CORRECTION&gt;
&lt;/MISTAKE&gt;
&lt;MISTAKE id=&amp;quot;B1-0112-2&amp;quot; location=&amp;quot;46&amp;quot;&gt;
&lt;WRONG&gt;網路站&lt;/WRONG&gt;
&lt;CORRECTION&gt;網路上&lt;/CORRECTION&gt;
&lt;/MISTAKE&gt;
&lt;/ESSAY&gt;
</figure>
<figureCaption confidence="0.998965">
Figure 1. An essay represented in SGML format
</figureCaption>
<listItem confidence="0.932382">
• Dryrun Set
</listItem>
<bodyText confidence="0.999670090909091">
A total of 20 passages were given to partici-
pants to familiarize themselves with the final
testing process. Each participant can submit
several runs generated using different models
with different parameter settings. In addition to
make sure the submitted results can be correctly
evaluated, participants can fine-tune their devel-
oped models in the dryrun phase. The purpose
of dryrun is output format validation only, and
no dryrun outcomes were considered in the offi-
cial evaluation
</bodyText>
<listItem confidence="0.989636">
• Test Set
</listItem>
<bodyText confidence="0.999929">
Table 2 shows a statistical summary of the
prepared test set. The set consists of 1,062 testing
passages, each with an average of 50 characters.
Half of these passages contained no spelling er-
rors, while the other half included at least one
spelling error each for a total of 792 spelling er-
rors used to evaluate the spelling checkers. The
evaluation was conducted as an open test. In ad-
dition to the data sets provided, registered re-
</bodyText>
<page confidence="0.986829">
127
</page>
<bodyText confidence="0.990510518518518">
search teams were allowed to employ any lin-
guistic and computational resources to detect and
correct spelling errors. Besides, passages written
by CFL learners may suffer from grammatical
errors, missing or redundant words, poor word
selection, or word ordering problems. The task in
question focuses exclusively on spelling error
correction.
Test Set Stat.
Number of essays 413
Number of passages 1,062
Number of characters 53,114
Average number of characters 50.01
in all passages
Number of passages with errors 531
Total number of characters 26,609
in the passages with errors
Number of erroneous characters 792
Average number of characters 50.11
in passages with errors
Average number of spelling errors 1.49
in passages with errors
Number of passages without errors 531
Total number of characters 26,505
in the passages without errors
Average number of characters 49.92
in passages without errors
</bodyText>
<tableCaption confidence="0.992121">
Table 2. Descriptive statistics of the test set.
</tableCaption>
<sectionHeader confidence="0.993118" genericHeader="method">
4 Performance Metrics
</sectionHeader>
<bodyText confidence="0.999725">
Table 3 shows the confusion matrix used for per-
formance evaluation. In the matrix, TP (True
Positive) is the number of passages with spelling
errors that are correctly identified by the spelling
checker; FP (False Positive) is the number of
passages in which non-existent errors are identi-
fied; TN (True Negative) is the number of pas-
sages without spelling errors which are correctly
identified as such; FN (False Negative) is the
number of passages with spelling errors for
which no errors are detected.
Correctness is determined at two levels. (1)
Detection level: all locations of incorrect charac-
ters in a given passage should be completely
identical with the gold standard. (2) Correction
level: all locations and corresponding corrections
of incorrect characters should be completely
identical with the gold standard. The following
metrics are measured at both levels with the help
of the confusion matrix.
</bodyText>
<listItem confidence="0.9913842">
• False Positive Rate (FPR) = FP / (FP+TN)
• Accuracy = (TP+TN) / (TP+FP+TN+FN)
• Precision = TP / (TP+FP)
• Recall = TP / (TP+FN)
• F1= 2 *Precision*Recall/(Precision+Recall)
</listItem>
<table confidence="0.999541714285714">
Confusion System Result
Matrix
Positive Negative
(Erroneous) (Correct)
Gold Positive TP FN
Standard
Negative FP TN
</table>
<tableCaption confidence="0.999882">
Table 3. Confusion matrix for evaluation.
</tableCaption>
<bodyText confidence="0.871061666666667">
Take for example, 5 testing inputs with gold
standards shown as “C1-1765-2, 0”, “C1-1833-2,
3, 再, 47, 反”, “B1-0176-3, 15, 棄, 22, 身”, “B1-
0206-5, 0”, and “B1-2707-4, 48, 現”. The system
may output the result shown as “C1-1765-2, 0”,
“C1-1833-2, 3, 再, 47, 返”, “B1-0176-3, 7, 氣,
22, 身, 35, 徳”, “B1-0206-5, 13, fj”, and “B1-
2707-4, 48, 現”. The evaluation tool will yield
the following performance.
■ False Positive Rate (FPR) = 0.5 (=1/2)
Notes: {“B1-0206-5, 13, fj”}/{“C1-1765-
2, 0”, “B1-0206-5, 0”}
</bodyText>
<listItem confidence="0.922827933333333">
■ Detection-level
• Acc.=0.6 (=3/5). Notes: {“C1-1765-2,
0”, “C1-1833-2, 3, 47”, “B1-2707-4,
48”} / {“C1-1765-2, 0”, “C1-1833-2,
3, 47”, “B1-0176-3, 15, 22”, “B1-
0206-5, 0”, “B1-2707-4, 48”}
• Pre.= 0.5 (=2/4). Notes: {“C1-1833-2, 3,
47”, “B1-2707-4, 48”} / {“C1-1833-2,
3, 47”, “B1-0176-3, 7, 22, 35”, “B1-
0206-5, 13”, “B1-2707-4, 48”}
• Rec.= 0.67 (=2/3). Notes: {“C1-1833-2,
3, 47”, “B1-2707-4, 48”} / {“C1-
1833-2, 3, 47”, “B1-0176-3, 15, 22”,
“B1-2707-4, 48”}
• F1=0.57 (=2*0.5*0.67/(0.5+0.67))
</listItem>
<page confidence="0.8366">
128
</page>
<listItem confidence="0.849981">
■ Correction-level
• Acc.=0.4 (=2/5). Notes: {“C1-1765-2,
0”, “B1-2707-4, 48, �”} / {“C1-
</listItem>
<bodyText confidence="0.903492666666667">
1765-2, 0”, “C1-1833-2, 3, 再, 47,
反”, “B1-0176-3, 15, 棄, 22, 身”,
“B1-0206-5, 0”, “B1-2707-4, 48, 現”}
</bodyText>
<listItem confidence="0.8917538">
• Pre.= 0.25 (=1/4). Notes: { “B1-2707-4,
48, �9,”} / {“C1-1833-2, 3, 再, 47,
LR”, “B1-0176-3, 7, &amp; 22, 身, 35,
&apos;(E,&amp;,F”, “B1-0206-5, 13, ffJ”, and “B1-
2707-4, 48, 現”}
• Rec.= 0.33 (=1/3). Notes: {“B1-2707-4,
48, 現”} / {“C1-1833-2, 3, 再, 47,
反”, “B1-0176-3, 15, 棄, 22, 身”,
“B1-2707-4, 48, 現”}
• F1=0.28 (=2*0.25*0.33/(0.25+0.33))
</listItem>
<sectionHeader confidence="0.907858" genericHeader="evaluation">
5 Evaluation Results
</sectionHeader>
<bodyText confidence="0.9970034">
Table 4 summarizes the submission statistics for
19 participant teams including 10 from universi-
ties and research institutions in China (BIT, CAS,
CAU, LYFYU, NJUPT, PKU, SCAU, SJTU,
SUDA, and ZJOU), 8 from Taiwan (ITRI,
KUAS, NCTU &amp; NTUT, NCYU, NTHU, NTOU,
SinicaCKIP, and SinicaSLMP) and one private
firm (Lingage). Among 19 registered teams, 13
teams submitted their testing results. In formal
testing phase, each participant can submit at most
three runs that adopt different models or parame-
ter settings. In total, we had received 34 runs.
Table 5 summarizes the participants’ devel-
oped approaches and the usage of linguistic re-
sources for this bake-off evaluation. Among 13
teams that participated the official testing, KUAS
and PKU did not submit their reports of devel-
oped models. We can observe that most of par-
ticipants adopt statistical approaches such as n-
gram model, language model, and machine-
learning model. In addition to the Bakeoff 2013
CSC Datasets, some linguistic resources are used
popularly for this bake-off evaluation such as
Sinica Corpus, Web as Corpus, Google Web 1T
N-gram, and Chinese Gigaword Corpus.
</bodyText>
<table confidence="0.933566761904762">
Participant (Ordered by abbreviations of names) #Runs
Beijing Institute of Technology (BIT) 2
Chinese Academy of Sciences (CAS) 2
China Agriculture University (CAU) 0
Industrial Technology Research Institute (ITRI) 0
National Kaohsiung University of Applied Sciences (KUAS) 3
Lingage Inc. (Lingage) 0
Luoyang Foreign Language University (LYFYU) 0
National Chiao Tung University &amp; National Taipei University of Technology (NCTU &amp; NTUT) 2
National Chiayi University (NCYU) 3
Nanjing University of Posts and Telecommunications (NJUPT) 3
National Tsing Hua University (NTHU) 3
National Taiwan Ocean University (NTOU) 2
Peking University (PKU) 3
South China Agriculture University (SCAU) 3
Chinese Knowledge and Information Processing Group, IIS, Academia Sinica (SinicaCKIP) 3
Speech, Language and Music Processing Lab, IIS, Academia Sinica (SinicaSLMP) 0
Shanghai Jiao Tong University (SJTU) 3
Soochow University (SUDA) 2
Zhejiang Ocean University (ZJOU) 0
Total 34
</table>
<tableCaption confidence="0.999947">
Table 4. Submission statistics for all participants
</tableCaption>
<page confidence="0.961975">
129
</page>
<table confidence="0.6871205">
Participant Approach Linguistic Resources
BIT • N-gram Model • Bakeoff 2013 CSC Datasets
</table>
<listItem confidence="0.997729870967742">
• Heuristic Rules • Chinese Penn Treebank
• Layer-Based Chinese Parsing • HIT-CIR TongyiciCilin (Extended)
• OpenCC
• Sinica Corpus
• Tsai’s list of Chinese Words
CAS • Decision-Making Model • Bakeoff 2013 CSC Datasets,
• Web as Corpus
NCTU • CRF-based Word Segmentation • Chinese Gigaword Corpus
&amp; NTUT • Part-of-Speech Tagger • Chinese Information Retrieval Benchmark
• Tri-gram Language Model • Sinica Corpus
• Taiwan Panorama Magazine
• Wikipedia (zh-tw version)
NCYU • Rule Induction • Bakeoff 2013 CSC Datasets
• E-HowNet
NJUPT • 2-Chars &amp; 3-Chars Model • Bakeoff 2013 CSC Datasets
• CRF Model • Web as Corpus
NTHU • Noisy Channel Model • Bakeoff 2013 CSC Datasets
• Google Web 1T N-gram
• Sinica Corpus
NTOU • N-gram Model • Bakeoff 2013 CSC Datasets
• Language Model • Sinica Corpus
• Rule-based Classifier
• SVM-based Classifier
SCAU • N-gram Model • Web as Corpus
• Language Model
SinicaCKIP • Error Template Rule • Bakeoff 2013 CSC Datasets
• Tri-gram Language Model • Google Web 1T N-gram
SJTU • Graph Model • Bakeoff 2013 CSC Datasets
• CRF Model • OpenCC
• Rule-Based System • Sinica Corpus
• Sogou Chinese Dictionary
</listItem>
<table confidence="0.356731">
SUDA • 5-gram Language Model • Chinese Gigaword Corpus
</table>
<tableCaption confidence="0.994543">
Table 5. A summary of participants’ developed systems
</tableCaption>
<bodyText confidence="0.999638235294118">
Table 6 shows the task testing results. In addi-
tion to accurate error detection and correction,
another key performance criteria is reducing the
rate of false positives, i.e., the mistaken identifi-
cation of errors where none exist. The research
teams, KUAS, NCTU&amp;NTUT, NCYU and SU-
DA, achieved very low false positive rates, i.e.,
less than 0.05.
Detection-level evaluations are designed to
identify spelling errors and highlight their loca-
tions in the input passages. Accuracy is a key
performance criterion, but accuracy can be af-
fected by the distribution of testing instances. A
neutral baseline can be easily achieved by always
reporting all testing errors are correct without
errors. According to the test data distribution, the
baseline system can achieve an accuracy level of
0.5. Some systems (i.e., CAS, KUAS, and
NCYU) achieved promising results exceeding
0.6. Each participating team was allowed submit
up to three iterative runs based on the same input,
and several teams sent different runs aimed at
optimizing either recall or precision rates. We
thus used the F1 score to reflect the tradeoff be-
tween precision and recall. In the testing results,
KUAS provided the best error detection results,
providing a high F1 score of 0.633.
For correction-level evaluations, the systems
need to locate errors in the passages and indicate
the corresponding correct characters. The correc-
tion accuracy provided by the KUAS submission
(0.7081) significantly outperformed the other
teams. However, in terms of correction precision,
the spelling checker developed by KUAS and
</bodyText>
<page confidence="0.992825">
130
</page>
<bodyText confidence="0.999584071428571">
NCYU outperforms the others at 0.8. Most sys-
tems were unable to effectively correct spelling
errors, with the better systems (CAS, and KUAS)
achieving a correction recall rate of slightly
above 0.3. The system developed by KUAS pro-
vided the highest F1 score of 0.6125 for spelling
error correction. It is difficult to correct all
spelling errors found in the input passages, since
some sentences contain multiple errors and only
correcting some of them are regarded as a wrong
case. In summary, none of the submitted systems
provided superior performance in all metrics,
though those submitted by KUAS, NCYU, and
CAS provided best overall performance.
</bodyText>
<table confidence="0.999906861111111">
Submission FPR Detection-Level Correction-Level
Acc. Pre. Rec. F1 Acc. Pre. Rec. F1
BIT-Run1 0.3352 0.4313 0.371 0.1977 0.258 0.4115 0.3206 0.1582 0.2119
BIT-Run2 0.3277 0.4482 0.4061 0.2241 0.2888 0.4303 0.365 0.1883 0.2484
CAS-Run1 0.1525 0.6149 0.7148 0.3823 0.4982 0.5829 0.676 0.3183 0.4328
CAS-Run2 0.1563 0.613 0.7098 0.3823 0.4969 0.581 0.6706 0.3183 0.4317
KUAS-Run1 0.1073 0.6008 0.7421 0.3089 0.4362 0.5951 0.7349 0.2976 0.4236
KUAS-Run2 0.0452 0.7194 0.9146 0.484 0.633 0.7081 0.9108 0.4614 0.6125
KUAS-Run3 0.0452 0.6525 0.8857 0.3503 0.502 0.6488 0.8835 0.3427 0.4939
NCTU&amp;NTUT-Run1 0.0377 0.5132 0.6296 0.064 0.1162 0.5094 0.6 0.0565 0.1033
NCTU&amp;NTUT-Run2 0.0998 0.5028 0.5138 0.1055 0.175 0.4925 0.4592 0.0847 0.1431
NCYU-Run1 0.1827 0.4831 0.4489 0.1488 0.2235 0.467 0.3899 0.1168 0.1797
NCYU-Run2 0.0414 0.6008 0.8543 0.2429 0.3783 0.5885 0.8406 0.2185 0.3468
NCYU-Run3 0.0414 0.5913 0.844 0.2241 0.3542 0.5791 0.8281 0.1996 0.3217
NJUPT-Run1 0.3898 0.403 0.3344 0.1959 0.247 0.3964 0.3191 0.1827 0.2323
NJUPT-Run2 0.6026 0.275 0.202 0.1525 0.1738 0.258 0.1645 0.1186 0.1379
NJUPT-Run3 0.5593 0.2853 0.1885 0.1299 0.1538 0.2665 0.1416 0.0923 0.1117
NTHU-Run1 0.0829 0.5235 0.6106 0.1299 0.2143 0.5113 0.56 0.1055 0.1775
NTHU-Run2 0.1507 0.5047 0.5152 0.1601 0.2443 0.484 0.4406 0.1186 0.1869
NTHU-Run3 0.3691 0.4228 0.3677 0.2147 0.2711 0.3823 0.2659 0.1337 0.1779
NTOU-Run1 0.258 0.4652 0.4219 0.1883 0.2604 0.4557 0.3965 0.1695 0.2375
NTOU-Run2 0.9925 0.1045 0.1688 0.2015 0.1837 0.0678 0.1143 0.1281 0.1208
PKU-Run1 0.9454 0.0367 0.0195 0.0188 0.0192 0.0348 0.0157 0.0151 0.0154
PKU-Run2 0.1168 0.4915 0.4609 0.0998 0.1641 0.4783 0.3861 0.0734 0.1234
PKU-Run3 0.4087 0.3616 0.2439 0.1318 0.1711 0.3418 0.1842 0.0923 0.123
SCAU-Run1 0.2034 0.4821 0.4518 0.1676 0.2445 0.4774 0.4375 0.1582 0.2324
SCAU-Run2 0.6441 0.275 0.2315 0.194 0.2111 0.2627 0.2083 0.1695 0.1869
SCAU-Run3 0.5009 0.3522 0.2907 0.2053 0.2406 0.3427 0.2712 0.1864 0.221
SinicaCKIP-Run1 0.1149 0.5169 0.5643 0.1488 0.2355 0.516 0.5612 0.1469 0.2328
SinicaCKIP-Run2 0.1827 0.564 0.6298 0.3107 0.4161 0.5395 0.589 0.2618 0.3625
SinicaCKIP-Run3 0.2655 0.5367 0.5607 0.339 0.4225 0.5104 0.5188 0.2863 0.3689
SJTU-Run1 0.5951 0.3117 0.2685 0.2185 0.2409 0.2938 0.2349 0.1827 0.2055
SJTU-Run2 0.2279 0.5471 0.5856 0.322 0.4156 0.5377 0.5709 0.3032 0.3961
SJTU-Run3 0.1921 0.5367 0.5802 0.2655 0.3643 0.5311 0.5696 0.2542 0.3516
SUDA-Run1 0.2524 0.4539 0.3881 0.1601 0.2267 0.4426 0.3527 0.1375 0.1978
SUDA-Run2 0.032 0.5292 0.7385 0.0904 0.1611 0.5235 0.7119 0.0791 0.1424
</table>
<tableCaption confidence="0.9694">
Table 6. Testing results of our Chinese spelling check task.
</tableCaption>
<sectionHeader confidence="0.997133" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999977523809524">
This paper provides an overview of SIGHAN
2014 Bake-off Chinese spelling check, including
task design, data preparation, evaluation metrics,
and performance evaluation results. The task also
encourages the proposal of unorthodox and inno-
vative approaches which could lead to a break-
through. Regardless of actual performance, all
submissions contribute to the common effort to
produce an effective Chinese spell checker, and
the individual reports in the Bake-off proceed-
ings provide useful insight into Chinese language
processing.
We hope the data sets collected for this Bake-
off can facilitate and expedite the development
of effective Chinese spelling checkers. All data
sets with gold standards and evaluation tool are
publicly available for research purposes at
http://ir.itc.ntnu.edu.tw/lre/clp14csc.htm.
Based on the results of this Bake-off, we plan
to build new language resources to improve ex-
isting and develop new techniques for computer-
</bodyText>
<page confidence="0.994498">
131
</page>
<bodyText confidence="0.9996245">
aided Chinese language learning. In addition,
new data sets obtained from CFL learners will be
investigated for the future enrichment of this re-
search topic.
</bodyText>
<sectionHeader confidence="0.996588" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9968775">
Associate research fellow Dr. Li-Ping Chang, the
Vice Director of Mandarin Training Center in
National Taiwan Normal University (NTNU), is
appreciated for supporting the NTNU learner
corpus used in our evaluation. We would like to
thank Kuei-Ching Lee for developing the evalua-
tion tool. Finally, we thank all the participants
for taking part in our task.
This research was partially support by Minis-
try of Science and Technology, Taiwan, under
the grant MOST 102-2221-E-002-103-MY3,
MOST 102-2221-E-155-029-MY3, and MOST
103-2221-E-003-013-MY3, and the “Aim for the
Top University Project” of National Taiwan
Normal University, sponsored by the Ministry of
Education, Taiwan.
</bodyText>
<sectionHeader confidence="0.98991" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999404853658536">
Tao-Hsing Chang, Hsueh-Chih Chen, Yuen-Hsien
Tseng, and Jian-Liang Zheng. 2013. Automatic de-
tection and correction for Chinese misspelled
words using phonological and orthographic simi-
larities. Proceedings of the 7th SIGHAN Workshop
on Chinese Language Processing (SIGHAN-13),
pages 97-101.
Kuan-Yu Chen, Hung-Shin Lee, Chung-Han Lee,
Hsin-Min Wang, and Hsin-Hsi Chen. 2013. A
study of language modeling for Chinese spelling
check. Proceedings of the 7th SIGHAN Workshop
on Chinese Language Processing (SIGHAN-13),
pages 79-83.
Zongye Jia, Peilu Wang, and Hai Zhao. 2013. Graph
model for Chinese spelling checking. Proceedings
of the 7th SIGHAN Workshop on Chinese Language
Processing (SIGHAN-13), pages 88-92.
Xiaodong Liu, Fei Cheng, Yanyan Luo, Kevin Duh,
and Yuji Matsumoto. 2013. A hybrid Chinese
spelling correction using language and statistical
machine translation with reranking. Proceedings of
the 7th SIGHAN Workshop on Chinese Language
Processing (SIGHAN-13), pages 54-58.
Chao-Lin Liu, Min-Hua Lai, Kan-Wen Tien, Yi-
Hsuan Chuang, Shih-Hung Wu, and Chia-Ying Lee.
2011. Visually and phonologically similar
characters in incorrect Chinese words: Anal-
yses, identification, and applications. ACM
Transaction on Asian Language Information Pro-
cessing, 10(2), Article 10, 39 pages.
Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee.
2013. Chinese spelling check evaluation at
SIGHAN bake-off 2013. Proceedings of the 7th
SIGHAN Workshop on Chinese Language Pro-
cessing (SIGHAN-13), pages 35-42.
Liang-Chih Yu, Chao-Hong Liu, and Chung-Hsien
Wu. 2013. Candidate scoring using web-based
measure for Chinese spelling error correction. Pro-
ceedings of the 7th SIGHAN Workshop on Chinese
Language Processing (SIGHAN-13), pages 108-
112.
</reference>
<page confidence="0.997626">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.244732">
<title confidence="0.994912">Overview of SIGHAN 2014 Bake-off for Chinese Spelling Check</title>
<author confidence="0.990187">Lung-Hao Yuen-Hsien Hsin-Hsi</author>
<affiliation confidence="0.694579">of Information Management, Yuen-Ze Center for Big Data and Digital Convergence, Yuen-Ze Technology Center, National Taiwan Normal of Computer Science and Information Engineering, National Taiwan</affiliation>
<email confidence="0.906743">lcyu@saturn.yzu.edu.tw,samtseng@ntnu.edu.tw,hhchen@ntu.edu.tw</email>
<abstract confidence="0.986219363636364">This paper introduces a Chinese Spelling Check campaign organized for the SIGHAN 2014 bake-off, including task description, data preparation, performance metrics, and evaluation results based on essays written by Chinese as a foreign language learners. The hope is that such evaluations can produce more advanced Chinese spelling check techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tao-Hsing Chang</author>
<author>Hsueh-Chih Chen</author>
<author>Yuen-Hsien Tseng</author>
<author>Jian-Liang Zheng</author>
</authors>
<title>Automatic detection and correction for Chinese misspelled words using phonological and orthographic similarities.</title>
<date>2013</date>
<booktitle>Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13),</booktitle>
<pages>97--101</pages>
<contexts>
<context position="1887" citStr="Chang et al. 2013" startWordPosition="254" endWordPosition="257">licly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage composed of several sentences, the checker should identify all possible spelling errors, highlight their locations and suggest possible correction</context>
</contexts>
<marker>Chang, Chen, Tseng, Zheng, 2013</marker>
<rawString>Tao-Hsing Chang, Hsueh-Chih Chen, Yuen-Hsien Tseng, and Jian-Liang Zheng. 2013. Automatic detection and correction for Chinese misspelled words using phonological and orthographic similarities. Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13), pages 97-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuan-Yu Chen</author>
<author>Hung-Shin Lee</author>
<author>Chung-Han Lee</author>
<author>Hsin-Min Wang</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>A study of language modeling for Chinese spelling check.</title>
<date>2013</date>
<booktitle>Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13),</booktitle>
<pages>79--83</pages>
<contexts>
<context position="1606" citStr="Chen et al., 2013" startWordPosition="212" endWordPosition="215">lly similar, but semantically distinct (Liu et al., 2011). The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check</context>
</contexts>
<marker>Chen, Lee, Lee, Wang, Chen, 2013</marker>
<rawString>Kuan-Yu Chen, Hung-Shin Lee, Chung-Han Lee, Hsin-Min Wang, and Hsin-Hsi Chen. 2013. A study of language modeling for Chinese spelling check. Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13), pages 79-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zongye Jia</author>
<author>Peilu Wang</author>
<author>Hai Zhao</author>
</authors>
<title>Graph model for Chinese spelling checking.</title>
<date>2013</date>
<booktitle>Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13),</booktitle>
<pages>88--92</pages>
<contexts>
<context position="2144" citStr="Jia et al. 2013" startWordPosition="294" endWordPosition="297">resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage composed of several sentences, the checker should identify all possible spelling errors, highlight their locations and suggest possible corrections. While previous tasks were based on essays written by native Chinese speakers, the current task is based on essays written by learners of Chinese as a Foreign Language (CFL), which should provide a greater challenge The rest of this article is organized a</context>
</contexts>
<marker>Jia, Wang, Zhao, 2013</marker>
<rawString>Zongye Jia, Peilu Wang, and Hai Zhao. 2013. Graph model for Chinese spelling checking. Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13), pages 88-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Liu</author>
<author>Fei Cheng</author>
<author>Yanyan Luo</author>
<author>Kevin Duh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A hybrid Chinese spelling correction using language and statistical machine translation with reranking.</title>
<date>2013</date>
<booktitle>Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13),</booktitle>
<pages>54--58</pages>
<contexts>
<context position="1746" citStr="Liu et al. 2013" startWordPosition="234" endWordPosition="237">ta sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage compo</context>
</contexts>
<marker>Liu, Cheng, Luo, Duh, Matsumoto, 2013</marker>
<rawString>Xiaodong Liu, Fei Cheng, Yanyan Luo, Kevin Duh, and Yuji Matsumoto. 2013. A hybrid Chinese spelling correction using language and statistical machine translation with reranking. Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13), pages 54-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao-Lin Liu</author>
<author>Min-Hua Lai</author>
<author>Kan-Wen Tien</author>
<author>YiHsuan Chuang</author>
<author>Shih-Hung Wu</author>
<author>Chia-Ying Lee</author>
</authors>
<title>Visually and phonologically similar characters in incorrect Chinese words: Analyses, identification, and applications.</title>
<date>2011</date>
<journal>ACM Transaction on Asian Language Information Processing,</journal>
<volume>10</volume>
<issue>2</issue>
<pages>pages.</pages>
<contexts>
<context position="1045" citStr="Liu et al., 2011" startWordPosition="128" endWordPosition="131">w, lhlee@ntnu.edu.tw, samtseng@ntnu.edu.tw, hhchen@ntu.edu.tw Abstract This paper introduces a Chinese Spelling Check campaign organized for the SIGHAN 2014 bake-off, including task description, data preparation, performance metrics, and evaluation results based on essays written by Chinese as a foreign language learners. The hope is that such evaluations can produce more advanced Chinese spelling check techniques. 1 Introduction Chinese spelling errors frequently arise from confusion between multiple Chinese characters which are phonologically and visually similar, but semantically distinct (Liu et al., 2011). The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combin</context>
</contexts>
<marker>Liu, Lai, Tien, Chuang, Wu, Lee, 2011</marker>
<rawString>Chao-Lin Liu, Min-Hua Lai, Kan-Wen Tien, YiHsuan Chuang, Shih-Hung Wu, and Chia-Ying Lee. 2011. Visually and phonologically similar characters in incorrect Chinese words: Analyses, identification, and applications. ACM Transaction on Asian Language Information Processing, 10(2), Article 10, 39 pages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shih-Hung Wu</author>
<author>Chao-Lin Liu</author>
<author>Lung-Hao Lee</author>
</authors>
<title>Chinese spelling check evaluation at SIGHAN bake-off</title>
<date>2013</date>
<booktitle>Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13),</booktitle>
<pages>35--42</pages>
<contexts>
<context position="1238" citStr="Wu et al. 2013" startWordPosition="160" endWordPosition="163">data preparation, performance metrics, and evaluation results based on essays written by Chinese as a foreign language learners. The hope is that such evaluations can produce more advanced Chinese spelling check techniques. 1 Introduction Chinese spelling errors frequently arise from confusion between multiple Chinese characters which are phonologically and visually similar, but semantically distinct (Liu et al., 2011). The SIGHAN 2013 Chinese Spelling Check Bakeoff was the first campaign to provide data sets as benchmarks for the objective performance evaluation of Chinese spelling checkers (Wu et al. 2013). The collected data set is publicly available at http://ir.itc.ntnu.edu.tw/lre/sighan7csc.htm. The competition resulted in the integration of effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to </context>
</contexts>
<marker>Wu, Liu, Lee, 2013</marker>
<rawString>Shih-Hung Wu, Chao-Lin Liu, and Lung-Hao Lee. 2013. Chinese spelling check evaluation at SIGHAN bake-off 2013. Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13), pages 35-42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang-Chih Yu</author>
<author>Chao-Hong Liu</author>
<author>Chung-Hsien Wu</author>
</authors>
<title>Candidate scoring using web-based measure for Chinese spelling error correction.</title>
<date>2013</date>
<booktitle>Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13),</booktitle>
<pages>108--112</pages>
<contexts>
<context position="1996" citStr="Yu et al., 2013" startWordPosition="270" endWordPosition="273"> effective NLP techniques in the development of Chinese spelling checkers. Language modeling was used to glean extra semantic clues and collect web resources together to identify and correct spelling errors (Chen et al., 2013). A hybrid model was proposed to combine language models and statistical machine translation for spelling error correction (Liu et al. 2013). A linear regression model was trained using phonological and orthographic similarities to correct misspelled characters (Chang et al. 2013). Web-based measures were adopted to score candidates for Chinese spelling error correction (Yu et al., 2013). A graph model was used to represent the sentence, using the single source shortest path algorithm for correcting spelling errors (Jia et al. 2013) SIGHAN 2014 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage composed of several sentences, the checker should identify all possible spelling errors, highlight their locations and suggest possible corrections. While previous tasks were based on essays written by native Chinese speakers, the current task is based on</context>
</contexts>
<marker>Yu, Liu, Wu, 2013</marker>
<rawString>Liang-Chih Yu, Chao-Hong Liu, and Chung-Hsien Wu. 2013. Candidate scoring using web-based measure for Chinese spelling error correction. Proceedings of the 7th SIGHAN Workshop on Chinese Language Processing (SIGHAN-13), pages 108-112.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
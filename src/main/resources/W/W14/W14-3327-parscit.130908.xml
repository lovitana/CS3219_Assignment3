<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003746">
<title confidence="0.998624">
Postech’s System Description for Medical Text Translation Task
</title>
<author confidence="0.998884">
Jianri Li Se-Jong Kim Hwidong Ia Jong-Hyeok Lee
</author>
<affiliation confidence="0.9976665">
Department of Computer Science and Engineering
Pohang University of Science and Technology, Pohang, Republic of Korea
</affiliation>
<email confidence="0.979754">
{skywalker, sejong, leona, jhlee}@postech.ac.kr
</email>
<sectionHeader confidence="0.994615" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9731281">
This short paper presents a system description
for intrinsic evaluation of the WMT 14’s med-
ical text translation task. Our systems consist
of phrase-based statistical machine translation
system and query translation system between
German-English language pairs. Our work fo-
cuses on the query translation task and we
achieved the highest BLEU score among the
all submitted systems for the English-German
intrinsic query translation evaluation.
</bodyText>
<sectionHeader confidence="0.994288" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.999852171428572">
The goal of WMT14’s medical text translation
task is investigation of capability of machine
translation (MT) technologies when it is applied
to translating texts and query terms in medical
domain. In our work, we focus on its application
on cross-lingual information retrieval (CLIR)
and evaluation of query translation task.
CLIR techniques aim to increase the accessi-
bility of web documents written by foreign lan-
guage. One of the key techniques of cross-lingual
IR is query translation, which aims to translate
the input query into relevant terms in target lan-
guage.
One way to translate queries is dictionary-
based query translation. However, an input query
usually consists of multiple terms, which cause
low coverage of bilingual dictionary. Alternative
way is translating queries using statistical ma-
chine translation (SMT) system. However, trans-
lation model could contain some noise that is
meaningless translation. The goal of our method
is to overcome the shortcomings of these ap-
proaches by a heuristic hybrid approach.
As a baseline, we use phrase-based statistical
machine translation (PBSMT) (Koehn, Och, &amp;
Marcu, 2003) techniques to handle queries that
consist of multiple terms. To identify multiple
terms in a query, we analyze three cases of the
formation of queries and generate query transla-
tion candidates using term-to-term dictionaries
and PBSMT system, and then score these candi-
dates using co-occurrence word frequency meas-
ure to select the best candidate.
We have done experiment on two language
pairs
</bodyText>
<listItem confidence="0.999528">
• English-German
• German-English
</listItem>
<bodyText confidence="0.9998744">
The rest of parts in this paper are organized as
following: section 2 describes the techniques and
system settings used in our experiment, section 3
presents used corpus and experiment result, and
section 4 shows a brief conclusion of our work.
</bodyText>
<sectionHeader confidence="0.990092" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.95224">
2.1 Phrase-based machine translation sys-
tem
</subsectionHeader>
<bodyText confidence="0.999964833333333">
The phrase-based statistical machine translation
system is implemented using MOSE’S toolkits
(Koehn et al., 2007). Bidirectional word align-
ments were built by MGIZA 1, a multi-thread
version of GIZA++ (Och &amp; Ney, 2003), run on a
24 threads machine. The alignment symmetriza-
tion method is grow-diag-final-and (Koehn et al.,
2003), and lexicalized-reordering method is msd-
bidirectional-fe (Koehn et al., 2007).
For each monolingual corpus, we used a five-
gram language model, which was built by
IRSTLM toolkit2 (Federico, Bertoldi, &amp; Cettolo,
2008) with improved Kneser Ney smoothing
(Chen &amp; Goodman, 1996; Kneser &amp; Ney, 1995).
The language model was integrated as a log-
linear feature to decoder.
All the sentences in the training, development
and test corpus were tokenized by inserting spac-
es between words and punctuations, and then
converted to most probable cases by truecaseing.
Both tokenization and truecasing were done by
embedded tools in the MOSE’S toolkits. Finally,
all the sentences in the train corpus were cleaned
with maximum length 80.
</bodyText>
<footnote confidence="0.9994065">
1 http://www.kyloo.net/software
2 http://sourceforge.net/projects/irstlm
</footnote>
<page confidence="0.961544">
229
</page>
<note confidence="0.3165385">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 229–232,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.989641">
Figure 1. Flow from queries to query translation candidates for each case.
</figureCaption>
<subsectionHeader confidence="0.997797">
2.2 Query translation system
</subsectionHeader>
<bodyText confidence="0.999960641025641">
In general, an input query is not a full sentence.
Instead, most of queries contain one or more
phrases that consist of several keywords. Fur-
thermore, in the medical domain, many key-
words are unfamiliar terminologies for general
users. Therefore, term-to-term translation dic-
tionaries in medical domain could be useful re-
sources to translate the queries. In our experi-
ment, we used the parallel terms from Unified
Medical Language System (UMLS) and titles of
Wikipedia in medical domain, as the term-to-
term translation dictionary.
First of all, if a given query is a combination
of two or more phrases that concatenated by
terms like comma, coordinate conjunction, then
the given query is divided into several single
phrases, and each of them is translated by our
SMT system as a new single query. If the new
query satisfies one of cases shown in Figure 1,
then its query translation candidates are selected
according to the corresponding case, and select
the best one of them using proposed measures.
Otherwise, if the new query does not satisfy any
case, the top 1 result by our PBSMT system is
selected as the best query translation candidate.
Our method combines the translation results of
single queries by following rules: 1) if the origi-
nal query consists of multiple phrases concate-
nated by functional words like coordinate con-
junctions, then the translation results are com-
bined by translated functional words, 2) if the
original query is concatenated by punctuation,
then the results are combined by the original
punctuation. Finally, the final result is selected
by comparing the result from QT system and
PBSMT system using the co-occurrence word
frequency measure (see Section 2.2.4). The fol-
lowing three subsections describe how we select
translation candidate case by case.
</bodyText>
<subsectionHeader confidence="0.875195">
2.2.1 Case 1: Full matching
</subsectionHeader>
<bodyText confidence="0.999936625">
If a single query exactly matches one instance in
the dictionary, query translation candidates are
the target-side entries in the translation diction-
ary (Case 1 in Figure 1). If a query translation
candidate qt is a sequence of words (w1 to wn), it
is ranked by the co-occurrence word frequency
measure (CF) using the provided articles of Wik-
ipedia in the medical domain:
</bodyText>
<equation confidence="0.885027">
, (1)
</equation>
<bodyText confidence="0.998709">
where freq(w1) is the frequency of a unigram w1
in the articles; freq(w;, w;-1) is the frequency of a
</bodyText>
<page confidence="0.97109">
230
</page>
<bodyText confidence="0.999207333333333">
bigram “wi wi-1” in the articles; and Nuni and Nbi is
the sum of frequency of all unigram and bigram,
respectively.
</bodyText>
<subsectionHeader confidence="0.62687">
2.2.2 Case 2: Full inclusion
</subsectionHeader>
<bodyText confidence="0.9997465">
If a source-side entry of the term-to-term transla-
tion dictionary exactly includes a query, its query
translation candidate is its SMT result whose all
words appear in the target-side entry of the trans-
lation dictionary (Case 2 in Figure 1). Among the
top 10 results by our PBSMT system, we select
the results satisfying this case, and rank them
using CF and our PBSMT result score
(ScoreSMT):
where A is the weight by the provided develop-
ment set; and QT is the set of query translation
candidates for a query.
</bodyText>
<subsectionHeader confidence="0.899333">
2.2.3 Case 1: Full matching
</subsectionHeader>
<bodyText confidence="0.999978333333333">
If the left phrase tleft or right phrase tright of a que-
ry exactly matches one instance in the dictionary,
its query translation candidate is its SMT result
that includes all words in the target-side entry of
the translation dictionary (Case 3 in Figure 1).
To rank our SMT results satisfying this case, if
the total number of words in tleft and tright is same
or larger than that in a query, ScoreQT is used,
and the other case uses the weighted ScoreQT
(WScoreQT):
where N(tleft) is the number of words in tleft; and q
is a given query.
</bodyText>
<subsectionHeader confidence="0.808709">
2.2.4 Select final result
</subsectionHeader>
<bodyText confidence="0.9998835">
If a query satisfies any case above, and the can-
didate with highest score is selected, then we
compare the candidate with translation of origi-
nal query directly obtained from PBSMT system
using equation (1). The final result would be the
result with higher score between them.
</bodyText>
<sectionHeader confidence="0.998831" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.987192">
3.1 Corpus
</subsectionHeader>
<bodyText confidence="0.98528675">
We only use constrained data provided by WMT
2014 medical translation task.
To train PBSMT system, we use parallel cor-
pora
</bodyText>
<listItem confidence="0.9998606">
• EMEA
• MuchMore
• Wikipedia-titles
• Patent-abstract, claim, title
• UMLS
</listItem>
<bodyText confidence="0.991832">
We simply mixed up all available parallel cor-
pora to train a unique translation model.
And for English-German language pair we use
monolingual corpora
</bodyText>
<listItem confidence="0.922877769230769">
• Wikipedia-articles
• Patent-descriptions
• UMLS descriptions
And for German-English language pair we use
monolingual corpora
• Wikipedia-articles
• Patent-descriptions
• UMLS descriptions
• AACT
• GENIA
• GREC
• FMA
• PIL
</listItem>
<bodyText confidence="0.9989719">
We also use target side of parallel corpora as
additional monolingual resource to train lan-
guage model. We separately train a 5-gram lan-
guage model for each monolingual corpus and
integrate them as features to log-linear model in
the PBSMT system.
For the query translation (QT) system, we use
parallel corpus Wikipedia-titles and UMLS dic-
tionary, and use monolingual corpus Wikipedia-
articles.
</bodyText>
<subsectionHeader confidence="0.999793">
3.2 Experiment Setting
</subsectionHeader>
<bodyText confidence="0.999585285714286">
For the tuning of PBSMT system, we use devel-
opment set provided by WMT 14 medical task
(khresmoi-summary-dev). And we use query
translation development set (khresmoi-query-
dev) for the tuning of QT system.
We test our systems on two test set provided
by WMT 14 medical task.
</bodyText>
<listItem confidence="0.999958">
• khresmoi-summary-test (for PBSMT)
• khresmoi-query-test (for QT)
</listItem>
<page confidence="0.996275">
231
</page>
<bodyText confidence="0.999952625">
For comparison with result from QT system,
we translate the test set of query translation task
(khresmoi-query-test) using PBSMT system
without any post-processing.
In our experiment, the performance of transla-
tion system is measured by BLEU (%) and trans-
lation error rate - TER (%). All these results are
evaluated from the evaluation website3.
</bodyText>
<subsectionHeader confidence="0.997009">
3.3 Experiment Result
</subsectionHeader>
<bodyText confidence="0.9999557">
Table 1 shows the results for the task of transla-
tion of sentences from summaries of medical
articles.
Table 2 shows the results for the task of trans-
lation of queries entered by users of medical in-
formation search engines. The performance of
QT system is relatively higher than PBSMT sys-
tem. Especially, the BLEU score of QT system
on English-German language pair is the highest
score among the all submitted systems.
</bodyText>
<table confidence="0.998818333333333">
Language Pair BLEU TER
English-German 15.8 0.746
German-English 26.9 0.618
</table>
<tableCaption confidence="0.982237">
Table 1: BLEU scores of result from PBSMT system
for summary translation task.
</tableCaption>
<table confidence="0.999947">
Language Pair BLEU TER
PBSMT English-German 15.1 0.748
German-English 22.1 0.638
QT English-German 15.3 0.746
German-English 24.5 0.586
</table>
<tableCaption confidence="0.855889">
Table 2: BLEU scores of result for query translation
task.
</tableCaption>
<sectionHeader confidence="0.998464" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9998013">
We describe the PBSMT system and QT system
that are developed for summary translation and
query translation of WMT 14 medical translation
task. We focus on intrinsic query translation
evaluation and propose a hybrid approach by
combining dictionary-based approach and SMT
based approach using heuristics. The result of
query translation experiment shows that our
method obtained higher translation accuracy than
the baseline (PBSMT) system.
</bodyText>
<sectionHeader confidence="0.998607" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.716809">
This work was supported in part by the National
Korea Science and Engineering Foundation
</bodyText>
<footnote confidence="0.575786">
3 http://matrix.statmt.org
</footnote>
<reference confidence="0.68883925">
(KOSEF) (NRF-2010-0012662), in part by the
Brain Korea 21+ Project, and in part by the Ko-
rea Ministry of Knowledge Economy (MKE)
under Grant No.10041807.
</reference>
<sectionHeader confidence="0.845933" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998683">
Chen, S. F., &amp; Goodman, J. (1996). An empirical
study of smoothing techniques for language
modeling. Paper presented at the Proceedings of
the 34th annual meeting on Association for
Computational Linguistics.
Federico, M., Bertoldi, N., &amp; Cettolo, M. (2008).
IRSTLM: an open source toolkit for handling large
scale language models. Paper presented at the
Interspeech.
Kneser, R., &amp; Ney, H. (1995). Improved backing-off
for m-gram language modeling. Paper presented at
the Acoustics, Speech, and Signal Processing, 1995.
ICASSP-95., 1995 International Conference on.
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C.,
Federico, M., Bertoldi, N., . . . Herbst, E. (2007).
Moses: open source toolkit for statistical machine
translation. Paper presented at the Proceedings of
the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, Prague, Czech
Republic.
Koehn, P., Och, F. J., &amp; Marcu, D. (2003). Statistical
phrase-based translation. Paper presented at the
Proceedings of the 2003 Conference of the North
American Chapter of the Association for
Computational Linguistics on Human Language
Technology-Volume 1.
Och, F. J., &amp; Ney, H. (2003). A systematic
comparison of various statistical alignment models.
Comput. Linguist., 29(1), 19-51. doi:
10.1162/089120103321337421
</reference>
<page confidence="0.995151">
232
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001129">
<title confidence="0.997637">Postech’s System Description for Medical Text Translation Task</title>
<author confidence="0.999852">Jianri Li Se-Jong Kim Hwidong Ia Jong-Hyeok Lee</author>
<affiliation confidence="0.8690935">Department of Computer Science and Engineering Pohang University of Science and Technology, Pohang, Republic of</affiliation>
<email confidence="0.955086">skywalker@postech.ac.kr</email>
<email confidence="0.955086">sejong@postech.ac.kr</email>
<email confidence="0.955086">leona@postech.ac.kr</email>
<email confidence="0.955086">jhlee@postech.ac.kr</email>
<abstract confidence="0.969145060606061">This short paper presents a system description for intrinsic evaluation of the WMT 14’s medical text translation task. Our systems consist of phrase-based statistical machine translation system and query translation system between German-English language pairs. Our work focuses on the query translation task and we achieved the highest BLEU score among the all submitted systems for the English-German intrinsic query translation evaluation. 1 Overview The goal of WMT14’s medical text translation task is investigation of capability of machine translation (MT) technologies when it is applied to translating texts and query terms in medical domain. In our work, we focus on its application on cross-lingual information retrieval (CLIR) and evaluation of query translation task. CLIR techniques aim to increase the accessibility of web documents written by foreign language. One of the key techniques of cross-lingual IR is query translation, which aims to translate the input query into relevant terms in target language. One way to translate queries is dictionarybased query translation. However, an input query usually consists of multiple terms, which cause low coverage of bilingual dictionary. Alternative way is translating queries using statistical machine translation (SMT) system. However, translation model could contain some noise that is meaningless translation. The goal of our method is to overcome the shortcomings of these approaches by a heuristic hybrid approach. As a baseline, we use phrase-based statistical machine translation (PBSMT) (Koehn, Och, &amp; Marcu, 2003) techniques to handle queries that consist of multiple terms. To identify multiple terms in a query, we analyze three cases of the formation of queries and generate query translation candidates using term-to-term dictionaries and PBSMT system, and then score these candiusing word frequency measselect the best candidate. We have done experiment on two language pairs • English-German • German-English The rest of parts in this paper are organized as following: section 2 describes the techniques and system settings used in our experiment, section 3 presents used corpus and experiment result, and section 4 shows a brief conclusion of our work. 2 Method 2.1 Phrase-based machine translation system The phrase-based statistical machine translation system is implemented using MOSE’S toolkits (Koehn et al., 2007). Bidirectional word alignwere built by MGIZA a multi-thread version of GIZA++ (Och &amp; Ney, 2003), run on a 24 threads machine. The alignment symmetrizamethod is et al., and lexicalized-reordering method is msdet al., 2007). For each monolingual corpus, we used a fivegram language model, which was built by (Federico, Bertoldi, &amp; Cettolo, 2008) with improved Kneser Ney smoothing (Chen &amp; Goodman, 1996; Kneser &amp; Ney, 1995). The language model was integrated as a loglinear feature to decoder. All the sentences in the training, development and test corpus were tokenized by inserting spaces between words and punctuations, and then converted to most probable cases by truecaseing. Both tokenization and truecasing were done by embedded tools in the MOSE’S toolkits. Finally, all the sentences in the train corpus were cleaned with maximum length 80. 1http://www.kyloo.net/software 2http://sourceforge.net/projects/irstlm 229 of the Ninth Workshop on Statistical Machine pages Maryland USA, June 26–27, 2014. Association for Computational Linguistics Figure 1. Flow from queries to query translation candidates for each case. 2.2 Query translation system In general, an input query is not a full sentence. Instead, most of queries contain one or more phrases that consist of several keywords. Furthermore, in the medical domain, many keywords are unfamiliar terminologies for general users. Therefore, term-to-term translation dictionaries in medical domain could be useful resources to translate the queries. In our experiment, we used the parallel terms from Unified Medical Language System (UMLS) and titles of Wikipedia in medical domain, as the term-toterm translation dictionary. First of all, if a given query is a combination of two or more phrases that concatenated by terms like comma, coordinate conjunction, then the given query is divided into several single phrases, and each of them is translated by our SMT system as a new single query. If the new query satisfies one of cases shown in Figure 1, then its query translation candidates are selected according to the corresponding case, and select the best one of them using proposed measures. Otherwise, if the new query does not satisfy any case, the top 1 result by our PBSMT system is selected as the best query translation candidate. Our method combines the translation results of single queries by following rules: 1) if the original query consists of multiple phrases concatenated by functional words like coordinate conjunctions, then the translation results are combined by translated functional words, 2) if the original query is concatenated by punctuation, then the results are combined by the original punctuation. Finally, the final result is selected by comparing the result from QT system and PBSMT system using the co-occurrence word frequency measure (see Section 2.2.4). The following three subsections describe how we select translation candidate case by case. 2.2.1 Case 1: Full matching If a single query exactly matches one instance in the dictionary, query translation candidates are the target-side entries in the translation dictionary (Case 1 in Figure 1). If a query translation a sequence of words it is ranked by the co-occurrence word frequency using the provided articles of Wikipedia in the medical domain: , (1) is the frequency of a unigram the articles; is the frequency of a 230 in the articles; and the sum of frequency of all unigram and bigram, respectively. 2.2.2 Case 2: Full inclusion If a source-side entry of the term-to-term translation dictionary exactly includes a query, its query translation candidate is its SMT result whose all words appear in the target-side entry of the translation dictionary (Case 2 in Figure 1). Among the top 10 results by our PBSMT system, we select the results satisfying this case, and rank them our PBSMT result score the weight by the provided developset; and the set of query translation candidates for a query. 2.2.3 Case 1: Full matching the left phrase right phrase a query exactly matches one instance in the dictionary, its query translation candidate is its SMT result that includes all words in the target-side entry of the translation dictionary (Case 3 in Figure 1). To rank our SMT results satisfying this case, if total number of words in same larger than that in a query, used, the other case uses the weighted is the number of words in and is a given query. 2.2.4 Select final result If a query satisfies any case above, and the candidate with highest score is selected, then we compare the candidate with translation of original query directly obtained from PBSMT system using equation (1). The final result would be the result with higher score between them. 3 Experiment 3.1 Corpus We only use constrained data provided by WMT 2014 medical translation task. To train PBSMT system, we use parallel corpora • EMEA • MuchMore • Wikipedia-titles • Patent-abstract, claim, title • UMLS We simply mixed up all available parallel corpora to train a unique translation model. And for English-German language pair we use monolingual corpora • Wikipedia-articles • Patent-descriptions • UMLS descriptions And for German-English language pair we use monolingual corpora • Wikipedia-articles • Patent-descriptions • UMLS descriptions • AACT • GENIA • GREC • FMA • PIL We also use target side of parallel corpora as additional monolingual resource to train language model. We separately train a 5-gram language model for each monolingual corpus and integrate them as features to log-linear model in the PBSMT system. For the query translation (QT) system, we use corpus dicand use monolingual corpus Wikipedia- 3.2 Experiment Setting For the tuning of PBSMT system, we use development set provided by WMT 14 medical task And we use query development set for the tuning of QT system. We test our systems on two test set provided by WMT 14 medical task. • khresmoi-summary-test (for PBSMT) • khresmoi-query-test (for QT) 231 For comparison with result from QT system, we translate the test set of query translation task using PBSMT system without any post-processing. In our experiment, the performance of translation system is measured by BLEU (%) and translation error rate - TER (%). All these results are from the evaluation 3.3 Experiment Result Table 1 shows the results for the task of translation of sentences from summaries of medical articles. Table 2 shows the results for the task of translation of queries entered by users of medical information search engines. The performance of QT system is relatively higher than PBSMT system. Especially, the BLEU score of QT system on English-German language pair is the highest score among the all submitted systems. Language Pair BLEU TER English-German 15.8 0.746 German-English 26.9 0.618 Table 1: BLEU scores of result from PBSMT system for summary translation task. Language Pair BLEU TER PBSMT English-German 15.1 0.748 German-English 22.1 0.638 QT English-German 15.3 0.746 German-English 24.5 0.586 Table 2: BLEU scores of result for query translation task. 4 Conclusion We describe the PBSMT system and QT system that are developed for summary translation and query translation of WMT 14 medical translation task. We focus on intrinsic query translation evaluation and propose a hybrid approach by combining dictionary-based approach and SMT based approach using heuristics. The result of query translation experiment shows that our method obtained higher translation accuracy than the baseline (PBSMT) system.</abstract>
<note confidence="0.797864575">Acknowledgments This work was supported in part by the National Korea Science and Engineering Foundation 3http://matrix.statmt.org (KOSEF) (NRF-2010-0012662), in part by the Brain Korea 21+ Project, and in part by the Korea Ministry of Knowledge Economy (MKE) under Grant No.10041807. References S. F., &amp; Goodman, J. (1996). empirical study of smoothing techniques for language presented at the Proceedings of the 34th annual meeting on Association for Computational Linguistics. Federico, M., Bertoldi, N., &amp; Cettolo, M. (2008). IRSTLM: an open source toolkit for handling large language models. presented at the Interspeech. R., &amp; Ney, H. (1995). backing-off m-gram language modeling. presented at the Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on. Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., . . . Herbst, E. (2007). Moses: open source toolkit for statistical machine Paper presented at the Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, Prague, Czech Republic. P., Och, F. J., &amp; Marcu, D. (2003). translation. presented at the Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1. Och, F. J., &amp; Ney, H. (2003). A systematic comparison of various statistical alignment models. Linguist., 19-51. doi: 10.1162/089120103321337421 232</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>in part by the Brain Korea 21+ Project, and in part by the Korea Ministry of Knowledge Economy (MKE) under Grant No.10041807.</title>
<marker></marker>
<rawString>(KOSEF) (NRF-2010-0012662), in part by the Brain Korea 21+ Project, and in part by the Korea Ministry of Knowledge Economy (MKE) under Grant No.10041807.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>Paper presented at the Proceedings of the 34th annual meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3201" citStr="Chen &amp; Goodman, 1996" startWordPosition="479" endWordPosition="482">ranslation system The phrase-based statistical machine translation system is implemented using MOSE’S toolkits (Koehn et al., 2007). Bidirectional word alignments were built by MGIZA 1, a multi-thread version of GIZA++ (Och &amp; Ney, 2003), run on a 24 threads machine. The alignment symmetrization method is grow-diag-final-and (Koehn et al., 2003), and lexicalized-reordering method is msdbidirectional-fe (Koehn et al., 2007). For each monolingual corpus, we used a fivegram language model, which was built by IRSTLM toolkit2 (Federico, Bertoldi, &amp; Cettolo, 2008) with improved Kneser Ney smoothing (Chen &amp; Goodman, 1996; Kneser &amp; Ney, 1995). The language model was integrated as a loglinear feature to decoder. All the sentences in the training, development and test corpus were tokenized by inserting spaces between words and punctuations, and then converted to most probable cases by truecaseing. Both tokenization and truecasing were done by embedded tools in the MOSE’S toolkits. Finally, all the sentences in the train corpus were cleaned with maximum length 80. 1 http://www.kyloo.net/software 2 http://sourceforge.net/projects/irstlm 229 Proceedings of the Ninth Workshop on Statistical Machine Translation, page</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Chen, S. F., &amp; Goodman, J. (1996). An empirical study of smoothing techniques for language modeling. Paper presented at the Proceedings of the 34th annual meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>M Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models. Paper presented at the Interspeech.</title>
<date>2008</date>
<contexts>
<context position="3143" citStr="Federico, Bertoldi, &amp; Cettolo, 2008" startWordPosition="469" endWordPosition="473">shows a brief conclusion of our work. 2 Method 2.1 Phrase-based machine translation system The phrase-based statistical machine translation system is implemented using MOSE’S toolkits (Koehn et al., 2007). Bidirectional word alignments were built by MGIZA 1, a multi-thread version of GIZA++ (Och &amp; Ney, 2003), run on a 24 threads machine. The alignment symmetrization method is grow-diag-final-and (Koehn et al., 2003), and lexicalized-reordering method is msdbidirectional-fe (Koehn et al., 2007). For each monolingual corpus, we used a fivegram language model, which was built by IRSTLM toolkit2 (Federico, Bertoldi, &amp; Cettolo, 2008) with improved Kneser Ney smoothing (Chen &amp; Goodman, 1996; Kneser &amp; Ney, 1995). The language model was integrated as a loglinear feature to decoder. All the sentences in the training, development and test corpus were tokenized by inserting spaces between words and punctuations, and then converted to most probable cases by truecaseing. Both tokenization and truecasing were done by embedded tools in the MOSE’S toolkits. Finally, all the sentences in the train corpus were cleaned with maximum length 80. 1 http://www.kyloo.net/software 2 http://sourceforge.net/projects/irstlm 229 Proceedings of t</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Federico, M., Bertoldi, N., &amp; Cettolo, M. (2008). IRSTLM: an open source toolkit for handling large scale language models. Paper presented at the Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>H Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling. Paper presented at the Acoustics, Speech, and Signal Processing,</title>
<date>1995</date>
<booktitle>ICASSP-95., 1995 International Conference on.</booktitle>
<contexts>
<context position="3222" citStr="Kneser &amp; Ney, 1995" startWordPosition="483" endWordPosition="486">phrase-based statistical machine translation system is implemented using MOSE’S toolkits (Koehn et al., 2007). Bidirectional word alignments were built by MGIZA 1, a multi-thread version of GIZA++ (Och &amp; Ney, 2003), run on a 24 threads machine. The alignment symmetrization method is grow-diag-final-and (Koehn et al., 2003), and lexicalized-reordering method is msdbidirectional-fe (Koehn et al., 2007). For each monolingual corpus, we used a fivegram language model, which was built by IRSTLM toolkit2 (Federico, Bertoldi, &amp; Cettolo, 2008) with improved Kneser Ney smoothing (Chen &amp; Goodman, 1996; Kneser &amp; Ney, 1995). The language model was integrated as a loglinear feature to decoder. All the sentences in the training, development and test corpus were tokenized by inserting spaces between words and punctuations, and then converted to most probable cases by truecaseing. Both tokenization and truecasing were done by embedded tools in the MOSE’S toolkits. Finally, all the sentences in the train corpus were cleaned with maximum length 80. 1 http://www.kyloo.net/software 2 http://sourceforge.net/projects/irstlm 229 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 229–232, Baltimore,</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Kneser, R., &amp; Ney, H. (1995). Improved backing-off for m-gram language modeling. Paper presented at the Acoustics, Speech, and Signal Processing, 1995. ICASSP-95., 1995 International Conference on.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>Paper presented at the Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2712" citStr="Koehn et al., 2007" startWordPosition="403" endWordPosition="406">ries and PBSMT system, and then score these candidates using co-occurrence word frequency measure to select the best candidate. We have done experiment on two language pairs • English-German • German-English The rest of parts in this paper are organized as following: section 2 describes the techniques and system settings used in our experiment, section 3 presents used corpus and experiment result, and section 4 shows a brief conclusion of our work. 2 Method 2.1 Phrase-based machine translation system The phrase-based statistical machine translation system is implemented using MOSE’S toolkits (Koehn et al., 2007). Bidirectional word alignments were built by MGIZA 1, a multi-thread version of GIZA++ (Och &amp; Ney, 2003), run on a 24 threads machine. The alignment symmetrization method is grow-diag-final-and (Koehn et al., 2003), and lexicalized-reordering method is msdbidirectional-fe (Koehn et al., 2007). For each monolingual corpus, we used a fivegram language model, which was built by IRSTLM toolkit2 (Federico, Bertoldi, &amp; Cettolo, 2008) with improved Kneser Ney smoothing (Chen &amp; Goodman, 1996; Kneser &amp; Ney, 1995). The language model was integrated as a loglinear feature to decoder. All the sentences i</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, 2007</marker>
<rawString>Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., . . . Herbst, E. (2007). Moses: open source toolkit for statistical machine translation. Paper presented at the Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation. Paper presented at the</title>
<date>2003</date>
<booktitle>Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1.</booktitle>
<contexts>
<context position="1871" citStr="Koehn, Och, &amp; Marcu, 2003" startWordPosition="272" endWordPosition="276">anslate the input query into relevant terms in target language. One way to translate queries is dictionarybased query translation. However, an input query usually consists of multiple terms, which cause low coverage of bilingual dictionary. Alternative way is translating queries using statistical machine translation (SMT) system. However, translation model could contain some noise that is meaningless translation. The goal of our method is to overcome the shortcomings of these approaches by a heuristic hybrid approach. As a baseline, we use phrase-based statistical machine translation (PBSMT) (Koehn, Och, &amp; Marcu, 2003) techniques to handle queries that consist of multiple terms. To identify multiple terms in a query, we analyze three cases of the formation of queries and generate query translation candidates using term-to-term dictionaries and PBSMT system, and then score these candidates using co-occurrence word frequency measure to select the best candidate. We have done experiment on two language pairs • English-German • German-English The rest of parts in this paper are organized as following: section 2 describes the techniques and system settings used in our experiment, section 3 presents used corpus </context>
<context position="2927" citStr="Koehn et al., 2003" startWordPosition="438" endWordPosition="441">t of parts in this paper are organized as following: section 2 describes the techniques and system settings used in our experiment, section 3 presents used corpus and experiment result, and section 4 shows a brief conclusion of our work. 2 Method 2.1 Phrase-based machine translation system The phrase-based statistical machine translation system is implemented using MOSE’S toolkits (Koehn et al., 2007). Bidirectional word alignments were built by MGIZA 1, a multi-thread version of GIZA++ (Och &amp; Ney, 2003), run on a 24 threads machine. The alignment symmetrization method is grow-diag-final-and (Koehn et al., 2003), and lexicalized-reordering method is msdbidirectional-fe (Koehn et al., 2007). For each monolingual corpus, we used a fivegram language model, which was built by IRSTLM toolkit2 (Federico, Bertoldi, &amp; Cettolo, 2008) with improved Kneser Ney smoothing (Chen &amp; Goodman, 1996; Kneser &amp; Ney, 1995). The language model was integrated as a loglinear feature to decoder. All the sentences in the training, development and test corpus were tokenized by inserting spaces between words and punctuations, and then converted to most probable cases by truecaseing. Both tokenization and truecasing were done by </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., Och, F. J., &amp; Marcu, D. (2003). Statistical phrase-based translation. Paper presented at the Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<pages>10--1162</pages>
<contexts>
<context position="2817" citStr="Och &amp; Ney, 2003" startWordPosition="421" endWordPosition="424">the best candidate. We have done experiment on two language pairs • English-German • German-English The rest of parts in this paper are organized as following: section 2 describes the techniques and system settings used in our experiment, section 3 presents used corpus and experiment result, and section 4 shows a brief conclusion of our work. 2 Method 2.1 Phrase-based machine translation system The phrase-based statistical machine translation system is implemented using MOSE’S toolkits (Koehn et al., 2007). Bidirectional word alignments were built by MGIZA 1, a multi-thread version of GIZA++ (Och &amp; Ney, 2003), run on a 24 threads machine. The alignment symmetrization method is grow-diag-final-and (Koehn et al., 2003), and lexicalized-reordering method is msdbidirectional-fe (Koehn et al., 2007). For each monolingual corpus, we used a fivegram language model, which was built by IRSTLM toolkit2 (Federico, Bertoldi, &amp; Cettolo, 2008) with improved Kneser Ney smoothing (Chen &amp; Goodman, 1996; Kneser &amp; Ney, 1995). The language model was integrated as a loglinear feature to decoder. All the sentences in the training, development and test corpus were tokenized by inserting spaces between words and punctuat</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Och, F. J., &amp; Ney, H. (2003). A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1), 19-51. doi: 10.1162/089120103321337421</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006674">
<title confidence="0.9969425">
Generating Patient Problem Lists from the ShARe Corpus using
SNOMED CT/SNOMED CT CORE Problem List
</title>
<author confidence="0.970999">
Danielle Mowery
Janyce Wiebe
</author>
<affiliation confidence="0.997649">
University of Pittsburgh
</affiliation>
<address confidence="0.562742">
Pittsburgh, PA
</address>
<email confidence="0.9882805">
dlm31@pitt.edu
wiebe@cs.pitt.edu
</email>
<author confidence="0.981808">
Mindy Ross
</author>
<affiliation confidence="0.996532">
University of California
</affiliation>
<address confidence="0.9332375">
San Diego
La Jolla, CA
</address>
<email confidence="0.998605">
mkross@ucsd.edu
</email>
<author confidence="0.975262">
Sumithra Velupillai
</author>
<affiliation confidence="0.978612">
Stockholm University
</affiliation>
<address confidence="0.933125">
Stockholm, SE
</address>
<email confidence="0.980952">
sumithra@dsv.su.se
</email>
<author confidence="0.8609035">
Stephane Meystre
Wendy W Chapman
</author>
<affiliation confidence="0.7709415">
University of Utah
Salt Lake City, UT
</affiliation>
<email confidence="0.726908">
stephane.meystre,
wendy.chapman@utah.edu
</email>
<sectionHeader confidence="0.993584" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992134375">
An up-to-date problem list is useful for
assessing a patient’s current clinical sta-
tus. Natural language processing can help
maintain an accurate problem list. For in-
stance, a patient problem list from a clin-
ical document can be derived from indi-
vidual problem mentions within the clin-
ical document once these mentions are
mapped to a standard vocabulary. In
order to develop and evaluate accurate
document-level inference engines for this
task, a patient problem list could be gen-
erated using a standard vocabulary. Ad-
equate coverage by standard vocabularies
is important for supporting a clear rep-
resentation of the patient problem con-
cepts described in the texts and for interop-
erability between clinical systems within
and outside the care facilities. In this
pilot study, we report the reliability of
domain expert generation of a patient
problem list from a variety of clinical
texts and evaluate the coverage of anno-
tated patient problems against SNOMED
CT and SNOMED Clinical Observation
Recording and Encoding (CORE) Prob-
lem List. Across report types, we learned
that patient problems can be annotated
with agreement ranging from 77.1% to
89.6% F1-score and mapped to the CORE
with moderate coverage ranging from
45%-67% of patient problems.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925787234043">
In the late 1960’s, Lawrence Weed published
about the importance of problem-oriented medi-
cal records and the utilization of a problem list
to facilitate care provider’s clinical reasoning by
reducing the cognitive burden of tracking cur-
rent, active problems from past, inactive problems
from the patient health record (Weed, 1970). Al-
though electronic health records (EHR) can help
achieve better documentation of problem-specific
information, in most cases, the problem list is
manually created and updated by care providers.
Thus, the problem list can be out-of-date con-
taining resolved problems or missing new prob-
lems. Providing care providers with problem list
update suggestions generated from clinical docu-
ments can improve the completeness and timeli-
ness of the problem list (Meystre and Haug, 2008).
In recent years, national incentive and standard
programs have endorsed the use of problem lists
in the EHR for tracking patient diagnoses over
time. For example, as part of the Electronic Health
Record Incentive Program, the Center for Medi-
care and Medicaid Services defined demonstra-
tion of Meaningful Use of adopted health infor-
mation technology in the Core Measure 3 objec-
tive as “maintaining an up-to-date problem list of
current and active diagnoses in addition to histor-
ical diagnoses relevant to the patients care” (Cen-
ter for Medicare and Medicaid Services, 2013).
More recently, the Systematized Nomenclature of
Medicine Clinical Terms (SNOMED CT) has be-
come the standard vocabulary for representing and
documenting patient problems within the clinical
record. Since 2008, this list is iteratively refined
four times each year to produce a subset of gen-
eralizable clinical problems called the SNOMED
CT CORE Problem List. This CORE list repre-
sents the most frequent problem terms and con-
cepts across eight major healthcare institutions in
the United States and is designed to support in-
teroperability between regional healthcare institu-
tions (National Library of Medicine, 2009).
In practice, there are several methodologies ap-
plied to generate a patient problem list from clin-
ical text. Problem lists can be generated from
coded diagnoses such as the International Statis-
tical Classification of Disease (ICD-9 codes) or
</bodyText>
<page confidence="0.978859">
54
</page>
<bodyText confidence="0.957643846153846">
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 54–58,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
concept labels such as Unified Medical Language
System concept unique identifiers (UMLS CUIs).
For example, Meystre and Haug (2005) defined 80
of the most frequent problem concepts from coded
diagnoses for cardiac patients. This list was gen-
erated by a physician and later validated by two
physicians independently. Coverage of coded pa-
tient problems were evaluated against the ICD-9-
CM vocabulary. Solti et al. (2008) extended the
work of Meystre and Haug (2005) by not limit-
ing the types of patient problems from any list
or vocabulary to generate the patient problem list.
They observed 154 unique problem concepts in
their reference standard. Although both studies
demonstrate valid methods for developing a pa-
tient problem list reference standard, neither study
leverages a standard vocabulary designed specifi-
cally for generating problem lists.
The goals of this study are 1) determine how
reliably two domain experts can generate a pa-
tient problem list leveraging SNOMED CT from
a variety of clinical texts and 2) assess the cover-
age of annotated patient problems from this corpus
against the CORE Problem List.
</bodyText>
<sectionHeader confidence="0.988399" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.999988818181818">
In this IRB-approved study, we obtained the
Shared Annotated Resource (ShARe) corpus
originally generated from the Beth Israel Dea-
coness Medical Center (Elhadad et al., un-
der review) and stored in the Multiparameter
Intelligent Monitoring in Intensive Care, ver-
sion 2.5 (MIMIC II) database (Saeed et al.,
2002). This corpus consists of discharge sum-
maries (DS), radiology (RAD), electrocardiogram
(ECG), and echocardiogram (ECHO) reports from
the Intensive Care Unit (ICU). The ShARe cor-
pus was selected because it 1) contains a variety of
clinical text sources, 2) links to additional patient
structured data that can be leveraged for further
system development and evaluation, and 3) has en-
coded individual problem mentions with semantic
annotations within each clinical document that can
be leveraged to develop and test document-level
inference engines. We elected to study ICU pa-
tients because they represent a sensitive cohort that
requires up-to-date summaries of their clinical sta-
tus for providing timely and effective care.
</bodyText>
<subsectionHeader confidence="0.999245">
2.1 Annotation Study
</subsectionHeader>
<bodyText confidence="0.999977416666667">
For this annotation study, two annotators - a physi-
cian and nurse - were provided independent train-
ing to annotate clinically relevant problems e.g.,
signs, symptoms, diseases, and disorders, at the
document-level for 20 reports. The annotators
were given feedback based on errors over two it-
erations. For each patient problem in the remain-
ing set, the physician was instructed to review the
full text, span the a problem mention, and map the
problem to a CUI from SNOMED-CT using the
extensible Human Oracle Suite of Tools (eHOST)
annotation tool (South et al., 2012). If a CUI did
not exist in the vocabulary for the problem, the
physician was instructed to assign a “CUI-less” la-
bel. Finally, the physician then assigned one of
five possible status labels - Active, Inactive, Re-
solved, Proposed, and Other - based on our pre-
vious study (Mowery et al., 2013) to the men-
tion representing its last status change at the con-
clusion of the care encounter. Patient problems
were not annotated as Negated since patient prob-
lem concepts are assumed absent at a document-
level (Meystre and Haug, 2005). If the patient
was healthy, the physician assigned “Healthy - no
problems” to the text. To reduce the cognitive bur-
den of annotation and create a more robust refer-
ence standard, these annotations were then pro-
vided to a nurse for review. The nurse was in-
structed to add missing, modify existing, or delete
spurious patient problems based on the guidelines.
We assessed how reliably annotators agreed
with each other’s patient problem lists using inter-
annotator agreement (IAA) at the document-level.
We evaluated IAA in two ways: 1) by problem
CUI and 2) by problem CUI and status. Since
the number of problems not annotated (i.e., true
negatives (TN)) are very large, we calculated F1-
score as a surrogate for kappa (Hripcsak and Roth-
schild, 2005). F1-score is the harmonic mean of
recall and precision, calculated from true posi-
tive, false positive, and false negative annotations,
which were defined as follows:
true positive (TP) = the physician and nurse prob-
lem annotation was assigned the same CUI
(and status)
false positive (FP) =the physician problem anno-
tation (and status) did not exist among the
nurse problem annotations
</bodyText>
<page confidence="0.993425">
55
</page>
<bodyText confidence="0.9860694">
FN as a patient problem CUI not occurring in the
vocabulary.
false negative (FN) = the nurse problem anno-
tation (and status) did not exist among the
physician problem annotations
</bodyText>
<equation confidence="0.9170168">
Recall = TP 3 Results
Precision = (1) We report the results of our annotation study on
(TP + FN) the full set and vocabulary coverage study on the
training set.
TP
(2)
(TP + FP)
F1-score =
2 (Recall ∗ Precision) (3)
(Recall + Precision)
</equation>
<bodyText confidence="0.866021333333333">
We sampled 50% of the corpus and determined
the most common errors. These errors with
examples were programmatically adjudicated
with the following solutions:
Spurious problems: procedures
solution: exclude non-problems via guidelines
</bodyText>
<listItem confidence="0.917298833333333">
Problem specificity: CUI specificity differences
solution: select most general CUIs
Conflicting status: negated vs. resolved
solution: select second reviewer’s status
CUI/CUI-less: C0031039 vs. CUI-less
solution: select CUI since clinically useful
</listItem>
<bodyText confidence="0.99954775">
We split the dataset into about two-thirds train-
ing and one-third test for each report type. The re-
maining data analysis was performed on the train-
ing set.
</bodyText>
<subsectionHeader confidence="0.99978">
2.2 Coverage Study
</subsectionHeader>
<bodyText confidence="0.999947571428572">
We characterized the composition of the reference
standard patient problem lists against two stan-
dard vocabularies SNOMED-CT and SNOMED-
CT CORE Problem List. We evaluated the cover-
age of patient problems against the SNOMED CT
CORE Problem List since the list was developed
to support encoding clinical observations such as
findings, diseases, and disorders for generating pa-
tient summaries like problem lists. We evaluated
the coverage of patient problems from the corpus
against the SNOMED-CT January 2012 Release
which leverages the UMLS version 2011AB. We
assessed recall (Eq 1), defining a TP as a patient
problem CUI occurring in the vocabulary and a
</bodyText>
<subsectionHeader confidence="0.99988">
3.1 Annotation Study
</subsectionHeader>
<bodyText confidence="0.9925046">
The full dataset is comprised of 298 clinical doc-
uments - 136 (45.6%) DS, 54 (18.1%) ECHO,
54 (18.1%) RAD, and 54 (18.1%) ECG. Seventy-
four percent (221) of the corpus was annotated by
both annotators. Table 1 shows agreement overall
and by report, matching problem CUI and prob-
lem CUI with status. Inter-annotator agreement
for problem with status was slightly lower for all
report types with the largest agreement drop for
DS at 15% (11.6 points).
</bodyText>
<table confidence="0.9990462">
Report Type CUI CUI + Status
DS 77.1 65.5
ECHO 83.9 82.8
RAD 84.7 82.8
ECG 89.6 84.8
</table>
<tableCaption confidence="0.987486">
Table 1: Document-level IAA by report type for problem
(CUI) and problem with status (CUI + status)
</tableCaption>
<bodyText confidence="0.999766">
We report the most common errors by frequency
in Table 2. By report type, the most common er-
rors for ECHO, RAD, and ECG were CUI/CUI-
less, and DS was Spurious Concepts.
</bodyText>
<table confidence="0.997611333333333">
Errors DS ECHO RAD ECG
SP 423 (42%) 26 (23%) 30 (35%) 8 (18%)
PS 139 (14%) 31 (27%) 8 (9%) 0 (0%)
CS 318 (32%) 9 (8%) 8 (9%) 14 (32%)
CC 110 (11%) 34 (30%) 37 (44%) 22 (50%)
Other 6 (&gt;1%) 14 (13%) 2 (2%) 0 (0%)
</table>
<tableCaption confidence="0.981746666666667">
Table 2: Error types by frequency - Spurious Problems (SP),
Problem Specificity (PS), Conflicting status (CS), CUI/CUI-
less (CC)
</tableCaption>
<subsectionHeader confidence="0.999821">
3.2 Coverage Study
</subsectionHeader>
<bodyText confidence="0.989653625">
In the training set, there were 203 clinical docu-
ments - 93 DS, 37 ECHO, 38 RAD, and 35 ECG.
The average number of problems were 22±10 DS,
10±4 ECHO, 6±2 RAD, and 4±1 ECG. There
are 5843 total current problems in SNOMED-CT
CORE Problem List. We observed a range of
unique SNOMED-CT problem concept frequen-
cies: 776 DS, 63 ECHO, 113 RAD, and 36 ECG
</bodyText>
<page confidence="0.992555">
56
</page>
<bodyText confidence="0.9997999">
by report type. The prevalence of covered prob-
lem concepts by CORE is 461 (59%) DS, 36
(57%) ECHO, 71 (63%) RAD, and 16 (44%)
ECG. In Table 3, we report coverage of patient
problems for each vocabulary. No reports were
annotated as “Healthy - no problems”. All reports
have SNOMED CT coverage of problem mentions
above 80%. After mapping problem mentions to
CORE, we observed coverage drops for all report
types, 24 to 36 points.
</bodyText>
<table confidence="0.999817833333334">
Report Patient Annotated with Mapped to
Type Problems SNOMED CT CORE
DS 2000 1813 (91%) 1335 (67%)
ECHO 349 300 (86%) 173 (50%)
RAD 190 156 (82%) 110 (58%)
ECG 95 77(81%) 43 (45%)
</table>
<tableCaption confidence="0.9689345">
Table 3: Patient problem coverage by SNOMED-CT and
SNOMED-CT CORE
</tableCaption>
<sectionHeader confidence="0.997363" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.9999304">
In this feasibility study, we evaluated how reliably
two domain experts can generate a patient problem
list and assessed the coverage of annotated patient
problems against two standard clinical vocabular-
ies.
</bodyText>
<subsectionHeader confidence="0.999859">
4.1 Annotation Study
</subsectionHeader>
<bodyText confidence="0.999070696969697">
Overall, we demonstrated that problems can be re-
liably annotated with moderate to high agreement
between domain experts (Table 1). For DS, agree-
ment scores were lowest and dropped most when
considering the problem status in the match crite-
ria. The most prevalent disagreement for DS was
Spurious problems (Table 2). Spurious problems
included additional events (e.g., C2939181: Mo-
tor vehicle accident), procedures (e.g., C0199470:
Mechanical ventilation), and modes of administra-
tion (e.g., C0041281: Tube feeding ofpatient) that
were outside our patient problem list inclusion cri-
teria. Some pertinent findings were also missed.
These findings are not surprising given on average
more problems occur in DS and the length of DS
documents are much longer than other document
types. Indeed, annotators are more likely to miss
a problem as the number of patient problems in-
crease.
Also, status differences can be attributed to mul-
tiple status change descriptions using expressions
of time e.g., “cough improved then” and modal-
ity “rule out pneumonia”, which are harder to
track and interpret over a longer document. The
most prevalent disagreements for all other doc-
ument types were CUI/CUI-less in which iden-
tifying a CUI representative of a clinical obser-
vation proved more difficult. An example of
Other disagreement was a sidedness mismatch
or redundant patient problem annotation. For
example, C0344911: Left ventricular dilatation
vs. C0344893: Right ventricular dilatation or
C0032285: Pneumonia was recorded twice.
</bodyText>
<subsectionHeader confidence="0.997734">
4.2 Coverage Study
</subsectionHeader>
<bodyText confidence="0.999983235294118">
We observed that DS and RAD reports have higher
counts and coverage of unique patient problem
concepts. We suspect this might be because other
document types like ECG reports are more likely
to have laboratory observations, which may be
less prevalent findings in CORE. Across document
types, coverage of patient problems in the corpus
by SNOMED CT were high ranging from 81%
to 91% (Table 3). However, coverage of patient
problems by CORE dropped to moderate cover-
ages ranging from 45% to 67%. This suggests that
the CORE Problem List is more restrictive and
may not be as useful for capturing patient prob-
lems from these document types. A similar report
of moderate problem coverage with a more restric-
tive concept list was also reported by Meystre and
Haug (2005).
</bodyText>
<sectionHeader confidence="0.998333" genericHeader="method">
5 Limitations
</sectionHeader>
<bodyText confidence="0.999908333333333">
Our study has limitations. We did not apply a tra-
ditional adjudication review between domain ex-
perts. In addition, we selected the ShARe corpus
from an ICU database in which vocabulary cover-
age of patient problems could be very different for
other domains and specialties.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999942916666667">
Based on this feasibility study, we conclude that
we can generate a reliable patient problem list
reference standard for the ShARe corpus and
SNOMED CT provides better coverage of patient
problems than the CORE Problem List. In fu-
ture work, we plan to evaluate from each ShARe
report type, how well these patient problem lists
can be derived and visualized from the individ-
ual disease/disorder problem mentions leveraging
temporality and modality attributes using natu-
ral language processing and machine learning ap-
proaches.
</bodyText>
<page confidence="0.998062">
57
</page>
<sectionHeader confidence="0.998329" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999012">
This work was partially funded by NLM
(5T15LM007059 and 1R01LM010964), ShARe
(R01GM090187), Swedish Research Council
(350-2012-6658), and Swedish Fulbright Com-
mission.
</bodyText>
<sectionHeader confidence="0.999145" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999700759259259">
Center for Medicare and Medicaid Services. 2013.
EHR Incentive Programs-Maintain Problem
List. http://www.cms.gov/Regulations-and-
Guidance/Legislation/EHRIncentivePrograms/
downloads/3 Maintain Problem ListEP.pdf.
Noemie Elhadad, Wendy Chapman, Tim OGorman,
Martha Palmer, and Guergana. Under Review
Savova. under review. The ShARe Schema for
the Syntactic and Semantic Annotation of Clinical
Texts.
George Hripcsak and Adam S. Rothschild. 2005.
Agreement, the F-measure, and Reliability in In-
formation Retrieval. J Am Med Inform Assoc,
12(3):296–298.
Stephane Meystre and Peter Haug. 2005. Automation
of a Problem List using Natural Language Process-
ing. BMC Medical Informatics and Decision Mak-
ing, 5(30).
Stephane M. Meystre and Peter J. Haug. 2008. Ran-
domized Controlled Trial of an Automated Problem
List with Improved Sensitivity. International Jour-
nal of Medical Informatics, 77:602–12.
Danielle L. Mowery, Pamela W. Jordan, Janyce M.
Wiebe, Henk Harkema, John Dowling, and
Wendy W. Chapman. 2013. Semantic Annotation
of Clinical Events for Generating a Problem List. In
AMIA Annu Symp Proc, pages 1032–1041.
National Library of Medicine. 2009. The
CORE Problem List Subset of SNOMED-
CT. Unified Medical Language System 2011.
http://www.nlm.nih.gov/research/umls/SNOMED-
CT/core subset.html.
Mohammed Saeed, C. Lieu, G. Raber, and Roger G.
Mark. 2002. MIMIC II: a massive temporal ICU
patient database to support research in intelligent pa-
tient monitoring. Comput Cardiol, 29.
Imre Solti, Barry Aaronson, Grant Fletcher, Magdolna
Solti, John H. Gennari, Melissa Cooper, and Thomas
Payne. 2008. Building an Automated Problem List
based on Natural Language Processing: Lessons
Learned in the Early Phase of Development. pages
687–691.
Brett R. South, Shuying Shen, Jianwei Leng, Tyler B.
Forbush, Scott L. DuVall, and Wendy W. Chapman.
2012. A prototype tool set to support machine-
assisted annotation. In Proceedings of the 2012
Workshop on Biomedical Natural Language Pro-
cessing, BioNLP ’12, pages 130–139. Association
for Computational Linguistics.
Lawrence Weed. 1970. Medical Records, Med-
ical Education and Patient Care: The Problem-
Oriented Record as a Basic Tool. Medical Pub-
lishers: Press of Case Western Reserve University,
Cleveland: Year Book.
</reference>
<page confidence="0.999262">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.021147">
<title confidence="0.8597355">Generating Patient Problem Lists from the ShARe Corpus SNOMED CT/SNOMED CT CORE Problem List</title>
<author confidence="0.788409">Danielle</author>
<affiliation confidence="0.8451">Janyce University of</affiliation>
<address confidence="0.776321">Pittsburgh, PA</address>
<email confidence="0.998912">wiebe@cs.pitt.edu</email>
<author confidence="0.895517">Mindy</author>
<affiliation confidence="0.9305495">University of San</affiliation>
<address confidence="0.748899">La Jolla, CA</address>
<email confidence="0.996845">mkross@ucsd.edu</email>
<affiliation confidence="0.353385">Sumithra</affiliation>
<address confidence="0.6870205">Stockholm Stockholm, SE</address>
<email confidence="0.852133">sumithra@dsv.su.se</email>
<title confidence="0.695061">Stephane</title>
<author confidence="0.873954">W Wendy</author>
<affiliation confidence="0.998746">University of</affiliation>
<address confidence="0.821179">Salt Lake City, UT</address>
<email confidence="0.942753">stephane.meystre,wendy.chapman@utah.edu</email>
<abstract confidence="0.998560242424242">An up-to-date problem list is useful for assessing a patient’s current clinical status. Natural language processing can help maintain an accurate problem list. For instance, a patient problem list from a clinical document can be derived from individual problem mentions within the clinical document once these mentions are mapped to a standard vocabulary. In order to develop and evaluate accurate document-level inference engines for this task, a patient problem list could be generated using a standard vocabulary. Adequate coverage by standard vocabularies is important for supporting a clear representation of the patient problem concepts described in the texts and for interoperability between clinical systems within and outside the care facilities. In this pilot study, we report the reliability of domain expert generation of a patient problem list from a variety of clinical texts and evaluate the coverage of annotated patient problems against SNOMED and SNOMED and (CORE) Problem List. Across report types, we learned that patient problems can be annotated with agreement ranging from 77.1% to 89.6% F1-score and mapped to the CORE with moderate coverage ranging from 45%-67% of patient problems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Medicare and Medicaid Services.</title>
<date>2013</date>
<booktitle>EHR Incentive Programs-Maintain Problem List. http://www.cms.gov/Regulations-andGuidance/Legislation/EHRIncentivePrograms/ downloads/3 Maintain Problem ListEP.pdf.</booktitle>
<institution>Center for</institution>
<marker>2013</marker>
<rawString>Center for Medicare and Medicaid Services. 2013. EHR Incentive Programs-Maintain Problem List. http://www.cms.gov/Regulations-andGuidance/Legislation/EHRIncentivePrograms/ downloads/3 Maintain Problem ListEP.pdf.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Noemie Elhadad</author>
<author>Wendy Chapman</author>
<author>Tim OGorman</author>
<author>Martha Palmer</author>
<author>Guergana</author>
</authors>
<title>Under Review Savova. under review. The ShARe Schema for the Syntactic and Semantic Annotation of Clinical Texts.</title>
<marker>Elhadad, Chapman, OGorman, Palmer, Guergana, </marker>
<rawString>Noemie Elhadad, Wendy Chapman, Tim OGorman, Martha Palmer, and Guergana. Under Review Savova. under review. The ShARe Schema for the Syntactic and Semantic Annotation of Clinical Texts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Hripcsak</author>
<author>Adam S Rothschild</author>
</authors>
<title>Agreement, the F-measure, and Reliability in Information Retrieval.</title>
<date>2005</date>
<journal>J Am Med Inform Assoc,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="8203" citStr="Hripcsak and Rothschild, 2005" startWordPosition="1278" endWordPosition="1282">of annotation and create a more robust reference standard, these annotations were then provided to a nurse for review. The nurse was instructed to add missing, modify existing, or delete spurious patient problems based on the guidelines. We assessed how reliably annotators agreed with each other’s patient problem lists using interannotator agreement (IAA) at the document-level. We evaluated IAA in two ways: 1) by problem CUI and 2) by problem CUI and status. Since the number of problems not annotated (i.e., true negatives (TN)) are very large, we calculated F1- score as a surrogate for kappa (Hripcsak and Rothschild, 2005). F1-score is the harmonic mean of recall and precision, calculated from true positive, false positive, and false negative annotations, which were defined as follows: true positive (TP) = the physician and nurse problem annotation was assigned the same CUI (and status) false positive (FP) =the physician problem annotation (and status) did not exist among the nurse problem annotations 55 FN as a patient problem CUI not occurring in the vocabulary. false negative (FN) = the nurse problem annotation (and status) did not exist among the physician problem annotations Recall = TP 3 Results Precision</context>
</contexts>
<marker>Hripcsak, Rothschild, 2005</marker>
<rawString>George Hripcsak and Adam S. Rothschild. 2005. Agreement, the F-measure, and Reliability in Information Retrieval. J Am Med Inform Assoc, 12(3):296–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephane Meystre</author>
<author>Peter Haug</author>
</authors>
<title>Automation of a Problem List using Natural Language Processing.</title>
<date>2005</date>
<journal>BMC Medical Informatics and Decision Making,</journal>
<volume>5</volume>
<issue>30</issue>
<contexts>
<context position="4305" citStr="Meystre and Haug (2005)" startWordPosition="646" endWordPosition="649">re institutions (National Library of Medicine, 2009). In practice, there are several methodologies applied to generate a patient problem list from clinical text. Problem lists can be generated from coded diagnoses such as the International Statistical Classification of Disease (ICD-9 codes) or 54 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 54–58, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics concept labels such as Unified Medical Language System concept unique identifiers (UMLS CUIs). For example, Meystre and Haug (2005) defined 80 of the most frequent problem concepts from coded diagnoses for cardiac patients. This list was generated by a physician and later validated by two physicians independently. Coverage of coded patient problems were evaluated against the ICD-9- CM vocabulary. Solti et al. (2008) extended the work of Meystre and Haug (2005) by not limiting the types of patient problems from any list or vocabulary to generate the patient problem list. They observed 154 unique problem concepts in their reference standard. Although both studies demonstrate valid methods for developing a patient problem li</context>
<context position="7452" citStr="Meystre and Haug, 2005" startWordPosition="1153" endWordPosition="1156">D-CT using the extensible Human Oracle Suite of Tools (eHOST) annotation tool (South et al., 2012). If a CUI did not exist in the vocabulary for the problem, the physician was instructed to assign a “CUI-less” label. Finally, the physician then assigned one of five possible status labels - Active, Inactive, Resolved, Proposed, and Other - based on our previous study (Mowery et al., 2013) to the mention representing its last status change at the conclusion of the care encounter. Patient problems were not annotated as Negated since patient problem concepts are assumed absent at a documentlevel (Meystre and Haug, 2005). If the patient was healthy, the physician assigned “Healthy - no problems” to the text. To reduce the cognitive burden of annotation and create a more robust reference standard, these annotations were then provided to a nurse for review. The nurse was instructed to add missing, modify existing, or delete spurious patient problems based on the guidelines. We assessed how reliably annotators agreed with each other’s patient problem lists using interannotator agreement (IAA) at the document-level. We evaluated IAA in two ways: 1) by problem CUI and 2) by problem CUI and status. Since the number</context>
<context position="15077" citStr="Meystre and Haug (2005)" startWordPosition="2418" endWordPosition="2421">document types like ECG reports are more likely to have laboratory observations, which may be less prevalent findings in CORE. Across document types, coverage of patient problems in the corpus by SNOMED CT were high ranging from 81% to 91% (Table 3). However, coverage of patient problems by CORE dropped to moderate coverages ranging from 45% to 67%. This suggests that the CORE Problem List is more restrictive and may not be as useful for capturing patient problems from these document types. A similar report of moderate problem coverage with a more restrictive concept list was also reported by Meystre and Haug (2005). 5 Limitations Our study has limitations. We did not apply a traditional adjudication review between domain experts. In addition, we selected the ShARe corpus from an ICU database in which vocabulary coverage of patient problems could be very different for other domains and specialties. 6 Conclusion Based on this feasibility study, we conclude that we can generate a reliable patient problem list reference standard for the ShARe corpus and SNOMED CT provides better coverage of patient problems than the CORE Problem List. In future work, we plan to evaluate from each ShARe report type, how well</context>
</contexts>
<marker>Meystre, Haug, 2005</marker>
<rawString>Stephane Meystre and Peter Haug. 2005. Automation of a Problem List using Natural Language Processing. BMC Medical Informatics and Decision Making, 5(30).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephane M Meystre</author>
<author>Peter J Haug</author>
</authors>
<title>Randomized Controlled Trial of an Automated Problem List with Improved Sensitivity.</title>
<date>2008</date>
<journal>International Journal of Medical Informatics,</journal>
<pages>77--602</pages>
<contexts>
<context position="2544" citStr="Meystre and Haug, 2008" startWordPosition="378" endWordPosition="381">ing by reducing the cognitive burden of tracking current, active problems from past, inactive problems from the patient health record (Weed, 1970). Although electronic health records (EHR) can help achieve better documentation of problem-specific information, in most cases, the problem list is manually created and updated by care providers. Thus, the problem list can be out-of-date containing resolved problems or missing new problems. Providing care providers with problem list update suggestions generated from clinical documents can improve the completeness and timeliness of the problem list (Meystre and Haug, 2008). In recent years, national incentive and standard programs have endorsed the use of problem lists in the EHR for tracking patient diagnoses over time. For example, as part of the Electronic Health Record Incentive Program, the Center for Medicare and Medicaid Services defined demonstration of Meaningful Use of adopted health information technology in the Core Measure 3 objective as “maintaining an up-to-date problem list of current and active diagnoses in addition to historical diagnoses relevant to the patients care” (Center for Medicare and Medicaid Services, 2013). More recently, the Syste</context>
</contexts>
<marker>Meystre, Haug, 2008</marker>
<rawString>Stephane M. Meystre and Peter J. Haug. 2008. Randomized Controlled Trial of an Automated Problem List with Improved Sensitivity. International Journal of Medical Informatics, 77:602–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danielle L Mowery</author>
<author>Pamela W Jordan</author>
<author>Janyce M Wiebe</author>
<author>Henk Harkema</author>
<author>John Dowling</author>
<author>Wendy W Chapman</author>
</authors>
<title>Semantic Annotation of Clinical Events for Generating a Problem List.</title>
<date>2013</date>
<booktitle>In AMIA Annu Symp Proc,</booktitle>
<pages>1032--1041</pages>
<contexts>
<context position="7219" citStr="Mowery et al., 2013" startWordPosition="1113" endWordPosition="1116">tors were given feedback based on errors over two iterations. For each patient problem in the remaining set, the physician was instructed to review the full text, span the a problem mention, and map the problem to a CUI from SNOMED-CT using the extensible Human Oracle Suite of Tools (eHOST) annotation tool (South et al., 2012). If a CUI did not exist in the vocabulary for the problem, the physician was instructed to assign a “CUI-less” label. Finally, the physician then assigned one of five possible status labels - Active, Inactive, Resolved, Proposed, and Other - based on our previous study (Mowery et al., 2013) to the mention representing its last status change at the conclusion of the care encounter. Patient problems were not annotated as Negated since patient problem concepts are assumed absent at a documentlevel (Meystre and Haug, 2005). If the patient was healthy, the physician assigned “Healthy - no problems” to the text. To reduce the cognitive burden of annotation and create a more robust reference standard, these annotations were then provided to a nurse for review. The nurse was instructed to add missing, modify existing, or delete spurious patient problems based on the guidelines. We asses</context>
</contexts>
<marker>Mowery, Jordan, Wiebe, Harkema, Dowling, Chapman, 2013</marker>
<rawString>Danielle L. Mowery, Pamela W. Jordan, Janyce M. Wiebe, Henk Harkema, John Dowling, and Wendy W. Chapman. 2013. Semantic Annotation of Clinical Events for Generating a Problem List. In AMIA Annu Symp Proc, pages 1032–1041.</rawString>
</citation>
<citation valid="true">
<date>2009</date>
<booktitle>The CORE Problem List Subset of SNOMEDCT. Unified Medical Language System</booktitle>
<institution>National Library of Medicine.</institution>
<note>http://www.nlm.nih.gov/research/umls/SNOMEDCT/core subset.html.</note>
<marker>2009</marker>
<rawString>National Library of Medicine. 2009. The CORE Problem List Subset of SNOMEDCT. Unified Medical Language System 2011. http://www.nlm.nih.gov/research/umls/SNOMEDCT/core subset.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Saeed</author>
<author>C Lieu</author>
<author>G Raber</author>
<author>Roger G Mark</author>
</authors>
<title>MIMIC II: a massive temporal ICU patient database to support research in intelligent patient monitoring.</title>
<date>2002</date>
<journal>Comput Cardiol,</journal>
<volume>29</volume>
<contexts>
<context position="5614" citStr="Saeed et al., 2002" startWordPosition="855" endWordPosition="858"> for generating problem lists. The goals of this study are 1) determine how reliably two domain experts can generate a patient problem list leveraging SNOMED CT from a variety of clinical texts and 2) assess the coverage of annotated patient problems from this corpus against the CORE Problem List. 2 Methods In this IRB-approved study, we obtained the Shared Annotated Resource (ShARe) corpus originally generated from the Beth Israel Deaconess Medical Center (Elhadad et al., under review) and stored in the Multiparameter Intelligent Monitoring in Intensive Care, version 2.5 (MIMIC II) database (Saeed et al., 2002). This corpus consists of discharge summaries (DS), radiology (RAD), electrocardiogram (ECG), and echocardiogram (ECHO) reports from the Intensive Care Unit (ICU). The ShARe corpus was selected because it 1) contains a variety of clinical text sources, 2) links to additional patient structured data that can be leveraged for further system development and evaluation, and 3) has encoded individual problem mentions with semantic annotations within each clinical document that can be leveraged to develop and test document-level inference engines. We elected to study ICU patients because they repres</context>
</contexts>
<marker>Saeed, Lieu, Raber, Mark, 2002</marker>
<rawString>Mohammed Saeed, C. Lieu, G. Raber, and Roger G. Mark. 2002. MIMIC II: a massive temporal ICU patient database to support research in intelligent patient monitoring. Comput Cardiol, 29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imre Solti</author>
<author>Barry Aaronson</author>
<author>Grant Fletcher</author>
<author>Magdolna Solti</author>
<author>John H Gennari</author>
<author>Melissa Cooper</author>
<author>Thomas Payne</author>
</authors>
<date>2008</date>
<booktitle>Building an Automated Problem List based on Natural Language Processing: Lessons Learned in the Early Phase of Development.</booktitle>
<pages>687--691</pages>
<contexts>
<context position="4593" citStr="Solti et al. (2008)" startWordPosition="692" endWordPosition="695">or 54 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 54–58, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics concept labels such as Unified Medical Language System concept unique identifiers (UMLS CUIs). For example, Meystre and Haug (2005) defined 80 of the most frequent problem concepts from coded diagnoses for cardiac patients. This list was generated by a physician and later validated by two physicians independently. Coverage of coded patient problems were evaluated against the ICD-9- CM vocabulary. Solti et al. (2008) extended the work of Meystre and Haug (2005) by not limiting the types of patient problems from any list or vocabulary to generate the patient problem list. They observed 154 unique problem concepts in their reference standard. Although both studies demonstrate valid methods for developing a patient problem list reference standard, neither study leverages a standard vocabulary designed specifically for generating problem lists. The goals of this study are 1) determine how reliably two domain experts can generate a patient problem list leveraging SNOMED CT from a variety of clinical texts and </context>
</contexts>
<marker>Solti, Aaronson, Fletcher, Solti, Gennari, Cooper, Payne, 2008</marker>
<rawString>Imre Solti, Barry Aaronson, Grant Fletcher, Magdolna Solti, John H. Gennari, Melissa Cooper, and Thomas Payne. 2008. Building an Automated Problem List based on Natural Language Processing: Lessons Learned in the Early Phase of Development. pages 687–691.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brett R South</author>
<author>Shuying Shen</author>
<author>Jianwei Leng</author>
<author>Tyler B Forbush</author>
<author>Scott L DuVall</author>
<author>Wendy W Chapman</author>
</authors>
<title>A prototype tool set to support machineassisted annotation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Workshop on Biomedical Natural Language Processing, BioNLP ’12,</booktitle>
<pages>130--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6927" citStr="South et al., 2012" startWordPosition="1061" endWordPosition="1064">ng timely and effective care. 2.1 Annotation Study For this annotation study, two annotators - a physician and nurse - were provided independent training to annotate clinically relevant problems e.g., signs, symptoms, diseases, and disorders, at the document-level for 20 reports. The annotators were given feedback based on errors over two iterations. For each patient problem in the remaining set, the physician was instructed to review the full text, span the a problem mention, and map the problem to a CUI from SNOMED-CT using the extensible Human Oracle Suite of Tools (eHOST) annotation tool (South et al., 2012). If a CUI did not exist in the vocabulary for the problem, the physician was instructed to assign a “CUI-less” label. Finally, the physician then assigned one of five possible status labels - Active, Inactive, Resolved, Proposed, and Other - based on our previous study (Mowery et al., 2013) to the mention representing its last status change at the conclusion of the care encounter. Patient problems were not annotated as Negated since patient problem concepts are assumed absent at a documentlevel (Meystre and Haug, 2005). If the patient was healthy, the physician assigned “Healthy - no problems</context>
</contexts>
<marker>South, Shen, Leng, Forbush, DuVall, Chapman, 2012</marker>
<rawString>Brett R. South, Shuying Shen, Jianwei Leng, Tyler B. Forbush, Scott L. DuVall, and Wendy W. Chapman. 2012. A prototype tool set to support machineassisted annotation. In Proceedings of the 2012 Workshop on Biomedical Natural Language Processing, BioNLP ’12, pages 130–139. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Weed</author>
</authors>
<title>Medical Records, Medical Education and Patient Care: The ProblemOriented Record as a Basic Tool.</title>
<date>1970</date>
<publisher>Medical Publishers: Press</publisher>
<institution>of Case Western Reserve University,</institution>
<location>Cleveland:</location>
<contexts>
<context position="2067" citStr="Weed, 1970" startWordPosition="308" endWordPosition="309">bservation Recording and Encoding (CORE) Problem List. Across report types, we learned that patient problems can be annotated with agreement ranging from 77.1% to 89.6% F1-score and mapped to the CORE with moderate coverage ranging from 45%-67% of patient problems. 1 Introduction In the late 1960’s, Lawrence Weed published about the importance of problem-oriented medical records and the utilization of a problem list to facilitate care provider’s clinical reasoning by reducing the cognitive burden of tracking current, active problems from past, inactive problems from the patient health record (Weed, 1970). Although electronic health records (EHR) can help achieve better documentation of problem-specific information, in most cases, the problem list is manually created and updated by care providers. Thus, the problem list can be out-of-date containing resolved problems or missing new problems. Providing care providers with problem list update suggestions generated from clinical documents can improve the completeness and timeliness of the problem list (Meystre and Haug, 2008). In recent years, national incentive and standard programs have endorsed the use of problem lists in the EHR for tracking </context>
</contexts>
<marker>Weed, 1970</marker>
<rawString>Lawrence Weed. 1970. Medical Records, Medical Education and Patient Care: The ProblemOriented Record as a Basic Tool. Medical Publishers: Press of Case Western Reserve University, Cleveland: Year Book.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
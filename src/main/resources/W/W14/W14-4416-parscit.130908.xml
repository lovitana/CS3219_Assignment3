<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.953856">
Classifiers for data-driven deep sentence generation
</title>
<author confidence="0.99312">
Miguel Ballesteros&apos;, Simon Mille&apos; and Leo Wanner2,&apos;
</author>
<affiliation confidence="0.992384">
&apos;NLP Group, Department of Information and Communication Technologies
Pompeu Fabra University, Barcelona
2Catalan Institute for Research and Advanced Studies (ICREA)
</affiliation>
<email confidence="0.999145">
&lt;fname&gt;.&lt;lname&gt;@upf.edu
</email>
<sectionHeader confidence="0.997393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999645">
State-of-the-art statistical sentence gener-
ators deal with isomorphic structures only.
Therefore, given that semantic and syntac-
tic structures tend to differ in their topol-
ogy and number of nodes, i.e., are not iso-
morphic, statistical generation saw so far
itself confined to shallow, syntactic gener-
ation. In this paper, we present a series
of fine-grained classifiers that are essen-
tial for data-driven deep sentence genera-
tion in that they handle the problem of the
projection of non-isomorphic structures.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.96124808">
Deep data-driven (or stochastic) sentence gener-
ation needs to be able to map abstract seman-
tic structures onto syntactic structures. This has
been a problem so far since both types of struc-
tures differ in their topology and number of nodes
(i.e., are non-isomorphic). For instance, a truly
semantic structure will not contain any functional
nodes,1 while a surface-syntactic structure or a
chain of tokens in a linearized tree will con-
tain all of them. Some state-of-the-art propos-
als use a rule-based module to handle the projec-
tion between non-isomorphic semantic and syn-
tactic structures/chains of tokens, e.g., (Varges and
Mellish, 2001; Belz, 2008; Bohnet et al., 2011),
and some adapt the semantic structures to be iso-
morphic with syntactic structures (Bohnet et al.,
2010). In this paper, we present two alternative
stochastic approaches to the projection between
non-isomorphic structures, both based on a cas-
cade of Support Vector Machine (SVM) classi-
fiers.2 The first approach addresses the projection
as a generic non-isomorphic graph transduction
1See, for instance, (Bouayad-Agha et al., 2012).
2Obviously, other machine learning techniques could also
be used.
problem in terms of four classifiers for 1. identi-
fication of the (non-isomorphic) correspondences
between fragments of the source and target struc-
ture, 2. generation of the nodes of the target struc-
ture, 3. generation of the dependencies between
corresponding fragments of the source and target
structure, and 4. generation of the internal depen-
dencies in all fragments of the target structure.
The second approach takes advantage of the lin-
guistic knowledge about the projection of the in-
dividual linguistic token types. It replaces each
of the above four classifiers by a set of classifiers,
with each single classifier dealing with only one
individual linguistic token type (verb, noun, ad-
verb, etc.) or with a configuration thereof. As will
be seen, the linguistic knowledge pays off: the sec-
ond approach achieves considerably better results.
Since our goal is to address the challenge of the
projection of non-isomorphic structures, we focus,
in what follows, on this task. That is, we do not
build a complete generation pipeline until the sur-
face. This could be done, for instance, by feed-
ing the output obtained from the projection of a
semantic onto a syntactic structure to the surface
realizer described in (Bohnet et al., 2010).
</bodyText>
<sectionHeader confidence="0.96213" genericHeader="method">
2 The Task
</sectionHeader>
<bodyText confidence="0.999984888888889">
The difference in the linguistic abstraction of se-
mantic and syntactic structures leads to diver-
gences that impede the isomorphy between the
two and make the mapping between them a chal-
lenge for statistical generation. Let us, before we
come to the implementation, give some theoretical
details on these structures as we picture them and
on the possible approaches to the projection of a
semantic structure to a syntactic one.
</bodyText>
<subsectionHeader confidence="0.9588765">
2.1 The Notion of semantic and syntactic
structures
</subsectionHeader>
<bodyText confidence="0.9957735">
As semantic structure, we assume a shallow se-
mantic representation that is very similar to the
</bodyText>
<page confidence="0.985683">
108
</page>
<bodyText confidence="0.973799055555556">
Proceedings of the 8th International Natural Language Generation Conference, pages 108–112,
Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics
PropBank (Babko-Malaya, 2005) and deep anno-
tations as used in the Surface Realisation Shared
Task (Belz et al., 2011): the deep-syntactic layer
of the AnCora-UPF corpus (Mille et al., 2013).
Deep-syntactic structures (DSyntSs) do not
contain any punctuation and functional nodes, i.e.,
governed prepositions and conjunctions, auxil-
iaries and determiners.3
As syntactic structure (in the terminology
of Ancora-UPF: surface-syntactic structures,
SSyntSs), we assume dependency trees in which
the nodes are labeled by open or closed class
lexemes and the edges by grammatical function
relations of the type subject, oblique object,
adverbial, modifier, etc.; cf.4 See Figure 1 for a
contrastive illustration of DSyntS and SSyntS.
</bodyText>
<figure confidence="0.943876">
II
rumor want new song be successful
conj
a rumor wants that the new song will be successful
</figure>
<figureCaption confidence="0.986225">
Figure 1: DSyntS (above) and SSyntS (below) of
an English Sentence.
</figureCaption>
<bodyText confidence="0.9995815">
Note, however, that the proposal outlined be-
low for the projection of non-isomorphic struc-
tures is trainable on any multi-layered treebanks
where different layers are not isomorphic.
</bodyText>
<subsectionHeader confidence="0.999949">
2.2 Projection of DSyntSs onto SSyntSs
</subsectionHeader>
<bodyText confidence="0.9979245">
In order to project a DSyntS onto its correspond-
ing SSyntS in the course of sentence generation,
the following types of actions need to be per-
formed:
</bodyText>
<listItem confidence="0.998289416666667">
1. Project each node in the DSyntS onto its SSynS-
correspondence. This correspondence can be a
single node, as, e.g., successful → successful, or a
subtree (hypernode, known as syntagm in linguis-
tics), as, e.g., song → the song ‘DT NN’ (where
‘DT’ is a determiner and ‘NN’ a noun) or be
→ that will be ‘IN VAUX VB’ (where ‘IN’ is a
preposition, ‘VAUX’ an auxiliary and ‘VB’ a full
verb). In formal terms, we assume any SSyntS-
correspondence to be a hypernode with a cardinal-
ity ≥ 1.
2. Generate the correct lemma for the nodes in
</listItem>
<footnote confidence="0.866038333333333">
3For more details on the SSyntS, see (Mille et al., 2013).
4DSyntSs and their corresponding SSyntSs are stored in
the 14-column CoNLL’08 format.
</footnote>
<bodyText confidence="0.658597">
SSyntS that do not have a 1:1 correspondence in
the SSyntS (as ‘DT’, ‘IN’ and ‘VAUX’ above).
</bodyText>
<listItem confidence="0.998953">
3. Establish the dependencies within the individ-
ual SSyntS-hypernodes.
4. Establish the dependencies between the
SSyntS-hypernodes (more precisely, between the
nodes of different SSyntS-hypernodes) to obtain a
connected SSyntS-tree.
</listItem>
<sectionHeader confidence="0.991642" genericHeader="method">
3 Classifiers
</sectionHeader>
<bodyText confidence="0.999810714285714">
As mentioned in the Introduction, the realization
of the actions 1.– 4. can be approached either in
terms of 4 generic classifiers (Section 3.1) or in
terms of 4 sets of fine-grained (micro) classifiers
(Section 3.2) that map one representation onto an-
other. As also mentioned above, we realize both
approaches as Support Vector Machines (SVMs).
</bodyText>
<subsectionHeader confidence="0.999119">
3.1 Generic classifier approach
</subsectionHeader>
<bodyText confidence="0.8848901875">
Each of the generic classifiers deals with one of
the following tasks.
a. Hypernode Identification: Given a deep
syntactic node nd from the DSyntS, the system
must find the shape of the surface hypernode (=
syntagm) that corresponds to nd in the SSyntS.
The hypernode identification SVM uses the fol-
lowing features:
POS of nd, POS of nd’s head, voice,
temp. constituency, finiteness, tense, lemma of
nd, and nd’s dependencies.
In order to simplify the task, we define the shape
of a surface hypernode as a list of surface PoS-
tags. This list contains the PoS of each of the lem-
mas within the hypernode and a tag that signals the
original deep node; for instance:
</bodyText>
<listItem confidence="0.960132">
[ VB(deep), VAUX, IN]
b. Lemma Generation. Once the hypernodes
of the SSyntS under construction have been pro-
duced, the functional nodes that have been newly
introduced in the hypernodes must be assigned a
lemma. The lemma generation SVM uses the fol-
lowing features of the deep nodes nd in the hyper-
nodes:
• finiteness, • definiteness, • PoS of nd, • lemma
of nd, • PoS of the head of nd
</listItem>
<bodyText confidence="0.890366666666667">
to select the most likely lemma.
c. Intra-hypernode Dependency Generation.
Given a hypernode and its lemmas provided by
the two previous stages, the dependencies (i.e., the
dependency attachments and dependency labels)
between the elements of the hypernode must be
</bodyText>
<figure confidence="0.990610142857143">
II
ATTR I
I
subj
det dobj
det
modif subj analyt fut copul
</figure>
<page confidence="0.991721">
109
</page>
<bodyText confidence="0.88805575">
determined (and thus also the governor of the hy-
pernode). For this task, the intra-hypernode de-
pendency generation SVM uses the following fea-
tures:
• lemmas included in the hypernode, • PoS-tags
of the lemmas in the hypernode, • voice of the
head h of the hypernode, • deep dependency re-
lation to h.
</bodyText>
<figure confidence="0.3456575">
analyt fut
[ VB(deep), VAUX, IN]
</figure>
<figureCaption confidence="0.992483">
Figure 2: Internal dependency within a hypernode.
</figureCaption>
<bodyText confidence="0.747818">
d. Inter-hypernode Dependency Generation.
Once the individual hypernodes have been con-
verted into connected dependency subtrees, the
hypernodes must be connected between each
other, such that we obtain a complete SSyntS. The
inter-hypernode dependency generation SVM uses
the following features of a hypernode ss:
</bodyText>
<listItem confidence="0.776563">
• the internal dependencies of s3, • the head of
s3, • the lemmas of s3, • the PoS of the depen-
dent of the head of s3 in DSyntS
</listItem>
<bodyText confidence="0.914257">
to determine for each hypernode its governor.
</bodyText>
<equation confidence="0.866735">
subj
[ VB(deep), VAUX, IN] [ NN(deep), DT]
</equation>
<figureCaption confidence="0.980982">
Figure 3: Surface dependencies between two hy-
pernodes.
</figureCaption>
<subsectionHeader confidence="0.9026025">
3.2 Implementation of sets of micro
classifiers
</subsectionHeader>
<bodyText confidence="0.999913157894737">
In this alternative approach, a single classifier is
foreseen for each kind of input. Thus, for the
hypernode identification module, for each deep
PoS tag (which can be one of the following four:
‘N’ (noun), ‘V’ (verb), ‘Adv’ (adverb), ‘A’ (ad-
jective)), a separate multi-class classifier is de-
fined. For instance, in the case of ‘N’, the N-
classifier will use the above features to assign
to the a DSynt-node with PoS ‘N’ the most ap-
propriate (most likely) hypernode—in this case,
[NN(deep), DT]. In a similar way, in the case of
the lemma generation module, for each surface
PoS tag, a separate classifier is defined. Thus,
the DT-classifier would pick for the hypernode
[NN(deep), DT] the most likely lemma for the DT-
node (optimally, a determiner).
For the intra-hypernode attachments module,
for each kind of hypernode, dynamically a sepa-
rate classifier is generated.5 In the case of the hy-
</bodyText>
<footnote confidence="0.669967">
5This implies that the number of classifiers varies depend-
ing on the training set, in the intra-hypernode dependency
generation there are 108 SVMs.
</footnote>
<bodyText confidence="0.999858">
pernode [ VB(deep), VAUX, IN], the correspond-
ing classifier will create a link between the prepo-
sition and the auxiliary, and between the auxiliary
and the verb, with respectively the preposition and
the auxiliary as heads because it is the best link
that it can find; cf. Figure 2 for illustration.
Finally, for the inter-hypernode attachments
module, for each hypernode with a distinct in-
ternal dependency pattern, a separate classifier is
dynamically derived (for our treebank, we ob-
tained 114 different SVM classifiers because it
also takes into account hypernodes with just one
token). For instance, the classifier for the hypern-
ode [ NN(deep), DT] is most likely to identify as
its governor VAUX in the hypernode [ VB(deep),
VAUX, IN]; cf. Figure 3.
</bodyText>
<sectionHeader confidence="0.999203" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999516375">
In this section, we present the performance of the
two approaches to DSyntS–SSyntS projection on
the DSyntS- and SSynt-layers of the AnCora-UPF
treebank (Mille et al., 2013).6 Table 1 displays
the results for the generic classifier for all tasks
on the development and the test set, while Table
2 displays the results obtained through the sets of
micro classifiers.
</bodyText>
<table confidence="0.99878">
Dev.set # %
Hypernode identification 3131/3441 90.99
Lemma generation 818/936 87.39
Intra-hypernode dep. generation 545/798 68.30
Inter-hypernode dep. generation 2588/3055 84.71
Test set # %
Hypernode identification 5166/5887 87.75
Lemma generation 1822/2084 87.43
Intra-hypernode dep. generation 1093/1699 64.33
Inter-hypernode dep. generation 4679/5385 86.89
</table>
<tableCaption confidence="0.9496875">
Table 1: Results of the evaluation of the generic
classifiers for the non-isomorphic transduction.
</tableCaption>
<bodyText confidence="0.999861375">
The results show that for hypernode identifica-
tion and inter-hypernode dependency generation,
the results of both types of classifiers are compara-
ble, be it on the development set or on the test set.
However, thanks to the micro classifiers, with the
same features, the lemma generation model based
on micro classifiers improves by 4 points and the
intra-hypernode dependency generation by nearly
</bodyText>
<footnote confidence="0.605442">
6Following a classical machine learning set-up, we di-
vided the treebank into: (i) a development set (219 sen-
tences, 3271 tokens in the DSyntS treebank and 4953 tokens
in the SSyntS treebank); (ii) a training set (3036 sentences,
57665 tokens in the DSyntS treebank and 86984 tokens in
the SSyntS treebank); and a (iii) a held-out test for evalua-
tion (258 sentences, 5641 tokens in the DSyntS treebank and
8955 tokens in the SSyntS treebank).
prepos
</footnote>
<page confidence="0.84344">
110
</page>
<table confidence="0.9994342">
Dev.set # %
Hypernode identification 3133/3441 91.05
Lemma generation 851/936 90.92
Intra-hypernode dep. generation 767/798 96.12
Inter-hypernode dep. generation 2574/3055 84.26
Test set # %
Hypernode identification 5169/5886 87.82
Lemma generation 1913/2084 91.79
Intra-hypernode dep. generation 1630/1699 95.94
Inter-hypernode dep. generation 4648/5385 86.31
</table>
<tableCaption confidence="0.9731465">
Table 2: Results of the evaluation of the micro
classifiers for the non-isomorphic transduction.
</tableCaption>
<bodyText confidence="0.99943562962963">
30 points. This means that the intra-hypernode de-
pendency generation task is too sparse to be real-
ized as a single classifier. The micro classifiers
are in this case binary, i.e., 2:1, or unary, i.e., 1:1
classifiers, which implies a tremendous reduction
of the search space (and thus higher accuracy). In
contrast, the single classifier is a multi-class clas-
sifier that must decide among more than 60 pos-
sible classes. Although most of these 60 classes
are diferentiated by features, the differentiation
is not perfect. In the case of lemma generation,
we observe a similar phenomenon. In this case,
the micro-classifiers are multi-class classifiers that
normally have to cope with 5 different classes
(lemmas in this case), while the unique classi-
fier has to cope with around 60 different classes
(or lemmas). Hypernode identification and inter-
hypernode dependency generation are completely
guided by the input; thus, it seems that they do not
err in the same way.
Although the micro classifier approach leads
to significantly better results, we believe that it
can still be improved. First, the introduction of
prepositions causes most errors in hypernode de-
tection and lemma generation: when a preposition
should be introduced or not and which preposi-
tion should be introduced depends exclusively on
the sub-categorization frame of the governor of
the deep node. A treebank of a limited size as
used in our experiments simply does not contain
subcategorization patterns of all predicative lexi-
cal items (especially of nouns)—which would be
crucial. Thus, in the test set evaluation, out of the
171 lemma errors 147 are prepositions and out of
the 717 errors on hypernode identification, more
than 500 are due to nouns and preposition. The in-
crease of the size of the treebank would therefore
be an advantage.
Second, in the case of inter-hypernode depen-
dency, errors are due to the labels of the dependen-
cies more than to the attachements, and are quite
distributed over the different types of configura-
tions. The generation of these dependencies suf-
fers from the fact that the SSyntS tag-set is very
fine-grained. For instance, there are 9 different
types of verbal objects in SSyntS,7 which capture
very specific syntactic properties of Spanish, such
as “can the dependent can be replaced by a clitic
pronoun? Can the dependent be moved away from
its governor? Etc. This kind of information is not
of a high relevance for generation of well-formed
text. Using a more reduced (more coarse-grained)
SSyntS tag set would definitely improve the qual-
ity of the projection.
</bodyText>
<sectionHeader confidence="0.999941" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999988315789474">
There is an increasing amount of work on sta-
tistical sentence generation; see, e.g., (Bangalore
and Rambow, 2000; Langkilde-Geary, 2002; Fil-
ippova and Strube, 2008). However, hardly any
addresses the problem of the projection between
non-isomorphic semantic and syntactic structures.
In general, structure prediction approaches use
a single classifier model (Smith, 2011). But
see, e.g., (Carreras et al., 2008), who use dif-
ferent models to predict each part of the triplet
for spinal model pruning, and (Bj¨orkelund et al.,
2010; Johansson and Nugues, 2008), who use
a set of classifiers for predicate identification in
the context of semantic role labelling. Amalgam
(Corston-Oliver et al., 2002), which maps a logi-
cal input onto sentences with intermediate syntac-
tic (phrase-based) representation, uses language-
specific decision trees in order to predict when to
introduce auxiliaries, determiners, cases, etc.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999987666666667">
We presented two alternative classifier approaches
to deep generation that cope with the projection
of non-isomorphic semantic and syntactic struc-
tures and argued that the micro classifier approach
is more adequate. In spite of possible improve-
ments presented in Section 4, each set of micro
classifiers achieves results above 86% on the test
set. For intra-hypernode dependency generation,
it even reaches 95.94% .
</bodyText>
<sectionHeader confidence="0.998815" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.976168333333333">
This work has been partially funded by the Euro-
pean Commission under the contract number FP7-
ICT-610411.
</bodyText>
<footnote confidence="0.988049">
7There are 47 SSynt dependencies in total, to compare to
the 7 dependencies in the DSyntS.
</footnote>
<page confidence="0.997784">
111
</page>
<sectionHeader confidence="0.996326" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998385975609756">
Olga Babko-Malaya, 2005. Propbank Annotation
Guidelines.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gener-
ation. In Proceedings of the 18th International Con-
ference on Computational Linguistics (COLING),
pages 42–48, Saarbr¨ucken, Germany.
Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first Surface Realisation Shared Task: Overview and
evaluation results. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation (ENLG), pages
217–226, Nancy, France.
Anja Belz. 2008. Automatic generation of weather
forecast texts using comprehensive probabilistic
generation-space models. Journal of Natural Lan-
guage Engineering, 14(4):431–455.
A. Bj¨orkelund, B. Bohnet, L. Hafdell, and P. Nugues.
2010. A high-performance syntactic and semantic
dependency parser. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics : Demonstration Volume (COLING), pages 33–
36, Beijing, China.
Bernd Bohnet, Leo Wanner, Simon Mille, and Ali-
cia Burga. 2010. Broad coverage multilingual
deep sentence generation with a stochastic multi-
level realizer. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics
(COLING), pages 98–106, Beijing, China.
Bernd Bohnet, Simon Mille, Benoit Favre, and Leo
Wanner. 2011. StuMaBa: From deep representation
to surface. In Proceedings of the Generation Chal-
lenges Session at the 13th European Workshop on
Natural Language Generation (ENLG), pages 232–
235, Nancy, France.
Nadjet Bouayad-Agha, Gerard Casamayor, Simon
Mille, and Leo Wanner. 2012. Perspective-oriented
generation of football match summaries: Old tasks,
new challenges. ACM Transactions on Speech and
Language Processing, 9(2):3:1–3:31.
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the per-
ceptron for efficient, feature-rich parsing. In Pro-
ceedings of the 12th Conference on Computational
Natural Language Learning (CoNLL), pages 9–16,
Manchester, UK.
Simon Corston-Oliver, Michael Gamon, Eric Ringger,
and Robert Moore. 2002. An overview of Amal-
gam: A machine-learned generation module. In
Proceedings of the 2nd International Natural Lan-
guage Generation Conference (INLG), pages 33–40,
New-York, NY, USA.
Katja Filippova and Michael Strube. 2008. Sen-
tence fusion via dependency graph compression.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 177–185, Honolulu, Hawaii.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based Semantic Role Labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 69–78, Honolulu, Hawaii.
Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proceedings of
the 2nd International Natural Language Generation
Conference (INLG), pages 17–24, New-York, NY,
USA. Citeseer.
Simon Mille, Alicia Burga, and Leo Wanner. 2013.
AnCora-UPF: A multi-level annotation of Spanish.
In Proceedings of the 2nd International Conference
on Dependency Linguistics (DepLing), pages 217–
226, Prague, Czech Republic.
Noah A. Smith. 2011. Linguistic Structure Prediction.
Synthesis Lectures on Human Language Technolo-
gies. Morgan and Claypool.
Sebastian Varges and Chris Mellish. 2001. Instance-
based Natural Language Generation. In Proceed-
ings of the 2nd Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL), pages 1–8, Pittsburgh, PA, USA.
</reference>
<page confidence="0.998207">
112
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.962265">
<title confidence="0.999466">Classifiers for data-driven deep sentence generation</title>
<author confidence="0.999965">Simon Leo</author>
<affiliation confidence="0.989344333333333">Group, Department of Information and Communication Pompeu Fabra University, Institute for Research and Advanced Studies</affiliation>
<abstract confidence="0.999527846153846">State-of-the-art statistical sentence generators deal with isomorphic structures only. Therefore, given that semantic and syntactic structures tend to differ in their topology and number of nodes, i.e., are not isomorphic, statistical generation saw so far itself confined to shallow, syntactic generation. In this paper, we present a series of fine-grained classifiers that are essential for data-driven deep sentence generation in that they handle the problem of the projection of non-isomorphic structures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Olga Babko-Malaya</author>
</authors>
<title>Propbank Annotation Guidelines.</title>
<date>2005</date>
<contexts>
<context position="4047" citStr="Babko-Malaya, 2005" startWordPosition="618" endWordPosition="619">them a challenge for statistical generation. Let us, before we come to the implementation, give some theoretical details on these structures as we picture them and on the possible approaches to the projection of a semantic structure to a syntactic one. 2.1 The Notion of semantic and syntactic structures As semantic structure, we assume a shallow semantic representation that is very similar to the 108 Proceedings of the 8th International Natural Language Generation Conference, pages 108–112, Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics PropBank (Babko-Malaya, 2005) and deep annotations as used in the Surface Realisation Shared Task (Belz et al., 2011): the deep-syntactic layer of the AnCora-UPF corpus (Mille et al., 2013). Deep-syntactic structures (DSyntSs) do not contain any punctuation and functional nodes, i.e., governed prepositions and conjunctions, auxiliaries and determiners.3 As syntactic structure (in the terminology of Ancora-UPF: surface-syntactic structures, SSyntSs), we assume dependency trees in which the nodes are labeled by open or closed class lexemes and the edges by grammatical function relations of the type subject, oblique object, </context>
</contexts>
<marker>Babko-Malaya, 2005</marker>
<rawString>Olga Babko-Malaya, 2005. Propbank Annotation Guidelines.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Owen Rambow</author>
</authors>
<title>Exploiting a probabilistic hierarchical model for generation.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>42--48</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="15720" citStr="Bangalore and Rambow, 2000" startWordPosition="2503" endWordPosition="2506">at the SSyntS tag-set is very fine-grained. For instance, there are 9 different types of verbal objects in SSyntS,7 which capture very specific syntactic properties of Spanish, such as “can the dependent can be replaced by a clitic pronoun? Can the dependent be moved away from its governor? Etc. This kind of information is not of a high relevance for generation of well-formed text. Using a more reduced (more coarse-grained) SSyntS tag set would definitely improve the quality of the projection. 5 Related work There is an increasing amount of work on statistical sentence generation; see, e.g., (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008). However, hardly any addresses the problem of the projection between non-isomorphic semantic and syntactic structures. In general, structure prediction approaches use a single classifier model (Smith, 2011). But see, e.g., (Carreras et al., 2008), who use different models to predict each part of the triplet for spinal model pruning, and (Bj¨orkelund et al., 2010; Johansson and Nugues, 2008), who use a set of classifiers for predicate identification in the context of semantic role labelling. Amalgam (Corston-Oliver et al., 2002), which maps a</context>
</contexts>
<marker>Bangalore, Rambow, 2000</marker>
<rawString>Srinivas Bangalore and Owen Rambow. 2000. Exploiting a probabilistic hierarchical model for generation. In Proceedings of the 18th International Conference on Computational Linguistics (COLING), pages 42–48, Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Michael White</author>
<author>Dominic Espinosa</author>
<author>Eric Kow</author>
<author>Deirdre Hogan</author>
<author>Amanda Stent</author>
</authors>
<title>The first Surface Realisation Shared Task: Overview and evaluation results.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>217--226</pages>
<location>Nancy, France.</location>
<contexts>
<context position="4135" citStr="Belz et al., 2011" startWordPosition="632" endWordPosition="635">, give some theoretical details on these structures as we picture them and on the possible approaches to the projection of a semantic structure to a syntactic one. 2.1 The Notion of semantic and syntactic structures As semantic structure, we assume a shallow semantic representation that is very similar to the 108 Proceedings of the 8th International Natural Language Generation Conference, pages 108–112, Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics PropBank (Babko-Malaya, 2005) and deep annotations as used in the Surface Realisation Shared Task (Belz et al., 2011): the deep-syntactic layer of the AnCora-UPF corpus (Mille et al., 2013). Deep-syntactic structures (DSyntSs) do not contain any punctuation and functional nodes, i.e., governed prepositions and conjunctions, auxiliaries and determiners.3 As syntactic structure (in the terminology of Ancora-UPF: surface-syntactic structures, SSyntSs), we assume dependency trees in which the nodes are labeled by open or closed class lexemes and the edges by grammatical function relations of the type subject, oblique object, adverbial, modifier, etc.; cf.4 See Figure 1 for a contrastive illustration of DSyntS an</context>
</contexts>
<marker>Belz, White, Espinosa, Kow, Hogan, Stent, 2011</marker>
<rawString>Anja Belz, Michael White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The first Surface Realisation Shared Task: Overview and evaluation results. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation (ENLG), pages 217–226, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
</authors>
<title>Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models.</title>
<date>2008</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="1480" citStr="Belz, 2008" startWordPosition="217" endWordPosition="218">generation needs to be able to map abstract semantic structures onto syntactic structures. This has been a problem so far since both types of structures differ in their topology and number of nodes (i.e., are non-isomorphic). For instance, a truly semantic structure will not contain any functional nodes,1 while a surface-syntactic structure or a chain of tokens in a linearized tree will contain all of them. Some state-of-the-art proposals use a rule-based module to handle the projection between non-isomorphic semantic and syntactic structures/chains of tokens, e.g., (Varges and Mellish, 2001; Belz, 2008; Bohnet et al., 2011), and some adapt the semantic structures to be isomorphic with syntactic structures (Bohnet et al., 2010). In this paper, we present two alternative stochastic approaches to the projection between non-isomorphic structures, both based on a cascade of Support Vector Machine (SVM) classifiers.2 The first approach addresses the projection as a generic non-isomorphic graph transduction 1See, for instance, (Bouayad-Agha et al., 2012). 2Obviously, other machine learning techniques could also be used. problem in terms of four classifiers for 1. identification of the (non-isomorp</context>
</contexts>
<marker>Belz, 2008</marker>
<rawString>Anja Belz. 2008. Automatic generation of weather forecast texts using comprehensive probabilistic generation-space models. Journal of Natural Language Engineering, 14(4):431–455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bj¨orkelund</author>
<author>B Bohnet</author>
<author>L Hafdell</author>
<author>P Nugues</author>
</authors>
<title>A high-performance syntactic and semantic dependency parser.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics : Demonstration Volume (COLING),</booktitle>
<pages>33--36</pages>
<location>Beijing, China.</location>
<marker>Bj¨orkelund, Bohnet, Hafdell, Nugues, 2010</marker>
<rawString>A. Bj¨orkelund, B. Bohnet, L. Hafdell, and P. Nugues. 2010. A high-performance syntactic and semantic dependency parser. In Proceedings of the 23rd International Conference on Computational Linguistics : Demonstration Volume (COLING), pages 33– 36, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Leo Wanner</author>
<author>Simon Mille</author>
<author>Alicia Burga</author>
</authors>
<title>Broad coverage multilingual deep sentence generation with a stochastic multilevel realizer.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>98--106</pages>
<location>Beijing, China.</location>
<contexts>
<context position="1607" citStr="Bohnet et al., 2010" startWordPosition="236" endWordPosition="239"> far since both types of structures differ in their topology and number of nodes (i.e., are non-isomorphic). For instance, a truly semantic structure will not contain any functional nodes,1 while a surface-syntactic structure or a chain of tokens in a linearized tree will contain all of them. Some state-of-the-art proposals use a rule-based module to handle the projection between non-isomorphic semantic and syntactic structures/chains of tokens, e.g., (Varges and Mellish, 2001; Belz, 2008; Bohnet et al., 2011), and some adapt the semantic structures to be isomorphic with syntactic structures (Bohnet et al., 2010). In this paper, we present two alternative stochastic approaches to the projection between non-isomorphic structures, both based on a cascade of Support Vector Machine (SVM) classifiers.2 The first approach addresses the projection as a generic non-isomorphic graph transduction 1See, for instance, (Bouayad-Agha et al., 2012). 2Obviously, other machine learning techniques could also be used. problem in terms of four classifiers for 1. identification of the (non-isomorphic) correspondences between fragments of the source and target structure, 2. generation of the nodes of the target structure, </context>
<context position="3241" citStr="Bohnet et al., 2010" startWordPosition="494" endWordPosition="497">ier dealing with only one individual linguistic token type (verb, noun, adverb, etc.) or with a configuration thereof. As will be seen, the linguistic knowledge pays off: the second approach achieves considerably better results. Since our goal is to address the challenge of the projection of non-isomorphic structures, we focus, in what follows, on this task. That is, we do not build a complete generation pipeline until the surface. This could be done, for instance, by feeding the output obtained from the projection of a semantic onto a syntactic structure to the surface realizer described in (Bohnet et al., 2010). 2 The Task The difference in the linguistic abstraction of semantic and syntactic structures leads to divergences that impede the isomorphy between the two and make the mapping between them a challenge for statistical generation. Let us, before we come to the implementation, give some theoretical details on these structures as we picture them and on the possible approaches to the projection of a semantic structure to a syntactic one. 2.1 The Notion of semantic and syntactic structures As semantic structure, we assume a shallow semantic representation that is very similar to the 108 Proceedin</context>
</contexts>
<marker>Bohnet, Wanner, Mille, Burga, 2010</marker>
<rawString>Bernd Bohnet, Leo Wanner, Simon Mille, and Alicia Burga. 2010. Broad coverage multilingual deep sentence generation with a stochastic multilevel realizer. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 98–106, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Simon Mille</author>
<author>Benoit Favre</author>
<author>Leo Wanner</author>
</authors>
<title>StuMaBa: From deep representation to surface.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation (ENLG),</booktitle>
<pages>232--235</pages>
<location>Nancy, France.</location>
<contexts>
<context position="1502" citStr="Bohnet et al., 2011" startWordPosition="219" endWordPosition="222">eeds to be able to map abstract semantic structures onto syntactic structures. This has been a problem so far since both types of structures differ in their topology and number of nodes (i.e., are non-isomorphic). For instance, a truly semantic structure will not contain any functional nodes,1 while a surface-syntactic structure or a chain of tokens in a linearized tree will contain all of them. Some state-of-the-art proposals use a rule-based module to handle the projection between non-isomorphic semantic and syntactic structures/chains of tokens, e.g., (Varges and Mellish, 2001; Belz, 2008; Bohnet et al., 2011), and some adapt the semantic structures to be isomorphic with syntactic structures (Bohnet et al., 2010). In this paper, we present two alternative stochastic approaches to the projection between non-isomorphic structures, both based on a cascade of Support Vector Machine (SVM) classifiers.2 The first approach addresses the projection as a generic non-isomorphic graph transduction 1See, for instance, (Bouayad-Agha et al., 2012). 2Obviously, other machine learning techniques could also be used. problem in terms of four classifiers for 1. identification of the (non-isomorphic) correspondences b</context>
</contexts>
<marker>Bohnet, Mille, Favre, Wanner, 2011</marker>
<rawString>Bernd Bohnet, Simon Mille, Benoit Favre, and Leo Wanner. 2011. StuMaBa: From deep representation to surface. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation (ENLG), pages 232– 235, Nancy, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadjet Bouayad-Agha</author>
<author>Gerard Casamayor</author>
<author>Simon Mille</author>
<author>Leo Wanner</author>
</authors>
<title>Perspective-oriented generation of football match summaries: Old tasks, new challenges.</title>
<date>2012</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="1934" citStr="Bouayad-Agha et al., 2012" startWordPosition="283" endWordPosition="286">sals use a rule-based module to handle the projection between non-isomorphic semantic and syntactic structures/chains of tokens, e.g., (Varges and Mellish, 2001; Belz, 2008; Bohnet et al., 2011), and some adapt the semantic structures to be isomorphic with syntactic structures (Bohnet et al., 2010). In this paper, we present two alternative stochastic approaches to the projection between non-isomorphic structures, both based on a cascade of Support Vector Machine (SVM) classifiers.2 The first approach addresses the projection as a generic non-isomorphic graph transduction 1See, for instance, (Bouayad-Agha et al., 2012). 2Obviously, other machine learning techniques could also be used. problem in terms of four classifiers for 1. identification of the (non-isomorphic) correspondences between fragments of the source and target structure, 2. generation of the nodes of the target structure, 3. generation of the dependencies between corresponding fragments of the source and target structure, and 4. generation of the internal dependencies in all fragments of the target structure. The second approach takes advantage of the linguistic knowledge about the projection of the individual linguistic token types. It replac</context>
</contexts>
<marker>Bouayad-Agha, Casamayor, Mille, Wanner, 2012</marker>
<rawString>Nadjet Bouayad-Agha, Gerard Casamayor, Simon Mille, and Leo Wanner. 2012. Perspective-oriented generation of football match summaries: Old tasks, new challenges. ACM Transactions on Speech and Language Processing, 9(2):3:1–3:31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>9--16</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="16019" citStr="Carreras et al., 2008" startWordPosition="2544" endWordPosition="2547">ind of information is not of a high relevance for generation of well-formed text. Using a more reduced (more coarse-grained) SSyntS tag set would definitely improve the quality of the projection. 5 Related work There is an increasing amount of work on statistical sentence generation; see, e.g., (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008). However, hardly any addresses the problem of the projection between non-isomorphic semantic and syntactic structures. In general, structure prediction approaches use a single classifier model (Smith, 2011). But see, e.g., (Carreras et al., 2008), who use different models to predict each part of the triplet for spinal model pruning, and (Bj¨orkelund et al., 2010; Johansson and Nugues, 2008), who use a set of classifiers for predicate identification in the context of semantic role labelling. Amalgam (Corston-Oliver et al., 2002), which maps a logical input onto sentences with intermediate syntactic (phrase-based) representation, uses languagespecific decision trees in order to predict when to introduce auxiliaries, determiners, cases, etc. 6 Conclusions We presented two alternative classifier approaches to deep generation that cope wit</context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL), pages 9–16, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Michael Gamon</author>
<author>Eric Ringger</author>
<author>Robert Moore</author>
</authors>
<title>An overview of Amalgam: A machine-learned generation module.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Natural Language Generation Conference (INLG),</booktitle>
<pages>33--40</pages>
<location>New-York, NY, USA.</location>
<contexts>
<context position="16306" citStr="Corston-Oliver et al., 2002" startWordPosition="2590" endWordPosition="2593">on; see, e.g., (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008). However, hardly any addresses the problem of the projection between non-isomorphic semantic and syntactic structures. In general, structure prediction approaches use a single classifier model (Smith, 2011). But see, e.g., (Carreras et al., 2008), who use different models to predict each part of the triplet for spinal model pruning, and (Bj¨orkelund et al., 2010; Johansson and Nugues, 2008), who use a set of classifiers for predicate identification in the context of semantic role labelling. Amalgam (Corston-Oliver et al., 2002), which maps a logical input onto sentences with intermediate syntactic (phrase-based) representation, uses languagespecific decision trees in order to predict when to introduce auxiliaries, determiners, cases, etc. 6 Conclusions We presented two alternative classifier approaches to deep generation that cope with the projection of non-isomorphic semantic and syntactic structures and argued that the micro classifier approach is more adequate. In spite of possible improvements presented in Section 4, each set of micro classifiers achieves results above 86% on the test set. For intra-hypernode de</context>
</contexts>
<marker>Corston-Oliver, Gamon, Ringger, Moore, 2002</marker>
<rawString>Simon Corston-Oliver, Michael Gamon, Eric Ringger, and Robert Moore. 2002. An overview of Amalgam: A machine-learned generation module. In Proceedings of the 2nd International Natural Language Generation Conference (INLG), pages 33–40, New-York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Filippova</author>
<author>Michael Strube</author>
</authors>
<title>Sentence fusion via dependency graph compression.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>177--185</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="15772" citStr="Filippova and Strube, 2008" startWordPosition="2509" endWordPosition="2513">tance, there are 9 different types of verbal objects in SSyntS,7 which capture very specific syntactic properties of Spanish, such as “can the dependent can be replaced by a clitic pronoun? Can the dependent be moved away from its governor? Etc. This kind of information is not of a high relevance for generation of well-formed text. Using a more reduced (more coarse-grained) SSyntS tag set would definitely improve the quality of the projection. 5 Related work There is an increasing amount of work on statistical sentence generation; see, e.g., (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008). However, hardly any addresses the problem of the projection between non-isomorphic semantic and syntactic structures. In general, structure prediction approaches use a single classifier model (Smith, 2011). But see, e.g., (Carreras et al., 2008), who use different models to predict each part of the triplet for spinal model pruning, and (Bj¨orkelund et al., 2010; Johansson and Nugues, 2008), who use a set of classifiers for predicate identification in the context of semantic role labelling. Amalgam (Corston-Oliver et al., 2002), which maps a logical input onto sentences with intermediate synt</context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Katja Filippova and Michael Strube. 2008. Sentence fusion via dependency graph compression. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177–185, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based Semantic Role Labeling of PropBank.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>69--78</pages>
<location>Honolulu, Hawaii.</location>
<contexts>
<context position="16166" citStr="Johansson and Nugues, 2008" startWordPosition="2569" endWordPosition="2572">uld definitely improve the quality of the projection. 5 Related work There is an increasing amount of work on statistical sentence generation; see, e.g., (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008). However, hardly any addresses the problem of the projection between non-isomorphic semantic and syntactic structures. In general, structure prediction approaches use a single classifier model (Smith, 2011). But see, e.g., (Carreras et al., 2008), who use different models to predict each part of the triplet for spinal model pruning, and (Bj¨orkelund et al., 2010; Johansson and Nugues, 2008), who use a set of classifiers for predicate identification in the context of semantic role labelling. Amalgam (Corston-Oliver et al., 2002), which maps a logical input onto sentences with intermediate syntactic (phrase-based) representation, uses languagespecific decision trees in order to predict when to introduce auxiliaries, determiners, cases, etc. 6 Conclusions We presented two alternative classifier approaches to deep generation that cope with the projection of non-isomorphic semantic and syntactic structures and argued that the micro classifier approach is more adequate. In spite of po</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based Semantic Role Labeling of PropBank. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 69–78, Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde-Geary</author>
</authors>
<title>An empirical verification of coverage and correctness for a generalpurpose sentence generator.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Natural Language Generation Conference (INLG),</booktitle>
<pages>17--24</pages>
<publisher>Citeseer.</publisher>
<location>New-York, NY, USA.</location>
<contexts>
<context position="15743" citStr="Langkilde-Geary, 2002" startWordPosition="2507" endWordPosition="2508">y fine-grained. For instance, there are 9 different types of verbal objects in SSyntS,7 which capture very specific syntactic properties of Spanish, such as “can the dependent can be replaced by a clitic pronoun? Can the dependent be moved away from its governor? Etc. This kind of information is not of a high relevance for generation of well-formed text. Using a more reduced (more coarse-grained) SSyntS tag set would definitely improve the quality of the projection. 5 Related work There is an increasing amount of work on statistical sentence generation; see, e.g., (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008). However, hardly any addresses the problem of the projection between non-isomorphic semantic and syntactic structures. In general, structure prediction approaches use a single classifier model (Smith, 2011). But see, e.g., (Carreras et al., 2008), who use different models to predict each part of the triplet for spinal model pruning, and (Bj¨orkelund et al., 2010; Johansson and Nugues, 2008), who use a set of classifiers for predicate identification in the context of semantic role labelling. Amalgam (Corston-Oliver et al., 2002), which maps a logical input onto sen</context>
</contexts>
<marker>Langkilde-Geary, 2002</marker>
<rawString>Irene Langkilde-Geary. 2002. An empirical verification of coverage and correctness for a generalpurpose sentence generator. In Proceedings of the 2nd International Natural Language Generation Conference (INLG), pages 17–24, New-York, NY, USA. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Mille</author>
<author>Alicia Burga</author>
<author>Leo Wanner</author>
</authors>
<title>AnCora-UPF: A multi-level annotation of Spanish.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd International Conference on Dependency Linguistics (DepLing),</booktitle>
<pages>217--226</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="4207" citStr="Mille et al., 2013" startWordPosition="643" endWordPosition="646">and on the possible approaches to the projection of a semantic structure to a syntactic one. 2.1 The Notion of semantic and syntactic structures As semantic structure, we assume a shallow semantic representation that is very similar to the 108 Proceedings of the 8th International Natural Language Generation Conference, pages 108–112, Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics PropBank (Babko-Malaya, 2005) and deep annotations as used in the Surface Realisation Shared Task (Belz et al., 2011): the deep-syntactic layer of the AnCora-UPF corpus (Mille et al., 2013). Deep-syntactic structures (DSyntSs) do not contain any punctuation and functional nodes, i.e., governed prepositions and conjunctions, auxiliaries and determiners.3 As syntactic structure (in the terminology of Ancora-UPF: surface-syntactic structures, SSyntSs), we assume dependency trees in which the nodes are labeled by open or closed class lexemes and the edges by grammatical function relations of the type subject, oblique object, adverbial, modifier, etc.; cf.4 See Figure 1 for a contrastive illustration of DSyntS and SSyntS. II rumor want new song be successful conj a rumor wants that t</context>
<context position="5863" citStr="Mille et al., 2013" startWordPosition="917" endWordPosition="920"> of actions need to be performed: 1. Project each node in the DSyntS onto its SSynScorrespondence. This correspondence can be a single node, as, e.g., successful → successful, or a subtree (hypernode, known as syntagm in linguistics), as, e.g., song → the song ‘DT NN’ (where ‘DT’ is a determiner and ‘NN’ a noun) or be → that will be ‘IN VAUX VB’ (where ‘IN’ is a preposition, ‘VAUX’ an auxiliary and ‘VB’ a full verb). In formal terms, we assume any SSyntScorrespondence to be a hypernode with a cardinality ≥ 1. 2. Generate the correct lemma for the nodes in 3For more details on the SSyntS, see (Mille et al., 2013). 4DSyntSs and their corresponding SSyntSs are stored in the 14-column CoNLL’08 format. SSyntS that do not have a 1:1 correspondence in the SSyntS (as ‘DT’, ‘IN’ and ‘VAUX’ above). 3. Establish the dependencies within the individual SSyntS-hypernodes. 4. Establish the dependencies between the SSyntS-hypernodes (more precisely, between the nodes of different SSyntS-hypernodes) to obtain a connected SSyntS-tree. 3 Classifiers As mentioned in the Introduction, the realization of the actions 1.– 4. can be approached either in terms of 4 generic classifiers (Section 3.1) or in terms of 4 sets of fi</context>
<context position="11058" citStr="Mille et al., 2013" startWordPosition="1777" endWordPosition="1780">achments module, for each hypernode with a distinct internal dependency pattern, a separate classifier is dynamically derived (for our treebank, we obtained 114 different SVM classifiers because it also takes into account hypernodes with just one token). For instance, the classifier for the hypernode [ NN(deep), DT] is most likely to identify as its governor VAUX in the hypernode [ VB(deep), VAUX, IN]; cf. Figure 3. 4 Experiments and Results In this section, we present the performance of the two approaches to DSyntS–SSyntS projection on the DSyntS- and SSynt-layers of the AnCora-UPF treebank (Mille et al., 2013).6 Table 1 displays the results for the generic classifier for all tasks on the development and the test set, while Table 2 displays the results obtained through the sets of micro classifiers. Dev.set # % Hypernode identification 3131/3441 90.99 Lemma generation 818/936 87.39 Intra-hypernode dep. generation 545/798 68.30 Inter-hypernode dep. generation 2588/3055 84.71 Test set # % Hypernode identification 5166/5887 87.75 Lemma generation 1822/2084 87.43 Intra-hypernode dep. generation 1093/1699 64.33 Inter-hypernode dep. generation 4679/5385 86.89 Table 1: Results of the evaluation of the gene</context>
</contexts>
<marker>Mille, Burga, Wanner, 2013</marker>
<rawString>Simon Mille, Alicia Burga, and Leo Wanner. 2013. AnCora-UPF: A multi-level annotation of Spanish. In Proceedings of the 2nd International Conference on Dependency Linguistics (DepLing), pages 217– 226, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
</authors>
<title>Linguistic Structure Prediction. Synthesis Lectures on Human Language Technologies.</title>
<date>2011</date>
<publisher>Morgan</publisher>
<contexts>
<context position="15979" citStr="Smith, 2011" startWordPosition="2539" endWordPosition="2540">from its governor? Etc. This kind of information is not of a high relevance for generation of well-formed text. Using a more reduced (more coarse-grained) SSyntS tag set would definitely improve the quality of the projection. 5 Related work There is an increasing amount of work on statistical sentence generation; see, e.g., (Bangalore and Rambow, 2000; Langkilde-Geary, 2002; Filippova and Strube, 2008). However, hardly any addresses the problem of the projection between non-isomorphic semantic and syntactic structures. In general, structure prediction approaches use a single classifier model (Smith, 2011). But see, e.g., (Carreras et al., 2008), who use different models to predict each part of the triplet for spinal model pruning, and (Bj¨orkelund et al., 2010; Johansson and Nugues, 2008), who use a set of classifiers for predicate identification in the context of semantic role labelling. Amalgam (Corston-Oliver et al., 2002), which maps a logical input onto sentences with intermediate syntactic (phrase-based) representation, uses languagespecific decision trees in order to predict when to introduce auxiliaries, determiners, cases, etc. 6 Conclusions We presented two alternative classifier app</context>
</contexts>
<marker>Smith, 2011</marker>
<rawString>Noah A. Smith. 2011. Linguistic Structure Prediction. Synthesis Lectures on Human Language Technologies. Morgan and Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Varges</author>
<author>Chris Mellish</author>
</authors>
<title>Instancebased Natural Language Generation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<pages>1--8</pages>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="1468" citStr="Varges and Mellish, 2001" startWordPosition="213" endWordPosition="216"> (or stochastic) sentence generation needs to be able to map abstract semantic structures onto syntactic structures. This has been a problem so far since both types of structures differ in their topology and number of nodes (i.e., are non-isomorphic). For instance, a truly semantic structure will not contain any functional nodes,1 while a surface-syntactic structure or a chain of tokens in a linearized tree will contain all of them. Some state-of-the-art proposals use a rule-based module to handle the projection between non-isomorphic semantic and syntactic structures/chains of tokens, e.g., (Varges and Mellish, 2001; Belz, 2008; Bohnet et al., 2011), and some adapt the semantic structures to be isomorphic with syntactic structures (Bohnet et al., 2010). In this paper, we present two alternative stochastic approaches to the projection between non-isomorphic structures, both based on a cascade of Support Vector Machine (SVM) classifiers.2 The first approach addresses the projection as a generic non-isomorphic graph transduction 1See, for instance, (Bouayad-Agha et al., 2012). 2Obviously, other machine learning techniques could also be used. problem in terms of four classifiers for 1. identification of the </context>
</contexts>
<marker>Varges, Mellish, 2001</marker>
<rawString>Sebastian Varges and Chris Mellish. 2001. Instancebased Natural Language Generation. In Proceedings of the 2nd Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 1–8, Pittsburgh, PA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
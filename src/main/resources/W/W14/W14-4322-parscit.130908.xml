<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9976735">
In-depth Exploitation of Noun and Verb Semantics
to Identify Causation in Verb-Noun Pairs
</title>
<author confidence="0.977742">
Mehwish Riaz and Roxana Girju
</author>
<affiliation confidence="0.859233666666667">
Department of Computer Science and Beckman Institute
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
</affiliation>
<email confidence="0.99867">
{mriaz2,girju}@illinois.edu
</email>
<sectionHeader confidence="0.997381" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999430631578948">
Recognition of causality is important to
achieve natural language discourse under-
standing. Previous approaches rely on
shallow linguistic features. In this work,
we propose to identify causality in verb-
noun pairs by exploiting deeper seman-
tics of nouns and verbs. Particularly, we
acquire and employ three novel types of
knowledge: (1) semantic classes of nouns
with a high and low tendency to encode
causality along with information regard-
ing metonymies, (2) data-driven seman-
tic classes of verbal events with the least
tendency to encode causality, and (3) ten-
dencies of verb frames to encode causal-
ity. Using these knowledge sources, we
achieve around 15% improvement in F-
score over a supervised classifier trained
using linguistic features.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997991666666667">
The identification of cause-effect relations is crit-
ical to achieve natural language discourse under-
standing. Causal relations are encoded in text us-
ing various linguistic constructions e.g., between
two verbs, a verb and a noun, two discourse seg-
ments, etc. In this research, we focus on identify-
ing causality encoded between a verb and a noun
(or noun phrase). For example, consider the fol-
lowing example:
</bodyText>
<listItem confidence="0.335921">
1. At least 1,833 people died in the hurricane.
</listItem>
<bodyText confidence="0.998126326530613">
In example (1), the verb-noun phrase pair
“died”-“the hurricane” encodes causality where
event “died” is the effect of “hurricane” event.
Previously several approaches have been pro-
posed to identify causality between two verbs
(Bethard and Martin, 2008; Riaz and Girju, 2010;
Do et al., 2011; Riaz and Girju, 2013) and dis-
course segments (Sporleder and Lascarides, 2008;
Pitler and Nenkova, 2009; Pitler et al., 2009).
However, the problem of identifying causality in
verb-noun pairs has not received a considerable
attention. For example, Do et al. (2011) have
studied this task but they worked only with a list
of predefined nouns representing events. In this
work, we focus on the linguistic construction of
verb-noun (or noun phrase) pairs where noun can
be of any semantic type.
Traditional approaches for identifying causal-
ity mainly employ linguistic features (e.g., lexical
items, part-of-speech tags of words, etc.) in the
framework of supervised learning (Girju, 2003;
Sporleder and Lascarides, 2008; Bethard and Mar-
tin, 2008; Pitler and Nenkova, 2009; Pitler et al.,
2009) and do not involve deeper semantics of lan-
guage. Analysis of such approaches by Sporleder
and Lascarides (2008) have revealed that the lin-
guistic features are not always sufficient to achieve
a good performance on the task of identifying se-
mantic relations including causality. In this work,
we propose a model that deeply processes and
acquires the specific semantic information about
the participants of a verb-noun phrase (v-np) pair
(i.e., noun and verb semantics) to identify causal-
ity with a better performance over the baseline
model depending merely on shallow linguistic fea-
tures.
The work in this paper builds on our recent work
reported in Riaz and Girju (2014). In that previ-
ous model, we identified the semantic classes of
nouns and verbs with a high and low tendency to
encode causation. For example, a named entity
such as LOCATION may have the least tendency
to encode causation. We leveraged such informa-
tion about nouns to filter false positives. Sim-
ilarly, we utilized the TimeBank’s (Pustejovsky
et al., 2006) classification of verbal events (i.e.,
Occurrence, Perception, Aspectual, State, I State,
I Action and Reporting) and their definitions to
claim that the reporting events (e.g., say, tell, etc.)
</bodyText>
<page confidence="0.976534">
161
</page>
<note confidence="0.7313435">
Proceedings of the SIGDIAL 2014 Conference, pages 161–170,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.953461897435897">
just describe and narrate other events instead of
encoding causality with them. We proposed an In-
teger Linear Programming (ILP) model (Roth and
Yih, 2004; Do et al., 2011) to combine noun and
verb semantics with the decisions of a supervised
classifier which only relies on linguistic features.
In this paper, we extend our previous model by
acquiring and exploiting the following three novel
types of knowledge:
1. We learn the information about tendencies of
various verb frames to encode causation. For
example, our model identifies if the subject of
verb “destroy” (“occur”) has a high (low) ten-
dency to encode causation. Such information
helps gain performance by exploiting causal
semantics of each verb frame separately. We
also learn and incorporate information about
the verb frames in general e.g., how likely it is
for the subject of any verb to encode causation
with its verb.
2. In Riaz and Girju (2014), we utilized the Time-
Bank’s definition of reporting events to argue
that such events have the least tendency to en-
code causation. Instead of relying on human
judgment we now introduce a data intensive ap-
proach to identify the TimeBank’s classes of
events with the least tendency to encode cau-
sation.
3. Although, information about the nouns with
the least tendency to encode causation helps to
filter false positives it can lead to false nega-
tives when metonymic readings are associated
with such nouns. Therefore, we introduce a
metonymy resolver on top of our current model
to avoid false negatives.
We provide details of our previously proposed
model in section 3. We introduce new model and
discuss its performance in sections 4 and 5. Sec-
tion 6 concludes the current research.
</bodyText>
<sectionHeader confidence="0.998589" genericHeader="introduction">
2 Relevant Work
</sectionHeader>
<bodyText confidence="0.999692551020408">
In Natural Language Processing (NLP), re-
searchers are showing lots of interest in the task
of identifying causality due to its various applica-
tions e.g., question answering (Girju, 2003), sum-
marization (Chklovski and Pantel, 2004), future
prediction (Radinsky and Horvitz, 2013), etc.
Several approaches have been proposed to iden-
tify causality in pairs of verbal events (Bethard
and Martin, 2008; Riaz and Girju, 2010; Do et
al., 2011; Riaz and Girju, 2013) and discourse
segments (Sporleder and Lascarides, 2008; Pitler
and Nenkova, 2009; Pitler et al., 2009). However
causality a pervasive relation of language can be
encoded via various linguistic constructions. For
example, verbs and nouns are the key components
of language to represent events. Therefore in this
work we focus on identifying causality in verb-
noun pairs.
Previously researchers have followed the path
of utilizing linguistic features in the framework
of supervised learning (Girju, 2003; Bethard and
Martin, 2008; Sporleder and Lascarides, 2008;
Pitler and Nenkova, 2009; Pitler et al., 2009).
Though linguistic features are important but other
sources of knowledge are also critically required
to achieve progress on the current task.
In recent years, researchers have proposed un-
supervised metrics to identify causality between
events (Riaz and Girju, 2010; Do et al., 2011).
For example, Riaz and Girju (2010) and Do et
al. (2011) introduced unsupervised metrics to
learn causal dependencies between events. These
metrics mainly depend on probabilities of co-
occurrences of events and do not distinguish well
causality from any other types of correlation (Riaz
and Girju, 2013). In order to overcome this prob-
lem Riaz and Girju (2013) proposed some ad-
vanced metrics which combine probabilities of co-
occurrences of events with the supervised esti-
mates of cause and non-cause relations.
Considering the importance of employing rich
sources of knowledge other than linguistic features
for the current task, we have recently proposed a
model that incorporates semantic classes of nouns
and verbs with a high and low tendency to encode
causation (Riaz and Girju, 2014). In this work, we
exploit information about verb frames, data-driven
verb semantics and metonymies to achieve more
progress on our recent work.
</bodyText>
<sectionHeader confidence="0.99867" genericHeader="method">
3 Model for Recognizing Causality
</sectionHeader>
<bodyText confidence="0.9998359">
In this section we provide an overview of our pre-
vious model (Riaz and Girju, 2014) for identifying
causality in v-np pairs where v (np) stands for verb
(noun phrase). This model works in the following
two stages: (1) A supervised classifier is used to
make binary predictions (i.e., the label cause (C)
or non-cause (¬C)) employing linguistic features,
and (2) noun and verb semantics are then com-
bined with the predictions of supervised classifier
in the ILP framework to identify causality.
</bodyText>
<page confidence="0.995869">
162
</page>
<subsectionHeader confidence="0.998768">
3.1 Supervised Classifier
</subsectionHeader>
<bodyText confidence="0.999984470588235">
To the best of our knowledge, there is no data
set of v-np pairs with the labels C and ¬C avail-
able to us. For the current task we employ some
heuristics to extract a training corpus of v-np pairs
using FrameNet (Baker et al., 1998). FrameNet
provides frame elements for the verbs and hand
annotated examples (aka annotations) of these
frame elements. Consider the following annota-
tion from FrameNet “They died [Causefrom shot-
gun wounds]” where the frame element “Cause”
is given for the verb “died”. We remove the prepo-
sition “from” from the above annotation of frame
element to acquire an instance of v-np (i.e., died-
shotgun wounds) pair. We extract all annotations
for verbs from FrameNet in which a frame element
must contain at least one noun and no verb in it.
We found such annotations for 729 distinct frame
elements. We manually assigned the labels C and
¬C to these frame elements. Cause, Purpose, Rea-
son, Result, Explanation are some examples of the
frame elements to which we assigned the label C.
Using the above mentioned assignments of labels
C and ¬C to frame elements, we have acquired
a training corpus of 4,141 (77,119) C (¬C) in-
stances from FrameNet. In order to avoid class im-
balance while training we employ an equal num-
ber of instances of both labels.
Due to space constraints, we refer the reader to
Appendix A for the details of linguistic features
to build the supervised classifier. We employ both
Naive Bayes (NB) and Maximum Entropy (Max-
Ent) algorithms to acquire predictions and prob-
abilities of assignments of labels. We set up the
following ILP using these probabilities:
</bodyText>
<equation confidence="0.9933445">
EZ1 = max E x1(v-np, l)P(v-np, l) (1)
v-npEI lEL1
</equation>
<subsectionHeader confidence="0.996723">
3.2 Noun and Verb Semantics
</subsectionHeader>
<bodyText confidence="0.9999602">
We automatically acquire and employ semantic
classes of nouns and verbs with a high and low
tendency to encode causation. Such information
helps to reduce errors in predictions of the super-
vised classifier.
We derive two semantic classes of nouns for our
purpose i.e., Cnp and ¬Cnp where the class Cnp
(¬Cnp) represents the noun phrases with a high
(low) tendency to encode causation. For exam-
ple, a noun phrase expression for a location has
the least tendency to encode causation unless a
metonymic reading is associated with it. In or-
der to acquire these classes, we extract annotations
of 936 distinct frame elements from FrameNet
in which a frame element must contain at least
one noun and no verb in it. These annotations
of frame elements roughly represent instances of
noun phrases (np). We manually assigned the la-
bels Cnp and ¬Cnp to the frame elements. For
example, we assign the label ¬Cnp to the frame
element “Place” which represents a location (see
Appendix B for some examples of the frame ele-
ments with labels Cnp and ¬Cnp). We also fol-
low the approach similar to Girju and Moldovan
(2002) to employ WordNet senses of nouns to ac-
quire more instances of the classes Cnp and ¬Cnp
(see Appendix B for the details). We have ac-
quired a total of 280,212 instances of np (50%
for each of the two classes i.e., Cnp and ¬Cnp)
using both FrameNet and WordNet. Using these
instances, we build a supervised classifier to iden-
tify the semantic class of np (see Appendix B for
the details of features to build the classifier). We
incorporate the knowledge of semantic classes of
nouns by making the following additions to ILP:
</bodyText>
<equation confidence="0.9996915">
EZ2 = Z1 + E x2(fnp(v-np), l)P(fnp(v-np), l)
E x1(v-np, l) = 1 V v-np E I (2) v-npEI−M lEL2 (4)
lEL1
x1(v-np, l) E {0, 1} V v-np E I Vl E L1 (3)
</equation>
<bodyText confidence="0.9961532">
Here, L1 = {C, ¬C}, I is the set of all v-np pairs.
x1(v-np, l) is a binary decision variable set to 1
only if the label l ∈ L1 is assigned to a v-np pair
and only one label out of |L1 |choices can be as-
signed to a v-np pair (see constraints 2 and 3). In
particular, we maximize the objective function Z1
(1) assigning the labels l ∈ {L1} to v-np pairs de-
pending on the probabilities of assignments (i.e.,
P(v-np, l)) obtained through the supervised clas-
sifier.
</bodyText>
<equation confidence="0.999648333333333">
E x2(fnp(v-np), l) = 1 V v-np E I − M (5)
lEL2
x2(fnp(v-np), l) E {0, 1} V v-np E I − M (6)
Vl E L2
x1(v-np, -C) − x2(fnp(v-np), -Cnp) &gt; 0 (7)
V v-np E I − M
</equation>
<bodyText confidence="0.992537">
Here L2 = {Cnp, ¬Cnp}. fnp(v-np) is a func-
tion which returns np of a v-np pair. M is the set
of v-np pairs with metonymic readings associated
with np. Currently, this set is empty and in sec-
tion 4.3 we introduce a metonymy resolver to pop-
</bodyText>
<page confidence="0.994711">
163
</page>
<bodyText confidence="0.992079212765958">
ulate this set. x2(fnp(v-np), l) is a binary decision
variable set to 1 only if the label l E L2 is assigned
to np and only one label out of |L2 |choices can
be assigned to np (see constraints 5 and 6). Con-
straint 7 enforces that if an np belongs to the class
¬Cnp then its corresponding v-np pair is assigned
the label ¬C. In particular, we maximize the ob-
jective function Z2 (4) subject to the constraints
introduced till now. For each v-np pair, we predict
the semantic class of np using our supervised clas-
sifier for the labels l E L2 and set the probabilities
– i.e., P(fnp(v-np), l) = 1, P(fnp(v-np), {L2} −
{l}) = 0 if the label l E L2 is assigned to np. Also
before running our supervised classifier, we run a
named entity recognizer (Finkel et al., 2005) and
assign the label ¬Cnp to all noun phrases identi-
fied as named entities. We also determine associa-
tion of metonymies with the noun phrases identi-
fied as named entities.
For the current task we also acquire two seman-
tic classes of verbs i.e., Cev and ¬Cev where the
class Cev (¬Cev) contains the verbal events with a
high (low) tendency to encode causation. In order
to derive these two classes we exploit the Time-
Bank corpus (Pustejovsky et al., 2003) which pro-
vides seven semantic classes of verbal events – i.e.,
Occurrence, Perception, Aspectual, State, I State,
I Action and Reporting. According to the defini-
tions of these classes, we claim that the report-
ing events (e.g., say, tell, etc.) just describe and
narrate other events instead of encoding causality
with them. Using this claim, we consider that all
instances of reporting verbal events of TimeBank
belong to the class ¬Cev and the rest of instances
of verbal events lie in the class Cev. After ac-
quiring instances of the classes Cev and ¬Cev, we
build a supervised classifier for these two classes.
We use the features introduced by Bethard and
Martin (2006) to build this classifier (see Bethard
and Martin (2006) for the details). Employing pre-
dictions and probabilities of assignments of the la-
bels Cev and ¬Cev we add the following two con-
straints to ILP: (1) if the event represented by v
belongs to ¬Cev then the corresponding v-np pair
must be labeled with ¬C and (2) if a v-np pair is
a causal pair then the event represented by v must
be labeled with Cev.
</bodyText>
<sectionHeader confidence="0.960169" genericHeader="method">
4 Enriched Verb and Noun Semantics
</sectionHeader>
<bodyText confidence="0.9999855">
This section describes the novel contributions of
this work i.e., identification of semantics of verb
frames, semantic classes of verbal events via a data
intensive approach and association of metonymic
readings with noun phrases to identify causality
with a better performance.
</bodyText>
<subsectionHeader confidence="0.994171">
4.1 Verb Frames
</subsectionHeader>
<bodyText confidence="0.9998935">
We introduce a method to acquire tendencies of
various verb frames to encode causation. Consider
the following two examples to understand the ten-
dencies of verb frames of form {v, gr} to encode
causation where v is the verb and gr is the gram-
matical relation of np with the verb v.
</bodyText>
<listItem confidence="0.9684978">
1. The Great Storm of October 1987 almost totally de-
stroyed the eighty year old pinetum at Nymans Garden
in Sussex. (Cause (C))
2. The explosion occurred in the city’s main business area.
(Non-Cause (¬C))
</listItem>
<bodyText confidence="0.99751875">
In above two examples the nps “The Great
Storm of October 1987” and “The explosion” have
the grammatical relations of subject with the verbs
“destroyed” and “died”. In examples (1) and (2)
the verb frames {destroy, subject} and {occur,
subject} encode cause and non-cause relations.
These examples reveal that each verb frame has
its own tendency to encode causation. This type
of knowledge helps gain performance by exploit-
ing the semantics of each verb frame separately.
We leverage FrameNet annotations to acquire
such type of knowledge. We collect all annota-
tions of verbs from FrameNet and assign the la-
bels C and ¬C to the frame elements as discussed
in section 3.1. In FrameNet, example (1) is given
as follows:
</bodyText>
<listItem confidence="0.993888333333333">
3. [Cause The Great Storm of October 1987] [Degree almost
totally] destroyed [Undergoer the eighty year old pinetum
at Nymans Garden in Sussex].
</listItem>
<bodyText confidence="0.999865733333333">
According to our assignments of labels C and
¬C to the frame elements, example (1) is given
as “[C The Great Storm of October 1987] [,C al-
most totally] destroyed [,C the eighty year old
pinetum at Nymans Garden in Sussex].”. After ac-
quiring instances of the labels C and ¬C from ex-
ample (1), we populate the fields of a knowledge
base of verb frames (see Table 1). Fields of this
knowledge base are {v, gr}, count({v, gr}, C) and
count({v, gr}, ¬C). gr is the dependency relation
of the frame element with the verb v. We use Stan-
ford’s dependency parser (Marneffe et al., 2006)
to collect dependency relations. count({v, gr}, C)
(count({v, gr}, ¬C)) is the count of the label C
(¬C) of the frame {v, gr}. As shown in Table 1,
</bodyText>
<page confidence="0.993889">
164
</page>
<bodyText confidence="0.999504357142857">
for the frame element “The Great Storm of Octo-
ber 1987”, the word “Storm” has the dependency
relation of “nsubj” with the verb “destroy”. If
there exists more than one dependency relations
between the frame element and its verb then we
choose the very first relation in the text order. Ac-
cording to the counts given in Table 1, {destroy,
nsubj} has more tendency to encode a cause re-
lation than the non-cause one. We have acquired
7,156 and 114,898 instances of the labels C and
¬C from FrameNet for populating the knowledge
base of verb frames. We compute tendencies of
verb frames to encode causality using the follow-
ing scores:
</bodyText>
<equation confidence="0.997418">
S({v, gr}, l) = S1({v, gr}, l) x S2({*, gr}, l) (8)
count({v,gr},l)
S1({v, gr}, l) =
count({v,gr},l)+count({v,gr},L1−{l})
* count({*,gr},l)
S2({ , gr}, l) = count({*,gr},l)+count({*,gr},L1−{l})
</equation>
<bodyText confidence="0.999890916666667">
Counts of first component (S1) can be taken
from the knowledge base of verb frames of form
{v, gr}. The second component (S2) with counts
count({*, gr}, l) and count({*, gr}, L1 − {l})
captures tendencies of verb frames in general.
For example, what is the tendency of any subject
to encode causality with its verb i.e., the score
S2({∗, nsubj}, C). We populate the knowledge
base of Table 1 with equal number of C and ¬C
instances to calculate counts for S2. We make the
following additions to ILP to incorporate informa-
tion about verb frames:
</bodyText>
<equation confidence="0.979541157894737">
�Z3 = Z2 + E x3(g(v-np), l)S(g(v-np), l)
v-np∈I∧ l∈L1
g(v-np)∈KB∧
fnp(v-np)∈Cnp
(9)
v-np∈I∧
g(v-np)∈KB∧ (10)
fnp(v-np)∈Cnp
v-np∈I∧
x3(g(v-np), l) E {0, 1} ∀l E L1, ∀ g(v-np)∈KB∧ (11)
fnp(v-np)∈Cnp
x3(g(v-np), l) &lt; x1(v-np, l) ∀l E L1, (12)
v-np∈I
∀ ∧g(v-np)∈KB
∧ fnp(v-np)∈Cnp
x1(v-np, l) &lt; x3(g(v-np), l) ∀l E L1, (13)
v-np∈I∧
∀g(v-np)∈KB∧
fnp(v-np)∈Cnp
</equation>
<bodyText confidence="0.917088">
Here, KB is the knowledge base of verb frames
and g(v-np) is the function which returns the verb
frame i.e., {v, gr}. This function returns NULL
value if there is no grammatical relation between
v and np in an instance. The above changes in
ILP are only applicable for the v-np pairs with
{v, gr} count({v, gr}, C) count({v, gr}, ,C)
{destroy,nsubj} 1 0
{destroy,advmod} 0 1
{destroy,dobj} 0 1
[C The Great Storm of October 1987] [¬C almost totally] de-
stroyed [¬C the eighty year old pinetum at Nymans Garden in
Sussex].
</bodyText>
<tableCaption confidence="0.904235">
Table 1: A knowledge base of verb frames. This
</tableCaption>
<bodyText confidence="0.969982842105263">
knowledge base is populated using the instances
of C and ¬C labels given in this table.
g(v-np) E KB and np identified as of class Cnp
because we have already filtered the cases of np E
¬Cnp in section 3.2. x3(g(v-np), l) is a binary de-
cision variable set to 1 only if the label l E L1
is assigned to g(v-np) and only one label out of
|L1 |choices can be assigned to g(v-np) (see con-
straints 10 and 11). We add information about verb
frames using constraints 12 and 13. These con-
straints enforce the predictions of the supervised
classifier of causality (section 3.1) to be consis-
tent with the predictions using tendencies of verb
frames (i.e., score S({v, gr}, l)). We maximize
objective function (9) subject to the above con-
straints. We remove those {v, gr} from KB which
have count({v, gr}, C)+count({v, gr}, ¬C) &lt; 5
to avoid wrong predictions based on the small
counts of verb frames.
</bodyText>
<subsectionHeader confidence="0.99121">
4.2 Data-driven Verb Semantics
</subsectionHeader>
<bodyText confidence="0.999919708333333">
In section 3.2 we considered that reporting events
belong to the class ¬Cev with the least tendency
to encode causation using the definition of these
events in the TimeBank corpus. Instead of re-
lying on definitions of events we now introduce
a data intensive approach to automatically iden-
tify the class ¬Cev of verbal events. In order
to identify this class we extract training instances
of verbal events encoding C and ¬C relations.
Verbal events encode cause-effect relations using
verb-verb (e.g., Five shoppers were killed when a
car blew up.) and verb-noun linguistic construc-
tions. Therefore for the current purpose we use
the following two types of training instances: (A)
a training corpus of 240K instances of verb-verb
(vi-vj) pairs encoding C and ¬C relations (named
as Trainingvi-vj) (we refer the reader to Riaz and
Girju (2013) for the details of this training corpus)
and (B) the training corpus v-np instances intro-
duced in section 3.1 (named as Trainingv-np).
Following is the procedure to derive V,C ⊆
V where V={Occurrence, Perception, Aspectual,
State, I State, I Action, Reporting} and the set
V,C contains the TimeBank’s semantic classes
</bodyText>
<equation confidence="0.997174">
E x3(g(v-np), l) = 1 ∀
l∈L1
</equation>
<page confidence="0.985194">
165
</page>
<bodyText confidence="0.923823">
with the least tendency to encode a cause relation.
</bodyText>
<listItem confidence="0.953719428571429">
1. Input: Training corpus, V
2. Output: Set V¬C
3. For each training instance k employ the supervised clas-
sifier of Bethard and Martin (2006) to do the following:
(a) if k E Trainingvi-vj then identify the semantic class
(sc) of both events represented by both verbs vi and
vj and add this information to a set i.e., T = T U
(kvi, scvi, l) U (kvj, scvj, l) where scvi is the se-
mantic class of event of the verb vi of instance k
and l E {C, -C}.
(b) Else if k E Trainingv-np then identify the semantic
class (sc) of event represented by the verb v and set
T = T U (kv, scv, l).
4. Using results of step 3, calculate tendency of each se-
</listItem>
<bodyText confidence="0.9290886">
mantic class sc E V to encode non-causality (i.e.,
score(sc, -C)) as follows:
where count(m, n) is the number of instances of verbal
events with the labels m and n and count(m) is the number
of instances of verbal events with the label m.
</bodyText>
<listItem confidence="0.783319666666667">
5. Acquire a ranked list of semantic classes listsc = [sc1, sc2,
... scm] s.t. score(sci, -C) &gt; score(sci+1, -C). From
this list we remove the class sci if either score1(sci, -c)
</listItem>
<bodyText confidence="0.81734025">
&lt; 0 or score2(sci, -c) &lt; 0.
� The following steps are used to determine the cutoff
class sci E listsc s.t. the semantic classes {sc1, sc2, ...,
sci-1} have the least tendency to encode causation.
</bodyText>
<listItem confidence="0.983791090909091">
6. resultsc−1 = 0 and resultsc0 = 0.
7. Remove sci from the front of listsc and do the following:
(c) Predict the label (l) -C for all tuples of form
(m, sc, l) E T if sc E {sc1, sc2, ..., sci} and pre-
dict C for the rest of the tuples.
(d) Using predictions from step (c), calculate the
resultsci = F1-score x accuracy for the label l E
{C, -C}.
(e) If resultsci−resultsci-1 &lt; resultsci-1−resultsci-2
then output {sc1, sc2, ..., sci−1}
(f) Else go to step 7.
</listItem>
<bodyText confidence="0.999596875">
Using the above procedure, we obtain the
sets {Aspectual} and {Reporting, I State} with
Trainingvi-vj and Trainingv-np corpora. We con-
sider that the Aspectual, Reporting and I State
events of the TimeBank corpus belong to the class
¬Cev and rest of the events lie in Cev. Using these
semantic classes we apply the constraints intro-
duced in section 3.2.
</bodyText>
<subsectionHeader confidence="0.994647">
4.3 Metonymy Resolution:
</subsectionHeader>
<bodyText confidence="0.939807849056604">
Metonymy resolution is the task to determine if a
literal or non-literal reading is associated with a
{v, gr} count({v, gr}, Cnp) count({v, gr}, ¬Cnp)
{kill,nsubj} 1 0
{kill,dobj} 0 1
[CnpPissed off Angelus] just kills [¬Cnpme]
Table 2: A knowledge base of verb frames. This
knowledge base is populated using the instances
of Cnp and ¬Cnp labels given in this table.
natural language expression (Markert and Nissim,
2009). Consider the following example:
4. The United States has killed Osama bin Laden and has
custody of his body. (Cause (C))
In example (4) “The United States” refers to a
non-literal reading i.e., the event of “raid in Ab-
bottabad on May 2, 2011 by the United States”
rather than merely referring to a literal sense i.e.,
a country. The association of non-literal reading
with “The United States” results in killing event.
Previously, researchers have worked with hand-
annotated selectional restrictions violation for this
task (Markert and Nissim, 2009). In the exam-
ple (4) a country cannot “kill” someone and thus
a metonymic reading is associated with it. In this
work we identify association of metonymies with
noun phrases via verb frames and prepositions as
explained below in this section.
In the first part of our approach we employ
violations of tendencies of verb frames to iden-
tify if a non-literal reading is associated with a
noun phrases. Particularly, we build a knowledge
base of verb frames using Cnp and ¬Cnp classes
as discussed in section 4.1. Consider the knowl-
edge base given in Table 2 populated using the
following FrameNet annotations “[StimulusPissed
off Angelus] just kills [Experiencerme].” with as-
signments of labels Cnp and ¬Cnp to the frame
elements. We populate the knowledge base using
only those FrameNet annotations in which a frame
element does not contain a verb.
Now we introduce our method to identify the
association of non-literal reading with the “The
United States” in example (4). The supervised
classifier predicts the class ¬Cnp for the np “The
United States”. However, in the current state
of knowledge base (Table 2) P({destroy, nsubj},
Cnp) &gt; P({destroy, nsubj}, ¬Cnp) where P is
the probability. The prediction of ¬Cnp for “The
United States” violates the above probabilities.
Considering this violation, we predict the associa-
tion of metonymy with np.
In the second part of our approach we iden-
tify tendencies of prepositions to encode causation
</bodyText>
<equation confidence="0.9998052">
score(sc, -C) = score1(sc, -C) x score2(sc, -C)
count(sc, -C)
count(-C)
count(sc, C) )
count(sc)
count(C) )
score1(sc, -C) = ( count(sc, -C)
count(sc)
score2(sc, -C) = (
count(sc, C)
</equation>
<page confidence="0.992641">
166
</page>
<bodyText confidence="0.999556833333333">
and use violation of these tendencies to identify
metonymies. For this purpose, we use the training
corpus of v-np pairs with 4,141 C and 77,119 -C
training instances (see section 3.1). We employ
only those training instances in which a preposi-
tion appears between v and np and there appears
no verb between them. From these instances, we
acquire a set of prepositions that appear between
v and np. Using this set of prepositions (PR) as
input to the following procedure, we acquire a set
of prepositions (PRC) with the highest tendency
to encode causation:
</bodyText>
<listItem confidence="0.99451975">
1. Input: Training Corpus of v-np pairs, PR
2. Output: PRC
3. Calculate tendency of each preposition pr E PR to en-
code causality (i.e., score(pr, C)) as follows:
</listItem>
<equation confidence="0.99907625">
score(pr, C) = score1(pr, C) x score2(pr, C)
score1(pr, C) = (count(pr, C) count(pr)
count(pr, C)
score2(pr C) = ( count(C)
</equation>
<listItem confidence="0.988890333333333">
4. Acquire a ranked list of prepositions listpr = [pr1, pr2, .. .
prm] s.t. score(pri, C) &gt; score(pri+1, C). From this
list we remove pri if either score1(pri, C) or score2(pri,
C) &lt; 0.
5. resultpr−1 = 0, resultpr0 = 0
6. Remove pri from the front of the listpr and do the follow-
ing:
(a) Predict the label C for all v-np training instances
with pr E {pr1, pr2, ..., pri} and assign the label
-C to the rest of the instances.
(b) Using predictions from step (a) calculate the
resultpr; = F1-score x accuracy.
(c) If resultpr;-resultpr;-1 &lt; resultpr;-1-resultpr;-2 then
output {pr1, pr2, ..., pri−1}.
(d) Else go to step 6.
</listItem>
<bodyText confidence="0.999970722222222">
The above procedure outputs the set PRC =
{for, by}. Now we introduce method to identify
association of non-literal reading for the example
“All weapon sites in Iraq were destroyed by the
United States” where “the United States” E -Cnp
as identified by the supervised classifier. However,
the preposition “by” has a high tendency to en-
code causation and thus “the United States” may
encode causation. Therefore, there is a possibil-
ity that this noun phrase has a non-literal sense
attached to it which results in encoding causality.
Using this method, we predict metonymies only
for the v-np instances where preposition appears
between v and np and there appears no verb be-
tween them. If any of two methods of metonymy
resolution predicts the association of metonymy
with np then we add v-np to the set M used in
ILP (see section 3.2).
</bodyText>
<sectionHeader confidence="0.971418" genericHeader="method">
5 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.999969619047619">
In this section we present experiments and discus-
sion on the performance achieved for the current
task. In order to evaluate our model, we generated
a test set of instances of v-np pairs. For this pur-
pose, we collected three wiki articles on the topics
of Hurricane Katrina, Iraq War and Egyptian Rev-
olution of 2011. We apply a part-of-speech tagger
and a dependency parser on all sentences of these
three articles (Toutanova et al., 2003; Marneffe et
al., 2006). We extracted all v-np pairs from each
sentence of these articles. For each of the these
three articles, we selected first 500 instances of v-
np pairs. Two annotators were asked to provide the
labels C and -C to the instances of v-np pairs us-
ing the annotation guidelines from Riaz and Girju
(2010). We have achieved a 0.64 kappa score for
the human inter-annotator agreement on a total of
1,500 v-np instances. This results in a total of
1,365 instances of v-np pairs with 11.86% C pairs.
In this section, we present performance of the
following models (see Table 3):
</bodyText>
<listItem confidence="0.96305075">
1. Baseline: NB and MaxEnt (McCallum, 2002)
supervised classifiers using only the shallow
linguistic features (see section 3.1).
2. Basic noun and verb semantics: ILP with
the addition of semantic classes of nouns
without metonymy (denoted by +N! M) and
the addition of semantic classes of verbs
where -Cev={(R)eporting events} (denoted by
+N! M+V{R}). These models represent the
work proposed in Riaz and Girju (2014) (sec-
tion 3).
3. Noun semantics with metonymies: ILP
</listItem>
<bodyText confidence="0.670222888888889">
with the addition of noun semantics involv-
ing metonymies resolved via verb frames (de-
noted by +NM1), metonymies resolved via verb
frames {v, gr} where gr E GR = {csubj, csub-
jpass, nsubj, nsubjpass, xsubj, dobj, iobj, pobj,
agent} a set of core dependency relations of
subjects and objects (denoted by +NM1GR ) and
metonymies resolved via both verb frames and
prepositions (denoted by +NM1GR + M2).
</bodyText>
<listItem confidence="0.997058375">
4. Verb frames and data-driven verb seman-
tics: ILP with the addition of information about
verb frames (denoted by +NM+VF where M =
M1GR + M2), data-driven verb semantics i.e.,
-Cev ={(A)spectual, (R)eporting, (I) (S)tate
events} (denoted by +NM+V{A,R,rS}) and both
verb frames and data-driven verb semantics
(denoted by +NM+VF+V{A,R,rS})
</listItem>
<equation confidence="0.98945225">
count(pr, -C) )
count(pr)
count(pr, -C)
count(-C) )
</equation>
<page confidence="0.993873">
167
</page>
<table confidence="0.998725666666667">
S B +Ni M +Ni M+VIRI +NM1 +NM1GR +NM1GR + M2 +NM+VF +NM+VIA,R,ISI +NM+VF +VIA,R,ISI
A 28.86 71.86 73.40 71.35 71.42 71.64 72.96 75.16 76.19
P 13.52 26.18 27.21 26.29 26.34 27.54 28.39 29.93 30.82
R 92.59 75.30 74.07 78.39 78.39 85.18 83.95 81.48 80.86
F 23.60 38.85 39.80 39.37 39.44 41.62 42.43 43.78 44.63
A 61.46 80.73 81.17 80.65 80.73 81.02 81.39 81.75 82.05
P 19.46 32.02 32.72 32.41 32.52 34.09 34.66 35.25 35.64
R 71.60 55.55 55.55 58.02 58.24 64.19 64.19 64.19 63.58
F 30.60 40.63 41.18 41.59 41.68 44.53 45.02 45.51 45.67
</table>
<tableCaption confidence="0.8713038">
Table 3: Performance of (B)aseline, +N! M, +N! M+V{R}, +NM1, +NM1GR, +NM1GR + M2, +NM+VF,
+NM+V{A,R,IS} and +NM+VF+V{A,R,IS} (see text for details) in terms of (S)cores of (A)ccuracy,
(P)recision, (R)ecall, (F)-score. The row 1 (2) of this table presents results over NB (MaxEnt) base-
line supervised classifier, respectively.
Table 3 shows that MaxEnt gives a very high ac-
</tableCaption>
<bodyText confidence="0.930615078947368">
curacy and F-score as compared with NB. Model
+N! M+V{R} with basic noun and verb seman-
tics introduced in section 3.2 results in more than
10% improvement in F-score over NB and Max-
Ent classifiers relying only on shallow linguis-
tic features. Model +NM+VF+V{A,R,IS} with en-
riched verb and noun semantics brings more than
4% improvement in F-score over +N! M+V{R}
with MaxEnt as baseline. We perform statis-
tical significance test using bootstrap sampling
method given in Berg-Kirkpatrick et al. (2012)
(see Berg-Kirkpatrick et al. (2012) for the de-
tails). +NM+VF+V{A,R,IS} brings significant im-
provement in F-score over +N! M+V{R} with p-
value 0.0.
Though +N! M gives significantly better F-score
over baseline, it drops recall by more than 16%.
Metonymy resolution helps perform quite bet-
ter by recovering more than 8% recall with
+NM1GR + M2 over +N! M. +NM 1GR + M2 also re-
sults in 3.9% improvement in F-score over +N! M
with MaxEnt as baseline model (significant im-
provement with p-value 0.0). Metonymies re-
solved via verb frames with all and core grammat-
ical relations (i.e., set GR) recover more than 2%
recall and slightly improve F-score.
Model with the addition of information of verb
frames (i.e.,+NM+VF) brings 0.49% improve-
ment in F-score over +NM1GR + M2 using Max-
Ent as baseline model (significant improvement
with p-value 0.027). Model with the addition of
data-driven verb semantics (i.e., +NM+V{A,R,IS})
results in 0.98% improvement in F-score over
+NM1GR + M2 using MaxEnt as baseline model
(significant improvement with p-value 0.0021).
Overall the model +NM+VF+V{A, R, IS} yields
more than 16% (20%) F-score (accuracy) over the
baseline models build via NB and MaxEnt.
</bodyText>
<subsectionHeader confidence="0.83066">
5.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999969038461538">
We performed error analysis for the model
+NM+VF+V{A,R,IS} by randomly selecting 50
False Positives (FP) and 50 False Negatives (FN).
For 32% FP instances information about verb
frames is not available in the knowledge base of
verb frames. To avoid this problem researchers
should exploit some abstractions e.g., {semantic
sense of v, gr} frames. Our model fails to iden-
tify the class ¬Cnp for noun phrases of 29% FP
instances due to the lack of enough training data
for the semantic classes of nouns. In 21% FP
instances v and np are not even relevant to each
other. Our model first needs to determine rele-
vance between v and np before identifying causal-
ity. Remaining 18% instances have v and np in
temporal only sense, comparison relation or both
represent parts of same event. There is need to ex-
tract more knowledge sources to better distinguish
causality from any other type of relation.
77% FN instances are classified as non-causal
due to the lack of enough v-np training data and
require more sources of knowledge e.g., back-
ground knowledge. On remaining 23% FN in-
stances our model fails to identify Cnp class due
to the lack of enough training data for the seman-
tic classes of nouns.
</bodyText>
<sectionHeader confidence="0.999384" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999415">
This work has revealed that enriched semantics
of nouns and verbs help gain significant improve-
ment in performance over a baseline relying only
on shallow linguistic features. Through empiri-
cal evaluation and error analysis of our model we
have highlighted strengths and weaknesses of our
model for the current task. Our work has provided
a novel direction to exploit semantics of partici-
pants of causal relations to solve the challenge of
identifying causality.
</bodyText>
<page confidence="0.990949">
168
</page>
<note confidence="0.576179875">
Emily Pitler, Annie Louis and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In proceedings of ACL-IJCNLP,
2009.
References
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In proceed-
ings of COLING-ACL. Montreal, Canada.
</note>
<reference confidence="0.998869239130434">
Taylor Berg-Kirkpatrick, David Burkett, and Dan
Klein. 2012. An empirical investigation of statisti-
cal significance in NLP. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL)..
Steven Bethard and James H. Martin. 2006. Identifica-
tion of Event Mentions and their Semantic Class. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Steven Bethard and James H. Martin. 2008. Learning
Semantic Links from a Corpus of Parallel Temporal
and Causal Relations. In proceedings of ACL-08:
HLT.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In proceedings of Conference on
Empirical Methods in Natural Language Processing
(EMNLP-04). Barcelona, Spain.
Quang X. Do, Yee S. Chen and Dan Roth. 2011. Min-
imally Supervised Event Causality Identication. In
proceedings of EMNLP-2011.
Jenny R. Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the Association for
Computational Linguistics (ACL).
Roxana Girju and Dan Moldovan. 2002. Mining An-
swers for Causation Questions. In American Asso-
ciations of Artificial Intelligence (AAAI), 2002 Sym-
posium.
Roxana Girju. 2003. Automatic detection of causal
relations for Question Answering. Association for
Computational Linguistics ACL, Workshop on Mul-
tilingual Summarization and Question Answering
Machine Learning and Beyond 2003.
Katja Markert and Malvina Nissim 2009. Data and
models for metonymy resolution. Language Re-
sources and Evaluation Volume 43 Issue 2, Pages
123−138.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).
Andrew K. McCallum. 2002. MALLET:
A Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In proceedings of ACL-IJCNLP, 2009.
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics.
Kira Radinsky and Eric Horvitz. 2013. Mining the
Web to Predict Future Events. In proceedings of
sixth ACM international conference on Web search
and data mining, WSDM ’13.
Mehwish Riaz and Roxana Girju. 2010. Another Look
at Causality: Discovering Scenario-Specific Contin-
gency Relationships with No Supervision. In pro-
ceedings of the IEEE 4th International Conference
on Semantic Computing (ICSC).
Mehwish Riaz and Roxana Girju 2013. Toward
a Better Understanding of Causality between Ver-
bal Events: Extraction and Analysis of the Causal
Power of Verb-Verb Associations. Proceedings of
the annual SIGdial Meeting on Discourse and Dia-
logue (SIGDIAL).
Mehwish Riaz and Roxana Girju 2014. Recognizing
Causality in Verb-Noun Pairs via Noun and Verb Se-
mantics. Proceedings of the Workshop on Computa-
tional Approaches to Causality in Language EACL,
2014.
Dan Roth and Wen-tau Yih 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In Proceedings of the Annual Con-
ference on Computational Natural Language Learn-
ing (CoNLL).
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetor-
ical relations: An assessment. Journal of Natural
Language Engineering Volume 14 Issue 3, July 2008
Pages 369−416.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of Human Language Technol-
ogy and North American Chapter of the Association
for Computational Linguistics (HLT-NAACL).
</reference>
<page confidence="0.999582">
169
</page>
<sectionHeader confidence="0.992164" genericHeader="method">
Appendix A. Supervised Classifier
</sectionHeader>
<bodyText confidence="0.994796">
In this appendix, we provide a set of linguistic fea-
tures taken from Riaz and Girju (2014) to iden-
tify causality in v-np pairs employing a supervised
classifier (see section 3.1 for the details).
</bodyText>
<listItem confidence="0.993296125">
• Lexical Features: verb, lemma of verb,
noun phrase, lemmas of all words of
noun phrase, head noun of noun phrase,
lemmas of all words between verb and
noun phrase.
• Syntatic Features: part-of-speech tags of verb
and head noun of noun phrase.
• Semantic Features: We adopted this fea-
</listItem>
<bodyText confidence="0.99279375">
ture from Girju (2003) to capture semantics
of nouns. The 9 noun hierarchies of Word-
Net i.e., entity, psychological feature, abstrac-
tion, state, event, act, group, possession, phe-
nomenon are used as this feature. Each of these
hierarchies is set to 1 if any sense of head noun
of noun phrase lies in that hierarchy, otherwise
set to 0.
</bodyText>
<listItem confidence="0.879391">
• Structural Features: This feature is applied
</listItem>
<bodyText confidence="0.9160851">
by considering both subject (i.e., sub in np)
and object (i.e., obj in np) of verb (v). For ex-
ample, for the pair v-np the variable sub in np
is set to 1 if the subject E np, set to 0 if the
subject E� np and set to -1 if the subject is not
available in an instance. The subject and object
of a verb are its core arguments and may some-
time be part of an event represented by a verb.
Therefore, these arguments may have high ten-
dency to encode non-causation with their verb.
</bodyText>
<listItem confidence="0.819731">
• Pairs: The following pairs (verb, head noun
</listItem>
<bodyText confidence="0.710046">
of noun phrase), (subjectverb, head noun of
noun phrase) and (objectverb, head noun of
noun phrase) are used to capture relations.
</bodyText>
<sectionHeader confidence="0.981862" genericHeader="method">
Appendix B. Noun Semantics
</sectionHeader>
<bodyText confidence="0.999541333333333">
In this appendix, some examples of the frame ele-
ments of FrameNet and the WordNet senses be-
longing to the classes Cnp and ¬Cnp are given
in Tables 4 and 5 (see section 3.2 for the de-
tails). We employ training instances acquired us-
ing the FrameNet annotations and WordNet senses
for building a supervised classifier for the classes
Cnp and ¬Cnp. Following is the list of features we
use for this supervised classifier:
</bodyText>
<listItem confidence="0.714301">
• Lexical Features: All words of noun phrase,
lemmas of all words of noun phrase, head noun
of noun phrase, first two (three) (four) letters
</listItem>
<sectionHeader confidence="0.488944" genericHeader="method">
SC FrameNet Labels
</sectionHeader>
<reference confidence="0.99758675">
cnp Event, Goal, Purpose, Cause, Internal cause, External
cause, Result, Means, Reason, Phenomena, Coordi-
nated event, Action, Activity, Circumstances, Desired
goal, Explanation
_cnp Artist, Performer, Duration, Time, Place, Distributor,
Area, Path, Direction, Sub-region Frequency, Body
part, Area, Degree, Angle, Fixed location, Path shape,
Addressee, Interval
</reference>
<tableCaption confidence="0.619944333333333">
Table 4: Some examples of the frame elements of
FrameNet to which we assign the semantic classes
Cnp and ¬Cnp.
</tableCaption>
<sectionHeader confidence="0.425025" genericHeader="method">
SC WordNet Senses
</sectionHeader>
<bodyText confidence="0.973296">
cnp {act, deed, human action, human activity},
{phenomenon}, {state}, {psychological feature},
{event}, {causal agent, cause, causal agency}
_cnp {time period, period of time, period}, {measure,
quantity, amount}, {group, grouping}, {organization,
organisation}, {time unit, unit of time}, {clock time,
time}
Table 5: This table shows our selected Word-
Net senses of nouns belonging to classes Cnp and
¬Cnp. For example, using the information pro-
vided in this table we assume that any noun con-
cept whose all senses of WordNet lie in the seman-
tic hierarchy of the sense {time period, period of
time, period} is of class ¬Cnp. We use English
Gigaword corpus to collect instances of noun (or
noun phrases) and label them with Cnp and ¬Cnp
according to their senses in WordNet.
of head noun of noun phrase, last two, (three)
(four) letters of head noun of noun phrase.
</bodyText>
<listItem confidence="0.9815364">
• Word Class Features: part-of-speech tags of
all words of noun phrase and head noun of
noun phrase.
• Semantic Features: Frequent sense of head
noun of noun phrase.
</listItem>
<page confidence="0.99526">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.888661">
<title confidence="0.983164">In-depth Exploitation of Noun and Verb to Identify Causation in Verb-Noun Pairs</title>
<author confidence="0.964721">Mehwish Riaz</author>
<author confidence="0.964721">Roxana</author>
<affiliation confidence="0.996464">Department of Computer Science and Beckman University of Illinois at</affiliation>
<address confidence="0.964943">Urbana, IL 61801,</address>
<abstract confidence="0.9994635">Recognition of causality is important to achieve natural language discourse understanding. Previous approaches rely on shallow linguistic features. In this work, we propose to identify causality in verbnoun pairs by exploiting deeper semantics of nouns and verbs. Particularly, we acquire and employ three novel types of knowledge: (1) semantic classes of nouns with a high and low tendency to encode causality along with information regarding metonymies, (2) data-driven semantic classes of verbal events with the least tendency to encode causality, and (3) tendencies of verb frames to encode causality. Using these knowledge sources, we achieve around 15% improvement in Fscore over a supervised classifier trained using linguistic features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Taylor Berg-Kirkpatrick</author>
<author>David Burkett</author>
<author>Dan Klein</author>
</authors>
<title>An empirical investigation of statistical significance in NLP.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)..</booktitle>
<contexts>
<context position="32706" citStr="Berg-Kirkpatrick et al. (2012)" startWordPosition="5563" endWordPosition="5566">able presents results over NB (MaxEnt) baseline supervised classifier, respectively. Table 3 shows that MaxEnt gives a very high accuracy and F-score as compared with NB. Model +N! M+V{R} with basic noun and verb semantics introduced in section 3.2 results in more than 10% improvement in F-score over NB and MaxEnt classifiers relying only on shallow linguistic features. Model +NM+VF+V{A,R,IS} with enriched verb and noun semantics brings more than 4% improvement in F-score over +N! M+V{R} with MaxEnt as baseline. We perform statistical significance test using bootstrap sampling method given in Berg-Kirkpatrick et al. (2012) (see Berg-Kirkpatrick et al. (2012) for the details). +NM+VF+V{A,R,IS} brings significant improvement in F-score over +N! M+V{R} with pvalue 0.0. Though +N! M gives significantly better F-score over baseline, it drops recall by more than 16%. Metonymy resolution helps perform quite better by recovering more than 8% recall with +NM1GR + M2 over +N! M. +NM 1GR + M2 also results in 3.9% improvement in F-score over +N! M with MaxEnt as baseline model (significant improvement with p-value 0.0). Metonymies resolved via verb frames with all and core grammatical relations (i.e., set GR) recover more </context>
</contexts>
<marker>Berg-Kirkpatrick, Burkett, Klein, 2012</marker>
<rawString>Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An empirical investigation of statistical significance in NLP. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)..</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>James H Martin</author>
</authors>
<title>Identification of Event Mentions and their Semantic Class.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="14716" citStr="Bethard and Martin (2006)" startWordPosition="2470" endWordPosition="2473"> events – i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting. According to the definitions of these classes, we claim that the reporting events (e.g., say, tell, etc.) just describe and narrate other events instead of encoding causality with them. Using this claim, we consider that all instances of reporting verbal events of TimeBank belong to the class ¬Cev and the rest of instances of verbal events lie in the class Cev. After acquiring instances of the classes Cev and ¬Cev, we build a supervised classifier for these two classes. We use the features introduced by Bethard and Martin (2006) to build this classifier (see Bethard and Martin (2006) for the details). Employing predictions and probabilities of assignments of the labels Cev and ¬Cev we add the following two constraints to ILP: (1) if the event represented by v belongs to ¬Cev then the corresponding v-np pair must be labeled with ¬C and (2) if a v-np pair is a causal pair then the event represented by v must be labeled with Cev. 4 Enriched Verb and Noun Semantics This section describes the novel contributions of this work i.e., identification of semantics of verb frames, semantic classes of verbal events via a data int</context>
<context position="22133" citStr="Bethard and Martin (2006)" startWordPosition="3749" endWordPosition="3752"> relations (named as Trainingvi-vj) (we refer the reader to Riaz and Girju (2013) for the details of this training corpus) and (B) the training corpus v-np instances introduced in section 3.1 (named as Trainingv-np). Following is the procedure to derive V,C ⊆ V where V={Occurrence, Perception, Aspectual, State, I State, I Action, Reporting} and the set V,C contains the TimeBank’s semantic classes E x3(g(v-np), l) = 1 ∀ l∈L1 165 with the least tendency to encode a cause relation. 1. Input: Training corpus, V 2. Output: Set V¬C 3. For each training instance k employ the supervised classifier of Bethard and Martin (2006) to do the following: (a) if k E Trainingvi-vj then identify the semantic class (sc) of both events represented by both verbs vi and vj and add this information to a set i.e., T = T U (kvi, scvi, l) U (kvj, scvj, l) where scvi is the semantic class of event of the verb vi of instance k and l E {C, -C}. (b) Else if k E Trainingv-np then identify the semantic class (sc) of event represented by the verb v and set T = T U (kv, scv, l). 4. Using results of step 3, calculate tendency of each semantic class sc E V to encode non-causality (i.e., score(sc, -C)) as follows: where count(m, n) is the numb</context>
</contexts>
<marker>Bethard, Martin, 2006</marker>
<rawString>Steven Bethard and James H. Martin. 2006. Identification of Event Mentions and their Semantic Class. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>James H Martin</author>
</authors>
<title>Learning Semantic Links from a Corpus of Parallel Temporal and Causal Relations.</title>
<date>2008</date>
<booktitle>In proceedings of ACL-08: HLT.</booktitle>
<contexts>
<context position="1743" citStr="Bethard and Martin, 2008" startWordPosition="262" endWordPosition="265"> discourse understanding. Causal relations are encoded in text using various linguistic constructions e.g., between two verbs, a verb and a noun, two discourse segments, etc. In this research, we focus on identifying causality encoded between a verb and a noun (or noun phrase). For example, consider the following example: 1. At least 1,833 people died in the hurricane. In example (1), the verb-noun phrase pair “died”-“the hurricane” encodes causality where event “died” is the effect of “hurricane” event. Previously several approaches have been proposed to identify causality between two verbs (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, the problem of identifying causality in verb-noun pairs has not received a considerable attention. For example, Do et al. (2011) have studied this task but they worked only with a list of predefined nouns representing events. In this work, we focus on the linguistic construction of verb-noun (or noun phrase) pairs where noun can be of any semantic type. Traditional approaches for identifying causality mainly employ lingu</context>
<context position="6062" citStr="Bethard and Martin, 2008" startWordPosition="954" endWordPosition="957">model to avoid false negatives. We provide details of our previously proposed model in section 3. We introduce new model and discuss its performance in sections 4 and 5. Section 6 concludes the current research. 2 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), summarization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc. Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However causality a pervasive relation of language can be encoded via various linguistic constructions. For example, verbs and nouns are the key components of language to represent events. Therefore in this work we focus on identifying causality in verbnoun pairs. Previously researchers have followed the path of utilizing linguistic features in the framework of supervised learning (Girju, 2003; Bethard and Martin, 2008; Sporleder</context>
</contexts>
<marker>Bethard, Martin, 2008</marker>
<rawString>Steven Bethard and James H. Martin. 2008. Learning Semantic Links from a Corpus of Parallel Temporal and Causal Relations. In proceedings of ACL-08: HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.</title>
<date>2004</date>
<booktitle>In proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-04).</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="5896" citStr="Chklovski and Pantel, 2004" startWordPosition="929" endWordPosition="932">false positives it can lead to false negatives when metonymic readings are associated with such nouns. Therefore, we introduce a metonymy resolver on top of our current model to avoid false negatives. We provide details of our previously proposed model in section 3. We introduce new model and discuss its performance in sections 4 and 5. Section 6 concludes the current research. 2 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), summarization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc. Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However causality a pervasive relation of language can be encoded via various linguistic constructions. For example, verbs and nouns are the key components of language to represent events. Therefore in this work we focus on identifying causality in verbnoun pairs. Pr</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations. In proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP-04). Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quang X Do</author>
<author>Yee S Chen</author>
<author>Dan Roth</author>
</authors>
<title>Minimally Supervised Event Causality Identication.</title>
<date>2011</date>
<booktitle>In proceedings of EMNLP-2011.</booktitle>
<contexts>
<context position="1782" citStr="Do et al., 2011" startWordPosition="270" endWordPosition="273">ncoded in text using various linguistic constructions e.g., between two verbs, a verb and a noun, two discourse segments, etc. In this research, we focus on identifying causality encoded between a verb and a noun (or noun phrase). For example, consider the following example: 1. At least 1,833 people died in the hurricane. In example (1), the verb-noun phrase pair “died”-“the hurricane” encodes causality where event “died” is the effect of “hurricane” event. Previously several approaches have been proposed to identify causality between two verbs (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, the problem of identifying causality in verb-noun pairs has not received a considerable attention. For example, Do et al. (2011) have studied this task but they worked only with a list of predefined nouns representing events. In this work, we focus on the linguistic construction of verb-noun (or noun phrase) pairs where noun can be of any semantic type. Traditional approaches for identifying causality mainly employ linguistic features (e.g., lexical items, pa</context>
<context position="4122" citStr="Do et al., 2011" startWordPosition="638" endWordPosition="641"> nouns to filter false positives. Similarly, we utilized the TimeBank’s (Pustejovsky et al., 2006) classification of verbal events (i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting) and their definitions to claim that the reporting events (e.g., say, tell, etc.) 161 Proceedings of the SIGDIAL 2014 Conference, pages 161–170, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics just describe and narrate other events instead of encoding causality with them. We proposed an Integer Linear Programming (ILP) model (Roth and Yih, 2004; Do et al., 2011) to combine noun and verb semantics with the decisions of a supervised classifier which only relies on linguistic features. In this paper, we extend our previous model by acquiring and exploiting the following three novel types of knowledge: 1. We learn the information about tendencies of various verb frames to encode causation. For example, our model identifies if the subject of verb “destroy” (“occur”) has a high (low) tendency to encode causation. Such information helps gain performance by exploiting causal semantics of each verb frame separately. We also learn and incorporate information a</context>
<context position="6101" citStr="Do et al., 2011" startWordPosition="962" endWordPosition="965">ls of our previously proposed model in section 3. We introduce new model and discuss its performance in sections 4 and 5. Section 6 concludes the current research. 2 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), summarization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc. Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However causality a pervasive relation of language can be encoded via various linguistic constructions. For example, verbs and nouns are the key components of language to represent events. Therefore in this work we focus on identifying causality in verbnoun pairs. Previously researchers have followed the path of utilizing linguistic features in the framework of supervised learning (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenko</context>
</contexts>
<marker>Do, Chen, Roth, 2011</marker>
<rawString>Quang X. Do, Yee S. Chen and Dan Roth. 2011. Minimally Supervised Event Causality Identication. In proceedings of EMNLP-2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="13588" citStr="Finkel et al., 2005" startWordPosition="2276" endWordPosition="2279">2 |choices can be assigned to np (see constraints 5 and 6). Constraint 7 enforces that if an np belongs to the class ¬Cnp then its corresponding v-np pair is assigned the label ¬C. In particular, we maximize the objective function Z2 (4) subject to the constraints introduced till now. For each v-np pair, we predict the semantic class of np using our supervised classifier for the labels l E L2 and set the probabilities – i.e., P(fnp(v-np), l) = 1, P(fnp(v-np), {L2} − {l}) = 0 if the label l E L2 is assigned to np. Also before running our supervised classifier, we run a named entity recognizer (Finkel et al., 2005) and assign the label ¬Cnp to all noun phrases identified as named entities. We also determine association of metonymies with the noun phrases identified as named entities. For the current task we also acquire two semantic classes of verbs i.e., Cev and ¬Cev where the class Cev (¬Cev) contains the verbal events with a high (low) tendency to encode causation. In order to derive these two classes we exploit the TimeBank corpus (Pustejovsky et al., 2003) which provides seven semantic classes of verbal events – i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting. Accord</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny R. Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
</authors>
<title>Mining Answers for Causation Questions.</title>
<date>2002</date>
<booktitle>In American Associations of Artificial Intelligence (AAAI),</booktitle>
<note>Symposium.</note>
<contexts>
<context position="11294" citStr="Girju and Moldovan (2002)" startWordPosition="1820" endWordPosition="1823"> unless a metonymic reading is associated with it. In order to acquire these classes, we extract annotations of 936 distinct frame elements from FrameNet in which a frame element must contain at least one noun and no verb in it. These annotations of frame elements roughly represent instances of noun phrases (np). We manually assigned the labels Cnp and ¬Cnp to the frame elements. For example, we assign the label ¬Cnp to the frame element “Place” which represents a location (see Appendix B for some examples of the frame elements with labels Cnp and ¬Cnp). We also follow the approach similar to Girju and Moldovan (2002) to employ WordNet senses of nouns to acquire more instances of the classes Cnp and ¬Cnp (see Appendix B for the details). We have acquired a total of 280,212 instances of np (50% for each of the two classes i.e., Cnp and ¬Cnp) using both FrameNet and WordNet. Using these instances, we build a supervised classifier to identify the semantic class of np (see Appendix B for the details of features to build the classifier). We incorporate the knowledge of semantic classes of nouns by making the following additions to ILP: EZ2 = Z1 + E x2(fnp(v-np), l)P(fnp(v-np), l) E x1(v-np, l) = 1 V v-np E I (2</context>
</contexts>
<marker>Girju, Moldovan, 2002</marker>
<rawString>Roxana Girju and Dan Moldovan. 2002. Mining Answers for Causation Questions. In American Associations of Artificial Intelligence (AAAI), 2002 Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Automatic detection of causal relations for Question Answering. Association for Computational Linguistics</title>
<date>2003</date>
<booktitle>ACL, Workshop on Multilingual Summarization and Question Answering Machine Learning and Beyond</booktitle>
<contexts>
<context position="2468" citStr="Girju, 2003" startWordPosition="378" endWordPosition="379">008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, the problem of identifying causality in verb-noun pairs has not received a considerable attention. For example, Do et al. (2011) have studied this task but they worked only with a list of predefined nouns representing events. In this work, we focus on the linguistic construction of verb-noun (or noun phrase) pairs where noun can be of any semantic type. Traditional approaches for identifying causality mainly employ linguistic features (e.g., lexical items, part-of-speech tags of words, etc.) in the framework of supervised learning (Girju, 2003; Sporleder and Lascarides, 2008; Bethard and Martin, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009) and do not involve deeper semantics of language. Analysis of such approaches by Sporleder and Lascarides (2008) have revealed that the linguistic features are not always sufficient to achieve a good performance on the task of identifying semantic relations including causality. In this work, we propose a model that deeply processes and acquires the specific semantic information about the participants of a verb-noun phrase (v-np) pair (i.e., noun and verb semantics) to identify causality wi</context>
<context position="5852" citStr="Girju, 2003" startWordPosition="925" endWordPosition="926">de causation helps to filter false positives it can lead to false negatives when metonymic readings are associated with such nouns. Therefore, we introduce a metonymy resolver on top of our current model to avoid false negatives. We provide details of our previously proposed model in section 3. We introduce new model and discuss its performance in sections 4 and 5. Section 6 concludes the current research. 2 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), summarization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc. Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However causality a pervasive relation of language can be encoded via various linguistic constructions. For example, verbs and nouns are the key components of language to represent events. Therefore in this work we focus on</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>Roxana Girju. 2003. Automatic detection of causal relations for Question Answering. Association for Computational Linguistics ACL, Workshop on Multilingual Summarization and Question Answering Machine Learning and Beyond 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katja Markert</author>
<author>Malvina Nissim</author>
</authors>
<title>Data and models for metonymy resolution.</title>
<date>2009</date>
<journal>Language Resources and Evaluation</journal>
<volume>43</volume>
<pages>123--138</pages>
<contexts>
<context position="24498" citStr="Markert and Nissim, 2009" startWordPosition="4186" endWordPosition="4189">I State events of the TimeBank corpus belong to the class ¬Cev and rest of the events lie in Cev. Using these semantic classes we apply the constraints introduced in section 3.2. 4.3 Metonymy Resolution: Metonymy resolution is the task to determine if a literal or non-literal reading is associated with a {v, gr} count({v, gr}, Cnp) count({v, gr}, ¬Cnp) {kill,nsubj} 1 0 {kill,dobj} 0 1 [CnpPissed off Angelus] just kills [¬Cnpme] Table 2: A knowledge base of verb frames. This knowledge base is populated using the instances of Cnp and ¬Cnp labels given in this table. natural language expression (Markert and Nissim, 2009). Consider the following example: 4. The United States has killed Osama bin Laden and has custody of his body. (Cause (C)) In example (4) “The United States” refers to a non-literal reading i.e., the event of “raid in Abbottabad on May 2, 2011 by the United States” rather than merely referring to a literal sense i.e., a country. The association of non-literal reading with “The United States” results in killing event. Previously, researchers have worked with handannotated selectional restrictions violation for this task (Markert and Nissim, 2009). In the example (4) a country cannot “kill” some</context>
</contexts>
<marker>Markert, Nissim, 2009</marker>
<rawString>Katja Markert and Malvina Nissim 2009. Data and models for metonymy resolution. Language Resources and Evaluation Volume 43 Issue 2, Pages 123−138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC).</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew K McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="30083" citStr="McCallum, 2002" startWordPosition="5138" endWordPosition="5139">06). We extracted all v-np pairs from each sentence of these articles. For each of the these three articles, we selected first 500 instances of vnp pairs. Two annotators were asked to provide the labels C and -C to the instances of v-np pairs using the annotation guidelines from Riaz and Girju (2010). We have achieved a 0.64 kappa score for the human inter-annotator agreement on a total of 1,500 v-np instances. This results in a total of 1,365 instances of v-np pairs with 11.86% C pairs. In this section, we present performance of the following models (see Table 3): 1. Baseline: NB and MaxEnt (McCallum, 2002) supervised classifiers using only the shallow linguistic features (see section 3.1). 2. Basic noun and verb semantics: ILP with the addition of semantic classes of nouns without metonymy (denoted by +N! M) and the addition of semantic classes of verbs where -Cev={(R)eporting events} (denoted by +N! M+V{R}). These models represent the work proposed in Riaz and Girju (2014) (section 3). 3. Noun semantics with metonymies: ILP with the addition of noun semantics involving metonymies resolved via verb frames (denoted by +NM1), metonymies resolved via verb frames {v, gr} where gr E GR = {csubj, csu</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew K. McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using Syntax to Disambiguate Explicit Discourse Connectives in Text.</title>
<date>2009</date>
<booktitle>In proceedings of ACL-IJCNLP,</booktitle>
<contexts>
<context position="1886" citStr="Pitler and Nenkova, 2009" startWordPosition="286" endWordPosition="289">n, two discourse segments, etc. In this research, we focus on identifying causality encoded between a verb and a noun (or noun phrase). For example, consider the following example: 1. At least 1,833 people died in the hurricane. In example (1), the verb-noun phrase pair “died”-“the hurricane” encodes causality where event “died” is the effect of “hurricane” event. Previously several approaches have been proposed to identify causality between two verbs (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, the problem of identifying causality in verb-noun pairs has not received a considerable attention. For example, Do et al. (2011) have studied this task but they worked only with a list of predefined nouns representing events. In this work, we focus on the linguistic construction of verb-noun (or noun phrase) pairs where noun can be of any semantic type. Traditional approaches for identifying causality mainly employ linguistic features (e.g., lexical items, part-of-speech tags of words, etc.) in the framework of supervised learning (Girju, 2003; Sporleder and La</context>
<context position="6205" citStr="Pitler and Nenkova, 2009" startWordPosition="977" endWordPosition="980">mance in sections 4 and 5. Section 6 concludes the current research. 2 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), summarization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc. Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However causality a pervasive relation of language can be encoded via various linguistic constructions. For example, verbs and nouns are the key components of language to represent events. Therefore in this work we focus on identifying causality in verbnoun pairs. Previously researchers have followed the path of utilizing linguistic features in the framework of supervised learning (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). Though linguistic features are important but other sources of knowledge </context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Using Syntax to Disambiguate Explicit Discourse Connectives in Text. In proceedings of ACL-IJCNLP, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Saur</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
</authors>
<title>The TIMEBANK Corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics.</booktitle>
<institution>Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro and</institution>
<contexts>
<context position="14043" citStr="Pustejovsky et al., 2003" startWordPosition="2357" endWordPosition="2360"> P(fnp(v-np), {L2} − {l}) = 0 if the label l E L2 is assigned to np. Also before running our supervised classifier, we run a named entity recognizer (Finkel et al., 2005) and assign the label ¬Cnp to all noun phrases identified as named entities. We also determine association of metonymies with the noun phrases identified as named entities. For the current task we also acquire two semantic classes of verbs i.e., Cev and ¬Cev where the class Cev (¬Cev) contains the verbal events with a high (low) tendency to encode causation. In order to derive these two classes we exploit the TimeBank corpus (Pustejovsky et al., 2003) which provides seven semantic classes of verbal events – i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting. According to the definitions of these classes, we claim that the reporting events (e.g., say, tell, etc.) just describe and narrate other events instead of encoding causality with them. Using this claim, we consider that all instances of reporting verbal events of TimeBank belong to the class ¬Cev and the rest of instances of verbal events lie in the class Cev. After acquiring instances of the classes Cev and ¬Cev, we build a supervised classifier for these</context>
</contexts>
<marker>Pustejovsky, Hanks, Saur, See, Gaizauskas, Setzer, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Saur, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro and Marcia Lazo. 2003. The TIMEBANK Corpus. In Proceedings of Corpus Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eric Horvitz</author>
</authors>
<title>Mining the Web to Predict Future Events.</title>
<date>2013</date>
<booktitle>In proceedings of sixth ACM international conference on Web search and data mining, WSDM ’13.</booktitle>
<contexts>
<context position="5944" citStr="Radinsky and Horvitz, 2013" startWordPosition="935" endWordPosition="938">hen metonymic readings are associated with such nouns. Therefore, we introduce a metonymy resolver on top of our current model to avoid false negatives. We provide details of our previously proposed model in section 3. We introduce new model and discuss its performance in sections 4 and 5. Section 6 concludes the current research. 2 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), summarization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc. Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However causality a pervasive relation of language can be encoded via various linguistic constructions. For example, verbs and nouns are the key components of language to represent events. Therefore in this work we focus on identifying causality in verbnoun pairs. Previously researchers have followed the path of u</context>
</contexts>
<marker>Radinsky, Horvitz, 2013</marker>
<rawString>Kira Radinsky and Eric Horvitz. 2013. Mining the Web to Predict Future Events. In proceedings of sixth ACM international conference on Web search and data mining, WSDM ’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehwish Riaz</author>
<author>Roxana Girju</author>
</authors>
<title>Another Look at Causality: Discovering Scenario-Specific Contingency Relationships with No Supervision.</title>
<date>2010</date>
<booktitle>In proceedings of the IEEE 4th International Conference on Semantic Computing (ICSC).</booktitle>
<contexts>
<context position="1765" citStr="Riaz and Girju, 2010" startWordPosition="266" endWordPosition="269">Causal relations are encoded in text using various linguistic constructions e.g., between two verbs, a verb and a noun, two discourse segments, etc. In this research, we focus on identifying causality encoded between a verb and a noun (or noun phrase). For example, consider the following example: 1. At least 1,833 people died in the hurricane. In example (1), the verb-noun phrase pair “died”-“the hurricane” encodes causality where event “died” is the effect of “hurricane” event. Previously several approaches have been proposed to identify causality between two verbs (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, the problem of identifying causality in verb-noun pairs has not received a considerable attention. For example, Do et al. (2011) have studied this task but they worked only with a list of predefined nouns representing events. In this work, we focus on the linguistic construction of verb-noun (or noun phrase) pairs where noun can be of any semantic type. Traditional approaches for identifying causality mainly employ linguistic features (e.g., </context>
<context position="6084" citStr="Riaz and Girju, 2010" startWordPosition="958" endWordPosition="961">ives. We provide details of our previously proposed model in section 3. We introduce new model and discuss its performance in sections 4 and 5. Section 6 concludes the current research. 2 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), summarization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc. Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However causality a pervasive relation of language can be encoded via various linguistic constructions. For example, verbs and nouns are the key components of language to represent events. Therefore in this work we focus on identifying causality in verbnoun pairs. Previously researchers have followed the path of utilizing linguistic features in the framework of supervised learning (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008;</context>
<context position="29769" citStr="Riaz and Girju (2010)" startWordPosition="5082" endWordPosition="5085">generated a test set of instances of v-np pairs. For this purpose, we collected three wiki articles on the topics of Hurricane Katrina, Iraq War and Egyptian Revolution of 2011. We apply a part-of-speech tagger and a dependency parser on all sentences of these three articles (Toutanova et al., 2003; Marneffe et al., 2006). We extracted all v-np pairs from each sentence of these articles. For each of the these three articles, we selected first 500 instances of vnp pairs. Two annotators were asked to provide the labels C and -C to the instances of v-np pairs using the annotation guidelines from Riaz and Girju (2010). We have achieved a 0.64 kappa score for the human inter-annotator agreement on a total of 1,500 v-np instances. This results in a total of 1,365 instances of v-np pairs with 11.86% C pairs. In this section, we present performance of the following models (see Table 3): 1. Baseline: NB and MaxEnt (McCallum, 2002) supervised classifiers using only the shallow linguistic features (see section 3.1). 2. Basic noun and verb semantics: ILP with the addition of semantic classes of nouns without metonymy (denoted by +N! M) and the addition of semantic classes of verbs where -Cev={(R)eporting events} (</context>
</contexts>
<marker>Riaz, Girju, 2010</marker>
<rawString>Mehwish Riaz and Roxana Girju. 2010. Another Look at Causality: Discovering Scenario-Specific Contingency Relationships with No Supervision. In proceedings of the IEEE 4th International Conference on Semantic Computing (ICSC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehwish Riaz</author>
<author>Roxana Girju</author>
</authors>
<title>Toward a Better Understanding of Causality between Verbal Events: Extraction and Analysis of the Causal Power of Verb-Verb Associations.</title>
<date>2013</date>
<booktitle>Proceedings of the annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL).</booktitle>
<contexts>
<context position="1805" citStr="Riaz and Girju, 2013" startWordPosition="274" endWordPosition="277">ing various linguistic constructions e.g., between two verbs, a verb and a noun, two discourse segments, etc. In this research, we focus on identifying causality encoded between a verb and a noun (or noun phrase). For example, consider the following example: 1. At least 1,833 people died in the hurricane. In example (1), the verb-noun phrase pair “died”-“the hurricane” encodes causality where event “died” is the effect of “hurricane” event. Previously several approaches have been proposed to identify causality between two verbs (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, the problem of identifying causality in verb-noun pairs has not received a considerable attention. For example, Do et al. (2011) have studied this task but they worked only with a list of predefined nouns representing events. In this work, we focus on the linguistic construction of verb-noun (or noun phrase) pairs where noun can be of any semantic type. Traditional approaches for identifying causality mainly employ linguistic features (e.g., lexical items, part-of-speech tags of wo</context>
<context position="6124" citStr="Riaz and Girju, 2013" startWordPosition="966" endWordPosition="969">sly proposed model in section 3. We introduce new model and discuss its performance in sections 4 and 5. Section 6 concludes the current research. 2 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), summarization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc. Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However causality a pervasive relation of language can be encoded via various linguistic constructions. For example, verbs and nouns are the key components of language to represent events. Therefore in this work we focus on identifying causality in verbnoun pairs. Previously researchers have followed the path of utilizing linguistic features in the framework of supervised learning (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al.</context>
<context position="7375" citStr="Riaz and Girju (2013)" startWordPosition="1157" endWordPosition="1160">tures are important but other sources of knowledge are also critically required to achieve progress on the current task. In recent years, researchers have proposed unsupervised metrics to identify causality between events (Riaz and Girju, 2010; Do et al., 2011). For example, Riaz and Girju (2010) and Do et al. (2011) introduced unsupervised metrics to learn causal dependencies between events. These metrics mainly depend on probabilities of cooccurrences of events and do not distinguish well causality from any other types of correlation (Riaz and Girju, 2013). In order to overcome this problem Riaz and Girju (2013) proposed some advanced metrics which combine probabilities of cooccurrences of events with the supervised estimates of cause and non-cause relations. Considering the importance of employing rich sources of knowledge other than linguistic features for the current task, we have recently proposed a model that incorporates semantic classes of nouns and verbs with a high and low tendency to encode causation (Riaz and Girju, 2014). In this work, we exploit information about verb frames, data-driven verb semantics and metonymies to achieve more progress on our recent work. 3 Model for Recognizing Ca</context>
<context position="21589" citStr="Riaz and Girju (2013)" startWordPosition="3657" endWordPosition="3660">nts we now introduce a data intensive approach to automatically identify the class ¬Cev of verbal events. In order to identify this class we extract training instances of verbal events encoding C and ¬C relations. Verbal events encode cause-effect relations using verb-verb (e.g., Five shoppers were killed when a car blew up.) and verb-noun linguistic constructions. Therefore for the current purpose we use the following two types of training instances: (A) a training corpus of 240K instances of verb-verb (vi-vj) pairs encoding C and ¬C relations (named as Trainingvi-vj) (we refer the reader to Riaz and Girju (2013) for the details of this training corpus) and (B) the training corpus v-np instances introduced in section 3.1 (named as Trainingv-np). Following is the procedure to derive V,C ⊆ V where V={Occurrence, Perception, Aspectual, State, I State, I Action, Reporting} and the set V,C contains the TimeBank’s semantic classes E x3(g(v-np), l) = 1 ∀ l∈L1 165 with the least tendency to encode a cause relation. 1. Input: Training corpus, V 2. Output: Set V¬C 3. For each training instance k employ the supervised classifier of Bethard and Martin (2006) to do the following: (a) if k E Trainingvi-vj then iden</context>
</contexts>
<marker>Riaz, Girju, 2013</marker>
<rawString>Mehwish Riaz and Roxana Girju 2013. Toward a Better Understanding of Causality between Verbal Events: Extraction and Analysis of the Causal Power of Verb-Verb Associations. Proceedings of the annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehwish Riaz</author>
<author>Roxana Girju</author>
</authors>
<title>Recognizing Causality in Verb-Noun Pairs via Noun and Verb Semantics.</title>
<date>2014</date>
<booktitle>Proceedings of the Workshop on Computational Approaches to Causality in Language EACL,</booktitle>
<contexts>
<context position="3247" citStr="Riaz and Girju (2014)" startWordPosition="502" endWordPosition="505">alysis of such approaches by Sporleder and Lascarides (2008) have revealed that the linguistic features are not always sufficient to achieve a good performance on the task of identifying semantic relations including causality. In this work, we propose a model that deeply processes and acquires the specific semantic information about the participants of a verb-noun phrase (v-np) pair (i.e., noun and verb semantics) to identify causality with a better performance over the baseline model depending merely on shallow linguistic features. The work in this paper builds on our recent work reported in Riaz and Girju (2014). In that previous model, we identified the semantic classes of nouns and verbs with a high and low tendency to encode causation. For example, a named entity such as LOCATION may have the least tendency to encode causation. We leveraged such information about nouns to filter false positives. Similarly, we utilized the TimeBank’s (Pustejovsky et al., 2006) classification of verbal events (i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting) and their definitions to claim that the reporting events (e.g., say, tell, etc.) 161 Proceedings of the SIGDIAL 2014 Conference,</context>
<context position="4867" citStr="Riaz and Girju (2014)" startWordPosition="760" endWordPosition="763">In this paper, we extend our previous model by acquiring and exploiting the following three novel types of knowledge: 1. We learn the information about tendencies of various verb frames to encode causation. For example, our model identifies if the subject of verb “destroy” (“occur”) has a high (low) tendency to encode causation. Such information helps gain performance by exploiting causal semantics of each verb frame separately. We also learn and incorporate information about the verb frames in general e.g., how likely it is for the subject of any verb to encode causation with its verb. 2. In Riaz and Girju (2014), we utilized the TimeBank’s definition of reporting events to argue that such events have the least tendency to encode causation. Instead of relying on human judgment we now introduce a data intensive approach to identify the TimeBank’s classes of events with the least tendency to encode causation. 3. Although, information about the nouns with the least tendency to encode causation helps to filter false positives it can lead to false negatives when metonymic readings are associated with such nouns. Therefore, we introduce a metonymy resolver on top of our current model to avoid false negative</context>
<context position="7804" citStr="Riaz and Girju, 2014" startWordPosition="1224" endWordPosition="1227">obabilities of cooccurrences of events and do not distinguish well causality from any other types of correlation (Riaz and Girju, 2013). In order to overcome this problem Riaz and Girju (2013) proposed some advanced metrics which combine probabilities of cooccurrences of events with the supervised estimates of cause and non-cause relations. Considering the importance of employing rich sources of knowledge other than linguistic features for the current task, we have recently proposed a model that incorporates semantic classes of nouns and verbs with a high and low tendency to encode causation (Riaz and Girju, 2014). In this work, we exploit information about verb frames, data-driven verb semantics and metonymies to achieve more progress on our recent work. 3 Model for Recognizing Causality In this section we provide an overview of our previous model (Riaz and Girju, 2014) for identifying causality in v-np pairs where v (np) stands for verb (noun phrase). This model works in the following two stages: (1) A supervised classifier is used to make binary predictions (i.e., the label cause (C) or non-cause (¬C)) employing linguistic features, and (2) noun and verb semantics are then combined with the predicti</context>
<context position="30458" citStr="Riaz and Girju (2014)" startWordPosition="5194" endWordPosition="5197">agreement on a total of 1,500 v-np instances. This results in a total of 1,365 instances of v-np pairs with 11.86% C pairs. In this section, we present performance of the following models (see Table 3): 1. Baseline: NB and MaxEnt (McCallum, 2002) supervised classifiers using only the shallow linguistic features (see section 3.1). 2. Basic noun and verb semantics: ILP with the addition of semantic classes of nouns without metonymy (denoted by +N! M) and the addition of semantic classes of verbs where -Cev={(R)eporting events} (denoted by +N! M+V{R}). These models represent the work proposed in Riaz and Girju (2014) (section 3). 3. Noun semantics with metonymies: ILP with the addition of noun semantics involving metonymies resolved via verb frames (denoted by +NM1), metonymies resolved via verb frames {v, gr} where gr E GR = {csubj, csubjpass, nsubj, nsubjpass, xsubj, dobj, iobj, pobj, agent} a set of core dependency relations of subjects and objects (denoted by +NM1GR ) and metonymies resolved via both verb frames and prepositions (denoted by +NM1GR + M2). 4. Verb frames and data-driven verb semantics: ILP with the addition of information about verb frames (denoted by +NM+VF where M = M1GR + M2), data-d</context>
</contexts>
<marker>Riaz, Girju, 2014</marker>
<rawString>Mehwish Riaz and Roxana Girju 2014. Recognizing Causality in Verb-Noun Pairs via Noun and Verb Semantics. Proceedings of the Workshop on Computational Approaches to Causality in Language EACL, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A Linear Programming Formulation for Global Inference in Natural Language Tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Annual Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="4104" citStr="Roth and Yih, 2004" startWordPosition="634" endWordPosition="637">ch information about nouns to filter false positives. Similarly, we utilized the TimeBank’s (Pustejovsky et al., 2006) classification of verbal events (i.e., Occurrence, Perception, Aspectual, State, I State, I Action and Reporting) and their definitions to claim that the reporting events (e.g., say, tell, etc.) 161 Proceedings of the SIGDIAL 2014 Conference, pages 161–170, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics just describe and narrate other events instead of encoding causality with them. We proposed an Integer Linear Programming (ILP) model (Roth and Yih, 2004; Do et al., 2011) to combine noun and verb semantics with the decisions of a supervised classifier which only relies on linguistic features. In this paper, we extend our previous model by acquiring and exploiting the following three novel types of knowledge: 1. We learn the information about tendencies of various verb frames to encode causation. For example, our model identifies if the subject of verb “destroy” (“occur”) has a high (low) tendency to encode causation. Such information helps gain performance by exploiting causal semantics of each verb frame separately. We also learn and incorpo</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih 2004. A Linear Programming Formulation for Global Inference in Natural Language Tasks. In Proceedings of the Annual Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Alex Lascarides</author>
</authors>
<title>Using automatically labelled examples to classify rhetorical relations: An assessment.</title>
<date>2008</date>
<journal>Journal of Natural Language Engineering Volume</journal>
<volume>14</volume>
<pages>369--416</pages>
<contexts>
<context position="1860" citStr="Sporleder and Lascarides, 2008" startWordPosition="282" endWordPosition="285">ween two verbs, a verb and a noun, two discourse segments, etc. In this research, we focus on identifying causality encoded between a verb and a noun (or noun phrase). For example, consider the following example: 1. At least 1,833 people died in the hurricane. In example (1), the verb-noun phrase pair “died”-“the hurricane” encodes causality where event “died” is the effect of “hurricane” event. Previously several approaches have been proposed to identify causality between two verbs (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, the problem of identifying causality in verb-noun pairs has not received a considerable attention. For example, Do et al. (2011) have studied this task but they worked only with a list of predefined nouns representing events. In this work, we focus on the linguistic construction of verb-noun (or noun phrase) pairs where noun can be of any semantic type. Traditional approaches for identifying causality mainly employ linguistic features (e.g., lexical items, part-of-speech tags of words, etc.) in the framework of supervised learning (Gir</context>
<context position="6179" citStr="Sporleder and Lascarides, 2008" startWordPosition="973" endWordPosition="976">new model and discuss its performance in sections 4 and 5. Section 6 concludes the current research. 2 Relevant Work In Natural Language Processing (NLP), researchers are showing lots of interest in the task of identifying causality due to its various applications e.g., question answering (Girju, 2003), summarization (Chklovski and Pantel, 2004), future prediction (Radinsky and Horvitz, 2013), etc. Several approaches have been proposed to identify causality in pairs of verbal events (Bethard and Martin, 2008; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and discourse segments (Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However causality a pervasive relation of language can be encoded via various linguistic constructions. For example, verbs and nouns are the key components of language to represent events. Therefore in this work we focus on identifying causality in verbnoun pairs. Previously researchers have followed the path of utilizing linguistic features in the framework of supervised learning (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). Though linguistic features are important but o</context>
</contexts>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>Caroline Sporleder and Alex Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: An assessment. Journal of Natural Language Engineering Volume 14 Issue 3, July 2008 Pages 369−416.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology and North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="29447" citStr="Toutanova et al., 2003" startWordPosition="5024" endWordPosition="5027">hem. If any of two methods of metonymy resolution predicts the association of metonymy with np then we add v-np to the set M used in ILP (see section 3.2). 5 Evaluation and Discussion In this section we present experiments and discussion on the performance achieved for the current task. In order to evaluate our model, we generated a test set of instances of v-np pairs. For this purpose, we collected three wiki articles on the topics of Hurricane Katrina, Iraq War and Egyptian Revolution of 2011. We apply a part-of-speech tagger and a dependency parser on all sentences of these three articles (Toutanova et al., 2003; Marneffe et al., 2006). We extracted all v-np pairs from each sentence of these articles. For each of the these three articles, we selected first 500 instances of vnp pairs. Two annotators were asked to provide the labels C and -C to the instances of v-np pairs using the annotation guidelines from Riaz and Girju (2010). We have achieved a 0.64 kappa score for the human inter-annotator agreement on a total of 1,500 v-np instances. This results in a total of 1,365 instances of v-np pairs with 11.86% C pairs. In this section, we present performance of the following models (see Table 3): 1. Base</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network. In Proceedings of Human Language Technology and North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="false">
<authors>
<author>cnp Event</author>
<author>Purpose Goal</author>
</authors>
<title>Cause, Internal cause, External cause, Result, Means, Reason, Phenomena, Coordi- nated event, Action, Activity, Circumstances, Desired goal, Explanation _cnp Artist,</title>
<location>Performer, Duration, Time, Place, Distributor, Area, Path, Direction, Sub-region Frequency, Body part, Area, Degree, Angle, Fixed</location>
<note>location, Path shape, Addressee, Interval</note>
<marker>Event, Goal, </marker>
<rawString>cnp Event, Goal, Purpose, Cause, Internal cause, External cause, Result, Means, Reason, Phenomena, Coordi- nated event, Action, Activity, Circumstances, Desired goal, Explanation _cnp Artist, Performer, Duration, Time, Place, Distributor, Area, Path, Direction, Sub-region Frequency, Body part, Area, Degree, Angle, Fixed location, Path shape, Addressee, Interval</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
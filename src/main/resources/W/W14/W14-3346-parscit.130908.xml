<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.658888">
A Systematic Comparison of Smoothing Techniques for Sentence-Level
BLEU
</title>
<author confidence="0.688093">
Boxing Chen and Colin Cherry
</author>
<affiliation confidence="0.5767005">
National Research Council Canada
first.last@nrc-cnrc.gc.ca
</affiliation>
<sectionHeader confidence="0.981027" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999804235294118">
BLEU is the de facto standard machine
translation (MT) evaluation metric. How-
ever, because BLEU computes a geo-
metric mean of n-gram precisions, it of-
ten correlates poorly with human judg-
ment on the sentence-level. There-
fore, several smoothing techniques have
been proposed. This paper systemati-
cally compares 7 smoothing techniques
for sentence-level BLEU. Three of them
are first proposed in this paper, and they
correlate better with human judgments on
the sentence-level than other smoothing
techniques. Moreover, we also compare
the performance of using the 7 smoothing
techniques in statistical machine transla-
tion tuning.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995173">
Since its invention, BLEU (Papineni et al., 2002)
has been the most widely used metric for both
machine translation (MT) evaluation and tuning.
Many other metrics correlate better with human
judgments of translation quality than BLEU, as
shown in recent WMT Evaluation Task reports
(Callison-Burch et al., 2011; Callison-Burch et al.,
2012). However, BLEU remains the de facto stan-
dard evaluation and tuning metric. This is proba-
bly due to the following facts:
</bodyText>
<listItem confidence="0.899759777777778">
1. BLEU is language independent (except for
word segmentation decisions).
2. BLEU can be computed quickly. This is im-
portant when choosing a tuning metric.
3. BLEU seems to be the best tuning metric
from a quality point of view - i.e., models
trained using BLEU obtain the highest scores
from humans and even from other metrics
(Cer et al., 2010).
</listItem>
<bodyText confidence="0.99990928">
One of the main criticisms of BLEU is that it
has a poor correlation with human judgments on
the sentence-level. Because it computes a geomet-
ric mean of n-gram precisions, if a higher order
n-gram precision (eg. n = 4) of a sentence is
0, then the BLEU score of the entire sentence is
0, no matter how many 1-grams or 2-grams are
matched. Therefore, several smoothing techniques
for sentence-level BLEU have been proposed (Lin
and Och, 2004; Gao and He, 2013).
In this paper, we systematically compare 7
smoothing techniques for sentence-level BLEU.
Three of them are first proposed in this paper, and
they correlate better with human judgments on the
sentence-level than other smoothing techniques on
the WMT metrics task. Moreover, we compare
the performance of using the 7 smoothing tech-
niques in statistical machine translation tuning on
NIST Chinese-to-English and Arabic-to-English
tasks. We show that when tuning optimizes the
expected sum of these sentence-level metrics (as
advocated by Cherry and Foster (2012) and Gao
and He (2013) among others), all of these metrics
perform similarly in terms of their ability to pro-
duce strong BLEU scores on a held-out test set.
</bodyText>
<sectionHeader confidence="0.983209" genericHeader="introduction">
2 BLEU and smoothing
</sectionHeader>
<subsectionHeader confidence="0.535504">
2.1 BLEU
</subsectionHeader>
<bodyText confidence="0.999599333333333">
Suppose we have a translation T and its reference
R, BLEU is computed with precision P(N, T, R)
and brevity penalty BP(T,R):
</bodyText>
<equation confidence="0.902263">
BLEU(N, T, R) = P(N, T, R) × BP(T, R) (1)
</equation>
<bodyText confidence="0.866189">
where P(N, T, R) is the geometric mean of n-
gram precisions:
</bodyText>
<equation confidence="0.822353375">
P(N, T, R) =
1
N
(2)
N
ri
n=1
�pn
</equation>
<page confidence="0.976534">
362
</page>
<bodyText confidence="0.384988">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362–367,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
and where:
</bodyText>
<equation confidence="0.9234235">
mn (3)
ln
</equation>
<bodyText confidence="0.9997805">
mn is the number of matched n-grams between
translation T and its reference R, and ln is the total
number of n-grams in the translation T. BLEU’s
brevity penalty punishes the score if the translation
length len(T) is shorter than the reference length
len(R), using this equation:
</bodyText>
<equation confidence="0.914876">
BP(T, R) = min 1.0, exp 1
( ( len(R)
len
) J) R)11 4
( )
</equation>
<subsectionHeader confidence="0.999777">
2.2 Smoothing techniques
</subsectionHeader>
<bodyText confidence="0.999849833333333">
The original BLEU was designed for the
document-level; as such, it required no smooth-
ing, as some sentence would have at least one 4-
gram match. We now describe 7 smoothing tech-
niques that work better for sentence-level evalua-
tion. Suppose we consider matching n-grams for
n = 1... N (typically, N = 4). Let mn be the
original match count, and m′n be the modified n-
gram match count.
Smoothing 1: if the number of matched n-
grams is 0, we use a small positive value ǫ to re-
place the 0 for n ranging from 1 to N. The number
</bodyText>
<equation confidence="0.9333885">
ǫ is set empirically.
m′n = ǫ, if mn = 0. (5)
</equation>
<bodyText confidence="0.988582">
Smoothing 2: this smoothing technique was
proposed in (Lin and Och, 2004). It adds 1 to the
matched n-gram count and the total n-gram count
for n ranging from 2 to N.
</bodyText>
<equation confidence="0.9999075">
m′n = mn + 1, for n in 2 ... N, (6)
l′n = ln + 1, for n in 2 ... N. (7)
</equation>
<bodyText confidence="0.9903188">
Smoothing 3: this smoothing technique is im-
plemented in the NIST official BLEU toolkit
mteval-v13a.pl.1 The algorithm is given below. It
assigns a geometric sequence starting from 1/2 to
the n-grams with 0 matches.
</bodyText>
<listItem confidence="0.997639857142857">
1. invcnt = 1
2. for n in 1 to N
3. if mn = 0
4. invcnt = invcnt x 2
5. m′n = 1/invcnt
6. endif
7. endfor
</listItem>
<footnote confidence="0.98573">
1available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/
</footnote>
<bodyText confidence="0.99701425">
Smoothing 4: this smoothing technique is novel
to this paper. We modify Smoothing 3 to address
the concern that shorter translations may have in-
flated precision values due to having smaller de-
nominators; therefore, we give them proportion-
ally smaller smoothed counts. Instead of scaling
invcnt with a fixed value of 2, we replace line 4 in
Smoothing 3’s algorithm with Equation 8 below.
</bodyText>
<equation confidence="0.999202333333333">
K
invcnt = invcnt x (8)
ln(len(T))
</equation>
<bodyText confidence="0.9995026">
It assigns larger values to invcnt for shorter sen-
tences, resulting in a smaller smoothed count. K
is set empirically.
Smoothing 5: this smoothing technique is also
novel to this paper. It is inspired by the intuition
that matched counts for similar values of n should
be similar. To a calculate the n-gram matched
count, it averages the n − 1, n and n + 1 –gram
matched counts. We define m′0 = m1 + 1, and
calculate m′n for n &gt; 0 as follows:
</bodyText>
<equation confidence="0.980234333333333">
m′n = (9)
m′n−1 + mn + mn+1
3
</equation>
<bodyText confidence="0.9995304">
Smoothing 6: this smoothing technique was
proposed in (Gao and He, 2013). It interpolates
the maximum likelihood estimate of the precision
pn with a prior estimate p0 n. The prior is estimated
by assuming that the ratio between pn and pn−1
will be the same as that between pn−1 and pn−2.
Formally, the precisions of lower order n-grams,
i.e., p1 and p2, are not smoothed, while the pre-
cisions of higher order n-grams, i.e. n &gt; 2, are
smoothed as follows:
</bodyText>
<equation confidence="0.9871652">
pn = mn + αp0n (10)
ln + α
where α is set empirically, and p0n is computed as
p0n = pn−1 x pn−1
pn−2 (11)
</equation>
<bodyText confidence="0.998186166666667">
Smoothing 7: this novel smoothing technique
combines smoothing 4 and smoothing 5. That is,
we first compute a smoothed count for those 0
matched n-gram counts using Smoothing 4, and
then take the average of three counts to set the fi-
nal matched n-gram count as in Equation 9.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9988995">
We carried out two series of experiments. The
7 smoothing techniques were first compared in
</bodyText>
<equation confidence="0.714383">
pn =
</equation>
<page confidence="0.99566">
363
</page>
<table confidence="0.940721666666667">
set year lang. #system #seg. pair
dev 2008 xx-en 43 7,804
test1 2012 xx-en 49 34,909
test2 2013 xx-en 94 281,666
test3 2012 en-xx 54 47,875
test4 2013 en-xx 95 220,808
</table>
<tableCaption confidence="0.999833">
Table 1: Statistics of the WMT dev and test sets.
</tableCaption>
<bodyText confidence="0.99871425">
the metric task as evaluation metrics, then they
were compared as metrics for tuning SMT systems
to maximize the sum of expected sentence-level
BLEU scores.
</bodyText>
<subsectionHeader confidence="0.999108">
3.1 Evaluation task
</subsectionHeader>
<bodyText confidence="0.983359555555555">
We first compare the correlations with human
judgment for the 7 smoothing techniques on WMT
data; the development set (dev) is the WMT 2008
all-to-English data; the test sets are the WMT 2012
and WMT 2013 all-to-English, and English-to-all
submissions. The languages “all” (“xx” in Ta-
ble 1) include French, Spanish, German, Czech
and Russian. Table 1 summarizes the dev/test set
statistics.
Following WMT 2013’s metric task (Mach´aˇcek
and Bojar, 2013), for the segment level, we use
Kendall’s rank correlation coefficient -r to measure
the correlation with human judgment:
#concordant-pairs − #discordant-pairs
#concordant-pairs + #discordant-pairs
(12)
We extract all pairwise comparisons where one
system’s translation of a particular segment was
judged to be better than the other system’s trans-
lation, i.e., we removed all tied human judg-
ments for a particular segment. If two transla-
tions for a particular segment are assigned the
same BLEU score, then the #concordant-pairs
and #discordant-pairs both get a half count. In
this way, we can keep the number of total pairs
consistent for all different smoothing techniques.
For the system-level, we used Spearman’s rank
correlation coefficient p and Pearson’s correla-
tion coefficient y to measure the correlation of
the metric with human judgments of translation.
If we compute document-level BLEU as usual,
all 7 smoothing techniques actually get the same
result, as document-level BLEU does not need
smoothing. We therefore compute the document-
level BLEU as the weighted average of sentence-
level BLEU, with the weights being the reference
</bodyText>
<table confidence="0.999371090909091">
Into-English
smooth seg -r sys y sys p
crp – 0.720 0.887
0 0.165 0.759 0.887
1 0.224 0.760 0.887
2 0.226 0.757 0.887
3 0.224 0.760 0.887
4 0.228 0.763 0.887
5 0.234 0.765 0.887
6 0.230 0.754 0.887
7 0.236 0.766 0.887
</table>
<tableCaption confidence="0.96044875">
Table 2: Correlations with human judgment on
WMT data for Into-English task. Results are av-
eraged on 4 test sets. “crp” is the origianl IBM
corpus-level BLEU.
</tableCaption>
<equation confidence="0.9151805">
lengths:
_P, len(Ri)BLEUi
BLEUd = D (13)
_i=1 len(Ri)
</equation>
<bodyText confidence="0.999861666666667">
where BLEUi is the BLEU score of sentence i,
and D is the size of the document in sentences.
We first set the free parameters of each smooth-
ing method by grid search to optimize the
sentence-level score on the dev set. We set E to 0.1
for Smoothing 1; K = 5 for Smoothing 4; α = 5
for Smoothing 6.
Tables 2 and 3 report our results on the met-
rics task. We compared the 7 smoothing tech-
niques described in Section 2.2 to a baseline with
no smoothing (Smoothing 0). All scores match n-
grams n = 1 to 4. Smoothing 3 is implemented
in the standard official NIST evaluation toolkit
(mteval-v13a.pl). Results are averaged across the
4 test sets.
All smoothing techniques improved sentence-
level correlations (-r) over no smoothing. Smooth-
ing method 7 got the best sentence-level results on
both the Into-English and Out-of-English tasks.
On the system-level, our weighted average of
sentence-level BLEU scores (see Equation 13)
achieved a better correlation with human judge-
ment than the original IBM corpus-level BLEU.
However, the choice of which smoothing tech-
nique is used in the average did not make a very
big difference; in particular, the system-level rank
correlation p did not change for 13 out of 14 cases.
These methods help when comparing one hypoth-
esis to another, but taken as a part of a larger aver-
age, all seven methods assign relatively low scores
</bodyText>
<equation confidence="0.651801">
-r =
</equation>
<page confidence="0.982438">
364
</page>
<table confidence="0.998500181818182">
Out-of-English
smooth seg τ sys y sys p
crp – 0.712 0.744
0 0.119 0.715 0.744
1 0.178 0.722 0.748
2 0.180 0.725 0.744
3 0.178 0.724 0.744
4 0.181 0.727 0.744
5 0.184 0.731 0.744
6 0.182 0.725 0.744
7 0.187 0.734 0.744
</table>
<tableCaption confidence="0.941678">
Table 3: Correlations with human judgment on
WMT data for Out-of-English task. Results are
averaged on 4 test sets. “crp” is the origianl IBM
corpus-level BLEU.
</tableCaption>
<bodyText confidence="0.684864">
to the cases that require smoothing, resulting in
similar system-level rankings.
</bodyText>
<subsectionHeader confidence="0.999528">
3.2 Tuning task
</subsectionHeader>
<bodyText confidence="0.999992769230769">
In this section, we explore the various BLEU
smoothing methods in the context of SMT param-
eter tuning, which is used to set the decoder’s
linear model weights w. In particular, we use
a tuning method that maximizes the sum of ex-
pected sentence-level BLEU scores, which has
been shown to be a simple and effective method
for tuning with large feature sets by both Cherry
and Foster (2012) and Gao and He (2013), but
which requires a smoothed sentence-level BLEU
approximation. For a source sentence fi, the prob-
ability of the kth translation hypothesis eki is its ex-
ponentiated and normalized model score:
</bodyText>
<equation confidence="0.95208775">
(ek|f) exp (scorew (ek, fi))
P
w z i =Ek′ exp(scorew(ek′
i , fi))
</equation>
<bodyText confidence="0.993256333333333">
where k′ ranges over all hypotheses in a K-best
list.2 We then use stochastic gradient descent
(SGD) to minimize:
</bodyText>
<equation confidence="0.9924135">
[ ( )i
len(Ri) x EP. BLEU(ek i , fi)
</equation>
<bodyText confidence="0.999775714285714">
Note that we scale the expectation by reference
length to place more emphasis on longer sen-
tences. We set the regularization parameter A,
which determines the trade-off between a high ex-
pected BLEU and a small norm, to A = 10.
Following Cherry and Foster (2012), we tune
with a MERT-like batch architecture: fixing a set
</bodyText>
<footnote confidence="0.337874">
2We use K = 100 in our experiments.
</footnote>
<table confidence="0.997037">
corpus # segs # en tok
Chinese-English
train 10.1M 283M
tune 1,506 161K
MT06 1,664 189K
MT08 1,357 164K
Arabic-English
train 1,512K 47.8M
tune 1,664 202K
MT08 1,360 205K
MT09 1,313 187K
</table>
<tableCaption confidence="0.8981265">
Table 4: Statistics of the NIST Chinese-English
and Arabic-English data.
</tableCaption>
<bodyText confidence="0.999812628571428">
of K-best lists, optimizing, and then re-decoding
the entire dev set to K-best and aggregating with
previous lists to create a better K-best approxima-
tion. We repeat this outer loop 15 times.
We carried out experiments in two different set-
tings, both involving data from NIST Open MT
2012.3 The first setting is based on data from the
Chinese-to-English constrained track, comprising
about 283 million English running words. The
second setting uses NIST 2012 Arabic-to-English
data, but excludes the UN data. There are about
47.8 million English running words in these train-
ing data. The dev set (tune) for the Chinese-to-
English task was taken from the NIST 2005 eval-
uation set, augmented with some web-genre mate-
rial reserved from other NIST corpora. We test on
the evaluation sets from NIST 2006 and 2008. For
the Arabic-to-English task, we use the evaluation
sets from NIST 2006, 2008, and 2009 as our dev
set and two test sets, respectively. Table 4 summa-
rizes the training, dev and test sets.
Experiments were carried out with an in-house,
state-of-the-art phrase-based system. Each corpus
was word-aligned using IBM2, HMM, and IBM4
models, and the phrase table was the union of
phrase pairs extracted from these separate align-
ments, with a length limit of 7. The translation
model (TM) was smoothed in both directions with
Kneser-Ney smoothing (Chen et al., 2011). We
use the hierarchical lexicalized reordering model
(RM) (Galley and Manning, 2008), with a dis-
tortion limit of 7. Other features include lexi-
cal weighting in both directions, word count, a
distance-based RM, a 4-gram LM trained on the
target side of the parallel data, and a 6-gram En-
</bodyText>
<footnote confidence="0.619846">
3http://www.nist.gov/itl/iad/mig/openmt12.cfm
</footnote>
<equation confidence="0.924827">
�
A||w||2 −
i
</equation>
<page confidence="0.99368">
365
</page>
<table confidence="0.999073">
Tune std MT06 std MT08 std
0 27.6 0.1 35.6 0.1 29.0 0.2
1 27.6 0.0 35.7 0.1 29.1 0.1
2 27.5 0.1 35.8 0.1 29.1 0.1
3 27.6 0.1 35.8 0.1 29.1 0.1
4 27.6 0.1 35.7 0.2 29.1 0.2
5 27.6 0.1 35.5 0.1 28.9 0.2
6 27.5 0.1 35.7 0.1 29.0 0.2
7 27.6 0.1 35.6 0.1 29.0 0.1
</table>
<tableCaption confidence="0.887967333333333">
Table 5: Chinese-to-English Results for the small
feature set tuning task. Results are averaged across
5 replications; std is the standard deviation.
</tableCaption>
<bodyText confidence="0.995402428571429">
glish Gigaword LM.
We also conducted a set of experiments with a
much larger feature set. This system used only
GIZA++ for word alignment, increased the distor-
tion limit from 7 to 9, and is trained on a high-
quality subset of the parallel corpora used ear-
lier. Most importantly, it includes the full set of
sparse phrase-pair features used by both Hopkins
and May (2011) and Cherry and Foster (2012),
which results in nearly 7,000 features.
Our evaluation metric is the original IBM
BLEU, which performs case-insensitive matching
of n-grams up to n = 4. We perform random
replications of parameter tuning, as suggested by
Clark et al. (2011). Each replication uses a differ-
ent random seed to determine the order in which
SGD visits tuning sentences. We test for signifi-
cance using the MultEval tool,4 which uses a strat-
ified approximate randomization test to account
for multiple replications. We report results aver-
aged across replications as well as standard devia-
tions, which indicate optimizer stability.
Results for the small feature set are shown in
Tables 5 and 6. All 7 smoothing techniques, as
well as the no smoothing baseline, all yield very
similar results on both Chinese and Arabic tasks.
We did not find any two results to be significantly
different. This is somewhat surprising, as other
groups have suggested that choosing an appropri-
ate BLEU approximation is very important. In-
stead, our experiments indicate that the selected
BLEU smoothing method is not very important.
The large-feature experiments were only con-
ducted with the most promising methods accord-
ing to correlation with human judgments:
</bodyText>
<footnote confidence="0.996922">
4available at https://github.com/jhclark/multeval
</footnote>
<table confidence="0.981291222222222">
Tune std MT08 std MT09 std
0 46.9 0.1 46.5 0.1 49.1 0.1
1 46.9 0.0 46.4 0.1 49.1 0.1
2 46.9 0.0 46.4 0.1 49.0 0.1
3 47.0 0.0 46.5 0.1 49.2 0.1
4 47.0 0.0 46.5 0.1 49.2 0.1
5 46.9 0.0 46.4 0.1 49.1 0.1
6 47.0 0.0 46.4 0.1 49.1 0.1
7 47.0 0.0 46.4 0.1 49.0 0.1
</table>
<tableCaption confidence="0.993829666666667">
Table 6: Arabic-to-English Results for the small
feature set tuning task. Results are averaged across
5 replications; std is the standard deviation.
</tableCaption>
<table confidence="0.998461">
Tune std MT06 std MT08 std
mira 29.9 0.1 38.0 0.1 31.0 0.1
0 29.5 0.1 37.9 0.1 31.4 0.3
2 29.6 0.3 38.0 0.2 31.1 0.2
4 29.9 0.2 38.1 0.1 31.2 0.2
6 29.7 0.1 37.9 0.2 31.0 0.2
7 29.7 0.2 38.0 0.2 31.2 0.1
</table>
<tableCaption confidence="0.965743">
Table 7: Chinese-to-English Results for the large
</tableCaption>
<bodyText confidence="0.881187185185185">
feature set tuning task. Results are averaged
across 5 replications; std is the standard deviation.
Significant improvements over the no-smoothing
baseline (p ≤ 0.05) are marked in bold.
0: No smoothing (baseline)
2: Add 1 smoothing (Lin and Och, 2004)
4: Length-scaled pseudo-counts (this paper)
6: Interpolation with a precision prior (Gao and
He, 2013)
7: Combining Smoothing 4 with the match in-
terpolation of Smoothing 5 (this paper)
The results of the large feature set experiments are
shown in Table 7 for Chinese-to-English and Ta-
ble 8 for Arabic-to-English. For a sanity check, we
compared these results to tuning with our very sta-
ble Batch k-best MIRA implementation (Cherry
and Foster, 2012), listed as mira, which shows that
all of our expected BLEU tuners are behaving rea-
sonably, if not better than expected.
Comparing the various smoothing methods in
the large feature scenario, we are able to see signif-
icant improvements over the no-smoothing base-
line. Notably, Method 7 achieves a significant
improvement over the no-smoothing baseline in 3
out of 4 scenarios, more than any other method.
Unfortunately, in the Chinese-English MT08 sce-
nario, the no-smoothing baseline significantly out-
</bodyText>
<page confidence="0.995576">
366
</page>
<table confidence="0.994318714285715">
Tune std MT08 std MT09 std
mira 47.9 0.1 47.3 0.0 49.3 0.1
0 48.1 0.1 47.2 0.1 49.5 0.1
2 48.0 0.1 47.4 0.1 49.7 0.1
4 48.1 0.2 47.4 0.1 49.6 0.1
6 48.2 0.0 47.3 0.1 49.7 0.1
7 48.1 0.1 47.3 0.1 49.7 0.1
</table>
<tableCaption confidence="0.987148">
Table 8: Arabic-to-English Results for the large
</tableCaption>
<bodyText confidence="0.979663363636364">
feature set tuning task. Results are averaged
across 5 replications; std is the standard deviation.
Significant improvements over the no-smoothing
baseline (p ≤ 0.05) are marked in bold.
performs all smoothed BLEU methods, making it
difficult to draw any conclusions at all from these
experiments. We had hoped to see at least a clear
improvement in the tuning set, and one does see
a nice progression as smoothing improves in the
Chinese-to-English scenario, but no correspond-
ing pattern emerges for Arabic-to-English.
</bodyText>
<sectionHeader confidence="0.999664" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999459">
In this paper, we compared seven smoothing
techniques for sentence-level BLEU. Three of
them are newly proposed in this paper. The
new smoothing techniques got better sentence-
level correlations with human judgment than other
smoothing techniques. On the other hand, when
we compare the techniques in the context of tun-
ing, using a method that requires sentence-level
BLEU approximations, they all have similar per-
formance.
</bodyText>
<sectionHeader confidence="0.99931" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999874952380952">
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22–64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
Daniel Cer, Christopher D. Manning, and Daniel Juraf-
sky. 2010. The best lexical metric for phrase-based
statistical mt system optimization. In Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics, pages 555–563, Los
Angeles, California, June. Association for Compu-
tational Linguistics.
Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and transform-
ing feature functions: New ways to smooth phrase
tables. In MT Summit 2011.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
NAACL 2012.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In ACL 2011.
Michel Galley and C. D. Manning. 2008. A simple
and effective hierarchical phrase reordering model.
In EMNLP 2008, pages 848–856, Hawaii, October.
Jianfeng Gao and Xiaodong He. 2013. Training mrf-
based phrase translation models using gradient as-
cent. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 450–459, Atlanta, Georgia, June.
Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In EMNLP 2011.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic evaluation of machine translation quality us-
ing longest common subsequence and skip-bigram
statistics. In Proceedings of the 42nd Meeting
of the Association for Computational Linguistics
(ACL’04), Main Volume, pages 605–612, Barcelona,
Spain, July.
Matouˇs Mach´aˇcek and Ondˇrej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45–51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311–318,
Philadelphia, July. ACL.
</reference>
<page confidence="0.998364">
367
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.734274">
<title confidence="0.912078">A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU</title>
<author confidence="0.889408">Boxing Chen</author>
<author confidence="0.889408">Colin</author>
<affiliation confidence="0.974904">National Research Council</affiliation>
<email confidence="0.991147">first.last@nrc-cnrc.gc.ca</email>
<abstract confidence="0.999097888888889">is the facto machine translation (MT) evaluation metric. However, because BLEU computes a geomean of precisions, it ofcorrelates poorly with human judgon the sentence-level. fore, several smoothing techniques have been proposed. This paper systematically compares 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1122" citStr="Callison-Burch et al., 2011" startWordPosition="161" endWordPosition="164">iques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning. 1 Introduction Since its invention, BLEU (Papineni et al., 2002) has been the most widely used metric for both machine translation (MT) evaluation and tuning. Many other metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et al., 2011; Callison-Burch et al., 2012). However, BLEU remains the de facto standard evaluation and tuning metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a tuning metric. 3. BLEU seems to be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sent</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1152" citStr="Callison-Burch et al., 2012" startWordPosition="165" endWordPosition="168">. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning. 1 Introduction Since its invention, BLEU (Papineni et al., 2002) has been the most widely used metric for both machine translation (MT) evaluation and tuning. Many other metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et al., 2011; Callison-Burch et al., 2012). However, BLEU remains the de facto standard evaluation and tuning metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a tuning metric. 3. BLEU seems to be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sentence-level. Because it compute</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
<author>Daniel Jurafsky</author>
</authors>
<title>The best lexical metric for phrase-based statistical mt system optimization.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>555--563</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="1619" citStr="Cer et al., 2010" startWordPosition="247" endWordPosition="250"> judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et al., 2011; Callison-Burch et al., 2012). However, BLEU remains the de facto standard evaluation and tuning metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a tuning metric. 3. BLEU seems to be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sentence-level. Because it computes a geometric mean of n-gram precisions, if a higher order n-gram precision (eg. n = 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, no matter how many 1-grams or 2-grams are matched. Therefore, several smoothing techniques for sentence-level BLEU have been proposed (Lin and Och, 2004; Gao and He, 2013). In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, </context>
</contexts>
<marker>Cer, Manning, Jurafsky, 2010</marker>
<rawString>Daniel Cer, Christopher D. Manning, and Daniel Jurafsky. 2010. The best lexical metric for phrase-based statistical mt system optimization. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 555–563, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
<author>Howard Johnson</author>
</authors>
<title>Unpacking and transforming feature functions: New ways to smooth phrase tables.</title>
<date>2011</date>
<booktitle>In MT Summit</booktitle>
<contexts>
<context position="13760" citStr="Chen et al., 2011" startWordPosition="2376" endWordPosition="2379"> We test on the evaluation sets from NIST 2006 and 2008. For the Arabic-to-English task, we use the evaluation sets from NIST 2006, 2008, and 2009 as our dev set and two test sets, respectively. Table 4 summarizes the training, dev and test sets. Experiments were carried out with an in-house, state-of-the-art phrase-based system. Each corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram En3http://www.nist.gov/itl/iad/mig/openmt12.cfm � A||w||2 − i 365 Tune std MT06 std MT08 std 0 27.6 0.1 35.6 0.1 29.0 0.2 1 27.6 0.0 35.7 0.1 29.1 0.1 2 27.5 0.1 35.8 0.1 29.1 0.1 3 27.6 0.1 35.8 0.1 29.1 0.1 4 27.6 0.1 35.7 0.2 29.1 0.2 5 27.6 0.1 35.5 0.1 28.9 0.2 6 27.5 0.1 35.7 0.1 29.0 0.2 7 27.6 0.1 35.6 0.</context>
</contexts>
<marker>Chen, Kuhn, Foster, Johnson, 2011</marker>
<rawString>Boxing Chen, Roland Kuhn, George Foster, and Howard Johnson. 2011. Unpacking and transforming feature functions: New ways to smooth phrase tables. In MT Summit 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In NAACL</booktitle>
<contexts>
<context position="2641" citStr="Cherry and Foster (2012)" startWordPosition="412" endWordPosition="415">l BLEU have been proposed (Lin and Och, 2004; Gao and He, 2013). In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques on the WMT metrics task. Moreover, we compare the performance of using the 7 smoothing techniques in statistical machine translation tuning on NIST Chinese-to-English and Arabic-to-English tasks. We show that when tuning optimizes the expected sum of these sentence-level metrics (as advocated by Cherry and Foster (2012) and Gao and He (2013) among others), all of these metrics perform similarly in terms of their ability to produce strong BLEU scores on a held-out test set. 2 BLEU and smoothing 2.1 BLEU Suppose we have a translation T and its reference R, BLEU is computed with precision P(N, T, R) and brevity penalty BP(T,R): BLEU(N, T, R) = P(N, T, R) × BP(T, R) (1) where P(N, T, R) is the geometric mean of ngram precisions: P(N, T, R) = 1 N (2) N ri n=1 �pn 362 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 362–367, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association f</context>
<context position="11340" citStr="Cherry and Foster (2012)" startWordPosition="1966" endWordPosition="1969"> with human judgment on WMT data for Out-of-English task. Results are averaged on 4 test sets. “crp” is the origianl IBM corpus-level BLEU. to the cases that require smoothing, resulting in similar system-level rankings. 3.2 Tuning task In this section, we explore the various BLEU smoothing methods in the context of SMT parameter tuning, which is used to set the decoder’s linear model weights w. In particular, we use a tuning method that maximizes the sum of expected sentence-level BLEU scores, which has been shown to be a simple and effective method for tuning with large feature sets by both Cherry and Foster (2012) and Gao and He (2013), but which requires a smoothed sentence-level BLEU approximation. For a source sentence fi, the probability of the kth translation hypothesis eki is its exponentiated and normalized model score: (ek|f) exp (scorew (ek, fi)) P w z i =Ek′ exp(scorew(ek′ i , fi)) where k′ ranges over all hypotheses in a K-best list.2 We then use stochastic gradient descent (SGD) to minimize: [ ( )i len(Ri) x EP. BLEU(ek i , fi) Note that we scale the expectation by reference length to place more emphasis on longer sentences. We set the regularization parameter A, which determines the trade-</context>
<context position="14919" citStr="Cherry and Foster (2012)" startWordPosition="2586" endWordPosition="2589">35.5 0.1 28.9 0.2 6 27.5 0.1 35.7 0.1 29.0 0.2 7 27.6 0.1 35.6 0.1 29.0 0.1 Table 5: Chinese-to-English Results for the small feature set tuning task. Results are averaged across 5 replications; std is the standard deviation. glish Gigaword LM. We also conducted a set of experiments with a much larger feature set. This system used only GIZA++ for word alignment, increased the distortion limit from 7 to 9, and is trained on a highquality subset of the parallel corpora used earlier. Most importantly, it includes the full set of sparse phrase-pair features used by both Hopkins and May (2011) and Cherry and Foster (2012), which results in nearly 7,000 features. Our evaluation metric is the original IBM BLEU, which performs case-insensitive matching of n-grams up to n = 4. We perform random replications of parameter tuning, as suggested by Clark et al. (2011). Each replication uses a different random seed to determine the order in which SGD visits tuning sentences. We test for significance using the MultEval tool,4 which uses a stratified approximate randomization test to account for multiple replications. We report results averaged across replications as well as standard deviations, which indicate optimizer s</context>
<context position="17552" citStr="Cherry and Foster, 2012" startWordPosition="3036" endWordPosition="3039">he standard deviation. Significant improvements over the no-smoothing baseline (p ≤ 0.05) are marked in bold. 0: No smoothing (baseline) 2: Add 1 smoothing (Lin and Och, 2004) 4: Length-scaled pseudo-counts (this paper) 6: Interpolation with a precision prior (Gao and He, 2013) 7: Combining Smoothing 4 with the match interpolation of Smoothing 5 (this paper) The results of the large feature set experiments are shown in Table 7 for Chinese-to-English and Table 8 for Arabic-to-English. For a sanity check, we compared these results to tuning with our very stable Batch k-best MIRA implementation (Cherry and Foster, 2012), listed as mira, which shows that all of our expected BLEU tuners are behaving reasonably, if not better than expected. Comparing the various smoothing methods in the large feature scenario, we are able to see significant improvements over the no-smoothing baseline. Notably, Method 7 achieves a significant improvement over the no-smoothing baseline in 3 out of 4 scenarios, more than any other method. Unfortunately, in the Chinese-English MT08 scenario, the no-smoothing baseline significantly out366 Tune std MT08 std MT09 std mira 47.9 0.1 47.3 0.0 49.3 0.1 0 48.1 0.1 47.2 0.1 49.5 0.1 2 48.0 </context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In NAACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="15161" citStr="Clark et al. (2011)" startWordPosition="2625" endWordPosition="2628"> also conducted a set of experiments with a much larger feature set. This system used only GIZA++ for word alignment, increased the distortion limit from 7 to 9, and is trained on a highquality subset of the parallel corpora used earlier. Most importantly, it includes the full set of sparse phrase-pair features used by both Hopkins and May (2011) and Cherry and Foster (2012), which results in nearly 7,000 features. Our evaluation metric is the original IBM BLEU, which performs case-insensitive matching of n-grams up to n = 4. We perform random replications of parameter tuning, as suggested by Clark et al. (2011). Each replication uses a different random seed to determine the order in which SGD visits tuning sentences. We test for significance using the MultEval tool,4 which uses a stratified approximate randomization test to account for multiple replications. We report results averaged across replications as well as standard deviations, which indicate optimizer stability. Results for the small feature set are shown in Tables 5 and 6. All 7 smoothing techniques, as well as the no smoothing baseline, all yield very similar results on both Chinese and Arabic tasks. We did not find any two results to be </context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>C D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In EMNLP 2008,</booktitle>
<pages>848--856</pages>
<location>Hawaii,</location>
<contexts>
<context position="13846" citStr="Galley and Manning, 2008" startWordPosition="2388" endWordPosition="2391">ish task, we use the evaluation sets from NIST 2006, 2008, and 2009 as our dev set and two test sets, respectively. Table 4 summarizes the training, dev and test sets. Experiments were carried out with an in-house, state-of-the-art phrase-based system. Each corpus was word-aligned using IBM2, HMM, and IBM4 models, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model (TM) was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (RM) (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram En3http://www.nist.gov/itl/iad/mig/openmt12.cfm � A||w||2 − i 365 Tune std MT06 std MT08 std 0 27.6 0.1 35.6 0.1 29.0 0.2 1 27.6 0.0 35.7 0.1 29.1 0.1 2 27.5 0.1 35.8 0.1 29.1 0.1 3 27.6 0.1 35.8 0.1 29.1 0.1 4 27.6 0.1 35.7 0.2 29.1 0.2 5 27.6 0.1 35.5 0.1 28.9 0.2 6 27.5 0.1 35.7 0.1 29.0 0.2 7 27.6 0.1 35.6 0.1 29.0 0.1 Table 5: Chinese-to-English Results for the small feature set tuning task. </context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and C. D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In EMNLP 2008, pages 848–856, Hawaii, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
</authors>
<title>Training mrfbased phrase translation models using gradient ascent.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>450--459</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="2080" citStr="Gao and He, 2013" startWordPosition="329" endWordPosition="332">g metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sentence-level. Because it computes a geometric mean of n-gram precisions, if a higher order n-gram precision (eg. n = 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, no matter how many 1-grams or 2-grams are matched. Therefore, several smoothing techniques for sentence-level BLEU have been proposed (Lin and Och, 2004; Gao and He, 2013). In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques on the WMT metrics task. Moreover, we compare the performance of using the 7 smoothing techniques in statistical machine translation tuning on NIST Chinese-to-English and Arabic-to-English tasks. We show that when tuning optimizes the expected sum of these sentence-level metrics (as advocated by Cherry and Foster (2012) and Gao and He (2013) among others), a</context>
<context position="5809" citStr="Gao and He, 2013" startWordPosition="1012" endWordPosition="1015">3’s algorithm with Equation 8 below. K invcnt = invcnt x (8) ln(len(T)) It assigns larger values to invcnt for shorter sentences, resulting in a smaller smoothed count. K is set empirically. Smoothing 5: this smoothing technique is also novel to this paper. It is inspired by the intuition that matched counts for similar values of n should be similar. To a calculate the n-gram matched count, it averages the n − 1, n and n + 1 –gram matched counts. We define m′0 = m1 + 1, and calculate m′n for n &gt; 0 as follows: m′n = (9) m′n−1 + mn + mn+1 3 Smoothing 6: this smoothing technique was proposed in (Gao and He, 2013). It interpolates the maximum likelihood estimate of the precision pn with a prior estimate p0 n. The prior is estimated by assuming that the ratio between pn and pn−1 will be the same as that between pn−1 and pn−2. Formally, the precisions of lower order n-grams, i.e., p1 and p2, are not smoothed, while the precisions of higher order n-grams, i.e. n &gt; 2, are smoothed as follows: pn = mn + αp0n (10) ln + α where α is set empirically, and p0n is computed as p0n = pn−1 x pn−1 pn−2 (11) Smoothing 7: this novel smoothing technique combines smoothing 4 and smoothing 5. That is, we first compute a s</context>
<context position="11362" citStr="Gao and He (2013)" startWordPosition="1971" endWordPosition="1974">ata for Out-of-English task. Results are averaged on 4 test sets. “crp” is the origianl IBM corpus-level BLEU. to the cases that require smoothing, resulting in similar system-level rankings. 3.2 Tuning task In this section, we explore the various BLEU smoothing methods in the context of SMT parameter tuning, which is used to set the decoder’s linear model weights w. In particular, we use a tuning method that maximizes the sum of expected sentence-level BLEU scores, which has been shown to be a simple and effective method for tuning with large feature sets by both Cherry and Foster (2012) and Gao and He (2013), but which requires a smoothed sentence-level BLEU approximation. For a source sentence fi, the probability of the kth translation hypothesis eki is its exponentiated and normalized model score: (ek|f) exp (scorew (ek, fi)) P w z i =Ek′ exp(scorew(ek′ i , fi)) where k′ ranges over all hypotheses in a K-best list.2 We then use stochastic gradient descent (SGD) to minimize: [ ( )i len(Ri) x EP. BLEU(ek i , fi) Note that we scale the expectation by reference length to place more emphasis on longer sentences. We set the regularization parameter A, which determines the trade-off between a high exp</context>
<context position="17206" citStr="Gao and He, 2013" startWordPosition="2978" endWordPosition="2981">iation. Tune std MT06 std MT08 std mira 29.9 0.1 38.0 0.1 31.0 0.1 0 29.5 0.1 37.9 0.1 31.4 0.3 2 29.6 0.3 38.0 0.2 31.1 0.2 4 29.9 0.2 38.1 0.1 31.2 0.2 6 29.7 0.1 37.9 0.2 31.0 0.2 7 29.7 0.2 38.0 0.2 31.2 0.1 Table 7: Chinese-to-English Results for the large feature set tuning task. Results are averaged across 5 replications; std is the standard deviation. Significant improvements over the no-smoothing baseline (p ≤ 0.05) are marked in bold. 0: No smoothing (baseline) 2: Add 1 smoothing (Lin and Och, 2004) 4: Length-scaled pseudo-counts (this paper) 6: Interpolation with a precision prior (Gao and He, 2013) 7: Combining Smoothing 4 with the match interpolation of Smoothing 5 (this paper) The results of the large feature set experiments are shown in Table 7 for Chinese-to-English and Table 8 for Arabic-to-English. For a sanity check, we compared these results to tuning with our very stable Batch k-best MIRA implementation (Cherry and Foster, 2012), listed as mira, which shows that all of our expected BLEU tuners are behaving reasonably, if not better than expected. Comparing the various smoothing methods in the large feature scenario, we are able to see significant improvements over the no-smooth</context>
</contexts>
<marker>Gao, He, 2013</marker>
<rawString>Jianfeng Gao and Xiaodong He. 2013. Training mrfbased phrase translation models using gradient ascent. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 450–459, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In EMNLP</booktitle>
<contexts>
<context position="14890" citStr="Hopkins and May (2011)" startWordPosition="2581" endWordPosition="2584">.7 0.2 29.1 0.2 5 27.6 0.1 35.5 0.1 28.9 0.2 6 27.5 0.1 35.7 0.1 29.0 0.2 7 27.6 0.1 35.6 0.1 29.0 0.1 Table 5: Chinese-to-English Results for the small feature set tuning task. Results are averaged across 5 replications; std is the standard deviation. glish Gigaword LM. We also conducted a set of experiments with a much larger feature set. This system used only GIZA++ for word alignment, increased the distortion limit from 7 to 9, and is trained on a highquality subset of the parallel corpora used earlier. Most importantly, it includes the full set of sparse phrase-pair features used by both Hopkins and May (2011) and Cherry and Foster (2012), which results in nearly 7,000 features. Our evaluation metric is the original IBM BLEU, which performs case-insensitive matching of n-grams up to n = 4. We perform random replications of parameter tuning, as suggested by Clark et al. (2011). Each replication uses a different random seed to determine the order in which SGD visits tuning sentences. We test for significance using the MultEval tool,4 which uses a stratified approximate randomization test to account for multiple replications. We report results averaged across replications as well as standard deviation</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In EMNLP 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>605--612</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2061" citStr="Lin and Och, 2004" startWordPosition="325" endWordPosition="328">o be the best tuning metric from a quality point of view - i.e., models trained using BLEU obtain the highest scores from humans and even from other metrics (Cer et al., 2010). One of the main criticisms of BLEU is that it has a poor correlation with human judgments on the sentence-level. Because it computes a geometric mean of n-gram precisions, if a higher order n-gram precision (eg. n = 4) of a sentence is 0, then the BLEU score of the entire sentence is 0, no matter how many 1-grams or 2-grams are matched. Therefore, several smoothing techniques for sentence-level BLEU have been proposed (Lin and Och, 2004; Gao and He, 2013). In this paper, we systematically compare 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques on the WMT metrics task. Moreover, we compare the performance of using the 7 smoothing techniques in statistical machine translation tuning on NIST Chinese-to-English and Arabic-to-English tasks. We show that when tuning optimizes the expected sum of these sentence-level metrics (as advocated by Cherry and Foster (2012) and Gao and He (201</context>
<context position="4294" citStr="Lin and Och, 2004" startWordPosition="724" endWordPosition="727">esigned for the document-level; as such, it required no smoothing, as some sentence would have at least one 4- gram match. We now describe 7 smoothing techniques that work better for sentence-level evaluation. Suppose we consider matching n-grams for n = 1... N (typically, N = 4). Let mn be the original match count, and m′n be the modified ngram match count. Smoothing 1: if the number of matched ngrams is 0, we use a small positive value ǫ to replace the 0 for n ranging from 1 to N. The number ǫ is set empirically. m′n = ǫ, if mn = 0. (5) Smoothing 2: this smoothing technique was proposed in (Lin and Och, 2004). It adds 1 to the matched n-gram count and the total n-gram count for n ranging from 2 to N. m′n = mn + 1, for n in 2 ... N, (6) l′n = ln + 1, for n in 2 ... N. (7) Smoothing 3: this smoothing technique is implemented in the NIST official BLEU toolkit mteval-v13a.pl.1 The algorithm is given below. It assigns a geometric sequence starting from 1/2 to the n-grams with 0 matches. 1. invcnt = 1 2. for n in 1 to N 3. if mn = 0 4. invcnt = invcnt x 2 5. m′n = 1/invcnt 6. endif 7. endfor 1available at http://www.itl.nist.gov/iad/mig/tests/mt/2009/ Smoothing 4: this smoothing technique is novel to th</context>
<context position="17103" citStr="Lin and Och, 2004" startWordPosition="2963" endWordPosition="2966">r the small feature set tuning task. Results are averaged across 5 replications; std is the standard deviation. Tune std MT06 std MT08 std mira 29.9 0.1 38.0 0.1 31.0 0.1 0 29.5 0.1 37.9 0.1 31.4 0.3 2 29.6 0.3 38.0 0.2 31.1 0.2 4 29.9 0.2 38.1 0.1 31.2 0.2 6 29.7 0.1 37.9 0.2 31.0 0.2 7 29.7 0.2 38.0 0.2 31.2 0.1 Table 7: Chinese-to-English Results for the large feature set tuning task. Results are averaged across 5 replications; std is the standard deviation. Significant improvements over the no-smoothing baseline (p ≤ 0.05) are marked in bold. 0: No smoothing (baseline) 2: Add 1 smoothing (Lin and Och, 2004) 4: Length-scaled pseudo-counts (this paper) 6: Interpolation with a precision prior (Gao and He, 2013) 7: Combining Smoothing 4 with the match interpolation of Smoothing 5 (this paper) The results of the large feature set experiments are shown in Table 7 for Chinese-to-English and Table 8 for Arabic-to-English. For a sanity check, we compared these results to tuning with our very stable Batch k-best MIRA implementation (Cherry and Foster, 2012), listed as mira, which shows that all of our expected BLEU tuners are behaving reasonably, if not better than expected. Comparing the various smoothin</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 605–612, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matouˇs Mach´aˇcek</author>
<author>Ondˇrej Bojar</author>
</authors>
<title>Results of the WMT13 metrics shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>45--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Mach´aˇcek, Bojar, 2013</marker>
<rawString>Matouˇs Mach´aˇcek and Ondˇrej Bojar. 2013. Results of the WMT13 metrics shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45–51, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<publisher>ACL.</publisher>
<location>Philadelphia,</location>
<contexts>
<context position="861" citStr="Papineni et al., 2002" startWordPosition="121" endWordPosition="124">ic. However, because BLEU computes a geometric mean of n-gram precisions, it often correlates poorly with human judgment on the sentence-level. Therefore, several smoothing techniques have been proposed. This paper systematically compares 7 smoothing techniques for sentence-level BLEU. Three of them are first proposed in this paper, and they correlate better with human judgments on the sentence-level than other smoothing techniques. Moreover, we also compare the performance of using the 7 smoothing techniques in statistical machine translation tuning. 1 Introduction Since its invention, BLEU (Papineni et al., 2002) has been the most widely used metric for both machine translation (MT) evaluation and tuning. Many other metrics correlate better with human judgments of translation quality than BLEU, as shown in recent WMT Evaluation Task reports (Callison-Burch et al., 2011; Callison-Burch et al., 2012). However, BLEU remains the de facto standard evaluation and tuning metric. This is probably due to the following facts: 1. BLEU is language independent (except for word segmentation decisions). 2. BLEU can be computed quickly. This is important when choosing a tuning metric. 3. BLEU seems to be the best tun</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, July. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
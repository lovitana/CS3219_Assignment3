<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.855716">
Experimental Design to Improve Topic Analysis Based Summarization
</title>
<author confidence="0.917177">
John E. Miller
</author>
<affiliation confidence="0.922622">
Computer &amp; Information Sciences
University of Delaware
</affiliation>
<address confidence="0.892287">
Newark, DE 19711
</address>
<email confidence="0.998855">
jmiller@udel.edu
</email>
<author confidence="0.468234">
Kathleen F. McCoy
</author>
<affiliation confidence="0.5593515">
Computer &amp; Information Sciences
University of Delaware
</affiliation>
<address confidence="0.688428">
Newark, DE 19711
</address>
<email confidence="0.998523">
mccoy@udel.edu
</email>
<sectionHeader confidence="0.994791" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999930857142857">
We use efficient screening experiments
to investigate and improve topic analysis
based multi-document extractive summa-
rization. In our summarization process,
topic analysis determines the weighted
topic content vectors that characterize the
corpora, and then Jensen-Shannon diver-
gence extracts sentences that best match
the weighted content vectors to assemble
the summaries. We use screening experi-
ments to investigate several control param-
eters in this process, gaining better under-
standing of and improving the topic anal-
ysis based summarization process.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990693">
We use efficient experimental design to investi-
gate and improve topic analysis based multiple
document extractive summarization. Our process
proceeds in two steps: Latent Dirichlet Anal-
ysis (LDA) topic analysis determines the top-
ics that characterize the multi-document corpus,
and Jensen-Shannon divergence selects sentences
from the corpus. This process offers many poten-
tial control settings for understanding and improv-
ing the summarization process.
Figure 1 shows topic analysis with corpus input,
control settings, and product outputs of topics and
probability estimates of topic compositions and
document mixtures. There are controls for doc-
ument preparation (headlines) and analysis (num-
ber of topics, initial α and 0, number of iterations,
and whether to optimize α and 0 in process).
Figure 2 shows summarization with corpus and
topic inputs, control settings, and the text summa-
rization product. There are controls for extraction
of sentences (Extract α and JSD Divisor) and for
composing the summary (Order policy).
Topic analysis has become a popular choice for
text summarization as seen in Text Analysis Con-
</bodyText>
<figureCaption confidence="0.9999895">
Figure 1: Topic Analysis
Figure 2: Text Summarization
</figureCaption>
<bodyText confidence="0.99702819047619">
ferences (TAC, 2010; TAC, 2011) with individual
team reports (Delort and Alfonseca, 2011; Lui et
al., 2011; Mason and Charniak, 2011). Nenkova
and McKeown (2012; 2011) included topic anal-
ysis among standard methods in their surveys of
text summarization methodologies. Haghighi and
Vanderwende (2009) explored extensions of LDA
topic analysis for use in multiple document sum-
marization tasks. Yet there are many control set-
tings that can affect summarization that have not
been explicitly studied or documented, and that
are important for reproducing research results.
In this text summarization pilot study, we exper-
iment with several control settings. As in Mason
and Charniak (2011) we do a general rather than
guided summarization. Our primary contribution
is illustrating the use of efficient experimental de-
sign on control settings to help understand and im-
prove the text summarization process. We enjoy
some success in this endeavor even as we are sur-
prised by some of our results.
</bodyText>
<figure confidence="0.998924105263158">
Topics
�
40
Analyze
Topics
# Iterations
Optimize a, fi
# Topics
Initial a, fi
Headlines
Corpus
Corpus
Summary
Topics
Analyze
Topics
Order policy
Extract a
JSD divisor
</figure>
<page confidence="0.987709">
74
</page>
<note confidence="0.727863">
Proceedings of the 8th International Natural Language Generation Conference, pages 74–82,
Philadelphia, Pennsylvania, 19-21 June 2014. c�2014 Association for Computational Linguistics
words
</note>
<sectionHeader confidence="0.925226" genericHeader="method">
2 Technical Background
</sectionHeader>
<subsectionHeader confidence="0.927982">
2.1 LDA Topic Analysis
</subsectionHeader>
<bodyText confidence="0.977314210526316">
LDA topic analysis uses a per document bag of
words approach to determine topic compositions
of words and document mixtures of topics. Anal-
ysis constructs topic compositions and document
mixtures by assigning words to topics within doc-
uments. Weighted topic compositions can then be
used as a basis for selecting the most informative
text to include in summarizations.
LDA topic analysis is based on a generative
probabilistic model. Document mixtures of top-
ics are generated by a multinomial distribution,
Θ, and topic compositions of words are gener-
ated by a multinomial distribution, Φ. Both Θ and
Φ in turn are generated by Dirichlet distributions
with parameters α and 0 respectively. Figure 3
(Steyvers and Griffiths, 2007) shows a corpus ex-
plained as the product of topic word compositions
(Φ) and document topic mixtures (Θ).
documents topics documents
</bodyText>
<figure confidence="0.649837">
Corpus
</figure>
<figureCaption confidence="0.990781">
Figure 3: Topic Model
</figureCaption>
<bodyText confidence="0.972943">
The joint distribution of words and topics (Grif-
fiths and Steyvers (2004)) is given by P(w, z) =
P(wIz)P(z) where in generating a document the
topics are generated with probability P(z) and the
words given the topics are generated with proba-
bility P(wIz). Here
</bodyText>
<equation confidence="0.9940416">
Γ(0
(0•) Z ��Z-� Qv Γ (nzv + 0)
Γ (0)V H Γ (nz• + 0•) ,
z=1
(1)
</equation>
<bodyText confidence="0.9786214">
where nzv is the number of times word v occurs in
topic z, nz• is the number of times topic z occurs,
0• is the sum of the 0 scalar over all word types,
and Γ ( ) is the gamma function (Knuth, 2004),
and
</bodyText>
<equation confidence="0.999252">
!D YD Q
Γ (α•) z Γ (nzd + α)
P(z) = Γ (α)Z Γ (n•d + α•) , (2) d=1
</equation>
<bodyText confidence="0.9999055">
where nzd is the number of times topic z occurs in
document d, n•d is the number of times document
d occurs, and α• is the sum of αs over topics.
Analysis reverses the generative model. Given
a corpus, topic analysis identifies weighted topic
word compositions and document topic mixtures
from the corpus. We assign topics to words in
the training corpus using Gibbs sampling (Gel-
man et al., 2004) where each word is considered in
turn in making the topic assignment. We monitor
training progress by log P(w, z) where a greater
log P(w, z) indicates better fit. After sufficient it-
erations through the corpus the log P(w, z) typi-
cally converges to steady state.
Analysis products are topic determinations for
the corpus as well as weighted estimates of topic
word compositions Φ and document topic mix-
tures Θ. The α and 0 priors are optimized (re-
estimated) during training and the asymmetric α
which varies by topic can be used as a measure of
topic importance in our summarization step.
The topic analysis implementation used in this
pilot study borrows from the UMass Mallet topic
analysis (McCallum, 2002).
</bodyText>
<subsectionHeader confidence="0.995538">
2.2 Jensen-Shannon Divergence
</subsectionHeader>
<bodyText confidence="0.997530038461539">
From the topic word compositions and optimized
αs, we form a weighted aggregate vector of the
prominent topics, and select sentences from the
corpus that have minimal divergence from the ag-
gregate topic. The operating assumption is that the
aggregate topic vector adequately represents the
content of an ideal summary. So the closer to zero
divergence from the aggregate topic, the closer we
are to the ideal summary.
We seek to minimize the Jensen-Shannon Di-
vergence, JSD(C||T), a symmetric Kullback-
Liebler (KL) divergence, between the extractive
summary content, C, and the aggregate topic, T,
using a greedy search method of adding at each
pass through the corpus the sentence that most
reduces the divergence. Haghighi and Vander-
wende (2009) made similar use of KL divergence
in their Topic Sum method.
In preliminary studies, this minimize JSD cri-
terion seemed to give overly long sentences be-
cause the greedy method favored the greatest re-
duction in JSD regardless of the length of the sen-
tence. This affected readability and rapidly used
up all available target document size. Therefore
we modified the greedy search method to consider
sentence length as well.1
</bodyText>
<footnote confidence="0.741874666666667">
1Global optimization of JSD(C||T) could address both
of these issues; we will investigate this option in a future ef-
fort.
</footnote>
<equation confidence="0.971169">
words
topics
P(w|z) =
</equation>
<page confidence="0.976426">
75
</page>
<bodyText confidence="0.9975">
In selecting each new sentence we seek to maxi-
mize the reduction in divergence corrected for sen-
tence length
</bodyText>
<equation confidence="0.9980445">
(J5D(Ct−1||T) − J5D(5t, Ct−1||T))(3)
function(length(5t)) ,
</equation>
<bodyText confidence="0.95229025">
where 5t is the sentence under consideration and
Ct−1 is the content from the previously completed
iterations, and the function of length of 5t, is ei-
ther the constant 1 (i.e. no correction for sentence
</bodyText>
<equation confidence="0.937017">
V/
length) or length(5t).
</equation>
<sectionHeader confidence="0.900693" genericHeader="method">
3 Pilot Study Using TAC 2010 Samples
</sectionHeader>
<bodyText confidence="0.99997212">
Our goal is to investigate and optimize factors that
impact multi-document extractive summarization.
We hope to subsequently extend our findings and
experience to abstractive summarization as well.
For our pilot, we’ve chosen summarization of
the 2010 Text Analysis Conference (2010) sam-
ple themes, which are conveniently available and
of a manageable size. The three sample themes
are from different summarization categories out of
a total of 46 news themes over five different cat-
egories, with 10 original and 10 follow-up news
reports each. In the original TAC 2010 task, par-
ticipants were asked to do focused queries varying
with the summarization category. In our pilot we
perform an undirected summarization of the orig-
inal news reports.
NIST provides 4 model summaries for each
news theme annotated for the focused summary,
and we use these model summaries in scoring our
extractive summarizations.2 We also include a
measure of fluency in our assessment.
Our document summarization task is then: mul-
tiple document extractive summarization using 10
documents of less than 250 words each to con-
struct summaries of 100 words.
</bodyText>
<subsectionHeader confidence="0.999614">
3.1 Preliminary Results of Topic Analysis
</subsectionHeader>
<bodyText confidence="0.979587441860465">
Topic analysis is such a complex methodology that
it makes sense to fix some parameters before using
it in the summarization process.
We use the commonly accepted initial α value
of 1 for each topic giving a sum of α values equal
to the number of topics. Later, we experiment with
a single individual topic initial α value, but we al-
ways maintain an initial α sum equal to the num-
ber of topics. Likewise we use the scalar 0 value
2Comparison of our summarization results versus the
TAC 2010 task will necessarily be imprecise given the dif-
ferences in focus of our pilot study from TAC 2010.
0.1 typical of a modest number of word types (less
than 1000 in this study).
In prior studies, we found that re-estimating α
and 0 frequently adds little cost to topic analysis
and drives better and more rapid convergence. We
optimize α and 0 every 5 iterations, starting at it-
eration 50.
How Many Topics to Use
The number of topics depends on the problem it-
self. The problem of size of P 2000 words per
news theme would indicate a number of topics be-
tween 3 and 20 as adequate to explain document
word use where the log(|Corpus|) is the mini-
mum and V/ |Corpus  |is the maximum number of
topics to use (Meil˘a, 2007).
A common way to select the correct number of
topics is to optimize log P (w) on held-out doc-
uments, where greater log likelihoods indicate a
better number of topics. While it would be im-
practical to do such a study for each news theme
or each document summary, it is reasonable to do
so on a few sample themes and then generalize to
similar corpora. We look at log likelihood for 3, 5,
and 10 topics using the TAC 2010 sample themes.
As there are only 10 documents for each theme,
we use the TAC 2010 update documents as held-
out documents for calculating the log likelihoods.
Topic word distributions, Φ, from training are
used to infer document mixtures, O, on the held-
out data, and the log P(w) is calculated (Teh et
al., 2007) as:
</bodyText>
<equation confidence="0.977666">
�
nzd + α (4)
n•d + α• ,
</equation>
<bodyText confidence="0.9999752">
where the sum is over all possible topics for a
given word and the product is over all documents
and words.
Table 1 shows mean log likelihoods for the news
themes at 3, 5 and 10 topics each. There is lit-
tle practical difference between the log likelihood
measures even though the 3 topic model has a sig-
nificantly lower log likelihood (p &lt; 0.05) than
the 5 and 10 topic models. We assess topic quality
more directly to see which model is better.
</bodyText>
<sectionHeader confidence="0.432673" genericHeader="method">
3 Topics 5 Topics 10 Topics
</sectionHeader>
<tableCaption confidence="0.6697325">
-6.00 -5.97 -5.96
Table 1: Held-out Log Likelihood Number Topics.
</tableCaption>
<equation confidence="0.555542">
rl � nzwi + 0
P(w) = z nz• + 0•
d,i
</equation>
<page confidence="0.778133">
76
</page>
<bodyText confidence="0.996709454545455">
Useful topic quality measures are:
Importance measured by number of documents
(or optimized αs). Low importance topics,
with very few documents related to a topic,
indicate that we have more topics than neces-
sary. While not a fatal flaw, the topic model
may be over fit.
Coherence measured as a log sum of co-
occurrence proportions of each topic’s high
frequency words across multiple docu-
ments (Mimno et al., 2011). The more neg-
ative the coherence measure, the poorer the
coherence. A few poor coherence topics is
not fatal, but the topic model may be over fit.
Similarity to other topics measured by cosine
distance between topic vectors is undesirable.
The more similar the topics, the more diffi-
cult it is to distinguish between them. Many
similar topics makes it difficult to discrimi-
nate among topics over the corpus.
Reviewing the document quality for 3, 5 and 10
topics we find:
</bodyText>
<listItem confidence="0.980964">
• More low importance topics in 10 versus 5
and 3 topic models,
• Somewhat better topic coherence in 3 and 5
topic models,
• Undesirable greater topic similarity for the 3
</listItem>
<bodyText confidence="0.98180425">
versus 5 versus 10 topic models.
We choose the 10 topic model giving higher pri-
ority to the problem of undesirable topic similar-
ity, recognizing that we may get some unimportant
or less coherent topics. As our summarization pro-
cess only uses the most important topics for the ag-
gregate topic, the occasional unimportant and less
coherent topic should not matter.
</bodyText>
<subsectionHeader confidence="0.961801">
Document Preparation
</subsectionHeader>
<bodyText confidence="0.999785625">
Document cleaning removed all HTML, as well
as all header information not related to the articles
themselves; document dates, references, and head-
lines were saved for use in the document summa-
rization step. Document headlines were optionally
folded into the document text. Stop words were re-
moved and remaining words lemmatized for topic
analysis.
</bodyText>
<sectionHeader confidence="0.991574" genericHeader="method">
4 Design of Experiments
</sectionHeader>
<bodyText confidence="0.9976448">
As our information about the various controls in
the process and the expected results is fairly rudi-
mentary, we use efficient screening experimental
designs to evaluate several factors at the same time
with a minimum number of trials. We define the
factors (control parameters) in our experiment, the
dependent variables we will measure, and finally
select the screening design itself.
Most of the process of topic analysis will re-
main fixed such as the use of 10 topics, initial α
sum of 10, initial scalar 0 of 0.1, optimization of
α and 0 every 5 iterations and 500 total iterations
before saving the final topic vector weights and
corresponding topic alphas.
From our experimentation we hope to find:
</bodyText>
<listItem confidence="0.9998655">
• Factors impacting dependent variables,
• Gross magnitude of impact on dependent
variables,
• Factors to followup with in more detail.
</listItem>
<subsectionHeader confidence="0.968343">
4.1 Experimental Factors
</subsectionHeader>
<bodyText confidence="0.999960387096774">
In screening experiments, we chose factors about
which we have crude information, and which we
think could impact intermediate or final product
results. To learn as much as possible about factor
effects, we choose to vary them between default
and extreme settings or between two extremes
where we hope to see some positive impact.
Our experimental factors are:
Save headline text as part of document prepara-
tion (Yes, No). Headlines often contain im-
portant summary information. We test to see
if such information improves summaries.
Single fixed α proportion of the α sum (*, 0.5).
Topic analysis typically selects (weights) a
few important topic vectors with substantial
proportions of the α sum. We want to see if
biasing selection of a single important vec-
tor at a 0.5 proportion of the α sum improves
summaries versus unbiased α weighting (*).
Aggregate topic policy as a proportion of the α
sum for selecting the topic aggregate used in
summarization (0.5, 0.75). We order topics
based on the optimized (re-estimated) αs and
aggregate topics summing and weighting by
the αs until we reach the aggregate topic pol-
icy proportion. We want to see which policy
(0.5 or 0.75) proportion of the α sum results
in better summaries.
JSD divisor to use with iterative greedy search
for sentences (ONE, SQRT). Prior work
shows the JSD Divisor impacts the length of
</bodyText>
<page confidence="0.993308">
77
</page>
<bodyText confidence="0.9918495">
sentences selected. We test the impact on the
summaries themselves.
Order policy for constructing the summary
from selected sentences (DATE-DOC,
SALIENCE-DOC). Ordering sentences by
news report date or by salience as measured
by reduction in JSD should impact the
fluency of summaries.
</bodyText>
<subsectionHeader confidence="0.985431">
4.2 Dependent Variables
</subsectionHeader>
<bodyText confidence="0.992519166666667">
We want readable and informative text that sum-
marizes content of the input documents in the al-
lowable space. We measure several intermedi-
ate process variables as well as evaluate the sum-
maries themselves.
Intermediate measures include:
</bodyText>
<listItem confidence="0.995360692307692">
• Initial selected sentence Jensen-Shannon di-
vergence from the aggregate topic. The first
sentence selected should substantially reduce
divergence.
• Final selected sentence Jensen-Shannon di-
vergence from the aggregate topic. Diver-
gence close to zero would indicate broad cov-
erage of the aggregate topic; it may be related
to summary content.
• Number of topics in the aggregate topic.
• Average sentence length. This should be im-
pacted by the JSD divisor; it may be related
to summary fluency.
</listItem>
<bodyText confidence="0.998842964285714">
ROUGE (Lin, 2011) is a package for auto-
matic evaluation of summaries that compares sys-
tem produced summaries to model (gold stan-
dard) summaries and reports statistics such as R-
2, bi-gram co-occurrence statistics between sys-
tem and model summaries, and SU4, skip bi-gram
co-occurrence statistics where word pairs no more
than 4 words apart may also be counted as bi-
grams. The R-2 and SU4 are automated content
measures reported for TAC 2010, and the gold
standard summaries are readily available for the
samples topics. We use ROUGE R-2 and SU4 as
reliable dependent measures and for comparison
to TAC 2010 results.
We add a simple measure of fluency focused on
across sentence issues. The fluency score starts
at a value of 5 and then subtracts: 1 for each
non sequitur or obvious out of order sentence, 1/2
for each missing co-reference, non-informative,
ungrammatical, or redundant sentence. For sen-
tences of less than 20 words, when more than one
penalty applies only the most severe penalty is ap-
plied, so as not to penalize the same short phrase
multiple times. Scoring is done by one of the au-
thors without knowing the combination of experi-
mental factors of the summary (blind scoring).
Summary measures thus include: ROUGE R-2,
ROUGE SU4, and Fluency.
</bodyText>
<subsectionHeader confidence="0.999436">
4.3 Select Experimental Design
</subsectionHeader>
<bodyText confidence="0.9999147">
Screening designs focus on detecting and assess-
ing main effects and optionally low order inter-
action effects. When all experimental factors are
continuous, center points may also be included in
some designs. In subsequent stages of experimen-
tation, when factors have been reduced to a min-
imum, one can use more fine grained factor set-
tings to better map the response surface for those
factors. Two common families of screening de-
signs (Montgomery, 1997) are:
Two level fractional factorial Uses a power of
1/2 fraction of a full two level factorial design.
For example, instead of running all possible
combinations of 5 factors (i.e. 32 trials), you
could choose a 1/2 or even 1/4 fraction of the
design, based on how many experiments you
can run and how much confounding you are
willing to accept between main effects and
various interaction effects. The 1/2 fraction of
a 5 factor design would result in 16 trials be-
ing run with the main effects estimated clear
of any 2-way or 3-way interactions.
Plackett-Burman These screening designs are
available in multiples of 4 trials and can have
as many factors as the number of trials less
one. Main effects are confounded with all
other effects in the Plackett-Burman design
and so not estimable, but the confounding is
spread evenly among all main effects rather
than concentrated in specific interactions as
in the fractional factorial.
We’ve chosen the 12 run Plackett-Burman de-
sign with 5 factors and 6 degrees of freedom from
the unassigned (dummy) factors available to esti-
mate error. Assuming sparsity of effects (or equiv-
alently invoking the Pareto principal), there will
likely only be a few critical factors explaining
much of the variation in dependent variables.
Table 2 shows the resulting Plackett-Burman
design excluding dummy factors.
</bodyText>
<page confidence="0.997581">
78
</page>
<table confidence="0.998959428571428">
Run Fixed Aggr JSD Order Head
Alpha Topic Div Line
1 .5 .75 ONE SAL YES
2 * .75 SQRT DATE YES
3 .5 .5 SQRT SAL NO
4 * .75 ONE SAL YES
5 * .5 SQRT DATE YES
6 * .5 ONE SAL NO
7 .5 .5 ONE DATE YES
8 .5 .75 ONE DATE NO
9 .5 .75 SQRT DATE NO
10 * .75 SQRT SAL NO
11 .5 .5 SQRT SAL YES
12 * .5 ONE DATE NO
</table>
<tableCaption confidence="0.997663">
Table 2: Plackett-Burman 12 DOE.
</tableCaption>
<sectionHeader confidence="0.993251" genericHeader="conclusions">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999977375">
We analyze our experiment using conventional
analysis of variance (ANOVA) and show tables of
means for the various experimental conditions. As
this is a screening experiment, we treat a p-value &lt;
0.20 as informative and consider the correspond-
ing factor worth further consideration. To save
space, only significant p-values are reported rather
than the full ANOVAs.
</bodyText>
<subsectionHeader confidence="0.951374">
5.1 Intermediate Measures
</subsectionHeader>
<bodyText confidence="0.999719285714286">
Number of topics in the aggregate topic is directly
impacted by the AggrTopic setting; we simply re-
port the mean number of topics selected by Aggr-
Topic value (Table 3). The 1.0 average number of
topics for AggrTopic set to 0.5 indicates that only
one topic was ever selected for the aggregate topic
at this setting. This implies that the most impor-
tant topic always had an α proportion &gt; 0.50 of
the α sum even when the FixedAlpha setting was
* (for unbiased α weighting). This is unexpected
in that we thought the most important topic α de-
termined by topic analysis would be more variable
and show some α values with proportions less than
0.5 of the α sum.
</bodyText>
<figure confidence="0.9552125">
Aggr Number
Topic Topics
0.50 1.00
0.75 4.55
</figure>
<tableCaption confidence="0.982411">
Table 3: Average Number Topics.
</tableCaption>
<bodyText confidence="0.989224555555556">
Average sentence length in the summary may be
affected by any of the independent variables ex-
cept sentence order policy. JSD Divisor has a dra-
matic impact (p &lt; 0.0001) and AggrTopic a mod-
est impact (p &lt; 0.01) on average sentence length.
Using a divisor of ONE in the JSD based sentence
selection results in much longer sentences while
using AggrTopic of 0.5 results in shorter sentences
(Table 4).
</bodyText>
<table confidence="0.99969875">
Aggr Sentence JSD Sentence
Topic Length Divisor Length
0.5 20.3 ONE 26.8
0.75 23.9 SQRT 17.4
</table>
<tableCaption confidence="0.751952">
Standard Error of the mean = 0.78
Table 4: Average Sentence Length.
</tableCaption>
<bodyText confidence="0.9935202">
Initial selected sentence Jensen-Shannon diver-
gence (JSD) should be affected directly by JSD
Divisor in iterative sentence selection, but may
also be affected by any of the other independent
variables except for sentence order policy. Aggr-
Topic and JSD Divisor strongly impact initial sen-
tence JSD (p &lt; 0.00005).
The table of JSD initial sentence means by Ag-
grTopic and JSD Divisor is revealing (Table 5).
The JSD for the initial sentence selected is lower
for AggrTopic of 0.5. We observed above that only
one topic is selected for the aggregate topic when
AggrTopic is 0.5. Thus we achieve a lower di-
vergence of the initial sentence from the aggregate
topic when the aggregate is composed of only one
topic. For initial sentence JSD, aggregating topics
seems ineffective.
Similarly a JSD Divisor of ONE gives a lower
initial divergence than using the SQRT as the di-
visor. The interpretation is problematic here in
that a divisor of ONE seems to give lower ini-
tial divergence because it selects longer sentences,
which means that less space remains in the sum-
mary to select other sentences minimizing total di-
vergence.
</bodyText>
<table confidence="0.99921775">
Aggr JSD JSD JSD
Topic Initial Divisor Initial
0.5 0.665 ONE 0.658
0.75 0.735 SQRT 0.742
</table>
<tableCaption confidence="0.781251">
Standard Error of the mean = 0.0056
Table 5: Average Initial JSD.
</tableCaption>
<footnote confidence="0.728942333333333">
Table 6 shows the impact of AggrTopic and JSD
Divisor together on the JSD for the initial sen-
tence. There is still the issue of whether using a
JSD Divisor of ONE is appropriate given the ef-
fect on the remaining summary size, but the effects
appear additive.
</footnote>
<page confidence="0.99555">
79
</page>
<table confidence="0.936498">
Aggr JSD JSD
Topic Divisor initial
0.50 ONE 0.627
0.50 SQRT 0.703
0.75 ONE 0.690
0.75 SQRT 0.780
Standard Error of the mean = 0.0080
</table>
<tableCaption confidence="0.992098">
Table 6: Average Initial JSD.
</tableCaption>
<bodyText confidence="0.95899">
Final sentence Jensen-Shannon Divergence
(JSD) may be affected by any but the sentence or-
der policy variable. AggrTopic (p &lt; 0.00001) and
JSD Divisor (p &lt; 0.001) strongly impact the final
sentence JSD; there is also a possible effect from
including headlines in the summary (p &lt; 0.1).
The effect of the JSD Divisor has reversed from
the initial JSD; using a divisor of ONE results here
in a less desirable higher divergence for the final
sentence. The AggrTopic effect is about the same
as for initial JSD divergence; a single dominant
topic seems more effective than using an aggre-
gate topic.
</bodyText>
<table confidence="0.99822425">
Aggr JSD JSD JSD
Topic Final Divisor Final
0.5 0.422 ONE 0.487
0.75 0.513 SQRT 0.448
</table>
<tableCaption confidence="0.7682565">
Standard Error of the mean = 0.0047
Table 7: Average Initial JSD.
</tableCaption>
<bodyText confidence="0.913575">
Impact of AggrTopic and JSD Divisor together
on the JSD for the initial sentence (Table 8) seems
additive.
</bodyText>
<table confidence="0.9979425">
Aggr JSD JSD
Topic Divisor final
0.50 ONE 0.437
0.50 SQRT 0.407
0.75 ONE 0.537
0.75 SQRT 0.490
</table>
<tableCaption confidence="0.7813535">
Standard Error of the mean = 0.0066
Table 8: Average Final JSD.
</tableCaption>
<subsectionHeader confidence="0.999138">
5.2 Product Measures
</subsectionHeader>
<bodyText confidence="0.999931">
Based on the analysis of intermediate measures, it
would seem that using a JSD Divisor of the SQRT
and selecting only the dominant topic gives less
divergence from the aggregate topic. However,
we have to be careful here in drawing conclusions
based on intermediate variables; selecting only the
dominant topic may result in reduced divergence,
but this does not necessarily mean that the domi-
nant topic is representative of good summaries.
We examine product variables to provide direct
support in our study, and so we ask how ROUGE
R-2 and SU4, and fluency evaluations vary with
the experimental factors. This pilot studies un-
guided summarization of initial stories from the 3
sample news themes from 3 separate categories.
While results are not directly comparable with
those of the full TAC 2010 test corpus, we will use
the TAC 2010 results as a reference point versus
our own results. The average of all experiments
are reported along with the TAC 2010 results (Ta-
ble 9). Our ROUGE R-2 and SU4 performance
seems reasonable showing results better than the
baseline but not as good as the best system.
</bodyText>
<table confidence="0.987915">
Reference System R-2 SU4
Baseline - Lead sentences 5.4 8.6
Baseline - MEAD† 5.9 9.1
Best System 9.6 13.0
Pilot Average 6.7 10.1
Pilot Minimum 5.6 8.7
Pilot Maximum 8.1 11.9
†Text summarization system (Radevet al., 2004)
</table>
<tableCaption confidence="0.998774">
Table 9: TAC 2010 ROUGE Scores.
</tableCaption>
<bodyText confidence="0.997251523809524">
ROUGE R-2 results show no significant impact
from our experimental factors. This is disappoint-
ing as it gives us no handle on how to improve
performance.
ROUGE SU4 shows a modest impact for Aggr-
Topic (p &lt; 0.025) and the possible impact of JSD
Divisor (p &lt; 0.20). Note that we dropped Or-
der and FixedAlpha factors from the model; Or-
der because it can only effect sentence order and
FixedAlpha because the most important α deter-
mined automatically by topic analysis did not vary
much from the 0.5 FixedAlpha. A benefit of drop-
ping terms from the model is that we have more
dummy factors to estimate error.
The ROUGE SU4 means (Table 10) show the
same pattern as for the JSD final sentence, but the
differences are not as clear cut. Box and whiskers
plots for AggrTopic and JSD Divisor (Figures 4
and 5) offer more insight into the AggrTopic and
JSD Divisor effects.
There is a clear distinction between AggrTopic
</bodyText>
<page confidence="0.992697">
80
</page>
<table confidence="0.9085474">
Aggr ROUGE JSD ROUGE
Topic SU4 Divisor SU4
0.5 10.75 ONE 9.70
0.75 9.48 SQRT 10.53
Standard Error of the mean = 0.32
</table>
<tableCaption confidence="0.99195">
Table 10: Average ROUGE SU4.
</tableCaption>
<bodyText confidence="0.999983380952381">
levels 0.5 and 0.75 with better results at the 0.5
level, except for an outlier value of 9.1. Investiga-
tion shows no data coding error and nothing spe-
cial about the experimental conditions other than
if uses a JSD Divisor of ONE which also gives
lower SU4 scores. The box and whiskers plots for
JSD Divisor effects also suggest a positive effect
for JSD Divisor of SQRT, but the whiskers over-
lap the boxes indicating no strong effect.
(p &lt; 0.05) and possible effects of Order policy
and Headlines (p &lt; 0.20).
Fluency means (Table 12) show that fluency is
better for JSD Divisor ONE. From our experience
of scoring Fluency, this would seem to be because
the fewer and longer sentences with JSD Divisor
of ONE offer fewer chances for disfluencies. The
better Fluency with DATE ordering likely comes
from fewer out of order or non sequitur sentences,
and the better Fluency with NO headlines likely
results from fewer short ungrammatical headlines
as part of the text.
</bodyText>
<table confidence="0.99766325">
JSD Flu- Order Flu- Head Flu-
Div ency ency Lines ency
ONE 3.95 DATE 3.80 NO 3.80
SQRT 3.33 SAL 3.47 YES 3.47
</table>
<tableCaption confidence="0.749626">
Standard Error of the mean = 0.16
Table 12: Average Fluency.
</tableCaption>
<figure confidence="0.98546175">
6 Summary and Discussion
ROUGE SU4
90 95 100 105 11.0 11.5 120
ROUGE SU4
90 95 100 105 11.0 11.5 120
●
0.5 0.75 ONE SQRT
Alpha Extract Proportion JSD Divisor
</figure>
<figureCaption confidence="0.7986225">
Figure 4: ROUGE Figure 5: ROUGE
SU4 by Aggr Topic SU4 by JSD Divisor
</figureCaption>
<bodyText confidence="0.992002272727273">
We had speculated that the final sentence diver-
gence might be related to some of the end prod-
uct measures. Indeed, we find that JSD final sen-
tence is strongly inversely related to ROUGE SU4
as shown by regression analysis (Table 11). While
the residual error of 0.73 indicates that we can
only reliably predict ROUGE SU4 within 1.5 units
(for averages of 3 trials), this is still important.
A 0.1 reduction in final sentence divergence cor-
responds on the average to a 1.4 unit increase in
ROUGE SU4.
</bodyText>
<table confidence="0.6942888">
Estimate StdErr t Pr(&gt;|t|)
Intercept 16.865 1.934 8.721 ˜0.0
JSDfinal -14.435 4.112 -3.510 0.006
Residual standard error: 0.73 on 10 degrees of freedom
F-statistic: 12.32 on 1 and 10 DF, p-value: 0.0056
</table>
<tableCaption confidence="0.993267">
Table 11: Regression - ROUGE SU4.
</tableCaption>
<bodyText confidence="0.999979321428571">
We thought Simple Fluency would show an ef-
fect for sentence order policy and maybe other fac-
tors. Analysis shows an effect for JSD Divisor
Our pilot studied topic analysis based multi-
document extractive summarization using the
2010 TAC sample topics. Our experimental design
process identified control factors with their default
and extreme settings, defined intermediate and fi-
nal product dependent measures, designed the ex-
periment, ran, and analyzed the experiment.
We identified an intermediate variable, final se-
lected sentence divergence, that could be used as a
stand-in for the product content measure, ROUGE
SU4. We found that using a single dominant topic,
instead of an aggregate topic, and using a divisor
of the square root of sentence length in sentence
selection, improved final sentence divergence and
ROUGE SU4. However, using a divisor of one in
sentence selection improved fluency of summaries
which is at odds with the benefit of using square
root of sentence length to improve content.
Our planned experimentation has made obvious
and objective the process of describing and im-
proving our extractive summarization process. It
is an extremely useful process and furthermore a
process that when documented permits sharing of
results and even duplicating of results by others
working in this area.
</bodyText>
<page confidence="0.998047">
81
</page>
<sectionHeader confidence="0.993826" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999791514285714">
Jean-Yves Delort and Enrique Alfonseca. 2011. De-
scription of the Google Update Summarizer. 2011
TAC Proceedings.
Andrew Gelman, John B. Carlin, Hal S. Stern, and
Donald B. Rubin. 2004. Bayesian Data Analysis.
Chapman and Hall/CRC, New York, USA.
Tom L. Griffiths and Mark Steyvers. 2004. Finding
Scientific Topics. PNAS, 101(Suppl. 1):5228-5235.
Aria Haghighi and Lucy Vanderwalde. 2009.
Exploring Content Models for Multi-Document
Summarization. 2009 NACL Conference, HLT
Proceedings:362-370.
Donald E. Knuth. 1997. The Art of Computer Pro-
gramming, Volume 1 (Fundamental Algorithms).
Addison Wesley, New York, USA.
Chin-Yew Lin. 204. ROUGE: A Package for Auto-
matic Evaluation of Summaries. ACL 2004 Proceed-
ings of Workshop: Text Summarization Branches
Out.
Hongyan Liu, Pingan Liu, Wei Heng, and Lei Li.
2011. The CIST Summarization System at TAC
2011. 2011 TAC Proceedings.
Dragomir Radev, Timothy Allison, Sasha Blair-
Goldensohn, John Blitzer, Arda Celebi,
Stanko Dimitrov, Elliott Drabek, Ali Hakim,
Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi,
Horacio Saggion, Simone Teufel, Michael Topper,
Adam Winkel, and Zhu Zhang. 2004. MEAD
— A platform for multidocument multilingual
text summarization. Conference on Language
Resources and Evaluation LREC, Lisbon, Portugal,
(May 2004).
Rebecca Mason and Eugene Charniak. 2011. BLLIP
at TAC 2011: A General Summarization System for
a Guided Summarization Task. 2011 TAC Proceed-
ings.
Andres K. McCallum. 2002. MALLET: A
Machine Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Marina Meil˘a. 2007. Comparing Clusterings – an in-
formation based distance. J. Multivariate Analysis,
98(5):873-895.
David Mimno, Hanna M. Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing Semantic Coherence in Topic Models.
2011 EMNLP Conference, Proceedings:262-272.
Douglas C. Montgomery. 1997. Design and Analysis
of Experiments. John Wiley and Sons, New York,
USA.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic Summarization. Foundations and Trends in
Information Retrieval, 5(2-3):1003-233.
Ani Nenkova and Kathleen McKeown. 2012. A Sur-
vey of Text Summarization Techniques. Mining Text
Data. In Charu C. Aggarwal and ChengXiang Zhai
(eds.) Springer.
Mark Steyvers and Tom Griffiths. 2007. Probabilisitic
Topic Models. Latent Semantic Analysis: A road to
Meaning. In T. Landauer, S. D. McNamara &amp; W.
Kintsch (eds.) Laurence Erlbaum.
Task Analysis Conference 2010 –
Summarization Track. 2010.
http://www.nist.gov/tac/2010/Summarization/.
Task Analysis Conference 2011 –
Summarization Track. 2011.
http://www.nist.gov/tac/2011/Summarization/.
Yee Whye Teh, Dave Newman, and Max Welling.
2007. Collapsed Variational Inference for HDP.
Advances in Neural Information Processing
Systems:1481-1488.
</reference>
<page confidence="0.999084">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.651497">
<title confidence="0.999708">Experimental Design to Improve Topic Analysis Based Summarization</title>
<author confidence="0.995103">E John</author>
<affiliation confidence="0.911895666666667">Computer &amp; Information University of Newark, DE</affiliation>
<email confidence="0.994421">jmiller@udel.edu</email>
<author confidence="0.966197">F Kathleen</author>
<affiliation confidence="0.980803333333333">Computer &amp; Information University of Newark, DE</affiliation>
<email confidence="0.999039">mccoy@udel.edu</email>
<abstract confidence="0.998933266666666">We use efficient screening experiments to investigate and improve topic analysis based multi-document extractive summarization. In our summarization process, topic analysis determines the weighted topic content vectors that characterize the corpora, and then Jensen-Shannon divergence extracts sentences that best match the weighted content vectors to assemble the summaries. We use screening experiments to investigate several control parameters in this process, gaining better understanding of and improving the topic analysis based summarization process.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean-Yves Delort</author>
<author>Enrique Alfonseca</author>
</authors>
<title>Description of the Google Update Summarizer.</title>
<date>2011</date>
<note>TAC Proceedings.</note>
<contexts>
<context position="2121" citStr="Delort and Alfonseca, 2011" startWordPosition="303" endWordPosition="306">ls for document preparation (headlines) and analysis (number of topics, initial α and 0, number of iterations, and whether to optimize α and 0 in process). Figure 2 shows summarization with corpus and topic inputs, control settings, and the text summarization product. There are controls for extraction of sentences (Extract α and JSD Divisor) and for composing the summary (Order policy). Topic analysis has become a popular choice for text summarization as seen in Text Analysis ConFigure 1: Topic Analysis Figure 2: Text Summarization ferences (TAC, 2010; TAC, 2011) with individual team reports (Delort and Alfonseca, 2011; Lui et al., 2011; Mason and Charniak, 2011). Nenkova and McKeown (2012; 2011) included topic analysis among standard methods in their surveys of text summarization methodologies. Haghighi and Vanderwende (2009) explored extensions of LDA topic analysis for use in multiple document summarization tasks. Yet there are many control settings that can affect summarization that have not been explicitly studied or documented, and that are important for reproducing research results. In this text summarization pilot study, we experiment with several control settings. As in Mason and Charniak (2011) we</context>
</contexts>
<marker>Delort, Alfonseca, 2011</marker>
<rawString>Jean-Yves Delort and Enrique Alfonseca. 2011. Description of the Google Update Summarizer. 2011 TAC Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gelman</author>
<author>John B Carlin</author>
<author>Hal S Stern</author>
<author>Donald B Rubin</author>
</authors>
<title>Bayesian Data Analysis. Chapman and Hall/CRC,</title>
<date>2004</date>
<location>New York, USA.</location>
<contexts>
<context position="5310" citStr="Gelman et al., 2004" startWordPosition="843" endWordPosition="847">in topic z, nz• is the number of times topic z occurs, 0• is the sum of the 0 scalar over all word types, and Γ ( ) is the gamma function (Knuth, 2004), and !D YD Q Γ (α•) z Γ (nzd + α) P(z) = Γ (α)Z Γ (n•d + α•) , (2) d=1 where nzd is the number of times topic z occurs in document d, n•d is the number of times document d occurs, and α• is the sum of αs over topics. Analysis reverses the generative model. Given a corpus, topic analysis identifies weighted topic word compositions and document topic mixtures from the corpus. We assign topics to words in the training corpus using Gibbs sampling (Gelman et al., 2004) where each word is considered in turn in making the topic assignment. We monitor training progress by log P(w, z) where a greater log P(w, z) indicates better fit. After sufficient iterations through the corpus the log P(w, z) typically converges to steady state. Analysis products are topic determinations for the corpus as well as weighted estimates of topic word compositions Φ and document topic mixtures Θ. The α and 0 priors are optimized (reestimated) during training and the asymmetric α which varies by topic can be used as a measure of topic importance in our summarization step. The topic</context>
</contexts>
<marker>Gelman, Carlin, Stern, Rubin, 2004</marker>
<rawString>Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. 2004. Bayesian Data Analysis. Chapman and Hall/CRC, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom L Griffiths</author>
<author>Mark Steyvers</author>
</authors>
<title>Finding Scientific Topics.</title>
<date>2004</date>
<journal>PNAS,</journal>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="4391" citStr="Griffiths and Steyvers (2004)" startWordPosition="656" endWordPosition="660">text to include in summarizations. LDA topic analysis is based on a generative probabilistic model. Document mixtures of topics are generated by a multinomial distribution, Θ, and topic compositions of words are generated by a multinomial distribution, Φ. Both Θ and Φ in turn are generated by Dirichlet distributions with parameters α and 0 respectively. Figure 3 (Steyvers and Griffiths, 2007) shows a corpus explained as the product of topic word compositions (Φ) and document topic mixtures (Θ). documents topics documents Corpus Figure 3: Topic Model The joint distribution of words and topics (Griffiths and Steyvers (2004)) is given by P(w, z) = P(wIz)P(z) where in generating a document the topics are generated with probability P(z) and the words given the topics are generated with probability P(wIz). Here Γ(0 (0•) Z ��Z-� Qv Γ (nzv + 0) Γ (0)V H Γ (nz• + 0•) , z=1 (1) where nzv is the number of times word v occurs in topic z, nz• is the number of times topic z occurs, 0• is the sum of the 0 scalar over all word types, and Γ ( ) is the gamma function (Knuth, 2004), and !D YD Q Γ (α•) z Γ (nzd + α) P(z) = Γ (α)Z Γ (n•d + α•) , (2) d=1 where nzd is the number of times topic z occurs in document d, n•d is the numb</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>Tom L. Griffiths and Mark Steyvers. 2004. Finding Scientific Topics. PNAS, 101(Suppl. 1):5228-5235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwalde</author>
</authors>
<title>Exploring Content Models for Multi-Document Summarization.</title>
<date>2009</date>
<booktitle>NACL Conference, HLT Proceedings:362-370.</booktitle>
<marker>Haghighi, Vanderwalde, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwalde. 2009. Exploring Content Models for Multi-Document Summarization. 2009 NACL Conference, HLT Proceedings:362-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald E Knuth</author>
</authors>
<title>The Art of Computer Programming,</title>
<date>1997</date>
<volume>1</volume>
<publisher>Algorithms). Addison Wesley,</publisher>
<location>New York, USA.</location>
<marker>Knuth, 1997</marker>
<rawString>Donald E. Knuth. 1997. The Art of Computer Programming, Volume 1 (Fundamental Algorithms). Addison Wesley, New York, USA.</rawString>
</citation>
<citation valid="false">
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<booktitle>ACL 2004 Proceedings of Workshop: Text Summarization Branches Out.</booktitle>
<marker></marker>
<rawString>Chin-Yew Lin. 204. ROUGE: A Package for Automatic Evaluation of Summaries. ACL 2004 Proceedings of Workshop: Text Summarization Branches Out.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Liu</author>
<author>Pingan Liu</author>
<author>Wei Heng</author>
<author>Lei Li</author>
</authors>
<date>2011</date>
<booktitle>The CIST Summarization System at TAC</booktitle>
<note>TAC Proceedings.</note>
<marker>Liu, Liu, Heng, Li, 2011</marker>
<rawString>Hongyan Liu, Pingan Liu, Wei Heng, and Lei Li. 2011. The CIST Summarization System at TAC 2011. 2011 TAC Proceedings.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dragomir Radev</author>
<author>Timothy Allison</author>
<author>Sasha BlairGoldensohn</author>
<author>John Blitzer</author>
<author>Arda Celebi</author>
<author>Stanko Dimitrov</author>
<author>Elliott Drabek</author>
<author>Ali Hakim</author>
<author>Wai Lam</author>
<author>Danyu Liu</author>
<author>Jahna Otterbacher</author>
<author>Hong Qi</author>
<author>Horacio Saggion</author>
<author>Simone Teufel</author>
<author>Michael Topper</author>
<author>Adam Winkel</author>
<author>Zhu Zhang</author>
</authors>
<title>MEAD — A platform for multidocument multilingual text summarization.</title>
<date>2004</date>
<booktitle>Conference on Language Resources and Evaluation LREC,</booktitle>
<location>Lisbon, Portugal,</location>
<marker>Radev, Allison, BlairGoldensohn, Blitzer, Celebi, Dimitrov, Drabek, Hakim, Lam, Liu, Otterbacher, Qi, Saggion, Teufel, Topper, Winkel, Zhang, 2004</marker>
<rawString>Dragomir Radev, Timothy Allison, Sasha BlairGoldensohn, John Blitzer, Arda Celebi, Stanko Dimitrov, Elliott Drabek, Ali Hakim, Wai Lam, Danyu Liu, Jahna Otterbacher, Hong Qi, Horacio Saggion, Simone Teufel, Michael Topper, Adam Winkel, and Zhu Zhang. 2004. MEAD — A platform for multidocument multilingual text summarization. Conference on Language Resources and Evaluation LREC, Lisbon, Portugal, (May 2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Mason</author>
<author>Eugene Charniak</author>
</authors>
<title>BLLIP at TAC 2011: A General Summarization System for a Guided Summarization Task.</title>
<date>2011</date>
<note>TAC Proceedings.</note>
<contexts>
<context position="2166" citStr="Mason and Charniak, 2011" startWordPosition="311" endWordPosition="314">alysis (number of topics, initial α and 0, number of iterations, and whether to optimize α and 0 in process). Figure 2 shows summarization with corpus and topic inputs, control settings, and the text summarization product. There are controls for extraction of sentences (Extract α and JSD Divisor) and for composing the summary (Order policy). Topic analysis has become a popular choice for text summarization as seen in Text Analysis ConFigure 1: Topic Analysis Figure 2: Text Summarization ferences (TAC, 2010; TAC, 2011) with individual team reports (Delort and Alfonseca, 2011; Lui et al., 2011; Mason and Charniak, 2011). Nenkova and McKeown (2012; 2011) included topic analysis among standard methods in their surveys of text summarization methodologies. Haghighi and Vanderwende (2009) explored extensions of LDA topic analysis for use in multiple document summarization tasks. Yet there are many control settings that can affect summarization that have not been explicitly studied or documented, and that are important for reproducing research results. In this text summarization pilot study, we experiment with several control settings. As in Mason and Charniak (2011) we do a general rather than guided summarizatio</context>
</contexts>
<marker>Mason, Charniak, 2011</marker>
<rawString>Rebecca Mason and Eugene Charniak. 2011. BLLIP at TAC 2011: A General Summarization System for a Guided Summarization Task. 2011 TAC Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andres K McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="6021" citStr="McCallum, 2002" startWordPosition="966" endWordPosition="967">ess by log P(w, z) where a greater log P(w, z) indicates better fit. After sufficient iterations through the corpus the log P(w, z) typically converges to steady state. Analysis products are topic determinations for the corpus as well as weighted estimates of topic word compositions Φ and document topic mixtures Θ. The α and 0 priors are optimized (reestimated) during training and the asymmetric α which varies by topic can be used as a measure of topic importance in our summarization step. The topic analysis implementation used in this pilot study borrows from the UMass Mallet topic analysis (McCallum, 2002). 2.2 Jensen-Shannon Divergence From the topic word compositions and optimized αs, we form a weighted aggregate vector of the prominent topics, and select sentences from the corpus that have minimal divergence from the aggregate topic. The operating assumption is that the aggregate topic vector adequately represents the content of an ideal summary. So the closer to zero divergence from the aggregate topic, the closer we are to the ideal summary. We seek to minimize the Jensen-Shannon Divergence, JSD(C||T), a symmetric KullbackLiebler (KL) divergence, between the extractive summary content, C, </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andres K. McCallum. 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meil˘a</author>
</authors>
<title>Comparing Clusterings – an information based distance.</title>
<date>2007</date>
<journal>J. Multivariate Analysis,</journal>
<pages>98--5</pages>
<marker>Meil˘a, 2007</marker>
<rawString>Marina Meil˘a. 2007. Comparing Clusterings – an information based distance. J. Multivariate Analysis, 98(5):873-895.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Mimno</author>
<author>Hanna M Wallach</author>
<author>Edmund Talley</author>
<author>Miriam Leenders</author>
<author>Andrew McCallum</author>
</authors>
<title>Optimizing Semantic Coherence in Topic Models.</title>
<date>2011</date>
<booktitle>EMNLP Conference,</booktitle>
<pages>262--272</pages>
<contexts>
<context position="11937" citStr="Mimno et al., 2011" startWordPosition="1989" endWordPosition="1992">ls. We assess topic quality more directly to see which model is better. 3 Topics 5 Topics 10 Topics -6.00 -5.97 -5.96 Table 1: Held-out Log Likelihood Number Topics. rl � nzwi + 0 P(w) = z nz• + 0• d,i 76 Useful topic quality measures are: Importance measured by number of documents (or optimized αs). Low importance topics, with very few documents related to a topic, indicate that we have more topics than necessary. While not a fatal flaw, the topic model may be over fit. Coherence measured as a log sum of cooccurrence proportions of each topic’s high frequency words across multiple documents (Mimno et al., 2011). The more negative the coherence measure, the poorer the coherence. A few poor coherence topics is not fatal, but the topic model may be over fit. Similarity to other topics measured by cosine distance between topic vectors is undesirable. The more similar the topics, the more difficult it is to distinguish between them. Many similar topics makes it difficult to discriminate among topics over the corpus. Reviewing the document quality for 3, 5 and 10 topics we find: • More low importance topics in 10 versus 5 and 3 topic models, • Somewhat better topic coherence in 3 and 5 topic models, • Und</context>
</contexts>
<marker>Mimno, Wallach, Talley, Leenders, McCallum, 2011</marker>
<rawString>David Mimno, Hanna M. Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. 2011. Optimizing Semantic Coherence in Topic Models. 2011 EMNLP Conference, Proceedings:262-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas C Montgomery</author>
</authors>
<title>Design and Analysis of Experiments.</title>
<date>1997</date>
<publisher>John Wiley and Sons,</publisher>
<location>New York, USA.</location>
<contexts>
<context position="18348" citStr="Montgomery, 1997" startWordPosition="3047" endWordPosition="3048">ombination of experimental factors of the summary (blind scoring). Summary measures thus include: ROUGE R-2, ROUGE SU4, and Fluency. 4.3 Select Experimental Design Screening designs focus on detecting and assessing main effects and optionally low order interaction effects. When all experimental factors are continuous, center points may also be included in some designs. In subsequent stages of experimentation, when factors have been reduced to a minimum, one can use more fine grained factor settings to better map the response surface for those factors. Two common families of screening designs (Montgomery, 1997) are: Two level fractional factorial Uses a power of 1/2 fraction of a full two level factorial design. For example, instead of running all possible combinations of 5 factors (i.e. 32 trials), you could choose a 1/2 or even 1/4 fraction of the design, based on how many experiments you can run and how much confounding you are willing to accept between main effects and various interaction effects. The 1/2 fraction of a 5 factor design would result in 16 trials being run with the main effects estimated clear of any 2-way or 3-way interactions. Plackett-Burman These screening designs are available</context>
</contexts>
<marker>Montgomery, 1997</marker>
<rawString>Douglas C. Montgomery. 1997. Design and Analysis of Experiments. John Wiley and Sons, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<date>2011</date>
<booktitle>Automatic Summarization. Foundations and Trends in Information Retrieval,</booktitle>
<pages>5--2</pages>
<marker>Nenkova, McKeown, 2011</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2011. Automatic Summarization. Foundations and Trends in Information Retrieval, 5(2-3):1003-233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<title>A Survey of Text Summarization Techniques. Mining Text Data. In</title>
<date>2012</date>
<editor>Charu C. Aggarwal and ChengXiang Zhai (eds.)</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2193" citStr="Nenkova and McKeown (2012" startWordPosition="315" endWordPosition="318">nitial α and 0, number of iterations, and whether to optimize α and 0 in process). Figure 2 shows summarization with corpus and topic inputs, control settings, and the text summarization product. There are controls for extraction of sentences (Extract α and JSD Divisor) and for composing the summary (Order policy). Topic analysis has become a popular choice for text summarization as seen in Text Analysis ConFigure 1: Topic Analysis Figure 2: Text Summarization ferences (TAC, 2010; TAC, 2011) with individual team reports (Delort and Alfonseca, 2011; Lui et al., 2011; Mason and Charniak, 2011). Nenkova and McKeown (2012; 2011) included topic analysis among standard methods in their surveys of text summarization methodologies. Haghighi and Vanderwende (2009) explored extensions of LDA topic analysis for use in multiple document summarization tasks. Yet there are many control settings that can affect summarization that have not been explicitly studied or documented, and that are important for reproducing research results. In this text summarization pilot study, we experiment with several control settings. As in Mason and Charniak (2011) we do a general rather than guided summarization. Our primary contribution</context>
</contexts>
<marker>Nenkova, McKeown, 2012</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2012. A Survey of Text Summarization Techniques. Mining Text Data. In Charu C. Aggarwal and ChengXiang Zhai (eds.) Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Tom Griffiths</author>
</authors>
<title>Probabilisitic Topic Models. Latent Semantic Analysis: A road to Meaning. In</title>
<date>2007</date>
<editor>T. Landauer, S. D. McNamara &amp; W. Kintsch (eds.) Laurence Erlbaum.</editor>
<contexts>
<context position="4157" citStr="Steyvers and Griffiths, 2007" startWordPosition="619" endWordPosition="622">s and document mixtures of topics. Analysis constructs topic compositions and document mixtures by assigning words to topics within documents. Weighted topic compositions can then be used as a basis for selecting the most informative text to include in summarizations. LDA topic analysis is based on a generative probabilistic model. Document mixtures of topics are generated by a multinomial distribution, Θ, and topic compositions of words are generated by a multinomial distribution, Φ. Both Θ and Φ in turn are generated by Dirichlet distributions with parameters α and 0 respectively. Figure 3 (Steyvers and Griffiths, 2007) shows a corpus explained as the product of topic word compositions (Φ) and document topic mixtures (Θ). documents topics documents Corpus Figure 3: Topic Model The joint distribution of words and topics (Griffiths and Steyvers (2004)) is given by P(w, z) = P(wIz)P(z) where in generating a document the topics are generated with probability P(z) and the words given the topics are generated with probability P(wIz). Here Γ(0 (0•) Z ��Z-� Qv Γ (nzv + 0) Γ (0)V H Γ (nz• + 0•) , z=1 (1) where nzv is the number of times word v occurs in topic z, nz• is the number of times topic z occurs, 0• is the su</context>
</contexts>
<marker>Steyvers, Griffiths, 2007</marker>
<rawString>Mark Steyvers and Tom Griffiths. 2007. Probabilisitic Topic Models. Latent Semantic Analysis: A road to Meaning. In T. Landauer, S. D. McNamara &amp; W. Kintsch (eds.) Laurence Erlbaum.</rawString>
</citation>
<citation valid="true">
<title>Summarization Track.</title>
<date>2010</date>
<booktitle>Task Analysis Conference</booktitle>
<note>http://www.nist.gov/tac/2010/Summarization/.</note>
<contexts>
<context position="8087" citStr="(2010)" startWordPosition="1295" endWordPosition="1295">D(Ct−1||T) − J5D(5t, Ct−1||T))(3) function(length(5t)) , where 5t is the sentence under consideration and Ct−1 is the content from the previously completed iterations, and the function of length of 5t, is either the constant 1 (i.e. no correction for sentence V/ length) or length(5t). 3 Pilot Study Using TAC 2010 Samples Our goal is to investigate and optimize factors that impact multi-document extractive summarization. We hope to subsequently extend our findings and experience to abstractive summarization as well. For our pilot, we’ve chosen summarization of the 2010 Text Analysis Conference (2010) sample themes, which are conveniently available and of a manageable size. The three sample themes are from different summarization categories out of a total of 46 news themes over five different categories, with 10 original and 10 follow-up news reports each. In the original TAC 2010 task, participants were asked to do focused queries varying with the summarization category. In our pilot we perform an undirected summarization of the original news reports. NIST provides 4 model summaries for each news theme annotated for the focused summary, and we use these model summaries in scoring our extr</context>
</contexts>
<marker>2010</marker>
<rawString>Task Analysis Conference 2010 – Summarization Track. 2010. http://www.nist.gov/tac/2010/Summarization/.</rawString>
</citation>
<citation valid="true">
<date>2011</date>
<booktitle>Task Analysis Conference 2011 – Summarization Track.</booktitle>
<note>http://www.nist.gov/tac/2011/Summarization/.</note>
<contexts>
<context position="2200" citStr="(2012; 2011)" startWordPosition="318" endWordPosition="319">er of iterations, and whether to optimize α and 0 in process). Figure 2 shows summarization with corpus and topic inputs, control settings, and the text summarization product. There are controls for extraction of sentences (Extract α and JSD Divisor) and for composing the summary (Order policy). Topic analysis has become a popular choice for text summarization as seen in Text Analysis ConFigure 1: Topic Analysis Figure 2: Text Summarization ferences (TAC, 2010; TAC, 2011) with individual team reports (Delort and Alfonseca, 2011; Lui et al., 2011; Mason and Charniak, 2011). Nenkova and McKeown (2012; 2011) included topic analysis among standard methods in their surveys of text summarization methodologies. Haghighi and Vanderwende (2009) explored extensions of LDA topic analysis for use in multiple document summarization tasks. Yet there are many control settings that can affect summarization that have not been explicitly studied or documented, and that are important for reproducing research results. In this text summarization pilot study, we experiment with several control settings. As in Mason and Charniak (2011) we do a general rather than guided summarization. Our primary contribution is ill</context>
</contexts>
<marker>2011</marker>
<rawString>Task Analysis Conference 2011 – Summarization Track. 2011. http://www.nist.gov/tac/2011/Summarization/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Dave Newman</author>
<author>Max Welling</author>
</authors>
<date>2007</date>
<booktitle>Collapsed Variational Inference for HDP. Advances in Neural Information Processing Systems:1481-1488.</booktitle>
<contexts>
<context position="10914" citStr="Teh et al., 2007" startWordPosition="1796" endWordPosition="1799">r log likelihoods indicate a better number of topics. While it would be impractical to do such a study for each news theme or each document summary, it is reasonable to do so on a few sample themes and then generalize to similar corpora. We look at log likelihood for 3, 5, and 10 topics using the TAC 2010 sample themes. As there are only 10 documents for each theme, we use the TAC 2010 update documents as heldout documents for calculating the log likelihoods. Topic word distributions, Φ, from training are used to infer document mixtures, O, on the heldout data, and the log P(w) is calculated (Teh et al., 2007) as: � nzd + α (4) n•d + α• , where the sum is over all possible topics for a given word and the product is over all documents and words. Table 1 shows mean log likelihoods for the news themes at 3, 5 and 10 topics each. There is little practical difference between the log likelihood measures even though the 3 topic model has a significantly lower log likelihood (p &lt; 0.05) than the 5 and 10 topic models. We assess topic quality more directly to see which model is better. 3 Topics 5 Topics 10 Topics -6.00 -5.97 -5.96 Table 1: Held-out Log Likelihood Number Topics. rl � nzwi + 0 P(w) = z nz• + 0</context>
</contexts>
<marker>Teh, Newman, Welling, 2007</marker>
<rawString>Yee Whye Teh, Dave Newman, and Max Welling. 2007. Collapsed Variational Inference for HDP. Advances in Neural Information Processing Systems:1481-1488.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
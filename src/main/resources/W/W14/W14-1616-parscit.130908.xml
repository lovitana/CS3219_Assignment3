<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.990853">
Factored Markov Translation with Robust Modeling
</title>
<author confidence="0.979196">
Yang Feng† Trevor Cohn‡ Xinkai Du
</author>
<affiliation confidence="0.906963">
† Information Sciences Institue ‡ Computing and Information Systems
Computer Science Department The University of Melbourne
University of Southern California VIC 3010 Australia
</affiliation>
<email confidence="0.998474">
{yangfeng145, xinkaid}@gmail.com t.cohn@unimelb.edu.au
</email>
<sectionHeader confidence="0.993898" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999195882352941">
Phrase-based translation models usually
memorize local translation literally and
make independent assumption between
phrases which makes it neither generalize
well on unseen data nor model sentence-
level effects between phrases. In this pa-
per we present a new method to model
correlations between phrases as a Markov
model and meanwhile employ a robust
smoothing strategy to provide better gen-
eralization. This method defines a re-
cursive estimation process and backs off
in parallel paths to infer richer structures.
Our evaluation shows an 1.1–3.2% BLEU
improvement over competitive baselines
for Chinese-English and Arabic-English
translation.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999801031746032">
Phrase-based methods to machine translation
(Koehn et al., 2003; Koehn et al., 2007) have dras-
tically improved beyond word-based approaches,
primarily by using phrase-pairs as translation
units, which can memorize local lexical con-
text and reordering patterns. However, this lit-
eral memorization mechanism makes it general-
ize poorly to unseen data. Moreover, phrase-based
models make an independent assumption, stating
that the application of phrases in a derivation is in-
dependent to each other which conflicts with the
underlying truth that the translation decisions of
phrases should be dependent on context.
There are some work aiming to solve the two
problems. Feng and Cohn (2013) propose a
word-based Markov model to integrate translation
and reordering into one model and use the so-
phisticated hierarchical Pitman-Yor process which
backs off from larger to smaller context to pro-
vide dynamic adaptive smoothing. This model
shows good generalization to unseen data while
it uses words as the translation unit which can-
not handle multiple-to-multiple links in real word
alignments. Durrani et al. (2011) and Durrani et
al. (2013) propose an operation sequence model
(OSM) which models correlations between mini-
mal translation units (MTUs) and evaluates proba-
bilities with modified Kneser-Ney smoothing. On
one hand the use of MTUs can help retain the
multiple-to-multiple alignments, on the other hand
its definition of operations where source words
and target words are bundled into one operation
makes it subjected to sparsity. The common fea-
ture of the above two methods is they both back off
in one fixed path by dropping least recent events
first which precludes some useful structures. For
the segment pairs &lt;bˇa t¯a kˇaol`v j`ınq`u, take it into
account&gt; in Figure 1, the more common structure
is &lt;bˇa ... kˇaol`v jinq`u, take ... into account&gt;. If
we always drop the least recent events first, then
we can only learn the pattern &lt;... t¯a kˇaol`v j`ınq`u,
... it into account&gt;.
On these grounds, we propose a method with
new definition of correlations and more robust
probability modeling. This method defines a
Markov model over correlations between minimal
phrases where each is decomposed into three fac-
tors (source, target and jump). In the meantime
it employs a fancier smoothing strategy for the
Markov model which backs off by dropping mul-
tiple conditioning factors in parallel in order to
learn richer structures. Both the uses of factors
and parallel backoff give rise to robust modeling
against sparsity. In addition, modeling bilingual
information and reorderings into one model in-
stead of adding them to the linear model as sep-
arate features allows for using more sophisticated
estimation methods rather than get a loose weight
for each feature from tuning algorithms.
We compare the performance of our model with
that of the phrase-based model and the hierarchi-
cal phrase-based model on the Chinese-English
and Arabic-English NIST test sets, and get an im-
</bodyText>
<page confidence="0.978762">
151
</page>
<note confidence="0.820633">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 151–159,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.9907505">
Figure 1: Example Chinese-English sentence pair
with word alignments shown as filled grid squares.
</figureCaption>
<bodyText confidence="0.955218">
provement up to 3.2 BLEU points absolute.1
</bodyText>
<sectionHeader confidence="0.963775" genericHeader="introduction">
2 Modelling
</sectionHeader>
<bodyText confidence="0.996980875">
Our model is phrase-based and works like a
phrase-based decoder by generating target trans-
lation left to right using phrase-pairs while jump-
ing around the source sentence. For each deriva-
tion, we can easily get its minimal phrase (MPs)
sequence where MPs are ordered according to the
order of their target side. Then this sequence of
events is modeled as a Markov model and the log
probability under this Markov model is included
as an additional feature into the linear SMT model
(Och, 2003).
A MP denotes a phrase which cannot contain
other phrases. For example, in the sentence pair
in Figure 1, &lt;bˇa t¯a , take it&gt; is a phrase but not
a minimal phrase, as it contains smaller phrases
of &lt;bˇa , take&gt; and &lt;t¯a , it&gt;. MPs are a com-
plex event representation for sequence modelling,
and using these naively would be a poor choice
because few bigrams and trigrams will be seen
often enough for reliable estimation. In order
to reason more effectively from sparse data, we
consider more generalized representations by de-
composing MPs into their component events: the
source phrase (source ¯f), the target phrase (tar-
get ¯e) and the jump distance from the preceding
MP (jump j), where the jump distance is counted
in MPs, not in words. For sparsity reasons, we
do not use the jump distance directly but instead
group it into 12 buckets:
{insert, &lt; −5, −4, −3, −2, −1, 0,1, 2, 3, 4, &gt; 51,
where the jump factor is denoted as insert when
the source side is NULL. For the sentence pair in
</bodyText>
<footnote confidence="0.963331">
1We will contribute the code to Moses.
</footnote>
<figureCaption confidence="0.903815">
Figure 1, the MP sequence is shown in Figure 2.
</figureCaption>
<bodyText confidence="0.999720333333333">
To evaluate the Markov model, we condition
each MP on the previous k − 1 MPs and model
each of the three factors separately based on a
chain rule decomposition. Given a source sentence
f and a target translation e, the joint probability is
defined as
</bodyText>
<equation confidence="0.998737">
¯i−1 i¯i−1
fi−k+1 , ji−k+1 , ei−k+1)
¯fi−1
i−k+1, ji−1
i−k+1, ¯ei−1
i−k+1)
(1)
</equation>
<bodyText confidence="0.999838523809524">
where ¯fi, ¯ei and ji are the factors of MPi, ¯fI1 =
( ¯f1, ¯f2, ... , fI) is the sequence of source MPs,
¯eI1 = (¯e1, ¯e2,. . . , ¯eI) is the sequence of tar-
get MPs, and jI1 = (j1, j2, ... , jI) is the vec-
tor of jump distance between MPi−1 and MPi, or
insert for MPs with null source sides.2 To eval-
uate each of the k-gram models, we use modified
Keneser-Ney smoothing to back off from larger
context to smaller context recursively.
In summary, adding the Markov model into the
decoder involves two passes: 1) training a model
over the MP sequences extracted from a word
aligned parallel corpus; and 2) calculating the
probability of the Markov model for each trans-
lation hypothesis during decoding. This Markov
model is combined with a standard phrase-based
model3 (Koehn et al., 2007) and used as an addi-
tional feature in the linear model.
In what follows, we will describe how to estati-
mate the k-gram Markov model, focusing on back-
off (§2.1) and smoothing (§2.2).
</bodyText>
<subsectionHeader confidence="0.996677">
2.1 Parallel Backoff
</subsectionHeader>
<bodyText confidence="0.9991184">
Backoff is a technique used in language model —
when estimating a higher-order gram, instead of
using the raw occurrence count, only a portion is
used and the remainder is computed using a lower-
order model in which one of the context factors
</bodyText>
<footnote confidence="0.98222175">
2Note that factors at indices 0, −1, ... , −(k − 1) are set
to a sentinel value to denote the start of sentence.
3The phrase-based model considers larger phrase-pairs
than just MPs, while our Markov model consider only MPs.
As each phrase-pair is composed of a sequence of MPs un-
der fixed word alignment, by keeping the word alignment for
each phrase, a decoder derivation unambiguously specifies
the MP sequence for scoring under our Markov model.
</footnote>
<equation confidence="0.999791888888889">
p(¯eI1, jI1, ¯fI1) = I p(¯ei |¯i i
i=1 fi−k+1 , ji−k+1 ¯ei−1
i−k+1)
I
X p( fi|
i=1
I
X p(ji|
i=1
</equation>
<page confidence="0.996134">
152
</page>
<figure confidence="0.665210625">
index sentence pair minimal phrase sequence
wˇom´en y¯ingg¯ai bˇa t¯a yˇe kˇaol`v jinq`u jump source target
1 We T1 1 wˇom´en We
2 should T2 1 y¯ingg¯ai should
3 also T3 3 yˇe also
4 take T4 -2 bˇa take
5 it T5 1 t¯a it
6 into account T6 2 kˇaol`v jinq`u into account
</figure>
<figureCaption confidence="0.999658">
Figure 2: The minimal phrase sequence T1,..., T6 extracted from the sentence pair in Figure 1.
</figureCaption>
<equation confidence="0.918589388888889">
step 3-gram ¯e3 |f3, j3, ¯e2, f2, j2, ¯e1, f1, j1
0 into account  |kˇaol`v jinq`u, 2, it, t¯a, 1, take, bˇa, -2
↓ 1
1 into account  |kˇaol`v jinq`u, 2, it, t¯a, –, take, bˇa, -2
↓ t¯a
2 into account  |kˇaol`v jinq`u, 2, it, –, –, take, bˇa, -2
↓ it
3 into account  |kˇaol`v jinq`u, 2, –, –, –, take, bˇa, -2
↓ -2
4 into account  |kˇaol`v jinq`u, 2, –, –, –, take, bˇa, –
↓ bˇa
5 into account  |kˇaol`v jinq`u, 2, –, –, –, take,–, –
↓ take
6 into account  |kˇaol`v jinq`u, 2, , , , , ,
↓ 2
7 into account  |kˇaol`v jinq`u, , , , , , ,
↓ kˇaol`v jinq`u
8 into account  |, , , , , , , -
</equation>
<figureCaption confidence="0.955696">
Figure 3: One backoff path for the 3-gram in
Equation 2. The symbols besides each arrow mean
</figureCaption>
<bodyText confidence="0.971108583333333">
the current factor to drop; “–” is a placeholder for
factors which can take any value.
is dropped. Here the probabilities of the lower-
order which is used to construct the higher-order is
called the backoff probability of the higher-order
gram. Different from standard language models
which drop the least recent words first, we em-
ploy a different backoff strategy which considers
all possible backoff paths. Taking as an example
the 3-gram T4T5T6 in Figure 2, when estimating
the probability of the target factor
p(into account  |kˇaol`v j`ınq`u, 2, it, t¯a, 1, take, bˇa, -2 ) ,
</bodyText>
<page confidence="0.245707">
(2)
</page>
<figureCaption confidence="0.815006">
Figure 4: The backoff graph for the 3-gram model
</figureCaption>
<bodyText confidence="0.9638465">
of the target factor. The symbol beside each arrow
is the factor to drop.
we consider two backoff paths: path1 drops the
factors in the order -2, bˇa, take, 1, t¯a, it, 2,
kˇaol`v jinq`u; path2 uses order 1, t¯a, it, -2, bˇa,
take, 2, kˇaol`v jinq`u. Figure 3 shows the backoff
process for path2. In this example with two back-
off paths, the backoff probability g is estimated as
</bodyText>
<equation confidence="0.72582">
1 1
</equation>
<bodyText confidence="0.938829333333333">
g(into acc.|c) = 2 p(into acc.|c0)+2 p(into acc.|c00) ,
where c =&lt; kˇaol`v jinq`u, 2, it, t¯a, 1, take, bˇa, -2 &gt;,
c0 =&lt; kˇaol`v jinq`u, 2, it, t¯a, 1, take, bˇa, – &gt; and
c00 =&lt; kˇaol`v jinq`u, 2, it, t¯a, –, take, bˇa, -2 &gt;.
Formally, we use the notion of backoff graph to
define the recursive backoff process of a k-gram
</bodyText>
<page confidence="0.995578">
153
</page>
<bodyText confidence="0.9996063">
and denote as nodes the k-gram and the lower-
order grams generated by the backoff. Once one
node occurs in the training data fewer than τ times,
then estimates are calculated by backing off to the
nodes in the next lower level where one factor is
dropped (denoted using the placeholder – in Fig-
ure 4). One node can have one or several candidate
backoff nodes. In the latter case, the backoff prob-
ability is defined as the average of the probabilities
of the backoff nodes in the next lower level.
We define the backoff process for the 3-gram
model predicting the target factor, ¯e3, as illustrated
in Figure 4. The top level is the full 3-gram, from
which we derive two backoff paths by dropping
factors from contextual events, one at a time. For-
mally, the backoff strategy is to drop the previ-
ous two MPs one by one while for each MP the
dropping routine is first the jump factor, then the
source factor and final the target factor. Each step
on the path corresponds to dropping an individ-
ual contextual factor from the context. The paths
converge when only the third MP left, then the
backoff proceeds by dropping the jump action, j3,
then finally the source phrase, f3. The paths B-
D-F-H-J and C-E-G-I-K show all the possible or-
derings (corresponding to c00 and c0, respectively)
for dropping the two previous MPs. The exam-
ple backoff in Figure 3 corresponds the path A-
B-D-F-H-J-L-M-N in Figure 4, shown as heavier
lines. When generizing to the k-gram for target
p(¯ek |¯fk1 , jk1 , ¯ek−1
1 ), the backoff strategy is to first
drop the previous k-1 MPs one by one (for each
MP, still drops in the order of jump, source and
target), then the kth jump factor and finally the kth
source factor. According to the strategy, the top
node has k-1 nodes to back off to and for the node
¯ek |¯fk2 , jk 2 , ¯ek−1
2 where only the factors of MP1 are
dropped, there are k-2 nodes to back off to.
</bodyText>
<subsectionHeader confidence="0.998398">
2.2 Probability Estimation
</subsectionHeader>
<bodyText confidence="0.943847411764706">
We adopt the technique used in factor language
models (Bilmes and Kirchhoff, 2003; Kirchhoff et
al., 2007) to estimate the probability of a k-gram
p(¯ei|c) where c = ¯fii−k+1, jii−k+1, ¯e−1
i−k+1. Ac-
cording to the definition of backoff, only when the
count of the k-gram exceeds some given threshold,
its maximum-likelihood estimate, pML(¯ek|c) =
N(¯ek,c) is used, where N(·) is the count of an
N(c)
event and/or context. Otherwise, only a portion of
pML(¯ek|c) is used and the remainder is constructed
from a lower-level (by dropping a factor). In or-
der to ensure valid probability estimates, i.e. sums
to unity, probability mass needs to be “stolen”
from the higher level and given to the lower level.
Hence, the whole definition is
</bodyText>
<equation confidence="0.815612">
p(¯ei  |c) = dN(¯ei,c)pml (¯ei  |c) if N(¯ei, c) &gt; τk
α(c)g(¯ei, c) otherwise
(3)
</equation>
<bodyText confidence="0.997938888888889">
where dN(¯ei,c) is a discount parameter which re-
serves probability from the maximum-likelihood
estimate for backoff smoothing at the next lower-
level, and we estimate dN(¯ei,c) using modified
Kneser-Ney smoothing (Kneser and Ney, 1995;
Chen and Goodman, 1996); τk is the threshold for
the count of the k-gram, α(c) is the backoff weight
used to make sure the entire distribution still sums
to unity,
</bodyText>
<equation confidence="0.9956825">
1 − E¯e:N(¯e,c)&gt;τk dN(¯e,c)pML(¯e|c)
E ¯e:N(¯e,c)≤τk g(¯e, c) ,
</equation>
<bodyText confidence="0.93702">
and g(¯ei, c) is the backoff probability which we
estimate by averaging over the nodes in the next
lower level,
</bodyText>
<equation confidence="0.9851885">
1 � g(¯ei, c) =
φ c� p(¯ei|c0),
</equation>
<bodyText confidence="0.999971375">
where φ is the number of nodes to back off, c0 is
the lower-level context after dropping one factor
from c.
The k-gram for the source and jump factors are
estimated in the same way, using the same backoff
semantics.4 Note (3) is applied independently to
each of the three models, so the use of backoff may
differ in each case.
</bodyText>
<sectionHeader confidence="0.999702" genericHeader="method">
3 Discussion
</sectionHeader>
<bodyText confidence="0.996932125">
As a part of the backoff process our method
can introduce gaps in estimating rule probabili-
ties; these backoff patterns often bear close re-
semblance to SCFG productions in the hierarchi-
cal phrase-based model (Chiang, 2007). For ex-
ample, in step 0 in Figure 3, as all the jump factors
are present, this encodes the full ordering of the
MPs and gives rise to the aligned MP pairs shown
in Figure 5 (a). Note that an X 1 placeholder is
included to ensure the jump distance from the pre-
vious MP to the MP &lt;bˇa, take&gt; is -2. The ap-
proximate SCFG production for the MP pairs is
&lt;bˇa t¯a X 1 kˇaol`v jinq`u, X 1 take it into account&gt;.
4Although there are fewer final steps, L-M-N in Fig. 4,
as we assume the MP is generated in the order jump, source
phrase then target phrase in a chain rule decomposition.
</bodyText>
<equation confidence="0.97058">
α(c) =
</equation>
<page confidence="0.936463">
154
</page>
<bodyText confidence="0.969599">
t¯a · kˇaol`v jinq`u,
t¯a bˇa kˇaol`v jinq`u X 1 ,
t¯a · kˇaol`v jinq`u · · ·
where X and · can only hold one MP while · · ·
can cover zero or more MPs. In step 3 after drop-
ping t¯a and it, we introduce a gap X 2 as shown in
Figure 5 (b).
From above, we can see that our model has two
kinds of gaps: 1) in the source due to the left-to-
right target ordering (such as the · in step 3); and
2) in the target, arising from backoff (such as the
X 2 in step 3). Accordingly our model supports
rules than cannot be represented by a 2-SCFG
(e.g., step 3 in Figure 5 requires a 4-SCFG). In
contrast, the hierarchical phrase-based model al-
lows only 2-SCFG as each production can rewrite
as a maximum of two nonterminals. On the other
hand, our approach does not enforce a valid hier-
archically nested derivation which is the case for
Chiang’s approach.
</bodyText>
<sectionHeader confidence="0.998755" genericHeader="method">
4 Related Work
</sectionHeader>
<equation confidence="0.918582">
bˇa · X 1 ···
bˇa · X 1 ,
</equation>
<figureCaption confidence="0.611451">
Figure 5: Approximate SCFG patterns for step 0,
</figureCaption>
<bodyText confidence="0.9675311">
3 of Figure 3. X is a non-terminal which can only
be rewritten by one MP. · and · · · denote gaps
introduced by the left-to-right decoding algorithm
and · can only cover one MP while · · · can
cover zero or more MPs.
In step 1, as the jump factor 1 is dropped, we do
not know the orientation between bˇa and t¯a. How-
ever several jump distances are known: from X 1
to bˇa is distance -2 and t¯a to kˇaol`v jinq`u is 2. In
this case, the source side can be
</bodyText>
<equation confidence="0.481976">
bˇa t¯a X 1 kˇaol`v jinq`u,
</equation>
<bodyText confidence="0.999985533333334">
The method introduced in this paper uses fac-
tors defined in the same manner as in Feng and
Cohn (2013), but the two methods are quite differ-
ent. That method (Feng and Cohn, 2013) is word-
based and under the frame of Bayesian model
while this method is MP-based and uses a sim-
pler Kneser-Ney smoothing method. Durrani et
al. (2013) also present a Markov model based on
MPs (they call minimal translation units) and fur-
ther define operation sequence over MPs which
are taken as the events in the Markov model. For
the probability estimation, they use Kneser-Ney
smoothing with a single backoff path. Different
from operation sequence, our method gives a neat
definition of factors which uses jump distance di-
rectly and avoids the bundle of source words and
target words like in their method, and hence miti-
gates sparsity. Moreover, the use of parallel back-
off infers richer structures and provides robust
modeling.
There are several other work focusing on mod-
eling bilingual information into a Markov model.
Crego et al. (2011) develop a bilingual language
model which incorporates words in the source and
target languages to predict the next unit, and use
it as a feature in a translation system. This line
of work was extended by Le et al. (2012) who de-
velop a novel estimation algorithm based around
discriminative projection into continuous spaces.
Neither work includes the jump distance, and nor
</bodyText>
<page confidence="0.998173">
155
</page>
<bodyText confidence="0.999614636363637">
do they consider dynamic strategies for estimating
k-gram probabilities.
Galley and Manning (2010) propose a method
to introduce discontinuous phrases into the phrase-
based model. It makes use of the decoding mecha-
nism of the phrase-based model which jumps over
the source words and hence can hold discontin-
uous phrases naturally. However, their method
doesn’t touch the correlations between phrases and
probability modeling which are the key points we
focus on.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9987808">
We design experiments to first compare our
method with the phrase-based model (PB), the op-
eration sequence model (OSM) and the hierarchi-
cal phrase-based model (HPB), then we present
several experiments to test:
</bodyText>
<listItem confidence="0.9835701">
1. how each of the factors in our model and par-
allel backoff affect overall performance;
2. how the language model order affects the rel-
ative gains, in order to test if we are just learn-
ing a high order LM, or something more use-
ful;
3. how the Markov model interplay with the
distortion and lexical reordering models of
Moses, and are they complemenatary;
4. whether using MPs as translation units is bet-
</listItem>
<bodyText confidence="0.969564">
ter in our approach than the simpler tactic of
using only word pairs.
</bodyText>
<subsectionHeader confidence="0.990835">
5.1 Data Setup
</subsectionHeader>
<bodyText confidence="0.998826777777778">
We consider two language pairs: Chinese-English
and Arabic-English. The Chinese-English paral-
lel training data is made up of the non-UN por-
tions and non-HK Hansards portions of the NIST
training corpora, distributed by the LDC, having
1,658k sentence pairs with 40m and 44m Chinese
and English words. We used the NIST 02 test set
as the development set and evaluated performance
on the test sets from NIST 03 and 05.
For the Arabic-English task, the training data
comprises several LDC corpora,5 including 276k
sentence pairs and 8.21m and 8.97m words in Ara-
bic and English, respectively. We evaluated on the
NIST test sets from 2003 and 2005, and the NIST
02 test set was used for parameter tuning.
On both cases, we used the factor language
model module (Kirchhoff et al., 2007) of the
SRILM toolkit (Stolcke, 2002) to train a Markov
</bodyText>
<equation confidence="0.4095795">
5LDC2004E72, LDC2004T17, LDC2004T18,
LDC2006T02
</equation>
<bodyText confidence="0.999349466666667">
model with the order = 3 over the MP sequences.6
The threshold count of backoff for all nodes was
τ = 2.
We aligned the training data sets by first using
GIZA++ toolkit (Och and Ney, 2003) to produce
word alignments on both directions and then com-
bining them with the diag-final-and heuristic. All
experiments used a 5-gram language model which
was trained on the Xinhua portion of the GIGA-
WORD corpus using the SRILM toolkit. Transla-
tion performance was evaluated using BLEU (Pa-
pineni et al., 2002) with case-insensitive n ≤ 4-
grams. We used minimum error rate training (Och,
2003) to tune the feature weights to maximize the
BLEU score on the development set.
We used Moses for PB and Moses-chart for
HPB with the configuration as follows. For both,
max-phrase-length=7, ttable-limit7=20, stack-
size=50 and max-pop-limit=500; For Moses,
search-algorithm=1 and distortion-limit=6; For
Moses-chart, search-algorithm=3 and max-char-
span8=20 for Moses-chart. We used both the dis-
tortion model and the lexical reordering model for
Moses (denoted as Moses-l) except in §5.5 we only
used the distortion model (denoted as Moses-d).
We implemented the OSM according to Durrani
et al. (2013) and used the same configuration with
Moses-l. For our method we used the same config-
uration as Moses-l but adding an additional feature
of the Markov model over MPs.
</bodyText>
<subsectionHeader confidence="0.997448">
5.2 Performance Comparison
</subsectionHeader>
<bodyText confidence="0.999958533333333">
We first give the results of performance compar-
ison. Here we add another system (denoted as
Moses-l+trgLM): Moses-l together with the target
language model trained on the training data set,
using the same configuration with Moses-l. This
system is used to test whether our model gains im-
provement just for using additional information on
the training set. We use the open tool of Clark et
al. (2011) to control for optimizer stability and test
statistical significance.
The results are shown in Tables 1 and 2. The
two language pairs we used are quite different:
Chinese has a much bigger word order differ-
ence c.f. English than does Arabic. The results
show that our system can outperform the baseline
</bodyText>
<footnote confidence="0.964947571428571">
6We only employed MPs with the length ≤ 3. If a MP had
more than 3 words on either side, we omitted the alignment
links to the first target word of this MP and extracted MPs
according to the new alignment.
7The maximum number of lexical rules for each source
span.
8The maximum span on the source a rule can cover.
</footnote>
<page confidence="0.990741">
156
</page>
<table confidence="0.999431666666667">
System NIST 02 (dev) NIST 03 NIST 05
Moses-l 36.0 32.8 32.0
Moses-chart 36.9 33.6 32.6
Moses-l+trgLM 36.4 33.9 32.9
OSM 36.6 34.0 33.1
our model 37.9 36.0 35.1
</table>
<tableCaption confidence="0.939443">
Table 1: BLEU % scores on the Chinese-English
data set.
</tableCaption>
<table confidence="0.999907833333333">
System NIST 02 (dev) NIST 03 NIST 05
Moses-l 60.4 52.0 52.8
Moses-chart 60.7 51.8 52.4
Moses-l+trgLM 60.8 52.6 53.3
OSM 61.1 52.9 53.4
our model 62.2 53.6 53.9
</table>
<tableCaption confidence="0.9461035">
Table 2: BLEU % scores on the Arabic-English
data set.
</tableCaption>
<table confidence="0.999728714285714">
System Chinese-English Arabic-English
NIST 02 NIST 03 NIST 02 NIST 03
Moses-l 36.0 32.8 60.4 52.0
+t 36.3 33.8 60.9 52.4
+t+j 37.1 34.7 62.1 53.4
+t+j+s 37.6 34.8 62.5 53.9
+t+j+s+p 37.9 36.0 62.2 53.6
</table>
<tableCaption confidence="0.961626">
Table 3: The impact of factors and parallel back-
off. Key: t–target, j–jump, s–source, p–parallel
backoff.
</tableCaption>
<table confidence="0.999628">
System 2gram 3gram 4gram 5gram 6gram
Moses-l 27.2 32.4 33.0 32.8 33.2
our method 31.6 34.0 35.8 36.0 36.2
</table>
<tableCaption confidence="0.9904175">
Table 4: The impact of the order of the standard
language models.
</tableCaption>
<bodyText confidence="0.999969285714286">
systems significantly (with p &lt; 0.005) on both
language pairs, nevertheless, the improvement on
Chinese-English is bigger. The big improvement
over Moses-l+trgLM proves that the better perfor-
mance of our model does not solely comes from
the use of the training data. And the gain over
OSM means our definition of factors gives a better
handling to sparsity. We also notice that HPB does
not give a higher BLEU score on Arabic-English
than PB. The main difference between HPB and
PB is that HPB employs gapped rules, so this re-
sult suggests that gaps are detrimental for Arabic-
English translation. In §5.3, we experimentally
validate this claim with our Markov model.
</bodyText>
<subsectionHeader confidence="0.999041">
5.3 Impact of Factors and Parallel Backoff
</subsectionHeader>
<bodyText confidence="0.908123076923077">
We now seek to test the contribution of target,
jump, source factors, as well as the parallel back-
off technique in terms of BLEU score. We
performed experiments on both Chinese-English
and Arabic-English to test whether the contri-
bution was related to language pairs. We de-
signed the experiments as follows. We first
trained a 3-gram Markov model only over tar-
get factors, p(¯eI1|¯fI1) = �I i=1 p(¯ei|¯ei−1
i−2), de-
noted +t. Then we added the jump fac-
tor (+t+j), such that we now considering
both target and jump events, p(¯eI1, ¯jI1|¯fI1) =
</bodyText>
<equation confidence="0.690322">
7�7I ¯i¯i−1 i−1¯i−1
iii-1 p(ei  |ji−2,ei−2)pUi  |ji−2 , ei−2).Next we
</equation>
<bodyText confidence="0.99997792">
added the source factor (+t+j+s) such that now all
three factors are included from Equation 1. For
the above three Markov models we used simple
least-recent backoff (akin to a standard language
model), and consequently these methods cannot
represent gaps in the target. Finally, we trained an-
other Markov model by introducing parallel back-
off to the third one as described in §2.1. Each
of the four Markov model approaches are imple-
mented as adding an additional feature, respec-
tively, into the Moses-l baseline.
The results are shown in Table 3. Observe that
adding each factor results in near uniform per-
formance improvements on both language pairs.
The jump factor gives big improvements of about
1% BLEU in both language pairs. However when
using parallel backoff, the performance improves
greatly for Chinese-English but degrades slightly
on Arabic-English. The reason may be parallel
backoff is used to encode common structures to
capture the different word ordering between Chi-
nese and English while for Arabic-English there
are fewer consistent reordering patterns. This is
also consistent with the results in Table 1 and 2
where HPB gets a little bit lower BLEU scores.
</bodyText>
<subsectionHeader confidence="0.999214">
5.4 Impact of LM order
</subsectionHeader>
<bodyText confidence="0.999983428571429">
Our system resembles a language model in com-
mon use in SMT systems, in that it uses a Markov
model over target words, among other factors.
This raises the question of whether its improve-
ments are due to it functioning as a target language
model. Our experiments use order k = 3 over MP
sequences and each MP can have at most 3 words.
Therefore the model could in principle memorize
9-grams, although usually MPs are much smaller.
To test whether our improvements are from using
a higher-order language model or other reasons,
we evaluate our system and the baseline system
with a range of LMs of different order. If we can
get consistent improvements over the baseline for
</bodyText>
<page confidence="0.990353">
157
</page>
<table confidence="0.99969">
System NIST 02 (dev) NIST 03
Moses-d 35.1 31.3
Moses-l 36.0 32.8
Moses-d+M 36.4 34.8
Moses-l+M 37.9 36.0
</table>
<tableCaption confidence="0.996685">
Table 5: Comparison between our Markov model
</tableCaption>
<bodyText confidence="0.9924317">
(denoted as M) and the lexical reordering model
of Moses.
both small and large n, this suggests it’s not the
long context that plays the key role but is other
information we have learned (e.g., jumps or rich
structures).
Table 4 shows the results of using standard lan-
guage models with orders 2 − 6 in Moses-l and
our method. We can see that language model or-
der is very important. When we increase the order
from 2 to 4, the BLEU scores for both systems in-
creases drastically, but levels off for 4-gram and
larger. Note that our system outperforms Moses-l
by 4.4, 1.6, 2.8, 3.2 and 3.0 BLEU points, respec-
tively. The large gain for 2-grams is likely due to
the model behaving like a LM, however the fact
that consistent gains are still realized for higher
k suggests that the approach brings considerable
complementary information, i.e., it is doing much
more than simply language modelling.
</bodyText>
<subsectionHeader confidence="0.99587">
5.5 Comparison with Lexical Reordering
</subsectionHeader>
<bodyText confidence="0.999986181818182">
Our Markov model learns a joint model of jump,
source and target factors and this is similar to the
lexical reordering model of Moses (Koehn et al.,
2007), which learns general orientations of pairs
of adjacent phrases (classed as monotone, swap or
other). Our method is more complex, by learning
explicit jump distances, while also using broader
context. Here we compare the two methods, and
test whether our approach is complementary by re-
alizing gains over the lexicalized reordering base-
line. We test this hypothesis by comparing the
results of Moses with its simple distortion model
(Moses-d), then with both simple distortion and
lexicalized reordering (Moses-l), and then with our
Markov model (denoted as Moses-d+M or Moses-
l+M, for both baselines respectively).
The results are shown in Table 5. Comparing
the results of Moses-l and Moses-d, we can see that
the lexical reordering model outperforms the dis-
tortion model by a margin of 1.5% BLEU. Com-
paring Moses-d+M with Moses-l, our Markov
model provides further improvements of 2.0%
</bodyText>
<table confidence="0.99927425">
System NIST 02 (dev) NIST 03
Moses-l 36.0 32.8
Moses-l+word 36.9 34.0
Moses-l+MP 37.6 34.8
</table>
<tableCaption confidence="0.909605666666667">
Table 6: Comparison between the MP-based
Markov model and the word-based Markov model.
BLEU. Our approach does much more than model
reordering, so it is unlikely that this improvement
is solely due to being better a model of distor-
tion. This is underscored by the final result in
Table 5, for combining lexicalized distortion with
our model (Moses-l+M) which gives the highest
BLEU score, yielding another 1.2% increase.
</tableCaption>
<subsectionHeader confidence="0.989641">
5.6 Comparison with Word-based Markov
</subsectionHeader>
<bodyText confidence="0.9999966">
Our approach uses minimal phrases as its basic
unit of translation, in order to preserve the many-
to-many links found from the word alignments.
However we now seek to assess the impact of the
choice of these basic units, considering instead a
simpler word-based setting which retains only 1-
to-1 links in a Markov model. To do this, we
processed target words left-to-right and for tar-
get words with multiple links, we only retained
the link which had the highest lexical translation
probability. Then we trained a 3-gram word-based
Markov model which backs off by dropping the
factors of the least recent word pairs in the order of
first jump then source then target. This model was
included as a feature in the Moses-l baseline (de-
noted as Moses-l+word), which we compared to a
system using a MP-based Markov model backing
off in the same way (denoted as Moses-l+MP).
According to the results in Table 6, using MPs
leads to better performance. Surprisingly even
the word based method outperforms the baseline.
This points to inadequate phrase-pair features in
the baseline, which can be more robustly esti-
mated using a Markov decomposition. In addition
to allowing for advanced smoothing, the Markov
model can be considered to tile phrases over one
another (each k-gram overlaps k −1 others) rather
than enforcing a single segmentation as is done in
the PB and HPB approaches. Fox (2002) states
that phrases tend to move as a whole during re-
ordering, i.e., breaking MPs into words opens the
possibility of making more reordering errors. We
could easily use larger phrase pairs as the basic
unit, such as the phrases used during decoding.
However, doing this involves a hard segmentation
</bodyText>
<page confidence="0.994189">
158
</page>
<bodyText confidence="0.993102">
and would exacerbate issues of data sparsity. n-gram smt toolkit. Prague Bull. Math. Linguistics,
96:49–58.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999872">
In this paper we try to give a solution to the prob-
lems in phrase-based models, including weak gen-
eralization to unseen data and negligence of cor-
relations between phrases. Our solution is to de-
fine a Markov model over minimal phrases so as
to model translation conditioned on context and
meanwhile use a fancy smoothing technique to
learn richer structures such that can be applied to
unseen data. Our method further decomposes each
minimal phrase into three factors and operates in
the unit of factors in the backoff process to provide
a more robust modeling.
In our experiments, we prove that our defini-
tion of factored Markov model provides comple-
mentary information to lexicalized reordering and
high order language models and the use of paral-
lel backoff infers richer structures even those out
of the reach of 2-SCFG and hence brings big per-
formance improvements. Overall our approach
gives significant improvements over strong base-
lines, giving consistent improvements of between
1.1 and 3.2 BLEU points on large scale Chinese-
English and Arabic-English evaluations.
</bodyText>
<sectionHeader confidence="0.998829" genericHeader="acknowledgments">
7 Acknowledges
</sectionHeader>
<bodyText confidence="0.992618166666667">
The first author is supported by DARPA BOLT,
contract HR0011-12-C-0014. The second au-
thor is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105). Thank the anonymous reviews for
their insightful comments.
</bodyText>
<sectionHeader confidence="0.999204" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999903261538462">
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and generalized parallel backoff. In
Proc. of HLT-NAACL.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proc. of ACL, pages 310–318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33:201–228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proc. of ACL-HLT, pages 176–
181.
Josep Maria Crego, Franc¸ois Yvon, and Jos´e B.
Mari˜no. 2011. Ncode: an open source bilingual
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proc. of ACL-HLT, pages
1045–1054, June.
Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013. Model with minimal translation units, but de-
code with phrases. In Proc. of NAACL, pages 1–11.
Yang Feng and Trevor Cohn. 2013. A markov
model of machine translation using non-parametric
bayesian inference. In Proc. of ACL, pages 333–
342.
Heidi Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In Proc. of EMNLP, pages 304–
311, July.
Michel Galley and Christopher D. Manning. 2010.
Accurate non-hierarchical phrase-based translation.
In Proc. of NAACL, pages 966–974.
Katrin Kirchhoff, Jeff Bilmes, and Kevin Duh. 2007.
Factored language models tutorial.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In
Proceedings of the IEEE International Conference
on Acoustics, Speech and Signal Processing, vol-
ume 1, pages 181–184.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proc. of HLT-
NAACL, pages 127–133.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Christopher Callison-Burch, Marcello Federico,
Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-
tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL, Demonstration Ses-
sion.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous space translation models with
neural networks. In Proc. of NAACL, pages 39–48.
Frans J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19–51.
Frans J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. of ACL, pages
160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
ofACL, pages 311–318.
Andreas Stolcke. 2002. Srilm-an extensible language
modeling toolkit. In Proc. of ICSLP.
</reference>
<page confidence="0.998844">
159
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.770761">
<title confidence="0.999935">Factored Markov Translation with Robust Modeling</title>
<author confidence="0.998688">Trevor Xinkai Du</author>
<affiliation confidence="0.950520666666667">Sciences Institue and Information Systems Computer Science Department The University of Melbourne University of Southern California VIC 3010 Australia</affiliation>
<email confidence="0.997071">t.cohn@unimelb.edu.au</email>
<abstract confidence="0.994227">Phrase-based translation models usually memorize local translation literally and make independent assumption between phrases which makes it neither generalize well on unseen data nor model sentencelevel effects between phrases. In this paper we present a new method to model correlations between phrases as a Markov model and meanwhile employ a robust smoothing strategy to provide better generalization. This method defines a recursive estimation process and backs off in parallel paths to infer richer structures. Our evaluation shows an 1.1–3.2% BLEU improvement over competitive baselines for Chinese-English and Arabic-English translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jeff Bilmes</author>
<author>Katrin Kirchhoff</author>
</authors>
<title>Factored language models and generalized parallel backoff.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL.</booktitle>
<contexts>
<context position="12354" citStr="Bilmes and Kirchhoff, 2003" startWordPosition="2147" endWordPosition="2150">h AB-D-F-H-J-L-M-N in Figure 4, shown as heavier lines. When generizing to the k-gram for target p(¯ek |¯fk1 , jk1 , ¯ek−1 1 ), the backoff strategy is to first drop the previous k-1 MPs one by one (for each MP, still drops in the order of jump, source and target), then the kth jump factor and finally the kth source factor. According to the strategy, the top node has k-1 nodes to back off to and for the node ¯ek |¯fk2 , jk 2 , ¯ek−1 2 where only the factors of MP1 are dropped, there are k-2 nodes to back off to. 2.2 Probability Estimation We adopt the technique used in factor language models (Bilmes and Kirchhoff, 2003; Kirchhoff et al., 2007) to estimate the probability of a k-gram p(¯ei|c) where c = ¯fii−k+1, jii−k+1, ¯e−1 i−k+1. According to the definition of backoff, only when the count of the k-gram exceeds some given threshold, its maximum-likelihood estimate, pML(¯ek|c) = N(¯ek,c) is used, where N(·) is the count of an N(c) event and/or context. Otherwise, only a portion of pML(¯ek|c) is used and the remainder is constructed from a lower-level (by dropping a factor). In order to ensure valid probability estimates, i.e. sums to unity, probability mass needs to be “stolen” from the higher level and giv</context>
</contexts>
<marker>Bilmes, Kirchhoff, 2003</marker>
<rawString>Jeff Bilmes and Katrin Kirchhoff. 2003. Factored language models and generalized parallel backoff. In Proc. of HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="13346" citStr="Chen and Goodman, 1996" startWordPosition="2309" endWordPosition="2312"> pML(¯ek|c) is used and the remainder is constructed from a lower-level (by dropping a factor). In order to ensure valid probability estimates, i.e. sums to unity, probability mass needs to be “stolen” from the higher level and given to the lower level. Hence, the whole definition is p(¯ei |c) = dN(¯ei,c)pml (¯ei |c) if N(¯ei, c) &gt; τk α(c)g(¯ei, c) otherwise (3) where dN(¯ei,c) is a discount parameter which reserves probability from the maximum-likelihood estimate for backoff smoothing at the next lowerlevel, and we estimate dN(¯ei,c) using modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996); τk is the threshold for the count of the k-gram, α(c) is the backoff weight used to make sure the entire distribution still sums to unity, 1 − E¯e:N(¯e,c)&gt;τk dN(¯e,c)pML(¯e|c) E ¯e:N(¯e,c)≤τk g(¯e, c) , and g(¯ei, c) is the backoff probability which we estimate by averaging over the nodes in the next lower level, 1 � g(¯ei, c) = φ c� p(¯ei|c0), where φ is the number of nodes to back off, c0 is the lower-level context after dropping one factor from c. The k-gram for the source and jump factors are estimated in the same way, using the same backoff semantics.4 Note (3) is applied independently </context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. of ACL, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<pages>33--201</pages>
<contexts>
<context position="14257" citStr="Chiang, 2007" startWordPosition="2473" endWordPosition="2474"> next lower level, 1 � g(¯ei, c) = φ c� p(¯ei|c0), where φ is the number of nodes to back off, c0 is the lower-level context after dropping one factor from c. The k-gram for the source and jump factors are estimated in the same way, using the same backoff semantics.4 Note (3) is applied independently to each of the three models, so the use of backoff may differ in each case. 3 Discussion As a part of the backoff process our method can introduce gaps in estimating rule probabilities; these backoff patterns often bear close resemblance to SCFG productions in the hierarchical phrase-based model (Chiang, 2007). For example, in step 0 in Figure 3, as all the jump factors are present, this encodes the full ordering of the MPs and gives rise to the aligned MP pairs shown in Figure 5 (a). Note that an X 1 placeholder is included to ensure the jump distance from the previous MP to the MP &lt;bˇa, take&gt; is -2. The approximate SCFG production for the MP pairs is &lt;bˇa t¯a X 1 kˇaol`v jinq`u, X 1 take it into account&gt;. 4Although there are fewer final steps, L-M-N in Fig. 4, as we assume the MP is generated in the order jump, source phrase then target phrase in a chain rule decomposition. α(c) = 154 t¯a · kˇaol</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33:201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Chris Dyer</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Better hypothesis testing for statistical machine translation: Controlling for optimizer instability.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT,</booktitle>
<pages>176--181</pages>
<contexts>
<context position="21498" citStr="Clark et al. (2011)" startWordPosition="3750" endWordPosition="3753">g to Durrani et al. (2013) and used the same configuration with Moses-l. For our method we used the same configuration as Moses-l but adding an additional feature of the Markov model over MPs. 5.2 Performance Comparison We first give the results of performance comparison. Here we add another system (denoted as Moses-l+trgLM): Moses-l together with the target language model trained on the training data set, using the same configuration with Moses-l. This system is used to test whether our model gains improvement just for using additional information on the training set. We use the open tool of Clark et al. (2011) to control for optimizer stability and test statistical significance. The results are shown in Tables 1 and 2. The two language pairs we used are quite different: Chinese has a much bigger word order difference c.f. English than does Arabic. The results show that our system can outperform the baseline 6We only employed MPs with the length ≤ 3. If a MP had more than 3 words on either side, we omitted the alignment links to the first target word of this MP and extracted MPs according to the new alignment. 7The maximum number of lexical rules for each source span. 8The maximum span on the source</context>
</contexts>
<marker>Clark, Dyer, Lavie, Smith, 2011</marker>
<rawString>Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A. Smith. 2011. Better hypothesis testing for statistical machine translation: Controlling for optimizer instability. In Proc. of ACL-HLT, pages 176– 181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep Maria Crego</author>
<author>Franc¸ois Yvon</author>
<author>Jos´e B Mari˜no</author>
</authors>
<date>2011</date>
<note>Ncode: an open source bilingual</note>
<marker>Crego, Yvon, Mari˜no, 2011</marker>
<rawString>Josep Maria Crego, Franc¸ois Yvon, and Jos´e B. Mari˜no. 2011. Ncode: an open source bilingual</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A joint sequence translation model with integrated reordering.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT,</booktitle>
<pages>1045--1054</pages>
<contexts>
<context position="2093" citStr="Durrani et al. (2011)" startWordPosition="299" endWordPosition="302">h other which conflicts with the underlying truth that the translation decisions of phrases should be dependent on context. There are some work aiming to solve the two problems. Feng and Cohn (2013) propose a word-based Markov model to integrate translation and reordering into one model and use the sophisticated hierarchical Pitman-Yor process which backs off from larger to smaller context to provide dynamic adaptive smoothing. This model shows good generalization to unseen data while it uses words as the translation unit which cannot handle multiple-to-multiple links in real word alignments. Durrani et al. (2011) and Durrani et al. (2013) propose an operation sequence model (OSM) which models correlations between minimal translation units (MTUs) and evaluates probabilities with modified Kneser-Ney smoothing. On one hand the use of MTUs can help retain the multiple-to-multiple alignments, on the other hand its definition of operations where source words and target words are bundled into one operation makes it subjected to sparsity. The common feature of the above two methods is they both back off in one fixed path by dropping least recent events first which precludes some useful structures. For the seg</context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrated reordering. In Proc. of ACL-HLT, pages 1045–1054, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
</authors>
<title>Model with minimal translation units, but decode with phrases.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="2119" citStr="Durrani et al. (2013)" startWordPosition="304" endWordPosition="307">th the underlying truth that the translation decisions of phrases should be dependent on context. There are some work aiming to solve the two problems. Feng and Cohn (2013) propose a word-based Markov model to integrate translation and reordering into one model and use the sophisticated hierarchical Pitman-Yor process which backs off from larger to smaller context to provide dynamic adaptive smoothing. This model shows good generalization to unseen data while it uses words as the translation unit which cannot handle multiple-to-multiple links in real word alignments. Durrani et al. (2011) and Durrani et al. (2013) propose an operation sequence model (OSM) which models correlations between minimal translation units (MTUs) and evaluates probabilities with modified Kneser-Ney smoothing. On one hand the use of MTUs can help retain the multiple-to-multiple alignments, on the other hand its definition of operations where source words and target words are bundled into one operation makes it subjected to sparsity. The common feature of the above two methods is they both back off in one fixed path by dropping least recent events first which precludes some useful structures. For the segment pairs &lt;bˇa t¯a kˇaol`</context>
<context position="16589" citStr="Durrani et al. (2013)" startWordPosition="2942" endWordPosition="2945">n cover zero or more MPs. In step 1, as the jump factor 1 is dropped, we do not know the orientation between bˇa and t¯a. However several jump distances are known: from X 1 to bˇa is distance -2 and t¯a to kˇaol`v jinq`u is 2. In this case, the source side can be bˇa t¯a X 1 kˇaol`v jinq`u, The method introduced in this paper uses factors defined in the same manner as in Feng and Cohn (2013), but the two methods are quite different. That method (Feng and Cohn, 2013) is wordbased and under the frame of Bayesian model while this method is MP-based and uses a simpler Kneser-Ney smoothing method. Durrani et al. (2013) also present a Markov model based on MPs (they call minimal translation units) and further define operation sequence over MPs which are taken as the events in the Markov model. For the probability estimation, they use Kneser-Ney smoothing with a single backoff path. Different from operation sequence, our method gives a neat definition of factors which uses jump distance directly and avoids the bundle of source words and target words like in their method, and hence mitigates sparsity. Moreover, the use of parallel backoff infers richer structures and provides robust modeling. There are several</context>
<context position="20905" citStr="Durrani et al. (2013)" startWordPosition="3651" endWordPosition="3654">e training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. We used Moses for PB and Moses-chart for HPB with the configuration as follows. For both, max-phrase-length=7, ttable-limit7=20, stacksize=50 and max-pop-limit=500; For Moses, search-algorithm=1 and distortion-limit=6; For Moses-chart, search-algorithm=3 and max-charspan8=20 for Moses-chart. We used both the distortion model and the lexical reordering model for Moses (denoted as Moses-l) except in §5.5 we only used the distortion model (denoted as Moses-d). We implemented the OSM according to Durrani et al. (2013) and used the same configuration with Moses-l. For our method we used the same configuration as Moses-l but adding an additional feature of the Markov model over MPs. 5.2 Performance Comparison We first give the results of performance comparison. Here we add another system (denoted as Moses-l+trgLM): Moses-l together with the target language model trained on the training data set, using the same configuration with Moses-l. This system is used to test whether our model gains improvement just for using additional information on the training set. We use the open tool of Clark et al. (2011) to con</context>
</contexts>
<marker>Durrani, Fraser, Schmid, 2013</marker>
<rawString>Nadir Durrani, Alexander Fraser, and Helmut Schmid. 2013. Model with minimal translation units, but decode with phrases. In Proc. of NAACL, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Feng</author>
<author>Trevor Cohn</author>
</authors>
<title>A markov model of machine translation using non-parametric bayesian inference.</title>
<date>2013</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>333--342</pages>
<contexts>
<context position="1670" citStr="Feng and Cohn (2013)" startWordPosition="234" endWordPosition="237">oehn et al., 2007) have drastically improved beyond word-based approaches, primarily by using phrase-pairs as translation units, which can memorize local lexical context and reordering patterns. However, this literal memorization mechanism makes it generalize poorly to unseen data. Moreover, phrase-based models make an independent assumption, stating that the application of phrases in a derivation is independent to each other which conflicts with the underlying truth that the translation decisions of phrases should be dependent on context. There are some work aiming to solve the two problems. Feng and Cohn (2013) propose a word-based Markov model to integrate translation and reordering into one model and use the sophisticated hierarchical Pitman-Yor process which backs off from larger to smaller context to provide dynamic adaptive smoothing. This model shows good generalization to unseen data while it uses words as the translation unit which cannot handle multiple-to-multiple links in real word alignments. Durrani et al. (2011) and Durrani et al. (2013) propose an operation sequence model (OSM) which models correlations between minimal translation units (MTUs) and evaluates probabilities with modified</context>
<context position="16362" citStr="Feng and Cohn (2013)" startWordPosition="2901" endWordPosition="2904">roximate SCFG patterns for step 0, 3 of Figure 3. X is a non-terminal which can only be rewritten by one MP. · and · · · denote gaps introduced by the left-to-right decoding algorithm and · can only cover one MP while · · · can cover zero or more MPs. In step 1, as the jump factor 1 is dropped, we do not know the orientation between bˇa and t¯a. However several jump distances are known: from X 1 to bˇa is distance -2 and t¯a to kˇaol`v jinq`u is 2. In this case, the source side can be bˇa t¯a X 1 kˇaol`v jinq`u, The method introduced in this paper uses factors defined in the same manner as in Feng and Cohn (2013), but the two methods are quite different. That method (Feng and Cohn, 2013) is wordbased and under the frame of Bayesian model while this method is MP-based and uses a simpler Kneser-Ney smoothing method. Durrani et al. (2013) also present a Markov model based on MPs (they call minimal translation units) and further define operation sequence over MPs which are taken as the events in the Markov model. For the probability estimation, they use Kneser-Ney smoothing with a single backoff path. Different from operation sequence, our method gives a neat definition of factors which uses jump distance</context>
</contexts>
<marker>Feng, Cohn, 2013</marker>
<rawString>Yang Feng and Trevor Cohn. 2013. A markov model of machine translation using non-parametric bayesian inference. In Proc. of ACL, pages 333– 342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="30294" citStr="Fox (2002)" startWordPosition="5239" endWordPosition="5240"> system using a MP-based Markov model backing off in the same way (denoted as Moses-l+MP). According to the results in Table 6, using MPs leads to better performance. Surprisingly even the word based method outperforms the baseline. This points to inadequate phrase-pair features in the baseline, which can be more robustly estimated using a Markov decomposition. In addition to allowing for advanced smoothing, the Markov model can be considered to tile phrases over one another (each k-gram overlaps k −1 others) rather than enforcing a single segmentation as is done in the PB and HPB approaches. Fox (2002) states that phrases tend to move as a whole during reordering, i.e., breaking MPs into words opens the possibility of making more reordering errors. We could easily use larger phrase pairs as the basic unit, such as the phrases used during decoding. However, doing this involves a hard segmentation 158 and would exacerbate issues of data sparsity. n-gram smt toolkit. Prague Bull. Math. Linguistics, 96:49–58. 6 Conclusions In this paper we try to give a solution to the problems in phrase-based models, including weak generalization to unseen data and negligence of correlations between phrases. O</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi Fox. 2002. Phrasal cohesion and statistical machine translation. In Proc. of EMNLP, pages 304– 311, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate non-hierarchical phrase-based translation.</title>
<date>2010</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>966--974</pages>
<contexts>
<context position="17761" citStr="Galley and Manning (2010)" startWordPosition="3132" endWordPosition="3135">tures and provides robust modeling. There are several other work focusing on modeling bilingual information into a Markov model. Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, and use it as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Neither work includes the jump distance, and nor 155 do they consider dynamic strategies for estimating k-gram probabilities. Galley and Manning (2010) propose a method to introduce discontinuous phrases into the phrasebased model. It makes use of the decoding mechanism of the phrase-based model which jumps over the source words and hence can hold discontinuous phrases naturally. However, their method doesn’t touch the correlations between phrases and probability modeling which are the key points we focus on. 5 Experiments We design experiments to first compare our method with the phrase-based model (PB), the operation sequence model (OSM) and the hierarchical phrase-based model (HPB), then we present several experiments to test: 1. how each</context>
</contexts>
<marker>Galley, Manning, 2010</marker>
<rawString>Michel Galley and Christopher D. Manning. 2010. Accurate non-hierarchical phrase-based translation. In Proc. of NAACL, pages 966–974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Kirchhoff</author>
<author>Jeff Bilmes</author>
<author>Kevin Duh</author>
</authors>
<title>Factored language models tutorial.</title>
<date>2007</date>
<contexts>
<context position="12379" citStr="Kirchhoff et al., 2007" startWordPosition="2151" endWordPosition="2154"> 4, shown as heavier lines. When generizing to the k-gram for target p(¯ek |¯fk1 , jk1 , ¯ek−1 1 ), the backoff strategy is to first drop the previous k-1 MPs one by one (for each MP, still drops in the order of jump, source and target), then the kth jump factor and finally the kth source factor. According to the strategy, the top node has k-1 nodes to back off to and for the node ¯ek |¯fk2 , jk 2 , ¯ek−1 2 where only the factors of MP1 are dropped, there are k-2 nodes to back off to. 2.2 Probability Estimation We adopt the technique used in factor language models (Bilmes and Kirchhoff, 2003; Kirchhoff et al., 2007) to estimate the probability of a k-gram p(¯ei|c) where c = ¯fii−k+1, jii−k+1, ¯e−1 i−k+1. According to the definition of backoff, only when the count of the k-gram exceeds some given threshold, its maximum-likelihood estimate, pML(¯ek|c) = N(¯ek,c) is used, where N(·) is the count of an N(c) event and/or context. Otherwise, only a portion of pML(¯ek|c) is used and the remainder is constructed from a lower-level (by dropping a factor). In order to ensure valid probability estimates, i.e. sums to unity, probability mass needs to be “stolen” from the higher level and given to the lower level. He</context>
<context position="19619" citStr="Kirchhoff et al., 2007" startWordPosition="3447" endWordPosition="3450">ansards portions of the NIST training corpora, distributed by the LDC, having 1,658k sentence pairs with 40m and 44m Chinese and English words. We used the NIST 02 test set as the development set and evaluated performance on the test sets from NIST 03 and 05. For the Arabic-English task, the training data comprises several LDC corpora,5 including 276k sentence pairs and 8.21m and 8.97m words in Arabic and English, respectively. We evaluated on the NIST test sets from 2003 and 2005, and the NIST 02 test set was used for parameter tuning. On both cases, we used the factor language model module (Kirchhoff et al., 2007) of the SRILM toolkit (Stolcke, 2002) to train a Markov 5LDC2004E72, LDC2004T17, LDC2004T18, LDC2006T02 model with the order = 3 over the MP sequences.6 The threshold count of backoff for all nodes was τ = 2. We aligned the training data sets by first using GIZA++ toolkit (Och and Ney, 2003) to produce word alignments on both directions and then combining them with the diag-final-and heuristic. All experiments used a 5-gram language model which was trained on the Xinhua portion of the GIGAWORD corpus using the SRILM toolkit. Translation performance was evaluated using BLEU (Papineni et al., 20</context>
</contexts>
<marker>Kirchhoff, Bilmes, Duh, 2007</marker>
<rawString>Katrin Kirchhoff, Jeff Bilmes, and Kevin Duh. 2007. Factored language models tutorial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling. In</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<contexts>
<context position="13321" citStr="Kneser and Ney, 1995" startWordPosition="2305" endWordPosition="2308">ise, only a portion of pML(¯ek|c) is used and the remainder is constructed from a lower-level (by dropping a factor). In order to ensure valid probability estimates, i.e. sums to unity, probability mass needs to be “stolen” from the higher level and given to the lower level. Hence, the whole definition is p(¯ei |c) = dN(¯ei,c)pml (¯ei |c) if N(¯ei, c) &gt; τk α(c)g(¯ei, c) otherwise (3) where dN(¯ei,c) is a discount parameter which reserves probability from the maximum-likelihood estimate for backoff smoothing at the next lowerlevel, and we estimate dN(¯ei,c) using modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996); τk is the threshold for the count of the k-gram, α(c) is the backoff weight used to make sure the entire distribution still sums to unity, 1 − E¯e:N(¯e,c)&gt;τk dN(¯e,c)pML(¯e|c) E ¯e:N(¯e,c)≤τk g(¯e, c) , and g(¯ei, c) is the backoff probability which we estimate by averaging over the nodes in the next lower level, 1 � g(¯ei, c) = φ c� p(¯ei|c0), where φ is the number of nodes to back off, c0 is the lower-level context after dropping one factor from c. The k-gram for the source and jump factors are estimated in the same way, using the same backoff semantics.4 Note (3) </context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, volume 1, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLTNAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1047" citStr="Koehn et al., 2003" startWordPosition="139" endWordPosition="142">ion between phrases which makes it neither generalize well on unseen data nor model sentencelevel effects between phrases. In this paper we present a new method to model correlations between phrases as a Markov model and meanwhile employ a robust smoothing strategy to provide better generalization. This method defines a recursive estimation process and backs off in parallel paths to infer richer structures. Our evaluation shows an 1.1–3.2% BLEU improvement over competitive baselines for Chinese-English and Arabic-English translation. 1 Introduction Phrase-based methods to machine translation (Koehn et al., 2003; Koehn et al., 2007) have drastically improved beyond word-based approaches, primarily by using phrase-pairs as translation units, which can memorize local lexical context and reordering patterns. However, this literal memorization mechanism makes it generalize poorly to unseen data. Moreover, phrase-based models make an independent assumption, stating that the application of phrases in a derivation is independent to each other which conflicts with the underlying truth that the translation decisions of phrases should be dependent on context. There are some work aiming to solve the two problem</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLTNAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch Mayne</author>
<author>Christopher Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL, Demonstration Session.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="1068" citStr="Koehn et al., 2007" startWordPosition="143" endWordPosition="146">which makes it neither generalize well on unseen data nor model sentencelevel effects between phrases. In this paper we present a new method to model correlations between phrases as a Markov model and meanwhile employ a robust smoothing strategy to provide better generalization. This method defines a recursive estimation process and backs off in parallel paths to infer richer structures. Our evaluation shows an 1.1–3.2% BLEU improvement over competitive baselines for Chinese-English and Arabic-English translation. 1 Introduction Phrase-based methods to machine translation (Koehn et al., 2003; Koehn et al., 2007) have drastically improved beyond word-based approaches, primarily by using phrase-pairs as translation units, which can memorize local lexical context and reordering patterns. However, this literal memorization mechanism makes it generalize poorly to unseen data. Moreover, phrase-based models make an independent assumption, stating that the application of phrases in a derivation is independent to each other which conflicts with the underlying truth that the translation decisions of phrases should be dependent on context. There are some work aiming to solve the two problems. Feng and Cohn (201</context>
<context position="6993" citStr="Koehn et al., 2007" startWordPosition="1137" endWordPosition="1140">nd jI1 = (j1, j2, ... , jI) is the vector of jump distance between MPi−1 and MPi, or insert for MPs with null source sides.2 To evaluate each of the k-gram models, we use modified Keneser-Ney smoothing to back off from larger context to smaller context recursively. In summary, adding the Markov model into the decoder involves two passes: 1) training a model over the MP sequences extracted from a word aligned parallel corpus; and 2) calculating the probability of the Markov model for each translation hypothesis during decoding. This Markov model is combined with a standard phrase-based model3 (Koehn et al., 2007) and used as an additional feature in the linear model. In what follows, we will describe how to estatimate the k-gram Markov model, focusing on backoff (§2.1) and smoothing (§2.2). 2.1 Parallel Backoff Backoff is a technique used in language model — when estimating a higher-order gram, instead of using the raw occurrence count, only a portion is used and the remainder is computed using a lowerorder model in which one of the context factors 2Note that factors at indices 0, −1, ... , −(k − 1) are set to a sentinel value to denote the start of sentence. 3The phrase-based model considers larger p</context>
<context position="27467" citStr="Koehn et al., 2007" startWordPosition="4776" endWordPosition="4779">astically, but levels off for 4-gram and larger. Note that our system outperforms Moses-l by 4.4, 1.6, 2.8, 3.2 and 3.0 BLEU points, respectively. The large gain for 2-grams is likely due to the model behaving like a LM, however the fact that consistent gains are still realized for higher k suggests that the approach brings considerable complementary information, i.e., it is doing much more than simply language modelling. 5.5 Comparison with Lexical Reordering Our Markov model learns a joint model of jump, source and target factors and this is similar to the lexical reordering model of Moses (Koehn et al., 2007), which learns general orientations of pairs of adjacent phrases (classed as monotone, swap or other). Our method is more complex, by learning explicit jump distances, while also using broader context. Here we compare the two methods, and test whether our approach is complementary by realizing gains over the lexicalized reordering baseline. We test this hypothesis by comparing the results of Moses with its simple distortion model (Moses-d), then with both simple distortion and lexicalized reordering (Moses-l), and then with our Markov model (denoted as Moses-d+M or Mosesl+M, for both baselines</context>
</contexts>
<marker>Koehn, Hoang, Mayne, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne, Christopher Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL,</booktitle>
<pages>39--48</pages>
<contexts>
<context position="17505" citStr="Le et al. (2012)" startWordPosition="3096" endWordPosition="3099">e, our method gives a neat definition of factors which uses jump distance directly and avoids the bundle of source words and target words like in their method, and hence mitigates sparsity. Moreover, the use of parallel backoff infers richer structures and provides robust modeling. There are several other work focusing on modeling bilingual information into a Markov model. Crego et al. (2011) develop a bilingual language model which incorporates words in the source and target languages to predict the next unit, and use it as a feature in a translation system. This line of work was extended by Le et al. (2012) who develop a novel estimation algorithm based around discriminative projection into continuous spaces. Neither work includes the jump distance, and nor 155 do they consider dynamic strategies for estimating k-gram probabilities. Galley and Manning (2010) propose a method to introduce discontinuous phrases into the phrasebased model. It makes use of the decoding mechanism of the phrase-based model which jumps over the source words and hence can hold discontinuous phrases naturally. However, their method doesn’t touch the correlations between phrases and probability modeling which are the key </context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous space translation models with neural networks. In Proc. of NAACL, pages 39–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<pages>29--19</pages>
<contexts>
<context position="19911" citStr="Och and Ney, 2003" startWordPosition="3498" endWordPosition="3501">ning data comprises several LDC corpora,5 including 276k sentence pairs and 8.21m and 8.97m words in Arabic and English, respectively. We evaluated on the NIST test sets from 2003 and 2005, and the NIST 02 test set was used for parameter tuning. On both cases, we used the factor language model module (Kirchhoff et al., 2007) of the SRILM toolkit (Stolcke, 2002) to train a Markov 5LDC2004E72, LDC2004T17, LDC2004T18, LDC2006T02 model with the order = 3 over the MP sequences.6 The threshold count of backoff for all nodes was τ = 2. We aligned the training data sets by first using GIZA++ toolkit (Och and Ney, 2003) to produce word alignments on both directions and then combining them with the diag-final-and heuristic. All experiments used a 5-gram language model which was trained on the Xinhua portion of the GIGAWORD corpus using the SRILM toolkit. Translation performance was evaluated using BLEU (Papineni et al., 2002) with case-insensitive n ≤ 4- grams. We used minimum error rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. We used Moses for PB and Moses-chart for HPB with the configuration as follows. For both, max-phrase-length=7, ttable-limit7=</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Frans J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frans J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="4798" citStr="Och, 2003" startWordPosition="735" endWordPosition="736">ntence pair with word alignments shown as filled grid squares. provement up to 3.2 BLEU points absolute.1 2 Modelling Our model is phrase-based and works like a phrase-based decoder by generating target translation left to right using phrase-pairs while jumping around the source sentence. For each derivation, we can easily get its minimal phrase (MPs) sequence where MPs are ordered according to the order of their target side. Then this sequence of events is modeled as a Markov model and the log probability under this Markov model is included as an additional feature into the linear SMT model (Och, 2003). A MP denotes a phrase which cannot contain other phrases. For example, in the sentence pair in Figure 1, &lt;bˇa t¯a , take it&gt; is a phrase but not a minimal phrase, as it contains smaller phrases of &lt;bˇa , take&gt; and &lt;t¯a , it&gt;. MPs are a complex event representation for sequence modelling, and using these naively would be a poor choice because few bigrams and trigrams will be seen often enough for reliable estimation. In order to reason more effectively from sparse data, we consider more generalized representations by decomposing MPs into their component events: the source phrase (source ¯f), </context>
<context position="20306" citStr="Och, 2003" startWordPosition="3565" endWordPosition="3566">17, LDC2004T18, LDC2006T02 model with the order = 3 over the MP sequences.6 The threshold count of backoff for all nodes was τ = 2. We aligned the training data sets by first using GIZA++ toolkit (Och and Ney, 2003) to produce word alignments on both directions and then combining them with the diag-final-and heuristic. All experiments used a 5-gram language model which was trained on the Xinhua portion of the GIGAWORD corpus using the SRILM toolkit. Translation performance was evaluated using BLEU (Papineni et al., 2002) with case-insensitive n ≤ 4- grams. We used minimum error rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. We used Moses for PB and Moses-chart for HPB with the configuration as follows. For both, max-phrase-length=7, ttable-limit7=20, stacksize=50 and max-pop-limit=500; For Moses, search-algorithm=1 and distortion-limit=6; For Moses-chart, search-algorithm=3 and max-charspan8=20 for Moses-chart. We used both the distortion model and the lexical reordering model for Moses (denoted as Moses-l) except in §5.5 we only used the distortion model (denoted as Moses-d). We implemented the OSM according to Durrani et al. (2013) </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Frans J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="20222" citStr="Papineni et al., 2002" startWordPosition="3548" endWordPosition="3552">hhoff et al., 2007) of the SRILM toolkit (Stolcke, 2002) to train a Markov 5LDC2004E72, LDC2004T17, LDC2004T18, LDC2006T02 model with the order = 3 over the MP sequences.6 The threshold count of backoff for all nodes was τ = 2. We aligned the training data sets by first using GIZA++ toolkit (Och and Ney, 2003) to produce word alignments on both directions and then combining them with the diag-final-and heuristic. All experiments used a 5-gram language model which was trained on the Xinhua portion of the GIGAWORD corpus using the SRILM toolkit. Translation performance was evaluated using BLEU (Papineni et al., 2002) with case-insensitive n ≤ 4- grams. We used minimum error rate training (Och, 2003) to tune the feature weights to maximize the BLEU score on the development set. We used Moses for PB and Moses-chart for HPB with the configuration as follows. For both, max-phrase-length=7, ttable-limit7=20, stacksize=50 and max-pop-limit=500; For Moses, search-algorithm=1 and distortion-limit=6; For Moses-chart, search-algorithm=3 and max-charspan8=20 for Moses-chart. We used both the distortion model and the lexical reordering model for Moses (denoted as Moses-l) except in §5.5 we only used the distortion mo</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings ofACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of ICSLP.</booktitle>
<contexts>
<context position="19656" citStr="Stolcke, 2002" startWordPosition="3455" endWordPosition="3456"> distributed by the LDC, having 1,658k sentence pairs with 40m and 44m Chinese and English words. We used the NIST 02 test set as the development set and evaluated performance on the test sets from NIST 03 and 05. For the Arabic-English task, the training data comprises several LDC corpora,5 including 276k sentence pairs and 8.21m and 8.97m words in Arabic and English, respectively. We evaluated on the NIST test sets from 2003 and 2005, and the NIST 02 test set was used for parameter tuning. On both cases, we used the factor language model module (Kirchhoff et al., 2007) of the SRILM toolkit (Stolcke, 2002) to train a Markov 5LDC2004E72, LDC2004T17, LDC2004T18, LDC2006T02 model with the order = 3 over the MP sequences.6 The threshold count of backoff for all nodes was τ = 2. We aligned the training data sets by first using GIZA++ toolkit (Och and Ney, 2003) to produce word alignments on both directions and then combining them with the diag-final-and heuristic. All experiments used a 5-gram language model which was trained on the Xinhua portion of the GIGAWORD corpus using the SRILM toolkit. Translation performance was evaluated using BLEU (Papineni et al., 2002) with case-insensitive n ≤ 4- gram</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proc. of ICSLP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
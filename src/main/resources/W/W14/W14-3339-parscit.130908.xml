<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000169">
<title confidence="0.991033">
Referential Translation Machines for Predicting Translation Quality
</title>
<author confidence="0.984346">
Andy Way
</author>
<affiliation confidence="0.991213333333333">
Centre for Next Generation Localisation
School of Computing
Dublin City University, Dublin, Ireland.
</affiliation>
<email confidence="0.952622">
away@computing.dcu.ie
</email>
<author confidence="0.986627">
Ergun Bic¸ici
</author>
<affiliation confidence="0.991695333333333">
Centre for Next Generation Localisation
School of Computing
Dublin City University, Dublin, Ireland.
</affiliation>
<email confidence="0.996424">
ergun.bicici@computing.dcu.ie
</email>
<sectionHeader confidence="0.993836" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99997662962963">
We use referential translation machines
(RTM) for quality estimation of translation
outputs. RTMs are a computational model
for identifying the translation acts between
any two data sets with respect to interpre-
tants selected in the same domain, which
are effective when making monolingual
and bilingual similarity judgments. RTMs
achieve top performance in automatic, ac-
curate, and language independent predic-
tion of sentence-level and word-level sta-
tistical machine translation (SMT) qual-
ity. RTMs remove the need to access any
SMT system specific information or prior
knowledge of the training data or models
used when generating the translations and
achieve the top performance in WMT13
quality estimation task (QET13). We im-
prove our RTM models with the Parallel
FDA5 instance selection model, with ad-
ditional features for predicting the trans-
lation performance, and with improved
learning models. We develop RTM mod-
els for each WMT14 QET (QET14) sub-
task, obtain improvements over QET13 re-
sults, and rank 1st in all of the tasks and
subtasks of QET14.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990036068965517">
We use referential translation machines (RTM) for
quality estimation of translation outputs, which is
a computational model for identifying the acts of
translation for translating between any given two
data sets with respect to a reference corpus se-
lected in the same domain. RTMs reduce our de-
pendence on any task dependent resource. Predic-
tion of translation quality is important because the
expected translation performance can help in esti-
mating the effort required for correcting the trans-
lations during post-editing by human translators.
Bicici et al. (2013) develop the Machine Trans-
lation Performance Predictor (MTPP), a state-of-
the-art, language independent, and SMT system
extrinsic machine translation performance predic-
tor, which can predict translation quality by look-
ing at the test source sentences and becomes the
2nd overall after also looking at the translation
outputs as well in QET12 (Callison-Burch et al.,
2012). RTMs achieve the top performance in
QET13 (Bojar et al., 2013), ranking 1st or 2nd in
all of the subtasks. RTMs rank 1st in all of the
tasks and subtasks of QET14 (Bojar et al., 2014).
Referential translation models (Section 2)
present an accurate and language independent so-
lution for predicting the performance of natural
language tasks such as the quality estimation of
translation. We improve our RTM models (Bic¸ici,
2013) by:
</bodyText>
<listItem confidence="0.998182666666667">
• using a parameterized, fast implementation
of FDA, FDA5, and our Parallel FDA5 in-
stance selection model (Bic¸ici et al., 2014),
• better modeling of the language in which
similarity judgments are made with improved
optimization and selection of the LM data,
• increased feature set for also modeling the
structural properties of sentences,
• extended learning models.
</listItem>
<sectionHeader confidence="0.972638" genericHeader="method">
2 Referential Translation Machine
(RTM)
</sectionHeader>
<bodyText confidence="0.999494625">
Referential translation machines provide a compu-
tational model for quality and semantic similarity
judgments in monolingual and bilingual settings
using retrieval of relevant training data (Bic¸ici,
2011; Bic¸ici and Yuret, 2014) as interpretants for
reaching shared semantics (Bic¸ici, 2008). RTMs
achieve top performance when predicting the qual-
ity of translations in QET14 and QET13 (Bic¸ici,
</bodyText>
<page confidence="0.992016">
313
</page>
<note confidence="0.7143135">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 313–321,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.994765029411765">
2013), top performance when predicting mono-
lingual cross-level semantic similarity (Jurgens
et al., 2014), good performance when evaluat-
ing the semantic relatedness of sentences and
their entailment (Marelli et al., 2014), and a
language independent solution and good perfor-
mance when judging the semantic similarity of
sentences (Agirre et al., 2014; Bic¸ici and Way,
2014).
RTM is a computational model for identifying
the acts of translation for translating between any
given two data sets with respect to a reference
corpus selected in the same domain. An RTM
model is based on the selection of interpretants,
data close to both the training set and the test set,
which allow shared semantics by providing con-
text for similarity judgments. In semiotics, an in-
terpretant I interprets the signs used to refer to the
real objects (Bic¸ici, 2008). Each RTM model is
a data translation model between the instances in
the training set and the test set. We use the Parallel
FDA5 (Feature Decay Algorithms) instance selec-
tion model for selecting the interpretants (Bic¸ici
et al., 2014; Bic¸ici and Yuret, 2014) this year,
which allows efficient parameterization, optimiza-
tion, and implementation of FDA, and build an
MTPP model (Section 2.1). We view that acts of
translation are ubiquitously used during commu-
nication:
Every act of communication is an act of
translation (Bliss, 2012).
Given a training set train, a test set test, and
some corpus C, preferably in the same domain as
the training and test sets, the RTM steps are:
</bodyText>
<listItem confidence="0.9975252">
1. FDA5(train, test, C) → I
2. MTPP(I, train) → Ftrain
3. MTPP(I, test) → Ftest
4. learn(M,Ftrain) → M
5. predict(M,Ftest) → qˆ
</listItem>
<bodyText confidence="0.999854823529412">
Step 1 selects the interpretants, I, relevant to both
the training and test data. Steps 2 and 3 use I
to map train and test to a new space where
similarities between translation acts can be derived
more easily. Step 4 trains a learning model M over
the training features, Ftrain, and Step 5 obtains
the predictions. RTM relies on the representative-
ness of I as a medium for building data translation
models between train and test.
Our encouraging results in QET provides a
greater understanding of the acts of translation we
ubiquitously use and how they can be used to pre-
dict the performance of translation and judging the
semantic similarity between text. RTM and MTPP
models are not data or language specific and their
modeling power and good performance are appli-
cable in different domains and tasks.
</bodyText>
<subsectionHeader confidence="0.9919005">
2.1 The Machine Translation Performance
Predictor (MTPP)
</subsectionHeader>
<bodyText confidence="0.9994725">
MTPP (Bic¸ici et al., 2013) is a state-of-the-art and
top performing machine translation performance
predictor, which uses machine learning models
over features measuring how well the test set
matches the training set to predict the quality of
a translation without using a reference translation.
</bodyText>
<subsectionHeader confidence="0.995567">
2.2 MTPP Features for Translation Acts
</subsectionHeader>
<bodyText confidence="0.999995896551724">
MTPP measures the coverage of individual test
sentence features found in the training set and
derives indicators of the closeness of test sen-
tences to the available training data, the difficulty
of translating the sentence, and the presence of
acts of translation for data transformation. Fea-
ture functions use statistics involving the training
set and the test sentences to determine their close-
ness. Since they are language independent, MTPP
allows quality estimation to be performed extrin-
sically. MTPP uses n-gram features defined over
text or common cover link (CCL) (Seginer, 2007)
structures as the basic units of information over
which similarity calculations are made. Unsuper-
vised parsing with CCL extracts links from base
words to head words, representing the grammati-
cal information instantiated in the training and test
data.
We extend the MTPP model we used last
year (Bic¸ici, 2013) in its learning module and the
features included. Categories for the features (S
for source, T for target) used are listed below
where the number of features are given in brackets
for S and T, {#S, #T}, and the detailed descriptions
for some of the features are presented in (Bic¸ici et
al., 2013). The number of features for each task
differs since we perform an initial feature selection
step on the tree structural features (Section 2.3).
The number of features are in the range 337−437.
</bodyText>
<listItem confidence="0.9604036">
• Coverage {56, 54}: Measures the degree to
which the test features are found in the train-
ing set for both S ({56}) and T ({54}).
• Perplexity {45, 45}: Measures the fluency of
the sentences according to language models
</listItem>
<page confidence="0.998436">
314
</page>
<bodyText confidence="0.9942595">
(LM). We use both forward (1301) and back-
ward (1151) LM features for S and T.
</bodyText>
<listItem confidence="0.9954136">
• TreeF 10, 10-1101: 10 base features and up
to 100 selected features of T among parse tree
structures (Section 2.3).
• Retrieval Closeness 116, 121: Measures the
degree to which sentences close to the test set
are found in the selected training set, Z, using
FDA (Bic¸ici and Yuret, 2011a) and BLEU,
F1 (Bic¸ici, 2011), dice, and tf-idf cosine sim-
ilarity metrics.
• IBM2 Alignment Features 10, 221: Calcu-
</listItem>
<bodyText confidence="0.992541416666667">
lates the sum of the entropy of the dis-
tribution of alignment probabilities for S
(E,∈S −p log p for p = p(t|s) where s and
t are tokens) and T, their average for S and
T, the number of entries with p &gt; 0.2 and
p &gt; 0.01, the entropy of the word align-
ment between S and T and its average, and
word alignment log probability and its value
in terms of bits per word. We also com-
pute word alignment percentage as in (Ca-
margo de Souza et al., 2013) and potential
BLEU, F1, WER, PER scores for S and T.
</bodyText>
<listItem confidence="0.922764307692308">
• IBM1 Translation Probability 14, 121: Cal-
culates the translation probability of test
sentences using the selected training set,
Z (Brown et al., 1993).
• Feature Vector Similarity 18, 81: Calculates
similarities between vector representations.
• Entropy 12, 81: Calculates the distributional
similarity of test sentences to the training set
over top N retrieved sentences (Bic¸ici et al.,
2013).
• Length 16, 31: Calculates the number of
words and characters for S and T and their
average token lengths and their ratios.
• Diversity 13, 31: Measures the diversity of
co-occurring features in the training set.
• Synthetic Translation Performance 13, 31:
Calculates translation scores achievable ac-
cording to the n-gram coverage.
• Character n-grams 151: Calculates cosine
between character n-grams (for n=2,3,4,5,6)
obtained for S and T (B¨ar et al., 2012).
• Minimum Bayes Retrieval Risk 10, 41: Cal-
culates the translation probability for the
translation having the minimum Bayes risk
among the retrieved training instances.
• Sentence Translation Performance 10, 31:
</listItem>
<bodyText confidence="0.840895416666667">
Calculates translation scores obtained ac-
cording to q(T, R) using BLEU (Papineni
et al., 2002), NIST (Doddington, 2002), or
F1 (Bic¸ici and Yuret, 2011b) for q.
• LIX 11, 11: Calculates the LIX readability
score (Wikipedia, 2013; Bj¨ornsson, 1968) for
Sand T. 1
For Task 1.1, we have additionally used com-
parative BLEU, NIST, and F1 scores as additional
features, which are obtained by comparing the
translations with each other and averaging the re-
sult (Bic¸ici, 2011).
</bodyText>
<subsectionHeader confidence="0.999703">
2.3 Bracketing Tree Structural Features
</subsectionHeader>
<bodyText confidence="0.9993338">
We use the parse tree outputs obtained by CCL
to derive features based on the bracketing struc-
ture. We derive 5 statistics based on the geometric
properties of the parse trees: number of brackets
used (numB), depth (depthB), average depth (avg
depthB), number of brackets on the right branches
over the number of brackets on the left (R/L) 2, av-
erage right to left branching over all internal tree
nodes (avg R/L). The ratio of the number of right
to left branches shows the degree to which the sen-
tence is right branching or not. Additionally, we
capture the different types of branching present
in a given parse tree identified by the number of
nodes in each of its children.
Table 1 depicts the parsing output obtained by
CCL for the following sentence from WSJ23 3:
Many fund managers argue that now ’s the time
to buy.
We use Tregex (Levy and Andrew, 2006) for vi-
sualizing the output parse trees presented on the
left. The bracketing structure statistics and fea-
tures are given on the right hand side. The root
node of each tree structural feature represents the
number of times that feature is present in the pars-
ing output of a document.
</bodyText>
<sectionHeader confidence="0.897852" genericHeader="method">
3 RTM in the Quality Estimation Task
</sectionHeader>
<bodyText confidence="0.9522583">
We participate in all of the four challenges of the
quality estimation task (QET) (Bojar et al., 2014),
which include English to Spanish (en-es), Span-
ish to English (es-en), English to German (en-
de), and German to English (de-en) translation di-
rections. There are two main categories of chal-
lenges: sentence-level prediction (Task 1.*) and
1LIX= s + C 100, where A is the number of words, C is
words longer than 6 characters, B is words that start or end
with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012).
</bodyText>
<footnote confidence="0.9867605">
2For nodes with uneven number of children, the nodes in
the odd child contribute to the right branches.
3Wall Street Journal (WSJ) corpus section 23, distributed
with Penn Treebank version 3 (Marcus et al., 1993).
</footnote>
<page confidence="0.999126">
315
</page>
<tableCaption confidence="0.999562">
Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP).
</tableCaption>
<figure confidence="0.987860238095238">
2
1
1
1
1
3 1
3 4
5 1
7 15
CCL
numB depthB avg depthB R/L avg R/L
24.0 9.0 0.375 2.1429 3.401
1 8
2 10
1 1
1 2
1
1
1
1
1 13
</figure>
<bodyText confidence="0.999928027777778">
word-level prediction (Task 2). Task 1.1 is about
predicting post-editing effort (PEE), Task 1.2 is
about predicting HTER (human-targeted transla-
tion edit rate) (Snover et al., 2006) scores of trans-
lations, Task 1.3 is about predicting post-editing
time (PET), and Task 2 is about binary, ternary, or
multi-class classification of word-level quality.
For each task, we develop individual RTM mod-
els using the parallel corpora and the LM corpora
distributed by the translation task (WMT14) (Bo-
jar et al., 2014) and the LM corpora provided by
LDC for English (Parker et al., 2011) and Span-
ish ( ˆAngelo Mendonc¸a, 2011) 4. The parallel cor-
pora contain 4.5M sentences for de-en with 110M
words for de and 116M words for en and 15.1M
sentences for en-es with 412M words for en and
462M words for es. We do not use any resources
provided by QET including data, software, or
baseline features. Instance selection for the train-
ing set and the language model (LM) corpus is
handled by parallel FDA5 (Bic¸ici et al., 2014),
whose parameters are optimized for each transla-
tion task. LM are trained using SRILM (Stolcke,
2002). We tokenize and true-case all of the cor-
pora. The true-caser is trained on all of the avail-
able training corpus using Moses (Koehn et al.,
2007). Table 2 lists the number of sentences in
the training and test sets for each task.
For each task or subtask, we select 375 thousand
(K) training instances from the available parallel
training corpora as interpretants for the individual
RTM models using parallel FDA5. We add the
selected training set to the 3 million (M) sentences
selected from the available monolingual corpora
for each LM corpus. The statistics of the training
data selected by and used as interpretants in the
</bodyText>
<footnote confidence="0.760385">
4English Gigaword 5th, Spanish Gigaword 3rd edition.
</footnote>
<table confidence="0.999668181818182">
Task Train Test
Task 1.1 (en-es) 3816 600
Task 1.1 (es-en) 1050 450
Task 1.1 (en-de) 1400 600
Task 1.1 (de-en) 1050 450
Task 1.2 (en-es) 896 208
Task 1.3 (en-es) 650 208
Task 2 (en-es) 1957 382
Task 2 (es-en) 900 150
Task 2 (en-de) 715 150
Task 2 (de-en) 350 100
</table>
<tableCaption confidence="0.994534">
Table 2: Number of sentences in different tasks.
</tableCaption>
<bodyText confidence="0.788166666666667">
RTM models is given in Table 3. The details of
instance selection with parallel FDA5 are provided
in (Bic¸ici et al., 2014).
</bodyText>
<table confidence="0.999618545454545">
Task S T
Task 1.1 (en-es) 6.2 6.9
Task 1.1 (es-en) 7.9 7.4
Task 1.1 (en-de) 6.1 6
Task 1.1 (de-en) 6.9 6.4
Task 1.2 (en-es) 6.1 6.7
Task 1.3 (en-es) 6.2 6.8
Task 2 (en-es) 6.2 6.8
Task 2 (es-en) 7.5 7
Task 2 (en-de) 5.9 5.9
Task 2 (de-en) 6.3 6.8
</table>
<tableCaption confidence="0.9858085">
Table 3: Number of words in Z (in millions) se-
lected for each task (S for source, T for target).
</tableCaption>
<subsectionHeader confidence="0.99971">
3.1 Learning Models and Optimization:
</subsectionHeader>
<bodyText confidence="0.999276">
We use ridge regression (RR), support vector re-
gression (SVR) with RBF (radial basis functions)
kernel (Smola and Sch¨olkopf, 2004), and ex-
</bodyText>
<page confidence="0.995732">
316
</page>
<table confidence="0.999823923076923">
Task Translation Model r RMSE MAE RAE
Task1.1 es-en FS-RR 0.3512 0.6394 0.5319 0.9114
es-en PLS-RR 0.3579 0.6746 0.5488 0.9405
en-de PLS-TREE 0.2922 0.7496 0.6223 0.9404
en-de TREE 0.2845 0.7485 0.6241 0.9431
en-es TREE 0.4485 0.619 0.45 0.9271
en-es PLS-TREE 0.4354 0.6213 0.4723 0.973
de-en RR 0.3415 0.7475 0.6245 0.9653
de-en PLS-RR 0.3561 0.7711 0.6236 0.9639
Task1.2 en-es SVR 0.4769 0.203 0.1378 0.8443
en-es TREE 0.4708 0.2031 0.1372 0.8407
Task1.3 en-es SVR 0.6974 21543 14866 0.6613
en-es RR 0.6991 21226 15325 0.6817
</table>
<tableCaption confidence="0.999949">
Table 4: Training performance of the top 2 individual RTM models prepared for different tasks.
</tableCaption>
<bodyText confidence="0.999939375">
tremely randomized trees (TREE) (Geurts et al.,
2006) as the learning models. TREE is an en-
semble learning method over randomized decision
trees. These models learn a regression function
using the features to estimate a numerical target
value. We also use these learning models after
a feature subset selection with recursive feature
elimination (RFE) (Guyon et al., 2002) or a di-
mensionality reduction and mapping step using
partial least squares (PLS) (Specia et al., 2009),
both of which are described in (Bic¸ici et al., 2013).
We optimize the learning parameters, the num-
ber of features to select, the number of dimen-
sions used for PLS, and the parameters for paral-
lel FDA5. More detailed descriptions of the opti-
mization processes are given in (Bic¸ici et al., 2013;
Bic¸ici et al., 2014). We optimize the learning pa-
rameters by selecting e close to the standard de-
viation of the noise in the training set (Bic¸ici,
2013) since the optimal value for e is shown to
have linear dependence to the noise level for dif-
ferent noise models (Smola et al., 1998). We select
the top 2 systems according to their performance
on the training set. For Task 2, we use both Global
Linear Models (GLM) (Collins, 2002) and GLM
with dynamic learning (GLMd) we developed last
year (Bic¸ici, 2013). GLM relies on Viterbi de-
coding, perceptron learning, and flexible feature
definitions. GLMd extends the GLM framework
by parallel perceptron training (McDonald et al.,
2010) and dynamic learning with adaptive weight
updates in the perceptron learning algorithm:
</bodyText>
<equation confidence="0.994593">
w = w + α (Φ(xi,yi) − Φ(xi,ˆy)), (1)
</equation>
<bodyText confidence="0.9999504">
where Φ returns a global representation for in-
stance i and the weights are updated by α, which
dynamically decays the amount of the change dur-
ing weight updates at later stages and prevents
large fluctuations with updates.
</bodyText>
<subsectionHeader confidence="0.999605">
3.2 Training Results
</subsectionHeader>
<bodyText confidence="0.999980285714286">
We use mean absolute error (MAE), relative
absolute error (RAE), root mean squared error
(RMSE), and correlation (r) to evaluate (Bic¸ici,
2013). DeltaAvg (Callison-Burch et al., 2012) cal-
culates the average quality difference between the
top n − 1 quartiles and the overall quality for the
test set. Table 4 provides the training results.
</bodyText>
<subsectionHeader confidence="0.999926">
3.3 Test Results
</subsectionHeader>
<bodyText confidence="0.976983333333333">
Task 1.1: Predicting the Post-Editing Effort for
Sentence Translations: Task 1.1 is about pre-
dicting post-editing effort (PEE) and their rank-
ing. The results on the test set are given in Ta-
ble 5 where QuEst (Shah et al., 2013) SVR lists
the baseline system results. Rank lists the overall
ranking in the task out of about 10 submissions.
We obtain the rankings by sorting according to the
predicted scores and randomly assigning ranks in
case of ties. RTMs with SVR PLS learning is able
to achieve the top rank in this task.
Task 1.2: Predicting HTER of Sentence Trans-
lations Task 1.2 is about predicting HTER
(human-targeted translation edit rate) (Snover et
al., 2006), where case insensitive translation edit
rate (TER) scores obtained by TERp (Snover et
al., 2009) and their ranking. We derive features
over sentences that are true-cased. The results on
the test set are given in Table 6 where the ranks are
out of about 11 submissions. We are also able to
achieve the top ranking in this task.
</bodyText>
<page confidence="0.997252">
317
</page>
<table confidence="0.999962846153846">
Ranking Translations DeltaAvg r Rank
TREE 0.26 -0.41 1
en-es PLS-TREE 0.26 -0.38 2
QuEst SVR 0.14 -0.22
PLS-RR 0.20 -0.35 2
es-en FS-RR 0.19 -0.36 3
QuEst SVR 0.12 -0.21
en-de TREE 0.39 -0.54 1
PLS-TREE 0.33 -0.42 2
QuEst SVR 0.23 -0.34
de-en RR 0.38 -0.51 1
PLS-RR 0.35 -0.45 2
QuEst SVR 0.21 -0.25
Scoring Translations MAE RMSE Rank
TREE 0.49 0.61 1
en-es PLS-TREE 0.49 0.61 2
QuEst SVR 0.52 0.66
FS-RR 0.53 0.64 1
es-en PLS-RR 0.55 0.71 2
QuEst SVR 0.57 0.68
en-de TREE 0.58 0.68 1
PLS-TREE 0.60 0.71 2
QuEst SVR 0.64 0.76
de-en RR 0.55 0.67 1
PLS-RR 0.57 0.74 2
QuEst SVR 0.65 0.78
</table>
<tableCaption confidence="0.9674605">
Table 5: RTM-DCU Task1.1 results on the test set
and baseline results.
</tableCaption>
<table confidence="0.998151066666667">
Ranking Translations DeltaAvg r Rank
9.31 0.53 1
8.57 0.48 2
5.08 0.31
Scoring Translations MAE RMSE Rank
13.40 16.69 2
14.03 17.48 4
Ranking Translations DeltaAvg r Rank
17.02 0.68 1
16.60 0.67 2
14.71 0.57
Scoring Translations MAE RMSE Rank
16.77 26.17 1
17.50 25.97 7
21.49 34.28
</table>
<tableCaption confidence="0.9842655">
Table 7: RTM-DCU Task1.3 results on the test set
and baseline results.
</tableCaption>
<bodyText confidence="0.9282138">
contains digit or punctuation).
The results on the test set are given in Table 8
where the ranks are out of about 8 submissions.
RTMs with GLM or GLMd learning becomes the
top this task as well.
</bodyText>
<table confidence="0.9995389">
Model Binary Ternary Multi-class
wFl Rank wFl Rank wFl Rank
GLM 0.351 6 0.299 5 0.268 1
en-es GLMd 0.329 7 0.266 6 0.032 7
GLM 0.269 2 0.220 2 0.087 1
es-en GLMd 0.291 1 0.239 1 0.082 2
en-de GLM 0.453 1 0.211 2 0.150 1
GLMd 0.369 2 0.219 1 0.125 2
GLM 0.261 1 0.083 2 0.024 2
en-es GLMd 0.230 2 0.086 1 0.031 1
</table>
<tableCaption confidence="0.884334">
Table 8: RTM-DCU Task 2 results on the test set.
wF1 is the average weighted F1 score.
</tableCaption>
<figure confidence="0.975488307692308">
SVR
en-es TREE
QuEst SVR
SVR
en-es TREE
RR
en-es SVR
QuEst SVR
SVR
en-es RR
QuEst SVR
QuEst SVR
15.23 19.48
</figure>
<tableCaption confidence="0.726317">
Table 6: RTM-DCU Task1.2 results on the test set
and baseline results.
</tableCaption>
<bodyText confidence="0.962364">
Task 1.3: Predicting Post-Editing Time for Sen-
tence Translations Task 1.3 involves the predic-
tion of the post-editing time (PET) for a translator
to post-edit the MT output. The results on the test
set are given in Table 7 where the ranks are out of
about 10 submissions. RTMs become the top in all
metrics with RR and SVR learning models.
Task 2: Prediction of Word-level Translation
Quality Task 2 is about binary, ternary, or multi-
class classification of word-level quality. We de-
velop individual RTM models for each subtask and
use the GLM and GLMd learning models (Bic¸ici,
2013), for predicting the quality at the word-level.
The features used are similar to last year’s (Bic¸ici,
2013) and broadly categorized as CCL links, word
context based on surrounding words, word align-
ments, word lengths, word locations, word pre-
fixes and suffixes, and word forms (i.e. capital,
</bodyText>
<subsectionHeader confidence="0.98769">
3.4 RTMs Across Tasks and Years
</subsectionHeader>
<bodyText confidence="0.999973">
We compare the difficulty of tasks according to the
RAE levels achieved. RAE measures the error rel-
ative to the error when predicting the actual mean.
A high RAE is an indicator that the task is hard. In
Table 9, we list the test results including the RAE
obtained for different tasks and subtasks including
RTM results at QET13 (Bic¸ici, 2013). The best
results are obtained for Task 1.3, which shows that
we can only reduce the error with respect to know-
ing and predicting the mean by about 28%.
</bodyText>
<sectionHeader confidence="0.997176" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99975075">
Referential translation machines achieve top per-
formance in automatic, accurate, and language in-
dependent prediction of sentence-level and word-
level statistical machine translation (SMT) qual-
ity. RTMs remove the need to access any SMT
system specific information or prior knowledge of
the training data or models used when generating
the translations.
</bodyText>
<page confidence="0.995934">
318
</page>
<table confidence="0.999837647058824">
Task Translation Model r RMSE MAE RAE
Task1.1 es-en FS-RR 0.3285 0.6373 0.5308 0.9
es-en PLS-RR 0.3105 0.7124 0.5549 0.9409
en-de PLS-TREE 0.4427 0.7091 0.6028 0.8883
en-de TREE 0.5256 0.6788 0.5838 0.8602
en-es TREE 0.4087 0.6114 0.4938 1.0983
en-es PLS-TREE 0.4163 0.6084 0.4852 1.0794
de-en RR 0.5399 0.6735 0.5513 0.8204
de-en PLS-RR 0.4878 0.737 0.567 0.8437
Task1.2 en-es SVR 0.5499 0.1669 0.134 0.8532
en-es TREE 0.5175 0.1748 0.1403 0.8931
Task1.3 en-es SVR 0.6336 26174 16770 0.7223
en-es RR 0.6359 25966 17496 0.7536
QET13 Task1.1 en-es PLS-SVR 0.5596 0.1683 0.1326 0.8849
SVR 0.5082 0.1728 0.1385 0.924
QET13 Task1.3 en-es PLS-SVR 0.6752 86.62 49.62 0.6919
SVR 0.6682 90.36 49.21 0.6862
</table>
<tableCaption confidence="0.99089">
Table 9: Test performance of the top 2 individual RTM models prepared for different tasks and RTM
results from QET13 on similar tasks (Bic¸ici, 2013).
</tableCaption>
<sectionHeader confidence="0.985043" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999846111111111">
This work is supported in part by SFI
(07/CE/I1142) as part of the CNGL Centre
for Global Intelligent Content (www.cngl.org)
at Dublin City University and in part by the
European Commission through the QTLaunchPad
FP7 project (No: 296347). We also thank the
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational
facilities and support.
</bodyText>
<sectionHeader confidence="0.992469" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99434584">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei
Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilingual
semantic textual similarity. In Proceedings of the
8th International Workshop on Semantic Evaluation
(SemEval-2014), Dublin, Ireland, August.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. Ukp: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In *SEM 2012: The First Joint
Conference on Lexical and Computational Seman-
tics – Volume 1: Proceedings of the main conference
and the shared task, and Volume 2: Proceedings of
the Sixth International Workshop on Semantic Eval-
uation (SemEval 2012), pages 435–440, Montr´eal,
Canada, 7-8 June. Association for Computational
Linguistics.
Ergun Bic¸ici and Andy Way. 2014. RTM-DCU: Ref-
erential translation machines for semantic similarity.
In SemEval-2014: Semantic Evaluation Exercises
- International Workshop on Semantic Evaluation,
Dublin, Ireland, 23-24 August.
Ergun Bic¸ici and Deniz Yuret. 2011a. Instance se-
lection for machine translation using feature decay
algorithms. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 272–283,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic¸ici and Deniz Yuret. 2011b. RegMT system
for machine translation, system combination, and
evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 323–329,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
Ergun Bic¸ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Ergun Bic¸ici, Declan Groves, and Josef van Genabith.
2013. Predicting sentence translation quality using
extrinsic and language independent features. Ma-
chine Translation, 27:171–192, December.
Ergun Bic¸ici, Qun Liu, and Andy Way. 2014. Par-
allel FDA5 for fast deployment of accurate statisti-
cal machine translation systems. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, Baltimore, USA, June. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.997049">
319
</page>
<table confidence="0.442513666666667">
Ergun Bic¸ici. 2011. The Regression Model of Machine ’02, pages 1–8, Stroudsburg, PA, USA. Association
Translation. Ph.D. thesis, Koc¸ University. Supervi- for Computational Linguistics.
sor: Deniz Yuret.
</table>
<reference confidence="0.996086761904762">
Ergun Bic¸ici. 2013. Referential translation machines
for quality estimation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, Sofia,
Bulgaria, August. Association for Computational
Linguistics.
Ergun Bic¸ici. 2008. Consensus ontologies in socially
interacting multiagent systems. Journal of Multia-
gent and Grid Systems.
Carl Hugo Bj¨ornsson. 1968. L¨asbarhet. Liber.
Chris Bliss. 2012. Comedy is transla-
tion, February. http://www.ted.com/talks/
chris bliss comedy is translation.html.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Proc. of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 1–44, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Ondˇrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Matouˇs Mach´aˇcek,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, and Lucia Specia.
2014. Findings of the 2014 workshop on statisti-
cal machine translation. In Proc. of the Ninth Work-
shop on Statistical Machine Translation, Balrimore,
USA, June. Association for Computational Linguis-
tics.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2):263–311, June.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proc. of the Seventh Work-
shop on Statistical Machine Translation, pages 10–
51, Montr´eal, Canada, June. Association for Com-
putational Linguistics.
Jos´e Guilherme Camargo de Souza, Christian Buck,
Marco Turchi, and Matteo Negri. 2013. FBK-
UEdin participation to the WMT13 quality estima-
tion shared task. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 352–
358, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138–145, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006. Extremely randomized trees. Machine Learn-
ing, 63(1):3–42.
Isabelle Guyon, Jason Weston, Stephen Barnhill, and
Vladimir Vapnik. 2002. Gene selection for cancer
classification using support vector machines. Ma-
chine Learning, 46(1-3):389–422.
Kenth Hagstr¨om. 2012. Swedish readabil-
ity calculator. https://github.com/keha76/Swedish-
Readability-Calculator.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. SemEval-2014 Task 3:
Cross-level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval-2014), Dublin, Ireland, August.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL, pages 177–180, Prague, Czech Republic, June.
Roger Levy and Galen Andrew. 2006. Tregex and
Tsurgeon: tools for querying and manipulating tree
data structures. In Proceedings of the fifth interna-
tional conference on Language Resources and Eval-
uation.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: the penn treebank. Comput.
Linguist., 19(2):313–330, June.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014. SemEval-2014 Task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland, August.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456–464, Los Angeles, California,
June. Association for Computational Linguistics.
</reference>
<page confidence="0.973526">
320
</page>
<reference confidence="0.999899981481482">
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a method for auto-
matic evaluation of machine translation. In Proc.
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2011. English Gigaword fifth edi-
tion, Linguistic Data Consortium.
Yoav Seginer. 2007. Learning Syntactic Structure.
Ph.D. thesis, Universiteit van Amsterdam.
Kashif Shah, Eleftherios Avramidis, Ergun Bic¸ici, and
Lucia Specia. 2013. Quest - design, implementation
and extensions of a framework for machine transla-
tion quality estimation. Prague Bull. Math. Linguis-
tics, 100:19–30.
Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tu-
torial on support vector regression. Statistics and
Computing, 14(3):199–222, August.
A. J. Smola, N. Murata, B. Sch¨olkopf, and K.-R.
M¨uller. 1998. Asymptotically optimal choice of
ε-loss for support vector machines. In L. Niklas-
son, M. Boden, and T. Ziemke, editors, Proceedings
of the International Conference on Artificial Neural
Networks, Perspectives in Neural Computing, pages
105–110, Berlin. Springer.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings ofAssociation for Machine
Translation in the Americas,.
Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and
Richard Schwartz. 2009. Fluency, adequacy,
or hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, StatMT ’09, pages 259–268, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Estimat-
ing the sentence-level quality of machine translation
systems. In Proceedings of the 13th Annual Con-
ference of the European Association for Machine
Translation (EAMT), pages 28–35, Barcelona, May.
EAMT.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proc. Intl. Conf. on Spoken
Language Processing, pages 901–904.
Wikipedia. 2013. Lix.
http://en.wikipedia.org/wiki/LIX.
David Graff Denise DiPersio ˆAngelo Mendonc¸a,
Daniel Jaquette. 2011. Spanish Gigaword third edi-
tion, Linguistic Data Consortium.
</reference>
<page confidence="0.99877">
321
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.656393">
<title confidence="0.998915">Referential Translation Machines for Predicting Translation Quality</title>
<author confidence="0.99879">Andy</author>
<affiliation confidence="0.996669666666667">Centre for Next Generation School of Dublin City University, Dublin,</affiliation>
<email confidence="0.869646">away@computing.dcu.ie</email>
<affiliation confidence="0.996345666666667">Centre for Next Generation School of Dublin City University, Dublin,</affiliation>
<email confidence="0.99309">ergun.bicici@computing.dcu.ie</email>
<abstract confidence="0.991870321428571">We use referential translation machines (RTM) for quality estimation of translation outputs. RTMs are a computational model for identifying the translation acts between any two data sets with respect to interpretants selected in the same domain, which are effective when making monolingual and bilingual similarity judgments. RTMs achieve top performance in automatic, accurate, and language independent prediction of sentence-level and word-level statistical machine translation (SMT) quality. RTMs remove the need to access any SMT system specific information or prior knowledge of the training data or models used when generating the translations and achieve the top performance in WMT13 quality estimation task (QET13). We improve our RTM models with the Parallel FDA5 instance selection model, with additional features for predicting the translation performance, and with improved learning models. We develop RTM models for each WMT14 QET (QET14) subtask, obtain improvements over QET13 reand rank in all of the tasks and subtasks of QET14.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<title>SemEval-2014 Task 10: Multilingual semantic textual similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="4131" citStr="Agirre et al., 2014" startWordPosition="609" endWordPosition="612">op performance when predicting the quality of translations in QET14 and QET13 (Bic¸ici, 313 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 313–321, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics 2013), top performance when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014), good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014), and a language independent solution and good performance when judging the semantic similarity of sentences (Agirre et al., 2014; Bic¸ici and Way, 2014). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the real objects (Bic¸ici, 2008). Each RTM model is a data translation model between the instances in the training set and the test set.</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 Task 10: Multilingual semantic textual similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>435--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal,</location>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. In *SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pages 435–440, Montr´eal, Canada, 7-8 June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Andy Way</author>
</authors>
<title>RTM-DCU: Referential translation machines for semantic similarity.</title>
<date>2014</date>
<booktitle>In SemEval-2014: Semantic Evaluation Exercises - International Workshop on Semantic Evaluation,</booktitle>
<location>Dublin,</location>
<marker>Bic¸ici, Way, 2014</marker>
<rawString>Ergun Bic¸ici and Andy Way. 2014. RTM-DCU: Referential translation machines for semantic similarity. In SemEval-2014: Semantic Evaluation Exercises - International Workshop on Semantic Evaluation, Dublin, Ireland, 23-24 August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>Instance selection for machine translation using feature decay algorithms.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>272--283</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Bic¸ici, Yuret, 2011</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2011a. Instance selection for machine translation using feature decay algorithms. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 272–283, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>RegMT system for machine translation, system combination, and evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>323--329</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<marker>Bic¸ici, Yuret, 2011</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2011b. RegMT system for machine translation, system combination, and evaluation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 323–329, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Deniz Yuret</author>
</authors>
<title>Optimizing instance selection for statistical machine translation with feature decay algorithms.</title>
<date>2014</date>
<journal>IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP).</journal>
<marker>Bic¸ici, Yuret, 2014</marker>
<rawString>Ergun Bic¸ici and Deniz Yuret. 2014. Optimizing instance selection for statistical machine translation with feature decay algorithms. IEEE/ACM Transactions On Audio, Speech, and Language Processing (TASLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Declan Groves</author>
<author>Josef van Genabith</author>
</authors>
<title>Predicting sentence translation quality using extrinsic and language independent features.</title>
<date>2013</date>
<journal>Machine Translation,</journal>
<pages>27--171</pages>
<marker>Bic¸ici, Groves, van Genabith, 2013</marker>
<rawString>Ergun Bic¸ici, Declan Groves, and Josef van Genabith. 2013. Predicting sentence translation quality using extrinsic and language independent features. Machine Translation, 27:171–192, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
<author>Qun Liu</author>
<author>Andy Way</author>
</authors>
<title>Parallel FDA5 for fast deployment of accurate statistical machine translation systems.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, USA,</location>
<marker>Bic¸ici, Liu, Way, 2014</marker>
<rawString>Ergun Bic¸ici, Qun Liu, and Andy Way. 2014. Parallel FDA5 for fast deployment of accurate statistical machine translation systems. In Proceedings of the Ninth Workshop on Statistical Machine Translation, Baltimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>Referential translation machines for quality estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria,</location>
<marker>Bic¸ici, 2013</marker>
<rawString>Ergun Bic¸ici. 2013. Referential translation machines for quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ergun Bic¸ici</author>
</authors>
<title>Consensus ontologies in socially interacting multiagent systems.</title>
<date>2008</date>
<journal>Journal of Multiagent and Grid Systems.</journal>
<marker>Bic¸ici, 2008</marker>
<rawString>Ergun Bic¸ici. 2008. Consensus ontologies in socially interacting multiagent systems. Journal of Multiagent and Grid Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Hugo Bj¨ornsson</author>
</authors>
<date>1968</date>
<note>L¨asbarhet. Liber.</note>
<marker>Bj¨ornsson, 1968</marker>
<rawString>Carl Hugo Bj¨ornsson. 1968. L¨asbarhet. Liber.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Bliss</author>
</authors>
<title>Comedy is translation, February. http://www.ted.com/talks/ chris bliss comedy is translation.html.</title>
<date>2012</date>
<contexts>
<context position="5163" citStr="Bliss, 2012" startWordPosition="780" endWordPosition="781">ant I interprets the signs used to refer to the real objects (Bic¸ici, 2008). Each RTM model is a data translation model between the instances in the training set and the test set. We use the Parallel FDA5 (Feature Decay Algorithms) instance selection model for selecting the interpretants (Bic¸ici et al., 2014; Bic¸ici and Yuret, 2014) this year, which allows efficient parameterization, optimization, and implementation of FDA, and build an MTPP model (Section 2.1). We view that acts of translation are ubiquitously used during communication: Every act of communication is an act of translation (Bliss, 2012). Given a training set train, a test set test, and some corpus C, preferably in the same domain as the training and test sets, the RTM steps are: 1. FDA5(train, test, C) → I 2. MTPP(I, train) → Ftrain 3. MTPP(I, test) → Ftest 4. learn(M,Ftrain) → M 5. predict(M,Ftest) → qˆ Step 1 selects the interpretants, I, relevant to both the training and test data. Steps 2 and 3 use I to map train and test to a new space where similarities between translation acts can be derived more easily. Step 4 trains a learning model M over the training features, Ftrain, and Step 5 obtains the predictions. RTM relies</context>
</contexts>
<marker>Bliss, 2012</marker>
<rawString>Chris Bliss. 2012. Comedy is translation, February. http://www.ted.com/talks/ chris bliss comedy is translation.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proc. of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2424" citStr="Bojar et al., 2013" startWordPosition="356" endWordPosition="359">nt because the expected translation performance can help in estimating the effort required for correcting the translations during post-editing by human translators. Bicici et al. (2013) develop the Machine Translation Performance Predictor (MTPP), a state-ofthe-art, language independent, and SMT system extrinsic machine translation performance predictor, which can predict translation quality by looking at the test source sentences and becomes the 2nd overall after also looking at the translation outputs as well in QET12 (Callison-Burch et al., 2012). RTMs achieve the top performance in QET13 (Bojar et al., 2013), ranking 1st or 2nd in all of the subtasks. RTMs rank 1st in all of the tasks and subtasks of QET14 (Bojar et al., 2014). Referential translation models (Section 2) present an accurate and language independent solution for predicting the performance of natural language tasks such as the quality estimation of translation. We improve our RTM models (Bic¸ici, 2013) by: • using a parameterized, fast implementation of FDA, FDA5, and our Parallel FDA5 instance selection model (Bic¸ici et al., 2014), • better modeling of the language in which similarity judgments are made with improved optimization </context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proc. of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Matouˇs Mach´aˇcek</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
<author>Herve Saint-Amand</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2014 workshop on statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proc. of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Balrimore, USA,</location>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Mach´aˇcek, Monz, Pecina, Post, Saint-Amand, Soricut, Specia, 2014</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Matouˇs Mach´aˇcek, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, and Lucia Specia. 2014. Findings of the 2014 workshop on statistical machine translation. In Proc. of the Ninth Workshop on Statistical Machine Translation, Balrimore, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="9381" citStr="Brown et al., 1993" startWordPosition="1506" endWordPosition="1509">f the distribution of alignment probabilities for S (E,∈S −p log p for p = p(t|s) where s and t are tokens) and T, their average for S and T, the number of entries with p &gt; 0.2 and p &gt; 0.01, the entropy of the word alignment between S and T and its average, and word alignment log probability and its value in terms of bits per word. We also compute word alignment percentage as in (Camargo de Souza et al., 2013) and potential BLEU, F1, WER, PER scores for S and T. • IBM1 Translation Probability 14, 121: Calculates the translation probability of test sentences using the selected training set, Z (Brown et al., 1993). • Feature Vector Similarity 18, 81: Calculates similarities between vector representations. • Entropy 12, 81: Calculates the distributional similarity of test sentences to the training set over top N retrieved sentences (Bic¸ici et al., 2013). • Length 16, 31: Calculates the number of words and characters for S and T and their average token lengths and their ratios. • Diversity 13, 31: Measures the diversity of co-occurring features in the training set. • Synthetic Translation Performance 13, 31: Calculates translation scores achievable according to the n-gram coverage. • Character n-grams 1</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="2360" citStr="Callison-Burch et al., 2012" startWordPosition="345" endWordPosition="348">any task dependent resource. Prediction of translation quality is important because the expected translation performance can help in estimating the effort required for correcting the translations during post-editing by human translators. Bicici et al. (2013) develop the Machine Translation Performance Predictor (MTPP), a state-ofthe-art, language independent, and SMT system extrinsic machine translation performance predictor, which can predict translation quality by looking at the test source sentences and becomes the 2nd overall after also looking at the translation outputs as well in QET12 (Callison-Burch et al., 2012). RTMs achieve the top performance in QET13 (Bojar et al., 2013), ranking 1st or 2nd in all of the subtasks. RTMs rank 1st in all of the tasks and subtasks of QET14 (Bojar et al., 2014). Referential translation models (Section 2) present an accurate and language independent solution for predicting the performance of natural language tasks such as the quality estimation of translation. We improve our RTM models (Bic¸ici, 2013) by: • using a parameterized, fast implementation of FDA, FDA5, and our Parallel FDA5 instance selection model (Bic¸ici et al., 2014), • better modeling of the language in</context>
<context position="18340" citStr="Callison-Burch et al., 2012" startWordPosition="3036" endWordPosition="3039"> the GLM framework by parallel perceptron training (McDonald et al., 2010) and dynamic learning with adaptive weight updates in the perceptron learning algorithm: w = w + α (Φ(xi,yi) − Φ(xi,ˆy)), (1) where Φ returns a global representation for instance i and the weights are updated by α, which dynamically decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates. 3.2 Training Results We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) to evaluate (Bic¸ici, 2013). DeltaAvg (Callison-Burch et al., 2012) calculates the average quality difference between the top n − 1 quartiles and the overall quality for the test set. Table 4 provides the training results. 3.3 Test Results Task 1.1: Predicting the Post-Editing Effort for Sentence Translations: Task 1.1 is about predicting post-editing effort (PEE) and their ranking. The results on the test set are given in Table 5 where QuEst (Shah et al., 2013) SVR lists the baseline system results. Rank lists the overall ranking in the task out of about 10 submissions. We obtain the rankings by sorting according to the predicted scores and randomly assignin</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proc. of the Seventh Workshop on Statistical Machine Translation, pages 10– 51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Guilherme Camargo de Souza</author>
<author>Christian Buck</author>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
</authors>
<title>FBKUEdin participation to the WMT13 quality estimation shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>352--358</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>de Souza, Buck, Turchi, Negri, 2013</marker>
<rawString>Jos´e Guilherme Camargo de Souza, Christian Buck, Marco Turchi, and Matteo Negri. 2013. FBKUEdin participation to the WMT13 quality estimation shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 352– 358, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP</booktitle>
<contexts>
<context position="17535" citStr="Collins, 2002" startWordPosition="2912" endWordPosition="2913">o select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More detailed descriptions of the optimization processes are given in (Bic¸ici et al., 2013; Bic¸ici et al., 2014). We optimize the learning parameters by selecting e close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for e is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). We select the top 2 systems according to their performance on the training set. For Task 2, we use both Global Linear Models (GLM) (Collins, 2002) and GLM with dynamic learning (GLMd) we developed last year (Bic¸ici, 2013). GLM relies on Viterbi decoding, perceptron learning, and flexible feature definitions. GLMd extends the GLM framework by parallel perceptron training (McDonald et al., 2010) and dynamic learning with adaptive weight updates in the perceptron learning algorithm: w = w + α (Φ(xi,yi) − Φ(xi,ˆy)), (1) where Φ returns a global representation for instance i and the weights are updated by α, which dynamically decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates. </context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing - Volume 10, EMNLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="10419" citStr="Doddington, 2002" startWordPosition="1665" endWordPosition="1666">ccurring features in the training set. • Synthetic Translation Performance 13, 31: Calculates translation scores achievable according to the n-gram coverage. • Character n-grams 151: Calculates cosine between character n-grams (for n=2,3,4,5,6) obtained for S and T (B¨ar et al., 2012). • Minimum Bayes Retrieval Risk 10, 41: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance 10, 31: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • LIX 11, 11: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 1 For Task 1.1, we have additionally used comparative BLEU, NIST, and F1 scores as additional features, which are obtained by comparing the translations with each other and averaging the result (Bic¸ici, 2011). 2.3 Bracketing Tree Structural Features We use the parse tree outputs obtained by CCL to derive features based on the bracketing structure. We derive 5 statistics based on the geometric properties of the parse trees: number of brackets used (numB),</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierre Geurts</author>
<author>Damien Ernst</author>
<author>Louis Wehenkel</author>
</authors>
<title>Extremely randomized trees.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>63--1</pages>
<contexts>
<context position="16381" citStr="Geurts et al., 2006" startWordPosition="2712" endWordPosition="2715">R 0.3512 0.6394 0.5319 0.9114 es-en PLS-RR 0.3579 0.6746 0.5488 0.9405 en-de PLS-TREE 0.2922 0.7496 0.6223 0.9404 en-de TREE 0.2845 0.7485 0.6241 0.9431 en-es TREE 0.4485 0.619 0.45 0.9271 en-es PLS-TREE 0.4354 0.6213 0.4723 0.973 de-en RR 0.3415 0.7475 0.6245 0.9653 de-en PLS-RR 0.3561 0.7711 0.6236 0.9639 Task1.2 en-es SVR 0.4769 0.203 0.1378 0.8443 en-es TREE 0.4708 0.2031 0.1372 0.8407 Task1.3 en-es SVR 0.6974 21543 14866 0.6613 en-es RR 0.6991 21226 15325 0.6817 Table 4: Training performance of the top 2 individual RTM models prepared for different tasks. tremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. TREE is an ensemble learning method over randomized decision trees. These models learn a regression function using the features to estimate a numerical target value. We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), both of which are described in (Bic¸ici et al., 2013). We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the par</context>
</contexts>
<marker>Geurts, Ernst, Wehenkel, 2006</marker>
<rawString>Pierre Geurts, Damien Ernst, and Louis Wehenkel. 2006. Extremely randomized trees. Machine Learning, 63(1):3–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isabelle Guyon</author>
<author>Jason Weston</author>
<author>Stephen Barnhill</author>
<author>Vladimir Vapnik</author>
</authors>
<title>Gene selection for cancer classification using support vector machines.</title>
<date>2002</date>
<booktitle>Machine Learning,</booktitle>
<pages>46--1</pages>
<contexts>
<context position="16700" citStr="Guyon et al., 2002" startWordPosition="2762" endWordPosition="2765">es SVR 0.4769 0.203 0.1378 0.8443 en-es TREE 0.4708 0.2031 0.1372 0.8407 Task1.3 en-es SVR 0.6974 21543 14866 0.6613 en-es RR 0.6991 21226 15325 0.6817 Table 4: Training performance of the top 2 individual RTM models prepared for different tasks. tremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. TREE is an ensemble learning method over randomized decision trees. These models learn a regression function using the features to estimate a numerical target value. We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), both of which are described in (Bic¸ici et al., 2013). We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More detailed descriptions of the optimization processes are given in (Bic¸ici et al., 2013; Bic¸ici et al., 2014). We optimize the learning parameters by selecting e close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for e is shown to ha</context>
</contexts>
<marker>Guyon, Weston, Barnhill, Vapnik, 2002</marker>
<rawString>Isabelle Guyon, Jason Weston, Stephen Barnhill, and Vladimir Vapnik. 2002. Gene selection for cancer classification using support vector machines. Machine Learning, 46(1-3):389–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenth Hagstr¨om</author>
</authors>
<date>2012</date>
<note>Swedish readability calculator. https://github.com/keha76/SwedishReadability-Calculator.</note>
<marker>Hagstr¨om, 2012</marker>
<rawString>Kenth Hagstr¨om. 2012. Swedish readability calculator. https://github.com/keha76/SwedishReadability-Calculator.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>SemEval-2014 Task 3: Cross-level semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="3886" citStr="Jurgens et al., 2014" startWordPosition="572" endWordPosition="575">l for quality and semantic similarity judgments in monolingual and bilingual settings using retrieval of relevant training data (Bic¸ici, 2011; Bic¸ici and Yuret, 2014) as interpretants for reaching shared semantics (Bic¸ici, 2008). RTMs achieve top performance when predicting the quality of translations in QET14 and QET13 (Bic¸ici, 313 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 313–321, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics 2013), top performance when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014), good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014), and a language independent solution and good performance when judging the semantic similarity of sentences (Agirre et al., 2014; Bic¸ici and Way, 2014). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, data close to both the training set and the test set, which allow shared semantics by provi</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. SemEval-2014 Task 3: Cross-level semantic similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL,</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="14207" citStr="Koehn et al., 2007" startWordPosition="2336" endWordPosition="2339">contain 4.5M sentences for de-en with 110M words for de and 116M words for en and 15.1M sentences for en-es with 412M words for en and 462M words for es. We do not use any resources provided by QET including data, software, or baseline features. Instance selection for the training set and the language model (LM) corpus is handled by parallel FDA5 (Bic¸ici et al., 2014), whose parameters are optimized for each translation task. LM are trained using SRILM (Stolcke, 2002). We tokenize and true-case all of the corpora. The true-caser is trained on all of the available training corpus using Moses (Koehn et al., 2007). Table 2 lists the number of sentences in the training and test sets for each task. For each task or subtask, we select 375 thousand (K) training instances from the available parallel training corpora as interpretants for the individual RTM models using parallel FDA5. We add the selected training set to the 3 million (M) sentences selected from the available monolingual corpora for each LM corpus. The statistics of the training data selected by and used as interpretants in the 4English Gigaword 5th, Spanish Gigaword 3rd edition. Task Train Test Task 1.1 (en-es) 3816 600 Task 1.1 (es-en) 1050 </context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Galen Andrew</author>
</authors>
<title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the fifth international conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="11672" citStr="Levy and Andrew, 2006" startWordPosition="1881" endWordPosition="1884">(avg depthB), number of brackets on the right branches over the number of brackets on the left (R/L) 2, average right to left branching over all internal tree nodes (avg R/L). The ratio of the number of right to left branches shows the degree to which the sentence is right branching or not. Additionally, we capture the different types of branching present in a given parse tree identified by the number of nodes in each of its children. Table 1 depicts the parsing output obtained by CCL for the following sentence from WSJ23 3: Many fund managers argue that now ’s the time to buy. We use Tregex (Levy and Andrew, 2006) for visualizing the output parse trees presented on the left. The bracketing structure statistics and features are given on the right hand side. The root node of each tree structural feature represents the number of times that feature is present in the parsing output of a document. 3 RTM in the Quality Estimation Task We participate in all of the four challenges of the quality estimation task (QET) (Bojar et al., 2014), which include English to Spanish (en-es), Spanish to English (es-en), English to German (ende), and German to English (de-en) translation directions. There are two main catego</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Roger Levy and Galen Andrew. 2006. Tregex and Tsurgeon: tools for querying and manipulating tree data structures. In Proceedings of the fifth international conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: the penn treebank.</title>
<date>1993</date>
<journal>Comput. Linguist.,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="12721" citStr="Marcus et al., 1993" startWordPosition="2065" endWordPosition="2068">h include English to Spanish (en-es), Spanish to English (es-en), English to German (ende), and German to English (de-en) translation directions. There are two main categories of challenges: sentence-level prediction (Task 1.*) and 1LIX= s + C 100, where A is the number of words, C is words longer than 6 characters, B is words that start or end with any of “.”, “:”, “!”, “?” similar to (Hagstr¨om, 2012). 2For nodes with uneven number of children, the nodes in the odd child contribute to the right branches. 3Wall Street Journal (WSJ) corpus section 23, distributed with Penn Treebank version 3 (Marcus et al., 1993). 315 Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP). 2 1 1 1 1 3 1 3 4 5 1 7 15 CCL numB depthB avg depthB R/L avg R/L 24.0 9.0 0.375 2.1429 3.401 1 8 2 10 1 1 1 2 1 1 1 1 1 13 word-level prediction (Task 2). Task 1.1 is about predicting post-editing effort (PEE), Task 1.2 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006) scores of translations, Task 1.3 is about predicting post-editing time (PET), and Task 2 is about binary, ternary, or multi-class classification of word-level quality. For each task, we develo</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: the penn treebank. Comput. Linguist., 19(2):313–330, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Stefano Menini</author>
<author>Marco Baroni</author>
<author>Luisa Bentivogli</author>
<author>Raffaella Bernardi</author>
<author>Roberto Zamparelli</author>
</authors>
<title>SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<contexts>
<context position="4002" citStr="Marelli et al., 2014" startWordPosition="589" endWordPosition="592">aining data (Bic¸ici, 2011; Bic¸ici and Yuret, 2014) as interpretants for reaching shared semantics (Bic¸ici, 2008). RTMs achieve top performance when predicting the quality of translations in QET14 and QET13 (Bic¸ici, 313 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 313–321, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics 2013), top performance when predicting monolingual cross-level semantic similarity (Jurgens et al., 2014), good performance when evaluating the semantic relatedness of sentences and their entailment (Marelli et al., 2014), and a language independent solution and good performance when judging the semantic similarity of sentences (Agirre et al., 2014; Bic¸ici and Way, 2014). RTM is a computational model for identifying the acts of translation for translating between any given two data sets with respect to a reference corpus selected in the same domain. An RTM model is based on the selection of interpretants, data close to both the training set and the test set, which allow shared semantics by providing context for similarity judgments. In semiotics, an interpretant I interprets the signs used to refer to the rea</context>
</contexts>
<marker>Marelli, Menini, Baroni, Bentivogli, Bernardi, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Keith Hall</author>
<author>Gideon Mann</author>
</authors>
<title>Distributed training strategies for the structured perceptron.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>456--464</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Los Angeles, California,</location>
<contexts>
<context position="17786" citStr="McDonald et al., 2010" startWordPosition="2947" endWordPosition="2950">electing e close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for e is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). We select the top 2 systems according to their performance on the training set. For Task 2, we use both Global Linear Models (GLM) (Collins, 2002) and GLM with dynamic learning (GLMd) we developed last year (Bic¸ici, 2013). GLM relies on Viterbi decoding, perceptron learning, and flexible feature definitions. GLMd extends the GLM framework by parallel perceptron training (McDonald et al., 2010) and dynamic learning with adaptive weight updates in the perceptron learning algorithm: w = w + α (Φ(xi,yi) − Φ(xi,ˆy)), (1) where Φ returns a global representation for instance i and the weights are updated by α, which dynamically decays the amount of the change during weight updates at later stages and prevents large fluctuations with updates. 3.2 Training Results We use mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), and correlation (r) to evaluate (Bic¸ici, 2013). DeltaAvg (Callison-Burch et al., 2012) calculates the average quality difference bet</context>
</contexts>
<marker>McDonald, Hall, Mann, 2010</marker>
<rawString>Ryan McDonald, Keith Hall, and Gideon Mann. 2010. Distributed training strategies for the structured perceptron. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 456–464, Los Angeles, California, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="10394" citStr="Papineni et al., 2002" startWordPosition="1660" endWordPosition="1663">Measures the diversity of co-occurring features in the training set. • Synthetic Translation Performance 13, 31: Calculates translation scores achievable according to the n-gram coverage. • Character n-grams 151: Calculates cosine between character n-grams (for n=2,3,4,5,6) obtained for S and T (B¨ar et al., 2012). • Minimum Bayes Retrieval Risk 10, 41: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance 10, 31: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • LIX 11, 11: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 1 For Task 1.1, we have additionally used comparative BLEU, NIST, and F1 scores as additional features, which are obtained by comparing the translations with each other and averaging the result (Bic¸ici, 2011). 2.3 Bracketing Tree Structural Features We use the parse tree outputs obtained by CCL to derive features based on the bracketing structure. We derive 5 statistics based on the geometric properties of the parse trees: number</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English Gigaword fifth edition, Linguistic Data Consortium.</title>
<date>2011</date>
<contexts>
<context position="13524" citStr="Parker et al., 2011" startWordPosition="2214" endWordPosition="2217">.1429 3.401 1 8 2 10 1 1 1 2 1 1 1 1 1 13 word-level prediction (Task 2). Task 1.1 is about predicting post-editing effort (PEE), Task 1.2 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006) scores of translations, Task 1.3 is about predicting post-editing time (PET), and Task 2 is about binary, ternary, or multi-class classification of word-level quality. For each task, we develop individual RTM models using the parallel corpora and the LM corpora distributed by the translation task (WMT14) (Bojar et al., 2014) and the LM corpora provided by LDC for English (Parker et al., 2011) and Spanish ( ˆAngelo Mendonc¸a, 2011) 4. The parallel corpora contain 4.5M sentences for de-en with 110M words for de and 116M words for en and 15.1M sentences for en-es with 412M words for en and 462M words for es. We do not use any resources provided by QET including data, software, or baseline features. Instance selection for the training set and the language model (LM) corpus is handled by parallel FDA5 (Bic¸ici et al., 2014), whose parameters are optimized for each translation task. LM are trained using SRILM (Stolcke, 2002). We tokenize and true-case all of the corpora. The true-caser </context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2011</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English Gigaword fifth edition, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Seginer</author>
</authors>
<title>Learning Syntactic Structure.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Universiteit van Amsterdam.</institution>
<contexts>
<context position="7223" citStr="Seginer, 2007" startWordPosition="1119" endWordPosition="1120">tures for Translation Acts MTPP measures the coverage of individual test sentence features found in the training set and derives indicators of the closeness of test sentences to the available training data, the difficulty of translating the sentence, and the presence of acts of translation for data transformation. Feature functions use statistics involving the training set and the test sentences to determine their closeness. Since they are language independent, MTPP allows quality estimation to be performed extrinsically. MTPP uses n-gram features defined over text or common cover link (CCL) (Seginer, 2007) structures as the basic units of information over which similarity calculations are made. Unsupervised parsing with CCL extracts links from base words to head words, representing the grammatical information instantiated in the training and test data. We extend the MTPP model we used last year (Bic¸ici, 2013) in its learning module and the features included. Categories for the features (S for source, T for target) used are listed below where the number of features are given in brackets for S and T, {#S, #T}, and the detailed descriptions for some of the features are presented in (Bic¸ici et al</context>
</contexts>
<marker>Seginer, 2007</marker>
<rawString>Yoav Seginer. 2007. Learning Syntactic Structure. Ph.D. thesis, Universiteit van Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
</authors>
<title>Eleftherios Avramidis, Ergun Bic¸ici, and Lucia Specia.</title>
<date>2013</date>
<journal>Prague Bull. Math. Linguistics,</journal>
<pages>100--19</pages>
<marker>Shah, 2013</marker>
<rawString>Kashif Shah, Eleftherios Avramidis, Ergun Bic¸ici, and Lucia Specia. 2013. Quest - design, implementation and extensions of a framework for machine translation quality estimation. Prague Bull. Math. Linguistics, 100:19–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex J Smola</author>
<author>Bernhard Sch¨olkopf</author>
</authors>
<title>A tutorial on support vector regression.</title>
<date>2004</date>
<journal>Statistics and Computing,</journal>
<volume>14</volume>
<issue>3</issue>
<marker>Smola, Sch¨olkopf, 2004</marker>
<rawString>Alex J. Smola and Bernhard Sch¨olkopf. 2004. A tutorial on support vector regression. Statistics and Computing, 14(3):199–222, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Smola</author>
<author>N Murata</author>
<author>B Sch¨olkopf</author>
<author>K-R M¨uller</author>
</authors>
<title>Asymptotically optimal choice of ε-loss for support vector machines. In</title>
<date>1998</date>
<booktitle>Proceedings of the International Conference on Artificial Neural Networks, Perspectives in Neural Computing,</booktitle>
<pages>105--110</pages>
<editor>L. Niklasson, M. Boden, and T. Ziemke, editors,</editor>
<publisher>Springer.</publisher>
<location>Berlin.</location>
<marker>Smola, Murata, Sch¨olkopf, M¨uller, 1998</marker>
<rawString>A. J. Smola, N. Murata, B. Sch¨olkopf, and K.-R. M¨uller. 1998. Asymptotically optimal choice of ε-loss for support vector machines. In L. Niklasson, M. Boden, and T. Ziemke, editors, Proceedings of the International Conference on Artificial Neural Networks, Perspectives in Neural Computing, pages 105–110, Berlin. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas,.</booktitle>
<contexts>
<context position="13128" citStr="Snover et al., 2006" startWordPosition="2148" endWordPosition="2151"> 2For nodes with uneven number of children, the nodes in the odd child contribute to the right branches. 3Wall Street Journal (WSJ) corpus section 23, distributed with Penn Treebank version 3 (Marcus et al., 1993). 315 Table 1: Tree features for a parsing output by CCL (immediate non-terminals replaced with NP). 2 1 1 1 1 3 1 3 4 5 1 7 15 CCL numB depthB avg depthB R/L avg R/L 24.0 9.0 0.375 2.1429 3.401 1 8 2 10 1 1 1 2 1 1 1 1 1 13 word-level prediction (Task 2). Task 1.1 is about predicting post-editing effort (PEE), Task 1.2 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006) scores of translations, Task 1.3 is about predicting post-editing time (PET), and Task 2 is about binary, ternary, or multi-class classification of word-level quality. For each task, we develop individual RTM models using the parallel corpora and the LM corpora distributed by the translation task (WMT14) (Bojar et al., 2014) and the LM corpora provided by LDC for English (Parker et al., 2011) and Spanish ( ˆAngelo Mendonc¸a, 2011) 4. The parallel corpora contain 4.5M sentences for de-en with 110M words for de and 116M words for en and 15.1M sentences for en-es with 412M words for en and 462M </context>
<context position="19183" citStr="Snover et al., 2006" startWordPosition="3181" endWordPosition="3184">entence Translations: Task 1.1 is about predicting post-editing effort (PEE) and their ranking. The results on the test set are given in Table 5 where QuEst (Shah et al., 2013) SVR lists the baseline system results. Rank lists the overall ranking in the task out of about 10 submissions. We obtain the rankings by sorting according to the predicted scores and randomly assigning ranks in case of ties. RTMs with SVR PLS learning is able to achieve the top rank in this task. Task 1.2: Predicting HTER of Sentence Translations Task 1.2 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006), where case insensitive translation edit rate (TER) scores obtained by TERp (Snover et al., 2009) and their ranking. We derive features over sentences that are true-cased. The results on the test set are given in Table 6 where the ranks are out of about 11 submissions. We are also able to achieve the top ranking in this task. 317 Ranking Translations DeltaAvg r Rank TREE 0.26 -0.41 1 en-es PLS-TREE 0.26 -0.38 2 QuEst SVR 0.14 -0.22 PLS-RR 0.20 -0.35 2 es-en FS-RR 0.19 -0.36 3 QuEst SVR 0.12 -0.21 en-de TREE 0.39 -0.54 1 PLS-TREE 0.33 -0.42 2 QuEst SVR 0.23 -0.34 de-en RR 0.38 -0.51 1 PLS-RR 0</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings ofAssociation for Machine Translation in the Americas,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, adequacy, or hter?: exploring different human judgments with a tunable mt metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09,</booktitle>
<pages>259--268</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="19281" citStr="Snover et al., 2009" startWordPosition="3196" endWordPosition="3199">e results on the test set are given in Table 5 where QuEst (Shah et al., 2013) SVR lists the baseline system results. Rank lists the overall ranking in the task out of about 10 submissions. We obtain the rankings by sorting according to the predicted scores and randomly assigning ranks in case of ties. RTMs with SVR PLS learning is able to achieve the top rank in this task. Task 1.2: Predicting HTER of Sentence Translations Task 1.2 is about predicting HTER (human-targeted translation edit rate) (Snover et al., 2006), where case insensitive translation edit rate (TER) scores obtained by TERp (Snover et al., 2009) and their ranking. We derive features over sentences that are true-cased. The results on the test set are given in Table 6 where the ranks are out of about 11 submissions. We are also able to achieve the top ranking in this task. 317 Ranking Translations DeltaAvg r Rank TREE 0.26 -0.41 1 en-es PLS-TREE 0.26 -0.38 2 QuEst SVR 0.14 -0.22 PLS-RR 0.20 -0.35 2 es-en FS-RR 0.19 -0.36 3 QuEst SVR 0.12 -0.21 en-de TREE 0.39 -0.54 1 PLS-TREE 0.33 -0.42 2 QuEst SVR 0.23 -0.34 de-en RR 0.38 -0.51 1 PLS-RR 0.35 -0.45 2 QuEst SVR 0.21 -0.25 Scoring Translations MAE RMSE Rank TREE 0.49 0.61 1 en-es PLS-TRE</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie J. Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or hter?: exploring different human judgments with a tunable mt metric. In Proceedings of the Fourth Workshop on Statistical Machine Translation, StatMT ’09, pages 259–268, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Marco Turchi</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the sentence-level quality of machine translation systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>28--35</pages>
<publisher>EAMT.</publisher>
<location>Barcelona,</location>
<contexts>
<context position="16803" citStr="Specia et al., 2009" startWordPosition="2779" endWordPosition="2782">3 14866 0.6613 en-es RR 0.6991 21226 15325 0.6817 Table 4: Training performance of the top 2 individual RTM models prepared for different tasks. tremely randomized trees (TREE) (Geurts et al., 2006) as the learning models. TREE is an ensemble learning method over randomized decision trees. These models learn a regression function using the features to estimate a numerical target value. We also use these learning models after a feature subset selection with recursive feature elimination (RFE) (Guyon et al., 2002) or a dimensionality reduction and mapping step using partial least squares (PLS) (Specia et al., 2009), both of which are described in (Bic¸ici et al., 2013). We optimize the learning parameters, the number of features to select, the number of dimensions used for PLS, and the parameters for parallel FDA5. More detailed descriptions of the optimization processes are given in (Bic¸ici et al., 2013; Bic¸ici et al., 2014). We optimize the learning parameters by selecting e close to the standard deviation of the noise in the training set (Bic¸ici, 2013) since the optimal value for e is shown to have linear dependence to the noise level for different noise models (Smola et al., 1998). We select the </context>
</contexts>
<marker>Specia, Cancedda, Dymetman, Turchi, Cristianini, 2009</marker>
<rawString>Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the sentence-level quality of machine translation systems. In Proceedings of the 13th Annual Conference of the European Association for Machine Translation (EAMT), pages 28–35, Barcelona, May. EAMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. Intl. Conf. on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="14061" citStr="Stolcke, 2002" startWordPosition="2311" endWordPosition="2312">., 2014) and the LM corpora provided by LDC for English (Parker et al., 2011) and Spanish ( ˆAngelo Mendonc¸a, 2011) 4. The parallel corpora contain 4.5M sentences for de-en with 110M words for de and 116M words for en and 15.1M sentences for en-es with 412M words for en and 462M words for es. We do not use any resources provided by QET including data, software, or baseline features. Instance selection for the training set and the language model (LM) corpus is handled by parallel FDA5 (Bic¸ici et al., 2014), whose parameters are optimized for each translation task. LM are trained using SRILM (Stolcke, 2002). We tokenize and true-case all of the corpora. The true-caser is trained on all of the available training corpus using Moses (Koehn et al., 2007). Table 2 lists the number of sentences in the training and test sets for each task. For each task or subtask, we select 375 thousand (K) training instances from the available parallel training corpora as interpretants for the individual RTM models using parallel FDA5. We add the selected training set to the 3 million (M) sentences selected from the available monolingual corpora for each LM corpus. The statistics of the training data selected by and </context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proc. Intl. Conf. on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<date>2013</date>
<note>Lix. http://en.wikipedia.org/wiki/LIX.</note>
<contexts>
<context position="10528" citStr="Wikipedia, 2013" startWordPosition="1684" endWordPosition="1685">s achievable according to the n-gram coverage. • Character n-grams 151: Calculates cosine between character n-grams (for n=2,3,4,5,6) obtained for S and T (B¨ar et al., 2012). • Minimum Bayes Retrieval Risk 10, 41: Calculates the translation probability for the translation having the minimum Bayes risk among the retrieved training instances. • Sentence Translation Performance 10, 31: Calculates translation scores obtained according to q(T, R) using BLEU (Papineni et al., 2002), NIST (Doddington, 2002), or F1 (Bic¸ici and Yuret, 2011b) for q. • LIX 11, 11: Calculates the LIX readability score (Wikipedia, 2013; Bj¨ornsson, 1968) for Sand T. 1 For Task 1.1, we have additionally used comparative BLEU, NIST, and F1 scores as additional features, which are obtained by comparing the translations with each other and averaging the result (Bic¸ici, 2011). 2.3 Bracketing Tree Structural Features We use the parse tree outputs obtained by CCL to derive features based on the bracketing structure. We derive 5 statistics based on the geometric properties of the parse trees: number of brackets used (numB), depth (depthB), average depth (avg depthB), number of brackets on the right branches over the number of brac</context>
</contexts>
<marker>Wikipedia, 2013</marker>
<rawString>Wikipedia. 2013. Lix. http://en.wikipedia.org/wiki/LIX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>Denise DiPersio ˆAngelo Mendonc¸a, Daniel Jaquette.</title>
<date>2011</date>
<marker>Graff, 2011</marker>
<rawString>David Graff Denise DiPersio ˆAngelo Mendonc¸a, Daniel Jaquette. 2011. Spanish Gigaword third edition, Linguistic Data Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
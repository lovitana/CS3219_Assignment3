<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000013">
<title confidence="0.974535">
Comparison of different feature sets for identification of variants in
progressive aphasia
</title>
<author confidence="0.939948">
Kathleen C. Fraser1, Graeme Hirst1, Naida L. Graham2, Jed A. Meltzer3,
Sandra E. Black4, and Elizabeth Rochon2
</author>
<affiliation confidence="0.995505">
1Dept. of Computer Science, University of Toronto
2Dept. of Speech-Language Pathology, University of Toronto, &amp; Toronto Rehabilitation Institute
3Rotman Research Institute, Baycrest Centre, Toronto
</affiliation>
<address confidence="0.877054">
4LC Campbell Cognitive Neurology Research Unit, Sunnybrook Health Sciences Centre, Toronto
</address>
<email confidence="0.9833895">
{kfraser,gh}@cs.toronto.edu, {naida.graham,elizabeth.rochon}@utoronto.ca
jmeltzer@research.baycrest.org, sandra.black@sunnybrook.ca
</email>
<sectionHeader confidence="0.993801" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986666666667">
We use computational techniques to ex-
tract a large number of different features
from the narrative speech of individuals
with primary progressive aphasia (PPA).
We examine several different types of fea-
tures, including part-of-speech, complex-
ity, context-free grammar, fluency, psy-
cholinguistic, vocabulary richness, and
acoustic, and discuss the circumstances
under which they can be extracted. We
consider the task of training a machine
learning classifier to determine whether a
participant is a control, or has the fluent or
nonfluent variant of PPA. We first evaluate
the individual feature sets on their classifi-
cation accuracy, then perform an ablation
study to determine the optimal combina-
tion of feature sets. Finally, we rank the
features in four practical scenarios: given
audio data only, given unsegmented tran-
scripts only, given segmented transcripts
only, and given both audio and segmented
transcripts. We find that psycholinguis-
tic features are highly discriminative in
most cases, and that acoustic, context-free
grammar, and part-of-speech features can
also be important in some circumstances.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963296296296">
In some types of dementia, such as primary pro-
gressive aphasia, language deficit is a core symp-
tom, and the analysis of narrative or conversa-
tional speech is important for assessing the extent
of an individual’s language impairment. Analy-
sis of connected speech has been limited in the
past because it is time-consuming and requires ex-
pert annotation. However, studies have shown that
it is possible for machine learning classifiers to
achieve high accuracy on some diagnostic tasks
when trained on features which were automati-
cally extracted from speech transcripts.
In this paper, we summarize previous research
on the automatic analysis of speech samples from
individuals with dementia, focusing in particular
on primary progressive aphasia. We discuss in de-
tail different types of features and compare their
effectiveness in the classification task. We sug-
gest some benefits and drawbacks of these differ-
ent features. We also examine the interactions be-
tween different feature sets, and discuss the rela-
tive importance of individual features across fea-
ture sets. Because we examine a large number
of features on a relatively small data set, we em-
phasize that this work is exploratory in nature;
nonetheless, our results are consistent with, and
extend, previous work in the field.
</bodyText>
<sectionHeader confidence="0.97986" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999903666666667">
In recent years, there has been growing interest in
using computer techniques to automatically detect
dementia from speech and language features de-
rived from a sample of narrative speech. Some re-
searchers have explored ways to use methods such
as part-of-speech tagging, statistical parsing, and
speech signal analysis to detect disorders such as
dementia of the Alzheimer’s type (DAT) (Bucks et
al., 2000; Singh et al., 2001; Thomas et al., 2005;
Jarrold et al., 2010) and mild cognitive impairment
(MCI) (Roark et al., 2011).
Here, we focus on a type of dementia called
primary progressive aphasia (PPA). PPA is a sub-
type of frontotemporal dementia (FTD) which is
characterized by progressive language impairment
without other notable cognitive impairment. There
are three subtypes of PPA: semantic dementia
(SD), progressive nonfluent aphasia (PNFA), and
logopenic progressive aphasia (LPA). SD, some-
times called “fluent” progressive aphasia, is typi-
cally marked by fluent but empty speech, anomia,
</bodyText>
<page confidence="0.991603">
17
</page>
<note confidence="0.3535895">
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 17–26,
Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999547627450981">
deficits in comprehension, and spared grammar
and syntax (Gorno-Tempini et al., 2011). In
contrast, PNFA is characterized by halting and
sometimes agrammatic speech, reduced syntac-
tic complexity, word-finding difficulties, and rela-
tively spared single-word comprehension (Gorno-
Tempini et al., 2011). The third subtype, LPA, is
characterized by slow speech and frequent word
finding difficulties; this subtype is not included in
the current analysis.
Although clear diagnostic criteria for PPA have
been established (Gorno-Tempini et al., 2011),
there is no one test which can provide a diagno-
sis. Classification of PPA into subtypes requires
evaluation of spoken output, as well as neuropsy-
chological assessment and brain imaging. Quali-
tative evaluation of speech often can be done accu-
rately by clinicians or researchers, but the ability
to do this evaluation can require years of training
and experience. Some researchers have performed
detailed quantitative characterization of speech in
PPA, but the precise characteristics of speech are
not yet fully understood and this process is too
time-consuming for most clinicians.
Peintner et al. (2008) conducted one of the earli-
est automatic analyses of speech from individuals
with FTD, including SD and PNFA as well as a
behavioural variant. They considered psycholin-
guistic features as well as phoneme duration fea-
tures extracted from the audio samples. Although
they were fairly successful in classifying partici-
pants according to their subtype, they did not re-
port many details regarding the specific features
which were useful or how those features might re-
flect the underlying impairment of the speakers.
Pakhomov et al. (2010a) examined FTD speech
from an information-theoretic approach. They
constructed a language model of healthy control
speech, and then calculated the perplexity and out-
of-vocabulary rate for each of the patient groups
relative to that model. In another study, Pakhomov
et al. (2010b) extracted speech and language fea-
tures from samples of FTD speech. In a principal
components analysis, they discovered four com-
ponents which accounted for most of the variance
in their data: speech length, hesitancy, empty con-
tent, and grammaticality. However, they did not
perform any classification experiments.
Fraser et al. (2013a) attempted to classify par-
ticipants as either SD patients, PNFA patients, or
healthy controls using a large number of language
</bodyText>
<equation confidence="0.6852435">
SD PNFA Control
(N = 11) (N = 13) (N = 16)
</equation>
<table confidence="0.994674">
Male/Female 8/3 7/6 9/7
Age (yrs) 65.9 (7.1) 64.5 (10.4) 67.8 (8.2)
Education (yrs) 17.5 (5.8) 14.0 (3.5) 16.8 (4.3)
</table>
<tableCaption confidence="0.999111">
Table 1: Demographic information. Numbers are
</tableCaption>
<bodyText confidence="0.973080636363636">
given in the form: mean (standard deviation).
features extracted from manually-transcribed tran-
scripts. They distinguished between SD and con-
trol participants with very high accuracy, and were
also successful at distinguishing between PNFA
and control participants. However, their method
did not perform as well on the task of classify-
ing SD vs. PNFA speakers. In subsequent work
(Fraser et al., 2013b), they expanded their feature
set to include acoustic features extracted directly
from the audio file.
</bodyText>
<sectionHeader confidence="0.998258" genericHeader="method">
3 Methods
</sectionHeader>
<subsectionHeader confidence="0.991822">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.9999705">
Twenty-four patients with PPA were recruited
through three Toronto memory clinics, and 16 age-
and education-matched healthy controls were re-
cruited through a volunteer pool. All participants
were native speakers of English, or had completed
some of their education in English. Exclusion cri-
teria included a known history of drug or alcohol
abuse and a history of neurological or major psy-
chiatric illness. Each patient was diagnosed by a
behavioural neurologist and all met current crite-
ria for PPA (Gorno-Tempini et al., 2011). Table 1
shows demographic information for each group.
To elicit a sample of narrative speech, partici-
pants were asked to tell the well-known story of
Cinderella. They were given a wordless picture
book to remind them of the story; then the book
was removed and they were asked to tell the story
in their own words. This procedure, described in
full by Saffran et al. (1989), is commonly used in
studies of connected speech in aphasia.
The narrative samples were transcribed by
trained research assistants. The transcriptions in-
clude filled pauses, repetitions, and false starts,
and were annotated with the total speech time.
Sentence boundaries were marked according to se-
mantic, syntactic, and prosodic cues.
</bodyText>
<subsectionHeader confidence="0.995687">
3.2 Classification framework
</subsectionHeader>
<bodyText confidence="0.9866525">
Given the audio files and transcripts, we can then
calculate our features (described in detail below)
</bodyText>
<page confidence="0.998504">
18
</page>
<bodyText confidence="0.9998018">
and use them to train a support vector machine
(SVM) classifier. We use a leave-one-out cross-
validation framework and report the average ac-
curacy (i.e. proportion of correctly classified in-
stances) across folds. We optimize the complexity
parameter and the kernel type in a nested cross-
validation loop over the training set. For compar-
ison, we also tested a naive Bayes classifier; how-
ever we found that the results were consistently
poorer and we do not report them here.
</bodyText>
<subsectionHeader confidence="0.985191">
3.3 Features
</subsectionHeader>
<bodyText confidence="0.999914875">
In the following sections we will describe each of
the feature sets that we use and explain how the
features are computed, and we will discuss some
of the potential advantages and disadvantages as-
sociated with each set. In particular, we discuss
what types of data are necessary for the extraction
of these features. The data types are: unsegmented
transcripts, segmented transcripts, and audio.
</bodyText>
<subsectionHeader confidence="0.888352">
3.3.1 Part-of-speech features
</subsectionHeader>
<bodyText confidence="0.999688333333334">
Different categories of words may be selectively
impaired in different types of dementia. In PPA,
individuals with SD tend to be more impaired
with respect to nouns than verbs, and may replace
nouns with pronouns or circumlocutory phrases.
In contrast, individuals with PNFA may have more
difficulty with verbs and may even demonstrate
agrammatism, which can result in the omission
of grammatical morphemes and function words.
Thus, it is often useful to compare the relative fre-
quencies with which words representing the differ-
ent parts-of-speech (POS) are produced in a sam-
ple, as in Table 2. Similar features have been re-
ported in computational studies of MCI (Roark et
al., 2011), FTD (Pakhomov et al., 2010b), and
DAT (Bucks et al., 2000). Numerous POS taggers
exist, although we use the Stanford tagger here
(Toutanova et al., 2003).
</bodyText>
<subsectionHeader confidence="0.96504">
3.3.2 Complexity features
</subsectionHeader>
<bodyText confidence="0.9976643">
Changes in linguistic complexity may accompany
the onset of dementia, although some studies have
found a decrease in complexity (e.g. Kemper et al.
(2001)) while others have found an increase (e.g.
Le et al. (2011)).
The features in Table 3 vary in their ease of
computation. Mean word length can be calculated
from an unsegmented transcript, while mean sen-
tence length requires only sentence boundary seg-
mentation. Other measures, such as Yngve depth
</bodyText>
<table confidence="0.890278">
Nouns # nouns / # words
Verbs # verbs / # words
Noun-verb ratio # nouns / # verbs
Noun ratio # nouns / (# nouns + # verbs)
Inflected verbs # inflected verbs / # verbs
Determiners # determiners / # words
Demonstratives # demonstratives / # words
Prepositions # prepositions / # words
Adjectives # adjectives / # words
Adverbs # adverbs / # words
Pronoun ratio # pronouns / (# nouns + # pronouns)
Function words # function words / # words
Interjections # interjections / # words
</table>
<tableCaption confidence="0.993429">
Table 2: Part-of-speech features.
</tableCaption>
<figureCaption confidence="0.81119225">
Max depth maximum Yngve depth of each parse tree,
averaged over all sentences
Mean depth mean Yngve depth of each node in the
parse tree, averaged over all sentences
</figureCaption>
<bodyText confidence="0.8412469375">
Total depth total sum of the Yngve depths of each node
in the parse tree, averaged over all sentences
Tree height height of each parse tree, averaged over
all sentences
MLS mean length of sentence
MLC mean length of clause
MLT mean length of T-unit
Subordinate conjunctions number of subordinate
conjunctions
Coordinate conjunctions number of coordinate con-
junctions
Subordinate:coordinate ratio ratio of number of sub-
ordinate conjunctions to number of coordinate
conjunctions
Mean word length mean length, in letters, of each
word in the sample
</bodyText>
<tableCaption confidence="0.985491">
Table 3: Complexity features.
</tableCaption>
<bodyText confidence="0.999575">
(Yngve, 1960), require full parses of the sentences
(we use the Stanford parser (Klein and Manning,
2003) and Lu’s Syntactic Complexity Analyzer
(Lu, 2010)).
</bodyText>
<subsectionHeader confidence="0.862428">
3.3.3 CFG features
</subsectionHeader>
<bodyText confidence="0.999906466666667">
Although many of the complexity features above
are derived from parse trees, in this section we
present a set of features that take into account
the context-free grammar (CFG) labels on each
of the nodes. CFG features have been previously
used to assess the grammaticality of sentences in
an artificial error corpus (Wong and Dras, 2010)
and to distinguish human from machine transla-
tions (Chae and Nenkova, 2009). However, this
is the first time such features have been applied to
speech from participants with dementia.
In Table 4 we list a few examples of our 134
CFG features, as well as the three phrase-level fea-
tures (calculated for noun phrases, verb phrases,
and prepositional phrases).
</bodyText>
<page confidence="0.983902">
19
</page>
<bodyText confidence="0.8902265">
NP → NNS Noun phrases consisting of only a plural
noun
VP → VBN PP Verb phrases consisting of a past-
participle verb and a prepositional phrase
ROOT → INTJ Trees consisting of only an interjec-
tion
Phrase type proportion Length of each phrase type
(noun phrase, verb phrase, or prepositional
phrase), divided by total narrative length
Average phrase type length Total number of words in
a phrase type, divided by the number of phrases
of that type
Phrase type rate Number of phrases of a given type,
divided by total narrative length
</bodyText>
<tableCaption confidence="0.90116125">
Table 4: CFG features.
Um Frequency of filled pause um
Uh Frequency of filled pause uh
NID Frequency of words Not In Dictionary (e.g. para-
phasias, neologisms)
Verbal rate Number of words per minute
Total words Total number of words produced
Table 5: Fluency features.
</tableCaption>
<subsectionHeader confidence="0.794007">
3.3.4 Fluency features
</subsectionHeader>
<bodyText confidence="0.999987777777777">
Park et al. (2011) found that listeners’ judgements
of fluency were affected by a number of different
variables, and the three most discriminative fea-
tures were “speech rate, speech productivity, and
audible struggle.” For our list of fluency features
(Table 5), we include only those features which
could be extracted from the transcripts alone (as-
suming the total speech time is given). We count
pauses filled by um and uh separately, as research
has suggested that they may indicate different cog-
nitive processes (Clark and Fox Tree, 2002).
The number of words in a sample could be eas-
ily generated using the word count feature in most
text-editing software (although we first exclude
filled pauses and NID tokens), and the verbal rate
can subsequently be calculated directly. The other
three features are easily calculated using string
matching and an electronic dictionary.
</bodyText>
<subsectionHeader confidence="0.847419">
3.3.5 Psycholinguistic features
</subsectionHeader>
<bodyText confidence="0.9953652">
Some types of dementia are characterized by im-
pairments in semantic access. Such impairments
may be sensitive to psycholinguistic features such
as lexical frequency, familiarity, imageability, and
age of acquisition (Table 6). We use the SUBTL
frequency norms (Brysbaert and New, 2009) and
the combined Bristol and Gilhooly-Logie norms
(Stadthagen-Gonzalez and Davis, 2006; Gilhooly
and Logie, 1980) for familiarity, imageability, and
Frequency Frequency with which a word occurs in
some corpus of natural language
Familiarity Subjective rating of how familiar a word
seems
Imageability Subjective rating of how easily a word
generates an image in the mind
Age of acquisition Subjective rating of how old a per-
son is when they first learn that word
Light verbs Number of occurrences of be, have, come,
go, give, take, make, do, get, move, and put,
normalized by total number of verbs
</bodyText>
<tableCaption confidence="0.919964">
Table 6: Psycholinguistic features.
</tableCaption>
<bodyText confidence="0.9999318">
age of acquisition (see Table 6). We compute the
average of each of these measures for all content
words, as well as for nouns and verbs separately.
Another measure that fits into this category is
the frequency of occurrence of light verbs, which
an impaired speaker may use to replace a more
specific verb. We use the same list of light verbs
as Breedin et al. (1998), given in Table 6.
One challenge associated with psycholinguis-
tic features is finding norms which provide ade-
quate coverage for the given data. Fraser et al.
(2013a) reported that the SUBTL frequency norms
had a coverage of above 90% on their data, but the
Bristol-Gilhooly-Logie norms had a coverage of
only around 30%.
</bodyText>
<subsectionHeader confidence="0.838251">
3.3.6 Vocabulary richness features
</subsectionHeader>
<bodyText confidence="0.999961782608696">
Individuals experiencing semantic difficulty may
use a limited range of vocabulary. We can mea-
sure the vocabulary richness or lexical diversity
of a narrative sample using a number of different
metrics (see Table 7). Type-token ratio has been
a popular choice, perhaps due to its simplicity;
however it is sensitive to the length of the sample.
Bucks et al. (2000) were the first to apply Honor´e’s
statistic and Brun´et’s index to the study of demen-
tia, and found significant differences between in-
dividuals with DAT and healthy controls. Cov-
ington and McFall (2010) proposed a new mea-
sure called the moving-average type-token ratio
(MATTR), which is independent of text length.
This feature was later applied to aphasic speech in
a study by Fergadiotis and Wright (2011), and was
found to be one of the most unbiased indicators of
lexical diversity in impaired speakers.
The measures given in Table 7 are easily com-
puted from their respective formulae. In this work,
we lemmatize each word using NLTK (Bird et
al., 2009) before calculating the features. For
MATTR, we consider w = 10,20,30,40,50.
</bodyText>
<page confidence="0.97371">
20
</page>
<bodyText confidence="0.98005625">
Honor´e’s statistic NV−0.165/ where V is the number of
word types and N is the number of word tokens.
Brun´et’s index 100logN/(1−V1/V) where V1 is the
number of words used only once, V is the total
number of word types, and N is the number of
word tokens.
Type-token ratio (TTR) V/N where V is the num-
ber of word types and N is the number of word
tokens.
Moving-average type-token ratio (MATTRw) TTR
calculated over a moving window of size w,
and averaged over all windows.
</bodyText>
<tableCaption confidence="0.987699">
Table 7: Vocabulary richness features.
</tableCaption>
<subsectionHeader confidence="0.817786">
3.3.7 Acoustic features
</subsectionHeader>
<bodyText confidence="0.999825933333333">
What we call acoustic features are extracted di-
rectly from the audio file. We consider the fea-
tures given in Table 8. Acoustic features have been
shown to be useful when automatically detecting
conditions such as Parkinson’s disease, in which
changes in speech are common (Little et al., 2009;
Tsanas et al., 2012). Acoustic features have also
been examined in studies of DAT (Meil´an et al.,
2014), FTD (Pakhomov et al., 2010b), and PPA
(Fraser et al., 2013b, whose software we use here).
An obvious benefit to acoustic features is that
they do not require a transcription, and can be cal-
culated immediately given an audio sample. The
corresponding drawback is that they tell us noth-
ing about the linguistic content of the narrative.
</bodyText>
<sectionHeader confidence="0.999735" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999992">
We report the results of three experiments explor-
ing the discriminative power of the different fea-
tures. We first compare the classification accura-
cies using each individual feature set. We then per-
form an ablation study to determine which com-
bination of feature sets leads to the highest clas-
sification accuracy. We also look at individual
features across sets and discuss which ones are
the most discriminative, particularly in situations
where data might be limited.
</bodyText>
<subsectionHeader confidence="0.973445">
4.1 Individual comparison of accuracy by set
</subsectionHeader>
<bodyText confidence="0.999535">
The accuracies which result from using each fea-
ture set individually are given in Table 9. The
highest accuracy across the three tasks is achieved
in distinguishing SD participants from controls.
An accuracy of .963 can be achieved using all
the features together, or using the psycholinguis-
tic or POS features alone. This is consistent with
the semantic impairments that are observed in SD.
</bodyText>
<table confidence="0.8524228">
Total duration of speech Total length of all non-silent
segments
Phonation rate Total duration of speech / total dura-
tion of the sample (including pauses)
Mean pause duration Mean length of pauses &gt; 0.15
ms
Short pause count # Pauses &gt; 0.15 ms and &lt; 0.4 ms
Long pause count # Pauses &gt; 0.4 ms
Pause:word ratio Ratio of silent segments longer than
150 ms to non-silent segments
F0:3 mean Mean of the fundamental frequency and the
first three formant frequencies
F0:3 variance Variance of the fundamental frequency
and the first three formant frequencies
Mean instantaneous power Measure related to the
loudness of the signal
Mean 1st ACF Mean first autocorrelation function
Max 1st ACF Maximum first autocorrelation function
Skewness Measure of lack of symmetry, associated
with tense or creaky voice
Kurtosis Measure of the peakedness of the signal
ZCR Zero-crossing rate, can be used to distinguish
between voiced and unvoiced regions
MRPDE Mean recurrence period density entropy, a
measure of periodicity
</table>
<tableCaption confidence="0.8976296">
Jitter Measure of the short-term variation in the pitch
(frequency) of a voice
Shimmer Measure of the short-term variation in the
loudness (amplitude) of a voice
Table 8: Acoustic features.
</tableCaption>
<bodyText confidence="0.999960538461538">
The measures of vocabulary richness do not distin-
guish between the SD and control groups, suggest-
ing it is the words themselves, and not the number
of different words being used, that is important.
In the case of PNFA participants vs. controls,
we find that the highest accuracy is achieved us-
ing all the features, and the second highest by us-
ing only acoustic features. This is not surprising,
considering that the acoustic features include mea-
sures of pausing and phonation rate, which can
detect the characteristic halting speech of PNFA.
The third best accuracy is achieved using the flu-
ency features, which also fits with this explana-
tion. However, we might have expected that the
complexity and CFG features would be more sen-
sitive to the grammatical impairments of PNFA.
Finally, the best accuracy for SD vs. PNFA
is lower than in the previous two cases, and is
achieved using only CFG features. This sug-
gests that there are some grammatical construc-
tions which occur with different frequencies in
the two groups. These differences do not appear
to be captured by the complexity features, which
could explain why Fraser et al. (2013a) did not find
syntactic differences between the SD and PNFA
groups. Interestingly, the results using CFG fea-
</bodyText>
<page confidence="0.998864">
21
</page>
<table confidence="0.98452925">
Feature set SD vs. PNFA vs. SD vs.
controls controls PNFA
All .963 .931 .708
Acoustic .778 .862 .167
Psycholinguistic .963 .724 .708
POS .963 .690 .375
Complexity .852 .621 .667
Fluency .667 .828 .500
Vocab. richness .481 .586 .583
CFG .630 .690 .792
(a) SD vs. controls.
Removed Remaining Features Accuracy
A+P+POS+C+F+VR+CFG .963
F A+P+POS+C+VR+CFG .963
A P+POS+C+VR+CFG 1.00
VR P+POS+C+CFG .926
CFG P+POS+C .926
C P+POS 1.00
POS P .963
(b) PNFA vs. controls.
</table>
<tableCaption confidence="0.770118">
Table 9: Classification accuracies for each feature
set individually using a SVM classifier. Bold indi-
cates the highest accuracy for each task.
</tableCaption>
<bodyText confidence="0.89518925">
tures are actually higher than the results using all
features. This demonstrates that classifier perfor-
mance can be adversely affected by the presence
of irrelevant features, especially in small data sets.
</bodyText>
<subsectionHeader confidence="0.997395">
4.2 Combining feature sets
</subsectionHeader>
<bodyText confidence="0.999449848484849">
In the previous section we examined the feature
sets individually; however, one type of feature
may complement the information contained in an-
other feature set, or it may contain redundant in-
formation. To examine the interactions between
the feature sets, we perform an ablation study.
Starting with all the features, we remove each fea-
ture set one at a time and measure the accuracy
of the classifier. The feature set whose removal
causes the smallest decrease in accuracy is then re-
moved permanently from the experiment, the rea-
soning being that the most important feature sets
will cause the greatest decrease in accuracy when
removed. In some cases, we observe that the clas-
sification accuracy actually increases when a set
is removed, which suggests that those features are
not relevant to the classification (at least in combi-
nation with the other sets). In the case of a tie, we
remove the feature set whose individual classifica-
tion accuracy on that task is lowest. The procedure
is then repeated on the remaining feature sets, con-
tinuing until only one set remains.
The results for SD vs. controls are given in Ta-
ble 10a. The best result, 1.00, is achieved by
combining the psycholinguistic and POS features.
This is unsurprising, since each of those feature
sets perform well individually. Curiously, the
same result can also be achieved by also including
the complexity, vocabulary richness, and CFG fea-
tures, but not in the intermediate stages between
those two optimal sets. We attribute this to the in-
teractions between features and the small data set.
For PNFA vs. controls, shown in Table 10b, the
</bodyText>
<table confidence="0.635186764705883">
Removed Remaining Features Accuracy
A+P+POS+C+F+VR+CFG .931
VR A+P+POS+C+F+CFG .931
C A+P+POS+F+CFG .931
POS A+P+F+CFG .931
CFG A+P+F .966
F A+P .966
P A .862
(c) SD vs. PNFA.
Removed Remaining Features Accuracy
A+P+POS+C+F+VR+CFG .708
POS A+P+C+F+VR+CFG .750
VR A+P+C+F+CFG .833
F A+P+C+CFG .833
A P+C+CFG .792
C P+CFG .917
P CFG .792
</table>
<tableCaption confidence="0.758902">
Table 10: A=acoustic, P=psycholinguistic,
</tableCaption>
<bodyText confidence="0.974353227272727">
POS=part-of-speech, C=complexity, F=fluency,
VR=vocabulary richness, CFG=CFG production
rule features. Bold indicates the highest accuracy
with the fewest feature sets.
best result of .966 is achieved using a combina-
tion of acoustic and psycholinguistic features. In
this case the removal of the fluency features, which
gave the second highest individual accuracy, does
not make a difference to the accuracy. This sug-
gests that the fluency features contain similar in-
formation to one of the remaining sets, presum-
ably the acoustic set.
In the case of SD vs. PNFA, we again see that
the best accuracy can be achieved by combining
two feature sets, as shown in Table 10c. Us-
ing psycholinguistic and CFG features, we can
achieve an accuracy of .917, a substantial im-
provement over the best accuracy for this task in
Table 9. In fact, in all three cases we see that us-
ing a carefully selected combination of feature sets
can result in better accuracy than using all the fea-
ture sets together or using any one set individually.
</bodyText>
<subsectionHeader confidence="0.992165">
4.3 Best features for incomplete data
</subsectionHeader>
<bodyText confidence="0.985571">
Up to this point, we have examined complete fea-
ture sets. We now briefly explore which individual
</bodyText>
<page confidence="0.991439">
22
</page>
<bodyText confidence="0.999831">
features are the most discriminative across feature
sets. We approach this as a practical consideration:
given the data that a researcher has, and limited re-
sources, what are the best features to measure? We
consider the following four scenarios:
</bodyText>
<listItem confidence="0.9918668">
1. Given audio files only. This scenario often
arises because it is relatively easy to record
speech, but difficult to have it transcribed.
Only acoustic features can be extracted.
2. Given basic transcriptions only (no audio).
We assume there is no sentence segmentation
and the time is not marked (e.g. as in the out-
put of automatic speech recognition). Thus,
we can measure psycholinguistic, POS, and
vocabulary measures. We can also measure
the fluency features except for verbal rate,
as well as mean word length and subordi-
nate/coordinate conjunctions from the com-
plexity set. Without sentence boundaries, we
cannot parse the transcripts.
3. Given fully segmented transcripts (no audio).
We can measure all features except for acous-
tic features.
4. Given audio and fully segmented transcripts.
We can measure all features.
</listItem>
<bodyText confidence="0.999793568965518">
For each scenario, we rank the available fea-
tures by their X2 value and choose the top 10 only
as input to the SVM classifier (see Manning et al.
(2008) for a complete explanation of X2 feature se-
lection). We only include features if X2 &gt; 0, so in
cases where there are very few relevant features,
fewer than 10 features may be selected. Because
we perform cross-validation, the selected features
may vary across different folds. In the tables that
follow, we present the features ranked by the num-
ber of folds in which they appear (i.e. a feature
with the value 1.00 was selected in every fold).
Due to space constraints, only the top 10 ranked
features are shown.
The results for Scenario 1 are given in Ta-
ble 11a. For the SD vs. controls and PNFA vs.
controls, the most highly ranked features tend to
be related to fluency and rate of speech, as well
as voice quality (skewness and MRPDE). How-
ever, when distinguishing between the two patient
groups, the acoustic features are essentially use-
less. In most cases, we see that none of the acous-
tic features had a non-zero X2 value, and thus the
classifier could not be properly trained.
For Scenario 2 (Table 11b), the results for SD
vs. controls show that within the psycholinguistic
and POS feature sets, features relating to familiar-
ity and frequency are very important, as well as
nouns and demonstratives. In the PNFA vs. con-
trols case, we see that a number of the vocabulary
richness features are selected, which is in contrast
to the previous two experiments. However, it ap-
pears that only the MATTR feature is important
(with varying window lengths), so when we con-
sidered only full feature sets, that information was
obscured by the other, irrelevant features in that
set. The SD vs. PNFA case shows a mix of fea-
tures from the previous two cases.
For Scenario 3 (Table 11c), we add the com-
plexity and CFG features. These features do not
have a large effect in the SD vs. controls case, but
a few CFG features are selected in the PNFA vs.
controls and SD vs. PNFA cases.
In Scenario 4 (Table 11d), we consider all fea-
tures. In the SD vs. controls case this increases
the accuracy. However, for PNFA vs. controls and
SD vs. PNFA, the classification accuracy actually
decreases, relative to Scenario 3. When the num-
ber of features increases, the potential to overfit to
the training data fold also increases, and it seems
likely that that is occurring here. Nonetheless, we
expect that the features which are selected in every
fold are still highly relevant. These features are
unchanged between Scenarios 3 and 4 in the SD
vs. controls and SD vs. PNFA case, however in the
PNFA vs. controls case, the acoustic features are
now ranked more highly than some of the vocabu-
lary richness and CFG features from Scenario 3.
</bodyText>
<sectionHeader confidence="0.998277" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999976">
While it may be tempting to calculate as many
features as possible and use them all in a classi-
fier, we have shown here that better results can be
achieved by choosing a small, relevant subset of
features. In particular, psycholinguistic features
such as frequency and familiarity were useful in all
three classification tasks. Acoustic features were
useful in discriminating patients from controls, but
not for discriminating between the two PPA sub-
types. We also found that MATTR was relevant
in some cases, although the other vocabulary rich-
ness features were not, and that the CFG features
were more useful than traditional measures of syn-
tactic complexity. POS features were useful only
in distinguishing between SD and controls.
One of the biggest challenges in this type
of work is the small amount of data available.
</bodyText>
<page confidence="0.99167">
23
</page>
<figure confidence="0.972277466666667">
(a) Scenario 1: audio only.
SD vs. control, Acc:.852 PNFA vs. control, Acc:.793 SD vs. PNFA, Acc:.500
1.00 skewness 1.00 long pause count .083 max 1st ACF
1.00 phonation rate 1.00 phonation rate .042 mean F3
1.00 MRPDE 1.00 short pause count
1.00 mean duration of pauses 1.00 MRPDE
.037 long pause count 1.00 mean duration of pauses
.037 mean 1st ACF .966 pause:word ratio
.037 kurtosis .793 skewness
.793 ZCR
.345 mean inst. power
.035 jitter
(b) Scenario 2: unsegmented transcripts.
SD vs. control, Acc:.926 PNFA vs. control, Acc:.621 SD vs. PNFA, Acc:.792
(c) Scenario 3: segmented transcripts.
</figure>
<table confidence="0.872766434782609">
SD vs. control, Acc:.926 PNFA vs. control, Acc:.897 SD vs. PNFA, Acc:.792
1.00 word length 1.00 MATTR 50 1.00 WHADVP → WRB
1.00 familiarity 1.00 MATTR 40 1.00 familiarity
1.00 noun frequency 1.00 WHNP → WP 1.00 noun familiarity
1.00 noun familiarity 1.00 frequency 1.00 noun frequency
1.00 frequency 1.00 MATTR 20 1.00 MATTR 20
1.00 demonstratives 1.00 verbal rate 1.00 NP → NNS
.889 nouns .966 MATTR 30 1.00 SBAR → WHADVP S
.852 verb frequency .827 S1 → INTJ .667 MATTR 10
.630 MLS .483 total words .500 NP → DT JJ NNS
.630 total Yngve depth .414 word length .458 SQ → AUX NP VP
(d) Scenario 4: segmented transcripts + audio.
SD vs. control, Acc:.963 PNFA vs. control, Acc:.793 SD vs. PNFA, Acc:.750
1.00 word length 1.00 frequency 1.00 WHADVP → WRB
1.00 familiarity 1.00 phonation rate 1.00 familiarity
1.00 noun frequency 1.00 MRPDE 1.00 noun familiarity
1.00 noun familiarity 1.00 verbal rate 1.00 noun frequency
1.00 frequency 1.00 mean duration of pauses 1.00 MATTR 20
1.00 demonstratives .897 MATTR 50 1.00 NP → NNS
.963 phonation rate .897 WHNP → WP 1.00 SBAR → WHADVP S
.741 verb frequency .897 MATTR 20 .625 MATTR 10
.593 nouns .690 MATTR 40 .500 NP → DT JJ NNS
.333 MLS .690 MATTR 30 .458 SQ → AUX NP VP
</table>
<tableCaption confidence="0.999357">
Table 11: Classification accuracies and top 10 features for four different data scenarios.
</tableCaption>
<figure confidence="0.96899755">
1.00 familiarity 1.00
1.00 noun frequency 1.00
1.00 noun familiarity 1.00
1.00 frequency 1.00
1.00 verb frequency 1.00
1.00 nouns .931
1.00 demonstratives .759
.778 pronoun ratio .690
.667 noun imageability .241
.630 Honor´e’s statistic .241
MATTR 50 1.00 familiarity
MATTR 40 1.00 noun frequency
MATTR 30 1.00 noun familiarity
frequency 1.00 MATTR 20
MATTR 20 .708 MATTR 10
total words .208 MATTR 30
light verbs .042 MATTR 50
adjectives .042 MATTR 40
age of acquisition .042 light verbs
MATTR 10 .042 verbs
</figure>
<bodyText confidence="0.999464">
Psychological studies are typically on the or-
der of only tens to possibly hundreds of partic-
ipants, while machine learning researchers often
tackle problems with thousands to millions of data
points. We have chosen techniques appropriate for
small data sets, but acknowledging the potential
weaknesses of machine learning methods when
training data are limited, these findings must be
considered preliminary. However, we also believe
that this is a promising approach for future ap-
plications, including automated screening for lan-
guage impairment, support for clinical diagnosis,
tracking severity of symptoms over time, and eval-
uating therapeutic interventions.
</bodyText>
<sectionHeader confidence="0.9979" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<footnote confidence="0.8190505">
This research was supported by the Natural Sciences and En-
gineering Research Council of Canada and the Canadian In-
stitutes of Health Research (grant #MOP-8277). Thanks to
Frank Rudzicz for the acoustic features software.
</footnote>
<page confidence="0.998955">
24
</page>
<sectionHeader confidence="0.989679" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999579596638655">
Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural
language processing with Python. O’Reilly Media, Inc.
Sarah D. Breedin, Eleanor M. Saffran, and Myrna F.
Schwartz. 1998. Semantic factors in verb retrieval: An
effect of complexity. Brain and Language, 63:1–31.
Marc Brysbaert and Boris New. 2009. Moving beyond
Kuˇcera and Francis: A critical evaluation of current word
frequency norms and the introduction of a new and im-
proved word frequency measure for American English.
Behavior Research Methods, 41(4):977–990.
R.S. Bucks, S. Singh, J.M. Cuerden, and G.K. Wilcock.
2000. Analysis of spontaneous, conversational speech in
dementia of Alzheimer type: Evaluation of an objective
technique for analysing lexical performance. Aphasiol-
ogy, 14(1):71–91.
Jieun Chae and Ani Nenkova. 2009. Predicting the fluency
of text with shallow structural features: case studies of ma-
chine translation and human-written text. In Proceedings
of the 12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 139–147.
Association for Computational Linguistics.
Herbert H. Clark and Jean E. Fox Tree. 2002. Using uh and
um in spontaneous speaking. Cognition, 84(1):73–111.
Michael A. Covington and Joe D. McFall. 2010. Cutting
the Gordian knot: The moving-average type–token ratio
(MATTR). Journal of Quantitative Linguistics, 17(2):94–
100.
Gerasimos Fergadiotis and Heather Harris Wright. 2011.
Lexical diversity for adults with and without apha-
sia across discourse elicitation tasks. Aphasiology,
25(11):1414–1430.
Kathleen C. Fraser, Jed A. Meltzer, Naida L. Graham, Carol
Leonard, Graeme Hirst, Sandra E. Black, and Elizabeth
Rochon. 2013a. Automated classification of primary
progressive aphasia subtypes from narrative speech tran-
scripts. Cortex.
Kathleen C. Fraser, Frank Rudzicz, and Elizabeth Rochon.
2013b. Using text and acoustic features to diagnose pro-
gressive aphasia and its subtypes. In Proceedings of Inter-
speech.
K.J. Gilhooly and R.H. Logie. 1980. Age-of-acquisition, im-
agery, concreteness, familiarity, and ambiguity measures
for 1,944 words. Behavior Research Methods, 12:395–
427.
M.L. Gorno-Tempini, A.E. Hillis, S. Weintraub, A. Kertesz,
M. Mendez, S.F. Cappa, J.M. Ogar, J.D. Rohrer, S. Black,
B.F. Boeve, F. Manes, N.F. Dronkers, R. Vandenberghe,
K. Rascovsky, K. Patterson, B.L. Miller, D.S. Knopman,
J.R. Hodges, M.M. Mesulam, and M. Grossman. 2011.
Classification of primary progressive aphasia and its vari-
ants. Neurology, 76:1006–1014.
William Jarrold, Bart Peintner, Eric Yeh, Ruth Krasnow,
Harold Javitz, and Gary Swan. 2010. Language ana-
lytics for assessing brain health: Cognitive impairment,
depression and pre-symptomatic Alzheimer’s disease. In
Yiyu Yao, Ron Sun, Tomaso Poggio, Jiming Liu, Ning
Zhong, and Jimmy Huang, editors, Brain Informatics, vol-
ume 6334 of Lecture Notes in Computer Science, pages
299–307. Springer Berlin / Heidelberg.
Susan Kemper, Marilyn Thompson, and Janet Marquis.
2001. Longitudinal change in language production: Ef-
fects of aging and dementia on grammatical complex-
ity and propositional content. Psychology and Aging,
16(4):600–614.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st Meeting
of the Association for Computational Linguistics, pages
423–430.
Xuan Le, Ian Lancashire, Graeme Hirst, and Regina Jokel.
2011. Longitudinal detection of dementia through lex-
ical and syntactic changes in writing: a case study of
three British novelists. Literary and Linguistic Comput-
ing, 26(4):435–461.
Max A. Little, Patrick E. McSharry, Eric J. Hunter, Jennifer
Spielman, and Lorraine O. Ramig. 2009. Suitability
of dysphonia measurements for telemonitoring of Parkin-
son’s disease. Biomedical Engineering, IEEE Transac-
tions on, 56(4):1015–1022.
Xiaofei Lu. 2010. Automatic analysis of syntactic complex-
ity in second language writing. International Journal of
Corpus Linguistics, 15(4):474–496.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich
Sch¨utze. 2008. Introduction to Information Retrieval.
Cambridge University Press.
Juan Jos´e G. Meil´an, Francisco Martinez-S´anchez, Juan
Carro, Dolores E. L´opez, Lymarie Millian-Morell, and
Jos´e M. Arana. 2014. Speech in Alzheimer’s disease:
Can temporal and acoustic parameters discriminate de-
mentia? Dementia and Geriatric Cognitive Disorders,
37(5-6):327–334.
Serguei V.S. Pakhomov, Glen E. Smith, Susan Marino, An-
gela Birnbaum, Neill Graff-Radford, Richard Caselli,
Bradley Boeve, and David D. Knopman. 2010a. A com-
puterized technique to asses language use patterns in pa-
tients with frontotemporal dementia. Journal of Neurolin-
guistics, 23:127–144.
S.V. Pakhomov, G.E. Smith, D. Chacon, Y. Feliciano,
N. Graff-Radford, R. Caselli, and D. S. Knopman. 2010b.
Computerized analysis of speech and language to identify
psycholinguistic correlates of frontotemporal lobar degen-
eration. Cognitive and Behavioral Neurology, 23:165–
177.
Hyejin Park, Yvonne Rogalski, Amy D. Rodriguez, Zvinka
Zlatar, Michelle Benjamin, Stacy Harnish, Jeffrey Ben-
nett, John C. Rosenbek, Bruce Crosson, and Jamie Reilly.
2011. Perceptual cues used by listeners to discriminate
fluent from nonfluent narrative discourse. Aphasiology,
25(9):998–1015.
Bart Peintner, William Jarrold, Dimitra Vergyri, Colleen
Richey, Maria Luisa Gorno Tempini, and Jennifer Ogar.
2008. Learning diagnostic models using speech and lan-
guage measures. In Engineering in Medicine and Biol-
ogy Society, 2008. EMBS 2008. 30th Annual International
Conference of the IEEE, pages 4648–4651.
Brian Roark, Margaret Mitchell, John-Paul Hosom, Kristy
Hollingshead, and Jeffery Kaye. 2011. Spoken language
derived measures for detecting mild cognitive impairment.
IEEE Transactions on Audio, Speech, and Language Pro-
cessing, 19(7):2081–2090.
</reference>
<page confidence="0.964644">
25
</page>
<reference confidence="0.999758885714286">
Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F.
Schwartz. 1989. The quantitative analysis of agrammatic
production: procedure and data. Brain and Language,
37:440–479.
Sameer Singh, Romola S. Bucks, and Joanne M. Cuerden.
2001. Evaluation of an objective technique for analysing
temporal variables in DAT spontaneous speech. Aphasiol-
ogy, 15(6):571–583.
Hans Stadthagen-Gonzalez and Colin J. Davis. 2006. The
Bristol norms for age of acquisition, imageability, and fa-
miliarity. Behavior Research Methods, 38(4):598–605.
Calvin Thomas, Vlado Keselj, Nick Cercone, Kenneth Rock-
wood, and Elissa Asp. 2005. Automatic detection and
rating of dementia of Alzheimer type through lexical anal-
ysis of spontaneous speech. In Proceedings of the IEEE
International Conference on Mechatronics and Automa-
tion, pages 1569–1574.
Kristina Toutanova, Dan Klein, Christopher Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proceedings of the
2003 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Lan-
guage Technologies, pages 252–259.
Athanasios Tsanas, Max A. Little, Patrick E. McSharry, Jen-
nifer Spielman, and Lorraine O. Ramig. 2012. Novel
speech signal processing algorithms for high-accuracy
classification of Parkinson’s disease. IEEE Transactions
on Biomedical Engineering, 59(5):1264–1271.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser features
for sentence grammaticality classification. In Proceed-
ings of the Australasian Language Technology Association
Workshop, pages 67–75.
Victor Yngve. 1960. A model and hypothesis for language
structure. Proceedings of the American Physical Society,
104:444–466.
</reference>
<page confidence="0.998049">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.167923">
<title confidence="0.6713365">Comparison of different feature sets for identification of variants progressive aphasia</title>
<author confidence="0.351096">C Graeme Naida L Jed A E</author>
<author confidence="0.351096">Elizabeth</author>
<affiliation confidence="0.743141333333333">of Computer Science, University of of Speech-Language Pathology, University of Toronto, &amp; Toronto Rehabilitation Research Institute, Baycrest Centre,</affiliation>
<address confidence="0.978953">Campbell Cognitive Neurology Research Unit, Sunnybrook Health Sciences Centre, Toronto</address>
<email confidence="0.984984">jmeltzer@research.baycrest.org,sandra.black@sunnybrook.ca</email>
<abstract confidence="0.999684107142857">We use computational techniques to extract a large number of different features from the narrative speech of individuals with primary progressive aphasia (PPA). We examine several different types of features, including part-of-speech, complexity, context-free grammar, fluency, psycholinguistic, vocabulary richness, and acoustic, and discuss the circumstances under which they can be extracted. We consider the task of training a machine learning classifier to determine whether a participant is a control, or has the fluent or nonfluent variant of PPA. We first evaluate the individual feature sets on their classification accuracy, then perform an ablation study to determine the optimal combination of feature sets. Finally, we rank the features in four practical scenarios: given audio data only, given unsegmented transcripts only, given segmented transcripts only, and given both audio and segmented transcripts. We find that psycholinguistic features are highly discriminative in most cases, and that acoustic, context-free grammar, and part-of-speech features can also be important in some circumstances.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural language processing with Python.</title>
<date>2009</date>
<publisher>O’Reilly Media, Inc.</publisher>
<contexts>
<context position="17561" citStr="Bird et al., 2009" startWordPosition="2760" endWordPosition="2763">stic and Brun´et’s index to the study of dementia, and found significant differences between individuals with DAT and healthy controls. Covington and McFall (2010) proposed a new measure called the moving-average type-token ratio (MATTR), which is independent of text length. This feature was later applied to aphasic speech in a study by Fergadiotis and Wright (2011), and was found to be one of the most unbiased indicators of lexical diversity in impaired speakers. The measures given in Table 7 are easily computed from their respective formulae. In this work, we lemmatize each word using NLTK (Bird et al., 2009) before calculating the features. For MATTR, we consider w = 10,20,30,40,50. 20 Honor´e’s statistic NV−0.165/ where V is the number of word types and N is the number of word tokens. Brun´et’s index 100logN/(1−V1/V) where V1 is the number of words used only once, V is the total number of word types, and N is the number of word tokens. Type-token ratio (TTR) V/N where V is the number of word types and N is the number of word tokens. Moving-average type-token ratio (MATTRw) TTR calculated over a moving window of size w, and averaged over all windows. Table 7: Vocabulary richness features. 3.3.7 A</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python. O’Reilly Media, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah D Breedin</author>
<author>Eleanor M Saffran</author>
<author>Myrna F Schwartz</author>
</authors>
<title>Semantic factors in verb retrieval: An effect of complexity.</title>
<date>1998</date>
<journal>Brain and Language,</journal>
<pages>63--1</pages>
<contexts>
<context position="16182" citStr="Breedin et al. (1998)" startWordPosition="2533" endWordPosition="2536">uisition Subjective rating of how old a person is when they first learn that word Light verbs Number of occurrences of be, have, come, go, give, take, make, do, get, move, and put, normalized by total number of verbs Table 6: Psycholinguistic features. age of acquisition (see Table 6). We compute the average of each of these measures for all content words, as well as for nouns and verbs separately. Another measure that fits into this category is the frequency of occurrence of light verbs, which an impaired speaker may use to replace a more specific verb. We use the same list of light verbs as Breedin et al. (1998), given in Table 6. One challenge associated with psycholinguistic features is finding norms which provide adequate coverage for the given data. Fraser et al. (2013a) reported that the SUBTL frequency norms had a coverage of above 90% on their data, but the Bristol-Gilhooly-Logie norms had a coverage of only around 30%. 3.3.6 Vocabulary richness features Individuals experiencing semantic difficulty may use a limited range of vocabulary. We can measure the vocabulary richness or lexical diversity of a narrative sample using a number of different metrics (see Table 7). Type-token ratio has been </context>
</contexts>
<marker>Breedin, Saffran, Schwartz, 1998</marker>
<rawString>Sarah D. Breedin, Eleanor M. Saffran, and Myrna F. Schwartz. 1998. Semantic factors in verb retrieval: An effect of complexity. Brain and Language, 63:1–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Brysbaert</author>
<author>Boris New</author>
</authors>
<title>Moving beyond Kuˇcera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English.</title>
<date>2009</date>
<journal>Behavior Research Methods,</journal>
<volume>41</volume>
<issue>4</issue>
<contexts>
<context position="15179" citStr="Brysbaert and New, 2009" startWordPosition="2365" endWordPosition="2368">ple could be easily generated using the word count feature in most text-editing software (although we first exclude filled pauses and NID tokens), and the verbal rate can subsequently be calculated directly. The other three features are easily calculated using string matching and an electronic dictionary. 3.3.5 Psycholinguistic features Some types of dementia are characterized by impairments in semantic access. Such impairments may be sensitive to psycholinguistic features such as lexical frequency, familiarity, imageability, and age of acquisition (Table 6). We use the SUBTL frequency norms (Brysbaert and New, 2009) and the combined Bristol and Gilhooly-Logie norms (Stadthagen-Gonzalez and Davis, 2006; Gilhooly and Logie, 1980) for familiarity, imageability, and Frequency Frequency with which a word occurs in some corpus of natural language Familiarity Subjective rating of how familiar a word seems Imageability Subjective rating of how easily a word generates an image in the mind Age of acquisition Subjective rating of how old a person is when they first learn that word Light verbs Number of occurrences of be, have, come, go, give, take, make, do, get, move, and put, normalized by total number of verbs T</context>
</contexts>
<marker>Brysbaert, New, 2009</marker>
<rawString>Marc Brysbaert and Boris New. 2009. Moving beyond Kuˇcera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English. Behavior Research Methods, 41(4):977–990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Bucks</author>
<author>S Singh</author>
<author>J M Cuerden</author>
<author>G K Wilcock</author>
</authors>
<title>Analysis of spontaneous, conversational speech in dementia of Alzheimer type: Evaluation of an objective technique for analysing lexical performance.</title>
<date>2000</date>
<journal>Aphasiology,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="3464" citStr="Bucks et al., 2000" startWordPosition="500" endWordPosition="503"> a large number of features on a relatively small data set, we emphasize that this work is exploratory in nature; nonetheless, our results are consistent with, and extend, previous work in the field. 2 Background In recent years, there has been growing interest in using computer techniques to automatically detect dementia from speech and language features derived from a sample of narrative speech. Some researchers have explored ways to use methods such as part-of-speech tagging, statistical parsing, and speech signal analysis to detect disorders such as dementia of the Alzheimer’s type (DAT) (Bucks et al., 2000; Singh et al., 2001; Thomas et al., 2005; Jarrold et al., 2010) and mild cognitive impairment (MCI) (Roark et al., 2011). Here, we focus on a type of dementia called primary progressive aphasia (PPA). PPA is a subtype of frontotemporal dementia (FTD) which is characterized by progressive language impairment without other notable cognitive impairment. There are three subtypes of PPA: semantic dementia (SD), progressive nonfluent aphasia (PNFA), and logopenic progressive aphasia (LPA). SD, sometimes called “fluent” progressive aphasia, is typically marked by fluent but empty speech, anomia, 17 </context>
<context position="10456" citStr="Bucks et al., 2000" startWordPosition="1591" endWordPosition="1594">o be more impaired with respect to nouns than verbs, and may replace nouns with pronouns or circumlocutory phrases. In contrast, individuals with PNFA may have more difficulty with verbs and may even demonstrate agrammatism, which can result in the omission of grammatical morphemes and function words. Thus, it is often useful to compare the relative frequencies with which words representing the different parts-of-speech (POS) are produced in a sample, as in Table 2. Similar features have been reported in computational studies of MCI (Roark et al., 2011), FTD (Pakhomov et al., 2010b), and DAT (Bucks et al., 2000). Numerous POS taggers exist, although we use the Stanford tagger here (Toutanova et al., 2003). 3.3.2 Complexity features Changes in linguistic complexity may accompany the onset of dementia, although some studies have found a decrease in complexity (e.g. Kemper et al. (2001)) while others have found an increase (e.g. Le et al. (2011)). The features in Table 3 vary in their ease of computation. Mean word length can be calculated from an unsegmented transcript, while mean sentence length requires only sentence boundary segmentation. Other measures, such as Yngve depth Nouns # nouns / # words V</context>
<context position="16903" citStr="Bucks et al. (2000)" startWordPosition="2650" endWordPosition="2653">vide adequate coverage for the given data. Fraser et al. (2013a) reported that the SUBTL frequency norms had a coverage of above 90% on their data, but the Bristol-Gilhooly-Logie norms had a coverage of only around 30%. 3.3.6 Vocabulary richness features Individuals experiencing semantic difficulty may use a limited range of vocabulary. We can measure the vocabulary richness or lexical diversity of a narrative sample using a number of different metrics (see Table 7). Type-token ratio has been a popular choice, perhaps due to its simplicity; however it is sensitive to the length of the sample. Bucks et al. (2000) were the first to apply Honor´e’s statistic and Brun´et’s index to the study of dementia, and found significant differences between individuals with DAT and healthy controls. Covington and McFall (2010) proposed a new measure called the moving-average type-token ratio (MATTR), which is independent of text length. This feature was later applied to aphasic speech in a study by Fergadiotis and Wright (2011), and was found to be one of the most unbiased indicators of lexical diversity in impaired speakers. The measures given in Table 7 are easily computed from their respective formulae. In this w</context>
</contexts>
<marker>Bucks, Singh, Cuerden, Wilcock, 2000</marker>
<rawString>R.S. Bucks, S. Singh, J.M. Cuerden, and G.K. Wilcock. 2000. Analysis of spontaneous, conversational speech in dementia of Alzheimer type: Evaluation of an objective technique for analysing lexical performance. Aphasiology, 14(1):71–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jieun Chae</author>
<author>Ani Nenkova</author>
</authors>
<title>Predicting the fluency of text with shallow structural features: case studies of machine translation and human-written text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>139--147</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12874" citStr="Chae and Nenkova, 2009" startWordPosition="1990" endWordPosition="1993">le Table 3: Complexity features. (Yngve, 1960), require full parses of the sentences (we use the Stanford parser (Klein and Manning, 2003) and Lu’s Syntactic Complexity Analyzer (Lu, 2010)). 3.3.3 CFG features Although many of the complexity features above are derived from parse trees, in this section we present a set of features that take into account the context-free grammar (CFG) labels on each of the nodes. CFG features have been previously used to assess the grammaticality of sentences in an artificial error corpus (Wong and Dras, 2010) and to distinguish human from machine translations (Chae and Nenkova, 2009). However, this is the first time such features have been applied to speech from participants with dementia. In Table 4 we list a few examples of our 134 CFG features, as well as the three phrase-level features (calculated for noun phrases, verb phrases, and prepositional phrases). 19 NP → NNS Noun phrases consisting of only a plural noun VP → VBN PP Verb phrases consisting of a pastparticiple verb and a prepositional phrase ROOT → INTJ Trees consisting of only an interjection Phrase type proportion Length of each phrase type (noun phrase, verb phrase, or prepositional phrase), divided by tota</context>
</contexts>
<marker>Chae, Nenkova, 2009</marker>
<rawString>Jieun Chae and Ani Nenkova. 2009. Predicting the fluency of text with shallow structural features: case studies of machine translation and human-written text. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 139–147. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Jean E Fox Tree</author>
</authors>
<title>Using uh and um in spontaneous speaking.</title>
<date>2002</date>
<journal>Cognition,</journal>
<volume>84</volume>
<issue>1</issue>
<marker>Clark, Tree, 2002</marker>
<rawString>Herbert H. Clark and Jean E. Fox Tree. 2002. Using uh and um in spontaneous speaking. Cognition, 84(1):73–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
<author>Joe D McFall</author>
</authors>
<title>Cutting the Gordian knot: The moving-average type–token ratio (MATTR).</title>
<date>2010</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>17</volume>
<issue>2</issue>
<pages>100</pages>
<contexts>
<context position="17106" citStr="Covington and McFall (2010)" startWordPosition="2682" endWordPosition="2686">rage of only around 30%. 3.3.6 Vocabulary richness features Individuals experiencing semantic difficulty may use a limited range of vocabulary. We can measure the vocabulary richness or lexical diversity of a narrative sample using a number of different metrics (see Table 7). Type-token ratio has been a popular choice, perhaps due to its simplicity; however it is sensitive to the length of the sample. Bucks et al. (2000) were the first to apply Honor´e’s statistic and Brun´et’s index to the study of dementia, and found significant differences between individuals with DAT and healthy controls. Covington and McFall (2010) proposed a new measure called the moving-average type-token ratio (MATTR), which is independent of text length. This feature was later applied to aphasic speech in a study by Fergadiotis and Wright (2011), and was found to be one of the most unbiased indicators of lexical diversity in impaired speakers. The measures given in Table 7 are easily computed from their respective formulae. In this work, we lemmatize each word using NLTK (Bird et al., 2009) before calculating the features. For MATTR, we consider w = 10,20,30,40,50. 20 Honor´e’s statistic NV−0.165/ where V is the number of word types</context>
</contexts>
<marker>Covington, McFall, 2010</marker>
<rawString>Michael A. Covington and Joe D. McFall. 2010. Cutting the Gordian knot: The moving-average type–token ratio (MATTR). Journal of Quantitative Linguistics, 17(2):94– 100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerasimos Fergadiotis</author>
<author>Heather Harris Wright</author>
</authors>
<title>Lexical diversity for adults with and without aphasia across discourse elicitation tasks.</title>
<date>2011</date>
<journal>Aphasiology,</journal>
<volume>25</volume>
<issue>11</issue>
<contexts>
<context position="17311" citStr="Fergadiotis and Wright (2011)" startWordPosition="2716" endWordPosition="2719"> of a narrative sample using a number of different metrics (see Table 7). Type-token ratio has been a popular choice, perhaps due to its simplicity; however it is sensitive to the length of the sample. Bucks et al. (2000) were the first to apply Honor´e’s statistic and Brun´et’s index to the study of dementia, and found significant differences between individuals with DAT and healthy controls. Covington and McFall (2010) proposed a new measure called the moving-average type-token ratio (MATTR), which is independent of text length. This feature was later applied to aphasic speech in a study by Fergadiotis and Wright (2011), and was found to be one of the most unbiased indicators of lexical diversity in impaired speakers. The measures given in Table 7 are easily computed from their respective formulae. In this work, we lemmatize each word using NLTK (Bird et al., 2009) before calculating the features. For MATTR, we consider w = 10,20,30,40,50. 20 Honor´e’s statistic NV−0.165/ where V is the number of word types and N is the number of word tokens. Brun´et’s index 100logN/(1−V1/V) where V1 is the number of words used only once, V is the total number of word types, and N is the number of word tokens. Type-token rat</context>
</contexts>
<marker>Fergadiotis, Wright, 2011</marker>
<rawString>Gerasimos Fergadiotis and Heather Harris Wright. 2011. Lexical diversity for adults with and without aphasia across discourse elicitation tasks. Aphasiology, 25(11):1414–1430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen C Fraser</author>
<author>Jed A Meltzer</author>
<author>Naida L Graham</author>
<author>Carol Leonard</author>
<author>Graeme Hirst</author>
<author>Sandra E Black</author>
<author>Elizabeth Rochon</author>
</authors>
<title>Automated classification of primary progressive aphasia subtypes from narrative speech transcripts.</title>
<date>2013</date>
<journal>Cortex.</journal>
<contexts>
<context position="6569" citStr="Fraser et al. (2013" startWordPosition="965" endWordPosition="968">2010a) examined FTD speech from an information-theoretic approach. They constructed a language model of healthy control speech, and then calculated the perplexity and outof-vocabulary rate for each of the patient groups relative to that model. In another study, Pakhomov et al. (2010b) extracted speech and language features from samples of FTD speech. In a principal components analysis, they discovered four components which accounted for most of the variance in their data: speech length, hesitancy, empty content, and grammaticality. However, they did not perform any classification experiments. Fraser et al. (2013a) attempted to classify participants as either SD patients, PNFA patients, or healthy controls using a large number of language SD PNFA Control (N = 11) (N = 13) (N = 16) Male/Female 8/3 7/6 9/7 Age (yrs) 65.9 (7.1) 64.5 (10.4) 67.8 (8.2) Education (yrs) 17.5 (5.8) 14.0 (3.5) 16.8 (4.3) Table 1: Demographic information. Numbers are given in the form: mean (standard deviation). features extracted from manually-transcribed transcripts. They distinguished between SD and control participants with very high accuracy, and were also successful at distinguishing between PNFA and control participants.</context>
<context position="16346" citStr="Fraser et al. (2013" startWordPosition="2560" endWordPosition="2563">move, and put, normalized by total number of verbs Table 6: Psycholinguistic features. age of acquisition (see Table 6). We compute the average of each of these measures for all content words, as well as for nouns and verbs separately. Another measure that fits into this category is the frequency of occurrence of light verbs, which an impaired speaker may use to replace a more specific verb. We use the same list of light verbs as Breedin et al. (1998), given in Table 6. One challenge associated with psycholinguistic features is finding norms which provide adequate coverage for the given data. Fraser et al. (2013a) reported that the SUBTL frequency norms had a coverage of above 90% on their data, but the Bristol-Gilhooly-Logie norms had a coverage of only around 30%. 3.3.6 Vocabulary richness features Individuals experiencing semantic difficulty may use a limited range of vocabulary. We can measure the vocabulary richness or lexical diversity of a narrative sample using a number of different metrics (see Table 7). Type-token ratio has been a popular choice, perhaps due to its simplicity; however it is sensitive to the length of the sample. Bucks et al. (2000) were the first to apply Honor´e’s statisti</context>
<context position="18636" citStr="Fraser et al., 2013" startWordPosition="2945" endWordPosition="2948">oken ratio (MATTRw) TTR calculated over a moving window of size w, and averaged over all windows. Table 7: Vocabulary richness features. 3.3.7 Acoustic features What we call acoustic features are extracted directly from the audio file. We consider the features given in Table 8. Acoustic features have been shown to be useful when automatically detecting conditions such as Parkinson’s disease, in which changes in speech are common (Little et al., 2009; Tsanas et al., 2012). Acoustic features have also been examined in studies of DAT (Meil´an et al., 2014), FTD (Pakhomov et al., 2010b), and PPA (Fraser et al., 2013b, whose software we use here). An obvious benefit to acoustic features is that they do not require a transcription, and can be calculated immediately given an audio sample. The corresponding drawback is that they tell us nothing about the linguistic content of the narrative. 4 Experiments We report the results of three experiments exploring the discriminative power of the different features. We first compare the classification accuracies using each individual feature set. We then perform an ablation study to determine which combination of feature sets leads to the highest classification accur</context>
<context position="22169" citStr="Fraser et al. (2013" startWordPosition="3524" endWordPosition="3527">lting speech of PNFA. The third best accuracy is achieved using the fluency features, which also fits with this explanation. However, we might have expected that the complexity and CFG features would be more sensitive to the grammatical impairments of PNFA. Finally, the best accuracy for SD vs. PNFA is lower than in the previous two cases, and is achieved using only CFG features. This suggests that there are some grammatical constructions which occur with different frequencies in the two groups. These differences do not appear to be captured by the complexity features, which could explain why Fraser et al. (2013a) did not find syntactic differences between the SD and PNFA groups. Interestingly, the results using CFG fea21 Feature set SD vs. PNFA vs. SD vs. controls controls PNFA All .963 .931 .708 Acoustic .778 .862 .167 Psycholinguistic .963 .724 .708 POS .963 .690 .375 Complexity .852 .621 .667 Fluency .667 .828 .500 Vocab. richness .481 .586 .583 CFG .630 .690 .792 (a) SD vs. controls. Removed Remaining Features Accuracy A+P+POS+C+F+VR+CFG .963 F A+P+POS+C+VR+CFG .963 A P+POS+C+VR+CFG 1.00 VR P+POS+C+CFG .926 CFG P+POS+C .926 C P+POS 1.00 POS P .963 (b) PNFA vs. controls. Table 9: Classification a</context>
</contexts>
<marker>Fraser, Meltzer, Graham, Leonard, Hirst, Black, Rochon, 2013</marker>
<rawString>Kathleen C. Fraser, Jed A. Meltzer, Naida L. Graham, Carol Leonard, Graeme Hirst, Sandra E. Black, and Elizabeth Rochon. 2013a. Automated classification of primary progressive aphasia subtypes from narrative speech transcripts. Cortex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen C Fraser</author>
<author>Frank Rudzicz</author>
<author>Elizabeth Rochon</author>
</authors>
<title>Using text and acoustic features to diagnose progressive aphasia and its subtypes.</title>
<date>2013</date>
<booktitle>In Proceedings of Interspeech.</booktitle>
<contexts>
<context position="6569" citStr="Fraser et al. (2013" startWordPosition="965" endWordPosition="968">2010a) examined FTD speech from an information-theoretic approach. They constructed a language model of healthy control speech, and then calculated the perplexity and outof-vocabulary rate for each of the patient groups relative to that model. In another study, Pakhomov et al. (2010b) extracted speech and language features from samples of FTD speech. In a principal components analysis, they discovered four components which accounted for most of the variance in their data: speech length, hesitancy, empty content, and grammaticality. However, they did not perform any classification experiments. Fraser et al. (2013a) attempted to classify participants as either SD patients, PNFA patients, or healthy controls using a large number of language SD PNFA Control (N = 11) (N = 13) (N = 16) Male/Female 8/3 7/6 9/7 Age (yrs) 65.9 (7.1) 64.5 (10.4) 67.8 (8.2) Education (yrs) 17.5 (5.8) 14.0 (3.5) 16.8 (4.3) Table 1: Demographic information. Numbers are given in the form: mean (standard deviation). features extracted from manually-transcribed transcripts. They distinguished between SD and control participants with very high accuracy, and were also successful at distinguishing between PNFA and control participants.</context>
<context position="16346" citStr="Fraser et al. (2013" startWordPosition="2560" endWordPosition="2563">move, and put, normalized by total number of verbs Table 6: Psycholinguistic features. age of acquisition (see Table 6). We compute the average of each of these measures for all content words, as well as for nouns and verbs separately. Another measure that fits into this category is the frequency of occurrence of light verbs, which an impaired speaker may use to replace a more specific verb. We use the same list of light verbs as Breedin et al. (1998), given in Table 6. One challenge associated with psycholinguistic features is finding norms which provide adequate coverage for the given data. Fraser et al. (2013a) reported that the SUBTL frequency norms had a coverage of above 90% on their data, but the Bristol-Gilhooly-Logie norms had a coverage of only around 30%. 3.3.6 Vocabulary richness features Individuals experiencing semantic difficulty may use a limited range of vocabulary. We can measure the vocabulary richness or lexical diversity of a narrative sample using a number of different metrics (see Table 7). Type-token ratio has been a popular choice, perhaps due to its simplicity; however it is sensitive to the length of the sample. Bucks et al. (2000) were the first to apply Honor´e’s statisti</context>
<context position="18636" citStr="Fraser et al., 2013" startWordPosition="2945" endWordPosition="2948">oken ratio (MATTRw) TTR calculated over a moving window of size w, and averaged over all windows. Table 7: Vocabulary richness features. 3.3.7 Acoustic features What we call acoustic features are extracted directly from the audio file. We consider the features given in Table 8. Acoustic features have been shown to be useful when automatically detecting conditions such as Parkinson’s disease, in which changes in speech are common (Little et al., 2009; Tsanas et al., 2012). Acoustic features have also been examined in studies of DAT (Meil´an et al., 2014), FTD (Pakhomov et al., 2010b), and PPA (Fraser et al., 2013b, whose software we use here). An obvious benefit to acoustic features is that they do not require a transcription, and can be calculated immediately given an audio sample. The corresponding drawback is that they tell us nothing about the linguistic content of the narrative. 4 Experiments We report the results of three experiments exploring the discriminative power of the different features. We first compare the classification accuracies using each individual feature set. We then perform an ablation study to determine which combination of feature sets leads to the highest classification accur</context>
<context position="22169" citStr="Fraser et al. (2013" startWordPosition="3524" endWordPosition="3527">lting speech of PNFA. The third best accuracy is achieved using the fluency features, which also fits with this explanation. However, we might have expected that the complexity and CFG features would be more sensitive to the grammatical impairments of PNFA. Finally, the best accuracy for SD vs. PNFA is lower than in the previous two cases, and is achieved using only CFG features. This suggests that there are some grammatical constructions which occur with different frequencies in the two groups. These differences do not appear to be captured by the complexity features, which could explain why Fraser et al. (2013a) did not find syntactic differences between the SD and PNFA groups. Interestingly, the results using CFG fea21 Feature set SD vs. PNFA vs. SD vs. controls controls PNFA All .963 .931 .708 Acoustic .778 .862 .167 Psycholinguistic .963 .724 .708 POS .963 .690 .375 Complexity .852 .621 .667 Fluency .667 .828 .500 Vocab. richness .481 .586 .583 CFG .630 .690 .792 (a) SD vs. controls. Removed Remaining Features Accuracy A+P+POS+C+F+VR+CFG .963 F A+P+POS+C+VR+CFG .963 A P+POS+C+VR+CFG 1.00 VR P+POS+C+CFG .926 CFG P+POS+C .926 C P+POS 1.00 POS P .963 (b) PNFA vs. controls. Table 9: Classification a</context>
</contexts>
<marker>Fraser, Rudzicz, Rochon, 2013</marker>
<rawString>Kathleen C. Fraser, Frank Rudzicz, and Elizabeth Rochon. 2013b. Using text and acoustic features to diagnose progressive aphasia and its subtypes. In Proceedings of Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K J Gilhooly</author>
<author>R H Logie</author>
</authors>
<title>Age-of-acquisition, imagery, concreteness, familiarity, and ambiguity measures for 1,944 words.</title>
<date>1980</date>
<journal>Behavior Research Methods,</journal>
<volume>12</volume>
<pages>427</pages>
<contexts>
<context position="15293" citStr="Gilhooly and Logie, 1980" startWordPosition="2380" endWordPosition="2383">de filled pauses and NID tokens), and the verbal rate can subsequently be calculated directly. The other three features are easily calculated using string matching and an electronic dictionary. 3.3.5 Psycholinguistic features Some types of dementia are characterized by impairments in semantic access. Such impairments may be sensitive to psycholinguistic features such as lexical frequency, familiarity, imageability, and age of acquisition (Table 6). We use the SUBTL frequency norms (Brysbaert and New, 2009) and the combined Bristol and Gilhooly-Logie norms (Stadthagen-Gonzalez and Davis, 2006; Gilhooly and Logie, 1980) for familiarity, imageability, and Frequency Frequency with which a word occurs in some corpus of natural language Familiarity Subjective rating of how familiar a word seems Imageability Subjective rating of how easily a word generates an image in the mind Age of acquisition Subjective rating of how old a person is when they first learn that word Light verbs Number of occurrences of be, have, come, go, give, take, make, do, get, move, and put, normalized by total number of verbs Table 6: Psycholinguistic features. age of acquisition (see Table 6). We compute the average of each of these measu</context>
</contexts>
<marker>Gilhooly, Logie, 1980</marker>
<rawString>K.J. Gilhooly and R.H. Logie. 1980. Age-of-acquisition, imagery, concreteness, familiarity, and ambiguity measures for 1,944 words. Behavior Research Methods, 12:395– 427.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M L Gorno-Tempini</author>
<author>A E Hillis</author>
<author>S Weintraub</author>
<author>A Kertesz</author>
<author>M Mendez</author>
<author>S F Cappa</author>
<author>J M Ogar</author>
<author>J D Rohrer</author>
<author>S Black</author>
<author>B F Boeve</author>
<author>F Manes</author>
<author>N F Dronkers</author>
<author>R Vandenberghe</author>
<author>K Rascovsky</author>
<author>K Patterson</author>
<author>B L Miller</author>
<author>D S Knopman</author>
<author>J R Hodges</author>
<author>M M Mesulam</author>
<author>M Grossman</author>
</authors>
<title>Classification of primary progressive aphasia and its variants.</title>
<date>2011</date>
<journal>Neurology,</journal>
<pages>76--1006</pages>
<contexts>
<context position="4358" citStr="Gorno-Tempini et al., 2011" startWordPosition="629" endWordPosition="632">terized by progressive language impairment without other notable cognitive impairment. There are three subtypes of PPA: semantic dementia (SD), progressive nonfluent aphasia (PNFA), and logopenic progressive aphasia (LPA). SD, sometimes called “fluent” progressive aphasia, is typically marked by fluent but empty speech, anomia, 17 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 17–26, Baltimore, Maryland USA, June 27, 2014. c�2014 Association for Computational Linguistics deficits in comprehension, and spared grammar and syntax (Gorno-Tempini et al., 2011). In contrast, PNFA is characterized by halting and sometimes agrammatic speech, reduced syntactic complexity, word-finding difficulties, and relatively spared single-word comprehension (GornoTempini et al., 2011). The third subtype, LPA, is characterized by slow speech and frequent word finding difficulties; this subtype is not included in the current analysis. Although clear diagnostic criteria for PPA have been established (Gorno-Tempini et al., 2011), there is no one test which can provide a diagnosis. Classification of PPA into subtypes requires evaluation of spoken output, as well as neu</context>
<context position="7954" citStr="Gorno-Tempini et al., 2011" startWordPosition="1186" endWordPosition="1189"> feature set to include acoustic features extracted directly from the audio file. 3 Methods 3.1 Data Twenty-four patients with PPA were recruited through three Toronto memory clinics, and 16 ageand education-matched healthy controls were recruited through a volunteer pool. All participants were native speakers of English, or had completed some of their education in English. Exclusion criteria included a known history of drug or alcohol abuse and a history of neurological or major psychiatric illness. Each patient was diagnosed by a behavioural neurologist and all met current criteria for PPA (Gorno-Tempini et al., 2011). Table 1 shows demographic information for each group. To elicit a sample of narrative speech, participants were asked to tell the well-known story of Cinderella. They were given a wordless picture book to remind them of the story; then the book was removed and they were asked to tell the story in their own words. This procedure, described in full by Saffran et al. (1989), is commonly used in studies of connected speech in aphasia. The narrative samples were transcribed by trained research assistants. The transcriptions include filled pauses, repetitions, and false starts, and were annotated </context>
</contexts>
<marker>Gorno-Tempini, Hillis, Weintraub, Kertesz, Mendez, Cappa, Ogar, Rohrer, Black, Boeve, Manes, Dronkers, Vandenberghe, Rascovsky, Patterson, Miller, Knopman, Hodges, Mesulam, Grossman, 2011</marker>
<rawString>M.L. Gorno-Tempini, A.E. Hillis, S. Weintraub, A. Kertesz, M. Mendez, S.F. Cappa, J.M. Ogar, J.D. Rohrer, S. Black, B.F. Boeve, F. Manes, N.F. Dronkers, R. Vandenberghe, K. Rascovsky, K. Patterson, B.L. Miller, D.S. Knopman, J.R. Hodges, M.M. Mesulam, and M. Grossman. 2011. Classification of primary progressive aphasia and its variants. Neurology, 76:1006–1014.</rawString>
</citation>
<citation valid="false">
<authors>
<author>William Jarrold</author>
<author>Bart Peintner</author>
<author>Eric Yeh</author>
<author>Ruth Krasnow</author>
<author>Harold Javitz</author>
<author>Gary Swan</author>
</authors>
<title>Language analytics for assessing brain health: Cognitive impairment, depression and pre-symptomatic Alzheimer’s disease.</title>
<date>2010</date>
<booktitle>In Yiyu</booktitle>
<volume>6334</volume>
<pages>299--307</pages>
<editor>Sun, Tomaso Poggio, Jiming Liu, Ning Zhong, and Jimmy Huang, editors, Brain Informatics,</editor>
<publisher>Springer</publisher>
<location>Yao, Ron</location>
<contexts>
<context position="3528" citStr="Jarrold et al., 2010" startWordPosition="512" endWordPosition="515">e emphasize that this work is exploratory in nature; nonetheless, our results are consistent with, and extend, previous work in the field. 2 Background In recent years, there has been growing interest in using computer techniques to automatically detect dementia from speech and language features derived from a sample of narrative speech. Some researchers have explored ways to use methods such as part-of-speech tagging, statistical parsing, and speech signal analysis to detect disorders such as dementia of the Alzheimer’s type (DAT) (Bucks et al., 2000; Singh et al., 2001; Thomas et al., 2005; Jarrold et al., 2010) and mild cognitive impairment (MCI) (Roark et al., 2011). Here, we focus on a type of dementia called primary progressive aphasia (PPA). PPA is a subtype of frontotemporal dementia (FTD) which is characterized by progressive language impairment without other notable cognitive impairment. There are three subtypes of PPA: semantic dementia (SD), progressive nonfluent aphasia (PNFA), and logopenic progressive aphasia (LPA). SD, sometimes called “fluent” progressive aphasia, is typically marked by fluent but empty speech, anomia, 17 Workshop on Computational Linguistics and Clinical Psychology: F</context>
</contexts>
<marker>Jarrold, Peintner, Yeh, Krasnow, Javitz, Swan, 2010</marker>
<rawString>William Jarrold, Bart Peintner, Eric Yeh, Ruth Krasnow, Harold Javitz, and Gary Swan. 2010. Language analytics for assessing brain health: Cognitive impairment, depression and pre-symptomatic Alzheimer’s disease. In Yiyu Yao, Ron Sun, Tomaso Poggio, Jiming Liu, Ning Zhong, and Jimmy Huang, editors, Brain Informatics, volume 6334 of Lecture Notes in Computer Science, pages 299–307. Springer Berlin / Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Kemper</author>
<author>Marilyn Thompson</author>
<author>Janet Marquis</author>
</authors>
<title>Longitudinal change in language production: Effects of aging and dementia on grammatical complexity and propositional content.</title>
<date>2001</date>
<journal>Psychology and Aging,</journal>
<volume>16</volume>
<issue>4</issue>
<contexts>
<context position="10733" citStr="Kemper et al. (2001)" startWordPosition="1633" endWordPosition="1636">phemes and function words. Thus, it is often useful to compare the relative frequencies with which words representing the different parts-of-speech (POS) are produced in a sample, as in Table 2. Similar features have been reported in computational studies of MCI (Roark et al., 2011), FTD (Pakhomov et al., 2010b), and DAT (Bucks et al., 2000). Numerous POS taggers exist, although we use the Stanford tagger here (Toutanova et al., 2003). 3.3.2 Complexity features Changes in linguistic complexity may accompany the onset of dementia, although some studies have found a decrease in complexity (e.g. Kemper et al. (2001)) while others have found an increase (e.g. Le et al. (2011)). The features in Table 3 vary in their ease of computation. Mean word length can be calculated from an unsegmented transcript, while mean sentence length requires only sentence boundary segmentation. Other measures, such as Yngve depth Nouns # nouns / # words Verbs # verbs / # words Noun-verb ratio # nouns / # verbs Noun ratio # nouns / (# nouns + # verbs) Inflected verbs # inflected verbs / # verbs Determiners # determiners / # words Demonstratives # demonstratives / # words Prepositions # prepositions / # words Adjectives # adject</context>
</contexts>
<marker>Kemper, Thompson, Marquis, 2001</marker>
<rawString>Susan Kemper, Marilyn Thompson, and Janet Marquis. 2001. Longitudinal change in language production: Effects of aging and dementia on grammatical complexity and propositional content. Psychology and Aging, 16(4):600–614.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="12389" citStr="Klein and Manning, 2003" startWordPosition="1912" endWordPosition="1915"> the parse tree, averaged over all sentences Tree height height of each parse tree, averaged over all sentences MLS mean length of sentence MLC mean length of clause MLT mean length of T-unit Subordinate conjunctions number of subordinate conjunctions Coordinate conjunctions number of coordinate conjunctions Subordinate:coordinate ratio ratio of number of subordinate conjunctions to number of coordinate conjunctions Mean word length mean length, in letters, of each word in the sample Table 3: Complexity features. (Yngve, 1960), require full parses of the sentences (we use the Stanford parser (Klein and Manning, 2003) and Lu’s Syntactic Complexity Analyzer (Lu, 2010)). 3.3.3 CFG features Although many of the complexity features above are derived from parse trees, in this section we present a set of features that take into account the context-free grammar (CFG) labels on each of the nodes. CFG features have been previously used to assess the grammaticality of sentences in an artificial error corpus (Wong and Dras, 2010) and to distinguish human from machine translations (Chae and Nenkova, 2009). However, this is the first time such features have been applied to speech from participants with dementia. In Tab</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan Le</author>
<author>Ian Lancashire</author>
<author>Graeme Hirst</author>
<author>Regina Jokel</author>
</authors>
<title>Longitudinal detection of dementia through lexical and syntactic changes in writing: a case study of three British novelists. Literary and Linguistic Computing,</title>
<date>2011</date>
<contexts>
<context position="10793" citStr="Le et al. (2011)" startWordPosition="1644" endWordPosition="1647">he relative frequencies with which words representing the different parts-of-speech (POS) are produced in a sample, as in Table 2. Similar features have been reported in computational studies of MCI (Roark et al., 2011), FTD (Pakhomov et al., 2010b), and DAT (Bucks et al., 2000). Numerous POS taggers exist, although we use the Stanford tagger here (Toutanova et al., 2003). 3.3.2 Complexity features Changes in linguistic complexity may accompany the onset of dementia, although some studies have found a decrease in complexity (e.g. Kemper et al. (2001)) while others have found an increase (e.g. Le et al. (2011)). The features in Table 3 vary in their ease of computation. Mean word length can be calculated from an unsegmented transcript, while mean sentence length requires only sentence boundary segmentation. Other measures, such as Yngve depth Nouns # nouns / # words Verbs # verbs / # words Noun-verb ratio # nouns / # verbs Noun ratio # nouns / (# nouns + # verbs) Inflected verbs # inflected verbs / # verbs Determiners # determiners / # words Demonstratives # demonstratives / # words Prepositions # prepositions / # words Adjectives # adjectives / # words Adverbs # adverbs / # words Pronoun ratio # p</context>
</contexts>
<marker>Le, Lancashire, Hirst, Jokel, 2011</marker>
<rawString>Xuan Le, Ian Lancashire, Graeme Hirst, and Regina Jokel. 2011. Longitudinal detection of dementia through lexical and syntactic changes in writing: a case study of three British novelists. Literary and Linguistic Computing, 26(4):435–461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max A Little</author>
<author>Patrick E McSharry</author>
<author>Eric J Hunter</author>
<author>Jennifer Spielman</author>
<author>Lorraine O Ramig</author>
</authors>
<title>Suitability of dysphonia measurements for telemonitoring of Parkinson’s disease. Biomedical Engineering,</title>
<date>2009</date>
<journal>IEEE Transactions on,</journal>
<volume>56</volume>
<issue>4</issue>
<contexts>
<context position="18470" citStr="Little et al., 2009" startWordPosition="2916" endWordPosition="2919">rd types, and N is the number of word tokens. Type-token ratio (TTR) V/N where V is the number of word types and N is the number of word tokens. Moving-average type-token ratio (MATTRw) TTR calculated over a moving window of size w, and averaged over all windows. Table 7: Vocabulary richness features. 3.3.7 Acoustic features What we call acoustic features are extracted directly from the audio file. We consider the features given in Table 8. Acoustic features have been shown to be useful when automatically detecting conditions such as Parkinson’s disease, in which changes in speech are common (Little et al., 2009; Tsanas et al., 2012). Acoustic features have also been examined in studies of DAT (Meil´an et al., 2014), FTD (Pakhomov et al., 2010b), and PPA (Fraser et al., 2013b, whose software we use here). An obvious benefit to acoustic features is that they do not require a transcription, and can be calculated immediately given an audio sample. The corresponding drawback is that they tell us nothing about the linguistic content of the narrative. 4 Experiments We report the results of three experiments exploring the discriminative power of the different features. We first compare the classification ac</context>
</contexts>
<marker>Little, McSharry, Hunter, Spielman, Ramig, 2009</marker>
<rawString>Max A. Little, Patrick E. McSharry, Eric J. Hunter, Jennifer Spielman, and Lorraine O. Ramig. 2009. Suitability of dysphonia measurements for telemonitoring of Parkinson’s disease. Biomedical Engineering, IEEE Transactions on, 56(4):1015–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei Lu</author>
</authors>
<title>Automatic analysis of syntactic complexity in second language writing.</title>
<date>2010</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="12439" citStr="Lu, 2010" startWordPosition="1921" endWordPosition="1922">f each parse tree, averaged over all sentences MLS mean length of sentence MLC mean length of clause MLT mean length of T-unit Subordinate conjunctions number of subordinate conjunctions Coordinate conjunctions number of coordinate conjunctions Subordinate:coordinate ratio ratio of number of subordinate conjunctions to number of coordinate conjunctions Mean word length mean length, in letters, of each word in the sample Table 3: Complexity features. (Yngve, 1960), require full parses of the sentences (we use the Stanford parser (Klein and Manning, 2003) and Lu’s Syntactic Complexity Analyzer (Lu, 2010)). 3.3.3 CFG features Although many of the complexity features above are derived from parse trees, in this section we present a set of features that take into account the context-free grammar (CFG) labels on each of the nodes. CFG features have been previously used to assess the grammaticality of sentences in an artificial error corpus (Wong and Dras, 2010) and to distinguish human from machine translations (Chae and Nenkova, 2009). However, this is the first time such features have been applied to speech from participants with dementia. In Table 4 we list a few examples of our 134 CFG feature</context>
</contexts>
<marker>Lu, 2010</marker>
<rawString>Xiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4):474–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press.</publisher>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juan Jos´e G Meil´an</author>
<author>Francisco Martinez-S´anchez</author>
<author>Juan Carro</author>
<author>Dolores E L´opez</author>
<author>Lymarie Millian-Morell</author>
<author>Jos´e M Arana</author>
</authors>
<title>Speech in Alzheimer’s disease: Can temporal and acoustic parameters discriminate dementia? Dementia and Geriatric Cognitive Disorders,</title>
<date>2014</date>
<pages>37--5</pages>
<marker>Meil´an, Martinez-S´anchez, Carro, L´opez, Millian-Morell, Arana, 2014</marker>
<rawString>Juan Jos´e G. Meil´an, Francisco Martinez-S´anchez, Juan Carro, Dolores E. L´opez, Lymarie Millian-Morell, and Jos´e M. Arana. 2014. Speech in Alzheimer’s disease: Can temporal and acoustic parameters discriminate dementia? Dementia and Geriatric Cognitive Disorders, 37(5-6):327–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serguei V S Pakhomov</author>
<author>Glen E Smith</author>
<author>Susan Marino</author>
<author>Angela Birnbaum</author>
<author>Neill Graff-Radford</author>
<author>Richard Caselli</author>
<author>Bradley Boeve</author>
<author>David D Knopman</author>
</authors>
<title>A computerized technique to asses language use patterns in patients with frontotemporal dementia.</title>
<date>2010</date>
<journal>Journal of Neurolinguistics,</journal>
<pages>23--127</pages>
<contexts>
<context position="5954" citStr="Pakhomov et al. (2010" startWordPosition="872" endWordPosition="875">rstood and this process is too time-consuming for most clinicians. Peintner et al. (2008) conducted one of the earliest automatic analyses of speech from individuals with FTD, including SD and PNFA as well as a behavioural variant. They considered psycholinguistic features as well as phoneme duration features extracted from the audio samples. Although they were fairly successful in classifying participants according to their subtype, they did not report many details regarding the specific features which were useful or how those features might reflect the underlying impairment of the speakers. Pakhomov et al. (2010a) examined FTD speech from an information-theoretic approach. They constructed a language model of healthy control speech, and then calculated the perplexity and outof-vocabulary rate for each of the patient groups relative to that model. In another study, Pakhomov et al. (2010b) extracted speech and language features from samples of FTD speech. In a principal components analysis, they discovered four components which accounted for most of the variance in their data: speech length, hesitancy, empty content, and grammaticality. However, they did not perform any classification experiments. Fras</context>
<context position="10424" citStr="Pakhomov et al., 2010" startWordPosition="1585" endWordPosition="1588">In PPA, individuals with SD tend to be more impaired with respect to nouns than verbs, and may replace nouns with pronouns or circumlocutory phrases. In contrast, individuals with PNFA may have more difficulty with verbs and may even demonstrate agrammatism, which can result in the omission of grammatical morphemes and function words. Thus, it is often useful to compare the relative frequencies with which words representing the different parts-of-speech (POS) are produced in a sample, as in Table 2. Similar features have been reported in computational studies of MCI (Roark et al., 2011), FTD (Pakhomov et al., 2010b), and DAT (Bucks et al., 2000). Numerous POS taggers exist, although we use the Stanford tagger here (Toutanova et al., 2003). 3.3.2 Complexity features Changes in linguistic complexity may accompany the onset of dementia, although some studies have found a decrease in complexity (e.g. Kemper et al. (2001)) while others have found an increase (e.g. Le et al. (2011)). The features in Table 3 vary in their ease of computation. Mean word length can be calculated from an unsegmented transcript, while mean sentence length requires only sentence boundary segmentation. Other measures, such as Yngve</context>
<context position="18604" citStr="Pakhomov et al., 2010" startWordPosition="2939" endWordPosition="2942">word tokens. Moving-average type-token ratio (MATTRw) TTR calculated over a moving window of size w, and averaged over all windows. Table 7: Vocabulary richness features. 3.3.7 Acoustic features What we call acoustic features are extracted directly from the audio file. We consider the features given in Table 8. Acoustic features have been shown to be useful when automatically detecting conditions such as Parkinson’s disease, in which changes in speech are common (Little et al., 2009; Tsanas et al., 2012). Acoustic features have also been examined in studies of DAT (Meil´an et al., 2014), FTD (Pakhomov et al., 2010b), and PPA (Fraser et al., 2013b, whose software we use here). An obvious benefit to acoustic features is that they do not require a transcription, and can be calculated immediately given an audio sample. The corresponding drawback is that they tell us nothing about the linguistic content of the narrative. 4 Experiments We report the results of three experiments exploring the discriminative power of the different features. We first compare the classification accuracies using each individual feature set. We then perform an ablation study to determine which combination of feature sets leads to </context>
</contexts>
<marker>Pakhomov, Smith, Marino, Birnbaum, Graff-Radford, Caselli, Boeve, Knopman, 2010</marker>
<rawString>Serguei V.S. Pakhomov, Glen E. Smith, Susan Marino, Angela Birnbaum, Neill Graff-Radford, Richard Caselli, Bradley Boeve, and David D. Knopman. 2010a. A computerized technique to asses language use patterns in patients with frontotemporal dementia. Journal of Neurolinguistics, 23:127–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S V Pakhomov</author>
<author>G E Smith</author>
<author>D Chacon</author>
<author>Y Feliciano</author>
<author>N Graff-Radford</author>
<author>R Caselli</author>
<author>D S Knopman</author>
</authors>
<title>Computerized analysis of speech and language to identify psycholinguistic correlates of frontotemporal lobar degeneration. Cognitive and Behavioral Neurology,</title>
<date>2010</date>
<pages>23--165</pages>
<contexts>
<context position="5954" citStr="Pakhomov et al. (2010" startWordPosition="872" endWordPosition="875">rstood and this process is too time-consuming for most clinicians. Peintner et al. (2008) conducted one of the earliest automatic analyses of speech from individuals with FTD, including SD and PNFA as well as a behavioural variant. They considered psycholinguistic features as well as phoneme duration features extracted from the audio samples. Although they were fairly successful in classifying participants according to their subtype, they did not report many details regarding the specific features which were useful or how those features might reflect the underlying impairment of the speakers. Pakhomov et al. (2010a) examined FTD speech from an information-theoretic approach. They constructed a language model of healthy control speech, and then calculated the perplexity and outof-vocabulary rate for each of the patient groups relative to that model. In another study, Pakhomov et al. (2010b) extracted speech and language features from samples of FTD speech. In a principal components analysis, they discovered four components which accounted for most of the variance in their data: speech length, hesitancy, empty content, and grammaticality. However, they did not perform any classification experiments. Fras</context>
<context position="10424" citStr="Pakhomov et al., 2010" startWordPosition="1585" endWordPosition="1588">In PPA, individuals with SD tend to be more impaired with respect to nouns than verbs, and may replace nouns with pronouns or circumlocutory phrases. In contrast, individuals with PNFA may have more difficulty with verbs and may even demonstrate agrammatism, which can result in the omission of grammatical morphemes and function words. Thus, it is often useful to compare the relative frequencies with which words representing the different parts-of-speech (POS) are produced in a sample, as in Table 2. Similar features have been reported in computational studies of MCI (Roark et al., 2011), FTD (Pakhomov et al., 2010b), and DAT (Bucks et al., 2000). Numerous POS taggers exist, although we use the Stanford tagger here (Toutanova et al., 2003). 3.3.2 Complexity features Changes in linguistic complexity may accompany the onset of dementia, although some studies have found a decrease in complexity (e.g. Kemper et al. (2001)) while others have found an increase (e.g. Le et al. (2011)). The features in Table 3 vary in their ease of computation. Mean word length can be calculated from an unsegmented transcript, while mean sentence length requires only sentence boundary segmentation. Other measures, such as Yngve</context>
<context position="18604" citStr="Pakhomov et al., 2010" startWordPosition="2939" endWordPosition="2942">word tokens. Moving-average type-token ratio (MATTRw) TTR calculated over a moving window of size w, and averaged over all windows. Table 7: Vocabulary richness features. 3.3.7 Acoustic features What we call acoustic features are extracted directly from the audio file. We consider the features given in Table 8. Acoustic features have been shown to be useful when automatically detecting conditions such as Parkinson’s disease, in which changes in speech are common (Little et al., 2009; Tsanas et al., 2012). Acoustic features have also been examined in studies of DAT (Meil´an et al., 2014), FTD (Pakhomov et al., 2010b), and PPA (Fraser et al., 2013b, whose software we use here). An obvious benefit to acoustic features is that they do not require a transcription, and can be calculated immediately given an audio sample. The corresponding drawback is that they tell us nothing about the linguistic content of the narrative. 4 Experiments We report the results of three experiments exploring the discriminative power of the different features. We first compare the classification accuracies using each individual feature set. We then perform an ablation study to determine which combination of feature sets leads to </context>
</contexts>
<marker>Pakhomov, Smith, Chacon, Feliciano, Graff-Radford, Caselli, Knopman, 2010</marker>
<rawString>S.V. Pakhomov, G.E. Smith, D. Chacon, Y. Feliciano, N. Graff-Radford, R. Caselli, and D. S. Knopman. 2010b. Computerized analysis of speech and language to identify psycholinguistic correlates of frontotemporal lobar degeneration. Cognitive and Behavioral Neurology, 23:165– 177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyejin Park</author>
<author>Yvonne Rogalski</author>
<author>Amy D Rodriguez</author>
<author>Zvinka Zlatar</author>
<author>Michelle Benjamin</author>
<author>Stacy Harnish</author>
<author>Jeffrey Bennett</author>
<author>John C Rosenbek</author>
<author>Bruce Crosson</author>
<author>Jamie Reilly</author>
</authors>
<title>Perceptual cues used by listeners to discriminate fluent from nonfluent narrative discourse.</title>
<date>2011</date>
<journal>Aphasiology,</journal>
<volume>25</volume>
<issue>9</issue>
<contexts>
<context position="14001" citStr="Park et al. (2011)" startWordPosition="2183" endWordPosition="2186">th of each phrase type (noun phrase, verb phrase, or prepositional phrase), divided by total narrative length Average phrase type length Total number of words in a phrase type, divided by the number of phrases of that type Phrase type rate Number of phrases of a given type, divided by total narrative length Table 4: CFG features. Um Frequency of filled pause um Uh Frequency of filled pause uh NID Frequency of words Not In Dictionary (e.g. paraphasias, neologisms) Verbal rate Number of words per minute Total words Total number of words produced Table 5: Fluency features. 3.3.4 Fluency features Park et al. (2011) found that listeners’ judgements of fluency were affected by a number of different variables, and the three most discriminative features were “speech rate, speech productivity, and audible struggle.” For our list of fluency features (Table 5), we include only those features which could be extracted from the transcripts alone (assuming the total speech time is given). We count pauses filled by um and uh separately, as research has suggested that they may indicate different cognitive processes (Clark and Fox Tree, 2002). The number of words in a sample could be easily generated using the word c</context>
</contexts>
<marker>Park, Rogalski, Rodriguez, Zlatar, Benjamin, Harnish, Bennett, Rosenbek, Crosson, Reilly, 2011</marker>
<rawString>Hyejin Park, Yvonne Rogalski, Amy D. Rodriguez, Zvinka Zlatar, Michelle Benjamin, Stacy Harnish, Jeffrey Bennett, John C. Rosenbek, Bruce Crosson, and Jamie Reilly. 2011. Perceptual cues used by listeners to discriminate fluent from nonfluent narrative discourse. Aphasiology, 25(9):998–1015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Peintner</author>
<author>William Jarrold</author>
<author>Dimitra Vergyri</author>
<author>Colleen Richey</author>
<author>Maria Luisa Gorno Tempini</author>
<author>Jennifer Ogar</author>
</authors>
<title>Learning diagnostic models using speech and language measures.</title>
<date>2008</date>
<booktitle>In Engineering in Medicine and Biology Society,</booktitle>
<pages>4648--4651</pages>
<contexts>
<context position="5422" citStr="Peintner et al. (2008)" startWordPosition="787" endWordPosition="790">ni et al., 2011), there is no one test which can provide a diagnosis. Classification of PPA into subtypes requires evaluation of spoken output, as well as neuropsychological assessment and brain imaging. Qualitative evaluation of speech often can be done accurately by clinicians or researchers, but the ability to do this evaluation can require years of training and experience. Some researchers have performed detailed quantitative characterization of speech in PPA, but the precise characteristics of speech are not yet fully understood and this process is too time-consuming for most clinicians. Peintner et al. (2008) conducted one of the earliest automatic analyses of speech from individuals with FTD, including SD and PNFA as well as a behavioural variant. They considered psycholinguistic features as well as phoneme duration features extracted from the audio samples. Although they were fairly successful in classifying participants according to their subtype, they did not report many details regarding the specific features which were useful or how those features might reflect the underlying impairment of the speakers. Pakhomov et al. (2010a) examined FTD speech from an information-theoretic approach. They </context>
</contexts>
<marker>Peintner, Jarrold, Vergyri, Richey, Tempini, Ogar, 2008</marker>
<rawString>Bart Peintner, William Jarrold, Dimitra Vergyri, Colleen Richey, Maria Luisa Gorno Tempini, and Jennifer Ogar. 2008. Learning diagnostic models using speech and language measures. In Engineering in Medicine and Biology Society, 2008. EMBS 2008. 30th Annual International Conference of the IEEE, pages 4648–4651.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Margaret Mitchell</author>
<author>John-Paul Hosom</author>
<author>Kristy Hollingshead</author>
<author>Jeffery Kaye</author>
</authors>
<title>Spoken language derived measures for detecting mild cognitive impairment.</title>
<date>2011</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>19</volume>
<issue>7</issue>
<contexts>
<context position="3585" citStr="Roark et al., 2011" startWordPosition="521" endWordPosition="524">eless, our results are consistent with, and extend, previous work in the field. 2 Background In recent years, there has been growing interest in using computer techniques to automatically detect dementia from speech and language features derived from a sample of narrative speech. Some researchers have explored ways to use methods such as part-of-speech tagging, statistical parsing, and speech signal analysis to detect disorders such as dementia of the Alzheimer’s type (DAT) (Bucks et al., 2000; Singh et al., 2001; Thomas et al., 2005; Jarrold et al., 2010) and mild cognitive impairment (MCI) (Roark et al., 2011). Here, we focus on a type of dementia called primary progressive aphasia (PPA). PPA is a subtype of frontotemporal dementia (FTD) which is characterized by progressive language impairment without other notable cognitive impairment. There are three subtypes of PPA: semantic dementia (SD), progressive nonfluent aphasia (PNFA), and logopenic progressive aphasia (LPA). SD, sometimes called “fluent” progressive aphasia, is typically marked by fluent but empty speech, anomia, 17 Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 17–26, B</context>
<context position="10396" citStr="Roark et al., 2011" startWordPosition="1580" endWordPosition="1583">ferent types of dementia. In PPA, individuals with SD tend to be more impaired with respect to nouns than verbs, and may replace nouns with pronouns or circumlocutory phrases. In contrast, individuals with PNFA may have more difficulty with verbs and may even demonstrate agrammatism, which can result in the omission of grammatical morphemes and function words. Thus, it is often useful to compare the relative frequencies with which words representing the different parts-of-speech (POS) are produced in a sample, as in Table 2. Similar features have been reported in computational studies of MCI (Roark et al., 2011), FTD (Pakhomov et al., 2010b), and DAT (Bucks et al., 2000). Numerous POS taggers exist, although we use the Stanford tagger here (Toutanova et al., 2003). 3.3.2 Complexity features Changes in linguistic complexity may accompany the onset of dementia, although some studies have found a decrease in complexity (e.g. Kemper et al. (2001)) while others have found an increase (e.g. Le et al. (2011)). The features in Table 3 vary in their ease of computation. Mean word length can be calculated from an unsegmented transcript, while mean sentence length requires only sentence boundary segmentation. O</context>
</contexts>
<marker>Roark, Mitchell, Hosom, Hollingshead, Kaye, 2011</marker>
<rawString>Brian Roark, Margaret Mitchell, John-Paul Hosom, Kristy Hollingshead, and Jeffery Kaye. 2011. Spoken language derived measures for detecting mild cognitive impairment. IEEE Transactions on Audio, Speech, and Language Processing, 19(7):2081–2090.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor M Saffran</author>
<author>Rita Sloan Berndt</author>
<author>Myrna F Schwartz</author>
</authors>
<title>The quantitative analysis of agrammatic production: procedure and data. Brain and Language,</title>
<date>1989</date>
<pages>37--440</pages>
<contexts>
<context position="8329" citStr="Saffran et al. (1989)" startWordPosition="1252" endWordPosition="1255">usion criteria included a known history of drug or alcohol abuse and a history of neurological or major psychiatric illness. Each patient was diagnosed by a behavioural neurologist and all met current criteria for PPA (Gorno-Tempini et al., 2011). Table 1 shows demographic information for each group. To elicit a sample of narrative speech, participants were asked to tell the well-known story of Cinderella. They were given a wordless picture book to remind them of the story; then the book was removed and they were asked to tell the story in their own words. This procedure, described in full by Saffran et al. (1989), is commonly used in studies of connected speech in aphasia. The narrative samples were transcribed by trained research assistants. The transcriptions include filled pauses, repetitions, and false starts, and were annotated with the total speech time. Sentence boundaries were marked according to semantic, syntactic, and prosodic cues. 3.2 Classification framework Given the audio files and transcripts, we can then calculate our features (described in detail below) 18 and use them to train a support vector machine (SVM) classifier. We use a leave-one-out crossvalidation framework and report the</context>
</contexts>
<marker>Saffran, Berndt, Schwartz, 1989</marker>
<rawString>Eleanor M. Saffran, Rita Sloan Berndt, and Myrna F. Schwartz. 1989. The quantitative analysis of agrammatic production: procedure and data. Brain and Language, 37:440–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Singh</author>
<author>Romola S Bucks</author>
<author>Joanne M Cuerden</author>
</authors>
<title>Evaluation of an objective technique for analysing temporal variables in DAT spontaneous speech.</title>
<date>2001</date>
<journal>Aphasiology,</journal>
<volume>15</volume>
<issue>6</issue>
<contexts>
<context position="3484" citStr="Singh et al., 2001" startWordPosition="504" endWordPosition="507">eatures on a relatively small data set, we emphasize that this work is exploratory in nature; nonetheless, our results are consistent with, and extend, previous work in the field. 2 Background In recent years, there has been growing interest in using computer techniques to automatically detect dementia from speech and language features derived from a sample of narrative speech. Some researchers have explored ways to use methods such as part-of-speech tagging, statistical parsing, and speech signal analysis to detect disorders such as dementia of the Alzheimer’s type (DAT) (Bucks et al., 2000; Singh et al., 2001; Thomas et al., 2005; Jarrold et al., 2010) and mild cognitive impairment (MCI) (Roark et al., 2011). Here, we focus on a type of dementia called primary progressive aphasia (PPA). PPA is a subtype of frontotemporal dementia (FTD) which is characterized by progressive language impairment without other notable cognitive impairment. There are three subtypes of PPA: semantic dementia (SD), progressive nonfluent aphasia (PNFA), and logopenic progressive aphasia (LPA). SD, sometimes called “fluent” progressive aphasia, is typically marked by fluent but empty speech, anomia, 17 Workshop on Computat</context>
</contexts>
<marker>Singh, Bucks, Cuerden, 2001</marker>
<rawString>Sameer Singh, Romola S. Bucks, and Joanne M. Cuerden. 2001. Evaluation of an objective technique for analysing temporal variables in DAT spontaneous speech. Aphasiology, 15(6):571–583.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Stadthagen-Gonzalez</author>
<author>Colin J Davis</author>
</authors>
<title>The Bristol norms for age of acquisition, imageability, and familiarity.</title>
<date>2006</date>
<journal>Behavior Research Methods,</journal>
<volume>38</volume>
<issue>4</issue>
<contexts>
<context position="15266" citStr="Stadthagen-Gonzalez and Davis, 2006" startWordPosition="2376" endWordPosition="2379">ing software (although we first exclude filled pauses and NID tokens), and the verbal rate can subsequently be calculated directly. The other three features are easily calculated using string matching and an electronic dictionary. 3.3.5 Psycholinguistic features Some types of dementia are characterized by impairments in semantic access. Such impairments may be sensitive to psycholinguistic features such as lexical frequency, familiarity, imageability, and age of acquisition (Table 6). We use the SUBTL frequency norms (Brysbaert and New, 2009) and the combined Bristol and Gilhooly-Logie norms (Stadthagen-Gonzalez and Davis, 2006; Gilhooly and Logie, 1980) for familiarity, imageability, and Frequency Frequency with which a word occurs in some corpus of natural language Familiarity Subjective rating of how familiar a word seems Imageability Subjective rating of how easily a word generates an image in the mind Age of acquisition Subjective rating of how old a person is when they first learn that word Light verbs Number of occurrences of be, have, come, go, give, take, make, do, get, move, and put, normalized by total number of verbs Table 6: Psycholinguistic features. age of acquisition (see Table 6). We compute the ave</context>
</contexts>
<marker>Stadthagen-Gonzalez, Davis, 2006</marker>
<rawString>Hans Stadthagen-Gonzalez and Colin J. Davis. 2006. The Bristol norms for age of acquisition, imageability, and familiarity. Behavior Research Methods, 38(4):598–605.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Calvin Thomas</author>
<author>Vlado Keselj</author>
<author>Nick Cercone</author>
<author>Kenneth Rockwood</author>
<author>Elissa Asp</author>
</authors>
<title>Automatic detection and rating of dementia of Alzheimer type through lexical analysis of spontaneous speech.</title>
<date>2005</date>
<booktitle>In Proceedings of the IEEE International Conference on Mechatronics and Automation,</booktitle>
<pages>1569--1574</pages>
<contexts>
<context position="3505" citStr="Thomas et al., 2005" startWordPosition="508" endWordPosition="511">ely small data set, we emphasize that this work is exploratory in nature; nonetheless, our results are consistent with, and extend, previous work in the field. 2 Background In recent years, there has been growing interest in using computer techniques to automatically detect dementia from speech and language features derived from a sample of narrative speech. Some researchers have explored ways to use methods such as part-of-speech tagging, statistical parsing, and speech signal analysis to detect disorders such as dementia of the Alzheimer’s type (DAT) (Bucks et al., 2000; Singh et al., 2001; Thomas et al., 2005; Jarrold et al., 2010) and mild cognitive impairment (MCI) (Roark et al., 2011). Here, we focus on a type of dementia called primary progressive aphasia (PPA). PPA is a subtype of frontotemporal dementia (FTD) which is characterized by progressive language impairment without other notable cognitive impairment. There are three subtypes of PPA: semantic dementia (SD), progressive nonfluent aphasia (PNFA), and logopenic progressive aphasia (LPA). SD, sometimes called “fluent” progressive aphasia, is typically marked by fluent but empty speech, anomia, 17 Workshop on Computational Linguistics and</context>
</contexts>
<marker>Thomas, Keselj, Cercone, Rockwood, Asp, 2005</marker>
<rawString>Calvin Thomas, Vlado Keselj, Nick Cercone, Kenneth Rockwood, and Elissa Asp. 2005. Automatic detection and rating of dementia of Alzheimer type through lexical analysis of spontaneous speech. In Proceedings of the IEEE International Conference on Mechatronics and Automation, pages 1569–1574.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="10551" citStr="Toutanova et al., 2003" startWordPosition="1606" endWordPosition="1609"> circumlocutory phrases. In contrast, individuals with PNFA may have more difficulty with verbs and may even demonstrate agrammatism, which can result in the omission of grammatical morphemes and function words. Thus, it is often useful to compare the relative frequencies with which words representing the different parts-of-speech (POS) are produced in a sample, as in Table 2. Similar features have been reported in computational studies of MCI (Roark et al., 2011), FTD (Pakhomov et al., 2010b), and DAT (Bucks et al., 2000). Numerous POS taggers exist, although we use the Stanford tagger here (Toutanova et al., 2003). 3.3.2 Complexity features Changes in linguistic complexity may accompany the onset of dementia, although some studies have found a decrease in complexity (e.g. Kemper et al. (2001)) while others have found an increase (e.g. Le et al. (2011)). The features in Table 3 vary in their ease of computation. Mean word length can be calculated from an unsegmented transcript, while mean sentence length requires only sentence boundary segmentation. Other measures, such as Yngve depth Nouns # nouns / # words Verbs # verbs / # words Noun-verb ratio # nouns / # verbs Noun ratio # nouns / (# nouns + # verb</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Athanasios Tsanas</author>
<author>Max A Little</author>
<author>Patrick E McSharry</author>
<author>Jennifer Spielman</author>
<author>Lorraine O Ramig</author>
</authors>
<title>Novel speech signal processing algorithms for high-accuracy classification of Parkinson’s disease.</title>
<date>2012</date>
<journal>IEEE Transactions on Biomedical Engineering,</journal>
<volume>59</volume>
<issue>5</issue>
<contexts>
<context position="18492" citStr="Tsanas et al., 2012" startWordPosition="2920" endWordPosition="2923">e number of word tokens. Type-token ratio (TTR) V/N where V is the number of word types and N is the number of word tokens. Moving-average type-token ratio (MATTRw) TTR calculated over a moving window of size w, and averaged over all windows. Table 7: Vocabulary richness features. 3.3.7 Acoustic features What we call acoustic features are extracted directly from the audio file. We consider the features given in Table 8. Acoustic features have been shown to be useful when automatically detecting conditions such as Parkinson’s disease, in which changes in speech are common (Little et al., 2009; Tsanas et al., 2012). Acoustic features have also been examined in studies of DAT (Meil´an et al., 2014), FTD (Pakhomov et al., 2010b), and PPA (Fraser et al., 2013b, whose software we use here). An obvious benefit to acoustic features is that they do not require a transcription, and can be calculated immediately given an audio sample. The corresponding drawback is that they tell us nothing about the linguistic content of the narrative. 4 Experiments We report the results of three experiments exploring the discriminative power of the different features. We first compare the classification accuracies using each in</context>
</contexts>
<marker>Tsanas, Little, McSharry, Spielman, Ramig, 2012</marker>
<rawString>Athanasios Tsanas, Max A. Little, Patrick E. McSharry, Jennifer Spielman, and Lorraine O. Ramig. 2012. Novel speech signal processing algorithms for high-accuracy classification of Parkinson’s disease. IEEE Transactions on Biomedical Engineering, 59(5):1264–1271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Parser features for sentence grammaticality classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop,</booktitle>
<pages>67--75</pages>
<contexts>
<context position="12798" citStr="Wong and Dras, 2010" startWordPosition="1978" endWordPosition="1981">ctions Mean word length mean length, in letters, of each word in the sample Table 3: Complexity features. (Yngve, 1960), require full parses of the sentences (we use the Stanford parser (Klein and Manning, 2003) and Lu’s Syntactic Complexity Analyzer (Lu, 2010)). 3.3.3 CFG features Although many of the complexity features above are derived from parse trees, in this section we present a set of features that take into account the context-free grammar (CFG) labels on each of the nodes. CFG features have been previously used to assess the grammaticality of sentences in an artificial error corpus (Wong and Dras, 2010) and to distinguish human from machine translations (Chae and Nenkova, 2009). However, this is the first time such features have been applied to speech from participants with dementia. In Table 4 we list a few examples of our 134 CFG features, as well as the three phrase-level features (calculated for noun phrases, verb phrases, and prepositional phrases). 19 NP → NNS Noun phrases consisting of only a plural noun VP → VBN PP Verb phrases consisting of a pastparticiple verb and a prepositional phrase ROOT → INTJ Trees consisting of only an interjection Phrase type proportion Length of each phra</context>
</contexts>
<marker>Wong, Dras, 2010</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2010. Parser features for sentence grammaticality classification. In Proceedings of the Australasian Language Technology Association Workshop, pages 67–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Yngve</author>
</authors>
<title>A model and hypothesis for language structure.</title>
<date>1960</date>
<booktitle>Proceedings of the American Physical Society,</booktitle>
<pages>104--444</pages>
<contexts>
<context position="12297" citStr="Yngve, 1960" startWordPosition="1899" endWordPosition="1900">ged over all sentences Total depth total sum of the Yngve depths of each node in the parse tree, averaged over all sentences Tree height height of each parse tree, averaged over all sentences MLS mean length of sentence MLC mean length of clause MLT mean length of T-unit Subordinate conjunctions number of subordinate conjunctions Coordinate conjunctions number of coordinate conjunctions Subordinate:coordinate ratio ratio of number of subordinate conjunctions to number of coordinate conjunctions Mean word length mean length, in letters, of each word in the sample Table 3: Complexity features. (Yngve, 1960), require full parses of the sentences (we use the Stanford parser (Klein and Manning, 2003) and Lu’s Syntactic Complexity Analyzer (Lu, 2010)). 3.3.3 CFG features Although many of the complexity features above are derived from parse trees, in this section we present a set of features that take into account the context-free grammar (CFG) labels on each of the nodes. CFG features have been previously used to assess the grammaticality of sentences in an artificial error corpus (Wong and Dras, 2010) and to distinguish human from machine translations (Chae and Nenkova, 2009). However, this is the </context>
</contexts>
<marker>Yngve, 1960</marker>
<rawString>Victor Yngve. 1960. A model and hypothesis for language structure. Proceedings of the American Physical Society, 104:444–466.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
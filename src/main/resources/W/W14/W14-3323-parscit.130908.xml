<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011814">
<title confidence="0.9945505">
Manawi: Using Multi-Word Expressions and Named Entities to Improve
Machine Translation
</title>
<author confidence="0.591255">
Liling Tan and Santanu Pal
</author>
<bodyText confidence="0.47291275">
Applied Linguistics, Translation and Interpretation Department
Universit¨at des Saarlandes
liling.tan@uni-saarland.de
santanu.pal@uni-saarland.de
</bodyText>
<sectionHeader confidence="0.965904" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999582055555556">
We describe the Manawi1 (�F-ft) sys-
tem submitted to the 2014 WMT transla-
tion shared task. We participated in the
English-Hindi (EN-HI) and Hindi-English
(HI-EN) language pair and achieved 0.792
for the Translation Error Rate (TER)
score2 for EN-HI, the lowest among the
competing systems. Our main innova-
tions are (i) the usage of outputs from
NLP tools, viz. billingual multi-word ex-
pression extractor and named-entity rec-
ognizer to improve SMT quality and (ii)
the introduction of a novel filter method
based on sentence-alignment features. The
Manawi system showed the potential of
improving translation quality by incorpo-
rating multiple NLP tools within the MT
pipeline.
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995725">
In this paper, we present Saarland University
(USAAR) submission to Workshop for Machine
Translation 2014 (WMT 2014) using the Manawi
MT system. We participated in the generic trans-
lation shared task for the English-Hindi (EN-HI)
and Hindi-English (HI-EN) language pairs.
Our Manawi system showcased the incorpora-
tion of NLP tools output within the MT pipeline; a
bilingual MWE extractor and a bilingual NE rec-
ognizer for English and Hindi were implemented.
The output from these NLP tools was appended to
the training corpus prior to the SMT model train-
ing with the MOSES toolkit (Koehn et al., 2007).
The resulting system achieves the lowest Transla-
tion Error Rate (TER) among competing systems
for the English-Hindi language pair.
</bodyText>
<footnote confidence="0.830280666666667">
1Multi-word expression And Named-entity And
Wikipedia titles (Manawi)
2Lower TER often results in better translation
</footnote>
<bodyText confidence="0.999551">
The rest of the paper is structured as follow:
Section 2 describes the implementation of the NLP
tools; Section 3 outlines the corpus pre-processing
before the MT training process; Section 4 de-
scribes the MT system setup; Section 5 describes
a simple post-processing component to handle
Out-Of-Vocabulary words; Section 6 presents the
WMT shared task results for the Manawi system
and Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.996174" genericHeader="method">
2 NLP Tools Implementation
</sectionHeader>
<subsectionHeader confidence="0.952158">
2.1 Bilingual MWE in MT
</subsectionHeader>
<bodyText confidence="0.999504607142857">
Multi-Word Expressions (MWE) are defined as
“idiosyncratic interpretations that cross word
boundaries” (Sag et al., 2002). MWE can be made
up of collocations (e.g. seem ridiculous : behuda
dikhai), frozen expressions (e.g. exception han-
dling : apavada sancalaka) or name entities (e.g.
Johnny Cash : Johni Kesh). Jackendoff (1997)
claims that the frequency of MWE and the fre-
quency of single words in a speaker’s lexicon are
almost equivalent.
Bilingual MWE has shown to be useful for
a variety of NLP applications such as multilin-
gual information retrieval (Vechtomova, 2005)
and Crosslingual/Multilingual Word Sense Dis-
ambiguation (Tan and Bond, 2013; Finlayson and
Kulkarni, 2011). For machine translation, vari-
ous studies had introduced bilingual MWE to im-
prove MT system performance. Lambert (2005)
introduced bilingual MWE by grouping them as
a single token before training alignment models
and they showed that it improved alignment and
translation quality. Ren et al. (2009) integrated
an in-domain bilingual MWE using log likelihood
ratio based hierarchical reducing algorithm and
gained +0.61 BLEU score. Similarly, Santanu et
al. (2010) single tokenized MWE before training a
phrase-based SMT model and achieved 50% im-
provement in BLEU score.
</bodyText>
<page confidence="0.975372">
201
</page>
<bodyText confidence="0.857277777777778">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 201–206,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
In order to improve the word alignment quality,
Venkatapathy and Joshi (2006) reported a discrim-
inative approach to use the compositionality infor-
mation of verb-based multi-word expressions. Pal
et al. (2011) discussed the effects of incorporating
prior alignment of MWE and NEs directly or indi-
rectly into Phrase-based SMT systems.
</bodyText>
<subsectionHeader confidence="0.997425">
2.2 Bilingual MWE Extraction
</subsectionHeader>
<bodyText confidence="0.999957424242424">
Monolingual MWE extraction revolves around
three approaches (i) rule-based methods relying
on morphosyntactic patterns, (ii) statistical meth-
ods which use association/frequency measures to
determine ngrams as MWE and (iii) hybrid ap-
proaches that combine the rule-based and statis-
tical methods.
However, where bilingual MWE extraction
techniques are concerned, they operate around
two main modus operandi (i) extracting mono-
lingual MWE separately and aligning them at
word/phrasal level afterwards or (ii) aligning par-
allel text at word/phrasal level and then extracting
MWE.
We implemented a language independent bilin-
gual MWE extractor, (Muwee), that produces a
parallel dictionary of MWE without the need for
any word/phrasal-level alignment. Muwee makes
use of the fact that the number of highly collocated
MWE should be the same for each sentences pair.
Muwee first extracts MWE separately from the
source and target sentences; the MWE are ex-
tracted based on bigrams that reports a Point-
wise Mutual Information (PMI) score of above
10. Then for each parallel sentence, if the number
of MWE are equivalent for the source and target,
the bigrams are joint together as a string and con-
tiguous duplicate words are deleted. The removal
of contiguous duplicate words is grounded on the
fact that linguistically motivated MWE that forms
grammatical phrases had shown to improve SMT
performances (Pal et al., 2013). Figure 1 presents
an example of the MWE extraction process.
</bodyText>
<figureCaption confidence="0.9979">
Figure 1: Muwee Extraction Process
</figureCaption>
<subsectionHeader confidence="0.992691">
2.3 Named-entity Recognition
</subsectionHeader>
<bodyText confidence="0.999292212121212">
Named-Entity (NE) recognition is the task of iden-
tifying entities such as names of people, organi-
zations and locations. Given a perfect MWE ex-
traction system, NEs would have been captured by
MWE extraction. However, the state-of-art MWE
extractors have yet been perfected.
To compliment the MWE extracted by Muwee,
we implemented a bilingual NE extractor by
combining outputs from the (i) Stanford English
NE Recognizer (NER)3 and (ii) a Do-It-Yourself
(DIY) Hindi NER using CRF++ toolkit4 with an-
notated data from NER-SSEA 2008 shared task
(Rajeev Sangal and Singh, 2008). We trained a
Conditional Random Field classifier for the Hindi
NER using unigram features, bigram features and
a context window of two words to the left and to
the right. And we used the DIY Hindi NER and
Stanford NER tool to monolingually annotate the
NEs from training corpus for the EN-HI / HI-EN
language pair.
Similar to the Muwee bilingual extraction cri-
teria, if the number of NEs are the same on the
source and target language, the NEs were joint to-
gether as a string. We note that sometimes the
bilingual NER output contains more than one NE
per sentence. For example, our bilingual NER ex-
tractor outputs “Kalpna Chawla Gurdeep Pand-
her”, which contains two NEs ‘Kalpna Chawla’
and ‘Gurdeep Pandher’. Although the resulting
bilingual NE does not provide a perfect NE dic-
tionary, it filters out NEs from the sentence and
improves word alignments at the start of the MT
pipeline.
</bodyText>
<sectionHeader confidence="0.963998" genericHeader="method">
3 Corpus Preprocessing
</sectionHeader>
<bodyText confidence="0.998986666666667">
The performance of any data driven SMT depends
on the quality of training data. Previous stud-
ies had shown that filtering out low quality sen-
tence pairs improves the quality of machine trans-
lation. For instance, the Moore-Lewis filter re-
moves sentence pairs based on source-side cross-
entropy differences (Moore and Lewis, 2010) and
the Edinburgh’s MT system used the Modified
Moore-Lewis filtering (Axelrod et al., 2011) in
WMT 2013 shared task (Durrani et al., 2013).
CNGL-DCU system extended the Moore-Lewis
filter by incorporating lemmas and named enti-
</bodyText>
<footnote confidence="0.999923">
3http://nlp.stanford.edu/software/CRF-NER.shtml
4http://crfpp.googlecode.com
</footnote>
<page confidence="0.997147">
202
</page>
<bodyText confidence="0.99875195">
ties in their definition of perplexity5 (Rubino et al.,
2013; Toral, 2013).
The RWTH Aachen system filtered the Com-
mon Crawl Corpus by keeping only sentence pairs
that contains at least 70% of the word from a
known vocabulary dataset extracted from the other
corpora in the WMT 2013 shared task (Peitz et
al., 2013). The Docent system from Uppsala Uni-
versity also performed data cleaning on the Com-
mon Crawl dataset prior to SMT but they were
using more aggressive conditions by (i) remov-
ing documents that were identified correctly us-
ing a language identification module and (ii) re-
moving documents that falls below a threshold
value of alignment points and sentence length ra-
tio (Stymne et al., 2013). Our approach to data
cleaning is similar to the Uppsala’s system but in-
stead of capitalizing on word-alignments features,
we were cleaning the data based on sentence align-
ment features.
</bodyText>
<subsectionHeader confidence="0.979829">
3.1 GaCha Filtering: Filter by Character
Mean Ratio
</subsectionHeader>
<bodyText confidence="0.999423346153846">
Stymne et al. (2013) improved translation qual-
ity by cleaning the Common Crawl corpus during
the WMT 2013 shared task. They filtered out doc-
uments exceeding 60 words and cleaned the re-
mainder of the corpus by exploiting the number
of alignment points in word alignments between
sentence pairs. Their hypothesis was that sentence
pairs with very few alignment points in the inter-
section would mostly likely not be parallel. This
is based on the fact that when using GIZA++ (Och
and Ney, 2003), the intersection of alignments is
more sparse than the standard SMT symmetriza-
tion heuristics like grow-diag-final-and (Koehn,
2005).
Different from Stymne et al., our hypothesis for
non-parallelness adheres to sentence level align-
ment criteria as defined in the Gale-Church algo-
rithm (Gale and Church, 1993). If a sentence pair
is parallel, the ratio of the number of characters in
the source and target sentence should be coherent
to the global ratio of the number of source-target
characters in a fully parallel corpus. The Gale-
Church algorithm had its parameters tuned to suit
European languages and Tan (2013) had demon-
strated that sentence-level alignments can be im-
proved by using corpus specific parameters. When
</bodyText>
<footnote confidence="0.8940905">
5The exponent of cross-entropy may be regarded as per-
plexity
</footnote>
<bodyText confidence="0.999906842105263">
using variable parameters to the Gale-Church al-
gorithm, Tan showed that instead of the default
parameters set in the original Gale-Church algo-
rithm, using mean ratio of the noisy corpus can
also improve sentence level alignments although
the ratio from a clean corpus would achieve even
better alignments.
Given the premises of the sentence level align-
ment hypothesis, we clean the training corpus by
first calculating the global mean ratio of the num-
ber of characters of source sentence to target sen-
tence and then filter out sentence pairs that exceeds
or fall below 20% of the global ratio. We call this
method, GaCha filtering; this cleaning method is
more aggressive than cleaning methods described
by Stymne et al. but it filters out noisy sen-
tence level alignments created by non-language
specific parameters used by sentence aligners such
as Gale-Church algorithm.
</bodyText>
<subsectionHeader confidence="0.99968">
3.2 Filtering Noise in HindEnCorp
</subsectionHeader>
<bodyText confidence="0.9999365">
After manual inspection 100 random sentence
pairs from the HindEnCorp (Bojar et al., 2014),
we found that documents were often misaligned
at sentence level or contains HTML special char-
acters. To further reduce the noise in the Hin-
dEnCorp, the Manawi system was only trained
a subset of the HindEnCorp from the follow-
ing sources (i) DanielPipes, (ii) TIDES and (iii)
EILMT. Lastly, we filtered the training data on al-
lowing a maximum of 100 tokens per language per
sentence.
Finally, the cleaned data contained 87,692 sen-
tences, only ∼36% of the original HindEnCorp
training data.
</bodyText>
<sectionHeader confidence="0.994263" genericHeader="method">
4 System Setup
</sectionHeader>
<bodyText confidence="0.99967725">
Data: To train the baseline translation model,
we have used the cleaned subset of the data as
described in Section 3. For the Manawi model,
we added the NLP outputs from the MWE and
NE extractors presented in Section 2. To train the
monolingual language model, we used the Hindi
sentences from the HindEnCorp.
System: We used the standard log-linear
Phrase based SMT model provided from the
MOSES toolkit.
Configuration: We experimented with various
maximum phrase length for the translation and n-
</bodyText>
<page confidence="0.99793">
203
</page>
<table confidence="0.990893857142857">
Manawi Submissions (EN-HI) BLEU BLEU TER
(cased)
PB-SMT + MWE + NE 9.9 7.1 0.869
PB-SMT + MWE + NE + Wiki (Manawi) 7.7 7.6 0.864
Manawi + GaCha Filter 8.9 8.9 0.818
Manawi + GaCha Filter + Handle OOV 8.8 8.8 0.800
Manawi + GaCha Filter + Remove OOV 8,9 8.8 0.792
</table>
<tableCaption confidence="0.991774">
Table 1: Manawi System Submissions @ WMT 2014 Translation Shared Task for English-Hindi
</tableCaption>
<table confidence="0.9874345">
Manawi Submissions (HI-EN) BLEU BLEU TER
(cased)
PB-SMT + MWE + NE + Wiki (Manawi) 7.7 7.6 0.864
Manawi + GaCha Filter 8.9 8.9 0.818
</table>
<tableCaption confidence="0.997814">
Table 2: Manawi System Submissions @ WMT 2014 Translation Shared Task for Hindi-English
</tableCaption>
<bodyText confidence="0.991931666666667">
gram settings for the language model. And we
found that using a maximum phrase length of 5
and 4-gram language model produced best result
in terms of BLEU and TER for our baseline model
(i.e. without the incorporation of outputs from the
NLP tools). The other experimental settings were:
</bodyText>
<listItem confidence="0.9015798125">
• GIZA++ implementation of IBM word align-
ment model 4 with grow-diagonal-final-and
heuristics for performing word alignment and
phrase-extraction (Koehn et al., 2003)
• Minimum Error Rate Training (MERT) (Och,
2003) on a held-out development set, target
language model with Kneser-Ney smoothing
(Kneser and Ney, 1995) using language mod-
els trained with SRILM (Stolcke, 2002)
• Reordering model6 was trained on bidirec-
tional (i.e. using both forward and back-
ward models) and conditioned on both source
and target language. The reordering model
is built by calculating the probabilities of the
phrase pair being associated with the given
orientation.
</listItem>
<bodyText confidence="0.999320333333333">
Innovation: We demonstrated the incorporation
of multiple NLP tools outputs in the SMT pipline
by simply using automatically extracted bilingual
MWE and NEs as additional parallel data to the
cleaned data and ran the translation and statistical
model as per the baseline configurations.
</bodyText>
<footnote confidence="0.99580375">
6For reordering we used lexicalized reordering model,
which consists of three different types of reordering by
conditioning the orientation of previous and next phrases-
monotone (m), swap (s) and discontinuous (d).
</footnote>
<sectionHeader confidence="0.84224" genericHeader="method">
5 Post-processing
</sectionHeader>
<bodyText confidence="0.999955">
The MOSES decoder produces translations with
Out-Of-Vocabulary (OOV) words that were not
translated from the source language. The Manawi
system post-processed the decoder output by (i)
handling OOV words by replacing each OOV
word with the most probable translation using the
lexical files generated by GIZA++ and (ii) remov-
ing OOV words from the decoded outputs.
</bodyText>
<sectionHeader confidence="0.999841" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999929857142857">
Table 1 summarizes the Manawi system sub-
missions for the English-Hindi language pair for
WMT 2014 generic translation shared task. The
basic Manawi system is a Phrase-based SMT
(PB-SMT) setup using extracted MWE and NEs
and Wikipedia titles as additional parallel data (i.e.
PB-SMT+MWE+NE+Wiki in Table 1). The ba-
sic Manawi system achieved 7.7 BLEU score and
0.864 TER.
After filtering the data before training the trans-
lation model, the Manawi system performed bet-
ter at 8.9 BLEU and 0.818 TER. By adding the
post-processing component, we achieved the low-
est TER score among competing team at 0.792.
</bodyText>
<sectionHeader confidence="0.993769" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999947857142857">
The Manawi system showed how simple yet ef-
fective pre-processing and integration of output
from NLP tools improves the performance of MT
systems. Using GaCha filtering to remove noisy
data and using automatically extracted MWE and
NEs as additional parallel data improve word and
phrasal alignments at the start of the MT pipeline
</bodyText>
<page confidence="0.99659">
204
</page>
<bodyText confidence="0.999973">
which eventually improves the quality of machine
translation. The best setup for the Manawi system
achieved the best TER score among the competing
system.
Also, the incremental improvements made by
step-wise implementation of (i) filtering, (ii) in-
corporating outputs from NLP tools and (iii) post-
processing showed that individual components of
the Manawi can be integrated into other MT sys-
tems without detrimental effects.
</bodyText>
<sectionHeader confidence="0.983951" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999611111111111">
The research leading to these results has received
funding from the People Programme (Marie
Curie Actions) of the European Union’s Seventh
Framework Programme FP7/2007-2013/ under
REA grant agreement n ◦ 317471.
The authors of this paper also thank our col-
leagues J¨org Knappen and Jos´e M.M. Martinez
for their help in setting up the server that made the
Manawi system possible.
</bodyText>
<sectionHeader confidence="0.998162" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998201512195121">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 355–362. Association for Computational Lin-
guistics.
Ond&amp;quot;rej Bojar, Vojt&amp;quot;ech Diatka, Pavel Rychl´y, Pavel
Stra&amp;quot;n´ak, Ale&amp;quot;s Tamchyna, and Dan Zeman. 2014.
Hindi-English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth Interna-
tional Language Resources and Evaluation Confer-
ence (LREC’14), Reykjavik, Iceland, may. ELRA,
European Language Resources Association. in prep.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburghs machine transla-
tion systems for european language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 112–119.
Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting multi-word expressions improves word sense
disambiguation. In Proceedings of the Workshop on
Multiword Expressions: From Parsing and Gener-
ation to the Real World, MWE ’11, pages 20–24,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
William A Gale and Kenneth W Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational linguistics, 19(1):75–102.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. MT summit, 5:79–
86.
Patrik Lambert. 2005. Data inferred multi-word ex-
pressions for statistical machine translation. In In
MT Summit X.
Robert C Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220–224. Association for Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.
Santanu Pal, Tanmoy Chakraborty, and Sivaji Bandy-
opadhyay. 2011. Handling multiword expressions
in phrase-based statistical machine translation. In In
Proceedings of the 13th Machine Translation Sum-
mit, pages 215–224. MT Summit 2011.
Santanu Pal, Mahammed Hasanuzzaman, Sudip Ku-
mar Naskar, and Sivaji Bandyopadhyay.
2013. Impact of linguistically motivated
shallow phrases in pb-smt. In ICON 2013
http://sivajibandyopadhyay.com/publications/Icon-
v1.3-camera.pdf. ICON 2013.
Stephan Peitz, Jan-Thorsten Peter Saab Mansour,
Christoph Schmidt, Joern Wuebker, Matthias Huck,
Markus Freitag, and Hermann Ney. 2013. The rwth
aachen machine translation system for wmt 2013. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 191–197.
Dipti Misra Sharma Rajeev Sangal and Anil Kumar
Singh, editors. 2008. Proceedings of the IJCNLP-
08 Workshop on Named Entity Recognition for South
and South East Asian Languages. Asian Federation
of Natural Language Processing, Hyderabad, India,
January.
Zhixiang Ren, Yajuan L¨u, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine trans-
lation using domain bilingual multiword expres-
sions. In Proceedings of the Workshop on Multiword
Expressions: Identification, Interpretation, Disam-
biguation and Applications, MWE ’09, pages 47–
54, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.979463">
205
</page>
<reference confidence="0.998019775">
Raphael Rubino, Antonio Toral, S Cort´es Vaıllo, Jun
Xie, Xiaofeng Wu, Stephen Doherty, and Qun Liu.
2013. The cngl-dcu-prompsit translation systems
for wmt13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 211–216.
Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 1–15. Springer Berlin Heidelberg.
Pal Santanu, Sudip Kumar Naskar, Pavel Pecina, Sivaji
Bandyopadhyay, and Andy Way. 2010. Handling
named entities and compound verbs in phrase-based
statistical machine translation. In 23rd International
Conference of Computational Linguistics (Coling
2010), Beijing, Chaina, pages 46–54.
Sara Stymne, Christian Hardmeier, J¨org Tiedemann,
and Joakim Nivre. 2013. Tunable distortion lim-
its and corpus cleaning for smt. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 225–231.
Liling Tan and Francis Bond. 2013. Xling: Match-
ing query sentences to a parallel corpus using topic
models for word sense disambiguation.
Liling Tan. 2013. Gachalign: Gale-church sentence-
level alignments with variable parameters [soft-
ware]. Retrieved from https://db.tt/LLrul4zP and
https://code.google.com/p/gachalign/.
Antonio Toral. 2013. Hybrid selection of language
model training data using linguistic information and
perplexity. ACL 2013, page 8.
Olga Vechtomova. 2005. The role of multi-word units
in interactive information retrieval. In ECIR, pages
403–420.
Sriram Venkatapathy and Aravind K Joshi. 2006. Us-
ing information about multi-word expressions for
the word-alignment task. In Proceedings of the
Workshop on Multiword Expressions: Identifying
and Exploiting Underlying Properties, pages 20–27.
Association for Computational Linguistics.
</reference>
<page confidence="0.99889">
206
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.242918">
<title confidence="0.9971355">Using Multi-Word Expressions and Named Entities to Improve Machine Translation</title>
<author confidence="0.714652">Liling Tan</author>
<author confidence="0.714652">Santanu</author>
<affiliation confidence="0.780343">Applied Linguistics, Translation and Interpretation Universit¨at des</affiliation>
<email confidence="0.941923">santanu.pal@uni-saarland.de</email>
<abstract confidence="0.996874916666667">describe the system submitted to the 2014 WMT translation shared task. We participated in the English-Hindi (EN-HI) and Hindi-English (HI-EN) language pair and achieved 0.792 for the Translation Error Rate (TER) for EN-HI, the lowest among the competing systems. Our main innovations are (i) the usage of outputs from NLP tools, viz. billingual multi-word expression extractor and named-entity rec-</abstract>
<note confidence="0.400859">ognizer to improve SMT quality and (ii)</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>355--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7513" citStr="Axelrod et al., 2011" startWordPosition="1157" endWordPosition="1160"> Although the resulting bilingual NE does not provide a perfect NE dictionary, it filters out NEs from the sentence and improves word alignments at the start of the MT pipeline. 3 Corpus Preprocessing The performance of any data driven SMT depends on the quality of training data. Previous studies had shown that filtering out low quality sentence pairs improves the quality of machine translation. For instance, the Moore-Lewis filter removes sentence pairs based on source-side crossentropy differences (Moore and Lewis, 2010) and the Edinburgh’s MT system used the Modified Moore-Lewis filtering (Axelrod et al., 2011) in WMT 2013 shared task (Durrani et al., 2013). CNGL-DCU system extended the Moore-Lewis filter by incorporating lemmas and named enti3http://nlp.stanford.edu/software/CRF-NER.shtml 4http://crfpp.googlecode.com 202 ties in their definition of perplexity5 (Rubino et al., 2013; Toral, 2013). The RWTH Aachen system filtered the Common Crawl Corpus by keeping only sentence pairs that contains at least 70% of the word from a known vocabulary dataset extracted from the other corpora in the WMT 2013 shared task (Peitz et al., 2013). The Docent system from Uppsala University also performed data clean</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 355–362. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Vojtech Diatka</author>
<author>Pavel Rychl´y</author>
<author>Pavel Stran´ak</author>
<author>Ales Tamchyna</author>
<author>Dan Zeman</author>
</authors>
<title>Hindi-English and Hindi-only Corpus for Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Language Resources and Evaluation Conference (LREC’14),</booktitle>
<location>Reykjavik, Iceland,</location>
<note>in prep.</note>
<marker>Bojar, Diatka, Rychl´y, Stran´ak, Tamchyna, Zeman, 2014</marker>
<rawString>Ond&amp;quot;rej Bojar, Vojt&amp;quot;ech Diatka, Pavel Rychl´y, Pavel Stra&amp;quot;n´ak, Ale&amp;quot;s Tamchyna, and Dan Zeman. 2014. Hindi-English and Hindi-only Corpus for Machine Translation. In Proceedings of the Ninth International Language Resources and Evaluation Conference (LREC’14), Reykjavik, Iceland, may. ELRA, European Language Resources Association. in prep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Barry Haddow</author>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburghs machine translation systems for european language pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>112--119</pages>
<contexts>
<context position="7560" citStr="Durrani et al., 2013" startWordPosition="1166" endWordPosition="1169">rovide a perfect NE dictionary, it filters out NEs from the sentence and improves word alignments at the start of the MT pipeline. 3 Corpus Preprocessing The performance of any data driven SMT depends on the quality of training data. Previous studies had shown that filtering out low quality sentence pairs improves the quality of machine translation. For instance, the Moore-Lewis filter removes sentence pairs based on source-side crossentropy differences (Moore and Lewis, 2010) and the Edinburgh’s MT system used the Modified Moore-Lewis filtering (Axelrod et al., 2011) in WMT 2013 shared task (Durrani et al., 2013). CNGL-DCU system extended the Moore-Lewis filter by incorporating lemmas and named enti3http://nlp.stanford.edu/software/CRF-NER.shtml 4http://crfpp.googlecode.com 202 ties in their definition of perplexity5 (Rubino et al., 2013; Toral, 2013). The RWTH Aachen system filtered the Common Crawl Corpus by keeping only sentence pairs that contains at least 70% of the word from a known vocabulary dataset extracted from the other corpora in the WMT 2013 shared task (Peitz et al., 2013). The Docent system from Uppsala University also performed data cleaning on the Common Crawl dataset prior to SMT bu</context>
</contexts>
<marker>Durrani, Haddow, Heafield, Koehn, 2013</marker>
<rawString>Nadir Durrani, Barry Haddow, Kenneth Heafield, and Philipp Koehn. 2013. Edinburghs machine translation systems for european language pairs. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 112–119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Alan Finlayson</author>
<author>Nidhi Kulkarni</author>
</authors>
<title>Detecting multi-word expressions improves word sense disambiguation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: From Parsing and Generation to the Real World, MWE ’11,</booktitle>
<pages>20--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2960" citStr="Finlayson and Kulkarni, 2011" startWordPosition="442" endWordPosition="445">atic interpretations that cross word boundaries” (Sag et al., 2002). MWE can be made up of collocations (e.g. seem ridiculous : behuda dikhai), frozen expressions (e.g. exception handling : apavada sancalaka) or name entities (e.g. Johnny Cash : Johni Kesh). Jackendoff (1997) claims that the frequency of MWE and the frequency of single words in a speaker’s lexicon are almost equivalent. Bilingual MWE has shown to be useful for a variety of NLP applications such as multilingual information retrieval (Vechtomova, 2005) and Crosslingual/Multilingual Word Sense Disambiguation (Tan and Bond, 2013; Finlayson and Kulkarni, 2011). For machine translation, various studies had introduced bilingual MWE to improve MT system performance. Lambert (2005) introduced bilingual MWE by grouping them as a single token before training alignment models and they showed that it improved alignment and translation quality. Ren et al. (2009) integrated an in-domain bilingual MWE using log likelihood ratio based hierarchical reducing algorithm and gained +0.61 BLEU score. Similarly, Santanu et al. (2010) single tokenized MWE before training a phrase-based SMT model and achieved 50% improvement in BLEU score. 201 Proceedings of the Ninth </context>
</contexts>
<marker>Finlayson, Kulkarni, 2011</marker>
<rawString>Mark Alan Finlayson and Nidhi Kulkarni. 2011. Detecting multi-word expressions improves word sense disambiguation. In Proceedings of the Workshop on Multiword Expressions: From Parsing and Generation to the Real World, MWE ’11, pages 20–24, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="9472" citStr="Gale and Church, 1993" startWordPosition="1468" endWordPosition="1471">he remainder of the corpus by exploiting the number of alignment points in word alignments between sentence pairs. Their hypothesis was that sentence pairs with very few alignment points in the intersection would mostly likely not be parallel. This is based on the fact that when using GIZA++ (Och and Ney, 2003), the intersection of alignments is more sparse than the standard SMT symmetrization heuristics like grow-diag-final-and (Koehn, 2005). Different from Stymne et al., our hypothesis for non-parallelness adheres to sentence level alignment criteria as defined in the Gale-Church algorithm (Gale and Church, 1993). If a sentence pair is parallel, the ratio of the number of characters in the source and target sentence should be coherent to the global ratio of the number of source-target characters in a fully parallel corpus. The GaleChurch algorithm had its parameters tuned to suit European languages and Tan (2013) had demonstrated that sentence-level alignments can be improved by using corpus specific parameters. When 5The exponent of cross-entropy may be regarded as perplexity using variable parameters to the Gale-Church algorithm, Tan showed that instead of the default parameters set in the original </context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>William A Gale and Kenneth W Church. 1993. A program for aligning sentences in bilingual corpora. Computational linguistics, 19(1):75–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1560" citStr="Koehn et al., 2007" startWordPosition="229" endWordPosition="232">e. 1 Introduction In this paper, we present Saarland University (USAAR) submission to Workshop for Machine Translation 2014 (WMT 2014) using the Manawi MT system. We participated in the generic translation shared task for the English-Hindi (EN-HI) and Hindi-English (HI-EN) language pairs. Our Manawi system showcased the incorporation of NLP tools output within the MT pipeline; a bilingual MWE extractor and a bilingual NE recognizer for English and Hindi were implemented. The output from these NLP tools was appended to the training corpus prior to the SMT model training with the MOSES toolkit (Koehn et al., 2007). The resulting system achieves the lowest Translation Error Rate (TER) among competing systems for the English-Hindi language pair. 1Multi-word expression And Named-entity And Wikipedia titles (Manawi) 2Lower TER often results in better translation The rest of the paper is structured as follow: Section 2 describes the implementation of the NLP tools; Section 3 outlines the corpus pre-processing before the MT training process; Section 4 describes the MT system setup; Section 5 describes a simple post-processing component to handle Out-Of-Vocabulary words; Section 6 presents the WMT shared task</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation. MT summit,</title>
<date>2005</date>
<pages>5--79</pages>
<contexts>
<context position="9296" citStr="Koehn, 2005" startWordPosition="1443" endWordPosition="1444"> (2013) improved translation quality by cleaning the Common Crawl corpus during the WMT 2013 shared task. They filtered out documents exceeding 60 words and cleaned the remainder of the corpus by exploiting the number of alignment points in word alignments between sentence pairs. Their hypothesis was that sentence pairs with very few alignment points in the intersection would mostly likely not be parallel. This is based on the fact that when using GIZA++ (Och and Ney, 2003), the intersection of alignments is more sparse than the standard SMT symmetrization heuristics like grow-diag-final-and (Koehn, 2005). Different from Stymne et al., our hypothesis for non-parallelness adheres to sentence level alignment criteria as defined in the Gale-Church algorithm (Gale and Church, 1993). If a sentence pair is parallel, the ratio of the number of characters in the source and target sentence should be coherent to the global ratio of the number of source-target characters in a fully parallel corpus. The GaleChurch algorithm had its parameters tuned to suit European languages and Tan (2013) had demonstrated that sentence-level alignments can be improved by using corpus specific parameters. When 5The expone</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. MT summit, 5:79– 86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
</authors>
<title>Data inferred multi-word expressions for statistical machine translation.</title>
<date>2005</date>
<booktitle>In In MT Summit X.</booktitle>
<contexts>
<context position="3080" citStr="Lambert (2005)" startWordPosition="462" endWordPosition="463">dikhai), frozen expressions (e.g. exception handling : apavada sancalaka) or name entities (e.g. Johnny Cash : Johni Kesh). Jackendoff (1997) claims that the frequency of MWE and the frequency of single words in a speaker’s lexicon are almost equivalent. Bilingual MWE has shown to be useful for a variety of NLP applications such as multilingual information retrieval (Vechtomova, 2005) and Crosslingual/Multilingual Word Sense Disambiguation (Tan and Bond, 2013; Finlayson and Kulkarni, 2011). For machine translation, various studies had introduced bilingual MWE to improve MT system performance. Lambert (2005) introduced bilingual MWE by grouping them as a single token before training alignment models and they showed that it improved alignment and translation quality. Ren et al. (2009) integrated an in-domain bilingual MWE using log likelihood ratio based hierarchical reducing algorithm and gained +0.61 BLEU score. Similarly, Santanu et al. (2010) single tokenized MWE before training a phrase-based SMT model and achieved 50% improvement in BLEU score. 201 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 201–206, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Associatio</context>
</contexts>
<marker>Lambert, 2005</marker>
<rawString>Patrik Lambert. 2005. Data inferred multi-word expressions for statistical machine translation. In In MT Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>220--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7420" citStr="Moore and Lewis, 2010" startWordPosition="1143" endWordPosition="1146">“Kalpna Chawla Gurdeep Pandher”, which contains two NEs ‘Kalpna Chawla’ and ‘Gurdeep Pandher’. Although the resulting bilingual NE does not provide a perfect NE dictionary, it filters out NEs from the sentence and improves word alignments at the start of the MT pipeline. 3 Corpus Preprocessing The performance of any data driven SMT depends on the quality of training data. Previous studies had shown that filtering out low quality sentence pairs improves the quality of machine translation. For instance, the Moore-Lewis filter removes sentence pairs based on source-side crossentropy differences (Moore and Lewis, 2010) and the Edinburgh’s MT system used the Modified Moore-Lewis filtering (Axelrod et al., 2011) in WMT 2013 shared task (Durrani et al., 2013). CNGL-DCU system extended the Moore-Lewis filter by incorporating lemmas and named enti3http://nlp.stanford.edu/software/CRF-NER.shtml 4http://crfpp.googlecode.com 202 ties in their definition of perplexity5 (Rubino et al., 2013; Toral, 2013). The RWTH Aachen system filtered the Common Crawl Corpus by keeping only sentence pairs that contains at least 70% of the word from a known vocabulary dataset extracted from the other corpora in the WMT 2013 shared t</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, pages 220–224. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="9162" citStr="Och and Ney, 2003" startWordPosition="1423" endWordPosition="1426"> features, we were cleaning the data based on sentence alignment features. 3.1 GaCha Filtering: Filter by Character Mean Ratio Stymne et al. (2013) improved translation quality by cleaning the Common Crawl corpus during the WMT 2013 shared task. They filtered out documents exceeding 60 words and cleaned the remainder of the corpus by exploiting the number of alignment points in word alignments between sentence pairs. Their hypothesis was that sentence pairs with very few alignment points in the intersection would mostly likely not be parallel. This is based on the fact that when using GIZA++ (Och and Ney, 2003), the intersection of alignments is more sparse than the standard SMT symmetrization heuristics like grow-diag-final-and (Koehn, 2005). Different from Stymne et al., our hypothesis for non-parallelness adheres to sentence level alignment criteria as defined in the Gale-Church algorithm (Gale and Church, 1993). If a sentence pair is parallel, the ratio of the number of characters in the source and target sentence should be coherent to the global ratio of the number of source-target characters in a fully parallel corpus. The GaleChurch algorithm had its parameters tuned to suit European language</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Santanu Pal</author>
<author>Tanmoy Chakraborty</author>
<author>Sivaji Bandyopadhyay</author>
</authors>
<title>Handling multiword expressions in phrase-based statistical machine translation. In</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th Machine Translation Summit,</booktitle>
<pages>215--224</pages>
<publisher>MT</publisher>
<location>Summit</location>
<contexts>
<context position="3920" citStr="Pal et al. (2011)" startWordPosition="582" endWordPosition="585"> log likelihood ratio based hierarchical reducing algorithm and gained +0.61 BLEU score. Similarly, Santanu et al. (2010) single tokenized MWE before training a phrase-based SMT model and achieved 50% improvement in BLEU score. 201 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 201–206, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics In order to improve the word alignment quality, Venkatapathy and Joshi (2006) reported a discriminative approach to use the compositionality information of verb-based multi-word expressions. Pal et al. (2011) discussed the effects of incorporating prior alignment of MWE and NEs directly or indirectly into Phrase-based SMT systems. 2.2 Bilingual MWE Extraction Monolingual MWE extraction revolves around three approaches (i) rule-based methods relying on morphosyntactic patterns, (ii) statistical methods which use association/frequency measures to determine ngrams as MWE and (iii) hybrid approaches that combine the rule-based and statistical methods. However, where bilingual MWE extraction techniques are concerned, they operate around two main modus operandi (i) extracting monolingual MWE separately </context>
</contexts>
<marker>Pal, Chakraborty, Bandyopadhyay, 2011</marker>
<rawString>Santanu Pal, Tanmoy Chakraborty, and Sivaji Bandyopadhyay. 2011. Handling multiword expressions in phrase-based statistical machine translation. In In Proceedings of the 13th Machine Translation Summit, pages 215–224. MT Summit 2011.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Santanu Pal</author>
</authors>
<title>Mahammed Hasanuzzaman, Sudip Kumar Naskar, and Sivaji Bandyopadhyay.</title>
<marker>Pal, </marker>
<rawString>Santanu Pal, Mahammed Hasanuzzaman, Sudip Kumar Naskar, and Sivaji Bandyopadhyay.</rawString>
</citation>
<citation valid="true">
<title>Impact of linguistically motivated shallow phrases in pb-smt.</title>
<date>2013</date>
<booktitle>In ICON 2013 http://sivajibandyopadhyay.com/publications/Iconv1.3-camera.pdf. ICON</booktitle>
<contexts>
<context position="8691" citStr="(2013)" startWordPosition="1346" endWordPosition="1346">ty also performed data cleaning on the Common Crawl dataset prior to SMT but they were using more aggressive conditions by (i) removing documents that were identified correctly using a language identification module and (ii) removing documents that falls below a threshold value of alignment points and sentence length ratio (Stymne et al., 2013). Our approach to data cleaning is similar to the Uppsala’s system but instead of capitalizing on word-alignments features, we were cleaning the data based on sentence alignment features. 3.1 GaCha Filtering: Filter by Character Mean Ratio Stymne et al. (2013) improved translation quality by cleaning the Common Crawl corpus during the WMT 2013 shared task. They filtered out documents exceeding 60 words and cleaned the remainder of the corpus by exploiting the number of alignment points in word alignments between sentence pairs. Their hypothesis was that sentence pairs with very few alignment points in the intersection would mostly likely not be parallel. This is based on the fact that when using GIZA++ (Och and Ney, 2003), the intersection of alignments is more sparse than the standard SMT symmetrization heuristics like grow-diag-final-and (Koehn, </context>
</contexts>
<marker>2013</marker>
<rawString>2013. Impact of linguistically motivated shallow phrases in pb-smt. In ICON 2013 http://sivajibandyopadhyay.com/publications/Iconv1.3-camera.pdf. ICON 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Peitz</author>
<author>Jan-Thorsten Peter Saab Mansour</author>
<author>Christoph Schmidt</author>
<author>Joern Wuebker</author>
<author>Matthias Huck</author>
<author>Markus Freitag</author>
<author>Hermann Ney</author>
</authors>
<title>The rwth aachen machine translation system for wmt</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>191--197</pages>
<contexts>
<context position="8044" citStr="Peitz et al., 2013" startWordPosition="1236" endWordPosition="1239">the Edinburgh’s MT system used the Modified Moore-Lewis filtering (Axelrod et al., 2011) in WMT 2013 shared task (Durrani et al., 2013). CNGL-DCU system extended the Moore-Lewis filter by incorporating lemmas and named enti3http://nlp.stanford.edu/software/CRF-NER.shtml 4http://crfpp.googlecode.com 202 ties in their definition of perplexity5 (Rubino et al., 2013; Toral, 2013). The RWTH Aachen system filtered the Common Crawl Corpus by keeping only sentence pairs that contains at least 70% of the word from a known vocabulary dataset extracted from the other corpora in the WMT 2013 shared task (Peitz et al., 2013). The Docent system from Uppsala University also performed data cleaning on the Common Crawl dataset prior to SMT but they were using more aggressive conditions by (i) removing documents that were identified correctly using a language identification module and (ii) removing documents that falls below a threshold value of alignment points and sentence length ratio (Stymne et al., 2013). Our approach to data cleaning is similar to the Uppsala’s system but instead of capitalizing on word-alignments features, we were cleaning the data based on sentence alignment features. 3.1 GaCha Filtering: Filt</context>
</contexts>
<marker>Peitz, Mansour, Schmidt, Wuebker, Huck, Freitag, Ney, 2013</marker>
<rawString>Stephan Peitz, Jan-Thorsten Peter Saab Mansour, Christoph Schmidt, Joern Wuebker, Matthias Huck, Markus Freitag, and Hermann Ney. 2013. The rwth aachen machine translation system for wmt 2013. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 191–197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipti Misra</author>
</authors>
<title>Sharma Rajeev Sangal and Anil Kumar Singh, editors.</title>
<date>2008</date>
<booktitle>Proceedings of the IJCNLP08 Workshop on Named Entity Recognition for South and South East Asian Languages. Asian Federation of Natural Language Processing,</booktitle>
<location>Hyderabad, India,</location>
<marker>Misra, 2008</marker>
<rawString>Dipti Misra Sharma Rajeev Sangal and Anil Kumar Singh, editors. 2008. Proceedings of the IJCNLP08 Workshop on Named Entity Recognition for South and South East Asian Languages. Asian Federation of Natural Language Processing, Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhixiang Ren</author>
<author>Yajuan L¨u</author>
<author>Jie Cao</author>
<author>Qun Liu</author>
<author>Yun Huang</author>
</authors>
<title>Improving statistical machine translation using domain bilingual multiword expressions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications, MWE ’09,</booktitle>
<pages>47--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Ren, L¨u, Cao, Liu, Huang, 2009</marker>
<rawString>Zhixiang Ren, Yajuan L¨u, Jie Cao, Qun Liu, and Yun Huang. 2009. Improving statistical machine translation using domain bilingual multiword expressions. In Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications, MWE ’09, pages 47– 54, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Rubino</author>
<author>Antonio Toral</author>
<author>S Cort´es Vaıllo</author>
<author>Jun Xie</author>
<author>Xiaofeng Wu</author>
<author>Stephen Doherty</author>
<author>Qun Liu</author>
</authors>
<title>The cngl-dcu-prompsit translation systems for wmt13.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>211--216</pages>
<contexts>
<context position="7789" citStr="Rubino et al., 2013" startWordPosition="1191" endWordPosition="1194">a. Previous studies had shown that filtering out low quality sentence pairs improves the quality of machine translation. For instance, the Moore-Lewis filter removes sentence pairs based on source-side crossentropy differences (Moore and Lewis, 2010) and the Edinburgh’s MT system used the Modified Moore-Lewis filtering (Axelrod et al., 2011) in WMT 2013 shared task (Durrani et al., 2013). CNGL-DCU system extended the Moore-Lewis filter by incorporating lemmas and named enti3http://nlp.stanford.edu/software/CRF-NER.shtml 4http://crfpp.googlecode.com 202 ties in their definition of perplexity5 (Rubino et al., 2013; Toral, 2013). The RWTH Aachen system filtered the Common Crawl Corpus by keeping only sentence pairs that contains at least 70% of the word from a known vocabulary dataset extracted from the other corpora in the WMT 2013 shared task (Peitz et al., 2013). The Docent system from Uppsala University also performed data cleaning on the Common Crawl dataset prior to SMT but they were using more aggressive conditions by (i) removing documents that were identified correctly using a language identification module and (ii) removing documents that falls below a threshold value of alignment points and s</context>
</contexts>
<marker>Rubino, Toral, Vaıllo, Xie, Wu, Doherty, Liu, 2013</marker>
<rawString>Raphael Rubino, Antonio Toral, S Cort´es Vaıllo, Jun Xie, Xiaofeng Wu, Stephen Doherty, and Qun Liu. 2013. The cngl-dcu-prompsit translation systems for wmt13. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 211–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for nlp.</title>
<date>2002</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>1--15</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="2398" citStr="Sag et al., 2002" startWordPosition="354" endWordPosition="357">esults in better translation The rest of the paper is structured as follow: Section 2 describes the implementation of the NLP tools; Section 3 outlines the corpus pre-processing before the MT training process; Section 4 describes the MT system setup; Section 5 describes a simple post-processing component to handle Out-Of-Vocabulary words; Section 6 presents the WMT shared task results for the Manawi system and Section 6 concludes the paper. 2 NLP Tools Implementation 2.1 Bilingual MWE in MT Multi-Word Expressions (MWE) are defined as “idiosyncratic interpretations that cross word boundaries” (Sag et al., 2002). MWE can be made up of collocations (e.g. seem ridiculous : behuda dikhai), frozen expressions (e.g. exception handling : apavada sancalaka) or name entities (e.g. Johnny Cash : Johni Kesh). Jackendoff (1997) claims that the frequency of MWE and the frequency of single words in a speaker’s lexicon are almost equivalent. Bilingual MWE has shown to be useful for a variety of NLP applications such as multilingual information retrieval (Vechtomova, 2005) and Crosslingual/Multilingual Word Sense Disambiguation (Tan and Bond, 2013; Finlayson and Kulkarni, 2011). For machine translation, various stu</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for nlp. In Computational Linguistics and Intelligent Text Processing, pages 1–15. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pal Santanu</author>
<author>Sudip Kumar Naskar</author>
<author>Pavel Pecina</author>
<author>Sivaji Bandyopadhyay</author>
<author>Andy Way</author>
</authors>
<title>Handling named entities and compound verbs in phrase-based statistical machine translation.</title>
<date>2010</date>
<booktitle>In 23rd International Conference of Computational Linguistics (Coling 2010),</booktitle>
<pages>46--54</pages>
<location>Beijing, Chaina,</location>
<contexts>
<context position="3424" citStr="Santanu et al. (2010)" startWordPosition="511" endWordPosition="514">tilingual information retrieval (Vechtomova, 2005) and Crosslingual/Multilingual Word Sense Disambiguation (Tan and Bond, 2013; Finlayson and Kulkarni, 2011). For machine translation, various studies had introduced bilingual MWE to improve MT system performance. Lambert (2005) introduced bilingual MWE by grouping them as a single token before training alignment models and they showed that it improved alignment and translation quality. Ren et al. (2009) integrated an in-domain bilingual MWE using log likelihood ratio based hierarchical reducing algorithm and gained +0.61 BLEU score. Similarly, Santanu et al. (2010) single tokenized MWE before training a phrase-based SMT model and achieved 50% improvement in BLEU score. 201 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 201–206, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics In order to improve the word alignment quality, Venkatapathy and Joshi (2006) reported a discriminative approach to use the compositionality information of verb-based multi-word expressions. Pal et al. (2011) discussed the effects of incorporating prior alignment of MWE and NEs directly or indirectly into Phras</context>
</contexts>
<marker>Santanu, Naskar, Pecina, Bandyopadhyay, Way, 2010</marker>
<rawString>Pal Santanu, Sudip Kumar Naskar, Pavel Pecina, Sivaji Bandyopadhyay, and Andy Way. 2010. Handling named entities and compound verbs in phrase-based statistical machine translation. In 23rd International Conference of Computational Linguistics (Coling 2010), Beijing, Chaina, pages 46–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Christian Hardmeier</author>
<author>J¨org Tiedemann</author>
<author>Joakim Nivre</author>
</authors>
<title>Tunable distortion limits and corpus cleaning for smt.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>225--231</pages>
<contexts>
<context position="8431" citStr="Stymne et al., 2013" startWordPosition="1301" endWordPosition="1304">TH Aachen system filtered the Common Crawl Corpus by keeping only sentence pairs that contains at least 70% of the word from a known vocabulary dataset extracted from the other corpora in the WMT 2013 shared task (Peitz et al., 2013). The Docent system from Uppsala University also performed data cleaning on the Common Crawl dataset prior to SMT but they were using more aggressive conditions by (i) removing documents that were identified correctly using a language identification module and (ii) removing documents that falls below a threshold value of alignment points and sentence length ratio (Stymne et al., 2013). Our approach to data cleaning is similar to the Uppsala’s system but instead of capitalizing on word-alignments features, we were cleaning the data based on sentence alignment features. 3.1 GaCha Filtering: Filter by Character Mean Ratio Stymne et al. (2013) improved translation quality by cleaning the Common Crawl corpus during the WMT 2013 shared task. They filtered out documents exceeding 60 words and cleaned the remainder of the corpus by exploiting the number of alignment points in word alignments between sentence pairs. Their hypothesis was that sentence pairs with very few alignment p</context>
</contexts>
<marker>Stymne, Hardmeier, Tiedemann, Nivre, 2013</marker>
<rawString>Sara Stymne, Christian Hardmeier, J¨org Tiedemann, and Joakim Nivre. 2013. Tunable distortion limits and corpus cleaning for smt. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 225–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
<author>Francis Bond</author>
</authors>
<title>Xling: Matching query sentences to a parallel corpus using topic models for word sense disambiguation.</title>
<date>2013</date>
<contexts>
<context position="2929" citStr="Tan and Bond, 2013" startWordPosition="438" endWordPosition="441">efined as “idiosyncratic interpretations that cross word boundaries” (Sag et al., 2002). MWE can be made up of collocations (e.g. seem ridiculous : behuda dikhai), frozen expressions (e.g. exception handling : apavada sancalaka) or name entities (e.g. Johnny Cash : Johni Kesh). Jackendoff (1997) claims that the frequency of MWE and the frequency of single words in a speaker’s lexicon are almost equivalent. Bilingual MWE has shown to be useful for a variety of NLP applications such as multilingual information retrieval (Vechtomova, 2005) and Crosslingual/Multilingual Word Sense Disambiguation (Tan and Bond, 2013; Finlayson and Kulkarni, 2011). For machine translation, various studies had introduced bilingual MWE to improve MT system performance. Lambert (2005) introduced bilingual MWE by grouping them as a single token before training alignment models and they showed that it improved alignment and translation quality. Ren et al. (2009) integrated an in-domain bilingual MWE using log likelihood ratio based hierarchical reducing algorithm and gained +0.61 BLEU score. Similarly, Santanu et al. (2010) single tokenized MWE before training a phrase-based SMT model and achieved 50% improvement in BLEU score</context>
</contexts>
<marker>Tan, Bond, 2013</marker>
<rawString>Liling Tan and Francis Bond. 2013. Xling: Matching query sentences to a parallel corpus using topic models for word sense disambiguation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liling Tan</author>
</authors>
<title>Gachalign: Gale-church sentencelevel alignments with variable parameters [software]. Retrieved from https://db.tt/LLrul4zP and https://code.google.com/p/gachalign/.</title>
<date>2013</date>
<contexts>
<context position="9778" citStr="Tan (2013)" startWordPosition="1522" endWordPosition="1523">intersection of alignments is more sparse than the standard SMT symmetrization heuristics like grow-diag-final-and (Koehn, 2005). Different from Stymne et al., our hypothesis for non-parallelness adheres to sentence level alignment criteria as defined in the Gale-Church algorithm (Gale and Church, 1993). If a sentence pair is parallel, the ratio of the number of characters in the source and target sentence should be coherent to the global ratio of the number of source-target characters in a fully parallel corpus. The GaleChurch algorithm had its parameters tuned to suit European languages and Tan (2013) had demonstrated that sentence-level alignments can be improved by using corpus specific parameters. When 5The exponent of cross-entropy may be regarded as perplexity using variable parameters to the Gale-Church algorithm, Tan showed that instead of the default parameters set in the original Gale-Church algorithm, using mean ratio of the noisy corpus can also improve sentence level alignments although the ratio from a clean corpus would achieve even better alignments. Given the premises of the sentence level alignment hypothesis, we clean the training corpus by first calculating the global me</context>
</contexts>
<marker>Tan, 2013</marker>
<rawString>Liling Tan. 2013. Gachalign: Gale-church sentencelevel alignments with variable parameters [software]. Retrieved from https://db.tt/LLrul4zP and https://code.google.com/p/gachalign/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Toral</author>
</authors>
<title>Hybrid selection of language model training data using linguistic information and perplexity.</title>
<date>2013</date>
<booktitle>ACL 2013,</booktitle>
<pages>8</pages>
<contexts>
<context position="7803" citStr="Toral, 2013" startWordPosition="1195" endWordPosition="1196">ad shown that filtering out low quality sentence pairs improves the quality of machine translation. For instance, the Moore-Lewis filter removes sentence pairs based on source-side crossentropy differences (Moore and Lewis, 2010) and the Edinburgh’s MT system used the Modified Moore-Lewis filtering (Axelrod et al., 2011) in WMT 2013 shared task (Durrani et al., 2013). CNGL-DCU system extended the Moore-Lewis filter by incorporating lemmas and named enti3http://nlp.stanford.edu/software/CRF-NER.shtml 4http://crfpp.googlecode.com 202 ties in their definition of perplexity5 (Rubino et al., 2013; Toral, 2013). The RWTH Aachen system filtered the Common Crawl Corpus by keeping only sentence pairs that contains at least 70% of the word from a known vocabulary dataset extracted from the other corpora in the WMT 2013 shared task (Peitz et al., 2013). The Docent system from Uppsala University also performed data cleaning on the Common Crawl dataset prior to SMT but they were using more aggressive conditions by (i) removing documents that were identified correctly using a language identification module and (ii) removing documents that falls below a threshold value of alignment points and sentence length</context>
</contexts>
<marker>Toral, 2013</marker>
<rawString>Antonio Toral. 2013. Hybrid selection of language model training data using linguistic information and perplexity. ACL 2013, page 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Vechtomova</author>
</authors>
<title>The role of multi-word units in interactive information retrieval.</title>
<date>2005</date>
<booktitle>In ECIR,</booktitle>
<pages>403--420</pages>
<contexts>
<context position="2853" citStr="Vechtomova, 2005" startWordPosition="430" endWordPosition="431">s Implementation 2.1 Bilingual MWE in MT Multi-Word Expressions (MWE) are defined as “idiosyncratic interpretations that cross word boundaries” (Sag et al., 2002). MWE can be made up of collocations (e.g. seem ridiculous : behuda dikhai), frozen expressions (e.g. exception handling : apavada sancalaka) or name entities (e.g. Johnny Cash : Johni Kesh). Jackendoff (1997) claims that the frequency of MWE and the frequency of single words in a speaker’s lexicon are almost equivalent. Bilingual MWE has shown to be useful for a variety of NLP applications such as multilingual information retrieval (Vechtomova, 2005) and Crosslingual/Multilingual Word Sense Disambiguation (Tan and Bond, 2013; Finlayson and Kulkarni, 2011). For machine translation, various studies had introduced bilingual MWE to improve MT system performance. Lambert (2005) introduced bilingual MWE by grouping them as a single token before training alignment models and they showed that it improved alignment and translation quality. Ren et al. (2009) integrated an in-domain bilingual MWE using log likelihood ratio based hierarchical reducing algorithm and gained +0.61 BLEU score. Similarly, Santanu et al. (2010) single tokenized MWE before </context>
</contexts>
<marker>Vechtomova, 2005</marker>
<rawString>Olga Vechtomova. 2005. The role of multi-word units in interactive information retrieval. In ECIR, pages 403–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriram Venkatapathy</author>
<author>Aravind K Joshi</author>
</authors>
<title>Using information about multi-word expressions for the word-alignment task.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,</booktitle>
<pages>20--27</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3789" citStr="Venkatapathy and Joshi (2006)" startWordPosition="563" endWordPosition="566">gnment models and they showed that it improved alignment and translation quality. Ren et al. (2009) integrated an in-domain bilingual MWE using log likelihood ratio based hierarchical reducing algorithm and gained +0.61 BLEU score. Similarly, Santanu et al. (2010) single tokenized MWE before training a phrase-based SMT model and achieved 50% improvement in BLEU score. 201 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 201–206, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics In order to improve the word alignment quality, Venkatapathy and Joshi (2006) reported a discriminative approach to use the compositionality information of verb-based multi-word expressions. Pal et al. (2011) discussed the effects of incorporating prior alignment of MWE and NEs directly or indirectly into Phrase-based SMT systems. 2.2 Bilingual MWE Extraction Monolingual MWE extraction revolves around three approaches (i) rule-based methods relying on morphosyntactic patterns, (ii) statistical methods which use association/frequency measures to determine ngrams as MWE and (iii) hybrid approaches that combine the rule-based and statistical methods. However, where biling</context>
</contexts>
<marker>Venkatapathy, Joshi, 2006</marker>
<rawString>Sriram Venkatapathy and Aravind K Joshi. 2006. Using information about multi-word expressions for the word-alignment task. In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 20–27. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
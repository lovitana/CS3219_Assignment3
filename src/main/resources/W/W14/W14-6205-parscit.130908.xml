<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<title confidence="0.999235333333333">
Semi-supervised Sequence Labeling for Named Entity Extraction
based on Tri-Training:
Case Study on Chinese Person Name Extraction
</title>
<author confidence="0.998768">
Chien-Lung Chou1
</author>
<affiliation confidence="0.898244">
National Central University,
Taoyuan, Taiwan
</affiliation>
<email confidence="0.940379">
formatc.chou@gmail.com
</email>
<author confidence="0.976577">
Chia-Hui Chang
</author>
<affiliation confidence="0.889626">
National Central University,
Taoyuan, Taiwan
</affiliation>
<email confidence="0.947743">
chia@csie.ncu.edu.tw
</email>
<note confidence="0.714774333333333">
Shin-Yi Wu
Industrial Technology
Research Institute, Taiwan
</note>
<email confidence="0.889434">
sywu@itri.org.tw
</email>
<sectionHeader confidence="0.980069" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917916666667">
Named entity extraction is a fundamental task for many knowledge engineering applications.
Existing studies rely on annotated training data, which is quite expensive when used to obtain
large data sets, limiting the effectiveness of recognition. In this research, we propose an auto-
matic labeling procedure to prepare training data from structured resources which contain known
named entities. While this automatically labeled training data may contain noise, a self-testing
procedure may be used as a follow-up to remove low-confidence annotation and increase the
extraction performance with less training data. In addition to the preparation of labeled training
data, we also employed semi-supervised learning to utilize large unlabeled training data. By
modifying tri-training for sequence labeling and deriving the proper initialization, we can further
improve entity extraction. In the task of Chinese personal name extraction with 364,685 sen-
tences (8,672 news articles) and 54,449 (11,856 distinct) person names, an F-measure of 90.4%
can be achieved.
</bodyText>
<sectionHeader confidence="0.992537" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981">
Detecting named entities in documents is one of the most important tasks for message understanding.
For example, the #Microposts 2014 Workshop hosted an â€œEntity Extraction and Linking Challengeâ€,
which aimed to automatically extract entities from English microposts and link them to the correspond-
ing English DBpedia v3.9 resources (if a linkage existed). Like many other types of research, this task
relies on annotated training examples that require large amounts of manual labeling, leading to a limited
number of training examples (e.g. 2.3K tweets). While human-labelled training examples (ğ¿) have high
quality, their cost is very high. Thus the major concern in this paper is how to prepare training data for
entity extraction learning on the Web.
In practice, sometimes there are existing structured databases of known entities that are valuable to
improve extraction accuracy. For examples, personal names, school names, and company names can be
obtained from a Whoâ€™s Who website, and accessible government data for registered schools and busi-
nesses, respectively. Meanwhile, there are many unlabeled training examples that can be used for many
information extraction tasks. If we can automatically label known entities in the unlabeled training ex-
amples, we can obtain large labeled training set. While such training data may contain errors, self-testing
can be applied to filter unreliable labeling with less confidence.
On the other hand, the use of unlabeled training examples (ğ‘ˆ) has also been proved to be a promising
technique for classification. For example, co-training (Blum and Mitchell, 1998) and tri-training (Zhou
et al. 2005) are two successful techniques that use examples with high-confidence as predicted by the
other classifier or examples with consensus answers from the other two classifiers in order to prepare
new labeled training data for learning. By estimating the error rate of each learned classifier, we can
calculate the maximum number of new consensus answers for learning to ensure the error rates are
reduced.
In this paper, we explore the possibility of extending semi-supervised learning to sequence labeling
via tri-training so that unlabeled training examples can also be used in the learning phase. The challenge
here is to obtain a common label sequence as a consensus answer from multiple models. As enumerating
</bodyText>
<footnote confidence="0.415629">
1 This research was partially supported by ITRI, Taiwan under grant B2-101052.
</footnote>
<page confidence="0.758697">
33
</page>
<note confidence="0.8637215">
Proceedings of Third Workshop on Semantic Web and Information Extraction, pages 33â€“40,
Dublin, Ireland, 24 August, 2014.
</note>
<bodyText confidence="0.999855555555556">
all possible label sequences will be too time-consuming, we employ a confidence level to control the
co-labeling answer such that a label sequence with the largest probability is selected. Comparing with a
common label sequence from multiple models, the most probable label sequence has larger chance to
obtain a consensus answer for training and testing.
In addition to the extension of tri-training algorithm to sequence labeling, another key issue with tri-
training is the assumption of the initial error rate (0.5), leading to a limited number of co-labeling ex-
amples for training and early termination for large set training. Therefore, a new estimation method is
devised for the estimation of initial error rate to alleviate the problem and improve the overall perfor-
mance.
To validate the proposed method, we conduct experiments on Chinese personal name extraction using
7,000 known Chinese celebrity names (abbreviated as CCN). We collect news articles containing these
personal names from Googleâ€™s search engine (using these names as keywords) and automatically label
these articles containing CCN and known reportersâ€™ names. In a test set of 8,672 news articles (364,685
sentences) containing 54,449 personal names (11,856 distinct names), the basic model built on CRF
(conditional random field) has a performance of 76.8% F-measure when using 500 celebrity names for
preparing training data, and is improved to 86.4% F-measure when 7,000 celebrity names are used. With
self-testing, the performance is improved to 88.9%. Finally, tri-training can further improve the perfor-
mance through unlabeled data to 90.4%.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999272970588235">
Entity extraction is the task of recognizing named entities from unstructured text documents, which is
one of the information tasks to test how well a machine can understand the messages written in natural
language and automate mundane tasks normally performed by human. The development of machine
learning research from classification to sequence labeling such as the HMM (Hidden Markov Model)
(Bikel et al., 1997) and the CRF (Conditional Random Field) (McCallum and Wei, 2003) has been
widely discussed in recent years. While supervised learning shows an impressive improvement over
unsupervised learning, it requires large training data to be labeled with answers. Therefore, semi-super-
vised approaches are proposed.
Semi-supervised learning refers to techniques that also make use of unlabeled data for training. Many
approaches have been previously proposed for semi-supervised learning, including: generative models,
self-learning, co-training, graph-based methods (Zhou et al. 2005) and information-theoretic regulariza-
tion (Zheng et al. 2009). In contrast, although a number of semi-supervised classifications have been
proposed, semi-supervised learning for sequence segmentation has received considerably less attention
and is designed according to a different philosophy.
Co-training and tri-training have been mainly discussed for classification tasks with relatively few
labeled training examples. For example, the original co-training paper by Blum and Mitchell (1998)
described experiments to classify web pages into two classes using only 12 labeled web pages as exam-
ples. This co-training algorithm requires two views of the training data and learns a separate classifier
for each view using labeled examples. Nigam and Ghani (2000) demonstrated that co-training performed
better when the independent feature set assumption is valid. For comparison, they conducted their ex-
periments on the same (WebKB course) data set used by Blum and Mitchell.
Goldman and Zhou (2000) relaxed the redundant and independent assumption and presented an algo-
rithm that uses two different supervised learning algorithms to learn a separate classifier from the pro-
vided labeled data. Empirical results demonstrated that two standard classifiers can be used to success-
fully label data for each other with 95% confidence interval.
Tri-training (Zhou, et al. 2005) was an improvement of co-training, which used three classifiers and
a voting mechanism to solve the confidence issue of co-labeled answers by two classifiers. In each round
of tri-training, the classifiers hj and hk choose some examples in U to label for hi (i, j, kïƒ{1,2,3},
iï‚¹jï‚¹k). Let Ll denote the set of examples that are labeled for hi in the t-th round. Then the training set
for hi in the t-th round are L U Ll. Note that the unlabeled examples labeled in the t-th round, i.e. Ll,
wonâ€™t be put into the original labeled example set, i.e. L. Instead, in the (t + 1)-th round all the examples
in Ll will be regarded as unlabeled and put into U again.
While Tri-training has been used in many classification tasks, the application in sequence labeling
tasks is limited. Chen et al. (2006) proposed an agreement measure that computed the unit consistency
</bodyText>
<page confidence="0.689839">
34
</page>
<figureCaption confidence="0.998032">
Figure 1 Semi-Supervised Named Entity Extraction Based on Automatic Labeling and Tri-training
</figureCaption>
<bodyText confidence="0.9995142">
between two label sequences from two models. Then based on the agreement measure, the idea is to
choose a sentence, which is correctly labeled by â„ğ‘— and â„ğ‘˜ but is not parsed correctly by the target
classifier â„ğ‘–, to be a new training sample. A control parameter is used to determine the percentage (30%)
of examples selected for the next round. The process iterates until no more unlabeled examples are
available. Thus, Chen et al.â€™s method does not ensure the PAC learning theory.
</bodyText>
<sectionHeader confidence="0.927679" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.999988176470588">
Due to the high cost of labeling, most benchmarks for NER are limited to several thousand sentences.
For example, the English dataset for the CoNLL 2003 shared task (Tjong et al., 2003) consists of 14,987
training sentences for four entity categories, PER, LOC, ORG, and MISC. But it is unclear whether
sufficient data is provided for training or the learning algorithms have reached their capacity. Therefore,
two intuitive ways are considered in this paper: one is automatic labeling of unlabeled data for preparing
a large amount of annotated training examples, and the other is semi-supervised learning for making use
of both labeled and unlabeled data during learning.
For the former, automatic labeling is sometimes possible, especially for named entities which can be
obtained from Web resources like DBpedia. For example, suppose we want to train a named entity
extractor for the Reuters Corpus, we can use the known entities from CoNLL 2003 shared task as queries
to obtain documents that contain queries from the Reuters Corpus and label the articles automatically.
While such automatic annotation may involve wrong labeling, we can apply self-testing to filter low-
confidence labels. Overall, the benefit of the large amount of labeled training examples is greater than
the noise it may cause.
In this paper, we propose a hybrid model composed of the following modules: automatic labeling,
feature engineering, and tri-training based algorithm for training and testing. The framework is illus-
trated in Figure 1.
</bodyText>
<subsectionHeader confidence="0.997168">
3.1 Tri-training for Classification
</subsectionHeader>
<bodyText confidence="0.9986615">
Let ğ¿ denote the labeled example set with size |ğ¿  |and ğ‘ˆ denote the unlabeled example set with size |ğ‘ˆ|.
In each round, ğ‘¡, tri-training uses two models, â„ğ‘— and â„ğ‘˜, to label the answer of each instance ğ‘¥ from
unlabeled training data ğ‘ˆ. If â„ğ‘— and â„ğ‘˜ give the same answer, then we could use ğ‘¥ and the common an-
swer pair as newly training example, i.e. ğ¿ğ‘–ğ‘¡ = {(ğ‘¥, ğ‘¦): ğ‘¥ âˆˆ ğ‘ˆ, ğ‘¦ = â„ğ‘—ğ‘¡ (ğ‘¥) = â„ğ‘˜ğ‘¡ (ğ‘¥)}) for model â„ğ‘–
</bodyText>
<equation confidence="0.958920333333333">
(ğ‘–, ğ‘—, ğ‘˜ïƒ{1,2,3}, ğ‘–ï‚¹ğ‘—ï‚¹ğ‘˜). To ensure that the error rate is reduced through iterations, when training â„ğ‘–, Eq.
(1) must be satisfied,
ğ‘’ğ‘–ğ‘¡ |ğ¿ğ‘–ğ‘¡ |&lt; ğ‘’ğ‘–ğ‘¡âˆ’1|ğ¿ğ‘–ğ‘¡âˆ’1 |(1)
</equation>
<bodyText confidence="0.999937">
where ğ‘’ğ‘–ğ‘¡ denotes the error rate of model â„ğ‘– in ğ¿ğ‘–ğ‘¡, which is estimated by â„ğ‘— and â„ğ‘˜ in the t-th round using
the labeled data ğ¿ by dividing the number of labeled examples on which both â„ğ‘— and â„ğ‘˜ make an incor-
rect estimation by the number of labeled examples for which the estimation made by â„ğ‘— is the same as
that made by â„ğ‘˜, as shown in Eq. (2).2,
</bodyText>
<equation confidence="0.836036733333333">
2 Assuming that the unlabeled examples hold the same distribution as that held by the labeled ones.
35
ğ‘¡ =|{(ğ‘¥,ğ‘¦)âˆˆğ¿, â„ğ‘—ğ‘¡(ğ‘¥)=â„ğ‘˜ğ‘¡(ğ‘¥) â‰ ğ‘¦}|
ğ‘’ğ‘– (2)
|{(ğ‘¥,ğ‘¦)âˆˆğ¿, â„ğ‘— ğ‘¡(ğ‘¥)=â„ğ‘˜ ğ‘¡ (ğ‘¥)}|
If |ğ¿ğ‘–ğ‘¡  |is too large, such that Eq. (1) is violated, it would be necessary to sample maximum ğ‘¢ examples
from ğ¿ğ‘–ğ‘¡ such that Eq. (1) can be satisfied. (3)
ğ‘¡âˆ’1 |ğ‘¢) ğ‘£ğ‘–ğ‘œğ‘™ğ‘ğ‘¡ğ‘’ğ‘‘ ğ¸ğ‘. (1) (4)
ğ‘¡âˆ’1|ğ¿ğ‘– ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’
1âŒ‰
ğ‘¢ = âŒˆğ‘’ğ‘–
ğ‘¡ âˆ’
ğ‘’ğ‘–ğ‘¡ = {ğ‘†ğ‘¢ğ‘ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’(ğ¿ğ‘– ğ‘¡,
ğ‘†ğ‘–
ğ¿ğ‘– ğ‘¡
</equation>
<bodyText confidence="0.999644">
For the last step in each round, the union of the labeled training examples ğ¿ and ğ‘†ğ‘–ğ‘¡, i.e. ğ¿ â‹ƒğ‘†ğ‘–ğ‘¡, is used
as training data to update classifier â„ğ‘– for this iteration.
</bodyText>
<subsectionHeader confidence="0.986377">
3.2 Modification for the Initialization
</subsectionHeader>
<bodyText confidence="0.999503666666667">
According to Eq. (1), the product of error rate and new training examples define an upper bound for the
next iteration. Meanwhile, |ğ¿ğ‘–tâˆ’1 |should satisfy Eq. (5) such that |ğ¿ğ‘–ğ‘¡  |after subsampling, i.e., ğ‘¢, is still
bigger than |ğ¿ğ‘–ğ‘¡âˆ’1|.
</bodyText>
<equation confidence="0.97607675">
ğ‘¡
|ğ¿ğ‘– ğ‘¡âˆ’1 |&gt; ğ‘’ğ‘– ğ‘’ğ‘¡âˆ’
1âˆ’et
ğ‘– i
</equation>
<bodyText confidence="0.99957">
In order to estimate the size of |ğ¿ğ‘–1|, i.e., the number of new training examples for the first round, we
need to estimate ğ‘’ğ‘–0, ğ‘’ğ‘–1, and |ğ¿ğ‘–0 |first. Zhou et al. assumed a 0.5 error rate for ğ‘’ğ‘–0, computed ğ‘’ğ‘–1 by â„ğ‘—
and â„ğ‘˜, and estimated the lower bound for |ğ¿ğ‘– 0 |by Eq. (6), thus:
</bodyText>
<table confidence="0.994180142857143">
1 1
|ğ¿ğ‘– 0 |= âŒŠ ğ‘’ğ‘– + 1âŒ‹ âŒŠ ğ‘’ğ‘– + 1âŒ‹ (6)
=
1 1
0.5âˆ’ğ‘’ğ‘–
0âˆ’ğ‘’ğ‘–
ğ‘’ğ‘–
</table>
<bodyText confidence="0.905450692307692">
The problem with this initialization is that, for a larger dataset |ğ¿|, such an initialization will have no
effect on retraining and will lead to an early stop for tri-training. For example, consider the case when
the error rate ğ‘’ğ‘–1 is less than 0.4, then the value of |ğ¿ğ‘–0 |will be no more than 5, leading to a small upper
bound for ğ‘’ğ‘–1|ğ¿ğ‘–1 |according to Eq. (1). That is to say, we can only sample a small subset |ğ‘†ğ‘–1 |from ğ¿ğ‘–1
for training â„ğ‘– based on Eq. (4). On the other hand, if ğ‘’ğ‘–1 is close to 0.5 such that the value of |ğ¿ğ‘–0 |is
greater than the original dataset |ğ¿ |, it may completely alter the behavior of â„ğ‘–.
To avoid this difficulty, we propose a new estimation for the product ğ‘’ ğ‘–0|ğ¿ğ‘–0|. Let ğ¿ğ¶ (â„ğ‘—, â„ğ‘˜) denote
the set of labeled examples (from ğ¿) on which the classification made by â„ğ‘— is the same as that made by
â„ğ‘˜ in the initial round, and ğ¿ğ‘Š (â„ğ‘—, â„ğ‘˜) denote the set of examples from ğ¿ğ¶ (â„ğ‘—, â„ğ‘˜) on which both â„ğ‘— and
â„ğ‘˜ make incorrect classification, as shown in Eq. (7) and (8). In addition, we define ğ¿ğ‘–ğ‘Š (â„ğ‘—, â„ğ‘˜) to be
the set of examples from ğ¿ğ¶ (â„ğ‘—, â„ğ‘˜) on which â„ğ‘– makes incorrect classification in the initial round, as
shown in Eq. (9). The relationship among ğ¿ğ¶ (â„ğ‘—, â„ğ‘˜), ğ¿ğ‘Š (â„ğ‘—, â„ğ‘˜), and ğ¿ğ‘–ğ‘Š (â„ğ‘—, â„ğ‘˜) is illustrated in Figure
2.
</bodyText>
<equation confidence="0.826021666666667">
ğ¿ğ¶(â„ğ‘—,â„ğ‘˜) = {(ğ‘¥, ğ‘¦) âˆˆ ğ¿: â„ğ‘— (ğ‘¥) = â„ğ‘˜ (ğ‘¥)}
ğ¿ğ‘Š (â„ğ‘—, â„ğ‘˜) = {(ğ‘¥, ğ‘¦) âˆˆ ğ¿ğ¶ (â„ğ‘—, â„ğ‘˜): â„ğ‘— (ğ‘¥) â‰  ğ‘¦}
ğ¿ğ‘–ğ‘Š(â„ğ‘—, â„ğ‘˜) = {(ğ‘¥, ğ‘¦) âˆˆ ğ¿ğ¶ (â„ğ‘—, â„ğ‘˜): â„ğ‘– (ğ‘¥) â‰  ğ‘¦}
</equation>
<bodyText confidence="0.822827">
By replacing ğ‘’ğ‘–0|ğ¿ğ‘–0 |with ğ¿ğ‘–ğ‘Š (â„ğ‘—, â„ğ‘˜) and estimation of ğ‘’ğ‘–1 by |ğ¿ğ‘Š (â„ğ‘—, â„ğ‘˜)|/|ğ¿ğ¶ (â„ğ‘—, â„ğ‘˜)|), we can es-
timate an upper bound for |ğ¿ğ‘–0 |via Eq. (3). That is to say, we can compute an upper bound for |ğ¿ğ‘–0 |and
replace Eq. (3) by Eq. (10) to estimate the maximum data size of |ğ¿ğ‘– 1|, in the first round.
</bodyText>
<equation confidence="0.7548642">
0|ğ¿ğ‘–
|ğ¿ğ‘– 0 |= âŒˆğ‘’ğ‘– 1 âˆ’ 1âŒ‰ = âŒˆğ¿ğ‘–
0 |ğ‘Š(â„ğ‘—,â„ğ‘˜)âˆ—ğ¿ğ¶(â„ğ‘—,â„ğ‘˜)
ğ¿ğ‘Š(â„ğ‘—,â„ğ‘˜) âˆ’ 1âŒ‰ (10)
ğ‘’ğ‘–
</equation>
<figure confidence="0.7267695">
(5)
36
</figure>
<figureCaption confidence="0.993867">
Figure 2 The relationship among Eq. (7), (8), and (9).
</figureCaption>
<subsectionHeader confidence="0.988673">
3.3 Modification for Co-Labeling
</subsectionHeader>
<bodyText confidence="0.999917153846154">
The tri-training algorithm was originally designed for traditional classification. For sequence labeling,
we need to define what should be the common labels for the input example x when two models (training
time) or three models (testing time) are involved. In Chen et al.â€™s work, they only consider the most
probable label sequence from each model; the selection method chooses examples (for hi) with the high-
est-agreement labeled sentences by hj and hk, and the lowest-agreement labeled sentences by hi and hj;
finally, the newly training samples were labeled by hj (ignoring the label result by hk).
As the probability for two sequence labelers to output the same label sequence is low (a total of 5|i|
(BIEOS Tagging) possible label sequences with length l), we propose a different method to resolve this
issue. Assume that each model can output the m best label sequences with highest probability (m=5).
Let Pi(Y|x) denote the probability that an instance x has label Y estimated by hi. We select the label
with the largest probability sum by the co-labeling models. In other words, we could use hj and hk to
estimate possible labels, then choose the label Y with the maximum probability sum, Pi (Y |x) + Pk (Y|x),
to re-train hi. Thus, the set of examples, Ll, prepared for hi in the t-th round is defined as follows:
</bodyText>
<equation confidence="0.999822333333333">
Ll =
[(x, Y): xEU, max
ï¿½(Pj(Y|x) + Pk(Y|x)) &gt; B * 2} (11)
</equation>
<bodyText confidence="0.999980375">
where B (default 0.5) is a threshold that controls the quality of the training examples provided to hi.
During testing, the label Y for an instance x is determined by three models hl, h2 and h3. We choose
the output with the largest probability sum from 3 models with a confidence B * 3 or B * 2. If the label
with the largest probability sum from 3 models is not greater than B * 3, then we choose the one with
the largest probability from single model with a maximum probability. That is to say, if the label with
the largest probability sum from three models is not greater than B * 3, then we choose the one with the
largest probability sum from two models with a confidence of B * 2. The last selection criterion is the
label with the maximum probability estimated by the three models as shown in Eq. (12).
</bodyText>
<equation confidence="0.991576181818182">
(Pi(Y|x) + P2(Y|x) + P3(Y|x)) &gt;_ 0 * 3
max
Y
Y= max
Y
max(Pi(Y|x) + Pj(Y|x)) &gt;_ 0 * 2, i, jE{1,2,3}, iej
ï¿½
max
(Pi(Y |x), P2(Y |x), P3(Y |x))
Y
(12)
</equation>
<sectionHeader confidence="0.989057" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999993625">
We apply our proposed approach on Chinese personal name extraction. We use known celebrity names
to query search engines for news articles from four websites (including Liberty Times, Apple Daily,
China Times, and United Daily News) and collect the top 10 search results for sentences that contain
the query keyword and uses these query keyword as extraction target via automatic labeling. Given
different numbers of personal names, we prepare six datasets by automatically labeling as mentioned in
the beginning of Section 3 and consider them as labeled training examples. We also crawl these four
news websites from 2013/01/01 to 2013/03/31 and obtain 20,974 articles for unlabeled and testing data.
To increase the possibility of containing person names, we select sentences that include some common
</bodyText>
<page confidence="0.977296">
37
</page>
<tableCaption confidence="0.999367">
Table 1 Labeled dataset (L) and unlabeled dataset (U) for Chinese person name extraction
</tableCaption>
<table confidence="0.997278">
Dataset 1 Dataset 2 L Dataset 4 Dataset 5 Dataset 6 U
Dataset 3 --
#Names 500 1,000 2,000 3,000 5,000 7,053 --
Sentences 5,548 10,928 21,267 30,653 50,738 67,104 240,994
Words 106,535 208,383 400,111 567,794 913,516 1,188,822 4,251,861
</table>
<bodyText confidence="0.999346909090909">
surname followed by some common first name to obtain 240,994 as unlabeled data (U) (Table 1). For
testing, we manually labeled 8,672 news articles, yielding a total of 364,685 sentences with 54,449
person names (11,856 distinct person names).
For the tagging scheme, we used BIEOS to mark the named entities to be extracted. Fourteen features
were used in the experiment including, common surnames, first names, job titles, numeric tokens, al-
phabet tokens, punctuation symbol, and common characters in front or behind personal names. The
predefined dictionaries contain 486 job titles, 224 surnames, 38,261 first names, and 107 symbols as
well as 223 common words in front of and behind person name. We use CRF++ (Kudo 2004) for the
following experiment. With a template involving unigram macros and the previous three tokens and
behind, a total of 195 features are produced. We define precision, recall and F-measure based on the
number personal names as follows:
</bodyText>
<equation confidence="0.7687982">
Precision = Correctly identified names (13)
Identified names
Recall = Correctly identified names (14)
Real names
F âˆ’ Measure = 2PR/(P + R) (15)
</equation>
<subsectionHeader confidence="0.998024">
4.1 Performance of Automatic Labeling &amp; Self-Testing
</subsectionHeader>
<bodyText confidence="0.9999893125">
As mentioned above, using the query keyword itself to label the collected news articles (called uni-
labeling) only labels a small part of known person names. Therefore, we also use all celebrity names
and six report name patterns such as â€œUDN [reporter name]/Taipeiâ€ (4ï¿½ [oi; ]/p3t ï¿½), to
label all collected articles (called Full-labelling). While this automatic labelling procedure does not en-
sure perfect training data, it provides acceptable labelled training for semi-supervised learning. As
shown in Figure 3, the automatic labelling procedure can greatly improve the performance on the testing
data.
Based on this basic model, we apply self-testing to filter examples with low confidence and retrain a
new model with the set of high confidence examples. The idea is to use the trained CRF model to test
the training data themselves and output the conditional probability for the most possible label sequence.
By removing examples with low confidence we can retrain a new model with the set of high confidence
examples. As indicated by black-dashed line (with + symbol) in Figure 4, the F-measures increases as
the data size increases. The performance of self-testing is improved for all datasets with confidence
levels from 0.5 to 0.9. An F-measure of 0.815 (Dataset 1) to 0.889 (Dataset 6) can be obtained, depend-
ing on the number of celebrity names we have. The best performance is achieved at confidence level 0.8
for all data sets except for dataset 3 which has the best performance when T = 0.9.
</bodyText>
<subsectionHeader confidence="0.998379">
4.2 Performance of Tri-Training
</subsectionHeader>
<bodyText confidence="0.9915745">
Next, we evaluate the effect of using unlabeled training data based on tri-training. In our initial attempt
to apply original tri-training, we obtained no improvement for all datasets. As shown in Figure 5, the
final data size used for training and the performance is similar to those values obtained for the self-
testing results (with confidence level 0.8). This is because we have a very small estimation of ILiÂ°I by
Eq. (6) when a 0.5 initial error rate for eiÂ° (iïƒ{1,2,3}) is assumed. Therefore, it does not make any im-
provement on retraining.
</bodyText>
<figure confidence="0.991238804347826">
38
Data Size
Uni-Labeling
Full-Labeling
0.90
0.80
0.70
0.60
0.50
0.40
0.30
0.20
0.10
Dataset 1
0.7677
0.1916
5,548
Performance of Automatic Labeling
Dataset 2
0.7974
0.2750
10,928
Dataset 3
0.8254
0.3939
21,267
Dataset 4
0.8329
0.4762
30,653
Dataset 5
0.8544
0.6249
50,738
Dataset 6
0.8636
0.6916
67,104
40,000
0
70,000
60,000
50,000
30,000
20,000
10,000
</figure>
<figureCaption confidence="0.989255">
Figure 3 Performance Comparison of automatic labeling
Figure 4 Performance Comparison of self-testing
</figureCaption>
<figure confidence="0.491435">
Sa
</figure>
<figureCaption confidence="0.906">
Figure 5 Performance of Tri-training with different initialization for |L1|
</figureCaption>
<subsectionHeader confidence="0.731155">
S
</subsectionHeader>
<bodyText confidence="0.999773285714286">
However, with the new initialization by Eq. (10), the number of examples that can be sampled from
unlabeled dataset |Ll |is greatly increased. For dataset 1, the unlabeled data selected is five times the
original data size (an increase from 4,637 to 25,234), leading to an improvement of 2.4% in F-measure
(from 0.815 to 0.839). For dataset 2, the final data size is twice the original data size (from 8,881 to
26,173) with an F-measure improvement of 2.7% (from 0.830 to 0.857). For dataset 6, since |Ll |is too
large to be loaded for training with L, we only use 75% for experiment. The improvement in F-measure
is 1.5%. Overall, an improvement of 1.2% ~ 2.7% can be obtained with this tri-training algorithm.
</bodyText>
<figure confidence="0.980905451612903">
F-Measure with Various Filtering Threshold
Dataset
Dataset 5
Dataset
Dataset
3
Dataset The same dataset
2
Dataset 1
4
0 10,000 20,000 30,000 40,000 50,000 60,000 70,000
Data Size
None T=0.5 T=0.6 T=0.7 T=0.8 T=0.9
0.90
0.89
0.88
0.87
0.86
0.85
0.84
0.83
0.82
0.81
0.80
0.79
0.78
0.77
0.76
0.75
0.74
39
</figure>
<sectionHeader confidence="0.938902" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999565">
Named entity extraction has been approached with supervised approaches that require large labeled
training examples to achieve good performance. This research makes use of automatic labeling based
on known entity names to create a large corpus of labeled training data. While such data may contain
noise, the benefit with large labeled training data still is more significant than noise it inherits. In practice,
we might have a large amount of unlabeled data. Therefore, we applied tri-training to make use of such
unlabeled data and to modify the co-labeling mechanism for sequence labeling to improve the
performance. Instead of assuming a constant error rate for the initial error of each classifier, we proposed
a new way to estimate the number of examples selected from unlabeled data. As shown in the
experiments, such a semi-supervised approach can further improve the F-measure to 0.904 for dataset 6
with 7,000 celebrity names.
</bodyText>
<sectionHeader confidence="0.932101" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999301083333333">
Rie Kubota Ando and Tong Zhang. 2005. A high-performance semi-supervised learning method for text chunking.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL &apos;05). pp.1-9.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. COLT&apos; 98 Proceed-
ings of the eleventh annual conference on Computational learning theory, pp. 92-100.
Wenliang Chen, Yujie Zhang and Hitoshi Isahara. Chinese Chunking with Tri-training Learning, The 21st Inter-
national Conference on the Computer Processing of Oriental Languages (ICCPOL2006), LNCS, Vol. 4285,
Springer, pp. 466-473, Singapore, Dec. 2006.
Sally Goldman and Yan Zhou. 2000. Enhancing supervised learning with unlabeled data. ICML&apos;00 Proceedings
of the 17th International Conference on Machine Learning, pp. 327-334.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, and Dale Schuurmans. 2006. Semi-supervised condi-
tional random fields for improved sequence segmentation and labeling. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational
Linguistics (ACL-44), pp. 209-216.
Taku Kudo. CRF++: Yet Another CRF toolkit. http://crfpp.googlecode.com/svn/trunk/doc/index.html
Wei Li, and Andrew McCallum. 2005. Semi-supervised sequence modeling with syntactic topic models. In Pro-
ceedings of the National Conference on Artificial Intelligence - Volume 2 (AAAI &apos;05), pp. 813-818.
Gideon S. Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with
weakly labeled data. Journal of machine learning research, Volume 11, pp.955-984.
Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields,
feature induction and web-enhanced lexicons. In Proceedings of the seventh conference on Natural language
learning at HLT-NAACL 2003 - Volume 4 (CONLL &apos;03), pp. 188-191.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. CIKM &apos;00
Proceedings of the ninth international conference on Information and knowledge management, pp. 86-93.
CÃ­cero Nogueira dos Santos, Ruy Luiz MilidiÃº. 2012. Named entity recognition. Entropy Guided Transformation
Learning: Algorithms and Applications, Springer, Briefs in Computer Science, pp. 51-58.
Erik F. Tjong, Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: language-
independent named entity recognition. In Proceedings of the seventh conference on Natural language learning
at HLT-NAACL 2003 - Volume 4 (CONLL &apos;03), pp. 142-147.
Lei Zheng, Shaojun Wang, Yan Liu, and Chi-Hoon Lee. 2009. Information theoretic regularization for semi-su-
pervised boosting. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery
and data mining (KDD &apos;09), pp. 1017-1026.
Dengyong Zhou, Jiayuan Huang, and Bernhard SchÃ¶lkopf. 2005. Learning from labeled and unlabeled data on a
directed graph. In Proceedings of the 22nd international conference on Machine learning (ICML &apos;05), pp. 1036-
1043.
Zhi-Hua Zhou and Ming Li. 2005. Tri-Training: Exploiting Unlabeled Data Using Three Classifiers. IEEE Trans-
actions on Knowledge and Data Engineering archive, Volume 17 Issue 11, pp. 1529-1541.
</reference>
<page confidence="0.915454">
40
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.126369">
<title confidence="0.830992666666667">Semi-supervised Sequence Labeling for Named Entity based on Tri-Training: Case Study on Chinese Person Name Extraction</title>
<affiliation confidence="0.8279435">National Central Taoyuan,</affiliation>
<email confidence="0.999854">formatc.chou@gmail.com</email>
<author confidence="0.687518">Chia-Hui</author>
<affiliation confidence="0.995925">National Central</affiliation>
<address confidence="0.812249">Taoyuan,</address>
<email confidence="0.987985">chia@csie.ncu.edu.tw</email>
<author confidence="0.805394">Shin-Yi</author>
<affiliation confidence="0.969862">Industrial Research Institute,</affiliation>
<email confidence="0.933438">sywu@itri.org.tw</email>
<abstract confidence="0.996603615384615">Named entity extraction is a fundamental task for many knowledge engineering applications. Existing studies rely on annotated training data, which is quite expensive when used to obtain large data sets, limiting the effectiveness of recognition. In this research, we propose an automatic labeling procedure to prepare training data from structured resources which contain known named entities. While this automatically labeled training data may contain noise, a self-testing procedure may be used as a follow-up to remove low-confidence annotation and increase the extraction performance with less training data. In addition to the preparation of labeled training data, we also employed semi-supervised learning to utilize large unlabeled training data. By modifying tri-training for sequence labeling and deriving the proper initialization, we can further improve entity extraction. In the task of Chinese personal name extraction with 364,685 sentences (8,672 news articles) and 54,449 (11,856 distinct) person names, an F-measure of 90.4% can be achieved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rie Kubota Ando</author>
<author>Tong Zhang</author>
</authors>
<title>A high-performance semi-supervised learning method for text chunking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL &apos;05).</booktitle>
<pages>1--9</pages>
<marker>Ando, Zhang, 2005</marker>
<rawString>Rie Kubota Ando and Tong Zhang. 2005. A high-performance semi-supervised learning method for text chunking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL &apos;05). pp.1-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training. COLT&apos;</title>
<date>1998</date>
<booktitle>98 Proceedings of the eleventh annual conference on Computational learning theory,</booktitle>
<pages>92--100</pages>
<contexts>
<context position="3068" citStr="Blum and Mitchell, 1998" startWordPosition="438" endWordPosition="441"> and accessible government data for registered schools and businesses, respectively. Meanwhile, there are many unlabeled training examples that can be used for many information extraction tasks. If we can automatically label known entities in the unlabeled training examples, we can obtain large labeled training set. While such training data may contain errors, self-testing can be applied to filter unreliable labeling with less confidence. On the other hand, the use of unlabeled training examples (ğ‘ˆ) has also been proved to be a promising technique for classification. For example, co-training (Blum and Mitchell, 1998) and tri-training (Zhou et al. 2005) are two successful techniques that use examples with high-confidence as predicted by the other classifier or examples with consensus answers from the other two classifiers in order to prepare new labeled training data for learning. By estimating the error rate of each learned classifier, we can calculate the maximum number of new consensus answers for learning to ensure the error rates are reduced. In this paper, we explore the possibility of extending semi-supervised learning to sequence labeling via tri-training so that unlabeled training examples can als</context>
<context position="7139" citStr="Blum and Mitchell (1998)" startWordPosition="1040" endWordPosition="1043">osed for semi-supervised learning, including: generative models, self-learning, co-training, graph-based methods (Zhou et al. 2005) and information-theoretic regularization (Zheng et al. 2009). In contrast, although a number of semi-supervised classifications have been proposed, semi-supervised learning for sequence segmentation has received considerably less attention and is designed according to a different philosophy. Co-training and tri-training have been mainly discussed for classification tasks with relatively few labeled training examples. For example, the original co-training paper by Blum and Mitchell (1998) described experiments to classify web pages into two classes using only 12 labeled web pages as examples. This co-training algorithm requires two views of the training data and learns a separate classifier for each view using labeled examples. Nigam and Ghani (2000) demonstrated that co-training performed better when the independent feature set assumption is valid. For comparison, they conducted their experiments on the same (WebKB course) data set used by Blum and Mitchell. Goldman and Zhou (2000) relaxed the redundant and independent assumption and presented an algorithm that uses two diffe</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. COLT&apos; 98 Proceedings of the eleventh annual conference on Computational learning theory, pp. 92-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
</authors>
<title>Yujie Zhang and Hitoshi Isahara. Chinese Chunking with Tri-training Learning,</title>
<date>2006</date>
<booktitle>The 21st International Conference on the Computer Processing of Oriental Languages (ICCPOL2006), LNCS,</booktitle>
<volume>4285</volume>
<pages>466--473</pages>
<publisher>Springer,</publisher>
<marker>Chen, 2006</marker>
<rawString>Wenliang Chen, Yujie Zhang and Hitoshi Isahara. Chinese Chunking with Tri-training Learning, The 21st International Conference on the Computer Processing of Oriental Languages (ICCPOL2006), LNCS, Vol. 4285, Springer, pp. 466-473, Singapore, Dec. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sally Goldman</author>
<author>Yan Zhou</author>
</authors>
<title>Enhancing supervised learning with unlabeled data.</title>
<date>2000</date>
<booktitle>ICML&apos;00 Proceedings of the 17th International Conference on Machine Learning,</booktitle>
<pages>327--334</pages>
<contexts>
<context position="7643" citStr="Goldman and Zhou (2000)" startWordPosition="1119" endWordPosition="1122"> with relatively few labeled training examples. For example, the original co-training paper by Blum and Mitchell (1998) described experiments to classify web pages into two classes using only 12 labeled web pages as examples. This co-training algorithm requires two views of the training data and learns a separate classifier for each view using labeled examples. Nigam and Ghani (2000) demonstrated that co-training performed better when the independent feature set assumption is valid. For comparison, they conducted their experiments on the same (WebKB course) data set used by Blum and Mitchell. Goldman and Zhou (2000) relaxed the redundant and independent assumption and presented an algorithm that uses two different supervised learning algorithms to learn a separate classifier from the provided labeled data. Empirical results demonstrated that two standard classifiers can be used to successfully label data for each other with 95% confidence interval. Tri-training (Zhou, et al. 2005) was an improvement of co-training, which used three classifiers and a voting mechanism to solve the confidence issue of co-labeled answers by two classifiers. In each round of tri-training, the classifiers hj and hk choose some</context>
</contexts>
<marker>Goldman, Zhou, 2000</marker>
<rawString>Sally Goldman and Yan Zhou. 2000. Enhancing supervised learning with unlabeled data. ICML&apos;00 Proceedings of the 17th International Conference on Machine Learning, pp. 327-334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Jiao</author>
<author>Shaojun Wang</author>
<author>Chi-Hoon Lee</author>
<author>Russell Greiner</author>
<author>Dale Schuurmans</author>
</authors>
<title>Semi-supervised conditional random fields for improved sequence segmentation and labeling.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics (ACL-44),</booktitle>
<pages>209--216</pages>
<marker>Jiao, Wang, Lee, Greiner, Schuurmans, 2006</marker>
<rawString>Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell Greiner, and Dale Schuurmans. 2006. Semi-supervised conditional random fields for improved sequence segmentation and labeling. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics (ACL-44), pp. 209-216.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Taku Kudo</author>
</authors>
<title>CRF++: Yet Another CRF</title>
<note>toolkit. http://crfpp.googlecode.com/svn/trunk/doc/index.html</note>
<marker>Kudo, </marker>
<rawString>Taku Kudo. CRF++: Yet Another CRF toolkit. http://crfpp.googlecode.com/svn/trunk/doc/index.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Li</author>
<author>Andrew McCallum</author>
</authors>
<title>Semi-supervised sequence modeling with syntactic topic models.</title>
<date>2005</date>
<booktitle>In Proceedings of the National Conference on Artificial Intelligence - Volume</booktitle>
<volume>2</volume>
<pages>813--818</pages>
<marker>Li, McCallum, 2005</marker>
<rawString>Wei Li, and Andrew McCallum. 2005. Semi-supervised sequence modeling with syntactic topic models. In Proceedings of the National Conference on Artificial Intelligence - Volume 2 (AAAI &apos;05), pp. 813-818.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized expectation criteria for semi-supervised learning with weakly labeled data.</title>
<date>2010</date>
<journal>Journal of machine learning research, Volume</journal>
<volume>11</volume>
<pages>955--984</pages>
<marker>Mann, McCallum, 2010</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. Journal of machine learning research, Volume 11, pp.955-984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 - Volume 4 (CONLL &apos;03),</booktitle>
<pages>188--191</pages>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 - Volume 4 (CONLL &apos;03), pp. 188-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Rayid Ghani</author>
</authors>
<title>Analyzing the effectiveness and applicability of co-training.</title>
<date>2000</date>
<booktitle>CIKM &apos;00 Proceedings of the ninth international conference on Information and knowledge management,</booktitle>
<pages>86--93</pages>
<contexts>
<context position="7406" citStr="Nigam and Ghani (2000)" startWordPosition="1083" endWordPosition="1086"> proposed, semi-supervised learning for sequence segmentation has received considerably less attention and is designed according to a different philosophy. Co-training and tri-training have been mainly discussed for classification tasks with relatively few labeled training examples. For example, the original co-training paper by Blum and Mitchell (1998) described experiments to classify web pages into two classes using only 12 labeled web pages as examples. This co-training algorithm requires two views of the training data and learns a separate classifier for each view using labeled examples. Nigam and Ghani (2000) demonstrated that co-training performed better when the independent feature set assumption is valid. For comparison, they conducted their experiments on the same (WebKB course) data set used by Blum and Mitchell. Goldman and Zhou (2000) relaxed the redundant and independent assumption and presented an algorithm that uses two different supervised learning algorithms to learn a separate classifier from the provided labeled data. Empirical results demonstrated that two standard classifiers can be used to successfully label data for each other with 95% confidence interval. Tri-training (Zhou, et </context>
</contexts>
<marker>Nigam, Ghani, 2000</marker>
<rawString>Kamal Nigam and Rayid Ghani. 2000. Analyzing the effectiveness and applicability of co-training. CIKM &apos;00 Proceedings of the ninth international conference on Information and knowledge management, pp. 86-93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CÃ­cero Nogueira dos Santos</author>
<author>Ruy Luiz MilidiÃº</author>
</authors>
<title>Named entity recognition.</title>
<date>2012</date>
<journal>Entropy Guided Transformation Learning: Algorithms and Applications, Springer, Briefs in Computer Science,</journal>
<pages>51--58</pages>
<marker>Santos, MilidiÃº, 2012</marker>
<rawString>CÃ­cero Nogueira dos Santos, Ruy Luiz MilidiÃº. 2012. Named entity recognition. Entropy Guided Transformation Learning: Algorithms and Applications, Springer, Briefs in Computer Science, pp. 51-58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong</author>
<author>Kim Sang</author>
<author>Fien De Meulder</author>
</authors>
<title>Introduction to the CoNLL-2003 shared task: languageindependent named entity recognition.</title>
<date>2003</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 - Volume 4 (CONLL &apos;03),</booktitle>
<pages>142--147</pages>
<marker>Tjong, Sang, De Meulder, 2003</marker>
<rawString>Erik F. Tjong, Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: languageindependent named entity recognition. In Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003 - Volume 4 (CONLL &apos;03), pp. 142-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lei Zheng</author>
<author>Shaojun Wang</author>
<author>Yan Liu</author>
<author>Chi-Hoon Lee</author>
</authors>
<title>Information theoretic regularization for semi-supervised boosting.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD &apos;09),</booktitle>
<pages>1017--1026</pages>
<contexts>
<context position="6707" citStr="Zheng et al. 2009" startWordPosition="984" endWordPosition="987">itional Random Field) (McCallum and Wei, 2003) has been widely discussed in recent years. While supervised learning shows an impressive improvement over unsupervised learning, it requires large training data to be labeled with answers. Therefore, semi-supervised approaches are proposed. Semi-supervised learning refers to techniques that also make use of unlabeled data for training. Many approaches have been previously proposed for semi-supervised learning, including: generative models, self-learning, co-training, graph-based methods (Zhou et al. 2005) and information-theoretic regularization (Zheng et al. 2009). In contrast, although a number of semi-supervised classifications have been proposed, semi-supervised learning for sequence segmentation has received considerably less attention and is designed according to a different philosophy. Co-training and tri-training have been mainly discussed for classification tasks with relatively few labeled training examples. For example, the original co-training paper by Blum and Mitchell (1998) described experiments to classify web pages into two classes using only 12 labeled web pages as examples. This co-training algorithm requires two views of the training</context>
</contexts>
<marker>Zheng, Wang, Liu, Lee, 2009</marker>
<rawString>Lei Zheng, Shaojun Wang, Yan Liu, and Chi-Hoon Lee. 2009. Information theoretic regularization for semi-supervised boosting. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (KDD &apos;09), pp. 1017-1026.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dengyong Zhou</author>
<author>Jiayuan Huang</author>
<author>Bernhard SchÃ¶lkopf</author>
</authors>
<title>Learning from labeled and unlabeled data on a directed graph.</title>
<date>2005</date>
<booktitle>In Proceedings of the 22nd international conference on Machine learning (ICML &apos;05),</booktitle>
<pages>1036--1043</pages>
<contexts>
<context position="3104" citStr="Zhou et al. 2005" startWordPosition="444" endWordPosition="447">red schools and businesses, respectively. Meanwhile, there are many unlabeled training examples that can be used for many information extraction tasks. If we can automatically label known entities in the unlabeled training examples, we can obtain large labeled training set. While such training data may contain errors, self-testing can be applied to filter unreliable labeling with less confidence. On the other hand, the use of unlabeled training examples (ğ‘ˆ) has also been proved to be a promising technique for classification. For example, co-training (Blum and Mitchell, 1998) and tri-training (Zhou et al. 2005) are two successful techniques that use examples with high-confidence as predicted by the other classifier or examples with consensus answers from the other two classifiers in order to prepare new labeled training data for learning. By estimating the error rate of each learned classifier, we can calculate the maximum number of new consensus answers for learning to ensure the error rates are reduced. In this paper, we explore the possibility of extending semi-supervised learning to sequence labeling via tri-training so that unlabeled training examples can also be used in the learning phase. The</context>
<context position="6646" citStr="Zhou et al. 2005" startWordPosition="976" endWordPosition="979">(Hidden Markov Model) (Bikel et al., 1997) and the CRF (Conditional Random Field) (McCallum and Wei, 2003) has been widely discussed in recent years. While supervised learning shows an impressive improvement over unsupervised learning, it requires large training data to be labeled with answers. Therefore, semi-supervised approaches are proposed. Semi-supervised learning refers to techniques that also make use of unlabeled data for training. Many approaches have been previously proposed for semi-supervised learning, including: generative models, self-learning, co-training, graph-based methods (Zhou et al. 2005) and information-theoretic regularization (Zheng et al. 2009). In contrast, although a number of semi-supervised classifications have been proposed, semi-supervised learning for sequence segmentation has received considerably less attention and is designed according to a different philosophy. Co-training and tri-training have been mainly discussed for classification tasks with relatively few labeled training examples. For example, the original co-training paper by Blum and Mitchell (1998) described experiments to classify web pages into two classes using only 12 labeled web pages as examples. </context>
<context position="8015" citStr="Zhou, et al. 2005" startWordPosition="1175" endWordPosition="1178">ni (2000) demonstrated that co-training performed better when the independent feature set assumption is valid. For comparison, they conducted their experiments on the same (WebKB course) data set used by Blum and Mitchell. Goldman and Zhou (2000) relaxed the redundant and independent assumption and presented an algorithm that uses two different supervised learning algorithms to learn a separate classifier from the provided labeled data. Empirical results demonstrated that two standard classifiers can be used to successfully label data for each other with 95% confidence interval. Tri-training (Zhou, et al. 2005) was an improvement of co-training, which used three classifiers and a voting mechanism to solve the confidence issue of co-labeled answers by two classifiers. In each round of tri-training, the classifiers hj and hk choose some examples in U to label for hi (i, j, k{1,2,3}, ijk). Let Ll denote the set of examples that are labeled for hi in the t-th round. Then the training set for hi in the t-th round are L U Ll. Note that the unlabeled examples labeled in the t-th round, i.e. Ll, wonâ€™t be put into the original labeled example set, i.e. L. Instead, in the (t + 1)-th round all the examples </context>
</contexts>
<marker>Zhou, Huang, SchÃ¶lkopf, 2005</marker>
<rawString>Dengyong Zhou, Jiayuan Huang, and Bernhard SchÃ¶lkopf. 2005. Learning from labeled and unlabeled data on a directed graph. In Proceedings of the 22nd international conference on Machine learning (ICML &apos;05), pp. 1036-1043.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Hua Zhou</author>
<author>Ming Li</author>
</authors>
<title>Tri-Training: Exploiting Unlabeled Data Using Three Classifiers.</title>
<date>2005</date>
<journal>IEEE Transactions on Knowledge and Data Engineering archive, Volume</journal>
<volume>17</volume>
<pages>1529--1541</pages>
<marker>Zhou, Li, 2005</marker>
<rawString>Zhi-Hua Zhou and Ming Li. 2005. Tri-Training: Exploiting Unlabeled Data Using Three Classifiers. IEEE Transactions on Knowledge and Data Engineering archive, Volume 17 Issue 11, pp. 1529-1541.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
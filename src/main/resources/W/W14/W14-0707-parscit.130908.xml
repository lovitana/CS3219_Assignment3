<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000165">
<title confidence="0.997265">
Recognizing Causality in Verb-Noun Pairs
via Noun and Verb Semantics
</title>
<author confidence="0.977167">
Mehwish Riaz and Roxana Girju
</author>
<affiliation confidence="0.859233333333333">
Department of Computer Science and Beckman Institute
University of Illinois at Urbana-Champaign
Urbana, IL 61801, USA
</affiliation>
<email confidence="0.99867">
{mriaz2,girju}@illinois.edu
</email>
<sectionHeader confidence="0.997385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999840772727273">
Several supervised approaches have been
proposed for causality identification by re-
lying on shallow linguistic features. How-
ever, such features do not lead to improved
performance. Therefore, novel sources
of knowledge are required to achieve
progress on this problem. In this paper,
we propose a model for the recognition of
causality in verb-noun pairs by employing
additional types of knowledge along with
linguistic features. In particular, we fo-
cus on identifying and employing seman-
tic classes of nouns and verbs with high
tendency to encode cause or non-cause re-
lations. Our model incorporates the in-
formation about these classes to minimize
errors in predictions made by a basic su-
pervised classifier relying merely on shal-
low linguistic features. As compared with
this basic classifier our model achieves
14.74% (29.57%) improvement in F-score
(accuracy), respectively.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9998104">
The automatic detection of causal relations is im-
portant for various natural language processing ap-
plications such as question answering, text sum-
marization, text understanding and event predic-
tion. Causality can be expressed using various nat-
ural language constructions (Girju and Moldovan,
2002; Chang and Choi, 2006). Consider the fol-
lowing examples where causal relations are en-
coded using (1) a verb-verb pair, (2) a noun-noun
pair and (3) a verb-noun pair.
</bodyText>
<listItem confidence="0.9971045">
1. Five shoppers were killed when a car blew up
at an outdoor market.
2. The attack on Kirkuk’s police intelligence
complex sees further deaths after violence
spilled over a nearby shopping mall.
3. At least 1,833 people died in hurricane.
</listItem>
<bodyText confidence="0.999890823529412">
Since, the task of automatic recognition of
causality is quite challenging, researchers have
addressed this problem by considering specific
constructions. For example, various models
have been proposed to identify causation between
verbs (Bethard and Martin, 2008; Beamer and
Girju, 2009; Riaz and Girju, 2010; Do et al., 2011;
Riaz and Girju, 2013) and between nouns (Girju
and Moldovan, 2002; Girju, 2003). Do et al.
(2011) have worked with verb-noun pairs for
causality detection but they focused only on a
small list of predefined nouns representing events.
In this paper, we focus on the task of identi-
fying causality encoded by verb-noun pairs (ex-
ample 3). We propose a novel model which first
predicts cause or non-cause relations using a su-
pervised classifier and then incorporates additional
types of knowledge to reduce errors in predictions.
Using a supervised classifier, our model identi-
fies causation by employing shallow linguistic fea-
tures (e.g., lemmas of verb and noun, words be-
tween verb and noun). Such features have been
used successfully for various NLP tasks (e.g., part-
of-speech tagging, named entity recognition, etc.)
but confinement to such features does not help
much to achieve performance for identifying cau-
sation (Riaz and Girju, 2013). Therefore, in our
model we plug in additional types of knowledge
to obtain better predictions for the current task.
For example, we identify the semantic classes of
nouns and verbs with high tendency to encode
cause or non-causal relations and use this knowl-
edge to achieve better performance. Specifically,
the contributions of this paper are as follows:
</bodyText>
<listItem confidence="0.881091166666667">
• In order to build a supervised classifier, we
use the annotations of FrameNet to generate a
training corpus of verb-noun instances encod-
ing cause and non-cause relations. We propose
a set of linguistic features to learn and identify
causal relations.
</listItem>
<page confidence="0.989903">
48
</page>
<note confidence="0.9886845">
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 48–57,
Gothenburg, Sweden, April 26, 2014. c�2014 Association for Computational Linguistics
</note>
<listItem confidence="0.494750571428571">
• In order to make intelligent predictions, it is
important for our model to have knowledge
about the semantic classes of nouns with high
tendency to encode causal or non-causal re-
lations. For example, a named entity such
as person, organization or location may have
high tendency to encode non-causality unless a
metonymic reading is associated with it. In our
approach, we identify such semantic classes of
nouns by exploiting a named entity recognizer,
the annotations of frame elements provided in
FrameNet and WordNet.
• Verbs are the important components of lan-
guage for expressing events of various types.
</listItem>
<bodyText confidence="0.999622">
For example, Pustejovsky et al. (2003)
have classified events into eight semantic
classes: OCCURRENCE, PERCEPTION, RE-
PORTING, ASPECTUAL, STATE, I STATE,
I ACTION and MODAL. We argue that there
are some semantic classes in this list with high
tendency to encode cause or non-cause rela-
tions. For example, reporting events repre-
sented by verbs say, tell, etc., have high ten-
dency to just report other events instead of en-
coding causality with them. In our model, we
use such information to reduce errors in predic-
tions.
</bodyText>
<listItem confidence="0.990904055555555">
• Each causal relation is characterized by two
roles i.e., cause and its effect. In example 3
above, the noun “hurricane” is cause and the
verb “died” is its effect. However, a verb-noun
pair may not encode causality when a verb
and a noun represent same event. For exam-
ple, in instance “Colin Powell presented fur-
ther evidence in his presentation.”, the verb
“presented” and the noun “presentation” rep-
resent same event of “presenting” and thus en-
coding non-cause relation with each other. In
our model, we determine the verb-noun pairs
representing same or distinct events to make
predictions accordingly.
• We adopt the framework of Integer Linear Pro-
gramming (ILP) (Roth and Yih, 2004; Do et al.,
2011) to combine all the above types of knowl-
edge for the current task.
</listItem>
<bodyText confidence="0.999921833333333">
This paper is organized as follows. In next sec-
tion, we briefly review the previous research done
for identifying causality. We introduce our model
and evaluation with discussion on results in sec-
tion 3 and 4, respectively. The section 5 of the
paper concludes our current research.
</bodyText>
<sectionHeader confidence="0.999786" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99994406">
In computational linguistics, researchers have al-
ways shown interest in the task of automatic
recognition of causal relations because success on
this task is critical for various natural language
applications (Girju, 2003; Chklovski and Pantel,
2004; Radinsky and Horvitz, 2013).
Following the successful employment of lin-
guistic features for various tasks (e.g., part-of-
speech tagging, named entity recognition, etc.),
initially NLP researchers proposed approaches re-
lying mainly on such features to identify causal-
ity (Girju, 2003; Bethard and Martin, 2008;
Sporleder and Lascarides, 2008; Pitler and
Nenkova, 2009; Pitler et al., 2009). However,
researchers have recently shifted their attention
from these features and tried to consider other
sources of knowledge for extracting causal rela-
tions (Beamer and Girju, 2009; Riaz and Girju,
2010; Do et al., 2011; Riaz and Girju, 2013).
For example, Riaz and Girju (2010) and Do et
al. (2011) have proposed unsupervised metrics for
learning causal dependencies between two events.
Do et al. (2011) have also incorporated minimal
supervision with unsupervised metrics. For a pair
of events (a, b), their model makes the decision of
cause or non-cause relation based on unsupervised
co-occurrence counts and then improves this deci-
sion by using minimal supervision from the causal
and non-causal discourse markers (e.g., because,
although, etc.).
In search of novel and effective types of knowl-
edge to identify causation between two verbal
events, Riaz and Girju (2013) have proposed a
model to learn a Knowledge Base (KB,) of verb-
verb pairs. In this knowledge base, the English
language verb-verb pairs are automatically clas-
sified into three categories: (1) Strongly Causal,
(2) Ambiguous and (2) Strongly Non-Causal. The
Strongly Causal and Strongly Non-Causal cate-
gories contain verb-verb pairs with highest and
least tendency to encode causality, respectively
and rest of the verb-verb pairs are considered am-
biguous with tendency to encode both types of
relations. They claim that this knowledge base
of verb-verb pairs is a rich source of causal as-
sociations. The incorporation of this resource
into a causality detection model can help identi-
fying causality with better performance. In this
research, we also try to go beyond the scope of
shallow linguistic features and identify additional
</bodyText>
<page confidence="0.998967">
49
</page>
<bodyText confidence="0.966367">
interesting types of knowledge for the current task.
</bodyText>
<sectionHeader confidence="0.9757565" genericHeader="method">
3 Computational Model for Identifying
Causality
</sectionHeader>
<bodyText confidence="0.999983444444444">
In this section, we introduce our model for iden-
tifying causality encoded by verb-noun pairs.
Specifically, we extract all main verbs and noun
phrases from a sentence and predict cause or non-
cause relation on verb-noun phrase (v-np) pairs.
In order to make task easier, we consider only
those v-np pairs where v (verb) is grammatically
connected to np (noun phrase). We assume that a v
and np are grammatically connected if there exists
a dependency relation between them in the depen-
dency tree. We apply a dependency parser (Marn-
effe et al., 2006) to identify such dependencies.
Our model first employs a supervised classifier re-
lying on linguistic features to make binary predic-
tions (i.e., does a verb-noun phrase pair encode a
cause or non-cause relation?). We then incorpo-
rate additional types of knowledge on top of these
binary predictions to improve performance.
</bodyText>
<subsectionHeader confidence="0.999597">
3.1 Supervised Classifier
</subsectionHeader>
<bodyText confidence="0.999685214285714">
In this section, we propose a basic supervised
classifier to identify causation encoded by v-np
pairs. To set up this supervised classifier, we need
a training corpus of instances of v-np pairs en-
coding cause and non-cause relations. For this
purpose, we employ the annotations of FrameNet
project (Baker et al., 1998) provided for verbs.
For example, consider the following annotation
from FrameNet for the verb “dying” with ar-
gument “solvent abuse” where the pair “dying-
solvent abuse” encodes causality.
A campaign has started to try to cut the
rising number of children dying [cause
from solvent abuse].
To generate a training corpus, we collect anno-
tations of verbs from FrameNet s.t. the annotated
element (aka. frame element) is a noun phrase.
For example, we get a causal training instance of
“dying-solvent abuse” pair from the above anno-
tation. We assume that if a FrameNet’s annotated
element contains a verb in it then this may not rep-
resent a training instance of v-np pair. For exam-
ple, we do not consider the following annotation
in our training corpus where causality is encoded
between two verbs i.e., “died-fell”.
A fitness fanatic died [cause when 26
stone of weights fell on him as he ex-
ercised].
After extracting training instances from
FrameNet, we assign them cause (c) and non-
cause (¬ c) labels. We manually examined the
inventory of labels of FrameNet and use the
following scheme to assign the c or ¬c to each
training instance. All the annotations of FrameNet
with following labels are considered as causal
training instances and rest of the annotations are
considered as non-causal training instances.
Purpose, Internal cause, Result, Exter-
nal cause, Cause, Reason, Explanation,
Required situation, Purpose of Event,
Negative consequences, resulting ac-
tion, Effect, Cause of shine, Purpose of
Goods, Response action, Enabled situa-
tion, Grinding cause, Trigger
For this work, we have acquired 2,158
(65, 777) cause (non-cause) training instances
from FrameNet. Since, the non-cause instances
are very large in number, our supervised model
tends to assign non-cause labels to almost all in-
stances. Therefore, we employ equal number of
cause and non-cause instances for training. In fu-
ture, we plan to extract more annotations from
the FrameNet and employ more than one human
annotators to assign the labels of cause and non-
cause relations to the full inventory of labels of
FrameNet.
</bodyText>
<listItem confidence="0.913544210526316">
• Lexical Features: verb, lemma of verb, noun
phrase, lemma of all words of noun phrase,
head noun of noun phrase, lemmas of all words
between verb and head noun of noun phrase.
• Semantic Features: We adopted this feature
from Girju (2003) to capture the semantics of
nouns. The 9 noun hierarchies of WordNet i.e.,
entity, psychological feature, abstraction, state,
event, act, group, possession, phenomenon are
used as this feature. Each of these hierarchies
is set to 1 if any sense of the head noun of noun
phrase lies in that hierarchy otherwise set to 0.
• Structural Features: This feature is applied
by considering both subject (i.e., sub in np)
and object (i.e., obj in np) of a verb. For ex-
ample, for a v-np pair the variable sub in np is
set to 1 if the subject of v is contained in np,
set to 0 if the subject of v is not contained in
np and set to -1 if the subject of v is not avail-
</listItem>
<bodyText confidence="0.7595566">
able in the instance. The subject and object of
a verb are its core arguments and may some-
time be part of an event represented by a verb.
Therefore, these argument may have high ten-
dency to encode non-cause relations.
</bodyText>
<page confidence="0.975291">
50
</page>
<bodyText confidence="0.999474333333333">
We set up the following integer linear program
after acquiring predictions of c and ¬ c labels us-
ing our supervised classifier.
</bodyText>
<equation confidence="0.999449">
E x1(v-np, l) = 1 V v-np E I (2)
lEL1
x1(v-np, l) E {0, 1} V v-np E I Vl E L1 (3)
</equation>
<bodyText confidence="0.999667058823529">
Here L1 = {c, ¬c}, I is the set of all instances
of v-np pairs and x1(v-np, l) is the decision vari-
able set to 1 only if the label l ∈ L1 is assigned
to v-np. The Equation 2 constraints that only one
label out of |L1 |choices can be assigned to a v-np
pair. The equation 3 requires x1(v-np, l) to be a
binary variable. Specifically, we try to maximize
the objective function Z1 (equation 1) which as-
signs the label cause or non-cause to all v-np pairs
(i.e., set the variables x1(v-np, l) to 1 or 0 for all
l ∈ L1 and for all v-np pairs in I) depending
on the probabilities of assignment of labels (i.e.,
P(v-np,l))1. These probabilities can be obtained
by running a supervised classification algorithm
(e.g., Naive Bayes and Maximum Entropy). In our
experiments, we provide results using the follow-
ing probabilities acquired with Naive Bayes.
</bodyText>
<equation confidence="0.9992332">
�n k=1 lo9P(fk  |c)
P(v-np, c) = 1.0 ��n
�lE(c,¬c) lo9P(fk  |l)
k=1
P(v-np, -c) = 1.0 - P((v, np), c) (4)
</equation>
<bodyText confidence="0.999869333333333">
where fk is a feature, n is total number of fea-
tures and P(fk  |l) is the smoothed probability of
a feature fk given the training instances of label l.
</bodyText>
<subsectionHeader confidence="0.999961">
3.2 Knowledge of Semantic classes of nouns
</subsectionHeader>
<bodyText confidence="0.99991225">
Philospher Jaegwon Kim (Kim, 1993) (as cited by
Girju and Moldovan (2002)) pointed out that the
entities which represent either causes or effects are
often events, but also conditions, states, phenom-
ena, processes, and sometimes even facts. There-
fore, according to this our model should have
knowledge of the semantic classes of noun phrases
with high tendency to encode cause or non-cause
relations. Considering this type of knowledge, we
can automatically review and correct the wrong
predictions made by our basic supervised classi-
fier.
</bodyText>
<footnote confidence="0.635184">
1We use the integer linear program solver available at
http://sourceforge.net/projects/lpsolve/
</footnote>
<bodyText confidence="0.999928666666667">
We argue that if a noun phrase represents a
named entity then it can have least tendency to en-
code causal relations unless there is a metonymic
reading associated with it. For example, con-
sider the following cause and non-cause examples
where noun phrase is a named entity.
</bodyText>
<listItem confidence="0.967083333333333">
4. Sandy hit Cuba as a Category 3 hurricane.
5. Almost all the weapon sites in Iraq were de-
stroyed by the United States.
</listItem>
<bodyText confidence="0.999735853658537">
In example 4, Cuba is location and does not
encode causality. However, in example 5 the
pair “destroyed-the United States” encode causal-
ity where a metonymic reading is associated with
the location. We apply Named Entity Recog-
nizer (Finkel et al., 2005) and assume if a noun
phrase is identified as a named entity then its cor-
responding verb-noun phrase pair encodes non-
cause relation. This constraint can lead to a false
negative prediction when the metonymic reading
is associated with a noun phrase. In order to
avoid as much false negatives as possible, we im-
ply the following simple rule i.e., if one of the
following cue words appear between a verb and a
noun phrase then do not apply the constraint stated
above.
by, from, because of, through, for
In our experiments, the above simple rule helps
avoiding some false negatives but in future any
subsequent improvement with a better metonomy
resolver (Markert and Nissim, 2009) should im-
prove the performance of our model.
In addition to named entities, there can be var-
ious noun phrases with least tendency to encode
causation. Consider the following example, where
“city” is a location and does not encode cause-
effect relation with the verb “remained”.
Substantially fewer people remained in
the city during the Hurricane Ivan evac-
uation.
In this work, we identify the semantic classes
of noun phrases which do not normally represent
events, conditions, states, phenomena, processes
and thus have high tendency to encode non-cause
relations. For this purpose, we manually examine
the inventory of labels assigned to noun phrases
in FrameNet (see table 1) and classify these labels
into two classes (cnp and ¬cnp). Here, the class
cnp (¬cnp) represents the labels of noun phrases
with high (less) tendency to encode cause-effect
relations. For example, the label “Place” ∈ ¬cnp
</bodyText>
<equation confidence="0.999087333333333">
�Z1 = max E
v-npEIlEL1
x1(v-np, l)P(v-np, l) (1)
</equation>
<page confidence="0.970927">
51
</page>
<bodyText confidence="0.999833428571429">
(see table 1) represents a location and it may have
least tendency to encode causality if metonymy is
not associated with it. Using the classification of
frame elements in table 1, we obtain the annota-
tions of noun phrases from FrameNet and catego-
rize these annotations into cnp and ¬cnp classes.
On top of the annotations of these two semantic
classes, we build a supervised classifier for pre-
dicting cnp or ¬cnp label for the noun phrases.
After obtaining predictions, we select all noun
phrases lying in class ¬cnp and apply the same
constraint stated above for the named entities. We
use the following set of features to set up a super-
vised classifier for cnp and ¬cnp labels.
</bodyText>
<listItem confidence="0.9907364">
• Lexical Features: words of noun phrase, lem-
mas of all words of noun phrase, head word
of noun phrase, first two (three) (four) letters
of head noun of noun phrase, last two, (three)
(four) letters of head noun of noun phrase.
• Word Class Features: part-of-speech tags of
all words of noun phrase, part-of-speech tag of
head noun of noun phrase.
• Semantic Features: all (frequent) sense(s) of
head noun of noun phrase.
</listItem>
<bodyText confidence="0.99876840625">
We have acquired 23,334 (81,279) training in-
stances of cnp (¬cnp) class, respectively for this
work. We also use WordNet to obtain more train-
ing instances of these classes. We follow the ap-
proach similar to Girju and Moldovan (2002) and
adopt some senses of WordNet (shown in table 1)
to acquire training instances of noun phrases. For
example, considering the table 1, we assign ¬cnp
label to any noun whose all senses in WordNet lie
in the semantic hierarchy originated by the sense
{time period, period of time, period}. Follow-
ing this scheme, we extract instances of nouns and
noun phrases from English GigaWord corpus and
assign the labels cnp and ¬cnp to them by em-
ploying WordNet senses given in table 1. Girju
and Moldovan (2002) have used similar scheme
to rank noun phrases according to their tendency
to encode causation. In comparison to them, we
use the WordNet senses to increase the size of
our training set of noun phrases obtained using
FrameNet above. In addition to this, we build a
automatic classifier on the training data obtained
using labels of FrameNet and WordNet senses to
classify noun phrases of test instances into two se-
mantics classes (i.e., cnp and ¬cnp). In our train-
ing corpus of there are 2, 214, 68 instances of noun
phrases (50% belongs to each of cnp and ¬cnp
classes).
We incorporate the knowledge of semantics of
nouns in our model by making the following ad-
ditions to the integer linear program introduced in
section 3.1.
</bodyText>
<equation confidence="0.999889714285714">
�Z2 = Z1 +
np:v-np∈I l∈L2
E x2(np, l) = 1 `dnp : v-np E I − M (6)
l∈L2
x2(np, l) E {0, 1} `dnp : v-np E I − M (7)
`dl E L2
x1(v-np, -c) − x2(np, -cnp) &gt; 0 (8)
</equation>
<bodyText confidence="0.982589">
`d np : v-np, `d v-np E I − M
Here L2 = {cnp, ¬cnp} and M is the set of
instances of those v-np pairs for which we con-
sider the possibility of attachment of metonymic
reading with np, x2(np, l) is the decision variable
set to 1 only if the label l E L2 is assigned to
np. The Equation 6 constraints that only one la-
bel out of |L2 |choices can be assigned to a np.
The equation 7 requires x2(np, l) to be a binary
variable. The constraint 8 assumes that if an np
belongs to the semantic class ¬cnp then its corre-
sponding pair v-np is assigned the label ¬c. We
maximize the objective function Z2 (equation 5)
of our integer linear program subject to the con-
straints introduced above. We predict the semantic
class of a noun phrase using the supervised classi-
fier for cnp and ¬cnp classes and set the probabil-
ities i.e., P(np, l) = 1, P(np, {L2} − {l}) = 0 if
the label l E L2 is assigned to np. Again we use
Naive Bayes to predict the labels for noun phrases.
Also before running this supervised classifier, we
run the named entity recognizer and assign ¬cnp
labels to all noun phrases identified as named en-
tities. For our model, we apply named entity rec-
ognizer for seven classes i.e., LOCATION, PER-
SON, ORGANIZATION, DATE, TIME, MONEY,
PERCENT (Finkel et al., 2005).
</bodyText>
<subsectionHeader confidence="0.999825">
3.3 Knowledge of Semantic classes of verbs
</subsectionHeader>
<bodyText confidence="0.9999674">
In this section, we introduce our method to in-
corporate the knowledge of semantic classes of
verbs to identify causation. Verbs are the com-
ponents of language for expressing events of var-
ious types. In TimeBank corpus, Pustejovsky et
al. (2003) have introduced eight semantic classes
of events i.e., OCCURRENCE, PERCEPTION,
REPORTING, ASPECTUAL, STATE, I STATE,
I ACTION and MODAL. According to the defi-
nitions of these classes provided by Pustejovsky
</bodyText>
<equation confidence="0.977447">
x2(np, l)P(np, l) (5)
</equation>
<page confidence="0.98805">
52
</page>
<subsectionHeader confidence="0.61461">
Semantic FrameNet Labels WordNet Senses
Class
</subsectionHeader>
<bodyText confidence="0.981271">
{act, deed, human action, human activity},
{phenomenon}, {state}, {psychological
feature}, {event}, {causal agent, cause,
causalagency}
{time period, period of time, period},
{measure, quantity, amount}, {group,
grouping}, {organization, organisation},
{time unit, unit of time}, {clock time, time}
</bodyText>
<table confidence="0.959181">
cnp Event, Goal, Purpose, Cause, Internal
cause, External cause, Result, Means, Rea-
son, Phenomena
¬cnp Artist, Performer, Duration, Time, Place,
Distributor, Area, Path, Direction, Sub-
region
</table>
<tableCaption confidence="0.742834">
Table 1: This table presents some examples of FrameNet labels in cnp and ¬cnp classes. The full set of
labels in both semantic classes are given in appendix A. It also presents the WordNet senses of nouns
lying in cnp and ¬cnp classes.
</tableCaption>
<bodyText confidence="0.999987088235294">
et al. (2003), the reporting events describe the
action of a person, declare something or narrate
an event e.g., the reporting events represented by
verbs say, tell, etc. Here, we argue that a reporting
event has the least tendency to encode causation
because such an event only describes or narrates
another event instead of encoding causality with
it. We assume that the verbs representing report-
ing events have least tendency to encode causa-
tion and thus their corresponding v-np pairs have
least tendency to encode causation. To add this
knowledge to our model, we consider two classes
of verbs i.e., c„ and ¬c„ where the class c„ (¬c„)
contains the verbs with high (less) tendency to en-
code causation. Using above argument we claim
that all verbs representing reporting events belong
¬c„ class and verbs representing rest of the types
of events belong to c„ class. We build a supervised
classifier which automatically classifies verbs into
c„ and ¬c„ classes. We extract the instances of
verbal events (i.e., verbs or verbal phrases) from
TimeBank corpus and assign the labels c„ and
¬c„ to these instances. Using these labeled in-
stances, we build a supervised classifier by adopt-
ing the same set features as introduced in Bethard
and Martin (2006) to identify semantic classes of
verbs. Due to space constraint, we refer the reader
to Bethard and Martin (2006) for the details of fea-
tures. Again we use Naive Bayes to take predic-
tions of c„ and ¬c„ labels and their corresponding
probabilities using equation 4.
We incorporate the knowledge of semantics of
verbs in our model by making the following addi-
tions to the integer linear program.
</bodyText>
<equation confidence="0.984755222222222">
�Z3 = Z2 +
v:v-np∈I
E x3(v, l) = 1 ` dv : v-np E I (10)
l∈L3
x3(v, l) E {0, 1} ` dv : v-np E I `dl E L3 (11)
x1(v-np, -c) − x3(v, -c„) &gt;= 0 (12)
`d v : v-np, `d v-np E I
x3(v, c„) − x1(v-np, c) &gt;= 0 (13)
`d v : v-np, `d v-np E I
</equation>
<bodyText confidence="0.9996513125">
Here L3 = {c„, ¬c„}, x3(v, l) is the decision
variable set to 1 only if the label l ∈ L3 is as-
signed to v. The Equation 10 constraints that only
one label out of |L3 |choices can be assigned to
a v. The equation 11 requires x3(v, l) to be a bi-
nary variable. The constraint 12 assumes if a verb
v belongs to the class c„ (i.e., has least potential
to encode causation) then its corresponding pair
v-np encodes non-causality. The constraint 12 en-
forces that if a verb v belongs to the class ¬c„ then
its corresponding v-np pair is assigned the label
¬c. Similarly, the constraint 16 enforces that if
a v-np pair encodes causality then its verb v has
potential to encode causal relation. We maximize
the objective function Z3 subject to the constraints
introduced above.
</bodyText>
<subsectionHeader confidence="0.964556">
3.4 Knowledge of Indistinguishable Verb and
Noun
</subsectionHeader>
<bodyText confidence="0.999909625">
As introduced earlier, each causal relation is char-
acterized by two roles i.e., cause and its effect. In
order to encode causal relation, two components
of an instance of verb-noun phrase pair need to
represent distinct events, processes or phenomena.
Employing simple lexical matching, we determine
if a verb and a noun phrase represent same event
or not as follows:
</bodyText>
<equation confidence="0.839007">
E x3(v, l)P(v, l) (9)
l∈L3
</equation>
<page confidence="0.99499">
53
</page>
<bodyText confidence="0.910108166666667">
• We use NOMLEX (Macleod et al., 2009) to
transform a verb into its corresponding nomi-
nalization and use the following text segments
for lexical matching.
Tv = [Subject] verb [Object]2
Tn = Head noun of noun phrase
</bodyText>
<listItem confidence="0.813681333333333">
• We remove stopwords and duplicate words
from Tv and Tn and take lemmas of all words.
If the subject or object or both arguments are
contained in noun phrase then we remove these
arguments from Tv. We determine the proba-
bility of a verb (v) and a noun phrase (np) rep-
resenting same event as follows. If head noun
(i.e., Tn) lexically matches with any word of Tv
then set P(v - np) to 1 and 0 otherwise.
</listItem>
<bodyText confidence="0.999987">
We assign non-cause relation if P(v - np) = 1.
Next, we incorporate the knowledge of indistin-
guishable verb and noun in our model using the
following additions to our integer linear program.
</bodyText>
<equation confidence="0.9993626">
E x4(v-np, l) = 1 V v-np E I (15)
l∈L4
x4(v-np, l) E {0, 1} V v-np E I, Vl E L4
x1(v-np, -c) − x4(v-np, -) &gt; 0 (16)
V v-np E I
</equation>
<bodyText confidence="0.999503727272727">
Here L4 = {-,�-} where the label - (�-) rep-
resents same (distinct) events, x4(v-np, l) is the
decision variable set to 1 only if the label l E L4
is assigned to v-np. The Equation 15 constraints
that only one label out of |L4 |choices can be as-
signed to a v-np pair. The equation 16 requires
x4(v-np, l) to be a binary variable. The con-
straint 16 enforces that if a v-np pair belongs to
the class - then this pair is assigned the label mac.
We maximize the objective function Z4 subject to
the constraints introduced above.
</bodyText>
<sectionHeader confidence="0.987157" genericHeader="evaluation">
4 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.992262786885246">
In this section we present the experiments, evalu-
ation procedures, and a discussion on the results
achieved through our model for the current task.
In order to evaluate our model, we generated a
test set with instances of form verb-noun phrase
where the verb is grammatically connected to the
noun phrase in an instance. For this purpose, we
2Following Riaz ang Girju (2010), we assume that the
subject and object of a verb are parts of an event represented
by a verb. Therefore, we use these arguments along with a
verb for lexical matching with a noun phrase.
collected three wiki articles on the topics of Hurri-
cane Katrina, Iraq War and Egyptian Revolution
of 2011. We selected first 100 sentences from
these articles and applied part-of-speech tagger
(Toutanova et al., 2003) and dependency parser
(Marneffe et al., 2006) on these sentences. Using
each sentence, we extracted all verb-noun phrase
pairs where the verb has a dependency relation
with any word of noun phrase. We manually in-
spected all of the extracted instances and removed
those instances in which a word had been wrongly
classified as a verb by the part-of-speech tagger.
There are total 1106 instances in our test set. We
assigned the task of annotation of these instances
with cause and non-cause relations to a human
annotator. Using manipulation theory of causal-
ity (Woodward, 2008), we adopted the annotation
guidelines from Riaz and Girju (2010) which is as
follows: “Assign cause label to a pair (a, b), if the
following two conditions are satisfied: (1), a tem-
porally precedes/overlap b in time, (2) while keep-
ing as many state of affairs constant as possible,
modifying a must entail predictably modifying b.
Otherwise assign non-cause label. ”
We have 149 (957) cause (non-cause) instances
in our test set3, respectively. We evaluate the per-
formance of our model using F-score and accuracy
evaluation measures (see table 2 for results).
The results in table 2 reveal that the ba-
sic supervised classifier is a naive model and
achieves only 27.27% F-score and 46.47% ac-
curacy. The addition of novel types of
knowledge introduced in section 3 (i.e., the
model Basic+SCNM+SCV+IVN) brings 14.74%
(29.57%) improvements in F-score (accuracy), re-
spectively. These results show that the knowledge
of semantics of nouns and verbs and the knowl-
edge of indistinguishable verb and noun are crit-
ical to achieve performance. The maximum im-
provement in results is achieved with the addition
of semantic classes of nouns (i.e., Basic+SCNM).
The consideration of association of metonymic
readings using model Basic+SCNM helps us to
maintain recall as compared with SCN!M and
therefore brings better F-score.
One can notice that almost all models suffer
from low precision which leads to lower F-scores.
Although, our model achieves 14.58% increase in
precision over basic supervised classifier, the lack
of high precision is still responsible for lower F-
</bodyText>
<footnote confidence="0.894427">
3We will make the test set available
</footnote>
<equation confidence="0.999636">
�Z4 = Z3 + E
v-np∈I l∈L4
x4(v-np, l)P(v-np, l) (14)
</equation>
<page confidence="0.995923">
54
</page>
<table confidence="0.9998436">
Model Basic +SCN!M +SCNM +SCNM+SCV +SCNM+SCV+IVN
Accuracy 46.47 75.76 74.41 75.31 76.04
Precision 16.69 28.14 29.53 30.47 31.27
Recall 74.49 50.66 64.00 64.00 64.00
F-score 27.27 39.19 40.42 41.29 42.01
</table>
<tableCaption confidence="0.984385">
Table 2: This table presents results of the basic supervised classifier (i.e., Basic) and the models after
</tableCaption>
<bodyText confidence="0.951900071428572">
incrementally adding the knowledge of semantic classes of nouns without consideration of metonymic
readings (i.e., + SCN!M), the knowledge of semantic classes of nouns with consideration of metonymic
readings (i.e., + SCNM), the knowledge of semantic classes of verbs (i.e., +SCNM+SCV) and the knowl-
edge of indistinguishable verb and noun (i.e., +SCNM+SCV+IVN).
score. The highly skewed distribution of test set
with only 13.47% causal instances results in lots
of false positives. We manually examined false
positives to determine the language features which
may help us reducing more false positives without
affecting F-score. We noticed that the direct ob-
jects of the verbs are mostly part of the event rep-
resented by the verbs and therefore encodes non-
causation with the verbs. For example, consider
following instances:
</bodyText>
<listItem confidence="0.896337666666667">
6. The hurricane surge protection failures
prompted a lawsuit.
7. They provided weather forecasts.
</listItem>
<bodyText confidence="0.999966217391305">
In example 6, “lawsuit” is the direct object of
the verb “prompted” and is part of the event rep-
resented by the verb “prompt”. However there
is a cause relation between “protection failures”
and “prompted”. Similarly in example 7, the di-
rect object “forecasts” is part of the “providing”
event and thus the noun phrase “weather fore-
casts” encode non-cause relation with the verb
“provide”. Therefore, following this observation
we employed the training corpus of cause and non-
cause relations (see section 3.1) and learned the
structure of verb-noun phrase pairs encoding non-
cause relations most of the time. We considered
only those training instances where the subject
and/or object of the verb was available. For the
current purpose, we picked up following four fea-
tures (1) sub in np, (2) !sub in np, (3) obj in np
and (4) !obj in np. Just to remind the reader, the
feature sub in np (!sub in np) is set to 1 if the
subject of the verb is (not) contained in the noun
phrase np, respectively. For each of the above four
features, the percentage of cause and entropy of
relations with that feature are as follows:
</bodyText>
<listItem confidence="0.999917">
• sub in np (%c = 34.72, Entropy = 0.931)
• !sub in np (%c = 59.71, Entropy = 0.972)
• obj in np (%c = 28.89, Entropy = 0.867)
• !obj in np (%c = 55.30, Entropy = 0.991).
</listItem>
<bodyText confidence="0.999990454545454">
There are two important observations from
above scores: (1) verbs mostly encode non-cause
relations with their objects and subjects (i.e., high
%mac with obj in np and sub in np), (2) among
obj in np and sub in np features, obj in np yields
least entropy i.e., there are least chances of encod-
ing causality of a verb with its object.
Considering the above statistics, we enforce the
constraint on each verb-noun phrase pair that if
the object of the verb is contained in the noun
phrase of the above pair then assigns non-cause
relation to that pair. Using this constraint, we ob-
tain 46.61% (80.74%) F-score (accuracy), respec-
tively. This confirms our observation that the ob-
ject of a verb is normally part of an event repre-
sented by the verb and thus it encodes non-cause
relation with the verb.
In this research, we have utilized novel types
of knowledge to improve the performance of our
model. In future, we need to consider more
additional information (e.g., predictions from
metonymy resolver) to achieve further progress.
</bodyText>
<sectionHeader confidence="0.999301" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999872769230769">
In this paper, we have proposed a model for iden-
tifying causality in verb-noun pairs by employing
the knowledge of semantic classes of nouns and
verbs and the knowledge of indistinguishable noun
and verb of an instance along with shallow linguis-
tic features. Our empirical evaluation of model
has revealed that such novel types of knowledge
are critical to achieve a better performance on the
current task. Following the encouraging results
achieved by our model, we invite researchers to
investigate more interesting types of knowledge in
future to make further progress on the task of rec-
ognizing causality.
</bodyText>
<page confidence="0.998151">
55
</page>
<sectionHeader confidence="0.995781" genericHeader="acknowledgments">
References
</sectionHeader>
<reference confidence="0.998546619047619">
Collin F. Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet project. In proceed-
ings of the Association for Computational Linguis-
tics and International Conference on Computational
Linguistics (COLING-ACL).
Brandon Beamer and Roxana Girju. 2009. Using
a Bigram Event Model to Predict Causal Poten-
tial. In proceedings of the Conference on Compu-
tational Linguistics and intelligent Text Processing
(CICLING).
Steven Bethard and James H. Martin. 2006. Identifica-
tion of Event Mentions and their Semantic Class. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Steven Bethard and James H. Martin. 2008. Learning
Semantic Links from a Corpus of Parallel Temporal
and Causal Relations. In proceedings of the Associ-
ation for Computational Linguistics (ACL).
Du-Seong Chang and Key-Sun Choi. 2006. Incremen-
tal cue phrase learning and bootstrapping method for
causality extraction using cue phrase and word pair
probabilities. Information Processing and Manage-
ment, volume 42 issue 3, 662678.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the Web for Fine-Grained Semantic
Verb Relations. In proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Quang X. Do, Yee S. Chen and Dan Roth. 2011. Min-
imally Supervised Event Causality Identication. In
proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
Jenny R. Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Informa-
tion into Information Extraction Systems by Gibbs
Sampling. In Proceedings of the Association for
Computational Linguistics (ACL).
Roxana Girju. 2003. Automatic detection of causal
relations for Question Answering. Association for
Computational Linguistics ACL, Workshop on Mul-
tilingual Summarization and Question Answering
Machine Learning and Beyond.
Roxana Girju and Dan Moldovan. 2002. Mining An-
swers for Causation Questions. In American Asso-
ciations of Artificial Intelligence (AAAI), 2002 Sym-
posium.
Jaegwon Kim. 1993. Causes and Events. Mackie on
Causation. In Cansation, Oxford Readings in Phi-
losophy, ed. Ernest Sosa, and Michael Tooley, Ox-
ford University Press.
Catherine Macleod, Ralph Grishman, Adam Meyers,
Leslie Barrett, Ruth Reeves. 1998. NOMLEX: A
Lexicon of Nominalizations. In proceedings of EU-
RALEX.
Katja Markert, Malvina Nissim 2009. Data and mod-
els for metonymy resolution. Language Resources
and Evaluation Volume 43 Issue 2, Pages 123−138.
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).
Emily Pitler, Annie Louis and Ani Nenkova. 2009.
Automatic Sense Prediction for Implicit Discourse
Relations in Text. In proceedings of ACL-IJCNLP.
Emily Pitler and Ani Nenkova. 2009. Using Syntax
to Disambiguate Explicit Discourse Connectives in
Text. In proceedings of ACL-IJCNLP.
James Pustejovsky, Patrick Hanks, Roser Saur, Andrew
See, Robert Gaizauskas, Andrea Setzer, Dragomir
Radev, Beth Sundheim, David Day, Lisa Ferro and
Marcia Lazo. 2003. The TIMEBANK Corpus. In
Proceedings of Corpus Linguistics.
Kira Radinsky and Eric Horvitz. 2013. Mining the
Web to Predict Future Events. In proceedings of
sixth ACM international conference on Web search
and data mining, (WSDM).
Mehwish Riaz and Roxana Girju. 2010. Another Look
at Causality: Discovering Scenario-Specific Contin-
gency Relationships with No Supervision. In pro-
ceedings of the IEEE 4th International Conference
on Semantic Computing (ICSC).
Mehwish Riaz and Roxana Girju 2013. Toward
a Better Understanding of Causality between Ver-
bal Events: Extraction and Analysis of the Causal
Power of Verb-Verb Associations. Proceedings of
the annual SIGdial Meeting on Discourse and Dia-
logue (SIGDIAL).
Dan Roth and Wen-tau Yih 2004. A Linear Program-
ming Formulation for Global Inference in Natural
Language Tasks. In Proceedings of the Annual Con-
ference on Computational Natural Language Learn-
ing (CoNLL).
Caroline Sporleder and Alex Lascarides. 2008. Using
automatically labelled examples to classify rhetor-
ical relations: An assessment. Journal of Natural
Language Engineering Volume 14 Issue 3.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of Human Language Technol-
ogy and North American Chapter of the Association
for Computational Linguistics (HLT-NAACL).
James Woodward. 2008. Causation and Manipulation.
Online Encyclopedia of Philosophy.
</reference>
<sectionHeader confidence="0.904426" genericHeader="references">
Appendix A. Semantic Classes of Nouns
</sectionHeader>
<bodyText confidence="0.9981255">
This appendix presents the FrameNet labels we as-
sign to cnp and ¬cnp classes (see section 3.2).
</bodyText>
<page confidence="0.996752">
56
</page>
<reference confidence="0.998756929824562">
Semantic Class FrameNet Labels
cnp Event, Goal, Purpose, Cause, Internal cause, External cause, Result, Means, Reason, Phenomena, Char-
acterization, Coordinated event, Final state, Information, Topic, Containing event, Mental content, Ac-
tion, Experience, Impactee, Impactor, Message, Question, Circumstances, Desired goal, Explanation,
Required situation, Complaint, Content, Activity, Intended goal, Phenomenon, State, Dependent state,
Forgery, Purpose of Event, Negative consequences, Inference, Appraisal, Noisy event, Function, Evi-
dence, Process, Paradigm, Standard, Old order, Focal occasion, Landmark occasion, resulting action,
Victim, Issue, Effect, State of affairs, Cause of shine, Qualification, Undesirable Event, Skill, Precept,
Outcome, Norm, Act, State of Affairs, Phenomenon 1, Phenomenon 2, Quality Eventuality, Expression,
Intended event, Cognate event, Epistemic stance, Goal conditions, Possession, Support Proposition,
Domain of Relevance, Charges, Idea, Initial subevent, Hypothetical event, Scene, Purpose of Goods,
Response action, Motivation, Executed, Affliction, Medication, Treatment, Stimulus, Last subevent,
Undesirable situation, Sleep state, Initial state, Enabled situation, Grinding cause, Finding, Case, Legal
Basis, Role of focal participant, Trigger, Authenticity, World state, Emotion, Emotional state, Evalua-
tion, New idea, Production, Performance, Undertaking, Destination event
¬cnp Artist, Performer, Duration, Time, Place, Distributor, Area, Path, Direction, Sub-region, Creator, Copy,
Original, Iteration, Manner, Frequency, Agent, Body part, Depictive, Theme, Subregion, Area, De-
gree, Angle, Fixed location, Path shape, Addressee, Entity, Individual 1, Individual 2, Road, Distance,
Speaker, Medium, Clothing, Wearer, Bodypart of agent, Locus, Cognizer, Salient entity, Name, Inspec-
tor, Ground, Unwanted entity, Location of inspector, Researcher, Population, Searcher, Sought entity,
Instrument, Created entity, Components, Forgoer, Desirable, Bad entity, Dodger, Experiencer, Vehicle,
Self mover, Speed, Cotheme, Consecutive, Re encoding, Supplier, Individuals, Driver, Complainer,
Communicator, Protagonist, Attribute, Final value, Item, Initial value, Difference, Group, Value range,
Co participant, Perceiver agentive, Target symbol, Location of perceiver, Location, Expected entity,
Focal participant, Time of Event, Variable, Limits, Limit1, Limit2, Point of contact, Goods, Lessee,
Lessor, Money, Rate, Unit, Reversive, Perceiver passive, Sound, Sound source, Location of source,
Fidelity, Official, Selector, Role, Concessive, New leader, Body, Old leader, Leader, Governed, Result
size, Size change, Dimension, Initial size, Elapsed time, Interval, Category, Criteria, Text, Final cor-
relate, Correlate, Initial correlate, Manipulator, Side 1, Sides, Side 2, Perpetrator, Value 1, Value 2,
Actor, Partner 2, Partner 1, Partners, Figure, Resident, Co resident, Student, Subject, Institution, Level,
Teacher, Undergoer, Subregion bodypart, Course, Owner, Defendant, Judge, Co abductee, Location
of appearance, Material, Accused, Arraign authority, Hair, Configuration, Emitter, Beam, Amount of
progress, Evaluee, Patient, Buyer, Seller, Recipient, Relay, Relative location, Connector, Items, Part
1, Part 2, Parts, Whole, Name source, Payer, Fine, Executioner, Interlocutor 1, Interlocutor 2, Inter-
locutors, Healer, Food, Cook, Container, Heating instrument, Temperature setting, Resource controller,
Resource, Donor, Constant location, Carrier, Sender, Co theme, Transport means, Holding location,
Rope, Knot, Handle, Containing object, Fastener, Enclosed region, Container portal, Aggregate, Sus-
pect, Authorities, Offense, Source of legal authority, Ingestor, Ingestibles, Sleeper, Pieces, Goal area,
Period of iterations, Mode of transportation, Produced food, Ingredients, Cognizer agent, Excreter,
Excreta, Air, Perceptual source, Escapee, Undesirable location, Evader, Capture, Pursuer, Amount of
discussion, Means of communication, Periodicity, Author, Honoree, Reader, Child, Mother, Father,
Egg, Flammables, Flame, Kindler, Mass theme, Address, Intermediary, Communication, Location of
communicator, Firearm, Indicated entity, Hearer, Sub region, Member, Object, Organization, Guardian,
New Status, Arguer, Criterion, Liquid, Impactors, Force, Coparticipant, Holding Location, Legal basis,
Precipitation, Quantity, Voice, Duration of endstate, Period of Iterations, Employer, Employee, Task,
Position, Compensation, Field, Place of employment, Amount of work, Contract basis, Recipients, Hot
Cold source, Temperature goal, Temperature change, Hot/Cold source, Dryee, Temperature, Traveler,
Iterations, Baggage, Deformer, Resistant surface, Fluid, Injured Party, Avenger, Injury, Punishment,
Offender, Grinder, Profiled item, Standard item, Profiled attribute, Standard attribute, Extent, Source
emitter, Emission, Sub source, Item 1, Item 2, Parameter, Form, Chosen, Change agent, Injuring entity,
Severity, Substance, Delivery device, Entry path, Wrong, Amends, Grounds, Expressor, Basis, Signs,
Manufacturer, Product, Factory, Consumer, Interested party, Performer1, Performer2, Whole patient,
Destroyer, Exporting area, Importing area, Accuracy, Time of Eventuality, Indicator, Indicated, Au-
dience, Valued entity, Journey, Duration of end state, Killer, Beneficiary, Destination time, Landmark
time, Seat of emotion, Arguers, Arguer1, Arguer2, Company, Asset, Origin, Sound maker, Static object,
Themes, Heat source, Following distance, Perceiver, Intended perceiver, Location of expressor, Path of
gaze, Relatives, Final temperature, Particular iteration, Participant 1, Language
</reference>
<tableCaption confidence="0.969997">
Table 3: This table presents the FrameNet labels we assign to cnp and ¬cnp classes.
</tableCaption>
<page confidence="0.998415">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.382861">
<title confidence="0.7490925">Recognizing Causality in Verb-Noun via Noun and Verb Semantics</title>
<author confidence="0.945759">Mehwish Riaz</author>
<author confidence="0.945759">Roxana</author>
<affiliation confidence="0.996629">Department of Computer Science and Beckman University of Illinois at</affiliation>
<address confidence="0.964926">Urbana, IL 61801,</address>
<abstract confidence="0.992173391304348">Several supervised approaches have been proposed for causality identification by relying on shallow linguistic features. However, such features do not lead to improved performance. Therefore, novel sources of knowledge are required to achieve progress on this problem. In this paper, we propose a model for the recognition of causality in verb-noun pairs by employing additional types of knowledge along with linguistic features. In particular, we focus on identifying and employing semantic classes of nouns and verbs with high tendency to encode cause or non-cause relations. Our model incorporates the information about these classes to minimize errors in predictions made by a basic supervised classifier relying merely on shallow linguistic features. As compared with this basic classifier our model achieves improvement in F-score (accuracy), respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In proceedings of the Association for Computational Linguistics and International Conference on Computational Linguistics (COLING-ACL).</booktitle>
<contexts>
<context position="9790" citStr="Baker et al., 1998" startWordPosition="1541" endWordPosition="1544">employs a supervised classifier relying on linguistic features to make binary predictions (i.e., does a verb-noun phrase pair encode a cause or non-cause relation?). We then incorporate additional types of knowledge on top of these binary predictions to improve performance. 3.1 Supervised Classifier In this section, we propose a basic supervised classifier to identify causation encoded by v-np pairs. To set up this supervised classifier, we need a training corpus of instances of v-np pairs encoding cause and non-cause relations. For this purpose, we employ the annotations of FrameNet project (Baker et al., 1998) provided for verbs. For example, consider the following annotation from FrameNet for the verb “dying” with argument “solvent abuse” where the pair “dyingsolvent abuse” encodes causality. A campaign has started to try to cut the rising number of children dying [cause from solvent abuse]. To generate a training corpus, we collect annotations of verbs from FrameNet s.t. the annotated element (aka. frame element) is a noun phrase. For example, we get a causal training instance of “dying-solvent abuse” pair from the above annotation. We assume that if a FrameNet’s annotated element contains a verb</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore and John B. Lowe. 1998. The Berkeley FrameNet project. In proceedings of the Association for Computational Linguistics and International Conference on Computational Linguistics (COLING-ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brandon Beamer</author>
<author>Roxana Girju</author>
</authors>
<title>Using a Bigram Event Model to Predict Causal Potential.</title>
<date>2009</date>
<booktitle>In proceedings of the Conference on Computational Linguistics and intelligent Text Processing (CICLING).</booktitle>
<contexts>
<context position="2138" citStr="Beamer and Girju, 2009" startWordPosition="318" endWordPosition="321">lations are encoded using (1) a verb-verb pair, (2) a noun-noun pair and (3) a verb-noun pair. 1. Five shoppers were killed when a car blew up at an outdoor market. 2. The attack on Kirkuk’s police intelligence complex sees further deaths after violence spilled over a nearby shopping mall. 3. At least 1,833 people died in hurricane. Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions. For example, various models have been proposed to identify causation between verbs (Bethard and Martin, 2008; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and between nouns (Girju and Moldovan, 2002; Girju, 2003). Do et al. (2011) have worked with verb-noun pairs for causality detection but they focused only on a small list of predefined nouns representing events. In this paper, we focus on the task of identifying causality encoded by verb-noun pairs (example 3). We propose a novel model which first predicts cause or non-cause relations using a supervised classifier and then incorporates additional types of knowledge to reduce errors in predictions. Using a supervised classifier, our</context>
<context position="6947" citStr="Beamer and Girju, 2009" startWordPosition="1087" endWordPosition="1090">cations (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model makes the decision of cause or non-cause relation based on unsupervised co-occurrence counts and then improves this decision by using minimal supervision from the causal and non-causal discourse markers (e.g., because, although, etc.). In search of novel and effec</context>
</contexts>
<marker>Beamer, Girju, 2009</marker>
<rawString>Brandon Beamer and Roxana Girju. 2009. Using a Bigram Event Model to Predict Causal Potential. In proceedings of the Conference on Computational Linguistics and intelligent Text Processing (CICLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>James H Martin</author>
</authors>
<title>Identification of Event Mentions and their Semantic Class.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="23806" citStr="Bethard and Martin (2006)" startWordPosition="3965" endWordPosition="3968">ass c„ (¬c„) contains the verbs with high (less) tendency to encode causation. Using above argument we claim that all verbs representing reporting events belong ¬c„ class and verbs representing rest of the types of events belong to c„ class. We build a supervised classifier which automatically classifies verbs into c„ and ¬c„ classes. We extract the instances of verbal events (i.e., verbs or verbal phrases) from TimeBank corpus and assign the labels c„ and ¬c„ to these instances. Using these labeled instances, we build a supervised classifier by adopting the same set features as introduced in Bethard and Martin (2006) to identify semantic classes of verbs. Due to space constraint, we refer the reader to Bethard and Martin (2006) for the details of features. Again we use Naive Bayes to take predictions of c„ and ¬c„ labels and their corresponding probabilities using equation 4. We incorporate the knowledge of semantics of verbs in our model by making the following additions to the integer linear program. �Z3 = Z2 + v:v-np∈I E x3(v, l) = 1 ` dv : v-np E I (10) l∈L3 x3(v, l) E {0, 1} ` dv : v-np E I `dl E L3 (11) x1(v-np, -c) − x3(v, -c„) &gt;= 0 (12) `d v : v-np, `d v-np E I x3(v, c„) − x1(v-np, c) &gt;= 0 (13) `d</context>
</contexts>
<marker>Bethard, Martin, 2006</marker>
<rawString>Steven Bethard and James H. Martin. 2006. Identification of Event Mentions and their Semantic Class. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>James H Martin</author>
</authors>
<title>Learning Semantic Links from a Corpus of Parallel Temporal and Causal Relations.</title>
<date>2008</date>
<booktitle>In proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="2114" citStr="Bethard and Martin, 2008" startWordPosition="314" endWordPosition="317">g examples where causal relations are encoded using (1) a verb-verb pair, (2) a noun-noun pair and (3) a verb-noun pair. 1. Five shoppers were killed when a car blew up at an outdoor market. 2. The attack on Kirkuk’s police intelligence complex sees further deaths after violence spilled over a nearby shopping mall. 3. At least 1,833 people died in hurricane. Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions. For example, various models have been proposed to identify causation between verbs (Bethard and Martin, 2008; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and between nouns (Girju and Moldovan, 2002; Girju, 2003). Do et al. (2011) have worked with verb-noun pairs for causality detection but they focused only on a small list of predefined nouns representing events. In this paper, we focus on the task of identifying causality encoded by verb-noun pairs (example 3). We propose a novel model which first predicts cause or non-cause relations using a supervised classifier and then incorporates additional types of knowledge to reduce errors in predictions. Using a su</context>
<context position="6682" citStr="Bethard and Martin, 2008" startWordPosition="1048" endWordPosition="1051">ion 5 of the paper concludes our current research. 2 Related Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model</context>
</contexts>
<marker>Bethard, Martin, 2008</marker>
<rawString>Steven Bethard and James H. Martin. 2008. Learning Semantic Links from a Corpus of Parallel Temporal and Causal Relations. In proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Du-Seong Chang</author>
<author>Key-Sun Choi</author>
</authors>
<title>Incremental cue phrase learning and bootstrapping method for causality extraction using cue phrase and word pair probabilities.</title>
<date>2006</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>42</volume>
<pages>662678</pages>
<contexts>
<context position="1467" citStr="Chang and Choi, 2006" startWordPosition="212" endWordPosition="215">rporates the information about these classes to minimize errors in predictions made by a basic supervised classifier relying merely on shallow linguistic features. As compared with this basic classifier our model achieves 14.74% (29.57%) improvement in F-score (accuracy), respectively. 1 Introduction The automatic detection of causal relations is important for various natural language processing applications such as question answering, text summarization, text understanding and event prediction. Causality can be expressed using various natural language constructions (Girju and Moldovan, 2002; Chang and Choi, 2006). Consider the following examples where causal relations are encoded using (1) a verb-verb pair, (2) a noun-noun pair and (3) a verb-noun pair. 1. Five shoppers were killed when a car blew up at an outdoor market. 2. The attack on Kirkuk’s police intelligence complex sees further deaths after violence spilled over a nearby shopping mall. 3. At least 1,833 people died in hurricane. Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions. For example, various models have been proposed to identify ca</context>
</contexts>
<marker>Chang, Choi, 2006</marker>
<rawString>Du-Seong Chang and Key-Sun Choi. 2006. Incremental cue phrase learning and bootstrapping method for causality extraction using cue phrase and word pair probabilities. Information Processing and Management, volume 42 issue 3, 662678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations.</title>
<date>2004</date>
<booktitle>In proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="6373" citStr="Chklovski and Pantel, 2004" startWordPosition="1004" endWordPosition="1007">t al., 2011) to combine all the above types of knowledge for the current task. This paper is organized as follows. In next section, we briefly review the previous research done for identifying causality. We introduce our model and evaluation with discussion on results in section 3 and 4, respectively. The section 5 of the paper concludes our current research. 2 Related Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the Web for Fine-Grained Semantic Verb Relations. In proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quang X Do</author>
<author>Yee S Chen</author>
<author>Dan Roth</author>
</authors>
<title>Minimally Supervised Event Causality Identication.</title>
<date>2011</date>
<booktitle>In proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="2177" citStr="Do et al., 2011" startWordPosition="326" endWordPosition="329">, (2) a noun-noun pair and (3) a verb-noun pair. 1. Five shoppers were killed when a car blew up at an outdoor market. 2. The attack on Kirkuk’s police intelligence complex sees further deaths after violence spilled over a nearby shopping mall. 3. At least 1,833 people died in hurricane. Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions. For example, various models have been proposed to identify causation between verbs (Bethard and Martin, 2008; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and between nouns (Girju and Moldovan, 2002; Girju, 2003). Do et al. (2011) have worked with verb-noun pairs for causality detection but they focused only on a small list of predefined nouns representing events. In this paper, we focus on the task of identifying causality encoded by verb-noun pairs (example 3). We propose a novel model which first predicts cause or non-cause relations using a supervised classifier and then incorporates additional types of knowledge to reduce errors in predictions. Using a supervised classifier, our model identifies causation by employin</context>
<context position="5759" citStr="Do et al., 2011" startWordPosition="905" endWordPosition="908">e 3 above, the noun “hurricane” is cause and the verb “died” is its effect. However, a verb-noun pair may not encode causality when a verb and a noun represent same event. For example, in instance “Colin Powell presented further evidence in his presentation.”, the verb “presented” and the noun “presentation” represent same event of “presenting” and thus encoding non-cause relation with each other. In our model, we determine the verb-noun pairs representing same or distinct events to make predictions accordingly. • We adopt the framework of Integer Linear Programming (ILP) (Roth and Yih, 2004; Do et al., 2011) to combine all the above types of knowledge for the current task. This paper is organized as follows. In next section, we briefly review the previous research done for identifying causality. We introduce our model and evaluation with discussion on results in section 3 and 4, respectively. The section 5 of the paper concludes our current research. 2 Related Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski an</context>
<context position="6986" citStr="Do et al., 2011" startWordPosition="1095" endWordPosition="1098">04; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model makes the decision of cause or non-cause relation based on unsupervised co-occurrence counts and then improves this decision by using minimal supervision from the causal and non-causal discourse markers (e.g., because, although, etc.). In search of novel and effective types of knowledge to identify cau</context>
</contexts>
<marker>Do, Chen, Roth, 2011</marker>
<rawString>Quang X. Do, Yee S. Chen and Dan Roth. 2011. Minimally Supervised Event Causality Identication. In proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny R Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="15634" citStr="Finkel et al., 2005" startWordPosition="2559" endWordPosition="2562">se represents a named entity then it can have least tendency to encode causal relations unless there is a metonymic reading associated with it. For example, consider the following cause and non-cause examples where noun phrase is a named entity. 4. Sandy hit Cuba as a Category 3 hurricane. 5. Almost all the weapon sites in Iraq were destroyed by the United States. In example 4, Cuba is location and does not encode causality. However, in example 5 the pair “destroyed-the United States” encode causality where a metonymic reading is associated with the location. We apply Named Entity Recognizer (Finkel et al., 2005) and assume if a noun phrase is identified as a named entity then its corresponding verb-noun phrase pair encodes noncause relation. This constraint can lead to a false negative prediction when the metonymic reading is associated with a noun phrase. In order to avoid as much false negatives as possible, we imply the following simple rule i.e., if one of the following cue words appear between a verb and a noun phrase then do not apply the constraint stated above. by, from, because of, through, for In our experiments, the above simple rule helps avoiding some false negatives but in future any su</context>
<context position="21259" citStr="Finkel et al., 2005" startWordPosition="3562" endWordPosition="3565">o the constraints introduced above. We predict the semantic class of a noun phrase using the supervised classifier for cnp and ¬cnp classes and set the probabilities i.e., P(np, l) = 1, P(np, {L2} − {l}) = 0 if the label l E L2 is assigned to np. Again we use Naive Bayes to predict the labels for noun phrases. Also before running this supervised classifier, we run the named entity recognizer and assign ¬cnp labels to all noun phrases identified as named entities. For our model, we apply named entity recognizer for seven classes i.e., LOCATION, PERSON, ORGANIZATION, DATE, TIME, MONEY, PERCENT (Finkel et al., 2005). 3.3 Knowledge of Semantic classes of verbs In this section, we introduce our method to incorporate the knowledge of semantic classes of verbs to identify causation. Verbs are the components of language for expressing events of various types. In TimeBank corpus, Pustejovsky et al. (2003) have introduced eight semantic classes of events i.e., OCCURRENCE, PERCEPTION, REPORTING, ASPECTUAL, STATE, I STATE, I ACTION and MODAL. According to the definitions of these classes provided by Pustejovsky x2(np, l)P(np, l) (5) 52 Semantic FrameNet Labels WordNet Senses Class {act, deed, human action, human </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny R. Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<title>Automatic detection of causal relations for Question Answering. Association for Computational Linguistics</title>
<date>2003</date>
<booktitle>ACL, Workshop on Multilingual Summarization and Question Answering Machine Learning and Beyond.</booktitle>
<contexts>
<context position="2258" citStr="Girju, 2003" startWordPosition="341" endWordPosition="342"> car blew up at an outdoor market. 2. The attack on Kirkuk’s police intelligence complex sees further deaths after violence spilled over a nearby shopping mall. 3. At least 1,833 people died in hurricane. Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions. For example, various models have been proposed to identify causation between verbs (Bethard and Martin, 2008; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and between nouns (Girju and Moldovan, 2002; Girju, 2003). Do et al. (2011) have worked with verb-noun pairs for causality detection but they focused only on a small list of predefined nouns representing events. In this paper, we focus on the task of identifying causality encoded by verb-noun pairs (example 3). We propose a novel model which first predicts cause or non-cause relations using a supervised classifier and then incorporates additional types of knowledge to reduce errors in predictions. Using a supervised classifier, our model identifies causation by employing shallow linguistic features (e.g., lemmas of verb and noun, words between verb </context>
<context position="6345" citStr="Girju, 2003" startWordPosition="1002" endWordPosition="1003">h, 2004; Do et al., 2011) to combine all the above types of knowledge for the current task. This paper is organized as follows. In next section, we briefly review the previous research done for identifying causality. We introduce our model and evaluation with discussion on results in section 3 and 4, respectively. The section 5 of the paper concludes our current research. 2 Related Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 20</context>
<context position="12121" citStr="Girju (2003)" startWordPosition="1925" endWordPosition="1926">umber, our supervised model tends to assign non-cause labels to almost all instances. Therefore, we employ equal number of cause and non-cause instances for training. In future, we plan to extract more annotations from the FrameNet and employ more than one human annotators to assign the labels of cause and noncause relations to the full inventory of labels of FrameNet. • Lexical Features: verb, lemma of verb, noun phrase, lemma of all words of noun phrase, head noun of noun phrase, lemmas of all words between verb and head noun of noun phrase. • Semantic Features: We adopted this feature from Girju (2003) to capture the semantics of nouns. The 9 noun hierarchies of WordNet i.e., entity, psychological feature, abstraction, state, event, act, group, possession, phenomenon are used as this feature. Each of these hierarchies is set to 1 if any sense of the head noun of noun phrase lies in that hierarchy otherwise set to 0. • Structural Features: This feature is applied by considering both subject (i.e., sub in np) and object (i.e., obj in np) of a verb. For example, for a v-np pair the variable sub in np is set to 1 if the subject of v is contained in np, set to 0 if the subject of v is not contai</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>Roxana Girju. 2003. Automatic detection of causal relations for Question Answering. Association for Computational Linguistics ACL, Workshop on Multilingual Summarization and Question Answering Machine Learning and Beyond.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
</authors>
<title>Mining Answers for Causation Questions.</title>
<date>2002</date>
<booktitle>In American Associations of Artificial Intelligence (AAAI),</booktitle>
<note>Symposium.</note>
<contexts>
<context position="1444" citStr="Girju and Moldovan, 2002" startWordPosition="208" endWordPosition="211"> relations. Our model incorporates the information about these classes to minimize errors in predictions made by a basic supervised classifier relying merely on shallow linguistic features. As compared with this basic classifier our model achieves 14.74% (29.57%) improvement in F-score (accuracy), respectively. 1 Introduction The automatic detection of causal relations is important for various natural language processing applications such as question answering, text summarization, text understanding and event prediction. Causality can be expressed using various natural language constructions (Girju and Moldovan, 2002; Chang and Choi, 2006). Consider the following examples where causal relations are encoded using (1) a verb-verb pair, (2) a noun-noun pair and (3) a verb-noun pair. 1. Five shoppers were killed when a car blew up at an outdoor market. 2. The attack on Kirkuk’s police intelligence complex sees further deaths after violence spilled over a nearby shopping mall. 3. At least 1,833 people died in hurricane. Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions. For example, various models have been </context>
<context position="14423" citStr="Girju and Moldovan (2002)" startWordPosition="2363" endWordPosition="2366"> labels (i.e., P(v-np,l))1. These probabilities can be obtained by running a supervised classification algorithm (e.g., Naive Bayes and Maximum Entropy). In our experiments, we provide results using the following probabilities acquired with Naive Bayes. �n k=1 lo9P(fk |c) P(v-np, c) = 1.0 ��n �lE(c,¬c) lo9P(fk |l) k=1 P(v-np, -c) = 1.0 - P((v, np), c) (4) where fk is a feature, n is total number of features and P(fk |l) is the smoothed probability of a feature fk given the training instances of label l. 3.2 Knowledge of Semantic classes of nouns Philospher Jaegwon Kim (Kim, 1993) (as cited by Girju and Moldovan (2002)) pointed out that the entities which represent either causes or effects are often events, but also conditions, states, phenomena, processes, and sometimes even facts. Therefore, according to this our model should have knowledge of the semantic classes of noun phrases with high tendency to encode cause or non-cause relations. Considering this type of knowledge, we can automatically review and correct the wrong predictions made by our basic supervised classifier. 1We use the integer linear program solver available at http://sourceforge.net/projects/lpsolve/ We argue that if a noun phrase repres</context>
<context position="18602" citStr="Girju and Moldovan (2002)" startWordPosition="3062" endWordPosition="3065">noun phrase, lemmas of all words of noun phrase, head word of noun phrase, first two (three) (four) letters of head noun of noun phrase, last two, (three) (four) letters of head noun of noun phrase. • Word Class Features: part-of-speech tags of all words of noun phrase, part-of-speech tag of head noun of noun phrase. • Semantic Features: all (frequent) sense(s) of head noun of noun phrase. We have acquired 23,334 (81,279) training instances of cnp (¬cnp) class, respectively for this work. We also use WordNet to obtain more training instances of these classes. We follow the approach similar to Girju and Moldovan (2002) and adopt some senses of WordNet (shown in table 1) to acquire training instances of noun phrases. For example, considering the table 1, we assign ¬cnp label to any noun whose all senses in WordNet lie in the semantic hierarchy originated by the sense {time period, period of time, period}. Following this scheme, we extract instances of nouns and noun phrases from English GigaWord corpus and assign the labels cnp and ¬cnp to them by employing WordNet senses given in table 1. Girju and Moldovan (2002) have used similar scheme to rank noun phrases according to their tendency to encode causation.</context>
</contexts>
<marker>Girju, Moldovan, 2002</marker>
<rawString>Roxana Girju and Dan Moldovan. 2002. Mining Answers for Causation Questions. In American Associations of Artificial Intelligence (AAAI), 2002 Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaegwon Kim</author>
</authors>
<title>Causes and Events. Mackie on Causation.</title>
<date>1993</date>
<booktitle>In Cansation, Oxford Readings in Philosophy,</booktitle>
<editor>ed. Ernest Sosa, and Michael Tooley,</editor>
<publisher>University Press.</publisher>
<location>Oxford</location>
<contexts>
<context position="14384" citStr="Kim, 1993" startWordPosition="2358" endWordPosition="2359">ilities of assignment of labels (i.e., P(v-np,l))1. These probabilities can be obtained by running a supervised classification algorithm (e.g., Naive Bayes and Maximum Entropy). In our experiments, we provide results using the following probabilities acquired with Naive Bayes. �n k=1 lo9P(fk |c) P(v-np, c) = 1.0 ��n �lE(c,¬c) lo9P(fk |l) k=1 P(v-np, -c) = 1.0 - P((v, np), c) (4) where fk is a feature, n is total number of features and P(fk |l) is the smoothed probability of a feature fk given the training instances of label l. 3.2 Knowledge of Semantic classes of nouns Philospher Jaegwon Kim (Kim, 1993) (as cited by Girju and Moldovan (2002)) pointed out that the entities which represent either causes or effects are often events, but also conditions, states, phenomena, processes, and sometimes even facts. Therefore, according to this our model should have knowledge of the semantic classes of noun phrases with high tendency to encode cause or non-cause relations. Considering this type of knowledge, we can automatically review and correct the wrong predictions made by our basic supervised classifier. 1We use the integer linear program solver available at http://sourceforge.net/projects/lpsolve</context>
</contexts>
<marker>Kim, 1993</marker>
<rawString>Jaegwon Kim. 1993. Causes and Events. Mackie on Causation. In Cansation, Oxford Readings in Philosophy, ed. Ernest Sosa, and Michael Tooley, Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catherine Macleod</author>
<author>Ralph Grishman</author>
<author>Adam Meyers</author>
<author>Leslie Barrett</author>
<author>Ruth Reeves</author>
</authors>
<title>NOMLEX: A Lexicon of Nominalizations.</title>
<date>1998</date>
<booktitle>In proceedings of EURALEX.</booktitle>
<marker>Macleod, Grishman, Meyers, Barrett, Reeves, 1998</marker>
<rawString>Catherine Macleod, Ralph Grishman, Adam Meyers, Leslie Barrett, Ruth Reeves. 1998. NOMLEX: A Lexicon of Nominalizations. In proceedings of EURALEX.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Katja Markert</author>
</authors>
<title>Malvina Nissim 2009. Data and models for metonymy resolution.</title>
<journal>Language Resources and Evaluation</journal>
<volume>43</volume>
<pages>123--138</pages>
<marker>Markert, </marker>
<rawString>Katja Markert, Malvina Nissim 2009. Data and models for metonymy resolution. Language Resources and Evaluation Volume 43 Issue 2, Pages 123−138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC).</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>Automatic Sense Prediction for Implicit Discourse Relations in Text.</title>
<date>2009</date>
<booktitle>In proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="6762" citStr="Pitler et al., 2009" startWordPosition="1060" endWordPosition="1063">inguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model makes the decision of cause or non-cause relation based on unsupervised co-occu</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>Emily Pitler, Annie Louis and Ani Nenkova. 2009. Automatic Sense Prediction for Implicit Discourse Relations in Text. In proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Using Syntax to Disambiguate Explicit Discourse Connectives in Text.</title>
<date>2009</date>
<booktitle>In proceedings of ACL-IJCNLP.</booktitle>
<contexts>
<context position="6740" citStr="Pitler and Nenkova, 2009" startWordPosition="1056" endWordPosition="1059">ed Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model makes the decision of cause or non-cause relation based o</context>
</contexts>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>Emily Pitler and Ani Nenkova. 2009. Using Syntax to Disambiguate Explicit Discourse Connectives in Text. In proceedings of ACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Saur</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
</authors>
<title>The TIMEBANK Corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of Corpus Linguistics.</booktitle>
<institution>Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro and</institution>
<contexts>
<context position="4573" citStr="Pustejovsky et al. (2003)" startWordPosition="706" endWordPosition="709">gent predictions, it is important for our model to have knowledge about the semantic classes of nouns with high tendency to encode causal or non-causal relations. For example, a named entity such as person, organization or location may have high tendency to encode non-causality unless a metonymic reading is associated with it. In our approach, we identify such semantic classes of nouns by exploiting a named entity recognizer, the annotations of frame elements provided in FrameNet and WordNet. • Verbs are the important components of language for expressing events of various types. For example, Pustejovsky et al. (2003) have classified events into eight semantic classes: OCCURRENCE, PERCEPTION, REPORTING, ASPECTUAL, STATE, I STATE, I ACTION and MODAL. We argue that there are some semantic classes in this list with high tendency to encode cause or non-cause relations. For example, reporting events represented by verbs say, tell, etc., have high tendency to just report other events instead of encoding causality with them. In our model, we use such information to reduce errors in predictions. • Each causal relation is characterized by two roles i.e., cause and its effect. In example 3 above, the noun “hurricane</context>
<context position="21548" citStr="Pustejovsky et al. (2003)" startWordPosition="3610" endWordPosition="3613">he labels for noun phrases. Also before running this supervised classifier, we run the named entity recognizer and assign ¬cnp labels to all noun phrases identified as named entities. For our model, we apply named entity recognizer for seven classes i.e., LOCATION, PERSON, ORGANIZATION, DATE, TIME, MONEY, PERCENT (Finkel et al., 2005). 3.3 Knowledge of Semantic classes of verbs In this section, we introduce our method to incorporate the knowledge of semantic classes of verbs to identify causation. Verbs are the components of language for expressing events of various types. In TimeBank corpus, Pustejovsky et al. (2003) have introduced eight semantic classes of events i.e., OCCURRENCE, PERCEPTION, REPORTING, ASPECTUAL, STATE, I STATE, I ACTION and MODAL. According to the definitions of these classes provided by Pustejovsky x2(np, l)P(np, l) (5) 52 Semantic FrameNet Labels WordNet Senses Class {act, deed, human action, human activity}, {phenomenon}, {state}, {psychological feature}, {event}, {causal agent, cause, causalagency} {time period, period of time, period}, {measure, quantity, amount}, {group, grouping}, {organization, organisation}, {time unit, unit of time}, {clock time, time} cnp Event, Goal, Purpo</context>
</contexts>
<marker>Pustejovsky, Hanks, Saur, See, Gaizauskas, Setzer, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Saur, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro and Marcia Lazo. 2003. The TIMEBANK Corpus. In Proceedings of Corpus Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eric Horvitz</author>
</authors>
<title>Mining the Web to Predict Future Events.</title>
<date>2013</date>
<booktitle>In proceedings of sixth ACM international conference on Web search and data mining,</booktitle>
<location>(WSDM).</location>
<contexts>
<context position="6402" citStr="Radinsky and Horvitz, 2013" startWordPosition="1008" endWordPosition="1011">the above types of knowledge for the current task. This paper is organized as follows. In next section, we briefly review the previous research done for identifying causality. We introduce our model and evaluation with discussion on results in section 3 and 4, respectively. The section 5 of the paper concludes our current research. 2 Related Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju</context>
</contexts>
<marker>Radinsky, Horvitz, 2013</marker>
<rawString>Kira Radinsky and Eric Horvitz. 2013. Mining the Web to Predict Future Events. In proceedings of sixth ACM international conference on Web search and data mining, (WSDM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehwish Riaz</author>
<author>Roxana Girju</author>
</authors>
<title>Another Look at Causality: Discovering Scenario-Specific Contingency Relationships with No Supervision.</title>
<date>2010</date>
<booktitle>In proceedings of the IEEE 4th International Conference on Semantic Computing (ICSC).</booktitle>
<contexts>
<context position="2160" citStr="Riaz and Girju, 2010" startWordPosition="322" endWordPosition="325">g (1) a verb-verb pair, (2) a noun-noun pair and (3) a verb-noun pair. 1. Five shoppers were killed when a car blew up at an outdoor market. 2. The attack on Kirkuk’s police intelligence complex sees further deaths after violence spilled over a nearby shopping mall. 3. At least 1,833 people died in hurricane. Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions. For example, various models have been proposed to identify causation between verbs (Bethard and Martin, 2008; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and between nouns (Girju and Moldovan, 2002; Girju, 2003). Do et al. (2011) have worked with verb-noun pairs for causality detection but they focused only on a small list of predefined nouns representing events. In this paper, we focus on the task of identifying causality encoded by verb-noun pairs (example 3). We propose a novel model which first predicts cause or non-cause relations using a supervised classifier and then incorporates additional types of knowledge to reduce errors in predictions. Using a supervised classifier, our model identifies caus</context>
<context position="6969" citStr="Riaz and Girju, 2010" startWordPosition="1091" endWordPosition="1094">klovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model makes the decision of cause or non-cause relation based on unsupervised co-occurrence counts and then improves this decision by using minimal supervision from the causal and non-causal discourse markers (e.g., because, although, etc.). In search of novel and effective types of knowledg</context>
<context position="28556" citStr="Riaz and Girju (2010)" startWordPosition="4840" endWordPosition="4843"> (Marneffe et al., 2006) on these sentences. Using each sentence, we extracted all verb-noun phrase pairs where the verb has a dependency relation with any word of noun phrase. We manually inspected all of the extracted instances and removed those instances in which a word had been wrongly classified as a verb by the part-of-speech tagger. There are total 1106 instances in our test set. We assigned the task of annotation of these instances with cause and non-cause relations to a human annotator. Using manipulation theory of causality (Woodward, 2008), we adopted the annotation guidelines from Riaz and Girju (2010) which is as follows: “Assign cause label to a pair (a, b), if the following two conditions are satisfied: (1), a temporally precedes/overlap b in time, (2) while keeping as many state of affairs constant as possible, modifying a must entail predictably modifying b. Otherwise assign non-cause label. ” We have 149 (957) cause (non-cause) instances in our test set3, respectively. We evaluate the performance of our model using F-score and accuracy evaluation measures (see table 2 for results). The results in table 2 reveal that the basic supervised classifier is a naive model and achieves only 27</context>
</contexts>
<marker>Riaz, Girju, 2010</marker>
<rawString>Mehwish Riaz and Roxana Girju. 2010. Another Look at Causality: Discovering Scenario-Specific Contingency Relationships with No Supervision. In proceedings of the IEEE 4th International Conference on Semantic Computing (ICSC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehwish Riaz</author>
<author>Roxana Girju</author>
</authors>
<title>Toward a Better Understanding of Causality between Verbal Events: Extraction and Analysis of the Causal Power of Verb-Verb Associations.</title>
<date>2013</date>
<booktitle>Proceedings of the annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL).</booktitle>
<contexts>
<context position="2200" citStr="Riaz and Girju, 2013" startWordPosition="330" endWordPosition="333"> pair and (3) a verb-noun pair. 1. Five shoppers were killed when a car blew up at an outdoor market. 2. The attack on Kirkuk’s police intelligence complex sees further deaths after violence spilled over a nearby shopping mall. 3. At least 1,833 people died in hurricane. Since, the task of automatic recognition of causality is quite challenging, researchers have addressed this problem by considering specific constructions. For example, various models have been proposed to identify causation between verbs (Bethard and Martin, 2008; Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013) and between nouns (Girju and Moldovan, 2002; Girju, 2003). Do et al. (2011) have worked with verb-noun pairs for causality detection but they focused only on a small list of predefined nouns representing events. In this paper, we focus on the task of identifying causality encoded by verb-noun pairs (example 3). We propose a novel model which first predicts cause or non-cause relations using a supervised classifier and then incorporates additional types of knowledge to reduce errors in predictions. Using a supervised classifier, our model identifies causation by employing shallow linguistic fe</context>
<context position="7009" citStr="Riaz and Girju, 2013" startWordPosition="1099" endWordPosition="1102">Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model makes the decision of cause or non-cause relation based on unsupervised co-occurrence counts and then improves this decision by using minimal supervision from the causal and non-causal discourse markers (e.g., because, although, etc.). In search of novel and effective types of knowledge to identify causation between two verb</context>
</contexts>
<marker>Riaz, Girju, 2013</marker>
<rawString>Mehwish Riaz and Roxana Girju 2013. Toward a Better Understanding of Causality between Verbal Events: Extraction and Analysis of the Causal Power of Verb-Verb Associations. Proceedings of the annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>A Linear Programming Formulation for Global Inference in Natural Language Tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of the Annual Conference on Computational Natural Language Learning (CoNLL).</booktitle>
<contexts>
<context position="5741" citStr="Roth and Yih, 2004" startWordPosition="901" endWordPosition="904">ts effect. In example 3 above, the noun “hurricane” is cause and the verb “died” is its effect. However, a verb-noun pair may not encode causality when a verb and a noun represent same event. For example, in instance “Colin Powell presented further evidence in his presentation.”, the verb “presented” and the noun “presentation” represent same event of “presenting” and thus encoding non-cause relation with each other. In our model, we determine the verb-noun pairs representing same or distinct events to make predictions accordingly. • We adopt the framework of Integer Linear Programming (ILP) (Roth and Yih, 2004; Do et al., 2011) to combine all the above types of knowledge for the current task. This paper is organized as follows. In next section, we briefly review the previous research done for identifying causality. We introduce our model and evaluation with discussion on results in section 3 and 4, respectively. The section 5 of the paper concludes our current research. 2 Related Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, </context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Dan Roth and Wen-tau Yih 2004. A Linear Programming Formulation for Global Inference in Natural Language Tasks. In Proceedings of the Annual Conference on Computational Natural Language Learning (CoNLL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Sporleder</author>
<author>Alex Lascarides</author>
</authors>
<title>Using automatically labelled examples to classify rhetorical relations: An assessment.</title>
<date>2008</date>
<journal>Journal of Natural Language Engineering Volume</journal>
<volume>14</volume>
<contexts>
<context position="6714" citStr="Sporleder and Lascarides, 2008" startWordPosition="1052" endWordPosition="1055">es our current research. 2 Related Work In computational linguistics, researchers have always shown interest in the task of automatic recognition of causal relations because success on this task is critical for various natural language applications (Girju, 2003; Chklovski and Pantel, 2004; Radinsky and Horvitz, 2013). Following the successful employment of linguistic features for various tasks (e.g., part-ofspeech tagging, named entity recognition, etc.), initially NLP researchers proposed approaches relying mainly on such features to identify causality (Girju, 2003; Bethard and Martin, 2008; Sporleder and Lascarides, 2008; Pitler and Nenkova, 2009; Pitler et al., 2009). However, researchers have recently shifted their attention from these features and tried to consider other sources of knowledge for extracting causal relations (Beamer and Girju, 2009; Riaz and Girju, 2010; Do et al., 2011; Riaz and Girju, 2013). For example, Riaz and Girju (2010) and Do et al. (2011) have proposed unsupervised metrics for learning causal dependencies between two events. Do et al. (2011) have also incorporated minimal supervision with unsupervised metrics. For a pair of events (a, b), their model makes the decision of cause or </context>
</contexts>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>Caroline Sporleder and Alex Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: An assessment. Journal of Natural Language Engineering Volume 14 Issue 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of Human Language Technology and North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</booktitle>
<contexts>
<context position="27913" citStr="Toutanova et al., 2003" startWordPosition="4735" endWordPosition="4738">to evaluate our model, we generated a test set with instances of form verb-noun phrase where the verb is grammatically connected to the noun phrase in an instance. For this purpose, we 2Following Riaz ang Girju (2010), we assume that the subject and object of a verb are parts of an event represented by a verb. Therefore, we use these arguments along with a verb for lexical matching with a noun phrase. collected three wiki articles on the topics of Hurricane Katrina, Iraq War and Egyptian Revolution of 2011. We selected first 100 sentences from these articles and applied part-of-speech tagger (Toutanova et al., 2003) and dependency parser (Marneffe et al., 2006) on these sentences. Using each sentence, we extracted all verb-noun phrase pairs where the verb has a dependency relation with any word of noun phrase. We manually inspected all of the extracted instances and removed those instances in which a word had been wrongly classified as a verb by the part-of-speech tagger. There are total 1106 instances in our test set. We assigned the task of annotation of these instances with cause and non-cause relations to a human annotator. Using manipulation theory of causality (Woodward, 2008), we adopted the annot</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network. In Proceedings of Human Language Technology and North American Chapter of the Association for Computational Linguistics (HLT-NAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Woodward</author>
</authors>
<title>Causation and Manipulation. Online Encyclopedia of Philosophy.</title>
<date>2008</date>
<contexts>
<context position="28491" citStr="Woodward, 2008" startWordPosition="4832" endWordPosition="4833">peech tagger (Toutanova et al., 2003) and dependency parser (Marneffe et al., 2006) on these sentences. Using each sentence, we extracted all verb-noun phrase pairs where the verb has a dependency relation with any word of noun phrase. We manually inspected all of the extracted instances and removed those instances in which a word had been wrongly classified as a verb by the part-of-speech tagger. There are total 1106 instances in our test set. We assigned the task of annotation of these instances with cause and non-cause relations to a human annotator. Using manipulation theory of causality (Woodward, 2008), we adopted the annotation guidelines from Riaz and Girju (2010) which is as follows: “Assign cause label to a pair (a, b), if the following two conditions are satisfied: (1), a temporally precedes/overlap b in time, (2) while keeping as many state of affairs constant as possible, modifying a must entail predictably modifying b. Otherwise assign non-cause label. ” We have 149 (957) cause (non-cause) instances in our test set3, respectively. We evaluate the performance of our model using F-score and accuracy evaluation measures (see table 2 for results). The results in table 2 reveal that the </context>
</contexts>
<marker>Woodward, 2008</marker>
<rawString>James Woodward. 2008. Causation and Manipulation. Online Encyclopedia of Philosophy.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Semantic Class FrameNet Labels cnp Event</author>
<author>Purpose Goal</author>
<author>Internal cause Cause</author>
<author>External cause</author>
<author>Means Result</author>
<author>Phenomena Reason</author>
<author>Char- acterization</author>
<author>Coordinated event</author>
<author>Final state</author>
<author>Topic Information</author>
</authors>
<title>Containing event, Mental content, Ac- tion, Experience, Impactee, Impactor, Message, Question, Circumstances, Desired goal, Explanation, Required situation, Complaint, Content, Activity, Intended goal, Phenomenon, State, Dependent state, Forgery, Purpose of Event, Negative consequences, Inference, Appraisal, Noisy event, Function, Evi- dence, Process, Paradigm, Standard, Old order, Focal occasion, Landmark occasion, resulting action, Victim, Issue, Effect, State of affairs, Cause of shine, Qualification, Undesirable Event, Skill, Precept, Outcome, Norm, Act, State of Affairs, Phenomenon 1, Phenomenon 2, Quality Eventuality, Expression, Intended event, Cognate event, Epistemic stance, Goal conditions, Possession, Support Proposition, Domain of Relevance, Charges, Idea, Initial subevent, Hypothetical event, Scene, Purpose of Goods, Response action, Motivation, Executed, Affliction, Medication, Treatment, Stimulus, Last subevent, Undesirable situation, Sleep state, Initial state, Enabled situation, Grinding cause, Finding, Case, Legal Basis, Role of focal participant, Trigger, Authenticity, World state, Emotion, Emotional state, Evalua- tion, New idea, Production, Performance, Undertaking, Destination event ¬cnp Artist, Performer,</title>
<journal>abductee, Location of appearance, Material, Accused, Arraign authority, Hair, Configuration, Emitter, Beam, Amount of progress, Evaluee, Patient, Buyer, Seller, Recipient, Relay, Relative location, Connector, Items, Part</journal>
<booktitle>location, Path shape, Addressee, Entity, Individual 1, Individual 2,</booktitle>
<volume>1</volume>
<location>Duration, Time, Place, Distributor, Area, Path, Direction, Sub-region, Creator, Copy, Original, Iteration, Manner, Frequency, Agent, Body part, Depictive, Theme, Subregion, Area, De- gree, Angle, Fixed</location>
<marker>Event, Goal, Cause, cause, Result, Reason, acterization, event, state, Information, </marker>
<rawString>Semantic Class FrameNet Labels cnp Event, Goal, Purpose, Cause, Internal cause, External cause, Result, Means, Reason, Phenomena, Char- acterization, Coordinated event, Final state, Information, Topic, Containing event, Mental content, Ac- tion, Experience, Impactee, Impactor, Message, Question, Circumstances, Desired goal, Explanation, Required situation, Complaint, Content, Activity, Intended goal, Phenomenon, State, Dependent state, Forgery, Purpose of Event, Negative consequences, Inference, Appraisal, Noisy event, Function, Evi- dence, Process, Paradigm, Standard, Old order, Focal occasion, Landmark occasion, resulting action, Victim, Issue, Effect, State of affairs, Cause of shine, Qualification, Undesirable Event, Skill, Precept, Outcome, Norm, Act, State of Affairs, Phenomenon 1, Phenomenon 2, Quality Eventuality, Expression, Intended event, Cognate event, Epistemic stance, Goal conditions, Possession, Support Proposition, Domain of Relevance, Charges, Idea, Initial subevent, Hypothetical event, Scene, Purpose of Goods, Response action, Motivation, Executed, Affliction, Medication, Treatment, Stimulus, Last subevent, Undesirable situation, Sleep state, Initial state, Enabled situation, Grinding cause, Finding, Case, Legal Basis, Role of focal participant, Trigger, Authenticity, World state, Emotion, Emotional state, Evalua- tion, New idea, Production, Performance, Undertaking, Destination event ¬cnp Artist, Performer, Duration, Time, Place, Distributor, Area, Path, Direction, Sub-region, Creator, Copy, Original, Iteration, Manner, Frequency, Agent, Body part, Depictive, Theme, Subregion, Area, De- gree, Angle, Fixed location, Path shape, Addressee, Entity, Individual 1, Individual 2, Road, Distance, Speaker, Medium, Clothing, Wearer, Bodypart of agent, Locus, Cognizer, Salient entity, Name, Inspec- tor, Ground, Unwanted entity, Location of inspector, Researcher, Population, Searcher, Sought entity, Instrument, Created entity, Components, Forgoer, Desirable, Bad entity, Dodger, Experiencer, Vehicle, Self mover, Speed, Cotheme, Consecutive, Re encoding, Supplier, Individuals, Driver, Complainer, Communicator, Protagonist, Attribute, Final value, Item, Initial value, Difference, Group, Value range, Co participant, Perceiver agentive, Target symbol, Location of perceiver, Location, Expected entity, Focal participant, Time of Event, Variable, Limits, Limit1, Limit2, Point of contact, Goods, Lessee, Lessor, Money, Rate, Unit, Reversive, Perceiver passive, Sound, Sound source, Location of source, Fidelity, Official, Selector, Role, Concessive, New leader, Body, Old leader, Leader, Governed, Result size, Size change, Dimension, Initial size, Elapsed time, Interval, Category, Criteria, Text, Final cor- relate, Correlate, Initial correlate, Manipulator, Side 1, Sides, Side 2, Perpetrator, Value 1, Value 2, Actor, Partner 2, Partner 1, Partners, Figure, Resident, Co resident, Student, Subject, Institution, Level, Teacher, Undergoer, Subregion bodypart, Course, Owner, Defendant, Judge, Co abductee, Location of appearance, Material, Accused, Arraign authority, Hair, Configuration, Emitter, Beam, Amount of progress, Evaluee, Patient, Buyer, Seller, Recipient, Relay, Relative location, Connector, Items, Part 1, Part 2, Parts, Whole, Name source, Payer, Fine, Executioner, Interlocutor 1, Interlocutor 2, Inter- locutors, Healer, Food, Cook, Container, Heating instrument, Temperature setting, Resource controller, Resource, Donor, Constant location, Carrier, Sender, Co theme, Transport means, Holding location, Rope, Knot, Handle, Containing object, Fastener, Enclosed region, Container portal, Aggregate, Sus- pect, Authorities, Offense, Source of legal authority, Ingestor, Ingestibles, Sleeper, Pieces, Goal area, Period of iterations, Mode of transportation, Produced food, Ingredients, Cognizer agent, Excreter, Excreta, Air, Perceptual source, Escapee, Undesirable location, Evader, Capture, Pursuer, Amount of discussion, Means of communication, Periodicity, Author, Honoree, Reader, Child, Mother, Father, Egg, Flammables, Flame, Kindler, Mass theme, Address, Intermediary, Communication, Location of communicator, Firearm, Indicated entity, Hearer, Sub region, Member, Object, Organization, Guardian, New Status, Arguer, Criterion, Liquid, Impactors, Force, Coparticipant, Holding Location, Legal basis, Precipitation, Quantity, Voice, Duration of endstate, Period of Iterations, Employer, Employee, Task, Position, Compensation, Field, Place of employment, Amount of work, Contract basis, Recipients, Hot Cold source, Temperature goal, Temperature change, Hot/Cold source, Dryee, Temperature, Traveler, Iterations, Baggage, Deformer, Resistant surface, Fluid, Injured Party, Avenger, Injury, Punishment, Offender, Grinder, Profiled item, Standard item, Profiled attribute, Standard attribute, Extent, Source emitter, Emission, Sub source, Item 1, Item 2, Parameter, Form, Chosen, Change agent, Injuring entity, Severity, Substance, Delivery device, Entry path, Wrong, Amends, Grounds, Expressor, Basis, Signs, Manufacturer, Product, Factory, Consumer, Interested party, Performer1, Performer2, Whole patient, Destroyer, Exporting area, Importing area, Accuracy, Time of Eventuality, Indicator, Indicated, Au- dience, Valued entity, Journey, Duration of end state, Killer, Beneficiary, Destination time, Landmark time, Seat of emotion, Arguers, Arguer1, Arguer2, Company, Asset, Origin, Sound maker, Static object, Themes, Heat source, Following distance, Perceiver, Intended perceiver, Location of expressor, Path of gaze, Relatives, Final temperature, Particular iteration, Participant 1, Language</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
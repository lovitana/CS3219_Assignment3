<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000144">
<title confidence="0.99063">
Exploring System Combination approaches for Indo-Aryan MT Systems
</title>
<author confidence="0.9451245">
Karan Singla1, Nishkarsh Shastri2, Megha Jhunjhunwala2, Anupam Singh3,
Srinivas Bangalore4, Dipti Misra Sharma1
</author>
<affiliation confidence="0.440588">
1LTRC IIIT Hyderabad, 2IIT-Kharagpur, 3NIT-Durgapur,4AT&amp;T Labs-Research
</affiliation>
<sectionHeader confidence="0.973282" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997659611111111">
Statistical Machine Translation (SMT)
systems are heavily dependent on the qual-
ity of parallel corpora used to train transla-
tion models. Translation quality between
certain Indian languages is often poor due
to the lack of training data of good qual-
ity. We used triangulation as a technique
to improve the quality of translations in
cases where the direct translation model
did not perform satisfactorily. Triangula-
tion uses a third language as a pivot be-
tween the source and target languages to
achieve an improved and more efficient
translation model in most cases. We also
combined multi-pivot models using linear
mixture and obtained significant improve-
ment in BLEU scores compared to the di-
rect source-target models.
</bodyText>
<sectionHeader confidence="0.999002" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977391304348">
Current SMT systems rely heavily on large quan-
tities of training data in order to produce good
quality translations. In spite of several initiatives
taken by numerous organizations to generate par-
allel corpora for different language pairs, train-
ing data for many language pairs is either not
yet available or is insufficient for producing good
SMT systems. Indian Languages Corpora Initia-
tive (ILCI) (Choudhary and Jha, 2011) is currently
the only reliable source for multilingual parallel
corpora for Indian languages however the number
of parallel sentences is still not sufficient to create
high quality SMT systems.
This paper aims at improving SMT systems
trained on small parallel corpora using various re-
cently developed techniques in the field of SMTs.
Triangulation is a technique which has been found
to be very useful in improving the translations
when multilingual parallel corpora are present.
Triangulation is the process of using an interme-
diate language as a pivot to translate a source lan-
guage to a target language. We have used phrase
table triangulation instead of sentence based tri-
angulation as it gives better translations (Utiyama
and Isahara, 2007). As triangulation technique ex-
plores additional multi parallel data, it provides
us with separately estimated phrase-tables which
could be further smoothed using smoothing meth-
ods (Koehn et al. 2003). Our subsequent approach
will explore the various system combination tech-
niques through which these triangulated systems
can be utilized to improve the translations.
The rest of the paper is organized as follows.
We will first talk about the some of the related
works and then we will discuss the facts about the
data and also the scores obtained for the baseline
translation model. Section 3 covers the triangu-
lation approach and also discusses the possibility
of using combination approaches for combining
triangulated and direct models. Section 4 shows
results for the experiments described in previous
section and also describes some interesting obser-
vations from the results. Section 5 explains the
conclusions we reached based on our experiments.
We conclude the paper with a section about our fu-
ture work.
</bodyText>
<sectionHeader confidence="0.999147" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.999948583333333">
There are various works on combining the tri-
angulated models obtained from different pivots
with the direct model resulting in increased con-
fidence score for translations and increased cov-
erage by (Razmara and Sarkar, 2013; Ghannay et
al., 2014; Cohn and Lapata, 2007). Among these
techniques we explored two of the them. The first
one is the technique based on the confusion ma-
trix (dynamic) (Ghannay et al., 2014) and the other
one is based on mixing the models as explored
by (Cohn and Lapata, 2007). The paper also dis-
cusses the better choice of combination technique
</bodyText>
<page confidence="0.986628">
85
</page>
<footnote confidence="0.437495">
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85–91,
October 29, 2014, Doha, Qatar. (c 2014 Association for Computational Linguistics
</footnote>
<bodyText confidence="0.999892777777778">
among these two when we have limitations on
training data which in our case was small and re-
stricted to a small domain (Health &amp; Tourism).
As suggested in (Razmara and Sarkar, 2013),
we have shown that there is an increase in phrase
coverage when combining the different systems.
Conversely we can say that out of vocabulary
words (OOV) always decrease in the combined
systems.
</bodyText>
<sectionHeader confidence="0.998126" genericHeader="method">
3 Baseline Translation Model
</sectionHeader>
<bodyText confidence="0.999972157894737">
In our experiment, the baseline translation model
used was the direct system between the source and
target languages which was trained on the same
amount of data as the triangulated models. The
parallel corpora for 4 Indian languages namely
Hindi (hn), Marathi (mt), Gujarati (gj) and Bangla
(bn) was taken from Indian Languages Corpora
Initiative (ILCI) (Choudhary and Jha, 2011) . The
parallel corpus used in our experiments belonged
to two domains - health and tourism and the train-
ing set consisted of 28000 sentences. The develop-
ment and evaluation set contained 500 sentences
each. We used MOSES (Koehn et al., 2007) to
train the baseline Phrase-based SMT system for all
the language pairs on the above mentioned paral-
lel corpus as training, development and evaluation
data. Trigram language models were trained using
SRILM (Stolcke and others, 2002). Table 1 below
shows the BLEU score for all the trained pairs.
</bodyText>
<construct confidence="0.987022153846154">
Language Pair BLEU Score
bn-mt 18.13
mt-bn 21.83
bn-gj 22.45
gj-mt 23.02
gj-bn 24.26
mt-gj 25.5
hn-mt 30.01
hn-bn 32.92
bn-hn 34.99
mt-hn 36.82
hn-gj 40.06
gj-hn 43.48
</construct>
<tableCaption confidence="0.995814">
Table 1: BLEU scores of baseline models
</tableCaption>
<sectionHeader confidence="0.6635575" genericHeader="method">
4 Triangulation: Methodology and
Experiment
</sectionHeader>
<bodyText confidence="0.999993533333333">
We first define the term triangulation in our con-
text. Each source phrase s is first translated to an
intermediate (pivot) language i, and then to a tar-
get language t. This two stage translation process
is termed as triangulation.
Our basic approach involved making triangu-
lated models by triangulating through different
pivots and then interpolating triangulated models
with the direct source-target model to make our
combined model.
In line with various previous works, we will
be using multiple translation models to overcome
the problems faced due to data sparseness and in-
crease translational coverage. Rather than using
sentence translation (Utiyama and Isahara, 2007)
from source to pivot and then pivot to target, a
phrase based translation model is built.
Hence the main focus of our approach is on
phrases rather than on sentences. Instead of using
combination techniques on the output of several
translation systems, we constructed a combined
phrase table to be used by the decoder thus avoid-
ing the additional inefficiencies observed while
merging the output of various translation systems.
Our method focuses on exploiting the availability
of multi-parallel data, albeit small in size, to im-
prove the phrase coverage and quality of our SMT
system.
Our approach can be divided into different steps
which are presented in the following sections.
</bodyText>
<subsectionHeader confidence="0.991571">
4.1 Phrase-table triangulation
</subsectionHeader>
<bodyText confidence="0.999981636363637">
Our emphasis is on building an enhanced phrase
table that incorporates the translation phrase tables
of different models. This combined phrase table
will be used by the decoder during translation.
Phrase table triangulation depends mainly on
phrase level combination of the two different
phrase based systems mainly source (src) - pivot
(pvt) and pivot (pvt) - target (tgt) using pivot lan-
guage as a basis for combination. Before stating
the mathematical approach for triangulation, we
present an example.
</bodyText>
<subsectionHeader confidence="0.813148">
4.1.1 Basic methodology
</subsectionHeader>
<bodyText confidence="0.999602">
Suppose we have a Bengali-Hindi phrase-table
(TBH) and a Hindi-Marathi phrase-table (THM).
From these tables, we have to construct a Bengali-
Marathi phrase-table (TBM). For that we need
</bodyText>
<page confidence="0.991021">
86
</page>
<table confidence="0.74087475">
Triangulated Full-Triangulation Triangulation with top 40 Full Triangulation Triangulation with top 40
System (phrase-table length) (Length of phrase table) (BLEU Score) (BLEU SCORE)
gj - hn - mt 3,585,450 1,086,528 24.70 24.66
gj - bn - mt 7,916,661 1,968,383 20.55 20.04
</table>
<tableCaption confidence="0.963972">
Table 2: Comparison between triangulated systems in systems with full phrase table and the other having
top 40 phrase-table entries
</tableCaption>
<bodyText confidence="0.9982095">
to estimate four feature functions: phrase trans-
lation probabilities for both directions φ( b |¯m)
and φ( ¯m|¯b), and lexical translation probabilities
for both directions lex(¯b |¯m) and lex( ¯m|¯b) where
¯b and m¯ are Bengali and Marathi phrases that
will appear in our triangulated Bengali-Marathi
</bodyText>
<equation confidence="0.9563036">
phrase-table TBM.
1:
φ(¯b |¯m) _ φ(
hETBHnTHM
1:
φ( ¯m|¯b) _ φ(¯m|
hETBHnTHM
lex( b |h)lex( h |¯m) (3)
lex( ¯m |h)lex( h |b) (4)
hETBHnTHM
</equation>
<bodyText confidence="0.999993846153846">
In these equations a conditional independence
assumption has been made that source phrase ¯b
and target phrase m¯ are independent given their
corresponding pivot phrase(s) ¯h. Thus, we can
derive φ(¯b |¯m), φ( ¯m|¯b), lex(¯b |¯m), lex( ¯m|¯b) by as-
suming that these probabilities are mutually inde-
pendent given a Hindi phrase ¯h.
The equation given requires that all phrases in
the Hindi-Marathi bitext must also be present in
the Bengali-Hindi bitext. Clearly there would be
many phrases not following the above require-
ment. For this paper we completely discarded the
missing phrases. One important point to note is
that although the problem of missing contextual
phrases is uncommon in multi-parallel corpora, as
it is in our case, it becomes more evident when the
bitexts are taken out from different sources.
In general, wider range of possible translations
are found for any source phrase through triangula-
tion. We found that in the direct model, a source
phrase is aligned to three phrases then there is
high possibility of it being aligned to three phrases
in intermediate language. The intermediate lan-
guage phrases are further aligned to three or more
phrases in target language. This results in increase
in number of translations of each source phrase.
</bodyText>
<subsectionHeader confidence="0.960423">
4.1.2 Reducing the size of phrase-table
</subsectionHeader>
<bodyText confidence="0.999985904761905">
While triangulation is intuitively appealing, it suf-
fers from a few problems. First, the phrasal trans-
lation estimates are based on noisy automatic word
alignments. This leads to many errors and omis-
sions in the phrase-table. With a standard source-
target phrase-table these errors are only encoun-
tered once, however with triangulation they are en-
countered twice, and therefore the errors are com-
pounded. This leads to much noisier estimates
than in the source-target phrase-table. Secondly,
the increased exposure to noise means that trian-
gulation will omit a greater proportion of large or
rare phrases than the standard method. An align-
ment error in either of the source-intermediate bi-
text or intermediate-target bitext can prevent the
extraction of a source-target phrase pair.
As will be explained in the next section, the sec-
ond kind of problem can be ameliorated by using
the triangulated phrase-based table in conjunction
with the standard phrase based table referred to as
direct src-to-pvt phrase table in our case.
For the first kind of problem, not only the com-
pounding of errors leads to increased complex-
ity but also results in an absurdly large triangu-
lated phrase based table. To tackle the problem of
unwanted phrase-translation, we followed a novel
approach.
A general observation is that while triangulat-
ing between src-pvt and pvt-tgt systems, the re-
sultant src-tgt phrase table formed will be very
large since for a translation s¯ to ¯i in the src-to-
pvt table there may be many translations from
¯i to ¯t1, ¯t2...¯tn. For example, the Bengali-Hindi
phrase-table(TBH) consisted of 846,106 transla-
tions and Hindi-Marathi phrase-table(THM) con-
sisted of 680,415 translations and after triangu-
lating these two tables our new Bengali-Marathi
triangulated table(TBM) consisted of 3,585,450
translations as shown in Table 2. Tuning with
such a large phrase-table is complex and time-
consuming. To reduce the complexity of the
phrase-table, we used only the top-40 transla-
</bodyText>
<equation confidence="0.982914833333333">
b |h)φ( h |¯m) (1)
h)φ( h |b) (2)
lex( �
¯b |¯m) _
hETBHnTHM
lex( ¯m |¯b) _ �
</equation>
<page confidence="0.97078">
87
</page>
<bodyText confidence="0.954819227272727">
tions (translation with 40 maximum values of
P(¯f|¯e) for every source phrase in our triangulated
phrase-table(TBM) which reduced the phrase table
to 1,086,528 translations.
We relied on P(¯f|¯e)(inverse phrase translation
probability) to choose 40 phrase translations for
each phrase, since in the direct model, MERT
training assigned the most weight to this param-
eter.
It is clearly evident from Table 2 that we have
got a massive reduction in the length of the phrase-
table after taking in our phrase table and still the
results have no significant difference in our output
models.
4.2 Combining different triangulated models
and the direct model
Combining Machine translation (MT) systems has
become an important part of Statistical MT in the
past few years. There have been several works by
(Rosti et al., 2007; Karakos et al., 2008; Leusch
and Ney, 2010);
We followed two approaches
</bodyText>
<listItem confidence="0.999395857142857">
1. A system combination based on confusion
network using open-source tool kit MANY
(Barrault, 2010), which can work dynami-
cally in combining the systems
2. Combine the models by linearly interpolating
them and then using MERT to tune the com-
bined system.
</listItem>
<subsubsectionHeader confidence="0.51064">
4.2.1 Combination based on confusion
</subsubsectionHeader>
<bodyText confidence="0.93080725">
matrix
MANY tool was used for this and initially it was
configured to work with TERp evaluation matrix,
but we modified it to work using METEOR-Hindi
(Gupta et al., 2010), as it has been shown by
(Kalyani et al., 2014), that METEOR evaluation
metric is closer to human evaluation for morpho-
logically rich Indian Languages.
</bodyText>
<subsubsectionHeader confidence="0.56919">
4.2.2 Linearly Interpolated Models
</subsubsectionHeader>
<bodyText confidence="0.762450636363637">
We used two different approaches while merging
the different triangulated models and direct src-tgt
model and we observed that both produced com-
parable results in most cases. We implemented the
linear mixture approach, since linear mixtures of-
ten outperform log-linear ones (Cohn and Lapata,
2007). Note that in our combination approaches
the reordering tables were left intact.
1. Our first approach was to use linear interpola-
tion to combine all the three models (Bangla-
Hin-Marathi, Bangla-Guj-Marathi and di-
rect Bangla-Marathi models) with uniform
weights, i.e 0.3 each in our case.
2. In the next approach, the triangulated phrase
tables are combined first into a single trian-
gulated phrase-table using uniform weights.
The combined triangulated phrase-table and
direct src-tgt phrase table is then combined
using uniform weights. In other words, we
combined all the three systems, Ban-Mar,
Ban-Hin-Mar, and Ban-Guj-Mar with 0.5,
0.25 and 0.25 weights respectively. This
weight distribution reflects the intuition that
the direct model is less noisy than the trian-
gulated models.
In the experiments below, both weight settings
produced comparable results. Since we performed
triangulation only through two languages, we
could not determine which approach would per-
form better. An ideal approach will be to train the
weights for each system for each language pair
using standard tuning algorithms such as MERT
(Zaidan, 2009).
</bodyText>
<subsectionHeader confidence="0.824347">
4.2.3 Choosing Combination Approach
</subsectionHeader>
<bodyText confidence="0.99981647826087">
In order to compare the approaches on our data,
we performed experiments on Hindi-Marathi pair
following both approaches discussed in Section
4.2.1 and 4.2.2. We also generated triangulated
models through Bengali and Gujarati as pivot lan-
guages.
Also, the approach presented in section 4.2.1
depends heavily on LM (Language Model).In or-
der to study the impact of size, we worked on
training Phrase-based SMT systems with subsets
of data in sets of 5000, 10000, 150000 sentences
and LM was trained for 28000 sentences for com-
paring these approaches. The combination results
were compared following the approach mentioned
in 4.2.1 and 4.2.2.
Table 3, shows that the approach discussed in
4.2.1 works better if there is more data for LM
but we suffer from the limitation that there is no
other in-domain data available for these languages.
From the Table, it can also be seen that combin-
ing systems with the approach explained in 4.2.2
can also give similar or better results if there is
scarcity of data for LM. Therefore we followed the
</bodyText>
<page confidence="0.99835">
88
</page>
<table confidence="0.99890225">
#Training #LM Data Comb-1 Comb-2
5000 28000 21.09 20.27
10000 28000 24.02 24.27
15000 28000 27.10 27.63
</table>
<tableCaption confidence="0.9015944">
Table 3: BLEU scores for Hindi-Marathi Model
comparing approaches described in 3.2.1(Comb-
1) and 3.2.2(Comb-2)
approach from Section 4.2.2 for our experiments
on other language pairs.
</tableCaption>
<sectionHeader confidence="0.933441" genericHeader="method">
5 Observation and Resuslts
</sectionHeader>
<bodyText confidence="0.99915475">
Table 4, shows the BLEU scores of triangulated
models when using the two languages out of the
4 Indian languages Hin, Guj, Mar, Ban as source
and target and the remaining two as the pivot lan-
guage. The first row mentions the BLEU score
of the direct src-tgt model for all the language
pairs. The second and third rows provide the tri-
angulated model scores through pivots which have
been listed. The fourth and fifth rows show the
BLEU scores for the combined models (triangu-
lated+direct) with the combination done using the
first and second approach respectively that have
been elucidated in the Section 4.2.2
As expected, both the combined models have
performed better than the direct models in all
cases.
</bodyText>
<figureCaption confidence="0.995258">
Figure 1: Phrase-table coverage of the evaluation
set for all the language pairs
Figure 1, shows the phrase-table coverage of the
</figureCaption>
<bodyText confidence="0.9999735">
evaluation set for all the language pairs. Phrase-
table coverage is defined as the percentage of un-
igrams in the evaluation set for which translations
are present in the phrase-table. The first bar cor-
responds to the direct model for each language
pair, the second and third bars show the cover-
age for triangulated models through the 2 piv-
ots, while the fourth bar is the coverage for the
combined model (direct+triangulated). The graph
clearly shows that even though the phrase table
coverage may increase or decrease by triangula-
tion through a single pivot the combined model
(direct+triangulated) always gives a higher cover-
age than the direct model.
Moreover, there exists some triangulation mod-
els whose coverage and subsequent BLEU scores
for translation is found to be better than that of the
direct model. This is a particularly interesting ob-
servation as it increases the probability of obtain-
ing better or at least comparable translation mod-
els even when direct source-target parallel corpus
is absent.
</bodyText>
<sectionHeader confidence="0.999752" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999445">
Dravidian languages are different from Indo-aryan
languages but they are closely related amongst
themselves. So we explored similar experiments
with Malayalam-Telugu pair of languages with
similar parallel data and with Hindi as pivot.
The hypothesis was that the direct model for
Malayalam-Telegu would have performed better
due to relatedness of the two languages. However
the results via Hindi were better as can be seen in
Table 5.
As Malayalam-Telegu are comparatively closer
than compared to Hindi, so the results via Hindi
should have been worse but it seems more like a
biased property of training data which considers
that all languages are closer to Hindi, as the trans-
lation data was created from Hindi.
</bodyText>
<sectionHeader confidence="0.999565" genericHeader="discussions">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999986666666667">
It becomes increasingly important for us to im-
prove these techniques for such languages having
rare corpora. The technique discussed in the paper
is although efficient but still have scope for im-
provements.
As we have seen from our two approaches of
combining the phrase tables and subsequent in-
terpolation with direct one, the best combination
among the two is also not fixed. If we can find the
</bodyText>
<page confidence="0.99919">
89
</page>
<table confidence="0.999906714285714">
BLEU scores gj-mt mt-gj gj-hn hn-gj hn-mt mt-hn
Direct model 23.02 25.50 43.48 40.06 30.01 36.82
Triangulated hn 24.66 hn 27.09 mt 36.76 mt 33.69 gj 29.27 gj 33.86
through pivots
bn 20.04 bn 22.02 bn 35.07 bn 32.66 bn 26.72 bn 31.34
Mixture-1 26.12 27.46 43.23 39.99 33.09 38.50
Mixture-2 26.25 27.32 44.04 41.45 33.36 38.44
BLEU scores bn-gj gj-bn bn-hn hn-bn mt-bn bn-mt
Direct model 22.45 24.26 34.99 32.92 21.83 18.13
Triangulated hn 23.97 hn 26.26 gj 31.69 gj 29.60 hn 23.80 hn 21.04
through pivots
mt 20.70 mt 22.32 mt 28.96 mt 27.95 gj 22.41 gj 18.15
Mixture-1 25.80 27.45 35.14 34.77 24.99 22.16
Mixture-2 24.66 27.39 35.02 34.85 24.86 22.75
</table>
<tableCaption confidence="0.837697">
Table 4: Table (a) &amp; (b) show results for all language pairs after making triangulated models and then
combining them with linear interpolation with the two approaches described in 3.2.2. In Mixture-1,
uniform weights were given to all three models but in Mixture-2, direct model is given 0.5 weight relative
to the other models (.25 weight to each)
</tableCaption>
<table confidence="0.998533333333333">
System Blue Score
Direct Model 4.63
Triangulated via Hindi 14.32
</table>
<tableCaption confidence="0.923947">
Table 5: Results for Malayalam-Telegu Pair for
</tableCaption>
<bodyText confidence="0.952761777777778">
same data used for other languages
best possible weights to be assigned to each table,
then we can see improvement in translation. This
can be implemented by making the machine learn
from various iterations of combining and adjusting
the scores accordingly.(Nakov and Ng, 2012) have
indeed shown that results show significant devia-
tions associated with different weights assigned to
the tables.
</bodyText>
<sectionHeader confidence="0.998545" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999049521739131">
Loic Barrault. 2010. Many: Open source machine
translation system combination. The Prague Bul-
letin of Mathematical Linguistics, 93:147–155.
Narayan Choudhary and Girish Nath Jha. 2011. Cre-
ating multilingual parallel corpora in indian lan-
guages. In Proceedings of Language and Technol-
ogy Conference.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Citeseer.
Sahar Ghannay, France Le Mans, and Loıc Barrault.
2014. Using hypothesis selection based features for
confusion network mt system combination. In Pro-
ceedings of the 3rd Workshop on Hybrid Approaches
to Translation (HyTra)@ EACL, pages 1–5.
Ankush Gupta, Sriram Venkatapathy, and Rajeev San-
gal. 2010. Meteor-hindi: Automatic mt evaluation
metric for hindi as a target language. In Proceed-
ings of ICON-2010: 8th International Conference
on Natural Language Processing.
Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, and
Ajai Kumar. 2014. Assessing the quality of mt sys-
tems for hindi to english translation. arXiv preprint
arXiv:1404.3992.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human
Language Technologies: Short Papers, pages 81–84.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177–180. Association for Computational Lin-
guistics.
Gregor Leusch and Hermann Ney. 2010. The rwth
system combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
</reference>
<page confidence="0.977509">
90
</page>
<reference confidence="0.999645148148148">
Machine Translation and MetricsMATR, pages 315–
320. Association for Computational Linguistics.
Preslav Nakov and Hwee Tou Ng. 2012. Improv-
ing statistical machine translation for a resource-
poor language using related resource-rich lan-
guages. Journal of Artificial Intelligence Research,
44(1):179–222.
Majid Razmara and Anoop Sarkar. 2013. Ensemble
triangulation for statistical machine translation. In
Proceedings of the Sixth International Joint Confer-
ence on Natural Language Processing, pages 252–
260.
Antti-Veikko I Rosti, Spyridon Matsoukas, and
Richard Schwartz. 2007. Improved word-level sys-
tem combination for machine translation. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 312.
Citeseer.
Andreas Stolcke et al. 2002. Srilm-an extensible lan-
guage modeling toolkit. In INTERSPEECH.
Masao Utiyama and Hitoshi Isahara. 2007. A compari-
son of pivot methods for phrase-based statistical ma-
chine translation. In HLT-NAACL, pages 484–491.
Omar Zaidan. 2009. Z-mert: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79–88.
</reference>
<page confidence="0.999171">
91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.727177">
<title confidence="0.9282745">Exploring System Combination approaches for Indo-Aryan MT Systems Nishkarsh Megha Anupam</title>
<author confidence="0.99254">Dipti Misra</author>
<affiliation confidence="0.866819">IIIT Hyderabad, Labs-Research</affiliation>
<abstract confidence="0.998947105263158">Statistical Machine Translation (SMT) systems are heavily dependent on the quality of parallel corpora used to train translation models. Translation quality between certain Indian languages is often poor due to the lack of training data of good quality. We used triangulation as a technique to improve the quality of translations in cases where the direct translation model did not perform satisfactorily. Triangulation uses a third language as a pivot between the source and target languages to achieve an improved and more efficient translation model in most cases. We also combined multi-pivot models using linear mixture and obtained significant improvement in BLEU scores compared to the direct source-target models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Loic Barrault</author>
</authors>
<title>Many: Open source machine translation system combination.</title>
<date>2010</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>93--147</pages>
<contexts>
<context position="12840" citStr="Barrault, 2010" startWordPosition="2033" endWordPosition="2034">t is clearly evident from Table 2 that we have got a massive reduction in the length of the phrasetable after taking in our phrase table and still the results have no significant difference in our output models. 4.2 Combining different triangulated models and the direct model Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 2. Combine the models by linearly interpolating them and then using MERT to tune the combined system. 4.2.1 Combination based on confusion matrix MANY tool was used for this and initially it was configured to work with TERp evaluation matrix, but we modified it to work using METEOR-Hindi (Gupta et al., 2010), as it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is closer to human evaluation for morphologically rich Indian Languages. 4.2.2 Linearly Interpolated Models We used two different approaches while merging th</context>
</contexts>
<marker>Barrault, 2010</marker>
<rawString>Loic Barrault. 2010. Many: Open source machine translation system combination. The Prague Bulletin of Mathematical Linguistics, 93:147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Narayan Choudhary</author>
<author>Girish Nath Jha</author>
</authors>
<title>Creating multilingual parallel corpora in indian languages.</title>
<date>2011</date>
<booktitle>In Proceedings of Language and Technology Conference.</booktitle>
<contexts>
<context position="1421" citStr="Choudhary and Jha, 2011" startWordPosition="208" endWordPosition="211">nslation model in most cases. We also combined multi-pivot models using linear mixture and obtained significant improvement in BLEU scores compared to the direct source-target models. 1 Introduction Current SMT systems rely heavily on large quantities of training data in order to produce good quality translations. In spite of several initiatives taken by numerous organizations to generate parallel corpora for different language pairs, training data for many language pairs is either not yet available or is insufficient for producing good SMT systems. Indian Languages Corpora Initiative (ILCI) (Choudhary and Jha, 2011) is currently the only reliable source for multilingual parallel corpora for Indian languages however the number of parallel sentences is still not sufficient to create high quality SMT systems. This paper aims at improving SMT systems trained on small parallel corpora using various recently developed techniques in the field of SMTs. Triangulation is a technique which has been found to be very useful in improving the translations when multilingual parallel corpora are present. Triangulation is the process of using an intermediate language as a pivot to translate a source language to a target l</context>
<context position="4743" citStr="Choudhary and Jha, 2011" startWordPosition="737" endWordPosition="740"> and Sarkar, 2013), we have shown that there is an increase in phrase coverage when combining the different systems. Conversely we can say that out of vocabulary words (OOV) always decrease in the combined systems. 3 Baseline Translation Model In our experiment, the baseline translation model used was the direct system between the source and target languages which was trained on the same amount of data as the triangulated models. The parallel corpora for 4 Indian languages namely Hindi (hn), Marathi (mt), Gujarati (gj) and Bangla (bn) was taken from Indian Languages Corpora Initiative (ILCI) (Choudhary and Jha, 2011) . The parallel corpus used in our experiments belonged to two domains - health and tourism and the training set consisted of 28000 sentences. The development and evaluation set contained 500 sentences each. We used MOSES (Koehn et al., 2007) to train the baseline Phrase-based SMT system for all the language pairs on the above mentioned parallel corpus as training, development and evaluation data. Trigram language models were trained using SRILM (Stolcke and others, 2002). Table 1 below shows the BLEU score for all the trained pairs. Language Pair BLEU Score bn-mt 18.13 mt-bn 21.83 bn-gj 22.45</context>
</contexts>
<marker>Choudhary, Jha, 2011</marker>
<rawString>Narayan Choudhary and Girish Nath Jha. 2011. Creating multilingual parallel corpora in indian languages. In Proceedings of Language and Technology Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Machine translation by triangulation: Making effective use of multi-parallel corpora.</title>
<date>2007</date>
<booktitle>In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<volume>45</volume>
<pages>728</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="3467" citStr="Cohn and Lapata, 2007" startWordPosition="530" endWordPosition="533">ination approaches for combining triangulated and direct models. Section 4 shows results for the experiments described in previous section and also describes some interesting observations from the results. Section 5 explains the conclusions we reached based on our experiments. We conclude the paper with a section about our future work. 2 Related Works There are various works on combining the triangulated models obtained from different pivots with the direct model resulting in increased confidence score for translations and increased coverage by (Razmara and Sarkar, 2013; Ghannay et al., 2014; Cohn and Lapata, 2007). Among these techniques we explored two of the them. The first one is the technique based on the confusion matrix (dynamic) (Ghannay et al., 2014) and the other one is based on mixing the models as explored by (Cohn and Lapata, 2007). The paper also discusses the better choice of combination technique 85 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85–91, October 29, 2014, Doha, Qatar. (c 2014 Association for Computational Linguistics among these two when we have limitations on training data which in our case was small and restricted to a small</context>
<context position="13688" citStr="Cohn and Lapata, 2007" startWordPosition="2167" endWordPosition="2170">s and initially it was configured to work with TERp evaluation matrix, but we modified it to work using METEOR-Hindi (Gupta et al., 2010), as it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is closer to human evaluation for morphologically rich Indian Languages. 4.2.2 Linearly Interpolated Models We used two different approaches while merging the different triangulated models and direct src-tgt model and we observed that both produced comparable results in most cases. We implemented the linear mixture approach, since linear mixtures often outperform log-linear ones (Cohn and Lapata, 2007). Note that in our combination approaches the reordering tables were left intact. 1. Our first approach was to use linear interpolation to combine all the three models (BanglaHin-Marathi, Bangla-Guj-Marathi and direct Bangla-Marathi models) with uniform weights, i.e 0.3 each in our case. 2. In the next approach, the triangulated phrase tables are combined first into a single triangulated phrase-table using uniform weights. The combined triangulated phrase-table and direct src-tgt phrase table is then combined using uniform weights. In other words, we combined all the three systems, Ban-Mar, Ba</context>
</contexts>
<marker>Cohn, Lapata, 2007</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2007. Machine translation by triangulation: Making effective use of multi-parallel corpora. In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 728. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sahar Ghannay</author>
<author>France Le Mans</author>
<author>Loıc Barrault</author>
</authors>
<title>Using hypothesis selection based features for confusion network mt system combination.</title>
<date>2014</date>
<booktitle>In Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra)@ EACL,</booktitle>
<pages>1--5</pages>
<marker>Ghannay, Le Mans, Barrault, 2014</marker>
<rawString>Sahar Ghannay, France Le Mans, and Loıc Barrault. 2014. Using hypothesis selection based features for confusion network mt system combination. In Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra)@ EACL, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ankush Gupta</author>
<author>Sriram Venkatapathy</author>
<author>Rajeev Sangal</author>
</authors>
<title>Meteor-hindi: Automatic mt evaluation metric for hindi as a target language.</title>
<date>2010</date>
<booktitle>In Proceedings of ICON-2010: 8th International Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="13203" citStr="Gupta et al., 2010" startWordPosition="2093" endWordPosition="2096">ical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 2. Combine the models by linearly interpolating them and then using MERT to tune the combined system. 4.2.1 Combination based on confusion matrix MANY tool was used for this and initially it was configured to work with TERp evaluation matrix, but we modified it to work using METEOR-Hindi (Gupta et al., 2010), as it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is closer to human evaluation for morphologically rich Indian Languages. 4.2.2 Linearly Interpolated Models We used two different approaches while merging the different triangulated models and direct src-tgt model and we observed that both produced comparable results in most cases. We implemented the linear mixture approach, since linear mixtures often outperform log-linear ones (Cohn and Lapata, 2007). Note that in our combination approaches the reordering tables were left intact. 1. Our first approach was to use </context>
</contexts>
<marker>Gupta, Venkatapathy, Sangal, 2010</marker>
<rawString>Ankush Gupta, Sriram Venkatapathy, and Rajeev Sangal. 2010. Meteor-hindi: Automatic mt evaluation metric for hindi as a target language. In Proceedings of ICON-2010: 8th International Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aditi Kalyani</author>
<author>Hemant Kumud</author>
<author>Shashi Pal Singh</author>
<author>Ajai Kumar</author>
</authors>
<title>Assessing the quality of mt systems for hindi to english translation. arXiv preprint arXiv:1404.3992.</title>
<date>2014</date>
<contexts>
<context position="13251" citStr="Kalyani et al., 2014" startWordPosition="2103" endWordPosition="2106"> several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 2. Combine the models by linearly interpolating them and then using MERT to tune the combined system. 4.2.1 Combination based on confusion matrix MANY tool was used for this and initially it was configured to work with TERp evaluation matrix, but we modified it to work using METEOR-Hindi (Gupta et al., 2010), as it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is closer to human evaluation for morphologically rich Indian Languages. 4.2.2 Linearly Interpolated Models We used two different approaches while merging the different triangulated models and direct src-tgt model and we observed that both produced comparable results in most cases. We implemented the linear mixture approach, since linear mixtures often outperform log-linear ones (Cohn and Lapata, 2007). Note that in our combination approaches the reordering tables were left intact. 1. Our first approach was to use linear interpolation to combine all the three mo</context>
</contexts>
<marker>Kalyani, Kumud, Singh, Kumar, 2014</marker>
<rawString>Aditi Kalyani, Hemant Kumud, Shashi Pal Singh, and Ajai Kumar. 2014. Assessing the quality of mt systems for hindi to english translation. arXiv preprint arXiv:1404.3992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damianos Karakos</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
<author>Markus Dreyer</author>
</authors>
<title>Machine translation system combination using itg-based alignments.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,</booktitle>
<pages>81--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12689" citStr="Karakos et al., 2008" startWordPosition="2008" endWordPosition="2011">lation probability) to choose 40 phrase translations for each phrase, since in the direct model, MERT training assigned the most weight to this parameter. It is clearly evident from Table 2 that we have got a massive reduction in the length of the phrasetable after taking in our phrase table and still the results have no significant difference in our output models. 4.2 Combining different triangulated models and the direct model Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 2. Combine the models by linearly interpolating them and then using MERT to tune the combined system. 4.2.1 Combination based on confusion matrix MANY tool was used for this and initially it was configured to work with TERp evaluation matrix, but we modified it to work using METEOR-Hindi (Gupta et al., 2010), as it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is clo</context>
</contexts>
<marker>Karakos, Eisner, Khudanpur, Dreyer, 2008</marker>
<rawString>Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Markus Dreyer. 2008. Machine translation system combination using itg-based alignments. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 81–84. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4985" citStr="Koehn et al., 2007" startWordPosition="779" endWordPosition="782">our experiment, the baseline translation model used was the direct system between the source and target languages which was trained on the same amount of data as the triangulated models. The parallel corpora for 4 Indian languages namely Hindi (hn), Marathi (mt), Gujarati (gj) and Bangla (bn) was taken from Indian Languages Corpora Initiative (ILCI) (Choudhary and Jha, 2011) . The parallel corpus used in our experiments belonged to two domains - health and tourism and the training set consisted of 28000 sentences. The development and evaluation set contained 500 sentences each. We used MOSES (Koehn et al., 2007) to train the baseline Phrase-based SMT system for all the language pairs on the above mentioned parallel corpus as training, development and evaluation data. Trigram language models were trained using SRILM (Stolcke and others, 2002). Table 1 below shows the BLEU score for all the trained pairs. Language Pair BLEU Score bn-mt 18.13 mt-bn 21.83 bn-gj 22.45 gj-mt 23.02 gj-bn 24.26 mt-gj 25.5 hn-mt 30.01 hn-bn 32.92 bn-hn 34.99 mt-hn 36.82 hn-gj 40.06 gj-hn 43.48 Table 1: BLEU scores of baseline models 4 Triangulation: Methodology and Experiment We first define the term triangulation in our cont</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>The rwth system combination system for wmt 2010.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>315--320</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12712" citStr="Leusch and Ney, 2010" startWordPosition="2012" endWordPosition="2015"> choose 40 phrase translations for each phrase, since in the direct model, MERT training assigned the most weight to this parameter. It is clearly evident from Table 2 that we have got a massive reduction in the length of the phrasetable after taking in our phrase table and still the results have no significant difference in our output models. 4.2 Combining different triangulated models and the direct model Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 2. Combine the models by linearly interpolating them and then using MERT to tune the combined system. 4.2.1 Combination based on confusion matrix MANY tool was used for this and initially it was configured to work with TERp evaluation matrix, but we modified it to work using METEOR-Hindi (Gupta et al., 2010), as it has been shown by (Kalyani et al., 2014), that METEOR evaluation metric is closer to human evaluation</context>
</contexts>
<marker>Leusch, Ney, 2010</marker>
<rawString>Gregor Leusch and Hermann Ney. 2010. The rwth system combination system for wmt 2010. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 315– 320. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Improving statistical machine translation for a resourcepoor language using related resource-rich languages.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<issue>1</issue>
<marker>Nakov, Ng, 2012</marker>
<rawString>Preslav Nakov and Hwee Tou Ng. 2012. Improving statistical machine translation for a resourcepoor language using related resource-rich languages. Journal of Artificial Intelligence Research, 44(1):179–222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
<author>Anoop Sarkar</author>
</authors>
<title>Ensemble triangulation for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Sixth International Joint Conference on Natural Language Processing,</booktitle>
<pages>252--260</pages>
<contexts>
<context position="3421" citStr="Razmara and Sarkar, 2013" startWordPosition="522" endWordPosition="525">and also discusses the possibility of using combination approaches for combining triangulated and direct models. Section 4 shows results for the experiments described in previous section and also describes some interesting observations from the results. Section 5 explains the conclusions we reached based on our experiments. We conclude the paper with a section about our future work. 2 Related Works There are various works on combining the triangulated models obtained from different pivots with the direct model resulting in increased confidence score for translations and increased coverage by (Razmara and Sarkar, 2013; Ghannay et al., 2014; Cohn and Lapata, 2007). Among these techniques we explored two of the them. The first one is the technique based on the confusion matrix (dynamic) (Ghannay et al., 2014) and the other one is based on mixing the models as explored by (Cohn and Lapata, 2007). The paper also discusses the better choice of combination technique 85 Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 85–91, October 29, 2014, Doha, Qatar. (c 2014 Association for Computational Linguistics among these two when we have limitations on training data which i</context>
</contexts>
<marker>Razmara, Sarkar, 2013</marker>
<rawString>Majid Razmara and Anoop Sarkar. 2013. Ensemble triangulation for statistical machine translation. In Proceedings of the Sixth International Joint Conference on Natural Language Processing, pages 252– 260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Spyridon Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,</booktitle>
<volume>45</volume>
<pages>312</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="12667" citStr="Rosti et al., 2007" startWordPosition="2004" endWordPosition="2007">inverse phrase translation probability) to choose 40 phrase translations for each phrase, since in the direct model, MERT training assigned the most weight to this parameter. It is clearly evident from Table 2 that we have got a massive reduction in the length of the phrasetable after taking in our phrase table and still the results have no significant difference in our output models. 4.2 Combining different triangulated models and the direct model Combining Machine translation (MT) systems has become an important part of Statistical MT in the past few years. There have been several works by (Rosti et al., 2007; Karakos et al., 2008; Leusch and Ney, 2010); We followed two approaches 1. A system combination based on confusion network using open-source tool kit MANY (Barrault, 2010), which can work dynamically in combining the systems 2. Combine the models by linearly interpolating them and then using MERT to tune the combined system. 4.2.1 Combination based on confusion matrix MANY tool was used for this and initially it was configured to work with TERp evaluation matrix, but we modified it to work using METEOR-Hindi (Gupta et al., 2010), as it has been shown by (Kalyani et al., 2014), that METEOR ev</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko I Rosti, Spyridon Matsoukas, and Richard Schwartz. 2007. Improved word-level system combination for machine translation. In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, volume 45, page 312. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In INTERSPEECH.</booktitle>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke et al. 2002. Srilm-an extensible language modeling toolkit. In INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masao Utiyama</author>
<author>Hitoshi Isahara</author>
</authors>
<title>A comparison of pivot methods for phrase-based statistical machine translation.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>484--491</pages>
<contexts>
<context position="2169" citStr="Utiyama and Isahara, 2007" startWordPosition="326" endWordPosition="329">el sentences is still not sufficient to create high quality SMT systems. This paper aims at improving SMT systems trained on small parallel corpora using various recently developed techniques in the field of SMTs. Triangulation is a technique which has been found to be very useful in improving the translations when multilingual parallel corpora are present. Triangulation is the process of using an intermediate language as a pivot to translate a source language to a target language. We have used phrase table triangulation instead of sentence based triangulation as it gives better translations (Utiyama and Isahara, 2007). As triangulation technique explores additional multi parallel data, it provides us with separately estimated phrase-tables which could be further smoothed using smoothing methods (Koehn et al. 2003). Our subsequent approach will explore the various system combination techniques through which these triangulated systems can be utilized to improve the translations. The rest of the paper is organized as follows. We will first talk about the some of the related works and then we will discuss the facts about the data and also the scores obtained for the baseline translation model. Section 3 covers</context>
<context position="6209" citStr="Utiyama and Isahara, 2007" startWordPosition="971" endWordPosition="974">ntext. Each source phrase s is first translated to an intermediate (pivot) language i, and then to a target language t. This two stage translation process is termed as triangulation. Our basic approach involved making triangulated models by triangulating through different pivots and then interpolating triangulated models with the direct source-target model to make our combined model. In line with various previous works, we will be using multiple translation models to overcome the problems faced due to data sparseness and increase translational coverage. Rather than using sentence translation (Utiyama and Isahara, 2007) from source to pivot and then pivot to target, a phrase based translation model is built. Hence the main focus of our approach is on phrases rather than on sentences. Instead of using combination techniques on the output of several translation systems, we constructed a combined phrase table to be used by the decoder thus avoiding the additional inefficiencies observed while merging the output of various translation systems. Our method focuses on exploiting the availability of multi-parallel data, albeit small in size, to improve the phrase coverage and quality of our SMT system. Our approach </context>
</contexts>
<marker>Utiyama, Isahara, 2007</marker>
<rawString>Masao Utiyama and Hitoshi Isahara. 2007. A comparison of pivot methods for phrase-based statistical machine translation. In HLT-NAACL, pages 484–491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar Zaidan</author>
</authors>
<title>Z-mert: A fully configurable open source tool for minimum error rate training of machine translation systems.</title>
<date>2009</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>91--79</pages>
<contexts>
<context position="14818" citStr="Zaidan, 2009" startWordPosition="2341" endWordPosition="2342">ing uniform weights. In other words, we combined all the three systems, Ban-Mar, Ban-Hin-Mar, and Ban-Guj-Mar with 0.5, 0.25 and 0.25 weights respectively. This weight distribution reflects the intuition that the direct model is less noisy than the triangulated models. In the experiments below, both weight settings produced comparable results. Since we performed triangulation only through two languages, we could not determine which approach would perform better. An ideal approach will be to train the weights for each system for each language pair using standard tuning algorithms such as MERT (Zaidan, 2009). 4.2.3 Choosing Combination Approach In order to compare the approaches on our data, we performed experiments on Hindi-Marathi pair following both approaches discussed in Section 4.2.1 and 4.2.2. We also generated triangulated models through Bengali and Gujarati as pivot languages. Also, the approach presented in section 4.2.1 depends heavily on LM (Language Model).In order to study the impact of size, we worked on training Phrase-based SMT systems with subsets of data in sets of 5000, 10000, 150000 sentences and LM was trained for 28000 sentences for comparing these approaches. The combinati</context>
</contexts>
<marker>Zaidan, 2009</marker>
<rawString>Omar Zaidan. 2009. Z-mert: A fully configurable open source tool for minimum error rate training of machine translation systems. The Prague Bulletin of Mathematical Linguistics, 91:79–88.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
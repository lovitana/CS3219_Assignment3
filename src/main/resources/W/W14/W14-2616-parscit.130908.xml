<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027603">
<title confidence="0.992193">
Sentiment classification of online political discussions: a comparison of a
word-based and dependency-based method
</title>
<author confidence="0.991489">
Hugo Lewi Hammer Per Erik Solberg Lilja Øvrelid
</author>
<affiliation confidence="0.986621666666667">
Oslo and Akershus Spr˚akbanken Department of Informatics
University College The National Library University of Oslo
Department of Computer Science of Norway liljao@ifi.uio.no
</affiliation>
<email confidence="0.994713">
hugo.hammer@hioa.no p.e.solberg@ifikk.uio.no
</email>
<sectionHeader confidence="0.993808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866052631579">
Online political discussions have received
a lot of attention over the past years. In
this paper we compare two sentiment lexi-
con approaches to classify the sentiment of
sentences from political discussions. The
first approach is based on applying the
number of words between the target and
the sentiment words to weight the sen-
tence sentiment score. The second ap-
proach is based on using the shortest paths
between target and sentiment words in a
dependency graph and linguistically mo-
tivated syntactic patterns expressed as de-
pendency paths. The methods are tested
on a corpus of sentences from online Nor-
wegian political discussions. The results
show that the method based on depen-
dency graphs performs significantly better
than the word-based approach.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959952380952">
Over the past years online political discussions
have received a lot of attention. E.g. the
Obama 2012 election team initiated an extensive
use of text analytics and machine learning tech-
niques towards online material to guide advertis-
ing campaigns, identifying key voters, and im-
prove fundraising (Issenberg, 2012). There has
also been a lot of concern about the alarming
growth in hate and racism against minorities like
Muslims, Jews and Gypsies in online discussions
(Goodwin et al., 2013; Bartlett et al., 2013). Sen-
timent analysis (SA) is the discipline of automat-
ically determining sentiment in text material and
may be one important tool in understanding the
diversity of opinions on the Internet.
In this paper we focus on classifying the sen-
timent towards religious/political topics, say the
Quran, in Norwegian political discussion. We use
a lexicon-based approach where we classify the
sentiment of a sentence based on the polarity of
sentiment words in relation to a set of target words
in the sentence. We expect that statistically the
importance of a sentiment word towards the tar-
get word is related to the number of words be-
tween the sentiment and target word as suggested
by Ding et al. (2008). Information about the syn-
tactic environment of certain words or phrases has
in previous work also been shown to be useful for
the task of sentiment classification (Wilson et al.,
2009; Jiang et al., 2011). In this work we therefore
compare the results obtained using a token-based
distance measure with a novel syntax-based dis-
tance measure obtained using dependency graphs
and further augmented with linguistically moti-
vated syntactic patterns expressed as dependency
paths. In order to evaluate the proposed methods,
we furthermore present a freely available corpus of
Norwegian political discussion related to religion
and immigration, which has been manually anno-
tated for the sentiment expressed towards a set of
target words, as well as a manually translated sen-
timent lexicon.
</bodyText>
<sectionHeader confidence="0.995924" genericHeader="method">
2 Previous work
</sectionHeader>
<bodyText confidence="0.999958666666667">
Sentiment classification aims to classify a docu-
ment or sentence as either positive or negative and
sometimes also neutral. There are mainly two ap-
proaches, one based on machine learning and one
based on using a list of words with given senti-
ment scores (lexicon-based approach). For ma-
chine learning any existing method can be used,
e.g. naive Bayes and support vector machine,
(Joachims, 1999; Shawe-Taylor and Cristianini,
2000). One simple lexicon-based approach is to
count the number of words with positive and neg-
ative sentiment in the document as suggested by
Hu and Liu (2004). One may classify the opin-
ion of larger documents like movie or product re-
views or smaller documents like tweets, comments
</bodyText>
<page confidence="0.970774">
90
</page>
<bodyText confidence="0.984758615384616">
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 90–96,
Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics
or sentences. See Liu (2012), chapters three to five
and references therein for the description of sev-
eral opinion classification methods.
SA has mostly been used to analyze opinions
in comments and reviews about commercial prod-
ucts, but there are also examples of SA towards
political tweets and discussions, see e.g. Tumas-
jan et al. (2010); Chen et al. (2010). SA of politi-
cal discussions is known to be a difficult task since
citations, irony and sarcasm is very common (Liu,
2012).
</bodyText>
<sectionHeader confidence="0.997538" genericHeader="method">
3 Proposed SA methods
</sectionHeader>
<bodyText confidence="0.999921807692308">
In this section we present two methods to clas-
sify sentences as either positive, neutral or neg-
ative towards a target word. Both methods fol-
low the same general algorithm presented below
which is inspired by Ding et al. (2008) and is
based on a list of sentiment words each associ-
ated with a sentiment score representing the polar-
ity and strength of the sentiment word (sentiment
lexicon). Both target words, sentiment words and
sentiment shifters can in general appear several
times in a sentence. Sentiment shifters are words
that potentially shift the sentiment of a sentence
from positive to negative or negative to positive.
E.g. “not happy” have the opposite polarity than
just “happy”. Let twi, i E 11, 2, ... , I} represent
appearance number i of the target word in the sen-
tence. Note that we only consider one target word
at the time. E.g. if a sentence contains two target
words, e.g. Quran and Islam, the sentence is first
classified with respect to Quran and then with re-
spect to Islam. Further let swj, j E 11, 2, ... , J}
be appearance number j of a sentiment word in the
sentence. Finally let ss = (ss1, ss2,... , ssK) rep-
resent the sentiment shifters in the sentence. We
compute a sentiment score, S, for the sentence as
follows
</bodyText>
<equation confidence="0.9408395">
imp(twi, swj)shift(swj, ss)
(1)
</equation>
<bodyText confidence="0.99867875">
where the function imp computes the importance
of the sentiment word swj on the target word ap-
pearance twi. This will be computed in different
ways as described below. Further, the function
shift(swj, ss) computes whether the sentiment of
swj should be shifted based on all the sentiment
shifters in the sentence. It returns −1 (sentiment
shift) if some of the sentiment shifters are within
dp words in front or dn words behind swj, re-
spectively. Else the function, returns 1 (no sen-
timent shift). We classify the sentiment towards
the target word to be positive, neutral or negative
if S &gt;= tp, tp &gt; S &gt; tn and S &lt;= tn, respec-
tively. The parameters dp, dn, tp and tn is tuned
using a training set, as described in section 5 be-
low.
</bodyText>
<subsectionHeader confidence="0.998696">
3.1 Word distance method
</subsectionHeader>
<bodyText confidence="0.9975175">
For the word distance method we use the follow-
ing imp function
</bodyText>
<equation confidence="0.9924325">
sentsc(swj)
imp(twi, swj) = worddist(twi, swj) (2)
</equation>
<bodyText confidence="0.999867">
where sentsc(swj) is the sentiment score
of swj from the sentiment lexicon and
worddist(twi, swj) is the number of words
between twi and swj in the sentence plus one.
</bodyText>
<subsectionHeader confidence="0.998794">
3.2 Parse tree method
</subsectionHeader>
<bodyText confidence="0.999485482758621">
When determining the sentiment expressed to-
wards a specific target word, the syntactic environ-
ment of this word and how it relates to sentiment-
bearing words in the context may clearly be of im-
portance. In the following we present a modifi-
cation of the scoring function described above to
also take into account the syntactic environment
of the target words. The function is defined over
dependency graphs, i.e. connected, acyclic graphs
expressing bilexical relations.
Dependency distance One way of expressing
the syntactic environment of a target word with re-
spect to a sentiment word is to determine its dis-
tance in the dependency graph. We therefore de-
fine a distance function depdist(twi, swj) which
returns the number of nodes in the shortest depen-
dency path from the target word to the sentiment
word in the dependency graph. The shortest path
is determined using Dijkstra’s shortest path algo-
rithm (Dijkstra, 1959).
Dependency paths A second way of determin-
ing the importance of a sentiment word towards
a target based on syntactically parsed texts, is to
establish a list of grammatical dependency paths
between words, and test whether such paths exist
between the targets and sentiment words (Jiang
et al., 2011). The assumption would be that two
words most likely are semantically related to each
other if there is a meaningful grammatical relation
</bodyText>
<equation confidence="0.866932571428571">
1
S =
I
I
i=1
J
j=1
</equation>
<page confidence="0.9691">
91
</page>
<bodyText confidence="0.998454684210527">
between them. Furthermore, it is reasonable to ex-
pect than some paths are stronger indicators of the
overall sentiment of the sentence than others. To
test this method, we have manually created a list
of 42 grammatical dependency paths, divided into
four groups, and given them a score from 0 − 1.
The higher the score is, the better indicator of sen-
timent the path is assumed to be. In the following
paragraphs, we will briefly present the groups of
paths and the maximum score we have assigned
in each group. The paths are represented in the
following format: postag-target:postag-sentiment
word DEPREL up/dn( DEPREL up/dn etc.).
Up and do indicate the direction of the traversed
arc in the graph.
A first group consists of paths from sub-
ject targets to sentiment predicates. Such
paths can e.g. go from a subject to a
verbal predicate, subst:verb SUBJ up, or
from a subject to an adjectival or nominal
predicate in the context of a copular verb,
subst:adj/subst SUBJ up SPRED dn. Paths
in this group can get the maximum score, 1.
The combination of a subject and a predicate
will result in a proposition, a statement which
is evaluated as true or false. We expect that a
proposition typically will represent the opinion of
the speaker, although e.g. irony and certain kinds
of embedding can shift the truth evaluation in
some cases. Secondly, if the predicate represents
an event brought about by an intentional agent,
the subject will typically represent that agent. If
the predicate has a positive or negative sentiment,
we expect that this sentiment is directed towards
this intentional agent.
A second group we have considered, contains
paths from subject targets to sentiment words em-
bedded within the predicate, such as from the
subject to the nominal direct object of a verb,
subst:subst SUBJ up DOBJ dn. Paths from
subjects into different kinds of adverbials are also
a part of this group. We consider paths from sub-
jects to objects to be good indicators of sentiment
and assign them the highest score, 1 . The rea-
soning is much the same as for subject predicate
paths: The statement is a proposition and the sub-
ject will often be the agent of the event. Also, the
object and the verb are presumably closely seman-
tically connected, as the former is an argument of
the latter. Paths into adverbials get lower values,
as adverbials often are less semantically connected
to the predicate than objects.
The paths in our third group go from targets to
sentiment words within the predicate. These in-
clude paths from nominal direct object target to
verbal predicates, subst:verb DOBJ up, and from
various kinds of adverbials to verbal predicates,
etc. We assume that predicate-internal paths are
less good indicators of sentiment than the above
groups, as such paths do not constitute a proposi-
tion. Also, arguments within the predicate usually
do not represent intentional agents. Such paths
will get the score 1/3.
Our fourth and final group of dependency paths
contains paths internal to the nominal phrase,
such as from target nouns to attributive adjec-
tives, subst:adj ATR dn, and from target comple-
ments of attributive prepositions to target nouns,
subst:subst PUTFYLL up ATR up. A posi-
tively or negatively qualified noun will probably
often represent the sentiment of the speaker. At
the same time, a nominal phrase of this kind can be
used in many different contexts where the holder
of the sentiment is not the speaker. We assign 2/3
as the maximum score. Table 1 summarizes the
groups of dependency paths.
</bodyText>
<table confidence="0.9943226">
Path group Number Score range
Subj. to pred. 9 1
Subj. to pred.-internal 13 1/3 − 1
Pred.-internal 6 1/3
NP-internal 14 1/3 − 2/3
</table>
<tableCaption confidence="0.998715">
Table 1: Grouping of dependency paths with the
</tableCaption>
<bodyText confidence="0.9728038">
number of paths and score range for each group.
Modified scoring function Let D denote the
set of all salient dependency paths. The func-
tion gram(twi, swj) returns the dependency path,
and if gram(twi, swj) ∈ D, then the function
Wdep(twi, swj) ∈ [0, 1], returns the salience
score of the path. Further let depdist(twi, swj)
return the dependency distance, as described
above. The imp function is computed as follows.
If gram(twi, swj) ∈ D we use
</bodyText>
<equation confidence="0.9774644">
imp(twi, swj) =
α · sentsc(swj)Wdep(twi, swj) (3)
sentsc(swj)
+(1 − α) ·
depdist(twi, swj)
</equation>
<bodyText confidence="0.99883">
where α ∈ [0, 1] is a parameter that weights the
score from the salient dependency path and the
</bodyText>
<page confidence="0.980433">
92
</page>
<bodyText confidence="0.991788">
tree distance and can be tuned using a training set.
If gram(twi7 swj) ∈� D we simply use
</bodyText>
<equation confidence="0.9957165">
sentsc(swj)
imp(twi7 swj) = depdist(twi7 swj) (4)
</equation>
<bodyText confidence="0.962513">
Note that when α = 0, (3) reduces to (4).
</bodyText>
<sectionHeader confidence="0.990346" genericHeader="method">
4 Linguistic resources
</sectionHeader>
<subsectionHeader confidence="0.992973">
4.1 Sentiment corpus
</subsectionHeader>
<bodyText confidence="0.999965463414634">
We did not find any suitable annotated text ma-
terial related to political discussions in Norwe-
gian and therefore created our own. We manu-
ally selected 46 debate articles from the Norwe-
gian online newspapers NRK Ytring, Dagbladet,
Aftenposten, VG and Bergens Tidene. To each de-
bate article there were attached a discussion thread
where readers could express their opinions and
feelings towards the content of the debate arti-
cle. All the text from the debate articles and the
subsequent discussions were collected using text
scraping (Hammer et al., 2013). The debate arti-
cles were related to religion and immigration and
we wanted to classify the sentiment towards all
forms of the following target words: islam, mus-
lim, quran, allah, muhammed, imam and mosque.
These represent topics that typically create a lot of
active discussions and disagreements.
We automatically divided the material into sen-
tences and all sentences containing at least one tar-
get word and one sentiment word were kept for
further analysis. If a sentence contained more than
one target word, e.g. both Islam and Quran, the
sentence was repeated one time for each target
word in the final text material. We could then clas-
sify the sentiment towards each of the target words
in the sentence consecutively. To assure that we do
not underestimate the uncertainty in the statistical
analysis, we see each repetition of the sentence as
the same sentence with respect to the sentence ran-
dom effect in the regression model in Section 5.1
Each sentence was manually annotated as to
whether the commenter was positive, negative
or neutral towards the target word in the sen-
tence. Each sentence was evaluated individually.
The sentences were annotated based on real-world
knowledge, e.g. a sentence like “Muhammed is
like Hitler” would be annotated as a negative sen-
timent towards Muhammed. Further, if a com-
menter presented a negative fact about the target
word, the sentence would be denoted as negative.
</bodyText>
<table confidence="0.996736666666667">
Negative Neutral Positive
Training 174 (46%) 162 (42%) 46 (12%)
Test 102 (33%) 182 (59%) 24 (8%)
</table>
<tableCaption confidence="0.852758">
Table 2: Manual annotation of training and test
set.
</tableCaption>
<bodyText confidence="0.99968325">
In order to assess inter-annotator agreement, a
random sample of 65 sentences from the original
text material was annotated by a second annota-
tor. These sentences were not included in either
the training or test set. For these sentences, the
two annotators agreed on 58, which is an 89%
agreement, with a 95% confidence interval equal
to (79%, 95%) assuming that each sentence is in-
dependent. Since the sentences are drawn ran-
domly from the population of all sentences this is
a fair assumption.
Finally the material was divided into two parts
where the first half of the debate articles with sub-
sequent discussions make up the training set and
the rest constitutes a held-out test set. In the man-
ual development of the salient dependency paths,
only the training set was used. After the division,
the training and test set consisted of a total of 382
and 308 sentences, respectively. Table 4.1 summa-
rizes the annotation found in the corpus.
</bodyText>
<subsectionHeader confidence="0.998401">
4.2 Corpus postprocessing
</subsectionHeader>
<bodyText confidence="0.9999514">
The sentiment corpus was PoS-tagged and parsed
using the Bohnet&amp;Nivre-parser (Bohnet and
Nivre, 2012). This parser is a transition-based
dependency parser with joint tagger that imple-
ments global learning and a beam search for non-
projective labeled dependency parsing. This lat-
ter parser has recently outperformed pipeline sys-
tems (such as the Malt and MST parsers) both in
terms of tagging and parsing accuracy for typolog-
ically diverse languages such as Chinese, English,
and German. It has been reported to obtain a la-
beled accuracy of 87.7 for Norwegian (Solberg et
al., 2014). The parser is trained on the Norwe-
gian Dependency Treebank (NDT). The NDT is a
treebank created at the National Library of Nor-
way in the period 2011-2013, manually annotated
with part-of-speech tags, morphological features,
syntactic functions and dependency graphs (Sol-
berg et al., 2014; Solberg, 2013). It consists of
approximately 600 000 tokens, equally distributed
</bodyText>
<page confidence="0.997223">
93
</page>
<bodyText confidence="0.9998544">
between Norwegian Bokm˚al and Nynorsk, the two
Norwegian written standards. Only the Bokm˚al
subcorpus has been used here. Detailed annota-
tion guidelines in English will be made available
in April 2014 (Kinn et al., 2014).
</bodyText>
<subsectionHeader confidence="0.999602">
4.3 Sentiment lexicon and sentiment shifters
</subsectionHeader>
<bodyText confidence="0.9999518">
Unfortunately, no sentiment lexicon existed for the
Norwegian language and therefore we developed
our own by manually translating the AFINN list
(Nielsen, 2011). We also manually added 1590
words relevant to political discussions like ’de-
port’, ’expel’, ’extremist’ and ’terrorist’, ending
up with a list of 4067 Norwegian sentiment words.
Each word were given a score from −5 to 5 rang-
ing from words with extremely negative sentiment
(e.g. ’behead’) to highly positive sentiment words
(e.g. ’breathtaking’).
Several Norwegian sentiment shifters were con-
sidered but only the basic shifter ’not’ improved
the sentiment classification and therefore only this
word was used in the method.
</bodyText>
<sectionHeader confidence="0.998501" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9995865">
In this study we compare four different methods
based on the general algorithm in (1).
</bodyText>
<listItem confidence="0.995410785714286">
• We use the imp-function presented in (2). We
denote this method WD (word distance).
• For this method and the two below we use the
imp-function in (3). Further we set α = 0
which means that we do not use the salient
dependency paths. We denote this method A0
(α = 0).
• We set α = 1 and for all dependency paths
we set Wdep = 2/3. We denote this method
CW (constant weights).
• We set α = 1 and for Wdep we use the
weights presented in Table 1. We denote
this method OD (optimal use of dependency
paths)
</listItem>
<bodyText confidence="0.99385275">
For each method we used the training set to man-
ually tune the parameters dp, dn, tp and tn of the
method. The parameters were tuned to optimize
the number of correct classifications.
</bodyText>
<subsectionHeader confidence="0.895231">
5.1 Statistical analysis of classification
performance
</subsectionHeader>
<bodyText confidence="0.9953415">
We compare the classification performance of
a set of M different methods, denoted as
</bodyText>
<table confidence="0.9974118">
dp dn tp tn Accuracy p-val
WD 2 0 0.7 0.0 47%
A0 2 0 2.0 0.3 52% 0.023
CW 2 0 2.0 0.3 52% 0.024
OD 2 0 2.0 0.3 53% 0.016
</table>
<tableCaption confidence="0.999585">
Table 3: The second to the fifth column show
</tableCaption>
<bodyText confidence="0.994046428571429">
the optimal values of the parameters of the model
tuned using the training set. The sixth column
show the number of correct classifications and the
last column shows p-values testing whether the
method performs better than WD.
111,112, ... , 11M, using random effect logistic re-
gression. Let the stochastic variable Ytm E
</bodyText>
<listItem confidence="0.561566">
10, 1} represents whether method 11m, m E
</listItem>
<bodyText confidence="0.954183294117647">
11, 2, ... , M} classified the correct sentiment to
sentence number t E 11, 2, ... , T}, where T is
the number of sentences in the test set. We let
Ytm be the dependent variable of the regression
model. The different methods 111,112, ... , 11M is
included as a categorical independent variable in
the regression model. We also assume that classi-
fication performance of the different methods de-
pends on the sentence to be classified, thus the sen-
tence number is included as a random effect. Fit-
ting the model to the observed classification per-
formance of the different methods we are able to
see if the probability of classifying correctly sig-
nificantly vary between the methods.
The statistical analysis is performed using the
statistical program R (R Core Team, 2013) and the
R package lme4 (Bates et al., 2013).
</bodyText>
<subsectionHeader confidence="0.628251">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999992285714286">
Table 3 shows the optimal parameter values of
dp, dn, tp and tn tuned using the training set, and
classification performance for the different meth-
ods on the test set using the parameter values tuned
from the training set. The p-values are computed
using the regression model presented in Section
5.1. We see that dn = 0, meaning that the sen-
timent shifter ’not’ only has a positive effect on
the classification performance when it is in front
of the sentiment word. We see that using depen-
dency distances (method A0) the classification re-
sults are significantly improved compared to us-
ing word distances in the sentence (method WD)
(p-value = 0.023). Also classification based on
</bodyText>
<page confidence="0.997829">
94
</page>
<bodyText confidence="0.9998662">
salient dependency paths (method OD) performs
significantly better than WD. We also see that OD
performs better than A0 (162 correct compared to
161), but this improvement is not statistically sig-
nificant.
</bodyText>
<sectionHeader confidence="0.981109" genericHeader="conclusions">
6 Closing remarks
</sectionHeader>
<bodyText confidence="0.99971975">
Classifying sentiment in political discussions is
hard because of the frequent use of irony, sar-
casm and citations. In this paper we have com-
pared the use of word distance between target
word and sentiment word against metrics incor-
porating syntactic information. Our results show
that using dependency tree distances or salient de-
pendency paths, improves the classification per-
formance compared to using word distance.
Manually selecting salient dependency paths for
the aim of sentiment analysis is a hard task. A nat-
ural further step of our analysis is to expand the
training and test material and use machine learn-
ing to see if there exists dependency paths that im-
prove results compared to using dependency dis-
tance.
</bodyText>
<sectionHeader confidence="0.998079" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999552307692308">
Jamie Bartlett, Jonathan Birdwell, and Mark Littler.
2013. The rise of populism in Europe can be
traced through online behaviour... Demos, http:
//www.demos.co.uk/files/Demos_
OSIPOP_Book-web_03.pdf?1320601634.
[Online; accessed 21-January-2014].
Douglas Bates, Martin Maechler, and Ben Bolker,
2013. lme4: Linear mixed-effects models using S4
classes. R package version 0.999999-2.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1455–1465.
Association for Computational Linguistics.
Bi Chen, Leilei Zhu, Daniel Kifer, and Dongwon Lee.
2010. What Is an Opinion About? Exploring Polit-
ical Standpoints Using Opinion Scoring Model. In
AAAI.
E. W. Dijkstra. 1959. A note on two problems in
connexion with graphs. Numerische Mathematik,
1:269–271.
Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A
Holistic Lexicon-based Approach to Opinion Min-
ing. In Proceedings of the 2008 International Con-
ference on Web Search and Data Mining, WSDM
’08, pages 231–240, New York, NY, USA. ACM.
Matthew Goodwin, Vidhya Ramalingam, and
Rachel Briggs. 2013. The New Radical
Right: Violent and Non-Violent Movements
in Europe. Institute for Strategic Dialogue,
http://www.strategicdialogue.org/
ISD%20Far%20Right%20Feb2012.pdf.
[Online; accessed 21-January-2014].
Hugo Hammer, Alfred Bratterud, and Siri Fagernes.
2013. Crawling Javascript websites using WebKit
with application to analysis of hate speech in online
discussions. In Norwegian informatics conference.
Irene Heim and Angelika Kratzer. 1998. Semantics in
Generative Grammar. Blackwell.
Joan B. Hooper and Sandra A. Thompson. 1973. On
the Applicability of Root Transformations. Linguis-
tic Inquiry, 4(4):465–497.
Minqing Hu and Bing Liu. 2004. Mining and Sum-
marizing Customer Reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD ’04,
pages 168–177, New York, NY, USA. ACM.
Sasha Issenberg. 2012. How President Obamas
campaign used big data to rally individual vot-
ers. http://www.technologyreview.
com/featuredstory/509026/
how-obamas-team-used-big-data-to-rally-voters
[Online; accessed 21-March-2014].
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and
Tiejun Zhao. 2011. Target-dependent Twitter Senti-
ment Classification. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ’11, pages 151–160, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Thorsten Joachims. 1999. Making large-scale SVM
Learning Practical. In Advances in Kernel Methods.
Kari Kinn, Pl Kristian Eriksen, and Per Erik Solberg.
2014. NDT Guidelines for Morphological and Syn-
tactic Annotation. Technical report, National Li-
brary of Norway.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan &amp; Claypool Publishers.
Finn ˚Arup Nielsen. 2011. Anew ANEW: Evaluation
of a word list for sentiment analysis in microblogs.
CoRR, abs/1103.2903.
R Core Team, 2013. R: A Language and Environment
for Statistical Computing. R Foundation for Statis-
tical Computing, Vienna, Austria.
John Shawe-Taylor and Nello Cristianini. 2000. Sup-
port Vector Machines. Cambridge University Press.
</reference>
<page confidence="0.980484">
95
</page>
<reference confidence="0.999768052631579">
Per Erik Solberg, Arne Skjærholt, Lilja Øvrelid, Kristin
Hagen, and Janne Bondi Johannessen. 2014. The
Norwegian Dependency Treebank. In Proceedings
of LREC 2014. Accepted.
Per Erik Solberg. 2013. Building Gold-Standard Tree-
banks for Norwegian. In Proceedings of NODAL-
IDA 2013, Linkping Electronic Conference Proceed-
ings no. 85, pages 459–464, Linkping, Sweden. LiU
Electronic Press.
Andranik Tumasjan, Timm O Sprenger, Philipp G
Sandner, and Isabell M Welpe. 2010. Predicting
elections with twitter: What 140 characters reveal
about political sentiment. In Proceedings of the
fourth international aaai conference on weblogs and
social media, pages 178–185.
Theresa Wilson, Janyce Wiebe, and Paul Hoffman.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analy-
sis. Computational Linguistics, 35(3):399 – 433.
</reference>
<page confidence="0.99846">
96
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.679491">
<title confidence="0.9928715">Sentiment classification of online political discussions: a comparison of word-based and dependency-based method</title>
<author confidence="0.999683">Hugo Lewi Hammer Per Erik Solberg Lilja Øvrelid</author>
<affiliation confidence="0.971522333333333">Oslo and Akershus Spr˚akbanken Department of Informatics University College The National Library University of Oslo of Computer Science of Norway</affiliation>
<email confidence="0.720142">hugo.hammer@hioa.nop.e.solberg@ifikk.uio.no</email>
<abstract confidence="0.9996346">Online political discussions have received a lot of attention over the past years. In this paper we compare two sentiment lexicon approaches to classify the sentiment of sentences from political discussions. The first approach is based on applying the number of words between the target and the sentiment words to weight the sentence sentiment score. The second approach is based on using the shortest paths between target and sentiment words in a dependency graph and linguistically motivated syntactic patterns expressed as dependency paths. The methods are tested on a corpus of sentences from online Norwegian political discussions. The results show that the method based on dependency graphs performs significantly better than the word-based approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jamie Bartlett</author>
<author>Jonathan Birdwell</author>
<author>Mark Littler</author>
</authors>
<title>The rise of populism</title>
<date>2013</date>
<note>in Europe can be traced through online behaviour... Demos, http: //www.demos.co.uk/files/Demos_ OSIPOP_Book-web_03.pdf?1320601634. [Online; accessed 21-January-2014].</note>
<contexts>
<context position="1680" citStr="Bartlett et al., 2013" startWordPosition="248" endWordPosition="251">hod based on dependency graphs performs significantly better than the word-based approach. 1 Introduction Over the past years online political discussions have received a lot of attention. E.g. the Obama 2012 election team initiated an extensive use of text analytics and machine learning techniques towards online material to guide advertising campaigns, identifying key voters, and improve fundraising (Issenberg, 2012). There has also been a lot of concern about the alarming growth in hate and racism against minorities like Muslims, Jews and Gypsies in online discussions (Goodwin et al., 2013; Bartlett et al., 2013). Sentiment analysis (SA) is the discipline of automatically determining sentiment in text material and may be one important tool in understanding the diversity of opinions on the Internet. In this paper we focus on classifying the sentiment towards religious/political topics, say the Quran, in Norwegian political discussion. We use a lexicon-based approach where we classify the sentiment of a sentence based on the polarity of sentiment words in relation to a set of target words in the sentence. We expect that statistically the importance of a sentiment word towards the target word is related </context>
</contexts>
<marker>Bartlett, Birdwell, Littler, 2013</marker>
<rawString>Jamie Bartlett, Jonathan Birdwell, and Mark Littler. 2013. The rise of populism in Europe can be traced through online behaviour... Demos, http: //www.demos.co.uk/files/Demos_ OSIPOP_Book-web_03.pdf?1320601634. [Online; accessed 21-January-2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Bates</author>
<author>Martin Maechler</author>
<author>Ben Bolker</author>
</authors>
<title>lme4: Linear mixed-effects models using S4 classes. R package version 0.999999-2.</title>
<date>2013</date>
<contexts>
<context position="20164" citStr="Bates et al., 2013" startWordPosition="3378" endWordPosition="3381">on model. The different methods 111,112, ... , 11M is included as a categorical independent variable in the regression model. We also assume that classification performance of the different methods depends on the sentence to be classified, thus the sentence number is included as a random effect. Fitting the model to the observed classification performance of the different methods we are able to see if the probability of classifying correctly significantly vary between the methods. The statistical analysis is performed using the statistical program R (R Core Team, 2013) and the R package lme4 (Bates et al., 2013). 5.2 Results Table 3 shows the optimal parameter values of dp, dn, tp and tn tuned using the training set, and classification performance for the different methods on the test set using the parameter values tuned from the training set. The p-values are computed using the regression model presented in Section 5.1. We see that dn = 0, meaning that the sentiment shifter ’not’ only has a positive effect on the classification performance when it is in front of the sentiment word. We see that using dependency distances (method A0) the classification results are significantly improved compared to us</context>
</contexts>
<marker>Bates, Maechler, Bolker, 2013</marker>
<rawString>Douglas Bates, Martin Maechler, and Ben Bolker, 2013. lme4: Linear mixed-effects models using S4 classes. R package version 0.999999-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1455--1465</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16083" citStr="Bohnet and Nivre, 2012" startWordPosition="2677" endWordPosition="2680">ation of all sentences this is a fair assumption. Finally the material was divided into two parts where the first half of the debate articles with subsequent discussions make up the training set and the rest constitutes a held-out test set. In the manual development of the salient dependency paths, only the training set was used. After the division, the training and test set consisted of a total of 382 and 308 sentences, respectively. Table 4.1 summarizes the annotation found in the corpus. 4.2 Corpus postprocessing The sentiment corpus was PoS-tagged and parsed using the Bohnet&amp;Nivre-parser (Bohnet and Nivre, 2012). This parser is a transition-based dependency parser with joint tagger that implements global learning and a beam search for nonprojective labeled dependency parsing. This latter parser has recently outperformed pipeline systems (such as the Malt and MST parsers) both in terms of tagging and parsing accuracy for typologically diverse languages such as Chinese, English, and German. It has been reported to obtain a labeled accuracy of 87.7 for Norwegian (Solberg et al., 2014). The parser is trained on the Norwegian Dependency Treebank (NDT). The NDT is a treebank created at the National Library</context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1455–1465. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bi Chen</author>
<author>Leilei Zhu</author>
<author>Daniel Kifer</author>
<author>Dongwon Lee</author>
</authors>
<title>What Is an Opinion About? Exploring Political Standpoints Using Opinion Scoring Model.</title>
<date>2010</date>
<booktitle>In AAAI.</booktitle>
<contexts>
<context position="4467" citStr="Chen et al. (2010)" startWordPosition="695" endWordPosition="698">ller documents like tweets, comments 90 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 90–96, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics or sentences. See Liu (2012), chapters three to five and references therein for the description of several opinion classification methods. SA has mostly been used to analyze opinions in comments and reviews about commercial products, but there are also examples of SA towards political tweets and discussions, see e.g. Tumasjan et al. (2010); Chen et al. (2010). SA of political discussions is known to be a difficult task since citations, irony and sarcasm is very common (Liu, 2012). 3 Proposed SA methods In this section we present two methods to classify sentences as either positive, neutral or negative towards a target word. Both methods follow the same general algorithm presented below which is inspired by Ding et al. (2008) and is based on a list of sentiment words each associated with a sentiment score representing the polarity and strength of the sentiment word (sentiment lexicon). Both target words, sentiment words and sentiment shifters can i</context>
</contexts>
<marker>Chen, Zhu, Kifer, Lee, 2010</marker>
<rawString>Bi Chen, Leilei Zhu, Daniel Kifer, and Dongwon Lee. 2010. What Is an Opinion About? Exploring Political Standpoints Using Opinion Scoring Model. In AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E W Dijkstra</author>
</authors>
<title>A note on two problems in connexion with graphs.</title>
<date>1959</date>
<journal>Numerische Mathematik,</journal>
<pages>1--269</pages>
<contexts>
<context position="7873" citStr="Dijkstra, 1959" startWordPosition="1291" endWordPosition="1292">unt the syntactic environment of the target words. The function is defined over dependency graphs, i.e. connected, acyclic graphs expressing bilexical relations. Dependency distance One way of expressing the syntactic environment of a target word with respect to a sentiment word is to determine its distance in the dependency graph. We therefore define a distance function depdist(twi, swj) which returns the number of nodes in the shortest dependency path from the target word to the sentiment word in the dependency graph. The shortest path is determined using Dijkstra’s shortest path algorithm (Dijkstra, 1959). Dependency paths A second way of determining the importance of a sentiment word towards a target based on syntactically parsed texts, is to establish a list of grammatical dependency paths between words, and test whether such paths exist between the targets and sentiment words (Jiang et al., 2011). The assumption would be that two words most likely are semantically related to each other if there is a meaningful grammatical relation 1 S = I I i=1 J j=1 91 between them. Furthermore, it is reasonable to expect than some paths are stronger indicators of the overall sentiment of the sentence than</context>
</contexts>
<marker>Dijkstra, 1959</marker>
<rawString>E. W. Dijkstra. 1959. A note on two problems in connexion with graphs. Numerische Mathematik, 1:269–271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaowen Ding</author>
<author>Bing Liu</author>
<author>Philip S Yu</author>
</authors>
<title>A Holistic Lexicon-based Approach to Opinion Mining.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08,</booktitle>
<pages>231--240</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2375" citStr="Ding et al. (2008)" startWordPosition="365" endWordPosition="368">iment in text material and may be one important tool in understanding the diversity of opinions on the Internet. In this paper we focus on classifying the sentiment towards religious/political topics, say the Quran, in Norwegian political discussion. We use a lexicon-based approach where we classify the sentiment of a sentence based on the polarity of sentiment words in relation to a set of target words in the sentence. We expect that statistically the importance of a sentiment word towards the target word is related to the number of words between the sentiment and target word as suggested by Ding et al. (2008). Information about the syntactic environment of certain words or phrases has in previous work also been shown to be useful for the task of sentiment classification (Wilson et al., 2009; Jiang et al., 2011). In this work we therefore compare the results obtained using a token-based distance measure with a novel syntax-based distance measure obtained using dependency graphs and further augmented with linguistically motivated syntactic patterns expressed as dependency paths. In order to evaluate the proposed methods, we furthermore present a freely available corpus of Norwegian political discuss</context>
<context position="4840" citStr="Ding et al. (2008)" startWordPosition="761" endWordPosition="764">lassification methods. SA has mostly been used to analyze opinions in comments and reviews about commercial products, but there are also examples of SA towards political tweets and discussions, see e.g. Tumasjan et al. (2010); Chen et al. (2010). SA of political discussions is known to be a difficult task since citations, irony and sarcasm is very common (Liu, 2012). 3 Proposed SA methods In this section we present two methods to classify sentences as either positive, neutral or negative towards a target word. Both methods follow the same general algorithm presented below which is inspired by Ding et al. (2008) and is based on a list of sentiment words each associated with a sentiment score representing the polarity and strength of the sentiment word (sentiment lexicon). Both target words, sentiment words and sentiment shifters can in general appear several times in a sentence. Sentiment shifters are words that potentially shift the sentiment of a sentence from positive to negative or negative to positive. E.g. “not happy” have the opposite polarity than just “happy”. Let twi, i E 11, 2, ... , I} represent appearance number i of the target word in the sentence. Note that we only consider one target </context>
</contexts>
<marker>Ding, Liu, Yu, 2008</marker>
<rawString>Xiaowen Ding, Bing Liu, and Philip S. Yu. 2008. A Holistic Lexicon-based Approach to Opinion Mining. In Proceedings of the 2008 International Conference on Web Search and Data Mining, WSDM ’08, pages 231–240, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Matthew Goodwin</author>
</authors>
<title>Vidhya Ramalingam,</title>
<location>and</location>
<marker>Goodwin, </marker>
<rawString>Matthew Goodwin, Vidhya Ramalingam, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachel Briggs</author>
</authors>
<title>The New Radical Right: Violent and Non-Violent Movements</title>
<date>2013</date>
<booktitle>in Europe. Institute for Strategic Dialogue, http://www.strategicdialogue.org/ ISD%20Far%20Right%20Feb2012.pdf. [Online; accessed</booktitle>
<pages>21--2014</pages>
<marker>Briggs, 2013</marker>
<rawString>Rachel Briggs. 2013. The New Radical Right: Violent and Non-Violent Movements in Europe. Institute for Strategic Dialogue, http://www.strategicdialogue.org/ ISD%20Far%20Right%20Feb2012.pdf. [Online; accessed 21-January-2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hammer</author>
<author>Alfred Bratterud</author>
<author>Siri Fagernes</author>
</authors>
<title>Crawling Javascript websites using WebKit with application to analysis of hate speech in online discussions.</title>
<date>2013</date>
<booktitle>In Norwegian informatics conference.</booktitle>
<contexts>
<context position="13447" citStr="Hammer et al., 2013" startWordPosition="2240" endWordPosition="2243">0, (3) reduces to (4). 4 Linguistic resources 4.1 Sentiment corpus We did not find any suitable annotated text material related to political discussions in Norwegian and therefore created our own. We manually selected 46 debate articles from the Norwegian online newspapers NRK Ytring, Dagbladet, Aftenposten, VG and Bergens Tidene. To each debate article there were attached a discussion thread where readers could express their opinions and feelings towards the content of the debate article. All the text from the debate articles and the subsequent discussions were collected using text scraping (Hammer et al., 2013). The debate articles were related to religion and immigration and we wanted to classify the sentiment towards all forms of the following target words: islam, muslim, quran, allah, muhammed, imam and mosque. These represent topics that typically create a lot of active discussions and disagreements. We automatically divided the material into sentences and all sentences containing at least one target word and one sentiment word were kept for further analysis. If a sentence contained more than one target word, e.g. both Islam and Quran, the sentence was repeated one time for each target word in t</context>
</contexts>
<marker>Hammer, Bratterud, Fagernes, 2013</marker>
<rawString>Hugo Hammer, Alfred Bratterud, and Siri Fagernes. 2013. Crawling Javascript websites using WebKit with application to analysis of hate speech in online discussions. In Norwegian informatics conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Heim</author>
<author>Angelika Kratzer</author>
</authors>
<title>Semantics in Generative Grammar.</title>
<date>1998</date>
<publisher>Blackwell.</publisher>
<marker>Heim, Kratzer, 1998</marker>
<rawString>Irene Heim and Angelika Kratzer. 1998. Semantics in Generative Grammar. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan B Hooper</author>
<author>Sandra A Thompson</author>
</authors>
<title>On the Applicability of Root Transformations.</title>
<date>1973</date>
<journal>Linguistic Inquiry,</journal>
<volume>4</volume>
<issue>4</issue>
<marker>Hooper, Thompson, 1973</marker>
<rawString>Joan B. Hooper and Sandra A. Thompson. 1973. On the Applicability of Root Transformations. Linguistic Inquiry, 4(4):465–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and Summarizing Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="3762" citStr="Hu and Liu (2004)" startWordPosition="585" endWordPosition="588">timent lexicon. 2 Previous work Sentiment classification aims to classify a document or sentence as either positive or negative and sometimes also neutral. There are mainly two approaches, one based on machine learning and one based on using a list of words with given sentiment scores (lexicon-based approach). For machine learning any existing method can be used, e.g. naive Bayes and support vector machine, (Joachims, 1999; Shawe-Taylor and Cristianini, 2000). One simple lexicon-based approach is to count the number of words with positive and negative sentiment in the document as suggested by Hu and Liu (2004). One may classify the opinion of larger documents like movie or product reviews or smaller documents like tweets, comments 90 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 90–96, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics or sentences. See Liu (2012), chapters three to five and references therein for the description of several opinion classification methods. SA has mostly been used to analyze opinions in comments and reviews about commercial products, but there are also exa</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and Summarizing Customer Reviews. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 168–177, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sasha Issenberg</author>
</authors>
<title>How President Obamas campaign used big data to rally individual voters. http://www.technologyreview. com/featuredstory/509026/ how-obamas-team-used-big-data-to-rally-voters [Online; accessed 21-March-2014].</title>
<date>2012</date>
<contexts>
<context position="1479" citStr="Issenberg, 2012" startWordPosition="216" endWordPosition="217">nguistically motivated syntactic patterns expressed as dependency paths. The methods are tested on a corpus of sentences from online Norwegian political discussions. The results show that the method based on dependency graphs performs significantly better than the word-based approach. 1 Introduction Over the past years online political discussions have received a lot of attention. E.g. the Obama 2012 election team initiated an extensive use of text analytics and machine learning techniques towards online material to guide advertising campaigns, identifying key voters, and improve fundraising (Issenberg, 2012). There has also been a lot of concern about the alarming growth in hate and racism against minorities like Muslims, Jews and Gypsies in online discussions (Goodwin et al., 2013; Bartlett et al., 2013). Sentiment analysis (SA) is the discipline of automatically determining sentiment in text material and may be one important tool in understanding the diversity of opinions on the Internet. In this paper we focus on classifying the sentiment towards religious/political topics, say the Quran, in Norwegian political discussion. We use a lexicon-based approach where we classify the sentiment of a se</context>
</contexts>
<marker>Issenberg, 2012</marker>
<rawString>Sasha Issenberg. 2012. How President Obamas campaign used big data to rally individual voters. http://www.technologyreview. com/featuredstory/509026/ how-obamas-team-used-big-data-to-rally-voters [Online; accessed 21-March-2014].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Long Jiang</author>
<author>Mo Yu</author>
<author>Ming Zhou</author>
<author>Xiaohua Liu</author>
<author>Tiejun Zhao</author>
</authors>
<title>Target-dependent Twitter Sentiment Classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>151--160</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2581" citStr="Jiang et al., 2011" startWordPosition="400" endWordPosition="403">the Quran, in Norwegian political discussion. We use a lexicon-based approach where we classify the sentiment of a sentence based on the polarity of sentiment words in relation to a set of target words in the sentence. We expect that statistically the importance of a sentiment word towards the target word is related to the number of words between the sentiment and target word as suggested by Ding et al. (2008). Information about the syntactic environment of certain words or phrases has in previous work also been shown to be useful for the task of sentiment classification (Wilson et al., 2009; Jiang et al., 2011). In this work we therefore compare the results obtained using a token-based distance measure with a novel syntax-based distance measure obtained using dependency graphs and further augmented with linguistically motivated syntactic patterns expressed as dependency paths. In order to evaluate the proposed methods, we furthermore present a freely available corpus of Norwegian political discussion related to religion and immigration, which has been manually annotated for the sentiment expressed towards a set of target words, as well as a manually translated sentiment lexicon. 2 Previous work Sent</context>
<context position="8173" citStr="Jiang et al., 2011" startWordPosition="1338" endWordPosition="1341">its distance in the dependency graph. We therefore define a distance function depdist(twi, swj) which returns the number of nodes in the shortest dependency path from the target word to the sentiment word in the dependency graph. The shortest path is determined using Dijkstra’s shortest path algorithm (Dijkstra, 1959). Dependency paths A second way of determining the importance of a sentiment word towards a target based on syntactically parsed texts, is to establish a list of grammatical dependency paths between words, and test whether such paths exist between the targets and sentiment words (Jiang et al., 2011). The assumption would be that two words most likely are semantically related to each other if there is a meaningful grammatical relation 1 S = I I i=1 J j=1 91 between them. Furthermore, it is reasonable to expect than some paths are stronger indicators of the overall sentiment of the sentence than others. To test this method, we have manually created a list of 42 grammatical dependency paths, divided into four groups, and given them a score from 0 − 1. The higher the score is, the better indicator of sentiment the path is assumed to be. In the following paragraphs, we will briefly present th</context>
</contexts>
<marker>Jiang, Yu, Zhou, Liu, Zhao, 2011</marker>
<rawString>Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent Twitter Sentiment Classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 151–160, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>In Advances in Kernel Methods.</booktitle>
<contexts>
<context position="3571" citStr="Joachims, 1999" startWordPosition="556" endWordPosition="557"> political discussion related to religion and immigration, which has been manually annotated for the sentiment expressed towards a set of target words, as well as a manually translated sentiment lexicon. 2 Previous work Sentiment classification aims to classify a document or sentence as either positive or negative and sometimes also neutral. There are mainly two approaches, one based on machine learning and one based on using a list of words with given sentiment scores (lexicon-based approach). For machine learning any existing method can be used, e.g. naive Bayes and support vector machine, (Joachims, 1999; Shawe-Taylor and Cristianini, 2000). One simple lexicon-based approach is to count the number of words with positive and negative sentiment in the document as suggested by Hu and Liu (2004). One may classify the opinion of larger documents like movie or product reviews or smaller documents like tweets, comments 90 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 90–96, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics or sentences. See Liu (2012), chapters three to five and referenc</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM Learning Practical. In Advances in Kernel Methods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kari Kinn</author>
</authors>
<title>Pl Kristian Eriksen, and Per Erik Solberg.</title>
<date>2014</date>
<tech>Technical report,</tech>
<institution>National Library of Norway.</institution>
<marker>Kinn, 2014</marker>
<rawString>Kari Kinn, Pl Kristian Eriksen, and Per Erik Solberg. 2014. NDT Guidelines for Morphological and Syntactic Annotation. Technical report, National Library of Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies.</title>
<date>2012</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="4134" citStr="Liu (2012)" startWordPosition="642" endWordPosition="643">yes and support vector machine, (Joachims, 1999; Shawe-Taylor and Cristianini, 2000). One simple lexicon-based approach is to count the number of words with positive and negative sentiment in the document as suggested by Hu and Liu (2004). One may classify the opinion of larger documents like movie or product reviews or smaller documents like tweets, comments 90 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 90–96, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics or sentences. See Liu (2012), chapters three to five and references therein for the description of several opinion classification methods. SA has mostly been used to analyze opinions in comments and reviews about commercial products, but there are also examples of SA towards political tweets and discussions, see e.g. Tumasjan et al. (2010); Chen et al. (2010). SA of political discussions is known to be a difficult task since citations, irony and sarcasm is very common (Liu, 2012). 3 Proposed SA methods In this section we present two methods to classify sentences as either positive, neutral or negative towards a target wo</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on Human Language Technologies. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn ˚Arup Nielsen</author>
</authors>
<title>Anew ANEW: Evaluation of a word list for sentiment analysis in microblogs.</title>
<date>2011</date>
<journal>CoRR,</journal>
<pages>1103--2903</pages>
<contexts>
<context position="17365" citStr="Nielsen, 2011" startWordPosition="2879" endWordPosition="2880">speech tags, morphological features, syntactic functions and dependency graphs (Solberg et al., 2014; Solberg, 2013). It consists of approximately 600 000 tokens, equally distributed 93 between Norwegian Bokm˚al and Nynorsk, the two Norwegian written standards. Only the Bokm˚al subcorpus has been used here. Detailed annotation guidelines in English will be made available in April 2014 (Kinn et al., 2014). 4.3 Sentiment lexicon and sentiment shifters Unfortunately, no sentiment lexicon existed for the Norwegian language and therefore we developed our own by manually translating the AFINN list (Nielsen, 2011). We also manually added 1590 words relevant to political discussions like ’deport’, ’expel’, ’extremist’ and ’terrorist’, ending up with a list of 4067 Norwegian sentiment words. Each word were given a score from −5 to 5 ranging from words with extremely negative sentiment (e.g. ’behead’) to highly positive sentiment words (e.g. ’breathtaking’). Several Norwegian sentiment shifters were considered but only the basic shifter ’not’ improved the sentiment classification and therefore only this word was used in the method. 5 Experiments In this study we compare four different methods based on the</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn ˚Arup Nielsen. 2011. Anew ANEW: Evaluation of a word list for sentiment analysis in microblogs. CoRR, abs/1103.2903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Core Team</author>
</authors>
<title>R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing,</title>
<date>2013</date>
<location>Vienna, Austria.</location>
<contexts>
<context position="20120" citStr="Team, 2013" startWordPosition="3371" endWordPosition="3372">e dependent variable of the regression model. The different methods 111,112, ... , 11M is included as a categorical independent variable in the regression model. We also assume that classification performance of the different methods depends on the sentence to be classified, thus the sentence number is included as a random effect. Fitting the model to the observed classification performance of the different methods we are able to see if the probability of classifying correctly significantly vary between the methods. The statistical analysis is performed using the statistical program R (R Core Team, 2013) and the R package lme4 (Bates et al., 2013). 5.2 Results Table 3 shows the optimal parameter values of dp, dn, tp and tn tuned using the training set, and classification performance for the different methods on the test set using the parameter values tuned from the training set. The p-values are computed using the regression model presented in Section 5.1. We see that dn = 0, meaning that the sentiment shifter ’not’ only has a positive effect on the classification performance when it is in front of the sentiment word. We see that using dependency distances (method A0) the classification resul</context>
</contexts>
<marker>Team, 2013</marker>
<rawString>R Core Team, 2013. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Shawe-Taylor</author>
<author>Nello Cristianini</author>
</authors>
<title>Support Vector Machines.</title>
<date>2000</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3608" citStr="Shawe-Taylor and Cristianini, 2000" startWordPosition="558" endWordPosition="561">ssion related to religion and immigration, which has been manually annotated for the sentiment expressed towards a set of target words, as well as a manually translated sentiment lexicon. 2 Previous work Sentiment classification aims to classify a document or sentence as either positive or negative and sometimes also neutral. There are mainly two approaches, one based on machine learning and one based on using a list of words with given sentiment scores (lexicon-based approach). For machine learning any existing method can be used, e.g. naive Bayes and support vector machine, (Joachims, 1999; Shawe-Taylor and Cristianini, 2000). One simple lexicon-based approach is to count the number of words with positive and negative sentiment in the document as suggested by Hu and Liu (2004). One may classify the opinion of larger documents like movie or product reviews or smaller documents like tweets, comments 90 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 90–96, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics or sentences. See Liu (2012), chapters three to five and references therein for the description of sev</context>
</contexts>
<marker>Shawe-Taylor, Cristianini, 2000</marker>
<rawString>John Shawe-Taylor and Nello Cristianini. 2000. Support Vector Machines. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Per Erik Solberg</author>
<author>Arne Skjærholt</author>
<author>Lilja Øvrelid</author>
<author>Kristin Hagen</author>
<author>Janne Bondi Johannessen</author>
</authors>
<title>The Norwegian Dependency Treebank.</title>
<date>2014</date>
<booktitle>In Proceedings of LREC</booktitle>
<publisher>Accepted.</publisher>
<contexts>
<context position="16562" citStr="Solberg et al., 2014" startWordPosition="2756" endWordPosition="2759">in the corpus. 4.2 Corpus postprocessing The sentiment corpus was PoS-tagged and parsed using the Bohnet&amp;Nivre-parser (Bohnet and Nivre, 2012). This parser is a transition-based dependency parser with joint tagger that implements global learning and a beam search for nonprojective labeled dependency parsing. This latter parser has recently outperformed pipeline systems (such as the Malt and MST parsers) both in terms of tagging and parsing accuracy for typologically diverse languages such as Chinese, English, and German. It has been reported to obtain a labeled accuracy of 87.7 for Norwegian (Solberg et al., 2014). The parser is trained on the Norwegian Dependency Treebank (NDT). The NDT is a treebank created at the National Library of Norway in the period 2011-2013, manually annotated with part-of-speech tags, morphological features, syntactic functions and dependency graphs (Solberg et al., 2014; Solberg, 2013). It consists of approximately 600 000 tokens, equally distributed 93 between Norwegian Bokm˚al and Nynorsk, the two Norwegian written standards. Only the Bokm˚al subcorpus has been used here. Detailed annotation guidelines in English will be made available in April 2014 (Kinn et al., 2014). 4.</context>
</contexts>
<marker>Solberg, Skjærholt, Øvrelid, Hagen, Johannessen, 2014</marker>
<rawString>Per Erik Solberg, Arne Skjærholt, Lilja Øvrelid, Kristin Hagen, and Janne Bondi Johannessen. 2014. The Norwegian Dependency Treebank. In Proceedings of LREC 2014. Accepted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Per Erik Solberg</author>
</authors>
<title>Building Gold-Standard Treebanks for Norwegian.</title>
<date>2013</date>
<booktitle>In Proceedings of NODALIDA 2013, Linkping Electronic Conference Proceedings no. 85,</booktitle>
<pages>459--464</pages>
<publisher>LiU Electronic Press.</publisher>
<location>Linkping,</location>
<contexts>
<context position="16867" citStr="Solberg, 2013" startWordPosition="2805" endWordPosition="2806">is latter parser has recently outperformed pipeline systems (such as the Malt and MST parsers) both in terms of tagging and parsing accuracy for typologically diverse languages such as Chinese, English, and German. It has been reported to obtain a labeled accuracy of 87.7 for Norwegian (Solberg et al., 2014). The parser is trained on the Norwegian Dependency Treebank (NDT). The NDT is a treebank created at the National Library of Norway in the period 2011-2013, manually annotated with part-of-speech tags, morphological features, syntactic functions and dependency graphs (Solberg et al., 2014; Solberg, 2013). It consists of approximately 600 000 tokens, equally distributed 93 between Norwegian Bokm˚al and Nynorsk, the two Norwegian written standards. Only the Bokm˚al subcorpus has been used here. Detailed annotation guidelines in English will be made available in April 2014 (Kinn et al., 2014). 4.3 Sentiment lexicon and sentiment shifters Unfortunately, no sentiment lexicon existed for the Norwegian language and therefore we developed our own by manually translating the AFINN list (Nielsen, 2011). We also manually added 1590 words relevant to political discussions like ’deport’, ’expel’, ’extremi</context>
</contexts>
<marker>Solberg, 2013</marker>
<rawString>Per Erik Solberg. 2013. Building Gold-Standard Treebanks for Norwegian. In Proceedings of NODALIDA 2013, Linkping Electronic Conference Proceedings no. 85, pages 459–464, Linkping, Sweden. LiU Electronic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andranik Tumasjan</author>
<author>Timm O Sprenger</author>
<author>Philipp G Sandner</author>
<author>Isabell M Welpe</author>
</authors>
<title>Predicting elections with twitter: What 140 characters reveal about political sentiment.</title>
<date>2010</date>
<booktitle>In Proceedings of the fourth international aaai conference on weblogs and social media,</booktitle>
<pages>178--185</pages>
<contexts>
<context position="4447" citStr="Tumasjan et al. (2010)" startWordPosition="690" endWordPosition="694">r product reviews or smaller documents like tweets, comments 90 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 90–96, Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics or sentences. See Liu (2012), chapters three to five and references therein for the description of several opinion classification methods. SA has mostly been used to analyze opinions in comments and reviews about commercial products, but there are also examples of SA towards political tweets and discussions, see e.g. Tumasjan et al. (2010); Chen et al. (2010). SA of political discussions is known to be a difficult task since citations, irony and sarcasm is very common (Liu, 2012). 3 Proposed SA methods In this section we present two methods to classify sentences as either positive, neutral or negative towards a target word. Both methods follow the same general algorithm presented below which is inspired by Ding et al. (2008) and is based on a list of sentiment words each associated with a sentiment score representing the polarity and strength of the sentiment word (sentiment lexicon). Both target words, sentiment words and sent</context>
</contexts>
<marker>Tumasjan, Sprenger, Sandner, Welpe, 2010</marker>
<rawString>Andranik Tumasjan, Timm O Sprenger, Philipp G Sandner, and Isabell M Welpe. 2010. Predicting elections with twitter: What 140 characters reveal about political sentiment. In Proceedings of the fourth international aaai conference on weblogs and social media, pages 178–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffman</author>
</authors>
<title>Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<pages>433</pages>
<contexts>
<context position="2560" citStr="Wilson et al., 2009" startWordPosition="396" endWordPosition="399">olitical topics, say the Quran, in Norwegian political discussion. We use a lexicon-based approach where we classify the sentiment of a sentence based on the polarity of sentiment words in relation to a set of target words in the sentence. We expect that statistically the importance of a sentiment word towards the target word is related to the number of words between the sentiment and target word as suggested by Ding et al. (2008). Information about the syntactic environment of certain words or phrases has in previous work also been shown to be useful for the task of sentiment classification (Wilson et al., 2009; Jiang et al., 2011). In this work we therefore compare the results obtained using a token-based distance measure with a novel syntax-based distance measure obtained using dependency graphs and further augmented with linguistically motivated syntactic patterns expressed as dependency paths. In order to evaluate the proposed methods, we furthermore present a freely available corpus of Norwegian political discussion related to religion and immigration, which has been manually annotated for the sentiment expressed towards a set of target words, as well as a manually translated sentiment lexicon.</context>
</contexts>
<marker>Wilson, Wiebe, Hoffman, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffman. 2009. Recognizing contextual polarity: An exploration of features for phrase-level sentiment analysis. Computational Linguistics, 35(3):399 – 433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
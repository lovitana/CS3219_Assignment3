<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000328">
<title confidence="0.9972445">
Yandex School of Data Analysis
Russian-English Machine Translation System for WMT14
</title>
<author confidence="0.747649">
Alexey Borisov and Irina Galinskaya
</author>
<affiliation confidence="0.720939">
Yandex School of Data Analysis
</affiliation>
<address confidence="0.425683">
16, Leo Tolstoy street, Moscow, Russia
</address>
<email confidence="0.696688">
{alborisov, galinskaya}@yandex-team.ru
</email>
<sectionHeader confidence="0.991249" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999801176470588">
This paper describes the Yandex School
of Data Analysis Russian-English system
submitted to the ACL 2014 Ninth Work-
shop on Statistical Machine Translation
shared translation task. We start with the
system that we developed last year and in-
vestigate a few methods that were success-
ful at the previous translation task includ-
ing unpruned language model, operation
sequence model and the new reparameter-
ization of IBM Model 2. Next we propose
a {simple yet practical} algorithm to trans-
form Russian sentence into a more easily
translatable form before decoding. The al-
gorithm is based on the linguistic intuition
of native Russian speakers, also fluent in
English.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999963076923077">
The annual shared translation task organized
within the ACL Workshop on Statistical Machine
Translation (WMT) aims to evaluate the state of
the art in machine translation for a variety of lan-
guages. We participate in the Russian to English
translation direction.
The rest of the paper is organized as follows.
Our baseline system as well as the experiments
concerning the methods already discussed in lit-
erature are described in Section 2. In Section 3 we
present an algorithm we use to transform the Rus-
sian sentence before translation. In Section 4 we
discuss the results and conclude.
</bodyText>
<sectionHeader confidence="0.999584" genericHeader="method">
2 Initial System Development
</sectionHeader>
<bodyText confidence="0.998781333333333">
We use all the Russian-English parallel data avail-
able in the constraint track and the Common Crawl
English monolingual corpus.
</bodyText>
<subsectionHeader confidence="0.792313">
2.1 Baseline
</subsectionHeader>
<bodyText confidence="0.9994796">
We use the phrase-based Moses statistical ma-
chine translation system (Koehn et al., 2007) with
mostly default settings and a few changes (Borisov
et al., 2013) made in the following steps.
Data Preprocessing includes filtering out non
Russian-English sentence pairs and correction of
spelling errors.
Phrase Table Smoothing uses Good-Turing
scheme (Foster et al., 2006).
Consensus Decoding selects the translation
with minimum Bayes risk (Kumar and Byrne,
2004).
Handling of Unknown Words comprises incor-
poration of proper names from Wiki Headlines
parallel data provided by CMU1 and translitera-
tion. We improve the transliteration algorithm in
Section 2.4.
Note that unlike last year we do not use word
alignments computed for the lemmatized word
forms.
</bodyText>
<subsectionHeader confidence="0.992737">
2.2 Language Model
</subsectionHeader>
<bodyText confidence="0.997865666666667">
We use 5-gram unpruned language model with
modified Kneser-Ney discount estimated with
KenLM toolkit (Heafield et al., 2013).
</bodyText>
<subsectionHeader confidence="0.999586">
2.3 Word alignment
</subsectionHeader>
<bodyText confidence="0.9998138">
Word alignments are generated using the
fast_align tool (Dyer et al., 2013), which is much
faster than IBM Model 4 from MGIZA++ (Gao
and Vogel, 2008) and outperforms the latter in
terms of BLEU. Results are given in Table 1.
</bodyText>
<subsectionHeader confidence="0.982614">
2.4 Transliteration
</subsectionHeader>
<bodyText confidence="0.991166">
We employ machine transliteration to generate ad-
ditional translation options for out-of-vocabulary
</bodyText>
<footnote confidence="0.992528">
1http://www.statmt.org/wmt14/
wiki-titles.tgz
</footnote>
<page confidence="0.8761">
66
</page>
<affiliation confidence="0.3776255">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 66–70,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</affiliation>
<table confidence="0.9995135">
MGIZA++ fast_align
Run Time 22 h 14 m 2h 49 m
Perplexity
– ru→en 97.00 90.37
– en→ru 209.36 216.71
BLEU
– WMT13 25.27 25.49
– WMT14 31.76 31.92
</table>
<tableCaption confidence="0.999802">
Table 1: Comparison of word alignment tools:
</tableCaption>
<bodyText confidence="0.942977666666667">
MGIZA++ vs. fast_align. fast_align runs ten
times as fast and outperforms the IBM Model 4
from MGIZA++ in terms of BLEU scores.
words. The transformation model we use is a
transfeme based model (Duan and Hsu, 2011),
which is analogous to translation model in phrase-
based machine translation. Transformation units,
or transfemes, are trained with Moses using the
default settings. Decoding is very similar to beam
search. We build a trie from the words in English
monolingual corpus, and search in it, based on the
transformation model.
</bodyText>
<subsectionHeader confidence="0.987319">
2.5 Operation Sequence Model
</subsectionHeader>
<bodyText confidence="0.9998064">
The Operation Sequence N-gram Model (OSM)
(Durrani et al., 2011) integrates reordering opera-
tions and lexical translations into a heterogeneous
sequence of minimal translation units (MTUs) and
learns a Markov model over it. Reordering deci-
sions influence lexical selections and vice versa
thus improving the translation model. We use
OSM as a feature function in phrase-based SMT.
Please, refer to (Durrani et al., 2013) for imple-
mentation details.
</bodyText>
<sectionHeader confidence="0.978531" genericHeader="method">
3 Morphological Transformations
</sectionHeader>
<bodyText confidence="0.999803642857143">
Russian is a fusional synthetic language, mean-
ing that the relations between words are redundant
and encoded inside the words. Adjectives alter
their form to reflect the gender, case, number and
in some cases, animacy of the nouns, resulting in
dozens of different word forms matching a single
English word. An example is given in Table 2.
Verbs in Russian are typically constructed from
the morphemes corresponding to functional words
in English (to, shall, will, was, were, has, have,
had, been, etc.). This Russian phenomenon leads
to two problems: data sparsity and high number of
one-to-many alignments, which both may result in
translation quality degradation.
</bodyText>
<table confidence="0.997632434782609">
Number PL
SG
Case Gender
NOM MASC летний летние
NOM FEM летняя
NOM NEUT летнее
GEN MASC летнего летних
GEN FEM летней
GEN NEUT летнего
DAT MASC летнему летним
DAT FEM летней
DAT NEUT летнему
ACC MASC, AN летнего летним
ACC MASC, INAN летний
ACC FEM летнюю
ACC NEUT летнее
INS MASC летним летним
INS FEM летней
INS FEM летнею
INS NEUT летним
ABL MASC летнем летних
ABL FEM летней
ABL NEUT летнем
</table>
<tableCaption confidence="0.831936">
Table 2: Russian word forms corresponding to the
English word &amp;quot;summer&amp;quot; (adj.).
</tableCaption>
<bodyText confidence="0.9968678">
Hereafter, we propose an algorithm to transform
the original Russian sentence into a more easily
translatable form. The algorithm is based on the
linguistic intuition of native Russian speakers, also
fluent in English.
</bodyText>
<subsectionHeader confidence="0.997331">
3.1 Approach
</subsectionHeader>
<bodyText confidence="0.999285666666667">
Based on the output from Russian morphological
analyzer we rewrite the input sentence based on
the following principles:
</bodyText>
<listItem confidence="0.99980575">
1. the original sentence is restorable
(by a Russian native speaker)
2. redundant information is omitted
3. word alignment is less ambiguous
</listItem>
<subsectionHeader confidence="0.896442">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.999790333333333">
The algorithm consists of two steps.
On the first step we employ in-house Rus-
sian morphological analyzer similar to Mys-
tem (Segalovich, 2003) to convert each word
(WORD) into a tuple containing its canonical form
(LEMMA), part of speech tag (POS) and a set
</bodyText>
<page confidence="0.997649">
67
</page>
<table confidence="0.999837727272727">
Category Abbr. Values
Animacy ANIM AN, INAN
Aspect ASP IMPERF, PERF
Case CASE NOM, GEN, DAT, ACC, INS, ABL
Comparison Type COMP COMP, SURP
Gender GEND MASC, FEM, NEUT
Mood MOOD IND, IMP, COND, SBJV
Number NUM SG, PL
Participle Type PART ACT, PASS
Person PERS PERS1, PERS2, PERS3
Tense TNS PRES, NPST, PST
</table>
<tableCaption confidence="0.997699">
Table 3: Morphological Categories
</tableCaption>
<bodyText confidence="0.999397142857143">
of other grammemes associated with the word
(GRAMMEMES). The tuple is later referred to as
LPG. If the canonical form or part of speech are
ambiguous, we set LEMMA to WORD; POS to
&amp;quot;undefined&amp;quot;; and GRAMMEMES to ∅. Gram-
memes are grouped into grammatical categories
listed in Table 3.
</bodyText>
<equation confidence="0.845336">
WORD −→ LEMMA + POS + GRAMMEMES
</equation>
<bodyText confidence="0.999858625">
On the second step, the LPGs are converted into
tokens that, we hope, will better match English
structure. Some grammemes result in separate to-
kens, others stay with the lemma, and the rest get
dropped. The full set of morphological transfor-
mations we use is given in Table 4.
An example of applying the algorithm to a Rus-
sian sentence is given in Figure 1.
</bodyText>
<subsectionHeader confidence="0.628871">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.999203058823529">
The translation has been improved in several
ways:
Incorrect Use of Tenses happens quite often in
statistical machine translation, which is especially
vexing in simple cases such as asks instead of
asked, explains instead of explain along with more
difficult ones e.g. has increased instead of would
increase. The proposed algorithm achieves con-
siderable improvement, since it explicitly models
tenses and all its relevant properties.
Missing Articles is a common problem of
most Russian-English translation systems, be-
cause there are no articles in Russian. Our model
creates an auxiliary token for each noun, which re-
flects its case and motivates an article.
Use of Simple Vocabulary is not desirable
when the source text is a vocabulary-flourished
</bodyText>
<equation confidence="0.6330315">
neTxHM AxeM
neTxxr3, adj, Aexb, noun,
{inan, dat|ins, ¿, male|neut, sg|pl} {inan, ins, male, sg}
neTxxr3.adj+¿ ins Aexb.n+sg
</equation>
<bodyText confidence="0.454095">
on a summer day
</bodyText>
<figureCaption confidence="0.808974666666667">
Figure 1: An illustration of the proposed algorithm
to transform Russian sentence JIeTHIIM AH6M (let-
nim dnem), meaning on a summer day, into a more
</figureCaption>
<bodyText confidence="0.966699904761905">
easily translatable form. First, for each word we
extract its canonical form, part of speech tag and a
set of associated morphological properties (gram-
memes). Then we apply hand-crafted rules (Ta-
ble 4) to transform them into separate tokens.
one. News are full of academic, bookish, inkhorn,
and other rare words. Phrase Table smoothing
methods discount the translation probabilities for
rare phrase pairs, preventing them from appearing
in English translation, while many of these rare
phrase pairs are correct. The good thing is that the
phrase pairs containing the transformed Russian
words may not be rare themselves, and thereby are
not discounted so heavily. A more effective use of
English vocabulary has been observed on WMT13
test dataset (see Table 5).
We have demonstrated the improvements on a
qualitative level. The quantitative results are sum-
marized in Table 6 (baseline – without morpholog-
ical transformations; proposed – with morpholog-
ical transformations).
</bodyText>
<page confidence="0.999054">
68
</page>
<table confidence="0.999631594594595">
LPG ==&gt;. tokens
LEMMA, adj,
{ANIM, CASE, COMP, GEND, NUM}
4
LEMMA.adj+COMP
LEMMA, noun,
{ANIM, CASE, GEND, NUM}
4
CASE LEMMA.n+NUM
LEMMA, verb (ger), {ASP, TNS}
4
LEMMA.vg+ASP+TNS
LEMMA, verb (inf), {ASP}
4
LEMMA.vi+ASP
LEMMA, verb (part), {PART, ASP, TNS}
4
LEMMA.vp+PART+ASP+TNS
LEMMA, verb (–),
{PART, ASP, MOOD, TENSE,
NUM, PERS}
4
1. TNS={PRES}  |TNS={NPST} &amp; ASP={IMPERF}
a. PERS3 ∈ PERS &amp; SG ∈ NUM
LEMMA.v+pres+MOOD+PERS+NUM
b. otherwise
LEMMA.v+pres+MOOD
2. TNS={PST}
ASP LEMMA.v+pst+MOOD
3. TNS={NPST} &amp; ASP={IMPERF}
fut LEMMA.v+MOOD
4. if ambiguous
LEMMA.v+PART+ASP+MOOD
+TNS+NUM+PERS
LEMMA, OTHER, GRAMMEMES
4
LEMMA.POS+GRAMMEMES
</table>
<tableCaption confidence="0.877798666666667">
Table 4: A set of rules we use to transform
the LPGs (LEMMA, POS, GRAMMEMES), ex-
tracted on the first step, into individual tokens.
</tableCaption>
<sectionHeader confidence="0.993406" genericHeader="discussions">
4 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999319833333333">
We described the Yandex School of Data Anal-
ysis Russian-English system submitted to the
ACL 2014 Ninth Workshop on Statistical Machine
Translation shared translation task. The main con-
tribution of this work is an algorithm to transform
the Russian sentence into a more easily translat-
</bodyText>
<table confidence="0.998326714285714">
Input Translation
pa3aOraaCHR (a) differences
(raznoglasiya) (b) disputes
iipOiiarax,qHCTOM (a) promoter
(propagandistom) (b) propagandist
iipeHMyMeCTSeaaO (a) mainly
(preimuschestvenno) (b) predominantly
</table>
<tableCaption confidence="0.964153">
Table 5: Morphological Transformations lead to
</tableCaption>
<bodyText confidence="0.4238965">
more effective use of English vocabulary. Trans-
lations marked with &amp;quot;a&amp;quot; were produced using the
baseline system; with &amp;quot;b&amp;quot; also use Morphological
Transformations.
</bodyText>
<table confidence="0.999857">
Baseline Proposed
Distinct Words 899,992 564,354
OOV Words
– WMT13 829 590
– WMT14 884 660
Perplexity
– ru—*en 90.37 99.81
– en—*ru 216.71 128.15
BLEU
– WMT13 25.49 25.63
– WMT14 31.92 32.56
</table>
<tableCaption confidence="0.971522">
Table 6: Results of Morphological Transforma-
</tableCaption>
<bodyText confidence="0.999402913043478">
tions. We improved the statistical characteristics
of our models by reducing the number of distinct
words by 37% and managed to translate 25% of
previously untranslated words. BLEU scores were
improved by 0.14 and 0.64 points for WMT13 and
WMT14 test sets respectively.
able form before decoding. Significant improve-
ments in human satisfaction and BLEU scores
have been demonstrated from applying this algo-
rithm.
One limitation of the proposed algorithm is that
it does not take into account the relations between
words sharing the same root. E.g. the word aHCTH-
arRx (aistinyh) meaning stork (adj.) is handled in-
dependently from the word aHCT (aist) meaning
stork (n.). Our system as well as the major online
services (Bing, Google, Yandex) transliterated this
word, but the word aistinyh does not make much
sense to a non-Russian reader. It might be worth-
while to study this problem in more detail.
Another direction for future work is to apply
the proposed algorithm in reverse direction. We
suggest the following two-step procedure. English
</bodyText>
<page confidence="0.997716">
69
</page>
<bodyText confidence="0.999910666666667">
sentence is first translated into Russian∗ (Russian
after applying the morphological transformations),
and at the next step it is translated again with an
auxiliary SMT system trained on the (Russian*,
Russian) parallel corpus created from the Russian
monolingual corpus.
</bodyText>
<sectionHeader confidence="0.99894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999831902777778">
Alexey Borisov, Jacob Dlougach, and Irina Galinskaya.
2013. Yandex school of data analysis machine trans-
lation systems for wmt13. In Proceedings of the
Eighth Workshop on Statistical Machine Translation
(WMT), pages 97–101. Association for Computa-
tional Linguistics.
Huizhong Duan and Bo-June Paul Hsu. 2011. On-
line spelling correction for query completion. In
Proceedings of the 20th international conference on
World Wide Web (WWW), pages 117–126. ACM.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 1045–1054. Association
for Computational Linguistics.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh’s machine trans-
lation systems for european language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation (WMT), pages 112–119. Associa-
tion for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteriza-
tion of IBM model 2. In Proceedings of the Hu-
man Language Technology Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (HLT-NAACL), pages 644–648.
Association for Computational Linguistics.
George Foster, Roland Kuhn, and John Howard John-
son. 2006. Phrasetable smoothing for statistical ma-
chine translation. In Proceedings of the 44th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 53–61. Association for Com-
putational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In Proceedings
of the 46th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 49–57. As-
sociation for Computational Linguistics.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
690–696, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-burch, Richard Zens, Rwth Aachen,
Alexandra Constantin, Marcello Federico, Nicola
Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen,
Christine Moran, and Ondˇrej Bojar. 2007. Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 177–180. Association for Compu-
tational Linguistics.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics (HLT-NAACL), pages 163–171. Association for
Computational Linguistics.
Ilya Segalovich. 2003. A fast morphological algorithm
with unknown word guessing induced by a dictio-
nary for a web search engine. In Hamid R. Arab-
nia and Elena B. Kozerenko, editors, Proceedings of
the International Conference on Machine Learning;
Models, Technologies and Applications (MLMTA),
pages 273–280, Las Vegas, NV, USA, June. CSREA
Press.
</reference>
<page confidence="0.998449">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.228230">
<title confidence="0.8982505">Yandex School of Data Russian-English Machine Translation System for WMT14</title>
<author confidence="0.50611">Alexey Borisov</author>
<author confidence="0.50611">Irina</author>
<affiliation confidence="0.749465">Yandex School of Data</affiliation>
<address confidence="0.96712">16, Leo Tolstoy street, Moscow,</address>
<email confidence="0.997608">alborisov@yandex-team.ru</email>
<email confidence="0.997608">galinskaya@yandex-team.ru</email>
<abstract confidence="0.970920166666666">This paper describes the Yandex School of Data Analysis Russian-English system submitted to the ACL 2014 Ninth Workshop on Statistical Machine Translation shared translation task. We start with the system that we developed last year and investigate a few methods that were successful at the previous translation task including unpruned language model, operation sequence model and the new reparameterization of IBM Model 2. Next we propose a {simple yet practical} algorithm to transform Russian sentence into a more easily translatable form before decoding. The algorithm is based on the linguistic intuition of native Russian speakers, also fluent in English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexey Borisov</author>
<author>Jacob Dlougach</author>
<author>Irina Galinskaya</author>
</authors>
<title>Yandex school of data analysis machine translation systems for wmt13.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>97--101</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1832" citStr="Borisov et al., 2013" startWordPosition="283" endWordPosition="286">d as follows. Our baseline system as well as the experiments concerning the methods already discussed in literature are described in Section 2. In Section 3 we present an algorithm we use to transform the Russian sentence before translation. In Section 4 we discuss the results and conclude. 2 Initial System Development We use all the Russian-English parallel data available in the constraint track and the Common Crawl English monolingual corpus. 2.1 Baseline We use the phrase-based Moses statistical machine translation system (Koehn et al., 2007) with mostly default settings and a few changes (Borisov et al., 2013) made in the following steps. Data Preprocessing includes filtering out non Russian-English sentence pairs and correction of spelling errors. Phrase Table Smoothing uses Good-Turing scheme (Foster et al., 2006). Consensus Decoding selects the translation with minimum Bayes risk (Kumar and Byrne, 2004). Handling of Unknown Words comprises incorporation of proper names from Wiki Headlines parallel data provided by CMU1 and transliteration. We improve the transliteration algorithm in Section 2.4. Note that unlike last year we do not use word alignments computed for the lemmatized word forms. 2.2 </context>
</contexts>
<marker>Borisov, Dlougach, Galinskaya, 2013</marker>
<rawString>Alexey Borisov, Jacob Dlougach, and Irina Galinskaya. 2013. Yandex school of data analysis machine translation systems for wmt13. In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT), pages 97–101. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huizhong Duan</author>
<author>Bo-June Paul Hsu</author>
</authors>
<title>Online spelling correction for query completion.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference on World Wide Web (WWW),</booktitle>
<pages>117--126</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3563" citStr="Duan and Hsu, 2011" startWordPosition="547" endWordPosition="550">www.statmt.org/wmt14/ wiki-titles.tgz 66 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 66–70, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics MGIZA++ fast_align Run Time 22 h 14 m 2h 49 m Perplexity – ru→en 97.00 90.37 – en→ru 209.36 216.71 BLEU – WMT13 25.27 25.49 – WMT14 31.76 31.92 Table 1: Comparison of word alignment tools: MGIZA++ vs. fast_align. fast_align runs ten times as fast and outperforms the IBM Model 4 from MGIZA++ in terms of BLEU scores. words. The transformation model we use is a transfeme based model (Duan and Hsu, 2011), which is analogous to translation model in phrasebased machine translation. Transformation units, or transfemes, are trained with Moses using the default settings. Decoding is very similar to beam search. We build a trie from the words in English monolingual corpus, and search in it, based on the transformation model. 2.5 Operation Sequence Model The Operation Sequence N-gram Model (OSM) (Durrani et al., 2011) integrates reordering operations and lexical translations into a heterogeneous sequence of minimal translation units (MTUs) and learns a Markov model over it. Reordering decisions infl</context>
</contexts>
<marker>Duan, Hsu, 2011</marker>
<rawString>Huizhong Duan and Bo-June Paul Hsu. 2011. Online spelling correction for query completion. In Proceedings of the 20th international conference on World Wide Web (WWW), pages 117–126. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
</authors>
<title>A joint sequence translation model with integrated reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1045--1054</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3978" citStr="Durrani et al., 2011" startWordPosition="611" endWordPosition="614">vs. fast_align. fast_align runs ten times as fast and outperforms the IBM Model 4 from MGIZA++ in terms of BLEU scores. words. The transformation model we use is a transfeme based model (Duan and Hsu, 2011), which is analogous to translation model in phrasebased machine translation. Transformation units, or transfemes, are trained with Moses using the default settings. Decoding is very similar to beam search. We build a trie from the words in English monolingual corpus, and search in it, based on the transformation model. 2.5 Operation Sequence Model The Operation Sequence N-gram Model (OSM) (Durrani et al., 2011) integrates reordering operations and lexical translations into a heterogeneous sequence of minimal translation units (MTUs) and learns a Markov model over it. Reordering decisions influence lexical selections and vice versa thus improving the translation model. We use OSM as a feature function in phrase-based SMT. Please, refer to (Durrani et al., 2013) for implementation details. 3 Morphological Transformations Russian is a fusional synthetic language, meaning that the relations between words are redundant and encoded inside the words. Adjectives alter their form to reflect the gender, case,</context>
</contexts>
<marker>Durrani, Schmid, Fraser, 2011</marker>
<rawString>Nadir Durrani, Helmut Schmid, and Alexander Fraser. 2011. A joint sequence translation model with integrated reordering. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1045–1054. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Barry Haddow</author>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s machine translation systems for european language pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>112--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4334" citStr="Durrani et al., 2013" startWordPosition="665" endWordPosition="668">fault settings. Decoding is very similar to beam search. We build a trie from the words in English monolingual corpus, and search in it, based on the transformation model. 2.5 Operation Sequence Model The Operation Sequence N-gram Model (OSM) (Durrani et al., 2011) integrates reordering operations and lexical translations into a heterogeneous sequence of minimal translation units (MTUs) and learns a Markov model over it. Reordering decisions influence lexical selections and vice versa thus improving the translation model. We use OSM as a feature function in phrase-based SMT. Please, refer to (Durrani et al., 2013) for implementation details. 3 Morphological Transformations Russian is a fusional synthetic language, meaning that the relations between words are redundant and encoded inside the words. Adjectives alter their form to reflect the gender, case, number and in some cases, animacy of the nouns, resulting in dozens of different word forms matching a single English word. An example is given in Table 2. Verbs in Russian are typically constructed from the morphemes corresponding to functional words in English (to, shall, will, was, were, has, have, had, been, etc.). This Russian phenomenon leads to t</context>
</contexts>
<marker>Durrani, Haddow, Heafield, Koehn, 2013</marker>
<rawString>Nadir Durrani, Barry Haddow, Kenneth Heafield, and Philipp Koehn. 2013. Edinburgh’s machine translation systems for european language pairs. In Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT), pages 112–119. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of IBM model 2.</title>
<date>2013</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>644--648</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2667" citStr="Dyer et al., 2013" startWordPosition="407" endWordPosition="410">nsus Decoding selects the translation with minimum Bayes risk (Kumar and Byrne, 2004). Handling of Unknown Words comprises incorporation of proper names from Wiki Headlines parallel data provided by CMU1 and transliteration. We improve the transliteration algorithm in Section 2.4. Note that unlike last year we do not use word alignments computed for the lemmatized word forms. 2.2 Language Model We use 5-gram unpruned language model with modified Kneser-Ney discount estimated with KenLM toolkit (Heafield et al., 2013). 2.3 Word alignment Word alignments are generated using the fast_align tool (Dyer et al., 2013), which is much faster than IBM Model 4 from MGIZA++ (Gao and Vogel, 2008) and outperforms the latter in terms of BLEU. Results are given in Table 1. 2.4 Transliteration We employ machine transliteration to generate additional translation options for out-of-vocabulary 1http://www.statmt.org/wmt14/ wiki-titles.tgz 66 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 66–70, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics MGIZA++ fast_align Run Time 22 h 14 m 2h 49 m Perplexity – ru→en 97.00 90.37 – en→ru 209.36 216.71 BLEU – W</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 644–648. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
<author>John Howard Johnson</author>
</authors>
<title>Phrasetable smoothing for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>53--61</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2042" citStr="Foster et al., 2006" startWordPosition="312" endWordPosition="315">n sentence before translation. In Section 4 we discuss the results and conclude. 2 Initial System Development We use all the Russian-English parallel data available in the constraint track and the Common Crawl English monolingual corpus. 2.1 Baseline We use the phrase-based Moses statistical machine translation system (Koehn et al., 2007) with mostly default settings and a few changes (Borisov et al., 2013) made in the following steps. Data Preprocessing includes filtering out non Russian-English sentence pairs and correction of spelling errors. Phrase Table Smoothing uses Good-Turing scheme (Foster et al., 2006). Consensus Decoding selects the translation with minimum Bayes risk (Kumar and Byrne, 2004). Handling of Unknown Words comprises incorporation of proper names from Wiki Headlines parallel data provided by CMU1 and transliteration. We improve the transliteration algorithm in Section 2.4. Note that unlike last year we do not use word alignments computed for the lemmatized word forms. 2.2 Language Model We use 5-gram unpruned language model with modified Kneser-Ney discount estimated with KenLM toolkit (Heafield et al., 2013). 2.3 Word alignment Word alignments are generated using the fast_align</context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George Foster, Roland Kuhn, and John Howard Johnson. 2006. Phrasetable smoothing for statistical machine translation. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL), pages 53–61. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel implementations of word alignment tool.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>49--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2741" citStr="Gao and Vogel, 2008" startWordPosition="421" endWordPosition="424"> Byrne, 2004). Handling of Unknown Words comprises incorporation of proper names from Wiki Headlines parallel data provided by CMU1 and transliteration. We improve the transliteration algorithm in Section 2.4. Note that unlike last year we do not use word alignments computed for the lemmatized word forms. 2.2 Language Model We use 5-gram unpruned language model with modified Kneser-Ney discount estimated with KenLM toolkit (Heafield et al., 2013). 2.3 Word alignment Word alignments are generated using the fast_align tool (Dyer et al., 2013), which is much faster than IBM Model 4 from MGIZA++ (Gao and Vogel, 2008) and outperforms the latter in terms of BLEU. Results are given in Table 1. 2.4 Transliteration We employ machine transliteration to generate additional translation options for out-of-vocabulary 1http://www.statmt.org/wmt14/ wiki-titles.tgz 66 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 66–70, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics MGIZA++ fast_align Run Time 22 h 14 m 2h 49 m Perplexity – ru→en 97.00 90.37 – en→ru 209.36 216.71 BLEU – WMT13 25.27 25.49 – WMT14 31.76 31.92 Table 1: Comparison of word alignment</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 49–57. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>690--696</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2571" citStr="Heafield et al., 2013" startWordPosition="392" endWordPosition="395">tion of spelling errors. Phrase Table Smoothing uses Good-Turing scheme (Foster et al., 2006). Consensus Decoding selects the translation with minimum Bayes risk (Kumar and Byrne, 2004). Handling of Unknown Words comprises incorporation of proper names from Wiki Headlines parallel data provided by CMU1 and transliteration. We improve the transliteration algorithm in Section 2.4. Note that unlike last year we do not use word alignments computed for the lemmatized word forms. 2.2 Language Model We use 5-gram unpruned language model with modified Kneser-Ney discount estimated with KenLM toolkit (Heafield et al., 2013). 2.3 Word alignment Word alignments are generated using the fast_align tool (Dyer et al., 2013), which is much faster than IBM Model 4 from MGIZA++ (Gao and Vogel, 2008) and outperforms the latter in terms of BLEU. Results are given in Table 1. 2.4 Transliteration We employ machine transliteration to generate additional translation options for out-of-vocabulary 1http://www.statmt.org/wmt14/ wiki-titles.tgz 66 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 66–70, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics MGIZA++ fas</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 690–696, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-burch</author>
<author>Richard Zens</author>
<author>Rwth Aachen</author>
<author>Alexandra Constantin</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Chris Dyer</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Ondˇrej Bojar</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1762" citStr="Koehn et al., 2007" startWordPosition="271" endWordPosition="274"> to English translation direction. The rest of the paper is organized as follows. Our baseline system as well as the experiments concerning the methods already discussed in literature are described in Section 2. In Section 3 we present an algorithm we use to transform the Russian sentence before translation. In Section 4 we discuss the results and conclude. 2 Initial System Development We use all the Russian-English parallel data available in the constraint track and the Common Crawl English monolingual corpus. 2.1 Baseline We use the phrase-based Moses statistical machine translation system (Koehn et al., 2007) with mostly default settings and a few changes (Borisov et al., 2013) made in the following steps. Data Preprocessing includes filtering out non Russian-English sentence pairs and correction of spelling errors. Phrase Table Smoothing uses Good-Turing scheme (Foster et al., 2006). Consensus Decoding selects the translation with minimum Bayes risk (Kumar and Byrne, 2004). Handling of Unknown Words comprises incorporation of proper names from Wiki Headlines parallel data provided by CMU1 and transliteration. We improve the transliteration algorithm in Section 2.4. Note that unlike last year we d</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-burch, Zens, Aachen, Constantin, Federico, Bertoldi, Dyer, Cowan, Shen, Moran, Bojar, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-burch, Richard Zens, Rwth Aachen, Alexandra Constantin, Marcello Federico, Nicola Bertoldi, Chris Dyer, Brooke Cowan, Wade Shen, Christine Moran, and Ondˇrej Bojar. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL), pages 177–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>163--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2134" citStr="Kumar and Byrne, 2004" startWordPosition="325" endWordPosition="328"> System Development We use all the Russian-English parallel data available in the constraint track and the Common Crawl English monolingual corpus. 2.1 Baseline We use the phrase-based Moses statistical machine translation system (Koehn et al., 2007) with mostly default settings and a few changes (Borisov et al., 2013) made in the following steps. Data Preprocessing includes filtering out non Russian-English sentence pairs and correction of spelling errors. Phrase Table Smoothing uses Good-Turing scheme (Foster et al., 2006). Consensus Decoding selects the translation with minimum Bayes risk (Kumar and Byrne, 2004). Handling of Unknown Words comprises incorporation of proper names from Wiki Headlines parallel data provided by CMU1 and transliteration. We improve the transliteration algorithm in Section 2.4. Note that unlike last year we do not use word alignments computed for the lemmatized word forms. 2.2 Language Model We use 5-gram unpruned language model with modified Kneser-Ney discount estimated with KenLM toolkit (Heafield et al., 2013). 2.3 Word alignment Word alignments are generated using the fast_align tool (Dyer et al., 2013), which is much faster than IBM Model 4 from MGIZA++ (Gao and Vogel</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL), pages 163–171. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Segalovich</author>
</authors>
<title>A fast morphological algorithm with unknown word guessing induced by a dictionary for a web search engine.</title>
<date>2003</date>
<booktitle>Proceedings of the International Conference on Machine Learning; Models, Technologies and Applications (MLMTA),</booktitle>
<pages>273--280</pages>
<editor>In Hamid R. Arabnia and Elena B. Kozerenko, editors,</editor>
<publisher>CSREA Press.</publisher>
<location>Las Vegas, NV, USA,</location>
<contexts>
<context position="6185" citStr="Segalovich, 2003" startWordPosition="964" endWordPosition="965">hm to transform the original Russian sentence into a more easily translatable form. The algorithm is based on the linguistic intuition of native Russian speakers, also fluent in English. 3.1 Approach Based on the output from Russian morphological analyzer we rewrite the input sentence based on the following principles: 1. the original sentence is restorable (by a Russian native speaker) 2. redundant information is omitted 3. word alignment is less ambiguous 3.2 Algorithm The algorithm consists of two steps. On the first step we employ in-house Russian morphological analyzer similar to Mystem (Segalovich, 2003) to convert each word (WORD) into a tuple containing its canonical form (LEMMA), part of speech tag (POS) and a set 67 Category Abbr. Values Animacy ANIM AN, INAN Aspect ASP IMPERF, PERF Case CASE NOM, GEN, DAT, ACC, INS, ABL Comparison Type COMP COMP, SURP Gender GEND MASC, FEM, NEUT Mood MOOD IND, IMP, COND, SBJV Number NUM SG, PL Participle Type PART ACT, PASS Person PERS PERS1, PERS2, PERS3 Tense TNS PRES, NPST, PST Table 3: Morphological Categories of other grammemes associated with the word (GRAMMEMES). The tuple is later referred to as LPG. If the canonical form or part of speech are am</context>
</contexts>
<marker>Segalovich, 2003</marker>
<rawString>Ilya Segalovich. 2003. A fast morphological algorithm with unknown word guessing induced by a dictionary for a web search engine. In Hamid R. Arabnia and Elena B. Kozerenko, editors, Proceedings of the International Conference on Machine Learning; Models, Technologies and Applications (MLMTA), pages 273–280, Las Vegas, NV, USA, June. CSREA Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
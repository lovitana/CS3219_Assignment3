<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.99877">
Improving Bilingual Lexicon Extraction Performance from
Comparable Corpora via Optimizing Translation Candidate Lists
</title>
<author confidence="0.99473">
Shaoqi Wang
</author>
<affiliation confidence="0.85477">
University of Science and Technology
of China, Institute of Intelligent Machines
Chinese Academy of Sciences
Hefei, China
</affiliation>
<email confidence="0.874106">
wsq2012@mail.ustc.edu.cn
</email>
<sectionHeader confidence="0.979101" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998384266666667">
In this paper, we propose a novel method
to optimize translation candidate lists
derived from window-based approach for
the task of bilingual lexicon extraction.
The optimizing process consists of two
cross-comparisons between 1th translation
candidate of each target word, and between
set of all the 1th candidates and that of each
word’s 2th to Nth ones. Experiment results
demonstrate that the proposed method
leads to a significant improvement on
accuracy over window-based approach in
bilingual lexicon extraction from both
English-Chinese and Chinese-English
comparable corpora.
</bodyText>
<sectionHeader confidence="0.992423" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962166666667">
Bilingual lexicon is a basic resource in the field
of Natural Language Processing such as
machine translation and cross-language
information retrieval (AbduI-Rauf et al., 2009).
Parallel corpora (Och and Ney, 2000) are
typically applied to automatically extracting
bilingual lexicon with high precision, but they
are difficult to obtain in several domains. Due to
the high cost of acquiring parallel corpora,
comparable corpora, which consist of sets of
documents in different languages dealing with a
given topic or domain and are much easier to
collect from the increasingly rich web data (Xiao
and McEnery, 2006), become an alternative
resource to the task. Based on comparable
corpora, researchers begin to use a variety of
approaches to exploit them for bilingual lexicon
extraction in recent years (Tanaka and Iwasaki,
</bodyText>
<note confidence="0.9817346">
Miao Li, Zede Zhu, Zhenxin Yang,
Shizhuang Weng
Institute of Intelligent Machines Chinese
Academy of Sciences
Hefei, China
</note>
<email confidence="0.820805">
mli@iim.ac.cn,
zhuzede@mail.ustc.edu.cn,
xinzyang@mail.ustc.edu.cn,
weng1989@mail.ustc.edu.cn
</email>
<bodyText confidence="0.994416918918919">
1996; Fung and McKeown, 1997; Fung and Yee,
1998; Rapp, 1999; Morin et al., 2007; Saralegui
et al., 2008; Kun Yu, Junichi Tsujii, 2009). These
approaches mainly share a standard strategy
based on the assumption that a word and its
translation appear in similar context.
These previous work shows that equivalent
extraction from comparable corpora is unstable
on all but the most frequent words. An
explanation for the phenomenon is that
translation candidate lists of target words,
coming from matrix of context similarities, are
always disturbed by lots of noises introduced by
many-to-many mapping between the contexts of
words in different languages and only more
frequent ones keep comparatively robust (Pekar
et al., 2006).
Regardless of the polysemy, in the candidate
list of a certain target word, there may be only
one correct candidate and the rest ones can be
regarded as noises. Moreover, the correct
candidate of one target word may become the
noise in the candidate list of another target one.
Therefore, to retain the correct candidate in one
list and remove it (viewed as noise) from others’
list when it appears, comparison between
candidates in each list need to be done.
In this paper, we propose a novel method to
remove these noises via optimizing translation
candidate lists. The optimizing process is on the
basis of cross-comparison which means
comparison object lies on different candidate
lists. Firstly, we adopt window-based approach
to acquire translation candidate lists (Rapp, 1999;
Chiao and Zweigenbaum, 2002). Then, we use
the proposed two cross-comparisons of
similarity. The first one called identical ranking
</bodyText>
<page confidence="0.738286">
18
</page>
<note confidence="0.905979">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 18–25,
Wuhan, China, 20-21 October 2014
</note>
<bodyText confidence="0.99989275">
cross-comparison is the comparison between 1th
translation candidate of each target word. The
second named distinct ranking cross-comparison
is the comparison between set of all the 1th
candidates and that of each word’s 2th to Nth ones.
Finally, we conduct the experiments to find
target words with different frequencies from
both Chinese-English and English-Chinese.
The organization of the paper is as follows:
Related work is presented in Section 2. Section 3
is devoted to the introduction of window-based
approach. In Section 4, we present the proposed
optimizing process. In Section 5 we describe the
experimental setup and report the results of
bilingual lexicon extraction. Section 6
summarizes the paper with a final conclusion.
</bodyText>
<sectionHeader confidence="0.999116" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999608923076923">
Previous work about bilingual lexicon extraction
from comparable corpora usually focused on
utilizing context similarity. Fung (1995) firstly
used context heterogeneity in the task.
Subsequently, context vectors were modeled and
similarities between source-language and
target-language contexts were measured with the
aid of a general dictionary by many researchers
(Fung, 2000; Chiao and Zweigenbaum, 2002;
Robitaille et al., 2006; Morin et al., 2007).
The approaches based on context vectors
differ in the way they defined word contexts.
Window-based approach uses the window of the
compared word to construct context (Rapp, 1999;
Chiao and Zweigenbaum, 2002; Dejean et al.,
2002; Gamallo, 2007). Apart from that,
Syntax-based approach utilizes syntactic
information for bilingual dictionary extraction
(Otero, 2007).
The above approaches simply yield candidates
according to the calculation of vector similarity
without any subsequent processing. The
proposed method can be viewed as the extension
of window-based approach. Different from
previous work, we emphasize the optimizing
process of translation candidate lists.
</bodyText>
<sectionHeader confidence="0.991984" genericHeader="method">
3 Window-Based Approach
</sectionHeader>
<bodyText confidence="0.999909833333333">
In window-based approach, some windows of
words are firstly considered as forming the
context vectors. The approach then translates
source words’ context vectors by using a general
bilingual dictionary, and calculates the similarity
between each source and target vector.
</bodyText>
<subsectionHeader confidence="0.999367">
3.1 Building Context Vectors
</subsectionHeader>
<bodyText confidence="0.997283875">
In this step, we first choose a window size ,B and
get ,B number words from both left and right of
every source word ws in corpora to form the
source context information set
Iw = {W�. , w�_ } . Similarly, we acquire the
target context information set
of target word , where Ns and Nt means the
number of words in and Iw, . The weight
</bodyText>
<equation confidence="0.829649">
W (ws , wsk) of word wsk (1 &lt;— k &lt;— I s) , which is
</equation>
<bodyText confidence="0.9700835">
represented as follows, is calculated on the basis
of mutual information.
</bodyText>
<equation confidence="0.966295">
. (1)
</equation>
<bodyText confidence="0.97887">
Where count(ws, wsk ) is the number of
co-occurrence between ws and ws, in all the
contexts. and take as
values the number of occurrence of ws and ws, .
We compute weights of every word
</bodyText>
<equation confidence="0.9496128">
wsk
Q&lt;— V&lt;— I s ) in to form the source
context vector Vw, . Similar method is adopted to
transfer to the target context vector Vw .
r t
</equation>
<subsectionHeader confidence="0.991003">
3.2 Vector Similarity
</subsectionHeader>
<bodyText confidence="0.99559575">
Using a general bilingual dictionary, we map the
Iw, into the target language context information
whose corresponding context vector is
Ws
</bodyText>
<equation confidence="0.931563333333333">
Vw&apos; : If kth component in equals to r gth
component in (1 &lt; k &lt; I s, 1 &lt; g ≤ Nt), we
S — — —
</equation>
<bodyText confidence="0.7438465">
assign the value of gth component in to kth
component in Vans ; if there is no equal word,
</bodyText>
<equation confidence="0.601054428571429">
WS
the value is zero.
By calculating Vw&apos; of each ws and Vw of
S
each wr , we create a vector matrix, where rows
correspond to Vw , columns to and cells
WS
</equation>
<bodyText confidence="0.9998798">
to similarities between each vectors. Finally, we
adopt the cosine measure (see equation 2) to
calculate the similarities in the matrix and
further rank them to generate translation
candidate lists.
</bodyText>
<equation confidence="0.681131">
19
. (2)
</equation>
<bodyText confidence="0.998335">
where v. and vi&amp;quot;&apos; is the component of vector
and V-17 respectively.
</bodyText>
<sectionHeader confidence="0.8966895" genericHeader="method">
4 Optimizing Translation Candidate
Lists
</sectionHeader>
<bodyText confidence="0.999846166666667">
We take into account top I ranking translation
candidates in the total M lists, where M means
the number of target words and I means the
lowest ranking considered in the section, and
optimize them with two cross-comparisons of
similarity between each candidate. The
optimizing process consists of 2 steps: identical
ranking cross-comparison between each first 1th
candidate; distinct ranking cross-comparison
between all the 1th candidates and each word’s
2th to Ith ones. The architecture of our method is
described in Fig.1.
</bodyText>
<subsectionHeader confidence="0.9078065">
Figure1: Architecture of the proposed method
4.1 Identical Ranking Cross-comparison
</subsectionHeader>
<bodyText confidence="0.98393475">
Identical ranking cross-comparison relies on the
assumption that each target word’s 1th candidate
is unique. When there are two words having the
same 1th candidate, we regard the one with
higher similarity as potential correct translation
and remove another one defined as noise. This
step is presented as follows:
Step1. Choose all the target words’ first top
</bodyText>
<equation confidence="0.9440545">
ranking candidates(T1 , • • •,T1 ) and extract
t1 tM
</equation>
<bodyText confidence="0.924792090909091">
their similarities (SimN,t1 , , Sim,).
Step2. Scan (T&apos;, • ,Tw�M) . If there exists
several equal candidates ( Tl Tl ) (1 ≤
a, b, c≤ M), jump to Step3. If all the candidates
are different, go to Step4.
Step3. Compare the corresponding
similarities ( Sim&apos; , Sim&apos; , Sim&apos; ). Retain
candidate with the highest value and remove
others. Jump to Step2.
Step4. Complete identical ranking
cross-comparison.
</bodyText>
<subsectionHeader confidence="0.997429">
4.2 Distinct Ranking Cross-comparison
</subsectionHeader>
<bodyText confidence="0.995689875">
In light of hypothesis that all target words’ 1th
candidates are regarded as optimal translations,
the main idea of distinct ranking
cross-comparison is that these 1th candidates are
assumed as noises when they appear in each
word’s 2th to Ith ones with higher similarities.
The following describes this step:
Step1. build a noise set (T1, • • •, T ) .
</bodyText>
<subsectionHeader confidence="0.307455">
W11 Wim
</subsectionHeader>
<bodyText confidence="0.898774">
Step2. use the noise set to scan rest candidates
</bodyText>
<equation confidence="0.96805975">
(T , • • •,Tw ) of (n ranging from 1 to M).
Z2. Z„ t„
Step3. when Tw (2 &lt; j ≤ I) equals to any
I„ -
element TN&apos;,t (2 &lt;- m &lt;- I ) in the noise set,
m
remove T&apos; if is higher than .
W„ Wim Wr„
</equation>
<subsectionHeader confidence="0.997827">
4.2 Algorithm Description and Illustration
</subsectionHeader>
<bodyText confidence="0.999625">
This part detailedly introduces the proposed
method by means of algorithm description. After
the description, we illustrate our method with a
specific example. Algorithm 1 depicts the
identical ranking cross-comparison as follows:
</bodyText>
<figure confidence="0.894429279069767">
Algorithm 1
Input:
Target words’ number M, Lowest ranking I
Unranked Candidate lists from 4 to LM
Unranked similarity lists from Sl to SM
Output:
New-ranking candidate lists from Liankto L
1: for i=1 to M do
2: rank Candidate list i:
3: i→Llank : (Tw,,V wti)
4: i→Stank : (SimNr , ... Simwr , ...)
5: end for
6: scan (TW&apos;1,Tw�M )
20
7: while equal candidates exist do
8: build , several sets consist of equal
candidates: ( Tl ) (Tl Tl )...
%t Wtc ,WtP&apos;Wt4
1 &lt; a, c, p, q &lt; M
9: build : corresponding similarity sets
10: Max = sum of , i ranging from 1 to Max
11: for i=1 to Max do
12: scan andSimSet
equ equ
13: find the highest similarity: Simw
th
14: other =0; 1 &lt; x &lt; M, x≠h
tx
15: end for
16: re-rank lists, scan (Tl , Tl , • • •, Tl )
W4 W12 WrM
17: end while
18: return all the candidate lists
The following Algorithm 2 realizes the
distinct ranking cross-comparison.
Algorithm 2
Input:
Target words’ number M
Lowest ranking I
Ranked Candidate lists from Llnk to LM k
Ranked similarity lists from Si ank to SM k
Output:
New-ranking candidate lists from L7k to L�M`
</figure>
<listItem confidence="0.941122">
1: for i=1 to M do
2: for j=1 to M do
3: for k=2 to I do
4: if = &amp; Simw &lt; Sim&apos; then
</listItem>
<equation confidence="0.654380666666667">
t, t, t,
5: =0;
t,
</equation>
<listItem confidence="0.984555666666667">
6: end if
7: end for
8: end for
9: re-rank candidate listrank
10: end for
11: return all the candidate lists
</listItem>
<bodyText confidence="0.995472371428571">
For example, following the above algorithm,
we get sorted candidate lists (see Tab.1). In
identical ranking cross-comparison, we scan all
the 1th candidates in each list (see red square in
Tab.1) and find two sets of equal candidates:
(‘market/0.6162’, ‘market/0.6097’) and
(‘economics/0.5627’, ‘economics/0.6492’) (see
black square in Tab.1) . Through the comparison
of similarity, the ‘market/0.6097’ and
‘economics/0.5627’ become ‘market/0’ and
‘economics/0’. Then we re-rank the lists and
scan again, finding that each 1th candidate is
unique. So Algorithm 1 is finished. Tab. 2 shows
the re-ranking lists after identical ranking
cross-comparison.
In distinct ranking cross-comparison, we build
a noise set (‘market/0.6162’, ‘theory/0.6012’,
‘art/0.4982’, ‘economics/6492’, ‘human/0.5627’)
(see red square in Tab.2) to scan each list’s 2th to
Nth candidates. Taking the list of word ‘教育’ as
example, we first use the noise set to scan the
remaining candidates (‘economics/0.5220’,
‘theory/0.5136’,‘education/0.5112’,‘art/0.5078’,
...) (see black square in Tab.2) , and then find
that ‘economics’, ‘art’ and ‘theory’ exist in the
noise set. So we compare the similarity between
‘economics/0.6492’ and ‘economics/0.5220’,
‘theory/0.6012’ and ‘theory/0.5136’, and
‘art/0.4982’ and ‘art/0.5078’. Thus,
‘economics/0.5220’ and ‘theory/0.5136’ with
lower value are turned into ‘economics/0’ and
‘theory/0’. Afterwards, we re-rank this list. Tab.
3 presents the finally optimized lists. Correct
translations in Tab.1 to Tab.3 are highlighted in
bold.
</bodyText>
<tableCaption confidence="0.998728">
Table 1: Ranked lists from window-based approach
</tableCaption>
<figure confidence="0.999322762711864">
Word
市场
理论
艺术
经济学
教育
1
market
0.6162
market
0.6097
human
0.5407
economics
0.6492
economics
0.5627
theory
0.5953
theory
0.6012
art
0.4982
market
0.5198
economics
0.5220
art
0.5837
human
0.5930
economy
0.4817
art/
0.5038
theory
0.5136
education
0.5716
family
0.5527
job
0.4721
education/
0.4786
education
0.5112
human
0.5330 &amp;quot;&apos;
education
0.5326 &amp;quot;&apos;
human
0.4330 &amp;quot;&apos;
state
0.4687 &amp;quot;&apos;
art
0.5078 &amp;quot;&apos;
Candidate/Similarity lists
2 3 4 5 &amp;quot;&apos;
</figure>
<page confidence="0.495648">
21
</page>
<table confidence="0.992946485714286">
Word Candidate/Similarity lists
1 2 3 4 5 ...
市场 market theory art education human
0.6162 0.5953 0.5837 0.5716 0.5330 ...
theory human family education nature
0.6012 0.5930 0.5527 0.5326 0.5008 ...
art economy job human market
0.4982 0.4817 0.4721 0.4330 0.4291 ...
economics market art education state
0.6492 0.5198 0.5038 0.4786 0.4687 ...
human
0.5407
理论
艺术
经济学
教育 economics theory education art
0.5220 0.5136 0.5112 0.5078 ...
Table 2: Lists after identical ranking cross-comparison
Word Candidate/Similarity lists
1 2 3 4 5 ...
市场 market art education job book
0.6162 0.5837 0.5716 0.5116 0.4930 ...
theory human family education nature
0.6012 0.5930 0.5527 0.5326 0.5008 ...
art economy job book physics
0.4982 0.4817 0.4721 0.4121 0.4052 ...
economics art education state application
0.6492 0.5038 0.4786 0.4687 0.4528 ...
human education art job state
0.5407 0.5112 0.5078 0.4992 0.4791 ...
counttopn
理论
艺术
经济学
教育
</table>
<tableCaption confidence="0.996057">
Table 3: Final optimized lists
</tableCaption>
<bodyText confidence="0.680262">
respectively.
</bodyText>
<subsectionHeader confidence="0.897481">
5.2 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.986825136363636">
We adopt the accuracy as evaluation metric.
Accuracy, which means precision among the top
n ranking, is a common metric in bilingual
lexicon extraction. In this paper, translation
candidates in lists from 1`h to 20`h ranking are
kept for automatic and manual evaluation of
accuracy, and score of accuracy is calculated in
the following equation:
Where n means top n evaluation (n ranging from
1 to 20), M means the number of target words
and means the number of correct
translation in top n ranking.
5.3 Results and analysis
Experiment 1: target words with random
frequency distribution
When we extract bilingual lexicon from
English-Chinese, 1000 (M=1000) target words
from the Chinese documents are randomly chose.
We calculate the vector similarities between
these Chinese words and all the English words
to generate translation candidate lists, and then
optimize them via the proposed method.
</bodyText>
<equation confidence="0.773554333333333">
opn
Accuracy  countt
M . (3)
</equation>
<sectionHeader confidence="0.948102" genericHeader="evaluation">
5 Experiments and Analysis
</sectionHeader>
<subsectionHeader confidence="0.999821">
5.1 Experiment Datasets and Setup
</subsectionHeader>
<bodyText confidence="0.99999512">
We conduct experiments on a Chinese-English
corpora derived from the data used in bilingual
Wikipedia with 3254 comparable document
pairs. The general bilingual dictionary is
constructed from an online dictionary which
contains 42,373 distinct entries. In addition, we
perform the following linguistic preprocessing
steps on the comparable corpora: tokenization,
lemmatization and removing stop words. After
these steps the corpora contain ca. 925,000
Chinese words, and ca. 785,000 English words.
The windows size β in building the context
vectors is defined as 5, and different sizes are
assessed and the above setting turns out to have
the best performance in window-based method.
Two experiments are performed on target
words with random frequency distribution and
certain frequency in order to evaluate the
proposed method. During each experiment we
also absorb in the extraction performance from
both English-Chinese and Chinese-English. The
baseline in our experiments is the window-based
approach without any optimizing, and we
successively use two cross-comparisons in the
proposed method and focus on performance
</bodyText>
<page confidence="0.663265">
22
</page>
<bodyText confidence="0.999859833333333">
Meanwhile, we conduct the experiment of
finding translations of 1000 target words from
English documents. I in this experiment is
assign as 1020. Fig. 2 and Fig. 3 demonstrate the
resulting accuracy of different methods from
two directions.
</bodyText>
<figureCaption confidence="0.993073333333333">
Figure 2: Extraction Results of different methods
from English-Chinese
Figure 3: Extraction Results from Chinese-English
</figureCaption>
<bodyText confidence="0.972677214285714">
The results show that accuracy is improved
significantly from both English-Chinese and
Chinese-English, thereby indicate the robustness
and effectiveness of our method. In particular,
two steps in the proposed method can gradually
improve the accuracy. Improvements of
accuracy in top1 and top5 are mainly attributed
to identical ranking cross-comparison as it
processes candidate lists’ top-ranking area.
Distinct ranking cross-comparison can markedly
boost accuracy in top10, top15 and top20, since
it removes noises in larger area of the lists.
Experiment 2: target words with certain
frequency
Previous work showed that frequent words’
correct translations are easier to be found than
infrequent ones (Pekar et al., 2006). Allowing
for this fact, we distinguish different frequency
ranges to assess the validity of the proposed
approach. Target words with frequency more
than 400 are defined as high-frequency words
(WH), whereas words with frequency less than
100 are low-frequency words (WL). The number
of target words from either Chinese or English
documents is 1000 (M=1000) and I equals to
1020. Extraction performance on accuracy
beyond WH and WL are showed in Fig. 4, Fig. 5,
Fig. 6 and Fig. 7.
</bodyText>
<figureCaption confidence="0.9966578">
Figure 4: Extraction Results of WH from
English-Chinese
Figure 5: Extraction Results of WH from
Chinese-English
Figure 6: Extraction Results of WL from
</figureCaption>
<figure confidence="0.817389">
English-Chinese
23
</figure>
<figureCaption confidence="0.9701525">
Figure 7: Extraction Results of WL from
Chinese-English
</figureCaption>
<bodyText confidence="0.999933818181818">
From Fig. 4 and Fig. 5 we have the following
observation: accuracy improvement effect of
identical ranking cross-comparison in top1 and
top5 becomes more obvious in the process on
WH. In addition, Fig. 6 and Fig. 7 indicate that in
processing WL distinct ranking cross-comparison
promotes accuracy in top10, top15 and top20 to
a larger extent. The main reason is that for WH
each word’s correct translation, which is also
high-frequency source word, happens to be noise
existing in top-ranking area of other words’ lists.
This situation leads to increasing number of
identical ranking cross-comparison which can
eliminate noises more effectively. Meanwhile,
for WL noises in each target word’s translation
candidate lists are all high-frequency source
words, leading high repetition rate between the
noises set and top I candidates in the lists.
Therefore, distinct ranking cross-comparison can
boost most optimal translations which locate in
lower ranking before to concentrate in the area
between 5th and 20th ranking.
</bodyText>
<sectionHeader confidence="0.998211" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999663">
In this paper, we address the ‘noise’ problem in
extracting translation equivalent from
comparable corpora. To solve the problem, we
develop a novel method to optimize translation
candidate lists. The optimizing process includes
two step cross-comparisons between translation
candidate of each target word. Experimental
results show that the proposed method can boost
accuracy significantly and outperform
window-based approach in bilingual lexicon
extraction from both English-Chinese and
Chinese-English. Moreover, identical ranking
and distinct ranking cross-comparison can
improve the accuracy respectively in different
ranking area, and their improvements depend on
the frequency of target words. Future work may
focuses on conducting experiment between the
proposed method and syntax-based approach,
and eliminating our method’s impact on
synonyms.
</bodyText>
<sectionHeader confidence="0.991193" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9566666">
The work is supported by the Informationization
Special Projects of Chinese Academy of Science
under No. XXH12504-1-10 and the Open
Projects Program of National Laboratory of
Pattern Recognition.
</bodyText>
<sectionHeader confidence="0.8967" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.993773646341464">
AbduI-Rauf S, Schwenk H. On the use of comparable
corpora to improve SMT performance[C]
//Proceedings of the 12th Conference of the
European Chapter of the Association for
Computational Linguistics. Association for
Computational Linguistics, 2009: 16-23.
Chiao Y-C, Zweigenbaum P. Looking for candidate
translational equivalents in specialized, comparable
corpora[C] //Proceedings of COLING. 2002.
Dejean H, Gaussier E, Sadat F. Bilingual terminology
extraction: an approach based on a multilingual
thesaurus applicable to comparable corpora[C]
//Proceedings of COLING, Tapei, Taiwan. 2002.
Fung Pascale. Compiling Bilingual Lexicon Entries
from a Nonparallel English-Chinese Corpus[C] //
Proceedings of the 3rd Annual Workshop on Very
Large Corpora. 1995: 173-183.
Fung Pascale, Kathleen McKeown. Finding
terminology translation from non-parallel corpora[C]
//5th Annual Workshop on Very Large Corpora,
Hong Kong. 1997: 192-202.
Fung Pascale, Lo Yuen Yee. An IR approach for
translating new words from nonparallel, comparable
texts[C] //Proceedings of the 17th international
conference on Computational linguistics, Montreal,
Quebec, Canada. 1998: 414–420.
Fung Pascale. A Statistical View on Bilingual
Lexicon Extraction from Parallel Corpora to
Non-parallel Corpora[J]. Parallel Text Processing:
Alignment and Use of Translation Corpora, 2000.
Hiroyuki Kaji. 2005. Extracting translation
equivalents from bilingual comparable corpora[C]
//Proceedings of the LREC-2008 Workshop on
Comparable Corpora. 2008: 313–323.
Morin E, Daille B, Takeuchi K, Kageura K. Bilingual
terminology mining – using brain, not brawn
24
comparable corpora[C] //Proceedings of the 45th
annual meeting of the Association of Computational
Linguistics, Prague, Czech Republic. 2007:
664–671.
Och F J, Ney H. Improved Statistical Alignment
Models[C] //Proceedings of ACL. 2000: 440-447.
Otero P. Learning Bilingual Lexicons from
Comparable English and Spanish Corpora[C] //
Proceedings of MT Summit XI. 2007: 191-198.
Pekar, Viktor, Ruslan Mitkov, Dimitar Blagoev,
Andrea Mulloni. Finding translations for
low-frequency words in comparable corpora[J].
Machine Translation, 2006, 20(4): 247–266.
Pablo Gamallo. Learning bilingual lexicons from
comparable english and spanish corpora[C] //
Machine Translation SUMMIT XI, Copenhagen,
Denmark. 2007.
Rapp, Reinhard. Automatic identification of word
translations from unrelated English and German
corpora[C] //Proceedings of the 37th Annual
Meeting of the Association for Computational
Linguistics, College Park, Maryland, USA. 1999:
519–526.
Robitaille X. Compiling French Japanese
Terminologies from the Web[C] //Proceedings of
EACL. 2006.
Saralegi X, San Vicente I, Gurrutxaga A. Automatic
generation of bilingual lexicons from comparable
corpora in a popular science domain[C] //Workshop
on Building and Using Comparable Corpora in
LREC. 2008.
Tanaka K, Iwasaki H. Extraction of lexical
translations from non-aligned corpora[C]
//Proceedings of COLING-96: The 16th
international conference on computational
linguistics, Copenhagen, Denmark. 1996: 580–585.
Xiao Z, McEnery A. Collocation, Semantic Prosody
and Near Synonymy: A Cross-linguistic
Perspective[J]. Applied Linguistics, 2006, 27(1):
103-129.
Yu Kun, Junichi Tsujii. Extracting bilingual
dictionary from comparable corpora with
dependency heterogeneity[C] //Proceedings of
HLTNAACL, Boulder, Colorado, USA. 2009:
121–124.
</reference>
<page confidence="0.923936">
25
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.314963">
<title confidence="0.999031">Improving Bilingual Lexicon Extraction Performance Comparable Corpora via Optimizing Translation Candidate Lists</title>
<author confidence="0.991131">Shaoqi</author>
<affiliation confidence="0.73864625">University of Science and of China, Institute of Intelligent Chinese Academy of Hefei,</affiliation>
<email confidence="0.946794">wsq2012@mail.ustc.edu.cn</email>
<abstract confidence="0.9969146875">In this paper, we propose a novel method to optimize translation candidate lists derived from window-based approach for the task of bilingual lexicon extraction. The optimizing process consists of two between translation candidate of each target word, and between of all the candidates and that of each to ones. results demonstrate that the proposed method leads to a significant improvement on window-based approach in bilingual lexicon extraction from both English-Chinese and Chinese-English comparable corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S AbduI-Rauf</author>
<author>H Schwenk</author>
</authors>
<title>On the use of comparable corpora to improve</title>
<date>2009</date>
<booktitle>SMT performance[C] //Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<marker>AbduI-Rauf, Schwenk, 2009</marker>
<rawString>AbduI-Rauf S, Schwenk H. On the use of comparable corpora to improve SMT performance[C] //Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2009: 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y-C Chiao</author>
<author>P Zweigenbaum</author>
</authors>
<title>Looking for candidate translational equivalents in specialized, comparable corpora[C] //Proceedings of COLING.</title>
<date>2002</date>
<contexts>
<context position="3469" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="509" endWordPosition="512">andidate of one target word may become the noise in the candidate list of another target one. Therefore, to retain the correct candidate in one list and remove it (viewed as noise) from others’ list when it appears, comparison between candidates in each list need to be done. In this paper, we propose a novel method to remove these noises via optimizing translation candidate lists. The optimizing process is on the basis of cross-comparison which means comparison object lies on different candidate lists. Firstly, we adopt window-based approach to acquire translation candidate lists (Rapp, 1999; Chiao and Zweigenbaum, 2002). Then, we use the proposed two cross-comparisons of similarity. The first one called identical ranking 18 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 18–25, Wuhan, China, 20-21 October 2014 cross-comparison is the comparison between 1th translation candidate of each target word. The second named distinct ranking cross-comparison is the comparison between set of all the 1th candidates and that of each word’s 2th to Nth ones. Finally, we conduct the experiments to find target words with different frequencies from both Chinese-English and English-C</context>
<context position="4868" citStr="Chiao and Zweigenbaum, 2002" startWordPosition="713" endWordPosition="716">, we present the proposed optimizing process. In Section 5 we describe the experimental setup and report the results of bilingual lexicon extraction. Section 6 summarizes the paper with a final conclusion. 2 Related work Previous work about bilingual lexicon extraction from comparable corpora usually focused on utilizing context similarity. Fung (1995) firstly used context heterogeneity in the task. Subsequently, context vectors were modeled and similarities between source-language and target-language contexts were measured with the aid of a general dictionary by many researchers (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). The approaches based on context vectors differ in the way they defined word contexts. Window-based approach uses the window of the compared word to construct context (Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Gamallo, 2007). Apart from that, Syntax-based approach utilizes syntactic information for bilingual dictionary extraction (Otero, 2007). The above approaches simply yield candidates according to the calculation of vector similarity without any subsequent processing. The proposed method can be viewed as the extension of wi</context>
</contexts>
<marker>Chiao, Zweigenbaum, 2002</marker>
<rawString>Chiao Y-C, Zweigenbaum P. Looking for candidate translational equivalents in specialized, comparable corpora[C] //Proceedings of COLING. 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Dejean</author>
<author>E Gaussier</author>
<author>F Sadat</author>
</authors>
<title>Bilingual terminology extraction: an approach based on a multilingual thesaurus applicable to comparable corpora[C]</title>
<date>2002</date>
<journal>Proceedings of COLING, Tapei,</journal>
<contexts>
<context position="5143" citStr="Dejean et al., 2002" startWordPosition="757" endWordPosition="760">le corpora usually focused on utilizing context similarity. Fung (1995) firstly used context heterogeneity in the task. Subsequently, context vectors were modeled and similarities between source-language and target-language contexts were measured with the aid of a general dictionary by many researchers (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). The approaches based on context vectors differ in the way they defined word contexts. Window-based approach uses the window of the compared word to construct context (Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Gamallo, 2007). Apart from that, Syntax-based approach utilizes syntactic information for bilingual dictionary extraction (Otero, 2007). The above approaches simply yield candidates according to the calculation of vector similarity without any subsequent processing. The proposed method can be viewed as the extension of window-based approach. Different from previous work, we emphasize the optimizing process of translation candidate lists. 3 Window-Based Approach In window-based approach, some windows of words are firstly considered as forming the context vectors. The approach then translates </context>
</contexts>
<marker>Dejean, Gaussier, Sadat, 2002</marker>
<rawString>Dejean H, Gaussier E, Sadat F. Bilingual terminology extraction: an approach based on a multilingual thesaurus applicable to comparable corpora[C] //Proceedings of COLING, Tapei, Taiwan. 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fung Pascale</author>
</authors>
<title>Compiling Bilingual Lexicon Entries from a Nonparallel English-Chinese Corpus[C]</title>
<date>1995</date>
<booktitle>Proceedings of the 3rd Annual Workshop on Very Large Corpora.</booktitle>
<pages>173--183</pages>
<marker>Pascale, 1995</marker>
<rawString>Fung Pascale. Compiling Bilingual Lexicon Entries from a Nonparallel English-Chinese Corpus[C] // Proceedings of the 3rd Annual Workshop on Very Large Corpora. 1995: 173-183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fung Pascale</author>
<author>Kathleen McKeown</author>
</authors>
<title>Finding terminology translation from non-parallel corpora[C] //5th Annual Workshop on Very Large Corpora, Hong Kong.</title>
<date>1997</date>
<pages>192--202</pages>
<marker>Pascale, McKeown, 1997</marker>
<rawString>Fung Pascale, Kathleen McKeown. Finding terminology translation from non-parallel corpora[C] //5th Annual Workshop on Very Large Corpora, Hong Kong. 1997: 192-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fung Pascale</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts[C]</title>
<date>1998</date>
<booktitle>Proceedings of the 17th international conference on Computational linguistics,</booktitle>
<pages>414--420</pages>
<location>Montreal, Quebec,</location>
<marker>Pascale, Yee, 1998</marker>
<rawString>Fung Pascale, Lo Yuen Yee. An IR approach for translating new words from nonparallel, comparable texts[C] //Proceedings of the 17th international conference on Computational linguistics, Montreal, Quebec, Canada. 1998: 414–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fung Pascale</author>
</authors>
<title>A Statistical View on Bilingual Lexicon Extraction from Parallel Corpora to Non-parallel Corpora[J]. Parallel Text Processing: Alignment and Use of Translation Corpora,</title>
<date>2000</date>
<marker>Pascale, 2000</marker>
<rawString>Fung Pascale. A Statistical View on Bilingual Lexicon Extraction from Parallel Corpora to Non-parallel Corpora[J]. Parallel Text Processing: Alignment and Use of Translation Corpora, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Kaji</author>
</authors>
<title>Extracting translation equivalents from bilingual comparable corpora[C]</title>
<date>2005</date>
<booktitle>Proceedings of the LREC-2008 Workshop on Comparable Corpora.</booktitle>
<pages>313--323</pages>
<marker>Kaji, 2005</marker>
<rawString>Hiroyuki Kaji. 2005. Extracting translation equivalents from bilingual comparable corpora[C] //Proceedings of the LREC-2008 Workshop on Comparable Corpora. 2008: 313–323.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Morin</author>
<author>B Daille</author>
<author>K Takeuchi</author>
<author>K Kageura</author>
</authors>
<title>Bilingual terminology mining – using brain, not brawn comparable corpora[C]</title>
<date>2007</date>
<booktitle>Proceedings of the 45th annual meeting of the Association of Computational Linguistics,</booktitle>
<pages>664--671</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2008" citStr="Morin et al., 2007" startWordPosition="279" endWordPosition="282">ven topic or domain and are much easier to collect from the increasingly rich web data (Xiao and McEnery, 2006), become an alternative resource to the task. Based on comparable corpora, researchers begin to use a variety of approaches to exploit them for bilingual lexicon extraction in recent years (Tanaka and Iwasaki, Miao Li, Zede Zhu, Zhenxin Yang, Shizhuang Weng Institute of Intelligent Machines Chinese Academy of Sciences Hefei, China mli@iim.ac.cn, zhuzede@mail.ustc.edu.cn, xinzyang@mail.ustc.edu.cn, weng1989@mail.ustc.edu.cn 1996; Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Morin et al., 2007; Saralegui et al., 2008; Kun Yu, Junichi Tsujii, 2009). These approaches mainly share a standard strategy based on the assumption that a word and its translation appear in similar context. These previous work shows that equivalent extraction from comparable corpora is unstable on all but the most frequent words. An explanation for the phenomenon is that translation candidate lists of target words, coming from matrix of context similarities, are always disturbed by lots of noises introduced by many-to-many mapping between the contexts of words in different languages and only more frequent ones</context>
<context position="4914" citStr="Morin et al., 2007" startWordPosition="721" endWordPosition="724">on 5 we describe the experimental setup and report the results of bilingual lexicon extraction. Section 6 summarizes the paper with a final conclusion. 2 Related work Previous work about bilingual lexicon extraction from comparable corpora usually focused on utilizing context similarity. Fung (1995) firstly used context heterogeneity in the task. Subsequently, context vectors were modeled and similarities between source-language and target-language contexts were measured with the aid of a general dictionary by many researchers (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). The approaches based on context vectors differ in the way they defined word contexts. Window-based approach uses the window of the compared word to construct context (Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Gamallo, 2007). Apart from that, Syntax-based approach utilizes syntactic information for bilingual dictionary extraction (Otero, 2007). The above approaches simply yield candidates according to the calculation of vector similarity without any subsequent processing. The proposed method can be viewed as the extension of window-based approach. Different from previous w</context>
</contexts>
<marker>Morin, Daille, Takeuchi, Kageura, 2007</marker>
<rawString>Morin E, Daille B, Takeuchi K, Kageura K. Bilingual terminology mining – using brain, not brawn comparable corpora[C] //Proceedings of the 45th annual meeting of the Association of Computational Linguistics, Prague, Czech Republic. 2007: 664–671.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved Statistical Alignment Models[C] //Proceedings of ACL.</title>
<date>2000</date>
<pages>440--447</pages>
<contexts>
<context position="1100" citStr="Och and Ney, 2000" startWordPosition="147" endWordPosition="150"> two cross-comparisons between 1th translation candidate of each target word, and between set of all the 1th candidates and that of each word’s 2th to Nth ones. Experiment results demonstrate that the proposed method leads to a significant improvement on accuracy over window-based approach in bilingual lexicon extraction from both English-Chinese and Chinese-English comparable corpora. 1 Introduction Bilingual lexicon is a basic resource in the field of Natural Language Processing such as machine translation and cross-language information retrieval (AbduI-Rauf et al., 2009). Parallel corpora (Och and Ney, 2000) are typically applied to automatically extracting bilingual lexicon with high precision, but they are difficult to obtain in several domains. Due to the high cost of acquiring parallel corpora, comparable corpora, which consist of sets of documents in different languages dealing with a given topic or domain and are much easier to collect from the increasingly rich web data (Xiao and McEnery, 2006), become an alternative resource to the task. Based on comparable corpora, researchers begin to use a variety of approaches to exploit them for bilingual lexicon extraction in recent years (Tanaka an</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Och F J, Ney H. Improved Statistical Alignment Models[C] //Proceedings of ACL. 2000: 440-447.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Otero</author>
</authors>
<title>Learning Bilingual Lexicons from Comparable English and Spanish Corpora[C]</title>
<date>2007</date>
<booktitle>Proceedings of MT Summit XI.</booktitle>
<pages>191--198</pages>
<contexts>
<context position="5280" citStr="Otero, 2007" startWordPosition="775" endWordPosition="776">ctors were modeled and similarities between source-language and target-language contexts were measured with the aid of a general dictionary by many researchers (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). The approaches based on context vectors differ in the way they defined word contexts. Window-based approach uses the window of the compared word to construct context (Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Gamallo, 2007). Apart from that, Syntax-based approach utilizes syntactic information for bilingual dictionary extraction (Otero, 2007). The above approaches simply yield candidates according to the calculation of vector similarity without any subsequent processing. The proposed method can be viewed as the extension of window-based approach. Different from previous work, we emphasize the optimizing process of translation candidate lists. 3 Window-Based Approach In window-based approach, some windows of words are firstly considered as forming the context vectors. The approach then translates source words’ context vectors by using a general bilingual dictionary, and calculates the similarity between each source and target vecto</context>
</contexts>
<marker>Otero, 2007</marker>
<rawString>Otero P. Learning Bilingual Lexicons from Comparable English and Spanish Corpora[C] // Proceedings of MT Summit XI. 2007: 191-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktor Pekar</author>
</authors>
<title>Ruslan Mitkov, Dimitar Blagoev, Andrea Mulloni. Finding translations for low-frequency words in comparable corpora[J]. Machine Translation,</title>
<date>2006</date>
<volume>20</volume>
<issue>4</issue>
<pages>247--266</pages>
<marker>Pekar, 2006</marker>
<rawString>Pekar, Viktor, Ruslan Mitkov, Dimitar Blagoev, Andrea Mulloni. Finding translations for low-frequency words in comparable corpora[J]. Machine Translation, 2006, 20(4): 247–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pablo Gamallo</author>
</authors>
<title>Learning bilingual lexicons from comparable english and spanish corpora[C]</title>
<date>2007</date>
<booktitle>Machine Translation SUMMIT XI,</booktitle>
<location>Copenhagen,</location>
<contexts>
<context position="5159" citStr="Gamallo, 2007" startWordPosition="761" endWordPosition="762">cused on utilizing context similarity. Fung (1995) firstly used context heterogeneity in the task. Subsequently, context vectors were modeled and similarities between source-language and target-language contexts were measured with the aid of a general dictionary by many researchers (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). The approaches based on context vectors differ in the way they defined word contexts. Window-based approach uses the window of the compared word to construct context (Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Gamallo, 2007). Apart from that, Syntax-based approach utilizes syntactic information for bilingual dictionary extraction (Otero, 2007). The above approaches simply yield candidates according to the calculation of vector similarity without any subsequent processing. The proposed method can be viewed as the extension of window-based approach. Different from previous work, we emphasize the optimizing process of translation candidate lists. 3 Window-Based Approach In window-based approach, some windows of words are firstly considered as forming the context vectors. The approach then translates source words’ co</context>
</contexts>
<marker>Gamallo, 2007</marker>
<rawString>Pablo Gamallo. Learning bilingual lexicons from comparable english and spanish corpora[C] // Machine Translation SUMMIT XI, Copenhagen, Denmark. 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated</title>
<date>1999</date>
<booktitle>English and German corpora[C] //Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>519--526</pages>
<location>College Park, Maryland, USA.</location>
<contexts>
<context position="1988" citStr="Rapp, 1999" startWordPosition="277" endWordPosition="278">ng with a given topic or domain and are much easier to collect from the increasingly rich web data (Xiao and McEnery, 2006), become an alternative resource to the task. Based on comparable corpora, researchers begin to use a variety of approaches to exploit them for bilingual lexicon extraction in recent years (Tanaka and Iwasaki, Miao Li, Zede Zhu, Zhenxin Yang, Shizhuang Weng Institute of Intelligent Machines Chinese Academy of Sciences Hefei, China mli@iim.ac.cn, zhuzede@mail.ustc.edu.cn, xinzyang@mail.ustc.edu.cn, weng1989@mail.ustc.edu.cn 1996; Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; Morin et al., 2007; Saralegui et al., 2008; Kun Yu, Junichi Tsujii, 2009). These approaches mainly share a standard strategy based on the assumption that a word and its translation appear in similar context. These previous work shows that equivalent extraction from comparable corpora is unstable on all but the most frequent words. An explanation for the phenomenon is that translation candidate lists of target words, coming from matrix of context similarities, are always disturbed by lots of noises introduced by many-to-many mapping between the contexts of words in different languages and onl</context>
<context position="3439" citStr="Rapp, 1999" startWordPosition="507" endWordPosition="508">he correct candidate of one target word may become the noise in the candidate list of another target one. Therefore, to retain the correct candidate in one list and remove it (viewed as noise) from others’ list when it appears, comparison between candidates in each list need to be done. In this paper, we propose a novel method to remove these noises via optimizing translation candidate lists. The optimizing process is on the basis of cross-comparison which means comparison object lies on different candidate lists. Firstly, we adopt window-based approach to acquire translation candidate lists (Rapp, 1999; Chiao and Zweigenbaum, 2002). Then, we use the proposed two cross-comparisons of similarity. The first one called identical ranking 18 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 18–25, Wuhan, China, 20-21 October 2014 cross-comparison is the comparison between 1th translation candidate of each target word. The second named distinct ranking cross-comparison is the comparison between set of all the 1th candidates and that of each word’s 2th to Nth ones. Finally, we conduct the experiments to find target words with different frequencies from both</context>
<context position="5093" citStr="Rapp, 1999" startWordPosition="751" endWordPosition="752">ilingual lexicon extraction from comparable corpora usually focused on utilizing context similarity. Fung (1995) firstly used context heterogeneity in the task. Subsequently, context vectors were modeled and similarities between source-language and target-language contexts were measured with the aid of a general dictionary by many researchers (Fung, 2000; Chiao and Zweigenbaum, 2002; Robitaille et al., 2006; Morin et al., 2007). The approaches based on context vectors differ in the way they defined word contexts. Window-based approach uses the window of the compared word to construct context (Rapp, 1999; Chiao and Zweigenbaum, 2002; Dejean et al., 2002; Gamallo, 2007). Apart from that, Syntax-based approach utilizes syntactic information for bilingual dictionary extraction (Otero, 2007). The above approaches simply yield candidates according to the calculation of vector similarity without any subsequent processing. The proposed method can be viewed as the extension of window-based approach. Different from previous work, we emphasize the optimizing process of translation candidate lists. 3 Window-Based Approach In window-based approach, some windows of words are firstly considered as forming </context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Rapp, Reinhard. Automatic identification of word translations from unrelated English and German corpora[C] //Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, College Park, Maryland, USA. 1999: 519–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Robitaille</author>
</authors>
<title>Compiling French Japanese Terminologies from the Web[C] //Proceedings of EACL.</title>
<date>2006</date>
<marker>Robitaille, 2006</marker>
<rawString>Robitaille X. Compiling French Japanese Terminologies from the Web[C] //Proceedings of EACL. 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Saralegi</author>
<author>San Vicente I</author>
<author>A Gurrutxaga</author>
</authors>
<title>Automatic generation of bilingual lexicons from comparable corpora in a popular science domain[C] //Workshop on Building and Using Comparable Corpora in LREC.</title>
<date>2008</date>
<marker>Saralegi, I, Gurrutxaga, 2008</marker>
<rawString>Saralegi X, San Vicente I, Gurrutxaga A. Automatic generation of bilingual lexicons from comparable corpora in a popular science domain[C] //Workshop on Building and Using Comparable Corpora in LREC. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tanaka</author>
<author>H Iwasaki</author>
</authors>
<title>Extraction of lexical translations from non-aligned corpora[C]</title>
<date>1996</date>
<booktitle>Proceedings of COLING-96: The 16th international conference on computational linguistics, Copenhagen,</booktitle>
<pages>580--585</pages>
<marker>Tanaka, Iwasaki, 1996</marker>
<rawString>Tanaka K, Iwasaki H. Extraction of lexical translations from non-aligned corpora[C] //Proceedings of COLING-96: The 16th international conference on computational linguistics, Copenhagen, Denmark. 1996: 580–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Xiao</author>
<author>McEnery A Collocation</author>
</authors>
<title>Semantic Prosody and Near Synonymy: A Cross-linguistic Perspective[J]. Applied Linguistics,</title>
<date>2006</date>
<volume>27</volume>
<issue>1</issue>
<pages>103--129</pages>
<marker>Xiao, Collocation, 2006</marker>
<rawString>Xiao Z, McEnery A. Collocation, Semantic Prosody and Near Synonymy: A Cross-linguistic Perspective[J]. Applied Linguistics, 2006, 27(1): 103-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu Kun</author>
</authors>
<title>Junichi Tsujii. Extracting bilingual dictionary from comparable corpora with dependency heterogeneity[C]</title>
<date>2009</date>
<booktitle>Proceedings of HLTNAACL,</booktitle>
<pages>121--124</pages>
<location>Boulder, Colorado, USA.</location>
<marker>Kun, 2009</marker>
<rawString>Yu Kun, Junichi Tsujii. Extracting bilingual dictionary from comparable corpora with dependency heterogeneity[C] //Proceedings of HLTNAACL, Boulder, Colorado, USA. 2009: 121–124.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
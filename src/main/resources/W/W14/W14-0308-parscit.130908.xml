<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<title confidence="0.9978835">
Beyond Linguistic Equivalence. An Empirical Study of Translation
Evaluation in a Translation Learner Corpus
</title>
<author confidence="0.991628">
Mihaela Vela Anne-Kathrin Schumann Andrea Wurm
</author>
<affiliation confidence="0.9347485">
Department of Applied Linguistics, Translation and Interpreting
Saarland University, Saarbrücken, Germany
</affiliation>
<email confidence="0.997806">
{m.vela, anne.schumann, a.wurm}@mx.uni-saarland.de
</email>
<sectionHeader confidence="0.993905" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973846153846">
The realisation that fully automatic trans-
lation in many settings is still far from
producing output that is equal or superior
to human translation has lead to an in-
tense interest in translation evaluation in
the MT community. However, research in
this field, by now, has not only largely ig-
nored the tremendous amount of relevant
knowledge available in a closely related
discipline, namely translation studies, but
also failed to provide a deeper understand-
ing of the nature of &amp;quot;translation errors&amp;quot; and
&amp;quot;translation quality&amp;quot;. This paper presents
an empirical take on the latter concept,
translation quality, by comparing human
and automatic evaluations of learner trans-
lations in the KOPTE corpus. We will
show that translation studies provide so-
phisticated concepts for translation qual-
ity estimation and error annotation. More-
over, by applying well-established MT
evaluation scores, namely BLEU and Me-
teor, to KOPTE learner translations that
were graded by a human expert, we hope
to shed light on properties (and potential
shortcomings) of these scores.
</bodyText>
<sectionHeader confidence="0.951284" genericHeader="method">
1 Translation quality assessment
</sectionHeader>
<bodyText confidence="0.999983307692308">
In recent years, researchers in the field of MT
evaluation have proposed a large variety of meth-
ods for assessing the quality of automatically pro-
duced translations. Approaches range from fully
automatic quality scoring to efforts aimed at the
development of &amp;quot;human&amp;quot; evaluation scores that try
to exploit the (often tacit) linguistic knowledge of
human evaluators. The criteria according to which
quality is estimated often include adequacy, the
degree of meaning preservation, and fluency, tar-
get language correctness (Callison-Burch et al.,
2007). The goals of both &amp;quot;human&amp;quot; evaluation and
fully automatic quality scoring are manifold and
cover system optimisation as well as benchmark-
ing and comparison.
In translation studies, the scientific (and pre-
scientific) discussion on how to assess the quality
of human translations has been going on for cen-
turies. In recent years, the development of appro-
priate concepts and tools has become even more
vital to the discipline due to the pressing needs
of the language industry. However, different from
the belief, typical to MT, that the &amp;quot;goodness&amp;quot; of a
translation can be scored on the basis of linguistic
criteria alone, the notion of &amp;quot;translation quality&amp;quot;,
in translation studies, has assumed a multi-faceted
shape, distancing itself from a simple strive for
equivalence and embracing concepts such as func-
tional, stylistic and pragmatic appropriateness as
well as textual coherence. In this section, we pro-
vide an overview over approaches to translation
quality assessment developed in MT and transla-
tion studies to specify how &amp;quot;quality&amp;quot; is being de-
fined in both fields and which methods and fea-
tures are used. Due to the amount of available
literature, this overview is necessarily incomplete,
but still insightful with respect to differences and
commonalities between MT and human transla-
tion evaluation.
</bodyText>
<subsectionHeader confidence="0.972681">
1.1 Automatic MT quality scores
</subsectionHeader>
<bodyText confidence="0.999521727272727">
MT output is usually evaluated by automatic
language-independent metrics which can be ap-
plied to any language produced by an MT sys-
tem. The use of automatic metrics for MT eval-
uation is legitimate, since MT systems deal with
large amounts of data, on which manual evaluation
would be very time-consuming and expensive.
Automatic metrics typically compute the close-
ness (adequacy) of a &amp;quot;hypothesis&amp;quot; to a &amp;quot;reference&amp;quot;
translation and differ from each other by how this
closeness is measured. The most popular MT eval-
</bodyText>
<page confidence="0.993694">
47
</page>
<note confidence="0.9604725">
Workshop on Humans and Computer-assisted Translation, pages 47–56,
Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999831288888889">
uation metrics are IBM BLEU (Papineni et al.,
2002) and NIST (Doddington, 2002) which are
used not only for tuning MT systems, but also as
evaluation metrics for shared tasks, such as the
Workshop on Statistical Machine Translation (Bo-
jar et al., 2013).
IBM BLEU uses n-gram precision by match-
ing machine translation output against one or more
reference translations. It accounts for adequacy
and fluency by calculating word precision, respec-
tively the n-gram precision. In order to deal with
the over generation of common words, precision
counts are clipped, meaning that a reference word
is exhausted after it is matched. This is then the
modified n-gram precision. For N=4 the modified
n-gram precision is calculated and the results are
combined by using the geometric mean. Instead of
recall, the brevity penalty (BP) is used. It penal-
izes candidate translations which are shorter than
the reference translations.
The NIST metric is derived from IBM BLEU.
The NIST score is the arithmetic mean of modi-
fied n-gram precision for N=5 scaled by BP. Addi-
tionally, NIST also considers the information gain
of each n-gram, giving more weight to more infor-
mative (less frequent) n-grams and less weight to
less informative (more frequent) n-grams.
Another often used machine translation eval-
uation metric is Meteor (Denkowski and Lavie,
2011). Different from IBM BLEU and NIST, Me-
teor evaluates a candidate translation by calcu-
lating precision and recall on the unigram level
and combining them into a parametrized harmonic
mean. The result from the harmonic mean is then
scaled by a fragmentation penalty which penalizes
gaps and differences in word order.
Besides these evaluation metrics, several other
metrics are sometimes used for the evaluation
of MT output. Some of these are the WER
(word error-rate) metric based on the Levens-
thein distance (Levenshtein, 1966), the position-
independent error rate metric PER (Tillmann et
al., 1997) and the translation edit rate metric
TER (Snover et al., 2006) with its newer version
TERp (Snover et al., 2009).
</bodyText>
<subsectionHeader confidence="0.960484">
1.2 Human MT quality evaluation
</subsectionHeader>
<bodyText confidence="0.99997504">
Human evaluation of MT output is performed in
different ways. The most frequently used evalua-
tion method seems to be a simple ranking of trans-
lated sentences by a &amp;quot;reasonable number of eval-
uators&amp;quot; (Farrús et al., 2010). According to Birch
et al. (2013), this form of evaluation was used,
among others, during the last STATMT workshops
and can thus be considered rather popular. AP-
PRAISE (Federmann, 2012) is a tool that can be
used for such as task, since it allows for the man-
ual ranking of sentences, quality estimation, error
annotation and post-editing.
Other forms of evaluation, however, exist. For
example, Birch et al. (2013) propose HMEANT,
an evaluation score based on MEANT (Lo and
Wu, 2011), a semi-automatic MT quality score
that measures the degree of meaning preservation
by comparing verb frames and semantic roles of
hypothesis translations to their respective coun-
terparts in the reference translation(s). Unfor-
tunately, Birch et al. (2013) report difficulty in
producing coherent role alignments between hy-
potheses and translations, a problem that affects
the final HMEANT score calculation. This, how-
ever, seems hardly surprising given the difficulty
of the annotation task (although, following the au-
thors’ description, some familiarity of the anno-
tators with the linguistic key concepts can be as-
sumed) and the fact that guidelines and training
are meant to be minimal.
Another (indirect) human evaluation method for
MT that is also employed for error analysis are
reading comprehension tests (e.g. Maney et al.
(2012), Weiss and Ahrenberg (2012)). More-
over, HTER (Snover et al., 2006) is a TER-based
repair-oriented metric which uses human annota-
tors (the only apparent qualificational requirement
being fluency in the target language) to generate
&amp;quot;targeted&amp;quot; reference translations by post-editing
the MT output or the existing reference trans-
lations, following the goal to find the shortest
path between the hypothesis and a &amp;quot;correct&amp;quot; refer-
ence. Snover et al. (2006) report a high correlation
between evaluation with HTER and traditional hu-
man adequacy and fluency judgements. Last but
not least, Somers (2011) mentions other repair-
oriented measures such as post-editing effort mea-
sured by the amount of key-strokes or time spent
on producing a &amp;quot;correct&amp;quot; translation on the basis
of MT output.
</bodyText>
<subsectionHeader confidence="0.897124">
1.3 The notion of quality in translation
studies
</subsectionHeader>
<bodyText confidence="0.994773">
Discussions of translation &amp;quot;quality&amp;quot;, in translation
studies, for a long time focused on equivalence
</bodyText>
<page confidence="0.998556">
48
</page>
<bodyText confidence="0.999983625">
which, in its oldest and simplest form, used to
echo adequacy as understood by today’s MT re-
searchers: &amp;quot;good&amp;quot; translation was viewed as an
optimal compromise between meaning preserva-
tion and target language correctness, which was
especially relevant to the translation of religious
texts. For example, Kußmaul (2000) emphatically
cites Martin Luther’s famous Bible translation into
German as an example of &amp;quot;good&amp;quot; translation be-
cause Luther, according to his own testimony and
following his reformative ambition, focused on
producing fluent, easily understandable text rather
than mimicking the linguistic structures of the He-
brew, Aramaic and Greek originals (see also Win-
dle and Pym (2011) for a further discussion).
More recent work in translation studies has
abandoned one-dimensional views of the relation
between source and target text and postulates that,
depending on the communicative context within
and for which a translation is produced, this re-
lation can vary greatly. That is, the degree of lin-
guistic or semantic &amp;quot;fidelity&amp;quot; of a good translation
towards the source text depends on functional cri-
teria. This view is echoed in the concepts of &amp;quot;pri-
mary vs. secondary&amp;quot;, &amp;quot;documentary vs. instru-
mental&amp;quot; and &amp;quot;covert vs. overt&amp;quot; translation (Hönig,
2003). The consequence of this shift in paradigms
is that, since different translation strategies may
be appropriately adopted in different situations,
evaluation criteria become essentially dependent
on the function that the translation is going to play
in the target language and culture. This view is
most prominently advocated by the so-called sko-
pos theory (cf. Dizdar (2003)). Translation errors,
then, are not just simple violations of the target
language system or outright failures to translate
words or segments, but violations of the transla-
tion task that can manifest themselves on all levels
of text production (Nord, 2003). It is important
to point out that, in this framework, linguistic er-
rors are just one type of error covering not only
one of the favourite MT error categories, namely
un- and mistranslated words (compare, for ex-
ample, Stymne and Ahrenberg (2012), Weiss and
Ahrenberg (2012), Popovi´c et al. (2013)), but also
phraseological, idiomatic, syntactic, grammatical,
modal, temporal, stylistic, cohesion and other
kinds of errors. Moreover, translation-specific er-
rors occur when the translation does not fulfill its
function because of pragmatic (e.g. text-type spe-
cific forms of address), cultural (e.g. text con-
ventions, proper names, or other conventions) or
formal (e. g. layout) defects (Nord, 2003). De-
pending on the appropriate translation strategy for
a given translation task, these error types may be
weighted differently. Furthermore, the commu-
nicative and functional view on translation also
dictates a change in the concept of equivalence
which is no longer considered to be adequately
described by the notions of &amp;quot;meaning preserva-
tion&amp;quot; or &amp;quot;fidelity&amp;quot;, but becomes dependent on aes-
thetic, connotational, textual, communicative, sit-
uational, functional and cognitive aspects (for a
detailed discussion see Horn-Helf (1999)). In MT
evaluation, most of these aspects have not yet or
only in part been considered.
Last but not least, the translation industry has
developed normative standards and proofreading
schemes. For example, the DIN EN 15038:2006-
08 (Deutsches Institut für Normung, 2006) dis-
cusses translation errors, quality management and
qualificational requirements for translators and
proofreaders, while the SAE J2450 standard (So-
ciety of Automotive Engineers, 2005) presents a
weighted &amp;quot;translation quality metric&amp;quot;. An appli-
cation perspective is given by Mertin (2006) who
discusses translation quality management proce-
dures in a big automotive company and, among
other things, develops a weighted translation error
scheme for proofreading.
</bodyText>
<subsectionHeader confidence="0.944334">
1.4 Discussion
</subsectionHeader>
<bodyText confidence="0.9999816">
The above discussion shows that, while the object
of evaluation is the same for both MT and trans-
lation studies, namely translation, the differences
between evaluation approaches developed in both
fields are considerable. Most importantly, in trans-
lation studies, translation evaluation is considered
an expert task for which fluency in one or several
languages is certainly not enough, but for which
translation-specific expert knowledge is required.
Another important distinction is that evaluation,
again in translation studies, is normally not car-
ried out on the sentence level, since sentences are
usually split up into several &amp;quot;units of translation&amp;quot;
and can certainly contain more than one &amp;quot;trans-
lation problem&amp;quot;. Consequently, the popular MT
practice of ranking whole sentences according to
some automatic score, by anonymous evaluators
or even users of Amazon Turk (e.g. in the intro-
duction to Bojar et al. (2013)), from a translation
studies point of view, is unlikely to provide reason-
</bodyText>
<page confidence="0.996742">
49
</page>
<bodyText confidence="0.999970833333333">
able evaluations. Last but not least, the MT com-
munity’s strive for adequacy or meaning preser-
vation does not match the notions of weighting
translation errors, of adopting different translation
strategies and, consequently, does not fit the com-
plicated source/target text relations that have been
acknowledged by translation studies. Evaluation
methods that are based on simple measures of lin-
guistic equality such as n-gram overlap (BLEU)
or, just slightly more complicated, the preservation
of syntactic frames and semantic roles (MEANT)
fail to provide straightforward criteria for distin-
guishing between legitimate and illegitimate vari-
ation. Moreover, semantic and pragmatic criteria
as well as the notion of &amp;quot;reference translation&amp;quot; re-
main, at best, rather unclear.
On the other hand, the MT community has
recognised translation evaluation as an unresolved
research problem. For example, Birch et al. (2013)
state that ranking judgements are difficult to gen-
eralise, while Callison-Burch et al. (2007) carry
out extensive correlation tests of a whole range
of automatic MT evaluation metrics in compar-
ison to human judgements, showing that BLEU
does not rank highest, but still remains in the top
segment. It still needs to be shown how MT re-
search can benefit from more sophisticated evalu-
ation measures and whether all the parameters that
are considered relevant to the evaluation of human
translations are relevant for MT usage scenarios,
too. In the remainder of this paper, we present a
study on how much and possibly for which reasons
automatic MT evaluation scores (namely BLEU
and Meteor) differ from translation expert quality
judgements on extracts of a French-German trans-
lation learner corpus.
</bodyText>
<sectionHeader confidence="0.933061" genericHeader="method">
2 The KOPTE corpus
</sectionHeader>
<subsectionHeader confidence="0.956687">
2.1 General corpus design
</subsectionHeader>
<bodyText confidence="0.9999707">
The KOPTE project (Wurm, 2013) was designed
to enable research on translation evaluation in
a university training course (master’s level) for
translators and to enlighten students’ translation
problems as well as their problem solving strate-
gies. To achieve this goal, a corpus of student
translations was compiled. The corpus consists of
several translations of the same source texts pro-
duced by student translators in a classroom set-
ting. As a whole, it covers 985 translations of
77 source texts amounting to a total of 318,467
tokens. Source texts were taken from French
newspapers and translated into German in class
over a span of several years, the translation brief
calling for a ready-to-publish text to be printed
in a German national newspaper. Consequently,
all translation tasks include the use of idiomatic
language, explanations of culture-specific items,
changes in the explicitness of macrotextual cohe-
sive elements, etc.1
</bodyText>
<subsectionHeader confidence="0.972834">
2.2 Annotation of translation features and
translation evaluation in KOPTE
</subsectionHeader>
<bodyText confidence="0.999661894736842">
Student translations were evaluated by one of the
authors, an experienced translation teacher, with
the aim of giving feedback to students. All trans-
lations were graded and errors as well as good
solutions were marked in the text according to a
fine-grained evaluation scheme. In this scheme,
the weight of evaluated items is indicated through
numbers ranging from plus/minus 1 (minor) to
plus/minus 8 (major). Based on these evaluations,
each translation was assigned a final grade accord-
ing to the German grading system on a scale rang-
ing from 1 (&amp;quot;very good&amp;quot;) to 6 (&amp;quot;highly erroneous&amp;quot;)
with in-between intervals at the levels of .0, .3 and
.7. To calculate this grade, positive and negative
evaluations were summed up separately, before the
negative score was subtracted from the positive
one. A score of around zero corresponds to the
grade &amp;quot;good&amp;quot; (=2), to achieve &amp;quot;very good&amp;quot; (=1) the
student needs a surplus of positive evaluations.
The evaluation scheme based on which student
translations are graded is divided into external
and internal factors. External characteristics de-
scribe the communicative situation given by the
source text and the translation brief (author, re-
cipient, medium, location, time). Internal fac-
tors, on the other hand, comprise eight categories:
form, structure, cohesion, stylistics/register, gram-
mar, lexis/semantics, translation-specific prob-
lems, function. These categories are containers for
more fine-grained criteria which can be applied to
segments of the (source or target) text or even to
the whole text, depending on the nature of the cri-
terion. Some internal subcriteria of the scheme are
summarised in Table 1. A quantitative analysis of
error types in KOPTE shows that semantic/lexical
errors are by far the most common error in the stu-
dent translations (Wurm, 2013).
Evaluations in KOPTE were carried out by just
</bodyText>
<footnote confidence="0.996268">
1More information about KOPTE is available from
http://fr46.uni-saarland.de/index.php?id=3702&amp;L=%2524L.
</footnote>
<page confidence="0.995208">
50
</page>
<bodyText confidence="0.999821866666667">
one evaluator for the reason that, in a classroom
setting, multiple evaluations are not feasible. Al-
though multiple evaluations would have been con-
sidered highly valuable, the data available from
KOPTE was evaluated by an experienced trans-
lation scholar with long-standing experience in
teaching translation. Moreover, the evaluation
scheme is much more detailed than error annota-
tion schemes that are normally described in the lit-
erature and it is theoretically well-motivated. An
analysis of the median grades in our data sample
(compare Tables 2–4) shows that grading varies
only slightly between different texts, considering
the maximum variation potential ranging from 1
to 6, and thus can be considered consistent.
</bodyText>
<table confidence="0.997183333333333">
Criteria Examples of
subcriteria
author, recipients,
medium, topic, —
location, time
form paragraphs, formatting
structure thematic, progression,
macrostructure, illustrations
cohesion reference, connections
stylistics style, genre
grammar determiners, modality, syntax
semantics textual semantics, idioms,
numbers, terminology
translation erroneous source
problems text, proper names, culture-specific
items, ideology, math. units,
pragmatics, allusions
function goal dependence
</table>
<tableCaption confidence="0.980092">
Table 1: Internal evaluation criteria in the KOPTE annotation
scheme.
</tableCaption>
<sectionHeader confidence="0.998538" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.9705356">
The goal of our experiments was to study
whether the human translation expert judgements
in KOPTE can be mimicked using simple au-
tomatic quality metrics as used in MT, namely
BLEU and Meteor. More specifically, we aim at:
</bodyText>
<listItem confidence="0.996672666666667">
• studying how automatic evaluation scores re-
late to fine-grained human expert evaluations,
• investigating whether a higher number of ref-
erences improves the automatic scores and
why (or why not),
• examining whether a higher number of ref-
</listItem>
<bodyText confidence="0.9215200625">
erences provides more reliable evaluation
scores as measured by an improved correla-
tion with the human expert judgments.
In order to study the behaviour of automatic MT
evaluation scores, we conducted three experiments
by applying IBM BLEU (Papineni et al., 2002)
and Meteor 1.4 (Denkowski and Lavie, 2011) to
a sample of KOPTE translations that were pro-
duced by translation students preparing for their
final master’s exams. Scores were calculated on
the complete texts. To evaluate the overall perfor-
mance of the automatic evaluation scores on these
texts, we calculated Kendall’s rank correlation co-
efficient for each text following the procedure de-
scribed in Sachs and Hedderich (2009). Correla-
tions were calculated for:
</bodyText>
<listItem confidence="0.994098666666667">
• the human expert grades and BLEU scores
for each translation,
• the human expert grades and Meteor scores
for each translation,
• BLEU and Meteor scores for each transla-
tion.
</listItem>
<subsectionHeader confidence="0.9988">
3.1 Experimental setup and results
</subsectionHeader>
<bodyText confidence="0.999998466666667">
In a first experiment, we applied the automatic
evaluation scores to the source texts given in Ta-
ble 2, choosing, for each text, the student transla-
tion with the best human grade as reference trans-
lation. The median human grades as well as mean
BLEU and Meteor and correlation scores obtained
for each text (excluding the reference translation)
are included in Table 2. In a second experiment,
we repeated this procedure, however, using a set
of three reference translations. Results are given
in Table 3. Finally, in a last experiment we used
five reference translations selected according to
their human expert grade (Table 4). In both steps,
source texts for which less than four hypotheses
were available were excluded from the data sets.
</bodyText>
<subsectionHeader confidence="0.979004">
3.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999995363636364">
The tables show that in the first experiment a set of
152 translations was evaluated, whereas in the sec-
ond and third experiment these numbers were re-
duced to 108 and 68 respectively due to the selec-
tion of more references. The human expert eval-
uations rated most of these translations at least as
acceptable, as can be seen from the median grade
for each experiment which was 2.3 in the first ex-
periment and consecutively decreased to 3.0 for
the third experiment, again due to the selection
of more &amp;quot;good&amp;quot; translations as references. The
</bodyText>
<page confidence="0.998316">
51
</page>
<table confidence="0.999598105263158">
Source Human trans./ Median Mean Mean Correlation Correlation Correlation
text source text grades BLEU Meteor Human-BLEU Human-Meteor BLEU-Meteor
AT001 7 2. 7 0. 15 0. 33 −0. 39 −0. 73 0. 24
AT002 12 2. 3 0. 15 0. 35 −0. 20 −0. 43 0. 49
AT004 12 2. 7 0. 19 0. 37 0. 14 0. 11 0. 63
AT005 12 2. 3 0. 20 0. 36 0. 32 0. 45 0. 45
AT008 10 2. 15 0. 23 0. 38 −0. 43 −0. 29 0. 78
AT010 11 2. 7 0. 25 0. 41 0. 06 −0. 10 0. 56
AT012 9 2. 0 0. 22 0. 40 −0. 30 −0. 36 0. 50
AT015 5 2. 0 0. 11 0. 28 0. 36 0. 12 0. 60
AT017 7 2. 3 0. 22 0. 38 −0. 20 0. 06 0. 71
AT021 4 3. 0 0. 18 0. 39 −0. 55 −0. 55 1. 00
AT023 6 2. 3 0. 22 0. 38 0. 50 −0. 07 −0. 20
AT025 4 2. 15 0. 13 0. 36 0. 33 0. 0 0. 00
AT026 21 3. 0 0. 12 0. 26 −0. 19 −0. 35 0. 67
AT039 13 3. 0 0. 10 0. 29 −0. 08 0. 03 0. 49
AT052 7 2. 0 0. 17 0. 31 −0. 32 0. 05 0. 00
AT053 7 2. 3 0. 18 0. 32 0. 62 0. 39 0. 33
AT059 5 2. 0 0. 24 0. 36 0. 00 0. 22 0. 80
</table>
<tableCaption confidence="0.9827725">
Table 2: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the
BLEU and Meteor scores per source text and Kendall’s rank correlation coefficients for the first experiment.
</tableCaption>
<table confidence="0.8685118">
Source Human trans./ Median Mean Mean Correlation Correlation Correlation
text source text grades BLEU Meteor Human−BLEU Human-Meteor BLEU-Meteor
AT001 5 3. 0 0. 17 0. 36 −0. 12 0. 36 0. 60
AT002 10 2. 3 0. 17 0. 36 −0. 14 0. 05 0. 38
AT004 10 2. 85 0. 20 0. 37 0. 39 0. 16 0. 51
AT005 10 2. 3 0. 20 0. 40 −0. 10 0. 05 0. 47
AT008 8 2. 5 0. 25 0. 45 −0. 67 −0. 15 0. 00
AT010 9 2. 7 0. 23 0. 41 −0. 10 −0. 50 0. 28
AT012 7 2. 3 0. 23 0. 43 0. 00 0. 11 0. 52
AT017 5 2. 3 0. 21 0. 43 0. 12 0. 36 0. 60
AT023 4 2. 5 0. 21 0. 38 0. 41 0. 81 0. 67
AT026 19 3. 3 0. 10 0. 26 −0. 31 −0. 41 0. 77
AT039 11 3. 0 0. 11 0. 34 0. 06 0. 14 0. 74
AT052 5 2. 0 0. 18 0. 40 0. 12 0. 36 0. 20
AT053 5 2. 3 0. 17 0. 35 0. 36 −0. 12 0. 40
</table>
<tableCaption confidence="0.989564">
Table 3: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the
BLEU and Meteor scores per source text and Kendall’s rank correlation coefficients for the second experiment.
</tableCaption>
<bodyText confidence="0.999978576923077">
grades for the best translations selected as refer-
ences range for the first and second experiment
between 1.0 and 2.3, whereas for the third exper-
iment the selected references were evaluated with
grades between 1.0 and 2.7. Nevertheless, the me-
dian grade for the references in all three exper-
iments is always 1.7. From the overall median
grade and the median grade of the selected trans-
lations as reference we can notice, that the trans-
lations selected as references were indeed &amp;quot;better&amp;quot;
than the remaining ones.
The BLEU and Meteor scores given in the ta-
bles are mean values over the individual transla-
tions’ scores for each source text. These scores
are very low, reaching a maximum of 0.25 over
all three experiments for BLEU and 0.45 for Me-
teor. However, given the human expert grades
the translations cannot be considered unreadable.
In fact, the correlation coefficients show that nei-
ther BLEU nor Meteor (except a few exceptional
cases) correlate with the human quality judge-
ments, however, they show a (weak) tendency to
correlate with each other. Moreover, the data
shows that the addition of reference translations
results neither in significantly higher BLEU or
Meteor scores nor in improved correlation.
</bodyText>
<subsectionHeader confidence="0.999908">
3.3 Qualitative analysis
</subsectionHeader>
<bodyText confidence="0.9998591">
Our finding that human quality judgements do not
correlate with automatic scores if the object of
evaluation is a translation produced by a human
(as opposed to a machine) matches earlier results
presented by Doddington (2002) within the con-
text of evaluating NIST. Doddington (2002) pro-
poses the explanation that &amp;quot;differences between
professional translators are far more subtle [than
differences between machine-produced transla-
tions, the authors] and thus less well characterized
</bodyText>
<page confidence="0.997815">
52
</page>
<table confidence="0.9980544">
Source Human trans./ Median Mean Mean Correlation Correlation Correlation
text source text grades BLEU Meteor Human-BLEU Human-Meteor BLEU-Meteor
AT002 8 2. 5 0. 17 0. 36 −0. 08 0. 00 0. 43
AT004 8 3. 0 0. 20 0. 36 0. 00 0. 23 0. 71
AT005 8 2. 3 0. 20 0. 42 0. 00 0. 08 0. 43
AT008 6 2. 85 0. 26 0. 45 −0. 55 −0. 14 0. 33
AT010 7 2. 7 0. 23 0. 41 0. 00 −0. 12 0. 05
AT012 5 2. 3 0. 23 0. 43 0. 22 0. 22 0. 40
AT026 17 3. 3 0. 11 0. 31 −0. 24 −0. 34 0. 62
AT039 9 3. 0 0. 10 0. 37 0. 22 0. 55 0. 22
</table>
<tableCaption confidence="0.991014">
Table 4: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the
BLEU and Meteor scores per source text and Kendall’s rank correlation coefficients for the third experiment.
</tableCaption>
<bodyText confidence="0.9998035">
by N-gram statistics.&amp;quot; We conducted a qualitative
analysis of some KOPTE translations in order to
check whether the differences between individual
translations are indeed as subtle as suggested by
Doddington and to come up at least with hypothe-
ses that could explain the poor performance of the
automatic scores. We selected three source texts
used in the second experiment, namely AT008,
AT023 and AT053 and compared their respective
reference translations to selected hypothesis trans-
lations. This analysis was conducted on the lex-
ical level alone, that is, most of the features of
KOPTE’s elaborated evaluation scheme were not
even considered. The analysis, however, shows
that the amount of variation that can be found just
on the lexical level is almost overwhelming. Some
examples are listed in Appendix A.
A common phenomenon is simple variation due
to synonyms or the use of phrasal variants or
paraphrases. Moreover, the listed examples show
that lexical variation can be triggered by differ-
ent source text elements. The phenomena shown
in the tables are well-known translation prob-
lems, e.g. proper names, colloquial or figurative
speech or numbers. The other categories in the
table are less clear-cut, that is, they can overlap.
In our analysis, source text elements that cannot
be translated literally, but instead call for a cre-
ative solution were classified as translation prob-
lems. Different translation strategies can be ap-
plied to different kinds of problems, most impor-
tantly to the translation of culture-specific items,
proper names, underspecified source text elements
or culture-specific arguments. The respective table
and other examples that we analysed show that for
this category some translators chose to add addi-
tional information, to adapt the perspective to the
German target audience (for example, by adapt-
ing pronouns or deictic elements) or to adapt the
formatting choices to the variant preferred by the
target culture (e.g. commas instead of fullstops,
different types of quotation marks), whereas other
translators chose to translate literally. Both strate-
gies are legitimate under certain circumstances,
however, it can be assumed that adaptations re-
quire a greater cognitive effort. Source ambigu-
ities, according to our preliminary typology, are
source text features that can be interpreted in dif-
ferent ways - at least for a translator translating
from a foreign language (as opposed to a native
speaker). Obviously, the line between this cat-
egory and outright translation errors is not very
clear.
However, it needs to be stated that also for the
other categories - while many variants are correct
and legitimate - not all are equally good. Best
solutions for given problems are distributed un-
equally across the translations studied. Beyond
the purely lexical level, extensive variation can be
witnessed on the syntactic, but also the grammat-
ical level. For example, some translators chose to
break the rather complicated syntax of the French
original into simpler, easily readable sentences,
producing, in some cases, considerable shifts in
the information structure of the text - often a legit-
imate strategy.
With respect to the performance of the auto-
matic scores, our preliminary study - that still calls
for larger-scale and in-depth verification - suggests
that neither BLEU nor Meteor are able to cope
with the amount of variation found in the data.
More specifically, they cannot distinguish between
legitimate and illegitimate variation or grave and
slight errors respectively, but seem to fail to match
acceptable variants because of lexical and phrasal
variation or divergent grammatical structures re-
sulting in different verb frames, word sequences
and text lengths, not to talk even about acceptable
variation on higher linguistic levels. Therefore,
automatic scores seem to overrate surface differ-
</bodyText>
<page confidence="0.997086">
53
</page>
<bodyText confidence="0.999794153846154">
ences and thus assign very low scores to many
translations that were found to be at least accept-
able by a human expert.
Considering the impact of these findings for MT
evaluation purposes, it is not straightforward to as-
sume that the differences that we have observed
between the human translations are more &amp;quot;subtle&amp;quot;
(in the sense of being unimportant) than the ones
produced by machine translation systems. On the
contrary, our analysis suggests that &amp;quot;good&amp;quot; trans-
lations are characterised by creative solutions that
are not easily reproducible but that help to achieve
target language readability and comprehensibility.
This is a fundamental quality aspect of translation
independently of its production mode. Moreover,
it is difficult to see why some of the variants that
we observed in the human translations selected
from KOPTE, once the context shifts from human
to machine translation, should be found valid in
one situation and invalid in another, depending on
the training and test data used for developing an
MT system: A high amount of the variation found
in the human translations goes back to the legiti-
mate use of the creative and constructive powers
of natural language, and it is, among others, these
powers that should be mimicked by MT output.
</bodyText>
<sectionHeader confidence="0.939558" genericHeader="method">
4 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999987950819672">
In this paper, we have studied the performance
of two fully automatic MT evaluation metrics,
namely BLEU and Meteor, in comparison to hu-
man translation expert evaluations on a sample of
learner translations from the KOPTE corpus. The
automatic scores were tested in three experiments
with a varying number of reference translations
and their performance was compared to the hu-
man evaluations by means of Kendall’s rank cor-
relation coefficient. The experiments suggest that
both BLEU and Meteor systematically underesti-
mate the quality of the translations tested, that is,
they assign scores that, given the human expert
evaluations, seem to be by far too low. Moreover,
they do not consistently correlate with the human
expert evaluations. Coming up with explanations
for this failure is not straightforward, however, the
results of our qualitative and explorative analysis
suggest that lexical similarity scores are not able
to cope satisfactorily neither with standard lexical
variation (paraphrases etc.) nor with dissimilari-
ties that can be traced back to the specific nature
of the translation process, leave alone linguistic
levels beyond the lexicon. For Meteor, this short-
coming may partly be alleviated by the provision
of richer sets of synonyms and paraphrases, how-
ever, the amount of uncovered variation is still im-
mense. In fact, it seems that many more reference
translations would be needed in order to cover the
whole range of legitimate variants that can be used
to translate a given source text - a scenario that
seems hardly feasible! So how can BLEU or Me-
teor scores be interpreted when they are given in
MT papers? Based on our analyses, it seems clear
that these scores are based on a data-driven no-
tion of translation quality, that is, they measure
the degree of compliance of a hypothesis transla-
tion with some reference set. This is insofar prob-
lematic as studies based on different reference sets
cannot be compared, neither can BLEU or Me-
teor scores be generalised to other domains. Even
more importantly, BLEU or Meteor scores cannot
be used to measure a data-independent concept of
quality or even the usability of a translation for
a target audience which, as we have shown, de-
pends on many more factors than just lexical sur-
face overlap.
However, our study also leads to some open
research questions. One of these questions is
whether automatic evaluation scores can still be
used for more coarse-grained distinctions, that is,
to distinguish &amp;quot;really bad&amp;quot; translations from &amp;quot;re-
ally good&amp;quot; ones. The fine-grained distinctions
made by the evaluator of KOPTE on generally
rather good translations do not allow us to answer
this question. Future work will also deal with a
comparison of mistakes made by MT systems as
opposed to human translators as well as with the
question how (and which) translation-specific as-
pects can be applied to the evaluation of MT sys-
tems.
</bodyText>
<sectionHeader confidence="0.998613" genericHeader="evaluation">
References
</sectionHeader>
<reference confidence="0.99811525">
Alexandra Birch, Barry Haddow, Ulrich Germann,
Maria Nadejde, Christian Buck, and Philipp Koehn.
2013. The feasibility of HMEANT as a human MT
evaluation metric. In Proceedings of the 8th Work-
shop on SMT, pages 52–61.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Barry Haddow, Philipp Koehn, Christof Monz, Matt
Post, Hervé Saint-Amand, Radu Soricut, and Lucia
Specia, editors. 2013. Proceedings of the 8th Work-
shop on SMT. ACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
</reference>
<page confidence="0.992125">
54
</page>
<reference confidence="0.995538659793815">
(Meta-) evaluation of machine translation. In Pro-
ceedings of the 2nd Workshop on SMT, pages 136–
158.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the 6th Workshop on SMT, pages 85–91.
Deutsches Institut für Normung. 2006. DIN
EN 15038:2006-08: Übersetzungsdienstleistungen-
Dienstleistungsanforderungen. Beuth.
Dilek Dizdar. 2003. Skopostheorie. In Handbuch
Translation, pages 104–107. Stauffenburg.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the 2nd In-
ternational Conference on HLT, pages 138–145.
Mireia Farrús, Marta R. Costa-Jussà, José B. Mar-
iño, and José A. R. Fonollosa. 2010. Linguistic-
based evaluation criteria to identify statistical ma-
chine translation errors. In Proceedings of the 14th
Annual Conference of the EAMT, pages 167–173.
Christian Federmann. 2012. Appraise: An open-
source toolkit for manual evaluation of machine
translation output. PBML, 98:25–35, 9.
Hans Hönig. 2003. Humanübersetzung (therapeutisch
vs. diagnostisch). In Handbuch Translation, pages
378–381. Stauffenburg.
Brigitte Horn-Helf. 1999. Technisches Übersetzen in
Theorie und Praxis. Franke.
Paul Kußmaul. 2000. Kreatives Übersetzen. Stauffen-
burg.
Vladimir Iosifovich Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions and rever-
sals. Soviet Physics Doklady, 10(8):707–710.
Chi-Kiu Lo and Dekai Wu. 2011. MEANT: An
inexpensive, high-accuracy, semi-automatic metric
for evaluating translation utility based on semantic
roles. In Proceedings of the 49th Annual Meeting of
the ACL, pages 220–229.
Tucker Maney, Linda Sibert, Dennis Perzanowski,
Kalyan Gupta, and Astrid Schmidt-Nielsen. 2012.
Toward determining the comprehensibility of ma-
chine translations. In Proceedings of the 1st PITR,
pages 1–7.
Elvira Mertin. 2006. Prozessorientiertes Qualitäts-
management im Dienstleistungsbereich Übersetzen.
Peter Lang.
Christiane Nord. 2003. Transparenz der Korrektur.
In Handbuch Translation, pages 384–387. Stauffen-
burg.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the ACL, pages 311–318.
Maja Popovi´c, Eleftherios Avramidis, Aljoscha Bur-
chardt, Sabine Hunsicker, Sven Schmeier, Cindy
Tscherwinka, David Vilar, and Hans Uszkoreit.
2013. Learning from human judgements of machine
translation output. In MT Summit, pages 231–238.
Lothar Sachs and Jürgen Hedderich. 2009. Ange-
wandte Statistik. Methodensammlung mit R.
Springer.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAMTA, pages 223–231.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with a
tunable MT metric. In Proceedings of the 4th Work-
shop on SMT, pages 259–268.
Society of Automotive Engineers. 2005. SAE
J2450:2005-08: Translation Quality Metric. SAE.
Harold Somers. 2011. Machine translation: History,
development, and limitations. In The Oxford Hand-
book of Translation Studies, pages 427–440. Oxford
University Press.
Sara Stymne and Lars Ahrenberg. 2012. On the prac-
tice of error analysis for machine translation evalua-
tion. In Proceedings of the 8th LREC, pages 1785–
1790.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alexander Zubiaga, and Hassan Sawaf. 1997. Ac-
celerated DP based search for statistical translation.
In Proceedings of the EUROSPEECH, pages 2667–
2670.
Sandra Weiss and Lars Ahrenberg. 2012. Error pro-
filing for evaluation of machine-translated text: a
polish-english case study. In Proceedings of the
Eighth LREC, pages 1764–1770.
Kevin Windle and Anthony Pym. 2011. European
thinking on secular translation. In The Oxford
Handbook of Translation Studies, pages 7–22. Ox-
ford University Press.
Andrea Wurm. 2013. Eigennamen und Re-
alia in einem Korpus studentischer Übersetzungen
(KOPTE). trans-kom, 6(2):381–419.
</reference>
<page confidence="0.99798">
55
</page>
<bodyText confidence="0.7522045">
A Examples of lexical variation in human translation
In the examples below, bold face indicates the French source.
</bodyText>
<figure confidence="0.935013066666667">
A.1 Proper names
président gabonais
Präsidenten von Gabon
Präsidenten Gabuns
Präsidenten von Gabun
Präsident des afrikanischen Landes Gabon
gabunesischen Präsidenten
la Commission nationale de l’informatique et des libertés (CNIL)
Commission nationale de l’informatique et des libertés (CNIL)
französische Datenschutzbehörde (CNIL)
französische Datenschutzkommission CNIL
französische Datenschutzbehörde CNIL
französische Kommission für Datenschutz (CNIL)
A.2 Problematic source text elements (translation problems)
pivot de l’influence française
</figure>
<bodyText confidence="0.6944864">
Stützpunkt des Einflusses Frankreichs
zentralen Figur des französischen Einfluss
Stütze für den Einfluss Frankreichs
Schlüsselfigur für den Einfluss Frankreichs
Garant für den französischen Einfluß
</bodyText>
<sectionHeader confidence="0.744534" genericHeader="conclusions">
A.3 Paraphrases
</sectionHeader>
<reference confidence="0.929669727272727">
sera-t-elle capable
es schaffen
fähig sein
in der Lage sein
sich als fähig erweisen
&amp;quot;doyen de l’Afrique&amp;quot;
obersten Würdenträgers Afrikas
&amp;quot;Alten Herrn von Afrika&amp;quot;
&amp;quot;Abtes von Afrika&amp;quot;
&amp;quot;Ältesten von Afrika&amp;quot;
&amp;quot;doyen de l’Afrique&amp;quot;
</reference>
<bodyText confidence="0.456429">
se tenir à la bonne distance
auf angemessener Distanz zu bleiben
sich nicht einzumischen
sich herauszuhalten
die gebührende Neutralität zu wahren
A.4 Culture-specific elements and underspecified source text items
</bodyText>
<figure confidence="0.888347857142857">
la &amp;quot;Françafrique&amp;quot;
&amp;quot;Françafrique&amp;quot;
Französisch-Afrika (&amp;quot;Françafrique&amp;quot;)
„Franzafrika“
&amp;quot;Frankafrika&amp;quot;
&amp;quot;Françafrique&amp;quot; d.h. der französisch beeinflussten Gebiete Afrikas
les &amp;quot;voitures Google&amp;quot;, équipées de caméras à 360 degrés
</figure>
<bodyText confidence="0.810304">
mit 360-Grad-Kameras ausgestatteten &amp;quot;Google-Kamerawagen&amp;quot;
Kamera-Autos
Street-View-Wagen mit ihren 360°-Kameras
&amp;quot;Google-Autos&amp;quot;, die auf dem Dach eine 360-Grad-Kamera montiert haben,
mit 360-Grad-Kameras ausgestatteten &amp;quot;Street View-Autos&amp;quot;
A.5 Source text ambiguities (syntactic and semantic)
la France a soutenu un régime autoritaire et prédateur, sans pitié pour les opposants
autoritären Systems [...], das kein Mitleid mit seinen Gegnern zeigte
hat Frankreich ohne Rücksicht auf Regimekritiker ein autoritäres Gewaltregime unterstützt
autoritäre und ausbeutende Regime [...], welches keine Gnade für seine Gegner kannte
autoritäres und angriffslustiges Regime [...], das kein Mitleid mit seinen Gegnern hatte
hat Frankreich dieses autoritäre und ausbeuterische System, ohne Mitleid mit dessen Gegnern, gestützt
</bodyText>
<subsectionHeader confidence="0.819724">
justes paroles
</subsectionHeader>
<bodyText confidence="0.9987458">
hat die Wahrheit gesagt
hat [...] die richtigen Worte gefunden
hat die richtigen Worte gefunden
Aussage [...] war nichts als Worte
hat genau das Richtige gesagt
</bodyText>
<sectionHeader confidence="0.896996" genericHeader="references">
A.6 Numbers
</sectionHeader>
<reference confidence="0.886548259259259">
une amende de 100 000 euros
Geldstrafe in Höhe von 100 000 Euro
Strafe von 100 000C
Geldstrafe von 100.000,- EUR
Geldstrafe in Höhe von 100.000 Euro
Bußgeld in Höhe von 100 000C
A.7 Colloquial or figurative speech
Je vais vite
Ich beeile mich
Ich mache es schnell
Ich bewege mich schnell
Ich hab’s eilig
Ich beeile mich
photographe Yann Arthus-Bertrand, 63 ans
63jährigen Fotografen Yann Arthus-Betrand
Fotographen Yann Arthus-Bertrand (63 Jahre)
Fotografen Yann Arthus-Bertrand (63)
63-jährigen Fotografen Y.A.B.
Fotografen Yann Arthus-Bertrand, 63
résultats des petits frères
Einnahmen der Vorgänger
Verdienste zusätzlicher kleiner Artikel
Einnahmen durch andere Produkte
Erlöse von Merchandising
Einnahmen aus dem Merchandising
A.8 Source text element triggering correct and incorrect translations
65 chaînes de télévision, dont France 2 et 23 chaînes en Afrique
</reference>
<page confidence="0.9565125">
65 Fernsehsendern, darunter auch France 2 und 23 afrikanische Sender
65 Fernsehsendern, unter anderem France 2 und 23 Sender in Afrika
65 Fernsehsender, darunter der französische Sender France 2 und 23 afrikanische Sender
65 Fernsehkanälen, u.a. 2 in Frankreich und 23 in Afrika
65 Fernsehkanälen, darunter France 2 und 23 afrikanische Sender
56
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9976455">Beyond Linguistic Equivalence. An Empirical Study of Evaluation in a Translation Learner Corpus</title>
<author confidence="0.99994">Mihaela Vela Anne-Kathrin Schumann Andrea Wurm</author>
<affiliation confidence="0.9789445">Department of Applied Linguistics, Translation and Saarland University, Saarbrücken, Germany</affiliation>
<email confidence="0.996799">m.vela@mx.uni-saarland.de</email>
<email confidence="0.996799">anne.schumann@mx.uni-saarland.de</email>
<email confidence="0.996799">a.wurm@mx.uni-saarland.de</email>
<abstract confidence="0.997857441666668">The realisation that fully automatic translation in many settings is still far from producing output that is equal or superior to human translation has lead to an intense interest in translation evaluation in the MT community. However, research in this field, by now, has not only largely ignored the tremendous amount of relevant knowledge available in a closely related discipline, namely translation studies, but also failed to provide a deeper understanding of the nature of &amp;quot;translation errors&amp;quot; and &amp;quot;translation quality&amp;quot;. This paper presents an empirical take on the latter concept, translation quality, by comparing human and automatic evaluations of learner translations in the KOPTE corpus. We will show that translation studies provide sophisticated concepts for translation quality estimation and error annotation. Moreover, by applying well-established MT evaluation scores, namely BLEU and Meteor, to KOPTE learner translations that were graded by a human expert, we hope to shed light on properties (and potential shortcomings) of these scores. 1 Translation quality assessment In recent years, researchers in the field of MT evaluation have proposed a large variety of methods for assessing the quality of automatically produced translations. Approaches range from fully automatic quality scoring to efforts aimed at the development of &amp;quot;human&amp;quot; evaluation scores that try to exploit the (often tacit) linguistic knowledge of human evaluators. The criteria according to which is estimated often include the of meaning preservation, and target language correctness (Callison-Burch et al., 2007). The goals of both &amp;quot;human&amp;quot; evaluation and fully automatic quality scoring are manifold and cover system optimisation as well as benchmarking and comparison. In translation studies, the scientific (and prescientific) discussion on how to assess the quality of human translations has been going on for centuries. In recent years, the development of appropriate concepts and tools has become even more vital to the discipline due to the pressing needs of the language industry. However, different from the belief, typical to MT, that the &amp;quot;goodness&amp;quot; of a translation can be scored on the basis of linguistic criteria alone, the notion of &amp;quot;translation quality&amp;quot;, in translation studies, has assumed a multi-faceted shape, distancing itself from a simple strive for equivalence and embracing concepts such as functional, stylistic and pragmatic appropriateness as well as textual coherence. In this section, we provide an overview over approaches to translation quality assessment developed in MT and translation studies to specify how &amp;quot;quality&amp;quot; is being defined in both fields and which methods and features are used. Due to the amount of available literature, this overview is necessarily incomplete, but still insightful with respect to differences and commonalities between MT and human translation evaluation. 1.1 Automatic MT quality scores MT output is usually evaluated by automatic language-independent metrics which can be applied to any language produced by an MT system. The use of automatic metrics for MT evaluation is legitimate, since MT systems deal with large amounts of data, on which manual evaluation would be very time-consuming and expensive. Automatic metrics typically compute the closeness (adequacy) of a &amp;quot;hypothesis&amp;quot; to a &amp;quot;reference&amp;quot; translation and differ from each other by how this is measured. The most popular MT eval- 47 on Humans and Computer-assisted pages Sweden, 26 April 2014. Association for Computational Linguistics uation metrics are IBM BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) which are used not only for tuning MT systems, but also as evaluation metrics for shared tasks, such as the Workshop on Statistical Machine Translation (Bojar et al., 2013). IBM BLEU uses n-gram precision by matching machine translation output against one or more reference translations. It accounts for adequacy and fluency by calculating word precision, respectively the n-gram precision. In order to deal with the over generation of common words, precision counts are clipped, meaning that a reference word is exhausted after it is matched. This is then the modified n-gram precision. For N=4 the modified n-gram precision is calculated and the results are combined by using the geometric mean. Instead of recall, the brevity penalty (BP) is used. It penalizes candidate translations which are shorter than the reference translations. The NIST metric is derived from IBM BLEU. The NIST score is the arithmetic mean of modified n-gram precision for N=5 scaled by BP. Additionally, NIST also considers the information gain of each n-gram, giving more weight to more informative (less frequent) n-grams and less weight to less informative (more frequent) n-grams. Another often used machine translation evaluation metric is Meteor (Denkowski and Lavie, 2011). Different from IBM BLEU and NIST, Meteor evaluates a candidate translation by calculating precision and recall on the unigram level and combining them into a parametrized harmonic mean. The result from the harmonic mean is then scaled by a fragmentation penalty which penalizes gaps and differences in word order. Besides these evaluation metrics, several other metrics are sometimes used for the evaluation of MT output. Some of these are the WER (word error-rate) metric based on the Levensthein distance (Levenshtein, 1966), the positionindependent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). 1.2 Human MT quality evaluation Human evaluation of MT output is performed in different ways. The most frequently used evaluation method seems to be a simple ranking of transsentences by a &amp;quot;reasonable number of evaluators&amp;quot; (Farrús et al., 2010). According to Birch et al. (2013), this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular. AP- PRAISE (Federmann, 2012) is a tool that can be used for such as task, since it allows for the manual ranking of sentences, quality estimation, error annotation and post-editing. Other forms of evaluation, however, exist. For example, Birch et al. (2013) propose HMEANT, an evaluation score based on MEANT (Lo and Wu, 2011), a semi-automatic MT quality score that measures the degree of meaning preservation by comparing verb frames and semantic roles of hypothesis translations to their respective counterparts in the reference translation(s). Unfortunately, Birch et al. (2013) report difficulty in producing coherent role alignments between hypotheses and translations, a problem that affects the final HMEANT score calculation. This, however, seems hardly surprising given the difficulty of the annotation task (although, following the authors’ description, some familiarity of the annotators with the linguistic key concepts can be assumed) and the fact that guidelines and training are meant to be minimal. Another (indirect) human evaluation method for MT that is also employed for error analysis are reading comprehension tests (e.g. Maney et al. (2012), Weiss and Ahrenberg (2012)). Moreover, HTER (Snover et al., 2006) is a TER-based repair-oriented metric which uses human annotators (the only apparent qualificational requirement being fluency in the target language) to generate &amp;quot;targeted&amp;quot; reference translations by post-editing the MT output or the existing reference translations, following the goal to find the shortest path between the hypothesis and a &amp;quot;correct&amp;quot; reference. Snover et al. (2006) report a high correlation between evaluation with HTER and traditional human adequacy and fluency judgements. Last but not least, Somers (2011) mentions other repairoriented measures such as post-editing effort measured by the amount of key-strokes or time spent on producing a &amp;quot;correct&amp;quot; translation on the basis of MT output. 1.3 The notion of quality in translation studies Discussions of translation &amp;quot;quality&amp;quot;, in translation for a long time focused on 48 which, in its oldest and simplest form, used to understood by today’s MT researchers: &amp;quot;good&amp;quot; translation was viewed as an optimal compromise between meaning preservation and target language correctness, which was especially relevant to the translation of religious texts. For example, Kußmaul (2000) emphatically cites Martin Luther’s famous Bible translation into German as an example of &amp;quot;good&amp;quot; translation because Luther, according to his own testimony and following his reformative ambition, focused on producing fluent, easily understandable text rather than mimicking the linguistic structures of the Hebrew, Aramaic and Greek originals (see also Windle and Pym (2011) for a further discussion). More recent work in translation studies has abandoned one-dimensional views of the relation between source and target text and postulates that, depending on the communicative context within and for which a translation is produced, this relation can vary greatly. That is, the degree of linguistic or semantic &amp;quot;fidelity&amp;quot; of a good translation towards the source text depends on functional criteria. This view is echoed in the concepts of &amp;quot;primary vs. secondary&amp;quot;, &amp;quot;documentary vs. instrumental&amp;quot; and &amp;quot;covert vs. overt&amp;quot; translation (Hönig, 2003). The consequence of this shift in paradigms that, since different strategies be appropriately adopted in different situations, evaluation criteria become essentially dependent the the translation is going to play in the target language and culture. This view is prominently advocated by the so-called skotheory Dizdar (2003)). then, are not just simple violations of the target language system or outright failures to translate or segments, but violations of the translatask can manifest themselves on all levels of text production (Nord, 2003). It is important point out that, in this framework, erjust one type of error covering not only one of the favourite MT error categories, namely unand mistranslated words (compare, for example, Stymne and Ahrenberg (2012), Weiss and Ahrenberg (2012), Popovi´c et al. (2013)), but also phraseological, idiomatic, syntactic, grammatical, modal, temporal, stylistic, cohesion and other of errors. Moreover, erwhen the translation does not fulfill its function because of pragmatic (e.g. text-type specific forms of address), cultural (e.g. text conventions, proper names, or other conventions) or formal (e. g. layout) defects (Nord, 2003). Depending on the appropriate translation strategy for a given translation task, these error types may be weighted differently. Furthermore, the communicative and functional view on translation also dictates a change in the concept of equivalence which is no longer considered to be adequately described by the notions of &amp;quot;meaning preservation&amp;quot; or &amp;quot;fidelity&amp;quot;, but becomes dependent on aesthetic, connotational, textual, communicative, situational, functional and cognitive aspects (for a detailed discussion see Horn-Helf (1999)). In MT evaluation, most of these aspects have not yet or only in part been considered. Last but not least, the translation industry has developed normative standards and proofreading schemes. For example, the DIN EN 15038:2006- 08 (Deutsches Institut für Normung, 2006) discusses translation errors, quality management and qualificational requirements for translators and proofreaders, while the SAE J2450 standard (Society of Automotive Engineers, 2005) presents a weighted &amp;quot;translation quality metric&amp;quot;. An application perspective is given by Mertin (2006) who discusses translation quality management procedures in a big automotive company and, among other things, develops a weighted translation error scheme for proofreading. 1.4 Discussion The above discussion shows that, while the object of evaluation is the same for both MT and translation studies, namely translation, the differences between evaluation approaches developed in both fields are considerable. Most importantly, in translation studies, translation evaluation is considered task which fluency in one or several languages is certainly not enough, but for which expert knowledge required. Another important distinction is that evaluation, again in translation studies, is normally not carried out on the sentence level, since sentences are usually split up into several &amp;quot;units of translation&amp;quot; and can certainly contain more than one &amp;quot;translation problem&amp;quot;. Consequently, the popular MT practice of ranking whole sentences according to some automatic score, by anonymous evaluators or even users of Amazon Turk (e.g. in the introduction to Bojar et al. (2013)), from a translation point of view, is unlikely to provide reason- 49 able evaluations. Last but not least, the MT community’s strive for adequacy or meaning preservation does not match the notions of weighting translation errors, of adopting different translation strategies and, consequently, does not fit the complicated source/target text relations that have been acknowledged by translation studies. Evaluation methods that are based on simple measures of linguistic equality such as n-gram overlap (BLEU) or, just slightly more complicated, the preservation of syntactic frames and semantic roles (MEANT) fail to provide straightforward criteria for distinbetween variation. Moreover, semantic and pragmatic criteria as well as the notion of &amp;quot;reference translation&amp;quot; remain, at best, rather unclear. On the other hand, the MT community has recognised translation evaluation as an unresolved research problem. For example, Birch et al. (2013) state that ranking judgements are difficult to generalise, while Callison-Burch et al. (2007) carry out extensive correlation tests of a whole range of automatic MT evaluation metrics in comparison to human judgements, showing that BLEU does not rank highest, but still remains in the top segment. It still needs to be shown how MT research can benefit from more sophisticated evaluation measures and whether all the parameters that are considered relevant to the evaluation of human translations are relevant for MT usage scenarios, too. In the remainder of this paper, we present a study on how much and possibly for which reasons automatic MT evaluation scores (namely BLEU and Meteor) differ from translation expert quality judgements on extracts of a French-German translation learner corpus. 2 The KOPTE corpus 2.1 General corpus design The KOPTE project (Wurm, 2013) was designed to enable research on translation evaluation in a university training course (master’s level) for translators and to enlighten students’ translation problems as well as their problem solving strategies. To achieve this goal, a corpus of student translations was compiled. The corpus consists of several translations of the same source texts produced by student translators in a classroom setting. As a whole, it covers 985 translations of 77 source texts amounting to a total of 318,467 tokens. Source texts were taken from French newspapers and translated into German in class over a span of several years, the translation brief calling for a ready-to-publish text to be printed in a German national newspaper. Consequently, all translation tasks include the use of idiomatic language, explanations of culture-specific items, changes in the explicitness of macrotextual coheelements, 2.2 Annotation of translation features and translation evaluation in KOPTE Student translations were evaluated by one of the authors, an experienced translation teacher, with the aim of giving feedback to students. All transwere graded and well as marked in the text according to a fine-grained evaluation scheme. In this scheme, the weight of evaluated items is indicated through numbers ranging from plus/minus 1 (minor) to plus/minus 8 (major). Based on these evaluations, each translation was assigned a final grade according to the German grading system on a scale ranging from 1 (&amp;quot;very good&amp;quot;) to 6 (&amp;quot;highly erroneous&amp;quot;) with in-between intervals at the levels of .0, .3 and .7. To calculate this grade, positive and negative evaluations were summed up separately, before the negative score was subtracted from the positive one. A score of around zero corresponds to the grade &amp;quot;good&amp;quot; (=2), to achieve &amp;quot;very good&amp;quot; (=1) the student needs a surplus of positive evaluations. The evaluation scheme based on which student translations are graded is divided into external internal factors. describe the communicative situation given by the source text and the translation brief (author, remedium, location, time). factors, on the other hand, comprise eight categories: form, structure, cohesion, stylistics/register, grammar, lexis/semantics, translation-specific problems, function. These categories are containers for more fine-grained criteria which can be applied to segments of the (source or target) text or even to the whole text, depending on the nature of the criterion. Some internal subcriteria of the scheme are summarised in Table 1. A quantitative analysis of error types in KOPTE shows that semantic/lexical errors are by far the most common error in the student translations (Wurm, 2013). Evaluations in KOPTE were carried out by just information about KOPTE is available from http://fr46.uni-saarland.de/index.php?id=3702&amp;L=%2524L. 50 one evaluator for the reason that, in a classroom setting, multiple evaluations are not feasible. Although multiple evaluations would have been considered highly valuable, the data available from KOPTE was evaluated by an experienced translation scholar with long-standing experience in teaching translation. Moreover, the evaluation scheme is much more detailed than error annotation schemes that are normally described in the literature and it is theoretically well-motivated. An analysis of the median grades in our data sample (compare Tables 2–4) shows that grading varies only slightly between different texts, considering the maximum variation potential ranging from 1 to 6, and thus can be considered consistent. Criteria Examples of subcriteria author, recipients, medium, topic, — location, time form paragraphs, formatting structure thematic, progression, macrostructure, illustrations cohesion reference, connections stylistics style, genre grammar determiners, modality, syntax semantics textual semantics, idioms, numbers, terminology translation erroneous source problems text, proper names, culture-specific items, ideology, math. units, pragmatics, allusions function goal dependence Table 1: Internal evaluation criteria in the KOPTE annotation scheme. 3 Experiments The goal of our experiments was to study whether the human translation expert judgements in KOPTE can be mimicked using simple automatic quality metrics as used in MT, namely BLEU and Meteor. More specifically, we aim at: • studying how automatic evaluation scores relate to fine-grained human expert evaluations, • investigating whether a higher number of references improves the automatic scores and why (or why not), • examining whether a higher number of references provides more reliable evaluation scores as measured by an improved correlation with the human expert judgments. In order to study the behaviour of automatic MT evaluation scores, we conducted three experiments by applying IBM BLEU (Papineni et al., 2002) and Meteor 1.4 (Denkowski and Lavie, 2011) to a sample of KOPTE translations that were produced by translation students preparing for their final master’s exams. Scores were calculated on the complete texts. To evaluate the overall performance of the automatic evaluation scores on these texts, we calculated Kendall’s rank correlation coefficient for each text following the procedure described in Sachs and Hedderich (2009). Correlations were calculated for: • the human expert grades and BLEU scores for each translation, • the human expert grades and Meteor scores for each translation, • BLEU and Meteor scores for each translation. 3.1 Experimental setup and results In a first experiment, we applied the automatic evaluation scores to the source texts given in Table 2, choosing, for each text, the student translation with the best human grade as reference translation. The median human grades as well as mean BLEU and Meteor and correlation scores obtained for each text (excluding the reference translation) are included in Table 2. In a second experiment, we repeated this procedure, however, using a set of three reference translations. Results are given in Table 3. Finally, in a last experiment we used five reference translations selected according to their human expert grade (Table 4). In both steps, source texts for which less than four hypotheses were available were excluded from the data sets. 3.2 Discussion The tables show that in the first experiment a set of 152 translations was evaluated, whereas in the second and third experiment these numbers were reduced to 108 and 68 respectively due to the selection of more references. The human expert evaluations rated most of these translations at least as acceptable, as can be seen from the median grade for each experiment which was 2.3 in the first experiment and consecutively decreased to 3.0 for the third experiment, again due to the selection of more &amp;quot;good&amp;quot; translations as references. The 51 Source text Human trans./ source text grades BLEU Mean Meteor Correlation Human-BLEU Correlation Human-Meteor Correlation BLEU-Meteor</abstract>
<note confidence="0.920662029411765">AT001 7 2. 7 0. 15 0. 33 39 73 0. 24 AT002 12 2. 3 0. 15 0. 35 20 43 0. 49 AT004 12 2. 7 0. 19 0. 37 0. 14 0. 11 0. 63 AT005 12 2. 3 0. 20 0. 36 0. 32 0. 45 0. 45 AT008 10 2. 15 0. 23 0. 38 43 29 0. 78 AT010 11 2. 7 0. 25 0. 41 0. 06 10 0. 56 AT012 9 2. 0 0. 22 0. 40 30 36 0. 50 AT015 5 2. 0 0. 11 0. 28 0. 36 0. 12 0. 60 AT017 7 2. 3 0. 22 0. 38 20 0. 06 0. 71 AT021 4 3. 0 0. 18 0. 39 55 55 1. 00 AT023 6 2. 3 0. 22 0. 38 0. 50 07 20 AT025 4 2. 15 0. 13 0. 36 0. 33 0. 0 0. 00 AT026 21 3. 0 0. 12 0. 26 19 35 0. 67 AT039 13 3. 0 0. 10 0. 29 08 0. 03 0. 49 AT052 7 2. 0 0. 17 0. 31 32 0. 05 0. 00 AT053 7 2. 3 0. 18 0. 32 0. 62 0. 39 0. 33 AT059 5 2. 0 0. 24 0. 36 0. 00 0. 22 0. 80 Table 2: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the BLEU and Meteor scores per source text and Kendall’s rank correlation coefficients for the first experiment. Source text Human trans./ source text grades BLEU Meteor Correlation Correlation Human-Meteor Correlation BLEU-Meteor AT001 5 3. 0 0. 17 0. 36 12 0. 36 0. 60 AT002 10 2. 3 0. 17 0. 36 14 0. 05 0. 38 AT004 10 2. 85 0. 20 0. 37 0. 39 0. 16 0. 51 AT005 10 2. 3 0. 20 0. 40 10 0. 05 0. 47 AT008 8 2. 5 0. 25 0. 45 67 15 0. 00 AT010 9 2. 7 0. 23 0. 41 10 50 0. 28 AT012 7 2. 3 0. 23 0. 43 0. 00 0. 11 0. 52 AT017 5 2. 3 0. 21 0. 43 0. 12 0. 36 0. 60 AT023 4 2. 5 0. 21 0. 38 0. 41 0. 81 0. 67 AT026 19 3. 3 0. 10 0. 26 31 41 0. 77 AT039 11 3. 0 0. 11 0. 34 0. 06 0. 14 0. 74 AT052 5 2. 0 0. 18 0. 40 0. 12 0. 36 0. 20 AT053 5 2. 3 0. 17 0. 35 0. 36 12 0. 40 Table 3: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the</note>
<abstract confidence="0.998003863013699">BLEU and Meteor scores per source text and Kendall’s rank correlation coefficients for the second experiment. grades for the best translations selected as references range for the first and second experiment between 1.0 and 2.3, whereas for the third experiment the selected references were evaluated with grades between 1.0 and 2.7. Nevertheless, the median grade for the references in all three experiments is always 1.7. From the overall median grade and the median grade of the selected translations as reference we can notice, that the translations selected as references were indeed &amp;quot;better&amp;quot; than the remaining ones. The BLEU and Meteor scores given in the tables are mean values over the individual translations’ scores for each source text. These scores are very low, reaching a maximum of 0.25 over all three experiments for BLEU and 0.45 for Meteor. However, given the human expert grades the translations cannot be considered unreadable. In fact, the correlation coefficients show that neither BLEU nor Meteor (except a few exceptional cases) correlate with the human quality judgements, however, they show a (weak) tendency to correlate with each other. Moreover, the data shows that the addition of reference translations results neither in significantly higher BLEU or Meteor scores nor in improved correlation. 3.3 Qualitative analysis Our finding that human quality judgements do not correlate with automatic scores if the object of evaluation is a translation produced by a human (as opposed to a machine) matches earlier results presented by Doddington (2002) within the context of evaluating NIST. Doddington (2002) proposes the explanation that &amp;quot;differences between professional translators are far more subtle [than differences between machine-produced translations, the authors] and thus less well characterized 52 Source text Human trans./ source text grades BLEU Meteor Correlation Human-BLEU Correlation Human-Meteor Correlation BLEU-Meteor AT002 8 2. 5 0. 17 0. 36 08 0. 00 0. 43 AT004 8 3. 0 0. 20 0. 36 0. 00 0. 23 0. 71 AT005 8 2. 3 0. 20 0. 42 0. 00 0. 08 0. 43 AT008 6 2. 85 0. 26 0. 45 55 14 0. 33 AT010 7 2. 7 0. 23 0. 41 0. 00 12 0. 05 AT012 5 2. 3 0. 23 0. 43 0. 22 0. 22 0. 40 AT026 17 3. 3 0. 11 0. 31 24 34 0. 62 AT039 9 3. 0 0. 10 0. 37 0. 22 0. 55 0. 22 Table 4: Source texts, number of human translations per source text, median of the obtained grade per source text, mean of the BLEU and Meteor scores per source text and Kendall’s rank correlation coefficients for the third experiment. by N-gram statistics.&amp;quot; We conducted a qualitative analysis of some KOPTE translations in order to check whether the differences between individual translations are indeed as subtle as suggested by Doddington and to come up at least with hypotheses that could explain the poor performance of the automatic scores. We selected three source texts used in the second experiment, namely AT008, AT023 and AT053 and compared their respective reference translations to selected hypothesis translations. This analysis was conducted on the lexical level alone, that is, most of the features of KOPTE’s elaborated evaluation scheme were not even considered. The analysis, however, shows that the amount of variation that can be found just on the lexical level is almost overwhelming. Some examples are listed in Appendix A. A common phenomenon is simple variation due to synonyms or the use of phrasal variants or paraphrases. Moreover, the listed examples show that lexical variation can be triggered by different source text elements. The phenomena shown in the tables are well-known translation problems, e.g. proper names, colloquial or figurative speech or numbers. The other categories in the table are less clear-cut, that is, they can overlap. In our analysis, source text elements that cannot be translated literally, but instead call for a creative solution were classified as translation problems. Different translation strategies can be applied to different kinds of problems, most importantly to the translation of culture-specific items, proper names, underspecified source text elements or culture-specific arguments. The respective table and other examples that we analysed show that for this category some translators chose to add additional information, to adapt the perspective to the German target audience (for example, by adapting pronouns or deictic elements) or to adapt the formatting choices to the variant preferred by the target culture (e.g. commas instead of fullstops, different types of quotation marks), whereas other translators chose to translate literally. Both strategies are legitimate under certain circumstances, however, it can be assumed that adaptations require a greater cognitive effort. Source ambiguities, according to our preliminary typology, are source text features that can be interpreted in different ways at least for a translator translating from a foreign language (as opposed to a native speaker). Obviously, the line between this category and outright translation errors is not very clear. However, it needs to be stated that also for the other categories while many variants are correct and legitimate not all are equally good. Best solutions for given problems are distributed unequally across the translations studied. Beyond the purely lexical level, extensive variation can be witnessed on the syntactic, but also the grammatical level. For example, some translators chose to break the rather complicated syntax of the French original into simpler, easily readable sentences, producing, in some cases, considerable shifts in the information structure of the text often a legitimate strategy. With respect to the performance of the automatic scores, our preliminary study that still calls for larger-scale and in-depth verification suggests that neither BLEU nor Meteor are able to cope with the amount of variation found in the data. More specifically, they cannot distinguish between legitimate and illegitimate variation or grave and slight errors respectively, but seem to fail to match acceptable variants because of lexical and phrasal variation or divergent grammatical structures resulting in different verb frames, word sequences and text lengths, not to talk even about acceptable variation on higher linguistic levels. Therefore, scores seem to overrate surface differ- 53 ences and thus assign very low scores to many translations that were found to be at least acceptable by a human expert. Considering the impact of these findings for MT evaluation purposes, it is not straightforward to assume that the differences that we have observed between the human translations are more &amp;quot;subtle&amp;quot; (in the sense of being unimportant) than the ones produced by machine translation systems. On the contrary, our analysis suggests that &amp;quot;good&amp;quot; translations are characterised by creative solutions that are not easily reproducible but that help to achieve target language readability and comprehensibility. This is a fundamental quality aspect of translation independently of its production mode. Moreover, it is difficult to see why some of the variants that we observed in the human translations selected from KOPTE, once the context shifts from human to machine translation, should be found valid in one situation and invalid in another, depending on the training and test data used for developing an MT system: A high amount of the variation found in the human translations goes back to the legitimate use of the creative and constructive powers of natural language, and it is, among others, these powers that should be mimicked by MT output. 4 Conclusion and future work In this paper, we have studied the performance of two fully automatic MT evaluation metrics, namely BLEU and Meteor, in comparison to human translation expert evaluations on a sample of learner translations from the KOPTE corpus. The automatic scores were tested in three experiments with a varying number of reference translations and their performance was compared to the human evaluations by means of Kendall’s rank correlation coefficient. The experiments suggest that both BLEU and Meteor systematically underestimate the quality of the translations tested, that is, they assign scores that, given the human expert evaluations, seem to be by far too low. Moreover, they do not consistently correlate with the human expert evaluations. Coming up with explanations for this failure is not straightforward, however, the results of our qualitative and explorative analysis suggest that lexical similarity scores are not able to cope satisfactorily neither with standard lexical variation (paraphrases etc.) nor with dissimilarities that can be traced back to the specific nature of the translation process, leave alone linguistic levels beyond the lexicon. For Meteor, this shortcoming may partly be alleviated by the provision of richer sets of synonyms and paraphrases, however, the amount of uncovered variation is still immense. In fact, it seems that many more reference translations would be needed in order to cover the whole range of legitimate variants that can be used to translate a given source text a scenario that seems hardly feasible! So how can BLEU or Meteor scores be interpreted when they are given in MT papers? Based on our analyses, it seems clear that these scores are based on a data-driven notion of translation quality, that is, they measure the degree of compliance of a hypothesis translation with some reference set. This is insofar problematic as studies based on different reference sets cannot be compared, neither can BLEU or Meteor scores be generalised to other domains. Even more importantly, BLEU or Meteor scores cannot be used to measure a data-independent concept of quality or even the usability of a translation for a target audience which, as we have shown, depends on many more factors than just lexical surface overlap. However, our study also leads to some open research questions. One of these questions is whether automatic evaluation scores can still be used for more coarse-grained distinctions, that is, to distinguish &amp;quot;really bad&amp;quot; translations from &amp;quot;really good&amp;quot; ones. The fine-grained distinctions made by the evaluator of KOPTE on generally rather good translations do not allow us to answer this question. Future work will also deal with a comparison of mistakes made by MT systems as opposed to human translators as well as with the question how (and which) translation-specific aspects can be applied to the evaluation of MT systems.</abstract>
<title confidence="0.753953">References</title>
<author confidence="0.7473">Alexandra Birch</author>
<author confidence="0.7473">Barry Haddow</author>
<author confidence="0.7473">Ulrich Germann</author>
<author confidence="0.7473">Maria Nadejde</author>
<author confidence="0.7473">Christian Buck</author>
<author confidence="0.7473">Philipp Koehn</author>
<note confidence="0.887474962962963">2013. The feasibility of HMEANT as a human MT metric. In of the 8th Workon pages 52–61. Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Hervé Saint-Amand, Radu Soricut, and Lucia editors. 2013. of the 8th Workon ACL. Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007. 54 evaluation of machine translation. In Proof the 2nd Workshop on pages 136– 158. Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and of machine translation systems. In Proof the 6th Workshop on pages 85–91. Institut für Normung. 2006. EN 15038:2006-08: Übersetzungsdienstleistungen- Beuth. Dizdar. 2003. Skopostheorie. In pages 104–107. Stauffenburg. George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram costatistics. In of the 2nd In- Conference on pages 138–145.</note>
<author confidence="0.524777">Mireia Farrús</author>
<author confidence="0.524777">Marta R Costa-Jussà</author>
<author confidence="0.524777">José B Mar-</author>
<abstract confidence="0.830569035714286">iño, and José A. R. Fonollosa. 2010. Linguisticbased evaluation criteria to identify statistical matranslation errors. In of the 14th Conference of the pages 167–173. Christian Federmann. 2012. Appraise: An opensource toolkit for manual evaluation of machine output. 98:25–35, 9. Hans Hönig. 2003. Humanübersetzung (therapeutisch diagnostisch). In pages 378–381. Stauffenburg. Horn-Helf. 1999. Übersetzen in und Franke. Kußmaul. 2000. Stauffenburg. Vladimir Iosifovich Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and rever- Physics 10(8):707–710. Chi-Kiu Lo and Dekai Wu. 2011. MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic In of the 49th Annual Meeting of pages 220–229. Tucker Maney, Linda Sibert, Dennis Perzanowski, Kalyan Gupta, and Astrid Schmidt-Nielsen. 2012. Toward determining the comprehensibility of matranslations. In of the 1st pages 1–7. Mertin. 2006. Qualitäts-</abstract>
<title confidence="0.712246">im Dienstleistungsbereich</title>
<author confidence="0.79612">Peter Lang</author>
<note confidence="0.72206692">Christiane Nord. 2003. Transparenz der Korrektur. pages 384–387. Stauffenburg. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evalof machine translation. In of the Annual Meeting of the pages 311–318. Maja Popovi´c, Eleftherios Avramidis, Aljoscha Burchardt, Sabine Hunsicker, Sven Schmeier, Cindy Tscherwinka, David Vilar, and Hans Uszkoreit. 2013. Learning from human judgements of machine output. In pages 231–238. Sachs and Jürgen Hedderich. 2009. Ange- Statistik. Methodensammlung mit Springer. Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. pages 223–231. Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a MT metric. In of the 4th Workon pages 259–268. of Automotive Engineers. 2005.</note>
<title confidence="0.883309">Translation Quality SAE.</title>
<author confidence="0.8205155">Machine translation History</author>
<author confidence="0.8205155">limitations In Oxford Hand-</author>
<affiliation confidence="0.731879">of Translation pages 427–440. Oxford University Press.</affiliation>
<note confidence="0.573377333333333">Sara Stymne and Lars Ahrenberg. 2012. On the practice of error analysis for machine translation evalua- In of the 8th pages 1785– 1790. Christoph Tillmann, Stephan Vogel, Hermann Ney, Alexander Zubiaga, and Hassan Sawaf. 1997. Ac-</note>
<abstract confidence="0.878122899082569">celerated DP based search for statistical translation. of the pages 2667– 2670. Sandra Weiss and Lars Ahrenberg. 2012. Error profiling for evaluation of machine-translated text: a case study. In of the pages 1764–1770. Kevin Windle and Anthony Pym. 2011. European on secular translation. In Oxford of Translation pages 7–22. Oxford University Press. Wurm. 2013. Eigennamen und Realia in einem Korpus studentischer Übersetzungen 6(2):381–419. 55 A Examples of lexical variation in human translation In the examples below, bold face indicates the French source. A.1 Proper names président gabonais Präsidenten von Gabon Präsidenten Gabuns Präsidenten von Gabun Präsident des afrikanischen Landes Gabon gabunesischen Präsidenten la Commission nationale de l’informatique et des libertés (CNIL) Commission nationale de l’informatique et des libertés (CNIL) französische Datenschutzbehörde (CNIL) französische Datenschutzkommission CNIL französische Datenschutzbehörde CNIL französische Kommission für Datenschutz (CNIL) A.2 Problematic source text elements (translation problems) pivot de l’influence française Stützpunkt des Einflusses Frankreichs zentralen Figur des französischen Einfluss Stütze für den Einfluss Frankreichs Schlüsselfigur für den Einfluss Frankreichs Garant für den französischen Einfluß A.3 Paraphrases sera-t-elle capable es schaffen fähig sein in der Lage sein sich als fähig erweisen &amp;quot;doyen de l’Afrique&amp;quot; obersten Würdenträgers Afrikas &amp;quot;Alten Herrn von Afrika&amp;quot; &amp;quot;Abtes von Afrika&amp;quot; &amp;quot;Ältesten von Afrika&amp;quot; &amp;quot;doyen de l’Afrique&amp;quot; se tenir à la bonne distance auf angemessener Distanz zu bleiben sich nicht einzumischen sich herauszuhalten die gebührende Neutralität zu wahren A.4 Culture-specific elements and underspecified source text items la &amp;quot;Françafrique&amp;quot; &amp;quot;Françafrique&amp;quot; Französisch-Afrika (&amp;quot;Françafrique&amp;quot;) „Franzafrika“ &amp;quot;Frankafrika&amp;quot; &amp;quot;Françafrique&amp;quot; d.h. der französisch beeinflussten Gebiete Afrikas les &amp;quot;voitures Google&amp;quot;, équipées de caméras à 360 degrés mit 360-Grad-Kameras ausgestatteten &amp;quot;Google-Kamerawagen&amp;quot; Kamera-Autos Street-View-Wagen mit ihren 360°-Kameras &amp;quot;Google-Autos&amp;quot;, die auf dem Dach eine 360-Grad-Kamera montiert haben, mit 360-Grad-Kameras ausgestatteten &amp;quot;Street View-Autos&amp;quot; A.5 Source text ambiguities (syntactic and semantic) la France a soutenu un régime autoritaire et prédateur, sans pitié pour les opposants autoritären Systems [...], das kein Mitleid mit seinen Gegnern zeigte hat Frankreich ohne Rücksicht auf Regimekritiker ein autoritäres Gewaltregime unterstützt autoritäre und ausbeutende Regime [...], welches keine Gnade für seine Gegner kannte autoritäres und angriffslustiges Regime [...], das kein Mitleid mit seinen Gegnern hatte hat Frankreich dieses autoritäre und ausbeuterische System, ohne Mitleid mit dessen Gegnern, gestützt justes paroles hat die Wahrheit gesagt hat [...] die richtigen Worte gefunden hat die richtigen Worte gefunden Aussage [...] war nichts als Worte hat genau das Richtige gesagt A.6 Numbers une amende de 100 000 euros Geldstrafe in Höhe von 100 000 Euro Strafe von 100 000C Geldstrafe von 100.000,- EUR Geldstrafe in Höhe von 100.000 Euro Bußgeld in Höhe von 100 000C A.7 Colloquial or figurative speech Je vais vite Ich beeile mich Ich mache es schnell Ich bewege mich schnell Ich hab’s eilig Ich beeile mich photographe Yann Arthus-Bertrand, 63 ans 63jährigen Fotografen Yann Arthus-Betrand Fotographen Yann Arthus-Bertrand (63 Jahre) Fotografen Yann Arthus-Bertrand (63) 63-jährigen Fotografen Y.A.B. Fotografen Yann Arthus-Bertrand, 63 résultats des petits frères Einnahmen der Vorgänger Verdienste zusätzlicher kleiner Artikel Einnahmen durch andere Produkte Erlöse von Merchandising Einnahmen aus dem Merchandising A.8 Source text element triggering correct and incorrect translations 65 chaînes de télévision, dont France 2 et 23 chaînes en Afrique 65 Fernsehsendern, darunter auch France 2 und 23 afrikanische Sender</abstract>
<address confidence="0.715163">65 Fernsehsendern, unter anderem France 2 und 23 Sender in Afrika 65 Fernsehsender, darunter der französische Sender France 2 und 23 afrikanische Sender 65 Fernsehkanälen, u.a. 2 in Frankreich und 23 in Afrika 65 Fernsehkanälen, darunter France 2 und 23 afrikanische Sender 56</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Barry Haddow</author>
<author>Ulrich Germann</author>
<author>Maria Nadejde</author>
<author>Christian Buck</author>
<author>Philipp Koehn</author>
</authors>
<title>The feasibility of HMEANT as a human MT evaluation metric.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on SMT,</booktitle>
<pages>52--61</pages>
<contexts>
<context position="6304" citStr="Birch et al. (2013)" startWordPosition="976" endWordPosition="979">etimes used for the evaluation of MT output. Some of these are the WER (word error-rate) metric based on the Levensthein distance (Levenshtein, 1966), the positionindependent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). 1.2 Human MT quality evaluation Human evaluation of MT output is performed in different ways. The most frequently used evaluation method seems to be a simple ranking of translated sentences by a &amp;quot;reasonable number of evaluators&amp;quot; (Farrús et al., 2010). According to Birch et al. (2013), this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular. APPRAISE (Federmann, 2012) is a tool that can be used for such as task, since it allows for the manual ranking of sentences, quality estimation, error annotation and post-editing. Other forms of evaluation, however, exist. For example, Birch et al. (2013) propose HMEANT, an evaluation score based on MEANT (Lo and Wu, 2011), a semi-automatic MT quality score that measures the degree of meaning preservation by comparing verb frames and semantic roles of hypothesis transla</context>
<context position="14298" citStr="Birch et al. (2013)" startWordPosition="2189" endWordPosition="2192">een acknowledged by translation studies. Evaluation methods that are based on simple measures of linguistic equality such as n-gram overlap (BLEU) or, just slightly more complicated, the preservation of syntactic frames and semantic roles (MEANT) fail to provide straightforward criteria for distinguishing between legitimate and illegitimate variation. Moreover, semantic and pragmatic criteria as well as the notion of &amp;quot;reference translation&amp;quot; remain, at best, rather unclear. On the other hand, the MT community has recognised translation evaluation as an unresolved research problem. For example, Birch et al. (2013) state that ranking judgements are difficult to generalise, while Callison-Burch et al. (2007) carry out extensive correlation tests of a whole range of automatic MT evaluation metrics in comparison to human judgements, showing that BLEU does not rank highest, but still remains in the top segment. It still needs to be shown how MT research can benefit from more sophisticated evaluation measures and whether all the parameters that are considered relevant to the evaluation of human translations are relevant for MT usage scenarios, too. In the remainder of this paper, we present a study on how mu</context>
</contexts>
<marker>Birch, Haddow, Germann, Nadejde, Buck, Koehn, 2013</marker>
<rawString>Alexandra Birch, Barry Haddow, Ulrich Germann, Maria Nadejde, Christian Buck, and Philipp Koehn. 2013. The feasibility of HMEANT as a human MT evaluation metric. In Proceedings of the 8th Workshop on SMT, pages 52–61.</rawString>
</citation>
<citation valid="true">
<date>2013</date>
<booktitle>Proceedings of the 8th Workshop on SMT. ACL.</booktitle>
<editor>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Hervé Saint-Amand, Radu Soricut, and Lucia Specia, editors.</editor>
<contexts>
<context position="6304" citStr="(2013)" startWordPosition="979" endWordPosition="979">or the evaluation of MT output. Some of these are the WER (word error-rate) metric based on the Levensthein distance (Levenshtein, 1966), the positionindependent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). 1.2 Human MT quality evaluation Human evaluation of MT output is performed in different ways. The most frequently used evaluation method seems to be a simple ranking of translated sentences by a &amp;quot;reasonable number of evaluators&amp;quot; (Farrús et al., 2010). According to Birch et al. (2013), this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular. APPRAISE (Federmann, 2012) is a tool that can be used for such as task, since it allows for the manual ranking of sentences, quality estimation, error annotation and post-editing. Other forms of evaluation, however, exist. For example, Birch et al. (2013) propose HMEANT, an evaluation score based on MEANT (Lo and Wu, 2011), a semi-automatic MT quality score that measures the degree of meaning preservation by comparing verb frames and semantic roles of hypothesis transla</context>
<context position="10711" citStr="(2013)" startWordPosition="1664" endWordPosition="1664">ed by the so-called skopos theory (cf. Dizdar (2003)). Translation errors, then, are not just simple violations of the target language system or outright failures to translate words or segments, but violations of the translation task that can manifest themselves on all levels of text production (Nord, 2003). It is important to point out that, in this framework, linguistic errors are just one type of error covering not only one of the favourite MT error categories, namely un- and mistranslated words (compare, for example, Stymne and Ahrenberg (2012), Weiss and Ahrenberg (2012), Popovi´c et al. (2013)), but also phraseological, idiomatic, syntactic, grammatical, modal, temporal, stylistic, cohesion and other kinds of errors. Moreover, translation-specific errors occur when the translation does not fulfill its function because of pragmatic (e.g. text-type specific forms of address), cultural (e.g. text conventions, proper names, or other conventions) or formal (e. g. layout) defects (Nord, 2003). Depending on the appropriate translation strategy for a given translation task, these error types may be weighted differently. Furthermore, the communicative and functional view on translation also</context>
<context position="13308" citStr="(2013)" startWordPosition="2044" endWordPosition="2044">k for which fluency in one or several languages is certainly not enough, but for which translation-specific expert knowledge is required. Another important distinction is that evaluation, again in translation studies, is normally not carried out on the sentence level, since sentences are usually split up into several &amp;quot;units of translation&amp;quot; and can certainly contain more than one &amp;quot;translation problem&amp;quot;. Consequently, the popular MT practice of ranking whole sentences according to some automatic score, by anonymous evaluators or even users of Amazon Turk (e.g. in the introduction to Bojar et al. (2013)), from a translation studies point of view, is unlikely to provide reason49 able evaluations. Last but not least, the MT community’s strive for adequacy or meaning preservation does not match the notions of weighting translation errors, of adopting different translation strategies and, consequently, does not fit the complicated source/target text relations that have been acknowledged by translation studies. Evaluation methods that are based on simple measures of linguistic equality such as n-gram overlap (BLEU) or, just slightly more complicated, the preservation of syntactic frames and seman</context>
</contexts>
<marker>2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Hervé Saint-Amand, Radu Soricut, and Lucia Specia, editors. 2013. Proceedings of the 8th Workshop on SMT. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2007</date>
<contexts>
<context position="1960" citStr="Callison-Burch et al., 2007" startWordPosition="282" endWordPosition="285"> (and potential shortcomings) of these scores. 1 Translation quality assessment In recent years, researchers in the field of MT evaluation have proposed a large variety of methods for assessing the quality of automatically produced translations. Approaches range from fully automatic quality scoring to efforts aimed at the development of &amp;quot;human&amp;quot; evaluation scores that try to exploit the (often tacit) linguistic knowledge of human evaluators. The criteria according to which quality is estimated often include adequacy, the degree of meaning preservation, and fluency, target language correctness (Callison-Burch et al., 2007). The goals of both &amp;quot;human&amp;quot; evaluation and fully automatic quality scoring are manifold and cover system optimisation as well as benchmarking and comparison. In translation studies, the scientific (and prescientific) discussion on how to assess the quality of human translations has been going on for centuries. In recent years, the development of appropriate concepts and tools has become even more vital to the discipline due to the pressing needs of the language industry. However, different from the belief, typical to MT, that the &amp;quot;goodness&amp;quot; of a translation can be scored on the basis of lingui</context>
<context position="14392" citStr="Callison-Burch et al. (2007)" startWordPosition="2203" endWordPosition="2206">measures of linguistic equality such as n-gram overlap (BLEU) or, just slightly more complicated, the preservation of syntactic frames and semantic roles (MEANT) fail to provide straightforward criteria for distinguishing between legitimate and illegitimate variation. Moreover, semantic and pragmatic criteria as well as the notion of &amp;quot;reference translation&amp;quot; remain, at best, rather unclear. On the other hand, the MT community has recognised translation evaluation as an unresolved research problem. For example, Birch et al. (2013) state that ranking judgements are difficult to generalise, while Callison-Burch et al. (2007) carry out extensive correlation tests of a whole range of automatic MT evaluation metrics in comparison to human judgements, showing that BLEU does not rank highest, but still remains in the top segment. It still needs to be shown how MT research can benefit from more sophisticated evaluation measures and whether all the parameters that are considered relevant to the evaluation of human translations are relevant for MT usage scenarios, too. In the remainder of this paper, we present a study on how much and possibly for which reasons automatic MT evaluation scores (namely BLEU and Meteor) diff</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007.</rawString>
</citation>
<citation valid="false">
<title>evaluation of machine translation.</title>
<booktitle>In Proceedings of the 2nd Workshop on SMT,</booktitle>
<pages>136--158</pages>
<marker></marker>
<rawString>(Meta-) evaluation of machine translation. In Proceedings of the 2nd Workshop on SMT, pages 136– 158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Workshop on SMT,</booktitle>
<pages>85--91</pages>
<contexts>
<context position="5306" citStr="Denkowski and Lavie, 2011" startWordPosition="812" endWordPosition="815">culated and the results are combined by using the geometric mean. Instead of recall, the brevity penalty (BP) is used. It penalizes candidate translations which are shorter than the reference translations. The NIST metric is derived from IBM BLEU. The NIST score is the arithmetic mean of modified n-gram precision for N=5 scaled by BP. Additionally, NIST also considers the information gain of each n-gram, giving more weight to more informative (less frequent) n-grams and less weight to less informative (more frequent) n-grams. Another often used machine translation evaluation metric is Meteor (Denkowski and Lavie, 2011). Different from IBM BLEU and NIST, Meteor evaluates a candidate translation by calculating precision and recall on the unigram level and combining them into a parametrized harmonic mean. The result from the harmonic mean is then scaled by a fragmentation penalty which penalizes gaps and differences in word order. Besides these evaluation metrics, several other metrics are sometimes used for the evaluation of MT output. Some of these are the WER (word error-rate) metric based on the Levensthein distance (Levenshtein, 1966), the positionindependent error rate metric PER (Tillmann et al., 1997) </context>
<context position="20175" citStr="Denkowski and Lavie, 2011" startWordPosition="3080" endWordPosition="3083">ty metrics as used in MT, namely BLEU and Meteor. More specifically, we aim at: • studying how automatic evaluation scores relate to fine-grained human expert evaluations, • investigating whether a higher number of references improves the automatic scores and why (or why not), • examining whether a higher number of references provides more reliable evaluation scores as measured by an improved correlation with the human expert judgments. In order to study the behaviour of automatic MT evaluation scores, we conducted three experiments by applying IBM BLEU (Papineni et al., 2002) and Meteor 1.4 (Denkowski and Lavie, 2011) to a sample of KOPTE translations that were produced by translation students preparing for their final master’s exams. Scores were calculated on the complete texts. To evaluate the overall performance of the automatic evaluation scores on these texts, we calculated Kendall’s rank correlation coefficient for each text following the procedure described in Sachs and Hedderich (2009). Correlations were calculated for: • the human expert grades and BLEU scores for each translation, • the human expert grades and Meteor scores for each translation, • BLEU and Meteor scores for each translation. 3.1 </context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the 6th Workshop on SMT, pages 85–91.</rawString>
</citation>
<citation valid="true">
<title>Deutsches Institut für Normung.</title>
<date>2006</date>
<journal>DIN EN</journal>
<pages>15038--2006</pages>
<publisher>ÜbersetzungsdienstleistungenDienstleistungsanforderungen. Beuth.</publisher>
<contexts>
<context position="8042" citStr="(2006)" startWordPosition="1249" endWordPosition="1249">inimal. Another (indirect) human evaluation method for MT that is also employed for error analysis are reading comprehension tests (e.g. Maney et al. (2012), Weiss and Ahrenberg (2012)). Moreover, HTER (Snover et al., 2006) is a TER-based repair-oriented metric which uses human annotators (the only apparent qualificational requirement being fluency in the target language) to generate &amp;quot;targeted&amp;quot; reference translations by post-editing the MT output or the existing reference translations, following the goal to find the shortest path between the hypothesis and a &amp;quot;correct&amp;quot; reference. Snover et al. (2006) report a high correlation between evaluation with HTER and traditional human adequacy and fluency judgements. Last but not least, Somers (2011) mentions other repairoriented measures such as post-editing effort measured by the amount of key-strokes or time spent on producing a &amp;quot;correct&amp;quot; translation on the basis of MT output. 1.3 The notion of quality in translation studies Discussions of translation &amp;quot;quality&amp;quot;, in translation studies, for a long time focused on equivalence 48 which, in its oldest and simplest form, used to echo adequacy as understood by today’s MT researchers: &amp;quot;good&amp;quot; translati</context>
<context position="12200" citStr="(2006)" startWordPosition="1878" endWordPosition="1878">(for a detailed discussion see Horn-Helf (1999)). In MT evaluation, most of these aspects have not yet or only in part been considered. Last but not least, the translation industry has developed normative standards and proofreading schemes. For example, the DIN EN 15038:2006- 08 (Deutsches Institut für Normung, 2006) discusses translation errors, quality management and qualificational requirements for translators and proofreaders, while the SAE J2450 standard (Society of Automotive Engineers, 2005) presents a weighted &amp;quot;translation quality metric&amp;quot;. An application perspective is given by Mertin (2006) who discusses translation quality management procedures in a big automotive company and, among other things, develops a weighted translation error scheme for proofreading. 1.4 Discussion The above discussion shows that, while the object of evaluation is the same for both MT and translation studies, namely translation, the differences between evaluation approaches developed in both fields are considerable. Most importantly, in translation studies, translation evaluation is considered an expert task for which fluency in one or several languages is certainly not enough, but for which translation</context>
</contexts>
<marker>2006</marker>
<rawString>Deutsches Institut für Normung. 2006. DIN EN 15038:2006-08: ÜbersetzungsdienstleistungenDienstleistungsanforderungen. Beuth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Dizdar</author>
</authors>
<date>2003</date>
<booktitle>Skopostheorie. In Handbuch Translation,</booktitle>
<pages>104--107</pages>
<publisher>Stauffenburg.</publisher>
<contexts>
<context position="10157" citStr="Dizdar (2003)" startWordPosition="1573" endWordPosition="1574">antic &amp;quot;fidelity&amp;quot; of a good translation towards the source text depends on functional criteria. This view is echoed in the concepts of &amp;quot;primary vs. secondary&amp;quot;, &amp;quot;documentary vs. instrumental&amp;quot; and &amp;quot;covert vs. overt&amp;quot; translation (Hönig, 2003). The consequence of this shift in paradigms is that, since different translation strategies may be appropriately adopted in different situations, evaluation criteria become essentially dependent on the function that the translation is going to play in the target language and culture. This view is most prominently advocated by the so-called skopos theory (cf. Dizdar (2003)). Translation errors, then, are not just simple violations of the target language system or outright failures to translate words or segments, but violations of the translation task that can manifest themselves on all levels of text production (Nord, 2003). It is important to point out that, in this framework, linguistic errors are just one type of error covering not only one of the favourite MT error categories, namely un- and mistranslated words (compare, for example, Stymne and Ahrenberg (2012), Weiss and Ahrenberg (2012), Popovi´c et al. (2013)), but also phraseological, idiomatic, syntact</context>
</contexts>
<marker>Dizdar, 2003</marker>
<rawString>Dilek Dizdar. 2003. Skopostheorie. In Handbuch Translation, pages 104–107. Stauffenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on HLT,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="4047" citStr="Doddington, 2002" startWordPosition="611" endWordPosition="612">ystem. The use of automatic metrics for MT evaluation is legitimate, since MT systems deal with large amounts of data, on which manual evaluation would be very time-consuming and expensive. Automatic metrics typically compute the closeness (adequacy) of a &amp;quot;hypothesis&amp;quot; to a &amp;quot;reference&amp;quot; translation and differ from each other by how this closeness is measured. The most popular MT eval47 Workshop on Humans and Computer-assisted Translation, pages 47–56, Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics uation metrics are IBM BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) which are used not only for tuning MT systems, but also as evaluation metrics for shared tasks, such as the Workshop on Statistical Machine Translation (Bojar et al., 2013). IBM BLEU uses n-gram precision by matching machine translation output against one or more reference translations. It accounts for adequacy and fluency by calculating word precision, respectively the n-gram precision. In order to deal with the over generation of common words, precision counts are clipped, meaning that a reference word is exhausted after it is matched. This is then the modified n-gram precision. For N=4 the</context>
<context position="25671" citStr="Doddington (2002)" startWordPosition="4175" endWordPosition="4176">e correlation coefficients show that neither BLEU nor Meteor (except a few exceptional cases) correlate with the human quality judgements, however, they show a (weak) tendency to correlate with each other. Moreover, the data shows that the addition of reference translations results neither in significantly higher BLEU or Meteor scores nor in improved correlation. 3.3 Qualitative analysis Our finding that human quality judgements do not correlate with automatic scores if the object of evaluation is a translation produced by a human (as opposed to a machine) matches earlier results presented by Doddington (2002) within the context of evaluating NIST. Doddington (2002) proposes the explanation that &amp;quot;differences between professional translators are far more subtle [than differences between machine-produced translations, the authors] and thus less well characterized 52 Source Human trans./ Median Mean Mean Correlation Correlation Correlation text source text grades BLEU Meteor Human-BLEU Human-Meteor BLEU-Meteor AT002 8 2. 5 0. 17 0. 36 −0. 08 0. 00 0. 43 AT004 8 3. 0 0. 20 0. 36 0. 00 0. 23 0. 71 AT005 8 2. 3 0. 20 0. 42 0. 00 0. 08 0. 43 AT008 6 2. 85 0. 26 0. 45 −0. 55 −0. 14 0. 33 AT010 7 2. 7 0. 23</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the 2nd International Conference on HLT, pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mireia Farrús</author>
<author>Marta R Costa-Jussà</author>
<author>José B Mariño</author>
<author>José A R Fonollosa</author>
</authors>
<title>Linguisticbased evaluation criteria to identify statistical machine translation errors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Annual Conference of the EAMT,</booktitle>
<pages>167--173</pages>
<contexts>
<context position="6270" citStr="Farrús et al., 2010" startWordPosition="970" endWordPosition="973">rics, several other metrics are sometimes used for the evaluation of MT output. Some of these are the WER (word error-rate) metric based on the Levensthein distance (Levenshtein, 1966), the positionindependent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). 1.2 Human MT quality evaluation Human evaluation of MT output is performed in different ways. The most frequently used evaluation method seems to be a simple ranking of translated sentences by a &amp;quot;reasonable number of evaluators&amp;quot; (Farrús et al., 2010). According to Birch et al. (2013), this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular. APPRAISE (Federmann, 2012) is a tool that can be used for such as task, since it allows for the manual ranking of sentences, quality estimation, error annotation and post-editing. Other forms of evaluation, however, exist. For example, Birch et al. (2013) propose HMEANT, an evaluation score based on MEANT (Lo and Wu, 2011), a semi-automatic MT quality score that measures the degree of meaning preservation by comparing verb frames and se</context>
</contexts>
<marker>Farrús, Costa-Jussà, Mariño, Fonollosa, 2010</marker>
<rawString>Mireia Farrús, Marta R. Costa-Jussà, José B. Mariño, and José A. R. Fonollosa. 2010. Linguisticbased evaluation criteria to identify statistical machine translation errors. In Proceedings of the 14th Annual Conference of the EAMT, pages 167–173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Federmann</author>
</authors>
<title>Appraise: An opensource toolkit for manual evaluation of machine translation output.</title>
<date>2012</date>
<journal>PBML,</journal>
<volume>98</volume>
<pages>9</pages>
<contexts>
<context position="6456" citStr="Federmann, 2012" startWordPosition="1002" endWordPosition="1003">positionindependent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). 1.2 Human MT quality evaluation Human evaluation of MT output is performed in different ways. The most frequently used evaluation method seems to be a simple ranking of translated sentences by a &amp;quot;reasonable number of evaluators&amp;quot; (Farrús et al., 2010). According to Birch et al. (2013), this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular. APPRAISE (Federmann, 2012) is a tool that can be used for such as task, since it allows for the manual ranking of sentences, quality estimation, error annotation and post-editing. Other forms of evaluation, however, exist. For example, Birch et al. (2013) propose HMEANT, an evaluation score based on MEANT (Lo and Wu, 2011), a semi-automatic MT quality score that measures the degree of meaning preservation by comparing verb frames and semantic roles of hypothesis translations to their respective counterparts in the reference translation(s). Unfortunately, Birch et al. (2013) report difficulty in producing coherent role </context>
</contexts>
<marker>Federmann, 2012</marker>
<rawString>Christian Federmann. 2012. Appraise: An opensource toolkit for manual evaluation of machine translation output. PBML, 98:25–35, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Hönig</author>
</authors>
<title>Humanübersetzung (therapeutisch vs. diagnostisch).</title>
<date>2003</date>
<booktitle>In Handbuch Translation,</booktitle>
<pages>378--381</pages>
<publisher>Stauffenburg.</publisher>
<contexts>
<context position="9782" citStr="Hönig, 2003" startWordPosition="1517" endWordPosition="1518">k originals (see also Windle and Pym (2011) for a further discussion). More recent work in translation studies has abandoned one-dimensional views of the relation between source and target text and postulates that, depending on the communicative context within and for which a translation is produced, this relation can vary greatly. That is, the degree of linguistic or semantic &amp;quot;fidelity&amp;quot; of a good translation towards the source text depends on functional criteria. This view is echoed in the concepts of &amp;quot;primary vs. secondary&amp;quot;, &amp;quot;documentary vs. instrumental&amp;quot; and &amp;quot;covert vs. overt&amp;quot; translation (Hönig, 2003). The consequence of this shift in paradigms is that, since different translation strategies may be appropriately adopted in different situations, evaluation criteria become essentially dependent on the function that the translation is going to play in the target language and culture. This view is most prominently advocated by the so-called skopos theory (cf. Dizdar (2003)). Translation errors, then, are not just simple violations of the target language system or outright failures to translate words or segments, but violations of the translation task that can manifest themselves on all levels </context>
</contexts>
<marker>Hönig, 2003</marker>
<rawString>Hans Hönig. 2003. Humanübersetzung (therapeutisch vs. diagnostisch). In Handbuch Translation, pages 378–381. Stauffenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brigitte Horn-Helf</author>
</authors>
<date>1999</date>
<booktitle>Technisches Übersetzen in Theorie und Praxis.</booktitle>
<location>Franke.</location>
<contexts>
<context position="11641" citStr="Horn-Helf (1999)" startWordPosition="1796" endWordPosition="1797">ions, proper names, or other conventions) or formal (e. g. layout) defects (Nord, 2003). Depending on the appropriate translation strategy for a given translation task, these error types may be weighted differently. Furthermore, the communicative and functional view on translation also dictates a change in the concept of equivalence which is no longer considered to be adequately described by the notions of &amp;quot;meaning preservation&amp;quot; or &amp;quot;fidelity&amp;quot;, but becomes dependent on aesthetic, connotational, textual, communicative, situational, functional and cognitive aspects (for a detailed discussion see Horn-Helf (1999)). In MT evaluation, most of these aspects have not yet or only in part been considered. Last but not least, the translation industry has developed normative standards and proofreading schemes. For example, the DIN EN 15038:2006- 08 (Deutsches Institut für Normung, 2006) discusses translation errors, quality management and qualificational requirements for translators and proofreaders, while the SAE J2450 standard (Society of Automotive Engineers, 2005) presents a weighted &amp;quot;translation quality metric&amp;quot;. An application perspective is given by Mertin (2006) who discusses translation quality manage</context>
</contexts>
<marker>Horn-Helf, 1999</marker>
<rawString>Brigitte Horn-Helf. 1999. Technisches Übersetzen in Theorie und Praxis. Franke.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kußmaul</author>
</authors>
<date>2000</date>
<journal>Kreatives Übersetzen. Stauffenburg.</journal>
<contexts>
<context position="8839" citStr="Kußmaul (2000)" startWordPosition="1371" endWordPosition="1372"> such as post-editing effort measured by the amount of key-strokes or time spent on producing a &amp;quot;correct&amp;quot; translation on the basis of MT output. 1.3 The notion of quality in translation studies Discussions of translation &amp;quot;quality&amp;quot;, in translation studies, for a long time focused on equivalence 48 which, in its oldest and simplest form, used to echo adequacy as understood by today’s MT researchers: &amp;quot;good&amp;quot; translation was viewed as an optimal compromise between meaning preservation and target language correctness, which was especially relevant to the translation of religious texts. For example, Kußmaul (2000) emphatically cites Martin Luther’s famous Bible translation into German as an example of &amp;quot;good&amp;quot; translation because Luther, according to his own testimony and following his reformative ambition, focused on producing fluent, easily understandable text rather than mimicking the linguistic structures of the Hebrew, Aramaic and Greek originals (see also Windle and Pym (2011) for a further discussion). More recent work in translation studies has abandoned one-dimensional views of the relation between source and target text and postulates that, depending on the communicative context within and for </context>
</contexts>
<marker>Kußmaul, 2000</marker>
<rawString>Paul Kußmaul. 2000. Kreatives Übersetzen. Stauffenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Iosifovich Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="5834" citStr="Levenshtein, 1966" startWordPosition="898" endWordPosition="899">her often used machine translation evaluation metric is Meteor (Denkowski and Lavie, 2011). Different from IBM BLEU and NIST, Meteor evaluates a candidate translation by calculating precision and recall on the unigram level and combining them into a parametrized harmonic mean. The result from the harmonic mean is then scaled by a fragmentation penalty which penalizes gaps and differences in word order. Besides these evaluation metrics, several other metrics are sometimes used for the evaluation of MT output. Some of these are the WER (word error-rate) metric based on the Levensthein distance (Levenshtein, 1966), the positionindependent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). 1.2 Human MT quality evaluation Human evaluation of MT output is performed in different ways. The most frequently used evaluation method seems to be a simple ranking of translated sentences by a &amp;quot;reasonable number of evaluators&amp;quot; (Farrús et al., 2010). According to Birch et al. (2013), this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular. APPR</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir Iosifovich Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-Kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the ACL,</booktitle>
<pages>220--229</pages>
<contexts>
<context position="6754" citStr="Lo and Wu, 2011" startWordPosition="1051" endWordPosition="1054"> evaluation method seems to be a simple ranking of translated sentences by a &amp;quot;reasonable number of evaluators&amp;quot; (Farrús et al., 2010). According to Birch et al. (2013), this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular. APPRAISE (Federmann, 2012) is a tool that can be used for such as task, since it allows for the manual ranking of sentences, quality estimation, error annotation and post-editing. Other forms of evaluation, however, exist. For example, Birch et al. (2013) propose HMEANT, an evaluation score based on MEANT (Lo and Wu, 2011), a semi-automatic MT quality score that measures the degree of meaning preservation by comparing verb frames and semantic roles of hypothesis translations to their respective counterparts in the reference translation(s). Unfortunately, Birch et al. (2013) report difficulty in producing coherent role alignments between hypotheses and translations, a problem that affects the final HMEANT score calculation. This, however, seems hardly surprising given the difficulty of the annotation task (although, following the authors’ description, some familiarity of the annotators with the linguistic key co</context>
</contexts>
<marker>Lo, Wu, 2011</marker>
<rawString>Chi-Kiu Lo and Dekai Wu. 2011. MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles. In Proceedings of the 49th Annual Meeting of the ACL, pages 220–229.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tucker Maney</author>
<author>Linda Sibert</author>
<author>Dennis Perzanowski</author>
<author>Kalyan Gupta</author>
<author>Astrid Schmidt-Nielsen</author>
</authors>
<title>Toward determining the comprehensibility of machine translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the 1st PITR,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="7592" citStr="Maney et al. (2012)" startWordPosition="1178" endWordPosition="1181">). Unfortunately, Birch et al. (2013) report difficulty in producing coherent role alignments between hypotheses and translations, a problem that affects the final HMEANT score calculation. This, however, seems hardly surprising given the difficulty of the annotation task (although, following the authors’ description, some familiarity of the annotators with the linguistic key concepts can be assumed) and the fact that guidelines and training are meant to be minimal. Another (indirect) human evaluation method for MT that is also employed for error analysis are reading comprehension tests (e.g. Maney et al. (2012), Weiss and Ahrenberg (2012)). Moreover, HTER (Snover et al., 2006) is a TER-based repair-oriented metric which uses human annotators (the only apparent qualificational requirement being fluency in the target language) to generate &amp;quot;targeted&amp;quot; reference translations by post-editing the MT output or the existing reference translations, following the goal to find the shortest path between the hypothesis and a &amp;quot;correct&amp;quot; reference. Snover et al. (2006) report a high correlation between evaluation with HTER and traditional human adequacy and fluency judgements. Last but not least, Somers (2011) menti</context>
</contexts>
<marker>Maney, Sibert, Perzanowski, Gupta, Schmidt-Nielsen, 2012</marker>
<rawString>Tucker Maney, Linda Sibert, Dennis Perzanowski, Kalyan Gupta, and Astrid Schmidt-Nielsen. 2012. Toward determining the comprehensibility of machine translations. In Proceedings of the 1st PITR, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elvira Mertin</author>
</authors>
<title>Prozessorientiertes Qualitätsmanagement im Dienstleistungsbereich Übersetzen. Peter Lang.</title>
<date>2006</date>
<contexts>
<context position="12200" citStr="Mertin (2006)" startWordPosition="1877" endWordPosition="1878">spects (for a detailed discussion see Horn-Helf (1999)). In MT evaluation, most of these aspects have not yet or only in part been considered. Last but not least, the translation industry has developed normative standards and proofreading schemes. For example, the DIN EN 15038:2006- 08 (Deutsches Institut für Normung, 2006) discusses translation errors, quality management and qualificational requirements for translators and proofreaders, while the SAE J2450 standard (Society of Automotive Engineers, 2005) presents a weighted &amp;quot;translation quality metric&amp;quot;. An application perspective is given by Mertin (2006) who discusses translation quality management procedures in a big automotive company and, among other things, develops a weighted translation error scheme for proofreading. 1.4 Discussion The above discussion shows that, while the object of evaluation is the same for both MT and translation studies, namely translation, the differences between evaluation approaches developed in both fields are considerable. Most importantly, in translation studies, translation evaluation is considered an expert task for which fluency in one or several languages is certainly not enough, but for which translation</context>
</contexts>
<marker>Mertin, 2006</marker>
<rawString>Elvira Mertin. 2006. Prozessorientiertes Qualitätsmanagement im Dienstleistungsbereich Übersetzen. Peter Lang.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Nord</author>
</authors>
<title>Transparenz der Korrektur. In Handbuch Translation,</title>
<date>2003</date>
<pages>384--387</pages>
<publisher>Stauffenburg.</publisher>
<contexts>
<context position="10413" citStr="Nord, 2003" startWordPosition="1613" endWordPosition="1614"> this shift in paradigms is that, since different translation strategies may be appropriately adopted in different situations, evaluation criteria become essentially dependent on the function that the translation is going to play in the target language and culture. This view is most prominently advocated by the so-called skopos theory (cf. Dizdar (2003)). Translation errors, then, are not just simple violations of the target language system or outright failures to translate words or segments, but violations of the translation task that can manifest themselves on all levels of text production (Nord, 2003). It is important to point out that, in this framework, linguistic errors are just one type of error covering not only one of the favourite MT error categories, namely un- and mistranslated words (compare, for example, Stymne and Ahrenberg (2012), Weiss and Ahrenberg (2012), Popovi´c et al. (2013)), but also phraseological, idiomatic, syntactic, grammatical, modal, temporal, stylistic, cohesion and other kinds of errors. Moreover, translation-specific errors occur when the translation does not fulfill its function because of pragmatic (e.g. text-type specific forms of address), cultural (e.g. </context>
</contexts>
<marker>Nord, 2003</marker>
<rawString>Christiane Nord. 2003. Transparenz der Korrektur. In Handbuch Translation, pages 384–387. Stauffenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="4019" citStr="Papineni et al., 2002" startWordPosition="605" endWordPosition="608"> any language produced by an MT system. The use of automatic metrics for MT evaluation is legitimate, since MT systems deal with large amounts of data, on which manual evaluation would be very time-consuming and expensive. Automatic metrics typically compute the closeness (adequacy) of a &amp;quot;hypothesis&amp;quot; to a &amp;quot;reference&amp;quot; translation and differ from each other by how this closeness is measured. The most popular MT eval47 Workshop on Humans and Computer-assisted Translation, pages 47–56, Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics uation metrics are IBM BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) which are used not only for tuning MT systems, but also as evaluation metrics for shared tasks, such as the Workshop on Statistical Machine Translation (Bojar et al., 2013). IBM BLEU uses n-gram precision by matching machine translation output against one or more reference translations. It accounts for adequacy and fluency by calculating word precision, respectively the n-gram precision. In order to deal with the over generation of common words, precision counts are clipped, meaning that a reference word is exhausted after it is matched. This is then the modified n</context>
<context position="20132" citStr="Papineni et al., 2002" startWordPosition="3073" endWordPosition="3076">e mimicked using simple automatic quality metrics as used in MT, namely BLEU and Meteor. More specifically, we aim at: • studying how automatic evaluation scores relate to fine-grained human expert evaluations, • investigating whether a higher number of references improves the automatic scores and why (or why not), • examining whether a higher number of references provides more reliable evaluation scores as measured by an improved correlation with the human expert judgments. In order to study the behaviour of automatic MT evaluation scores, we conducted three experiments by applying IBM BLEU (Papineni et al., 2002) and Meteor 1.4 (Denkowski and Lavie, 2011) to a sample of KOPTE translations that were produced by translation students preparing for their final master’s exams. Scores were calculated on the complete texts. To evaluate the overall performance of the automatic evaluation scores on these texts, we calculated Kendall’s rank correlation coefficient for each text following the procedure described in Sachs and Hedderich (2009). Correlations were calculated for: • the human expert grades and BLEU scores for each translation, • the human expert grades and Meteor scores for each translation, • BLEU a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Eleftherios Avramidis</author>
<author>Aljoscha Burchardt</author>
<author>Sabine Hunsicker</author>
<author>Sven Schmeier</author>
<author>Cindy Tscherwinka</author>
<author>David Vilar</author>
<author>Hans Uszkoreit</author>
</authors>
<title>Learning from human judgements of machine translation output.</title>
<date>2013</date>
<booktitle>In MT Summit,</booktitle>
<pages>231--238</pages>
<marker>Popovi´c, Avramidis, Burchardt, Hunsicker, Schmeier, Tscherwinka, Vilar, Uszkoreit, 2013</marker>
<rawString>Maja Popovi´c, Eleftherios Avramidis, Aljoscha Burchardt, Sabine Hunsicker, Sven Schmeier, Cindy Tscherwinka, David Vilar, and Hans Uszkoreit. 2013. Learning from human judgements of machine translation output. In MT Summit, pages 231–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lothar Sachs</author>
<author>Jürgen Hedderich</author>
</authors>
<title>Angewandte Statistik. Methodensammlung mit R.</title>
<date>2009</date>
<publisher>Springer.</publisher>
<contexts>
<context position="20558" citStr="Sachs and Hedderich (2009)" startWordPosition="3140" endWordPosition="3143"> an improved correlation with the human expert judgments. In order to study the behaviour of automatic MT evaluation scores, we conducted three experiments by applying IBM BLEU (Papineni et al., 2002) and Meteor 1.4 (Denkowski and Lavie, 2011) to a sample of KOPTE translations that were produced by translation students preparing for their final master’s exams. Scores were calculated on the complete texts. To evaluate the overall performance of the automatic evaluation scores on these texts, we calculated Kendall’s rank correlation coefficient for each text following the procedure described in Sachs and Hedderich (2009). Correlations were calculated for: • the human expert grades and BLEU scores for each translation, • the human expert grades and Meteor scores for each translation, • BLEU and Meteor scores for each translation. 3.1 Experimental setup and results In a first experiment, we applied the automatic evaluation scores to the source texts given in Table 2, choosing, for each text, the student translation with the best human grade as reference translation. The median human grades as well as mean BLEU and Meteor and correlation scores obtained for each text (excluding the reference translation) are inc</context>
</contexts>
<marker>Sachs, Hedderich, 2009</marker>
<rawString>Lothar Sachs and Jürgen Hedderich. 2009. Angewandte Statistik. Methodensammlung mit R. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="5968" citStr="Snover et al., 2006" startWordPosition="918" endWordPosition="921"> evaluates a candidate translation by calculating precision and recall on the unigram level and combining them into a parametrized harmonic mean. The result from the harmonic mean is then scaled by a fragmentation penalty which penalizes gaps and differences in word order. Besides these evaluation metrics, several other metrics are sometimes used for the evaluation of MT output. Some of these are the WER (word error-rate) metric based on the Levensthein distance (Levenshtein, 1966), the positionindependent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). 1.2 Human MT quality evaluation Human evaluation of MT output is performed in different ways. The most frequently used evaluation method seems to be a simple ranking of translated sentences by a &amp;quot;reasonable number of evaluators&amp;quot; (Farrús et al., 2010). According to Birch et al. (2013), this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular. APPRAISE (Federmann, 2012) is a tool that can be used for such as task, since it allows for the manual ranking of sentences, quality estim</context>
<context position="7659" citStr="Snover et al., 2006" startWordPosition="1189" endWordPosition="1192">ng coherent role alignments between hypotheses and translations, a problem that affects the final HMEANT score calculation. This, however, seems hardly surprising given the difficulty of the annotation task (although, following the authors’ description, some familiarity of the annotators with the linguistic key concepts can be assumed) and the fact that guidelines and training are meant to be minimal. Another (indirect) human evaluation method for MT that is also employed for error analysis are reading comprehension tests (e.g. Maney et al. (2012), Weiss and Ahrenberg (2012)). Moreover, HTER (Snover et al., 2006) is a TER-based repair-oriented metric which uses human annotators (the only apparent qualificational requirement being fluency in the target language) to generate &amp;quot;targeted&amp;quot; reference translations by post-editing the MT output or the existing reference translations, following the goal to find the shortest path between the hypothesis and a &amp;quot;correct&amp;quot; reference. Snover et al. (2006) report a high correlation between evaluation with HTER and traditional human adequacy and fluency judgements. Last but not least, Somers (2011) mentions other repairoriented measures such as post-editing effort measu</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAMTA, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the 4th Workshop on SMT,</booktitle>
<pages>259--268</pages>
<contexts>
<context position="6018" citStr="Snover et al., 2009" startWordPosition="927" endWordPosition="930">precision and recall on the unigram level and combining them into a parametrized harmonic mean. The result from the harmonic mean is then scaled by a fragmentation penalty which penalizes gaps and differences in word order. Besides these evaluation metrics, several other metrics are sometimes used for the evaluation of MT output. Some of these are the WER (word error-rate) metric based on the Levensthein distance (Levenshtein, 1966), the positionindependent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). 1.2 Human MT quality evaluation Human evaluation of MT output is performed in different ways. The most frequently used evaluation method seems to be a simple ranking of translated sentences by a &amp;quot;reasonable number of evaluators&amp;quot; (Farrús et al., 2010). According to Birch et al. (2013), this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular. APPRAISE (Federmann, 2012) is a tool that can be used for such as task, since it allows for the manual ranking of sentences, quality estimation, error annotation and post-editing. Other fo</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proceedings of the 4th Workshop on SMT, pages 259–268.</rawString>
</citation>
<citation valid="true">
<title>Society of Automotive Engineers.</title>
<date>2005</date>
<publisher>SAE.</publisher>
<marker>2005</marker>
<rawString>Society of Automotive Engineers. 2005. SAE J2450:2005-08: Translation Quality Metric. SAE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Somers</author>
</authors>
<title>Machine translation: History, development, and limitations.</title>
<date>2011</date>
<booktitle>In The Oxford Handbook of Translation Studies,</booktitle>
<pages>427--440</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="8186" citStr="Somers (2011)" startWordPosition="1270" endWordPosition="1271">Maney et al. (2012), Weiss and Ahrenberg (2012)). Moreover, HTER (Snover et al., 2006) is a TER-based repair-oriented metric which uses human annotators (the only apparent qualificational requirement being fluency in the target language) to generate &amp;quot;targeted&amp;quot; reference translations by post-editing the MT output or the existing reference translations, following the goal to find the shortest path between the hypothesis and a &amp;quot;correct&amp;quot; reference. Snover et al. (2006) report a high correlation between evaluation with HTER and traditional human adequacy and fluency judgements. Last but not least, Somers (2011) mentions other repairoriented measures such as post-editing effort measured by the amount of key-strokes or time spent on producing a &amp;quot;correct&amp;quot; translation on the basis of MT output. 1.3 The notion of quality in translation studies Discussions of translation &amp;quot;quality&amp;quot;, in translation studies, for a long time focused on equivalence 48 which, in its oldest and simplest form, used to echo adequacy as understood by today’s MT researchers: &amp;quot;good&amp;quot; translation was viewed as an optimal compromise between meaning preservation and target language correctness, which was especially relevant to the transl</context>
</contexts>
<marker>Somers, 2011</marker>
<rawString>Harold Somers. 2011. Machine translation: History, development, and limitations. In The Oxford Handbook of Translation Studies, pages 427–440. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Lars Ahrenberg</author>
</authors>
<title>On the practice of error analysis for machine translation evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th LREC,</booktitle>
<pages>1785--1790</pages>
<contexts>
<context position="10659" citStr="Stymne and Ahrenberg (2012)" startWordPosition="1653" endWordPosition="1656">in the target language and culture. This view is most prominently advocated by the so-called skopos theory (cf. Dizdar (2003)). Translation errors, then, are not just simple violations of the target language system or outright failures to translate words or segments, but violations of the translation task that can manifest themselves on all levels of text production (Nord, 2003). It is important to point out that, in this framework, linguistic errors are just one type of error covering not only one of the favourite MT error categories, namely un- and mistranslated words (compare, for example, Stymne and Ahrenberg (2012), Weiss and Ahrenberg (2012), Popovi´c et al. (2013)), but also phraseological, idiomatic, syntactic, grammatical, modal, temporal, stylistic, cohesion and other kinds of errors. Moreover, translation-specific errors occur when the translation does not fulfill its function because of pragmatic (e.g. text-type specific forms of address), cultural (e.g. text conventions, proper names, or other conventions) or formal (e. g. layout) defects (Nord, 2003). Depending on the appropriate translation strategy for a given translation task, these error types may be weighted differently. Furthermore, the c</context>
</contexts>
<marker>Stymne, Ahrenberg, 2012</marker>
<rawString>Sara Stymne and Lars Ahrenberg. 2012. On the practice of error analysis for machine translation evaluation. In Proceedings of the 8th LREC, pages 1785– 1790.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Alexander Zubiaga</author>
<author>Hassan Sawaf</author>
</authors>
<title>Accelerated DP based search for statistical translation.</title>
<date>1997</date>
<booktitle>In Proceedings of the EUROSPEECH,</booktitle>
<pages>2667--2670</pages>
<contexts>
<context position="5905" citStr="Tillmann et al., 1997" startWordPosition="907" endWordPosition="910">kowski and Lavie, 2011). Different from IBM BLEU and NIST, Meteor evaluates a candidate translation by calculating precision and recall on the unigram level and combining them into a parametrized harmonic mean. The result from the harmonic mean is then scaled by a fragmentation penalty which penalizes gaps and differences in word order. Besides these evaluation metrics, several other metrics are sometimes used for the evaluation of MT output. Some of these are the WER (word error-rate) metric based on the Levensthein distance (Levenshtein, 1966), the positionindependent error rate metric PER (Tillmann et al., 1997) and the translation edit rate metric TER (Snover et al., 2006) with its newer version TERp (Snover et al., 2009). 1.2 Human MT quality evaluation Human evaluation of MT output is performed in different ways. The most frequently used evaluation method seems to be a simple ranking of translated sentences by a &amp;quot;reasonable number of evaluators&amp;quot; (Farrús et al., 2010). According to Birch et al. (2013), this form of evaluation was used, among others, during the last STATMT workshops and can thus be considered rather popular. APPRAISE (Federmann, 2012) is a tool that can be used for such as task, sin</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>Christoph Tillmann, Stephan Vogel, Hermann Ney, Alexander Zubiaga, and Hassan Sawaf. 1997. Accelerated DP based search for statistical translation. In Proceedings of the EUROSPEECH, pages 2667– 2670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra Weiss</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Error profiling for evaluation of machine-translated text: a polish-english case study.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth LREC,</booktitle>
<pages>1764--1770</pages>
<contexts>
<context position="7620" citStr="Weiss and Ahrenberg (2012)" startWordPosition="1182" endWordPosition="1185">ch et al. (2013) report difficulty in producing coherent role alignments between hypotheses and translations, a problem that affects the final HMEANT score calculation. This, however, seems hardly surprising given the difficulty of the annotation task (although, following the authors’ description, some familiarity of the annotators with the linguistic key concepts can be assumed) and the fact that guidelines and training are meant to be minimal. Another (indirect) human evaluation method for MT that is also employed for error analysis are reading comprehension tests (e.g. Maney et al. (2012), Weiss and Ahrenberg (2012)). Moreover, HTER (Snover et al., 2006) is a TER-based repair-oriented metric which uses human annotators (the only apparent qualificational requirement being fluency in the target language) to generate &amp;quot;targeted&amp;quot; reference translations by post-editing the MT output or the existing reference translations, following the goal to find the shortest path between the hypothesis and a &amp;quot;correct&amp;quot; reference. Snover et al. (2006) report a high correlation between evaluation with HTER and traditional human adequacy and fluency judgements. Last but not least, Somers (2011) mentions other repairoriented mea</context>
<context position="10687" citStr="Weiss and Ahrenberg (2012)" startWordPosition="1657" endWordPosition="1660">lture. This view is most prominently advocated by the so-called skopos theory (cf. Dizdar (2003)). Translation errors, then, are not just simple violations of the target language system or outright failures to translate words or segments, but violations of the translation task that can manifest themselves on all levels of text production (Nord, 2003). It is important to point out that, in this framework, linguistic errors are just one type of error covering not only one of the favourite MT error categories, namely un- and mistranslated words (compare, for example, Stymne and Ahrenberg (2012), Weiss and Ahrenberg (2012), Popovi´c et al. (2013)), but also phraseological, idiomatic, syntactic, grammatical, modal, temporal, stylistic, cohesion and other kinds of errors. Moreover, translation-specific errors occur when the translation does not fulfill its function because of pragmatic (e.g. text-type specific forms of address), cultural (e.g. text conventions, proper names, or other conventions) or formal (e. g. layout) defects (Nord, 2003). Depending on the appropriate translation strategy for a given translation task, these error types may be weighted differently. Furthermore, the communicative and functional </context>
</contexts>
<marker>Weiss, Ahrenberg, 2012</marker>
<rawString>Sandra Weiss and Lars Ahrenberg. 2012. Error profiling for evaluation of machine-translated text: a polish-english case study. In Proceedings of the Eighth LREC, pages 1764–1770.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Windle</author>
<author>Anthony Pym</author>
</authors>
<title>European thinking on secular translation.</title>
<date>2011</date>
<booktitle>In The Oxford Handbook of Translation Studies,</booktitle>
<pages>7--22</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="9213" citStr="Windle and Pym (2011)" startWordPosition="1424" endWordPosition="1428">rstood by today’s MT researchers: &amp;quot;good&amp;quot; translation was viewed as an optimal compromise between meaning preservation and target language correctness, which was especially relevant to the translation of religious texts. For example, Kußmaul (2000) emphatically cites Martin Luther’s famous Bible translation into German as an example of &amp;quot;good&amp;quot; translation because Luther, according to his own testimony and following his reformative ambition, focused on producing fluent, easily understandable text rather than mimicking the linguistic structures of the Hebrew, Aramaic and Greek originals (see also Windle and Pym (2011) for a further discussion). More recent work in translation studies has abandoned one-dimensional views of the relation between source and target text and postulates that, depending on the communicative context within and for which a translation is produced, this relation can vary greatly. That is, the degree of linguistic or semantic &amp;quot;fidelity&amp;quot; of a good translation towards the source text depends on functional criteria. This view is echoed in the concepts of &amp;quot;primary vs. secondary&amp;quot;, &amp;quot;documentary vs. instrumental&amp;quot; and &amp;quot;covert vs. overt&amp;quot; translation (Hönig, 2003). The consequence of this shift</context>
</contexts>
<marker>Windle, Pym, 2011</marker>
<rawString>Kevin Windle and Anthony Pym. 2011. European thinking on secular translation. In The Oxford Handbook of Translation Studies, pages 7–22. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Wurm</author>
</authors>
<date>2013</date>
<booktitle>Eigennamen und Realia in einem Korpus studentischer Übersetzungen (KOPTE). trans-kom,</booktitle>
<pages>6--2</pages>
<contexts>
<context position="15172" citStr="Wurm, 2013" startWordPosition="2333" endWordPosition="2334">t still remains in the top segment. It still needs to be shown how MT research can benefit from more sophisticated evaluation measures and whether all the parameters that are considered relevant to the evaluation of human translations are relevant for MT usage scenarios, too. In the remainder of this paper, we present a study on how much and possibly for which reasons automatic MT evaluation scores (namely BLEU and Meteor) differ from translation expert quality judgements on extracts of a French-German translation learner corpus. 2 The KOPTE corpus 2.1 General corpus design The KOPTE project (Wurm, 2013) was designed to enable research on translation evaluation in a university training course (master’s level) for translators and to enlighten students’ translation problems as well as their problem solving strategies. To achieve this goal, a corpus of student translations was compiled. The corpus consists of several translations of the same source texts produced by student translators in a classroom setting. As a whole, it covers 985 translations of 77 source texts amounting to a total of 318,467 tokens. Source texts were taken from French newspapers and translated into German in class over a s</context>
<context position="17967" citStr="Wurm, 2013" startWordPosition="2767" endWordPosition="2768">tion, time). Internal factors, on the other hand, comprise eight categories: form, structure, cohesion, stylistics/register, grammar, lexis/semantics, translation-specific problems, function. These categories are containers for more fine-grained criteria which can be applied to segments of the (source or target) text or even to the whole text, depending on the nature of the criterion. Some internal subcriteria of the scheme are summarised in Table 1. A quantitative analysis of error types in KOPTE shows that semantic/lexical errors are by far the most common error in the student translations (Wurm, 2013). Evaluations in KOPTE were carried out by just 1More information about KOPTE is available from http://fr46.uni-saarland.de/index.php?id=3702&amp;L=%2524L. 50 one evaluator for the reason that, in a classroom setting, multiple evaluations are not feasible. Although multiple evaluations would have been considered highly valuable, the data available from KOPTE was evaluated by an experienced translation scholar with long-standing experience in teaching translation. Moreover, the evaluation scheme is much more detailed than error annotation schemes that are normally described in the literature and it</context>
</contexts>
<marker>Wurm, 2013</marker>
<rawString>Andrea Wurm. 2013. Eigennamen und Realia in einem Korpus studentischer Übersetzungen (KOPTE). trans-kom, 6(2):381–419.</rawString>
</citation>
<citation valid="false">
<title>sera-t-elle capable es schaffen fähig sein in der Lage sein sich als fähig erweisen &amp;quot;doyen de l’Afrique&amp;quot; obersten Würdenträgers Afrikas &amp;quot;Alten Herrn von Afrika&amp;quot; &amp;quot;Abtes von Afrika&amp;quot; &amp;quot;Ältesten von Afrika&amp;quot; &amp;quot;doyen de l’Afrique&amp;quot;</title>
<marker></marker>
<rawString>sera-t-elle capable es schaffen fähig sein in der Lage sein sich als fähig erweisen &amp;quot;doyen de l’Afrique&amp;quot; obersten Würdenträgers Afrikas &amp;quot;Alten Herrn von Afrika&amp;quot; &amp;quot;Abtes von Afrika&amp;quot; &amp;quot;Ältesten von Afrika&amp;quot; &amp;quot;doyen de l’Afrique&amp;quot;</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
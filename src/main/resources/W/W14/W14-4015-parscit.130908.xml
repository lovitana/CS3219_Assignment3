<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.073962">
<title confidence="0.992692">
Word’s Vector Representations meet Machine Translation
</title>
<author confidence="0.984715">
Eva Martinez Garcia J¨org Tiedemann Lluis M`arquez
</author>
<affiliation confidence="0.683416">
Cristina Espa˜na-Bonet Uppsala University Qatar Computing Research Institute
TALP Research Center Department of Linguistics Qatar Foundation
Univesitat Polit`ecnica de Catalunya and Philology lluism@lsi.upc.edu
</affiliation>
<email confidence="0.9802285">
emartinez@lsi.upc.edu jorg.tiedemann@lingfil.uu.se
cristinae@lsi.upc.edu
</email>
<sectionHeader confidence="0.993754" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995326">
Distributed vector representations of
words are useful in various NLP tasks.
We briefly review the CBOW approach
and propose a bilingual application of
this architecture with the aim to improve
consistency and coherence of Machine
Translation. The primary goal of the bilin-
gual extension is to handle ambiguous
words for which the different senses are
conflated in the monolingual setup.
</bodyText>
<sectionHeader confidence="0.998754" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999660904761905">
Machine Translation (MT) systems are nowadays
achieving a high-quality performance. However,
they are typically developed at sentence level
using only local information and ignoring the
document-level one. Recent work claims that
discourse-wide context can help to translate indi-
vidual words in a way that leads to more coherent
translations (Hardmeier et al., 2013; Hardmeier et
al., 2012; Gong et al., 2011; Xiao et al., 2011).
Standard SMT systems use n-gram models to
represent words in the target language. How-
ever, there are other word representation tech-
niques that use vectors of contextual information.
Recently, several distributed word representation
models have been introduced that have interesting
properties regarding to the semantic information
that they capture. In particular, we are interested
in the word2vec package available in (Mikolov et
al., 2013a). These models proved to be robust
and powerful for predicting semantic relations be-
tween words and even across languages. However,
they are not able to handle lexical ambiguity as
they conflate word senses of polysemous words
into one common representation. This limitation is
already discussed in (Mikolov et al., 2013b) and in
(Wolf et al., 2014), in which bilingual extensions
of the word2vec architecture are proposed. In con-
trast to their approach, we are not interested in
monolingual applications but instead like to con-
centrate directly on the bilingual case in connec-
tion with MT.
We built bilingual word representation mod-
els based on word-aligned parallel corpora by
an application of the Continuous Bag-of-Words
(CBOW) algorithm to the bilingual case (Sec-
tion 2). We made a twofold preliminary evalua-
tion of the acquired word-pair representations on
two different tasks (Section 3): predicting seman-
tically related words (3.1) and cross-lingual lexical
substitution (3.2). Section 4 draws the conclusions
and sets the future work in a direct application of
these models to MT.
</bodyText>
<sectionHeader confidence="0.967623" genericHeader="method">
2 Semantic Models using CBOW
</sectionHeader>
<bodyText confidence="0.999989">
The basic architecture that we use to build our
models is CBOW (Mikolov et al., 2013a). The
algorithm uses a neural network (NN) to predict
a word taking into account its context, but without
considering word order. Despite its drawbacks, we
chose to use it since we presume that the transla-
tion task applies the same strategy as the CBOW
architecture, i.e., from a set of context words try to
predict a translation of a specific given word.
In the monolingual case, the NN is trained using
a monolingual corpus to obtain the corresponding
projection matrix that encloses the vector repre-
sentations of the words. In order to introduce the
semantic information in a bilingual scenario, we
use a parallel corpus and automatic word align-
ment to extract a training corpus of word pairs:
(wi,S|wi,T). This approach is different from (Wolf
et al., 2014) who build an independent model for
each language. With our method, we try to cap-
ture simultaneously the semantic information as-
sociated to the source word and the information
in the target side of the translation. In this way,
we hope to better capture the semantic informa-
tion that is implicitly given by translating a text.
</bodyText>
<page confidence="0.958881">
132
</page>
<table confidence="0.781547833333333">
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 132–134,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
Model Accuracy Known words
mono en 32.47 % 64.67 %
mono es 10.24 % 44.96 %
bi en-es 23.68 % 13.74 %
</table>
<tableCaption confidence="0.999824">
Table 1: Accuracy on the Word Relationship set.
</tableCaption>
<sectionHeader confidence="0.988823" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999939454545455">
The semantic models are built using a combination
of freely available corpora for English and Span-
ish (EuropalV7, United Nations and Multilingual
United Nations, and Subtitles2012). They can
be found in the Opus site (Tiedemann, 2012).We
trained vectors to represent word pairs forms us-
ing this corpora with the word2vec CBOW imple-
mentation. We built a training set of almost 600
million words and used 600-dimension vectors in
the training. Regarding to the alignments, we only
used word-to-word ones to avoid noise.
</bodyText>
<subsectionHeader confidence="0.999457">
3.1 Accuracy of the Semantic Model
</subsectionHeader>
<bodyText confidence="0.99999116">
We first evaluate the quality of the models based
on the task of predicting semantically related
words. A Spanish native speaker built the bilin-
gual test set similarly to the process done to the
training data from a list of 19, 544 questions intro-
duced by (Mikolov et al., 2013c). In our bilingual
scenario, the task is to predict a pair of words given
two pairs of related words. For instance, given the
pair AthenslAtenas GreecelGrecia and
the question LondonlLondres, the task is to
predict England|Inglaterra.
Table 1 shows the results, both overall accuracy
and accuracy over the known words for the mod-
els. Using the first 30, 000 entries of the model
(the most frequent ones), we obtain 32% of ac-
curacy for English (mono en) and 10% for Span-
ish (mono es). We chose these parameters for our
system to obtain comparable results to the ones
in (Mikolov et al., 2013a) for a CBOW architec-
ture but trained with 783 million words (50.4%).
Decay for the model in Spanish can be due to the
fact that it was built from automatic translations.
In the bilingual case (bi en-es), the accuracy is
lower than for English probably due to the noise
in translations and word alignment.
</bodyText>
<subsectionHeader confidence="0.999812">
3.2 Cross-Lingual Lexical Substitution
</subsectionHeader>
<bodyText confidence="0.999968212121212">
Another way to evaluate the semantic models is
through the effect they have in translation. We im-
plemented the Cross-Lingual Lexical Substitution
task carried out in SemEval-2010 (Task2, 2010)
and applied it to a test set of news data from the
News Commentary corpus of 2011.
We identify those content words which are
translated in more than one way by a baseline
translation system (Moses trained with Europarl
v7). Given one of these content words, we take the
two previous and two following words and look
for their vector representations using our bilingual
models. We compute a linear combination of these
vectors to obtain a context vector. Then, to chose
the best translation option, we calculate a score
based on the similarity among the vector of every
possible translation option seen in the document
and the context vector.
In average there are 615 words per document
within the test set and 7% are translated in more
than one way by the baseline system. Our bilin-
gual models know in average 87.5% of the words
and 83.9% of the ambiguous ones, so although
there is a good coverage for this test set, still, some
of the candidates cannot be retranslated or some
of the options cannot be used because they are
missing in the models. The accuracy obtained af-
ter retranslation of the known ambiguous words
is 62.4% and this score is slightly better than the
result obtained by using the most frequent transla-
tion for ambiguous words (59.8%). Even though
this improvement is rather modest, it shows poten-
tial benefits of our model in MT.
</bodyText>
<sectionHeader confidence="0.997904" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.99988955">
We implemented a new application of word vec-
tor representations for MT. The system uses word
alignments to build bilingual models with the final
aim to improve the lexical selection for words that
can be translated in more than one sense.
The models have been evaluated regarding their
accuracy when trying to predict related words
(Section 3.1) and also regarding its possible effect
within a translation system (Section 3.2). In both
cases one observes that the quality of the transla-
tion and alignments previous to building the se-
mantic models are bottlenecks for the final perfor-
mance: part of the vocabulary, and therefore trans-
lation pairs, are lost in the training process.
Future work includes studying different kinds
of alignment heuristics. We plan to develop
new features based on the semantic models to
use them inside state-of-the-art SMT systems like
Moses (Koehn et al., 2007) or discourse-oriented
decoders like Docent (Hardmeier et al., 2013).
</bodyText>
<page confidence="0.998785">
133
</page>
<sectionHeader confidence="0.989767" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999585134615385">
Z. Gong, M. Zhang, and G. Zhou. 2011. Cache-based
document-level statistical machine translation. In
Proc. of the 2011 Conference on Empirical Methods
in NLP, pages 909–919, UK.
C. Hardmeier, J. Nivre, and J. Tiedemann. 2012.
Document-wide decoding for phrase-based statisti-
cal machine translation. In Proc. of the Joint Con-
ference on Empirical Methods in NLP and Compu-
tational Natural Language Learning, pages 1179–
1190, Korea.
C. Hardmeier, S. Stymne, J. Tiedemann, and J. Nivre.
2013. Docent: A document-level decoder for
phrase-based statistical machine translation. In
Proc. of the 51st ACL Conference, pages 193–198,
Bulgaria.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proc. of the 45th
ACL Conference, pages 177–180, Czech Republic.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.
Efficient estimation of word representations in vec-
tor space. In Proceedings of Workshop at ICLR.
http://code.google.com/p/word2vec.
T. Mikolov, Q. V. Le, and I. Sutskever. 2013b. Ex-
ploiting similarities among languages for machine
translation. In arXiv.
T. Mikolov, I. Sutskever, G. Corrado, and J. Dean.
2013c. Distributed representations of words and
phrases and their compositionality. In Proceedings
ofNIPS.
Task2. 2010. Cross-lingual lexi-
cal substitution task, semeval-2010.
http://semeval2.fbk.eu/semeval2.php?location=tasksT24.
J. Tiedemann. 2009. News from opus - a collection
of multilingual parallel corpora with tools and in-
terfaces. In N. Nicolov and K. Bontcheva and G.
Angelova and R. Mitkov (eds.) Recent Advances in
Natural Language Processing (vol V), pages 237–
248, Amsterdam/Philadelphia. John Benjamins.
J. Tiedemann. 2012. Parallel data, tools and interfaces
in opus. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC’2012). http://opus.lingfil.uu.se/.
L. Wolf, Y. Hanani, K. Bar, and N. Derschowitz. 2014.
Joint word2vec networks for bilingual semantic rep-
resentations. In Poster sessions at CICLING.
T. Xiao, J. Zhu, S. Yao, and H. Zhang. 2011.
Document-level consistency verification in machine
translation. In Proc. ofMachine Translation Summit
XIII, pages 131–138, China.
</reference>
<page confidence="0.998633">
134
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.858004">
<title confidence="0.999882">Word’s Vector Representations meet Machine Translation</title>
<author confidence="0.998073">Eva Martinez Garcia J¨org Tiedemann Lluis M`arquez</author>
<affiliation confidence="0.960753">Cristina University Qatar Computing Research Institute TALP Research Center Department of Linguistics Qatar Foundation Univesitat Polit`ecnica de Catalunya and Philology lluism@lsi.upc.edu</affiliation>
<email confidence="0.992116">emartinez@lsi.upc.educristinae@lsi.upc.edu</email>
<abstract confidence="0.997340909090909">Distributed vector representations of words are useful in various NLP tasks. We briefly review the CBOW approach and propose a bilingual application of this architecture with the aim to improve consistency and coherence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Z Gong</author>
<author>M Zhang</author>
<author>G Zhou</author>
</authors>
<title>Cache-based document-level statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proc. of the 2011 Conference on Empirical Methods in NLP,</booktitle>
<pages>909--919</pages>
<publisher>UK.</publisher>
<contexts>
<context position="1210" citStr="Gong et al., 2011" startWordPosition="159" endWordPosition="162">consistency and coherence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup. 1 Introduction Machine Translation (MT) systems are nowadays achieving a high-quality performance. However, they are typically developed at sentence level using only local information and ignoring the document-level one. Recent work claims that discourse-wide context can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between words and even across languages. However, the</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2011</marker>
<rawString>Z. Gong, M. Zhang, and G. Zhou. 2011. Cache-based document-level statistical machine translation. In Proc. of the 2011 Conference on Empirical Methods in NLP, pages 909–919, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hardmeier</author>
<author>J Nivre</author>
<author>J Tiedemann</author>
</authors>
<title>Document-wide decoding for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of the Joint Conference on Empirical Methods in NLP and Computational Natural Language Learning,</booktitle>
<pages>1179--1190</pages>
<contexts>
<context position="1191" citStr="Hardmeier et al., 2012" startWordPosition="155" endWordPosition="158">with the aim to improve consistency and coherence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup. 1 Introduction Machine Translation (MT) systems are nowadays achieving a high-quality performance. However, they are typically developed at sentence level using only local information and ignoring the document-level one. Recent work claims that discourse-wide context can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between words and even across lang</context>
</contexts>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>C. Hardmeier, J. Nivre, and J. Tiedemann. 2012. Document-wide decoding for phrase-based statistical machine translation. In Proc. of the Joint Conference on Empirical Methods in NLP and Computational Natural Language Learning, pages 1179– 1190, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Hardmeier</author>
<author>S Stymne</author>
<author>J Tiedemann</author>
<author>J Nivre</author>
</authors>
<title>Docent: A document-level decoder for phrase-based statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proc. of the 51st ACL Conference,</booktitle>
<pages>193--198</pages>
<contexts>
<context position="1167" citStr="Hardmeier et al., 2013" startWordPosition="151" endWordPosition="154">on of this architecture with the aim to improve consistency and coherence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup. 1 Introduction Machine Translation (MT) systems are nowadays achieving a high-quality performance. However, they are typically developed at sentence level using only local information and ignoring the document-level one. Recent work claims that discourse-wide context can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between wo</context>
</contexts>
<marker>Hardmeier, Stymne, Tiedemann, Nivre, 2013</marker>
<rawString>C. Hardmeier, S. Stymne, J. Tiedemann, and J. Nivre. 2013. Docent: A document-level decoder for phrase-based statistical machine translation. In Proc. of the 51st ACL Conference, pages 193–198, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th ACL Conference,</booktitle>
<pages>177--180</pages>
<location>Czech Republic.</location>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proc. of the 45th ACL Conference, pages 177–180, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>K Chen</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR. http://code.google.com/p/word2vec.</booktitle>
<contexts>
<context position="1673" citStr="Mikolov et al., 2013" startWordPosition="227" endWordPosition="230">t can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between words and even across languages. However, they are not able to handle lexical ambiguity as they conflate word senses of polysemous words into one common representation. This limitation is already discussed in (Mikolov et al., 2013b) and in (Wolf et al., 2014), in which bilingual extensions of the word2vec architecture are proposed. In contrast to their approach, we are not interested in monolingual applications but instead like to concentrate directly on the bilingual case in connection with MT. We buil</context>
<context position="5161" citStr="Mikolov et al., 2013" startWordPosition="799" endWordPosition="802">s site (Tiedemann, 2012).We trained vectors to represent word pairs forms using this corpora with the word2vec CBOW implementation. We built a training set of almost 600 million words and used 600-dimension vectors in the training. Regarding to the alignments, we only used word-to-word ones to avoid noise. 3.1 Accuracy of the Semantic Model We first evaluate the quality of the models based on the task of predicting semantically related words. A Spanish native speaker built the bilingual test set similarly to the process done to the training data from a list of 19, 544 questions introduced by (Mikolov et al., 2013c). In our bilingual scenario, the task is to predict a pair of words given two pairs of related words. For instance, given the pair AthenslAtenas GreecelGrecia and the question LondonlLondres, the task is to predict England|Inglaterra. Table 1 shows the results, both overall accuracy and accuracy over the known words for the models. Using the first 30, 000 entries of the model (the most frequent ones), we obtain 32% of accuracy for English (mono en) and 10% for Spanish (mono es). We chose these parameters for our system to obtain comparable results to the ones in (Mikolov et al., 2013a) for a</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at ICLR. http://code.google.com/p/word2vec.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>Q V Le</author>
<author>I Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation. In arXiv.</title>
<date>2013</date>
<contexts>
<context position="1673" citStr="Mikolov et al., 2013" startWordPosition="227" endWordPosition="230">t can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between words and even across languages. However, they are not able to handle lexical ambiguity as they conflate word senses of polysemous words into one common representation. This limitation is already discussed in (Mikolov et al., 2013b) and in (Wolf et al., 2014), in which bilingual extensions of the word2vec architecture are proposed. In contrast to their approach, we are not interested in monolingual applications but instead like to concentrate directly on the bilingual case in connection with MT. We buil</context>
<context position="5161" citStr="Mikolov et al., 2013" startWordPosition="799" endWordPosition="802">s site (Tiedemann, 2012).We trained vectors to represent word pairs forms using this corpora with the word2vec CBOW implementation. We built a training set of almost 600 million words and used 600-dimension vectors in the training. Regarding to the alignments, we only used word-to-word ones to avoid noise. 3.1 Accuracy of the Semantic Model We first evaluate the quality of the models based on the task of predicting semantically related words. A Spanish native speaker built the bilingual test set similarly to the process done to the training data from a list of 19, 544 questions introduced by (Mikolov et al., 2013c). In our bilingual scenario, the task is to predict a pair of words given two pairs of related words. For instance, given the pair AthenslAtenas GreecelGrecia and the question LondonlLondres, the task is to predict England|Inglaterra. Table 1 shows the results, both overall accuracy and accuracy over the known words for the models. Using the first 30, 000 entries of the model (the most frequent ones), we obtain 32% of accuracy for English (mono en) and 10% for Spanish (mono es). We chose these parameters for our system to obtain comparable results to the ones in (Mikolov et al., 2013a) for a</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>T. Mikolov, Q. V. Le, and I. Sutskever. 2013b. Exploiting similarities among languages for machine translation. In arXiv.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>I Sutskever</author>
<author>G Corrado</author>
<author>J Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings ofNIPS.</booktitle>
<contexts>
<context position="1673" citStr="Mikolov et al., 2013" startWordPosition="227" endWordPosition="230">t can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between words and even across languages. However, they are not able to handle lexical ambiguity as they conflate word senses of polysemous words into one common representation. This limitation is already discussed in (Mikolov et al., 2013b) and in (Wolf et al., 2014), in which bilingual extensions of the word2vec architecture are proposed. In contrast to their approach, we are not interested in monolingual applications but instead like to concentrate directly on the bilingual case in connection with MT. We buil</context>
<context position="5161" citStr="Mikolov et al., 2013" startWordPosition="799" endWordPosition="802">s site (Tiedemann, 2012).We trained vectors to represent word pairs forms using this corpora with the word2vec CBOW implementation. We built a training set of almost 600 million words and used 600-dimension vectors in the training. Regarding to the alignments, we only used word-to-word ones to avoid noise. 3.1 Accuracy of the Semantic Model We first evaluate the quality of the models based on the task of predicting semantically related words. A Spanish native speaker built the bilingual test set similarly to the process done to the training data from a list of 19, 544 questions introduced by (Mikolov et al., 2013c). In our bilingual scenario, the task is to predict a pair of words given two pairs of related words. For instance, given the pair AthenslAtenas GreecelGrecia and the question LondonlLondres, the task is to predict England|Inglaterra. Table 1 shows the results, both overall accuracy and accuracy over the known words for the models. Using the first 30, 000 entries of the model (the most frequent ones), we obtain 32% of accuracy for English (mono en) and 10% for Spanish (mono es). We chose these parameters for our system to obtain comparable results to the ones in (Mikolov et al., 2013a) for a</context>
</contexts>
<marker>Mikolov, Sutskever, Corrado, Dean, 2013</marker>
<rawString>T. Mikolov, I. Sutskever, G. Corrado, and J. Dean. 2013c. Distributed representations of words and phrases and their compositionality. In Proceedings ofNIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Task2</author>
</authors>
<title>Cross-lingual lexical substitution task,</title>
<date>2010</date>
<pages>2010--2</pages>
<marker>Task2, 2010</marker>
<rawString>Task2. 2010. Cross-lingual lexical substitution task, semeval-2010. http://semeval2.fbk.eu/semeval2.php?location=tasksT24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>News from opus - a collection of multilingual parallel corpora with tools and interfaces.</title>
<date>2009</date>
<booktitle>Recent Advances in Natural Language Processing (vol V),</booktitle>
<pages>237--248</pages>
<editor>In N. Nicolov and K. Bontcheva and G. Angelova and R. Mitkov (eds.)</editor>
<publisher>Amsterdam/Philadelphia. John Benjamins.</publisher>
<marker>Tiedemann, 2009</marker>
<rawString>J. Tiedemann. 2009. News from opus - a collection of multilingual parallel corpora with tools and interfaces. In N. Nicolov and K. Bontcheva and G. Angelova and R. Mitkov (eds.) Recent Advances in Natural Language Processing (vol V), pages 237– 248, Amsterdam/Philadelphia. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tiedemann</author>
</authors>
<title>Parallel data, tools and interfaces in opus.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’2012). http://opus.lingfil.uu.se/.</booktitle>
<contexts>
<context position="4565" citStr="Tiedemann, 2012" startWordPosition="699" endWordPosition="700">ranslating a text. 132 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 132–134, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Model Accuracy Known words mono en 32.47 % 64.67 % mono es 10.24 % 44.96 % bi en-es 23.68 % 13.74 % Table 1: Accuracy on the Word Relationship set. 3 Experiments The semantic models are built using a combination of freely available corpora for English and Spanish (EuropalV7, United Nations and Multilingual United Nations, and Subtitles2012). They can be found in the Opus site (Tiedemann, 2012).We trained vectors to represent word pairs forms using this corpora with the word2vec CBOW implementation. We built a training set of almost 600 million words and used 600-dimension vectors in the training. Regarding to the alignments, we only used word-to-word ones to avoid noise. 3.1 Accuracy of the Semantic Model We first evaluate the quality of the models based on the task of predicting semantically related words. A Spanish native speaker built the bilingual test set similarly to the process done to the training data from a list of 19, 544 questions introduced by (Mikolov et al., 2013c). </context>
</contexts>
<marker>Tiedemann, 2012</marker>
<rawString>J. Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’2012). http://opus.lingfil.uu.se/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wolf</author>
<author>Y Hanani</author>
<author>K Bar</author>
<author>N Derschowitz</author>
</authors>
<title>Joint word2vec networks for bilingual semantic representations.</title>
<date>2014</date>
<booktitle>In Poster sessions at CICLING.</booktitle>
<contexts>
<context position="2024" citStr="Wolf et al., 2014" startWordPosition="283" endWordPosition="286">mation. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between words and even across languages. However, they are not able to handle lexical ambiguity as they conflate word senses of polysemous words into one common representation. This limitation is already discussed in (Mikolov et al., 2013b) and in (Wolf et al., 2014), in which bilingual extensions of the word2vec architecture are proposed. In contrast to their approach, we are not interested in monolingual applications but instead like to concentrate directly on the bilingual case in connection with MT. We built bilingual word representation models based on word-aligned parallel corpora by an application of the Continuous Bag-of-Words (CBOW) algorithm to the bilingual case (Section 2). We made a twofold preliminary evaluation of the acquired word-pair representations on two different tasks (Section 3): predicting semantically related words (3.1) and cross</context>
<context position="3641" citStr="Wolf et al., 2014" startWordPosition="547" endWordPosition="550">rawbacks, we chose to use it since we presume that the translation task applies the same strategy as the CBOW architecture, i.e., from a set of context words try to predict a translation of a specific given word. In the monolingual case, the NN is trained using a monolingual corpus to obtain the corresponding projection matrix that encloses the vector representations of the words. In order to introduce the semantic information in a bilingual scenario, we use a parallel corpus and automatic word alignment to extract a training corpus of word pairs: (wi,S|wi,T). This approach is different from (Wolf et al., 2014) who build an independent model for each language. With our method, we try to capture simultaneously the semantic information associated to the source word and the information in the target side of the translation. In this way, we hope to better capture the semantic information that is implicitly given by translating a text. 132 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 132–134, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Model Accuracy Known words mono en 32.47 % 64.67 % mono es 10.24 % 44.96</context>
</contexts>
<marker>Wolf, Hanani, Bar, Derschowitz, 2014</marker>
<rawString>L. Wolf, Y. Hanani, K. Bar, and N. Derschowitz. 2014. Joint word2vec networks for bilingual semantic representations. In Poster sessions at CICLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Xiao</author>
<author>J Zhu</author>
<author>S Yao</author>
<author>H Zhang</author>
</authors>
<title>Document-level consistency verification in machine translation.</title>
<date>2011</date>
<booktitle>In Proc. ofMachine Translation Summit XIII,</booktitle>
<pages>131--138</pages>
<contexts>
<context position="1230" citStr="Xiao et al., 2011" startWordPosition="163" endWordPosition="166">erence of Machine Translation. The primary goal of the bilingual extension is to handle ambiguous words for which the different senses are conflated in the monolingual setup. 1 Introduction Machine Translation (MT) systems are nowadays achieving a high-quality performance. However, they are typically developed at sentence level using only local information and ignoring the document-level one. Recent work claims that discourse-wide context can help to translate individual words in a way that leads to more coherent translations (Hardmeier et al., 2013; Hardmeier et al., 2012; Gong et al., 2011; Xiao et al., 2011). Standard SMT systems use n-gram models to represent words in the target language. However, there are other word representation techniques that use vectors of contextual information. Recently, several distributed word representation models have been introduced that have interesting properties regarding to the semantic information that they capture. In particular, we are interested in the word2vec package available in (Mikolov et al., 2013a). These models proved to be robust and powerful for predicting semantic relations between words and even across languages. However, they are not able to ha</context>
</contexts>
<marker>Xiao, Zhu, Yao, Zhang, 2011</marker>
<rawString>T. Xiao, J. Zhu, S. Yao, and H. Zhang. 2011. Document-level consistency verification in machine translation. In Proc. ofMachine Translation Summit XIII, pages 131–138, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
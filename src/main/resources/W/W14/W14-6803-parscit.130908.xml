<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.889885">
COV Model and its Application in Chinese Part-of-Speech Tagging
</title>
<author confidence="0.982379">
Xing Fukun Song Rou
</author>
<affiliation confidence="0.8982455">
Luoyang Foreign Languages University Beijing Language and Cultural
471003 Henan University 100086 Beijing
</affiliation>
<email confidence="0.974382">
xingfukun@126.com songrou@126.com
</email>
<sectionHeader confidence="0.98357" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963090909091">
This article presents a new sequence
labeling model named Context
OVerlapping (COV) model, which
expands observation from single word
to n-gram unit and there is an
overlapping part between the
neighboring units. Due to the
co-occurrence constraint and transition
constraint, COV model reduces the
search space and improves tagging
accuracy. The 2-gram COV is applied
to Chinese PoS tagging and the
precision rate of the open test is as high
as 96.83%, which is higher than the
second order HMM, which is 95.73%.
The result is also comparable to the
discriminative models but COV takes
much less training time than them.
With symbol decoding COV prunes
many nodes before statistics decoding
and the search space of COV is
about10-20% less than that of HMM.
</bodyText>
<sectionHeader confidence="0.992437" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999417078947369">
Part of Speech (PoS) can provide much useful
information for most natural language
processing tasks such as word sense
disambiguation, chunk detection, sentence
parsing, speech synthesis, machine translation
and so on. Therefore lots of efforts have been
made to build effective and robust models for
automatic PoS tagging. According to Doug
Cutting (1992), a practical PoS tagger should be
“robust, efficient, accurate, tunable and
reusable”. With regard to efficiency the basic
requirement for a PoS tagger is that training and
test time should not be too long. And for a
robust tagger the tagging accuracy should be as
high as possible and can well deal with the
sparseness data.
Most of the approaches to PoS tagging can be
divided into two main classes, rule-based and
statistics-based approach. In rule-based
approaches, words are assigned tags based on a
set of rules and a lexicon. These rules can either
be manually crafted, or learned, as in the
transformation-based error-driven approach of
Brill (1995).
In the statistics-based approaches HMM is the
representative of generative models and is
widely used in PoS tagging (Church, 1988;
Cutting et al. 1992; Thede &amp; Harper 1999,
Huang et al. 2007, etc.) .
Maximum Entropy model and Conditional
Random Fields (CRFs) model are the
representatives of discriminative models and
are also applied in PoS tagging. Thanks to the
flexibility of features selection these
discriminative models achieve higher precision
rates than the generative models in PoS tagging
(Adwait, 1996; Lafferty, 2001 etc.). But the
training of discriminative models is
</bodyText>
<page confidence="0.656439">
3
</page>
<note confidence="0.9964335">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 3–10,
Wuhan, China, 20-21 October 2014
</note>
<bodyText confidence="0.999934911111111">
time-consuming and requires high-quality
computer processing power, which affects their
applications in the real tasks.
Concerning all the characteristics of generative
and discriminative models, we proposed a new
model on the basis of HMM. The new model
expand the observation from one single word to
n-gram unit and between the neighboring units
there is an n-1 gram part, which is shared by the
neighboring units. So the new model is called
Context OVerlapping (COV) model.
COV is a general sequence labeling model and
has been applied to Chinese and English PoS
tagging tasks. In these tasks COV achieves
better performance than HMM and its
performance is comparable to the discriminative
models. Meanwhile its training time is much
less than the discriminative models, which
makes the model more efficient and robust in
the real tasks.
The structure of the article is that: the first part
will briefly introduce PoS tagging, in the second
part we will introduce COV model. The third
part will compare COV with HMM. The fourth
part will address how to estimate parameters
and handle sparseness data. The fifth part is
about the algorithm of symbol decoding. The
sixth part is about evaluation criteria and the
seventh part presents the experiments and
results. The final part is some discussions and
future work to do.
elements are represented as isolated units,
independent from the other elements in an
observation sequence. More precisely, the
observation element at any given time may only
directly depend on the state at that time. This is
an appropriate assumption for a few simple data
sets, however most real-world observation
sequences such as sentences are best
represented in terms of multiple interacting
features and even long-range dependencies
between observation elements. Due to the
observation independence assumption the
performance of HMM is limited in PoS tagging.
For example, here are 2 Chinese sentences:
</bodyText>
<listItem confidence="0.871856166666667">
(1) $ -�-, /n 31A/v &apos;NIS/v a MRUa M, /u
Zf/P/vn f/P /n
(The mayor put emphasis on the careful
working style.)
(2) $ -�-,/n N/v &apos;NIS/v a []*/a M, /u ffA/n
V&apos;R, /f
</listItem>
<bodyText confidence="0.9906445">
(The mayor should care about those people in
troubles.)
For the convenience of analysis we assume that
in each sentence only “&apos;NIS”(careful or care)
has two parts of speech, adjective (a) or verb (v),
and other words only have one PoS. If we use
the first-order HMM model to predict the PoS of
“&apos;NIS” the prediction will be like:
</bodyText>
<equation confidence="0.999123142857143">
p (X
 |v)p(a

maxp(n)p(v |n)p(X  |v)p(a  |X)
X {a ,v}
p(u  |a)p(vn  |u)p(n
arg
Q 
1
 |v)
2 COV Model  |vn)p(  |n)p(  |vn)
�� �=A
p(  |X)p(  |a)p(  |a)p(
�� -4%C lb, T_�
</equation>
<bodyText confidence="0.99321">
COV model is based on HMM. HMM is a form
of generative model, that defines a joint
probability distribution p(X,Y) where X and Y
are random variables respectively ranging over
observation sequences and their corresponding
state sequences. In order to define a joint
distribution, generative models must enumerate
all possible observation sequences. For most
domains, it is intractable unless observation
</bodyText>
<equation confidence="0.912574">
p(��  |n)
</equation>
<bodyText confidence="0.9992098">
Q, denotes the state sequence of sentence (1)
and X denotes the possible state of “&apos;NIS”. For
only “&apos;NIS” is ambiguous and other words all
have only one PoS, the formula can be
simplified as:
</bodyText>
<equation confidence="0.940191">
X {a ,v}
 arg max |X)p(深入|X)
4
</equation>
<bodyText confidence="0.954613625">
And as same as sentence (1) we can get the
prediction formula of sentence (2) as: h
arg max q q
i  1 , i (p(q1)p(q2  |q1) p(qi1qi  |qi2qi1)
B41,4142,4243, . . . , 4h14h , 4hE
h i 3

Comparing the two formulae, we find that
</bodyText>
<equation confidence="0.893352">
p(o1  |q1)p(oi1oi  |qi1qi))
2
i

</equation>
<bodyText confidence="0.992383636363637">
and are the same, which means that HMM
tagger will not distinguish between the different
PoSs of “&apos;NA” in the two sentences. In fact
“&apos;NA” in sentence (1) is an adjective and in
sentence (2) is a verb. So HMM must make one
mistake either in sentence (1) or sentence (2).
The mistake shows the limitation of HMM in
PoS tagging.
In order to overcome the shortcomings of
observation independence assumption of HMM
and combine more context information into the
model, COV model is proposed in this paper.
The formalism of 2-gram COV is as follows
and the formalisms of other n-gram COV (n&gt;2)
models can be gotten according to the 2-gram
model.
In the 2-gram COV there is a basic state set
Q = {q1, q2.... qJ . The observation sequence is
of wi  1 and wi .
S= w1...Wh . The corresponding state of a
2-gram observation unit wi_1 wi (2&lt;i&lt;h) is a
state set ei = {q1qi } , in which q 1 is one
of the basic states ofwi-1 and is one of the
basic states ofwi . The state sequence q 1 q&apos;
is called one state unit of the observation unit
wi_1wi . It is notable that ei is the state set
when the word wi-1 and wi co-occur, which
is called Co-occurrence Constraint(CC). When
wi-1 and wi co-occur the amount of possible
states of wi-1 wi will not be more than the
amount of the combination of states
The search for the state sequence with the
highest joint probability can be computed like:
</bodyText>
<equation confidence="0.980440666666667">
Q arg max P(Q  |S)

= =
</equation>
<bodyText confidence="0.999330111111111">
Q denotes the state sequence and S denotes the
observation sequence. Q denotes the final state
sequence, whose joint probability is the
highest.
For the convenience of computation, we insert
2 “*B*”, whose state is “B” at the beginning of
the sequence and insert 2 “*E* ”, whose state is
“E” at the end of the sequence. And then the
above formula will be:
</bodyText>
<equation confidence="0.992340444444445">
h

Q  argmax( p(qi1qi  |qi2qi1)
q q
i  1 , i
h

 p(oi1oi  |qi1qi ))
1
</equation>
<bodyText confidence="0.985954066666667">
In this model there is an overlapping part
between the neighboring observation units
wi_2wi_1 and wi_twi . Forwa_1 is shared by
the neighboring units, the corresponding states
units of wi_2wi_1 and wi_1wi should also
share the same overlapping state. If g k 2 q k 1 is
one state of wi_2wi_1 and q i1q&apos; is one state
of wi_1wi, then only if qi 1 is the same as
q 1 then it is possible to transmit from state
g k 2 q k 1 to q 1 q&apos; , otherwise there is no
transition path from g k 2 q k 1 to q 1 q&apos; . The
constraint qi 1 = q,!, is called Transition
Constraint (TC).
Q is a sequence consisting of h+1 2-gram
state units like:
</bodyText>
<equation confidence="0.848528">
( Q )
</equation>
<bodyText confidence="0.99756">
It is obvious that the final state sequence can be
gotten from the above sequence.
</bodyText>
<equation confidence="0.5379495">
2
ˆ
i
1
2
i

5
</equation>
<sectionHeader confidence="0.688409" genericHeader="introduction">
3 Comparisons between COV and
HMM
</sectionHeader>
<bodyText confidence="0.999285461538461">
There are 3 different points between COV
and HMM.
First, in the nth HMM if each observation has k
states and then the amount of the history states
will be kn. But in the n-gram COV the amount
of the history states will usually be smaller than
kn because of the Constraint of Co-occurrence.
And then the search space of COV will also be
smaller than HMM.
Second, in the nth order HMM the emission
probability of to is only P(  |qt ). But
in the n-gram COV, there are n emission
probabilities relevant to and , which are
</bodyText>
<equation confidence="0.5744595">
P( of-n+1 ... o,  |qt-n+1 ... qt ) , ... ,
P( ot ... ot+n-1  |gt•••gt+n-1 ). For all of these
</equation>
<bodyText confidence="0.9990673">
emission probabilities are related to and ,
these observation units will make constraints
on the possible state units.
Third, in the nth order HMM the transition
probability from the history state to the current
state is P( I ). But in the n-gram
COV the transition path must obey TC, which
requires the overlapping part of the
neighboring state units must be the same. If the
neighboring state units obey TC the transition
probability is the same as that in nth order
HMM. If the neighboring state units don’t obey
TC there will be no transition path between
them. With TC a great amount of paths are
pruned, which makes the search space reduced.
Here is an example to illustrate the lattice
building and tagging process by 2-gram COV.
In particular, this example needn’t any
probability computation and can get the final
state sequence just with symbol comparing.
</bodyText>
<table confidence="0.999843416666667">
1 2 3 4 5
*B*-*B* *B*-领导 领导- 强调- 深入-细
强调 深入 致
B-B B-n n-v v-a a-a
B-vn v-ad ad-ad
B-v
6 7 8 9 10
细致-的 的-工作 工作-作风 作风- *E*-
*E* *E*
a-u u-v vn-n n-E E-E
u-n
u-vn
</table>
<tableCaption confidence="0.999257">
Table 1: An example to illustrate COV tagging
</tableCaption>
<bodyText confidence="0.983216297297297">
process (For the space limitation the table is
split to two)
In the above table each column is a 2-gram
observation unit and the neighboring units
share an overlapping part. For example, unit 2
is “*B*-领导”(*B*-leader) and unit 3 is “领
导 - 强 调 ” (leader-emphasizes) , “ 领 导 ”
(leader) is the overlapping part between unit
2 and unit 3. Unit 2 has 3 possible state units,
which are “B-n, B-vn, B-v”, and unit 3 has
only one possible state unit, which is “n-v”.
With Transition Constraint only if the
overlapping part of state unit 2 and state unit 3
is the same there can be a transition path. So in
the state units of unit 2 only “B-n” is remained
and the state units “B-vn” and “B-v” are all
eliminated for their overlapping parts (vn and v)
are not the same as the overlapping part of state
unit 3 (n). The shadowed grids in the table are
all the impossible states and are eliminated. In
this example after the symbol comparing and
elimination there remains only one path for the
sentence and the path is the final tagging result.
So this sentence is tagged without any
probability computation but only with the
symbol comparing. The process of symbol
comparing and elimination is called symbol
decoding.
Most times there may be more than one
possible paths remained after symbol decoding
and then the Viterbi algorithm will be applied
to get the best tagging sequence. Although
HMM also applies Viterbi for decoding, the
search space of HMM is bigger than that of
COV because COV has eliminated many
impossible states in the step of symbol
decoding.
</bodyText>
<sectionHeader confidence="0.471568" genericHeader="method">
4 Parameters estimation and strategy
</sectionHeader>
<subsectionHeader confidence="0.856838">
of handling sparseness data
</subsectionHeader>
<bodyText confidence="0.9918855">
There are 2 main parameters to be estimated in
COV:
</bodyText>
<listItem confidence="0.9995145">
(1) :State transition probability;
(2) :State emission probability.
</listItem>
<page confidence="0.894687">
6
</page>
<bodyText confidence="0.989789647058823">
We apply the maximum likelihood to estimate
these parameters from the tagged corpus. The
details of the estimation will not be introduced
here.
For the expansion of the observation the
sparseness problem in n-gram COV is more
serious than that in HMM. COV applies
back-off strategy to deal with the sparseness
data. The main idea is that if n-gram (n&gt;2)
wi_,,+, ... wi is not in the n-gram vocabulary,
which is gotten from the training corpus, it will
be replaced by n-1 gram . And if
wi_,wi is not in the 2-gram vocabulary then
the state units of will be replaced by
the combination of states of and . If
wi is not in the unigram vocabulary it will be
handled as same as in HMM.
</bodyText>
<sectionHeader confidence="0.942735" genericHeader="method">
5 Tagging Procedures and Decoding
Algorithm
</sectionHeader>
<bodyText confidence="0.957887">
The main procedures of COV tagging is
described in the following flow diagram.
</bodyText>
<figure confidence="0.843077">
Preprocessing
Symbol Decoding
</figure>
<figureCaption confidence="0.998877">
Figure 1: Flow diagram of PoS tagging by COV
</figureCaption>
<bodyText confidence="0.9662">
There are two steps of decoding in PoS tagging
by COV:
</bodyText>
<listItem confidence="0.9949905">
(1) Symbol decoding
(2) Statistics decoding
</listItem>
<bodyText confidence="0.970866952380953">
Statistics decoding applies Viterbi algorithm,
which is explained in detail by Rabiner (1989)
and will not be repeated here.
Here we will describe the symbol decoding
algorithm in detail. First we define the suffix
and prefix of a state sequence:
Suffix of is defined asqi-,,,2•••qi
Prefix of is defined asqi_„+, ...qi_,
The symbol decoding algorithm is as follows:
Input: word sequence S= wo ... w,, and all the
possible state units of each n-gram unit
(1) Comparing the neighboring n-gram state
units from left to right.
For any given neighboring observation units
Si_, = wi_n...Wi_, and = wi-n+1 ... wi , they
have the corresponding state unit setsei_,
and . And each state unit in the set is called
state node.
For each node Ei-1 in the state set of , a
comparison is made between the suffix of Ei-1
and the prefix of the node Ei in . If they are
the same then a parent-child relation is built
between the neighboring nodes Ei-1 and Ei .
If node Ei in has no parent node inei_,
then Ei will be eliminated and if node Ei-1 in
ei_, has no child node in , Ei-1 will also be
eliminated.
(2) Backward from right to left
A. If a node Ei-1 is eliminated in step (1) for
it doesn’t have any child node in , then the
relation between Ei-1 and its parent node Ei-2
will also be eliminated.
B. If the parent-child relation between Ei-2
and Ei-1 is eliminated in step A and Ei-2
doesn’t have any child node then Ei-2 will also
be eliminated.
Backward to the left end of the sequence and
the process of symbol decoding finishes.
Table 2 Symbol Decoding Algorithm
After symbol decoding the remaining nodes
construct a node lattice. If there is only one
path from left to right in the lattice then
</bodyText>
<figure confidence="0.990013666666667">
Text
Yes
One Path
No
Statistics Decoding
Tagging Result
</figure>
<page confidence="0.662818">
7
</page>
<bodyText confidence="0.999737333333333">
decoding finishes and the state sequence is output.
Otherwise Viterbi algorithm is used to calculate
and select the most probable path.
</bodyText>
<sectionHeader confidence="0.972253" genericHeader="method">
6 Evaluation Criteria
</sectionHeader>
<bodyText confidence="0.993737">
We use the following criteria to evaluate the
performances of COV.
</bodyText>
<listItem confidence="0.994808285714286">
(1) PA: Overall precision rate
(2) PM: Precision rate of the multi-class words,
(3)PO: Precision rate of OOV (Out Of
Vocabulary), not including the personal names,
location names and organization names, etc.
(4) PE: Error reduction rate, comparing with the
baseline model.
</listItem>
<bodyText confidence="0.640970666666667">
All the above criteria have been introduced in
Kupiec (1992) and Cutting (1992) etc and will
not be repeated here.
</bodyText>
<listItem confidence="0.925528">
(5) PS : State certainty rate
</listItem>
<bodyText confidence="0.9999292">
In order to measure the statistics decoding
complexity, we define State certainty rate PS.
Count(Total_State_Nodes) denotes the total
number of possible states for all the
observations in statistics decoding. Due to the
symbol decoding many states have been pruned
in COV and the search space for statistics
decoding is reduced accordingly. The level of
search space reduction can be indicated by the
criteria of Ps.
</bodyText>
<sectionHeader confidence="0.998539" genericHeader="evaluation">
7 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999637">
7.1 Corpus and Preprocessing
</subsectionHeader>
<bodyText confidence="0.9944976">
The training and test data are all taken from the
People’s Daily of 2000 year, which has been
segmented and manually assigned PoS tags by
the Peking university. The division of corpus is
as follows:
</bodyText>
<table confidence="0.981950333333333">
Group Usage of Months Amount of
corpus tokens
1 Training Feb. 1050934
2 Feb.-June. 6142402
3 Open Test Jan. 1235628
4 Close Test Feb. 1050934
</table>
<tableCaption confidence="0.995178">
Table 3 Division of corpus
</tableCaption>
<bodyText confidence="0.999661111111111">
The baseline model is the 2nd order HMM,
whose results will be compared with that of
2-gram COV.
Before training and tagging the corpus is
preprocessed. All the named entities such as
personal names, location names, organization
names and all the digits are replaced by some
particular symbols. For example, personal
names are all replaced by “*PerN*”.
</bodyText>
<sectionHeader confidence="0.418763" genericHeader="conclusions">
7.2 Results
</sectionHeader>
<table confidence="0.9993625">
PA PM
2nd order HMM 96.54% 92.76%
2-gram COV 98.29% 96.44%
PE 50.58% 50.83%
</table>
<tableCaption confidence="0.998336">
Table 4: Results of the close test.
</tableCaption>
<bodyText confidence="0.383917">
Corpus of group 2 in table 3 is used as the
training corpus.
</bodyText>
<table confidence="0.9988385">
Group 1 Group 2
2nd order HMM 94.63% 95.73%
2-gram COV 95.53% 96.79%
3-gram COV 95.63% 96.83%
</table>
<tableCaption confidence="0.8911085">
Table 5: PA of HMM, 2-gram and 3-gram COV
in open test.
</tableCaption>
<bodyText confidence="0.99375575">
The corpus of Group 1 and 2 are used as
training corpus.
The above results show that 2-gram and
3-gram COV all outperform second order
HMM. And 3-gram COV outperforms 2-gram
COV, which indicates that with the expansion
of observation the precision rate of COV will
not decline but increase.
</bodyText>
<table confidence="0.994531">
Group 1 Group 2
2nd order HMM 90.75% 92.02%
2-gram COV 92.66% 94.24%
PE 20.64% 27.85%
</table>
<tableCaption confidence="0.725231666666667">
Table 6: PM of HMM and COV in open test.
The result shows that COV has a better
performance in tagging multi-class words than
</tableCaption>
<table confidence="0.907523833333333">
8
HMM.
Group 1 Group 2
HMM 53.21% 55.07%
COV(2-gram) 92.24% 93.99%
COV (unigram) 53.27% 55.35%
</table>
<tableCaption confidence="0.998292">
Table 7: PO of HMM and COV
</tableCaption>
<bodyText confidence="0.994431866666667">
With regard to the 2-gram OOV, the OOV
precision rate of COV is higher than 90%,
which indicates that COV can well deal with
the OOV problem when the observation unit is
expanded.
We have done some experiments to compare
the time cost and precision rate among HMM,
COV and discriminative models such as
MaxEnt and CRFs. For the limitation of
computer processing power, we choose the
People’s Daily of January, 2000 as the training
data and the first 5000 paragraphs of the
People’s Daily of February , 2000 as test data.
The taggers are the MaxEnt tagger developed
by Standford University and CRF++.
</bodyText>
<table confidence="0.993478777777778">
HMM COV MaxEn CRF 1 CRF 2
t
Trainin 1mins 2mins 4.6hrs 63hrs 60 hrs
g
time
Test 4mins 8mins 11mins 17mins 11mins
time
PA 94.23 95.43 95.69% 95.67 95.80
% % %
</table>
<tableCaption confidence="0.889938">
Table 8: Training, test time and PA of different
models
</tableCaption>
<bodyText confidence="0.998797571428572">
The template of MaxEnt is: w-1, w0, w+1,
prefix of w0, suffix of w0, length of w0
The template of CRF1 is: w-1, w0, w+1, prefix
of w0
The template of CRF2 is: w-1, w0, w+1, prefix
of w0, suffix of w0, length of w0
The above data show that the precision rate of
COV is higher than HMM, and comparable to
the discriminative models. Moreover, training
time of COV is much less than the
discriminative models and almost at the same
level as HMM. High precision rate and low
time cost makes COV more competitive and
practical than other models.
</bodyText>
<table confidence="0.99973425">
Training HMM COV Reduction Reduction
Group of Ps rate of Ps
1 1.79 1.66 0.14 7.82%
2 2.03 1.57 0.46 22.66%
</table>
<tableCaption confidence="0.8670915">
Table 9: Ps of 2nd order HMM and 2-gram
COV
</tableCaption>
<bodyText confidence="0.9990152">
The above result shows that the search space in
statistics decoding of COV is smaller than
HMM.
We also count the tokens which can be tagged
with symbol decoding.
</bodyText>
<table confidence="0.998542666666667">
Tokens of Percentage
Training
Group Symbol of Symbol PA
Decoding Decoding
1 86187 6.98% 99.24%
2 92174 7.46% 99.42%
</table>
<tableCaption confidence="0.995896">
Table 10: Results of symbol decoding
</tableCaption>
<bodyText confidence="0.999115555555556">
The total tokens of test corpus is 1235631.
The above data shows that there are about 7%
tokens which can be tagged with symbol
decoding and without any probability
computation. Moreover, the precision rate of
symbol decoding is above 99%, which is much
higher than the average precision rate.
The smaller search space and higher precision
rate proves the efficiency and robustness of
COV in PoS tagging.
We also conducted some experiments of
English PoS tagging. The training and test data
are from the Wall Street Journal (WSJ) in Penn
Tree Bank. We use the texts of group 00 to 19
in WSJ as training data and group 00 to 04 as
close test data and group 23 to 24 as open test
data. The baseline model is also the 2nd order
HMM. Results are as follows.
</bodyText>
<table confidence="0.7334195">
PM of
PA of PA of PM of
9
HMM COV HMM COV
Close 97.85% 98.29% 94.85% 96.44%
Test
Open 96.48% 96.79% 93.92% 95.18%
Test
</table>
<tableCaption confidence="0.897614">
Table 11 Results of English PoS tagging
</tableCaption>
<bodyText confidence="0.859307">
Experiments
The above results show that COV also
outperforms HMM in English PoS tagging.
</bodyText>
<sectionHeader confidence="0.998758" genericHeader="acknowledgments">
8 Discussion
</sectionHeader>
<bodyText confidence="0.99994128">
COV is not only suitable to PoS tagging task.
We have applied it to the Chinese word
segmentation, sentence boundary detection and
chunk detection, in which COV also achieves
satisfactory results. COV is not limited to the
certain language but can be applied in the
tagging tasks of different languages.
Comparing with HMM, COV has the
advantages of smaller search space and higher
tagging precision rate. Comparing with the
discriminative models, COV has the
advantages of less training time and
comparable precision rate. All of these prove
that COV is a general, efficient and robust
model for sequence labeling.
Meanwhile we also find that it is difficult for
COV to combine more context and lexical
features as discriminative models can do. For
example, COV has not taken the suffix or
prefix of a word into the model. In fact such
information is important for guessing the PoS
of unknown words. In the future we will make
efforts to take more context and lexical
information into the model and improve its
performance.
</bodyText>
<sectionHeader confidence="0.994326" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999826153846153">
L. Rabiner. 1989. A Tutorial on Hidden Markov
Models and Selected Applications in Speech
Recognition, Proc. of the IEEE, 77(2).
Church, K. 1988. A stochastic parts program and
noun phrase parser for unrestricted text.
Proceedings of Second ACL Applied NLP,
136-143.
Scott M. Thede, Mary P. Harper. 1999. Second-order
hidden Markov model for part-of-speech tagging.
In ACL 37, 175–182.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part of speech tagging. Computational
Linguistics, 21(4):543-565.
Julian Kupiec. 1992. Robust part-of-speech tagging
using a hidden Markov model. Computer Speech
and Language, 6(3):225-242.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, pages 133-142.
Doug Cutting, Julian Kupiec, Jan Pedersen, and
Penelope Sibun. 1992. A practical part-of-speech
tagger. In Proceedings of the 3rd Conference on
Applied Natural Language Processing (ACL),
pages 133-140.
Scott M. Thede and Mary P. Harper. 1999. A
second-order hidden Markov model for
part-of-speech tagging. In ACL, pages 175–182.
J. Lafferty, A. McCallum, and F. Pereira. 2001.
Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In
Proceedings of ICML-01, pages 282-289.
Zhongqiang Huang , Mary P. Harper , Wen Wang.
2007. Mandarin Part-of-Speech Tagging and
Discriminative Reranking. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pp. 1093–1102.
</reference>
<page confidence="0.830883">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.768849">
<title confidence="0.995776">COV Model and its Application in Chinese Part-of-Speech Tagging</title>
<author confidence="0.884069">Xing Fukun Song Rou</author>
<affiliation confidence="0.868359">Luoyang Foreign Languages University Beijing Language and Cultural</affiliation>
<address confidence="0.987618">471003 Henan University 100086 Beijing</address>
<email confidence="0.980875">xingfukun@126.comsongrou@126.com</email>
<abstract confidence="0.996753260869565">This article presents a new sequence labeling model named Context OVerlapping (COV) model, which expands observation from single word to n-gram unit and there is an overlapping part between the neighboring units. Due to the co-occurrence constraint and transition constraint, COV model reduces the search space and improves tagging accuracy. The 2-gram COV is applied to Chinese PoS tagging and the precision rate of the open test is as high as 96.83%, which is higher than the second order HMM, which is 95.73%. The result is also comparable to the discriminative models but COV takes much less training time than them. With symbol decoding COV prunes many nodes before statistics decoding and the search space of COV is about10-20% less than that of HMM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition,</title>
<date>1989</date>
<booktitle>Proc. of the IEEE,</booktitle>
<volume>77</volume>
<issue>2</issue>
<contexts>
<context position="13306" citStr="Rabiner (1989)" startWordPosition="2342" endWordPosition="2343">eplaced by n-1 gram . And if wi_,wi is not in the 2-gram vocabulary then the state units of will be replaced by the combination of states of and . If wi is not in the unigram vocabulary it will be handled as same as in HMM. 5 Tagging Procedures and Decoding Algorithm The main procedures of COV tagging is described in the following flow diagram. Preprocessing Symbol Decoding Figure 1: Flow diagram of PoS tagging by COV There are two steps of decoding in PoS tagging by COV: (1) Symbol decoding (2) Statistics decoding Statistics decoding applies Viterbi algorithm, which is explained in detail by Rabiner (1989) and will not be repeated here. Here we will describe the symbol decoding algorithm in detail. First we define the suffix and prefix of a state sequence: Suffix of is defined asqi-,,,2•••qi Prefix of is defined asqi_„+, ...qi_, The symbol decoding algorithm is as follows: Input: word sequence S= wo ... w,, and all the possible state units of each n-gram unit (1) Comparing the neighboring n-gram state units from left to right. For any given neighboring observation units Si_, = wi_n...Wi_, and = wi-n+1 ... wi , they have the corresponding state unit setsei_, and . And each state unit in the set </context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>L. Rabiner. 1989. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, Proc. of the IEEE, 77(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>Proceedings of Second ACL Applied NLP,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="2150" citStr="Church, 1988" startWordPosition="334" endWordPosition="335">t time should not be too long. And for a robust tagger the tagging accuracy should be as high as possible and can well deal with the sparseness data. Most of the approaches to PoS tagging can be divided into two main classes, rule-based and statistics-based approach. In rule-based approaches, words are assigned tags based on a set of rules and a lexicon. These rules can either be manually crafted, or learned, as in the transformation-based error-driven approach of Brill (1995). In the statistics-based approaches HMM is the representative of generative models and is widely used in PoS tagging (Church, 1988; Cutting et al. 1992; Thede &amp; Harper 1999, Huang et al. 2007, etc.) . Maximum Entropy model and Conditional Random Fields (CRFs) model are the representatives of discriminative models and are also applied in PoS tagging. Thanks to the flexibility of features selection these discriminative models achieve higher precision rates than the generative models in PoS tagging (Adwait, 1996; Lafferty, 2001 etc.). But the training of discriminative models is 3 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 3–10, Wuhan, China, 20-21 October 2014 time-consuming</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, K. 1988. A stochastic parts program and noun phrase parser for unrestricted text. Proceedings of Second ACL Applied NLP, 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott M Thede</author>
<author>Mary P Harper</author>
</authors>
<title>Second-order hidden Markov model for part-of-speech tagging.</title>
<date>1999</date>
<booktitle>In ACL 37,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="2192" citStr="Thede &amp; Harper 1999" startWordPosition="340" endWordPosition="343">for a robust tagger the tagging accuracy should be as high as possible and can well deal with the sparseness data. Most of the approaches to PoS tagging can be divided into two main classes, rule-based and statistics-based approach. In rule-based approaches, words are assigned tags based on a set of rules and a lexicon. These rules can either be manually crafted, or learned, as in the transformation-based error-driven approach of Brill (1995). In the statistics-based approaches HMM is the representative of generative models and is widely used in PoS tagging (Church, 1988; Cutting et al. 1992; Thede &amp; Harper 1999, Huang et al. 2007, etc.) . Maximum Entropy model and Conditional Random Fields (CRFs) model are the representatives of discriminative models and are also applied in PoS tagging. Thanks to the flexibility of features selection these discriminative models achieve higher precision rates than the generative models in PoS tagging (Adwait, 1996; Lafferty, 2001 etc.). But the training of discriminative models is 3 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 3–10, Wuhan, China, 20-21 October 2014 time-consuming and requires high-quality computer proces</context>
</contexts>
<marker>Thede, Harper, 1999</marker>
<rawString>Scott M. Thede, Mary P. Harper. 1999. Second-order hidden Markov model for part-of-speech tagging. In ACL 37, 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<contexts>
<context position="2019" citStr="Brill (1995)" startWordPosition="314" endWordPosition="315">ficient, accurate, tunable and reusable”. With regard to efficiency the basic requirement for a PoS tagger is that training and test time should not be too long. And for a robust tagger the tagging accuracy should be as high as possible and can well deal with the sparseness data. Most of the approaches to PoS tagging can be divided into two main classes, rule-based and statistics-based approach. In rule-based approaches, words are assigned tags based on a set of rules and a lexicon. These rules can either be manually crafted, or learned, as in the transformation-based error-driven approach of Brill (1995). In the statistics-based approaches HMM is the representative of generative models and is widely used in PoS tagging (Church, 1988; Cutting et al. 1992; Thede &amp; Harper 1999, Huang et al. 2007, etc.) . Maximum Entropy model and Conditional Random Fields (CRFs) model are the representatives of discriminative models and are also applied in PoS tagging. Thanks to the flexibility of features selection these discriminative models achieve higher precision rates than the generative models in PoS tagging (Adwait, 1996; Lafferty, 2001 etc.). But the training of discriminative models is 3 Proceedings of</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part of speech tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>Robust part-of-speech tagging using a hidden Markov model.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--3</pages>
<contexts>
<context position="15516" citStr="Kupiec (1992)" startWordPosition="2735" endWordPosition="2736">t Yes One Path No Statistics Decoding Tagging Result 7 decoding finishes and the state sequence is output. Otherwise Viterbi algorithm is used to calculate and select the most probable path. 6 Evaluation Criteria We use the following criteria to evaluate the performances of COV. (1) PA: Overall precision rate (2) PM: Precision rate of the multi-class words, (3)PO: Precision rate of OOV (Out Of Vocabulary), not including the personal names, location names and organization names, etc. (4) PE: Error reduction rate, comparing with the baseline model. All the above criteria have been introduced in Kupiec (1992) and Cutting (1992) etc and will not be repeated here. (5) PS : State certainty rate In order to measure the statistics decoding complexity, we define State certainty rate PS. Count(Total_State_Nodes) denotes the total number of possible states for all the observations in statistics decoding. Due to the symbol decoding many states have been pruned in COV and the search space for statistics decoding is reduced accordingly. The level of search space reduction can be indicated by the criteria of Ps. 7 Experiments 7.1 Corpus and Preprocessing The training and test data are all taken from the Peopl</context>
</contexts>
<marker>Kupiec, 1992</marker>
<rawString>Julian Kupiec. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6(3):225-242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Cutting</author>
<author>Julian Kupiec</author>
<author>Jan Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A practical part-of-speech tagger.</title>
<date>1992</date>
<booktitle>In Proceedings of the 3rd Conference on Applied Natural Language Processing (ACL),</booktitle>
<pages>133--140</pages>
<contexts>
<context position="2171" citStr="Cutting et al. 1992" startWordPosition="336" endWordPosition="339">not be too long. And for a robust tagger the tagging accuracy should be as high as possible and can well deal with the sparseness data. Most of the approaches to PoS tagging can be divided into two main classes, rule-based and statistics-based approach. In rule-based approaches, words are assigned tags based on a set of rules and a lexicon. These rules can either be manually crafted, or learned, as in the transformation-based error-driven approach of Brill (1995). In the statistics-based approaches HMM is the representative of generative models and is widely used in PoS tagging (Church, 1988; Cutting et al. 1992; Thede &amp; Harper 1999, Huang et al. 2007, etc.) . Maximum Entropy model and Conditional Random Fields (CRFs) model are the representatives of discriminative models and are also applied in PoS tagging. Thanks to the flexibility of features selection these discriminative models achieve higher precision rates than the generative models in PoS tagging (Adwait, 1996; Lafferty, 2001 etc.). But the training of discriminative models is 3 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 3–10, Wuhan, China, 20-21 October 2014 time-consuming and requires high-qu</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>Doug Cutting, Julian Kupiec, Jan Pedersen, and Penelope Sibun. 1992. A practical part-of-speech tagger. In Proceedings of the 3rd Conference on Applied Natural Language Processing (ACL), pages 133-140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott M Thede</author>
<author>Mary P Harper</author>
</authors>
<title>A second-order hidden Markov model for part-of-speech tagging. In</title>
<date>1999</date>
<booktitle>ACL,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="2192" citStr="Thede &amp; Harper 1999" startWordPosition="340" endWordPosition="343">for a robust tagger the tagging accuracy should be as high as possible and can well deal with the sparseness data. Most of the approaches to PoS tagging can be divided into two main classes, rule-based and statistics-based approach. In rule-based approaches, words are assigned tags based on a set of rules and a lexicon. These rules can either be manually crafted, or learned, as in the transformation-based error-driven approach of Brill (1995). In the statistics-based approaches HMM is the representative of generative models and is widely used in PoS tagging (Church, 1988; Cutting et al. 1992; Thede &amp; Harper 1999, Huang et al. 2007, etc.) . Maximum Entropy model and Conditional Random Fields (CRFs) model are the representatives of discriminative models and are also applied in PoS tagging. Thanks to the flexibility of features selection these discriminative models achieve higher precision rates than the generative models in PoS tagging (Adwait, 1996; Lafferty, 2001 etc.). But the training of discriminative models is 3 Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 3–10, Wuhan, China, 20-21 October 2014 time-consuming and requires high-quality computer proces</context>
</contexts>
<marker>Thede, Harper, 1999</marker>
<rawString>Scott M. Thede and Mary P. Harper. 1999. A second-order hidden Markov model for part-of-speech tagging. In ACL, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML-01,</booktitle>
<pages>282--289</pages>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML-01, pages 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary P Harper</author>
</authors>
<title>Mandarin Part-of-Speech Tagging and Discriminative Reranking.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1093--1102</pages>
<marker>Harper, 2007</marker>
<rawString>Zhongqiang Huang , Mary P. Harper , Wen Wang. 2007. Mandarin Part-of-Speech Tagging and Discriminative Reranking. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 1093–1102.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
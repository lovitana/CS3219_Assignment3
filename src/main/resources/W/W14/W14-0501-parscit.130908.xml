<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.115704">
<title confidence="0.9919175">
Challenging incrementality in human language processing: two
operations for a cognitive architecture
</title>
<note confidence="0.508220333333333">
Philippe Blache
Aix-Marseille Universit´e &amp; CNRS
LPL (UMR7309), 13100, Aix-en-Provence, France
</note>
<email confidence="0.95838">
blache@blri.fr
</email>
<bodyText confidence="0.999917452830189">
The description of language complexity and the
cognitive load related to the different linguistic
phenomena is a key issue for the understanding
of language processing. Many studies have fo-
cused on the identification of specific parameters
that can lead to a simplification or on the con-
trary to a complexification of the processing (e.g.
the different difficulty models proposed in (Gib-
son, 2000), (Warren and Gibson, 2002), (Hawkins,
2001) ). Similarly, different simplification fac-
tors can be identified, such as the notion of activa-
tion, relying on syntactic priming effects making it
possible to predict (or activate) a word (Vasishth,
2003). Several studies have shown that complex-
ity factors are cumulative (Keller, 2005), but can
be offset by simplification (Blache et al., 2006). It
is therefore necessary to adopt a global point of
view of language processing, explaining the inter-
play between positive and negative cumulativity,
in other words compensation effects.
From the computational point of view, some
models can account more or less explicitly for
these phenomena. This is the case of the Surprisal
index (Hale, 2001), offering for each word an as-
sessment of its integration costs into the syntactic
structure. This evaluation is done starting from the
probability of the possible solutions. On their side,
symbolic approaches also provide an estimation
of the activation degree, depending on the num-
ber and weight of syntactic relations to the current
word (Blache et al., 2006); (Blache, 2013).
These approaches are based on the classical idea
that language processing is incremental and oc-
curs word by word. There are however several ex-
perimental evidences showing that a higher level
of processing is used by human subjects. Eye-
tracking data show for example that fixations are
done by chunks, not by words (Rauzy and Blache,
2012). Similarly, EEG experiments have shown
that processing multiword expressions (for exam-
ple idioms) relies on global mechanisms (Vespig-
nani et al., 2010); (Rommers et al., 2013).
Starting from the question of complexity and its
estimation, I will address in this presentation the
problem of language processing and its organiza-
tion. I propose more precisely, using computa-
tional complexity models, to define a cohesion in-
dex between words. Such an index makes it possi-
ble to define chunks (or more generally units) that
are built directly, by aggregation, instead of syn-
tactic analysis. In this hypothesis, parsing consists
in two different processes: aggregation and inte-
gration.
</bodyText>
<sectionHeader confidence="0.996316" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996535">
This work, carried out within the Labex BLRI
(ANR-11-LABX-0036), has benefited from sup-
port from the French government, managed by
the French National Agency for Research (ANR),
under the project title Investments of the Future
A*MIDEX (ANR-11-IDEX-0001-02).
</bodyText>
<subsectionHeader confidence="0.766587">
Short biography
</subsectionHeader>
<bodyText confidence="0.999253833333333">
Philipe Blache is Senior Researcher at CNRS
(Aix-Marseille University, France). He is the
Director of the BLRI (Brain and Language Re-
search Institute), federating 6 research laborato-
ries in Linguistics, Computer Science, Psychology
and Neurosciences.
Philippe Blache earned an MA in Linguistics
from Universit´e de Provence and a MSc in Com-
puter Science from Universit´e de la M´editerran´ee,
where he received in 1990 his PhD in Artificial In-
telligence.
During his career, Philippe Blache has focused
on Natural Language Processing and Formal Lin-
guistics, with a special interest in spoken language
analysis. He has proposed a linguistic theory,
called Property Grammars, suitable for describ-
ing language in its different uses, and explaining
linguistic domains interaction. His current aca-
</bodyText>
<page confidence="0.827371">
1
</page>
<note confidence="0.6174595">
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 1–2,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999775181818182">
demic works address the question of human lan-
guage processing and its complexity.
Philippe Blache has been director of two CNRS
laboratories in France (2LC and LPL). He has
served on numerous boards (European Chapter
of the ACL, ESSLLI standing committee, CSLP,
etc.). He is currently member of the Scien-
tific Council of Aix-Marseille Universit´e, mem-
ber of the “Comit´e National de la Recherche Sci-
entifique” in computer science and he chairs the
TALN conference standing committee.
</bodyText>
<sectionHeader confidence="0.990953" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998685042553191">
Philippe Blache, Barbara Hemforth, and St´ephane
Rauzy. 2006. Acceptability prediction by means of
grammaticality quantification. In ACL-44: Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of
the Association for Computational Linguistics. As-
sociation for Computational Linguistics, July.
Philippe Blache. 2013. Chunks et activation : un
mod`ele de facilitation du traitement linguistique. In
Proceedings of TALN-2014.
Edward Gibson. 2000. The Dependency Locality The-
ory: A Distance-Based Theory of Linguistic Com-
plexity. In Alec Marantz, Yasushi Miyashita, and
Wayne O’Neil, editors, Image, Language, Brain,
pages 95–126. Cambridge, Massachussetts, MIT
Press.
John Hale. 2001. A probabilistic earley parser as a
psycholinguistic model. In Proceeding of 2nd Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics, Pittsburgh,
PA.
John Hawkins. 2001. Why are categories adjacent.
Journal of Linguistics, 37.
Frank Keller. 2005. Linear Optimality Theory as a
Model of Gradience in Grammar. In Gradience in
Grammar: Generative Perspectives. Oxford Univer-
sity Press.
St´ephane Rauzy and Philippe Blache. 2012. Robust-
ness and processing difficulty models. a pilot study
for eye-tracking data on the french treebank. In
Proceedings of the 1st Eye-Tracking and NLP work-
shop.
Joost Rommers, Antje S Meyer, Peter Praamstra, and
Falk Huettig. 2013. Neuropsychologia. Neuropsy-
chologia, 51(3):437–447, February.
Shravan Vasishth. 2003. Quantifying processing dif-
ficulty in human sentence parsing: The role of de-
cay, activation, and similarity-based interference.
In Proceedings of the European Cognitive Science
Conference 2003.
Francesco Vespignani, Paolo Canal, Nicola Molinaro,
Sergio Fonda, and Cristina Cacciari. 2010. Predic-
tive mechanisms in idiom comprehension. Journal
of Cognitive Neuroscience, 22(8):1682–1700.
Tessa Warren and Ted Gibson. 2002. The influence of
referential processing on sentence complexity. Cog-
nition, 85:79–112.
</reference>
<page confidence="0.99672">
2
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000058">
<title confidence="0.969217">Challenging incrementality in human language processing: operations for a cognitive architecture</title>
<author confidence="0.670882">Philippe</author>
<affiliation confidence="0.414557">Aix-Marseille Universit´e &amp;</affiliation>
<abstract confidence="0.948996242105263">LPL (UMR7309), 13100, Aix-en-Provence, blache@blri.fr The description of language complexity and the cognitive load related to the different linguistic phenomena is a key issue for the understanding of language processing. Many studies have focused on the identification of specific parameters that can lead to a simplification or on the contrary to a complexification of the processing (e.g. the different difficulty models proposed in (Gibson, 2000), (Warren and Gibson, 2002), (Hawkins, 2001) ). Similarly, different simplification factors can be identified, such as the notion of activation, relying on syntactic priming effects making it possible to predict (or activate) a word (Vasishth, 2003). Several studies have shown that complexity factors are cumulative (Keller, 2005), but can be offset by simplification (Blache et al., 2006). It is therefore necessary to adopt a global point of view of language processing, explaining the interplay between positive and negative cumulativity, in other words compensation effects. From the computational point of view, some models can account more or less explicitly for these phenomena. This is the case of the Surprisal index (Hale, 2001), offering for each word an assessment of its integration costs into the syntactic structure. This evaluation is done starting from the probability of the possible solutions. On their side, symbolic approaches also provide an estimation of the activation degree, depending on the number and weight of syntactic relations to the current word (Blache et al., 2006); (Blache, 2013). These approaches are based on the classical idea that language processing is incremental and occurs word by word. There are however several experimental evidences showing that a higher level of processing is used by human subjects. Eyetracking data show for example that fixations are done by chunks, not by words (Rauzy and Blache, 2012). Similarly, EEG experiments have shown that processing multiword expressions (for example idioms) relies on global mechanisms (Vespignani et al., 2010); (Rommers et al., 2013). Starting from the question of complexity and its estimation, I will address in this presentation the problem of language processing and its organization. I propose more precisely, using computational complexity models, to define a cohesion index between words. Such an index makes it possible to define chunks (or more generally units) that are built directly, by aggregation, instead of syntactic analysis. In this hypothesis, parsing consists in two different processes: aggregation and integration. Acknowledgments This work, carried out within the Labex BLRI (ANR-11-LABX-0036), has benefited from support from the French government, managed by the French National Agency for Research (ANR), under the project title Investments of the Future A*MIDEX (ANR-11-IDEX-0001-02). Short biography Philipe Blache is Senior Researcher at CNRS (Aix-Marseille University, France). He is the of the BLRI and Language Refederating 6 research laboratories in Linguistics, Computer Science, Psychology and Neurosciences. Philippe Blache earned an MA in Linguistics from Universit´e de Provence and a MSc in Computer Science from Universit´e de la M´editerran´ee, where he received in 1990 his PhD in Artificial Intelligence. During his career, Philippe Blache has focused on Natural Language Processing and Formal Linguistics, with a special interest in spoken language analysis. He has proposed a linguistic theory, suitable for describing language in its different uses, and explaining domains interaction. His current aca- 1 of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL pages 1–2, Sweden, April 26 2014. Association for Computational Linguistics demic works address the question of human language processing and its complexity. Philippe Blache has been director of two CNRS laboratories in France (2LC and LPL). He has served on numerous boards (European Chapter of the ACL, ESSLLI standing committee, CSLP, etc.). He is currently member of the Scientific Council of Aix-Marseille Universit´e, member of the “Comit´e National de la Recherche Scientifique” in computer science and he chairs the TALN conference standing committee.</abstract>
<note confidence="0.650954236842105">References Philippe Blache, Barbara Hemforth, and St´ephane Rauzy. 2006. Acceptability prediction by means of quantification. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of Association for Computational Association for Computational Linguistics, July. Philippe Blache. 2013. Chunks et activation : un mod`ele de facilitation du traitement linguistique. In of Edward Gibson. 2000. The Dependency Locality Theory: A Distance-Based Theory of Linguistic Complexity. In Alec Marantz, Yasushi Miyashita, and O’Neil, editors, Language, pages 95–126. Cambridge, Massachussetts, MIT Press. John Hale. 2001. A probabilistic earley parser as a model. In of 2nd Conference of the North American Chapter of the Asfor Computational Pittsburgh, PA. John Hawkins. 2001. Why are categories adjacent. of 37. Frank Keller. 2005. Linear Optimality Theory as a of Gradience in Grammar. In in Generative Oxford University Press. St´ephane Rauzy and Philippe Blache. 2012. Robustness and processing difficulty models. a pilot study for eye-tracking data on the french treebank. In Proceedings of the 1st Eye-Tracking and NLP work- Joost Rommers, Antje S Meyer, Peter Praamstra, and Huettig. 2013. Neuropsychologia. Neuropsy- 51(3):437–447, February. Shravan Vasishth. 2003. Quantifying processing difficulty in human sentence parsing: The role of decay, activation, and similarity-based interference.</note>
<title confidence="0.787903">of the European Cognitive Science</title>
<author confidence="0.801298">Predic-</author>
<note confidence="0.736697833333333">mechanisms in idiom comprehension. Cognitive 22(8):1682–1700. Tessa Warren and Ted Gibson. 2002. The influence of processing on sentence complexity. Cog- 85:79–112. 2</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philippe Blache</author>
<author>Barbara Hemforth</author>
<author>St´ephane Rauzy</author>
</authors>
<title>Acceptability prediction by means of grammaticality quantification.</title>
<date>2006</date>
<booktitle>In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="998" citStr="Blache et al., 2006" startWordPosition="143" endWordPosition="146">. Many studies have focused on the identification of specific parameters that can lead to a simplification or on the contrary to a complexification of the processing (e.g. the different difficulty models proposed in (Gibson, 2000), (Warren and Gibson, 2002), (Hawkins, 2001) ). Similarly, different simplification factors can be identified, such as the notion of activation, relying on syntactic priming effects making it possible to predict (or activate) a word (Vasishth, 2003). Several studies have shown that complexity factors are cumulative (Keller, 2005), but can be offset by simplification (Blache et al., 2006). It is therefore necessary to adopt a global point of view of language processing, explaining the interplay between positive and negative cumulativity, in other words compensation effects. From the computational point of view, some models can account more or less explicitly for these phenomena. This is the case of the Surprisal index (Hale, 2001), offering for each word an assessment of its integration costs into the syntactic structure. This evaluation is done starting from the probability of the possible solutions. On their side, symbolic approaches also provide an estimation of the activat</context>
</contexts>
<marker>Blache, Hemforth, Rauzy, 2006</marker>
<rawString>Philippe Blache, Barbara Hemforth, and St´ephane Rauzy. 2006. Acceptability prediction by means of grammaticality quantification. In ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Blache</author>
</authors>
<title>Chunks et activation : un mod`ele de facilitation du traitement linguistique.</title>
<date>2013</date>
<booktitle>In Proceedings of TALN-2014.</booktitle>
<contexts>
<context position="1725" citStr="Blache, 2013" startWordPosition="260" endWordPosition="261">en positive and negative cumulativity, in other words compensation effects. From the computational point of view, some models can account more or less explicitly for these phenomena. This is the case of the Surprisal index (Hale, 2001), offering for each word an assessment of its integration costs into the syntactic structure. This evaluation is done starting from the probability of the possible solutions. On their side, symbolic approaches also provide an estimation of the activation degree, depending on the number and weight of syntactic relations to the current word (Blache et al., 2006); (Blache, 2013). These approaches are based on the classical idea that language processing is incremental and occurs word by word. There are however several experimental evidences showing that a higher level of processing is used by human subjects. Eyetracking data show for example that fixations are done by chunks, not by words (Rauzy and Blache, 2012). Similarly, EEG experiments have shown that processing multiword expressions (for example idioms) relies on global mechanisms (Vespignani et al., 2010); (Rommers et al., 2013). Starting from the question of complexity and its estimation, I will address in thi</context>
</contexts>
<marker>Blache, 2013</marker>
<rawString>Philippe Blache. 2013. Chunks et activation : un mod`ele de facilitation du traitement linguistique. In Proceedings of TALN-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Gibson</author>
</authors>
<title>The Dependency Locality Theory: A Distance-Based Theory of Linguistic Complexity.</title>
<date>2000</date>
<pages>95--126</pages>
<editor>In Alec Marantz, Yasushi Miyashita, and Wayne O’Neil, editors, Image, Language, Brain,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Massachussetts,</location>
<marker>Gibson, 2000</marker>
<rawString>Edward Gibson. 2000. The Dependency Locality Theory: A Distance-Based Theory of Linguistic Complexity. In Alec Marantz, Yasushi Miyashita, and Wayne O’Neil, editors, Image, Language, Brain, pages 95–126. Cambridge, Massachussetts, MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hale</author>
</authors>
<title>A probabilistic earley parser as a psycholinguistic model.</title>
<date>2001</date>
<booktitle>In Proceeding of 2nd Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1347" citStr="Hale, 2001" startWordPosition="200" endWordPosition="201">n of activation, relying on syntactic priming effects making it possible to predict (or activate) a word (Vasishth, 2003). Several studies have shown that complexity factors are cumulative (Keller, 2005), but can be offset by simplification (Blache et al., 2006). It is therefore necessary to adopt a global point of view of language processing, explaining the interplay between positive and negative cumulativity, in other words compensation effects. From the computational point of view, some models can account more or less explicitly for these phenomena. This is the case of the Surprisal index (Hale, 2001), offering for each word an assessment of its integration costs into the syntactic structure. This evaluation is done starting from the probability of the possible solutions. On their side, symbolic approaches also provide an estimation of the activation degree, depending on the number and weight of syntactic relations to the current word (Blache et al., 2006); (Blache, 2013). These approaches are based on the classical idea that language processing is incremental and occurs word by word. There are however several experimental evidences showing that a higher level of processing is used by huma</context>
</contexts>
<marker>Hale, 2001</marker>
<rawString>John Hale. 2001. A probabilistic earley parser as a psycholinguistic model. In Proceeding of 2nd Conference of the North American Chapter of the Association for Computational Linguistics, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Hawkins</author>
</authors>
<title>Why are categories adjacent.</title>
<date>2001</date>
<journal>Journal of Linguistics,</journal>
<volume>37</volume>
<contexts>
<context position="652" citStr="Hawkins, 2001" startWordPosition="91" endWordPosition="92">nguage processing: two operations for a cognitive architecture Philippe Blache Aix-Marseille Universit´e &amp; CNRS LPL (UMR7309), 13100, Aix-en-Provence, France blache@blri.fr The description of language complexity and the cognitive load related to the different linguistic phenomena is a key issue for the understanding of language processing. Many studies have focused on the identification of specific parameters that can lead to a simplification or on the contrary to a complexification of the processing (e.g. the different difficulty models proposed in (Gibson, 2000), (Warren and Gibson, 2002), (Hawkins, 2001) ). Similarly, different simplification factors can be identified, such as the notion of activation, relying on syntactic priming effects making it possible to predict (or activate) a word (Vasishth, 2003). Several studies have shown that complexity factors are cumulative (Keller, 2005), but can be offset by simplification (Blache et al., 2006). It is therefore necessary to adopt a global point of view of language processing, explaining the interplay between positive and negative cumulativity, in other words compensation effects. From the computational point of view, some models can account mo</context>
</contexts>
<marker>Hawkins, 2001</marker>
<rawString>John Hawkins. 2001. Why are categories adjacent. Journal of Linguistics, 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
</authors>
<title>Linear Optimality Theory as a Model of Gradience in Grammar. In Gradience in Grammar: Generative Perspectives.</title>
<date>2005</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="939" citStr="Keller, 2005" startWordPosition="135" endWordPosition="136">y issue for the understanding of language processing. Many studies have focused on the identification of specific parameters that can lead to a simplification or on the contrary to a complexification of the processing (e.g. the different difficulty models proposed in (Gibson, 2000), (Warren and Gibson, 2002), (Hawkins, 2001) ). Similarly, different simplification factors can be identified, such as the notion of activation, relying on syntactic priming effects making it possible to predict (or activate) a word (Vasishth, 2003). Several studies have shown that complexity factors are cumulative (Keller, 2005), but can be offset by simplification (Blache et al., 2006). It is therefore necessary to adopt a global point of view of language processing, explaining the interplay between positive and negative cumulativity, in other words compensation effects. From the computational point of view, some models can account more or less explicitly for these phenomena. This is the case of the Surprisal index (Hale, 2001), offering for each word an assessment of its integration costs into the syntactic structure. This evaluation is done starting from the probability of the possible solutions. On their side, sy</context>
</contexts>
<marker>Keller, 2005</marker>
<rawString>Frank Keller. 2005. Linear Optimality Theory as a Model of Gradience in Grammar. In Gradience in Grammar: Generative Perspectives. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>St´ephane Rauzy</author>
<author>Philippe Blache</author>
</authors>
<title>Robustness and processing difficulty models. a pilot study for eye-tracking data on the french treebank.</title>
<date>2012</date>
<booktitle>In Proceedings of the 1st Eye-Tracking and NLP workshop.</booktitle>
<contexts>
<context position="2065" citStr="Rauzy and Blache, 2012" startWordPosition="315" endWordPosition="318">his evaluation is done starting from the probability of the possible solutions. On their side, symbolic approaches also provide an estimation of the activation degree, depending on the number and weight of syntactic relations to the current word (Blache et al., 2006); (Blache, 2013). These approaches are based on the classical idea that language processing is incremental and occurs word by word. There are however several experimental evidences showing that a higher level of processing is used by human subjects. Eyetracking data show for example that fixations are done by chunks, not by words (Rauzy and Blache, 2012). Similarly, EEG experiments have shown that processing multiword expressions (for example idioms) relies on global mechanisms (Vespignani et al., 2010); (Rommers et al., 2013). Starting from the question of complexity and its estimation, I will address in this presentation the problem of language processing and its organization. I propose more precisely, using computational complexity models, to define a cohesion index between words. Such an index makes it possible to define chunks (or more generally units) that are built directly, by aggregation, instead of syntactic analysis. In this hypoth</context>
</contexts>
<marker>Rauzy, Blache, 2012</marker>
<rawString>St´ephane Rauzy and Philippe Blache. 2012. Robustness and processing difficulty models. a pilot study for eye-tracking data on the french treebank. In Proceedings of the 1st Eye-Tracking and NLP workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joost Rommers</author>
<author>Antje S Meyer</author>
<author>Peter Praamstra</author>
<author>Falk Huettig</author>
</authors>
<date>2013</date>
<journal>Neuropsychologia. Neuropsychologia,</journal>
<volume>51</volume>
<issue>3</issue>
<contexts>
<context position="2241" citStr="Rommers et al., 2013" startWordPosition="341" endWordPosition="344">on the number and weight of syntactic relations to the current word (Blache et al., 2006); (Blache, 2013). These approaches are based on the classical idea that language processing is incremental and occurs word by word. There are however several experimental evidences showing that a higher level of processing is used by human subjects. Eyetracking data show for example that fixations are done by chunks, not by words (Rauzy and Blache, 2012). Similarly, EEG experiments have shown that processing multiword expressions (for example idioms) relies on global mechanisms (Vespignani et al., 2010); (Rommers et al., 2013). Starting from the question of complexity and its estimation, I will address in this presentation the problem of language processing and its organization. I propose more precisely, using computational complexity models, to define a cohesion index between words. Such an index makes it possible to define chunks (or more generally units) that are built directly, by aggregation, instead of syntactic analysis. In this hypothesis, parsing consists in two different processes: aggregation and integration. Acknowledgments This work, carried out within the Labex BLRI (ANR-11-LABX-0036), has benefited f</context>
</contexts>
<marker>Rommers, Meyer, Praamstra, Huettig, 2013</marker>
<rawString>Joost Rommers, Antje S Meyer, Peter Praamstra, and Falk Huettig. 2013. Neuropsychologia. Neuropsychologia, 51(3):437–447, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shravan Vasishth</author>
</authors>
<title>Quantifying processing difficulty in human sentence parsing: The role of decay, activation, and similarity-based interference.</title>
<date>2003</date>
<booktitle>In Proceedings of the European Cognitive Science Conference</booktitle>
<contexts>
<context position="857" citStr="Vasishth, 2003" startWordPosition="123" endWordPosition="124">plexity and the cognitive load related to the different linguistic phenomena is a key issue for the understanding of language processing. Many studies have focused on the identification of specific parameters that can lead to a simplification or on the contrary to a complexification of the processing (e.g. the different difficulty models proposed in (Gibson, 2000), (Warren and Gibson, 2002), (Hawkins, 2001) ). Similarly, different simplification factors can be identified, such as the notion of activation, relying on syntactic priming effects making it possible to predict (or activate) a word (Vasishth, 2003). Several studies have shown that complexity factors are cumulative (Keller, 2005), but can be offset by simplification (Blache et al., 2006). It is therefore necessary to adopt a global point of view of language processing, explaining the interplay between positive and negative cumulativity, in other words compensation effects. From the computational point of view, some models can account more or less explicitly for these phenomena. This is the case of the Surprisal index (Hale, 2001), offering for each word an assessment of its integration costs into the syntactic structure. This evaluation </context>
</contexts>
<marker>Vasishth, 2003</marker>
<rawString>Shravan Vasishth. 2003. Quantifying processing difficulty in human sentence parsing: The role of decay, activation, and similarity-based interference. In Proceedings of the European Cognitive Science Conference 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francesco Vespignani</author>
<author>Paolo Canal</author>
<author>Nicola Molinaro</author>
<author>Sergio Fonda</author>
<author>Cristina Cacciari</author>
</authors>
<title>Predictive mechanisms in idiom comprehension.</title>
<date>2010</date>
<journal>Journal of Cognitive Neuroscience,</journal>
<volume>22</volume>
<issue>8</issue>
<contexts>
<context position="2217" citStr="Vespignani et al., 2010" startWordPosition="336" endWordPosition="340">tivation degree, depending on the number and weight of syntactic relations to the current word (Blache et al., 2006); (Blache, 2013). These approaches are based on the classical idea that language processing is incremental and occurs word by word. There are however several experimental evidences showing that a higher level of processing is used by human subjects. Eyetracking data show for example that fixations are done by chunks, not by words (Rauzy and Blache, 2012). Similarly, EEG experiments have shown that processing multiword expressions (for example idioms) relies on global mechanisms (Vespignani et al., 2010); (Rommers et al., 2013). Starting from the question of complexity and its estimation, I will address in this presentation the problem of language processing and its organization. I propose more precisely, using computational complexity models, to define a cohesion index between words. Such an index makes it possible to define chunks (or more generally units) that are built directly, by aggregation, instead of syntactic analysis. In this hypothesis, parsing consists in two different processes: aggregation and integration. Acknowledgments This work, carried out within the Labex BLRI (ANR-11-LAB</context>
</contexts>
<marker>Vespignani, Canal, Molinaro, Fonda, Cacciari, 2010</marker>
<rawString>Francesco Vespignani, Paolo Canal, Nicola Molinaro, Sergio Fonda, and Cristina Cacciari. 2010. Predictive mechanisms in idiom comprehension. Journal of Cognitive Neuroscience, 22(8):1682–1700.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tessa Warren</author>
<author>Ted Gibson</author>
</authors>
<title>The influence of referential processing on sentence complexity.</title>
<date>2002</date>
<journal>Cognition,</journal>
<pages>85--79</pages>
<contexts>
<context position="635" citStr="Warren and Gibson, 2002" startWordPosition="87" endWordPosition="90"> incrementality in human language processing: two operations for a cognitive architecture Philippe Blache Aix-Marseille Universit´e &amp; CNRS LPL (UMR7309), 13100, Aix-en-Provence, France blache@blri.fr The description of language complexity and the cognitive load related to the different linguistic phenomena is a key issue for the understanding of language processing. Many studies have focused on the identification of specific parameters that can lead to a simplification or on the contrary to a complexification of the processing (e.g. the different difficulty models proposed in (Gibson, 2000), (Warren and Gibson, 2002), (Hawkins, 2001) ). Similarly, different simplification factors can be identified, such as the notion of activation, relying on syntactic priming effects making it possible to predict (or activate) a word (Vasishth, 2003). Several studies have shown that complexity factors are cumulative (Keller, 2005), but can be offset by simplification (Blache et al., 2006). It is therefore necessary to adopt a global point of view of language processing, explaining the interplay between positive and negative cumulativity, in other words compensation effects. From the computational point of view, some mode</context>
</contexts>
<marker>Warren, Gibson, 2002</marker>
<rawString>Tessa Warren and Ted Gibson. 2002. The influence of referential processing on sentence complexity. Cognition, 85:79–112.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
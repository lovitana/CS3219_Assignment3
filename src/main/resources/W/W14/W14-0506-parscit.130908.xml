<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.185292">
<title confidence="0.9966025">
Distributional Learning as a Theory of Language Acquisition
(Extended Abstract)
</title>
<author confidence="0.986305">
Alexander Clark
</author>
<affiliation confidence="0.8067695">
Department of Philosophy
King’s College, London
</affiliation>
<address confidence="0.619458">
Strand, London
</address>
<email confidence="0.998012">
alexander.clark@kcl.ac.uk
</email>
<sectionHeader confidence="0.996953" genericHeader="abstract">
1 Abstract
</sectionHeader>
<bodyText confidence="0.999912862068965">
In recent years, a theory of distributional learning
of phrase structure grammars has been developed
starting with the simple algorithm presented in
(Clark and Eyraud, 2007). These ideas are based
on the classic ideas of American structuralist lin-
guistics (Wells, 1947; Harris, 1954). Since that
initial paper, the algorithms have been extended to
large classes of grammars, notably to the class of
Multiple Context-Free grammars by (Yoshinaka,
2011).
In this talk we will sketch a theory of language
acquisition based on these techniques, and con-
trast it with other proposals, such as the semantic
bootstrapping and parameter setting models. This
proposal is based on three recent results: first, a
weak learning result for a class of languages that
plausibly includes all natural languages (Clark and
Yoshinaka, 2013), secondly, a strong learning re-
sult for some context-free grammars, that includes
a general strategy for converting weak learners to
strong learners (Clark, 2013a), and finally a theo-
retical result that all minimal grammars for a lan-
guage will have distributionally definable syntac-
tic categories (Clark, 2013b). We argue that we
now have all of the pieces for a complete and ex-
planatory theory of language acquisition based on
distributional learning and sketch some of the non-
trivial predictions of this theory about the syntax
and syntax-semantics interface.
</bodyText>
<sectionHeader confidence="0.984745" genericHeader="acknowledgments">
2 Biography
</sectionHeader>
<bodyText confidence="0.998947866666667">
Alexander Clark is a Lecturer in Logic and Lin-
guistics in the Department of Philosophy at King’s
College London; before that he taught for sev-
eral years in the Computer Science department of
Royal Holloway, University of London. His first
degree was in Mathematics from the University
of Cambridge, and his Ph.D. is from the Univer-
sity of Sussex. He did postdoctoral research at the
University of Geneva. He is currently President
of SIGNLL and chair of the steering committee of
the International Conference on Grammatical In-
ference. His research is on unsupervised learn-
ing in computational linguistics, grammatical in-
ference, and theoretical and mathematical linguis-
tics.
</bodyText>
<sectionHeader confidence="0.991129" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.97208072">
Alexander Clark and R´emi Eyraud. 2007. Polynomial
identification in the limit of substitutable context-
free languages. Journal of Machine Learning Re-
search, 8:1725–1745, August.
Alexander Clark and Ryo Yoshinaka. 2013. Distri-
butional learning of parallel multiple context-free
grammars. Machine Learning, pages 1–27.
Alexander Clark. 2013a. Learning trees from strings:
A strong learning algorithm for some context-free
grammars. Journal of Machine Learning Research,
14:3537–3559.
Alexander Clark. 2013b. The syntactic concept lat-
tice: Another algebraic theory of the context-free
languages? Journal of Logic and Computation.
Zellig Harris. 1954. Distributional structure. Word,
10(2-3):146–62.
R. S. Wells. 1947. Immediate constituents. Language,
23(2):81–117.
R. Yoshinaka. 2011. Efficient learning of multiple
context-free languages with multidimensional sub-
stitutability from positive data. Theoretical Com-
puter Science, 412(19):1821 – 1831.
29
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, page 29,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.039847">
<title confidence="0.883357">Distributional Learning as a Theory of Language</title>
<note confidence="0.991838">(Extended Abstract)</note>
<author confidence="0.954565">Alexander</author>
<affiliation confidence="0.990858">Department of King’s College,</affiliation>
<address confidence="0.72063">Strand,</address>
<email confidence="0.839557">alexander.clark@kcl.ac.uk</email>
<abstract confidence="0.923944746478873">1 Abstract In recent years, a theory of distributional learning of phrase structure grammars has been developed starting with the simple algorithm presented in (Clark and Eyraud, 2007). These ideas are based on the classic ideas of American structuralist linguistics (Wells, 1947; Harris, 1954). Since that initial paper, the algorithms have been extended to large classes of grammars, notably to the class of Multiple Context-Free grammars by (Yoshinaka, 2011). In this talk we will sketch a theory of language acquisition based on these techniques, and contrast it with other proposals, such as the semantic bootstrapping and parameter setting models. This proposal is based on three recent results: first, a weak learning result for a class of languages that plausibly includes all natural languages (Clark and Yoshinaka, 2013), secondly, a strong learning result for some context-free grammars, that includes a general strategy for converting weak learners to strong learners (Clark, 2013a), and finally a theoretical result that all minimal grammars for a language will have distributionally definable syntactic categories (Clark, 2013b). We argue that we now have all of the pieces for a complete and explanatory theory of language acquisition based on distributional learning and sketch some of the nontrivial predictions of this theory about the syntax and syntax-semantics interface. 2 Biography Alexander Clark is a Lecturer in Logic and Linguistics in the Department of Philosophy at King’s College London; before that he taught for several years in the Computer Science department of Royal Holloway, University of London. His first degree was in Mathematics from the University of Cambridge, and his Ph.D. is from the University of Sussex. He did postdoctoral research at the University of Geneva. He is currently President of SIGNLL and chair of the steering committee of the International Conference on Grammatical Inference. His research is on unsupervised learning in computational linguistics, grammatical inference, and theoretical and mathematical linguistics. References Alexander Clark and R´emi Eyraud. 2007. Polynomial identification in the limit of substitutable contextlanguages. of Machine Learning Re- 8:1725–1745, August. Alexander Clark and Ryo Yoshinaka. 2013. Distributional learning of parallel multiple context-free pages 1–27. Alexander Clark. 2013a. Learning trees from strings: A strong learning algorithm for some context-free of Machine Learning 14:3537–3559. Alexander Clark. 2013b. The syntactic concept lattice: Another algebraic theory of the context-free of Logic and Harris. 1954. Distributional structure. 10(2-3):146–62. S. Wells. 1947. Immediate constituents. 23(2):81–117. R. Yoshinaka. 2011. Efficient learning of multiple context-free languages with multidimensional subfrom positive data. Com- 412(19):1821 – 1831. 29 of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL page</abstract>
<intro confidence="0.686757">Sweden, April 26 2014. Association for Computational Linguistics</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>R´emi Eyraud</author>
</authors>
<title>Polynomial identification in the limit of substitutable contextfree languages.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>8--1725</pages>
<marker>Clark, Eyraud, 2007</marker>
<rawString>Alexander Clark and R´emi Eyraud. 2007. Polynomial identification in the limit of substitutable contextfree languages. Journal of Machine Learning Research, 8:1725–1745, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Ryo Yoshinaka</author>
</authors>
<title>Distributional learning of parallel multiple context-free grammars.</title>
<date>2013</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--27</pages>
<contexts>
<context position="1015" citStr="Clark and Yoshinaka, 2013" startWordPosition="146" endWordPosition="149">sed on the classic ideas of American structuralist linguistics (Wells, 1947; Harris, 1954). Since that initial paper, the algorithms have been extended to large classes of grammars, notably to the class of Multiple Context-Free grammars by (Yoshinaka, 2011). In this talk we will sketch a theory of language acquisition based on these techniques, and contrast it with other proposals, such as the semantic bootstrapping and parameter setting models. This proposal is based on three recent results: first, a weak learning result for a class of languages that plausibly includes all natural languages (Clark and Yoshinaka, 2013), secondly, a strong learning result for some context-free grammars, that includes a general strategy for converting weak learners to strong learners (Clark, 2013a), and finally a theoretical result that all minimal grammars for a language will have distributionally definable syntactic categories (Clark, 2013b). We argue that we now have all of the pieces for a complete and explanatory theory of language acquisition based on distributional learning and sketch some of the nontrivial predictions of this theory about the syntax and syntax-semantics interface. 2 Biography Alexander Clark is a Lect</context>
</contexts>
<marker>Clark, Yoshinaka, 2013</marker>
<rawString>Alexander Clark and Ryo Yoshinaka. 2013. Distributional learning of parallel multiple context-free grammars. Machine Learning, pages 1–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>Learning trees from strings: A strong learning algorithm for some context-free grammars.</title>
<date>2013</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>14--3537</pages>
<contexts>
<context position="1177" citStr="Clark, 2013" startWordPosition="172" endWordPosition="173">ars, notably to the class of Multiple Context-Free grammars by (Yoshinaka, 2011). In this talk we will sketch a theory of language acquisition based on these techniques, and contrast it with other proposals, such as the semantic bootstrapping and parameter setting models. This proposal is based on three recent results: first, a weak learning result for a class of languages that plausibly includes all natural languages (Clark and Yoshinaka, 2013), secondly, a strong learning result for some context-free grammars, that includes a general strategy for converting weak learners to strong learners (Clark, 2013a), and finally a theoretical result that all minimal grammars for a language will have distributionally definable syntactic categories (Clark, 2013b). We argue that we now have all of the pieces for a complete and explanatory theory of language acquisition based on distributional learning and sketch some of the nontrivial predictions of this theory about the syntax and syntax-semantics interface. 2 Biography Alexander Clark is a Lecturer in Logic and Linguistics in the Department of Philosophy at King’s College London; before that he taught for several years in the Computer Science department</context>
</contexts>
<marker>Clark, 2013</marker>
<rawString>Alexander Clark. 2013a. Learning trees from strings: A strong learning algorithm for some context-free grammars. Journal of Machine Learning Research, 14:3537–3559.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>The syntactic concept lattice: Another algebraic theory of the context-free languages?</title>
<date>2013</date>
<journal>Journal of Logic and Computation.</journal>
<contexts>
<context position="1177" citStr="Clark, 2013" startWordPosition="172" endWordPosition="173">ars, notably to the class of Multiple Context-Free grammars by (Yoshinaka, 2011). In this talk we will sketch a theory of language acquisition based on these techniques, and contrast it with other proposals, such as the semantic bootstrapping and parameter setting models. This proposal is based on three recent results: first, a weak learning result for a class of languages that plausibly includes all natural languages (Clark and Yoshinaka, 2013), secondly, a strong learning result for some context-free grammars, that includes a general strategy for converting weak learners to strong learners (Clark, 2013a), and finally a theoretical result that all minimal grammars for a language will have distributionally definable syntactic categories (Clark, 2013b). We argue that we now have all of the pieces for a complete and explanatory theory of language acquisition based on distributional learning and sketch some of the nontrivial predictions of this theory about the syntax and syntax-semantics interface. 2 Biography Alexander Clark is a Lecturer in Logic and Linguistics in the Department of Philosophy at King’s College London; before that he taught for several years in the Computer Science department</context>
</contexts>
<marker>Clark, 2013</marker>
<rawString>Alexander Clark. 2013b. The syntactic concept lattice: Another algebraic theory of the context-free languages? Journal of Logic and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--2</pages>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(2-3):146–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Wells</author>
</authors>
<title>Immediate constituents.</title>
<date>1947</date>
<journal>Language,</journal>
<volume>23</volume>
<issue>2</issue>
<marker>Wells, 1947</marker>
<rawString>R. S. Wells. 1947. Immediate constituents. Language, 23(2):81–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Yoshinaka</author>
</authors>
<title>Efficient learning of multiple context-free languages with multidimensional substitutability from positive data.</title>
<date>2011</date>
<journal>Theoretical Computer Science,</journal>
<volume>412</volume>
<issue>19</issue>
<pages>1831</pages>
<contexts>
<context position="646" citStr="Yoshinaka, 2011" startWordPosition="89" endWordPosition="90">heory of Language Acquisition (Extended Abstract) Alexander Clark Department of Philosophy King’s College, London Strand, London alexander.clark@kcl.ac.uk 1 Abstract In recent years, a theory of distributional learning of phrase structure grammars has been developed starting with the simple algorithm presented in (Clark and Eyraud, 2007). These ideas are based on the classic ideas of American structuralist linguistics (Wells, 1947; Harris, 1954). Since that initial paper, the algorithms have been extended to large classes of grammars, notably to the class of Multiple Context-Free grammars by (Yoshinaka, 2011). In this talk we will sketch a theory of language acquisition based on these techniques, and contrast it with other proposals, such as the semantic bootstrapping and parameter setting models. This proposal is based on three recent results: first, a weak learning result for a class of languages that plausibly includes all natural languages (Clark and Yoshinaka, 2013), secondly, a strong learning result for some context-free grammars, that includes a general strategy for converting weak learners to strong learners (Clark, 2013a), and finally a theoretical result that all minimal grammars for a </context>
</contexts>
<marker>Yoshinaka, 2011</marker>
<rawString>R. Yoshinaka. 2011. Efficient learning of multiple context-free languages with multidimensional substitutability from positive data. Theoretical Computer Science, 412(19):1821 – 1831.</rawString>
</citation>
<citation valid="true">
<date>2014</date>
<booktitle>Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014,</booktitle>
<pages>29</pages>
<location>Gothenburg, Sweden,</location>
<marker>2014</marker>
<rawString>Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, page 29, Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
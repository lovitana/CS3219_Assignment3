<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.068571">
<title confidence="0.997883">
Towards Domain-Independent Assessment of Elementary
Students’ Science Competency using Soft Cardinality
</title>
<note confidence="0.70641">
Samuel P. Leeman-Munk, Angela Shelton, Eric N. Wiebe, James C. Lester
North Carolina State University
Raleigh, North Carolina 27695
{ spleeman, anshelto, wiebe, lester } @ ncsu.edu
</note>
<sectionHeader confidence="0.974741" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998571652173913">
Automated assessment of student learning
has become the subject of increasing atten-
tion. Students’ textual responses to short
answer questions offer a rich source of data
for assessment. However, automatically
analyzing textual constructed responses
poses significant computational challenges,
exacerbated by the disfluencies that occur
prominently in elementary students’ writ-
ing. With robust text analytics, there is the
potential to analyze a student’s text re-
sponses and accurately predict his or her
future success. In this paper, we propose
applying soft cardinality, a technique that
has shown success grading less disfluent
student answers, on a corpus of fourth-
grade responses to constructed response
questions. Based on decomposition of
words into their constituent character sub-
strings, soft cardinality’s evaluations of re-
sponses written by fourth graders correlates
with summative analyses of their content
knowledge.
</bodyText>
<sectionHeader confidence="0.998996" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999874545454545">
As a tool for automated assessment, short answer
questions reveal cognitive processes and states in
students that are difficult to uncover in multiple-
choice equivalents (Nicol, 2007). Even when it
seems that items could be designed to address the
same cognitive construct, success in devising
multiple-choice and short answer items that be-
have with psychometric equivalence has proven
to be limited (Kuechler &amp; Simkin, 2010). Be-
cause standards-based STEM education in the
United States explicitly promotes the develop-
ment of writing skills for which constructed re-
sponse items are ideally suited (NGSS Lead
States, 2013; Porter, McMaken, Hwang, &amp; Yang,
2011; Southavilay, Yacef, Reimann, &amp; Calvo,
2013), the prospect of designing text analytics
techniques for automatically assessing students’
textual responses has become even more appeal-
ing (Graesser, 2000; Jordan &amp; Butcher, 2013;
Labeke, Whitelock, &amp; Field, 2013).
An important family of short answer questions
is the constructed response question. A con-
structed response question is designed to elicit a
response of no more than a few sentences and
features a relatively clear distinction between
incorrect, partially correct, and correct answers.
Ideally, a system designed for constructed re-
sponse analysis (CRA) would be machine-
learned from examples that include both graded
student answers and expert-constructed “refer-
ence” answers (Dzikovska, Nielsen, &amp; Brew,
2012).
The challenges of creating an accurate ma-
chine-learning-based CRA system stem from the
variety of ways in which a student can express a
given concept. In addition to lexical and syntac-
tic variety, students often compose ill-formed
text replete with ungrammatical phrasings and
misspellings, which significantly complicate
analysis. The task of automated grading also be-
comes increasingly difficult as the material grad-
ed comes from questions and domains more and
more distant from that of human graded respons-
es on which the system is trained, leading to in-
terest in domain-independent CRA systems de-
signed to deal with this challenge (Dzikovska et
al., 2013).
In this paper we explore the applications of soft
cardinality (Jimenez, Becerra, &amp; Gelbukh, 2013),
an approach to constructed response analysis that
has shown prior success in domain-independent
CRA. We investigate whether soft cardinality is
robust to the disfluency common among elemen-
tary students and whether its analyses of a stu-
dent’s work as she progresses through a prob-
lem-solving session can be used to roughly pre-
dict the content knowledge she will have at the
end.
Because like other bag of words techniques,
soft cardinality is independent of word order, it is
robust to grammatical disfluencies. What distin-
guishes soft cardinality, however, is its character-
overlap technique, which allows it to evaluate
word similarity across misspellings. We evaluate
soft cardinality on a dataset of textual responses
to short-text science questions collected in a
</bodyText>
<page confidence="0.989318">
61
</page>
<bodyText confidence="0.938814619047619">
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 61–67,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
study conducted at elementary schools in two
states. Responders were in fourth grade and gen-
erally aged between nine and ten. We train our
system on student responses to circuits questions
and test it on two domains in the physical scienc-
es—circuits and magnetism. The results indicate
that, soft cardinality shows promise as a first step
for predicting a student’s future success with
similar content even grading unseen domains in
the presence of high disfluency.
This paper is structured as follows. Section 2
provides related work as a context for our re-
search. Section 3 introduces the corpus, collected
on tablet-based digital science notebook software
from elementary students. Section 4 describes
soft cardinality and an evaluation thereof. Sec-
tion 6 discusses the findings and explores how
soft cardinality may serve as the basis for future
approaches to real-time formative assessment.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999974370370371">
Short answer assessment is a much-studied area
that has received increased attention in recent
years. Disfluency and domain-independence
have been the beneficiaries of some of this atten-
tion, but cutting edge systems seem to be de-
signed first for correctly spelled in-domain text,
and then have domain-independence and disflu-
ency management added afterwards.
For example, one system from Educational
Testing Services (ETS) uses an approach to do-
main independence called “domain adaptation”
(Heilman &amp; Madnani, 2013). Domain adaptation
generates a copy of a given feature for grading
answers to seen questions, answers to unseen
questions in seen domain, and answers to ques-
tions in unseen domains, and each of these has a
separate weight. An item represented in the train-
ing data uses all three of these feature copies, and
an item from another domain will only use the
latter, “generic” feature copy.
Spell correction is also often treated as a sepa-
rate issue, handled in the data-cleaning step of a
CRA system. The common approach at this step
is to mark words as misspelled if they do not ap-
pear in a dictionary and replace them with their
most likely alternative. This technique only cor-
rects non-word spelling errors (Leacock &amp;
Chodorow, 2003). Another approach is to use
Soundex hashes that translate every word into a
normalized form based on its pronunciation (Ott,
Ziai, Hahn, &amp; Meurers, 2013). This second ap-
proach is generally featured alongside a more
traditional direct comparison.
The primary limitation of CRA for elementary
school education is that evaluations of state-of-
the-art systems on raw elementary student re-
sponse data are limited. C-rater provides a small
evaluation on fourth-grade student math respons-
es, but most evaluation is on seventh, eighth and
eleventh grade students (Leacock &amp; Chodorow,
2003; Sukkarieh &amp; Blackmore, 2009). Further-
more, the two datasets presented in SemEval’s
shared task (Dzikovska et al., 2013) for testing
and training featured relatively few spelling er-
rors. The BEETLE corpus was drawn from under-
graduate volunteers with a relatively strong
command of the English language, and the Sci-
EntsBank corpus, which was drawn from 3-6th
graders, was originally intended for speech and
as such was manually spell-corrected. The
Hewlett Foundation’s automated student assess-
ment prize (ASAP) shared task for short answer
scoring was drawn entirely from tenth grade stu-
dents (Hewlett, 2012).
</bodyText>
<sectionHeader confidence="0.996476" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.999979322580645">
We have been exploring constructed response
assessment in the context of science education
for upper elementary students with the LEONAR-
DO CYBERPAD (Leeman-Munk, Wiebe, &amp; Lester,
2014). Under development in our laboratory for
three years, the CYBERPAD is a digital science
notebook that runs on tablet and web based com-
puting platforms. The CYBERPAD integrates in-
telligent tutoring systems technologies into a dig-
ital science notebook that enables students to
model science phenomena graphically. With a
focus on the physical and earth sciences, the LE-
ONARDO PADMATE, a pedagogical agent, sup-
ports students’ learning with real-time problem-
solving advice. The CYBERPAD’s curriculum is
based on that of the Full Option Science System
(Foss Project, 2013). As students progress
through the curriculum, they utilize LEONARDO’s
virtual notebook, complete virtual labs, and write
responses to constructed response questions. To
date, the LEONARDO CYBERPAD has been im-
plemented in over 60 classrooms around the
United States.
The short answer and pre/post-test data used in
this investigation were gathered from fourth
grade students during implementations of The
CYBERPAD in public schools in California and
North Carolina. The data collection for each
class took place over a minimum of five class
periods with students completing one or more
new investigations each day. Students completed
</bodyText>
<page confidence="0.995698">
62
</page>
<bodyText confidence="0.999725888888889">
investigations in one or both of two modules,
“Energy and Circuits,” and “Magnetism.” Most
questions included “starter text” that students
were expected to complete. Students were able to
modify the starter text in any way including de-
leting or replacing it entirely, although most stu-
dents simply added to the starter text. Example
answers can be found in a previous work on the
same dataset (Leeman-Munk et al., 2014).
Two human graders scored students’ responses
from the circuits module on a science score ru-
bric with three categories: incorrect, partially
correct, and correct. The graders graded one
class of data and then conferred on disagreeing
results. They then graded other classes. On a
sample of 10% of the responses of the classes
graded after conferring, graders achieved a Co-
hen’s Kappa of 0.72.
The graders dealt with considerable disfluency
in the student responses in the LEONARDO cor-
pus. An analysis of constructed responses in the
Energy and Circuits module reveals that 4.7% of
tokens in all of student answers combined are not
found in a dictionary. This number is higher in
the Magnetism module, 7.8%. This is in contrast
to other similar datasets, such as the BEETLE
corpus of undergraduate text answers to science
questions, which features a 0.8% rate of out-of-
dictionary words (Dzikovska, Nielsen, &amp; Brew,
2012). In each case, the numbers underestimate
overall spelling errors. Misspellings such as ‘bat-
ter’ for ‘battery’, are not counted as missing in a
dictionary test. These real-word spelling errors
nevertheless misrepresent a student’s meaning
and complicate analysis. We describe how soft
cardinality addresses these issues in Section 4.
</bodyText>
<sectionHeader confidence="0.995969" genericHeader="method">
4 Methodology and Evaluation
</sectionHeader>
<bodyText confidence="0.999497464788732">
Soft cardinality (Jimenez, Becerra, &amp; Gelbukh,
2013) uses decompositions of words into charac-
ter sequences, known as q-grams, to gauge simi-
larity between two words. We use it here to
bridge the gap between misspellings of the same
word. Considering “dcells” in an example an-
swer, “mor dcells,” and “D-cells” in the refer-
ence answer, we can find overlaps in “ce,” “el,”
“ll,” “ls,” “ell,” “lls,” and so on up to and includ-
ing “cells.” This technique functions equally well
for real-word spelling errors such as if the stu-
dent had forgotten the “d” and typed only
“cells.” Such overlaps signify a close match for
both of these words. We evaluated the soft cardi-
nality implementation of a generic short answer
grading framework that we developed,
WRITEEVAL, based on an answer grading system
described in an earlier work (Leeman-Munk et
al., 2014). We used 100-fold cross-validation on
the “Energy and Circuits” module. We compare
WRITEEVAL using soft cardinality to the majority
class baseline and to WRITEEVAL using Prece-
dent Feature Collection (PFC), a latent semantic
analysis technique that performs competitively
with the second highest-scoring system in
Semeval Task 7 on unseen answers on the Sci-
EntsBank corpus (Dzikovska et al., 2013). Using
a Kruskal-Wallis test over one hundred folds,
both systems significantly outperform the base-
line (p&lt;.001), which achieved an accuracy score
of .61. We could not evaluate the scores directly
on the Magnetism dataset as we did not have any
human-graded gold standard for comparison.
To evaluate soft cardinality’s robustness to dis-
fluency, we created a duplicate of the Energy and
Circuits dataset and manually spell-corrected it.
Table 1 and Figures 1 and 2 show our results.
Using the Kruskal-Wallis Test, on the uncorrect-
ed data PFC’s accuracy suffered with marginal
significance (p = .054) while macro-averaged
precision and recall both suffered significantly (p
&lt; .01). Soft cardinality suffered much less, with a
marginally significant decrease in performance
(p=.075) only in recall. The decreases in accura-
cy and precision had p=.88 and p=.25 respective-
ly.
To determine the usefulness of automatic grad-
ing of science content in predicting the overall
trajectory of a student’s performance, we com-
puted a running average of the grades given by
soft cardinality (converted to ‘1’, ‘2’, and ‘3’ for
incorrect, partially correct, correct) on students’
answers as they progressed through the Energy
and Circuits module and the Magnetism module.
Because we would intend to be able to use this
technique in a classroom on entirely new ques-
tions and student answers, we use running aver-
age instead of a regression, which would require
prior data on the questions to determine the
weights.
Students completed a multiple-choice test be-
fore and after their interaction with the CYBER-
PAD. The Energy and Circuits module and the
Magnetism module each had different tests –
there were ten questions on the Energy and Cir-
cuits test and twenty on the Magnetism test. We
calculated the correlation of our running average
of formative assessments against the student’s
score on the final test.
A critical assumption underlying the running
average is that students answered each question
</bodyText>
<page confidence="0.997539">
63
</page>
<bodyText confidence="0.9996958125">
in order. Although WRITEEVAL does not prevent
students from answering questions out of order,
it is organized to strongly encourage linear pro-
gression.
We excluded empty responses from the running
average because we did not want an artificial
boost from simply noting what questions stu-
dents did and did not answer. Data from students
who did not take the pre or post-test was exclud-
ed, and students missing responses to more than
twenty out of twenty-nine questions in Mag-
netism or fifteen out of twenty questions in En-
ergy and Circuits were excluded from considera-
tion. After cleaning, our results include 85 stu-
dents in Energy and Circuits and 61 in Mag-
netism.
</bodyText>
<table confidence="0.999373">
Sp.Cr. System Accuracy Precision Recall
Yes SoftCr .68 .55 .54
No SoftCr .68 .52 .50*
Yes PFC .78 .61 .58
No PFC .74* .54** .52**
</table>
<tableCaption confidence="0.996551">
Table 1. Accuracy and Macro-Averaged Preci-
</tableCaption>
<bodyText confidence="0.994871285714286">
sion and Recall for Soft-Cardinality and PFC on
spell-corrected and uncorrected versions of the
LEONARDO Energy and Circuits module.
*marginally significant decrease from spell-
checked
**significant decrease from spell-checked
Figure 1 depicts the correlation between the
running average of automatic scoring by
WRITEEVAL soft cardinality, PFC, and human
scores with post-test score on the responses in
the Energy and Circuits module. When spell-
corrected, the correlation, as shown in Figure 2,
surprisingly becomes worse. We discuss a pos-
sible reason for this in the discussion section.
Figure 3 shows correlation of the running aver-
age of Magnetism’s automatic scores with post-
test. For soft cardinality, significant correlation
starts five questions in and stays for the rest of
the 29. As it relies heavily on relevant training
data, PFC is less stable and does not achieve
nearly as high a correlation.
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999763571428571">
The evaluation suggests that a relatively simple
technique such as soft cardinality, despite per-
forming less well than a domain specific tech-
nique in the presence of relevant training data, is
more robust to spelling errors and can be far
more effective at grading questions and domains
not present in the training data.
</bodyText>
<figureCaption confidence="0.995679230769231">
Figure 1. Correlation of grading systems on
Energy and Circuits with post-test score. Dark-
colored points indicate significant correlation
(p&lt;.05)
Figure 2. Correlation of grading systems on
spell-corrected Energy and Circuits with post-
test score. Dark-colored points indicate
significant correlation (p&lt;.05)
Figure 3. Correlation of the Running Average of
WRITEEVAL with soft cardinality with post-test
Scores on the Magnetism module of the LEO-
NARDO corpus. Dark-colored points indicate
significant correlation (p&lt;.05)
</figureCaption>
<bodyText confidence="0.998758714285714">
Soft cardinality is representative of the poten-
tial of domain independent, disfluency-robust
CRA systems.
The improvement against the gold standard on
spell-corrected data but loss of correlation
against the post-test scores suggests that poor
spelling is a predictor of poor post-test
</bodyText>
<figure confidence="0.997855230769231">
Correlation
0.6
0.4
0.2
0
1 3 5 7 9 11 13 15 17 19
Questions Graded
Human Soft Cardinality
WriteEval PFC
Correlation
-0.2
0.6
0.4
0.2
0
Questions Graded
Human Soft Cardinality
WriteEval PFC
1 3 5 7 9 11 13 15 17 19
Correlation
0.5
0.3
0.1
5 7 9 11 13 15 17 19 21 23 25 27 29
Questions Graded
Soft Cardinality WriteEval PFC
</figure>
<page confidence="0.996982">
64
</page>
<bodyText confidence="0.999588555555556">
knowledge at the end of a task. This could be
because the students were less able to learn the
material due to their poor language skills, they
were less able to complete the test effectively
despite knowing the material again due to poor
language skills, or it could be a latent factor that
affects both the students use of language and
their eventual circuits knowledge such as en-
gagement. This result shows the challenge of
separating different skills in evaluating students.
The significance of soft cardinality’s correla-
tion over the running average for all but the
eighth question as well as the generally high sig-
nificant correlation achieved in the magnetism
evaluation indicates the predictive potential of
soft cardinality. Soft cardinality’s performance in
Magnetism suggests that with only a relatively
limited breadth of training examples it can effec-
tively evaluate answers to questions in some un-
seen domains. It is important to note that Energy
and Circuits and Magnetism are both subjects in
the physical sciences, and the questions and ref-
erence answers themselves were authored by the
same individuals. As such this result should not
be overstated, but is still a promising first step
towards the goal of domain-independence in
constructed response analysis.
</bodyText>
<sectionHeader confidence="0.995299" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999973672131148">
This paper presents a novel application of the
soft cardinality text analytics method to support
assessment of highly disfluent elementary school
text. Using q-gram overlap to evaluate word sim-
ilarity across nonstandard spellings, soft cardi-
nality was evaluated on highly disfluent con-
structed response texts composed by fourth grade
students interacting with a tablet-based digital
science notebook. The evaluation included an in-
domain training corpus and another out-of-
domain corpus. The results of the evaluation
suggest that soft cardinality generates assess-
ments that are predictive of students’ post-test
performance even in highly disfluent out-of-
domain corpora. It offers the potential to produce
assessments in real-time that may serve as early
warning indicators to help teachers support stu-
dent learning.
Soft cardinality’s current performance levels
suggest several promising directions for future
work. First, it will be important to develop tech-
niques to deal with widely varying student re-
sponses without relying directly on training data.
These techniques will take inspiration in part
from bag-of-words techniques such as soft cardi-
nality and Precedent Feature Collection, but will
themselves likely take word order into account as
there is a sizeable subset of answers whose
meaning is dependent on word order. The use of
distributional semantics will also be of help in
resolving similarities between different words.
Secondly, work should be done to consider an-
swers in more detail than simple assessment of
correctness. More detailed rubrics such as Task
7’s 5-way rubric (Dzikovska et al., 2013) would
allow for more detailed feedback from tutors.
Further, detailed analysis of individual under-
standings and misconceptions within answers
would be even more helpful, and will be the fo-
cus of future work. Third, it will be instructive to
incorporate the WRITEEVAL framework into the
LEONARDO CYBERPAD digital science notebook
to investigate techniques for classroom-based
formative assessment that artfully utilize both
intelligent support by the PADMATE onboard
intelligent tutor and personalized support by the
teacher. Finally, it will be important to to inves-
tigate additional techniques to evaluate student
answers more accurately using less training data
from more distant domains.
Reliable analysis of constructed response items
not only provides additional summative analysis
of writing ability in science, but also gives the
teacher a powerful formative assessment tool that
can be used to guide instructional strategies at
either the individual student or whole class level.
Given that time for science instruction is limited
at the elementary level, the use of real-time as-
sessment to address student misconceptions or
missing knowledge immediately can be an inval-
uable classroom tool.
</bodyText>
<sectionHeader confidence="0.99831" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.995717166666667">
The authors wish to thank our colleagues on the
LEONARDO team for their contributions to the
design, development, and classroom implementa-
tions of LEONARDO: Courtney Behrle, Mike
Carter, Bradford Mott, Peter Andrew Smith, and
Robert Taylor. This material is based upon work
supported by the National Science Foundation
under Grant No. DRL1020229. Any opinions,
findings, and conclusions or recommendations
expressed in this material are those of the authors
and do not necessarily reflect the views of the
National Science Foundation.
</bodyText>
<page confidence="0.999482">
65
</page>
<sectionHeader confidence="0.990311" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998030267326733">
Dzikovska, M., Brew, C., Clark, P., Nielsen, R. D.,
Leacock, C., Mcgraw-hill, C. T. B., &amp;
Bentivogli, L. (2013). SemEval-2013 Task 7 :
The joint student response analysis and 8th
recognizing textual entailment challenge. In
Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval
2013) (Vol. 2, pp. 263–274).
Dzikovska, M., Nielsen, R., &amp; Brew, C. (2012).
Towards effective tutorial feedback for
explanation questions: A dataset and baselines.
In Proceedings of the 2012 Conference of the
North American Chapter of the Association for
Computational Linguistics: Human Language
Technologies (pp. 200–210). Montreal, Canada.
Retrieved from
http://dl.acm.org/citation.cfm?id=2382057
Foss Project. (2013). Welcome to FossWeb. Retrieved
October 20, 2013, from
http://www.fossweb.com/
Graesser, A. (2000). Using latent semantic analysis to
evaluate the contributions of students in
AutoTutor. Interactive Learning Environments,
8(2), 1–33. Retrieved from
http://www.tandfonline.com/doi/full/10.1076/10
49-4820(200008)8%3A2%3B1-B%3BFT129
Heilman, M., &amp; Madnani, N. (2013). ETS : Domain
adaptation and stacking for short answer
scoring. In Second Joint Conference on Lexical
and Computational Semantics (*SEM), Volume
2: Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval
2013) (Vol. 1, pp. 96–102).
Hewlett, W. (2012). The Hewlett Foundation: Short
answer scoring. Retrieved March 16, 2014,
from https://www.kaggle.com/c/asap-
sas/data?Data_Set_Descriptions.zip
Jimenez, S., Becerra, C., &amp; Gelbukh, A. (2013).
SOFTCARDINALITY: hierarchical text
overlap for student response analysis. In Second
Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of
the Seventh International Workshop on
Semantic Evaluation (SemEval 2013) (Vol. 2,
pp. 280–284). Retrieved from
http://www.gelbukh.com/CV/Publications/2013
/SOFTCARDINALITY Hierarchical Text
Overlap for Student Response Analysis.pdf
Jordan, S., &amp; Butcher, P. (2013). Does the Sun orbit
the Earth ? Challenges in using short free-text
computer-marked questions . In Proceedings of
HEA STEM Annual Learning and Teaching
Conference 2013: Where Practice and
Pedagogy Meet. Birmingham, UK.
Kuechler, W., &amp; Simkin, M. (2010). Why is
performance on multiple-choice tests and
constructed-response tests not more closely
related? Theory and an empirical test. Decision
Sciences Journal of Innovative Education, 8(1),
55–73. Retrieved from
http://onlinelibrary.wiley.com/doi/10.1111/j.154
0-4609.2009.00243.x/full
Labeke, N. Van, Whitelock, D., &amp; Field, D. (2013).
OpenEssayist: extractive summarisation and
formative assessment of free-text essays. In
First International Workshop on Discourse-
Centric Learning Analytics. Leuven, Belgium.
Retrieved from http://oro.open.ac.uk/37548/
Leacock, C., &amp; Chodorow, M. (2003). C-rater:
Automated scoring of short-answer questions.
Computers and the Humanities, 37(4), 389–405.
Retrieved from
http://link.springer.com/article/10.1023/A%3A1
025779619903
Leeman-Munk, S. P., Wiebe, E. N., &amp; Lester, J. C.
(2014). Assessing Elementary Students’ Science
Competency with Text Analytics. In
Proceedings of the Fourth International
Conference on Learning Analytics &amp;
Knowledge. Indianapolis, Indiana.
NGSS Lead States. (2013). Next Generation Science
Standards: For States, By States. Washington
DC: National Academic Press.
Nicol, D. (2007). E-assessment by design: Using
multiple-choice tests to good effect. Journal of
Further and Higher Education, 31(1), 53–64.
Retrieved from
http://www.tandfonline.com/doi/abs/10.1080/03
098770601167922
Ott, N., Ziai, R., Hahn, M., &amp; Meurers, D. (2013).
CoMeT : Integrating different levels of
linguistic modeling for meaning assessment. In
Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2:
Proceedings of the Seventh International
Workshop on Semantic Evaluation (SemEval
2013) (Vol. 2, pp. 608–616).
Porter, A., McMaken, J., Hwang, J., &amp; Yang, R.
(2011). Common core standards the new US
</reference>
<page confidence="0.854012">
66
</page>
<reference confidence="0.997904444444444">
intended curriculum. Educational Researcher,
40(3), 103–116. Retrieved from
http://edr.sagepub.com/content/40/3/103.short
Southavilay, V., Yacef, K., Reimann, P., &amp; Calvo, R.
A. (2013). Analysis of collaborative writing
processes using revision maps and probabilistic
topic models. In Proceedings of the Third
International Conference on Learning Analytics
and Knowledge - LAK ’13 (pp. 38–47). New
York, New York, USA: ACM Press.
doi:10.1145/2460296.2460307
Sukkarieh, J., &amp; Blackmore, J. (2009). C-rater:
Automatic content scoring for short constructed
responses. Proceedings of the 22nd
International FLAIRS Conference, 290–295.
Retrieved from
http://www.aaai.org/ocs/index.php/FLAIRS/200
9/paper/download/122/302
</reference>
<page confidence="0.999425">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.570193">
<title confidence="0.941233">Assessment of Elementary Students’ Science Competency using Soft Cardinality</title>
<author confidence="0.8751535">Samuel P Leeman-Munk</author>
<author confidence="0.8751535">Angela Shelton</author>
<author confidence="0.8751535">Eric N Wiebe</author>
<author confidence="0.8751535">James C Lester North Carolina State</author>
<address confidence="0.898356">Raleigh, North Carolina 27695</address>
<email confidence="0.989859">spleeman@ncsu.edu</email>
<email confidence="0.989859">anshelto@ncsu.edu</email>
<email confidence="0.989859">wiebe@ncsu.edu</email>
<email confidence="0.989859">lester@ncsu.edu</email>
<abstract confidence="0.998006916666667">Automated assessment of student learning has become the subject of increasing atten- Students’ textual responses short answer questions offer a rich source of data for assessment. However, automatically analyzing textual constructed responses poses significant computational challenges, exacerbated by the disfluencies that occur in elementary students’ writing. With robust text analytics, there is the to a student’s responses and accurately predict his or her future success. In this paper, we propose applying soft cardinality, a technique that has shown success grading less disfluent student answers, on a corpus of fourthgrade responses to constructed response questions. Based on decomposition of words into their constituent character subsoft evaluations of sponses written by fourth graders correlates with summative analyses of their content knowledge.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>M Dzikovska</author>
<author>C Brew</author>
<author>P Clark</author>
<author>R D Nielsen</author>
<author>C Leacock</author>
<author>C T B Mcgraw-hill</author>
<author>L Bentivogli</author>
</authors>
<title>The joint student response analysis and 8th recognizing textual entailment challenge.</title>
<date>2013</date>
<journal>SemEval-2013 Task</journal>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<volume>7</volume>
<pages>263--274</pages>
<contexts>
<context position="3327" citStr="Dzikovska et al., 2013" startWordPosition="485" endWordPosition="488">te machine-learning-based CRA system stem from the variety of ways in which a student can express a given concept. In addition to lexical and syntactic variety, students often compose ill-formed text replete with ungrammatical phrasings and misspellings, which significantly complicate analysis. The task of automated grading also becomes increasingly difficult as the material graded comes from questions and domains more and more distant from that of human graded responses on which the system is trained, leading to interest in domain-independent CRA systems designed to deal with this challenge (Dzikovska et al., 2013). In this paper we explore the applications of soft cardinality (Jimenez, Becerra, &amp; Gelbukh, 2013), an approach to constructed response analysis that has shown prior success in domain-independent CRA. We investigate whether soft cardinality is robust to the disfluency common among elementary students and whether its analyses of a student’s work as she progresses through a problem-solving session can be used to roughly predict the content knowledge she will have at the end. Because like other bag of words techniques, soft cardinality is independent of word order, it is robust to grammatical di</context>
<context position="7259" citStr="Dzikovska et al., 2013" startWordPosition="1102" endWordPosition="1105">ormalized form based on its pronunciation (Ott, Ziai, Hahn, &amp; Meurers, 2013). This second approach is generally featured alongside a more traditional direct comparison. The primary limitation of CRA for elementary school education is that evaluations of state-ofthe-art systems on raw elementary student response data are limited. C-rater provides a small evaluation on fourth-grade student math responses, but most evaluation is on seventh, eighth and eleventh grade students (Leacock &amp; Chodorow, 2003; Sukkarieh &amp; Blackmore, 2009). Furthermore, the two datasets presented in SemEval’s shared task (Dzikovska et al., 2013) for testing and training featured relatively few spelling errors. The BEETLE corpus was drawn from undergraduate volunteers with a relatively strong command of the English language, and the SciEntsBank corpus, which was drawn from 3-6th graders, was originally intended for speech and as such was manually spell-corrected. The Hewlett Foundation’s automated student assessment prize (ASAP) shared task for short answer scoring was drawn entirely from tenth grade students (Hewlett, 2012). 3 Corpus We have been exploring constructed response assessment in the context of science education for upper </context>
<context position="12090" citStr="Dzikovska et al., 2013" startWordPosition="1861" endWordPosition="1864"> these words. We evaluated the soft cardinality implementation of a generic short answer grading framework that we developed, WRITEEVAL, based on an answer grading system described in an earlier work (Leeman-Munk et al., 2014). We used 100-fold cross-validation on the “Energy and Circuits” module. We compare WRITEEVAL using soft cardinality to the majority class baseline and to WRITEEVAL using Precedent Feature Collection (PFC), a latent semantic analysis technique that performs competitively with the second highest-scoring system in Semeval Task 7 on unseen answers on the SciEntsBank corpus (Dzikovska et al., 2013). Using a Kruskal-Wallis test over one hundred folds, both systems significantly outperform the baseline (p&lt;.001), which achieved an accuracy score of .61. We could not evaluate the scores directly on the Magnetism dataset as we did not have any human-graded gold standard for comparison. To evaluate soft cardinality’s robustness to disfluency, we created a duplicate of the Energy and Circuits dataset and manually spell-corrected it. Table 1 and Figures 1 and 2 show our results. Using the Kruskal-Wallis Test, on the uncorrected data PFC’s accuracy suffered with marginal significance (p = .054) </context>
<context position="20192" citStr="Dzikovska et al., 2013" startWordPosition="3147" endWordPosition="3150">student responses without relying directly on training data. These techniques will take inspiration in part from bag-of-words techniques such as soft cardinality and Precedent Feature Collection, but will themselves likely take word order into account as there is a sizeable subset of answers whose meaning is dependent on word order. The use of distributional semantics will also be of help in resolving similarities between different words. Secondly, work should be done to consider answers in more detail than simple assessment of correctness. More detailed rubrics such as Task 7’s 5-way rubric (Dzikovska et al., 2013) would allow for more detailed feedback from tutors. Further, detailed analysis of individual understandings and misconceptions within answers would be even more helpful, and will be the focus of future work. Third, it will be instructive to incorporate the WRITEEVAL framework into the LEONARDO CYBERPAD digital science notebook to investigate techniques for classroom-based formative assessment that artfully utilize both intelligent support by the PADMATE onboard intelligent tutor and personalized support by the teacher. Finally, it will be important to to investigate additional techniques to e</context>
</contexts>
<marker>Dzikovska, Brew, Clark, Nielsen, Leacock, Mcgraw-hill, Bentivogli, 2013</marker>
<rawString>Dzikovska, M., Brew, C., Clark, P., Nielsen, R. D., Leacock, C., Mcgraw-hill, C. T. B., &amp; Bentivogli, L. (2013). SemEval-2013 Task 7 : The joint student response analysis and 8th recognizing textual entailment challenge. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013) (Vol. 2, pp. 263–274).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dzikovska</author>
<author>R Nielsen</author>
<author>C Brew</author>
</authors>
<title>Towards effective tutorial feedback for explanation questions: A dataset and baselines.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</booktitle>
<pages>200--210</pages>
<location>Montreal,</location>
<note>Retrieved from http://dl.acm.org/citation.cfm?id=2382057</note>
<contexts>
<context position="2665" citStr="Dzikovska, Nielsen, &amp; Brew, 2012" startWordPosition="380" endWordPosition="384"> textual responses has become even more appealing (Graesser, 2000; Jordan &amp; Butcher, 2013; Labeke, Whitelock, &amp; Field, 2013). An important family of short answer questions is the constructed response question. A constructed response question is designed to elicit a response of no more than a few sentences and features a relatively clear distinction between incorrect, partially correct, and correct answers. Ideally, a system designed for constructed response analysis (CRA) would be machinelearned from examples that include both graded student answers and expert-constructed “reference” answers (Dzikovska, Nielsen, &amp; Brew, 2012). The challenges of creating an accurate machine-learning-based CRA system stem from the variety of ways in which a student can express a given concept. In addition to lexical and syntactic variety, students often compose ill-formed text replete with ungrammatical phrasings and misspellings, which significantly complicate analysis. The task of automated grading also becomes increasingly difficult as the material graded comes from questions and domains more and more distant from that of human graded responses on which the system is trained, leading to interest in domain-independent CRA systems</context>
<context position="10485" citStr="Dzikovska, Nielsen, &amp; Brew, 2012" startWordPosition="1607" endWordPosition="1611"> of 10% of the responses of the classes graded after conferring, graders achieved a Cohen’s Kappa of 0.72. The graders dealt with considerable disfluency in the student responses in the LEONARDO corpus. An analysis of constructed responses in the Energy and Circuits module reveals that 4.7% of tokens in all of student answers combined are not found in a dictionary. This number is higher in the Magnetism module, 7.8%. This is in contrast to other similar datasets, such as the BEETLE corpus of undergraduate text answers to science questions, which features a 0.8% rate of out-ofdictionary words (Dzikovska, Nielsen, &amp; Brew, 2012). In each case, the numbers underestimate overall spelling errors. Misspellings such as ‘batter’ for ‘battery’, are not counted as missing in a dictionary test. These real-word spelling errors nevertheless misrepresent a student’s meaning and complicate analysis. We describe how soft cardinality addresses these issues in Section 4. 4 Methodology and Evaluation Soft cardinality (Jimenez, Becerra, &amp; Gelbukh, 2013) uses decompositions of words into character sequences, known as q-grams, to gauge similarity between two words. We use it here to bridge the gap between misspellings of the same word.</context>
</contexts>
<marker>Dzikovska, Nielsen, Brew, 2012</marker>
<rawString>Dzikovska, M., Nielsen, R., &amp; Brew, C. (2012). Towards effective tutorial feedback for explanation questions: A dataset and baselines. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 200–210). Montreal, Canada. Retrieved from http://dl.acm.org/citation.cfm?id=2382057</rawString>
</citation>
<citation valid="true">
<authors>
<author>Foss Project</author>
</authors>
<title>Welcome to FossWeb. Retrieved</title>
<date>2013</date>
<note>from http://www.fossweb.com/</note>
<contexts>
<context position="8510" citStr="Project, 2013" startWordPosition="1296" endWordPosition="1297"> CYBERPAD (Leeman-Munk, Wiebe, &amp; Lester, 2014). Under development in our laboratory for three years, the CYBERPAD is a digital science notebook that runs on tablet and web based computing platforms. The CYBERPAD integrates intelligent tutoring systems technologies into a digital science notebook that enables students to model science phenomena graphically. With a focus on the physical and earth sciences, the LEONARDO PADMATE, a pedagogical agent, supports students’ learning with real-time problemsolving advice. The CYBERPAD’s curriculum is based on that of the Full Option Science System (Foss Project, 2013). As students progress through the curriculum, they utilize LEONARDO’s virtual notebook, complete virtual labs, and write responses to constructed response questions. To date, the LEONARDO CYBERPAD has been implemented in over 60 classrooms around the United States. The short answer and pre/post-test data used in this investigation were gathered from fourth grade students during implementations of The CYBERPAD in public schools in California and North Carolina. The data collection for each class took place over a minimum of five class periods with students completing one or more new investigat</context>
</contexts>
<marker>Project, 2013</marker>
<rawString>Foss Project. (2013). Welcome to FossWeb. Retrieved October 20, 2013, from http://www.fossweb.com/</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Graesser</author>
</authors>
<title>Using latent semantic analysis to evaluate the contributions of students in AutoTutor.</title>
<date>2000</date>
<journal>Interactive Learning Environments,</journal>
<volume>8</volume>
<issue>2</issue>
<pages>1--33</pages>
<note>Retrieved from http://www.tandfonline.com/doi/full/10.1076/10</note>
<contexts>
<context position="2098" citStr="Graesser, 2000" startWordPosition="299" endWordPosition="300">he same cognitive construct, success in devising multiple-choice and short answer items that behave with psychometric equivalence has proven to be limited (Kuechler &amp; Simkin, 2010). Because standards-based STEM education in the United States explicitly promotes the development of writing skills for which constructed response items are ideally suited (NGSS Lead States, 2013; Porter, McMaken, Hwang, &amp; Yang, 2011; Southavilay, Yacef, Reimann, &amp; Calvo, 2013), the prospect of designing text analytics techniques for automatically assessing students’ textual responses has become even more appealing (Graesser, 2000; Jordan &amp; Butcher, 2013; Labeke, Whitelock, &amp; Field, 2013). An important family of short answer questions is the constructed response question. A constructed response question is designed to elicit a response of no more than a few sentences and features a relatively clear distinction between incorrect, partially correct, and correct answers. Ideally, a system designed for constructed response analysis (CRA) would be machinelearned from examples that include both graded student answers and expert-constructed “reference” answers (Dzikovska, Nielsen, &amp; Brew, 2012). The challenges of creating an </context>
</contexts>
<marker>Graesser, 2000</marker>
<rawString>Graesser, A. (2000). Using latent semantic analysis to evaluate the contributions of students in AutoTutor. Interactive Learning Environments, 8(2), 1–33. Retrieved from http://www.tandfonline.com/doi/full/10.1076/10 49-4820(200008)8%3A2%3B1-B%3BFT129</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Heilman</author>
<author>N Madnani</author>
</authors>
<title>ETS : Domain adaptation and stacking for short answer scoring.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<volume>1</volume>
<pages>96--102</pages>
<contexts>
<context position="5825" citStr="Heilman &amp; Madnani, 2013" startWordPosition="867" endWordPosition="870">t cardinality may serve as the basis for future approaches to real-time formative assessment. 2 Related Work Short answer assessment is a much-studied area that has received increased attention in recent years. Disfluency and domain-independence have been the beneficiaries of some of this attention, but cutting edge systems seem to be designed first for correctly spelled in-domain text, and then have domain-independence and disfluency management added afterwards. For example, one system from Educational Testing Services (ETS) uses an approach to domain independence called “domain adaptation” (Heilman &amp; Madnani, 2013). Domain adaptation generates a copy of a given feature for grading answers to seen questions, answers to unseen questions in seen domain, and answers to questions in unseen domains, and each of these has a separate weight. An item represented in the training data uses all three of these feature copies, and an item from another domain will only use the latter, “generic” feature copy. Spell correction is also often treated as a separate issue, handled in the data-cleaning step of a CRA system. The common approach at this step is to mark words as misspelled if they do not appear in a dictionary </context>
</contexts>
<marker>Heilman, Madnani, 2013</marker>
<rawString>Heilman, M., &amp; Madnani, N. (2013). ETS : Domain adaptation and stacking for short answer scoring. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013) (Vol. 1, pp. 96–102).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Hewlett</author>
</authors>
<title>The Hewlett Foundation: Short answer scoring. Retrieved</title>
<date>2012</date>
<note>from https://www.kaggle.com/c/asapsas/data?Data_Set_Descriptions.zip</note>
<contexts>
<context position="7747" citStr="Hewlett, 2012" startWordPosition="1179" endWordPosition="1180"> 2003; Sukkarieh &amp; Blackmore, 2009). Furthermore, the two datasets presented in SemEval’s shared task (Dzikovska et al., 2013) for testing and training featured relatively few spelling errors. The BEETLE corpus was drawn from undergraduate volunteers with a relatively strong command of the English language, and the SciEntsBank corpus, which was drawn from 3-6th graders, was originally intended for speech and as such was manually spell-corrected. The Hewlett Foundation’s automated student assessment prize (ASAP) shared task for short answer scoring was drawn entirely from tenth grade students (Hewlett, 2012). 3 Corpus We have been exploring constructed response assessment in the context of science education for upper elementary students with the LEONARDO CYBERPAD (Leeman-Munk, Wiebe, &amp; Lester, 2014). Under development in our laboratory for three years, the CYBERPAD is a digital science notebook that runs on tablet and web based computing platforms. The CYBERPAD integrates intelligent tutoring systems technologies into a digital science notebook that enables students to model science phenomena graphically. With a focus on the physical and earth sciences, the LEONARDO PADMATE, a pedagogical agent, </context>
</contexts>
<marker>Hewlett, 2012</marker>
<rawString>Hewlett, W. (2012). The Hewlett Foundation: Short answer scoring. Retrieved March 16, 2014, from https://www.kaggle.com/c/asapsas/data?Data_Set_Descriptions.zip</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Jimenez</author>
<author>C Becerra</author>
<author>A Gelbukh</author>
</authors>
<title>SOFTCARDINALITY: hierarchical text overlap for student response analysis.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<volume>2</volume>
<pages>280--284</pages>
<contexts>
<context position="3425" citStr="Jimenez, Becerra, &amp; Gelbukh, 2013" startWordPosition="499" endWordPosition="503">n express a given concept. In addition to lexical and syntactic variety, students often compose ill-formed text replete with ungrammatical phrasings and misspellings, which significantly complicate analysis. The task of automated grading also becomes increasingly difficult as the material graded comes from questions and domains more and more distant from that of human graded responses on which the system is trained, leading to interest in domain-independent CRA systems designed to deal with this challenge (Dzikovska et al., 2013). In this paper we explore the applications of soft cardinality (Jimenez, Becerra, &amp; Gelbukh, 2013), an approach to constructed response analysis that has shown prior success in domain-independent CRA. We investigate whether soft cardinality is robust to the disfluency common among elementary students and whether its analyses of a student’s work as she progresses through a problem-solving session can be used to roughly predict the content knowledge she will have at the end. Because like other bag of words techniques, soft cardinality is independent of word order, it is robust to grammatical disfluencies. What distinguishes soft cardinality, however, is its characteroverlap technique, which</context>
<context position="10900" citStr="Jimenez, Becerra, &amp; Gelbukh, 2013" startWordPosition="1666" endWordPosition="1670"> 7.8%. This is in contrast to other similar datasets, such as the BEETLE corpus of undergraduate text answers to science questions, which features a 0.8% rate of out-ofdictionary words (Dzikovska, Nielsen, &amp; Brew, 2012). In each case, the numbers underestimate overall spelling errors. Misspellings such as ‘batter’ for ‘battery’, are not counted as missing in a dictionary test. These real-word spelling errors nevertheless misrepresent a student’s meaning and complicate analysis. We describe how soft cardinality addresses these issues in Section 4. 4 Methodology and Evaluation Soft cardinality (Jimenez, Becerra, &amp; Gelbukh, 2013) uses decompositions of words into character sequences, known as q-grams, to gauge similarity between two words. We use it here to bridge the gap between misspellings of the same word. Considering “dcells” in an example answer, “mor dcells,” and “D-cells” in the reference answer, we can find overlaps in “ce,” “el,” “ll,” “ls,” “ell,” “lls,” and so on up to and including “cells.” This technique functions equally well for real-word spelling errors such as if the student had forgotten the “d” and typed only “cells.” Such overlaps signify a close match for both of these words. We evaluated the so</context>
</contexts>
<marker>Jimenez, Becerra, Gelbukh, 2013</marker>
<rawString>Jimenez, S., Becerra, C., &amp; Gelbukh, A. (2013). SOFTCARDINALITY: hierarchical text overlap for student response analysis. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013) (Vol. 2, pp. 280–284). Retrieved from http://www.gelbukh.com/CV/Publications/2013 /SOFTCARDINALITY Hierarchical Text Overlap for Student Response Analysis.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jordan</author>
<author>P Butcher</author>
</authors>
<title>Does the Sun orbit the Earth ? Challenges in using short free-text computer-marked questions .</title>
<date>2013</date>
<booktitle>In Proceedings of HEA STEM Annual Learning and Teaching Conference 2013: Where Practice and Pedagogy Meet.</booktitle>
<location>Birmingham, UK.</location>
<contexts>
<context position="2122" citStr="Jordan &amp; Butcher, 2013" startWordPosition="301" endWordPosition="304">e construct, success in devising multiple-choice and short answer items that behave with psychometric equivalence has proven to be limited (Kuechler &amp; Simkin, 2010). Because standards-based STEM education in the United States explicitly promotes the development of writing skills for which constructed response items are ideally suited (NGSS Lead States, 2013; Porter, McMaken, Hwang, &amp; Yang, 2011; Southavilay, Yacef, Reimann, &amp; Calvo, 2013), the prospect of designing text analytics techniques for automatically assessing students’ textual responses has become even more appealing (Graesser, 2000; Jordan &amp; Butcher, 2013; Labeke, Whitelock, &amp; Field, 2013). An important family of short answer questions is the constructed response question. A constructed response question is designed to elicit a response of no more than a few sentences and features a relatively clear distinction between incorrect, partially correct, and correct answers. Ideally, a system designed for constructed response analysis (CRA) would be machinelearned from examples that include both graded student answers and expert-constructed “reference” answers (Dzikovska, Nielsen, &amp; Brew, 2012). The challenges of creating an accurate machine-learnin</context>
</contexts>
<marker>Jordan, Butcher, 2013</marker>
<rawString>Jordan, S., &amp; Butcher, P. (2013). Does the Sun orbit the Earth ? Challenges in using short free-text computer-marked questions . In Proceedings of HEA STEM Annual Learning and Teaching Conference 2013: Where Practice and Pedagogy Meet. Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kuechler</author>
<author>M Simkin</author>
</authors>
<title>Why is performance on multiple-choice tests and constructed-response tests not more closely related? Theory and an empirical test.</title>
<date>2010</date>
<journal>Decision Sciences Journal of Innovative Education,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>55--73</pages>
<note>Retrieved from http://onlinelibrary.wiley.com/doi/10.1111/j.154 0-4609.2009.00243.x/full</note>
<contexts>
<context position="1664" citStr="Kuechler &amp; Simkin, 2010" startWordPosition="234" endWordPosition="237"> words into their constituent character substrings, soft cardinality’s evaluations of responses written by fourth graders correlates with summative analyses of their content knowledge. 1 Introduction As a tool for automated assessment, short answer questions reveal cognitive processes and states in students that are difficult to uncover in multiplechoice equivalents (Nicol, 2007). Even when it seems that items could be designed to address the same cognitive construct, success in devising multiple-choice and short answer items that behave with psychometric equivalence has proven to be limited (Kuechler &amp; Simkin, 2010). Because standards-based STEM education in the United States explicitly promotes the development of writing skills for which constructed response items are ideally suited (NGSS Lead States, 2013; Porter, McMaken, Hwang, &amp; Yang, 2011; Southavilay, Yacef, Reimann, &amp; Calvo, 2013), the prospect of designing text analytics techniques for automatically assessing students’ textual responses has become even more appealing (Graesser, 2000; Jordan &amp; Butcher, 2013; Labeke, Whitelock, &amp; Field, 2013). An important family of short answer questions is the constructed response question. A constructed respons</context>
</contexts>
<marker>Kuechler, Simkin, 2010</marker>
<rawString>Kuechler, W., &amp; Simkin, M. (2010). Why is performance on multiple-choice tests and constructed-response tests not more closely related? Theory and an empirical test. Decision Sciences Journal of Innovative Education, 8(1), 55–73. Retrieved from http://onlinelibrary.wiley.com/doi/10.1111/j.154 0-4609.2009.00243.x/full</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Van Labeke</author>
<author>D Whitelock</author>
<author>D Field</author>
</authors>
<title>OpenEssayist: extractive summarisation and formative assessment of free-text essays.</title>
<date>2013</date>
<booktitle>In First International Workshop on DiscourseCentric Learning Analytics.</booktitle>
<location>Leuven, Belgium.</location>
<note>Retrieved from http://oro.open.ac.uk/37548/</note>
<contexts>
<context position="2156" citStr="Labeke, Whitelock, &amp; Field, 2013" startWordPosition="305" endWordPosition="309">devising multiple-choice and short answer items that behave with psychometric equivalence has proven to be limited (Kuechler &amp; Simkin, 2010). Because standards-based STEM education in the United States explicitly promotes the development of writing skills for which constructed response items are ideally suited (NGSS Lead States, 2013; Porter, McMaken, Hwang, &amp; Yang, 2011; Southavilay, Yacef, Reimann, &amp; Calvo, 2013), the prospect of designing text analytics techniques for automatically assessing students’ textual responses has become even more appealing (Graesser, 2000; Jordan &amp; Butcher, 2013; Labeke, Whitelock, &amp; Field, 2013). An important family of short answer questions is the constructed response question. A constructed response question is designed to elicit a response of no more than a few sentences and features a relatively clear distinction between incorrect, partially correct, and correct answers. Ideally, a system designed for constructed response analysis (CRA) would be machinelearned from examples that include both graded student answers and expert-constructed “reference” answers (Dzikovska, Nielsen, &amp; Brew, 2012). The challenges of creating an accurate machine-learning-based CRA system stem from the v</context>
</contexts>
<marker>Labeke, Whitelock, Field, 2013</marker>
<rawString>Labeke, N. Van, Whitelock, D., &amp; Field, D. (2013). OpenEssayist: extractive summarisation and formative assessment of free-text essays. In First International Workshop on DiscourseCentric Learning Analytics. Leuven, Belgium. Retrieved from http://oro.open.ac.uk/37548/</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>C-rater: Automated scoring of short-answer questions.</title>
<date>2003</date>
<journal>Computers and the Humanities,</journal>
<volume>37</volume>
<issue>4</issue>
<pages>389--405</pages>
<note>Retrieved from http://link.springer.com/article/10.1023/A%3A1 025779619903</note>
<contexts>
<context position="6558" citStr="Leacock &amp; Chodorow, 2003" startWordPosition="995" endWordPosition="998">seen questions in seen domain, and answers to questions in unseen domains, and each of these has a separate weight. An item represented in the training data uses all three of these feature copies, and an item from another domain will only use the latter, “generic” feature copy. Spell correction is also often treated as a separate issue, handled in the data-cleaning step of a CRA system. The common approach at this step is to mark words as misspelled if they do not appear in a dictionary and replace them with their most likely alternative. This technique only corrects non-word spelling errors (Leacock &amp; Chodorow, 2003). Another approach is to use Soundex hashes that translate every word into a normalized form based on its pronunciation (Ott, Ziai, Hahn, &amp; Meurers, 2013). This second approach is generally featured alongside a more traditional direct comparison. The primary limitation of CRA for elementary school education is that evaluations of state-ofthe-art systems on raw elementary student response data are limited. C-rater provides a small evaluation on fourth-grade student math responses, but most evaluation is on seventh, eighth and eleventh grade students (Leacock &amp; Chodorow, 2003; Sukkarieh &amp; Blackm</context>
</contexts>
<marker>Leacock, Chodorow, 2003</marker>
<rawString>Leacock, C., &amp; Chodorow, M. (2003). C-rater: Automated scoring of short-answer questions. Computers and the Humanities, 37(4), 389–405. Retrieved from http://link.springer.com/article/10.1023/A%3A1 025779619903</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Leeman-Munk</author>
<author>E N Wiebe</author>
<author>J C Lester</author>
</authors>
<title>Assessing Elementary Students’ Science Competency with Text Analytics.</title>
<date>2014</date>
<booktitle>In Proceedings of the Fourth International Conference on Learning Analytics &amp; Knowledge.</booktitle>
<location>Indianapolis, Indiana.</location>
<contexts>
<context position="7941" citStr="Leeman-Munk, Wiebe, &amp; Lester, 2014" startWordPosition="1205" endWordPosition="1209">w spelling errors. The BEETLE corpus was drawn from undergraduate volunteers with a relatively strong command of the English language, and the SciEntsBank corpus, which was drawn from 3-6th graders, was originally intended for speech and as such was manually spell-corrected. The Hewlett Foundation’s automated student assessment prize (ASAP) shared task for short answer scoring was drawn entirely from tenth grade students (Hewlett, 2012). 3 Corpus We have been exploring constructed response assessment in the context of science education for upper elementary students with the LEONARDO CYBERPAD (Leeman-Munk, Wiebe, &amp; Lester, 2014). Under development in our laboratory for three years, the CYBERPAD is a digital science notebook that runs on tablet and web based computing platforms. The CYBERPAD integrates intelligent tutoring systems technologies into a digital science notebook that enables students to model science phenomena graphically. With a focus on the physical and earth sciences, the LEONARDO PADMATE, a pedagogical agent, supports students’ learning with real-time problemsolving advice. The CYBERPAD’s curriculum is based on that of the Full Option Science System (Foss Project, 2013). As students progress through </context>
<context position="9566" citStr="Leeman-Munk et al., 2014" startWordPosition="1458" endWordPosition="1461"> in California and North Carolina. The data collection for each class took place over a minimum of five class periods with students completing one or more new investigations each day. Students completed 62 investigations in one or both of two modules, “Energy and Circuits,” and “Magnetism.” Most questions included “starter text” that students were expected to complete. Students were able to modify the starter text in any way including deleting or replacing it entirely, although most students simply added to the starter text. Example answers can be found in a previous work on the same dataset (Leeman-Munk et al., 2014). Two human graders scored students’ responses from the circuits module on a science score rubric with three categories: incorrect, partially correct, and correct. The graders graded one class of data and then conferred on disagreeing results. They then graded other classes. On a sample of 10% of the responses of the classes graded after conferring, graders achieved a Cohen’s Kappa of 0.72. The graders dealt with considerable disfluency in the student responses in the LEONARDO corpus. An analysis of constructed responses in the Energy and Circuits module reveals that 4.7% of tokens in all of s</context>
<context position="11693" citStr="Leeman-Munk et al., 2014" startWordPosition="1802" endWordPosition="1805">of the same word. Considering “dcells” in an example answer, “mor dcells,” and “D-cells” in the reference answer, we can find overlaps in “ce,” “el,” “ll,” “ls,” “ell,” “lls,” and so on up to and including “cells.” This technique functions equally well for real-word spelling errors such as if the student had forgotten the “d” and typed only “cells.” Such overlaps signify a close match for both of these words. We evaluated the soft cardinality implementation of a generic short answer grading framework that we developed, WRITEEVAL, based on an answer grading system described in an earlier work (Leeman-Munk et al., 2014). We used 100-fold cross-validation on the “Energy and Circuits” module. We compare WRITEEVAL using soft cardinality to the majority class baseline and to WRITEEVAL using Precedent Feature Collection (PFC), a latent semantic analysis technique that performs competitively with the second highest-scoring system in Semeval Task 7 on unseen answers on the SciEntsBank corpus (Dzikovska et al., 2013). Using a Kruskal-Wallis test over one hundred folds, both systems significantly outperform the baseline (p&lt;.001), which achieved an accuracy score of .61. We could not evaluate the scores directly on th</context>
</contexts>
<marker>Leeman-Munk, Wiebe, Lester, 2014</marker>
<rawString>Leeman-Munk, S. P., Wiebe, E. N., &amp; Lester, J. C. (2014). Assessing Elementary Students’ Science Competency with Text Analytics. In Proceedings of the Fourth International Conference on Learning Analytics &amp; Knowledge. Indianapolis, Indiana.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NGSS Lead States</author>
</authors>
<title>Next Generation Science Standards: For States, By States. Washington DC:</title>
<date>2013</date>
<publisher>National Academic Press.</publisher>
<contexts>
<context position="1859" citStr="States, 2013" startWordPosition="266" endWordPosition="267">ol for automated assessment, short answer questions reveal cognitive processes and states in students that are difficult to uncover in multiplechoice equivalents (Nicol, 2007). Even when it seems that items could be designed to address the same cognitive construct, success in devising multiple-choice and short answer items that behave with psychometric equivalence has proven to be limited (Kuechler &amp; Simkin, 2010). Because standards-based STEM education in the United States explicitly promotes the development of writing skills for which constructed response items are ideally suited (NGSS Lead States, 2013; Porter, McMaken, Hwang, &amp; Yang, 2011; Southavilay, Yacef, Reimann, &amp; Calvo, 2013), the prospect of designing text analytics techniques for automatically assessing students’ textual responses has become even more appealing (Graesser, 2000; Jordan &amp; Butcher, 2013; Labeke, Whitelock, &amp; Field, 2013). An important family of short answer questions is the constructed response question. A constructed response question is designed to elicit a response of no more than a few sentences and features a relatively clear distinction between incorrect, partially correct, and correct answers. Ideally, a syste</context>
</contexts>
<marker>States, 2013</marker>
<rawString>NGSS Lead States. (2013). Next Generation Science Standards: For States, By States. Washington DC: National Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Nicol</author>
</authors>
<title>E-assessment by design: Using multiple-choice tests to good effect.</title>
<date>2007</date>
<journal>Journal of Further and Higher Education,</journal>
<volume>31</volume>
<issue>1</issue>
<pages>53--64</pages>
<note>Retrieved from http://www.tandfonline.com/doi/abs/10.1080/03 098770601167922</note>
<contexts>
<context position="1422" citStr="Nicol, 2007" startWordPosition="198" endWordPosition="199">ess. In this paper, we propose applying soft cardinality, a technique that has shown success grading less disfluent student answers, on a corpus of fourthgrade responses to constructed response questions. Based on decomposition of words into their constituent character substrings, soft cardinality’s evaluations of responses written by fourth graders correlates with summative analyses of their content knowledge. 1 Introduction As a tool for automated assessment, short answer questions reveal cognitive processes and states in students that are difficult to uncover in multiplechoice equivalents (Nicol, 2007). Even when it seems that items could be designed to address the same cognitive construct, success in devising multiple-choice and short answer items that behave with psychometric equivalence has proven to be limited (Kuechler &amp; Simkin, 2010). Because standards-based STEM education in the United States explicitly promotes the development of writing skills for which constructed response items are ideally suited (NGSS Lead States, 2013; Porter, McMaken, Hwang, &amp; Yang, 2011; Southavilay, Yacef, Reimann, &amp; Calvo, 2013), the prospect of designing text analytics techniques for automatically assessin</context>
</contexts>
<marker>Nicol, 2007</marker>
<rawString>Nicol, D. (2007). E-assessment by design: Using multiple-choice tests to good effect. Journal of Further and Higher Education, 31(1), 53–64. Retrieved from http://www.tandfonline.com/doi/abs/10.1080/03 098770601167922</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ott</author>
<author>R Ziai</author>
<author>M Hahn</author>
<author>D Meurers</author>
</authors>
<title>CoMeT : Integrating different levels of linguistic modeling for meaning assessment.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<volume>2</volume>
<pages>608--616</pages>
<contexts>
<context position="6711" citStr="Ott, Ziai, Hahn, &amp; Meurers, 2013" startWordPosition="1018" endWordPosition="1023">ining data uses all three of these feature copies, and an item from another domain will only use the latter, “generic” feature copy. Spell correction is also often treated as a separate issue, handled in the data-cleaning step of a CRA system. The common approach at this step is to mark words as misspelled if they do not appear in a dictionary and replace them with their most likely alternative. This technique only corrects non-word spelling errors (Leacock &amp; Chodorow, 2003). Another approach is to use Soundex hashes that translate every word into a normalized form based on its pronunciation (Ott, Ziai, Hahn, &amp; Meurers, 2013). This second approach is generally featured alongside a more traditional direct comparison. The primary limitation of CRA for elementary school education is that evaluations of state-ofthe-art systems on raw elementary student response data are limited. C-rater provides a small evaluation on fourth-grade student math responses, but most evaluation is on seventh, eighth and eleventh grade students (Leacock &amp; Chodorow, 2003; Sukkarieh &amp; Blackmore, 2009). Furthermore, the two datasets presented in SemEval’s shared task (Dzikovska et al., 2013) for testing and training featured relatively few sp</context>
</contexts>
<marker>Ott, Ziai, Hahn, Meurers, 2013</marker>
<rawString>Ott, N., Ziai, R., Hahn, M., &amp; Meurers, D. (2013). CoMeT : Integrating different levels of linguistic modeling for meaning assessment. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013) (Vol. 2, pp. 608–616).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Porter</author>
<author>J McMaken</author>
<author>J Hwang</author>
<author>R Yang</author>
</authors>
<date>2011</date>
<booktitle>Common core standards the new US intended curriculum. Educational Researcher,</booktitle>
<volume>40</volume>
<issue>3</issue>
<pages>103--116</pages>
<note>Retrieved from http://edr.sagepub.com/content/40/3/103.short</note>
<contexts>
<context position="1897" citStr="Porter, McMaken, Hwang, &amp; Yang, 2011" startWordPosition="268" endWordPosition="273">ed assessment, short answer questions reveal cognitive processes and states in students that are difficult to uncover in multiplechoice equivalents (Nicol, 2007). Even when it seems that items could be designed to address the same cognitive construct, success in devising multiple-choice and short answer items that behave with psychometric equivalence has proven to be limited (Kuechler &amp; Simkin, 2010). Because standards-based STEM education in the United States explicitly promotes the development of writing skills for which constructed response items are ideally suited (NGSS Lead States, 2013; Porter, McMaken, Hwang, &amp; Yang, 2011; Southavilay, Yacef, Reimann, &amp; Calvo, 2013), the prospect of designing text analytics techniques for automatically assessing students’ textual responses has become even more appealing (Graesser, 2000; Jordan &amp; Butcher, 2013; Labeke, Whitelock, &amp; Field, 2013). An important family of short answer questions is the constructed response question. A constructed response question is designed to elicit a response of no more than a few sentences and features a relatively clear distinction between incorrect, partially correct, and correct answers. Ideally, a system designed for constructed response an</context>
</contexts>
<marker>Porter, McMaken, Hwang, Yang, 2011</marker>
<rawString>Porter, A., McMaken, J., Hwang, J., &amp; Yang, R. (2011). Common core standards the new US intended curriculum. Educational Researcher, 40(3), 103–116. Retrieved from http://edr.sagepub.com/content/40/3/103.short</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Southavilay</author>
<author>K Yacef</author>
<author>P Reimann</author>
<author>R A Calvo</author>
</authors>
<title>Analysis of collaborative writing processes using revision maps and probabilistic topic models.</title>
<date>2013</date>
<booktitle>In Proceedings of the Third International Conference on Learning Analytics and Knowledge - LAK ’13</booktitle>
<pages>38--47</pages>
<publisher>ACM Press.</publisher>
<location>New York, New York, USA:</location>
<contexts>
<context position="1941" citStr="Southavilay, Yacef, Reimann, &amp; Calvo, 2013" startWordPosition="274" endWordPosition="279">reveal cognitive processes and states in students that are difficult to uncover in multiplechoice equivalents (Nicol, 2007). Even when it seems that items could be designed to address the same cognitive construct, success in devising multiple-choice and short answer items that behave with psychometric equivalence has proven to be limited (Kuechler &amp; Simkin, 2010). Because standards-based STEM education in the United States explicitly promotes the development of writing skills for which constructed response items are ideally suited (NGSS Lead States, 2013; Porter, McMaken, Hwang, &amp; Yang, 2011; Southavilay, Yacef, Reimann, &amp; Calvo, 2013), the prospect of designing text analytics techniques for automatically assessing students’ textual responses has become even more appealing (Graesser, 2000; Jordan &amp; Butcher, 2013; Labeke, Whitelock, &amp; Field, 2013). An important family of short answer questions is the constructed response question. A constructed response question is designed to elicit a response of no more than a few sentences and features a relatively clear distinction between incorrect, partially correct, and correct answers. Ideally, a system designed for constructed response analysis (CRA) would be machinelearned from ex</context>
</contexts>
<marker>Southavilay, Yacef, Reimann, Calvo, 2013</marker>
<rawString>Southavilay, V., Yacef, K., Reimann, P., &amp; Calvo, R. A. (2013). Analysis of collaborative writing processes using revision maps and probabilistic topic models. In Proceedings of the Third International Conference on Learning Analytics and Knowledge - LAK ’13 (pp. 38–47). New York, New York, USA: ACM Press. doi:10.1145/2460296.2460307</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sukkarieh</author>
<author>J Blackmore</author>
</authors>
<title>C-rater: Automatic content scoring for short constructed responses.</title>
<date>2009</date>
<booktitle>Proceedings of the 22nd International FLAIRS Conference,</booktitle>
<pages>290--295</pages>
<contexts>
<context position="7168" citStr="Sukkarieh &amp; Blackmore, 2009" startWordPosition="1088" endWordPosition="1091"> &amp; Chodorow, 2003). Another approach is to use Soundex hashes that translate every word into a normalized form based on its pronunciation (Ott, Ziai, Hahn, &amp; Meurers, 2013). This second approach is generally featured alongside a more traditional direct comparison. The primary limitation of CRA for elementary school education is that evaluations of state-ofthe-art systems on raw elementary student response data are limited. C-rater provides a small evaluation on fourth-grade student math responses, but most evaluation is on seventh, eighth and eleventh grade students (Leacock &amp; Chodorow, 2003; Sukkarieh &amp; Blackmore, 2009). Furthermore, the two datasets presented in SemEval’s shared task (Dzikovska et al., 2013) for testing and training featured relatively few spelling errors. The BEETLE corpus was drawn from undergraduate volunteers with a relatively strong command of the English language, and the SciEntsBank corpus, which was drawn from 3-6th graders, was originally intended for speech and as such was manually spell-corrected. The Hewlett Foundation’s automated student assessment prize (ASAP) shared task for short answer scoring was drawn entirely from tenth grade students (Hewlett, 2012). 3 Corpus We have be</context>
</contexts>
<marker>Sukkarieh, Blackmore, 2009</marker>
<rawString>Sukkarieh, J., &amp; Blackmore, J. (2009). C-rater: Automatic content scoring for short constructed responses. Proceedings of the 22nd International FLAIRS Conference, 290–295.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
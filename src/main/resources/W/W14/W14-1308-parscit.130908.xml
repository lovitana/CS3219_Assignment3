<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000349">
<title confidence="0.999511">
A Cascaded Approach for Social Media Text Normalization of Turkish
</title>
<author confidence="0.992838">
Dilara Torunoˇglu
</author>
<affiliation confidence="0.964552333333333">
Dep. of Computer Eng.
Istanbul Technical University
Istanbul, Turkey
</affiliation>
<email confidence="0.99242">
torunoglud@itu.edu.tr
</email>
<author confidence="0.992663">
G¨uls¸en Eryiˇgit
</author>
<affiliation confidence="0.964536">
Dep. of Computer Eng.
Istanbul Technical University
Istanbul, Turkey
</affiliation>
<email confidence="0.996368">
gulsen.cebiroglu@itu.edu.tr
</email>
<sectionHeader confidence="0.993853" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884789473684">
Text normalization is an indispensable
stage for natural language processing of
social media data with available NLP
tools. We divide the normalization prob-
lem into 7 categories, namely; letter case
transformation, replacement rules &amp; lexi-
con lookup, proper noun detection, deasci-
ification, vowel restoration, accent nor-
malization and spelling correction. We
propose a cascaded approach where each
ill formed word passes from these 7 mod-
ules and is investigated for possible trans-
formations. This paper presents the first
results for the normalization of Turkish
and tries to shed light on the different chal-
lenges in this area. We report a 40 per-
centage points improvement over a lexicon
lookup baseline and nearly 50 percentage
points over available spelling correctors.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999731125">
With the increasing number of people using micro
blogging sites like Facebook and Twitter, social
media became an indefinite source for machine
learning area especially for natural language pro-
cessing. This service is highly attractive for infor-
mation extraction, text mining and opinion min-
ing purposes as the large volumes of data available
online daily. The language used in this platform
differs severely from formally written text in that,
people do not feel forced to write grammatically
correct sentences, generally write like they talk or
try to impress their thoughts within a limited num-
ber of characters (such as in Twitter 140 charac-
ters). This results with a totally different language
than the conventional languages. The research on
text normalization of social media gained speed
towards the end of the last decade and as always,
almost all of these elementary studies are con-
ducted on the English language. We know from
earlier research results that morphologically rich
languages such as Turkish differ severely from En-
glish and the methods tailored for English do not
fit for these languages. It is the case for text nor-
malization as well.
Highly inflectional or agglutinative languages
share the same characteristic that a unique lemma
in these languages may have hundreds of possible
surface forms. This increases the data sparsity in
statistical models. For example, it’s pointed out in
Hakkani-T¨ur et al. (2000) that, it is due to Turk-
ish language’s inflectional and derivational mor-
phology that the number of distinct word forms
is very large compared to English distinct word
size (Table 1). This large vocabulary size is the
reason why the dictionary1 lookup or similarity
based approaches are not suitable for this kind of
languages. And in addition to this, it is not an
easy task to collect manually annotated data which
could cover all these surface forms and their re-
lated mistakes for statistical approaches.
</bodyText>
<table confidence="0.955006666666667">
Corpus Size Turkish English
1M words 106,547 33,398
10M words 417,775 97,734
</table>
<tableCaption confidence="0.798283">
Table 1: Vocabulary sizes for two Turkish and En-
glish corpora (Hakkani-T¨ur et al., 2000)
</tableCaption>
<bodyText confidence="0.999807727272727">
In this paper, we propose a cascaded approach
for the social text normalization (specifically for
Tweets) of Turkish language. The approach is
a combination of rule based and machine learn-
ing components for different layers of normaliza-
tion, namely; letter case transformation, replace-
ment rules &amp; lexicon lookup, proper noun detec-
tion, deasciification, vowel restoration, accent nor-
malization and spelling correction. Following the
work of Han and Baldwin (2011), we divided the
work into two stages: ill formed word detection
</bodyText>
<footnote confidence="0.9915">
1For these languages, it is theoretically impossible to put
every possible surface form into a dictionary.
</footnote>
<page confidence="0.97686">
62
</page>
<note confidence="0.991458">
Proceedings of the 5th Workshop on Language Anal sis for Social Media (LASM) @ EACL 2014, pages 62–70,
Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999310076923077">
and candidate word generation. Our contribution
is: 1. a new normalization model which could be
applied to other morphologically rich languages as
well with appropriate NLP tools 2. the first re-
sults and test data sets for the text normalization
of Turkish.
The paper is structured as follows: Section 2
and 3 give brief information about related work
and morphologically rich languages, Section 4
presents our normalization approach and Section
5 the experimental setup, Section 6 gives our ex-
perimental results and discussions and Section 7
the conclusion.
</bodyText>
<sectionHeader confidence="0.999814" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999944796296297">
An important part of the previous studies have
taken the normalization task either as a lexi-
con lookup (together with or without replacement
rules) or as a statistical problem. There also ex-
ist many studies which use their combination. In
these studies, a lexicon lookup is firstly employed
for most common usage of slang words, abbrevi-
ations etc. and then a machine learning method
is employed for the rest. Zhang et al. (2013) uses
replacement rules and a graph based model in or-
der to select the best rule combinations. Wang and
Ng (2013) uses a beam search decoder. Hassan
and Menezes (2013) propose an unsupervised ap-
proach which uses Random Walks on a contextual
similarity bipartite graph constructed from n-gram
sequences. In Han and Baldwin (2011), word sim-
ilarity and context is used during lexicon lookup.
Cook and Stevenson (2009) uses an unsupervised
noisy channel model. Clark and Araki (2011)
makes dictionary lookup. Liu et al. (2012) uses
a unified letter transformation to generate possi-
ble ill formed words in order to use them in the
training phase of a noisy channel model. Eisen-
stein (2013) analyzes phonological factors in so-
cial media writing.
Others, treating the normalization task as a
machine translation (MT) problem which tries
to translate from an ill formed language to a
conventional one, form also another important
group. For example the papers from Kaufmann
and Kalita (2010), Pennell and Liu (2011), Aw et
al. (2006) and Beaufort et al. (2010) may be col-
lected under this group. Since the emergence of
social media is very recent, only the latest stud-
ies are focused on this area and the earlier ones
generally work for the text normalization in TTS
(text-to-speech), ASR (automatic speech recogni-
tion) systems or SMS messages. Social media nor-
malization poses new challenges on top of these,
for example Twitter statuses contains mentions
(@user name), hashtags (#topic), variant number
of emoticons ( e.g. :) :@ &lt;3 @&gt;– ) and spe-
cial keywords (RT - retweet, DM - direct message
etc.).
Although very rare, there are also some stud-
ies on languages other than English and these
are mostly for speech recognition and SMS mes-
sages , e.g. Panchapagesan et al. (2004) for Hindi
TTS, Nguyen et al. (2010) for Vietnamese TTS,
Jia et al. (2008) for Mandarin TTS, Khan and
Karim (2012) for Urdu SMS. To the best of our
knowledge, our study is the first attempt for the
normalization of social media data for morpholog-
ically rich languages.
</bodyText>
<sectionHeader confidence="0.98285" genericHeader="method">
3 Morphologically Rich Languages
</sectionHeader>
<bodyText confidence="0.9503391875">
Morphologically rich languages such as Turkish,
Finnish, Korean, Hebrew etc., pose significant
challenges for natural language processing tasks
(Tsarfaty et al., 2013; Sarikaya et al., 2009). As
stated previously, the highly productive morphol-
ogy of these languages results in a very large num-
ber of word forms from a given stem. Table 2 lists
only a few (among hundreds of possible) surface
forms for the Turkish stem “ev” (house).
Surface form English
ev house
eve to the house
evde at the house
evdeki (which is) at the house
evdekiler those (who are) at the house
evdekilerde at those (who are)
</bodyText>
<tableCaption confidence="0.983561">
Table 2: Some surface forms for “ev” (house)
</tableCaption>
<bodyText confidence="0.994506">
Sarikaya et al. (2009) list the emerging prob-
lems as below:
</bodyText>
<listItem confidence="0.98316125">
1. increase in dictionary size
2. poor language model probability estimation
3. higher out-of-vocabulary (OOV) rate
4. inflection gap for machine translation2
</listItem>
<footnote confidence="0.9103202">
That is why, the normalization methods pro-
posed so far (adapting MT or language models or
2Since, the number of possible word surface forms after
inflections is very high, the alignment and translation accura-
cies in these languages are very badly affected.
</footnote>
<page confidence="0.999588">
63
</page>
<bodyText confidence="0.999805666666667">
lexicon lookup approaches) do not seem appropri-
ate for the processing of morphologically rich lan-
guages, as in our case for Turkish.
</bodyText>
<sectionHeader confidence="0.991817" genericHeader="method">
4 The Proposed Architecture
</sectionHeader>
<bodyText confidence="0.9991908">
We divide the normalization task into two parts:
Ill-formed word detection and candidate genera-
tion. Figure 1 presents the architecture of the pro-
posed normalization approach. The following sub-
sections provide the details for both of these two
parts and their components.
Before sending the input into these stages, we
first use our tokenizer specifically tailored for
Twitter for splitting the tweets into meaningful to-
kens. Our tokenizer is actually the first step of
our normalization process since: 1. It intelligently
splits the wrongly written word-punctuation com-
binations (e.g. “a,b” to [a , b]), while leaving “Ah-
met’den” (from Ahmet) is left as it is since the
apostrophe sign is used to append inflectional fea-
tures to a proper noun.) 2. It does special pro-
cessing for emoticons and consecutive punctua-
tion marks so that they still reside together after
the tokenization (e.g. :D or ! !!!! are output as they
occur).
</bodyText>
<figureCaption confidence="0.997147">
Figure 1: Normalization architecture
</figureCaption>
<subsectionHeader confidence="0.991076">
4.1 Ill-formed Word Detection
</subsectionHeader>
<bodyText confidence="0.999851333333333">
As stated earlier, since it is not possible to use a
lexicon lookup table for morphologically rich lan-
guages, we use a morphological analyzer (S¸ahin
et al., 2013) and an abbreviation list3 and a list of
1045 abbreviations for controlling in-vocabulary
(IV) words (labeled with a +NC “No Change” la-
bel for further use). By this way, we filter all the
out-of-vocabulary (OOV) words and transfer them
to the candidate generation process. Mentions
(@user name), hashtags (#topic), emoticons (:D) ,
vocatives (“ahahahaha”) and keywords (“RT”) are
also assumed to be OOV words since we want to
detect these and tag them with special labels to be
later used in higher-level NLP modules (e.g. POS
tagging, syntactic analysis).
</bodyText>
<subsectionHeader confidence="0.998433">
4.2 Candidate Generation
</subsectionHeader>
<bodyText confidence="0.965975">
In the candidate generation part, we have seven
components (rule based or machine learning mod-
els) which work sequentially. The outputs of each
of these components are controlled by the morpho-
logical analyzer and if the normalized form from a
component becomes an IV word then the process
is terminated and the output is labeled with a rele-
vant tag (provided in Table 3). Otherwise, the can-
didate generation process continues with the next
component over the original input (except for the
“Letter Case Transformation” and “Replacement
Rules &amp; Lexicon Lookup” components where the
input is replaced by the modified output although
it is still not an IV word, (see Section 4.2.1 and
4.2.2 for details).
</bodyText>
<table confidence="0.998279444444444">
Label Component
+NC No Change
+LCT Letter Case Transformation
+RR Replacement Rules &amp; Lexicon Lookup
+PND Proper Noun Detection
+DA Deasciification
+VR Vowel Restoration
+AN Accent Normalization
+NoN No Suggested Normalization
</table>
<tableCaption confidence="0.999127">
Table 3: Component Labels
</tableCaption>
<subsubsectionHeader confidence="0.654666">
4.2.1 Letter Case Transformation
</subsubsectionHeader>
<bodyText confidence="0.999919">
An OOV token, coming to this stage, may be in
one of the 4 different forms: lowercase, UPPER-
CASE, Proper Noun Case or miXEd CaSe. If
the token is in lowercase and does not possess
any specific punctuation marks for proper nouns
(i.e. ’ (apostrophe) or . (period)) , it is directly
</bodyText>
<footnote confidence="0.996718666666667">
3obtained from TLA (Turkish Language Association)
http://www.tdk.gov.tr/index.php?option=
com_content&amp;id=198:Kisaltmalar
</footnote>
<page confidence="0.999279">
64
</page>
<bodyText confidence="0.95432276">
transferred to the next stage without any change
(e.g. umuttan (from hope)). If the token is in
Proper Noun Case (e.g. Umut’tan), it is accepted
as a correct proper noun (even if it does not oc-
cur within the morphological analyzer’s lexicon or
was previously detected as an OOV word), left un-
touched (taking the label +NC) and excluded from
all future evaluations.
For UPPERCASE, miXEd CaSe and lowercase
words, we convert them into Proper Noun Case if
they either contain an apostrophe (which is used
in Turkish to separate inflectional suffixes from a
proper noun) or a period (.) which is used for-
mally in Turkish to denote abbreviations. These
words are labeled with a “+LCT” label after the
normalization. If the word does not contain any
of these two marks, it is then converted into low-
ercase form and processed by the morphological
analyzer as explained at the beginning of Sec-
tion 4.2. It should be noted that all words going
out from this component towards next stages are
transformed into lowercase from this point on.
“ahmet’ten” – Proper Noun
“AHMET’TEN” – Proper Noun
“EACL.”- Abbreviation
</bodyText>
<subsubsectionHeader confidence="0.601083">
4.2.2 Replacement Rules &amp; Lexicon Look-up
</subsubsectionHeader>
<bodyText confidence="0.999816">
While normalizing the tweets, we have to deal
with the following problems:
</bodyText>
<listItem confidence="0.99834825">
1. Slang words
2. Character repetition in interjections
3. Twitter-specific words
4. Emo style writing
</listItem>
<bodyText confidence="0.999404542857143">
We created a slang word lexicon of 272 words.
This lexicon contains entries as the following:
“kib” for “kendine iyi bak” (take care of your-
self), “nbr” for “ne haber” (what’s up). The tokens
within the lexicon are directly replaced with their
normalized forms.
Repetition of some characters within a word is
a very common method to express exclamation
in messages, such as in “litfeeeennnn” instead of
“litfen” (please), “c¸ooooooook” instead of “c¸ok”
(very) and “ayyyyy” instead of “ay” (oh!). We re-
duce the repeated characters into a single character
in the case that the consecutive occurrence count
is greater than 2.
The usage of Twitter-specific words such as
hashtags (“#topic”), mentions (“@user name”),
emoticons (“:)”), vocatives (“hahahhah”,
“h¨o¨o¨o¨o¨o”) and keywords (“RT”) also causes
a host of problems. The recurring patterns in
vocatives are reduced into minimal forms during
the normalization process, as for “haha” instead
of “hahahhah” and “h¨o” instead of “h¨o¨o¨o¨o¨o”.
Emo style writing, as in the example “$eker
4you” instead of “s¸eker senin ic¸in” (sweety, it’s
for you), is another problematic field for the nor-
malization task. We created 35 replacement rules
with regular expressions in order to automatically
correct or label the given input for Twitter-specific
words and Emo style writing. Examples include
“$ → s¸”, “6 → e”, “3 → e” and “!→ i”.
Through these replacement rules, we are able to
correct most instances of Emo style writing.
Our regular expressions also label the following
token types by the given specific labels for future
reference:
</bodyText>
<listItem confidence="0.984972153846154">
• Mentions: Nicknames that refer
to users on Twitter are labeled as e.g.
@mention[@dida]
• Hashtags: Hashtags that refer to trend-
ing topics on Twitter are labeled as e.g.
@hashtag[#geziparki]
• Vocatives: Vocatives are labeled as e.g.
@vocative[hehe]
• Smileys: Emoticons are labeled as e.g.
@smiley[:)]
• Twitter-specific Keywords: Keywords like
“RT”, “DM”, “MT”, “Reply” etc. are labeled as
e.g. @keyword[RT]
</listItem>
<bodyText confidence="0.999193111111111">
Figure 2 shows the normalized version of a
tweet in informal Turkish that could be translated
like “@dida what’s up, why don’t you call #of-
fended :(”, before and after being processed by this
component. Although the word “aramıon” also
needs normalization as “aramıyorsun” (you don’t
call), this transformation is not realized within the
current component and applied later in the accent
normalization component given in Section 4.2.6.
</bodyText>
<subsubsectionHeader confidence="0.786267">
4.2.3 Proper Noun Detection
</subsubsectionHeader>
<bodyText confidence="0.999852">
As previously stated, all OOV words coming to
this stage are in lowercase. In this component, our
aim is to detect proper nouns erroneously written
in lowercase (such as “ahmetten” or “ahmetden”)
and convert them to proper noun case with correct
formatting (“Ahmet’ten” for the aforementioned
examples).
</bodyText>
<page confidence="0.9922">
65
</page>
<figure confidence="0.992799">
�
@mention[@dida] ne haber neden aramion @hashtag[#kirildim] @smiley[: (]
nbr neden aramion
V
V
: (
#kirildim
V
@dida
V
V
</figure>
<figureCaption confidence="0.999304">
Figure 2: Normalization with Replacement Rules &amp; Lexicon Look-up
</figureCaption>
<bodyText confidence="0.997590057142857">
For this purpose, we use proper name gazetteers
from S¸eker and Eryi˘git (2012) together with a
newly added organization gazetteer of 122 tokens
in order to check whether a given word could
be a proper noun. Turkish proper nouns are
very frequently selected from common nouns such
as “C¸ic¸ek” (flower), “S¸eker” (sugar) and “˙Ipek”
(silk). Therefore, it is quite difficult to recog-
nize such words as proper nouns when they are
written in lowercase, as the task could not be ac-
complished by just checking the existence of such
words within the gazetteers.
For our proper noun detection component, we
use the below strategy:
1. We reduce the size of the gazetteers by remov-
ing all words with length ≤ 2 characters, or with
a ratio value under our specified threshold (1.5).
Ratio value is calculated, according to the formula
given in Equation 1, considering the occurrence
counts from two big corpora, the METU-Sabancı
Treebank (Say et al., 2002) and the web corpus
of Sak et al. (2011). Table 4 gives the counts for
three sample words. One may observe from the
table that “ahmet” occured 40 times in proper case
and 20 times in lower case form within the two
corpora resulting in a ratio value of 2.0. Since the
ratio value for “umut” is only 0.4 (which is un-
der our threshold), this noun is removed from our
gazetteers so that it would not be transformed into
proper case in case it is found to occur in low-
ercase form. A similar case holds for the word
“sa˘glam” (healthy). Although it is a very frequent
Turkish family name, it is observed in our corpora
mostly as a common noun with a ratio value of
0.09.
</bodyText>
<equation confidence="0.74212275">
Occurence in Propercase(wn)
ratio(wn) =
Occurence in Lowercase(wn)
(1)
</equation>
<bodyText confidence="0.910876333333333">
2. We pass the tokens to a morphological an-
alyzer for unknown words (S¸ahin et al., 2013)
and find possible lemmata as in the example be-
low. We then search for the longest possible stem
within our gazetteers (e.g. the longest stem for
“ahmetten” found within the name gazetteer is
</bodyText>
<table confidence="0.999425">
Proper Case Lowercase Sense Ratio
Sa˘glam=9 sa˘glam=100 healthy Ratio=0.09
Umut=40 umut=100 hope Ratio=0.4
Ahmet=40 ahmet=20 n/a Ratio=2.0
</table>
<tableCaption confidence="0.999803">
Table 4: Example of Ratio Values
</tableCaption>
<bodyText confidence="0.8680302">
“ahmet”), and when a stem is found within the
gazetteers, the initial letter of the stem is capital-
ized and the inflectional suffixes after the stem are
separated by use of an apostrophe (“Ahmet’ten”).
If none of the possible stems is found within the
gazetteers, the word is left as is and transferred to
the next stage in its original form.
“ahmet +Noun+A3sg+Pnon+Abl”
“ahmette +Noun+A3sg+Pnom+Loc”
“ahmetten +Noun+A3sg+Pnon+Nom”
</bodyText>
<subsectionHeader confidence="0.63298">
4.2.4 Deasciification
</subsectionHeader>
<bodyText confidence="0.999988142857143">
The role of the deasciifier is the reconstruction of
Turkish-specific characters with diacritics (i.e. ı,
˙I, s¸, ¨o, c¸, ˘g, ¨u) from their ASCII-compliant coun-
terparts (i.e. i, I, s, o, c, g, u). Most users of so-
cial media use asciified letters, which should be
corrected in order to obtain valid Turkish words.
The task is also not straightforward because of the
ambiguity potential in asciified forms, as between
the words “yasa” (law) and “yas¸a” (live). For
this stage, we use the deasciifier of Y¨uret (Y¨uret
and de la Maza, 2006) which implements the
GPA algorithm (which itself is basically a decision
tree implementation) in order to produce the most
likely deasciified form of the input.
</bodyText>
<subsectionHeader confidence="0.698066">
4.2.5 Vowel Restoration
</subsectionHeader>
<bodyText confidence="0.999741555555556">
There is a new trend of omitting vowels in typ-
ing among the Turkish social media users, in or-
der to reduce the message length. In this stage, we
process tokens written with consonants only (e.g.
“svyrm”), which is how vowel omission often hap-
pens. The aim of the vowel restoration is the gen-
eration of the original word by adding vowels into
the appropriate places (e.g. “svyrm” to “seviyo-
rum” (I love)). We employed a vocalizer (Adalı
</bodyText>
<page confidence="0.978931">
66
</page>
<bodyText confidence="0.9995145">
and Eryi˘git, 2014) which uses CRFs for the con-
struction of the most probable vocalized output.
</bodyText>
<subsectionHeader confidence="0.793471">
4.2.6 Accent Normalization
</subsectionHeader>
<bodyText confidence="0.998485263157895">
In the social media platform, people generally
write like they talk by transferring the pronounced
versions of the words directly to the written text.
Eisenstein (2013) also discusses the situation for
the English case. In the accent normalization mod-
ule we are trying to normalize this kind of writings
into proper forms. Some examples are given be-
low:
“gidicem” instead of “gidece˘gim”
(I’ll go)
“geliyonmu?” instead of “geliyor musun?”
(Are you coming?)
In this component, we first try to detect the most
common verb accents (generally endings such as
“-cem, -yom, -c¸az” etc.) used in social media and
then uses regular expression rules in order to re-
place these endings with their equivalent morpho-
logical analysis. One should note that since in
most of the morphologically rich languages, the
verb also carries inflections related to the person
agreement, we produce rules for catching all the
possible surface forms of these accents.
Table 5 introduces some of these re-
placement rules (column 1 and column 3).
As a result, the word “gidcem” becomes
“git+Verb+Pos+Fut+A1sg”4. We then use a
morphological generator and takes the cor-
rected output (if any) “gidece˘gim” (I’ll go) for
“git+Verb+Pos+Fut+A1sg”5.
We also have more complex replacement rules
in order to process more complex accent problems.
To give an example, the proper form of the word
“gidiyonmu” is actually “gidiyor musun” (are you
going) and in the formal form it is the question
enclitic (“mu”) which takes the person agreement
(“-sun” 2. person singular) where as in the accent
form the person agreement appears before “mu” as
a single letter “gidiyonmu”.
</bodyText>
<footnote confidence="0.971919428571429">
4Please note that, we also change the last letter of the stem
according to the harmonization rules of Turkish: the last let-
ters “bcdg” are changed to “pc¸tk”.
5the morphological tags in the table stands for: +Pos:
Positive, +Prog1: Present continuous tense, +A2sg: 2. per-
son singular, +Fut: Future tense, +A1sg: 1. person singular,
+A1pl: 1. person plural
</footnote>
<table confidence="0.99946">
Accent Correct Morph.
endings endings Analysis
+iyon +iyorsun +Verb+Pos+Prog1+A2sg
+cem +ece˘gim +Verb+Pos+Fut+A1sg
+caz +aca˘gız +Verb+Pos+Fut+A1pl
</table>
<tableCaption confidence="0.9149125">
Table 5: Accent Normalization Replacement
Rules
</tableCaption>
<subsectionHeader confidence="0.859315">
4.2.7 Spelling Correction
</subsectionHeader>
<bodyText confidence="0.998310705882353">
As the last component of our normalization ap-
proach, we propose to use a high performance
spelling corrector. This spelling corrector should
especially give a high precision score rather than
recall since the false positives have a very harm-
ing effect on the normalization task by producing
outputs with a totally different meaning. Unfortu-
nately, we could not find such a corrector for Turk-
ish. We tested with an MsWord plugin and the
spelling corrector of Zemberek (Akın and Akın,
2007) and obtained a negative impact by using
both. We are planning to create such a spelling
corrector as future work.
If an OOV word couldn’t still be normalized at
the end of the proposed iterative model (consisting
7 components), it is labeled with a “+NoN” label
and left in its original input format.
</bodyText>
<sectionHeader confidence="0.998679" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999904333333333">
In this section we provide information about our
used data sets, our evaluation strategy and the used
models in the experiments.
</bodyText>
<subsectionHeader confidence="0.99807">
5.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.999655647058824">
To test our success rates, we used a total of 1,200
tweets aligned and normalized manually. The
manual alignment is a one-to-many token align-
ment task from the original input towards the nor-
malized forms. To give an example, the slang us-
age “kib” will be aligned to 3 tokens (“kendine
iyi bak” (take care of yourself)) on the normal-
ized tweet. Although there are cases for many-to-
one alignment (such as in “cats,dogs”), these are
handled in the tokenization stage before the nor-
malization. We used half of this data set as our
validation set during the development of our pro-
posed components and reserved the remaining 600
tweets (collected from a different time slot) as a to-
tally unseen data set for using at the end. Table 6
provides some statistics over these data sets: the
number of tweets, the number of tokens and the
</bodyText>
<page confidence="0.99901">
67
</page>
<table confidence="0.999827">
Data Sets # Tweets # Tokens # OOV
Validation Set 600 6,322 2,708
Test Set 600 7,061 2,192
</table>
<tableCaption confidence="0.998907">
Table 6: Description of the Data Sets
</tableCaption>
<bodyText confidence="0.999532533333333">
number of OOV tokens.
Besides the aforementioned datasets, we also
had access to a much bigger Twitter data set
consisting of 4,049 manually normalized tweets
(Eryi˘git et al., 2013) (59,012 tokens in total). The
only difference of this data set is that the tweets
are not aligned on token level as in the previously
introduced data sets. That is why, it is not possi-
ble to use them for gold standard evaluation of our
system. But in order to be able to have an idea
about the performance of the previous approaches
regarding lexicon lookup, we decided to automat-
ically align this set and create a baseline lexicon
lookup model for comparison purposes. (see the
details in Section 5.3).
</bodyText>
<subsectionHeader confidence="0.99864">
5.2 Evaluation Method
</subsectionHeader>
<bodyText confidence="0.999994625">
We evaluated our work both for ill formed word
detection and candidate generation separately. For
ill formed word detection, we provide precision
(P), recall (R), f-measure (F) and accuracy (Acc.)
scores. For candidate generation, we provide only
the accuracy scores (the number of correctly nor-
malized tokens over the total number of detected
ill formed words).
</bodyText>
<subsectionHeader confidence="0.992946">
5.3 Compared Models
</subsectionHeader>
<bodyText confidence="0.999976538461539">
To the best of our knowledge this study is the
first attempt for the normalization of Turkish so-
cial media data. Since there are only spelling cor-
rector systems available for the task we compared
the proposed model with them. In other words, we
compared 3 different models with our proposed
system:
Model 1 (MsWord) is the model where we use an
api for getting the MsWord Turkish spelling sug-
gestions. Although this is not a tool developed for
normalization purposes we wanted to see its suc-
cess on our data sets. We accepted the top best
suggestion as the normalized version for the input
tokens.
Model 2 (Zemberek) (Akın and Akın, 2007) is also
an open source spelling corrector for Turkish.
Model 3 (Lookup Table) is a model that we devel-
oped with the aim of creating a baseline lookup
approach for comparison. For this purpose, we
first used GIZA++ (Och and Ney, 2000) in order
to automatically align the normalized tweets (us-
ing the 4,049 tweets’ data set presented in Sec-
tion 5.1) and created a lookup table with the pro-
duced aligned token sequences. We then used this
lookup table to check for the existence of each ill
formed word and get its normalized counterpart.
</bodyText>
<sectionHeader confidence="0.98837" genericHeader="method">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.998843">
Table 7 and Table 8 gives the results of the ill
formed word detection for different systems for
the validation set and the test set consecutively. In
these experiments, we do not provide the results of
the “Lookup Table” model since the ill formed de-
tection part of it is exactly the same with our pro-
posed model. For MsWord and Zemberek we con-
sidered each modified word as an ill formed word
detected by that system. We can see from the ta-
bles that our proposed model has an f-measure of
ill formed word detection 0.78. As it is explained
in Section 4.1, our ill formed word detection ap-
proach is very straightforward and it uses only a
morphological analyzer and an abbreviation list
in order to detect OOV words. Thus, one may
wonder why the scores for the proposed model
are not very close to 1 although it outperforms
all of its available rivals. This is because, there
exists nearly 20% of the ill formed tokens which
are not suspended to our morphological filter al-
though they are manually annotated as ill formed
by human annotators. This is certainly possible
for morphologically rich languages since a word
surface form may be the valid analysis of many
stems. The ill formed word “c¸alıs¸ıcım” is a good
example for this situation. Although this word
will be understood by most of the people as the ill
formed version of the word “c¸alıs¸aca˘gım” (I’m go-
ing to work), it is considered by the morphological
analyzer as a valid Turkish word since although
very rare, it could also be the surface form of
the word “c¸alıs¸” with additional derivational and
inflectional suffixes “c¸alıs¸+ıcı+m” meaning “my
worker”.
</bodyText>
<table confidence="0.998853">
Systems P R F Acc.
MsWord 0.25 0.59 0.35 0.58
Zemberek 0.21 0.17 0.19 0.21
Proposed Model 0.75 0.81 0.78 0.80
</table>
<tableCaption confidence="0.9663775">
Table 7: Ill Formed Word Detection Evaluation
Results on Validation Set
</tableCaption>
<page confidence="0.988285">
68
</page>
<table confidence="0.9997395">
Systems P R F Acc.
MsWord 0.24 0.19 0.21 0.56
Zemberek 0.11 0.29 0.20 0.11
Proposed Model 0.71 0.72 0.71 0.86
</table>
<tableCaption confidence="0.877947">
Table 8: Ill Formed Word Detection Evaluation
</tableCaption>
<table confidence="0.9686795">
Results on Test Set
Data Set Systems Accuracy
MsWord 0.25
Validation Set Zemberek 0.21
Lookup Table 0.34
Proposed Model 0.75
MsWord 0.24
Test Set Zemberek 0.11
Lookup Table 0.31
Proposed Model 0.71
</table>
<tableCaption confidence="0.848374">
Table 9: Candidate Generation Results on Data
Sets
</tableCaption>
<bodyText confidence="0.9995885">
may be improved by the addition of a lexicon
lookup (before the morphological filter) consisting
the most frequent normalization cases extracted
from manually normalized data if available. Thus,
as a future work we plan to extend our work both
on the ill formed word detection and on the cre-
ation of a spelling corrector with social web data
in focus.
</bodyText>
<sectionHeader confidence="0.936003" genericHeader="method">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999945888888889">
This work is part of our ongoing research project
“Parsing Turkish Web 2.0 Sentences” supported
by ICT COST Action IC1207 TUBITAK 1001
(grant no: 112E276). The authors want to thank
Turkcell Global Bilgi for sharing the manually
normalized data of user comments from the Tele-
com domain. We also want to thank Ozan Arkan
Can for his valuable discussions and helps during
the data preparation.
Table 9 gives the evaluation scores of each dif-
ferent system for both the validation and test data
sets. Although the lookup model is very basic,
one can observe from the table that it outperforms
both MsWord and Zemberek. Our proposed iter-
ative model obtains the highest scores (75% for
validation and 71% for test sets) with a relative
improvement of 40 percentage points over the lex-
icon lookup baseline.
</bodyText>
<sectionHeader confidence="0.998515" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999601352941176">
In this paper we presented a cascaded normaliza-
tion model for Turkish which could also be applied
to the morphologically rich languages with appro-
priate NLP tools. The model has two main parts:
ill formed word detection and candidate word gen-
eration consisting of 7 normalization stages (let-
ter case transformation, replacement rules &amp; lex-
icon lookup, proper noun detection, deasciifica-
tion, vowel restoration, accent normalization and
spelling correction) executed sequentially one on
top of the other one. We present the first and high-
est results for Turkish text normalization6 of so-
cial media data with a 86% accuracy of ill formed
word detection and 71% accuracy for candidate
word generation. A morphological analyzer is
used for the detection of ill formed words. But
we believe the accuracy of this first detection stage
</bodyText>
<footnote confidence="0.978037333333333">
6The produced test sets and the Web interface of the
Turkish Normalizer is available via http://tools.nlp.itu.edu.tr
(Eryi˘git, 2014)
</footnote>
<sectionHeader confidence="0.985211" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99720525">
K¨ubra Adalı and G¨uls¸en Eryi˘git. 2014. Vowel and
diacritic restoration for social media texts. In 5th
Workshop on Language Analysis for Social Media
(LASM) at EACL, Gothenburg, Sweden, April. As-
sociation for Computational Linguistics.
Ahmet Afsin Akın and Mehmet D¨undar Akın. 2007.
Zemberek, an open source nlp framework for turkic
languages. Structure.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for sms text nor-
malization. In Proc. of the COLING/ACL on
Main conference poster sessions, COLING-ACL
’06, pages 33–40, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Richard Beaufort, Sophie Roekhaut, Louise-Am´elie
Cougnon, and C´edrick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normal-
izing sms messages. In Proc. of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, ACL ’10, pages 770–779, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Eleanor Clark and Kenji Araki. 2011. Text normal-
ization in social media: progress, problems and ap-
plications for a pre-processing system of casual en-
glish. Procedia-Social and Behavioral Sciences,
27:2–11.
Paul Cook and Suzanne Stevenson. 2009. An
unsupervised model for text message normaliza-
tion. In Proc. of the Workshop on Computational
Approaches to Linguistic Creativity, CALC ’09,
pages 71–78, Stroudsburg, PA, USA. Association
for Computational Linguistics.
</reference>
<page confidence="0.993409">
69
</page>
<reference confidence="0.999931293577981">
Jacob Eisenstein. 2013. Phonological factors in social
media writing. In Proc. of the Workshop on Lan-
guage Analysis in Social Media, pages 11–19, At-
lanta, Georgia, June. Association for Computational
Linguistics.
G¨uls¸en Eryi˘git, Fatih Samet C¸etin, Meltem Yanık,
Tanel Temel, and ˙Iyas C¸ic¸ekli. 2013. Turksent:
A sentiment annotation tool for social media. In
Proc. of the 7th Linguistic Annotation Workshop
and Interoperability with Discourse, pages 131–134,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
G¨uls¸en Eryi˘git. 2014. ITU Turkish NLP web service.
In Proc. of the Demonstrations at the 14th Confer-
ence of the European Chapter of the Association
for Computational Linguistics (EACL), Gothenburg,
Sweden, April. Association for Computational Lin-
guistics.
Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur.
2000. Statistical morphological disambiguation for
agglutinative languages. In Proc. of the 18th confer-
ence on Computational linguistics - Volume 1, COL-
ING ’00, pages 285–291, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Proc. of the 49th ACL HLT, pages 368–378, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Hany Hassan and Arul Menezes. 2013. Social text nor-
malization using contextual graph random walks. In
Proc. of the 51st ACL, pages 1577–1586, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Yuxiang Jia, Dezhi Huang, Wu Liu, Shiwen Yu, and
Haila Wang. 2008. Text normalization in Mandarin
text-to-speech system. In ICASSP, pages 4693–
4696. IEEE.
Max Kaufmann and Jugal Kalita. 2010. Syntactic nor-
malization of Twitter messages. In Proc. of the 8th
International Conference on Natural Language Pro-
cessing (ICON 2010), Chennai, India. Macmillan In-
dia.
Osama A Khan and Asim Karim. 2012. A rule-based
model for normalization of sms text. In Tools with
Artificial Intelligence (ICTAI), 2012 IEEE 24th In-
ternational Conference on, volume 1, pages 634–
641. IEEE.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broad-coverage normalization system for social me-
dia language. In Proc. of the 50th ACL, pages 1035–
1044, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Thu-Trang Thi Nguyen, Thanh Thi Pham, and Do-Dat
Tran. 2010. A method for vietnamese text normal-
ization to improve the quality of speech synthesis.
In Proc. of the 2010 Symposium on Information and
Communication Technology, SoICT ’10, pages 78–
85, New York, NY, USA. ACM.
Franz Josef Och and Hermann Ney. 2000. Giza++:
Training of statistical translation models.
K Panchapagesan, Partha Pratim Talukdar, N Sridhar
Krishna, Kalika Bali, and AG Ramakrishnan. 2004.
Hindi text normalization. In Fifth International
Conference on Knowledge Based Computer Systems
(KBCS), pages 19–22. Citeseer.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
sms abbreviations. In IJCNLP, pages 974–982.
Muhammet S¸ahin, Umut Sulubacak, and G¨uls¸en
Eryi˘git. 2013. Redefinition of turkish morphology
using flag diacritics. In Proc. of The Tenth Sym-
posium on Natural Language Processing (SNLP-
2013), Phuket, Thailand, October.
Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2011.
Resources for Turkish morphological processing.
Lang. Resour. Eval., 45(2):249–261, May.
Ruhi Sarikaya, Katrin Kirchhoff, Tanja Schultz, and
Dilek Hakkani-Tur. 2009. Introduction to the spe-
cial issue on processing morphologically rich lan-
guages. Trans. Audio, Speech and Lang. Proc.,
17(5):861–862, July.
Bilge Say, Deniz Zeyrek, Kemal Oflazer, and Umut
¨Ozge. 2002. Development of a corpus and a tree-
bank for present-day written Turkish. In Proc. of the
Eleventh International Conference of Turkish Lin-
guistics, Famaguste, Cyprus, August.
G¨okhan Akın S¸eker and G¨uls¸en Eryi˘git. 2012. Initial
explorations on using CRFs for Turkish named en-
tity recognition. In Proc. of COLING 2012, Mum-
bai, India, 8-15 December.
Reut Tsarfaty, Djam´e Seddah, Sandra K¨ubler, and
Joakim Nivre. 2013. Parsing morphologically rich
languages: Introduction to the special issue. Com-
putational Linguistics, 39(1):15–22.
Pidong Wang and Hwee Tou Ng. 2013. A beam-
search decoder for normalization of social media
text with application to machine translation. In
Proc. of NAACL-HLT, pages 471–481.
Deniz Y¨uret and Michael de la Maza. 2006. The
greedy prepend algorithm for decision list induc-
tion. In Proc. of the 21st international conference
on Computer and Information Sciences, ISCIS’06,
pages 37–46, Berlin, Heidelberg. Springer-Verlag.
Congle Zhang, Tyler Baldwin, Howard Ho, Benny
Kimelfeld, and Yunyao Li. 2013. Adaptive parser-
centric text normalization. In Proc. of the 51st ACL,
pages 1159–1168, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.998483">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.283906">
<title confidence="0.994108">A Cascaded Approach for Social Media Text Normalization of Turkish</title>
<author confidence="0.474917">Dilara</author>
<affiliation confidence="0.983886">Dep. of Computer Istanbul Technical</affiliation>
<address confidence="0.805487">Istanbul,</address>
<email confidence="0.996814">torunoglud@itu.edu.tr</email>
<affiliation confidence="0.9850035">Dep. of Computer Istanbul Technical</affiliation>
<address confidence="0.805219">Istanbul,</address>
<email confidence="0.997862">gulsen.cebiroglu@itu.edu.tr</email>
<abstract confidence="0.99944535">Text normalization is an indispensable stage for natural language processing of social media data with available NLP tools. We divide the normalization problem into 7 categories, namely; letter case transformation, replacement rules &amp; lexicon lookup, proper noun detection, deasciification, vowel restoration, accent normalization and spelling correction. We propose a cascaded approach where each ill formed word passes from these 7 modules and is investigated for possible transformations. This paper presents the first results for the normalization of Turkish and tries to shed light on the different challenges in this area. We report a 40 percentage points improvement over a lexicon lookup baseline and nearly 50 percentage points over available spelling correctors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K¨ubra Adalı</author>
<author>G¨uls¸en Eryi˘git</author>
</authors>
<title>Vowel and diacritic restoration for social media texts.</title>
<date>2014</date>
<booktitle>In 5th Workshop on Language Analysis for Social Media (LASM) at EACL,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<marker>Adalı, Eryi˘git, 2014</marker>
<rawString>K¨ubra Adalı and G¨uls¸en Eryi˘git. 2014. Vowel and diacritic restoration for social media texts. In 5th Workshop on Language Analysis for Social Media (LASM) at EACL, Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<title>Ahmet Afsin Akın and Mehmet D¨undar Akın.</title>
<date>2007</date>
<journal>Structure.</journal>
<marker>2007</marker>
<rawString>Ahmet Afsin Akın and Mehmet D¨undar Akın. 2007. Zemberek, an open source nlp framework for turkic languages. Structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AiTi Aw</author>
<author>Min Zhang</author>
<author>Juan Xiao</author>
<author>Jian Su</author>
</authors>
<title>A phrase-based statistical model for sms text normalization.</title>
<date>2006</date>
<booktitle>In Proc. of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06,</booktitle>
<pages>33--40</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6053" citStr="Aw et al. (2006)" startWordPosition="951" endWordPosition="954">2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factors in social media writing. Others, treating the normalization task as a machine translation (MT) problem which tries to translate from an ill formed language to a conventional one, form also another important group. For example the papers from Kaufmann and Kalita (2010), Pennell and Liu (2011), Aw et al. (2006) and Beaufort et al. (2010) may be collected under this group. Since the emergence of social media is very recent, only the latest studies are focused on this area and the earlier ones generally work for the text normalization in TTS (text-to-speech), ASR (automatic speech recognition) systems or SMS messages. Social media normalization poses new challenges on top of these, for example Twitter statuses contains mentions (@user name), hashtags (#topic), variant number of emoticons ( e.g. :) :@ &lt;3 @&gt;– ) and special keywords (RT - retweet, DM - direct message etc.). Although very rare, there are </context>
</contexts>
<marker>Aw, Zhang, Xiao, Su, 2006</marker>
<rawString>AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A phrase-based statistical model for sms text normalization. In Proc. of the COLING/ACL on Main conference poster sessions, COLING-ACL ’06, pages 33–40, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Beaufort</author>
<author>Sophie Roekhaut</author>
<author>Louise-Am´elie Cougnon</author>
<author>C´edrick Fairon</author>
</authors>
<title>A hybrid rule/model-based finite-state framework for normalizing sms messages.</title>
<date>2010</date>
<booktitle>In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>770--779</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6080" citStr="Beaufort et al. (2010)" startWordPosition="956" endWordPosition="959">vised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factors in social media writing. Others, treating the normalization task as a machine translation (MT) problem which tries to translate from an ill formed language to a conventional one, form also another important group. For example the papers from Kaufmann and Kalita (2010), Pennell and Liu (2011), Aw et al. (2006) and Beaufort et al. (2010) may be collected under this group. Since the emergence of social media is very recent, only the latest studies are focused on this area and the earlier ones generally work for the text normalization in TTS (text-to-speech), ASR (automatic speech recognition) systems or SMS messages. Social media normalization poses new challenges on top of these, for example Twitter statuses contains mentions (@user name), hashtags (#topic), variant number of emoticons ( e.g. :) :@ &lt;3 @&gt;– ) and special keywords (RT - retweet, DM - direct message etc.). Although very rare, there are also some studies on langua</context>
</contexts>
<marker>Beaufort, Roekhaut, Cougnon, Fairon, 2010</marker>
<rawString>Richard Beaufort, Sophie Roekhaut, Louise-Am´elie Cougnon, and C´edrick Fairon. 2010. A hybrid rule/model-based finite-state framework for normalizing sms messages. In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 770–779, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor Clark</author>
<author>Kenji Araki</author>
</authors>
<title>Text normalization in social media: progress, problems and applications for a pre-processing system of casual english.</title>
<date>2011</date>
<booktitle>Procedia-Social and Behavioral Sciences,</booktitle>
<pages>27--2</pages>
<contexts>
<context position="5507" citStr="Clark and Araki (2011)" startWordPosition="861" endWordPosition="864">loyed for most common usage of slang words, abbreviations etc. and then a machine learning method is employed for the rest. Zhang et al. (2013) uses replacement rules and a graph based model in order to select the best rule combinations. Wang and Ng (2013) uses a beam search decoder. Hassan and Menezes (2013) propose an unsupervised approach which uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences. In Han and Baldwin (2011), word similarity and context is used during lexicon lookup. Cook and Stevenson (2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factors in social media writing. Others, treating the normalization task as a machine translation (MT) problem which tries to translate from an ill formed language to a conventional one, form also another important group. For example the papers from Kaufmann and Kalita (2010), Pennell and Liu (2011), Aw et al. (2006) and Beaufort et al. (2010) may be collected under thi</context>
</contexts>
<marker>Clark, Araki, 2011</marker>
<rawString>Eleanor Clark and Kenji Araki. 2011. Text normalization in social media: progress, problems and applications for a pre-processing system of casual english. Procedia-Social and Behavioral Sciences, 27:2–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Suzanne Stevenson</author>
</authors>
<title>An unsupervised model for text message normalization.</title>
<date>2009</date>
<booktitle>In Proc. of the Workshop on Computational Approaches to Linguistic Creativity, CALC ’09,</booktitle>
<pages>71--78</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5442" citStr="Cook and Stevenson (2009)" startWordPosition="851" endWordPosition="854">their combination. In these studies, a lexicon lookup is firstly employed for most common usage of slang words, abbreviations etc. and then a machine learning method is employed for the rest. Zhang et al. (2013) uses replacement rules and a graph based model in order to select the best rule combinations. Wang and Ng (2013) uses a beam search decoder. Hassan and Menezes (2013) propose an unsupervised approach which uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences. In Han and Baldwin (2011), word similarity and context is used during lexicon lookup. Cook and Stevenson (2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factors in social media writing. Others, treating the normalization task as a machine translation (MT) problem which tries to translate from an ill formed language to a conventional one, form also another important group. For example the papers from Kaufmann and Kalita (2010), Pennell and Liu (2011), Aw et</context>
</contexts>
<marker>Cook, Stevenson, 2009</marker>
<rawString>Paul Cook and Suzanne Stevenson. 2009. An unsupervised model for text message normalization. In Proc. of the Workshop on Computational Approaches to Linguistic Creativity, CALC ’09, pages 71–78, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
</authors>
<title>Phonological factors in social media writing.</title>
<date>2013</date>
<booktitle>In Proc. of the Workshop on Language Analysis in Social Media,</booktitle>
<pages>11--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="5712" citStr="Eisenstein (2013)" startWordPosition="898" endWordPosition="900">the best rule combinations. Wang and Ng (2013) uses a beam search decoder. Hassan and Menezes (2013) propose an unsupervised approach which uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences. In Han and Baldwin (2011), word similarity and context is used during lexicon lookup. Cook and Stevenson (2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factors in social media writing. Others, treating the normalization task as a machine translation (MT) problem which tries to translate from an ill formed language to a conventional one, form also another important group. For example the papers from Kaufmann and Kalita (2010), Pennell and Liu (2011), Aw et al. (2006) and Beaufort et al. (2010) may be collected under this group. Since the emergence of social media is very recent, only the latest studies are focused on this area and the earlier ones generally work for the text normalization in TTS (text-to-speech), ASR (au</context>
<context position="19838" citStr="Eisenstein (2013)" startWordPosition="3204" endWordPosition="3205">ssage length. In this stage, we process tokens written with consonants only (e.g. “svyrm”), which is how vowel omission often happens. The aim of the vowel restoration is the generation of the original word by adding vowels into the appropriate places (e.g. “svyrm” to “seviyorum” (I love)). We employed a vocalizer (Adalı 66 and Eryi˘git, 2014) which uses CRFs for the construction of the most probable vocalized output. 4.2.6 Accent Normalization In the social media platform, people generally write like they talk by transferring the pronounced versions of the words directly to the written text. Eisenstein (2013) also discusses the situation for the English case. In the accent normalization module we are trying to normalize this kind of writings into proper forms. Some examples are given below: “gidicem” instead of “gidece˘gim” (I’ll go) “geliyonmu?” instead of “geliyor musun?” (Are you coming?) In this component, we first try to detect the most common verb accents (generally endings such as “-cem, -yom, -c¸az” etc.) used in social media and then uses regular expression rules in order to replace these endings with their equivalent morphological analysis. One should note that since in most of the morph</context>
</contexts>
<marker>Eisenstein, 2013</marker>
<rawString>Jacob Eisenstein. 2013. Phonological factors in social media writing. In Proc. of the Workshop on Language Analysis in Social Media, pages 11–19, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨uls¸en Eryi˘git</author>
</authors>
<title>Fatih Samet C¸etin, Meltem Yanık, Tanel Temel, and ˙Iyas C¸ic¸ekli.</title>
<date>2013</date>
<booktitle>In Proc. of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>131--134</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Eryi˘git, 2013</marker>
<rawString>G¨uls¸en Eryi˘git, Fatih Samet C¸etin, Meltem Yanık, Tanel Temel, and ˙Iyas C¸ic¸ekli. 2013. Turksent: A sentiment annotation tool for social media. In Proc. of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 131–134, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨uls¸en Eryi˘git</author>
</authors>
<title>ITU Turkish NLP web service.</title>
<date>2014</date>
<booktitle>In Proc. of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<marker>Eryi˘git, 2014</marker>
<rawString>G¨uls¸en Eryi˘git. 2014. ITU Turkish NLP web service. In Proc. of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Z Hakkani-T¨ur</author>
<author>Kemal Oflazer</author>
<author>G¨okhan T¨ur</author>
</authors>
<title>Statistical morphological disambiguation for agglutinative languages.</title>
<date>2000</date>
<booktitle>In Proc. of the 18th conference on Computational linguistics - Volume 1, COLING ’00,</booktitle>
<pages>285--291</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Hakkani-T¨ur, Oflazer, T¨ur, 2000</marker>
<rawString>Dilek Z. Hakkani-T¨ur, Kemal Oflazer, and G¨okhan T¨ur. 2000. Statistical morphological disambiguation for agglutinative languages. In Proc. of the 18th conference on Computational linguistics - Volume 1, COLING ’00, pages 285–291, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proc. of the 49th ACL HLT,</booktitle>
<pages>368--378</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="3660" citStr="Han and Baldwin (2011)" startWordPosition="562" endWordPosition="565">ize Turkish English 1M words 106,547 33,398 10M words 417,775 97,734 Table 1: Vocabulary sizes for two Turkish and English corpora (Hakkani-T¨ur et al., 2000) In this paper, we propose a cascaded approach for the social text normalization (specifically for Tweets) of Turkish language. The approach is a combination of rule based and machine learning components for different layers of normalization, namely; letter case transformation, replacement rules &amp; lexicon lookup, proper noun detection, deasciification, vowel restoration, accent normalization and spelling correction. Following the work of Han and Baldwin (2011), we divided the work into two stages: ill formed word detection 1For these languages, it is theoretically impossible to put every possible surface form into a dictionary. 62 Proceedings of the 5th Workshop on Language Anal sis for Social Media (LASM) @ EACL 2014, pages 62–70, Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics and candidate word generation. Our contribution is: 1. a new normalization model which could be applied to other morphologically rich languages as well with appropriate NLP tools 2. the first results and test data sets for the text no</context>
<context position="5356" citStr="Han and Baldwin (2011)" startWordPosition="837" endWordPosition="840">cement rules) or as a statistical problem. There also exist many studies which use their combination. In these studies, a lexicon lookup is firstly employed for most common usage of slang words, abbreviations etc. and then a machine learning method is employed for the rest. Zhang et al. (2013) uses replacement rules and a graph based model in order to select the best rule combinations. Wang and Ng (2013) uses a beam search decoder. Hassan and Menezes (2013) propose an unsupervised approach which uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences. In Han and Baldwin (2011), word similarity and context is used during lexicon lookup. Cook and Stevenson (2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factors in social media writing. Others, treating the normalization task as a machine translation (MT) problem which tries to translate from an ill formed language to a conventional one, form also another important group.</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In Proc. of the 49th ACL HLT, pages 368–378, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hany Hassan</author>
<author>Arul Menezes</author>
</authors>
<title>Social text normalization using contextual graph random walks.</title>
<date>2013</date>
<booktitle>In Proc. of the 51st ACL,</booktitle>
<pages>1577--1586</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5195" citStr="Hassan and Menezes (2013)" startWordPosition="813" endWordPosition="816">he conclusion. 2 Related Work An important part of the previous studies have taken the normalization task either as a lexicon lookup (together with or without replacement rules) or as a statistical problem. There also exist many studies which use their combination. In these studies, a lexicon lookup is firstly employed for most common usage of slang words, abbreviations etc. and then a machine learning method is employed for the rest. Zhang et al. (2013) uses replacement rules and a graph based model in order to select the best rule combinations. Wang and Ng (2013) uses a beam search decoder. Hassan and Menezes (2013) propose an unsupervised approach which uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences. In Han and Baldwin (2011), word similarity and context is used during lexicon lookup. Cook and Stevenson (2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factors in social media writing. Others, treating the normal</context>
</contexts>
<marker>Hassan, Menezes, 2013</marker>
<rawString>Hany Hassan and Arul Menezes. 2013. Social text normalization using contextual graph random walks. In Proc. of the 51st ACL, pages 1577–1586, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuxiang Jia</author>
<author>Dezhi Huang</author>
<author>Wu Liu</author>
<author>Shiwen Yu</author>
<author>Haila Wang</author>
</authors>
<title>Text normalization in Mandarin text-to-speech system.</title>
<date>2008</date>
<booktitle>In ICASSP,</booktitle>
<pages>4693--4696</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6872" citStr="Jia et al. (2008)" startWordPosition="1093" endWordPosition="1096">r the text normalization in TTS (text-to-speech), ASR (automatic speech recognition) systems or SMS messages. Social media normalization poses new challenges on top of these, for example Twitter statuses contains mentions (@user name), hashtags (#topic), variant number of emoticons ( e.g. :) :@ &lt;3 @&gt;– ) and special keywords (RT - retweet, DM - direct message etc.). Although very rare, there are also some studies on languages other than English and these are mostly for speech recognition and SMS messages , e.g. Panchapagesan et al. (2004) for Hindi TTS, Nguyen et al. (2010) for Vietnamese TTS, Jia et al. (2008) for Mandarin TTS, Khan and Karim (2012) for Urdu SMS. To the best of our knowledge, our study is the first attempt for the normalization of social media data for morphologically rich languages. 3 Morphologically Rich Languages Morphologically rich languages such as Turkish, Finnish, Korean, Hebrew etc., pose significant challenges for natural language processing tasks (Tsarfaty et al., 2013; Sarikaya et al., 2009). As stated previously, the highly productive morphology of these languages results in a very large number of word forms from a given stem. Table 2 lists only a few (among hundreds o</context>
</contexts>
<marker>Jia, Huang, Liu, Yu, Wang, 2008</marker>
<rawString>Yuxiang Jia, Dezhi Huang, Wu Liu, Shiwen Yu, and Haila Wang. 2008. Text normalization in Mandarin text-to-speech system. In ICASSP, pages 4693– 4696. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Kaufmann</author>
<author>Jugal Kalita</author>
</authors>
<title>Syntactic normalization of Twitter messages.</title>
<date>2010</date>
<booktitle>In Proc. of the 8th International Conference on Natural Language Processing (ICON 2010),</booktitle>
<location>Chennai, India. Macmillan</location>
<contexts>
<context position="6011" citStr="Kaufmann and Kalita (2010)" startWordPosition="943" endWordPosition="946"> is used during lexicon lookup. Cook and Stevenson (2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factors in social media writing. Others, treating the normalization task as a machine translation (MT) problem which tries to translate from an ill formed language to a conventional one, form also another important group. For example the papers from Kaufmann and Kalita (2010), Pennell and Liu (2011), Aw et al. (2006) and Beaufort et al. (2010) may be collected under this group. Since the emergence of social media is very recent, only the latest studies are focused on this area and the earlier ones generally work for the text normalization in TTS (text-to-speech), ASR (automatic speech recognition) systems or SMS messages. Social media normalization poses new challenges on top of these, for example Twitter statuses contains mentions (@user name), hashtags (#topic), variant number of emoticons ( e.g. :) :@ &lt;3 @&gt;– ) and special keywords (RT - retweet, DM - direct mes</context>
</contexts>
<marker>Kaufmann, Kalita, 2010</marker>
<rawString>Max Kaufmann and Jugal Kalita. 2010. Syntactic normalization of Twitter messages. In Proc. of the 8th International Conference on Natural Language Processing (ICON 2010), Chennai, India. Macmillan India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Osama A Khan</author>
<author>Asim Karim</author>
</authors>
<title>A rule-based model for normalization of sms text.</title>
<date>2012</date>
<booktitle>In Tools with Artificial Intelligence (ICTAI), 2012 IEEE 24th International Conference on,</booktitle>
<volume>1</volume>
<pages>634--641</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6912" citStr="Khan and Karim (2012)" startWordPosition="1100" endWordPosition="1103">t-to-speech), ASR (automatic speech recognition) systems or SMS messages. Social media normalization poses new challenges on top of these, for example Twitter statuses contains mentions (@user name), hashtags (#topic), variant number of emoticons ( e.g. :) :@ &lt;3 @&gt;– ) and special keywords (RT - retweet, DM - direct message etc.). Although very rare, there are also some studies on languages other than English and these are mostly for speech recognition and SMS messages , e.g. Panchapagesan et al. (2004) for Hindi TTS, Nguyen et al. (2010) for Vietnamese TTS, Jia et al. (2008) for Mandarin TTS, Khan and Karim (2012) for Urdu SMS. To the best of our knowledge, our study is the first attempt for the normalization of social media data for morphologically rich languages. 3 Morphologically Rich Languages Morphologically rich languages such as Turkish, Finnish, Korean, Hebrew etc., pose significant challenges for natural language processing tasks (Tsarfaty et al., 2013; Sarikaya et al., 2009). As stated previously, the highly productive morphology of these languages results in a very large number of word forms from a given stem. Table 2 lists only a few (among hundreds of possible) surface forms for the Turkis</context>
</contexts>
<marker>Khan, Karim, 2012</marker>
<rawString>Osama A Khan and Asim Karim. 2012. A rule-based model for normalization of sms text. In Tools with Artificial Intelligence (ICTAI), 2012 IEEE 24th International Conference on, volume 1, pages 634– 641. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Liu</author>
<author>Fuliang Weng</author>
<author>Xiao Jiang</author>
</authors>
<title>A broad-coverage normalization system for social media language.</title>
<date>2012</date>
<booktitle>In Proc. of the 50th ACL,</booktitle>
<pages>1035--1044</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5550" citStr="Liu et al. (2012)" startWordPosition="868" endWordPosition="871">eviations etc. and then a machine learning method is employed for the rest. Zhang et al. (2013) uses replacement rules and a graph based model in order to select the best rule combinations. Wang and Ng (2013) uses a beam search decoder. Hassan and Menezes (2013) propose an unsupervised approach which uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences. In Han and Baldwin (2011), word similarity and context is used during lexicon lookup. Cook and Stevenson (2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factors in social media writing. Others, treating the normalization task as a machine translation (MT) problem which tries to translate from an ill formed language to a conventional one, form also another important group. For example the papers from Kaufmann and Kalita (2010), Pennell and Liu (2011), Aw et al. (2006) and Beaufort et al. (2010) may be collected under this group. Since the emergence of social medi</context>
</contexts>
<marker>Liu, Weng, Jiang, 2012</marker>
<rawString>Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-coverage normalization system for social media language. In Proc. of the 50th ACL, pages 1035– 1044, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thu-Trang Thi Nguyen</author>
<author>Thanh Thi Pham</author>
<author>Do-Dat Tran</author>
</authors>
<title>A method for vietnamese text normalization to improve the quality of speech synthesis.</title>
<date>2010</date>
<contexts>
<context position="6834" citStr="Nguyen et al. (2010)" startWordPosition="1086" endWordPosition="1089">ea and the earlier ones generally work for the text normalization in TTS (text-to-speech), ASR (automatic speech recognition) systems or SMS messages. Social media normalization poses new challenges on top of these, for example Twitter statuses contains mentions (@user name), hashtags (#topic), variant number of emoticons ( e.g. :) :@ &lt;3 @&gt;– ) and special keywords (RT - retweet, DM - direct message etc.). Although very rare, there are also some studies on languages other than English and these are mostly for speech recognition and SMS messages , e.g. Panchapagesan et al. (2004) for Hindi TTS, Nguyen et al. (2010) for Vietnamese TTS, Jia et al. (2008) for Mandarin TTS, Khan and Karim (2012) for Urdu SMS. To the best of our knowledge, our study is the first attempt for the normalization of social media data for morphologically rich languages. 3 Morphologically Rich Languages Morphologically rich languages such as Turkish, Finnish, Korean, Hebrew etc., pose significant challenges for natural language processing tasks (Tsarfaty et al., 2013; Sarikaya et al., 2009). As stated previously, the highly productive morphology of these languages results in a very large number of word forms from a given stem. Tabl</context>
</contexts>
<marker>Nguyen, Pham, Tran, 2010</marker>
<rawString>Thu-Trang Thi Nguyen, Thanh Thi Pham, and Do-Dat Tran. 2010. A method for vietnamese text normalization to improve the quality of speech synthesis.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proc. of the 2010 Symposium on Information and Communication Technology, SoICT ’10,</booktitle>
<pages>78--85</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker></marker>
<rawString>In Proc. of the 2010 Symposium on Information and Communication Technology, SoICT ’10, pages 78– 85, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Giza++: Training of statistical translation models.</title>
<date>2000</date>
<contexts>
<context position="25755" citStr="Och and Ney, 2000" startWordPosition="4190" endWordPosition="4193">3 different models with our proposed system: Model 1 (MsWord) is the model where we use an api for getting the MsWord Turkish spelling suggestions. Although this is not a tool developed for normalization purposes we wanted to see its success on our data sets. We accepted the top best suggestion as the normalized version for the input tokens. Model 2 (Zemberek) (Akın and Akın, 2007) is also an open source spelling corrector for Turkish. Model 3 (Lookup Table) is a model that we developed with the aim of creating a baseline lookup approach for comparison. For this purpose, we first used GIZA++ (Och and Ney, 2000) in order to automatically align the normalized tweets (using the 4,049 tweets’ data set presented in Section 5.1) and created a lookup table with the produced aligned token sequences. We then used this lookup table to check for the existence of each ill formed word and get its normalized counterpart. 6 Experimental Results Table 7 and Table 8 gives the results of the ill formed word detection for different systems for the validation set and the test set consecutively. In these experiments, we do not provide the results of the “Lookup Table” model since the ill formed detection part of it is e</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Giza++: Training of statistical translation models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Panchapagesan</author>
<author>Partha Pratim Talukdar</author>
<author>N Sridhar Krishna</author>
<author>Kalika Bali</author>
<author>AG Ramakrishnan</author>
</authors>
<title>Hindi text normalization.</title>
<date>2004</date>
<booktitle>In Fifth International Conference on Knowledge Based Computer Systems (KBCS),</booktitle>
<pages>pages</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6798" citStr="Panchapagesan et al. (2004)" startWordPosition="1079" endWordPosition="1082">y the latest studies are focused on this area and the earlier ones generally work for the text normalization in TTS (text-to-speech), ASR (automatic speech recognition) systems or SMS messages. Social media normalization poses new challenges on top of these, for example Twitter statuses contains mentions (@user name), hashtags (#topic), variant number of emoticons ( e.g. :) :@ &lt;3 @&gt;– ) and special keywords (RT - retweet, DM - direct message etc.). Although very rare, there are also some studies on languages other than English and these are mostly for speech recognition and SMS messages , e.g. Panchapagesan et al. (2004) for Hindi TTS, Nguyen et al. (2010) for Vietnamese TTS, Jia et al. (2008) for Mandarin TTS, Khan and Karim (2012) for Urdu SMS. To the best of our knowledge, our study is the first attempt for the normalization of social media data for morphologically rich languages. 3 Morphologically Rich Languages Morphologically rich languages such as Turkish, Finnish, Korean, Hebrew etc., pose significant challenges for natural language processing tasks (Tsarfaty et al., 2013; Sarikaya et al., 2009). As stated previously, the highly productive morphology of these languages results in a very large number o</context>
</contexts>
<marker>Panchapagesan, Talukdar, Krishna, Bali, Ramakrishnan, 2004</marker>
<rawString>K Panchapagesan, Partha Pratim Talukdar, N Sridhar Krishna, Kalika Bali, and AG Ramakrishnan. 2004. Hindi text normalization. In Fifth International Conference on Knowledge Based Computer Systems (KBCS), pages 19–22. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deana Pennell</author>
<author>Yang Liu</author>
</authors>
<title>A character-level machine translation approach for normalization of sms abbreviations.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>974--982</pages>
<contexts>
<context position="6035" citStr="Pennell and Liu (2011)" startWordPosition="947" endWordPosition="950">up. Cook and Stevenson (2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factors in social media writing. Others, treating the normalization task as a machine translation (MT) problem which tries to translate from an ill formed language to a conventional one, form also another important group. For example the papers from Kaufmann and Kalita (2010), Pennell and Liu (2011), Aw et al. (2006) and Beaufort et al. (2010) may be collected under this group. Since the emergence of social media is very recent, only the latest studies are focused on this area and the earlier ones generally work for the text normalization in TTS (text-to-speech), ASR (automatic speech recognition) systems or SMS messages. Social media normalization poses new challenges on top of these, for example Twitter statuses contains mentions (@user name), hashtags (#topic), variant number of emoticons ( e.g. :) :@ &lt;3 @&gt;– ) and special keywords (RT - retweet, DM - direct message etc.). Although ver</context>
</contexts>
<marker>Pennell, Liu, 2011</marker>
<rawString>Deana Pennell and Yang Liu. 2011. A character-level machine translation approach for normalization of sms abbreviations. In IJCNLP, pages 974–982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhammet S¸ahin</author>
<author>Umut Sulubacak</author>
<author>G¨uls¸en Eryi˘git</author>
</authors>
<title>Redefinition of turkish morphology using flag diacritics.</title>
<date>2013</date>
<booktitle>In Proc. of The Tenth Symposium on Natural Language Processing (SNLP2013),</booktitle>
<location>Phuket, Thailand,</location>
<marker>S¸ahin, Sulubacak, Eryi˘git, 2013</marker>
<rawString>Muhammet S¸ahin, Umut Sulubacak, and G¨uls¸en Eryi˘git. 2013. Redefinition of turkish morphology using flag diacritics. In Proc. of The Tenth Symposium on Natural Language Processing (SNLP2013), Phuket, Thailand, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Has¸im Sak</author>
<author>Tunga G¨ung¨or</author>
<author>Murat Sarac¸lar</author>
</authors>
<title>Resources for Turkish morphological processing.</title>
<date>2011</date>
<journal>Lang. Resour. Eval.,</journal>
<volume>45</volume>
<issue>2</issue>
<marker>Sak, G¨ung¨or, Sarac¸lar, 2011</marker>
<rawString>Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2011. Resources for Turkish morphological processing. Lang. Resour. Eval., 45(2):249–261, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruhi Sarikaya</author>
<author>Katrin Kirchhoff</author>
<author>Tanja Schultz</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>Introduction to the special issue on processing morphologically rich languages.</title>
<date>2009</date>
<journal>Trans. Audio, Speech</journal>
<volume>17</volume>
<issue>5</issue>
<contexts>
<context position="7290" citStr="Sarikaya et al., 2009" startWordPosition="1157" endWordPosition="1160">es on languages other than English and these are mostly for speech recognition and SMS messages , e.g. Panchapagesan et al. (2004) for Hindi TTS, Nguyen et al. (2010) for Vietnamese TTS, Jia et al. (2008) for Mandarin TTS, Khan and Karim (2012) for Urdu SMS. To the best of our knowledge, our study is the first attempt for the normalization of social media data for morphologically rich languages. 3 Morphologically Rich Languages Morphologically rich languages such as Turkish, Finnish, Korean, Hebrew etc., pose significant challenges for natural language processing tasks (Tsarfaty et al., 2013; Sarikaya et al., 2009). As stated previously, the highly productive morphology of these languages results in a very large number of word forms from a given stem. Table 2 lists only a few (among hundreds of possible) surface forms for the Turkish stem “ev” (house). Surface form English ev house eve to the house evde at the house evdeki (which is) at the house evdekiler those (who are) at the house evdekilerde at those (who are) Table 2: Some surface forms for “ev” (house) Sarikaya et al. (2009) list the emerging problems as below: 1. increase in dictionary size 2. poor language model probability estimation 3. higher</context>
</contexts>
<marker>Sarikaya, Kirchhoff, Schultz, Hakkani-Tur, 2009</marker>
<rawString>Ruhi Sarikaya, Katrin Kirchhoff, Tanja Schultz, and Dilek Hakkani-Tur. 2009. Introduction to the special issue on processing morphologically rich languages. Trans. Audio, Speech and Lang. Proc., 17(5):861–862, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bilge Say</author>
<author>Deniz Zeyrek</author>
<author>Kemal Oflazer</author>
<author>Umut ¨Ozge</author>
</authors>
<title>Development of a corpus and a treebank for present-day written Turkish.</title>
<date>2002</date>
<booktitle>In Proc. of the Eleventh International Conference of Turkish Linguistics,</booktitle>
<location>Famaguste, Cyprus,</location>
<marker>Say, Zeyrek, Oflazer, ¨Ozge, 2002</marker>
<rawString>Bilge Say, Deniz Zeyrek, Kemal Oflazer, and Umut ¨Ozge. 2002. Development of a corpus and a treebank for present-day written Turkish. In Proc. of the Eleventh International Conference of Turkish Linguistics, Famaguste, Cyprus, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨okhan Akın S¸eker</author>
<author>G¨uls¸en Eryi˘git</author>
</authors>
<title>Initial explorations on using CRFs for Turkish named entity recognition.</title>
<date>2012</date>
<booktitle>In Proc. of COLING 2012,</booktitle>
<location>Mumbai,</location>
<marker>S¸eker, Eryi˘git, 2012</marker>
<rawString>G¨okhan Akın S¸eker and G¨uls¸en Eryi˘git. 2012. Initial explorations on using CRFs for Turkish named entity recognition. In Proc. of COLING 2012, Mumbai, India, 8-15 December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reut Tsarfaty</author>
<author>Djam´e Seddah</author>
<author>Sandra K¨ubler</author>
<author>Joakim Nivre</author>
</authors>
<title>Parsing morphologically rich languages: Introduction to the special issue.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Tsarfaty, Seddah, K¨ubler, Nivre, 2013</marker>
<rawString>Reut Tsarfaty, Djam´e Seddah, Sandra K¨ubler, and Joakim Nivre. 2013. Parsing morphologically rich languages: Introduction to the special issue. Computational Linguistics, 39(1):15–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pidong Wang</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A beamsearch decoder for normalization of social media text with application to machine translation.</title>
<date>2013</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>471--481</pages>
<contexts>
<context position="5141" citStr="Wang and Ng (2013)" startWordPosition="804" endWordPosition="807">imental results and discussions and Section 7 the conclusion. 2 Related Work An important part of the previous studies have taken the normalization task either as a lexicon lookup (together with or without replacement rules) or as a statistical problem. There also exist many studies which use their combination. In these studies, a lexicon lookup is firstly employed for most common usage of slang words, abbreviations etc. and then a machine learning method is employed for the rest. Zhang et al. (2013) uses replacement rules and a graph based model in order to select the best rule combinations. Wang and Ng (2013) uses a beam search decoder. Hassan and Menezes (2013) propose an unsupervised approach which uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences. In Han and Baldwin (2011), word similarity and context is used during lexicon lookup. Cook and Stevenson (2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in order to use them in the training phase of a noisy channel model. Eisenstein (2013) analyzes phonological factor</context>
</contexts>
<marker>Wang, Ng, 2013</marker>
<rawString>Pidong Wang and Hwee Tou Ng. 2013. A beamsearch decoder for normalization of social media text with application to machine translation. In Proc. of NAACL-HLT, pages 471–481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Y¨uret</author>
<author>Michael de la Maza</author>
</authors>
<title>The greedy prepend algorithm for decision list induction.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st international conference on Computer and Information Sciences, ISCIS’06,</booktitle>
<pages>37--46</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Y¨uret, Maza, 2006</marker>
<rawString>Deniz Y¨uret and Michael de la Maza. 2006. The greedy prepend algorithm for decision list induction. In Proc. of the 21st international conference on Computer and Information Sciences, ISCIS’06, pages 37–46, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Congle Zhang</author>
<author>Tyler Baldwin</author>
<author>Howard Ho</author>
<author>Benny Kimelfeld</author>
<author>Yunyao Li</author>
</authors>
<title>Adaptive parsercentric text normalization.</title>
<date>2013</date>
<booktitle>In Proc. of the 51st ACL,</booktitle>
<pages>1159--1168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5028" citStr="Zhang et al. (2013)" startWordPosition="783" endWordPosition="786">ges, Section 4 presents our normalization approach and Section 5 the experimental setup, Section 6 gives our experimental results and discussions and Section 7 the conclusion. 2 Related Work An important part of the previous studies have taken the normalization task either as a lexicon lookup (together with or without replacement rules) or as a statistical problem. There also exist many studies which use their combination. In these studies, a lexicon lookup is firstly employed for most common usage of slang words, abbreviations etc. and then a machine learning method is employed for the rest. Zhang et al. (2013) uses replacement rules and a graph based model in order to select the best rule combinations. Wang and Ng (2013) uses a beam search decoder. Hassan and Menezes (2013) propose an unsupervised approach which uses Random Walks on a contextual similarity bipartite graph constructed from n-gram sequences. In Han and Baldwin (2011), word similarity and context is used during lexicon lookup. Cook and Stevenson (2009) uses an unsupervised noisy channel model. Clark and Araki (2011) makes dictionary lookup. Liu et al. (2012) uses a unified letter transformation to generate possible ill formed words in</context>
</contexts>
<marker>Zhang, Baldwin, Ho, Kimelfeld, Li, 2013</marker>
<rawString>Congle Zhang, Tyler Baldwin, Howard Ho, Benny Kimelfeld, and Yunyao Li. 2013. Adaptive parsercentric text normalization. In Proc. of the 51st ACL, pages 1159–1168, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
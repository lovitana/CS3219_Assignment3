<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000281">
<title confidence="0.979444">
Improved Pattern Learning for Bootstrapped Entity Extraction
</title>
<author confidence="0.995063">
Sonal Gupta Christopher D. Manning
</author>
<affiliation confidence="0.987145">
Department of Computer Science
Stanford University
</affiliation>
<email confidence="0.994064">
{sonal, manning}@cs.stanford.edu
</email>
<sectionHeader confidence="0.994689" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997213">
Bootstrapped pattern learning for entity
extraction usually starts with seed entities
and iteratively learns patterns and entities
from unlabeled text. Patterns are scored
by their ability to extract more positive en-
tities and less negative entities. A prob-
lem is that due to the lack of labeled data,
unlabeled entities are either assumed to be
negative or are ignored by the existing pat-
tern scoring measures. In this paper, we
improve pattern scoring by predicting the
labels of unlabeled entities. We use var-
ious unsupervised features based on con-
trasting domain-specific and general text,
and exploiting distributional similarity and
edit distances to learned entities. Our
system outperforms existing pattern scor-
ing algorithms for extracting drug-and-
treatment entities from four medical fo-
rums.
</bodyText>
<sectionHeader confidence="0.998785" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999986529411765">
This paper considers the problem of building ef-
fective entity extractors for custom entity types
from specialized domain corpora. We approach
the problem by learning rules bootstrapped us-
ing seed sets of entities. Though entity extrac-
tion using machine learning is common in aca-
demic research, rule-based systems dominate in
commercial use (Chiticariu et al., 2013), mainly
because rules are effective, interpretable, and are
easy to customize by non-experts to cope with er-
rors. They also have been shown to perform bet-
ter than state-of-the-art machine learning methods
on some specialized domains (Nallapati and Man-
ning, 2008; Gupta and Manning, 2014a). In ad-
dition, building supervised machine learning sys-
tems for a reasonably large domain-specific cor-
pus would require hand-labeling sufficient data to
</bodyText>
<figure confidence="0.881001833333333">
Seed dictionary for class ‘animal
Text:
I own a cat named Fluffy. I run
dog. I also nap with my pet cat.
Extractions = positive : {dog}, un
Extractions = positive : {dog}, un
</figure>
<figureCaption confidence="0.968267142857143">
Figure 1: An example pattern learning system for
the class ‘animals’ from the text. Pattern 1 and 2
are candidate patterns. Text matched with the pat-
terns is shown in italics and the extracted entities
are shown in bold.
Pattern 1: my pet X
Pattern 2: own a X
</figureCaption>
<bodyText confidence="0.989657">
train a model, which can be costly and time con-
suming. Bootstrapped machine-learned rules can
make extraction easier and more efficient on such
a corpus.
In a bootstrapped rule-based entity learning
system, seed dictionaries and/or patterns provide
weak supervision to label data. The system itera-
tively learns new entities belonging to a specific
class from unlabeled text (Riloff, 1996; Collins
and Singer, 1999). Rules are typically defined
by creating patterns around the entities, such
as lexico-syntactic surface word patterns (Hearst,
1992) and dependency tree patterns (Yangarber
et al., 2000). Patterns are scored by their abil-
ity to extract more positive entities and less neg-
ative entities. Top ranked patterns are used to
extract candidate entities from text. High scor-
ing candidate entities are added to the dictionaries
and are used to generate more candidate patterns
around them. In a supervised setting, the efficacy
of patterns can be judged by their performance
on a fully labeled dataset (Califf and Mooney,
1999; Ciravegna, 2001). In a bootstrapped sys-
tem, where the data is not fully labeled, existing
systems score patterns by either ignoring the un-
</bodyText>
<page confidence="0.871157">
98
</page>
<note confidence="0.6985495">
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 98–108,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999936333333333">
labeled entities or assuming them to be negative.
However, these scoring schemes cannot differenti-
ate between patterns that extract good versus bad
unlabeled entities. The problem is similar to the
closed world assumption in distantly supervised
information extraction systems, when all proposi-
tions missing from a knowledge base are consid-
ered false (Ritter et al., 2013; Xu et al., 2013).
Predicting labels of unlabeled entities can im-
prove scoring patterns. Consider the example
shown in Figure 1. Current pattern learning sys-
tems would score both patterns equally. However,
features like distributional similarity can predict
‘cat’ to be closer to {dog} than ‘car’, and a pat-
tern learning system can use that information to
rank ‘Pattern 1’ higher than ‘Pattern 2’.
In this paper, we work on bootstrapping en-
tity extraction using seed sets of entities and an
unlabeled text corpus. We improve the scoring
of patterns for an entity class by defining a pat-
tern’s score by the number of positive entities it
extracts and the ratio of number of positive entities
to expected number of negative entities it extracts.
Our main contribution is introducing the expected
number of negative entities in pattern scoring – we
predict probabilities of unlabeled entities belong-
ing to the negative class. We estimate an unla-
beled entity’s negative class probability by averag-
ing probabilities from various unsupervised class
predictors, such as distributional similarity, string
edit distances from learned entities, and TF-IDF
scores. Our system performs significantly better
than existing pattern scoring measures for extract-
ing drug-and-treatment entities from four medi-
cal forums on MedHelp1, a user health discussion
website.
We release the code for the systems described in
this paper at http://nlp.stanford.edu/
software/patternslearning.shtml.
We also release a visualization tool, described
in Gupta and Manning (2014b), that visualizes
and compares output of multiple pattern-based
entity extraction systems. It can be downloaded at
http://nlp.stanford.edu/software/
patternviz.shtml.
</bodyText>
<sectionHeader confidence="0.999845" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.992694333333333">
Rule based learning has been a topic of interest
for many years. Patwardhan (2010) gives a good
overview of the research in the field. Rule learn-
</bodyText>
<footnote confidence="0.953097">
1www.medhelp.org
</footnote>
<bodyText confidence="0.999946392156863">
ing systems differ in how they create rules, score
them, and score the entities they extract. Here, we
mainly discuss the rule scoring part of the previous
entity extraction research.
The pioneering work by Hearst (1992) used
hand written rules to automatically generate
more rules that were manually evaluated to
extract hypernym-hyponym pairs from text.
Other supervised systems like SRV (Freitag,
1998), SLIPPER (Cohen and Singer, 1999),
(LP)2 (Ciravegna, 2001), and RAPIER (Califf
and Mooney, 1999) used a fully labeled corpus to
either create or score rules.
Riloff (1996) used a set of seed entities to
bootstrap learning of rules for entity extraction
from unlabeled text. She scored a rule by a
weighted conditional probability measure esti-
mated by counting the number of positive entities
among all the entities extracted by the rule. Thelen
and Riloff (2002) extended the above bootstrap-
ping algorithm for multi-class learning. Yangar-
ber et al. (2002) and Lin et al. (2003) used a com-
bination of accuracy and confidence of a pattern
for multiclass entity learning, where the accuracy
measure ignored unlabeled entities and the con-
fidence measure treated them as negative. Gupta
and Manning (2014a) used the ratio of scaled fre-
quencies of positive entities among all extracted
entities. None of the above measures predict labels
of unlabeled entities to score patterns. Our sys-
tem outperforms them in our experiments. Steven-
son and Greenwood (2005) used Wordnet to assess
patterns, which is not feasible for domains that
have low coverage in Wordnet, such as medical
data.
More recently, open information extraction
systems have garnered attention. They focus
on extracting entities and relations from the
web. KnowItAll’s entity extraction from the
web (Downey et al., 2004; Etzioni et al., 2005)
used components such as list extractors, generic
and domain specific pattern learning, and subclass
learning. They learned domain-specific patterns
using a seed set and scored them by ignoring un-
labeled entities. One of our baselines is similar
to their domain-specific pattern learning compo-
nent. Carlson et al. (2010) learned multiple se-
mantic types using coupled semi-supervised train-
ing from web-scale data, which is not feasible for
all datasets and entity learning tasks. They as-
sessed patterns by their precision, assuming unla-
</bodyText>
<page confidence="0.997676">
99
</page>
<bodyText confidence="0.999865666666667">
beled entities to be negative; one of our baselines
is similar to their pattern assessment method.
Other open information extraction systems like
ReVerb (Fader et al., 2011) and OLLIE (Mausam
et al., 2012) are mainly geared towards generic,
domain-independent relation extractors for web
data. We tested learning an entity extractor for a
given class using ReVerb. We labeled the binary
and unary ReVerb extractions using the class seed
entities and retrained its confidence function, with
poor results. Poon and Domingos (2010) found
a similar result for inducing a probabilistic ontol-
ogy: an open information extraction system ex-
tracted low accuracy relational triples on a small
corpus.
In this paper, we use features such as distribu-
tional similarity and edit distances from learned
entities to score patterns. Similar measures have
been used before but for learning entities, label-
ing semantic classes, or for reducing noise in seed
sets (Pantel and Ravichandran, 2004; McIntosh
and Curran, 2009). Measures for improving en-
tity learning can be used alongside ours since we
focus on scoring candidate patterns.
</bodyText>
<sectionHeader confidence="0.993084" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999338">
We use lexico-syntactic surface word patterns to
extract entities from unlabeled text starting with
seed dictionaries of entities for multiple classes.
For ease of exposition, we present the approach
below for learning entities for one class C. It can
easily be generalized to multiple classes. We re-
fer to entities belonging to C as positive and en-
tities belonging to all other classes as negative.
The bootstrapping process involves the following
steps, iteratively performed until no more patterns
or entities can be learned.
</bodyText>
<listItem confidence="0.973859411764706">
1. Labeling data and creating patterns: The text
is labeled using the class dictionaries, start-
ing with the seed dictionaries in the first iter-
ation. A phrase matching a dictionary phrase
is labeled with the dictionary’s class. Patterns
are then created using the context around the
positively labeled entities to create candidate
patterns for C.
2. Scoring Patterns: Candidate patterns are
scored using a pattern scoring measure and
the top ones are added to the list of learned
patterns for C.
3. Learning entities: Learned patterns for the
class are applied to the text to extract candi-
date entities. An entity scorer ranks the can-
didate entities and adds the top entities to C’s
dictionary.
</listItem>
<bodyText confidence="0.9998776">
The success of bootstrapped pattern learning
methods crucially depends on the effectiveness of
the pattern scorer and the entity scorer. Here we
focus on improving the pattern scoring measure
(Step 2 above).
</bodyText>
<subsectionHeader confidence="0.999896">
3.1 Creating Patterns
</subsectionHeader>
<bodyText confidence="0.999847818181818">
Candidate patterns are created using contexts of
words or their lemmas in a window of two to four
words before and after a positively labeled token.
Context words that are labeled with one of the
classes are generalized with that class. The tar-
get term has a part-of-speech (POS) restriction,
which is the POS tag of the labeled token. We
create flexible patterns by ignoring the words {‘a’,
‘an’, ‘the’} and quotation marks when matching
patterns to the text. Some examples of the patterns
are shown in Table 4.
</bodyText>
<subsectionHeader confidence="0.999935">
3.2 Scoring Patterns
</subsectionHeader>
<bodyText confidence="0.999971117647059">
Judging the efficacy of patterns without using a
fully labeled dataset can be challenging because of
two types of failures: 1. penalizing good patterns
that extract good (that is, positive) unlabeled enti-
ties, and 2. giving high scores to bad patterns that
extract bad (that is, negative) unlabeled entities.
Existing systems that assume unlabeled entities as
negative are too conservative in scoring patterns
and suffer from the first problem. Systems that
ignore unlabeled entities can suffer from both the
problems. In this paper, we propose to estimate
the labels of unlabeled entities to more accurately
score the patterns.
For a pattern r, sets Pr, Nr, and Ur denote the
positive, negative, and unlabeled entities extracted
by r, respectively. The pattern score, ps(r) is cal-
culated as
</bodyText>
<equation confidence="0.86062">
ps(r) =  |Nr |+ Ee( |— score(e))
log(|Pr|)
∈U,1 score e
</equation>
<bodyText confidence="0.9975972">
where |. |denotes size of a set. The function
score(e) gives the probability of an entity e be-
longing to C. If e is a common word, score(e) is
0. Otherwise, score(e) is calculated as the aver-
age of five feature scores (explained below), each
</bodyText>
<page confidence="0.956703">
100
</page>
<bodyText confidence="0.985488673469388">
of which give a score between 0 and 1. The fea-
ture scores are calculated using the seed dictio-
naries, learned entities for all labels, Google N-
grams2, and clustering of domain words using dis-
tributional similarity. The log |Pr |term, inspired
from (Riloff, 1996), gives higher scores to patterns
that extract more positive entities. Candidate pat-
terns are ranked by ps(r) and the top patterns are
added to the list of learned patterns.
To calculate score(e), we use features that as-
sess unlabeled entities to be either closer to pos-
itive or negative entities in an unsupervised way.
We motivate our choice of the five features below
with the following insights. If the dataset consists
of informally written text, many unlabeled enti-
ties are spelling mistakes and morphological vari-
ations of labeled entities. We use two edit distance
based features to predict labels for these unlabeled
entities. Second, some unlabeled entities are sub-
strings of multi-word dictionary phrases but do not
necessarily belong to the dictionary’s class. For
example, for learning drug names, the positive dic-
tionary might contain ‘asthma meds’, but ‘asthma’
is negative and might occur in a negative dictio-
nary as ‘asthma disease’. To predict the labels of
entities that are a substring of dictionary phrases,
we use SemOdd, which was used in Gupta and
Manning (2014a) to learn entities. Third, for a
specialized domain, unlabeled entities that com-
monly occur in generic text are more likely to be
negative. We use Google Ngrams (called GN) to
get a fast, non-sparse estimate of the frequency of
entities over a broad range of domains. The above
features do not consider the context in which the
entities occur in text. We use the fifth feature, Dist-
Sim, to exploit contextual information of the la-
beled entities using distributional similarity. The
features are defined as:
Edit distance from positive entities (EDP): This
feature gives a score of 1 if e has low edit
distance to the positive entities. It is com-
puted as maxp∈P,.1(editDist(p,e) &lt; 0.2),
|p|
where 11(c) returns 1 if the condition c is true
and 0 otherwise, |p |is the length of p, and
editDist(p, e) is the Damerau-Levenshtein
string edit distance between p and e.
Edit distance from negative entities (EDN): It is
similar to EDP and gives a score of 1 if e has
</bodyText>
<footnote confidence="0.996395666666667">
2http://storage.googleapis.com/books/
ngrams/books/datasetsv2.html. Accessed Jan-
uary 2008.
</footnote>
<bodyText confidence="0.986691622641509">
high edit distance to the negative entities. It is
computed as 1 − maxn∈N,. 1(editDist(n,e) &lt;
|n|
0.2).
Semantic odds ratio (SemOdd): First, we cal-
culate the ratio of frequency of the entity
term in the positive entities to its frequency in
the negative entities with Laplace smoothing.
The ratio is then normalized using a softmax
function. The feature values for the unlabeled
entities extracted by all the candidate patterns
are then normalized using the min-max func-
tion to scale the values between 0 and 1.3
Google Ngram (GN): We calculate the ratio of
scaled frequency of e in the dataset to the fre-
quency in Google Ngrams. The scaling factor
is to balance the two frequencies and is com-
puted as the ratio of total number of phrases
in the dataset to the total of phrases in Google
Ngrams. The feature values are normalized
in the same way as SemOdd.
Distributional similarity score (DistSim): Words
that occur in similar contexts, such as
‘asthma’ and ‘depression’, are clustered us-
ing distributional similarity. Unlabeled en-
tities that get clustered with positive entities
are given higher score than the ones clustered
with negative entities. To score the clusters,
we learn a logistic regression classifier using
cluster ID as features, and use their weights
as scores for all the entities in those clusters.
The dataset for logistic regression is created
by considering all positively labeled words as
positive and sampling negative and unlabeled
words as negative. The scores for entities are
normalized in the same way as SemOdd and
GN.
Out of feature vocabulary entities for SemOdd,
GN, and DistSim are given a score of 0. We
use a simple way of combining the feature val-
ues: we give equal weights to all features and
average their scores. Features can be combined
using a weighted average by manually tuning the
weights on a development set; we leave it to the
future work. Another way of weighting the fea-
tures is to learn the weights using machine learn-
ing. We experimented with learning weights for
3We do min-max normalization on top of the softmax
normalization because the maximum and minimum value by
softmax might not be close to 1 and 0, respectively. And,
treating the out-of-feature-vocabulary entities same as the
worst scored entities by the feature, that is giving them a score
of 0, performed best on the development dataset.
</bodyText>
<page confidence="0.994491">
101
</page>
<bodyText confidence="0.999993416666667">
the features by training a logistic regression clas-
sifier. We considered all positive words as positive
and randomly sampled negative and unlabeled en-
tities as negative to predict score(e), but it per-
formed worse compared to averaging the scores
on the development dataset. Preliminary investi-
gation suggests that since the classifier was trained
on a dataset heuristically labeled using the seed
dictionaries, it was too noisy for the classifier to
learn accurate weights. Presumably, the classifier
also suffered from the closed world assumption of
treating unlabeled examples as negative.
</bodyText>
<subsectionHeader confidence="0.999506">
3.3 Learning Entities
</subsectionHeader>
<bodyText confidence="0.999599818181818">
We apply the learned patterns to the text and
extract candidate entities. We discard common
words, negative entities, and those containing non-
alphanumeric characters from the set. The rest are
scored by averaging the scores of DistSim, Sem-
Odd, EDO, and EDN features from Section 3.2
and the following features.
Pattern TF-IDF scoring (PTF): For an entity e, it
is calculated as log freqe �r∈R ps(r), where
R is the set of learned patterns that extract e
and freqe is the frequency of e in the cor-
pus. Entities that are extracted by many high
weighted patterns get higher weight. To mit-
igate the effect of many commonly occurring
entities also getting extracted by several pat-
terns, we normalize the feature value with the
log of the entity’s frequency. The values are
normalized in the same way as DistSim and
SemOdd.
Domain N-gram TF-IDF (DN): This feature
gives higher scores to entities that are more
prevalent in the corpus compared to the gen-
eral domain. For example, to learn enti-
ties about a specific disease from a disease-
related corpus, the feature favors entities re-
lated to the disease over generic medical en-
tities. It is calculated in the same way as GN
except the frequency is computed in the n-
grams of the generic domain text.
Including GN in the phrase scoring features or
including DN in the pattern scoring features did
not perform well on the development set in our pi-
lot experiments.
</bodyText>
<sectionHeader confidence="0.998984" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.927774">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999283976190476">
We evaluate our system on extracting drug-and-
treatment (DT) entities in sentences from four fo-
rums on the MedHelp user health discussion web-
site: 1. Acne, 2. Adult Type II Diabetes (called
Diabetes), 3. Ear Nose &amp; Throat (called ENT),
and 4. Asthma. The forums have discussion
threads by users concerning health related prob-
lems and treatments. The number of sentences
in each forum are: 215,623 in ENT, 39,637 in
Asthma, 63,355 in Diabetes, and 65,595 in Acne.
We used Asthma as the development forum for
feature engineering and parameter tuning. Simi-
lar to Gupta and Manning (2014a), a DT entity is
defined as a pharmaceutical drug, or any treatment
or intervention mentioned that may help a symp-
tom or a condition. It includes surgeries, lifestyle
changes, alternative treatments, home remedies,
and components of daily care and management of
a disease, but does not include diagnostic tests and
devices. More information is in the supplemen-
tal material. A few example sentences from the
dataset are below.
I plan to start cinnamon and holy basil - known
to lower glucose in many people.
She gave me albuteral and symbicort (plus
some hayfever meds and asked me to use the
peak flow meter.
My sinus infections were treated electrically,
with high voltage million volt electricity, which
solved the problem, but the treatment is not
FDA approved and generally unavailable, except
under experimental treatment protocols.
In these sentences, ‘cinanmon’, ‘holy basil’, ‘al-
buteral’, ‘symbicort’, ‘meds’, ‘high voltage mil-
lion volt electricity’, and ‘treatment’ are DT enti-
ties.
We used entities from the following classes as
negative: symptoms and conditions (SC), medi-
cal specialists, body parts, and common tempo-
ral nouns to remove dates and dosage informa-
tion. We used the DT and SC seed dictionaries
from Gupta and Manning (2014a).4 The lists of
</bodyText>
<footnote confidence="0.718553111111111">
4The DT seed dictionary (36,091 phrases) and SC seed
dictionary (97,211 phrases) were automatically constructed
from various sources on the Internet and expanded using
the OAC Consumer Health Vocabulary (http://www.
consumerhealthvocab.org), which maps medical jar-
gon to everyday phrases and their variants. Both dictionaries
are large because they contain many variants of entities. For
each system, the SC dictionary was further expanded by run-
ning the system with the SC class as positive (considering DT
</footnote>
<page confidence="0.996169">
102
</page>
<bodyText confidence="0.99978245">
body parts and temporal nouns were obtained from
Wordnet (Fellbaum, 1998). The common words
list was created using most common words on the
web and Twitter.5
For evaluation, the first author hand labeled the
learned entities pooled from all systems. A word
was evaluated by querying the word and the fo-
rum name on Google and manually inspecting the
results. More details on the labeling guidelines
are in the Supplement section. Inter annotator
agreement between the annotator and another re-
searcher was computed on 200 randomly sampled
learned entities from each of the Asthma and ENT
forum. The agreement for the entities from the
Asthma forum was 96% and from the ENT forum
was 92.46%. The Cohen’s kappa scores were 0.91
and 0.83, respectively. Most disagreements were
on food items like ‘yogurt’, which are hard to la-
bel. Note that we use the hand labeled entities only
as a test set for evaluation.
</bodyText>
<subsectionHeader confidence="0.979037">
4.2 Baselines
</subsectionHeader>
<bodyText confidence="0.99617016">
As in Section 3, the sets Pr, Nr, and Ur are defined
as the positive, negative, and unlabeled entities ex-
tracted by a pattern r, respectively. The set Ar is
defined as union of all the three sets. We com-
pare our system with the following pattern scoring
algorithms. Candidate entities are scored in the
same way as described in Section 3.3. It is impor-
tant to note that previous works also differ in how
they create patterns, apply patterns, and score en-
tities. Since we focus on only the pattern scoring
aspect, we run experiments that differ in only that
component.
PNOdd: Defined as Pr / Nr , this measure ig-
nores unlabeled entities and is similar to the
domain specific pattern learning component
of Etzioni et al. (2005) since all patterns with
Pr &lt; 2 were discarded (more details in the
next section).
PUNOdd: Defined as Pr /( Ur + Nr ), this
measure treats unlabeled entities as negative
entities.
RlogF: Measure used by Riloff (1996) and
Thelen and Riloff (2002), and calculated
as Rr log Pr , where Rr was defined as
Pr / Ar (labeled RlogF-PUN). It assumed
</bodyText>
<footnote confidence="0.9190145">
and other classes as negative) and adding the top 50 words ex-
tracted by the top 300 patterns to the SC class dictionary. This
helps in adding corpus specific SC words to the dictionary.
5We used top 10,000 words from Google N-grams and top
5,000 words from Twitter (www.twitter.com), accessed
from May 19 to 25, 2012.
</footnote>
<bodyText confidence="0.950961333333333">
unlabeled entities as negative entities. We
also compare with a variant that ignores the
unlabeled entities, that is by defining Rr as
</bodyText>
<equation confidence="0.376648">
Pr /( Pr + Nr ) (labeled RlogF-PN).
</equation>
<bodyText confidence="0.935876636363636">
Yangarber02: This measure from Yangarber et
al. (2002) calculated two scores, accr =
Pr / Nr and confr = ( Pr / Ar )log Pr .
Patterns with accr less than a threshold were
discarded and the rest were ranked using
confr. We empirically determined that a
threshold of 0.8 performed best on the devel-
opment forum.
Lin03: A measure proposed in Lin et al. (2003),
it was similar to Yangarber02, except confr
was defined as log Pr ( Pr − Nr )/ Ar .
In essence, it discards a pattern if it extracts
more negative entities than positive entities.
SqrtRatioAll: This pattern scoring method was
used in Gupta and Manning (2014a) and
defined as � \Ifreqk/ � Vl freqj,
k∈P,. j∈A,.
where freqi is the number of times entity
i is extracted by r. Sublinear scaling of
the term-frequency prevents high frequency
words from overshadowing the contribution
of low frequency words.
</bodyText>
<subsectionHeader confidence="0.997492">
4.3 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.998988181818182">
We used the same experimental setup for our sys-
tem and the baselines. When matching phrases
from a seed dictionary to text, a phrase is la-
beled with the dictionary’s class if the sequence of
phrase words or their lemmas match with the se-
quence of words of a dictionary phrase. Since our
corpora are from online discussion forums, they
have many spelling mistakes and morphological
variations of entities. To deal with the variations,
we do fuzzy matching of words – if two words are
one edit distance away and are more than 6 char-
acters long, then they are considered a match.
We used Stanford TokensRegex (Chang and
Manning, 2014) to create and apply surface word
patterns to text, and used the Stanford Part-of-
Speech (POS) tagger (Toutanova et al., 2003) to
find POS tags of tokens and lemmatize them.
When creating patterns, we discarded patterns
whose left or right context was 1 or 2 stop words to
avoid generating low precision patterns.6 In each
iteration, we learned a maximum 20 patterns with
ps(r) &gt; θr and maximum 10 words with score &gt;
</bodyText>
<footnote confidence="0.949239">
6Three or more stop words resulted in some good patterns
like ‘I am on X’. Our stop words list consists of punctuation
marks and around 200 very common English words.
</footnote>
<page confidence="0.998182">
103
</page>
<figure confidence="0.99957930882353">
ASTHMA ENT
Precision
0.95
0.85
0.75
0.9
0.8
1
OurSystem
RlogF-PUN
Yangarber02
SqrtAllRatio
Lin03
PUNOdd
Precision
0.96
0.94
0.92
0.88
0.86
0.84
0.82
0.78
0.76
0.9
0.8
OurSystem
RlogF-PUN
Yangarber02
SqrtAllRatio
Lin03
PUNOdd
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall (out of 221 correct entities)
ACNE
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall (out of 624 correct entities)
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Recall (out of 645 correct entities)
DIABETES
0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Recall (out of 118 correct entities)
OurSystem
RlogF-PUN
Yangarber02
SqrtAllRatio
Lin03
PUNOdd
OurSystem
RlogF-PUN
Yangarber02
SqrtAllRatio
Lin03
PUNOdd
Precision 1
0.95
0.9
0.85
0.8
0.75
Precision 0.9
0.88
0.86
0.84
0.82
0.8
0.78
0.76
</figure>
<figureCaption confidence="0.9607925">
Figure 2: Precision vs. Recall curves of our system and the baselines for the four forums. Rlog-PN and
PNOdd are not shown to improve clarity.
</figureCaption>
<bodyText confidence="0.9996552">
0.2. The initial value of 0, was 1.0, which was re-
duced to 0.8 × 0, whenever the system did not ex-
tract any more patterns and words. We discarded
patterns that extracted less than 2 positive entities.
We selected these parameters by their performance
on the development forum.
For calculating the DistSim feature used for
scoring patterns and entities, we clustered all of
MedHelp’s forum data into 1000 clusters using the
Brown clustering algorithm (Brown et al., 1992;
Liang, 2005).7 For calculating the Domain Ngram
feature for scoring entities, we used n-grams from
all user forums in MedHelp as the domain n-
grams.
We evaluate systems by their precision and re-
call in each iteration. Precision is defined as the
fraction of correct entities among the entities ex-
tracted. We stopped learning entities for a sys-
tem if the precision dropped below 75% to extract
entities with reasonably high precision. Recall is
defined as the fraction of correct entities among
the total unique correct entities pooled from all
systems while maintaining the precision ≥ 75%.
Note that true recall is very hard to compute since
our dataset is unlabeled. To compare the systems
</bodyText>
<footnote confidence="0.973533333333333">
7The data consisted of around 4 million tokens. Words
that occurred less than 50 times were discarded, which re-
sulted in 50353 unique words.
</footnote>
<bodyText confidence="0.8136065">
overall, we calculate the area under the precision-
recall curves (AUC-PR).
</bodyText>
<table confidence="0.999319111111111">
System Asthma ENT Diabetes Acne
OurSystem 68.36 60.71 67.62 68.01
PNOdd 51.62 50.31 05.91 58.45
PUNOdd 42.42 30.44 36.11 58.38
RlogF-PUN 56.13 54.11 48.70 57.04
RlogF-PN 53.46 52.84 16.59 62.35
SqrtRatioAll 41.49 40.44 35.47 46.46
Yangarber02 53.76 48.46 41.45 59.85
Lin03 54.58 47.98 56.15 60.79
</table>
<tableCaption confidence="0.974574">
Table 1: Area under Precision-Recall curves of the
systems.
</tableCaption>
<subsectionHeader confidence="0.934267">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999311454545455">
Figure 2 plots the precision and recall of systems.8
Table 1 shows AUC-PR scores for all systems.
RlogF-PN and PNOdd have low value for Dia-
betes because they learned generic patterns in ini-
tial iteration, which led them to learn incorrect en-
tities. Overall our system performed significantly
better than existing systems. All systems extract
more entities for Acne and ENT because different
drugs and treatments are more prevalent in these
forums. Diabetes and Asthma have more inter-
ventions and lifestyle changes that are harder to
</bodyText>
<footnote confidence="0.9312725">
8We do not show plots of PNOdd and RlogF-PN to im-
prove clarity. They performed similarly to other baselines.
</footnote>
<page confidence="0.987299">
104
</page>
<table confidence="0.999887714285714">
Feature Asthma ENT Diabetes Acne
All Features 68.36 60.71 67.62 68.01
EDP 68.66 59.07 60.03 65.15
EDN 59.39 59.21 16.75 65.96
SemOdd 67.07 58.41 60.51 65.04
GN 57.52 59.53 48.76 68.61
DistSim 64.87 59.05 71.11 69.48
</table>
<tableCaption confidence="0.9911835">
Table 2: Individual feature effectiveness: Area un-
der Precision-Recall curves when our system uses
individual features during pattern scoring. Other
features are still used for entity scoring.
</tableCaption>
<table confidence="0.999915">
Feature Asthma ENT Diabetes Acne
All Features 68.36 60.71 67.62 68.01
minusEDP 66.29 60.45 69.84 69.46
minusEDN 67.19 60.39 69.89 67.57
minusGN 65.53 60.33 66.07 67.28
minusSemOdd 66.66 60.76 70.79 68.25
minusDistSim 66.10 60.58 66.59 67.85
</table>
<tableCaption confidence="0.9423125">
Table 3: Feature ablation study: Area under
Precision-Recall curves when individual features
</tableCaption>
<bodyText confidence="0.985959">
are removed from our system during pattern scor-
ing. The feature is still used for entity scoring.
extract.
To compare the effectiveness of each feature in
our system, Table 2 shows the AUC-PR values
when each feature was individually used for pat-
tern scoring (other features were still used to learn
entities). EDP and DistSim were strong predictors
of labels of unlabeled entities because many good
unlabeled entities were spelling mistakes of DT
entities and occurred in similar context as them.
Table 3 shows the AUC-PR values when each fea-
ture was removed from the set of features used to
score patterns (the feature was still used for learn-
ing entities). Removing GN and DistSim reduced
the AUC-PR scores for all forums.
Table 4 shows some examples of patterns and
the entities they extracted along with their labels
when the pattern was learned. We learned the first
pattern because ‘pinacillin’ has low edit distance
from the positive entity ‘penicillin’. Similarly, we
scored the second pattern higher than the base-
line because ‘desoidne’ is a typo of the positive
entity ‘desonide’. Note that the seed dictionaries
are noisy – the entity ‘metro’, part of the positive
entity ‘metrogel’, was falsely considered a neg-
ative entity because it was in the common web
words list. Our system learned the third pattern
for two reasons: ‘inhaler’, ‘inhalers’, and ‘hfa’ oc-
curred frequently as sub-phrases in the DT dictio-
nary, and they were clustered with positive enti-
</bodyText>
<table confidence="0.991362">
Our System RlogF-PUN
low dose of X* mg of X
mg of X treat with X
X 10 mg take DT and X
she prescribe X be take X
X 500 mg she prescribe X
be take DT and X* put on X
ent put I on X* stop take X
DT ( like X:NN i be prescribe X
like DT and X have be take X
then prescribe X* tell I to take X
</table>
<tableCaption confidence="0.971303">
Table 5: Top 10 (simplified) patterns learned by
</tableCaption>
<bodyText confidence="0.987822090909091">
our system and RlogF-PUN from the ENT forum.
An asterisk denotes that the pattern was never
learned by the other system. X is the target word.
ties by distributional similarity. Since RlogF-PUN
does not distinguish between unlabeled and nega-
tive entities, it is does not learn the pattern. Table 5
shows top 10 patterns learned for the ENT forum
by our system and RlogF-PUN, the best perform-
ing baseline for the forum. Our system preferred
to learn patterns with longer contexts, which are
usually higher precision, first.
</bodyText>
<sectionHeader confidence="0.996637" genericHeader="conclusions">
5 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.999884833333333">
Our system extracted entities with higher preci-
sion and recall than other existing systems. How-
ever, learning entities from an informal text corpus
that is partially labeled from seed entities presents
some challenges. Our system made mistakes pri-
marily due to three reasons. One, it sometimes
extracted typos of negative entities that were not
easily predictable by the edit distance measures,
such as ‘knowwhere’. Second, patterns that ex-
tracted many good but some bad unlabeled en-
tities got high scores because of the good unla-
beled entities. However, the bad unlabeled enti-
ties extracted by the highly weighted patterns were
scored high by the PTF feature during the entity
scoring phase, leading to extraction of the bad en-
tities. Better features to predict negative entities
and robust text normalization would help mitigate
both the problems. Third, we used automatically
constructed seed dictionaries that were not dataset
specific, which led to incorrectly labeling of some
entities (for example, ‘metro’ as negative in Ta-
ble 4). Reducing noise in the dictionaries would
increase precision and recall.
In this paper, the features are weighted equally
</bodyText>
<page confidence="0.996947">
105
</page>
<table confidence="0.994775">
Forum Pattern Positive entities Negative Unlabeled Our Baseline
System
ENT he give I more X antibiotics, steroid, antibiotic pinacillin 68 NA
(RlogF-PUN)
Acne topical DT ( X prednisone, clindamycin, differin, metro desoidne 149 231
benzoyl peroxide, tretinoin, metro- (RlogF-PN)
gel
Asthma i be put on X cortisone, prednisone, asmanex, ad- inhaler, 8 NA
vair, augmentin, bypass, nebulizer, inhalers, (RlogF-PUN)
xolair, steroids, prilosec hfa
</table>
<tableCaption confidence="0.960777">
Table 4: Example patterns and the entities extracted by them, along with the rank at which the pattern
</tableCaption>
<bodyText confidence="0.999369425">
was added to the list of learned patterns. NA means that the system never learned the pattern. Baseline
refers to the best performing baseline system on the forum. The patterns have been simplified to show
just the sequence of lemmas. X refers to the target entity; all of them in these examples had noun POS
restriction. Terms that have already been identified as the positive class were generalized to their class
DT.
by taking the average of the feature scores. One
area of future work is to learn weights using
more sophisticated techniques; in pilot experi-
ments, learning a logistic regression classifier on
heuristically labeled data did not work well for ei-
ther pattern scoring or entity scoring.
One limitation of our system and evaluation is
that we learned single word entities, since calcu-
lating some features for multi-word phrases is not
straightforward. For example, word clusters using
distributional similarity were constructed for sin-
gle words. Our future work includes expanding
the features to evaluate multi-word phrases. An-
other avenue for future work is to use our pat-
tern scoring method for learning other kinds of
rules, such as dependency patterns, and in differ-
ent kinds of systems, such as hybrid entity learn-
ing systems (Etzioni et al., 2005; Carlson et al.,
2010). In addition, we did not explicitly address
the problem of semantic drift (Curran et al., 2007)
in this paper. In theory, learning better patterns
would help lessen the problem; we plan to investi-
gate this further.
In conclusion, we show that predicting the la-
bels of unlabeled entities in the pattern scorer of a
bootstrapped entity extraction system significantly
improves precision and recall of learned entities.
Our experiments demonstrate the importance of
having models that contrast domain-specific and
general domain text, and the usefulness of features
that allow spelling variations when dealing with
informal texts. Our pattern scorer outperforms ex-
isting pattern scoring methods for learning drug-
and-treatment entities from four medical web fo-
rums.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999942833333333">
We thank Diana MacLean for labeling the test data
for calculating the inter-annotator agreement. We
are also grateful to Gabor Angeli, Angel Chang,
Manolis Savva, and the anonymous reviewers for
their useful feedback. We thank MedHelp for
sharing their anonymized data with us.
</bodyText>
<sectionHeader confidence="0.997156" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998564481481481">
Peter F. Brown, Peter V. deSouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18:467–479.
Mary Elaine Califf and Raymond J. Mooney. 1999.
Relational learning of pattern-match rules for in-
formation extraction. In Proceedings of the 16th
National Conference on Artificial Intelligence and
the 11th Innovative Applications of Artificial Intelli-
gence Conference, AAAI-IAAI ’99, pages 328–334.
Andrew Carlson, Justin Betteridge, Richard C. Wang,
Estevam R. Hruschka, Jr., and Tom M. Mitchell.
2010. Coupled semi-supervised learning for infor-
mation extraction. In Proceedings of the 3rd ACM
International Conference on Web Search and Data
Mining, WSDM ’10, pages 101–110.
Angel X. Chang and Christopher D. Manning. 2014.
TokensRegex: Defining cascaded regular expres-
sions over tokens. Technical Report CSTR 2014-02,
Department of Computer Science, Stanford Univer-
sity.
Laura Chiticariu, Yunyao Li, and Frederick R. Reiss.
2013. Rule-based information extraction is dead!
Long live rule-based information extraction sys-
tems! In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’13, pages 827–832.
</reference>
<page confidence="0.975041">
106
</page>
<reference confidence="0.997227401869159">
Fabio Ciravegna. 2001. Adaptive information extrac-
tion from text by rule induction and generalisation.
In Proceedings of the 17th International Joint Con-
ference on Artificial Intelligence, IJCAI’01, pages
1251–1256.
William W. Cohen and Yoram Singer. 1999. A simple,
fast, and effective rule learner. In Proceedings of the
16th National Conference on Artificial Intelligence
and the 11th Innovative Applications ofArtificial In-
telligence Conference, pages 335–342.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In Pro-
ceedings of the Joint SIGDAT Conference on Empir-
ical Methods in Natural Language Processing and
Very Large Corpora, pages 100–110.
J. R. Curran, T. Murphy, and B. Scholz. 2007. Min-
imising semantic drift with mutual exclusion boot-
strapping. Proceedings of the Conference of the
Pacific Association for Computational Linguistics,
pages 172–180.
D. Downey, O. Etzioni, S. Soderland, and D. S. Weld.
2004. Learning Text Patterns for Web Informa-
tion Extraction and Assessment. In Proceedings of
AAAI 2004 Workshop on Adaptive Text Extraction
and Mining, ATEM ’04.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
An experimental study. Artificial Intelligence,
165(1):91 – 134.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 1535–1545.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
Dayne Freitag. 1998. Toward general-purpose learn-
ing for information extraction. In Proceedings of the
36th Annual Meeting of the Association for Compu-
tational Linguistics and 17th International Confer-
ence on Computational Linguistics, COLING-ACL
’98, pages 404–408.
Sonal Gupta and Christopher D. Manning. 2014a. In-
duced lexico-syntactic patterns improve information
extraction from online medical forums. Under Sub-
mission.
Sonal Gupta and Christopher D. Manning. 2014b.
Spied: Stanford pattern-based information extrac-
tion and diagnostics. In Proceedings of the ACL
2014 Workshop on Interactive Language Learning,
Visualization, and Interfaces (ACL-ILLVI).
Marti A Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th International Conference on Computational
linguistics, COLING ’92, pages 539–545.
Percy Liang. 2005. Semi-supervised learning for nat-
ural language. Master’s thesis, MIT EECS.
Winston Lin, Roman Yangarber, and Ralph Grishman.
2003. Bootstrapped learning of semantic classes
from positive and negative examples. In Proceed-
ings of the ICML 2003 Workshop on The Continuum
from Labeled to Unlabeled Data in Machine Learn-
ing and Data Mining.
Mausam, Michael Schmitz, Stephen Soderland, Robert
Bart, and Oren Etzioni. 2012. Open language learn-
ing for information extraction. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 523–534.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional sim-
ilarity. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, ACL-IJCNLP ’09, pages
396–404.
Ramesh Nallapati and Christopher D. Manning. 2008.
Legal docket-entry classification: Where machine
learning stumbles. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing, EMNLP ’08, pages 438–446.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proceedings
of the Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technologies, HLT-NAACL ’04,
pages 321–328.
S. Patwardhan. 2010. Widening the Field of View
of Information Extraction through Sentential Event
Recognition. Ph.D. thesis, University of Utah, May.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings
of the 48th Annual Meeting of the Association for
Computational Linguistics, ACL ’10, pages 296–
305.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings
of the 13th National Conference on Artificial Intelli-
gence, AAAI’96, pages 1044–1049.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. Transactions
of the Association for Computational Linguistics,
1:367–378.
</reference>
<page confidence="0.98337">
107
</page>
<reference confidence="0.99963890625">
Mark Stevenson and Mark A. Greenwood. 2005. A
semantic approach to IE pattern induction. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ’05, pages 379–
386.
Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using
extraction pattern contexts. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’02, pages 214–221.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology,
HLT-NAACL ’03, pages 173–180.
Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Gr-
ishman. 2013. Filling knowledge base gaps for dis-
tant supervision of relation extraction. In Proceed-
ings of the Association for Computational Linguis-
tics (ACL), pages 665–670.
Roman Yangarber, Ralph Grishman, and Pasi
Tapanainen. 2000. Automatic acquisition of
domain knowledge for information extraction. In
Proceedings of the 18th International Conference
on Computational Linguistics, COLING ’00, pages
940–946.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names.
In Proceedings of the 19th International Conference
on Computational Linguistics, COLING ’02.
</reference>
<page confidence="0.998356">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851976">
<title confidence="0.999149">Improved Pattern Learning for Bootstrapped Entity Extraction</title>
<author confidence="0.999823">Sonal Gupta Christopher D Manning</author>
<affiliation confidence="0.9998805">Department of Computer Stanford University</affiliation>
<abstract confidence="0.992854333333333">Bootstrapped pattern learning for entity extraction usually starts with seed entities and iteratively learns patterns and entities from unlabeled text. Patterns are scored by their ability to extract more positive entities and less negative entities. A problem is that due to the lack of labeled data, unlabeled entities are either assumed to be negative or are ignored by the existing pattern scoring measures. In this paper, we improve pattern scoring by predicting the labels of unlabeled entities. We use various unsupervised features based on contrasting domain-specific and general text, and exploiting distributional similarity and edit distances to learned entities. Our system outperforms existing pattern scoring algorithms for extracting drug-andtreatment entities from four medical forums.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="27500" citStr="Brown et al., 1992" startWordPosition="4483" endWordPosition="4486"> 0.76 Figure 2: Precision vs. Recall curves of our system and the baselines for the four forums. Rlog-PN and PNOdd are not shown to improve clarity. 0.2. The initial value of 0, was 1.0, which was reduced to 0.8 × 0, whenever the system did not extract any more patterns and words. We discarded patterns that extracted less than 2 positive entities. We selected these parameters by their performance on the development forum. For calculating the DistSim feature used for scoring patterns and entities, we clustered all of MedHelp’s forum data into 1000 clusters using the Brown clustering algorithm (Brown et al., 1992; Liang, 2005).7 For calculating the Domain Ngram feature for scoring entities, we used n-grams from all user forums in MedHelp as the domain ngrams. We evaluate systems by their precision and recall in each iteration. Precision is defined as the fraction of correct entities among the entities extracted. We stopped learning entities for a system if the precision dropped below 75% to extract entities with reasonably high precision. Recall is defined as the fraction of correct entities among the total unique correct entities pooled from all systems while maintaining the precision ≥ 75%. Note tha</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Elaine Califf</author>
<author>Raymond J Mooney</author>
</authors>
<title>Relational learning of pattern-match rules for information extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications of Artificial Intelligence Conference, AAAI-IAAI ’99,</booktitle>
<pages>328--334</pages>
<contexts>
<context position="3280" citStr="Califf and Mooney, 1999" startWordPosition="507" endWordPosition="510">1999). Rules are typically defined by creating patterns around the entities, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). Patterns are scored by their ability to extract more positive entities and less negative entities. Top ranked patterns are used to extract candidate entities from text. High scoring candidate entities are added to the dictionaries and are used to generate more candidate patterns around them. In a supervised setting, the efficacy of patterns can be judged by their performance on a fully labeled dataset (Califf and Mooney, 1999; Ciravegna, 2001). In a bootstrapped system, where the data is not fully labeled, existing systems score patterns by either ignoring the un98 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 98–108, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics labeled entities or assuming them to be negative. However, these scoring schemes cannot differentiate between patterns that extract good versus bad unlabeled entities. The problem is similar to the closed world assumption in distantly supervised information extraction system</context>
<context position="6369" citStr="Califf and Mooney, 1999" startWordPosition="968" endWordPosition="971">interest for many years. Patwardhan (2010) gives a good overview of the research in the field. Rule learn1www.medhelp.org ing systems differ in how they create rules, score them, and score the entities they extract. Here, we mainly discuss the rule scoring part of the previous entity extraction research. The pioneering work by Hearst (1992) used hand written rules to automatically generate more rules that were manually evaluated to extract hypernym-hyponym pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP)2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure </context>
</contexts>
<marker>Califf, Mooney, 1999</marker>
<rawString>Mary Elaine Califf and Raymond J. Mooney. 1999. Relational learning of pattern-match rules for information extraction. In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications of Artificial Intelligence Conference, AAAI-IAAI ’99, pages 328–334.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Richard C Wang</author>
<author>Estevam R Hruschka</author>
<author>Tom M Mitchell</author>
</authors>
<title>Coupled semi-supervised learning for information extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining, WSDM ’10,</booktitle>
<pages>101--110</pages>
<contexts>
<context position="7993" citStr="Carlson et al. (2010)" startWordPosition="1225" endWordPosition="1228">easible for domains that have low coverage in Wordnet, such as medical data. More recently, open information extraction systems have garnered attention. They focus on extracting entities and relations from the web. KnowItAll’s entity extraction from the web (Downey et al., 2004; Etzioni et al., 2005) used components such as list extractors, generic and domain specific pattern learning, and subclass learning. They learned domain-specific patterns using a seed set and scored them by ignoring unlabeled entities. One of our baselines is similar to their domain-specific pattern learning component. Carlson et al. (2010) learned multiple semantic types using coupled semi-supervised training from web-scale data, which is not feasible for all datasets and entity learning tasks. They assessed patterns by their precision, assuming unla99 beled entities to be negative; one of our baselines is similar to their pattern assessment method. Other open information extraction systems like ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012) are mainly geared towards generic, domain-independent relation extractors for web data. We tested learning an entity extractor for a given class using ReVerb. We labeled the bi</context>
<context position="35525" citStr="Carlson et al., 2010" startWordPosition="5800" endWordPosition="5803">ttern scoring or entity scoring. One limitation of our system and evaluation is that we learned single word entities, since calculating some features for multi-word phrases is not straightforward. For example, word clusters using distributional similarity were constructed for single words. Our future work includes expanding the features to evaluate multi-word phrases. Another avenue for future work is to use our pattern scoring method for learning other kinds of rules, such as dependency patterns, and in different kinds of systems, such as hybrid entity learning systems (Etzioni et al., 2005; Carlson et al., 2010). In addition, we did not explicitly address the problem of semantic drift (Curran et al., 2007) in this paper. In theory, learning better patterns would help lessen the problem; we plan to investigate this further. In conclusion, we show that predicting the labels of unlabeled entities in the pattern scorer of a bootstrapped entity extraction system significantly improves precision and recall of learned entities. Our experiments demonstrate the importance of having models that contrast domain-specific and general domain text, and the usefulness of features that allow spelling variations when </context>
</contexts>
<marker>Carlson, Betteridge, Wang, Hruschka, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Richard C. Wang, Estevam R. Hruschka, Jr., and Tom M. Mitchell. 2010. Coupled semi-supervised learning for information extraction. In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining, WSDM ’10, pages 101–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angel X Chang</author>
<author>Christopher D Manning</author>
</authors>
<title>TokensRegex: Defining cascaded regular expressions over tokens.</title>
<date>2014</date>
<tech>Technical Report CSTR 2014-02,</tech>
<institution>Department of Computer Science, Stanford University.</institution>
<contexts>
<context position="25571" citStr="Chang and Manning, 2014" startWordPosition="4151" endWordPosition="4154"> the same experimental setup for our system and the baselines. When matching phrases from a seed dictionary to text, a phrase is labeled with the dictionary’s class if the sequence of phrase words or their lemmas match with the sequence of words of a dictionary phrase. Since our corpora are from online discussion forums, they have many spelling mistakes and morphological variations of entities. To deal with the variations, we do fuzzy matching of words – if two words are one edit distance away and are more than 6 characters long, then they are considered a match. We used Stanford TokensRegex (Chang and Manning, 2014) to create and apply surface word patterns to text, and used the Stanford Part-ofSpeech (POS) tagger (Toutanova et al., 2003) to find POS tags of tokens and lemmatize them. When creating patterns, we discarded patterns whose left or right context was 1 or 2 stop words to avoid generating low precision patterns.6 In each iteration, we learned a maximum 20 patterns with ps(r) &gt; θr and maximum 10 words with score &gt; 6Three or more stop words resulted in some good patterns like ‘I am on X’. Our stop words list consists of punctuation marks and around 200 very common English words. 103 ASTHMA ENT Pr</context>
</contexts>
<marker>Chang, Manning, 2014</marker>
<rawString>Angel X. Chang and Christopher D. Manning. 2014. TokensRegex: Defining cascaded regular expressions over tokens. Technical Report CSTR 2014-02, Department of Computer Science, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Chiticariu</author>
<author>Yunyao Li</author>
<author>Frederick R Reiss</author>
</authors>
<title>Rule-based information extraction is dead! Long live rule-based information extraction systems!</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’13,</booktitle>
<pages>827--832</pages>
<contexts>
<context position="1371" citStr="Chiticariu et al., 2013" startWordPosition="197" endWordPosition="200">g domain-specific and general text, and exploiting distributional similarity and edit distances to learned entities. Our system outperforms existing pattern scoring algorithms for extracting drug-andtreatment entities from four medical forums. 1 Introduction This paper considers the problem of building effective entity extractors for custom entity types from specialized domain corpora. We approach the problem by learning rules bootstrapped using seed sets of entities. Though entity extraction using machine learning is common in academic research, rule-based systems dominate in commercial use (Chiticariu et al., 2013), mainly because rules are effective, interpretable, and are easy to customize by non-experts to cope with errors. They also have been shown to perform better than state-of-the-art machine learning methods on some specialized domains (Nallapati and Manning, 2008; Gupta and Manning, 2014a). In addition, building supervised machine learning systems for a reasonably large domain-specific corpus would require hand-labeling sufficient data to Seed dictionary for class ‘animal Text: I own a cat named Fluffy. I run dog. I also nap with my pet cat. Extractions = positive : {dog}, un Extractions = posi</context>
</contexts>
<marker>Chiticariu, Li, Reiss, 2013</marker>
<rawString>Laura Chiticariu, Yunyao Li, and Frederick R. Reiss. 2013. Rule-based information extraction is dead! Long live rule-based information extraction systems! In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’13, pages 827–832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Ciravegna</author>
</authors>
<title>Adaptive information extraction from text by rule induction and generalisation.</title>
<date>2001</date>
<booktitle>In Proceedings of the 17th International Joint Conference on Artificial Intelligence, IJCAI’01,</booktitle>
<pages>1251--1256</pages>
<contexts>
<context position="3298" citStr="Ciravegna, 2001" startWordPosition="511" endWordPosition="512">y defined by creating patterns around the entities, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). Patterns are scored by their ability to extract more positive entities and less negative entities. Top ranked patterns are used to extract candidate entities from text. High scoring candidate entities are added to the dictionaries and are used to generate more candidate patterns around them. In a supervised setting, the efficacy of patterns can be judged by their performance on a fully labeled dataset (Califf and Mooney, 1999; Ciravegna, 2001). In a bootstrapped system, where the data is not fully labeled, existing systems score patterns by either ignoring the un98 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 98–108, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics labeled entities or assuming them to be negative. However, these scoring schemes cannot differentiate between patterns that extract good versus bad unlabeled entities. The problem is similar to the closed world assumption in distantly supervised information extraction systems, when all propos</context>
<context position="6331" citStr="Ciravegna, 2001" startWordPosition="964" endWordPosition="965"> learning has been a topic of interest for many years. Patwardhan (2010) gives a good overview of the research in the field. Rule learn1www.medhelp.org ing systems differ in how they create rules, score them, and score the entities they extract. Here, we mainly discuss the rule scoring part of the previous entity extraction research. The pioneering work by Hearst (1992) used hand written rules to automatically generate more rules that were manually evaluated to extract hypernym-hyponym pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP)2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity</context>
</contexts>
<marker>Ciravegna, 2001</marker>
<rawString>Fabio Ciravegna. 2001. Adaptive information extraction from text by rule induction and generalisation. In Proceedings of the 17th International Joint Conference on Artificial Intelligence, IJCAI’01, pages 1251–1256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Yoram Singer</author>
</authors>
<title>A simple, fast, and effective rule learner.</title>
<date>1999</date>
<booktitle>In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications ofArtificial Intelligence Conference,</booktitle>
<pages>335--342</pages>
<contexts>
<context position="6306" citStr="Cohen and Singer, 1999" startWordPosition="959" endWordPosition="962">shtml. 2 Related Work Rule based learning has been a topic of interest for many years. Patwardhan (2010) gives a good overview of the research in the field. Rule learn1www.medhelp.org ing systems differ in how they create rules, score them, and score the entities they extract. Here, we mainly discuss the rule scoring part of the previous entity extraction research. The pioneering work by Hearst (1992) used hand written rules to automatically generate more rules that were manually evaluated to extract hypernym-hyponym pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP)2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a patt</context>
</contexts>
<marker>Cohen, Singer, 1999</marker>
<rawString>William W. Cohen and Yoram Singer. 1999. A simple, fast, and effective rule learner. In Proceedings of the 16th National Conference on Artificial Intelligence and the 11th Innovative Applications ofArtificial Intelligence Conference, pages 335–342.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>100--110</pages>
<contexts>
<context position="2662" citStr="Collins and Singer, 1999" startWordPosition="410" endWordPosition="413">the class ‘animals’ from the text. Pattern 1 and 2 are candidate patterns. Text matched with the patterns is shown in italics and the extracted entities are shown in bold. Pattern 1: my pet X Pattern 2: own a X train a model, which can be costly and time consuming. Bootstrapped machine-learned rules can make extraction easier and more efficient on such a corpus. In a bootstrapped rule-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific class from unlabeled text (Riloff, 1996; Collins and Singer, 1999). Rules are typically defined by creating patterns around the entities, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). Patterns are scored by their ability to extract more positive entities and less negative entities. Top ranked patterns are used to extract candidate entities from text. High scoring candidate entities are added to the dictionaries and are used to generate more candidate patterns around them. In a supervised setting, the efficacy of patterns can be judged by their performance on a fully labeled dataset (Calif</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 100–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
<author>T Murphy</author>
<author>B Scholz</author>
</authors>
<title>Minimising semantic drift with mutual exclusion bootstrapping.</title>
<date>2007</date>
<booktitle>Proceedings of the Conference of the Pacific Association for Computational Linguistics,</booktitle>
<pages>172--180</pages>
<contexts>
<context position="35621" citStr="Curran et al., 2007" startWordPosition="5816" endWordPosition="5819">ingle word entities, since calculating some features for multi-word phrases is not straightforward. For example, word clusters using distributional similarity were constructed for single words. Our future work includes expanding the features to evaluate multi-word phrases. Another avenue for future work is to use our pattern scoring method for learning other kinds of rules, such as dependency patterns, and in different kinds of systems, such as hybrid entity learning systems (Etzioni et al., 2005; Carlson et al., 2010). In addition, we did not explicitly address the problem of semantic drift (Curran et al., 2007) in this paper. In theory, learning better patterns would help lessen the problem; we plan to investigate this further. In conclusion, we show that predicting the labels of unlabeled entities in the pattern scorer of a bootstrapped entity extraction system significantly improves precision and recall of learned entities. Our experiments demonstrate the importance of having models that contrast domain-specific and general domain text, and the usefulness of features that allow spelling variations when dealing with informal texts. Our pattern scorer outperforms existing pattern scoring methods for</context>
</contexts>
<marker>Curran, Murphy, Scholz, 2007</marker>
<rawString>J. R. Curran, T. Murphy, and B. Scholz. 2007. Minimising semantic drift with mutual exclusion bootstrapping. Proceedings of the Conference of the Pacific Association for Computational Linguistics, pages 172–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Downey</author>
<author>O Etzioni</author>
<author>S Soderland</author>
<author>D S Weld</author>
</authors>
<title>Learning Text Patterns for Web Information Extraction and Assessment.</title>
<date>2004</date>
<booktitle>In Proceedings of AAAI 2004 Workshop on Adaptive Text Extraction and Mining, ATEM ’04.</booktitle>
<contexts>
<context position="7650" citStr="Downey et al., 2004" startWordPosition="1173" endWordPosition="1176">them as negative. Gupta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordnet to assess patterns, which is not feasible for domains that have low coverage in Wordnet, such as medical data. More recently, open information extraction systems have garnered attention. They focus on extracting entities and relations from the web. KnowItAll’s entity extraction from the web (Downey et al., 2004; Etzioni et al., 2005) used components such as list extractors, generic and domain specific pattern learning, and subclass learning. They learned domain-specific patterns using a seed set and scored them by ignoring unlabeled entities. One of our baselines is similar to their domain-specific pattern learning component. Carlson et al. (2010) learned multiple semantic types using coupled semi-supervised training from web-scale data, which is not feasible for all datasets and entity learning tasks. They assessed patterns by their precision, assuming unla99 beled entities to be negative; one of o</context>
</contexts>
<marker>Downey, Etzioni, Soderland, Weld, 2004</marker>
<rawString>D. Downey, O. Etzioni, S. Soderland, and D. S. Weld. 2004. Learning Text Patterns for Web Information Extraction and Assessment. In Proceedings of AAAI 2004 Workshop on Adaptive Text Extraction and Mining, ATEM ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: An experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<pages>134</pages>
<contexts>
<context position="7673" citStr="Etzioni et al., 2005" startWordPosition="1177" endWordPosition="1180">ta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordnet to assess patterns, which is not feasible for domains that have low coverage in Wordnet, such as medical data. More recently, open information extraction systems have garnered attention. They focus on extracting entities and relations from the web. KnowItAll’s entity extraction from the web (Downey et al., 2004; Etzioni et al., 2005) used components such as list extractors, generic and domain specific pattern learning, and subclass learning. They learned domain-specific patterns using a seed set and scored them by ignoring unlabeled entities. One of our baselines is similar to their domain-specific pattern learning component. Carlson et al. (2010) learned multiple semantic types using coupled semi-supervised training from web-scale data, which is not feasible for all datasets and entity learning tasks. They assessed patterns by their precision, assuming unla99 beled entities to be negative; one of our baselines is similar</context>
<context position="23226" citStr="Etzioni et al. (2005)" startWordPosition="3737" endWordPosition="3740">xtracted by a pattern r, respectively. The set Ar is defined as union of all the three sets. We compare our system with the following pattern scoring algorithms. Candidate entities are scored in the same way as described in Section 3.3. It is important to note that previous works also differ in how they create patterns, apply patterns, and score entities. Since we focus on only the pattern scoring aspect, we run experiments that differ in only that component. PNOdd: Defined as Pr / Nr , this measure ignores unlabeled entities and is similar to the domain specific pattern learning component of Etzioni et al. (2005) since all patterns with Pr &lt; 2 were discarded (more details in the next section). PUNOdd: Defined as Pr /( Ur + Nr ), this measure treats unlabeled entities as negative entities. RlogF: Measure used by Riloff (1996) and Thelen and Riloff (2002), and calculated as Rr log Pr , where Rr was defined as Pr / Ar (labeled RlogF-PUN). It assumed and other classes as negative) and adding the top 50 words extracted by the top 300 patterns to the SC class dictionary. This helps in adding corpus specific SC words to the dictionary. 5We used top 10,000 words from Google N-grams and top 5,000 words from Tw</context>
<context position="35502" citStr="Etzioni et al., 2005" startWordPosition="5796" endWordPosition="5799">ork well for either pattern scoring or entity scoring. One limitation of our system and evaluation is that we learned single word entities, since calculating some features for multi-word phrases is not straightforward. For example, word clusters using distributional similarity were constructed for single words. Our future work includes expanding the features to evaluate multi-word phrases. Another avenue for future work is to use our pattern scoring method for learning other kinds of rules, such as dependency patterns, and in different kinds of systems, such as hybrid entity learning systems (Etzioni et al., 2005; Carlson et al., 2010). In addition, we did not explicitly address the problem of semantic drift (Curran et al., 2007) in this paper. In theory, learning better patterns would help lessen the problem; we plan to investigate this further. In conclusion, we show that predicting the labels of unlabeled entities in the pattern scorer of a bootstrapped entity extraction system significantly improves precision and recall of learned entities. Our experiments demonstrate the importance of having models that contrast domain-specific and general domain text, and the usefulness of features that allow sp</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91 – 134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1535--1545</pages>
<contexts>
<context position="8384" citStr="Fader et al., 2011" startWordPosition="1286" endWordPosition="1289">subclass learning. They learned domain-specific patterns using a seed set and scored them by ignoring unlabeled entities. One of our baselines is similar to their domain-specific pattern learning component. Carlson et al. (2010) learned multiple semantic types using coupled semi-supervised training from web-scale data, which is not feasible for all datasets and entity learning tasks. They assessed patterns by their precision, assuming unla99 beled entities to be negative; one of our baselines is similar to their pattern assessment method. Other open information extraction systems like ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012) are mainly geared towards generic, domain-independent relation extractors for web data. We tested learning an entity extractor for a given class using ReVerb. We labeled the binary and unary ReVerb extractions using the class seed entities and retrained its confidence function, with poor results. Poon and Domingos (2010) found a similar result for inducing a probabilistic ontology: an open information extraction system extracted low accuracy relational triples on a small corpus. In this paper, we use features such as distributional similarity and edit distances</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1535–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>Bradford Books.</publisher>
<contexts>
<context position="21656" citStr="Fellbaum, 1998" startWordPosition="3465" endWordPosition="3466">anning (2014a).4 The lists of 4The DT seed dictionary (36,091 phrases) and SC seed dictionary (97,211 phrases) were automatically constructed from various sources on the Internet and expanded using the OAC Consumer Health Vocabulary (http://www. consumerhealthvocab.org), which maps medical jargon to everyday phrases and their variants. Both dictionaries are large because they contain many variants of entities. For each system, the SC dictionary was further expanded by running the system with the SC class as positive (considering DT 102 body parts and temporal nouns were obtained from Wordnet (Fellbaum, 1998). The common words list was created using most common words on the web and Twitter.5 For evaluation, the first author hand labeled the learned entities pooled from all systems. A word was evaluated by querying the word and the forum name on Google and manually inspecting the results. More details on the labeling guidelines are in the Supplement section. Inter annotator agreement between the annotator and another researcher was computed on 200 randomly sampled learned entities from each of the Asthma and ENT forum. The agreement for the entities from the Asthma forum was 96% and from the ENT fo</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dayne Freitag</author>
</authors>
<title>Toward general-purpose learning for information extraction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, COLING-ACL ’98,</booktitle>
<pages>404--408</pages>
<contexts>
<context position="6272" citStr="Freitag, 1998" startWordPosition="956" endWordPosition="957">edu/software/ patternviz.shtml. 2 Related Work Rule based learning has been a topic of interest for many years. Patwardhan (2010) gives a good overview of the research in the field. Rule learn1www.medhelp.org ing systems differ in how they create rules, score them, and score the entities they extract. Here, we mainly discuss the rule scoring part of the previous entity extraction research. The pioneering work by Hearst (1992) used hand written rules to automatically generate more rules that were manually evaluated to extract hypernym-hyponym pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP)2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of</context>
</contexts>
<marker>Freitag, 1998</marker>
<rawString>Dayne Freitag. 1998. Toward general-purpose learning for information extraction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, COLING-ACL ’98, pages 404–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonal Gupta</author>
<author>Christopher D Manning</author>
</authors>
<title>Induced lexico-syntactic patterns improve information extraction from online medical forums. Under Submission.</title>
<date>2014</date>
<contexts>
<context position="1658" citStr="Gupta and Manning, 2014" startWordPosition="242" endWordPosition="245">lem of building effective entity extractors for custom entity types from specialized domain corpora. We approach the problem by learning rules bootstrapped using seed sets of entities. Though entity extraction using machine learning is common in academic research, rule-based systems dominate in commercial use (Chiticariu et al., 2013), mainly because rules are effective, interpretable, and are easy to customize by non-experts to cope with errors. They also have been shown to perform better than state-of-the-art machine learning methods on some specialized domains (Nallapati and Manning, 2008; Gupta and Manning, 2014a). In addition, building supervised machine learning systems for a reasonably large domain-specific corpus would require hand-labeling sufficient data to Seed dictionary for class ‘animal Text: I own a cat named Fluffy. I run dog. I also nap with my pet cat. Extractions = positive : {dog}, un Extractions = positive : {dog}, un Figure 1: An example pattern learning system for the class ‘animals’ from the text. Pattern 1 and 2 are candidate patterns. Text matched with the patterns is shown in italics and the extracted entities are shown in bold. Pattern 1: my pet X Pattern 2: own a X train a mo</context>
<context position="5521" citStr="Gupta and Manning (2014" startWordPosition="844" endWordPosition="847">. We estimate an unlabeled entity’s negative class probability by averaging probabilities from various unsupervised class predictors, such as distributional similarity, string edit distances from learned entities, and TF-IDF scores. Our system performs significantly better than existing pattern scoring measures for extracting drug-and-treatment entities from four medical forums on MedHelp1, a user health discussion website. We release the code for the systems described in this paper at http://nlp.stanford.edu/ software/patternslearning.shtml. We also release a visualization tool, described in Gupta and Manning (2014b), that visualizes and compares output of multiple pattern-based entity extraction systems. It can be downloaded at http://nlp.stanford.edu/software/ patternviz.shtml. 2 Related Work Rule based learning has been a topic of interest for many years. Patwardhan (2010) gives a good overview of the research in the field. Rule learn1www.medhelp.org ing systems differ in how they create rules, score them, and score the entities they extract. Here, we mainly discuss the rule scoring part of the previous entity extraction research. The pioneering work by Hearst (1992) used hand written rules to automa</context>
<context position="7072" citStr="Gupta and Manning (2014" startWordPosition="1083" endWordPosition="1086"> a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure ignored unlabeled entities and the confidence measure treated them as negative. Gupta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordnet to assess patterns, which is not feasible for domains that have low coverage in Wordnet, such as medical data. More recently, open information extraction systems have garnered attention. They focus on extracting entities and relations from the web. KnowItAll’s entity extraction from the web (Downey et al., 2004; Etzioni et al., 2005</context>
<context position="13765" citStr="Gupta and Manning (2014" startWordPosition="2163" endWordPosition="2166">eled entities are spelling mistakes and morphological variations of labeled entities. We use two edit distance based features to predict labels for these unlabeled entities. Second, some unlabeled entities are substrings of multi-word dictionary phrases but do not necessarily belong to the dictionary’s class. For example, for learning drug names, the positive dictionary might contain ‘asthma meds’, but ‘asthma’ is negative and might occur in a negative dictionary as ‘asthma disease’. To predict the labels of entities that are a substring of dictionary phrases, we use SemOdd, which was used in Gupta and Manning (2014a) to learn entities. Third, for a specialized domain, unlabeled entities that commonly occur in generic text are more likely to be negative. We use Google Ngrams (called GN) to get a fast, non-sparse estimate of the frequency of entities over a broad range of domains. The above features do not consider the context in which the entities occur in text. We use the fifth feature, DistSim, to exploit contextual information of the labeled entities using distributional similarity. The features are defined as: Edit distance from positive entities (EDP): This feature gives a score of 1 if e has low ed</context>
<context position="19805" citStr="Gupta and Manning (2014" startWordPosition="3174" endWordPosition="3177">t experiments. 4 Experiments 4.1 Dataset We evaluate our system on extracting drug-andtreatment (DT) entities in sentences from four forums on the MedHelp user health discussion website: 1. Acne, 2. Adult Type II Diabetes (called Diabetes), 3. Ear Nose &amp; Throat (called ENT), and 4. Asthma. The forums have discussion threads by users concerning health related problems and treatments. The number of sentences in each forum are: 215,623 in ENT, 39,637 in Asthma, 63,355 in Diabetes, and 65,595 in Acne. We used Asthma as the development forum for feature engineering and parameter tuning. Similar to Gupta and Manning (2014a), a DT entity is defined as a pharmaceutical drug, or any treatment or intervention mentioned that may help a symptom or a condition. It includes surgeries, lifestyle changes, alternative treatments, home remedies, and components of daily care and management of a disease, but does not include diagnostic tests and devices. More information is in the supplemental material. A few example sentences from the dataset are below. I plan to start cinnamon and holy basil - known to lower glucose in many people. She gave me albuteral and symbicort (plus some hayfever meds and asked me to use the peak f</context>
<context position="21053" citStr="Gupta and Manning (2014" startWordPosition="3373" endWordPosition="3376">tions were treated electrically, with high voltage million volt electricity, which solved the problem, but the treatment is not FDA approved and generally unavailable, except under experimental treatment protocols. In these sentences, ‘cinanmon’, ‘holy basil’, ‘albuteral’, ‘symbicort’, ‘meds’, ‘high voltage million volt electricity’, and ‘treatment’ are DT entities. We used entities from the following classes as negative: symptoms and conditions (SC), medical specialists, body parts, and common temporal nouns to remove dates and dosage information. We used the DT and SC seed dictionaries from Gupta and Manning (2014a).4 The lists of 4The DT seed dictionary (36,091 phrases) and SC seed dictionary (97,211 phrases) were automatically constructed from various sources on the Internet and expanded using the OAC Consumer Health Vocabulary (http://www. consumerhealthvocab.org), which maps medical jargon to everyday phrases and their variants. Both dictionaries are large because they contain many variants of entities. For each system, the SC dictionary was further expanded by running the system with the SC class as positive (considering DT 102 body parts and temporal nouns were obtained from Wordnet (Fellbaum, 19</context>
<context position="24671" citStr="Gupta and Manning (2014" startWordPosition="3997" endWordPosition="4000">gF-PN). Yangarber02: This measure from Yangarber et al. (2002) calculated two scores, accr = Pr / Nr and confr = ( Pr / Ar )log Pr . Patterns with accr less than a threshold were discarded and the rest were ranked using confr. We empirically determined that a threshold of 0.8 performed best on the development forum. Lin03: A measure proposed in Lin et al. (2003), it was similar to Yangarber02, except confr was defined as log Pr ( Pr − Nr )/ Ar . In essence, it discards a pattern if it extracts more negative entities than positive entities. SqrtRatioAll: This pattern scoring method was used in Gupta and Manning (2014a) and defined as � \Ifreqk/ � Vl freqj, k∈P,. j∈A,. where freqi is the number of times entity i is extracted by r. Sublinear scaling of the term-frequency prevents high frequency words from overshadowing the contribution of low frequency words. 4.3 Experimental Setup We used the same experimental setup for our system and the baselines. When matching phrases from a seed dictionary to text, a phrase is labeled with the dictionary’s class if the sequence of phrase words or their lemmas match with the sequence of words of a dictionary phrase. Since our corpora are from online discussion forums, t</context>
</contexts>
<marker>Gupta, Manning, 2014</marker>
<rawString>Sonal Gupta and Christopher D. Manning. 2014a. Induced lexico-syntactic patterns improve information extraction from online medical forums. Under Submission.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonal Gupta</author>
<author>Christopher D Manning</author>
</authors>
<title>Spied: Stanford pattern-based information extraction and diagnostics.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Workshop on Interactive Language Learning, Visualization, and Interfaces (ACL-ILLVI).</booktitle>
<contexts>
<context position="1658" citStr="Gupta and Manning, 2014" startWordPosition="242" endWordPosition="245">lem of building effective entity extractors for custom entity types from specialized domain corpora. We approach the problem by learning rules bootstrapped using seed sets of entities. Though entity extraction using machine learning is common in academic research, rule-based systems dominate in commercial use (Chiticariu et al., 2013), mainly because rules are effective, interpretable, and are easy to customize by non-experts to cope with errors. They also have been shown to perform better than state-of-the-art machine learning methods on some specialized domains (Nallapati and Manning, 2008; Gupta and Manning, 2014a). In addition, building supervised machine learning systems for a reasonably large domain-specific corpus would require hand-labeling sufficient data to Seed dictionary for class ‘animal Text: I own a cat named Fluffy. I run dog. I also nap with my pet cat. Extractions = positive : {dog}, un Extractions = positive : {dog}, un Figure 1: An example pattern learning system for the class ‘animals’ from the text. Pattern 1 and 2 are candidate patterns. Text matched with the patterns is shown in italics and the extracted entities are shown in bold. Pattern 1: my pet X Pattern 2: own a X train a mo</context>
<context position="5521" citStr="Gupta and Manning (2014" startWordPosition="844" endWordPosition="847">. We estimate an unlabeled entity’s negative class probability by averaging probabilities from various unsupervised class predictors, such as distributional similarity, string edit distances from learned entities, and TF-IDF scores. Our system performs significantly better than existing pattern scoring measures for extracting drug-and-treatment entities from four medical forums on MedHelp1, a user health discussion website. We release the code for the systems described in this paper at http://nlp.stanford.edu/ software/patternslearning.shtml. We also release a visualization tool, described in Gupta and Manning (2014b), that visualizes and compares output of multiple pattern-based entity extraction systems. It can be downloaded at http://nlp.stanford.edu/software/ patternviz.shtml. 2 Related Work Rule based learning has been a topic of interest for many years. Patwardhan (2010) gives a good overview of the research in the field. Rule learn1www.medhelp.org ing systems differ in how they create rules, score them, and score the entities they extract. Here, we mainly discuss the rule scoring part of the previous entity extraction research. The pioneering work by Hearst (1992) used hand written rules to automa</context>
<context position="7072" citStr="Gupta and Manning (2014" startWordPosition="1083" endWordPosition="1086"> a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure ignored unlabeled entities and the confidence measure treated them as negative. Gupta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordnet to assess patterns, which is not feasible for domains that have low coverage in Wordnet, such as medical data. More recently, open information extraction systems have garnered attention. They focus on extracting entities and relations from the web. KnowItAll’s entity extraction from the web (Downey et al., 2004; Etzioni et al., 2005</context>
<context position="13765" citStr="Gupta and Manning (2014" startWordPosition="2163" endWordPosition="2166">eled entities are spelling mistakes and morphological variations of labeled entities. We use two edit distance based features to predict labels for these unlabeled entities. Second, some unlabeled entities are substrings of multi-word dictionary phrases but do not necessarily belong to the dictionary’s class. For example, for learning drug names, the positive dictionary might contain ‘asthma meds’, but ‘asthma’ is negative and might occur in a negative dictionary as ‘asthma disease’. To predict the labels of entities that are a substring of dictionary phrases, we use SemOdd, which was used in Gupta and Manning (2014a) to learn entities. Third, for a specialized domain, unlabeled entities that commonly occur in generic text are more likely to be negative. We use Google Ngrams (called GN) to get a fast, non-sparse estimate of the frequency of entities over a broad range of domains. The above features do not consider the context in which the entities occur in text. We use the fifth feature, DistSim, to exploit contextual information of the labeled entities using distributional similarity. The features are defined as: Edit distance from positive entities (EDP): This feature gives a score of 1 if e has low ed</context>
<context position="19805" citStr="Gupta and Manning (2014" startWordPosition="3174" endWordPosition="3177">t experiments. 4 Experiments 4.1 Dataset We evaluate our system on extracting drug-andtreatment (DT) entities in sentences from four forums on the MedHelp user health discussion website: 1. Acne, 2. Adult Type II Diabetes (called Diabetes), 3. Ear Nose &amp; Throat (called ENT), and 4. Asthma. The forums have discussion threads by users concerning health related problems and treatments. The number of sentences in each forum are: 215,623 in ENT, 39,637 in Asthma, 63,355 in Diabetes, and 65,595 in Acne. We used Asthma as the development forum for feature engineering and parameter tuning. Similar to Gupta and Manning (2014a), a DT entity is defined as a pharmaceutical drug, or any treatment or intervention mentioned that may help a symptom or a condition. It includes surgeries, lifestyle changes, alternative treatments, home remedies, and components of daily care and management of a disease, but does not include diagnostic tests and devices. More information is in the supplemental material. A few example sentences from the dataset are below. I plan to start cinnamon and holy basil - known to lower glucose in many people. She gave me albuteral and symbicort (plus some hayfever meds and asked me to use the peak f</context>
<context position="21053" citStr="Gupta and Manning (2014" startWordPosition="3373" endWordPosition="3376">tions were treated electrically, with high voltage million volt electricity, which solved the problem, but the treatment is not FDA approved and generally unavailable, except under experimental treatment protocols. In these sentences, ‘cinanmon’, ‘holy basil’, ‘albuteral’, ‘symbicort’, ‘meds’, ‘high voltage million volt electricity’, and ‘treatment’ are DT entities. We used entities from the following classes as negative: symptoms and conditions (SC), medical specialists, body parts, and common temporal nouns to remove dates and dosage information. We used the DT and SC seed dictionaries from Gupta and Manning (2014a).4 The lists of 4The DT seed dictionary (36,091 phrases) and SC seed dictionary (97,211 phrases) were automatically constructed from various sources on the Internet and expanded using the OAC Consumer Health Vocabulary (http://www. consumerhealthvocab.org), which maps medical jargon to everyday phrases and their variants. Both dictionaries are large because they contain many variants of entities. For each system, the SC dictionary was further expanded by running the system with the SC class as positive (considering DT 102 body parts and temporal nouns were obtained from Wordnet (Fellbaum, 19</context>
<context position="24671" citStr="Gupta and Manning (2014" startWordPosition="3997" endWordPosition="4000">gF-PN). Yangarber02: This measure from Yangarber et al. (2002) calculated two scores, accr = Pr / Nr and confr = ( Pr / Ar )log Pr . Patterns with accr less than a threshold were discarded and the rest were ranked using confr. We empirically determined that a threshold of 0.8 performed best on the development forum. Lin03: A measure proposed in Lin et al. (2003), it was similar to Yangarber02, except confr was defined as log Pr ( Pr − Nr )/ Ar . In essence, it discards a pattern if it extracts more negative entities than positive entities. SqrtRatioAll: This pattern scoring method was used in Gupta and Manning (2014a) and defined as � \Ifreqk/ � Vl freqj, k∈P,. j∈A,. where freqi is the number of times entity i is extracted by r. Sublinear scaling of the term-frequency prevents high frequency words from overshadowing the contribution of low frequency words. 4.3 Experimental Setup We used the same experimental setup for our system and the baselines. When matching phrases from a seed dictionary to text, a phrase is labeled with the dictionary’s class if the sequence of phrase words or their lemmas match with the sequence of words of a dictionary phrase. Since our corpora are from online discussion forums, t</context>
</contexts>
<marker>Gupta, Manning, 2014</marker>
<rawString>Sonal Gupta and Christopher D. Manning. 2014b. Spied: Stanford pattern-based information extraction and diagnostics. In Proceedings of the ACL 2014 Workshop on Interactive Language Learning, Visualization, and Interfaces (ACL-ILLVI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th International Conference on Computational linguistics, COLING ’92,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="2795" citStr="Hearst, 1992" startWordPosition="430" endWordPosition="431">ntities are shown in bold. Pattern 1: my pet X Pattern 2: own a X train a model, which can be costly and time consuming. Bootstrapped machine-learned rules can make extraction easier and more efficient on such a corpus. In a bootstrapped rule-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific class from unlabeled text (Riloff, 1996; Collins and Singer, 1999). Rules are typically defined by creating patterns around the entities, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). Patterns are scored by their ability to extract more positive entities and less negative entities. Top ranked patterns are used to extract candidate entities from text. High scoring candidate entities are added to the dictionaries and are used to generate more candidate patterns around them. In a supervised setting, the efficacy of patterns can be judged by their performance on a fully labeled dataset (Califf and Mooney, 1999; Ciravegna, 2001). In a bootstrapped system, where the data is not fully labeled, existing systems score patterns </context>
<context position="6087" citStr="Hearst (1992)" startWordPosition="930" endWordPosition="931">tion tool, described in Gupta and Manning (2014b), that visualizes and compares output of multiple pattern-based entity extraction systems. It can be downloaded at http://nlp.stanford.edu/software/ patternviz.shtml. 2 Related Work Rule based learning has been a topic of interest for many years. Patwardhan (2010) gives a good overview of the research in the field. Rule learn1www.medhelp.org ing systems differ in how they create rules, score them, and score the entities they extract. Here, we mainly discuss the rule scoring part of the previous entity extraction research. The pioneering work by Hearst (1992) used hand written rules to automatically generate more rules that were manually evaluated to extract hypernym-hyponym pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP)2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th International Conference on Computational linguistics, COLING ’92, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language. Master’s thesis,</title>
<date>2005</date>
<journal>MIT EECS.</journal>
<contexts>
<context position="27514" citStr="Liang, 2005" startWordPosition="4487" endWordPosition="4488">ision vs. Recall curves of our system and the baselines for the four forums. Rlog-PN and PNOdd are not shown to improve clarity. 0.2. The initial value of 0, was 1.0, which was reduced to 0.8 × 0, whenever the system did not extract any more patterns and words. We discarded patterns that extracted less than 2 positive entities. We selected these parameters by their performance on the development forum. For calculating the DistSim feature used for scoring patterns and entities, we clustered all of MedHelp’s forum data into 1000 clusters using the Brown clustering algorithm (Brown et al., 1992; Liang, 2005).7 For calculating the Domain Ngram feature for scoring entities, we used n-grams from all user forums in MedHelp as the domain ngrams. We evaluate systems by their precision and recall in each iteration. Precision is defined as the fraction of correct entities among the entities extracted. We stopped learning entities for a system if the precision dropped below 75% to extract entities with reasonably high precision. Recall is defined as the fraction of correct entities among the total unique correct entities pooled from all systems while maintaining the precision ≥ 75%. Note that true recall </context>
</contexts>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. Master’s thesis, MIT EECS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winston Lin</author>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
</authors>
<title>Bootstrapped learning of semantic classes from positive and negative examples.</title>
<date>2003</date>
<booktitle>In Proceedings of the ICML 2003 Workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining.</booktitle>
<contexts>
<context position="6850" citStr="Lin et al. (2003)" startWordPosition="1048" endWordPosition="1051">rvised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP)2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure ignored unlabeled entities and the confidence measure treated them as negative. Gupta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordnet to assess patterns, which is not feasible for domains that have low coverage in Wordnet, such as medical data. M</context>
<context position="24412" citStr="Lin et al. (2003)" startWordPosition="3951" endWordPosition="3954">ams and top 5,000 words from Twitter (www.twitter.com), accessed from May 19 to 25, 2012. unlabeled entities as negative entities. We also compare with a variant that ignores the unlabeled entities, that is by defining Rr as Pr /( Pr + Nr ) (labeled RlogF-PN). Yangarber02: This measure from Yangarber et al. (2002) calculated two scores, accr = Pr / Nr and confr = ( Pr / Ar )log Pr . Patterns with accr less than a threshold were discarded and the rest were ranked using confr. We empirically determined that a threshold of 0.8 performed best on the development forum. Lin03: A measure proposed in Lin et al. (2003), it was similar to Yangarber02, except confr was defined as log Pr ( Pr − Nr )/ Ar . In essence, it discards a pattern if it extracts more negative entities than positive entities. SqrtRatioAll: This pattern scoring method was used in Gupta and Manning (2014a) and defined as � \Ifreqk/ � Vl freqj, k∈P,. j∈A,. where freqi is the number of times entity i is extracted by r. Sublinear scaling of the term-frequency prevents high frequency words from overshadowing the contribution of low frequency words. 4.3 Experimental Setup We used the same experimental setup for our system and the baselines. Wh</context>
</contexts>
<marker>Lin, Yangarber, Grishman, 2003</marker>
<rawString>Winston Lin, Roman Yangarber, and Ralph Grishman. 2003. Bootstrapped learning of semantic classes from positive and negative examples. In Proceedings of the ICML 2003 Workshop on The Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schmitz Mausam</author>
<author>Stephen Soderland</author>
<author>Robert Bart</author>
<author>Oren Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>523--534</pages>
<contexts>
<context position="8416" citStr="Mausam et al., 2012" startWordPosition="1292" endWordPosition="1295"> domain-specific patterns using a seed set and scored them by ignoring unlabeled entities. One of our baselines is similar to their domain-specific pattern learning component. Carlson et al. (2010) learned multiple semantic types using coupled semi-supervised training from web-scale data, which is not feasible for all datasets and entity learning tasks. They assessed patterns by their precision, assuming unla99 beled entities to be negative; one of our baselines is similar to their pattern assessment method. Other open information extraction systems like ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012) are mainly geared towards generic, domain-independent relation extractors for web data. We tested learning an entity extractor for a given class using ReVerb. We labeled the binary and unary ReVerb extractions using the class seed entities and retrained its confidence function, with poor results. Poon and Domingos (2010) found a similar result for inducing a probabilistic ontology: an open information extraction system extracted low accuracy relational triples on a small corpus. In this paper, we use features such as distributional similarity and edit distances from learned entities to score </context>
</contexts>
<marker>Mausam, Soderland, Bart, Etzioni, 2012</marker>
<rawString>Mausam, Michael Schmitz, Stephen Soderland, Robert Bart, and Oren Etzioni. 2012. Open language learning for information extraction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 523–534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tara McIntosh</author>
<author>James R Curran</author>
</authors>
<title>Reducing semantic drift with bagging and distributional similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, ACL-IJCNLP ’09,</booktitle>
<pages>396--404</pages>
<contexts>
<context position="9212" citStr="McIntosh and Curran, 2009" startWordPosition="1414" endWordPosition="1417"> the binary and unary ReVerb extractions using the class seed entities and retrained its confidence function, with poor results. Poon and Domingos (2010) found a similar result for inducing a probabilistic ontology: an open information extraction system extracted low accuracy relational triples on a small corpus. In this paper, we use features such as distributional similarity and edit distances from learned entities to score patterns. Similar measures have been used before but for learning entities, labeling semantic classes, or for reducing noise in seed sets (Pantel and Ravichandran, 2004; McIntosh and Curran, 2009). Measures for improving entity learning can be used alongside ours since we focus on scoring candidate patterns. 3 Approach We use lexico-syntactic surface word patterns to extract entities from unlabeled text starting with seed dictionaries of entities for multiple classes. For ease of exposition, we present the approach below for learning entities for one class C. It can easily be generalized to multiple classes. We refer to entities belonging to C as positive and entities belonging to all other classes as negative. The bootstrapping process involves the following steps, iteratively perform</context>
</contexts>
<marker>McIntosh, Curran, 2009</marker>
<rawString>Tara McIntosh and James R. Curran. 2009. Reducing semantic drift with bagging and distributional similarity. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, ACL-IJCNLP ’09, pages 396–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Legal docket-entry classification: Where machine learning stumbles.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>438--446</pages>
<contexts>
<context position="1633" citStr="Nallapati and Manning, 2008" startWordPosition="237" endWordPosition="241">This paper considers the problem of building effective entity extractors for custom entity types from specialized domain corpora. We approach the problem by learning rules bootstrapped using seed sets of entities. Though entity extraction using machine learning is common in academic research, rule-based systems dominate in commercial use (Chiticariu et al., 2013), mainly because rules are effective, interpretable, and are easy to customize by non-experts to cope with errors. They also have been shown to perform better than state-of-the-art machine learning methods on some specialized domains (Nallapati and Manning, 2008; Gupta and Manning, 2014a). In addition, building supervised machine learning systems for a reasonably large domain-specific corpus would require hand-labeling sufficient data to Seed dictionary for class ‘animal Text: I own a cat named Fluffy. I run dog. I also nap with my pet cat. Extractions = positive : {dog}, un Extractions = positive : {dog}, un Figure 1: An example pattern learning system for the class ‘animals’ from the text. Pattern 1 and 2 are candidate patterns. Text matched with the patterns is shown in italics and the extracted entities are shown in bold. Pattern 1: my pet X Patt</context>
</contexts>
<marker>Nallapati, Manning, 2008</marker>
<rawString>Ramesh Nallapati and Christopher D. Manning. 2008. Legal docket-entry classification: Where machine learning stumbles. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 438–446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technologies, HLT-NAACL ’04,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="9184" citStr="Pantel and Ravichandran, 2004" startWordPosition="1410" endWordPosition="1413"> class using ReVerb. We labeled the binary and unary ReVerb extractions using the class seed entities and retrained its confidence function, with poor results. Poon and Domingos (2010) found a similar result for inducing a probabilistic ontology: an open information extraction system extracted low accuracy relational triples on a small corpus. In this paper, we use features such as distributional similarity and edit distances from learned entities to score patterns. Similar measures have been used before but for learning entities, labeling semantic classes, or for reducing noise in seed sets (Pantel and Ravichandran, 2004; McIntosh and Curran, 2009). Measures for improving entity learning can be used alongside ours since we focus on scoring candidate patterns. 3 Approach We use lexico-syntactic surface word patterns to extract entities from unlabeled text starting with seed dictionaries of entities for multiple classes. For ease of exposition, we present the approach below for learning entities for one class C. It can easily be generalized to multiple classes. We refer to entities belonging to C as positive and entities belonging to all other classes as negative. The bootstrapping process involves the followin</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technologies, HLT-NAACL ’04, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
</authors>
<title>Widening the Field of View of Information Extraction through Sentential Event Recognition.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Utah,</institution>
<contexts>
<context position="5787" citStr="Patwardhan (2010)" startWordPosition="881" endWordPosition="882">r than existing pattern scoring measures for extracting drug-and-treatment entities from four medical forums on MedHelp1, a user health discussion website. We release the code for the systems described in this paper at http://nlp.stanford.edu/ software/patternslearning.shtml. We also release a visualization tool, described in Gupta and Manning (2014b), that visualizes and compares output of multiple pattern-based entity extraction systems. It can be downloaded at http://nlp.stanford.edu/software/ patternviz.shtml. 2 Related Work Rule based learning has been a topic of interest for many years. Patwardhan (2010) gives a good overview of the research in the field. Rule learn1www.medhelp.org ing systems differ in how they create rules, score them, and score the entities they extract. Here, we mainly discuss the rule scoring part of the previous entity extraction research. The pioneering work by Hearst (1992) used hand written rules to automatically generate more rules that were manually evaluated to extract hypernym-hyponym pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP)2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labe</context>
</contexts>
<marker>Patwardhan, 2010</marker>
<rawString>S. Patwardhan. 2010. Widening the Field of View of Information Extraction through Sentential Event Recognition. Ph.D. thesis, University of Utah, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Pedro Domingos</author>
</authors>
<title>Unsupervised ontology induction from text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>296--305</pages>
<contexts>
<context position="8739" citStr="Poon and Domingos (2010)" startWordPosition="1340" endWordPosition="1343"> datasets and entity learning tasks. They assessed patterns by their precision, assuming unla99 beled entities to be negative; one of our baselines is similar to their pattern assessment method. Other open information extraction systems like ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012) are mainly geared towards generic, domain-independent relation extractors for web data. We tested learning an entity extractor for a given class using ReVerb. We labeled the binary and unary ReVerb extractions using the class seed entities and retrained its confidence function, with poor results. Poon and Domingos (2010) found a similar result for inducing a probabilistic ontology: an open information extraction system extracted low accuracy relational triples on a small corpus. In this paper, we use features such as distributional similarity and edit distances from learned entities to score patterns. Similar measures have been used before but for learning entities, labeling semantic classes, or for reducing noise in seed sets (Pantel and Ravichandran, 2004; McIntosh and Curran, 2009). Measures for improving entity learning can be used alongside ours since we focus on scoring candidate patterns. 3 Approach We</context>
</contexts>
<marker>Poon, Domingos, 2010</marker>
<rawString>Hoifung Poon and Pedro Domingos. 2010. Unsupervised ontology induction from text. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 296– 305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of the 13th National Conference on Artificial Intelligence, AAAI’96,</booktitle>
<pages>1044--1049</pages>
<contexts>
<context position="2635" citStr="Riloff, 1996" startWordPosition="408" endWordPosition="409">ng system for the class ‘animals’ from the text. Pattern 1 and 2 are candidate patterns. Text matched with the patterns is shown in italics and the extracted entities are shown in bold. Pattern 1: my pet X Pattern 2: own a X train a model, which can be costly and time consuming. Bootstrapped machine-learned rules can make extraction easier and more efficient on such a corpus. In a bootstrapped rule-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific class from unlabeled text (Riloff, 1996; Collins and Singer, 1999). Rules are typically defined by creating patterns around the entities, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). Patterns are scored by their ability to extract more positive entities and less negative entities. Top ranked patterns are used to extract candidate entities from text. High scoring candidate entities are added to the dictionaries and are used to generate more candidate patterns around them. In a supervised setting, the efficacy of patterns can be judged by their performance on a f</context>
<context position="6444" citStr="Riloff (1996)" startWordPosition="983" endWordPosition="984">e field. Rule learn1www.medhelp.org ing systems differ in how they create rules, score them, and score the entities they extract. Here, we mainly discuss the rule scoring part of the previous entity extraction research. The pioneering work by Hearst (1992) used hand written rules to automatically generate more rules that were manually evaluated to extract hypernym-hyponym pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP)2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure ignored unlabeled entities and the confidence measure treated them as negat</context>
<context position="12680" citStr="Riloff, 1996" startWordPosition="1989" endWordPosition="1990">tively. The pattern score, ps(r) is calculated as ps(r) = |Nr |+ Ee( |— score(e)) log(|Pr|) ∈U,1 score e where |. |denotes size of a set. The function score(e) gives the probability of an entity e belonging to C. If e is a common word, score(e) is 0. Otherwise, score(e) is calculated as the average of five feature scores (explained below), each 100 of which give a score between 0 and 1. The feature scores are calculated using the seed dictionaries, learned entities for all labels, Google Ngrams2, and clustering of domain words using distributional similarity. The log |Pr |term, inspired from (Riloff, 1996), gives higher scores to patterns that extract more positive entities. Candidate patterns are ranked by ps(r) and the top patterns are added to the list of learned patterns. To calculate score(e), we use features that assess unlabeled entities to be either closer to positive or negative entities in an unsupervised way. We motivate our choice of the five features below with the following insights. If the dataset consists of informally written text, many unlabeled entities are spelling mistakes and morphological variations of labeled entities. We use two edit distance based features to predict l</context>
<context position="23442" citStr="Riloff (1996)" startWordPosition="3777" endWordPosition="3778">Section 3.3. It is important to note that previous works also differ in how they create patterns, apply patterns, and score entities. Since we focus on only the pattern scoring aspect, we run experiments that differ in only that component. PNOdd: Defined as Pr / Nr , this measure ignores unlabeled entities and is similar to the domain specific pattern learning component of Etzioni et al. (2005) since all patterns with Pr &lt; 2 were discarded (more details in the next section). PUNOdd: Defined as Pr /( Ur + Nr ), this measure treats unlabeled entities as negative entities. RlogF: Measure used by Riloff (1996) and Thelen and Riloff (2002), and calculated as Rr log Pr , where Rr was defined as Pr / Ar (labeled RlogF-PUN). It assumed and other classes as negative) and adding the top 50 words extracted by the top 300 patterns to the SC class dictionary. This helps in adding corpus specific SC words to the dictionary. 5We used top 10,000 words from Google N-grams and top 5,000 words from Twitter (www.twitter.com), accessed from May 19 to 25, 2012. unlabeled entities as negative entities. We also compare with a variant that ignores the unlabeled entities, that is by defining Rr as Pr /( Pr + Nr ) (label</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the 13th National Conference on Artificial Intelligence, AAAI’96, pages 1044–1049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Luke Zettlemoyer</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Modeling missing data in distant supervision for information extraction.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--367</pages>
<contexts>
<context position="3976" citStr="Ritter et al., 2013" startWordPosition="609" endWordPosition="612">eled, existing systems score patterns by either ignoring the un98 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 98–108, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics labeled entities or assuming them to be negative. However, these scoring schemes cannot differentiate between patterns that extract good versus bad unlabeled entities. The problem is similar to the closed world assumption in distantly supervised information extraction systems, when all propositions missing from a knowledge base are considered false (Ritter et al., 2013; Xu et al., 2013). Predicting labels of unlabeled entities can improve scoring patterns. Consider the example shown in Figure 1. Current pattern learning systems would score both patterns equally. However, features like distributional similarity can predict ‘cat’ to be closer to {dog} than ‘car’, and a pattern learning system can use that information to rank ‘Pattern 1’ higher than ‘Pattern 2’. In this paper, we work on bootstrapping entity extraction using seed sets of entities and an unlabeled text corpus. We improve the scoring of patterns for an entity class by defining a pattern’s score </context>
</contexts>
<marker>Ritter, Zettlemoyer, Mausam, Etzioni, 2013</marker>
<rawString>Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Etzioni. 2013. Modeling missing data in distant supervision for information extraction. Transactions of the Association for Computational Linguistics, 1:367–378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Stevenson</author>
<author>Mark A Greenwood</author>
</authors>
<title>A semantic approach to IE pattern induction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>379--386</pages>
<contexts>
<context position="7324" citStr="Stevenson and Greenwood (2005)" startWordPosition="1122" endWordPosition="1126">ted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure ignored unlabeled entities and the confidence measure treated them as negative. Gupta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordnet to assess patterns, which is not feasible for domains that have low coverage in Wordnet, such as medical data. More recently, open information extraction systems have garnered attention. They focus on extracting entities and relations from the web. KnowItAll’s entity extraction from the web (Downey et al., 2004; Etzioni et al., 2005) used components such as list extractors, generic and domain specific pattern learning, and subclass learning. They learned domain-specific patterns using a seed set and scored them by ignoring unlabeled entities. One of our baselines is similar to th</context>
</contexts>
<marker>Stevenson, Greenwood, 2005</marker>
<rawString>Mark Stevenson and Mark A. Greenwood. 2005. A semantic approach to IE pattern induction. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 379– 386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Thelen</author>
<author>Ellen Riloff</author>
</authors>
<title>A bootstrapping method for learning semantic lexicons using extraction pattern contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’02,</booktitle>
<pages>214--221</pages>
<contexts>
<context position="6735" citStr="Thelen and Riloff (2002)" startWordPosition="1029" endWordPosition="1032"> to automatically generate more rules that were manually evaluated to extract hypernym-hyponym pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP)2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure ignored unlabeled entities and the confidence measure treated them as negative. Gupta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordn</context>
<context position="23471" citStr="Thelen and Riloff (2002)" startWordPosition="3780" endWordPosition="3783"> important to note that previous works also differ in how they create patterns, apply patterns, and score entities. Since we focus on only the pattern scoring aspect, we run experiments that differ in only that component. PNOdd: Defined as Pr / Nr , this measure ignores unlabeled entities and is similar to the domain specific pattern learning component of Etzioni et al. (2005) since all patterns with Pr &lt; 2 were discarded (more details in the next section). PUNOdd: Defined as Pr /( Ur + Nr ), this measure treats unlabeled entities as negative entities. RlogF: Measure used by Riloff (1996) and Thelen and Riloff (2002), and calculated as Rr log Pr , where Rr was defined as Pr / Ar (labeled RlogF-PUN). It assumed and other classes as negative) and adding the top 50 words extracted by the top 300 patterns to the SC class dictionary. This helps in adding corpus specific SC words to the dictionary. 5We used top 10,000 words from Google N-grams and top 5,000 words from Twitter (www.twitter.com), accessed from May 19 to 25, 2012. unlabeled entities as negative entities. We also compare with a variant that ignores the unlabeled entities, that is by defining Rr as Pr /( Pr + Nr ) (labeled RlogF-PN). Yangarber02: Th</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>Michael Thelen and Ellen Riloff. 2002. A bootstrapping method for learning semantic lexicons using extraction pattern contexts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’02, pages 214–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, HLT-NAACL ’03,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="25696" citStr="Toutanova et al., 2003" startWordPosition="4172" endWordPosition="4175">is labeled with the dictionary’s class if the sequence of phrase words or their lemmas match with the sequence of words of a dictionary phrase. Since our corpora are from online discussion forums, they have many spelling mistakes and morphological variations of entities. To deal with the variations, we do fuzzy matching of words – if two words are one edit distance away and are more than 6 characters long, then they are considered a match. We used Stanford TokensRegex (Chang and Manning, 2014) to create and apply surface word patterns to text, and used the Stanford Part-ofSpeech (POS) tagger (Toutanova et al., 2003) to find POS tags of tokens and lemmatize them. When creating patterns, we discarded patterns whose left or right context was 1 or 2 stop words to avoid generating low precision patterns.6 In each iteration, we learned a maximum 20 patterns with ps(r) &gt; θr and maximum 10 words with score &gt; 6Three or more stop words resulted in some good patterns like ‘I am on X’. Our stop words list consists of punctuation marks and around 200 very common English words. 103 ASTHMA ENT Precision 0.95 0.85 0.75 0.9 0.8 1 OurSystem RlogF-PUN Yangarber02 SqrtAllRatio Lin03 PUNOdd Precision 0.96 0.94 0.92 0.88 0.86</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, HLT-NAACL ’03, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Raphael Hoffmann</author>
<author>Le Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Filling knowledge base gaps for distant supervision of relation extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL),</booktitle>
<pages>665--670</pages>
<marker>Xu, Hoffmann, Le Zhao, Grishman, 2013</marker>
<rawString>Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Grishman. 2013. Filling knowledge base gaps for distant supervision of relation extraction. In Proceedings of the Association for Computational Linguistics (ACL), pages 665–670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
<author>Pasi Tapanainen</author>
</authors>
<title>Automatic acquisition of domain knowledge for information extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics, COLING ’00,</booktitle>
<pages>940--946</pages>
<contexts>
<context position="2849" citStr="Yangarber et al., 2000" startWordPosition="436" endWordPosition="439"> X Pattern 2: own a X train a model, which can be costly and time consuming. Bootstrapped machine-learned rules can make extraction easier and more efficient on such a corpus. In a bootstrapped rule-based entity learning system, seed dictionaries and/or patterns provide weak supervision to label data. The system iteratively learns new entities belonging to a specific class from unlabeled text (Riloff, 1996; Collins and Singer, 1999). Rules are typically defined by creating patterns around the entities, such as lexico-syntactic surface word patterns (Hearst, 1992) and dependency tree patterns (Yangarber et al., 2000). Patterns are scored by their ability to extract more positive entities and less negative entities. Top ranked patterns are used to extract candidate entities from text. High scoring candidate entities are added to the dictionaries and are used to generate more candidate patterns around them. In a supervised setting, the efficacy of patterns can be judged by their performance on a fully labeled dataset (Califf and Mooney, 1999; Ciravegna, 2001). In a bootstrapped system, where the data is not fully labeled, existing systems score patterns by either ignoring the un98 Proceedings of the Eightee</context>
</contexts>
<marker>Yangarber, Grishman, Tapanainen, 2000</marker>
<rawString>Roman Yangarber, Ralph Grishman, and Pasi Tapanainen. 2000. Automatic acquisition of domain knowledge for information extraction. In Proceedings of the 18th International Conference on Computational Linguistics, COLING ’00, pages 940–946.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Winston Lin</author>
<author>Ralph Grishman</author>
</authors>
<title>Unsupervised learning of generalized names.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th International Conference on Computational Linguistics, COLING ’02.</booktitle>
<contexts>
<context position="6828" citStr="Yangarber et al. (2002)" startWordPosition="1042" endWordPosition="1046"> pairs from text. Other supervised systems like SRV (Freitag, 1998), SLIPPER (Cohen and Singer, 1999), (LP)2 (Ciravegna, 2001), and RAPIER (Califf and Mooney, 1999) used a fully labeled corpus to either create or score rules. Riloff (1996) used a set of seed entities to bootstrap learning of rules for entity extraction from unlabeled text. She scored a rule by a weighted conditional probability measure estimated by counting the number of positive entities among all the entities extracted by the rule. Thelen and Riloff (2002) extended the above bootstrapping algorithm for multi-class learning. Yangarber et al. (2002) and Lin et al. (2003) used a combination of accuracy and confidence of a pattern for multiclass entity learning, where the accuracy measure ignored unlabeled entities and the confidence measure treated them as negative. Gupta and Manning (2014a) used the ratio of scaled frequencies of positive entities among all extracted entities. None of the above measures predict labels of unlabeled entities to score patterns. Our system outperforms them in our experiments. Stevenson and Greenwood (2005) used Wordnet to assess patterns, which is not feasible for domains that have low coverage in Wordnet, s</context>
<context position="24110" citStr="Yangarber et al. (2002)" startWordPosition="3893" endWordPosition="3896"> as Rr log Pr , where Rr was defined as Pr / Ar (labeled RlogF-PUN). It assumed and other classes as negative) and adding the top 50 words extracted by the top 300 patterns to the SC class dictionary. This helps in adding corpus specific SC words to the dictionary. 5We used top 10,000 words from Google N-grams and top 5,000 words from Twitter (www.twitter.com), accessed from May 19 to 25, 2012. unlabeled entities as negative entities. We also compare with a variant that ignores the unlabeled entities, that is by defining Rr as Pr /( Pr + Nr ) (labeled RlogF-PN). Yangarber02: This measure from Yangarber et al. (2002) calculated two scores, accr = Pr / Nr and confr = ( Pr / Ar )log Pr . Patterns with accr less than a threshold were discarded and the rest were ranked using confr. We empirically determined that a threshold of 0.8 performed best on the development forum. Lin03: A measure proposed in Lin et al. (2003), it was similar to Yangarber02, except confr was defined as log Pr ( Pr − Nr )/ Ar . In essence, it discards a pattern if it extracts more negative entities than positive entities. SqrtRatioAll: This pattern scoring method was used in Gupta and Manning (2014a) and defined as � \Ifreqk/ � Vl freqj</context>
</contexts>
<marker>Yangarber, Lin, Grishman, 2002</marker>
<rawString>Roman Yangarber, Winston Lin, and Ralph Grishman. 2002. Unsupervised learning of generalized names. In Proceedings of the 19th International Conference on Computational Linguistics, COLING ’02.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
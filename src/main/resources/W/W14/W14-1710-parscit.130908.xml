<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.998755">
Grammatical Error Detection and Correction
using a Single Maximum Entropy Model∗
</title>
<author confidence="0.99093">
Peilu Wang, Zhongye Jia and Hai Zhao†
</author>
<affiliation confidence="0.993799">
Key Laboratory of Shanghai Education Commission for
Intelligent Interaction and Cognitive Engineering,
Center for Brain-Like Computing and Machine Intelligence
Department of Computer Science and Engineering, Shanghai Jiao Tong University
</affiliation>
<address confidence="0.967316">
800 Dongchuan Road, Shanghai 200240, China
</address>
<email confidence="0.998715">
{plwang1990,jia.zhongye}@gmail.com,zhaohai@cs.sjtu.edu.cn
</email>
<sectionHeader confidence="0.993873" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999731875">
This paper describes the system of Shang-
hai Jiao Tong Unvierity team in the
CoNLL-2014 shared task. Error correc-
tion operations are encoded as a group of
predefined labels and therefore the task
is formulized as a multi-label classifica-
tion task. For training, labels are obtained
through a strict rule-based approach. For
decoding, errors are detected and correct-
ed according to the classification results.
A single maximum entropy model is used
for the classification implementation in-
corporated with an improved feature selec-
tion algorithm. Our system achieved pre-
cision of 29.83, recall of 5.16 and F 0.5
of 15.24 in the official evaluation.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968866666667">
The task of CoNLL-2014 is grammatical error cor-
rection which consists of detecting and correcting
the grammatical errors in English essays written
by non-native speakers (Ng et al., 2014). The re-
search of grammatical error correction can poten-
tially help millions of people in the world who are
learning English as foreign language. Although
there have been many works on grammatical error
correction, the current approaches mainly focus on
very limited error types and the result is far from
satisfactory.
The CoNLL-2014 shared task, compared with
the previous Help Our Own (HOO) tasks (Dale et
al., 2012) considering only determiner and prepo-
sition errors and the CoNLL-2013 shared task fo-
</bodyText>
<footnote confidence="0.827731625">
∗This work was partially supported by the National Natu-
ral Science Foundation of China (Grant No.60903119, Grant
No.61170114, and Grant No.61272248), the National Ba-
sic Research Program of China (Grant No.2013CB329401),
the Science and Technology Commission of Shanghai Mu-
nicipality (Grant No.13511500200), and the European Union
Seventh Framework Program (Grant No.247619).
†Corresponding author
</footnote>
<bodyText confidence="0.999877902439024">
cusing on five major types of errors, requires to
correct all 28 types of errors (Ng et al., 2014).
One traditional strategy is designing a system
combined of a set of sub-models, where each sub-
model is specialized for a specific subtask, for ex-
ample, correcting one type of errors. This strat-
egy is computationally efficient and can adopt d-
ifferent favorable features for each subtask. Top
ranked systems in CoNLL-2013 (Rozovskaya et
al., 2013; Kao et al., 2013; Xing et al., 2013;
Yoshimoto et al., 2013; Xiang et al., 2013) are
based on this strategy. However, the division of
the model relies on prior-knowledges and the de-
signing of different features for each sub-model
requires a large amount of manual works. This
shortage is especially notable in CoNLL-2014
shared task, since the number of error types is
much larger and the composition of errors is more
complicated than before.
In contrast, we follow the work in (Jia et al.,
2013a; Zhao et al., 2009a), integrating everything
into one model. This integrated system holds a
merit that a one-way feature selection benefits the
whole system and no additional process is needed
to deal with the conflict or error propagation of ev-
ery sub-models. Here is a glance of this method: A
set of more detailed error types are generated auto-
matically from the original 28 types of errors. The
detailed error type can be regarded as the label of
a word, thus the task of grammatical error detec-
tion is transformed to a multi-label classification
task using maximum entropy model (Berger et al.,
1996; Zhao et al., 2013). A feature selection ap-
proach is introduced to get effective features from
large amounts of feature candidates. Once errors
are detected through word label classification, a
rule-based method is used to make corrections ac-
cording to their labels.
The rest of the paper is organized as follows.
Section 2 describes the system architecture. Sec-
tion 3 introduces the feature selection approach
</bodyText>
<page confidence="0.986412">
74
</page>
<note confidence="0.967513">
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 74–82,
Baltimore, Maryland, 26-27 July 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.987469">
and the features we used. Experiments and result-
s are presented in section 5, followed by conclu-
sion.
</bodyText>
<sectionHeader confidence="0.912575" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.9999928">
In our approach, the grammatical error detection
is regarded as a multi-label classification task. At
first, each token in training corpus is assigned a la-
bel according to the golden annotation. The con-
struction of labels is rule based using an extend-
ed version of Levenshtein edit distance algorith-
m which will be discussed in the following sub-
section. Each label maps an edit operation to do
the correction, thus the generated labels are much
more detailed than the originial 28 error types.
Then, a maximum entropy (ME) model is adopted
as the classifier. With the labeled data, the process
of grammatical error correction is just applying the
edit operation mapped by each label, which is ba-
sically the reverse of the labeling phase.
</bodyText>
<subsectionHeader confidence="0.998776">
2.1 Data Labeling
</subsectionHeader>
<bodyText confidence="0.999616428571429">
In CoNLL-2014 shared task, there are 28 error
types but they can not be used directly as class la-
bels, since these types are too general that they can
hardly be corrected by applying one rule-based
edit. For example, the correction of Vform (ver-
b form) error type includes all verb form inflec-
tions such as converting a verb to its infinitive for-
m, gerund form, past form and past participle and
so on. Previous works (Dahlmeier et al., 2012;
Rozovskaya et al., 2012; Kochmar et al., 2012)
manually decompose each error types to more de-
tailed subtypes. For example, in (Dahlmeier et al.,
2012), the determinater errors are decomposed in-
to:
</bodyText>
<listItem confidence="0.999852">
• replacement determiner (RD): { a → the }
• missing determiner (MD): { E → a }
• unwanted determiner (UD): { a → E }
</listItem>
<bodyText confidence="0.98759275">
For a task with a few error types such as merely
determinative and preposition error in HOO 2012,
manually decomposition may be sufficient. How-
ever, for CoNLL-2014, all 28 error types are re-
quired to be corrected and some of these types
such as Rloc- (Local redundancy) and Um (Un-
clear meaning) are quite complex that the manu-
al decomposition is time consuming and requires
lots of grammatical knowledges. Therefore, an au-
tomatica decomposition method is proposed. It is
extended from the Levenshtein edit distance algo-
rithm and can divide error types into more detailed
subtypes that each subtype can be corrected by ap-
plying one simple rule. How to calculate the ex-
tended Levenshtein edit distance is described in
Algorithm 1.
</bodyText>
<equation confidence="0.957158735849057">
Algorithm 1 Extended Levenshtein Edit Distance
INPUT: tokssrc, toksdst
OUTPUT: E, P
lsrc, ldst — len(tokssrc), len(toksdst)
D[0 ... lsrc][0 ... ldst] — 0
B[0 ... lsrc][0 ... ldst] — (0, 0)
E[0 ... lsrc][0 ... ldst] — ϕ
for i — 1 ... lsrc do
D[i][0] — i
B[i][0] — (i-1, 0)
E[i][0] — D
end for
for j — 1 ... ldst do
D[0][j] — j
B[0][j] — (0, j-1)
E[0][j] — A
end for
for i — 1 ... lsrc; do
for j — 1 ... ldst do
if tokssrc[i-1] = toksdst[j-1] then
D[i][j] — D[i-1][j-1]
B[i][j] — (i-1, j-1)
E[i][j] — U
else
m = min(D[i-1][j-1], D[i-1][j], D[i][j-1])
if m = D[i-1][j-1] then
D[i][j] — D[i-1][j-1] + 1
B[i][j] — (i-1, j-1)
if lemma(tokssrc[i-1])
= lemma(toksdst[j-1]) then
E[i][j] — S
else
E[i][j] — Z
end if
else if m = D[i-1][j] then
D[i][j] — D[i-1][j] + 1
B[i][j] — (i-1, j)
E[i][j] — D
else if m = D[i][j-1] then
D[i][j] — D[i][j-1] + 1
B[i][j] — (i, j-1)
E[i][j] — A
end if
end if
end for
end for
i, j — lsrc, ldst
while i &gt; 0 V j &gt; 0 do
insert E[i][j] into head of E
insert toksdst[j − 1] into head of P
(i, j) — B[i][j]
end while
return (E, P)
</equation>
<bodyText confidence="0.99689175">
In this algorithm, tokssrc represents the tokens
that are annotated with one grammatical error and
toksdst represents the corrected tokens of tokssrc.
At first, three two dimensional matrixes D, B and
</bodyText>
<page confidence="0.997842">
75
</page>
<bodyText confidence="0.998646137254902">
E are initialized. For all i and j, D[i][j] holds
the Levenshtein distance between the first i tokens
of tokssrc and first j tokens of toksdst. B stores
the path of the Levenshtein distance and E stores
the edit operations in this path. The original Lev-
enshtein edit distance has 4 edit operations: un-
change (U), addition (A), deletion (D) and substi-
tution (S). We extend the “substitution” edit into
two types of edits: inflection (Z) and the original
substitution (S). If two different words have the
same lemma, the substitution operation is Z, else is
S. lemma(x) returns the lemma of token x. This
algorithm returns the edit operations E and the pa-
rameters of these operations P. Here is a simple
sample illustrating this algorithm. For the golden
edit {a red apple is —* red apples are}, tokssrc is
a red apple is, toksdst is red apples are, the output
edits E will be {D, U, Z, S}, and the parameters P
will be {-, red, apples, are}.
Then with the output of this extended Leven-
shtein distance algorithm, labels can be generated
by transforming these edit operations into readable
symbols. For those tokens without errors, we di-
rectly assign a special label “O” to them. A tricky
part of the labeling process is the problem of the
edit “addition”, A. A new token can only be added
before or after an existing token. Thus for edit op-
eration with addition, we must find an existing to-
ken that the label can be assigned to, and this sort
of token is defined as pivot. A pivot can be a token
that is not changed in an edit operation, such as the
“apple” in edit {apple —* an apple}, or some oth-
er types of edit such as the inflection of “look” to
“looking” in edit {look —* have been looking at}.
The names of these labels are based on BNF
syntax which is defined in Figure 1. The non-
terminal (word) can be substituted by all words
in the vocabulary. The non-terminal (inflection-
rules) can be substituted by terminals of inflection
rules that are used for correcting the error types of
noun number, verb form, and subject-verb agree-
ment errors. All the inflection rules are listed in
Table 1.
With the output of extended Levenshtein edits
distance algorithm, Algorithm 2 gives the process
to generate labels whose names are based on the
syntax defined in Figure 1. It takes the output E, P
of Algorithm 1 as inputs and returns the generat-
ed set of labels L. Each label in L corresponds to
one token in tokssrc in order. For our previous ex-
ample of edit {a red apple is —* red apples are},
</bodyText>
<equation confidence="0.998007705882353">
(label) ::= (simple-label) (compound-label)
(simple-label) ::= (pivot) (add-before)
(add-after)
(compound-label) ::= (add-before) (pivot)
(pivot) (add-after)
(add-before) (pivot) (add-after)
(pivot) ::= (unchange) (substitution)
(inflection)
(deletion)
(add-before) ::= (word)®
(word)®(add-before)
(add-after) ::= ®(word)
®(word)(add-after)
(substitution) ::= (word)
(inflection) ::= (inflection-rules)
(unchange) ::= O
(deletion) ::= e
</equation>
<figureCaption confidence="0.993139">
Figure 1: BNF syntax of label
</figureCaption>
<bodyText confidence="0.978135">
Rules Description
LEMMA change word to its lemma
NPLURAL change noun to its plural form
VSINGULAR change verb to its singular form
GERUND change verb to its gerund form
PAST change verb to its past form
PART change verb to its past partici-
ple
</bodyText>
<tableCaption confidence="0.991548">
Table 1: Inflection rules
</tableCaption>
<bodyText confidence="0.999906">
the L returned by Algorithm 2 is {e, O, NPLU-
RAL, ARE} corresponding to the tokens {a, red,
apple, is} in tokssrc. Some other examples of the
generated labels are presented in Table 2.
These labels are elaborately designed that each
of them can be interpreted easily as a series of ed-
it operations. Once the labels are determined by
classifier, the correction of the grammatical errors
is conducted by applying the edit operations inter-
preted from these labels.
</bodyText>
<page confidence="0.879049">
76
</page>
<figure confidence="0.732919411764706">
Algorithm 2 Labeling Algorithm
1: INPUT: E, P
2: OUTPUT: L
3: pivot ← number of edits in E that are not A
4: L ← ϕ
5: L ← ″6: while i &lt; length(E) do
7: if E[i] = A then
8: L ← L+ label of edit E[i] with P[i]
9: i ← i + 1
10: else
11: l ← L+ label of edit E[i] with P[i]
12: pivot ← pivot − 1
13: if pivot = 0 then
14: i ← i + 1
15: while i &lt; length of E do
16: l ← l + ⊕ + P[i]
17: i ← i + 1
18: end while
19: end if
20: push l into L
21: L ← ″22: end if
23: end while
24: L ← upper case of L
25: return L
Tokens Edit Label
to reveal ⊖
{to reveal→revealing} GERUND
a {a woman→women} ⊖
woman NPLURAL
developing {developing world THE⊕
wold →the developing world} ⊙
a {a→ e} ⊖
in {in→on} ON
apple {apple→an apple} AN⊕
</figure>
<tableCaption confidence="0.968275">
Table 2: Examples of labeling
</tableCaption>
<subsectionHeader confidence="0.989989">
2.2 Label Classification
</subsectionHeader>
<bodyText confidence="0.99992">
Using the approach described above, the training
corpus is converted to a sequence of words with
labels. Maximum entropy model is used as the
classifier. It allows a very rich set of features to be
used in a model and has shown good performance
in similiar tasks (Zhao et al., 2013). The features
we used are discussed in the next section.
</bodyText>
<sectionHeader confidence="0.961676" genericHeader="method">
3 Feature Selection and Generation
</sectionHeader>
<bodyText confidence="0.999539074074074">
One key factor affecting the performance of maxi-
mum entropy classifier is the features it used. A
good feature that contains useful information to
guide classification will significantly improve the
performance of the classifier. One direct way to
involve more good features is involving more fea-
tures.
In our approach, large amounts of candidate
features are collected at first. We carefully exam-
ine the factors involved in a wide range of fea-
tures that have been or can be used to the word
label classification task. Many features that are
considered effective in various of previous work-
s (Dahlmeier et al., 2012; Rozovskaya et al.,
2012; Han et al., 2006; Rozovskaya et al., 2011;
Tetreault, Joel R and Chodorow, Martin, 2008)
are included. Besides, features that are used in
the similar spell checking tasks (Jia et al., 2013b;
Yang et al., 2012) and some novel features show-
ing effectiveness in other NLP tasks (Wang et al.,
2013; Zhang and Zhao, 2013; Xu and Zhao, 2012;
Ma and Zhao, 2012; Zhao, 2009; Zhao et al.,
2009b) are also included. However, using too
many features is time consuming. Besides, it in-
creases the probability of overfitting and may lead
to a poor solution of the maximum-likelihood pa-
rameter estimate in the ME training.
</bodyText>
<figure confidence="0.652812891891892">
Algorithm 3 Greedy Feature Selection
1: INPUT: all feature candidates F
2: OUTPUT: selected features S
3: S = {f0, fi, ... , fk}, a random subset of F
4: while do
5: C = RECRUITMORE(S)
6: if C = {} then
7: return S
8: end if
9: S′ = SHAKEOFF(S+C)
10: if scr(M(S)) ≥ scr(M(S′)) then
11: return S
12: end if
13: S = S′
14: end while
15: function RECRUITMORE(S)
16: C = {}, and p = scr(M(S))
17: for each f ∈ F − S do
18: if p &lt; scr(M(S + {f})) then
19: C = C + {f}
20: end if
21: end for
22: end function
23: function SHAKEOFF(S)
24: while do
25: S′ = S0 = S
26: for each f ∈ S do
27: if scr(M(S′)) &lt; scr(M(S′ − {f})) then
28: S′ = S′ − {f}
29: end if
30: end for
31: S = S′
32: if S′ = So then
33: return S′
34: end if
35: end while
36: end function
</figure>
<bodyText confidence="0.90035975">
Therefore a feature selection algorithm is intro-
duced to filter out “bad” features at first and the re-
maining features will be used to generate new fea-
tures. The feature selection algorithm has shown
</bodyText>
<page confidence="0.9936">
77
</page>
<bodyText confidence="0.998327142857143">
effectiveness in (Zhao et al., 2013) and is present-
ed in Algorithm 3.
In this algorithm, M(5) represents the model
using feature set 5 and scr(M) represents the e-
valuation score of model M on a development da-
ta set. It repeats two main steps until no further
performance gain is achievable:
</bodyText>
<listItem confidence="0.710410166666667">
1. Include any features from the rest of F into
the current set of candidate features if the in-
clusion would lead to a performance gain.
2. Exclude any features from the current set of
candidate templates if the exclusion would
lead to no deterioration in performance.
</listItem>
<bodyText confidence="0.998949733333333">
By repeatedly adding the useful and removing
the useless features, the algorithm aims to return
a better and smaller set of features for next round.
Only 55 of the 109 candidate features remain af-
ter using this algorithm and they are presented in
Table 4. Table 3 gives an interpretation of the ab-
breviations used in Table 4. Each feature of a word
is set to that listed in feature column if the word
satisfies the condition listed in current word col-
umn, else the feature is set to “NULL”. For ex-
ample, if the current word satisfies the condition
in the first row of Table 4 which is the first word
in the left of a NC, feature 1 of this word is set to
all words in the NC, otherwise, feature 1 is set to
“NULL”.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.991367">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.9998595">
The CoNLL-2014 training data is a corpus of
learner English provided by (Dahlmeier et al.,
2013). This corpus consists of 1,397 articles, 12K
sentences and 116K tokens. The official blind test
data consists of 50 articles, 245 sentences and 30K
tokens. More detailed information about this data
is described in (Ng et al., 2014; Dahlmeier et al.,
2013).
In development phase, the entire training corpus
is splited by sentence. 80% sentences are picked
up randomly and used for training and the rest
20% are used as the developing corpus. For the fi-
nal submission, the entire corpus is used for train-
ing.
</bodyText>
<table confidence="0.999298740740741">
Abbreviation Description
NP Noun Phrase
NC Noun Compound and is ac-
tive if second to last word in
NP is tagged as noun
VP Verb Phrase
cw Current Word
pos part-of-speech of the current
word
X.lz the ith word in the left of X
X.rz the ith word in the right of X
NP[0] the first word of NP
NP.head the head word of NP
NP.(DT or word in NP whose pos is DT
IN or TO) or IN or TO
VP.verb word in VP whose pos is ver-
b
VP.NP NP in VP
dp the dependency relation gen-
erated by standford depen-
dency parser
dp.dep the dependent in the depen-
dency relation
dp.head the head in the dependency
relation
dp.rel the type of the dependency
relation
</table>
<tableCaption confidence="0.999997">
Table 3: The interpretation of the abbrevations in
Table 4
</tableCaption>
<subsectionHeader confidence="0.997207">
4.2 Data Labeling
</subsectionHeader>
<bodyText confidence="0.999987705882353">
The labeling algorithm described in section 2.1 is
firstly applied to the training corpus. Total 7047
labels are generated and those whose count is larg-
er than 15 is presented in Table 5. Directly ap-
plying these 7047 labels for correction receives an
M2 score of precision=90.2%, recall=87.0% and
F 0.5=89.5%. However, the number of labels
is too large that the training process is time con-
suming and those labels appears only few times
will hurt the generalization of the trained model.
Therefore, labels with low frequency which ap-
pear less than 30 times are cut out and 109 labels
remain. The M2 score of the system using this re-
fined labels is precision=83.9%, recall=64.0% and
F 0.5=79.0%. Note that even applying all labels,
the F 0.5 is not 100%. It is because some annota-
tions in the training corpus are not consistency.
</bodyText>
<page confidence="0.99096">
78
</page>
<figure confidence="0.946956652173913">
current word feature
NC.l1 NC
NP.l1 NP
NP[0] NP.l1.pos
NC.l1 NC
NC.l1 NC.l1.pos
NC.l1 and pos=DT NC
NC.l1 and pos=VB NC
NP.l1 and pos=VB NP
pos=VB cw
pos=DT cw
the cw.r1
a cw.r1
an cw.r1
NP[0] cw
NP[0] NP.l1
NP[0] NP.l2
NP[0] NP.l3
NP[0] NP.l1.pos
NP[0] NP.l2.pos
NP[0] NP.l3.pos
NP.l1 NP.head
NP.l1 NP.head.pos
</figure>
<table confidence="0.969898608695652">
NP.head NP. head
NP.head NP. head.bag
NP.head NP. head.pos
NP.head NP. head.pos.bag
NP.head NP. (JJ or CC)
NP.(DT or IN or TO) NP
NP.(DT or IN or TO) NP.head
NP.(DT or IN or TO) NP.head.pos
dp.dep dp.head
dp.head dp.dep
dp.dep dp.head.pos
dp.head dp.dep.pos
dp.dep dp.rel
dp.head dp.rel
VP.verb VP.NP
VP.verb VP.NP.head
VP.NP.head VP.verb
VP.verb VP.NP.head.pos
VP.NP.head VP.verb.pos
cw cw.li, i E {0, 1, 2, 31
cw cw.ri, i E {1, 2, 31
cw cw.li.pos, i E {0, 1, 2, 31
cw cw.ri.pos, i E {1, 2, 31
</table>
<tableCaption confidence="0.9837715">
Table 4: Remained features after the feature selec-
tion.
</tableCaption>
<table confidence="0.996033770833333">
Label
O
ED
NPLURAL
THE®
LEMMA
,®
A® PAST THE IN TO . IS OF ARE FOR
GERUND,
AND ON AN® A VSINGULAR WAS THEIR
ELDERLY IT OF® THEY WITH TO®
WERE THIS ; ITS .® THAT ’S® AND®
THAT® HAVE® CAN AS HAVE®PART
FROM BE WOULD BY
HAVE HAS® WILL HAS AT AN THESE ®,
THEM IN® INTO #® ARE® WHICH PEO-
PLE HAS®PART ECONOMIC IS® BE® SO
COULD TO®LEMMA MANY PART MAY
LESS IT® FOR® BEING®
NOT ABOUT WILL®LEMMA SHOULD
HIS BECAUSE AGED SUCH ALSO
WHICH® HAVE®PAST WILL® WHO
WHEN MUCH
ON® ’ THROUGH BE®PAST MORE
IF HELP THE®ELDERLY ’S ONE AS®
THERE THEIR® WITH® HAVE®O
ECONOMY DEVELOPMENT CON-
CERNED PEOPLE® PROBLEMS BUT
MEANS THEREFORE HOWEVER BE-
ING : UP PROBLEM ’® THE®LEMMA
IN®ADDITION HOWEVER®,® AMONG
;® WHERE THUS ONLY HEALTH
HAS®PAST FUNDING EXTENT ALSO®
TECHNOLOGICAL ” OR HAD WOULD®
VERY .®THIS ITS® IMPORTANT DEVEL-
OPED ®BEEN AGE ABOUT® WHO® USE
THEY® THAN NUMBER HOWEVER®,
GOVERNMENT FURTHERMORE DURING
BUT® YOUNGER RIGHT POPULATION
PERSON® FEWER ENVIRONMENTAL-
LY WOULD®LEMMA OTHER MAY®
LIMITED HE COULD®HAVE BEEN STIL-
L SPENDING SAFETY OVER ONE®’S
MAKE MADE LIFE HUMAN HAD®
FUNDS CARE ARGUED ALL ”® WHEN®
TIME THOSE SOCIETY RESEARCH
PROVIDE OLD NEEDS INCREASING DE-
VELOPING BECOME BE®O ADDITION
</table>
<tableCaption confidence="0.988031">
Table 5: Labels whose count is larger than 15.
</tableCaption>
<table confidence="0.998788888888889">
current word feature
NC.l1 NC, cw, cw.l1, cw.l1.pos,
cw.r1, cw.r1.pos
NP[0] NP.head, NP.l1, NP.l2 ,
cw, cw.l1, cw.l1.pos,
NP.head NP[0], NP.l1, NP.l2 , cw,
cw.l1, cw.l1.pos,
dp.head cw, cw.l1, cw.l2 dp.dep,
dp.dep.pos, dp.rel
</table>
<tableCaption confidence="0.999192">
Table 6: Examples of the new generated features.
</tableCaption>
<figure confidence="0.991038769230769">
Count
1091911
31507
3637
2822
2600
948
300˜900
50˜100
20˜50
15˜20
15˜20
15˜20
</figure>
<page confidence="0.983879">
79
</page>
<subsectionHeader confidence="0.999037">
4.3 Data Refinement
</subsectionHeader>
<bodyText confidence="0.999273285714286">
The training corpus is refined before used that sen-
tences which do not contain errors are filtered out.
Only 38% of the total sentences remain. With less
training corpus, it takes less time to train the ME
model. Table 7 presents the performance of sys-
tems using the unrefined training corpus and re-
fined corpus.
</bodyText>
<table confidence="0.997802">
System Presicion Recall F 0.5
unrefined 26.99% 1.67% 6.71%
refined 11.17% 3.1% 7.34%
</table>
<tableCaption confidence="0.986233">
Table 7: Comparison of systems with differen-
t training corpus.
</tableCaption>
<bodyText confidence="0.99661675">
All sets of these systems are kept the same ex-
cept the training corpus they use. It can be seen
that the refinement also improves the performance
of the system.
</bodyText>
<subsectionHeader confidence="0.990337">
4.4 Feature Selection
</subsectionHeader>
<figureCaption confidence="0.9433115">
Figure 2 shows the results of systems with dif-
ferent feature sets. sys 10 is the system with
Figure 2: Performance of systems with different
features.
</figureCaption>
<bodyText confidence="0.9999355625">
10 randomly chosen features which are used as
the initial set of features in Algorithm 3, sys 55
is the system with the refined 55 features. With
these refined features, various of new features are
generated by combining different features. This
combination is conducted empirically that features
which are considered having relations are com-
bined to generate new features. Using this method,
165 new features are generated and total 220 fea-
tures are used in sys 220. Table 6 gives a few
of examples showing the combined features. The
performance is evaluated by the precision, recal-
l and F 0.5 score of the M2 scorer according
to (Dahlmeier and Ng, 2012). It can be seen
that sys 220 with the most number of features
achieves the best performance.
</bodyText>
<subsectionHeader confidence="0.986496">
4.5 Evaluation Result
</subsectionHeader>
<bodyText confidence="0.988329">
The final system we use is sys 220 with refined
training data, the performance of our system on the
developing corpus and the blind official test data is
presented in Table 8. The score is calculated using
M2 scorer.
</bodyText>
<table confidence="0.998573666666667">
Data Set Precision Recall F 0.5
DEV 13.52% 6.41% 11.07%
OFFICIAL 29.83% 5.16% 15.24%
</table>
<tableCaption confidence="0.998978">
Table 8: Evaluation Results
</tableCaption>
<sectionHeader confidence="0.992528" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999944714285714">
In this paper, we describe the system of Shang-
hai Jiao Tong Univerity team in the CoNLL-2014
shared task. The grammatical error detection is re-
garded as a multi-label classification task and the
correction is conducted by applying a rule-based
approach based on these labels. A single max-
imum entropy classifier is introduced to do the
multi-label classification. Various features are in-
volved and a feature selection algorithm is used
to refine these features. Finally, large amounts of
feature templates that are generated by the combi-
nation of the refined features are used. This system
achieved precision of 29.83%, recall of 5.16% and
F 0.5 of 15.24% in the official evaluation.
</bodyText>
<sectionHeader confidence="0.988804" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.8154651875">
Adam L Berger, Vincent J Della Pietra, and Stephen
A Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing. Computa-
tional linguistics, 22(1):39–71.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Better
evaluation for grammatical error correction. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (NAA-
CL 2012), pages 568–572, Montreal, Canada.
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng
Ng. 2012. NUS at the HOO 2012 Shared Task. In
Proceedings of the Seventh Workshop on Building E-
ducationalApplications Using NLP, pages 216–224,
Montr´eal, Canada, June. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.985891">
80
</page>
<reference confidence="0.999197522123894">
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The nus corpus of learner english. In Pro-
ceedings of the Eighth Workshop on Innovative Use
of NLP for Building Educational Applications (BEA
2013), pages 22–31, Atlanta, Georgia, USA.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. Hoo 2012: A report on the preposition and
determiner error correction shared task. In Proceed-
ings of the Second Workshop on Building Education-
al Applications Using NLP, pages 54–62, Montr´eal,
Canada, June. Association for Computational Lin-
guistics.
NA-RAE Han, Martin Chodorow, and Claudia Lea-
cock. 2006. Detecting Errors in English Article
Usage by Non-Native Speakers. Natural Language
Engineering, 12:115–129, 5.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013a.
Grammatical error correction as multiclass classi-
fication with single model. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages 74–81,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013b. Graph
model for chinese spell checking. In Proceedings
of the Seventh SIGHAN Workshop on Chinese Lan-
guage Processing, pages 88–92, Nagoya, Japan, Oc-
tober. Asian Federation of Natural Language Pro-
cessing.
Ting-hui Kao, Yu-wei Chang, Hsun-wen Chiu, Tzu-Hsi
Yen, Joanne Boisson, Jian-cheng Wu, and Jason S.
Chang. 2013. Conll-2013 shared task: Grammati-
cal error correction nthu system description. In Pro-
ceedings of the Seventeenth Conference on Compu-
tational Natural Language Learning: Shared Task,
pages 20–25, Sofia, Bulgaria, August. Association
for Computational Linguistics.
Ekaterina Kochmar, Øistein Andersen, and Ted
Briscoe. 2012. HOO 2012 Error Recognition and
Correction Shared Task: Cambridge University Sub-
mission Report. In Proceedings of the Seventh
Workshop on Building Educational Applications Us-
ing NLP, pages 242–250, Montr´eal, Canada, June.
Association for Computational Linguistics.
Xuezhe Ma and Hai Zhao. 2012. Fourth-order depen-
dency parsing. In Proceedings of COLING 2012:
Posters, pages 785–796, Mumbai, India, December.
The COLING 2012 Organizing Committee.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The conll-2014 shared task
on grammatical error correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task (CoNLL-2014
Shared Task), Baltimore, Maryland, USA.
Alla Rozovskaya, Mark Sammons, Joshua Gioja, and
Dan Roth. 2011. University of Illinois System in
HOO Text Correction Shared Task. In Proceedings
of the 13th European Workshop on Natural Lan-
guage Generation, pages 263–266. Association for
Computational Linguistics.
Alla Rozovskaya, Mark Sammons, and Dan Roth.
2012. The UI System in the HOO 2012 Shared Task
on Error Correction. In Proceedings of the Seventh
Workshop on Building EducationalApplications Us-
ing NLP, pages 272–280, Montr´eal, Canada, June.
Association for Computational Linguistics.
Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,
and Dan Roth. 2013. The university of illinois sys-
tem in the conll-2013 shared task. In Proceedings of
the Seventeenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 13–19,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Tetreault, Joel R and Chodorow, Martin. 2008. The
Ups and Downs of Preposition Error Detection in
ESL Writing. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics-
Volume 1, pages 865–872. Association for Compu-
tational Linguistics.
Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumi-
ta, Hai Zhao, and Bao-Liang Lu. 2013. Convert-
ing continuous-space language models into n-gram
language models for statistical machine translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
845–850, Seattle, Washington, USA, October. Asso-
ciation for Computational Linguistics.
Yang Xiang, Bo Yuan, Yaoyun Zhang, Xiaolong Wang,
Wen Zheng, and Chongqiang Wei. 2013. A hybrid
model for grammatical error correction. In Proceed-
ings of the Seventeenth Conference on Computation-
al Natural Language Learning: Shared Task, pages
115–122, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Junwen Xing, Longyue Wang, Derek F. Wong, Lidi-
a S. Chao, and Xiaodong Zeng. 2013. Um-checker:
A hybrid system for english grammatical error cor-
rection. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learn-
ing: Shared Task, pages 34–42, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Qiongkai Xu and Hai Zhao. 2012. Using deep lin-
guistic features for finding deceptive opinion spam.
In Proceedings of COLING 2012: Posters, pages
1341–1350, Mumbai, India, December. The COL-
ING 2012 Organizing Committee.
Shaohua Yang, Hai Zhao, Xiaolin Wang, and Bao
liang Lu. 2012. Spell checking for chinese.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Mehmet U˘gur Do˘gan,
Bente Maegaard, Joseph Mariani, Jan Odijk, and
</reference>
<page confidence="0.983549">
81
</page>
<reference confidence="0.999559113636364">
Stelios Piperidis, editors, Proceedings of the Eighth
International Conference on Language Resources
and Evaluation (LREC-2012), pages 730–736, Is-
tanbul, Turkey, May. European Language Resources
Association (ELRA). ACL Anthology Identifier:
L12-1423.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuza-
wa, Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumo-
to. 2013. Naist at 2013 conll grammatical error
correction shared task. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 26–33, Sofi-
a, Bulgaria, August. Association for Computational
Linguistics.
Jingyi Zhang and Hai Zhao. 2013. Improving function
word alignment with frequency and syntactic infor-
mation. In Proceedings of the Twenty-Third inter-
national joint conference on Artificial Intelligence,
pages 2211–2217. AAAI Press, August.
Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009a.
Semantic dependency parsing of nombank and prop-
bank: An efficient integrated approach via a large-
scale feature selection. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 30–39, Singapore, August.
Association for Computational Linguistics.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009b. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural
Language Processing of the AFNLP, pages 55–63,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Hai Zhao, Xiaotian Zhang, and Chunyu Kit. 2013. In-
tegrative semantic dependency parsing via efficien-
t large-scale feature selection. Journal of Artificial
Intelligence Research, 46:203–233.
Hai Zhao. 2009. Character-level dependencies in chi-
nese: Usefulness and learning. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL 2009), pages 879–887, Athens, Greece,
March. Association for Computational Linguistics.
</reference>
<page confidence="0.999126">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.434130">
<title confidence="0.8599405">Grammatical Error Detection and a Single Maximum Entropy</title>
<author confidence="0.787964">Zhongye Jia Wang</author>
<author confidence="0.787964">Hai</author>
<affiliation confidence="0.87620475">Key Laboratory of Shanghai Education Commission Intelligent Interaction and Cognitive Center for Brain-Like Computing and Machine Department of Computer Science and Engineering, Shanghai Jiao Tong</affiliation>
<address confidence="0.993502">800 Dongchuan Road, Shanghai 200240,</address>
<abstract confidence="0.989322411764706">This paper describes the system of Shanghai Jiao Tong Unvierity team in the CoNLL-2014 shared task. Error correction operations are encoded as a group of predefined labels and therefore the task is formulized as a multi-label classification task. For training, labels are obtained through a strict rule-based approach. For decoding, errors are detected and corrected according to the classification results. A single maximum entropy model is used for the classification implementation incorporated with an improved feature selection algorithm. Our system achieved precision of 29.83, recall of 5.16 and F 0.5 of 15.24 in the official evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam L Berger</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational linguistics,</journal>
<pages>22--1</pages>
<contexts>
<context position="3764" citStr="Berger et al., 1996" startWordPosition="583" endWordPosition="586"> et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process is needed to deal with the conflict or error propagation of every sub-models. Here is a glance of this method: A set of more detailed error types are generated automatically from the original 28 types of errors. The detailed error type can be regarded as the label of a word, thus the task of grammatical error detection is transformed to a multi-label classification task using maximum entropy model (Berger et al., 1996; Zhao et al., 2013). A feature selection approach is introduced to get effective features from large amounts of feature candidates. Once errors are detected through word label classification, a rule-based method is used to make corrections according to their labels. The rest of the paper is organized as follows. Section 2 describes the system architecture. Section 3 introduces the feature selection approach 74 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 74–82, Baltimore, Maryland, 26-27 July 2014. c�2014 Association for Computational</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam L Berger, Vincent J Della Pietra, and Stephen A Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2012),</booktitle>
<pages>568--572</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="22386" citStr="Dahlmeier and Ng, 2012" startWordPosition="3936" endWordPosition="3939">h are used as the initial set of features in Algorithm 3, sys 55 is the system with the refined 55 features. With these refined features, various of new features are generated by combining different features. This combination is conducted empirically that features which are considered having relations are combined to generate new features. Using this method, 165 new features are generated and total 220 features are used in sys 220. Table 6 gives a few of examples showing the combined features. The performance is evaluated by the precision, recall and F 0.5 score of the M2 scorer according to (Dahlmeier and Ng, 2012). It can be seen that sys 220 with the most number of features achieves the best performance. 4.5 Evaluation Result The final system we use is sys 220 with refined training data, the performance of our system on the developing corpus and the blind official test data is presented in Table 8. The score is calculated using M2 scorer. Data Set Precision Recall F 0.5 DEV 13.52% 6.41% 11.07% OFFICIAL 29.83% 5.16% 15.24% Table 8: Evaluation Results 5 Conclusion In this paper, we describe the system of Shanghai Jiao Tong Univerity team in the CoNLL-2014 shared task. The grammatical error detection is </context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012. Better evaluation for grammatical error correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2012), pages 568–572, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
</authors>
<title>Hwee Tou Ng, and Eric Jun Feng Ng.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building EducationalApplications Using NLP,</booktitle>
<pages>216--224</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<marker>Dahlmeier, 2012</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng Ng. 2012. NUS at the HOO 2012 Shared Task. In Proceedings of the Seventh Workshop on Building EducationalApplications Using NLP, pages 216–224, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a large annotated corpus of learner english: The nus corpus of learner english.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2013),</booktitle>
<pages>22--31</pages>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="16432" citStr="Dahlmeier et al., 2013" startWordPosition="2897" endWordPosition="2900">orithm and they are presented in Table 4. Table 3 gives an interpretation of the abbreviations used in Table 4. Each feature of a word is set to that listed in feature column if the word satisfies the condition listed in current word column, else the feature is set to “NULL”. For example, if the current word satisfies the condition in the first row of Table 4 which is the first word in the left of a NC, feature 1 of this word is set to all words in the NC, otherwise, feature 1 is set to “NULL”. 4 Experiment 4.1 Data Sets The CoNLL-2014 training data is a corpus of learner English provided by (Dahlmeier et al., 2013). This corpus consists of 1,397 articles, 12K sentences and 116K tokens. The official blind test data consists of 50 articles, 245 sentences and 30K tokens. More detailed information about this data is described in (Ng et al., 2014; Dahlmeier et al., 2013). In development phase, the entire training corpus is splited by sentence. 80% sentences are picked up randomly and used for training and the rest 20% are used as the developing corpus. For the final submission, the entire corpus is used for training. Abbreviation Description NP Noun Phrase NC Noun Compound and is active if second to last wor</context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a large annotated corpus of learner english: The nus corpus of learner english. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2013), pages 22–31, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ilya Anisimoff</author>
<author>George Narroway</author>
</authors>
<title>Hoo 2012: A report on the preposition and determiner error correction shared task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>54--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1734" citStr="Dale et al., 2012" startWordPosition="253" endWordPosition="256">ntroduction The task of CoNLL-2014 is grammatical error correction which consists of detecting and correcting the grammatical errors in English essays written by non-native speakers (Ng et al., 2014). The research of grammatical error correction can potentially help millions of people in the world who are learning English as foreign language. Although there have been many works on grammatical error correction, the current approaches mainly focus on very limited error types and the result is far from satisfactory. The CoNLL-2014 shared task, compared with the previous Help Our Own (HOO) tasks (Dale et al., 2012) considering only determiner and preposition errors and the CoNLL-2013 shared task fo∗This work was partially supported by the National Natural Science Foundation of China (Grant No.60903119, Grant No.61170114, and Grant No.61272248), the National Basic Research Program of China (Grant No.2013CB329401), the Science and Technology Commission of Shanghai Municipality (Grant No.13511500200), and the European Union Seventh Framework Program (Grant No.247619). †Corresponding author cusing on five major types of errors, requires to correct all 28 types of errors (Ng et al., 2014). One traditional st</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>Robert Dale, Ilya Anisimoff, and George Narroway. 2012. Hoo 2012: A report on the preposition and determiner error correction shared task. In Proceedings of the Second Workshop on Building Educational Applications Using NLP, pages 54–62, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NA-RAE Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting Errors in English Article Usage by Non-Native Speakers.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<volume>12</volume>
<pages>5</pages>
<contexts>
<context position="13491" citStr="Han et al., 2006" startWordPosition="2328" endWordPosition="2331">f maximum entropy classifier is the features it used. A good feature that contains useful information to guide classification will significantly improve the performance of the classifier. One direct way to involve more good features is involving more features. In our approach, large amounts of candidate features are collected at first. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorith</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>NA-RAE Han, Martin Chodorow, and Claudia Leacock. 2006. Detecting Errors in English Article Usage by Non-Native Speakers. Natural Language Engineering, 12:115–129, 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongye Jia</author>
<author>Peilu Wang</author>
<author>Hai Zhao</author>
</authors>
<title>Grammatical error correction as multiclass classification with single model.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>74--81</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3158" citStr="Jia et al., 2013" startWordPosition="480" endWordPosition="483">and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process is needed to deal with the conflict or error propagation of every sub-models. Here is a glance of this method: A set of more detailed error types are generated automatically from the original 28 types of errors. The detailed error type can be regarded as the label of a word, thus the task of grammatical error detection is transformed to a multi-label classification task using maximum entropy model (Berger et al.</context>
<context position="13663" citStr="Jia et al., 2013" startWordPosition="2356" endWordPosition="2359">the classifier. One direct way to involve more good features is involving more features. In our approach, large amounts of candidate features are collected at first. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Selection 1: INPUT: all feature candidates F 2: OUTPUT: selected features S 3: S = {f0, fi, ... , fk}, a random subset of F 4: while do 5: C = RECRUITMOR</context>
</contexts>
<marker>Jia, Wang, Zhao, 2013</marker>
<rawString>Zhongye Jia, Peilu Wang, and Hai Zhao. 2013a. Grammatical error correction as multiclass classification with single model. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 74–81, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongye Jia</author>
<author>Peilu Wang</author>
<author>Hai Zhao</author>
</authors>
<title>Graph model for chinese spell checking.</title>
<date>2013</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>88--92</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="3158" citStr="Jia et al., 2013" startWordPosition="480" endWordPosition="483">and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process is needed to deal with the conflict or error propagation of every sub-models. Here is a glance of this method: A set of more detailed error types are generated automatically from the original 28 types of errors. The detailed error type can be regarded as the label of a word, thus the task of grammatical error detection is transformed to a multi-label classification task using maximum entropy model (Berger et al.</context>
<context position="13663" citStr="Jia et al., 2013" startWordPosition="2356" endWordPosition="2359">the classifier. One direct way to involve more good features is involving more features. In our approach, large amounts of candidate features are collected at first. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Selection 1: INPUT: all feature candidates F 2: OUTPUT: selected features S 3: S = {f0, fi, ... , fk}, a random subset of F 4: while do 5: C = RECRUITMOR</context>
</contexts>
<marker>Jia, Wang, Zhao, 2013</marker>
<rawString>Zhongye Jia, Peilu Wang, and Hai Zhao. 2013b. Graph model for chinese spell checking. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages 88–92, Nagoya, Japan, October. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting-hui Kao</author>
<author>Yu-wei Chang</author>
<author>Hsun-wen Chiu</author>
<author>Tzu-Hsi Yen</author>
<author>Joanne Boisson</author>
<author>Jian-cheng Wu</author>
<author>Jason S Chang</author>
</authors>
<title>Conll-2013 shared task: Grammatical error correction nthu system description.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2678" citStr="Kao et al., 2013" startWordPosition="398" endWordPosition="401">ogy Commission of Shanghai Municipality (Grant No.13511500200), and the European Union Seventh Framework Program (Grant No.247619). †Corresponding author cusing on five major types of errors, requires to correct all 28 types of errors (Ng et al., 2014). One traditional strategy is designing a system combined of a set of sub-models, where each submodel is specialized for a specific subtask, for example, correcting one type of errors. This strategy is computationally efficient and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way featu</context>
</contexts>
<marker>Kao, Chang, Chiu, Yen, Boisson, Wu, Chang, 2013</marker>
<rawString>Ting-hui Kao, Yu-wei Chang, Hsun-wen Chiu, Tzu-Hsi Yen, Joanne Boisson, Jian-cheng Wu, and Jason S. Chang. 2013. Conll-2013 shared task: Grammatical error correction nthu system description. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 20–25, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Kochmar</author>
<author>Øistein Andersen</author>
<author>Ted Briscoe</author>
</authors>
<title>Error Recognition and Correction Shared Task: Cambridge University Submission Report.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>242--250</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>HOO</location>
<contexts>
<context position="5746" citStr="Kochmar et al., 2012" startWordPosition="909" endWordPosition="912">n is just applying the edit operation mapped by each label, which is basically the reverse of the labeling phase. 2.1 Data Labeling In CoNLL-2014 shared task, there are 28 error types but they can not be used directly as class labels, since these types are too general that they can hardly be corrected by applying one rule-based edit. For example, the correction of Vform (verb form) error type includes all verb form inflections such as converting a verb to its infinitive form, gerund form, past form and past participle and so on. Previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Kochmar et al., 2012) manually decompose each error types to more detailed subtypes. For example, in (Dahlmeier et al., 2012), the determinater errors are decomposed into: • replacement determiner (RD): { a → the } • missing determiner (MD): { E → a } • unwanted determiner (UD): { a → E } For a task with a few error types such as merely determinative and preposition error in HOO 2012, manually decomposition may be sufficient. However, for CoNLL-2014, all 28 error types are required to be corrected and some of these types such as Rloc- (Local redundancy) and Um (Unclear meaning) are quite complex that the manual de</context>
</contexts>
<marker>Kochmar, Andersen, Briscoe, 2012</marker>
<rawString>Ekaterina Kochmar, Øistein Andersen, and Ted Briscoe. 2012. HOO 2012 Error Recognition and Correction Shared Task: Cambridge University Submission Report. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 242–250, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuezhe Ma</author>
<author>Hai Zhao</author>
</authors>
<title>Fourth-order dependency parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>785--796</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="13828" citStr="Ma and Zhao, 2012" startWordPosition="2387" endWordPosition="2390">. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Selection 1: INPUT: all feature candidates F 2: OUTPUT: selected features S 3: S = {f0, fi, ... , fk}, a random subset of F 4: while do 5: C = RECRUITMORE(S) 6: if C = {} then 7: return S 8: end if 9: S′ = SHAKEOFF(S+C) 10: if scr(M(S)) ≥ scr(M(S′)) then 11: return S 12: end if 13: S = S′ 14: end while 15: function R</context>
</contexts>
<marker>Ma, Zhao, 2012</marker>
<rawString>Xuezhe Ma and Hai Zhao. 2012. Fourth-order dependency parsing. In Proceedings of COLING 2012: Posters, pages 785–796, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Ted Briscoe</author>
<author>Christian Hadiwinoto</author>
<author>Raymond Hendy Susanto</author>
<author>Christopher Bryant</author>
</authors>
<title>The conll-2014 shared task on grammatical error correction.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task),</booktitle>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="1315" citStr="Ng et al., 2014" startWordPosition="185" endWordPosition="188">fication task. For training, labels are obtained through a strict rule-based approach. For decoding, errors are detected and corrected according to the classification results. A single maximum entropy model is used for the classification implementation incorporated with an improved feature selection algorithm. Our system achieved precision of 29.83, recall of 5.16 and F 0.5 of 15.24 in the official evaluation. 1 Introduction The task of CoNLL-2014 is grammatical error correction which consists of detecting and correcting the grammatical errors in English essays written by non-native speakers (Ng et al., 2014). The research of grammatical error correction can potentially help millions of people in the world who are learning English as foreign language. Although there have been many works on grammatical error correction, the current approaches mainly focus on very limited error types and the result is far from satisfactory. The CoNLL-2014 shared task, compared with the previous Help Our Own (HOO) tasks (Dale et al., 2012) considering only determiner and preposition errors and the CoNLL-2013 shared task fo∗This work was partially supported by the National Natural Science Foundation of China (Grant No</context>
<context position="16663" citStr="Ng et al., 2014" startWordPosition="2935" endWordPosition="2938">n, else the feature is set to “NULL”. For example, if the current word satisfies the condition in the first row of Table 4 which is the first word in the left of a NC, feature 1 of this word is set to all words in the NC, otherwise, feature 1 is set to “NULL”. 4 Experiment 4.1 Data Sets The CoNLL-2014 training data is a corpus of learner English provided by (Dahlmeier et al., 2013). This corpus consists of 1,397 articles, 12K sentences and 116K tokens. The official blind test data consists of 50 articles, 245 sentences and 30K tokens. More detailed information about this data is described in (Ng et al., 2014; Dahlmeier et al., 2013). In development phase, the entire training corpus is splited by sentence. 80% sentences are picked up randomly and used for training and the rest 20% are used as the developing corpus. For the final submission, the entire corpus is used for training. Abbreviation Description NP Noun Phrase NC Noun Compound and is active if second to last word in NP is tagged as noun VP Verb Phrase cw Current Word pos part-of-speech of the current word X.lz the ith word in the left of X X.rz the ith word in the right of X NP[0] the first word of NP NP.head the head word of NP NP.(DT or</context>
</contexts>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, Bryant, 2014</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. 2014. The conll-2014 shared task on grammatical error correction. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task), Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Mark Sammons</author>
<author>Joshua Gioja</author>
<author>Dan Roth</author>
</authors>
<title>System in HOO Text Correction Shared Task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>263--266</pages>
<publisher>Association</publisher>
<institution>University of Illinois</institution>
<contexts>
<context position="13516" citStr="Rozovskaya et al., 2011" startWordPosition="2332" endWordPosition="2335">classifier is the features it used. A good feature that contains useful information to guide classification will significantly improve the performance of the classifier. One direct way to involve more good features is involving more features. In our approach, large amounts of candidate features are collected at first. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Select</context>
</contexts>
<marker>Rozovskaya, Sammons, Gioja, Roth, 2011</marker>
<rawString>Alla Rozovskaya, Mark Sammons, Joshua Gioja, and Dan Roth. 2011. University of Illinois System in HOO Text Correction Shared Task. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 263–266. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>Shared Task on Error Correction.</title>
<date>2012</date>
<booktitle>The UI System in the HOO</booktitle>
<pages>272--280</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5723" citStr="Rozovskaya et al., 2012" startWordPosition="905" endWordPosition="908">ammatical error correction is just applying the edit operation mapped by each label, which is basically the reverse of the labeling phase. 2.1 Data Labeling In CoNLL-2014 shared task, there are 28 error types but they can not be used directly as class labels, since these types are too general that they can hardly be corrected by applying one rule-based edit. For example, the correction of Vform (verb form) error type includes all verb form inflections such as converting a verb to its infinitive form, gerund form, past form and past participle and so on. Previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Kochmar et al., 2012) manually decompose each error types to more detailed subtypes. For example, in (Dahlmeier et al., 2012), the determinater errors are decomposed into: • replacement determiner (RD): { a → the } • missing determiner (MD): { E → a } • unwanted determiner (UD): { a → E } For a task with a few error types such as merely determinative and preposition error in HOO 2012, manually decomposition may be sufficient. However, for CoNLL-2014, all 28 error types are required to be corrected and some of these types such as Rloc- (Local redundancy) and Um (Unclear meaning) are quite com</context>
<context position="13473" citStr="Rozovskaya et al., 2012" startWordPosition="2324" endWordPosition="2327">fecting the performance of maximum entropy classifier is the features it used. A good feature that contains useful information to guide classification will significantly improve the performance of the classifier. One direct way to involve more good features is involving more features. In our approach, large amounts of candidate features are collected at first. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME </context>
</contexts>
<marker>Rozovskaya, Sammons, Roth, 2012</marker>
<rawString>Alla Rozovskaya, Mark Sammons, and Dan Roth. 2012. The UI System in the HOO 2012 Shared Task on Error Correction. In Proceedings of the Seventh Workshop on Building EducationalApplications Using NLP, pages 272–280, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Kai-Wei Chang</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>The university of illinois system in the conll-2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>13--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2660" citStr="Rozovskaya et al., 2013" startWordPosition="394" endWordPosition="397">, the Science and Technology Commission of Shanghai Municipality (Grant No.13511500200), and the European Union Seventh Framework Program (Grant No.247619). †Corresponding author cusing on five major types of errors, requires to correct all 28 types of errors (Ng et al., 2014). One traditional strategy is designing a system combined of a set of sub-models, where each submodel is specialized for a specific subtask, for example, correcting one type of errors. This strategy is computationally efficient and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit th</context>
</contexts>
<marker>Rozovskaya, Chang, Sammons, Roth, 2013</marker>
<rawString>Alla Rozovskaya, Kai-Wei Chang, Mark Sammons, and Dan Roth. 2013. The university of illinois system in the conll-2013 shared task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 13–19, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel R Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>The Ups and Downs of Preposition Error Detection in ESL Writing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational LinguisticsVolume 1,</booktitle>
<pages>865--872</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>Tetreault, Joel R and Chodorow, Martin. 2008. The Ups and Downs of Preposition Error Detection in ESL Writing. In Proceedings of the 22nd International Conference on Computational LinguisticsVolume 1, pages 865–872. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>Masao Utiyama</author>
<author>Isao Goto</author>
<author>Eiichro Sumita</author>
<author>Hai Zhao</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Converting continuous-space language models into n-gram language models for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>845--850</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="13768" citStr="Wang et al., 2013" startWordPosition="2375" endWordPosition="2378">, large amounts of candidate features are collected at first. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Selection 1: INPUT: all feature candidates F 2: OUTPUT: selected features S 3: S = {f0, fi, ... , fk}, a random subset of F 4: while do 5: C = RECRUITMORE(S) 6: if C = {} then 7: return S 8: end if 9: S′ = SHAKEOFF(S+C) 10: if scr(M(S)) ≥ scr(M(S′)) then 11:</context>
</contexts>
<marker>Wang, Utiyama, Goto, Sumita, Zhao, Lu, 2013</marker>
<rawString>Rui Wang, Masao Utiyama, Isao Goto, Eiichro Sumita, Hai Zhao, and Bao-Liang Lu. 2013. Converting continuous-space language models into n-gram language models for statistical machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 845–850, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Xiang</author>
<author>Bo Yuan</author>
<author>Yaoyun Zhang</author>
<author>Xiaolong Wang</author>
<author>Wen Zheng</author>
<author>Chongqiang Wei</author>
</authors>
<title>A hybrid model for grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>115--122</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2742" citStr="Xiang et al., 2013" startWordPosition="410" endWordPosition="413">), and the European Union Seventh Framework Program (Grant No.247619). †Corresponding author cusing on five major types of errors, requires to correct all 28 types of errors (Ng et al., 2014). One traditional strategy is designing a system combined of a set of sub-models, where each submodel is specialized for a specific subtask, for example, correcting one type of errors. This strategy is computationally efficient and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process</context>
</contexts>
<marker>Xiang, Yuan, Zhang, Wang, Zheng, Wei, 2013</marker>
<rawString>Yang Xiang, Bo Yuan, Yaoyun Zhang, Xiaolong Wang, Wen Zheng, and Chongqiang Wei. 2013. A hybrid model for grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 115–122, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junwen Xing</author>
<author>Longyue Wang</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Xiaodong Zeng</author>
</authors>
<title>Um-checker: A hybrid system for english grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>34--42</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2697" citStr="Xing et al., 2013" startWordPosition="402" endWordPosition="405">Shanghai Municipality (Grant No.13511500200), and the European Union Seventh Framework Program (Grant No.247619). †Corresponding author cusing on five major types of errors, requires to correct all 28 types of errors (Ng et al., 2014). One traditional strategy is designing a system combined of a set of sub-models, where each submodel is specialized for a specific subtask, for example, correcting one type of errors. This strategy is computationally efficient and can adopt different favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefi</context>
</contexts>
<marker>Xing, Wang, Wong, Chao, Zeng, 2013</marker>
<rawString>Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S. Chao, and Xiaodong Zeng. 2013. Um-checker: A hybrid system for english grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 34–42, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qiongkai Xu</author>
<author>Hai Zhao</author>
</authors>
<title>Using deep linguistic features for finding deceptive opinion spam.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>1341--1350</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="13809" citStr="Xu and Zhao, 2012" startWordPosition="2383" endWordPosition="2386"> collected at first. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Selection 1: INPUT: all feature candidates F 2: OUTPUT: selected features S 3: S = {f0, fi, ... , fk}, a random subset of F 4: while do 5: C = RECRUITMORE(S) 6: if C = {} then 7: return S 8: end if 9: S′ = SHAKEOFF(S+C) 10: if scr(M(S)) ≥ scr(M(S′)) then 11: return S 12: end if 13: S = S′ 14: end w</context>
</contexts>
<marker>Xu, Zhao, 2012</marker>
<rawString>Qiongkai Xu and Hai Zhao. 2012. Using deep linguistic features for finding deceptive opinion spam. In Proceedings of COLING 2012: Posters, pages 1341–1350, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Shaohua Yang</author>
<author>Hai Zhao</author>
<author>Xiaolin Wang</author>
<author>Bao liang Lu</author>
</authors>
<title>Spell checking for chinese.</title>
<date>2012</date>
<booktitle>Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012),</booktitle>
<pages>730--736</pages>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet U˘gur Do˘gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="13684" citStr="Yang et al., 2012" startWordPosition="2360" endWordPosition="2363"> direct way to involve more good features is involving more features. In our approach, large amounts of candidate features are collected at first. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Selection 1: INPUT: all feature candidates F 2: OUTPUT: selected features S 3: S = {f0, fi, ... , fk}, a random subset of F 4: while do 5: C = RECRUITMORE(S) 6: if C = {} the</context>
</contexts>
<marker>Yang, Zhao, Wang, Lu, 2012</marker>
<rawString>Shaohua Yang, Hai Zhao, Xiaolin Wang, and Bao liang Lu. 2012. Spell checking for chinese. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet U˘gur Do˘gan, Bente Maegaard, Joseph Mariani, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012), pages 730–736, Istanbul, Turkey, May. European Language Resources Association (ELRA). ACL Anthology Identifier: L12-1423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ippei Yoshimoto</author>
</authors>
<title>Tomoya Kose, Kensuke Mitsuzawa, Keisuke Sakaguchi, Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi, and Yuji</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>26--33</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Yoshimoto, 2013</marker>
<rawString>Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa, Keisuke Sakaguchi, Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi, and Yuji Matsumoto. 2013. Naist at 2013 conll grammatical error correction shared task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 26–33, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingyi Zhang</author>
<author>Hai Zhao</author>
</authors>
<title>Improving function word alignment with frequency and syntactic information.</title>
<date>2013</date>
<booktitle>In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence,</booktitle>
<pages>2211--2217</pages>
<publisher>AAAI Press,</publisher>
<contexts>
<context position="13790" citStr="Zhang and Zhao, 2013" startWordPosition="2379" endWordPosition="2382">candidate features are collected at first. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Selection 1: INPUT: all feature candidates F 2: OUTPUT: selected features S 3: S = {f0, fi, ... , fk}, a random subset of F 4: while do 5: C = RECRUITMORE(S) 6: if C = {} then 7: return S 8: end if 9: S′ = SHAKEOFF(S+C) 10: if scr(M(S)) ≥ scr(M(S′)) then 11: return S 12: end if 1</context>
</contexts>
<marker>Zhang, Zhao, 2013</marker>
<rawString>Jingyi Zhang and Hai Zhao. 2013. Improving function word alignment with frequency and syntactic information. In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages 2211–2217. AAAI Press, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Wenliang Chen</author>
<author>Chunyu Kit</author>
</authors>
<title>Semantic dependency parsing of nombank and propbank: An efficient integrated approach via a largescale feature selection.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>30--39</pages>
<institution>Singapore, August. Association for Computational Linguistics.</institution>
<contexts>
<context position="3178" citStr="Zhao et al., 2009" startWordPosition="484" endWordPosition="487">rent favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process is needed to deal with the conflict or error propagation of every sub-models. Here is a glance of this method: A set of more detailed error types are generated automatically from the original 28 types of errors. The detailed error type can be regarded as the label of a word, thus the task of grammatical error detection is transformed to a multi-label classification task using maximum entropy model (Berger et al., 1996; Zhao et al.,</context>
<context position="13859" citStr="Zhao et al., 2009" startWordPosition="2393" endWordPosition="2396">ors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Selection 1: INPUT: all feature candidates F 2: OUTPUT: selected features S 3: S = {f0, fi, ... , fk}, a random subset of F 4: while do 5: C = RECRUITMORE(S) 6: if C = {} then 7: return S 8: end if 9: S′ = SHAKEOFF(S+C) 10: if scr(M(S)) ≥ scr(M(S′)) then 11: return S 12: end if 13: S = S′ 14: end while 15: function RECRUITMORE(S) 16: C = {}, and p</context>
</contexts>
<marker>Zhao, Chen, Kit, 2009</marker>
<rawString>Hai Zhao, Wenliang Chen, and Chunyu Kit. 2009a. Semantic dependency parsing of nombank and propbank: An efficient integrated approach via a largescale feature selection. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 30–39, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Yan Song</author>
<author>Chunyu Kit</author>
<author>Guodong Zhou</author>
</authors>
<title>Cross language dependency parsing using a bilingual lexicon.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>55--63</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="3178" citStr="Zhao et al., 2009" startWordPosition="484" endWordPosition="487">rent favorable features for each subtask. Top ranked systems in CoNLL-2013 (Rozovskaya et al., 2013; Kao et al., 2013; Xing et al., 2013; Yoshimoto et al., 2013; Xiang et al., 2013) are based on this strategy. However, the division of the model relies on prior-knowledges and the designing of different features for each sub-model requires a large amount of manual works. This shortage is especially notable in CoNLL-2014 shared task, since the number of error types is much larger and the composition of errors is more complicated than before. In contrast, we follow the work in (Jia et al., 2013a; Zhao et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process is needed to deal with the conflict or error propagation of every sub-models. Here is a glance of this method: A set of more detailed error types are generated automatically from the original 28 types of errors. The detailed error type can be regarded as the label of a word, thus the task of grammatical error detection is transformed to a multi-label classification task using maximum entropy model (Berger et al., 1996; Zhao et al.,</context>
<context position="13859" citStr="Zhao et al., 2009" startWordPosition="2393" endWordPosition="2396">ors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Selection 1: INPUT: all feature candidates F 2: OUTPUT: selected features S 3: S = {f0, fi, ... , fk}, a random subset of F 4: while do 5: C = RECRUITMORE(S) 6: if C = {} then 7: return S 8: end if 9: S′ = SHAKEOFF(S+C) 10: if scr(M(S)) ≥ scr(M(S′)) then 11: return S 12: end if 13: S = S′ 14: end while 15: function RECRUITMORE(S) 16: C = {}, and p</context>
</contexts>
<marker>Zhao, Song, Kit, Zhou, 2009</marker>
<rawString>Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou. 2009b. Cross language dependency parsing using a bilingual lexicon. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 55–63, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Xiaotian Zhang</author>
<author>Chunyu Kit</author>
</authors>
<title>Integrative semantic dependency parsing via efficient large-scale feature selection.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>46--203</pages>
<contexts>
<context position="3784" citStr="Zhao et al., 2013" startWordPosition="587" endWordPosition="590">et al., 2009a), integrating everything into one model. This integrated system holds a merit that a one-way feature selection benefits the whole system and no additional process is needed to deal with the conflict or error propagation of every sub-models. Here is a glance of this method: A set of more detailed error types are generated automatically from the original 28 types of errors. The detailed error type can be regarded as the label of a word, thus the task of grammatical error detection is transformed to a multi-label classification task using maximum entropy model (Berger et al., 1996; Zhao et al., 2013). A feature selection approach is introduced to get effective features from large amounts of feature candidates. Once errors are detected through word label classification, a rule-based method is used to make corrections according to their labels. The rest of the paper is organized as follows. Section 2 describes the system architecture. Section 3 introduces the feature selection approach 74 Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 74–82, Baltimore, Maryland, 26-27 July 2014. c�2014 Association for Computational Linguistics and the</context>
<context position="12740" citStr="Zhao et al., 2013" startWordPosition="2203" endWordPosition="2206"> 21: L ← ″22: end if 23: end while 24: L ← upper case of L 25: return L Tokens Edit Label to reveal ⊖ {to reveal→revealing} GERUND a {a woman→women} ⊖ woman NPLURAL developing {developing world THE⊕ wold →the developing world} ⊙ a {a→ e} ⊖ in {in→on} ON apple {apple→an apple} AN⊕ Table 2: Examples of labeling 2.2 Label Classification Using the approach described above, the training corpus is converted to a sequence of words with labels. Maximum entropy model is used as the classifier. It allows a very rich set of features to be used in a model and has shown good performance in similiar tasks (Zhao et al., 2013). The features we used are discussed in the next section. 3 Feature Selection and Generation One key factor affecting the performance of maximum entropy classifier is the features it used. A good feature that contains useful information to guide classification will significantly improve the performance of the classifier. One direct way to involve more good features is involving more features. In our approach, large amounts of candidate features are collected at first. We carefully examine the factors involved in a wide range of features that have been or can be used to the word label classific</context>
<context position="15071" citStr="Zhao et al., 2013" startWordPosition="2645" endWordPosition="2648"> and p = scr(M(S)) 17: for each f ∈ F − S do 18: if p &lt; scr(M(S + {f})) then 19: C = C + {f} 20: end if 21: end for 22: end function 23: function SHAKEOFF(S) 24: while do 25: S′ = S0 = S 26: for each f ∈ S do 27: if scr(M(S′)) &lt; scr(M(S′ − {f})) then 28: S′ = S′ − {f} 29: end if 30: end for 31: S = S′ 32: if S′ = So then 33: return S′ 34: end if 35: end while 36: end function Therefore a feature selection algorithm is introduced to filter out “bad” features at first and the remaining features will be used to generate new features. The feature selection algorithm has shown 77 effectiveness in (Zhao et al., 2013) and is presented in Algorithm 3. In this algorithm, M(5) represents the model using feature set 5 and scr(M) represents the evaluation score of model M on a development data set. It repeats two main steps until no further performance gain is achievable: 1. Include any features from the rest of F into the current set of candidate features if the inclusion would lead to a performance gain. 2. Exclude any features from the current set of candidate templates if the exclusion would lead to no deterioration in performance. By repeatedly adding the useful and removing the useless features, the algor</context>
</contexts>
<marker>Zhao, Zhang, Kit, 2013</marker>
<rawString>Hai Zhao, Xiaotian Zhang, and Chunyu Kit. 2013. Integrative semantic dependency parsing via efficient large-scale feature selection. Journal of Artificial Intelligence Research, 46:203–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
</authors>
<title>Character-level dependencies in chinese: Usefulness and learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>879--887</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="13840" citStr="Zhao, 2009" startWordPosition="2391" endWordPosition="2392">ine the factors involved in a wide range of features that have been or can be used to the word label classification task. Many features that are considered effective in various of previous works (Dahlmeier et al., 2012; Rozovskaya et al., 2012; Han et al., 2006; Rozovskaya et al., 2011; Tetreault, Joel R and Chodorow, Martin, 2008) are included. Besides, features that are used in the similar spell checking tasks (Jia et al., 2013b; Yang et al., 2012) and some novel features showing effectiveness in other NLP tasks (Wang et al., 2013; Zhang and Zhao, 2013; Xu and Zhao, 2012; Ma and Zhao, 2012; Zhao, 2009; Zhao et al., 2009b) are also included. However, using too many features is time consuming. Besides, it increases the probability of overfitting and may lead to a poor solution of the maximum-likelihood parameter estimate in the ME training. Algorithm 3 Greedy Feature Selection 1: INPUT: all feature candidates F 2: OUTPUT: selected features S 3: S = {f0, fi, ... , fk}, a random subset of F 4: while do 5: C = RECRUITMORE(S) 6: if C = {} then 7: return S 8: end if 9: S′ = SHAKEOFF(S+C) 10: if scr(M(S)) ≥ scr(M(S′)) then 11: return S 12: end if 13: S = S′ 14: end while 15: function RECRUITMORE(S</context>
</contexts>
<marker>Zhao, 2009</marker>
<rawString>Hai Zhao. 2009. Character-level dependencies in chinese: Usefulness and learning. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 879–887, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
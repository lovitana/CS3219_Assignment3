<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000477">
<title confidence="0.942923">
Chunking Clinical Text Containing Non-Canonical Language
</title>
<note confidence="0.418353">
Jackie Cassell
Primary Care and Public Health
</note>
<author confidence="0.966749">
Aleksandar Savkov
</author>
<affiliation confidence="0.9951955">
Department of Informatics
University of Sussex
</affiliation>
<address confidence="0.759168">
Brighton, UK
</address>
<email confidence="0.99711">
a.savkov@sussex.ac.uk
</email>
<author confidence="0.995307">
John Carroll
</author>
<affiliation confidence="0.9962085">
Department of Informatics
University of Sussex
</affiliation>
<address confidence="0.758632">
Brighton, UK
</address>
<email confidence="0.992867">
j.a.carroll@sussex.ac.uk
</email>
<note confidence="0.8338135">
Brighton and Sussex Medical School
Brighton, UK
</note>
<email confidence="0.985933">
j.cassell@bsms.ac.uk
</email>
<sectionHeader confidence="0.993618" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958055555556">
Free text notes typed by primary care
physicians during patient consultations
typically contain highly non-canonical
language. Shallow syntactic analysis of
free text notes can help to reveal valu-
able information for the study of disease
and treatment. We present an exploratory
study into chunking such text using off-
the-shelf language processing tools and
pre-trained statistical models. We evalu-
ate chunking accuracy with respect to part-
of-speech tagging quality, choice of chunk
representation, and breadth of context fea-
tures. Our results indicate that narrow con-
text feature windows give the best results,
but that chunk representation and minor
differences in tagging quality do not have
a significant impact on chunking accuracy.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990979878787879">
Clinical text contains rich, detailed information of
great potential use to scientists and health service
researchers. However, peculiarities of language
use make the text difficult to process, and the pres-
ence of sensitive information makes it hard to ob-
tain adequate quantities for developing processing
systems. The short term goal of most research
in the area is to achieve a reliable language pro-
cessing foundation that can support more complex
tasks such as named entity recognition (NER) to a
sufficiently reliable level.
Chunking is the task of identifying non-
recursive phrases in text (Abney, 1991). It is a
type of shallow parsing that is a less challeng-
ing task than dependency or constituency parsing.
This makes it likely to give more reliable results on
clinical text, since there is a very limited amount of
annotated (or even raw) text of this kind available
for system development. Even though chunking
does not provide as much syntactic information as
full parsing, it is an excellent method for identify-
ing base noun phrases (NP), which is a key issue
in symptom and disease identification. Identify-
ing symptoms and diseases is at the heart of har-
nessing the potential of clinical data for medical
research purposes.
There are few resources that enable researchers
to adapt general domain techniques to clinical text.
Using the Harvey Corpus1 – a chunk annotated
clinical text language resource – we present an ex-
ploratory study into adapting general domain tools
and models to apply to free text notes typed by UK
primary care physicians.
</bodyText>
<sectionHeader confidence="0.999789" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999818217391304">
The Mayo Clinic Corpus (Pakhomov et al., 2004)
is a key resource that has been widely used as
a gold standard in part-of-speech (POS) tagging
of clinical text. Based on that corpus and the
Penn TreeBank (Marcus et al., 1993), Coden et al.
(2005) present an analysis of the effects of domain
data on the performance of POS tagging mod-
els, demonstrating significant improvements with
models trained entirely on domain data. Savova
et al. (2010) use this corpus for the development
of cTAKES, Mayo Clinic’s processing pipeline for
clinical text.
Fan et al. (2011) show that using more diverse
clinical data can lead to more accurate POS tag-
ging. They report that models trained on clinical
text datasets from two different institutions per-
form on each of the datasets better than both mod-
els trained only on the same or the other dataset.
Fan et al. (2013) present guidelines for syntac-
tic parsing of clinical text and a clinical Treebank
annotated according to them. The guidelines are
designed to help the annotators handle the non-
canonical language that is typical of clinical text.
</bodyText>
<footnote confidence="0.9853845">
1An article describing the corpus is currently under re-
view.
</footnote>
<page confidence="0.983491">
77
</page>
<note confidence="0.349866">
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 77–82,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.994537" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999680090909091">
The Harvey Corpus is a chunk-annotated corpus
consisting of pairs of manually anonymised UK
primary care physician (General Practitioner, or
GP) notes and associated Read codes (Bentley et
al., 1996). Each Read code has a short textual
gloss. The purpose of the codes is to make it easy
to extract structured data from clinical records.
The reason we include the codes in the corpus is
that GPs often use their glosses as the beginning of
their note. Two typical examples (without chunk
annotation for clarity) are shown below.
</bodyText>
<construct confidence="0.992867875">
Birth details   ||Normal deliviery Girl (1)
Weight - 3. 960kg Apgar score @ 1min
- 9 Apgar score @ Smin - 9 Vit K given
Paed check NAD HC - 34. 9cm Hip test
performed
Chest pain   ||musculoskel pain last w/e, (2)
nil to find, ecg by paramedic no change,
reassured, rev sos
</construct>
<bodyText confidence="0.999771090909091">
The corpus comprises 890 pairs of Read codes
and notes, each annotated by medical experts us-
ing a chunk annotation scheme that includes non-
recursive noun phrases (NPs), main verb groups
(MVs), and a common annotation for adjectival
and adverbial phrases (APs). Example (3) be-
low illustrates the annotation. The majority of
the records (750) were double blind annotated by
medical experts, after which the resulting annota-
tion was adjudicated by a third medical expert an-
notator.
</bodyText>
<construct confidence="0.90462575">
[Chest pain]NP   ||[musculoskel pain]NP (3)
[last w/e]NP, [nil]AP to [find]MV, [ecg]NP
by [paramedic]NP [no change]NP,
[reassured]MV, [rev]MV [sos]AP
</construct>
<bodyText confidence="0.999838468085106">
Inter-annotator agreement was 0.86 f-score, tak-
ing one annotator to be the gold standard and the
other the candidate. We calculate the f-score ac-
cording to the MUC-7 (Chinchor, 1998) specifica-
tion, with the standard f-score formula. The calcu-
lation is kept symmetric with regard to the choice
of gold standard annotator by limiting the counting
of incorrect categories to one per tag, and equat-
ing the missing and spurious categories. For ex-
ample, three words annotated as one three-token
chunk by annotator A and three one-token chunks
by annotator B will have one incorrect and two
missing/spurious elements.
The rest of the records are a by-product of the
training process. Ninety records were triple anno-
tated by three different medical experts with the
help of a computational linguist, and fifty records
were double annotated by a medical expert – alone
and together with a computational linguist.
It is important to note that the text in the corpus
is not representative of all types of GP notes. It is
focused on text that represents the dominant part
of day-to-day notes, rather than standard edited
text such as copies of letters to specialists and
other medical practitioners.
Even though the corpus data is very rich in in-
formation, its non-canonical language means that
it is very different from other clinical corpora
such as the Mayo Clinic Corpus (Pakhomov et al.,
2004) and poses different challenges for process-
ing. The GP notes in the Harvey Corpus can be
regarded as groups of medical ‘tweets’ meant to
be used mainly by the author. Sentence segmenta-
tion in the classical sense of the term is often im-
possible, because there are no sentences. Instead
there are short bursts of phrases concatenated to-
gether often without any indication of their bound-
aries. The average length of a note is roughly 30
tokens including the Read code. This is in con-
trast to notes in other clinical text datasets, which
range from 100 to 400 tokens on average (Fan et
al., 2011; Pakhomov et al., 2004). As well as typ-
ical clinical text characteristics such as domain-
specific acronyms, slang, and abbreviations, punc-
tuation and casing are often misleading (if present
at all), and some common classes of words (e.g.
auxiliary verbs) are almost completely absent.
</bodyText>
<sectionHeader confidence="0.987374" genericHeader="method">
4 Chunking
</sectionHeader>
<bodyText confidence="0.999895076923077">
State-of-the-art text chunking accuracy reaches an
f-score of 95% (Sun et al., 2008). However, this
is for standard, edited text, and relies on accurate
POS tagging in a pre-processing step. However,
the characteristics of GP-written free text make ac-
curate part of speech (POS) tagging and chunking
difficult. Major problems are caused by unknown
tokens and ambiguities due to omitted words or
phrases.
We evaluate two standard chunking tools, Yam-
Cha (Kudo and Matsumoto, 2003) and CRF++2,
selected based on their support for trainable con-
text features. The tools were applied to the Har-
</bodyText>
<footnote confidence="0.9557125">
2http://crfpp.googlecode.com/svn/
trunk/doc/index.html
</footnote>
<page confidence="0.990241">
78
</page>
<table confidence="0.999828">
POS YamCha IOB YamCha BEISO CRF++ IOB CRF++ BEISO
ARKIRC 75.35 76.63 σ1.04 76.87 σ2.91 75.87 σ1.64 76.23 σ1.99
ARKTwitter – 76.72 σ2.11 77.53 σ1.65 76.63 σ2.36 77.23 σ1.06
ARKRitter 75.70 76.59 σ2.01 76.72 σ2.11 76.63 σ1.05 77.17 σ1.77
cTAKES 82.42 75.32 σ2.52 75.85 σ2.02 75.43 σ1.79 75.53 σ1.90
GENIA 80.63 71.70 σ2.27* 74.86 σ1.41 74.16 σ2.03* 74.19 σ1.72
RASP – 74.24 σ1.84 75.10 σ1.31 75.63 σ2.33 75.76 σ2.18
Stanford 80.68 76.40 σ1.69 76.36 σ2.92 75.95 σ1.25 75.94 σ1.91
SVMTool 76.40 74.32 σ2.57 74.30 σ2.71 74.66 σ1.77 74.68 σ2.28
Wapiti 73.39 74.74 σ2.29 74.78 σ1.33 73.59 σ2.62 73.83 σ2.31
baseline – 69.66 σ1.89* 69.76 σ1.24 67.05 σ1.15* 68.65 σ1.41
</table>
<tableCaption confidence="0.997886">
Table 1: Chunking results using YamCha and CRF++ on data automatically POS tagged using nine
</tableCaption>
<bodyText confidence="0.995663730769231">
different models; the baseline is with no tagging. The IOB and BEISO columns compare the impact
of two chunk representation strategies. The POS column indicates the part-of-speech tagging accuracy
for a subset of the corpus. Asterisks indicate pairs of significantly different YamCha and CRF++ results
(t-test with 0.05 p-value).
vey Corpus with automatically generated POS an-
notation. Given the small amount of data and
the challenges presented above, we expected that
our results would be lower than those reported by
Savova et al. (2010). The aim of these experi-
ments is to find the best performance obtainable
with standard chunking tools, which we will build
on in further stages of our research.
We conducted pairs of experiments, one with
each chunking tool, divided into three groups: the
first investigates the effects of choice of POS tag-
ger for training data annotation (Section 4.1); the
second compares two chunk representations (Sec-
tion 4.2); and the third searches for the optimal
context features (Section 4.3). All feature tuning
experiments were conducted on a development set
and tested using 10-fold cross-validation on the
rest of the data. We used 10% of the whole data
for the development set and 90% of the remain-
ing data for a training sample during development.
This guarantees the development model is trained
on the same amount of data as the testing model.
</bodyText>
<subsectionHeader confidence="0.990911">
4.1 Part-of-Speech Tagging
</subsectionHeader>
<bodyText confidence="0.999876108108108">
We evaluated and compared the results yielded
by the two chunkers, having applied each of
seven off-the-shelf POS taggers. Of these tag-
gers, cTAKES (Savova et al., 2010) and GENIA
(Tsuruoka et al., 2005) are the only ones trained
on data that resembles ours, which suggests that
they should have the best chance of performing
well. We also selected a number of other taggers
while trying to diversify their algorithms and train-
ing data as much as possible: the POS tagger part
of the Stanford NLP package (Toutanova et al.,
2003) because it is one of the most successfully
applied in the field; the RASP tagger (Briscoe
et al., 2006) because of its British National Cor-
pus (Clear, 1993) training data; the ARK tagger
(Owoputi et al., 2013) because of the terseness of
the tweet language; and the SVMTool (Gim´enez
and M`arquez, 2004) and Wapiti (Lavergne et al.,
2010) because they use SVM and CRF algorithms.
Our baseline model uses no part of speech infor-
mation.
Using the Penn TreeBank tagset (Marcus et al.,
1993), we manually annotated a subset of the cor-
pus of comparable size to the development set. Us-
ing this dataset we estimated the tagging accuracy
for all models that support that tagset (omitting
RASP and ARK Twitter since they use different
tagsets). In this evaluation, cTAKES is the best
performing model, followed closely by the Stan-
ford POS tagger and GENIA.
The results in Table 1 show that the differ-
ences between chunking models trained on differ-
ent POS annotations are small and mostly not sta-
tistically significant from each other. However, all
the results are significantly better than the base-
line, apart from those based on the GENIA tagger
output.
</bodyText>
<subsectionHeader confidence="0.990872">
4.2 Chunk Representation
</subsectionHeader>
<bodyText confidence="0.999917333333333">
The dominant chunk representation standard in-
side, outside, begin (IOB) introduced by Ramshaw
and Marcus (1995) and established with the
</bodyText>
<page confidence="0.993132">
79
</page>
<bodyText confidence="0.999897540540541">
CoNLL-2000 shared task (Sang and Buchholz,
2000) takes a minimalistic approach to the rep-
resentation problem in order to keep the number
of labels low. Note that for chunking representa-
tions the total number of labels is the product of
the chunk types and the set of representation types
plus the outside tag, meaning that for IOB with
our set of three chunk types (NP, MV, AP) there
are seven labels.
Alternative chunk representations, such as be-
gin, end, inside, single, outside (BEISO)3 as used
by Kudo and Matsumoto (2001), offer more fine-
grained tagsets, presumably at a performance cost.
That cost is unnecessary unless there is something
to be gained from a more fine-grained tagset at de-
coding time, because the two representations are
deterministically inter-convertible. For instance,
an end tag could be useful for better recognising
boundaries between chunks of the same type. The
BEISO tagset model looks for the boundary be-
fore and after crossing it, while an IOB model
only looks after. This should give only a small
gain with standard edited text because the chunk
type distribution is fairly well balanced and punc-
tuation divides ambiguous cases such as lists of
compound nouns. However, the Harvey Corpus
is NP-heavy and contains many sequences of NP
chunks that do not have any punctuation to mark
their boundaries.
We evaluated the two chunk representations in
combination with each POS tagger. Table 1 shows
that the differences between the results for the
two representations are small and never statisti-
cally significant. We also evaluated the two chunk
representations with different amounts of training
data. The resulting learning curves (Figure 1) are
almost identical.
</bodyText>
<subsectionHeader confidence="0.999472">
4.3 Context Features
</subsectionHeader>
<bodyText confidence="0.999970636363636">
We approached the feature tuning task by first ex-
ploring the smaller feature space of YamCha and
then using the trends there to constrain the fea-
tures of CRF++. YamCha has three groups of fea-
tures responsible for tokens, POS tags and dynam-
ically generated (i.e. preceding) chunk tags. For
all experiments we determined the best feature set
by exhaustively testing all context feature combi-
nations within a predefined range. We used the
same context window for the token and tag fea-
tures in order to reduce the search space. Given
</bodyText>
<footnote confidence="0.589479">
3Also sometimes abbreviated IOBSE
</footnote>
<bodyText confidence="0.850857565217391">
Feature Set CV Dev
W-1-W1, T-1-T1, C-1 77.28 σ1.9 75.28
W-1-W1, T-1-T1, C-2-C-1 77.27 σ2.6 74.70
W-1-W2, T-1-T2, C-1 76.86 σ1.5 74.08
W-2-W1, T-2-T1, C-2 76.46 σ1.3 74.00
W-1-W1, T-1-T1, C-2 76.89 σ2.1 73.92
W-2-W1, T-2-T1, C-3-C-1 76.52 σ0.9 73.91
W-1-W1, T-1-T1, C-3-C-1 77.02 σ2.0 73.90
W-2-W2, T-2-T2, C-1 77.03 σ1.9 73.86
W-1-W1, T-1-T1, C-3 77.15 σ1.5 73.63
W-3-W1, T-3-T1, C-2-C-1 75.71 σ1.9 73.63
Table 2: Development set and 10-fold cross-
validation results for the top ten feature sets of
YamCha models trained on ARKTwitter POS an-
notation. Token features are represented with
W, POS features with T, and dynamically gener-
ated chunk features with C. None of the cross-
validation results are significantly different from
each other (t-test with 0.05 p-value).
the terseness of the text we expected that wider
context windows of more than three tokens would
not be beneficial to the model, and therefore did
not consider them. Our experiments using Yam-
Cha confirmed this hypothesis and showed a con-
sistent trend among all experiments in favouring a
window of -1 to +1 for tokens and slightly wider
for chunk tags (see Table 2).
CRF++ provides a more powerful feature con-
figuration allowing for unary and pairwise4 fea-
tures of output tags. The unary features allow the
construction of token or POS tag bigrams and tri-
grams in addition to the standard context windows.
The feature tuning search space with so many pa-
rameters is enormous, which required us to use our
findings from the YamCha experiments to trim it
down and make it computationally feasible. First,
we decreased the search window of all features by
one in each direction from -3:3 to -2:2. Second, we
used the top scoring POS model from the first ex-
perimental runs to constrain the features even fur-
ther by selecting only the top one hundred for the
rest of the models.
We could not identify the same uniform trend in
the top feature sets as we could with YamCha. Our
results ranged from very small context windows
to the maximum size of our search space. How-
</bodyText>
<footnote confidence="0.9995402">
4The unary and pairwise features of output tags are re-
ferred to as unigram and bigram features of output tags on
the CRF++ web page. Although this is correct, it can also
be confused with unigrams and bigrams of tokens, which are
expressed as unary (unigram) output tag features.
</footnote>
<page confidence="0.992181">
80
</page>
<figure confidence="0.992193857142857">
80
75
70
65
60
55
20 80 140 200 260 320 380 440 500 560 620 680 740 800
</figure>
<figureCaption confidence="0.971790666666667">
Figure 1: Chunking results for YamCha IOB and
BEISO models with increasing amounts of train-
ing data.
</figureCaption>
<bodyText confidence="0.971042333333333">
ever, we noticed that BEISO feature sets tend to
be smaller than the IOB ones. We also found that
the pairwise features normally improve the results.
</bodyText>
<sectionHeader confidence="0.98746" genericHeader="conclusions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999970615384616">
We were surprised that the experiments did not
show a clear correlation between POS tagging ac-
curacy and chunking accuracy. On the other hand,
the chunking results using POS tagged data are
significantly better than the baseline, except when
using the GENIA tagger output. The small dif-
ferences between training sets of similar POS ac-
curacy could be explained due to the non-uniform
impact of the wrong POS tag on the chunking pro-
cess. Some mistakes such as labelling a noun as
a verb in the middle of a NP chunk are almost
sure to propagate and cause further chunking er-
rors, whereas others may have minimal or no ef-
fect, for example labelling a singular noun as a
proper noun. An error analysis of verb tags and
noun tags (Table 3) shows that the ARK models
tend to make more mistakes that keep the anno-
tation within the same tag group compared to the
GENIA model (see column pairs 1 and 3, and 2
and 4). This is a possible explanation for the lower
accuracy of the chunking model trained on data
tagged by GENIA.
Our experiments showed that the models using
the two chunk representations did not perform sig-
nificantly differently from each other. We also
showed that this conclusion is likely to hold if
</bodyText>
<table confidence="0.9994265">
Model Ngroup Vgroup Nouns Verbs
ARKIRC 67.17 78.26 88.26 85.99
ARKTwitter - - 86.97 88.71
ARKRitter 68.57 77.29 90.64 85.02
cTAKES 83.93 62.80 93.85 69.08
GENIA 81.56 61.83 92.03 71.01
RASP - - 84.59 83.58
Stanford 80.30 73.42 91.89 83.09
SVMTool 69.97 70.04 90.08 80.19
Wapiti 65.64 66.66 87.84 74.87
</table>
<tableCaption confidence="0.998819">
Table 3: Detailed view of the POS model re-
</tableCaption>
<bodyText confidence="0.999107214285715">
sults focusing on the noun and verb tag groups.
The leftmost two columns of figures show accura-
cies over tags in the respective groups; the right-
most two columns show the accuracies of the same
groups if all tags in a group are replaced with a
group tag, e.g. V for verbs5.
more training data were available.
There are a number of ways we could improve
chunking accuracy besides increasing the amount
of training data. Although our results do not show
a clear trend, Fan et al. (2011) demonstrate that the
domain of part-of-speech training data has a sig-
nificant impact on tagging accuracy, which could
potentially impact chunking results if it decreases
the number of errors that propagate during chunk-
ing. An important problem in that area is dealing
with present and past participles, which are almost
sure to cause error propagation if mislabelled (as
nouns or adjectives, respectively). Participles are
more ambiguous in terse contexts lacking auxil-
iary verbs, which are natural disambiguation indi-
cators. Another direction in processing that could
contribute to better chunking is better token and
sentence segmentation. Finally, unknown words,
which may potentially have the largest impact on
chunking accuracy, could be dealt with using a
generic solution such as feature expansion based
on distributional similarity.
</bodyText>
<sectionHeader confidence="0.999009" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993863666666667">
S. Abney. 1991. Parsing by chunks. In Robert C.
Berwick, Steven P. Abney, and Carol Tenny, editors,
Principle-Based Parsing: Computation and Psy-
cholinguistics, pages 257–278. Kluwer, Dordrecht.
T. Bentley, C. Price, and P. Brown. 1996. Structural
and lexical features of successive versions of the
</reference>
<footnote confidence="0.998177">
5Note that these results are different from what would be
yielded by a classifier trained on data subjected to the same
tag substitution.
</footnote>
<note confidence="0.552782">
IOB BEISO
</note>
<page confidence="0.991596">
81
</page>
<reference confidence="0.998937230769231">
read codes. In Proceedings of the Annual Confer-
ence of The Primary Health Care Specialist Group
of the British Computer Society, pages 91–103.
T. Briscoe, J. Carroll, and R. Watson. 2006. The sec-
ond release of the RASP system. In Proceedings of
the COLING/ACL on Interactive Presentation Ses-
sions, COLING-ACL’06, pages 77–80, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
N. Chinchor. 1998. Appendix B: Test scores. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7), Fairfax, VA, April.
J. Clear. 1993. The British National Corpus. In
George P. Landow and Paul Delany, editors, The
Digital Word, pages 163–187. MIT Press, Cam-
bridge, MA, USA.
A. Coden, S. Pakhomov, R. Ando, P. Duffy, and
C. Chute. 2005. Domain-specific language mod-
els and lexicons for tagging. Journal of Biomedical
Informatics, 38:422–430.
J.-W. Fan, R. Prasad, R.M. Yabut, R.M. Loomis, D.S.
Zisook, J.E. Mattison, and Y. Huang. 2011. Part-of-
speech tagging for clinical text: Wall or bridge be-
tween institutions? In American Medical Informat-
ics Association Annual Symposium, 1, pages 382–
391. American Medical Informatics Association.
J.-W. Fan, E. Yang, M. Jiang, R. Prasad, R. Loomis,
D. Zisook, J. Denny, H. Xu, and Y. Huang. 2013.
Research and applications: Syntactic parsing of clin-
ical text: guideline and corpus development with
handling ill-formed sentences. JAMIA, 20(6):1168–
1177.
J. Gim´enez and L. M`arquez. 2004. SVMTool: A gen-
eral POS tagger generator based on support vector
machines. In Proceedings of the 4th LREC, Lisbon,
Portugal.
T. Kudo and Y. Matsumoto. 2001. Chunking with sup-
port vector machines. In Proceedings of the Second
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics on Language
Technologies, NAACL’01, pages 1–8, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
T. Kudo and Y. Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 24–31, Morristown, NJ,
USA. Association for Computational Linguistics.
T. Lavergne, O. Capp´e, and F. Yvon. 2010. Practi-
cal very large scale CRFs. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 504–513. Association for
Computational Linguistics, July.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
O. Owoputi, B. O’Connor, C. Dyer, K. Gimpel,
N. Schneider, and N. Smith. 2013. Improved
part-of-speech tagging for online conversational text
with word clusters. In Proceedings of NAACL-HLT,
pages 380–390.
S. Pakhomov, A. Coden, and C. Chute. 2004. Creat-
ing a test corpus of clinical notes manually tagged
for part-of-speech information. In Proceedings of
the International Joint Workshop on Natural Lan-
guage Processing in Biomedicine and Its Applica-
tions, JNLPBA’04, pages 62–65, Stroudsburg, PA,
USA. Association for Computational Linguistics.
L. Ramshaw and M. Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proceedings
of the Third Workshop on Very Large Corpora, pages
82–94.
E. Sang and S. Buchholz. 2000. Introduction to the
CoNLL-2000 shared task: Chunking. In Proceed-
ings of the 2nd Workshop on Learning Language
in Logic and the 4th Conference on Computational
Natural Language Learning, pages 13–14.
G. Savova, J. Masanz, P. Ogren, J. Zheng, S. Sohn,
K. Kipper-Schuler, and C. Chute. 2010. Mayo clin-
ical text analysis and knowledge extraction system
(cTAKES): architecture, component evaluation and
applications. Journal of the American Medical In-
formatics Association, 17(5):507–513.
X. Sun, L.-P. Morency, D. Okanoharay, and J. Tsujii.
2008. Modeling latent-dynamic in shallow parsing:
A latent conditional model with improved inference.
In Proceedings of the 22nd International Confer-
ence on Computational Linguistics (COLING’08),
Manchester, UK, August.
K. Toutanova, D. Klein, C. D. Manning, and Y. Singer.
2003. Feature-rich part-of-speech tagging with a
cyclic dependency network. In Proceedings of
the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics on Human Language Technology - Volume 1,
NAACL’03, pages 173–180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Y. Tsuruoka, Y. Tateishi, J.-D. Kim, T. Ohta, J. Mc-
Naught, S. Ananiadou, and J. Tsujii. 2005. Devel-
oping a robust part-of-speech tagger for biomedical
text. In Proceedings of the 10th Panhellenic Con-
ference on Advances in Informatics, PCI’05, pages
382–392, Berlin, Heidelberg. Springer-Verlag.
</reference>
<page confidence="0.999131">
82
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.260299">
<title confidence="0.999333">Chunking Clinical Text Containing Non-Canonical Language</title>
<author confidence="0.739726333333333">Jackie Cassell Primary Care</author>
<author confidence="0.739726333333333">Public Health Aleksandar</author>
<affiliation confidence="0.998429">Department of University of</affiliation>
<address confidence="0.915328">Brighton, UK</address>
<email confidence="0.975909">a.savkov@sussex.ac.uk</email>
<author confidence="0.878835">John</author>
<affiliation confidence="0.999727">Department of Informatics University of</affiliation>
<address confidence="0.932696">Brighton, UK</address>
<email confidence="0.987189">j.a.carroll@sussex.ac.uk</email>
<affiliation confidence="0.984954">Brighton and Sussex Medical</affiliation>
<address confidence="0.971057">Brighton, UK</address>
<email confidence="0.998274">j.cassell@bsms.ac.uk</email>
<abstract confidence="0.999496263157895">Free text notes typed by primary care physicians during patient consultations typically contain highly non-canonical language. Shallow syntactic analysis of free text notes can help to reveal valuable information for the study of disease and treatment. We present an exploratory study into chunking such text using offthe-shelf language processing tools and pre-trained statistical models. We evaluate chunking accuracy with respect to partof-speech tagging quality, choice of chunk representation, and breadth of context features. Our results indicate that narrow context feature windows give the best results, but that chunk representation and minor differences in tagging quality do not have a significant impact on chunking accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks. In</title>
<date>1991</date>
<booktitle>Principle-Based Parsing: Computation and Psycholinguistics,</booktitle>
<pages>257--278</pages>
<editor>Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors,</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="1739" citStr="Abney, 1991" startWordPosition="249" endWordPosition="250">tion Clinical text contains rich, detailed information of great potential use to scientists and health service researchers. However, peculiarities of language use make the text difficult to process, and the presence of sensitive information makes it hard to obtain adequate quantities for developing processing systems. The short term goal of most research in the area is to achieve a reliable language processing foundation that can support more complex tasks such as named entity recognition (NER) to a sufficiently reliable level. Chunking is the task of identifying nonrecursive phrases in text (Abney, 1991). It is a type of shallow parsing that is a less challenging task than dependency or constituency parsing. This makes it likely to give more reliable results on clinical text, since there is a very limited amount of annotated (or even raw) text of this kind available for system development. Even though chunking does not provide as much syntactic information as full parsing, it is an excellent method for identifying base noun phrases (NP), which is a key issue in symptom and disease identification. Identifying symptoms and diseases is at the heart of harnessing the potential of clinical data fo</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>S. Abney. 1991. Parsing by chunks. In Robert C. Berwick, Steven P. Abney, and Carol Tenny, editors, Principle-Based Parsing: Computation and Psycholinguistics, pages 257–278. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Bentley</author>
<author>C Price</author>
<author>P Brown</author>
</authors>
<title>Structural and lexical features of successive versions of the read codes.</title>
<date>1996</date>
<booktitle>In Proceedings of the Annual Conference of The Primary Health Care Specialist Group of the British Computer Society,</booktitle>
<pages>91--103</pages>
<contexts>
<context position="4253" citStr="Bentley et al., 1996" startWordPosition="661" endWordPosition="664"> Treebank annotated according to them. The guidelines are designed to help the annotators handle the noncanonical language that is typical of clinical text. 1An article describing the corpus is currently under review. 77 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 77–82, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 3 Data The Harvey Corpus is a chunk-annotated corpus consisting of pairs of manually anonymised UK primary care physician (General Practitioner, or GP) notes and associated Read codes (Bentley et al., 1996). Each Read code has a short textual gloss. The purpose of the codes is to make it easy to extract structured data from clinical records. The reason we include the codes in the corpus is that GPs often use their glosses as the beginning of their note. Two typical examples (without chunk annotation for clarity) are shown below. Birth details ||Normal deliviery Girl (1) Weight - 3. 960kg Apgar score @ 1min - 9 Apgar score @ Smin - 9 Vit K given Paed check NAD HC - 34. 9cm Hip test performed Chest pain ||musculoskel pain last w/e, (2) nil to find, ecg by paramedic no change, reassured, rev sos Th</context>
</contexts>
<marker>Bentley, Price, Brown, 1996</marker>
<rawString>T. Bentley, C. Price, and P. Brown. 1996. Structural and lexical features of successive versions of the read codes. In Proceedings of the Annual Conference of The Primary Health Care Specialist Group of the British Computer Society, pages 91–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Briscoe</author>
<author>J Carroll</author>
<author>R Watson</author>
</authors>
<title>The second release of the RASP system.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive Presentation Sessions, COLING-ACL’06,</booktitle>
<pages>77--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11197" citStr="Briscoe et al., 2006" startWordPosition="1808" endWordPosition="1811"> and compared the results yielded by the two chunkers, having applied each of seven off-the-shelf POS taggers. Of these taggers, cTAKES (Savova et al., 2010) and GENIA (Tsuruoka et al., 2005) are the only ones trained on data that resembles ours, which suggests that they should have the best chance of performing well. We also selected a number of other taggers while trying to diversify their algorithms and training data as much as possible: the POS tagger part of the Stanford NLP package (Toutanova et al., 2003) because it is one of the most successfully applied in the field; the RASP tagger (Briscoe et al., 2006) because of its British National Corpus (Clear, 1993) training data; the ARK tagger (Owoputi et al., 2013) because of the terseness of the tweet language; and the SVMTool (Gim´enez and M`arquez, 2004) and Wapiti (Lavergne et al., 2010) because they use SVM and CRF algorithms. Our baseline model uses no part of speech information. Using the Penn TreeBank tagset (Marcus et al., 1993), we manually annotated a subset of the corpus of comparable size to the development set. Using this dataset we estimated the tagging accuracy for all models that support that tagset (omitting RASP and ARK Twitter si</context>
</contexts>
<marker>Briscoe, Carroll, Watson, 2006</marker>
<rawString>T. Briscoe, J. Carroll, and R. Watson. 2006. The second release of the RASP system. In Proceedings of the COLING/ACL on Interactive Presentation Sessions, COLING-ACL’06, pages 77–80, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
</authors>
<title>Appendix B: Test scores.</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7),</booktitle>
<location>Fairfax, VA,</location>
<contexts>
<context position="5660" citStr="Chinchor, 1998" startWordPosition="899" endWordPosition="900">nd a common annotation for adjectival and adverbial phrases (APs). Example (3) below illustrates the annotation. The majority of the records (750) were double blind annotated by medical experts, after which the resulting annotation was adjudicated by a third medical expert annotator. [Chest pain]NP ||[musculoskel pain]NP (3) [last w/e]NP, [nil]AP to [find]MV, [ecg]NP by [paramedic]NP [no change]NP, [reassured]MV, [rev]MV [sos]AP Inter-annotator agreement was 0.86 f-score, taking one annotator to be the gold standard and the other the candidate. We calculate the f-score according to the MUC-7 (Chinchor, 1998) specification, with the standard f-score formula. The calculation is kept symmetric with regard to the choice of gold standard annotator by limiting the counting of incorrect categories to one per tag, and equating the missing and spurious categories. For example, three words annotated as one three-token chunk by annotator A and three one-token chunks by annotator B will have one incorrect and two missing/spurious elements. The rest of the records are a by-product of the training process. Ninety records were triple annotated by three different medical experts with the help of a computational </context>
</contexts>
<marker>Chinchor, 1998</marker>
<rawString>N. Chinchor. 1998. Appendix B: Test scores. In Proceedings of the Seventh Message Understanding Conference (MUC-7), Fairfax, VA, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Clear</author>
</authors>
<title>The British National Corpus.</title>
<date>1993</date>
<booktitle>The Digital Word,</booktitle>
<pages>163--187</pages>
<editor>In George P. Landow and Paul Delany, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="11250" citStr="Clear, 1993" startWordPosition="1819" endWordPosition="1820">applied each of seven off-the-shelf POS taggers. Of these taggers, cTAKES (Savova et al., 2010) and GENIA (Tsuruoka et al., 2005) are the only ones trained on data that resembles ours, which suggests that they should have the best chance of performing well. We also selected a number of other taggers while trying to diversify their algorithms and training data as much as possible: the POS tagger part of the Stanford NLP package (Toutanova et al., 2003) because it is one of the most successfully applied in the field; the RASP tagger (Briscoe et al., 2006) because of its British National Corpus (Clear, 1993) training data; the ARK tagger (Owoputi et al., 2013) because of the terseness of the tweet language; and the SVMTool (Gim´enez and M`arquez, 2004) and Wapiti (Lavergne et al., 2010) because they use SVM and CRF algorithms. Our baseline model uses no part of speech information. Using the Penn TreeBank tagset (Marcus et al., 1993), we manually annotated a subset of the corpus of comparable size to the development set. Using this dataset we estimated the tagging accuracy for all models that support that tagset (omitting RASP and ARK Twitter since they use different tagsets). In this evaluation, </context>
</contexts>
<marker>Clear, 1993</marker>
<rawString>J. Clear. 1993. The British National Corpus. In George P. Landow and Paul Delany, editors, The Digital Word, pages 163–187. MIT Press, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Coden</author>
<author>S Pakhomov</author>
<author>R Ando</author>
<author>P Duffy</author>
<author>C Chute</author>
</authors>
<title>Domain-specific language models and lexicons for tagging.</title>
<date>2005</date>
<journal>Journal of Biomedical Informatics,</journal>
<pages>38--422</pages>
<contexts>
<context position="2951" citStr="Coden et al. (2005)" startWordPosition="454" endWordPosition="457">data for medical research purposes. There are few resources that enable researchers to adapt general domain techniques to clinical text. Using the Harvey Corpus1 – a chunk annotated clinical text language resource – we present an exploratory study into adapting general domain tools and models to apply to free text notes typed by UK primary care physicians. 2 Related Work The Mayo Clinic Corpus (Pakhomov et al., 2004) is a key resource that has been widely used as a gold standard in part-of-speech (POS) tagging of clinical text. Based on that corpus and the Penn TreeBank (Marcus et al., 1993), Coden et al. (2005) present an analysis of the effects of domain data on the performance of POS tagging models, demonstrating significant improvements with models trained entirely on domain data. Savova et al. (2010) use this corpus for the development of cTAKES, Mayo Clinic’s processing pipeline for clinical text. Fan et al. (2011) show that using more diverse clinical data can lead to more accurate POS tagging. They report that models trained on clinical text datasets from two different institutions perform on each of the datasets better than both models trained only on the same or the other dataset. Fan et al</context>
</contexts>
<marker>Coden, Pakhomov, Ando, Duffy, Chute, 2005</marker>
<rawString>A. Coden, S. Pakhomov, R. Ando, P. Duffy, and C. Chute. 2005. Domain-specific language models and lexicons for tagging. Journal of Biomedical Informatics, 38:422–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-W Fan</author>
<author>R Prasad</author>
<author>R M Yabut</author>
<author>R M Loomis</author>
<author>D S Zisook</author>
<author>J E Mattison</author>
<author>Y Huang</author>
</authors>
<title>Part-ofspeech tagging for clinical text: Wall or bridge between institutions?</title>
<date>2011</date>
<journal>In American Medical Informatics Association Annual Symposium,</journal>
<volume>1</volume>
<pages>382--391</pages>
<publisher>American Medical Informatics Association.</publisher>
<contexts>
<context position="3266" citStr="Fan et al. (2011)" startWordPosition="504" endWordPosition="507">otes typed by UK primary care physicians. 2 Related Work The Mayo Clinic Corpus (Pakhomov et al., 2004) is a key resource that has been widely used as a gold standard in part-of-speech (POS) tagging of clinical text. Based on that corpus and the Penn TreeBank (Marcus et al., 1993), Coden et al. (2005) present an analysis of the effects of domain data on the performance of POS tagging models, demonstrating significant improvements with models trained entirely on domain data. Savova et al. (2010) use this corpus for the development of cTAKES, Mayo Clinic’s processing pipeline for clinical text. Fan et al. (2011) show that using more diverse clinical data can lead to more accurate POS tagging. They report that models trained on clinical text datasets from two different institutions perform on each of the datasets better than both models trained only on the same or the other dataset. Fan et al. (2013) present guidelines for syntactic parsing of clinical text and a clinical Treebank annotated according to them. The guidelines are designed to help the annotators handle the noncanonical language that is typical of clinical text. 1An article describing the corpus is currently under review. 77 Proceedings o</context>
<context position="7455" citStr="Fan et al., 2011" startWordPosition="1203" endWordPosition="1206">s (Pakhomov et al., 2004) and poses different challenges for processing. The GP notes in the Harvey Corpus can be regarded as groups of medical ‘tweets’ meant to be used mainly by the author. Sentence segmentation in the classical sense of the term is often impossible, because there are no sentences. Instead there are short bursts of phrases concatenated together often without any indication of their boundaries. The average length of a note is roughly 30 tokens including the Read code. This is in contrast to notes in other clinical text datasets, which range from 100 to 400 tokens on average (Fan et al., 2011; Pakhomov et al., 2004). As well as typical clinical text characteristics such as domainspecific acronyms, slang, and abbreviations, punctuation and casing are often misleading (if present at all), and some common classes of words (e.g. auxiliary verbs) are almost completely absent. 4 Chunking State-of-the-art text chunking accuracy reaches an f-score of 95% (Sun et al., 2008). However, this is for standard, edited text, and relies on accurate POS tagging in a pre-processing step. However, the characteristics of GP-written free text make accurate part of speech (POS) tagging and chunking diff</context>
<context position="19386" citStr="Fan et al. (2011)" startWordPosition="3209" endWordPosition="3212">ord 80.30 73.42 91.89 83.09 SVMTool 69.97 70.04 90.08 80.19 Wapiti 65.64 66.66 87.84 74.87 Table 3: Detailed view of the POS model results focusing on the noun and verb tag groups. The leftmost two columns of figures show accuracies over tags in the respective groups; the rightmost two columns show the accuracies of the same groups if all tags in a group are replaced with a group tag, e.g. V for verbs5. more training data were available. There are a number of ways we could improve chunking accuracy besides increasing the amount of training data. Although our results do not show a clear trend, Fan et al. (2011) demonstrate that the domain of part-of-speech training data has a significant impact on tagging accuracy, which could potentially impact chunking results if it decreases the number of errors that propagate during chunking. An important problem in that area is dealing with present and past participles, which are almost sure to cause error propagation if mislabelled (as nouns or adjectives, respectively). Participles are more ambiguous in terse contexts lacking auxiliary verbs, which are natural disambiguation indicators. Another direction in processing that could contribute to better chunking </context>
</contexts>
<marker>Fan, Prasad, Yabut, Loomis, Zisook, Mattison, Huang, 2011</marker>
<rawString>J.-W. Fan, R. Prasad, R.M. Yabut, R.M. Loomis, D.S. Zisook, J.E. Mattison, and Y. Huang. 2011. Part-ofspeech tagging for clinical text: Wall or bridge between institutions? In American Medical Informatics Association Annual Symposium, 1, pages 382– 391. American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-W Fan</author>
<author>E Yang</author>
<author>M Jiang</author>
<author>R Prasad</author>
<author>R Loomis</author>
<author>D Zisook</author>
<author>J Denny</author>
<author>H Xu</author>
<author>Y Huang</author>
</authors>
<title>Research and applications: Syntactic parsing of clinical text: guideline and corpus development with handling ill-formed sentences.</title>
<date>2013</date>
<journal>JAMIA,</journal>
<volume>20</volume>
<issue>6</issue>
<pages>1177</pages>
<contexts>
<context position="3559" citStr="Fan et al. (2013)" startWordPosition="557" endWordPosition="560">l. (2005) present an analysis of the effects of domain data on the performance of POS tagging models, demonstrating significant improvements with models trained entirely on domain data. Savova et al. (2010) use this corpus for the development of cTAKES, Mayo Clinic’s processing pipeline for clinical text. Fan et al. (2011) show that using more diverse clinical data can lead to more accurate POS tagging. They report that models trained on clinical text datasets from two different institutions perform on each of the datasets better than both models trained only on the same or the other dataset. Fan et al. (2013) present guidelines for syntactic parsing of clinical text and a clinical Treebank annotated according to them. The guidelines are designed to help the annotators handle the noncanonical language that is typical of clinical text. 1An article describing the corpus is currently under review. 77 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 77–82, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 3 Data The Harvey Corpus is a chunk-annotated corpus consisting of pairs of manually anonymised UK primary care </context>
</contexts>
<marker>Fan, Yang, Jiang, Prasad, Loomis, Zisook, Denny, Xu, Huang, 2013</marker>
<rawString>J.-W. Fan, E. Yang, M. Jiang, R. Prasad, R. Loomis, D. Zisook, J. Denny, H. Xu, and Y. Huang. 2013. Research and applications: Syntactic parsing of clinical text: guideline and corpus development with handling ill-formed sentences. JAMIA, 20(6):1168– 1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gim´enez</author>
<author>L M`arquez</author>
</authors>
<title>SVMTool: A general POS tagger generator based on support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th LREC,</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>J. Gim´enez and L. M`arquez. 2004. SVMTool: A general POS tagger generator based on support vector machines. In Proceedings of the 4th LREC, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Chunking with support vector machines.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies, NAACL’01,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12931" citStr="Kudo and Matsumoto (2001)" startWordPosition="2098" endWordPosition="2101">d inside, outside, begin (IOB) introduced by Ramshaw and Marcus (1995) and established with the 79 CoNLL-2000 shared task (Sang and Buchholz, 2000) takes a minimalistic approach to the representation problem in order to keep the number of labels low. Note that for chunking representations the total number of labels is the product of the chunk types and the set of representation types plus the outside tag, meaning that for IOB with our set of three chunk types (NP, MV, AP) there are seven labels. Alternative chunk representations, such as begin, end, inside, single, outside (BEISO)3 as used by Kudo and Matsumoto (2001), offer more finegrained tagsets, presumably at a performance cost. That cost is unnecessary unless there is something to be gained from a more fine-grained tagset at decoding time, because the two representations are deterministically inter-convertible. For instance, an end tag could be useful for better recognising boundaries between chunks of the same type. The BEISO tagset model looks for the boundary before and after crossing it, while an IOB model only looks after. This should give only a small gain with standard edited text because the chunk type distribution is fairly well balanced and</context>
</contexts>
<marker>Kudo, Matsumoto, 2001</marker>
<rawString>T. Kudo and Y. Matsumoto. 2001. Chunking with support vector machines. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies, NAACL’01, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kudo</author>
<author>Y Matsumoto</author>
</authors>
<title>Fast methods for kernel-based text analysis.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>24--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8229" citStr="Kudo and Matsumoto, 2003" startWordPosition="1324" endWordPosition="1327"> casing are often misleading (if present at all), and some common classes of words (e.g. auxiliary verbs) are almost completely absent. 4 Chunking State-of-the-art text chunking accuracy reaches an f-score of 95% (Sun et al., 2008). However, this is for standard, edited text, and relies on accurate POS tagging in a pre-processing step. However, the characteristics of GP-written free text make accurate part of speech (POS) tagging and chunking difficult. Major problems are caused by unknown tokens and ambiguities due to omitted words or phrases. We evaluate two standard chunking tools, YamCha (Kudo and Matsumoto, 2003) and CRF++2, selected based on their support for trainable context features. The tools were applied to the Har2http://crfpp.googlecode.com/svn/ trunk/doc/index.html 78 POS YamCha IOB YamCha BEISO CRF++ IOB CRF++ BEISO ARKIRC 75.35 76.63 σ1.04 76.87 σ2.91 75.87 σ1.64 76.23 σ1.99 ARKTwitter – 76.72 σ2.11 77.53 σ1.65 76.63 σ2.36 77.23 σ1.06 ARKRitter 75.70 76.59 σ2.01 76.72 σ2.11 76.63 σ1.05 77.17 σ1.77 cTAKES 82.42 75.32 σ2.52 75.85 σ2.02 75.43 σ1.79 75.53 σ1.90 GENIA 80.63 71.70 σ2.27* 74.86 σ1.41 74.16 σ2.03* 74.19 σ1.72 RASP – 74.24 σ1.84 75.10 σ1.31 75.63 σ2.33 75.76 σ2.18 Stanford 80.68 76.</context>
</contexts>
<marker>Kudo, Matsumoto, 2003</marker>
<rawString>T. Kudo and Y. Matsumoto. 2003. Fast methods for kernel-based text analysis. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 24–31, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lavergne</author>
<author>O Capp´e</author>
<author>F Yvon</author>
</authors>
<title>Practical very large scale CRFs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>504--513</pages>
<marker>Lavergne, Capp´e, Yvon, 2010</marker>
<rawString>T. Lavergne, O. Capp´e, and F. Yvon. 2010. Practical very large scale CRFs. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504–513. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2930" citStr="Marcus et al., 1993" startWordPosition="450" endWordPosition="453">potential of clinical data for medical research purposes. There are few resources that enable researchers to adapt general domain techniques to clinical text. Using the Harvey Corpus1 – a chunk annotated clinical text language resource – we present an exploratory study into adapting general domain tools and models to apply to free text notes typed by UK primary care physicians. 2 Related Work The Mayo Clinic Corpus (Pakhomov et al., 2004) is a key resource that has been widely used as a gold standard in part-of-speech (POS) tagging of clinical text. Based on that corpus and the Penn TreeBank (Marcus et al., 1993), Coden et al. (2005) present an analysis of the effects of domain data on the performance of POS tagging models, demonstrating significant improvements with models trained entirely on domain data. Savova et al. (2010) use this corpus for the development of cTAKES, Mayo Clinic’s processing pipeline for clinical text. Fan et al. (2011) show that using more diverse clinical data can lead to more accurate POS tagging. They report that models trained on clinical text datasets from two different institutions perform on each of the datasets better than both models trained only on the same or the oth</context>
<context position="11581" citStr="Marcus et al., 1993" startWordPosition="1873" endWordPosition="1876">ify their algorithms and training data as much as possible: the POS tagger part of the Stanford NLP package (Toutanova et al., 2003) because it is one of the most successfully applied in the field; the RASP tagger (Briscoe et al., 2006) because of its British National Corpus (Clear, 1993) training data; the ARK tagger (Owoputi et al., 2013) because of the terseness of the tweet language; and the SVMTool (Gim´enez and M`arquez, 2004) and Wapiti (Lavergne et al., 2010) because they use SVM and CRF algorithms. Our baseline model uses no part of speech information. Using the Penn TreeBank tagset (Marcus et al., 1993), we manually annotated a subset of the corpus of comparable size to the development set. Using this dataset we estimated the tagging accuracy for all models that support that tagset (omitting RASP and ARK Twitter since they use different tagsets). In this evaluation, cTAKES is the best performing model, followed closely by the Stanford POS tagger and GENIA. The results in Table 1 show that the differences between chunking models trained on different POS annotations are small and mostly not statistically significant from each other. However, all the results are significantly better than the ba</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Owoputi</author>
<author>B O’Connor</author>
<author>C Dyer</author>
<author>K Gimpel</author>
<author>N Schneider</author>
<author>N Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>O. Owoputi, B. O’Connor, C. Dyer, K. Gimpel, N. Schneider, and N. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of NAACL-HLT, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pakhomov</author>
<author>A Coden</author>
<author>C Chute</author>
</authors>
<title>Creating a test corpus of clinical notes manually tagged for part-of-speech information.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications, JNLPBA’04,</booktitle>
<pages>62--65</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2752" citStr="Pakhomov et al., 2004" startWordPosition="418" endWordPosition="421">lent method for identifying base noun phrases (NP), which is a key issue in symptom and disease identification. Identifying symptoms and diseases is at the heart of harnessing the potential of clinical data for medical research purposes. There are few resources that enable researchers to adapt general domain techniques to clinical text. Using the Harvey Corpus1 – a chunk annotated clinical text language resource – we present an exploratory study into adapting general domain tools and models to apply to free text notes typed by UK primary care physicians. 2 Related Work The Mayo Clinic Corpus (Pakhomov et al., 2004) is a key resource that has been widely used as a gold standard in part-of-speech (POS) tagging of clinical text. Based on that corpus and the Penn TreeBank (Marcus et al., 1993), Coden et al. (2005) present an analysis of the effects of domain data on the performance of POS tagging models, demonstrating significant improvements with models trained entirely on domain data. Savova et al. (2010) use this corpus for the development of cTAKES, Mayo Clinic’s processing pipeline for clinical text. Fan et al. (2011) show that using more diverse clinical data can lead to more accurate POS tagging. The</context>
<context position="6864" citStr="Pakhomov et al., 2004" startWordPosition="1097" endWordPosition="1100">f a computational linguist, and fifty records were double annotated by a medical expert – alone and together with a computational linguist. It is important to note that the text in the corpus is not representative of all types of GP notes. It is focused on text that represents the dominant part of day-to-day notes, rather than standard edited text such as copies of letters to specialists and other medical practitioners. Even though the corpus data is very rich in information, its non-canonical language means that it is very different from other clinical corpora such as the Mayo Clinic Corpus (Pakhomov et al., 2004) and poses different challenges for processing. The GP notes in the Harvey Corpus can be regarded as groups of medical ‘tweets’ meant to be used mainly by the author. Sentence segmentation in the classical sense of the term is often impossible, because there are no sentences. Instead there are short bursts of phrases concatenated together often without any indication of their boundaries. The average length of a note is roughly 30 tokens including the Read code. This is in contrast to notes in other clinical text datasets, which range from 100 to 400 tokens on average (Fan et al., 2011; Pakhomo</context>
</contexts>
<marker>Pakhomov, Coden, Chute, 2004</marker>
<rawString>S. Pakhomov, A. Coden, and C. Chute. 2004. Creating a test corpus of clinical notes manually tagged for part-of-speech information. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and Its Applications, JNLPBA’04, pages 62–65, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<contexts>
<context position="12376" citStr="Ramshaw and Marcus (1995)" startWordPosition="2003" endWordPosition="2006"> that tagset (omitting RASP and ARK Twitter since they use different tagsets). In this evaluation, cTAKES is the best performing model, followed closely by the Stanford POS tagger and GENIA. The results in Table 1 show that the differences between chunking models trained on different POS annotations are small and mostly not statistically significant from each other. However, all the results are significantly better than the baseline, apart from those based on the GENIA tagger output. 4.2 Chunk Representation The dominant chunk representation standard inside, outside, begin (IOB) introduced by Ramshaw and Marcus (1995) and established with the 79 CoNLL-2000 shared task (Sang and Buchholz, 2000) takes a minimalistic approach to the representation problem in order to keep the number of labels low. Note that for chunking representations the total number of labels is the product of the chunk types and the set of representation types plus the outside tag, meaning that for IOB with our set of three chunk types (NP, MV, AP) there are seven labels. Alternative chunk representations, such as begin, end, inside, single, outside (BEISO)3 as used by Kudo and Matsumoto (2001), offer more finegrained tagsets, presumably </context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. Ramshaw and M. Marcus. 1995. Text chunking using transformation-based learning. In Proceedings of the Third Workshop on Very Large Corpora, pages 82–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning,</booktitle>
<pages>13--14</pages>
<contexts>
<context position="12453" citStr="Sang and Buchholz, 2000" startWordPosition="2015" endWordPosition="2018"> In this evaluation, cTAKES is the best performing model, followed closely by the Stanford POS tagger and GENIA. The results in Table 1 show that the differences between chunking models trained on different POS annotations are small and mostly not statistically significant from each other. However, all the results are significantly better than the baseline, apart from those based on the GENIA tagger output. 4.2 Chunk Representation The dominant chunk representation standard inside, outside, begin (IOB) introduced by Ramshaw and Marcus (1995) and established with the 79 CoNLL-2000 shared task (Sang and Buchholz, 2000) takes a minimalistic approach to the representation problem in order to keep the number of labels low. Note that for chunking representations the total number of labels is the product of the chunk types and the set of representation types plus the outside tag, meaning that for IOB with our set of three chunk types (NP, MV, AP) there are seven labels. Alternative chunk representations, such as begin, end, inside, single, outside (BEISO)3 as used by Kudo and Matsumoto (2001), offer more finegrained tagsets, presumably at a performance cost. That cost is unnecessary unless there is something to </context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E. Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of the 2nd Workshop on Learning Language in Logic and the 4th Conference on Computational Natural Language Learning, pages 13–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Savova</author>
<author>J Masanz</author>
<author>P Ogren</author>
<author>J Zheng</author>
<author>S Sohn</author>
<author>K Kipper-Schuler</author>
<author>C Chute</author>
</authors>
<title>Mayo clinical text analysis and knowledge extraction system (cTAKES): architecture, component evaluation and applications.</title>
<date>2010</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>17</volume>
<issue>5</issue>
<contexts>
<context position="3148" citStr="Savova et al. (2010)" startWordPosition="485" endWordPosition="488">language resource – we present an exploratory study into adapting general domain tools and models to apply to free text notes typed by UK primary care physicians. 2 Related Work The Mayo Clinic Corpus (Pakhomov et al., 2004) is a key resource that has been widely used as a gold standard in part-of-speech (POS) tagging of clinical text. Based on that corpus and the Penn TreeBank (Marcus et al., 1993), Coden et al. (2005) present an analysis of the effects of domain data on the performance of POS tagging models, demonstrating significant improvements with models trained entirely on domain data. Savova et al. (2010) use this corpus for the development of cTAKES, Mayo Clinic’s processing pipeline for clinical text. Fan et al. (2011) show that using more diverse clinical data can lead to more accurate POS tagging. They report that models trained on clinical text datasets from two different institutions perform on each of the datasets better than both models trained only on the same or the other dataset. Fan et al. (2013) present guidelines for syntactic parsing of clinical text and a clinical Treebank annotated according to them. The guidelines are designed to help the annotators handle the noncanonical la</context>
<context position="9691" citStr="Savova et al. (2010)" startWordPosition="1554" endWordPosition="1557">king results using YamCha and CRF++ on data automatically POS tagged using nine different models; the baseline is with no tagging. The IOB and BEISO columns compare the impact of two chunk representation strategies. The POS column indicates the part-of-speech tagging accuracy for a subset of the corpus. Asterisks indicate pairs of significantly different YamCha and CRF++ results (t-test with 0.05 p-value). vey Corpus with automatically generated POS annotation. Given the small amount of data and the challenges presented above, we expected that our results would be lower than those reported by Savova et al. (2010). The aim of these experiments is to find the best performance obtainable with standard chunking tools, which we will build on in further stages of our research. We conducted pairs of experiments, one with each chunking tool, divided into three groups: the first investigates the effects of choice of POS tagger for training data annotation (Section 4.1); the second compares two chunk representations (Section 4.2); and the third searches for the optimal context features (Section 4.3). All feature tuning experiments were conducted on a development set and tested using 10-fold cross-validation on </context>
</contexts>
<marker>Savova, Masanz, Ogren, Zheng, Sohn, Kipper-Schuler, Chute, 2010</marker>
<rawString>G. Savova, J. Masanz, P. Ogren, J. Zheng, S. Sohn, K. Kipper-Schuler, and C. Chute. 2010. Mayo clinical text analysis and knowledge extraction system (cTAKES): architecture, component evaluation and applications. Journal of the American Medical Informatics Association, 17(5):507–513.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Sun</author>
<author>L-P Morency</author>
<author>D Okanoharay</author>
<author>J Tsujii</author>
</authors>
<title>Modeling latent-dynamic in shallow parsing: A latent conditional model with improved inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLING’08),</booktitle>
<location>Manchester, UK,</location>
<contexts>
<context position="7835" citStr="Sun et al., 2008" startWordPosition="1262" endWordPosition="1265"> any indication of their boundaries. The average length of a note is roughly 30 tokens including the Read code. This is in contrast to notes in other clinical text datasets, which range from 100 to 400 tokens on average (Fan et al., 2011; Pakhomov et al., 2004). As well as typical clinical text characteristics such as domainspecific acronyms, slang, and abbreviations, punctuation and casing are often misleading (if present at all), and some common classes of words (e.g. auxiliary verbs) are almost completely absent. 4 Chunking State-of-the-art text chunking accuracy reaches an f-score of 95% (Sun et al., 2008). However, this is for standard, edited text, and relies on accurate POS tagging in a pre-processing step. However, the characteristics of GP-written free text make accurate part of speech (POS) tagging and chunking difficult. Major problems are caused by unknown tokens and ambiguities due to omitted words or phrases. We evaluate two standard chunking tools, YamCha (Kudo and Matsumoto, 2003) and CRF++2, selected based on their support for trainable context features. The tools were applied to the Har2http://crfpp.googlecode.com/svn/ trunk/doc/index.html 78 POS YamCha IOB YamCha BEISO CRF++ IOB </context>
</contexts>
<marker>Sun, Morency, Okanoharay, Tsujii, 2008</marker>
<rawString>X. Sun, L.-P. Morency, D. Okanoharay, and J. Tsujii. 2008. Modeling latent-dynamic in shallow parsing: A latent conditional model with improved inference. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING’08), Manchester, UK, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toutanova</author>
<author>D Klein</author>
<author>C D Manning</author>
<author>Y Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11093" citStr="Toutanova et al., 2003" startWordPosition="1789" endWordPosition="1792"> model is trained on the same amount of data as the testing model. 4.1 Part-of-Speech Tagging We evaluated and compared the results yielded by the two chunkers, having applied each of seven off-the-shelf POS taggers. Of these taggers, cTAKES (Savova et al., 2010) and GENIA (Tsuruoka et al., 2005) are the only ones trained on data that resembles ours, which suggests that they should have the best chance of performing well. We also selected a number of other taggers while trying to diversify their algorithms and training data as much as possible: the POS tagger part of the Stanford NLP package (Toutanova et al., 2003) because it is one of the most successfully applied in the field; the RASP tagger (Briscoe et al., 2006) because of its British National Corpus (Clear, 1993) training data; the ARK tagger (Owoputi et al., 2013) because of the terseness of the tweet language; and the SVMTool (Gim´enez and M`arquez, 2004) and Wapiti (Lavergne et al., 2010) because they use SVM and CRF algorithms. Our baseline model uses no part of speech information. Using the Penn TreeBank tagset (Marcus et al., 1993), we manually annotated a subset of the corpus of comparable size to the development set. Using this dataset we </context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL’03, pages 173–180, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tsuruoka</author>
<author>Y Tateishi</author>
<author>J-D Kim</author>
<author>T Ohta</author>
<author>J McNaught</author>
<author>S Ananiadou</author>
<author>J Tsujii</author>
</authors>
<title>Developing a robust part-of-speech tagger for biomedical text.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Panhellenic Conference on Advances in Informatics, PCI’05,</booktitle>
<pages>382--392</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="10767" citStr="Tsuruoka et al., 2005" startWordPosition="1732" endWordPosition="1735">l context features (Section 4.3). All feature tuning experiments were conducted on a development set and tested using 10-fold cross-validation on the rest of the data. We used 10% of the whole data for the development set and 90% of the remaining data for a training sample during development. This guarantees the development model is trained on the same amount of data as the testing model. 4.1 Part-of-Speech Tagging We evaluated and compared the results yielded by the two chunkers, having applied each of seven off-the-shelf POS taggers. Of these taggers, cTAKES (Savova et al., 2010) and GENIA (Tsuruoka et al., 2005) are the only ones trained on data that resembles ours, which suggests that they should have the best chance of performing well. We also selected a number of other taggers while trying to diversify their algorithms and training data as much as possible: the POS tagger part of the Stanford NLP package (Toutanova et al., 2003) because it is one of the most successfully applied in the field; the RASP tagger (Briscoe et al., 2006) because of its British National Corpus (Clear, 1993) training data; the ARK tagger (Owoputi et al., 2013) because of the terseness of the tweet language; and the SVMTool</context>
</contexts>
<marker>Tsuruoka, Tateishi, Kim, Ohta, McNaught, Ananiadou, Tsujii, 2005</marker>
<rawString>Y. Tsuruoka, Y. Tateishi, J.-D. Kim, T. Ohta, J. McNaught, S. Ananiadou, and J. Tsujii. 2005. Developing a robust part-of-speech tagger for biomedical text. In Proceedings of the 10th Panhellenic Conference on Advances in Informatics, PCI’05, pages 382–392, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
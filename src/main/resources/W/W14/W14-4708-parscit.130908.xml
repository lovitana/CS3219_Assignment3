<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014776">
<title confidence="0.997859">
Retrieving Word Associations with a Simple Neighborhood Algorithm
in a Graph-Based Resource
</title>
<author confidence="0.424579">
Gemma Bel-Enguix
</author>
<sectionHeader confidence="0.529317333333333" genericHeader="abstract">
LIF
Aix Marseille Universit6,
13288 Marseille
</sectionHeader>
<email confidence="0.990598">
gemma.belenguix@gmail.com
</email>
<sectionHeader confidence="0.995515" genericHeader="method">
Abstract
</sectionHeader>
<bodyText confidence="0.9999425">
The paper explains the procedure to obtain word associations starting from a graph that has not
been specifically built for that purpose. Our goal is being able to simulate human word asso-
ciations by using the simplest possible methods, including the basic tools of a co-occurrence
network from a non-annotated corpus, and a very simple search algorithm based on neighbor-
hood. The method has been tested in the Cogalex shared task, revealing the difficulty of
achieving word associations without semantic annotation.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998030107142857">
Building annotated computational resources for natural language is a difficult and time-consuming
task that not always produces the desired results. A good alternative to semantic annotation by hand
could be using statistics and graph-based operations in corpora. In order to implement a system capa-
ble to work with such methods we have designed co-occurrence networks from large existing corpora,
like Wikipedia or the British National Corpus (Burnard &amp; Aston, 1998). The underlying idea is that
systems based on mathematics and statistics can achieve comparable results to the ones obtained with
more sophisticated methods relying on semantic processing.
Non-annotated networks have been suggested and implemented, for example, by Ferrer-i-Cancho
and Sol6 (2001). The authors suggested non-semantically annotated graphs, building exclusively syn-
tagmatic networks. By this method, they reduced the syntagmatic-paradigmatic relations. The authors
used the BNC corpus to build two graphs G1 and G2. First, a so-called co-occurrence graph G1 in
which words are linked if they co-occur in at least one sentence within a span of maximal three tokens.
Then a collocation graph G2 is extracted in which only those links of G1 are retained whose end verti-
ces co-occur more frequent than expected by chance.
A non-annotated graph built from a large corpus (Bel-Enguix and Zock, 2013) is a good representa-
tion to allow for the discovery of a large number of word relationships. It can be used for a number of
tasks, one of them being computing word associations. To test the consistence of the results obtained
by our method, they will be compared with the Edinburgh Association Thesaurus, a collection of 8000
words whose association norms were produced by presenting each of the stimulus words to about 100
subjects each, and by collecting their responses. The subjects were 17 to 22 year old British students.
To perform the tests, we take a sample (EAT: http://www.eat.rl.ac.uk/) consisting in 100 words.
For building a network to deal with the specific task of producing word associations we have used
the British National Corpus (BNC) as a source.
The way the network has been constructed has also some interest and impact in the final results.
Firstly, for the sake of simplicity, we removed all words other than Nouns and Adjectives. Nouns have
been normalized to singular form. After this pre-processing, a graph has been built where the nouns
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.95496">
60
</page>
<note confidence="0.282711">
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 60–63,
Dublin, Ireland, August 23, 2014.
</note>
<bodyText confidence="0.998700833333333">
and adjectives in the corpus are the nodes, and where the edges between these nodes are zero at the
beginning, and are incremented by 1 whenever the two respective words co-occur in the corpus as di-
rect neighbors (i.e. more distant neighborhood was not taken into account). That is, after processing
the corpus the weight of each edge represents the number of times the respective words (nodes) co-
occur.
To build the graph our system runs through a pipeline of four modules:
</bodyText>
<listItem confidence="0.993705142857143">
• document cleaning (deletion of stop-words), extracting only ‘Nouns’ and ‘Adjectives’;
• lemmatisation of word forms to avoid duplicates (horse, horses);
• computation of the (un-directed) graph&apos;s edges. Links are created between direct neighbours;
• computation of the edges’ weights. The weight of an edge is equal to the number of its occur-
rences. We only use absolute values.
• computation of the node’s weights. As in the edges, the weight of a node is the number of it
occurrences.
</listItem>
<bodyText confidence="0.9541114">
The graph has been implemented with Python.
The resultant network has 427668 nodes (different words). Of them, 1894 are happax (occur only
once), only the 0,5%. There are 13654814 edges. From them, 9836987 with weight one; and 3817827
have a weight higher than one, on a percentage relation 72/28. The average degree of the nodes of the
network is 31, 92.
</bodyText>
<sectionHeader confidence="0.976398" genericHeader="method">
2 Searching method
</sectionHeader>
<bodyText confidence="0.988086">
The search of the target word in the graph has two different steps:
</bodyText>
<listItem confidence="0.9796545">
1. Determining the set of common neighbors of the clues,
2. Ranking the set of nodes obtained in 1, and picking the ‘best result’.
</listItem>
<subsectionHeader confidence="0.999633">
2.1 Search of neighbors
</subsectionHeader>
<bodyText confidence="0.99822525">
The search of the target word T in a graph G, is done via some clues c1, c2,..., cn, which act as in-
puts. G=(V, E) stands for the graph, with V expressing the set of vertices (words) and E the set of
edges (co-occurrences). The clues c1, c2,..., cn EV. N(i) expresses the neighbourhood of a node i EV,
and is defined as &apos;every jEV  |ei,j EE. The search algorithm is as follows:
</bodyText>
<listItem confidence="0.94041325">
• Define the neighbourhood of c1, c2,..., cn as N(c1), N(c2),..., N(cn);
• Get the set of nodes VT = N(c1) n N(c2) n ... nN(cn) and consider Vc={c1, c2,..., cn} to be the
set of nodes representing the clues. We define a subgraph of G, GT, that is a complete bipar-
tite graph, where every element of VT is connected to every element of Vc;
</listItem>
<bodyText confidence="0.999892142857143">
In the Cogalex shared task, five clues have been given, belonging to any grammatical category, and
in different inflected forms (ie., am, be, been or horse, horses). Since the graph has the limitation of
containing only Nouns and Adjectives, the system dismisses every word not belonging to the set of
nodes V and uses only the remaining clues. And being the words lemmatized, inflected forms are re-
duced to only one. Therefore, the application will never find ‘be’ from ‘am’, ‘been’, ‘is’.
To build the graph and perform the search, a Python module has been used, Networkx
(https://networkx.github.io/), that is extremely fast and efficient.
</bodyText>
<subsectionHeader confidence="0.999801">
2.2 Ranking the nodes
</subsectionHeader>
<bodyText confidence="0.994514888888889">
This task has been designed with a very simple algorithm. Let’s consider C the number of final
stimulus words; wc1,wc2,...,wcn is the weight in the graph of every node c E VC; wt1,wt2,...,wtn the
weight in the graph of every one of the nodes t E VT; wetc the weight for every edge of GT, where c E
VC and tE VT.
The nodes of the graph are gathered in groups in a logarithmic scale: up to 101, 102, 103, 104, 105,
106. We name a the power of 10, ie., for 106, a=6.
The nodes of VT are ranked with a simple algorithm, consisting in calculating Wt for every t E VT,
so as
The nodes are ranked according to the values of W.
</bodyText>
<page confidence="0.999411">
61
</page>
<sectionHeader confidence="0.999897" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999895333333333">
In some initial tests, the results were compared with the ones obtained in a sample of the Edinburgh
Association Thesaurus (EAT: http://www.eat.rl.ac.uk/) consisting in 100 words. The EAT (Kiss et al.,
1973) has 8000 words, and the 100 selected for the test were all of them nouns or adjectives, what
made the working easier for our system. There were 15 words that match the ones observed in the Ed-
inburgh Associative Thesaurus (EAT) as Primary Response (PR). There is a partial coincidence – the
word given has not a 0 in the EAT – in 54 of the outputs. This means that in more than 50% of the
cases the method retrieves a word corresponding to the one produced by a human in the association
experiment. This does not imply though that it is the most popular one.
Some other methods of evaluation (Evert &amp; Krenn, 2001) have been applied to the system (Bel-
Enguix et al., 2014), showing that the outcomes provided by the graph-based method are quite consis-
tent with human responses, and even optimize them in some specific classes.
In contrast with these results, the ones obtained in the Cogalex shared task were clearly worse.
From a total of 200 items, the number of matches was 182, which means an accuracy of the 9,10%.
There are several reasons for that: a) some of the targets were not Nouns or Adjectives, what makes
them not retrievable for the system, b) many stimulus words were not Nouns or Adjectives, what
makes the algorithm weaker, because such words are dismissed as clues, c) stimulus were not lemma-
tized and the lemmatization process for words without a context is not easy for the python lemmatiza-
tion module, d) probably many of the words of the first tested sample were very well-known relations,
while the ones in the Cogalex shared task could be less well-connected nodes, e) the ranking algorithm
can be clearly improved in order to retrieve the best word, not only one in the list, because we have
been asked only full matches.
</bodyText>
<sectionHeader confidence="0.769511" genericHeader="method">
4 Conclusions: strengths and weakness of the method
</sectionHeader>
<bodyText confidence="0.999955947368421">
Even though the results obtained were not good, there are several strengths that make this system
worth to be improved in the future.
Firstly, the network is easy to built and program is very fast. We have used the python package
‘networkx’ to build the graph, integrating its commands into the python script. The result is that in less
than one minute the system can compute the two thousand associations that were required. Therefore,
while an important improvement is needed in the ranking algorithm, there is room for it, because the
performance of the method can afford it.
Secondly, the system works with any co-occurrence graph made from any corpus. This allows us to
use specialized corpora as a basis, as well as collections of texts closer to the time the human associa-
tions have been produced.
However, there are important weaknesses in the procedure. In the first place, it is necessary to use a
network resource including other grammatical categories, at least verbs and adverbs. Even though
such graph exists, the difficulty in the application of the current ranking algorithm makes it not-usable
so far for this specific task. There is still another clear difficulty in the method, related to the one we
just stated: the lack of clustering. Not using semantic annotations is one of our axioms, because it
makes the system heavier. Nevertheless, a way to detect which words are more related is needed. This
is currently the strongest weakness of this graph-based algorithm. We propose for the future a very
simple clustering based on WordNet synsets (Miller, 1990), in a way the search can be oriented to-
wards the best choices for every word connection, even though their weight in the graph is lower.
</bodyText>
<sectionHeader confidence="0.999211" genericHeader="conclusions">
5 Aknowledgements
</sectionHeader>
<bodyText confidence="0.9812715">
I am very grateful to my colleagues Michael Zock and Reinhard Rapp for their expertise, comments
and constant support.
This work has been supported by the European Commission under a Marie Curie Fellowship, pro-
ject DynNetLAc.
</bodyText>
<page confidence="0.998986">
62
</page>
<sectionHeader confidence="0.995897" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999830266666667">
Bel-Enguix, G., Rapp, R. and Zock, M. (2014) A Graph-Based Approach for Computing Free Word Associa-
tions, Proceedings of LREC 2014, Ninth International Conference on Language Resources and Evaluation,
3027-3033.
Bel-Enguix, G. and Zock, M. (2013). Lexical Access via a Simple Co-occurrence Network, Proceedings of
TALN-RECITAL 2013, 596-603.
Burnard, L. and Aston, G. (1998). The BNC Handbook: Exploring the British National Corpus. Edinburgh: Ed-
inburgh University Press
Evert, S. and Krenn, B. (2001). Methods for qualitative evaluation of lexical association measures. In Proceed-
ings of the 39th Annual Meeting of the Association of Computational Linguistics, Toulouse, France, 188-915.
Ferrer-Cancho, R., Solé, R. (2001). The small-world of human language. Proceedings of the Royal Society of
London. Series B, Biological Sciences 268 (2001) 2261-2265.
Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J. (1973). An associative thesaurus of English and its computer
analysis. In: A. Aitken, R. Beiley, N. Hamilton-Smith (eds.): The Computer and Literary Studies. Edinburgh
University Press.
Miller, G. (1990). Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4).
</reference>
<page confidence="0.999464">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924612">
<title confidence="0.9991635">Retrieving Word Associations with a Simple Neighborhood Algorithm in a Graph-Based Resource</title>
<author confidence="0.971891">Gemma</author>
<affiliation confidence="0.977024">Aix Marseille</affiliation>
<address confidence="0.969432">13288 Marseille</address>
<email confidence="0.999929">gemma.belenguix@gmail.com</email>
<abstract confidence="0.999315857142857">The paper explains the procedure to obtain word associations starting from a graph that has not been specifically built for that purpose. Our goal is being able to simulate human word associations by using the simplest possible methods, including the basic tools of a co-occurrence network from a non-annotated corpus, and a very simple search algorithm based on neighborhood. The method has been tested in the Cogalex shared task, revealing the difficulty of achieving word associations without semantic annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Bel-Enguix</author>
<author>R Rapp</author>
<author>M Zock</author>
</authors>
<title>A Graph-Based Approach for Computing Free Word Associations,</title>
<date>2014</date>
<booktitle>Proceedings of LREC 2014, Ninth International Conference on Language Resources and Evaluation,</booktitle>
<pages>3027--3033</pages>
<marker>Bel-Enguix, Rapp, Zock, 2014</marker>
<rawString>Bel-Enguix, G., Rapp, R. and Zock, M. (2014) A Graph-Based Approach for Computing Free Word Associations, Proceedings of LREC 2014, Ninth International Conference on Language Resources and Evaluation, 3027-3033.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bel-Enguix</author>
<author>M Zock</author>
</authors>
<title>Lexical Access via a Simple Co-occurrence Network,</title>
<date>2013</date>
<booktitle>Proceedings of TALN-RECITAL 2013,</booktitle>
<pages>596--603</pages>
<contexts>
<context position="2097" citStr="Bel-Enguix and Zock, 2013" startWordPosition="309" endWordPosition="312">and Sol6 (2001). The authors suggested non-semantically annotated graphs, building exclusively syntagmatic networks. By this method, they reduced the syntagmatic-paradigmatic relations. The authors used the BNC corpus to build two graphs G1 and G2. First, a so-called co-occurrence graph G1 in which words are linked if they co-occur in at least one sentence within a span of maximal three tokens. Then a collocation graph G2 is extracted in which only those links of G1 are retained whose end vertices co-occur more frequent than expected by chance. A non-annotated graph built from a large corpus (Bel-Enguix and Zock, 2013) is a good representation to allow for the discovery of a large number of word relationships. It can be used for a number of tasks, one of them being computing word associations. To test the consistence of the results obtained by our method, they will be compared with the Edinburgh Association Thesaurus, a collection of 8000 words whose association norms were produced by presenting each of the stimulus words to about 100 subjects each, and by collecting their responses. The subjects were 17 to 22 year old British students. To perform the tests, we take a sample (EAT: http://www.eat.rl.ac.uk/) </context>
</contexts>
<marker>Bel-Enguix, Zock, 2013</marker>
<rawString>Bel-Enguix, G. and Zock, M. (2013). Lexical Access via a Simple Co-occurrence Network, Proceedings of TALN-RECITAL 2013, 596-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Burnard</author>
<author>G Aston</author>
</authors>
<title>The BNC Handbook: Exploring the British National Corpus. Edinburgh:</title>
<date>1998</date>
<publisher>Edinburgh University Press</publisher>
<contexts>
<context position="1189" citStr="Burnard &amp; Aston, 1998" startWordPosition="171" endWordPosition="174">The method has been tested in the Cogalex shared task, revealing the difficulty of achieving word associations without semantic annotation. 1 Introduction Building annotated computational resources for natural language is a difficult and time-consuming task that not always produces the desired results. A good alternative to semantic annotation by hand could be using statistics and graph-based operations in corpora. In order to implement a system capable to work with such methods we have designed co-occurrence networks from large existing corpora, like Wikipedia or the British National Corpus (Burnard &amp; Aston, 1998). The underlying idea is that systems based on mathematics and statistics can achieve comparable results to the ones obtained with more sophisticated methods relying on semantic processing. Non-annotated networks have been suggested and implemented, for example, by Ferrer-i-Cancho and Sol6 (2001). The authors suggested non-semantically annotated graphs, building exclusively syntagmatic networks. By this method, they reduced the syntagmatic-paradigmatic relations. The authors used the BNC corpus to build two graphs G1 and G2. First, a so-called co-occurrence graph G1 in which words are linked i</context>
</contexts>
<marker>Burnard, Aston, 1998</marker>
<rawString>Burnard, L. and Aston, G. (1998). The BNC Handbook: Exploring the British National Corpus. Edinburgh: Edinburgh University Press</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Evert</author>
<author>B Krenn</author>
</authors>
<title>Methods for qualitative evaluation of lexical association measures.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<location>Toulouse,</location>
<contexts>
<context position="7916" citStr="Evert &amp; Krenn, 2001" startWordPosition="1324" endWordPosition="1327">, 1973) has 8000 words, and the 100 selected for the test were all of them nouns or adjectives, what made the working easier for our system. There were 15 words that match the ones observed in the Edinburgh Associative Thesaurus (EAT) as Primary Response (PR). There is a partial coincidence – the word given has not a 0 in the EAT – in 54 of the outputs. This means that in more than 50% of the cases the method retrieves a word corresponding to the one produced by a human in the association experiment. This does not imply though that it is the most popular one. Some other methods of evaluation (Evert &amp; Krenn, 2001) have been applied to the system (BelEnguix et al., 2014), showing that the outcomes provided by the graph-based method are quite consistent with human responses, and even optimize them in some specific classes. In contrast with these results, the ones obtained in the Cogalex shared task were clearly worse. From a total of 200 items, the number of matches was 182, which means an accuracy of the 9,10%. There are several reasons for that: a) some of the targets were not Nouns or Adjectives, what makes them not retrievable for the system, b) many stimulus words were not Nouns or Adjectives, what </context>
</contexts>
<marker>Evert, Krenn, 2001</marker>
<rawString>Evert, S. and Krenn, B. (2001). Methods for qualitative evaluation of lexical association measures. In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics, Toulouse, France, 188-915.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ferrer-Cancho</author>
<author>R Solé</author>
</authors>
<title>The small-world of human language.</title>
<date>2001</date>
<journal>Proceedings of the Royal Society of London. Series B, Biological Sciences</journal>
<volume>268</volume>
<pages>2261--2265</pages>
<marker>Ferrer-Cancho, Solé, 2001</marker>
<rawString>Ferrer-Cancho, R., Solé, R. (2001). The small-world of human language. Proceedings of the Royal Society of London. Series B, Biological Sciences 268 (2001) 2261-2265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G R Kiss</author>
<author>C Armstrong</author>
<author>R Milroy</author>
<author>J Piper</author>
</authors>
<title>An associative thesaurus of English and its computer analysis.</title>
<date>1973</date>
<editor>In: A. Aitken, R. Beiley, N. Hamilton-Smith (eds.):</editor>
<publisher>Edinburgh University Press.</publisher>
<contexts>
<context position="7303" citStr="Kiss et al., 1973" startWordPosition="1209" endWordPosition="1212">ry one of the nodes t E VT; wetc the weight for every edge of GT, where c E VC and tE VT. The nodes of the graph are gathered in groups in a logarithmic scale: up to 101, 102, 103, 104, 105, 106. We name a the power of 10, ie., for 106, a=6. The nodes of VT are ranked with a simple algorithm, consisting in calculating Wt for every t E VT, so as The nodes are ranked according to the values of W. 61 3 Results In some initial tests, the results were compared with the ones obtained in a sample of the Edinburgh Association Thesaurus (EAT: http://www.eat.rl.ac.uk/) consisting in 100 words. The EAT (Kiss et al., 1973) has 8000 words, and the 100 selected for the test were all of them nouns or adjectives, what made the working easier for our system. There were 15 words that match the ones observed in the Edinburgh Associative Thesaurus (EAT) as Primary Response (PR). There is a partial coincidence – the word given has not a 0 in the EAT – in 54 of the outputs. This means that in more than 50% of the cases the method retrieves a word corresponding to the one produced by a human in the association experiment. This does not imply though that it is the most popular one. Some other methods of evaluation (Evert &amp;</context>
</contexts>
<marker>Kiss, Armstrong, Milroy, Piper, 1973</marker>
<rawString>Kiss, G.R., Armstrong, C., Milroy, R., and Piper, J. (1973). An associative thesaurus of English and its computer analysis. In: A. Aitken, R. Beiley, N. Hamilton-Smith (eds.): The Computer and Literary Studies. Edinburgh University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<marker>Miller, 1990</marker>
<rawString>Miller, G. (1990). Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
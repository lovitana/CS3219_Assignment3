<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022444">
<title confidence="0.974233">
The Impact of Machine Translation Quality on Human Post-editing
</title>
<author confidence="0.96812">
Philipp Koehn** Ulrich Germann*
</author>
<email confidence="0.953396">
pkoehn@inf.ed.ac.uk ugermann@inf.ed.ac.uk
</email>
<note confidence="0.926304">
*Center for Speech and Language Processing *School of Informatics
The Johns Hopkins University University of Edinburgh
</note>
<sectionHeader confidence="0.976003" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999357222222222">
We investigate the effect of four different
competitive machine translation systems
on post-editor productivity and behaviour.
The study involves four volunteers post-
editing automatic translations of news sto-
ries from English to German. We see sig-
nificant difference in productivity due to
the systems (about 20%), and even bigger
variance between post-editors.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996628607142857">
Statistical machine translation (SMT) has made
considerable progress over the past two decades.
Numerous recent studies have shown productivity
increases with post-editing of MT output over tra-
ditional work practices in human translation (e.g.,
Guerberof, 2009; Plitt and Masselot, 2010; Garcia,
2011; Pouliquen et al., 2011; Skadin¸ˇs et al., 2011;
den Bogaert and Sutter, 2013; Vazquez et al.,
2013; Green et al., 2013; L¨aubli et al., 2013).
The advances in statistical machine translation
over the past years have been driven to a large ex-
tent by frequent (friendly) competitive MT eval-
uation campaigns, such as the shared tasks at the
ACL WMT workshop series (Bojar et al., 2013)
and IWSLT (Cettolo et al., 2013), and the NIST
Open MT Evaluation.1 These evaluations usu-
ally apply a mix of automatic evaluation metrics,
most prominently the BLEU score (Papineni et al.,
2001), and more subjective human evaluation cri-
teria such as correctness, accuracy, and fluency.
How the quality increases measured by auto-
matic metrics and subjective evaluation criteria re-
late to actual increases in the productivity of post-
editors is still an open research question. It is
also not clear yet if some machine translation ap-
proaches — say, syntax-based models — are bet-
ter suited for post-editing than others. These re-
lationships may very well also depend on the lan-
</bodyText>
<footnote confidence="0.868105">
1 http://www.nist.gov/itl/iad/mig/openmt.cfm
</footnote>
<bodyText confidence="0.998927">
guage pair in question and the coarse level of MT
quality, from barely good enough for post-editing
to almost perfect.
The pilot study presented in this paper investi-
gates the influence of the underlying SMT system
on post-editing effort and efficiency. The study
focuses on translation of general news text from
English into German, with translations created by
non-professional post-editors working on output
from four different translation systems. The data
generated by this study is available for download.2
We find that the better systems lead to a produc-
tivity gain of roughly 20% and carry out in-depth
analysis of editing behavior. A significant find-
ing is the high variance in work styles between the
different post-editors, compared to the impact of
machine translation systems.
</bodyText>
<sectionHeader confidence="0.999749" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999925857142857">
Koponen (2012) examined the relationship be-
tween human assessment of post-editing efforts
and objective measures such as post-editing time
and number of edit operations. She found that seg-
ments that require a lot of reordering are perceived
as being more difficult, and that long sentences
are considered harder, even if only few words
changed. She also reports larger variance between
translators in post-editing time than in post-editing
operations — a finding that we confirm here as
well.
From a detailed analysis of the types of ed-
its performed in sentences with long versus short
post-edit times, Koponen et al. (2012) conclude
that the observed differences in edit times can be
explained at least in part also by the types of nec-
essary edits and the associated cognitive effort.
Deleting superfluous function words, for exam-
ple, appears to be cognitively simple and takes
little time, whereas inserting translations for un-
translated words requires more cognitive effort
</bodyText>
<footnote confidence="0.967252">
2 http://www.casmacat.eu/index.php?n=Main.Downloads
</footnote>
<page confidence="0.977699">
38
</page>
<note confidence="0.9872715">
Workshop on Humans and Computer-assisted Translation, pages 38–46,
Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics
</note>
<tableCaption confidence="0.997118">
Table 1: News stories used in the study (size is given in number of sentences)
</tableCaption>
<note confidence="0.8619476">
Source Size Title
BBC 49 Norway’s rakfisk: Is this the world’s smelliest fish?
BBC 47 Mexico’s Enrique Pena Nieto faces tough start
CNN 45 Bradley Manning didn’t complain about mistreatment, prosecutors contend
CNN 63 My Mexican-American identity crisis
</note>
<bodyText confidence="0.960148791666667">
Economist 55 Old battles, new Middle East
Guardian 38 Cigarette plain packaging laws come into force in Australia
NY Times 61 In a Constantly Plugged-In World, It’s Not All Bad to Be Bored
NY Times 47 In Colorado, No Playbook for New Marijuana Law
Telegraph 95 Petronella Wyatt: I was bullied out of Oxford for being a Tory
and takes longer. They also compare post-editing
styles of different post-editors working on identi-
cal post-editing tasks.
Another study by Koponen (2013) showed that
inter-translator variance is lower in a controlled
language setting when translators are given the
choice of output from three different machine
translation systems.
In the realm of machine translation research,
there has been an increasing interest in the use
of MT technology by post-editors. A major push
are the two EU-funded research projects MATE-
CAT3 and CASMACAT4, which are developing an
open source translation and post-editing work-
bench (Federico et al., 2012; Alabau et al., 2013).
At this point, we are not aware of any study that
compares directly the impact of different machine
translation systems on post-editor productivity and
behaviour.
</bodyText>
<sectionHeader confidence="0.998282" genericHeader="method">
3 Experimental Design
</sectionHeader>
<bodyText confidence="0.9999386">
We thus carried out an experiment on an English–
German news translation task, using output from
four different SMT systems, post-edited by fluent
bilingual native speakers of German with no prior
experience in professional translation.
</bodyText>
<subsectionHeader confidence="0.999108">
3.1 The Translation Task
</subsectionHeader>
<bodyText confidence="0.994941714285714">
The Workshop on Statistical Machine Translation
(Bojar et al., 2013) organises an annual evaluation
campaign for machine translation systems. The
subject matter is translation of news stories from
sources such as the New York Times or the BBC.
We decided to use output from systems submit-
ted to this evaluation campaign, not only because
</bodyText>
<footnote confidence="0.9996965">
3http://www.matecat.com/
4http://www.casmacat.eu/
</footnote>
<bodyText confidence="0.997726066666667">
their output is freely available,5 but also because
it comes with automatic metric scores and human
judgements of the translation quality.
The translation direction we chose was
English–German, partly due to convenience (the
authors of this study are fluent in both languages),
but also because this language pair poses special
challenges to current machine translation technol-
ogy, due to the syntactic divergence of the two
languages.
We selected data from the most recent evalua-
tion campaign. The subset chosen for our post-
editing task comprises 9 different news stories,
originally written in English, with a total of 500
sentences. Details are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.998816">
3.2 Machine Translation Systems
</subsectionHeader>
<bodyText confidence="0.9999715">
A total of 15 different machine translation systems
participated in the evaluation campaign. We se-
lected four different systems that differ in their ar-
chitecture and use of training data:
</bodyText>
<listItem confidence="0.999166363636364">
• an anonymized popular online translation
system built by a large Internet company
(ONLINE-B)
• the syntax-based translation system of the
University of Edinburgh (UEDIN-SYNTAX;
Nadejde et al., 2013)
• the phrase-based translation system of the
University of Edinburgh (UEDIN-PHRASE;
Durrani et al., 2013)
• the machine translation system of the Univer-
sity of Uppsala (UU; Stymne et al., 2013)
</listItem>
<bodyText confidence="0.950436">
In the 2013 WMT evaluation campaign, the sys-
tems translated a total of 3000 sentences, and their
</bodyText>
<footnote confidence="0.925181">
5 http://www.statmt.org/wmt13/results.html
</footnote>
<page confidence="0.999757">
39
</page>
<tableCaption confidence="0.981609666666667">
Table 2: Machine translation systems used in the
study, with quality scores in the WMT 2013 eval-
uation campaign.
</tableCaption>
<table confidence="0.9988972">
System BLEU SUBJECTIVE
ONLINE-B 20.7 0.637
UEDIN-SYNTAX 19.4 0.614
UEDIN-PHRASE 20.1 0.571
UU 16.1 0.361
</table>
<bodyText confidence="0.999064021276596">
output was judged with the BLEU score against a
professional reference translation and by subjec-
tive ranking. The scores obtained for the different
systems on the full test set are shown in Table 2.
The first three systems are fairly close in qual-
ity (although the differences in subjective hu-
man judgement scores are statistically significant),
whereas the fourth system (UU) clearly lags be-
hind. The best system ONLINE-B was ranked first
according to human judgement and thus can be
considered state of the art.
From casual observation, the syntax-based sys-
tem UEDIN-SYNTAX succeeds more frequently
in producing grammatically correct translations.
The phrase-based system UEDIN-PHRASE, even
though trained on the same parallel data, has
higher coverage since it does not have the require-
ment that translation rules have to match syntac-
tic constituents in the target language, which we
presume is the main cause behind the lower BLEU
score. The two systems use the same language
model.
System UU is also a phrase based system, with a
decoder that is able to consider the document level
context. It was trained on smaller corpora for both
the translation model and the language model.
We do not have any insight into the system
ONLINE-B, but we conjecture that it is a phrase-
based system with syntactic pre-reordering trained
on much larger data sets, but not optimised to-
wards the news domain.
Notice the inconsistency between BLEU score
and subjective score for the two systems from the
University of Edinburgh. Results from other eval-
uations have also shown (Callison-Burch et al.,
2012) that current automatic evaluation metrics
do not as much as human judges appreciate the
strengths of the syntax-based system, which builds
syntactic structures in the target language dur-
ing translation. Hence, we were particularly in-
terested how the syntax-based system fares with
post-editors.
As mentioned above, the nine documents chosen
for the post-editing task analysed in this paper (cf.
Table 1) were part of the WMT 2013 evaluation
data set. All nine documents had English as the
original source language.
</bodyText>
<subsectionHeader confidence="0.999895">
3.3 Post-Editors
</subsectionHeader>
<bodyText confidence="0.999977285714286">
We recruited four English-German bilingual, na-
tive German post-editors. Three were students,
staff, or faculty at the University of Edinburgh;
the fourth had been previously employed on a con-
tractual basis for linguistic annotation work.6 The
post-editors had no professional experience with
translation, and differed in language skills.
</bodyText>
<subsectionHeader confidence="0.999644">
3.4 Assignment of MT Output
</subsectionHeader>
<bodyText confidence="0.982897322580645">
The goal of this study was to investigate how post-
editors’ behaviour and productivity are influenced
by the quality of the underlying machine transla-
tion system. Ideally, we would want to present
output from different systems to the same post-
editor and see how their observable behaviour
changes.
However, a post-editor who has seen the out-
put from one MT system for a sentence will be
at an advantage when post-editing the output from
a second system, by having already spent signif-
icant time understanding the source sentence and
considering the best translation choices.
Hence we used 4 different post-editors, each to
post-edit the output in equal amounts from each of
the 4 machine translation systems under investiga-
tion, so that each post-editor worked on each sen-
tence once and the entire output from all systems
was post-edited once by one of the 4 post-editors.
A concern in this setup is that we never know
if we measure differences in post-editors or differ-
ences in machine translations systems when com-
paring the behaviour for any given sentence.
Therefore, each post-editor was assigned a
translation for each sentence randomly from any
of the machine translation systems. This random
assignment allows us to marginalise out the depen-
dence on the post-editor when assessing statistics
for the different systems.
6 The ordering here does not reflect the order of post-editors
in the discussion later in this paper.
</bodyText>
<page confidence="0.999019">
40
</page>
<tableCaption confidence="0.99996">
Table 3: Post-editing speed by editor and system.
</tableCaption>
<table confidence="0.993079">
System seconds / word words / hour
1 2 3 4 mean 1 2 3 4 mean
ONLINE-B 2.95 4.69 9.16 4.98 5.46 1,220 768 393 723 659
UEDIN-PHRASE 3.04 5.01 9.22 4.70 5.45 1,184 719 390 766 661
UEDIN-SYNTAX 3.03 4.41 9.20 4.97 5.38 1,188 816 391 724 669
UU 3.11 5.01 11.59 5.58 6.35 1,158 719 311 645 567
mean per editor 3.03 4.78 9.79 5.05 1,188 753 368 713
</table>
<sectionHeader confidence="0.985044" genericHeader="method">
4 Productivity
</sectionHeader>
<bodyText confidence="0.999965933333333">
The primary argument for post-editing machine
translation output as opposed to more traditional
approaches is the potential gain in productivity. If
translation professionals can work faster with ma-
chine translation, then this has real economic ben-
efits. There are also other considerations, for ex-
ample that post-editing might be done by profes-
sionals that are less skilled in the source language
(Koehn, 2010).
We measure productivity by time spent on each
sentence. This is not a perfect measure. When
working on a news story, post-editors tend to
speed up when moving down the story since they
have already solved some reoccurring translation
problems and get more familiar with the context.
</bodyText>
<subsectionHeader confidence="0.997875">
4.1 Productivity by MT System
</subsectionHeader>
<bodyText confidence="0.999925090909091">
Our main interests is the average translation speed,
broken down by machine translation system. The
columns labelled “mean” in Table 3 show the re-
sults. While the differences are not big for the top
three systems, the syntax-based system comes out
on top.
We used bootstrap resampling to test the speed
differences for statistical significance. Only sys-
tem UU is significantly worse than the others (at
p-level &lt; 0.01), with about 20% lower productiv-
ity.
</bodyText>
<subsectionHeader confidence="0.999095">
4.2 Productivity by Post-Editor
</subsectionHeader>
<bodyText confidence="0.999875066666667">
Post-editing speed is very strongly influenced by
the post-editor’s skill and effort. Our post-editors
were very diverse, showing large differences in
translation speed. See the columns labelled 1 to
4 in Table 3 for details.
In particular, post-editor 3 took more than three
times as much time as the fastest (PE 1). Accord-
ing to a post-study interview with Post-Editor 3,
there were two reasons for this. First, the post-
editor was feeling a bit “under the weather” dur-
ing the study and found it hard to focus. Second,
(s)he found the texts very difficult to translate and
struggled with idiomatic expressions and cultural
references that (s)he did not understand immedi-
ately.
</bodyText>
<subsectionHeader confidence="0.999673">
4.3 Productivity by System and Post-Editor
</subsectionHeader>
<bodyText confidence="0.999948461538462">
While the large differences between the post-
editors are unfortunate when the goal is consis-
tency in results, they provide some data on how
post-editors of different skill levels are influenced
by the quality of the machine translation systems.
Table 3 breaks down translation speed by ma-
chine translation system and post-editor. Interest-
ingly, machine translation quality has hardly any
effect on the fast Post-Editor 1, and the lower
MT performance of system UU affects only Post-
Editors 3 and 4. Post-Editor 2 is noticeably faster
with UEDIN-SYNTAX — an effect that cannot be
observed for the other post-editors. The differ-
ences between the other systems are not large for
any of the post-editors.
Statistically significant — as determined by
bootstrap resampling — are only the differences
in post-editing speed for Post-Editor 3 with sys-
tem UU versus ONLINE-B and UEDIN-PHRASE at
p-level &lt; 0.01, and against UEDIN-SYNTAX at p-
level &lt;0.02, and for Post-Editor 4 for UU versus
UEDIN-PHRASE at p-level &lt; 0.05. Note that the
absence of statistical significance in our data has
much to do with the small sample size; more ex-
tensive experiments may be necessary to ensure
more solid findings.
</bodyText>
<sectionHeader confidence="0.994568" genericHeader="method">
5 Translation Edit Rate
</sectionHeader>
<bodyText confidence="0.999977666666667">
Given the inherent difficulties in obtaining tim-
ing information, we can also measure the impact
of machine translation system quality on post-
editing effort in terms of how much the post-
editors change the machine translation output, as
done, for example in Cettolo et al. (2013).
</bodyText>
<page confidence="0.999616">
41
</page>
<tableCaption confidence="0.998821">
Table 4: Edit rate and types of edits per system
</tableCaption>
<table confidence="0.9997426">
System HTER ins del sub shift wide shift
ONLINE-B 35.7 4.8 7.4 18.9 4.6 5.8
UEDIN-PHRASE 37.9 5.5 7.4 20.0 5.0 6.6
UEDIN-SYNTAX 36.7 4.7 7.6 19.8 4.6 5.7
UU 43.7 4.6 11.4 21.9 5.8 7.2
</table>
<tableCaption confidence="0.971972">
Table 5: Edit rate and types of edits per post-editor
</tableCaption>
<table confidence="0.999583333333333">
P-E HTER ins del sub shift wide
shift
1 35.2 5.4 6.7 18.7 4.4 5.3
2 43.1 4.1 10.4 23.1 5.4 6.9
3 37.7 5.9 7.9 18.8 5.0 6.6
4 37.5 4.3 8.5 19.6 5.1 6.4
</table>
<bodyText confidence="0.999717125">
There are two ways to measure how much the
machine translation output was edited by the post-
editor. One way is to compare the final translation
with the original machine translation output. This
is what we will do in this section. In Section 6,
we will consider which parts of the final transla-
tion were actually changed by the post-editor and
discuss the difference.
</bodyText>
<subsectionHeader confidence="0.967426">
5.1 HTER as Quality Measure
</subsectionHeader>
<bodyText confidence="0.9999622">
The edit distance between machine translation
output and human reference translation can be
measured in the number of insertions, deletions,
substitutions and (phrasal) moves. A metric that
simply counts the minimal number of such edit op-
erations and divides it by the length of the human
reference translation is the translation edit rate,
short TER (Snover et al., 2006).
If the human reference translation is created
from the machine translation output to minimise
the number of edit operations needed for an ac-
ceptable translation, this variant is called human-
mediated TER, or HTER. Note that in our experi-
ment the post-editors are not strictly trying to min-
imise the number of edit operations — they may
be inclined to make additional changes due to ar-
bitrary considerations of style or perform edits that
are faster rather than minimise the number of oper-
ations (e.g., deleting whole passages and rewriting
them).
</bodyText>
<subsectionHeader confidence="0.994863">
5.2 Edits by MT System
</subsectionHeader>
<bodyText confidence="0.999978121212121">
Table 4 shows the HTER scores — keep in mind
our desiderata above — for the four systems. The
scores are similar to the productivity number, with
the three leading systems close together and the
trailing system UU well behind.
Notably, we draw more statistically significant
distinctions here. While as above, UU is signif-
icantly worse than all other systems (p-level &lt;
0.01), we also find that ONLINE-B is better than
UEDIN-PHRASE (p-level &lt; 0.01).
Hence, HTER is a more sensitive metric than
translation speed. This may be due to the fact
that the time measurements are noisier than the
count of edit operations. But it may also because
HTER and productivity (i.e., time) do not measure
the exactly the same thing. For instance, edits that
require only a few keystrokes may be cognitively
demanding (e.g., terminological choices), and thus
take more time.
We cannot make any strong claim based on
our numbers, but it is worth pointing out that
post-editing UEDIN-SYNTAX was slightly faster
than ONLINE-B (by 0.08 seconds/word), while the
HTER score is lower (by 1 point). A closer look
at the edit operations reveals that the post-edit
of UEDIN-SYNTAX output required slightly fewer
short and long shifts (movements of phrases), but
more substitutions. Intuitively, moving a phrase
around is a more time-consuming task than replac-
ing a word. The benefit of a syntax-based sys-
tem that aims to produce correct syntactic struc-
ture (including word order), may have real benefits
in terms of post-editing time.
</bodyText>
<subsectionHeader confidence="0.985794">
5.3 Edits by Post-Editor
</subsectionHeader>
<bodyText confidence="0.998446666666667">
Table 5 displays the edit rate broken down by post-
editor. There is little correlation between edit rate
and post-editor speed. While the fastest Post-
Editor 1 produces translations with the smallest
edit rate, the difference to two of the others (in-
cluded the slowest Post-Editor 3) is not large. The
</bodyText>
<page confidence="0.989641">
42
</page>
<table confidence="0.987866441860465">
+ + +
 |sec  |current_translation |
+ + +
 |0.000  |Norwegen ist es nicht oft auf die globale Agenda - und die meisten Nachrichten, wie es scheint. |
 |0.508  |Norwegen ist esnicht oft auf die globale Agenda - und die meisten Nachrichten, wie es scheint. |
 |0.733  |Norwegen ist enicht oft auf die globale Agenda - und die meisten Nachrichten, wie es scheint. |
 |0.970  |Norwegen ist enicht oft auf die globale Agenda - und die meisten Nachrichten, wie es scheint. |
 |0.975  |Norwegen ist nicht oft auf die globale Agenda - und die meisten Nachrichten, wie es scheint. |
 |3.317  |Norwegen ist nicht oft auf di globale Agenda - und die meisten Nachrichten, wie es scheint. |
 |3.413  |Norwegen ist nicht oft auf d globale Agenda - und die meisten Nachrichten, wie es scheint. |
 |3.524  |Norwegen ist nicht oft auf de globale Agenda - und die meisten Nachrichten, wie es scheint. |
 |3.588  |Norwegen ist nicht oft auf der globale Agenda - und die meisten Nachrichten, wie es scheint. |
 |5.116  |Norwegen ist nicht oft auf der globalen Agenda - und die meisten Nachrichten, wie es scheint. |
 |17.986  |Norwegen ist nicht oft auf der globalen Agenda - und die meisten , wie es scheint. |
 |19.386  |Norwegen ist nicht oft auf der globalen NachrichtenAgenda - und die meisten ,wie es scheint. |
 |20.116  |Norwegen ist nicht oft auf der globalen Nachrichtengenda - und die meisten ,wie es scheint. |
 |20.196  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten ,wie es scheint. |
 |20.298  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten ,wie es scheint. |
 |29.596  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheint. |
 |31.459  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten schein. |
 |31.659  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheine. |
 |31.796  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen. |
 |32.060  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen . |
 |34.283  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen d. |
 |34.380  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen da. |
 |34.443  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das. |
 |34.636  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das . |
 |35.507  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das s. |
 |35.637  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so. |
 |35.744  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so . |
 |35.949  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so z. |
 |36.053  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu. |
 |36.166  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu . |
 |36.805  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu w. |
 |38.011  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu wo. |
 |38.394  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu wol. |
 |38.699  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu woll. |
 |38.795  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu wolle. |
 |38.947  |Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu wollen. |
+ + +
 |char  |mmmmmmmmmmmmmmmmmmmmmmmmmmmmttmmmmmmmmtmppppppppppptmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmtttttttttttttttttttm |
 |word  |m m m m m x x x m m m m x t t t t m |
+ + +
</table>
<figureCaption confidence="0.8611498">
Figure 1: Analysis of the post-editing process: Most editing actions that result in changes in the transla-
tion are adding or deleting of a character by a keystroke. However, notice the cut (17.986 sec) and paste
(19.386 sec) action. Each character can be traced either to the original machine translation output (m), a
typing action of the post-editor (t), or a pasting action of the post-editor (p). This then allows tokens in
the output classified as either original MT (m), typed (t), pasted (not in figure) or partially edited (x).
</figureCaption>
<bodyText confidence="0.87854">
outlier here is Post-Editor 2, whose output has a
much larger edit rate.
</bodyText>
<sectionHeader confidence="0.952066" genericHeader="method">
6 Editing Actions
</sectionHeader>
<bodyText confidence="0.9996248">
The HTER is an analysis of the product of post-
editing. The final translation is compared to the
original machine translation output. In this sec-
tion, we examine how the process of post-editing
is influenced by the machine translation system.
Our post-editing workbench provides detailed log-
ging of each HCI interaction (key strokes, mouse
clicks, etc.). This allows us to reconstruct how a
translation was generated. See Figure 1 for an ex-
ample how a translated was edited.
</bodyText>
<tableCaption confidence="0.994162">
Table 6: Character provenance by system
</tableCaption>
<table confidence="0.9996402">
System MT typed pasted
ONLINE-B 68.3 28.0 3.3
UEDIN-PHRASE 62.9 31.3 5.2
UEDIN-SYNTAX 65.9 29.1 4.5
UU 56.1 37.9 5.6
</table>
<subsectionHeader confidence="0.966458">
6.1 Character Provenance by System
</subsectionHeader>
<bodyText confidence="0.999952">
If we follow the editing actions, we can trace the
origin of each letter in the final output: was it part
of the original MT output, was it typed in by the
user, or moved in a cut and paste action? Table 6
breaks down the characters in the final translations
</bodyText>
<page confidence="0.999899">
43
</page>
<tableCaption confidence="0.999573">
Table 7: Token provenance by system
</tableCaption>
<table confidence="0.9988486">
System MT typed pasted edited
ONLINE-B 65.2 21.4 2.3 10.8
UEDIN-PHRASE 60.5 24.7 3.9 10.6
UEDIN-SYNTAX 62.6 22.4 3.4 11.3
UU 53.2 31.0 4.0 11.7
</table>
<bodyText confidence="0.995634666666667">
by origin for each system. The numbers corre-
spond to the HTER scores, with a remarkable con-
sistency ranking for typed and pasted characters.
</bodyText>
<subsectionHeader confidence="0.999027">
6.2 Token Provenance by System
</subsectionHeader>
<bodyText confidence="0.999991866666667">
We perform a similar analysis on the word level,
introducing a fourth type of provenance: words
whose characters are of mixed origin, i.e., words
that were partially edited. Table 7 shows the num-
bers for each machine translation system. The sus-
picion from the HTER score that the syntax-based
system UEDIN-SYNTAX requires less movement is
not confirmed by these numbers. There are sig-
nificantly more words moved by pasting (3.4%)
than for ONLINE-B (2.3%). In general, cutting and
pasting is not as common as the HTER score would
suggest: the two types of shifts moved 10.3% and
10.2% of phrases, respectively. It seems that most
words that could be moved are rather deleted and
typed again.
</bodyText>
<subsectionHeader confidence="0.999716">
6.3 Behaviour By Post-Editor
</subsectionHeader>
<bodyText confidence="0.99987425">
The post-editors differ significantly in their be-
haviour, as the numbers in Table 8 illustrate. Post-
Editor 1, who is the fastest, leaves the most char-
acters unchanged (72.9% vs. 57.7–64.4% for the
others). Remarkably, this did not result in a dra-
matically lower HTER score (recall: 35.2 vs. 37.5–
43.1 for the others).
Post-Editor 3, while taking the longest time,
does not change the most number of characters.
However, (s)he uses dramatically more cutting and
pasting. Is this activity particularly slow? One
way to check is to examine more closely how the
</bodyText>
<tableCaption confidence="0.996435">
Table 8: Character provenance by post-editor
</tableCaption>
<table confidence="0.9993272">
Post-Editor MT typed pasted
1 72.9 22.9 3.5
2 57.7 39.4 2.7
3 58.9 29.5 10.7
4 64.4 33.5 1.9
</table>
<bodyText confidence="0.599809">
post-editors spread out their actions over time.
</bodyText>
<sectionHeader confidence="0.871352" genericHeader="method">
7 Editing Activities
</sectionHeader>
<bodyText confidence="0.999210333333333">
Koehn (2009) suggests to divide up the time spent
by translators and post-editors into intervals of the
following types:
</bodyText>
<listItem confidence="0.995267222222222">
• initial pauses: the pause at the beginning of
the translation, if it exists
• end pause: the pause at the end of the trans-
lation, if it exists
• short pause of length 2–6 seconds
• medium pauses of length 6–60 seconds
• big pauses longer than 60 seconds
• various working activities (in our case just
typing and mouse actions)
</listItem>
<bodyText confidence="0.999989482758621">
When we break up the time spent on each activ-
ity and normalise it by the number of words in
the original machine translation output, we get the
numbers in Table 9, per machine translation sys-
tem and post-editor.
The worse quality of the UU system causes
mainly more work activity, big medium pauses.
Each contributes roughly 0.3 seconds per word.
The syntax-based system UEDIN-SYNTAX may
pose fewer hard translation problems (showing up
in initial and big pauses) than the HTER-preferred
ONLINE-B system, but the effect is not strong.
We noted that ONLINE-B has a statistically sig-
nificant better HTER score than UEDIN-PHRASE.
While this is reflected in the additional working
activity for the latter (2.41 sec./word vs. 2.26
sec./word), time is made up in the pauses. Our data
is not sufficiently conclusive to gain any deeper in-
sight here — it is certainly a question that we want
to explore in the future.
The difference in post-editors mirrors some of
the earlier findings: The number of characters and
words changed leads to longer working activity,
but the slow Post-Editor 3 is mainly slowed down
by initial, big and medium pauses, indicating diffi-
culties with solving translation problems, and not
slow cutting and pasting actions. The faster Post-
Editor 1 rarely pauses long and is quick with typ-
ing and mouse movements.
</bodyText>
<sectionHeader confidence="0.998303" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998149666666667">
We compared how four different machine trans-
lation systems affect post-editing productivity and
behaviour by analysing final translations and user
</bodyText>
<page confidence="0.999531">
44
</page>
<tableCaption confidence="0.999405">
Table 9: Time spent on different activities, by machine translation system (top) and post-editor (bottom).
</tableCaption>
<table confidence="0.9989996">
System initial pause big pause med. pause short pause end pause working
ONLINE-B 0.37 s/w 0.61 s/w 1.88 s/w 0.30 s/w 0.00 s/w 2.26 s/w
UEDIN-PHRASE 0.32 s/w 0.55 s/w 1.74 s/w 0.32 s/w 0.00 s/w 2.41 s/w
UEDIN-SYNTAX 0.32 s/w 0.50 s/w 1.90 s/w 0.31 s/w 0.00 s/w 2.30 s/w
UU 0.28 s/w 0.74 s/w 2.14 s/w 0.34 s/w 0.00 s/w 2.75 s/w
Post-Editor initial pause big pause med. pause short pause end pause working
1 0.35 s/w 0.01 s/w 0.63 s/w 0.27 s/w 0.00 s/w 1.76 s/w
2 0.04 s/w 0.19 s/w 1.13 s/w 0.35 s/w 0.00 s/w 3.06 s/w
3 0.91 s/w 1.85 s/w 3.99 s/w 0.29 s/w 0.00 s/w 2.53 s/w
4 0.02 s/w 0.36 s/w 1.94 s/w 0.35 s/w 0.00 s/w 2.33 s/w
</table>
<bodyText confidence="0.999752565217391">
activity data. The best system under considera-
tion yielded abut 20% better productivity than the
worst, although the three systems on top are not
statistically significantly different in terms of pro-
ductivity.
We noted differences in metrics that measure
productivity and edit distance metrics. The lat-
ter allowed us to draw more statistically significant
conclusions, but may measure something distinct.
Productivity is the main concern of commercial
use of post-editing machine translation, and we
find that better machine translation leads to less
time spent on editing, but more importantly, less
time spent of figuring out harder translation prob-
lems (indicated by pauses of more than six sec-
onds).
Finally, an important finding is that the differ-
ences between post-editors is much larger than the
difference between machine translation systems.
This points towards the importance of skilled post-
editors, but this finding should be validated with
professional post-editors, and not the volunteers
used in this study.
</bodyText>
<sectionHeader confidence="0.994952" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999723">
This work was supported under the CASMACAT
project (grant agreement No 287576) by the
European Union 7th Framework Programme
(FP7/2007-2013).
</bodyText>
<sectionHeader confidence="0.998998" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999108854545455">
Alabau, Vicent, Ragnar Bonk, Christian Buck, Michael Carl,
Francisco Casacuberta, Mercedes Garcia-Martinez, Jes´us
Gonz´alez, Philipp Koehn, Luis Leiva, Bartolom´e Mesa-
Lao, Daniel Ortiz, Herve Saint-Amand, Germ´an San-
chis, and Chara Tsoukala. 2013. “CASMACAT: An open
source workbench for advanced computer aided transla-
tion.” The Prague Bulletin of Mathematical Linguistics,
100:101–112.
Bojar, Ondˇrej, Christian Buck, Chris Callison-Burch, Chris-
tian Federmann, Barry Haddow, Philipp Koehn, Christof
Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013.
“Findings of the 2013 Workshop on Statistical Machine
Translation.” Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, 1–44. Sofia, Bulgaria.
Callison-Burch, Chris, Philipp Koehn, Christof Monz, Matt
Post, Radu Soricut, and Lucia Specia. 2012. “Findings
of the 2012 workshop on statistical machine translation.”
Proceedings of the Seventh Workshop on Statistical Ma-
chine Translation, 10–48. Montreal, Canada.
Cettolo, Mauro, Jan Niehues, Sebastian St¨uker, Luisa Ben-
tivogli, and Marcello Federico. 2013. “Report on the
10th IWSLT evaluation campaign.” Proceedings of the
International Workshop on Spoken Language Translation
(IWSLT).
den Bogaert, Joachim Van and Nathalie De Sutter. 2013.
“Productivity or quality? Let’s do both.” Machine Trans-
lation Summit XIV, 381–390.
Durrani, Nadir, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. “Edinburgh’s machine translation
systems for European language pairs.” Proceedings of the
Eighth Workshop on Statistical Machine Translation, 114–
121. Sofia, Bulgaria.
Federico, Marcello, Alessandro Cattelan, and Marco Trom-
betti. 2012. “Measuring user productivity in machine
translation enhanced computer assisted translation.” Pro-
ceedings of the Tenth Conference of the Association for
Machine Translation in the Americas (AMTA).
Garcia, Ignacio. 2011. “Translating by post-editing: is it the
way forward?” Machine Translation, 25(3):217–237.
Green, Spence, Jeffrey Heer, and Christopher D. Manning.
2013. “The efficacy of human post-editing for language
translation.” ACM Human Factors in Computing Systems
(CHI).
Guerberof, Ana. 2009. “Productivity and quality in mt post-
editing.” MT Summit Workshop on New Tools for Transla-
tors.
Koehn, Philipp. 2009. “A process study of computer-aided
translation.” Machine Translation, 23(4):241–263.
Koehn, Philipp. 2010. “Enabling monolingual translators:
Post-editing vs. options.” Human Language Technolo-
gies: The 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguistics,
537–545. Los Angeles, California.
Koponen, Maarit. 2012. “Comparing human perceptions
of post-editing effort with post-editing operations.” Pro-
</reference>
<page confidence="0.984509">
45
</page>
<reference confidence="0.999449377358491">
ceedings of the Seventh Workshop on Statistical Machine
Translation, 227–236. Montreal, Canada.
Koponen, Maarit. 2013. “This translation is not too bad: an
analysis of post-editor choices in a machine-translation
post-editing task.” Proceedings of Workshop on Post-
editing Technology and Practice, 1–9.
Koponen, Maarit, Wilker Aziz, Luciana Ramos, and Lucia
Specia. 2012. “Post-editing time as a measure of cognitive
effort .” AMTA 2012 Workshop on Post-Editing Technol-
ogy and Practice (WPTP 2012), 11–20. San Diego, USA.
L¨aubli, Samuel, Mark Fishel, Gary Massey, Maureen
Ehrensberger-Dow, and Martin Volk. 2013. “Assessing
post-editing efficiency in a realistic translation environ-
ment.” Proceedings of Workshop on Post-editing Technol-
ogy and Practice, 83–91.
Nadejde, Maria, Philip Williams, and Philipp Koehn. 2013.
“Edinburgh’s syntax-based machine translation systems.”
Proceedings of the Eighth Workshop on Statistical Ma-
chine Translation, 170–176. Sofia, Bulgaria.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. BLEU: a Method for Automatic Evaluation of
Machine Translation. Tech. Rep. RC22176(W0109-022),
IBM Research Report.
Plitt, Mirko and Francois Masselot. 2010. “A productivity
test of statistical machine translation post-editing in a typi-
cal localisation context.” Prague Bulletin of Mathematical
Linguistics, 93:7–16.
Pouliquen, Bruno, Christophe Mazenc, and Aldo Iorio. 2011.
“Tapta: A user-driven translation system for patent docu-
ments based on domain-aware statistical machine trans-
lation.” Proceedings of th 15th International Confer-
ence of the European Association for Machine Translation
(EAMT), 5–12.
Skadin¸ˇs, Raivis, Maris Purin¸ˇs, Inguna Skadin¸a, and Andrejs
Vasil¸jevs. 2011. “Evaluation of SMT in localization to
under-resourced inflected language.” Proceedings of the
15th International Conference of the European Associa-
tion for Machine Translation (EAMT), 35–40.
Snover, Matthew, Bonnie J. Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. “A study of transla-
tion edit rate with targeted human annotation.” 5th Con-
ference of the Association for Machine Translation in the
Americas (AMTA). Boston, Massachusetts.
Stymne, Sara, Christian Hardmeier, J¨org Tiedemann, and
Joakim Nivre. 2013. “Tunable distortion limits and cor-
pus cleaning for SMT.” Proceedings of the Eighth Work-
shop on Statistical Machine Translation, 225–231. Sofia,
Bulgaria.
Vazquez, Lucia Morado, Silvia Rodriguez Vazquez, and Pier-
rette Bouillon. 2013. “Comparing forum data post-editing
performance using translation memory and machine trans-
lation output: A pilot study.” Machine Translation Summit
XIV, 249–256.
</reference>
<page confidence="0.999611">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.211804">
<title confidence="0.990049">The Impact of Machine Translation Quality on Human Post-editing</title>
<author confidence="0.354257">pkoehninf ed ac uk ugermanninf ed ac uk</author>
<affiliation confidence="0.273109">for Speech and Language Processing of Informatics The Johns Hopkins University University of Edinburgh</affiliation>
<abstract confidence="0.9989339">We investigate the effect of four different competitive machine translation systems on post-editor productivity and behaviour. The study involves four volunteers postediting automatic translations of news stories from English to German. We see significant difference in productivity due to the systems (about 20%), and even bigger variance between post-editors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Vicent Alabau</author>
<author>Ragnar Bonk</author>
<author>Christian Buck</author>
<author>Michael Carl</author>
<author>Francisco Casacuberta</author>
<author>Mercedes Garcia-Martinez</author>
<author>Jes´us Gonz´alez</author>
</authors>
<title>Sanchis, and Chara Tsoukala.</title>
<date>2013</date>
<pages>100--101</pages>
<location>Philipp Koehn, Luis Leiva, Bartolom´e MesaLao, Daniel Ortiz, Herve Saint-Amand, Germ´an</location>
<marker>Alabau, Bonk, Buck, Carl, Casacuberta, Garcia-Martinez, Gonz´alez, 2013</marker>
<rawString>Alabau, Vicent, Ragnar Bonk, Christian Buck, Michael Carl, Francisco Casacuberta, Mercedes Garcia-Martinez, Jes´us Gonz´alez, Philipp Koehn, Luis Leiva, Bartolom´e MesaLao, Daniel Ortiz, Herve Saint-Amand, Germ´an Sanchis, and Chara Tsoukala. 2013. “CASMACAT: An open source workbench for advanced computer aided translation.” The Prague Bulletin of Mathematical Linguistics, 100:101–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation.” Proceedings of the Eighth Workshop on Statistical Machine Translation, 1–44.</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1327" citStr="Bojar et al., 2013" startWordPosition="190" endWordPosition="193">ess over the past two decades. Numerous recent studies have shown productivity increases with post-editing of MT output over traditional work practices in human translation (e.g., Guerberof, 2009; Plitt and Masselot, 2010; Garcia, 2011; Pouliquen et al., 2011; Skadin¸ˇs et al., 2011; den Bogaert and Sutter, 2013; Vazquez et al., 2013; Green et al., 2013; L¨aubli et al., 2013). The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series (Bojar et al., 2013) and IWSLT (Cettolo et al., 2013), and the NIST Open MT Evaluation.1 These evaluations usually apply a mix of automatic evaluation metrics, most prominently the BLEU score (Papineni et al., 2001), and more subjective human evaluation criteria such as correctness, accuracy, and fluency. How the quality increases measured by automatic metrics and subjective evaluation criteria relate to actual increases in the productivity of posteditors is still an open research question. It is also not clear yet if some machine translation approaches — say, syntax-based models — are better suited for post-edit</context>
<context position="5865" citStr="Bojar et al., 2013" startWordPosition="894" endWordPosition="897">ing an open source translation and post-editing workbench (Federico et al., 2012; Alabau et al., 2013). At this point, we are not aware of any study that compares directly the impact of different machine translation systems on post-editor productivity and behaviour. 3 Experimental Design We thus carried out an experiment on an English– German news translation task, using output from four different SMT systems, post-edited by fluent bilingual native speakers of German with no prior experience in professional translation. 3.1 The Translation Task The Workshop on Statistical Machine Translation (Bojar et al., 2013) organises an annual evaluation campaign for machine translation systems. The subject matter is translation of news stories from sources such as the New York Times or the BBC. We decided to use output from systems submitted to this evaluation campaign, not only because 3http://www.matecat.com/ 4http://www.casmacat.eu/ their output is freely available,5 but also because it comes with automatic metric scores and human judgements of the translation quality. The translation direction we chose was English–German, partly due to convenience (the authors of this study are fluent in both languages), bu</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Bojar, Ondˇrej, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. “Findings of the 2013 Workshop on Statistical Machine Translation.” Proceedings of the Eighth Workshop on Statistical Machine Translation, 1–44. Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 workshop on statistical machine translation.” Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--48</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="9417" citStr="Callison-Burch et al., 2012" startWordPosition="1452" endWordPosition="1455">the same language model. System UU is also a phrase based system, with a decoder that is able to consider the document level context. It was trained on smaller corpora for both the translation model and the language model. We do not have any insight into the system ONLINE-B, but we conjecture that it is a phrasebased system with syntactic pre-reordering trained on much larger data sets, but not optimised towards the news domain. Notice the inconsistency between BLEU score and subjective score for the two systems from the University of Edinburgh. Results from other evaluations have also shown (Callison-Burch et al., 2012) that current automatic evaluation metrics do not as much as human judges appreciate the strengths of the syntax-based system, which builds syntactic structures in the target language during translation. Hence, we were particularly interested how the syntax-based system fares with post-editors. As mentioned above, the nine documents chosen for the post-editing task analysed in this paper (cf. Table 1) were part of the WMT 2013 evaluation data set. All nine documents had English as the original source language. 3.3 Post-Editors We recruited four English-German bilingual, native German post-edit</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Callison-Burch, Chris, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. “Findings of the 2012 workshop on statistical machine translation.” Proceedings of the Seventh Workshop on Statistical Machine Translation, 10–48. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Jan Niehues</author>
<author>Sebastian St¨uker</author>
<author>Luisa Bentivogli</author>
<author>Marcello Federico</author>
</authors>
<date>2013</date>
<booktitle>Report on the 10th IWSLT evaluation campaign.” Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<marker>Cettolo, Niehues, St¨uker, Bentivogli, Federico, 2013</marker>
<rawString>Cettolo, Mauro, Jan Niehues, Sebastian St¨uker, Luisa Bentivogli, and Marcello Federico. 2013. “Report on the 10th IWSLT evaluation campaign.” Proceedings of the International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Van and Nathalie De Sutter den Bogaert</author>
</authors>
<title>Productivity or quality?</title>
<date>2013</date>
<booktitle>Let’s do both.” Machine Translation Summit XIV,</booktitle>
<pages>381--390</pages>
<marker>den Bogaert, 2013</marker>
<rawString>den Bogaert, Joachim Van and Nathalie De Sutter. 2013. “Productivity or quality? Let’s do both.” Machine Translation Summit XIV, 381–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Barry Haddow</author>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s machine translation systems for European language pairs.”</title>
<date>2013</date>
<booktitle>Proceedings of the Eighth Workshop on Statistical Machine Translation, 114– 121.</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="7377" citStr="Durrani et al., 2013" startWordPosition="1123" endWordPosition="1126">s, originally written in English, with a total of 500 sentences. Details are shown in Table 1. 3.2 Machine Translation Systems A total of 15 different machine translation systems participated in the evaluation campaign. We selected four different systems that differ in their architecture and use of training data: • an anonymized popular online translation system built by a large Internet company (ONLINE-B) • the syntax-based translation system of the University of Edinburgh (UEDIN-SYNTAX; Nadejde et al., 2013) • the phrase-based translation system of the University of Edinburgh (UEDIN-PHRASE; Durrani et al., 2013) • the machine translation system of the University of Uppsala (UU; Stymne et al., 2013) In the 2013 WMT evaluation campaign, the systems translated a total of 3000 sentences, and their 5 http://www.statmt.org/wmt13/results.html 39 Table 2: Machine translation systems used in the study, with quality scores in the WMT 2013 evaluation campaign. System BLEU SUBJECTIVE ONLINE-B 20.7 0.637 UEDIN-SYNTAX 19.4 0.614 UEDIN-PHRASE 20.1 0.571 UU 16.1 0.361 output was judged with the BLEU score against a professional reference translation and by subjective ranking. The scores obtained for the different sy</context>
</contexts>
<marker>Durrani, Haddow, Heafield, Koehn, 2013</marker>
<rawString>Durrani, Nadir, Barry Haddow, Kenneth Heafield, and Philipp Koehn. 2013. “Edinburgh’s machine translation systems for European language pairs.” Proceedings of the Eighth Workshop on Statistical Machine Translation, 114– 121. Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Alessandro Cattelan</author>
<author>Marco Trombetti</author>
</authors>
<title>Measuring user productivity in machine translation enhanced computer assisted translation.”</title>
<date>2012</date>
<booktitle>Proceedings of the Tenth Conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<contexts>
<context position="5326" citStr="Federico et al., 2012" startWordPosition="813" endWordPosition="816">r. They also compare post-editing styles of different post-editors working on identical post-editing tasks. Another study by Koponen (2013) showed that inter-translator variance is lower in a controlled language setting when translators are given the choice of output from three different machine translation systems. In the realm of machine translation research, there has been an increasing interest in the use of MT technology by post-editors. A major push are the two EU-funded research projects MATECAT3 and CASMACAT4, which are developing an open source translation and post-editing workbench (Federico et al., 2012; Alabau et al., 2013). At this point, we are not aware of any study that compares directly the impact of different machine translation systems on post-editor productivity and behaviour. 3 Experimental Design We thus carried out an experiment on an English– German news translation task, using output from four different SMT systems, post-edited by fluent bilingual native speakers of German with no prior experience in professional translation. 3.1 The Translation Task The Workshop on Statistical Machine Translation (Bojar et al., 2013) organises an annual evaluation campaign for machine translat</context>
</contexts>
<marker>Federico, Cattelan, Trombetti, 2012</marker>
<rawString>Federico, Marcello, Alessandro Cattelan, and Marco Trombetti. 2012. “Measuring user productivity in machine translation enhanced computer assisted translation.” Proceedings of the Tenth Conference of the Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ignacio Garcia</author>
</authors>
<title>Translating by post-editing: is it the way forward?”</title>
<date>2011</date>
<journal>Machine Translation,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="943" citStr="Garcia, 2011" startWordPosition="126" endWordPosition="127">ranslation systems on post-editor productivity and behaviour. The study involves four volunteers postediting automatic translations of news stories from English to German. We see significant difference in productivity due to the systems (about 20%), and even bigger variance between post-editors. 1 Introduction Statistical machine translation (SMT) has made considerable progress over the past two decades. Numerous recent studies have shown productivity increases with post-editing of MT output over traditional work practices in human translation (e.g., Guerberof, 2009; Plitt and Masselot, 2010; Garcia, 2011; Pouliquen et al., 2011; Skadin¸ˇs et al., 2011; den Bogaert and Sutter, 2013; Vazquez et al., 2013; Green et al., 2013; L¨aubli et al., 2013). The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series (Bojar et al., 2013) and IWSLT (Cettolo et al., 2013), and the NIST Open MT Evaluation.1 These evaluations usually apply a mix of automatic evaluation metrics, most prominently the BLEU score (Papineni et al., 2001), and more subjective</context>
</contexts>
<marker>Garcia, 2011</marker>
<rawString>Garcia, Ignacio. 2011. “Translating by post-editing: is it the way forward?” Machine Translation, 25(3):217–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Jeffrey Heer</author>
<author>Christopher D Manning</author>
</authors>
<title>The efficacy of human post-editing for language translation.”</title>
<date>2013</date>
<journal>ACM Human Factors in Computing Systems (CHI).</journal>
<contexts>
<context position="1063" citStr="Green et al., 2013" startWordPosition="145" endWordPosition="148">atic translations of news stories from English to German. We see significant difference in productivity due to the systems (about 20%), and even bigger variance between post-editors. 1 Introduction Statistical machine translation (SMT) has made considerable progress over the past two decades. Numerous recent studies have shown productivity increases with post-editing of MT output over traditional work practices in human translation (e.g., Guerberof, 2009; Plitt and Masselot, 2010; Garcia, 2011; Pouliquen et al., 2011; Skadin¸ˇs et al., 2011; den Bogaert and Sutter, 2013; Vazquez et al., 2013; Green et al., 2013; L¨aubli et al., 2013). The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series (Bojar et al., 2013) and IWSLT (Cettolo et al., 2013), and the NIST Open MT Evaluation.1 These evaluations usually apply a mix of automatic evaluation metrics, most prominently the BLEU score (Papineni et al., 2001), and more subjective human evaluation criteria such as correctness, accuracy, and fluency. How the quality increases measured by automatic m</context>
</contexts>
<marker>Green, Heer, Manning, 2013</marker>
<rawString>Green, Spence, Jeffrey Heer, and Christopher D. Manning. 2013. “The efficacy of human post-editing for language translation.” ACM Human Factors in Computing Systems (CHI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana Guerberof</author>
</authors>
<title>Productivity and quality in mt postediting.” MT Summit Workshop on New Tools for Translators.</title>
<date>2009</date>
<contexts>
<context position="903" citStr="Guerberof, 2009" startWordPosition="120" endWordPosition="121">ect of four different competitive machine translation systems on post-editor productivity and behaviour. The study involves four volunteers postediting automatic translations of news stories from English to German. We see significant difference in productivity due to the systems (about 20%), and even bigger variance between post-editors. 1 Introduction Statistical machine translation (SMT) has made considerable progress over the past two decades. Numerous recent studies have shown productivity increases with post-editing of MT output over traditional work practices in human translation (e.g., Guerberof, 2009; Plitt and Masselot, 2010; Garcia, 2011; Pouliquen et al., 2011; Skadin¸ˇs et al., 2011; den Bogaert and Sutter, 2013; Vazquez et al., 2013; Green et al., 2013; L¨aubli et al., 2013). The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series (Bojar et al., 2013) and IWSLT (Cettolo et al., 2013), and the NIST Open MT Evaluation.1 These evaluations usually apply a mix of automatic evaluation metrics, most prominently the BLEU score (Pap</context>
</contexts>
<marker>Guerberof, 2009</marker>
<rawString>Guerberof, Ana. 2009. “Productivity and quality in mt postediting.” MT Summit Workshop on New Tools for Translators.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>A process study of computer-aided translation.”</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="26540" citStr="Koehn (2009)" startWordPosition="4329" endWordPosition="4330">(72.9% vs. 57.7–64.4% for the others). Remarkably, this did not result in a dramatically lower HTER score (recall: 35.2 vs. 37.5– 43.1 for the others). Post-Editor 3, while taking the longest time, does not change the most number of characters. However, (s)he uses dramatically more cutting and pasting. Is this activity particularly slow? One way to check is to examine more closely how the Table 8: Character provenance by post-editor Post-Editor MT typed pasted 1 72.9 22.9 3.5 2 57.7 39.4 2.7 3 58.9 29.5 10.7 4 64.4 33.5 1.9 post-editors spread out their actions over time. 7 Editing Activities Koehn (2009) suggests to divide up the time spent by translators and post-editors into intervals of the following types: • initial pauses: the pause at the beginning of the translation, if it exists • end pause: the pause at the end of the translation, if it exists • short pause of length 2–6 seconds • medium pauses of length 6–60 seconds • big pauses longer than 60 seconds • various working activities (in our case just typing and mouse actions) When we break up the time spent on each activity and normalise it by the number of words in the original machine translation output, we get the numbers in Table 9</context>
</contexts>
<marker>Koehn, 2009</marker>
<rawString>Koehn, Philipp. 2009. “A process study of computer-aided translation.” Machine Translation, 23(4):241–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Enabling monolingual translators: Post-editing vs. options.” Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>537--545</pages>
<location>Los Angeles, California.</location>
<contexts>
<context position="12565" citStr="Koehn, 2010" startWordPosition="1969" endWordPosition="1970">,184 719 390 766 661 UEDIN-SYNTAX 3.03 4.41 9.20 4.97 5.38 1,188 816 391 724 669 UU 3.11 5.01 11.59 5.58 6.35 1,158 719 311 645 567 mean per editor 3.03 4.78 9.79 5.05 1,188 753 368 713 4 Productivity The primary argument for post-editing machine translation output as opposed to more traditional approaches is the potential gain in productivity. If translation professionals can work faster with machine translation, then this has real economic benefits. There are also other considerations, for example that post-editing might be done by professionals that are less skilled in the source language (Koehn, 2010). We measure productivity by time spent on each sentence. This is not a perfect measure. When working on a news story, post-editors tend to speed up when moving down the story since they have already solved some reoccurring translation problems and get more familiar with the context. 4.1 Productivity by MT System Our main interests is the average translation speed, broken down by machine translation system. The columns labelled “mean” in Table 3 show the results. While the differences are not big for the top three systems, the syntax-based system comes out on top. We used bootstrap resampling </context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Koehn, Philipp. 2010. “Enabling monolingual translators: Post-editing vs. options.” Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 537–545. Los Angeles, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarit Koponen</author>
</authors>
<title>Comparing human perceptions of post-editing effort with post-editing operations.”</title>
<date>2012</date>
<booktitle>Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>227--236</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="2864" citStr="Koponen (2012)" startWordPosition="434" endWordPosition="435">MT system on post-editing effort and efficiency. The study focuses on translation of general news text from English into German, with translations created by non-professional post-editors working on output from four different translation systems. The data generated by this study is available for download.2 We find that the better systems lead to a productivity gain of roughly 20% and carry out in-depth analysis of editing behavior. A significant finding is the high variance in work styles between the different post-editors, compared to the impact of machine translation systems. 2 Related Work Koponen (2012) examined the relationship between human assessment of post-editing efforts and objective measures such as post-editing time and number of edit operations. She found that segments that require a lot of reordering are perceived as being more difficult, and that long sentences are considered harder, even if only few words changed. She also reports larger variance between translators in post-editing time than in post-editing operations — a finding that we confirm here as well. From a detailed analysis of the types of edits performed in sentences with long versus short post-edit times, Koponen et </context>
</contexts>
<marker>Koponen, 2012</marker>
<rawString>Koponen, Maarit. 2012. “Comparing human perceptions of post-editing effort with post-editing operations.” Proceedings of the Seventh Workshop on Statistical Machine Translation, 227–236. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarit Koponen</author>
</authors>
<title>This translation is not too bad: an analysis of post-editor choices in a machine-translation post-editing task.”</title>
<date>2013</date>
<booktitle>Proceedings of Workshop on Postediting Technology and Practice,</booktitle>
<volume>1</volume>
<contexts>
<context position="4844" citStr="Koponen (2013)" startWordPosition="741" endWordPosition="742">s tough start CNN 45 Bradley Manning didn’t complain about mistreatment, prosecutors contend CNN 63 My Mexican-American identity crisis Economist 55 Old battles, new Middle East Guardian 38 Cigarette plain packaging laws come into force in Australia NY Times 61 In a Constantly Plugged-In World, It’s Not All Bad to Be Bored NY Times 47 In Colorado, No Playbook for New Marijuana Law Telegraph 95 Petronella Wyatt: I was bullied out of Oxford for being a Tory and takes longer. They also compare post-editing styles of different post-editors working on identical post-editing tasks. Another study by Koponen (2013) showed that inter-translator variance is lower in a controlled language setting when translators are given the choice of output from three different machine translation systems. In the realm of machine translation research, there has been an increasing interest in the use of MT technology by post-editors. A major push are the two EU-funded research projects MATECAT3 and CASMACAT4, which are developing an open source translation and post-editing workbench (Federico et al., 2012; Alabau et al., 2013). At this point, we are not aware of any study that compares directly the impact of different ma</context>
</contexts>
<marker>Koponen, 2013</marker>
<rawString>Koponen, Maarit. 2013. “This translation is not too bad: an analysis of post-editor choices in a machine-translation post-editing task.” Proceedings of Workshop on Postediting Technology and Practice, 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarit Koponen</author>
<author>Wilker Aziz</author>
<author>Luciana Ramos</author>
<author>Lucia Specia</author>
</authors>
<title>Post-editing time as a measure of cognitive effort</title>
<date>2012</date>
<booktitle>AMTA 2012 Workshop on Post-Editing Technology and Practice (WPTP 2012),</booktitle>
<pages>11--20</pages>
<location>San Diego, USA.</location>
<contexts>
<context position="3474" citStr="Koponen et al. (2012)" startWordPosition="530" endWordPosition="533">onen (2012) examined the relationship between human assessment of post-editing efforts and objective measures such as post-editing time and number of edit operations. She found that segments that require a lot of reordering are perceived as being more difficult, and that long sentences are considered harder, even if only few words changed. She also reports larger variance between translators in post-editing time than in post-editing operations — a finding that we confirm here as well. From a detailed analysis of the types of edits performed in sentences with long versus short post-edit times, Koponen et al. (2012) conclude that the observed differences in edit times can be explained at least in part also by the types of necessary edits and the associated cognitive effort. Deleting superfluous function words, for example, appears to be cognitively simple and takes little time, whereas inserting translations for untranslated words requires more cognitive effort 2 http://www.casmacat.eu/index.php?n=Main.Downloads 38 Workshop on Humans and Computer-assisted Translation, pages 38–46, Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics Table 1: News stories used in the study (</context>
</contexts>
<marker>Koponen, Aziz, Ramos, Specia, 2012</marker>
<rawString>Koponen, Maarit, Wilker Aziz, Luciana Ramos, and Lucia Specia. 2012. “Post-editing time as a measure of cognitive effort .” AMTA 2012 Workshop on Post-Editing Technology and Practice (WPTP 2012), 11–20. San Diego, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel L¨aubli</author>
<author>Mark Fishel</author>
<author>Gary Massey</author>
<author>Maureen Ehrensberger-Dow</author>
<author>Martin Volk</author>
</authors>
<title>Assessing post-editing efficiency in a realistic translation environment.”</title>
<date>2013</date>
<booktitle>Proceedings of Workshop on Post-editing Technology and Practice,</booktitle>
<pages>83--91</pages>
<marker>L¨aubli, Fishel, Massey, Ehrensberger-Dow, Volk, 2013</marker>
<rawString>L¨aubli, Samuel, Mark Fishel, Gary Massey, Maureen Ehrensberger-Dow, and Martin Volk. 2013. “Assessing post-editing efficiency in a realistic translation environment.” Proceedings of Workshop on Post-editing Technology and Practice, 83–91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Nadejde</author>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s syntax-based machine translation systems.”</title>
<date>2013</date>
<booktitle>Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>170--176</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="7271" citStr="Nadejde et al., 2013" startWordPosition="1108" endWordPosition="1111">t recent evaluation campaign. The subset chosen for our postediting task comprises 9 different news stories, originally written in English, with a total of 500 sentences. Details are shown in Table 1. 3.2 Machine Translation Systems A total of 15 different machine translation systems participated in the evaluation campaign. We selected four different systems that differ in their architecture and use of training data: • an anonymized popular online translation system built by a large Internet company (ONLINE-B) • the syntax-based translation system of the University of Edinburgh (UEDIN-SYNTAX; Nadejde et al., 2013) • the phrase-based translation system of the University of Edinburgh (UEDIN-PHRASE; Durrani et al., 2013) • the machine translation system of the University of Uppsala (UU; Stymne et al., 2013) In the 2013 WMT evaluation campaign, the systems translated a total of 3000 sentences, and their 5 http://www.statmt.org/wmt13/results.html 39 Table 2: Machine translation systems used in the study, with quality scores in the WMT 2013 evaluation campaign. System BLEU SUBJECTIVE ONLINE-B 20.7 0.637 UEDIN-SYNTAX 19.4 0.614 UEDIN-PHRASE 20.1 0.571 UU 16.1 0.361 output was judged with the BLEU score agains</context>
</contexts>
<marker>Nadejde, Williams, Koehn, 2013</marker>
<rawString>Nadejde, Maria, Philip Williams, and Philipp Koehn. 2013. “Edinburgh’s syntax-based machine translation systems.” Proceedings of the Eighth Workshop on Statistical Machine Translation, 170–176. Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<tech>Tech. Rep. RC22176(W0109-022), IBM Research Report.</tech>
<contexts>
<context position="1522" citStr="Papineni et al., 2001" startWordPosition="222" endWordPosition="225">009; Plitt and Masselot, 2010; Garcia, 2011; Pouliquen et al., 2011; Skadin¸ˇs et al., 2011; den Bogaert and Sutter, 2013; Vazquez et al., 2013; Green et al., 2013; L¨aubli et al., 2013). The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series (Bojar et al., 2013) and IWSLT (Cettolo et al., 2013), and the NIST Open MT Evaluation.1 These evaluations usually apply a mix of automatic evaluation metrics, most prominently the BLEU score (Papineni et al., 2001), and more subjective human evaluation criteria such as correctness, accuracy, and fluency. How the quality increases measured by automatic metrics and subjective evaluation criteria relate to actual increases in the productivity of posteditors is still an open research question. It is also not clear yet if some machine translation approaches — say, syntax-based models — are better suited for post-editing than others. These relationships may very well also depend on the lan1 http://www.nist.gov/itl/iad/mig/openmt.cfm guage pair in question and the coarse level of MT quality, from barely good e</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. BLEU: a Method for Automatic Evaluation of Machine Translation. Tech. Rep. RC22176(W0109-022), IBM Research Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirko Plitt</author>
<author>Francois Masselot</author>
</authors>
<title>A productivity test of statistical machine translation post-editing in a typical localisation context.”</title>
<date>2010</date>
<booktitle>Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>93--7</pages>
<contexts>
<context position="929" citStr="Plitt and Masselot, 2010" startWordPosition="122" endWordPosition="125">rent competitive machine translation systems on post-editor productivity and behaviour. The study involves four volunteers postediting automatic translations of news stories from English to German. We see significant difference in productivity due to the systems (about 20%), and even bigger variance between post-editors. 1 Introduction Statistical machine translation (SMT) has made considerable progress over the past two decades. Numerous recent studies have shown productivity increases with post-editing of MT output over traditional work practices in human translation (e.g., Guerberof, 2009; Plitt and Masselot, 2010; Garcia, 2011; Pouliquen et al., 2011; Skadin¸ˇs et al., 2011; den Bogaert and Sutter, 2013; Vazquez et al., 2013; Green et al., 2013; L¨aubli et al., 2013). The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series (Bojar et al., 2013) and IWSLT (Cettolo et al., 2013), and the NIST Open MT Evaluation.1 These evaluations usually apply a mix of automatic evaluation metrics, most prominently the BLEU score (Papineni et al., 2001), and m</context>
</contexts>
<marker>Plitt, Masselot, 2010</marker>
<rawString>Plitt, Mirko and Francois Masselot. 2010. “A productivity test of statistical machine translation post-editing in a typical localisation context.” Prague Bulletin of Mathematical Linguistics, 93:7–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Pouliquen</author>
<author>Christophe Mazenc</author>
<author>Aldo Iorio</author>
</authors>
<title>Tapta: A user-driven translation system for patent documents based on domain-aware statistical machine translation.”</title>
<date>2011</date>
<booktitle>Proceedings of th 15th International Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>5--12</pages>
<contexts>
<context position="967" citStr="Pouliquen et al., 2011" startWordPosition="128" endWordPosition="131">tems on post-editor productivity and behaviour. The study involves four volunteers postediting automatic translations of news stories from English to German. We see significant difference in productivity due to the systems (about 20%), and even bigger variance between post-editors. 1 Introduction Statistical machine translation (SMT) has made considerable progress over the past two decades. Numerous recent studies have shown productivity increases with post-editing of MT output over traditional work practices in human translation (e.g., Guerberof, 2009; Plitt and Masselot, 2010; Garcia, 2011; Pouliquen et al., 2011; Skadin¸ˇs et al., 2011; den Bogaert and Sutter, 2013; Vazquez et al., 2013; Green et al., 2013; L¨aubli et al., 2013). The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series (Bojar et al., 2013) and IWSLT (Cettolo et al., 2013), and the NIST Open MT Evaluation.1 These evaluations usually apply a mix of automatic evaluation metrics, most prominently the BLEU score (Papineni et al., 2001), and more subjective human evaluation criter</context>
</contexts>
<marker>Pouliquen, Mazenc, Iorio, 2011</marker>
<rawString>Pouliquen, Bruno, Christophe Mazenc, and Aldo Iorio. 2011. “Tapta: A user-driven translation system for patent documents based on domain-aware statistical machine translation.” Proceedings of th 15th International Conference of the European Association for Machine Translation (EAMT), 5–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raivis Skadin¸ˇs</author>
<author>Maris Purin¸ˇs</author>
</authors>
<title>Inguna Skadin¸a, and Andrejs Vasil¸jevs.</title>
<date>2011</date>
<booktitle>Proceedings of the 15th International Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>35--40</pages>
<marker>Skadin¸ˇs, Purin¸ˇs, 2011</marker>
<rawString>Skadin¸ˇs, Raivis, Maris Purin¸ˇs, Inguna Skadin¸a, and Andrejs Vasil¸jevs. 2011. “Evaluation of SMT in localization to under-resourced inflected language.” Proceedings of the 15th International Conference of the European Association for Machine Translation (EAMT), 35–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie J Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human</title>
<date>2006</date>
<booktitle>annotation.” 5th Conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="16789" citStr="Snover et al., 2006" startWordPosition="2682" endWordPosition="2685"> the original machine translation output. This is what we will do in this section. In Section 6, we will consider which parts of the final translation were actually changed by the post-editor and discuss the difference. 5.1 HTER as Quality Measure The edit distance between machine translation output and human reference translation can be measured in the number of insertions, deletions, substitutions and (phrasal) moves. A metric that simply counts the minimal number of such edit operations and divides it by the length of the human reference translation is the translation edit rate, short TER (Snover et al., 2006). If the human reference translation is created from the machine translation output to minimise the number of edit operations needed for an acceptable translation, this variant is called humanmediated TER, or HTER. Note that in our experiment the post-editors are not strictly trying to minimise the number of edit operations — they may be inclined to make additional changes due to arbitrary considerations of style or perform edits that are faster rather than minimise the number of operations (e.g., deleting whole passages and rewriting them). 5.2 Edits by MT System Table 4 shows the HTER scores</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Snover, Matthew, Bonnie J. Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. “A study of translation edit rate with targeted human annotation.” 5th Conference of the Association for Machine Translation in the Americas (AMTA). Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Christian Hardmeier</author>
<author>J¨org Tiedemann</author>
<author>Joakim Nivre</author>
</authors>
<title>Tunable distortion limits and corpus cleaning for SMT.”</title>
<date>2013</date>
<booktitle>Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>225--231</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="7465" citStr="Stymne et al., 2013" startWordPosition="1139" endWordPosition="1142">le 1. 3.2 Machine Translation Systems A total of 15 different machine translation systems participated in the evaluation campaign. We selected four different systems that differ in their architecture and use of training data: • an anonymized popular online translation system built by a large Internet company (ONLINE-B) • the syntax-based translation system of the University of Edinburgh (UEDIN-SYNTAX; Nadejde et al., 2013) • the phrase-based translation system of the University of Edinburgh (UEDIN-PHRASE; Durrani et al., 2013) • the machine translation system of the University of Uppsala (UU; Stymne et al., 2013) In the 2013 WMT evaluation campaign, the systems translated a total of 3000 sentences, and their 5 http://www.statmt.org/wmt13/results.html 39 Table 2: Machine translation systems used in the study, with quality scores in the WMT 2013 evaluation campaign. System BLEU SUBJECTIVE ONLINE-B 20.7 0.637 UEDIN-SYNTAX 19.4 0.614 UEDIN-PHRASE 20.1 0.571 UU 16.1 0.361 output was judged with the BLEU score against a professional reference translation and by subjective ranking. The scores obtained for the different systems on the full test set are shown in Table 2. The first three systems are fairly clos</context>
</contexts>
<marker>Stymne, Hardmeier, Tiedemann, Nivre, 2013</marker>
<rawString>Stymne, Sara, Christian Hardmeier, J¨org Tiedemann, and Joakim Nivre. 2013. “Tunable distortion limits and corpus cleaning for SMT.” Proceedings of the Eighth Workshop on Statistical Machine Translation, 225–231. Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Morado Vazquez</author>
<author>Silvia Rodriguez Vazquez</author>
<author>Pierrette Bouillon</author>
</authors>
<title>Comparing forum data post-editing performance using translation memory and machine translation output: A pilot study.”</title>
<date>2013</date>
<booktitle>Machine Translation Summit XIV,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="1043" citStr="Vazquez et al., 2013" startWordPosition="141" endWordPosition="144">eers postediting automatic translations of news stories from English to German. We see significant difference in productivity due to the systems (about 20%), and even bigger variance between post-editors. 1 Introduction Statistical machine translation (SMT) has made considerable progress over the past two decades. Numerous recent studies have shown productivity increases with post-editing of MT output over traditional work practices in human translation (e.g., Guerberof, 2009; Plitt and Masselot, 2010; Garcia, 2011; Pouliquen et al., 2011; Skadin¸ˇs et al., 2011; den Bogaert and Sutter, 2013; Vazquez et al., 2013; Green et al., 2013; L¨aubli et al., 2013). The advances in statistical machine translation over the past years have been driven to a large extent by frequent (friendly) competitive MT evaluation campaigns, such as the shared tasks at the ACL WMT workshop series (Bojar et al., 2013) and IWSLT (Cettolo et al., 2013), and the NIST Open MT Evaluation.1 These evaluations usually apply a mix of automatic evaluation metrics, most prominently the BLEU score (Papineni et al., 2001), and more subjective human evaluation criteria such as correctness, accuracy, and fluency. How the quality increases mea</context>
</contexts>
<marker>Vazquez, Vazquez, Bouillon, 2013</marker>
<rawString>Vazquez, Lucia Morado, Silvia Rodriguez Vazquez, and Pierrette Bouillon. 2013. “Comparing forum data post-editing performance using translation memory and machine translation output: A pilot study.” Machine Translation Summit XIV, 249–256.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
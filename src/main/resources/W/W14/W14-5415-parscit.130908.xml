<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.227218">
<title confidence="0.985928">
Expression Recognition by Using Facial And Vocal Expressions
</title>
<author confidence="0.7615045">
Gholamreza Anbarjafari Alvo Aabloo
IMS Lab IMS Lab
</author>
<affiliation confidence="0.970147666666667">
Institute of Technology Institute of Technology
University of Tartu University of Tartu
Tartu, Estonia Tartu, Estonia
</affiliation>
<email confidence="0.984063">
sjafari@ut.ee alvo.aabloo@ut.ee
</email>
<sectionHeader confidence="0.993539" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913222222222">
Human behaviour may be monitored by analysing facial expressions and vocal expressions.
Hence an automatic technique which combines both these features will give a more accurate
overall estimation of expression. In this work we propose a new method which is uses facial
and vocal features to estimate the expression of the subject. Facial expressions are analysed
by extracting important facial features and then clustering the movement of these features. In
parallel the voice is processed by using considering sudden changes in amplitude and fre-
quency in order to recognize the expression. Finally a weighted sum rule is used to combine
the decisions obtained by facial and vocal expression recognition. The proposed technique is
tested on an ongoing set of real data monitored by a psychologist.
</bodyText>
<sectionHeader confidence="0.997646" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999860352941177">
Human-computer interaction has been centre of interest of many researchers for a number of years [1-
3]. In order to facilitate this interaction it is essential for the computer system, for example a robot, to
understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using
facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model
(HMM) [9], and translation of belief model [10] have been frequently used for implementation of
facial expression recognition.
Vocal expression recognition has also been used separately for expression recognition [11, 12]. The
combination of both facial and vocal expression is novel and many researchers are investigating an
intelligent model for this fusion [13]. One of the main issues with these techniques is the application of
HMM as the hidden states are usually unknown and need to be estimated based on assumed
probability distributions of the data [14]. In this work we are proposing a new expression recognition
tech- nique which is using both facial and vocal expressions in order to estimate the expression of the
speaker. The proposed technique is benefiting from the facial and vocal features. The proposed
technique is evaluated with many volunteers sitting in front of a camera under controlled illumination
reading texts with expressions of happiness, sadness, disgust, surprise, anger, fear, and contempt. The
initial experimental results are promising showing that the proposed technique out performs other
current techniques.
</bodyText>
<sectionHeader confidence="0.948659" genericHeader="method">
2 The Proposed Expression Recognition Technique
</sectionHeader>
<bodyText confidence="0.9999578">
In the proposed technique two parallel observations are made. Firstly the facial features are detected
using the Viola-Jones face detector. The important features are corners of lips, upper and lower part of
lips, corners and centre of eyes, nose, nose tip, corners of eyebrows, and chin. Then movements of
these facial features will be used in order to recognize each of the seven basic emotions. Fig. 1 is
showing the important features on the facial image which are used for expression recognition.
</bodyText>
<footnote confidence="0.9634905">
This work is licensed under a Creative Commons Attribution 4.0 International Licence. License details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<page confidence="0.936525">
103
</page>
<note confidence="0.9619135">
Proceedings of the 25th International Conference on Computational Linguistics, pages 103–105,
Dublin, Ireland, August 23-29 2014.
</note>
<figureCaption confidence="0.994777">
Fig. 1: 20 important facial features which are being used for facial expression recognition.
</figureCaption>
<bodyText confidence="0.99989">
In addition important features of the voice of the subject are analysed. First the system will be
trained by a vocal input with neutral emotion. In order to recognise expression changes in the tone of
the voice will be monitored by the sudden changes in amplitude and mid and high frequencies.
The extracted features from face and voice will be combined by using weighted sum in which the
weights are being assigned by experts and can be modified. In the early experimental results, the
normalize weights were 3/5 and 2/5 for face and voice respectively. More imporvement is expected to
achieved if supported vector machine (SVM) is employeed for fusion of the facial and vocal features.
The initial experiments conducted on the different volunteers are showing the exact recognition of
expressions for sadness, happiness, surprise, and anger. However expressions of contempt and fear are
not being recognized properly due to their high misinterpretation with other expressions. The primarly
tests are conducted on a small datasets and on the continuation of the work a standard database
containing both facial and vocal senarios will be used.
</bodyText>
<sectionHeader confidence="0.998492" genericHeader="method">
3 Conclusion
</sectionHeader>
<bodyText confidence="0.997987">
In this paper we were proposing a new technique for recognition of expressions by using facial and
vocal expression recognition using weighted sum. The proposed technique has been tested on a small
set of group of people. The early experimental results show the high potential of the proposal as an
accurate expression recognition technique.
</bodyText>
<sectionHeader confidence="0.905283" genericHeader="method">
Reference
</sectionHeader>
<reference confidence="0.962456315789474">
[reference stub]
1. Z. Obrenovic and D. Starcevic, &amp;quot;Modeling multimodal human-computer interaction&amp;quot;, Computer, 2004,
37(9), 65- 72.
2. S. Benbelkacem, M. Belhocine, N. Zenati-Henda, A. Bellarbi, and M. Tadjine, &amp;quot;Integrating human-
computer interaction and business practices for mixed reality systems design: a case study&amp;quot;, IET Software,
2014, 8(2), 86-101.
3. A. Kucukyilmaz, T. M. Sezgin, and C. Basdogan, &amp;quot;Intention Recognition for Dynamic Role Exchange in
Haptic Collaboration&amp;quot;, IEEE Transaction on Haptics, 2013, 6(1), 56-68.
4. Y. Tie and L. Guan, &amp;quot;A Deformable 3-D Facial Expression Model for Dynamic Human Emotional State
Recogni- tion&amp;quot;, IEEE Transactions on Circuits and Systems for Video Technology, 2013, 23(1), 142-157.
5. M. Pantic and L. J. M. Rothkrantz, &amp;quot;Toward an affect-sensitive multimodal human-computer interaction&amp;quot;,
Pro- cessing of the IEEE, 2003, 91(9), 1370-1390.
6. M. S. Bartlett, G. Littlewort, M. G. Frank, C. Lainscsek, I. R. Fasel, and J. R. Movellan, &amp;quot;Automatic
recognition of facial actions in spontaneous expressions&amp;quot;, Journal of Multimedia, 2006, 1(6), 22–35.
7. J. F. Cohn, L. I. Reed, Z. Ambadar, J. Xiao, T. Moriyama, and T. Moriyama, &amp;quot;Automatic analysis and
recognition of brow actions and head motion in spontaneous facial behavior&amp;quot;, IEEE International
Conference on Systems, Man and Cybernetics, 2004, 610–616.
8. M. Yeasin, B. Bullot, and R. Sharma, &amp;quot;From facial expression to level of interest: A spatio-temporal
approach&amp;quot;, IEEE Coference on Computer Vision and Pattern Recognition, 2004, 922–927.
</reference>
<page confidence="0.994305">
104
</page>
<reference confidence="0.999682153846154">
9. I. Cohen, N. Sebe, A. Garg, L. S. Chen, and T. S. Huang, &amp;quot;Facial expression recognition from video
sequences: temporal and static modeling&amp;quot;, Computer Vision Image Understanding, 2003, 91(1-2), 160–187.
10. Z. Hammal and M. Kunz,&amp;quot;Pain monitoring: A dynamic and context-sensitive system&amp;quot;, Pattern Recognition,
2012, 45(4), 1265–1280.
11. T. L. Nwe, S. W. Foo, and L. C. D. Silva, &amp;quot;Speech emotion recognition using hidden markov models&amp;quot;,
Speech Communication, 2003, 41(4), 603–623.
12. A. Nogueiras, A. Moreno, A. Bonafonte, and J. B. Marino, &amp;quot;Speech emotion recognition using hidden
markov models&amp;quot;, proceeding of INTER-SPEECH, 2001, 2679–2682.
13. H. Meng and N. Bianchi-Berthouze, &amp;quot;Affective State Level Recognition in Naturalistic Facial and Vocal
Expres- sions&amp;quot;, IEEE Transaction on Cybernetics, 2014, 44(3), 315-325.
14. F. Eyben, S. Petridis, B. Schuller, G. Tzimiropoulos, and S. Zafeiriou, &amp;quot;Audiovisual classification of vocal
out- bursts in human conversation using long-short-term memory networks&amp;quot;, International Conference on
Acoustics, Speech and Signal Processing, 2011, 5844–5847.
</reference>
<page confidence="0.999017">
105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.730838">
<title confidence="0.999935">Expression Recognition by Using Facial And Vocal Expressions</title>
<author confidence="0.989793">Gholamreza Alvo</author>
<affiliation confidence="0.999045666666667">IMS IMS Institute of Institute of University of University of</affiliation>
<address confidence="0.952606">Tartu, Estonia Tartu, Estonia</address>
<email confidence="0.811008">sjafari@ut.eealvo.aabloo@ut.ee</email>
<abstract confidence="0.9936684">Human behaviour may be monitored by analysing facial expressions and vocal expressions. Hence an automatic technique which combines both these features will give a more accurate overall estimation of expression. In this work we propose a new method which is uses facial and vocal features to estimate the expression of the subject. Facial expressions are analysed by extracting important facial features and then clustering the movement of these features. In parallel the voice is processed by using considering sudden changes in amplitude and frequency in order to recognize the expression. Finally a weighted sum rule is used to combine the decisions obtained by facial and vocal expression recognition. The proposed technique is tested on an ongoing set of real data monitored by a psychologist.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<note>[reference stub]</note>
<marker></marker>
<rawString> [reference stub]</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Obrenovic</author>
<author>D Starcevic</author>
</authors>
<title>Modeling multimodal human-computer interaction&amp;quot;,</title>
<date>2004</date>
<journal>Computer,</journal>
<volume>37</volume>
<issue>9</issue>
<pages>65--72</pages>
<marker>1.</marker>
<rawString>Z. Obrenovic and D. Starcevic, &amp;quot;Modeling multimodal human-computer interaction&amp;quot;, Computer, 2004, 37(9), 65- 72.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Benbelkacem</author>
<author>M Belhocine</author>
<author>N Zenati-Henda</author>
<author>A Bellarbi</author>
<author>M Tadjine</author>
</authors>
<title>Integrating humancomputer interaction and business practices for mixed reality systems design: a case study&amp;quot;,</title>
<journal>IET Software,</journal>
<volume>8</volume>
<issue>2</issue>
<pages>86--101</pages>
<marker>2.</marker>
<rawString>S. Benbelkacem, M. Belhocine, N. Zenati-Henda, A. Bellarbi, and M. Tadjine, &amp;quot;Integrating humancomputer interaction and business practices for mixed reality systems design: a case study&amp;quot;, IET Software, 2014, 8(2), 86-101.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Kucukyilmaz</author>
<author>T M Sezgin</author>
<author>C Basdogan</author>
</authors>
<title>Intention Recognition for Dynamic Role Exchange in Haptic Collaboration&amp;quot;,</title>
<journal>IEEE Transaction on Haptics,</journal>
<volume>6</volume>
<issue>1</issue>
<pages>56--68</pages>
<marker>3.</marker>
<rawString>A. Kucukyilmaz, T. M. Sezgin, and C. Basdogan, &amp;quot;Intention Recognition for Dynamic Role Exchange in Haptic Collaboration&amp;quot;, IEEE Transaction on Haptics, 2013, 6(1), 56-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tie</author>
<author>L Guan</author>
</authors>
<title>A Deformable 3-D Facial Expression Model for Dynamic Human Emotional State Recogni- tion&amp;quot;,</title>
<date>2013</date>
<booktitle>IEEE Transactions on Circuits and Systems for Video Technology,</booktitle>
<volume>23</volume>
<issue>1</issue>
<pages>142--157</pages>
<contexts>
<context position="1337" citStr="[4, 5]" startWordPosition="203" endWordPosition="204">arallel the voice is processed by using considering sudden changes in amplitude and frequency in order to recognize the expression. Finally a weighted sum rule is used to combine the decisions obtained by facial and vocal expression recognition. The proposed technique is tested on an ongoing set of real data monitored by a psychologist. 1 Introduction Human-computer interaction has been centre of interest of many researchers for a number of years [1- 3]. In order to facilitate this interaction it is essential for the computer system, for example a robot, to understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as </context>
</contexts>
<marker>4.</marker>
<rawString>Y. Tie and L. Guan, &amp;quot;A Deformable 3-D Facial Expression Model for Dynamic Human Emotional State Recogni- tion&amp;quot;, IEEE Transactions on Circuits and Systems for Video Technology, 2013, 23(1), 142-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pantic</author>
<author>L J M Rothkrantz</author>
</authors>
<title>Toward an affect-sensitive multimodal human-computer interaction&amp;quot;,</title>
<date>2003</date>
<booktitle>Pro- cessing of the IEEE,</booktitle>
<volume>91</volume>
<issue>9</issue>
<pages>1370--1390</pages>
<contexts>
<context position="1337" citStr="[4, 5]" startWordPosition="203" endWordPosition="204">arallel the voice is processed by using considering sudden changes in amplitude and frequency in order to recognize the expression. Finally a weighted sum rule is used to combine the decisions obtained by facial and vocal expression recognition. The proposed technique is tested on an ongoing set of real data monitored by a psychologist. 1 Introduction Human-computer interaction has been centre of interest of many researchers for a number of years [1- 3]. In order to facilitate this interaction it is essential for the computer system, for example a robot, to understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as </context>
</contexts>
<marker>5.</marker>
<rawString>M. Pantic and L. J. M. Rothkrantz, &amp;quot;Toward an affect-sensitive multimodal human-computer interaction&amp;quot;, Pro- cessing of the IEEE, 2003, 91(9), 1370-1390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Bartlett</author>
<author>G Littlewort</author>
<author>M G Frank</author>
<author>C Lainscsek</author>
<author>I R Fasel</author>
<author>J R Movellan</author>
</authors>
<title>Automatic recognition of facial actions in spontaneous expressions&amp;quot;,</title>
<date>2006</date>
<journal>Journal of Multimedia,</journal>
<volume>1</volume>
<issue>6</issue>
<pages>22--35</pages>
<contexts>
<context position="1433" citStr="[6, 7]" startWordPosition="217" endWordPosition="218"> order to recognize the expression. Finally a weighted sum rule is used to combine the decisions obtained by facial and vocal expression recognition. The proposed technique is tested on an ongoing set of real data monitored by a psychologist. 1 Introduction Human-computer interaction has been centre of interest of many researchers for a number of years [1- 3]. In order to facilitate this interaction it is essential for the computer system, for example a robot, to understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as the hidden states are usually unknown and need to be estimated based on assumed probability dist</context>
</contexts>
<marker>6.</marker>
<rawString>M. S. Bartlett, G. Littlewort, M. G. Frank, C. Lainscsek, I. R. Fasel, and J. R. Movellan, &amp;quot;Automatic recognition of facial actions in spontaneous expressions&amp;quot;, Journal of Multimedia, 2006, 1(6), 22–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Cohn</author>
<author>L I Reed</author>
<author>Z Ambadar</author>
<author>J Xiao</author>
<author>T Moriyama</author>
<author>T Moriyama</author>
</authors>
<title>Automatic analysis and recognition of brow actions and head motion in spontaneous facial behavior&amp;quot;,</title>
<date>2004</date>
<booktitle>IEEE International Conference on Systems, Man and Cybernetics,</booktitle>
<pages>610--616</pages>
<contexts>
<context position="1433" citStr="[6, 7]" startWordPosition="217" endWordPosition="218"> order to recognize the expression. Finally a weighted sum rule is used to combine the decisions obtained by facial and vocal expression recognition. The proposed technique is tested on an ongoing set of real data monitored by a psychologist. 1 Introduction Human-computer interaction has been centre of interest of many researchers for a number of years [1- 3]. In order to facilitate this interaction it is essential for the computer system, for example a robot, to understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as the hidden states are usually unknown and need to be estimated based on assumed probability dist</context>
</contexts>
<marker>7.</marker>
<rawString>J. F. Cohn, L. I. Reed, Z. Ambadar, J. Xiao, T. Moriyama, and T. Moriyama, &amp;quot;Automatic analysis and recognition of brow actions and head motion in spontaneous facial behavior&amp;quot;, IEEE International Conference on Systems, Man and Cybernetics, 2004, 610–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Yeasin</author>
<author>B Bullot</author>
<author>R Sharma</author>
</authors>
<title>From facial expression to level of interest: A spatio-temporal approach&amp;quot;,</title>
<date>2004</date>
<journal>IEEE Coference on Computer Vision and Pattern Recognition,</journal>
<pages>922--927</pages>
<contexts>
<context position="1475" citStr="[8]" startWordPosition="223" endWordPosition="223"> weighted sum rule is used to combine the decisions obtained by facial and vocal expression recognition. The proposed technique is tested on an ongoing set of real data monitored by a psychologist. 1 Introduction Human-computer interaction has been centre of interest of many researchers for a number of years [1- 3]. In order to facilitate this interaction it is essential for the computer system, for example a robot, to understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as the hidden states are usually unknown and need to be estimated based on assumed probability distributions of the data [14]. In this work w</context>
</contexts>
<marker>8.</marker>
<rawString>M. Yeasin, B. Bullot, and R. Sharma, &amp;quot;From facial expression to level of interest: A spatio-temporal approach&amp;quot;, IEEE Coference on Computer Vision and Pattern Recognition, 2004, 922–927.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Cohen</author>
<author>N Sebe</author>
<author>A Garg</author>
<author>L S Chen</author>
<author>T S Huang</author>
</authors>
<title>Facial expression recognition from video sequences: temporal and static modeling&amp;quot;, Computer Vision Image Understanding,</title>
<date>2003</date>
<pages>91--1</pages>
<contexts>
<context position="1506" citStr="[9]" startWordPosition="228" endWordPosition="228">ombine the decisions obtained by facial and vocal expression recognition. The proposed technique is tested on an ongoing set of real data monitored by a psychologist. 1 Introduction Human-computer interaction has been centre of interest of many researchers for a number of years [1- 3]. In order to facilitate this interaction it is essential for the computer system, for example a robot, to understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as the hidden states are usually unknown and need to be estimated based on assumed probability distributions of the data [14]. In this work we are proposing a new expressio</context>
</contexts>
<marker>9.</marker>
<rawString>I. Cohen, N. Sebe, A. Garg, L. S. Chen, and T. S. Huang, &amp;quot;Facial expression recognition from video sequences: temporal and static modeling&amp;quot;, Computer Vision Image Understanding, 2003, 91(1-2), 160–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Hammal</author>
<author>M</author>
</authors>
<title>Kunz,&amp;quot;Pain monitoring: A dynamic and context-sensitive system&amp;quot;, Pattern Recognition,</title>
<date>2012</date>
<volume>45</volume>
<issue>4</issue>
<pages>1265--1280</pages>
<contexts>
<context position="1544" citStr="[10]" startWordPosition="234" endWordPosition="234">al and vocal expression recognition. The proposed technique is tested on an ongoing set of real data monitored by a psychologist. 1 Introduction Human-computer interaction has been centre of interest of many researchers for a number of years [1- 3]. In order to facilitate this interaction it is essential for the computer system, for example a robot, to understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as the hidden states are usually unknown and need to be estimated based on assumed probability distributions of the data [14]. In this work we are proposing a new expression recognition tech- nique which is usi</context>
</contexts>
<marker>10.</marker>
<rawString>Z. Hammal and M. Kunz,&amp;quot;Pain monitoring: A dynamic and context-sensitive system&amp;quot;, Pattern Recognition, 2012, 45(4), 1265–1280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Nwe</author>
<author>S W Foo</author>
<author>L C D Silva</author>
</authors>
<title>Speech emotion recognition using hidden markov models&amp;quot;, Speech Communication,</title>
<date>2003</date>
<volume>41</volume>
<issue>4</issue>
<pages>603--623</pages>
<contexts>
<context position="1718" citStr="[11, 12]" startWordPosition="256" endWordPosition="257">n has been centre of interest of many researchers for a number of years [1- 3]. In order to facilitate this interaction it is essential for the computer system, for example a robot, to understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as the hidden states are usually unknown and need to be estimated based on assumed probability distributions of the data [14]. In this work we are proposing a new expression recognition tech- nique which is using both facial and vocal expressions in order to estimate the expression of the speaker. The proposed technique is benefiting from the facial and vocal features. The proposed</context>
</contexts>
<marker>11.</marker>
<rawString>T. L. Nwe, S. W. Foo, and L. C. D. Silva, &amp;quot;Speech emotion recognition using hidden markov models&amp;quot;, Speech Communication, 2003, 41(4), 603–623.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nogueiras</author>
<author>A Moreno</author>
<author>A Bonafonte</author>
<author>J B Marino</author>
</authors>
<title>Speech emotion recognition using hidden markov models&amp;quot;, proceeding of INTER-SPEECH,</title>
<date>2001</date>
<pages>2679--2682</pages>
<contexts>
<context position="1718" citStr="[11, 12]" startWordPosition="256" endWordPosition="257">n has been centre of interest of many researchers for a number of years [1- 3]. In order to facilitate this interaction it is essential for the computer system, for example a robot, to understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as the hidden states are usually unknown and need to be estimated based on assumed probability distributions of the data [14]. In this work we are proposing a new expression recognition tech- nique which is using both facial and vocal expressions in order to estimate the expression of the speaker. The proposed technique is benefiting from the facial and vocal features. The proposed</context>
</contexts>
<marker>12.</marker>
<rawString>A. Nogueiras, A. Moreno, A. Bonafonte, and J. B. Marino, &amp;quot;Speech emotion recognition using hidden markov models&amp;quot;, proceeding of INTER-SPEECH, 2001, 2679–2682.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Meng</author>
<author>N Bianchi-Berthouze</author>
</authors>
<title>Affective State Level Recognition in Naturalistic Facial and Vocal Expres- sions&amp;quot;,</title>
<journal>IEEE Transaction on Cybernetics,</journal>
<volume>44</volume>
<issue>3</issue>
<pages>315--325</pages>
<contexts>
<context position="1861" citStr="[13]" startWordPosition="279" endWordPosition="279">puter system, for example a robot, to understand the feelings of the user [4, 5]. Recognition of emotions has been mainly achieved by using facial expression recognition [6, 7]. Techniques using k-nearest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as the hidden states are usually unknown and need to be estimated based on assumed probability distributions of the data [14]. In this work we are proposing a new expression recognition tech- nique which is using both facial and vocal expressions in order to estimate the expression of the speaker. The proposed technique is benefiting from the facial and vocal features. The proposed technique is evaluated with many volunteers sitting in front of a camera under controlled illumination reading texts with expressions of happi</context>
</contexts>
<marker>13.</marker>
<rawString>H. Meng and N. Bianchi-Berthouze, &amp;quot;Affective State Level Recognition in Naturalistic Facial and Vocal Expres- sions&amp;quot;, IEEE Transaction on Cybernetics, 2014, 44(3), 315-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Eyben</author>
<author>S Petridis</author>
<author>B Schuller</author>
<author>G Tzimiropoulos</author>
<author>S Zafeiriou</author>
</authors>
<title>Audiovisual classification of vocal out- bursts in human conversation using long-short-term memory networks&amp;quot;,</title>
<date>2011</date>
<booktitle>International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>5844--5847</pages>
<contexts>
<context position="2059" citStr="[14]" startWordPosition="313" endWordPosition="313">earest neighbour [8], hidden Makrov model (HMM) [9], and translation of belief model [10] have been frequently used for implementation of facial expression recognition. Vocal expression recognition has also been used separately for expression recognition [11, 12]. The combination of both facial and vocal expression is novel and many researchers are investigating an intelligent model for this fusion [13]. One of the main issues with these techniques is the application of HMM as the hidden states are usually unknown and need to be estimated based on assumed probability distributions of the data [14]. In this work we are proposing a new expression recognition tech- nique which is using both facial and vocal expressions in order to estimate the expression of the speaker. The proposed technique is benefiting from the facial and vocal features. The proposed technique is evaluated with many volunteers sitting in front of a camera under controlled illumination reading texts with expressions of happiness, sadness, disgust, surprise, anger, fear, and contempt. The initial experimental results are promising showing that the proposed technique out performs other current techniques. 2 The Proposed </context>
</contexts>
<marker>14.</marker>
<rawString>F. Eyben, S. Petridis, B. Schuller, G. Tzimiropoulos, and S. Zafeiriou, &amp;quot;Audiovisual classification of vocal out- bursts in human conversation using long-short-term memory networks&amp;quot;, International Conference on Acoustics, Speech and Signal Processing, 2011, 5844–5847.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
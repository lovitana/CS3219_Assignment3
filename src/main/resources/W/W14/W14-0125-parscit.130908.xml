<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000202">
<title confidence="0.995548">
Parse Ranking with Semantic Dependencies and WordNet
</title>
<author confidence="0.98273">
Xiaocheng Yin&apos;s Jungjae Kim&apos;s Zinaida Pozen♦6 Francis Bond&apos;s
</author>
<affiliation confidence="0.992595">
&apos;s Nanyang Technological University, Singapore
° University of Washington, Seattle
</affiliation>
<email confidence="0.980568">
yinx0005@e.ntu.edu.sg,jungjae.kim@ntu.edu.sg,
zpozen@gmail.com,bond@ieee.org
</email>
<sectionHeader confidence="0.995478" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831769230769">
In this paper, we investigate which fea-
tures are useful for ranking semantic rep-
resentations of text. We show that two
methods of generalization improved re-
sults: extended grand-parenting and super-
types. The models are tested on a subset of
SemCor that has been annotated with both
Dependency Minimal Recursion Seman-
tic representations and WordNet senses.
Using both types of features gives a sig-
nificant improvement in whole sentence
parse selection accuracy over the baseline
model.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999345736842105">
In this paper we investigate various features to
improve the accuracy of semantic parse ranking.
There has been considerable successful work on
syntactic parse ranking and reranking (Toutanova
et al., 2005; Collins and Koo, 2006; McClosky
et al., 2006), but very little that uses pure semantic
representations. With recent work on building se-
mantic representations (from deep grammars such
as LFG (Butt et al., 1999) and HPSG (Sag et al.,
1999), directly through lambda calculus, or as in
intermediate step in machine translation) the ques-
tion of ranking them has become more important.
The closest related work is Fujita et al. (2010)
who ranked parses using semantic features from
Minimal Recursion Semantics (MRS) and syntac-
tic trees, using a Maximum Entropy Ranker. They
experimented with Japanese data, using the Hinoki
Treebank (Bond et al., 2008), using primarily ele-
mentary dependencies: single arcs between pred-
</bodyText>
<footnote confidence="0.36944">
♣Currently at PointInside, Inc.
</footnote>
<figure confidence="0.634222">
S
</figure>
<figureCaption confidence="0.987591">
Figure 1: Syntactic view of sentence “I treat dogs
and cats with worms”.
</figureCaption>
<bodyText confidence="0.999584888888889">
icates and their arguments. These can miss some
important connections between predicates.
An example parse tree for I treat dogs and cats
with worms is shown in Figure 1.1, for the interpre-
tation “I treat both dogs and cats that have worms”
(not “I treat, using worms, dogs and cats” or any
of the other possibilities)
The semantic representation we use is De-
pendency Minimal Recursion Semantics (DRMS:
Copestake, 2009). The Minimal Recursion Se-
mantics (MRS: Copestake et al., 2005) is a com-
putationally tractable flat semantics that under-
specifies quantifier scope. The Dependency MRS
is an MRS representation format that keeps all
the information from the MRS but is simpler to
manipulate. DMRSs differ from syntactic de-
pendency graphs in that the relations are defined
between slightly abstract predicates, not between
</bodyText>
<footnote confidence="0.724936">
1Simplified by omission of non-branching nodes.
</footnote>
<figure confidence="0.988203769230769">
NP
VP
N
PP
CONJ
and
N
cats
N V NP
I treat
N N
dogs
with worms
</figure>
<bodyText confidence="0.999490678571429">
surface forms. Some semantically empty surface
tokens (such as infinitive to) are not included,
while some predicates are inserted that are not in
the original text (such as the null article).
A simplified MRS representation of our exam-
ple sentence and its DMRS equivalent are shown
in Figure 2.
In the DMRS, the basic links between the nodes
are present. However, potentially interesting rela-
tions such as that between the verb treat and its
conjoined arguments dogs and cats are not linked
directly. Similarly, the relation between dogs and
cats and worms is conveyed by the preposition
with, which links them through its external argu-
ment (ARG1: and) and internal argument (ARG2:
worms). There is no direct link. We investigate
new features that make these links more direct
(Section 3.2).
We also explore the significance of the effec-
tiveness of links between words that are connected
arbitrarily far away in the semantic graph (Sec-
tion 3.2.3).
Finally, we experimented with generalizing
over semantic classes. We used WordNet semantic
files as supertypes to reduce data sparseness (Sec-
tion 3.2.4). This will generalize the lexical seman-
tics of the predicates, resulting in a reduction of
feature size and ambiguity.
</bodyText>
<sectionHeader confidence="0.993933" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999936923076923">
This paper follows up on the work of Fujita et al.
(2010) in ranking MRS semantic representations,
which was carried out for Japanese. We are con-
ducting a similar investigation for English, and
add new features and approaches. Fujita et al.
(2010) worked with the Japanese Hinoki Corpus
(Bond et al., 2008) data and used hypernym chains
from the Goi-Taikei Japanese ontology (Ikehara
et al., 1997) for variable-level semantic backoff.
This is in contrast to the uniform WordNet seman-
tic file backoff performed here. In addition, this
work only focuses on MRS ranking, whereas Fu-
jita et al. (2010) combined MRS features with syn-
tactic features to improve syntactic parse ranking
accuracy.
Our use of WordNet Semantic Files (SF) to re-
duce lexical feature sparseness is inspired by sev-
eral recent papers. Agirre et al. (2008, 2011) have
experimented with replacing open-class words
with their SFs. Agirre et al. (2008) have shown
an improvement in full parse and PP attachment
scores with statistical constituency parsers using
SFs. Agirre et al. (2011) have followed up on
those results and re-trained a dependency parser
on the data where words were replaced with their
SFs. This resulted in a very modest labeled at-
tachment score improvement, but with a signifi-
cantly reduced feature set. In a recent HPSG work,
MacKinlay et al. (2012) attempted to integrate lex-
ical semantic features, including SF backoff, into
a discriminative parse ranking model. However,
this was not shown to help, presumably because
the lexical semantic features were built from syn-
tactic constituents rather than MRS predicates.
The ancestor features found to be helpful here
are inspired by the use of grand-parenting in syn-
tactic parse ranking (Toutanova et al., 2005) and
chains in dependency parsing ranking (Le Roux
et al., 2012).
</bodyText>
<sectionHeader confidence="0.995274" genericHeader="method">
3 Resources and Methodology
</sectionHeader>
<bodyText confidence="0.999906">
In this section we introduce the corpus we work
on, and the features we extract from it.
</bodyText>
<subsectionHeader confidence="0.999512">
3.1 Corpus: SemCor
</subsectionHeader>
<bodyText confidence="0.9894416875">
To evaluate our ranking methods, we are using
the Redwoods Treebank (Oepen et al., 2004) of
manually disambiguated HPSG parses, storing full
signs for each analysis and supporting export into
a variety of formats, including the Dependency
MRS (DMRS) format used in this work.
The HPSG parses in Redwoods are based on
the English Resource Grammar (ERG; Flickinger,
2000) – a hand-crafted broad-coverage HPSG
grammar of English.
For our experiments, we used a subset of the
Redwoods Treebank, consisting of 2,590 sen-
tences drawn from SemCor (Landes et al., 1998).
In the SemCor corpus each of the sentences is
tagged with WordNet senses created at Princeton
University by the WordNet Project research team.
The average length of the Redwoods SemCor sen-
tences is 15.4 words, and the average number of
parses is 247.
From the treebank we can export the DMRS.
The choice of which words become predicates is
slightly different in the SemCor/WordNet and the
ERG. The ERG lexicon groups together all senses
that have the same syntactic properties, making
them underspecified for many sense differences.
Thus elementary predicate catn.1 could be any of
the WordNet senses catn.l “feline mammal usu-
I treat dogs and cats with worms.
Simplified by omission of quantifiers
Dashed lines show Preposition (P) features
Dotted lines show Conjunction (LR) features
Arc labels show the roles: 1 is ARG1, 2 is ARG2, ....
</bodyText>
<figureCaption confidence="0.986484">
Figure 2: MRS and DMRS for I treat cats and dogs with worms.
</figureCaption>
<equation confidence="0.972094568627451">
 
pron(0:1)
 
LBL h4 h
ARG0 x5 x
 
dogn:1(7:11)
 
LBL h17 h 
ARG0 x15
withp(21:25)

LBL h22  
ARG0 e24 e  
ARG1 x9 
ARG2 x25 x
treatv:1(2:6)
LBL h2 h   
ARG0 e3  
ARG1 x5 
ARG2 x9 x
 
andc(12:15)
 LBL h22 h  
 
ARG0 x9 
 L-INDEX x15 
R-INDEX x19
*RELS

                 
+                  
,
 
wormn:1(26:31)
 
, LBL h29 h 
ARG0 x25
 
catn:1(16:20)
 
, LBL h23 h 
ARG0 x19
2 1 with
1
1 2
2 L R 1 2
pron treatv:1 dogn:1 andc catn:1 withp wormn:1
mrs
LTOP h1 h
INDEX e3 e
</equation>
<bodyText confidence="0.913552615384615">
Topsn actn
animaln artifactn
attributen bodyn
cognitionn communicationn
eventn feelingn
foodn groupn
locationn motiven
objectn personn
phenomenonn plantn
possessionn processn
quantityn relationn
shapen staten
substancen timen
</bodyText>
<tableCaption confidence="0.993486">
Table 1: WordNet Noun Semantic Files.
</tableCaption>
<bodyText confidence="0.99984996">
ally having thick soft fur and no ability to roar”,
catn:2 “an informal term for a youth or man” and
six more.2 In some cases, DMRS decomposes a
single predicate into multiple predicates (e.g. here
into inp thisq placen). The ERG and WordNet also
often make different decisions about what consti-
tutes a multiword expression. For these reasons
the mapping between the two annotations is not
always straightforward. In this paper we use the
mapping between the DRMS and WordNet anno-
tations produced by Pozen (2013).
Using the mapping, we exploited the sense tag-
ging of the SemCor in several ways. We ex-
perimented both with replacing elementary pred-
icates with their synsets, their hypernyms at var-
ious levels and with their semantic files (Landes
et al., 1998), which generalize the meanings of
words that belong to the same broad semantic cat-
egories.3 These dozens of generalized semantic
tags help to address the issue of feature sparse-
ness, compared to thousands of synsets. We show
the semantic files for nouns and verbs in Tables 1
and 2. In this paper, we only report on the parse
selection accuracy using semantic files to reduce
ambiguity, as it gave the best results.
</bodyText>
<subsectionHeader confidence="0.999759">
3.2 Semantic Dependency Features
</subsectionHeader>
<bodyText confidence="0.99022825">
In this section we introduce the baseline features
for parse ranking.
Table 3 shows example features extracted from
the DMRS depicted in Figure 2.Features 1–16 are
</bodyText>
<footnote confidence="0.9687106">
2Elementary predicates are shown in sans-serif font, Word-
Net senses in bold italic, WordNet semantic files
are shown in bold typewriter.
3Semantic Files are also sometimes referred to as Seman-
tic Fields, Lexical Fields or Supersenses.
</footnote>
<table confidence="0.683117">
bodyv changev
cognitionv communicationv
competitionv consumptionv
contactv creationv
emotionv motionv
perceptionv possessionv
socialv stativev
weatherv
</table>
<tableCaption confidence="0.954744">
Table 2: WordNet Verb Semantic Files.
</tableCaption>
<table confidence="0.474703">
the semantic dependency features (Baseline). 17–
18 are the conjunctive features (LR). 19–22 are the
preposition role features (PR).
# Sample Features
</table>
<equation confidence="0.444522888888889">
0 (0 treatv:1 ARG1 pron ARG2 andc)
1 (0 andc L-IND dogn:1 R-IND catn:1)
2 (0 withp ARG1 andc ARG2 wormn:1)
3 (1 treatv:1 ARG1 pron)
4 (1 treatv:1 ARG2 andc)
5 (1 andc L-IND dogn:1)
6 (1 andc R-IND catn:1)
7 (1 withp ARG1 andc)
8 (1 withp ARG2 wormn:1)
</equation>
<table confidence="0.908567466666667">
9 (2 treatv:1 pron andc)
10 (2 withp andc wormn:1)
11 (3 treatv:1 pron)
12 (3 treatv:1 andc)
13 (3 andc dogn:1)
14 (3 andc catn:1)
15 (3 withp andc)
16 (3 withp wormn:1)
17 (1 treatv:1 ARG2 dogn:1)
18 (1 treatv:1 ARG2 catn:1)
19 ( 0 andc L-IND dogn:1 R-IND catn:1
withp wormn:1 )
20 (1 andcwithp wormn:1)
21 (2 andc wormn:1)
22 (3 andc wormn:1)
</table>
<tableCaption confidence="0.972248">
Table 3: Features for the DMRS in Fig 2.
</tableCaption>
<bodyText confidence="0.9936509">
Baseline features are those that directly reflect
the dependencies of the DMRS. In Table 3, fea-
ture type (0) (0–2) shows predicates with all their
arguments. Feature type (1) (3–8) shows each ar-
gument individually. Feature type (2) shows all ar-
guments without the argument types. Feature type
(3) is the least specified, showing individual argu-
ments without the labels. These types are the same
as the MRS features of Toutanova et al. (2005) and
the SEM-DEP features of Fujita et al. (2010).
</bodyText>
<subsectionHeader confidence="0.612924">
3.2.1 Conjunctive Features
</subsectionHeader>
<bodyText confidence="0.9997358">
We further create two more features, called
Left/Right Handle Features (LR), to link directly
the two arguments of conjunctive relations with
their parent, independently from the other ar-
gument. In Table 1, for example, the feature
(treatv:1 ARG2 andc), although valid, does not con-
vey the meaning of the sentence. Instead, we add
the two LR features (treatv:1 ARG2 dogn:1) (fea-
ture 17) and (treatv:1 ARG2 catn:1) (feature 18),
which better model the conjunction relation.
</bodyText>
<subsubsectionHeader confidence="0.633964">
3.2.2 Preposition Role Features
</subsubsectionHeader>
<bodyText confidence="0.999985321428572">
As shown in Figure 2, the node withp has two
links: to andc (ARG1) and to wormn:1 (ARG2). The
two relations together indicate a noun-preposition-
noun relationship. Instead of breaking the rela-
tionship into the two separate features, we intro-
duce it, as a whole, as a new type of feature, where
the two arguments of the preposition (e.g. andc,
wormn:1) will have a direct relation via the preposi-
tion (e.g. withp). We name these Preposition Role
features (PR), as they are similar in spirit to se-
mantic roles. Some sample PR features are given
in Table 3, features 19–22.
The new features explicitly convey, for exam-
ple, noun-preposition-noun relations. Parses con-
taining features like something at somewhere can
be further distinguished from parses containing at
somewhere and something at separately. When the
features become more representative, active parses
are more likely to be selected, though with the cost
of a larger feature set size.
As 4 types of features can be developed based
on one relationship, a Preposition Role link would
have 4 separate features. While the Conjunctive
features mentioned in previous section give 2 to 4
additional features, Baseline-PR features normally
give 4 more. Thus, the feature size of Baseline-
PR model is larger than that of the Baseline-LR
model.
</bodyText>
<subsectionHeader confidence="0.781836">
3.2.3 Ancestor Features
</subsectionHeader>
<bodyText confidence="0.999978571428571">
While the semantic dependency features corre-
spond to direct dependencies, we introduce a new
type of features that represent indirect dependen-
cies between ancestors and their descendants in
the DMRS. For each predicate, we collect all its
descendants linked through more than one depen-
dency and create features to represent the indirect
</bodyText>
<figure confidence="0.9529409375">
# Sample Features
0 (0 treatv:1 ARG1 pron ARG2 andc)
1 (0 andc L-IND dogn:1 R-IND catn:1)
2 (0 treatv:1 ARG2 dogn:1 ARG2 catn:1)
3 (1 treatv:1 ARG1 pron)
4 (1 treatv:1 ARG2 andc)
5 (1 treatv:1 ARG2 dogn:1)
6 (1 treatv:1 ARG2 catn:1)
7 (1 andc L-IND dogn:1)
8 (1 andc R-IND catn:1)
9 (2 treatv:1 pron andc)
10 (2 treatv:1 dogn:1 catn:1)
11 (3 treatv:1 pron)
12 (3 treatv:1 andc)
13 (3 treatv:1 dogn:1)
14 (3 treatv:1 catn:1)
</figure>
<tableCaption confidence="0.964814">
Table 4: Ancestor Features (AF).
</tableCaption>
<bodyText confidence="0.99128108">
dependencies between the predicate and the de-
scendants. We name these features Ancestor Fea-
tures (AF).
Table 4 has some sample AF features such as
that linking from treatv:1 to dogn:1 and catn:1 (i.e.
feature 2). This is a one-level ancestor, involving
two predicates, while multi-level ancestors deal
with more than two predicates linked in a se-
quence. Note that these are different from the LR
features (features 15, 16 in Table 1), in that AF
features include both arguments of a conjunction,
for example, connecting the predicate treatv:1 to its
grandchildren dogn:1 and catn:1 via the argument
role of andc in the predicate (feature 2 in Table 4).
When a sentence has n dependencies, our
method generates O(n(n−1)
2 ) = O(n2) AF fea-
tures. In the corpus we use, the dependency struc-
ture of a sentence typically has 4 levels. In prac-
tice the number of AF features is roughly triple
the number of Baseline features. In the evaluation
experiments, we investigated all the eight combi-
nations of the three types of LR, PR, and AF fea-
tures, where each combination is combined with
the baseline features.
</bodyText>
<subsectionHeader confidence="0.788425">
3.2.4 Semantic File Features
</subsectionHeader>
<bodyText confidence="0.998547285714286">
In the features up until now, words have been rep-
resented as elementary predicate semantic depen-
dencies (SD). Because SemCor also has WordNet
senses, we experiment with replacing open class
words with their supertypes, in this case using
the WordNet semantic files (SF). If a word is not
matched to a WordNet synset we continue to use
</bodyText>
<figure confidence="0.982086944444445">
# Sample Features
0 (0 bodyv ARG1 pron ARG2 andc)
1 (0 andc L-IND animaln R-IND animaln)
2 (0 withp ARG1 andc ARG2 animaln)
3 (1 bodyv ARG1 pron)
4 (1 bodyv ARG2 andc)
5 (1 andc L-IND animaln)
6 (1 andc R-IND animaln)
7 (1 withc ARG1 animaln)
8 (1 withc ARG2 animaln)
9 (2 bodyv pron andc)
10 (2 withpandc animaln)
11 (3 bodyv pron)
12 (3 bodyv andc)
13 (3 andc animaln)
14 (3 andc animaln)
15 (3 withc andc)
16 (3 withc animaln)
</figure>
<bodyText confidence="0.959059333333333">
Table 5: Baseline features with Semantic Files
(SF).
the elementary predicate. This SF representation
is also applied to the eight combinations of feature
types. A sample of the features in the SF represen-
tations are given in Table 5.
Sometimes two features, such as 13 and 14 in
Table 3, are replaced with the same feature, like
9 in Table 5, because dogn:1 and catn:1 are both
replaced with animaln. There are about half as
many Semantic File features as there are SD fea-
tures.
</bodyText>
<sectionHeader confidence="0.99987" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999587470588235">
We set up the evaluation task as reranking of the
top 500 Redwoods analyses, previously selected
by the syntactic MaxEnt ranker. The subset of
SemCor introduced in Section 3.1 is trained and
tested with the features introduced in Section 3.2.
We grouped the feature sets into two according to
the two word representation of basic Semantic De-
pendencies (SD) and generalized Semantic Files
(SF). Sometime two or more different parses of a
sentence have the same set of features. That is, the
features failed to distinguish between two parses:
often because of spurious syntactic ambiguity that
had no effect on the semantics. In this case we
merged duplicate feature sets to reduce the ambi-
guity in machine learning. If an inactive parse has
the same set of features as that of the active one,
the resulting merged parse was treated as active.
</bodyText>
<table confidence="0.9997668">
Features Accuracy Features
(%) (x1,000)
SD-Baseline 25.4 454
SD+LR 25.3 469
SD+PR 25.8 563
SD+LR+PR 25.6 582
SD+AF 24.8 1,430
SD+AF+LR 27.1 1,497
SD+AF+PR 25.8 1,761
SD+AF+LR+PR 26.3 1,842
</table>
<tableCaption confidence="0.95217">
Table 6: Parse selection results with SD.
</tableCaption>
<table confidence="0.999965">
Features Accuracy Features
(%) (x1,000)
SF-Baseline 25.0 223
SF+LR 25.1 235
SF+PR 26.3 306
SF+LR+PR 26.3 321
SF+AF 28.2 1,051
SF+AF+LR 28.0 1,101
SF+AF+PR 28.1 1,310
SF+AF+LR+PR 27.7 1,375
</table>
<tableCaption confidence="0.999586">
Table 7: Parse selection results with SF.
</tableCaption>
<bodyText confidence="0.99991655882353">
We used TADM (Toolkit for Advanced Dis-
criminative Modeling; Malouf, 2002) for the train-
ing and testing of our machine learning model, fol-
lowing Fujita et al. (2010). We carried out 10-fold
cross-validation for evaluation. We measured the
parse selection accuracy at the sentence level. A
parse was considered correct only when all the de-
pendencies of the parse are correct.
The results of parse selection based on SD and
SF representations are shown in Tables 6 and 7.
The addition of the ancestor features (AF) gives
the most increase in the parse selection accuracy.
This result indicates that indirect dependencies as
well as direct dependencies in a successful parse
frequently appear in other active parses. Second,
the SF representation shows better results than the
SD representation in most cases. The semantic ab-
straction of the semantic files reduces the problem
of feature sparseness and is enough to effectively
rerank parses, whose syntactic properties are al-
ready to some extent validated during parsing.
Third, the addition of the PR features also usu-
ally increases the parse selection accuracy. We
plan to (semi-)automatically find more such multi-
dependency structures whose combination shows
better performance than the individual dependen-
cies. Fourth, the LR features do not improve the
accuracy significantly in most cases, though the
SD+AF+LR combination shows the best results
among the feature sets of the SD representation.
This is understandable since the number of the LR
features in our corpus is much smaller than those
of the other features of SD, PR and AF. We need
to test it with a bigger corpus.
</bodyText>
<sectionHeader confidence="0.998" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999978185185185">
These results show the validity of our assumption
that long distance features and supertypes are both
useful for selecting the correct interpretation of a
sentence. Currently the SD+AF+LR model is the
best for using the elementary predicates. How-
ever the best overall results come from the SF+AF
model when we generalize to the semantic files.
In future work we will investigate on larger-sized
and more richly annotated corpora so that we can
discover more about the relation between feature
size and parse selection accuracy. In addition, we
expect that increasing the corpus size will lead di-
rectly to higher accuracy. Other avenues we would
like to explore is backing off not to the semantic
files, but rather to WordNet hypernyms at various
levels.
These results show that generalizing to seman-
tic supertypes allows us to build semantic ranking
models that are not only smaller, but more accu-
rate. In general, learning time was roughly pro-
portional to the number of features, so a smaller
model can be learned faster. We hypothesize that it
is the combination of dependencies and supertypes
that makes the difference: approaches that used se-
mantic features on phrase structure trees (such as
Bikel (2000) and MacKinlay et al. (2012)) have in
general failed to get much improvement.
</bodyText>
<figureCaption confidence="0.997972">
Figure 3: Learning curve for SF+AF.
</figureCaption>
<bodyText confidence="0.999990852941176">
The overall accuracy is still quite low, due prin-
cipally to the lack of training data. We show
the learning curves for the SF+AF configuration
in Figure 3 (the other configurations are similar).
The curve is still clearly rising: the accuracy of
parse selection on our corpus is far from saturated.
This observation gives us confidence that with a
larger corpus the accuracy of parse selection will
improve considerably. The learning curve in Fujita
et al. (2010) showed similar results for the same
amount of data, and increased rapidly with more
(they had a larger corpus for Japanese).
As there are so far still very few corpora with
both structural and lexical semantic annotation,
we are currently investigating the use of automatic
word sense disambiguation to create the features,
in a similar way to Agirre et al. (2008). Finally, we
would like to investigate even more features, such
as the dependency chains of Le Roux et al. (2012).
One exciting possibility is projecting ranking
features across languages: wordnet semantic files
are the supertypes for all wordnets linked to
the Princeton Wordnet, of which there are many
(Bond and Foster, 2013). The predicates that are
not in the wordnets are generally either named
entities or from smallish closed sets of function
words such as conjunctions, prepositions and pro-
nouns. We are currently investigating mapping
these between Japanese and English using trans-
ferrules from an existing machine translation sys-
tem (Bond et al., 2011). In principal, a small set
of mappings for closed class words could allow us
to quickly boot-strap a semantic ranking model for
any language with a wordnet.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999965217391304">
In summary, we showed some features that help
parse selection. In the SD group, LR features
together with AF features achieved a 1.75% im-
provement in accuracy over the basic Baseline
model (25.36% → 27.12%). However, LR feature
alone and AF feature alone both decrease the accu-
racy (25.36% → 25.28% and 25.36% → 24.84%).
PR features and combination of PR and AF fea-
tures both achieved small improvements (0.416%
Baseline → Baseline+PR, 0.410% Baseline →
Baseline-PR+AF). LR combined with PR features
did not improve the accuracy.
When features get generalized to supertypes, as
shown in the SF group, models with more fea-
tures achieved higher accuracies with the best be-
ing the model with ancestor features (AF) added.
This (SF+AF) achieved an improvement of 3.21%
absolute over the baseline model (24.97% →
28.18%). Adding more features to AF only de-
creases the accuracy. Generalizing to semantic su-
pertypes allows us to build dependency ranking
models that are not only smaller, but more accu-
rate.
</bodyText>
<sectionHeader confidence="0.997543" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999935">
The authors are grateful to Mathieu Morey and
other members of the Deep Linguistic Processing
with HPSG Initiative along with other members of
their research groups for many extremely helpful
discussions.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999745596153847">
Eneko Agirre, Timothy Baldwin, and David Martinez. 2008.
Improving parsing and PP attachment performance with
sense information. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguistics
(ACL HLT 2008), pages 317–325. Columbus, USA.
Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and
Joakim Nivre. 2011. Improving dependency parsing with
semantic classes. Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics, pages
699–703.
Daniel M. Bikel. 2000. A statistical model for parsing
and word-sense disambiguation. In Proceedings of the
Joint SIGDAT Conference on Empirical Methods in Natu-
ral Language Processing and Very Large Corpora, pages
155–163. Hong Kong.
Francis Bond and Ryan Foster. 2013. Linking and extending
an open multilingual wordnet. In 51st Annual Meeting
of the Association for Computational Linguistics: ACL-
2013, pages 1352–1362. Sofia. URL http://aclweb.
org/anthology/P13-1133.
Francis Bond, Sanae Fujita, and Takaaki Tanaka. 2008.
The Hinoki syntactic and semantic treebank of
Japanese. Language Resources and Evaluation,
42(2):243–251. URL http://dx.doi.org/
10.1007/s10579-008-9062-z, (Re-issue of DOI
10.1007/s10579-007-9036-6 as Springer lost the Japanese
text).
Francis Bond, Stephan Oepen, Eric Nichols, Dan Flickinger,
Erik Velldal, and Petter Haugereid. 2011. Deep
open source machine translation. Machine Transla-
tion, 25(2):87–105. URL http://dx.doi.org/10.
1007/s10590-011-9099-4, (Special Issue on Open
source Machine Translation).
Miriam Butt, Tracy Holloway King, Maria-Eugenia Ni˜no,
and Fr´ed´erique Segond. 1999. A Grammar Writer’s Cook-
book. CSLI publications.
Michael Collins and Terry Koo. 2006. Discriminative rerank-
ing for natural language parsing. Computational Linguis-
tics, 31(1).
Ann Copestake. 2009. Slacker semantics: Why superficiality,
dependency and avoidance of commitment can be the right
way to go. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 1–9.
Athens.
Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl Pol-
lard. 2005. Minimal Recursion Semantics. An introduc-
tion. Research on Language and Computation, 3(4):281–
332.
Dan Flickinger. 2000. On building a more efficient gram-
mar by exploiting types. Natural Language Engineering,
6 (1):15–28.
Sanae Fujita, Francis Bond, Takaaki Tanaka, and Stephan
Oepen. 2010. Exploiting semantic information for HPSG
parse selection. Research on Language and Computation,
8(1):1–22. URL http://dx.doi.org/10.1007/
s11168-010-9069-7.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei —
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Shari Landes, Claudia Leacock, and Christiane Fellbaum.
1998. Building semantic concordances. In Christine Fell-
baum, editor, WordNet: An Electronic Lexical Database,
chapter 8, pages 199–216. MIT Press.
Joseph Le Roux, Benoit Favre, Alexis Nasr, and Seyed Abol-
ghasem Mirroshandel. 2012. Generative constituent pars-
ing and discriminative dependency reranking: Experi-
ments on english and french. In Proceedings of the ACL
2012 Joint Workshop on Statistical Parsing and Seman-
tic Processing ofMorphologically Rich Languages, pages
89–99. Association for Computational Linguistics, Jeju,
Republic of Korea. URL http://www.aclweb.org/
anthology/W12-3412.
Andrew MacKinlay, Rebecca Dridan, Diana McCarthy, and
Timothy Baldwin. 2012. The effects of semantic an-
notations on precision parse ranking. In SEM 2012:
The First Joint Conference on Lexical and Computational
Semantics-, volume 2, pages 228–236. Association for
Computational Linguistics.
Robert Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In CONLL-2002,
pages 49–55. Taipei, Taiwan.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Reranking and self-training for parser adaptation. In
44th Annual Meeting of the Association for Computational
Linguistics and 21st International Conference on Compu-
tational Linguistics: COLING/ACL-2006, pages 337–344.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christoper D. Manning. 2004. LinGO Redwoods: A rich
and dynamic treebank for HPSG. Research on Language
and Computation, 2(4):575–596.
Zinaida Pozen. 2013. Using Lexical and Composi-
tional Semantics to Improve HPSG Parse Selection.
Master’s thesis, University of Washington. URL
https://digital.lib.washington.edu/
researchworks/handle/1773/%23469.
Ivan A. Sag, Tom Wasow, and Emily Bender. 1999. Syntactic
Theory: A Formal Introduction. CSLI Publications, Stan-
ford, second edition.
Kristina Toutanova, Christopher D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse disam-
biguation using the Redwoods corpus. Research on Lan-
guage and Computation, 3(1):83–105.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.114526">
<title confidence="0.9291405">Parse Ranking with Semantic Dependencies and WordNet Francis</title>
<abstract confidence="0.864217705882353">Technological of zpozen@gmail.com,bond@ieee.org Abstract In this paper, we investigate which features are useful for ranking semantic representations of text. We show that two methods of generalization improved results: extended grand-parenting and supertypes. The models are tested on a subset of SemCor that has been annotated with both Dependency Minimal Recursion Semantic representations and WordNet senses. Using both types of features gives a significant improvement in whole sentence parse selection accuracy over the baseline model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Timothy Baldwin</author>
<author>David Martinez</author>
</authors>
<title>Improving parsing and PP attachment performance with sense information.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL HLT</booktitle>
<pages>317--325</pages>
<location>Columbus, USA.</location>
<contexts>
<context position="4800" citStr="Agirre et al. (2008" startWordPosition="756" endWordPosition="759">features and approaches. Fujita et al. (2010) worked with the Japanese Hinoki Corpus (Bond et al., 2008) data and used hypernym chains from the Goi-Taikei Japanese ontology (Ikehara et al., 1997) for variable-level semantic backoff. This is in contrast to the uniform WordNet semantic file backoff performed here. In addition, this work only focuses on MRS ranking, whereas Fujita et al. (2010) combined MRS features with syntactic features to improve syntactic parse ranking accuracy. Our use of WordNet Semantic Files (SF) to reduce lexical feature sparseness is inspired by several recent papers. Agirre et al. (2008, 2011) have experimented with replacing open-class words with their SFs. Agirre et al. (2008) have shown an improvement in full parse and PP attachment scores with statistical constituency parsers using SFs. Agirre et al. (2011) have followed up on those results and re-trained a dependency parser on the data where words were replaced with their SFs. This resulted in a very modest labeled attachment score improvement, but with a significantly reduced feature set. In a recent HPSG work, MacKinlay et al. (2012) attempted to integrate lexical semantic features, including SF backoff, into a discri</context>
<context position="21414" citStr="Agirre et al. (2008)" startWordPosition="3587" endWordPosition="3590">still clearly rising: the accuracy of parse selection on our corpus is far from saturated. This observation gives us confidence that with a larger corpus the accuracy of parse selection will improve considerably. The learning curve in Fujita et al. (2010) showed similar results for the same amount of data, and increased rapidly with more (they had a larger corpus for Japanese). As there are so far still very few corpora with both structural and lexical semantic annotation, we are currently investigating the use of automatic word sense disambiguation to create the features, in a similar way to Agirre et al. (2008). Finally, we would like to investigate even more features, such as the dependency chains of Le Roux et al. (2012). One exciting possibility is projecting ranking features across languages: wordnet semantic files are the supertypes for all wordnets linked to the Princeton Wordnet, of which there are many (Bond and Foster, 2013). The predicates that are not in the wordnets are generally either named entities or from smallish closed sets of function words such as conjunctions, prepositions and pronouns. We are currently investigating mapping these between Japanese and English using transferrules</context>
</contexts>
<marker>Agirre, Baldwin, Martinez, 2008</marker>
<rawString>Eneko Agirre, Timothy Baldwin, and David Martinez. 2008. Improving parsing and PP attachment performance with sense information. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL HLT 2008), pages 317–325. Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Kepa Bengoetxea</author>
<author>Koldo Gojenola</author>
<author>Joakim Nivre</author>
</authors>
<title>Improving dependency parsing with semantic classes.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>699--703</pages>
<contexts>
<context position="5029" citStr="Agirre et al. (2011)" startWordPosition="791" endWordPosition="794">off. This is in contrast to the uniform WordNet semantic file backoff performed here. In addition, this work only focuses on MRS ranking, whereas Fujita et al. (2010) combined MRS features with syntactic features to improve syntactic parse ranking accuracy. Our use of WordNet Semantic Files (SF) to reduce lexical feature sparseness is inspired by several recent papers. Agirre et al. (2008, 2011) have experimented with replacing open-class words with their SFs. Agirre et al. (2008) have shown an improvement in full parse and PP attachment scores with statistical constituency parsers using SFs. Agirre et al. (2011) have followed up on those results and re-trained a dependency parser on the data where words were replaced with their SFs. This resulted in a very modest labeled attachment score improvement, but with a significantly reduced feature set. In a recent HPSG work, MacKinlay et al. (2012) attempted to integrate lexical semantic features, including SF backoff, into a discriminative parse ranking model. However, this was not shown to help, presumably because the lexical semantic features were built from syntactic constituents rather than MRS predicates. The ancestor features found to be helpful here</context>
</contexts>
<marker>Agirre, Bengoetxea, Gojenola, Nivre, 2011</marker>
<rawString>Eneko Agirre, Kepa Bengoetxea, Koldo Gojenola, and Joakim Nivre. 2011. Improving dependency parsing with semantic classes. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 699–703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>A statistical model for parsing and word-sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>155--163</pages>
<publisher>Hong Kong.</publisher>
<contexts>
<context position="20472" citStr="Bikel (2000)" startWordPosition="3432" endWordPosition="3433">ctly to higher accuracy. Other avenues we would like to explore is backing off not to the semantic files, but rather to WordNet hypernyms at various levels. These results show that generalizing to semantic supertypes allows us to build semantic ranking models that are not only smaller, but more accurate. In general, learning time was roughly proportional to the number of features, so a smaller model can be learned faster. We hypothesize that it is the combination of dependencies and supertypes that makes the difference: approaches that used semantic features on phrase structure trees (such as Bikel (2000) and MacKinlay et al. (2012)) have in general failed to get much improvement. Figure 3: Learning curve for SF+AF. The overall accuracy is still quite low, due principally to the lack of training data. We show the learning curves for the SF+AF configuration in Figure 3 (the other configurations are similar). The curve is still clearly rising: the accuracy of parse selection on our corpus is far from saturated. This observation gives us confidence that with a larger corpus the accuracy of parse selection will improve considerably. The learning curve in Fujita et al. (2010) showed similar results</context>
</contexts>
<marker>Bikel, 2000</marker>
<rawString>Daniel M. Bikel. 2000. A statistical model for parsing and word-sense disambiguation. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 155–163. Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Ryan Foster</author>
</authors>
<title>Linking and extending an open multilingual wordnet.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics: ACL2013,</booktitle>
<pages>1352--1362</pages>
<location>Sofia. URL http://aclweb.</location>
<contexts>
<context position="21743" citStr="Bond and Foster, 2013" startWordPosition="3639" endWordPosition="3642">ly with more (they had a larger corpus for Japanese). As there are so far still very few corpora with both structural and lexical semantic annotation, we are currently investigating the use of automatic word sense disambiguation to create the features, in a similar way to Agirre et al. (2008). Finally, we would like to investigate even more features, such as the dependency chains of Le Roux et al. (2012). One exciting possibility is projecting ranking features across languages: wordnet semantic files are the supertypes for all wordnets linked to the Princeton Wordnet, of which there are many (Bond and Foster, 2013). The predicates that are not in the wordnets are generally either named entities or from smallish closed sets of function words such as conjunctions, prepositions and pronouns. We are currently investigating mapping these between Japanese and English using transferrules from an existing machine translation system (Bond et al., 2011). In principal, a small set of mappings for closed class words could allow us to quickly boot-strap a semantic ranking model for any language with a wordnet. 6 Conclusion In summary, we showed some features that help parse selection. In the SD group, LR features to</context>
</contexts>
<marker>Bond, Foster, 2013</marker>
<rawString>Francis Bond and Ryan Foster. 2013. Linking and extending an open multilingual wordnet. In 51st Annual Meeting of the Association for Computational Linguistics: ACL2013, pages 1352–1362. Sofia. URL http://aclweb. org/anthology/P13-1133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Sanae Fujita</author>
<author>Takaaki Tanaka</author>
</authors>
<title>The Hinoki syntactic and semantic treebank of Japanese.</title>
<date>2008</date>
<journal>Language Resources and Evaluation,</journal>
<volume>42</volume>
<issue>2</issue>
<note>URL http://dx.doi.org/ 10.1007/s10579-008-9062-z, (Re-issue of DOI 10.1007/s10579-007-9036-6 as Springer lost the Japanese text).</note>
<contexts>
<context position="1636" citStr="Bond et al., 2008" startWordPosition="237" endWordPosition="240">sky et al., 2006), but very little that uses pure semantic representations. With recent work on building semantic representations (from deep grammars such as LFG (Butt et al., 1999) and HPSG (Sag et al., 1999), directly through lambda calculus, or as in intermediate step in machine translation) the question of ranking them has become more important. The closest related work is Fujita et al. (2010) who ranked parses using semantic features from Minimal Recursion Semantics (MRS) and syntactic trees, using a Maximum Entropy Ranker. They experimented with Japanese data, using the Hinoki Treebank (Bond et al., 2008), using primarily elementary dependencies: single arcs between pred♣Currently at PointInside, Inc. S Figure 1: Syntactic view of sentence “I treat dogs and cats with worms”. icates and their arguments. These can miss some important connections between predicates. An example parse tree for I treat dogs and cats with worms is shown in Figure 1.1, for the interpretation “I treat both dogs and cats that have worms” (not “I treat, using worms, dogs and cats” or any of the other possibilities) The semantic representation we use is Dependency Minimal Recursion Semantics (DRMS: Copestake, 2009). The M</context>
<context position="4285" citStr="Bond et al., 2008" startWordPosition="672" endWordPosition="675"> graph (Section 3.2.3). Finally, we experimented with generalizing over semantic classes. We used WordNet semantic files as supertypes to reduce data sparseness (Section 3.2.4). This will generalize the lexical semantics of the predicates, resulting in a reduction of feature size and ambiguity. 2 Previous Work This paper follows up on the work of Fujita et al. (2010) in ranking MRS semantic representations, which was carried out for Japanese. We are conducting a similar investigation for English, and add new features and approaches. Fujita et al. (2010) worked with the Japanese Hinoki Corpus (Bond et al., 2008) data and used hypernym chains from the Goi-Taikei Japanese ontology (Ikehara et al., 1997) for variable-level semantic backoff. This is in contrast to the uniform WordNet semantic file backoff performed here. In addition, this work only focuses on MRS ranking, whereas Fujita et al. (2010) combined MRS features with syntactic features to improve syntactic parse ranking accuracy. Our use of WordNet Semantic Files (SF) to reduce lexical feature sparseness is inspired by several recent papers. Agirre et al. (2008, 2011) have experimented with replacing open-class words with their SFs. Agirre et a</context>
</contexts>
<marker>Bond, Fujita, Tanaka, 2008</marker>
<rawString>Francis Bond, Sanae Fujita, and Takaaki Tanaka. 2008. The Hinoki syntactic and semantic treebank of Japanese. Language Resources and Evaluation, 42(2):243–251. URL http://dx.doi.org/ 10.1007/s10579-008-9062-z, (Re-issue of DOI 10.1007/s10579-007-9036-6 as Springer lost the Japanese text).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Stephan Oepen</author>
<author>Eric Nichols</author>
<author>Dan Flickinger</author>
<author>Erik Velldal</author>
<author>Petter Haugereid</author>
</authors>
<title>Deep open source machine translation.</title>
<date>2011</date>
<booktitle>Machine Translation, 25(2):87–105. URL http://dx.doi.org/10. 1007/s10590-011-9099-4, (Special Issue on Open source Machine Translation).</booktitle>
<contexts>
<context position="22078" citStr="Bond et al., 2011" startWordPosition="3691" endWordPosition="3694">ore features, such as the dependency chains of Le Roux et al. (2012). One exciting possibility is projecting ranking features across languages: wordnet semantic files are the supertypes for all wordnets linked to the Princeton Wordnet, of which there are many (Bond and Foster, 2013). The predicates that are not in the wordnets are generally either named entities or from smallish closed sets of function words such as conjunctions, prepositions and pronouns. We are currently investigating mapping these between Japanese and English using transferrules from an existing machine translation system (Bond et al., 2011). In principal, a small set of mappings for closed class words could allow us to quickly boot-strap a semantic ranking model for any language with a wordnet. 6 Conclusion In summary, we showed some features that help parse selection. In the SD group, LR features together with AF features achieved a 1.75% improvement in accuracy over the basic Baseline model (25.36% → 27.12%). However, LR feature alone and AF feature alone both decrease the accuracy (25.36% → 25.28% and 25.36% → 24.84%). PR features and combination of PR and AF features both achieved small improvements (0.416% Baseline → Baseli</context>
</contexts>
<marker>Bond, Oepen, Nichols, Flickinger, Velldal, Haugereid, 2011</marker>
<rawString>Francis Bond, Stephan Oepen, Eric Nichols, Dan Flickinger, Erik Velldal, and Petter Haugereid. 2011. Deep open source machine translation. Machine Translation, 25(2):87–105. URL http://dx.doi.org/10. 1007/s10590-011-9099-4, (Special Issue on Open source Machine Translation).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
<author>Tracy Holloway King</author>
<author>Maria-Eugenia Ni˜no</author>
<author>Fr´ed´erique Segond</author>
</authors>
<title>A Grammar Writer’s Cookbook.</title>
<date>1999</date>
<publisher>CSLI publications.</publisher>
<marker>Butt, King, Ni˜no, Segond, 1999</marker>
<rawString>Miriam Butt, Tracy Holloway King, Maria-Eugenia Ni˜no, and Fr´ed´erique Segond. 1999. A Grammar Writer’s Cookbook. CSLI publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1011" citStr="Collins and Koo, 2006" startWordPosition="137" endWordPosition="140">ow that two methods of generalization improved results: extended grand-parenting and supertypes. The models are tested on a subset of SemCor that has been annotated with both Dependency Minimal Recursion Semantic representations and WordNet senses. Using both types of features gives a significant improvement in whole sentence parse selection accuracy over the baseline model. 1 Introduction In this paper we investigate various features to improve the accuracy of semantic parse ranking. There has been considerable successful work on syntactic parse ranking and reranking (Toutanova et al., 2005; Collins and Koo, 2006; McClosky et al., 2006), but very little that uses pure semantic representations. With recent work on building semantic representations (from deep grammars such as LFG (Butt et al., 1999) and HPSG (Sag et al., 1999), directly through lambda calculus, or as in intermediate step in machine translation) the question of ranking them has become more important. The closest related work is Fujita et al. (2010) who ranked parses using semantic features from Minimal Recursion Semantics (MRS) and syntactic trees, using a Maximum Entropy Ranker. They experimented with Japanese data, using the Hinoki Tre</context>
</contexts>
<marker>Collins, Koo, 2006</marker>
<rawString>Michael Collins and Terry Koo. 2006. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>1--9</pages>
<location>Athens.</location>
<contexts>
<context position="2229" citStr="Copestake, 2009" startWordPosition="336" endWordPosition="337">ank (Bond et al., 2008), using primarily elementary dependencies: single arcs between pred♣Currently at PointInside, Inc. S Figure 1: Syntactic view of sentence “I treat dogs and cats with worms”. icates and their arguments. These can miss some important connections between predicates. An example parse tree for I treat dogs and cats with worms is shown in Figure 1.1, for the interpretation “I treat both dogs and cats that have worms” (not “I treat, using worms, dogs and cats” or any of the other possibilities) The semantic representation we use is Dependency Minimal Recursion Semantics (DRMS: Copestake, 2009). The Minimal Recursion Semantics (MRS: Copestake et al., 2005) is a computationally tractable flat semantics that underspecifies quantifier scope. The Dependency MRS is an MRS representation format that keeps all the information from the MRS but is simpler to manipulate. DMRSs differ from syntactic dependency graphs in that the relations are defined between slightly abstract predicates, not between 1Simplified by omission of non-branching nodes. NP VP N PP CONJ and N cats N V NP I treat N N dogs with worms surface forms. Some semantically empty surface tokens (such as infinitive to) are not i</context>
</contexts>
<marker>Copestake, 2009</marker>
<rawString>Ann Copestake. 2009. Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 1–9. Athens.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Ivan A Sag</author>
<author>Carl Pollard</author>
</authors>
<title>Minimal Recursion Semantics. An introduction.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>332</pages>
<contexts>
<context position="2292" citStr="Copestake et al., 2005" startWordPosition="344" endWordPosition="347">ndencies: single arcs between pred♣Currently at PointInside, Inc. S Figure 1: Syntactic view of sentence “I treat dogs and cats with worms”. icates and their arguments. These can miss some important connections between predicates. An example parse tree for I treat dogs and cats with worms is shown in Figure 1.1, for the interpretation “I treat both dogs and cats that have worms” (not “I treat, using worms, dogs and cats” or any of the other possibilities) The semantic representation we use is Dependency Minimal Recursion Semantics (DRMS: Copestake, 2009). The Minimal Recursion Semantics (MRS: Copestake et al., 2005) is a computationally tractable flat semantics that underspecifies quantifier scope. The Dependency MRS is an MRS representation format that keeps all the information from the MRS but is simpler to manipulate. DMRSs differ from syntactic dependency graphs in that the relations are defined between slightly abstract predicates, not between 1Simplified by omission of non-branching nodes. NP VP N PP CONJ and N cats N V NP I treat N N dogs with worms surface forms. Some semantically empty surface tokens (such as infinitive to) are not included, while some predicates are inserted that are not in the</context>
</contexts>
<marker>Copestake, Flickinger, Sag, Pollard, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Ivan A. Sag, and Carl Pollard. 2005. Minimal Recursion Semantics. An introduction. Research on Language and Computation, 3(4):281– 332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
</authors>
<title>On building a more efficient grammar by exploiting types.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<volume>6</volume>
<pages>1--15</pages>
<contexts>
<context position="6295" citStr="Flickinger, 2000" startWordPosition="999" endWordPosition="1000">ntactic parse ranking (Toutanova et al., 2005) and chains in dependency parsing ranking (Le Roux et al., 2012). 3 Resources and Methodology In this section we introduce the corpus we work on, and the features we extract from it. 3.1 Corpus: SemCor To evaluate our ranking methods, we are using the Redwoods Treebank (Oepen et al., 2004) of manually disambiguated HPSG parses, storing full signs for each analysis and supporting export into a variety of formats, including the Dependency MRS (DMRS) format used in this work. The HPSG parses in Redwoods are based on the English Resource Grammar (ERG; Flickinger, 2000) – a hand-crafted broad-coverage HPSG grammar of English. For our experiments, we used a subset of the Redwoods Treebank, consisting of 2,590 sentences drawn from SemCor (Landes et al., 1998). In the SemCor corpus each of the sentences is tagged with WordNet senses created at Princeton University by the WordNet Project research team. The average length of the Redwoods SemCor sentences is 15.4 words, and the average number of parses is 247. From the treebank we can export the DMRS. The choice of which words become predicates is slightly different in the SemCor/WordNet and the ERG. The ERG lexic</context>
</contexts>
<marker>Flickinger, 2000</marker>
<rawString>Dan Flickinger. 2000. On building a more efficient grammar by exploiting types. Natural Language Engineering, 6 (1):15–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanae Fujita</author>
<author>Francis Bond</author>
<author>Takaaki Tanaka</author>
<author>Stephan Oepen</author>
</authors>
<title>Exploiting semantic information for HPSG parse selection.</title>
<date>2010</date>
<journal>Research on Language and Computation,</journal>
<volume>8</volume>
<issue>1</issue>
<pages>11168--010</pages>
<contexts>
<context position="1418" citStr="Fujita et al. (2010)" startWordPosition="204" endWordPosition="207">e investigate various features to improve the accuracy of semantic parse ranking. There has been considerable successful work on syntactic parse ranking and reranking (Toutanova et al., 2005; Collins and Koo, 2006; McClosky et al., 2006), but very little that uses pure semantic representations. With recent work on building semantic representations (from deep grammars such as LFG (Butt et al., 1999) and HPSG (Sag et al., 1999), directly through lambda calculus, or as in intermediate step in machine translation) the question of ranking them has become more important. The closest related work is Fujita et al. (2010) who ranked parses using semantic features from Minimal Recursion Semantics (MRS) and syntactic trees, using a Maximum Entropy Ranker. They experimented with Japanese data, using the Hinoki Treebank (Bond et al., 2008), using primarily elementary dependencies: single arcs between pred♣Currently at PointInside, Inc. S Figure 1: Syntactic view of sentence “I treat dogs and cats with worms”. icates and their arguments. These can miss some important connections between predicates. An example parse tree for I treat dogs and cats with worms is shown in Figure 1.1, for the interpretation “I treat bot</context>
<context position="4036" citStr="Fujita et al. (2010)" startWordPosition="632" endWordPosition="635">t (ARG2: worms). There is no direct link. We investigate new features that make these links more direct (Section 3.2). We also explore the significance of the effectiveness of links between words that are connected arbitrarily far away in the semantic graph (Section 3.2.3). Finally, we experimented with generalizing over semantic classes. We used WordNet semantic files as supertypes to reduce data sparseness (Section 3.2.4). This will generalize the lexical semantics of the predicates, resulting in a reduction of feature size and ambiguity. 2 Previous Work This paper follows up on the work of Fujita et al. (2010) in ranking MRS semantic representations, which was carried out for Japanese. We are conducting a similar investigation for English, and add new features and approaches. Fujita et al. (2010) worked with the Japanese Hinoki Corpus (Bond et al., 2008) data and used hypernym chains from the Goi-Taikei Japanese ontology (Ikehara et al., 1997) for variable-level semantic backoff. This is in contrast to the uniform WordNet semantic file backoff performed here. In addition, this work only focuses on MRS ranking, whereas Fujita et al. (2010) combined MRS features with syntactic features to improve syn</context>
<context position="11287" citStr="Fujita et al. (2010)" startWordPosition="1895" endWordPosition="1898"> catn:1 withp wormn:1 ) 20 (1 andcwithp wormn:1) 21 (2 andc wormn:1) 22 (3 andc wormn:1) Table 3: Features for the DMRS in Fig 2. Baseline features are those that directly reflect the dependencies of the DMRS. In Table 3, feature type (0) (0–2) shows predicates with all their arguments. Feature type (1) (3–8) shows each argument individually. Feature type (2) shows all arguments without the argument types. Feature type (3) is the least specified, showing individual arguments without the labels. These types are the same as the MRS features of Toutanova et al. (2005) and the SEM-DEP features of Fujita et al. (2010). 3.2.1 Conjunctive Features We further create two more features, called Left/Right Handle Features (LR), to link directly the two arguments of conjunctive relations with their parent, independently from the other argument. In Table 1, for example, the feature (treatv:1 ARG2 andc), although valid, does not convey the meaning of the sentence. Instead, we add the two LR features (treatv:1 ARG2 dogn:1) (feature 17) and (treatv:1 ARG2 catn:1) (feature 18), which better model the conjunction relation. 3.2.2 Preposition Role Features As shown in Figure 2, the node withp has two links: to andc (ARG1)</context>
<context position="17786" citStr="Fujita et al. (2010)" startWordPosition="2995" endWordPosition="2998">es Accuracy Features (%) (x1,000) SD-Baseline 25.4 454 SD+LR 25.3 469 SD+PR 25.8 563 SD+LR+PR 25.6 582 SD+AF 24.8 1,430 SD+AF+LR 27.1 1,497 SD+AF+PR 25.8 1,761 SD+AF+LR+PR 26.3 1,842 Table 6: Parse selection results with SD. Features Accuracy Features (%) (x1,000) SF-Baseline 25.0 223 SF+LR 25.1 235 SF+PR 26.3 306 SF+LR+PR 26.3 321 SF+AF 28.2 1,051 SF+AF+LR 28.0 1,101 SF+AF+PR 28.1 1,310 SF+AF+LR+PR 27.7 1,375 Table 7: Parse selection results with SF. We used TADM (Toolkit for Advanced Discriminative Modeling; Malouf, 2002) for the training and testing of our machine learning model, following Fujita et al. (2010). We carried out 10-fold cross-validation for evaluation. We measured the parse selection accuracy at the sentence level. A parse was considered correct only when all the dependencies of the parse are correct. The results of parse selection based on SD and SF representations are shown in Tables 6 and 7. The addition of the ancestor features (AF) gives the most increase in the parse selection accuracy. This result indicates that indirect dependencies as well as direct dependencies in a successful parse frequently appear in other active parses. Second, the SF representation shows better results </context>
<context position="21049" citStr="Fujita et al. (2010)" startWordPosition="3526" endWordPosition="3529">hrase structure trees (such as Bikel (2000) and MacKinlay et al. (2012)) have in general failed to get much improvement. Figure 3: Learning curve for SF+AF. The overall accuracy is still quite low, due principally to the lack of training data. We show the learning curves for the SF+AF configuration in Figure 3 (the other configurations are similar). The curve is still clearly rising: the accuracy of parse selection on our corpus is far from saturated. This observation gives us confidence that with a larger corpus the accuracy of parse selection will improve considerably. The learning curve in Fujita et al. (2010) showed similar results for the same amount of data, and increased rapidly with more (they had a larger corpus for Japanese). As there are so far still very few corpora with both structural and lexical semantic annotation, we are currently investigating the use of automatic word sense disambiguation to create the features, in a similar way to Agirre et al. (2008). Finally, we would like to investigate even more features, such as the dependency chains of Le Roux et al. (2012). One exciting possibility is projecting ranking features across languages: wordnet semantic files are the supertypes for</context>
</contexts>
<marker>Fujita, Bond, Tanaka, Oepen, 2010</marker>
<rawString>Sanae Fujita, Francis Bond, Takaaki Tanaka, and Stephan Oepen. 2010. Exploiting semantic information for HPSG parse selection. Research on Language and Computation, 8(1):1–22. URL http://dx.doi.org/10.1007/ s11168-010-9069-7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Masahiro Miyazaki</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1997</date>
<booktitle>Goi-Taikei — A Japanese Lexicon. Iwanami Shoten,</booktitle>
<volume>5</volume>
<pages>volumes/CDROM.</pages>
<location>Tokyo.</location>
<contexts>
<context position="4376" citStr="Ikehara et al., 1997" startWordPosition="686" endWordPosition="689">. We used WordNet semantic files as supertypes to reduce data sparseness (Section 3.2.4). This will generalize the lexical semantics of the predicates, resulting in a reduction of feature size and ambiguity. 2 Previous Work This paper follows up on the work of Fujita et al. (2010) in ranking MRS semantic representations, which was carried out for Japanese. We are conducting a similar investigation for English, and add new features and approaches. Fujita et al. (2010) worked with the Japanese Hinoki Corpus (Bond et al., 2008) data and used hypernym chains from the Goi-Taikei Japanese ontology (Ikehara et al., 1997) for variable-level semantic backoff. This is in contrast to the uniform WordNet semantic file backoff performed here. In addition, this work only focuses on MRS ranking, whereas Fujita et al. (2010) combined MRS features with syntactic features to improve syntactic parse ranking accuracy. Our use of WordNet Semantic Files (SF) to reduce lexical feature sparseness is inspired by several recent papers. Agirre et al. (2008, 2011) have experimented with replacing open-class words with their SFs. Agirre et al. (2008) have shown an improvement in full parse and PP attachment scores with statistical</context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei — A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 volumes/CDROM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shari Landes</author>
<author>Claudia Leacock</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Building semantic concordances.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database, chapter 8,</booktitle>
<pages>199--216</pages>
<editor>In Christine Fellbaum, editor,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6486" citStr="Landes et al., 1998" startWordPosition="1028" endWordPosition="1031">on, and the features we extract from it. 3.1 Corpus: SemCor To evaluate our ranking methods, we are using the Redwoods Treebank (Oepen et al., 2004) of manually disambiguated HPSG parses, storing full signs for each analysis and supporting export into a variety of formats, including the Dependency MRS (DMRS) format used in this work. The HPSG parses in Redwoods are based on the English Resource Grammar (ERG; Flickinger, 2000) – a hand-crafted broad-coverage HPSG grammar of English. For our experiments, we used a subset of the Redwoods Treebank, consisting of 2,590 sentences drawn from SemCor (Landes et al., 1998). In the SemCor corpus each of the sentences is tagged with WordNet senses created at Princeton University by the WordNet Project research team. The average length of the Redwoods SemCor sentences is 15.4 words, and the average number of parses is 247. From the treebank we can export the DMRS. The choice of which words become predicates is slightly different in the SemCor/WordNet and the ERG. The ERG lexicon groups together all senses that have the same syntactic properties, making them underspecified for many sense differences. Thus elementary predicate catn.1 could be any of the WordNet sens</context>
<context position="8969" citStr="Landes et al., 1998" startWordPosition="1504" endWordPosition="1507">oses a single predicate into multiple predicates (e.g. here into inp thisq placen). The ERG and WordNet also often make different decisions about what constitutes a multiword expression. For these reasons the mapping between the two annotations is not always straightforward. In this paper we use the mapping between the DRMS and WordNet annotations produced by Pozen (2013). Using the mapping, we exploited the sense tagging of the SemCor in several ways. We experimented both with replacing elementary predicates with their synsets, their hypernyms at various levels and with their semantic files (Landes et al., 1998), which generalize the meanings of words that belong to the same broad semantic categories.3 These dozens of generalized semantic tags help to address the issue of feature sparseness, compared to thousands of synsets. We show the semantic files for nouns and verbs in Tables 1 and 2. In this paper, we only report on the parse selection accuracy using semantic files to reduce ambiguity, as it gave the best results. 3.2 Semantic Dependency Features In this section we introduce the baseline features for parse ranking. Table 3 shows example features extracted from the DMRS depicted in Figure 2.Feat</context>
</contexts>
<marker>Landes, Leacock, Fellbaum, 1998</marker>
<rawString>Shari Landes, Claudia Leacock, and Christiane Fellbaum. 1998. Building semantic concordances. In Christine Fellbaum, editor, WordNet: An Electronic Lexical Database, chapter 8, pages 199–216. MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joseph Le Roux</author>
<author>Benoit Favre</author>
<author>Alexis Nasr</author>
<author>Seyed Abolghasem Mirroshandel</author>
</authors>
<title>Generative constituent parsing and discriminative dependency reranking: Experiments on english and french.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing ofMorphologically Rich Languages,</booktitle>
<pages>89--99</pages>
<marker>Le Roux, Favre, Nasr, Mirroshandel, 2012</marker>
<rawString>Joseph Le Roux, Benoit Favre, Alexis Nasr, and Seyed Abolghasem Mirroshandel. 2012. Generative constituent parsing and discriminative dependency reranking: Experiments on english and french. In Proceedings of the ACL 2012 Joint Workshop on Statistical Parsing and Semantic Processing ofMorphologically Rich Languages, pages 89–99. Association for Computational Linguistics, Jeju, Republic of Korea. URL http://www.aclweb.org/ anthology/W12-3412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew MacKinlay</author>
<author>Rebecca Dridan</author>
<author>Diana McCarthy</author>
<author>Timothy Baldwin</author>
</authors>
<title>The effects of semantic annotations on precision parse ranking.</title>
<date>2012</date>
<booktitle>In SEM 2012: The First Joint Conference on Lexical and Computational Semantics-,</booktitle>
<volume>2</volume>
<pages>228--236</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5314" citStr="MacKinlay et al. (2012)" startWordPosition="840" endWordPosition="843">tic Files (SF) to reduce lexical feature sparseness is inspired by several recent papers. Agirre et al. (2008, 2011) have experimented with replacing open-class words with their SFs. Agirre et al. (2008) have shown an improvement in full parse and PP attachment scores with statistical constituency parsers using SFs. Agirre et al. (2011) have followed up on those results and re-trained a dependency parser on the data where words were replaced with their SFs. This resulted in a very modest labeled attachment score improvement, but with a significantly reduced feature set. In a recent HPSG work, MacKinlay et al. (2012) attempted to integrate lexical semantic features, including SF backoff, into a discriminative parse ranking model. However, this was not shown to help, presumably because the lexical semantic features were built from syntactic constituents rather than MRS predicates. The ancestor features found to be helpful here are inspired by the use of grand-parenting in syntactic parse ranking (Toutanova et al., 2005) and chains in dependency parsing ranking (Le Roux et al., 2012). 3 Resources and Methodology In this section we introduce the corpus we work on, and the features we extract from it. 3.1 Cor</context>
<context position="20500" citStr="MacKinlay et al. (2012)" startWordPosition="3435" endWordPosition="3438">curacy. Other avenues we would like to explore is backing off not to the semantic files, but rather to WordNet hypernyms at various levels. These results show that generalizing to semantic supertypes allows us to build semantic ranking models that are not only smaller, but more accurate. In general, learning time was roughly proportional to the number of features, so a smaller model can be learned faster. We hypothesize that it is the combination of dependencies and supertypes that makes the difference: approaches that used semantic features on phrase structure trees (such as Bikel (2000) and MacKinlay et al. (2012)) have in general failed to get much improvement. Figure 3: Learning curve for SF+AF. The overall accuracy is still quite low, due principally to the lack of training data. We show the learning curves for the SF+AF configuration in Figure 3 (the other configurations are similar). The curve is still clearly rising: the accuracy of parse selection on our corpus is far from saturated. This observation gives us confidence that with a larger corpus the accuracy of parse selection will improve considerably. The learning curve in Fujita et al. (2010) showed similar results for the same amount of data</context>
</contexts>
<marker>MacKinlay, Dridan, McCarthy, Baldwin, 2012</marker>
<rawString>Andrew MacKinlay, Rebecca Dridan, Diana McCarthy, and Timothy Baldwin. 2012. The effects of semantic annotations on precision parse ranking. In SEM 2012: The First Joint Conference on Lexical and Computational Semantics-, volume 2, pages 228–236. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In CONLL-2002,</booktitle>
<pages>49--55</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="17695" citStr="Malouf, 2002" startWordPosition="2980" endWordPosition="2981"> as that of the active one, the resulting merged parse was treated as active. Features Accuracy Features (%) (x1,000) SD-Baseline 25.4 454 SD+LR 25.3 469 SD+PR 25.8 563 SD+LR+PR 25.6 582 SD+AF 24.8 1,430 SD+AF+LR 27.1 1,497 SD+AF+PR 25.8 1,761 SD+AF+LR+PR 26.3 1,842 Table 6: Parse selection results with SD. Features Accuracy Features (%) (x1,000) SF-Baseline 25.0 223 SF+LR 25.1 235 SF+PR 26.3 306 SF+LR+PR 26.3 321 SF+AF 28.2 1,051 SF+AF+LR 28.0 1,101 SF+AF+PR 28.1 1,310 SF+AF+LR+PR 27.7 1,375 Table 7: Parse selection results with SF. We used TADM (Toolkit for Advanced Discriminative Modeling; Malouf, 2002) for the training and testing of our machine learning model, following Fujita et al. (2010). We carried out 10-fold cross-validation for evaluation. We measured the parse selection accuracy at the sentence level. A parse was considered correct only when all the dependencies of the parse are correct. The results of parse selection based on SD and SF representations are shown in Tables 6 and 7. The addition of the ancestor features (AF) gives the most increase in the parse selection accuracy. This result indicates that indirect dependencies as well as direct dependencies in a successful parse fr</context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In CONLL-2002, pages 49–55. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In 44th Annual Meeting of the Association for Computational Linguistics and 21st International Conference on Computational Linguistics: COLING/ACL-2006,</booktitle>
<pages>337--344</pages>
<contexts>
<context position="1035" citStr="McClosky et al., 2006" startWordPosition="141" endWordPosition="144">generalization improved results: extended grand-parenting and supertypes. The models are tested on a subset of SemCor that has been annotated with both Dependency Minimal Recursion Semantic representations and WordNet senses. Using both types of features gives a significant improvement in whole sentence parse selection accuracy over the baseline model. 1 Introduction In this paper we investigate various features to improve the accuracy of semantic parse ranking. There has been considerable successful work on syntactic parse ranking and reranking (Toutanova et al., 2005; Collins and Koo, 2006; McClosky et al., 2006), but very little that uses pure semantic representations. With recent work on building semantic representations (from deep grammars such as LFG (Butt et al., 1999) and HPSG (Sag et al., 1999), directly through lambda calculus, or as in intermediate step in machine translation) the question of ranking them has become more important. The closest related work is Fujita et al. (2010) who ranked parses using semantic features from Minimal Recursion Semantics (MRS) and syntactic trees, using a Maximum Entropy Ranker. They experimented with Japanese data, using the Hinoki Treebank (Bond et al., 2008</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Reranking and self-training for parser adaptation. In 44th Annual Meeting of the Association for Computational Linguistics and 21st International Conference on Computational Linguistics: COLING/ACL-2006, pages 337–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Dan Flickinger</author>
<author>Kristina Toutanova</author>
<author>Christoper D Manning</author>
</authors>
<title>LinGO Redwoods: A rich and dynamic treebank for HPSG.</title>
<date>2004</date>
<journal>Research on Language and Computation,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="6014" citStr="Oepen et al., 2004" startWordPosition="953" endWordPosition="956">iscriminative parse ranking model. However, this was not shown to help, presumably because the lexical semantic features were built from syntactic constituents rather than MRS predicates. The ancestor features found to be helpful here are inspired by the use of grand-parenting in syntactic parse ranking (Toutanova et al., 2005) and chains in dependency parsing ranking (Le Roux et al., 2012). 3 Resources and Methodology In this section we introduce the corpus we work on, and the features we extract from it. 3.1 Corpus: SemCor To evaluate our ranking methods, we are using the Redwoods Treebank (Oepen et al., 2004) of manually disambiguated HPSG parses, storing full signs for each analysis and supporting export into a variety of formats, including the Dependency MRS (DMRS) format used in this work. The HPSG parses in Redwoods are based on the English Resource Grammar (ERG; Flickinger, 2000) – a hand-crafted broad-coverage HPSG grammar of English. For our experiments, we used a subset of the Redwoods Treebank, consisting of 2,590 sentences drawn from SemCor (Landes et al., 1998). In the SemCor corpus each of the sentences is tagged with WordNet senses created at Princeton University by the WordNet Projec</context>
</contexts>
<marker>Oepen, Flickinger, Toutanova, Manning, 2004</marker>
<rawString>Stephan Oepen, Dan Flickinger, Kristina Toutanova, and Christoper D. Manning. 2004. LinGO Redwoods: A rich and dynamic treebank for HPSG. Research on Language and Computation, 2(4):575–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zinaida Pozen</author>
</authors>
<title>Using Lexical and Compositional Semantics to Improve HPSG Parse Selection. Master’s thesis,</title>
<date>2013</date>
<institution>University of Washington.</institution>
<note>URL https://digital.lib.washington.edu/ researchworks/handle/1773/%23469.</note>
<contexts>
<context position="8723" citStr="Pozen (2013)" startWordPosition="1464" endWordPosition="1465">n processn quantityn relationn shapen staten substancen timen Table 1: WordNet Noun Semantic Files. ally having thick soft fur and no ability to roar”, catn:2 “an informal term for a youth or man” and six more.2 In some cases, DMRS decomposes a single predicate into multiple predicates (e.g. here into inp thisq placen). The ERG and WordNet also often make different decisions about what constitutes a multiword expression. For these reasons the mapping between the two annotations is not always straightforward. In this paper we use the mapping between the DRMS and WordNet annotations produced by Pozen (2013). Using the mapping, we exploited the sense tagging of the SemCor in several ways. We experimented both with replacing elementary predicates with their synsets, their hypernyms at various levels and with their semantic files (Landes et al., 1998), which generalize the meanings of words that belong to the same broad semantic categories.3 These dozens of generalized semantic tags help to address the issue of feature sparseness, compared to thousands of synsets. We show the semantic files for nouns and verbs in Tables 1 and 2. In this paper, we only report on the parse selection accuracy using se</context>
</contexts>
<marker>Pozen, 2013</marker>
<rawString>Zinaida Pozen. 2013. Using Lexical and Compositional Semantics to Improve HPSG Parse Selection. Master’s thesis, University of Washington. URL https://digital.lib.washington.edu/ researchworks/handle/1773/%23469.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Tom Wasow</author>
<author>Emily Bender</author>
</authors>
<title>Syntactic Theory: A Formal Introduction.</title>
<date>1999</date>
<publisher>CSLI Publications,</publisher>
<location>Stanford,</location>
<note>second edition.</note>
<contexts>
<context position="1227" citStr="Sag et al., 1999" startWordPosition="173" endWordPosition="176">ntations and WordNet senses. Using both types of features gives a significant improvement in whole sentence parse selection accuracy over the baseline model. 1 Introduction In this paper we investigate various features to improve the accuracy of semantic parse ranking. There has been considerable successful work on syntactic parse ranking and reranking (Toutanova et al., 2005; Collins and Koo, 2006; McClosky et al., 2006), but very little that uses pure semantic representations. With recent work on building semantic representations (from deep grammars such as LFG (Butt et al., 1999) and HPSG (Sag et al., 1999), directly through lambda calculus, or as in intermediate step in machine translation) the question of ranking them has become more important. The closest related work is Fujita et al. (2010) who ranked parses using semantic features from Minimal Recursion Semantics (MRS) and syntactic trees, using a Maximum Entropy Ranker. They experimented with Japanese data, using the Hinoki Treebank (Bond et al., 2008), using primarily elementary dependencies: single arcs between pred♣Currently at PointInside, Inc. S Figure 1: Syntactic view of sentence “I treat dogs and cats with worms”. icates and their </context>
</contexts>
<marker>Sag, Wasow, Bender, 1999</marker>
<rawString>Ivan A. Sag, Tom Wasow, and Emily Bender. 1999. Syntactic Theory: A Formal Introduction. CSLI Publications, Stanford, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Stochastic HPSG parse disambiguation using the Redwoods corpus.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="988" citStr="Toutanova et al., 2005" startWordPosition="133" endWordPosition="136">entations of text. We show that two methods of generalization improved results: extended grand-parenting and supertypes. The models are tested on a subset of SemCor that has been annotated with both Dependency Minimal Recursion Semantic representations and WordNet senses. Using both types of features gives a significant improvement in whole sentence parse selection accuracy over the baseline model. 1 Introduction In this paper we investigate various features to improve the accuracy of semantic parse ranking. There has been considerable successful work on syntactic parse ranking and reranking (Toutanova et al., 2005; Collins and Koo, 2006; McClosky et al., 2006), but very little that uses pure semantic representations. With recent work on building semantic representations (from deep grammars such as LFG (Butt et al., 1999) and HPSG (Sag et al., 1999), directly through lambda calculus, or as in intermediate step in machine translation) the question of ranking them has become more important. The closest related work is Fujita et al. (2010) who ranked parses using semantic features from Minimal Recursion Semantics (MRS) and syntactic trees, using a Maximum Entropy Ranker. They experimented with Japanese dat</context>
<context position="5724" citStr="Toutanova et al., 2005" startWordPosition="903" endWordPosition="906"> the data where words were replaced with their SFs. This resulted in a very modest labeled attachment score improvement, but with a significantly reduced feature set. In a recent HPSG work, MacKinlay et al. (2012) attempted to integrate lexical semantic features, including SF backoff, into a discriminative parse ranking model. However, this was not shown to help, presumably because the lexical semantic features were built from syntactic constituents rather than MRS predicates. The ancestor features found to be helpful here are inspired by the use of grand-parenting in syntactic parse ranking (Toutanova et al., 2005) and chains in dependency parsing ranking (Le Roux et al., 2012). 3 Resources and Methodology In this section we introduce the corpus we work on, and the features we extract from it. 3.1 Corpus: SemCor To evaluate our ranking methods, we are using the Redwoods Treebank (Oepen et al., 2004) of manually disambiguated HPSG parses, storing full signs for each analysis and supporting export into a variety of formats, including the Dependency MRS (DMRS) format used in this work. The HPSG parses in Redwoods are based on the English Resource Grammar (ERG; Flickinger, 2000) – a hand-crafted broad-cover</context>
<context position="11238" citStr="Toutanova et al. (2005)" startWordPosition="1886" endWordPosition="1889">treatv:1 ARG2 catn:1) 19 ( 0 andc L-IND dogn:1 R-IND catn:1 withp wormn:1 ) 20 (1 andcwithp wormn:1) 21 (2 andc wormn:1) 22 (3 andc wormn:1) Table 3: Features for the DMRS in Fig 2. Baseline features are those that directly reflect the dependencies of the DMRS. In Table 3, feature type (0) (0–2) shows predicates with all their arguments. Feature type (1) (3–8) shows each argument individually. Feature type (2) shows all arguments without the argument types. Feature type (3) is the least specified, showing individual arguments without the labels. These types are the same as the MRS features of Toutanova et al. (2005) and the SEM-DEP features of Fujita et al. (2010). 3.2.1 Conjunctive Features We further create two more features, called Left/Right Handle Features (LR), to link directly the two arguments of conjunctive relations with their parent, independently from the other argument. In Table 1, for example, the feature (treatv:1 ARG2 andc), although valid, does not convey the meaning of the sentence. Instead, we add the two LR features (treatv:1 ARG2 dogn:1) (feature 17) and (treatv:1 ARG2 catn:1) (feature 18), which better model the conjunction relation. 3.2.2 Preposition Role Features As shown in Figur</context>
</contexts>
<marker>Toutanova, Manning, Flickinger, Oepen, 2005</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, Dan Flickinger, and Stephan Oepen. 2005. Stochastic HPSG parse disambiguation using the Redwoods corpus. Research on Language and Computation, 3(1):83–105.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
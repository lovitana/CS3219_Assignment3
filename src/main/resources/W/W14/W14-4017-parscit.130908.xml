<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.981985">
Evaluating Word Order Recursively over Permutation-Forests
</title>
<author confidence="0.992548">
Miloˇs Stanojevi´c and Khalil Sima’an
</author>
<affiliation confidence="0.997423">
Institute for Logic, Language and Computation
University of Amsterdam
</affiliation>
<address confidence="0.928254">
Science Park 107, 1098 XG Amsterdam, The Netherlands
</address>
<email confidence="0.99812">
{m.stanojevic,k.simaan}@uva.nl
</email>
<sectionHeader confidence="0.997377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999971870967742">
Automatically evaluating word order of
MT system output at the sentence-level is
challenging. At the sentence-level, ngram
counts are rather sparse which makes it
difficult to measure word order quality ef-
fectively using lexicalized units. Recent
approaches abstract away from lexicaliza-
tion by assigning a score to the permuta-
tion representing how word positions in
system output move around relative to a
reference translation. Metrics over per-
mutations exist (e.g., Kendal tau or Spear-
man Rho) and have been shown to be
useful in earlier work. However, none
of the existing metrics over permutations
groups word positions recursively into
larger phrase-like blocks, which makes it
difficult to account for long-distance re-
ordering phenomena. In this paper we ex-
plore novel metrics computed over Per-
mutation Forests (PEFs), packed charts
of Permutation Trees (PETs), which are
tree decompositions of a permutation into
primitive ordering units. We empirically
compare PEFs metric against five known
reordering metrics on WMT13 data for ten
language pairs. The PEFs metric shows
better correlation with human ranking than
the other metrics almost on all language
pairs. None of the other metrics exhibits
as stable behavior across language pairs.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999697926829268">
Evaluating word order (also reordering) in MT is
one of the main ingredients in automatic MT eval-
uation, e.g., (Papineni et al., 2002; Denkowski
and Lavie, 2011). To monitor progress on eval-
uating reordering, recent work explores dedicated
reordering evaluation metrics, cf. (Birch and Os-
borne, 2011; Isozaki et al., 2010; Talbot et al.,
2011). Existing work computes the correlation be-
tween the ranking of the outputs of different sys-
tems by an evaluation metric to human ranking, on
e.g., the WMT evaluation data.
For evaluating reordering, it is necessary to
word align system output with the correspond-
ing reference translation. For convenience, a 1:1
alignment (a permutation) is induced between the
words on both sides (Birch and Osborne, 2011),
possibly leaving words unaligned on either side.
Existing work then concentrates on defining mea-
sures of reordering over permutations, cf. (Lap-
ata, 2006; Birch and Osborne, 2011; Isozaki et al.,
2010; Talbot et al., 2011). Popular metrics over
permutations are: Kendall’s tau, Spearman, Ham-
ming distance, Ulam and Fuzzy score. These met-
rics treat a permutation as a flat sequence of inte-
gers or blocks, disregarding the possibility of hier-
archical grouping into phrase-like units, making it
difficult to measure long-range order divergence.
Next we will show by example that permutations
also contain latent atomic units that govern the re-
cursive reordering of phrase-like units. Account-
ing for these latent reorderings could actually be
far simpler than the flat view of a permutation.
Isozaki et al. (2010) argue that the conventional
metrics cannot measure well the long distance
reordering between an English reference sentence
“A because B” and a Japanese-English hypothesis
translation “B because A”, where A and B are
blocks of any length with internal monotonic
alignments. In this paper we explore the idea of
factorizing permutations into permutation-trees
(PETs) (Gildea et al., 2006) and defining new
</bodyText>
<page confidence="0.96896">
138
</page>
<note confidence="0.9210565">
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 138–147,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999932">
Figure 1: A permutation tree for (2, 4, 5, 6, 1, 3)
</figureCaption>
<bodyText confidence="0.999921416666667">
tree-based reordering metrics which aims at
dealing with this type of long range reorderings.
For the Isozaki et al. (2010) Japanese-English
example, there are two PETs (when leaving A and
B as encapsulated blocks):
Our PET-based metrics interpolate the scores over
the two inversion operators (2, 1) with the internal
scores for A and B, incorporating a weight
for subtree height. If both A and B are large
blocks, internally monotonically (also known as
straight) aligned, then our measure will not count
every single reordering of a word in A or B,
but will consider this case as block reordering.
From a PET perspective, the distance of the
reordering is far smaller than when looking at a
flat permutation. But does this hierarchical view
of reordering cohere better with human judgement
than string-based metrics?
The example above also shows that a permuta-
tion may factorize into different PETs, each corre-
sponding to a different segmentation of a sentence
pair into phrase-pairs. In this paper we introduce
permutation forests (PEFs); a PEF is a hypergraph
that compactly packs the set of PETs that factorize
a permutation.
There is yet a more profoud reasoning behind
PETs than only accounting for long-range reorder-
ings. The example in Figure 1 gives the flavor of
PETs. Observe how every internal node in this
PET dominates a subtree whose fringe1 is itself a
permutation over an integer sub-range of the orig-
inal permutation. Every node is decorated with a
permutation over the child positions (called oper-
ator). For example (4, 5, 6) constitutes a contigu-
ous range of integers (corresponding to a phrase
pair), and hence will be grouped into a subtree;
</bodyText>
<subsectionHeader confidence="0.630921">
1Ordered sequence of leaf nodes.
</subsectionHeader>
<bodyText confidence="0.998304484848485">
which in turn can be internally re-grouped into a
binary branching subtree. Every node in a PET is
minimum branching, i.e., the permutation factor-
izes into a minimum number of adjacent permuta-
tions over integer sub-ranges (Albert and Atkin-
son, 2005). The node operators in a PET are
known to be the atomic building blocks of all per-
mutations (called primal permutations). Because
these are building atomic units of reordering, it
makes sense to want to measure reordering as a
function of the individual cost of these operators.
In this work we propose to compute new reorder-
ing measures that aggregate over the individual
node-permutations in these PETs.
While PETs where exploited rather recently for
extracting features used in the BEER metric sys-
tem description (Stanojevi´c and Sima’an, 2014) in
the official WMT 2014 competition, this work is
the first to propose integral recursive metrics over
PETs and PEFs solely for measuring reordering
(as opposed to individual non-recursive features in
a full metric that measures at the same time both
fluency and adequacy). We empirically show that
a PEF-based evaluation measure correlates better
with human rankings than the string-based mea-
sures on eight of the ten language pairs in WMT13
data. For the 9th language pair it is close to best,
and for the 10th (English-Czech) we find a likely
explanation in the Findings of the 2013 WMT (Bo-
jar et al., 2013). Crucially, the PEF-based mea-
sure shows more stable ranking across language
pairs than any of the other measures. The metric
is available online as free software2.
</bodyText>
<sectionHeader confidence="0.90688" genericHeader="introduction">
2 Measures on permutations: Baselines
</sectionHeader>
<bodyText confidence="0.999628066666667">
In (Birch and Osborne, 2010; Birch and Osborne,
2011) Kendall’s tau and Hamming distance are
combined with unigram BLEU (BLEU-1) leading
to LRscore showing better correlation with human
judgment than BLEU-4. Birch et al. (2010) ad-
ditionally tests Ulam distance (longest common
subsequence – LCS – normalized by the permu-
tation length) and the square root of Kendall’s tau.
Isozaki et al. (2010) presents a similar approach
to (Birch and Osborne, 2011) additionally test-
ing Spearman rho as a distance measure. Talbot
et al. (2011) extracts a reordering measure from
METEOR (Denkowski and Lavie, 2011) dubbed
Fuzzy Reordering Score and evaluates it on MT
reordering quality.
</bodyText>
<footnote confidence="0.805093">
2https://github.com/stanojevic/beer
</footnote>
<figure confidence="0.98606625">
h2,4,1,3i
2 h1,2i
4 h1,2i
5 6
1 3
h2,1i
A h2,1i
because B
h2,1i
A because
B
h2,1i
</figure>
<page confidence="0.996485">
139
</page>
<bodyText confidence="0.989230894736842">
For an evaluation metric we need a function
which would have the standard behaviour of evalu-
ation metrics - the higher the score the better. Bel-
low we define the baseline metrics that were used
in our experiments.
Baselines A permutation over [1..n] (subrange
of the positive integers where n &gt; 1) is a bijective
function from [1..n] to itself. To represent permu-
tations we will use angle brackets as in (2, 4, 3, 1).
Given a permutation 7r over [1..n], the notation 7ri
(1 &lt; i &lt; n) stands for the integer in the ith posi-
tion in 7r; 7r(i) stands for the index of the position
in 7r where integer i appears; and 7rji stands for the
(contiguous) sub-sequence of integers 7ri, ... 7rj.
The definitions of five commonly used met-
rics over permutations are shown in Figure 2.
In these definitions, we use LC5 to stand for
Longest Common Subsequence, and Kronecker
S[a] which is 1 if (a == true) else zero, and
</bodyText>
<equation confidence="0.987023666666667">
An1 = (1, · · · , n) which is the identity permuta-
tion over [1..n]. We note that all existing metrics
spearman(7r) = 1 n(n2 − 1)
LC5(7r, An 1) − 1
ulam(7r) =
n − 1
c − 1
fuzzy(7r) = 1 −
n − 1
</equation>
<bodyText confidence="0.712725">
where c is # of monotone sub-permutations
</bodyText>
<figureCaption confidence="0.9104095">
Figure 2: Five commonly used metrics over per-
mutations
</figureCaption>
<bodyText confidence="0.99966825">
are defined directly over flat string-level permuta-
tions. In the next section we present an alternative
view of permutations are compositional, recursive
tree structures.
</bodyText>
<sectionHeader confidence="0.973283" genericHeader="method">
3 Measures on Permutation Forests
</sectionHeader>
<bodyText confidence="0.999953545454546">
Existing work, e.g., (Gildea et al., 2006), shows
how to factorize any permutation 7r over [1..n]
into a canonical permutation tree (PET). Here we
will summarize the relevant aspects and extend
PETs to permutation forests (PEFs).
A non-empty sub-sequence 7rji of a permutation
7r is isomorphic with a permutation over [1..(j −
i + 1)] iff the set {7ri, ... , 7rj} is a contiguous
range of positive integers. We will use the term
a sub-permutation of 7r to refer to a subsequence
of 7r that is isomorphic with a permutation. Note
that not every subsequence of a permutation 7r is
necessarily isomorphic with a permutation, e.g.,
the subsequence (3, 5) of (1, 2, 3, 5, 4) is not a
sub-permutation. One sub-permutation 7r1 of 7r is
smaller than another sub-permutation 7r2 of 7r iff
every integer in 7r1 is smaller than all integers in
7r2. In this sense we can put a full order on non-
overlapping sub-permutations of 7r and rank them
from the smallest to the largest.
For every permutation 7r there is a minimum
number of adjacent sub-permutations it can be fac-
torized into (see e.g., (Gildea et al., 2006)). We
will call this minimum number the arity of 7r and
denote it with a(7r) (or simply a when 7r is un-
derstood from the context). For example, the arity
of 7r = (5, 7, 4, 6, 3, 1, 2) is a = 2 because it can
be split into a minimum of two sub-permutations
(Figure 3), e.g. (5, 7, 4, 6, 3) and (1, 2) (but alter-
natively also (5, 7, 4, 6) and (3, 1,2)). In contrast,
7r = (2, 4, 1, 3) (also known as the Wu (1997) per-
mutation) cannot be split into less than four sub-
permutations, i.e., a = 4. Factorization can be
applied recursively to the sub-permutations of 7r,
resulting in a tree structure (see Figure 3) called a
permutation tree (PET) (Gildea et al., 2006; Zhang
and Gildea, 2007; Maillette de Buy Wenniger and
Sima’an, 2011).
Some permutations factorize into multiple alter-
native PETs. For 7r = (4, 3, 2,1) there are five
PETs shown in Figure 3. The alternative PETs
can be packed into an O(n2) permutation forest
(PEF). For many computational purposes, a sin-
gle canonical PET is sufficient, cf. (Gildea et al.,
2006). However, while different PETs of 7r exhibit
the same reordering pattern, their different binary
branching structures might indicate important dif-
ferences as we show in our experiments.
A permutation forest (akin to a parse forest)
F for 7r (over [1..n]) is a data structure consisting
of a subset of {[[i, j, Iji , Oji ]] 10 &lt; i &lt; j &lt; n},
where Iji is a (possibly empty) set of inferences
(sets of split points) for 7rji+1 and Oji is an oper-
ator shared by all inferences of 7rji+1. If 7rji+1 is
a sub-permutation and it has arity a &lt; (j − (i +
</bodyText>
<equation confidence="0.992853571428571">
(n2 − n)/2
En i=1 S[7ri == i]
hamming(7r) =
En−1 1 En i+1 S[7r(i) &lt; 7r(j)]
kendall(7r) = i=
n
3En i=1(7ri − i)2
</equation>
<page confidence="0.970465">
140
</page>
<figure confidence="0.999762846153846">
(2,1)
(2,1)
(2,4,1,3)
5 7 4 6
(1,2)
3
1 2
(2,1)
4 (2,1)
3 (2,1)
2 1
(2,1)
4 (2,1)
(2,1)
3 2
(2,1) (2,1)
(2,1) (2,1) (2,1)
1 4 3 2 1 (2,1)
4 3
(2,1)
(2,1)
4 (2,1)
3 2
1
2
1
</figure>
<figureCaption confidence="0.999829">
Figure 3: A PET for 7r = h5, 7, 4, 6, 3, 1, 2i. And five different PETs for 7r = h4, 3, 2, 1i.
</figureCaption>
<bodyText confidence="0.998814428571429">
1)), then each inference consists of a a − 1-tuple
[l1, ... , la−1], where for each 1 ≤ x ≤ (a − 1), lx
is a “split point” which is given by the index of the
last integer in the xth sub-permutation in 7r. The
permutation of the a sub-permutations (“children”
of 7rji+1) is stored in Oji and it is the same for all
inferences of that span (Zhang et al., 2008).
</bodyText>
<figureCaption confidence="0.997895">
Figure 4: The factorizations of 7r = h4, 3, 2, 1i.
</figureCaption>
<bodyText confidence="0.998608441176471">
Let us exemplify the inferences on 7r =
h4, 3, 2,1i (see Figure 4) which factorizes into
pairs of sub-permutations (a = 2): a split point
can be at positions with index l1 ∈ {1, 2, 3}.
Each of these split points (factorizations) of 7r will
be represented as an inference for the same root
node which covers the whole of 7r (placed in entry
[0, 4]); the operator of the inference here consists
of the permutation h2, 1i (swapping the two ranges
covered by the children sub-permutations) and in-
ference consists of a − 1 indexes l1, ... , la−1 sig-
nifying the split points of 7r into sub-permutations:
since a = 2 for 7r, then a single index l1 ∈
{1, 2, 3} is stored with every inference. For the
factorization ((4, 3), (2, 1)) the index l1 = 2 sig-
nifying that the second position is a split point into
h4, 3i (stored in entry [0, 2]) and h2, 1i (stored in
entry [2, 4]). For the other factorizations of 7r sim-
ilar inferences are stored in the permutation forest.
Figure 5 shows a simple top-down factorization
algorithm which starts out by computing the ar-
ity a using function a(7r). If a = 1, a single leaf
node is stored with an empty set of inferences. If
a &gt; 1 then the algorithm computes all possible
factorizations of 7r into a sub-permutations (a se-
quence of a −1 split points) and stores their infer-
ences together as Iji and their operator Oji asso-
ciated with a node in entry [[i, j, Iji , Oji ]]. Subse-
quently, the algorithm applies recursively to each
sub-permutation. Efficiency is a topic beyond
the scope of this paper, but this naive algorithm
has worst case time complexity O(n3), and when
computing only a single canonical PET this can be
O(n) (see e.g., (Zhang and Gildea, 2007)).
</bodyText>
<table confidence="0.529152333333333">
Function PEF (i, j, 7r, F);
# Args: sub-perm. π over [i..j] and forest Y
Output: Parse-Forest F(7r) for 7r;
F := F ∪ {[[i, j,Iji ,Oji]]};
Return F;
end;
</table>
<figureCaption confidence="0.884552">
Figure 5: Pseudo-code of permutation-forest fac-
torization algorithm. Function a(7r) returns the ar-
ity of 7r. Function RankListOf(r1, ... , rm) re-
turns the list of rank positions (i.e., a permutation)
of sub-permutations r1, ... , rm after sorting them
smallest first. The top-level call to this algorithm
uses 7r, i = 0, j = n and F = ∅.
</figureCaption>
<bodyText confidence="0.999842916666667">
Our measure (PEFscore) uses a function
opScore(p) which assigns a score to a given oper-
ator, which can be instantiated to any of the exist-
ing scoring measures listed in Section 2, but in this
case we opted for a very simple function which
gives score 1 to monotone permutation and score
0 to any other permutation.
Given an inference l ∈ Iji where l =
[l1, ... , la−1], we will use the notation lx to refer
to split point lx in l where 1 ≤ x ≤ (a − 1), with
the convenient boundary assumption that l0 = i
and la = j.
</bodyText>
<figure confidence="0.802997090909091">
(2,1)
4
3 2 1
(2,1)
4 3 2 1
(2,1)
1
4 3 2
begin
if ([[i, j, *]] ∈ F) then return F; #memoization
a := a(7r);
if a = 1 return F := F ∪ {[[i, j, ∅]]};
For each
set of split points {l1, . . . , la−1} do
Oj i := RankListOf (7r 11
%(o...,7r(_1+1));
Iji := Iji ∪ [l1, ... , la−1];
For each 7rv ∈ {7rl1
l0+1, 7rl2
(l1+1), . . . , 7rla
(la_1+1)} do
F := F ∪ PermForest(7rv);
</figure>
<page confidence="0.426826">
141
</page>
<equation confidence="0.939084416666667">
PEFscore(π) = φnode(0, n, PEF(π))
if (Zji == 0) then 1
else if (a(πji+1) = j − i) then op5core(Oji )
� else Q x op5core(Oji) + (1 − Q) x
jφinf(l,F,a(πji+1))
i|I
lEZ
z |
� �� �
Avg. inference score over Iji
φnode(i, j, F) = {
φinf (l,F, a) =�ax=1 δ[lx−lx−1&gt;lx]×φnode(l(x−1),lx,F)
</equation>
<figure confidence="0.67318225">
ya
Avg. score for non-terminal children
op5core(p) = f if (p == (1, 2)) then 1
l else 0
</figure>
<figureCaption confidence="0.993924">
Figure 6: The PEF Score
</figureCaption>
<equation confidence="0.488099">
x=1 δ[t-l(x−1)&gt;1]
</equation>
<bodyText confidence="0.991362923076923">
The PEF-score, PEFscore(π) in Figure 6,
computes a score for the single root node
[[0, n, Z0n, On0 ]]) in the permutation forest. This
score is the average inference score φinf over all
inferences of this node. The score of an inference
φinf interpolates (Q) between the op5core of the
operator in the current span and (1 − Q) the scores
of each child node. The interpolation parameter Q
can be tuned on a development set.
The PET-score (single PET) is a simplification
of the PEF- score where the summation over all in-
ferences of a node in φnode is replaced by
“Select a canonical l E Zji ”.
</bodyText>
<sectionHeader confidence="0.995398" genericHeader="method">
4 Experimental setting
</sectionHeader>
<bodyText confidence="0.999568">
Data The data that was used for experiments are
human rankings of translations from WMT13 (Bo-
jar et al., 2013). The data covers 10 language pairs
with a diverse set of systems used for translation.
Each human evaluator was presented with 5 differ-
ent translations, source sentence and a reference
translation and asked to rank system translations
by their quality (ties were allowed).3
Meta-evaluation The standard way for doing
meta-evaluation on the sentence level is with
Kendall’s tau correlation coefficient (Callison-
Burch et al., 2012) computed on the number of
times an evaluation metric and a human evaluator
agree (and disagree) on the rankings of pairs of
</bodyText>
<footnote confidence="0.920459">
3We would like to extend our work also to English-
Japanese but we do not have access to such data at the mo-
ment. In any case, the WMT13 data is the largest publicly
available data of this kind.
</footnote>
<bodyText confidence="0.9913025625">
translations. We extract pairs of translations from
human evaluated data and compute their scores
with all metrics. If the ranking assigned by a met-
ric is the same as the ranking assigned by a hu-
man evaluator then that pair is considered concor-
dant, otherwise it is a discordant pair. All pairs
which have the same score by the metric or are
judged as ties by human evaluators are not used
in meta-evaluation. The formula that was used for
computing Kendall’s tau correlation coefficient is
shown in Equation 1. Note that the formula for
Kendall tau rank correlation coefficient that is used
in meta-evaluation is different from the Kendall
tau similarity function used for evaluating permu-
tations. The values that it returns are in the range
[−1, 1], where −1 means that order is always op-
posite from the human judgment while the value 1
means that metric ranks the system translations in
the same way as humans do.
Ir = #concordant pairs−#discordant pairs (1)
#concordant pairs+#discordant pairs
Evaluating reordering Since system transla-
tions do not differ only in the word order but also
in lexical choice, we follow Birch and Osborne
(2010) and interpolate the score given by each re-
ordering metric with the same lexical score. For
lexical scoring we use unigram BLEU. The param-
eter that balances the weights for these two metrics
α is chosen to be 0.5 so it would not underesti-
mate the lexical differences between translations
(α « 0.5) but also would not turn the whole met-
ric into unigram BLEU (α » 0.5). The equation
</bodyText>
<page confidence="0.990203">
142
</page>
<bodyText confidence="0.854213">
for this interpolation is shown in Equation 2.4
</bodyText>
<equation confidence="0.914118">
FullMetric(ref, sys) = α lexical(ref, sys) +
(1 − α) × bp(|ref|, |7r|) × ordering(7r) (2)
</equation>
<bodyText confidence="0.9961363">
Where 7r(ref,sys) is the permutation represent-
ing the word alignment from sys to ref. The ef-
fect of α on the German-English evaluation is vis-
ible on Figure 7. The PET and PEF measures have
an extra parameter Q that gives importance to the
long distance errors that also needs to be tuned. On
Figure 8 we can see the effect of Q on German-
English for α = 0.5. For all language pairs for
Q = 0.6 both PETs and PEFs get good results so
we picked that as value for Q in our experiments.
</bodyText>
<figureCaption confidence="0.542165">
Figure 7: Effect of α on German-English evalua-
tion for Q = 0.6
</figureCaption>
<bodyText confidence="0.999963583333333">
Choice of word alignments The issue we did
not discuss so far is how to find a permutation
from system and reference translations. One way
is to first get alignments between the source sen-
tence and the system translation (from a decoder
or by automatically aligning sentences), and also
alignments between the source sentence and the
reference translation (manually or automatically
aligned). Subsequently we must make those align-
ments 1-to-1 and merge them into a permutation.
That is the approach that was followed in previ-
ous work (Birch and Osborne, 2011; Talbot et al.,
</bodyText>
<footnote confidence="0.7095">
4Note that for reordering evaluation it does not make
sense to tune α because that would blur the individual contri-
butions of reordering and adequacy during meta evaluation,
which is confirmed by Figure 7 showing that α » 0.5 leads
to similar performance for all metrics.
</footnote>
<figureCaption confidence="0.95696">
Figure 8: Effect of Q on German-English evalua-
</figureCaption>
<bodyText confidence="0.982526666666667">
tion for α = 0.5
2011). Alternatively, we may align system and ref-
erence translations directly. One of the simplest
ways to do that is by finding exact matches be-
tween words and bigrams between system and ref-
erence translation as done in (Isozaki et al., 2010).
The way we align system and reference transla-
tions is by using the aligner supplied with ME-
TEOR (Denkowski and Lavie, 2011) for finding
1-to-1 alignments which are later converted to a
permutation. The advantage of this method is that
it can do non-exact matching by stemming or us-
ing additional sources for semantic similarity such
as WordNets and paraphrase tables. Since we will
not have a perfect permutation as input, because
many words in the reference or system transla-
tions might not be aligned, we introduce a brevity
penalty (bp(·, ·) in Equation 2) for the ordering
component as in (Isozaki et al., 2010). The brevity
penalty is the same as in BLEU with the small
difference that instead of taking the length of sys-
tem and reference translation as its parameters, it
takes the length of the system permutation and the
length of the reference.
</bodyText>
<sectionHeader confidence="0.978239" genericHeader="method">
5 Empirical results
</sectionHeader>
<bodyText confidence="0.999947909090909">
The results are shown in Table 1 and Table 2.
These scores could be much higher if we used
some more sophisticated measure than unigram
BLEU for the lexical part (for example recall is
very useful in evaluation of the system translations
(Lavie et al., 2004)). However, this is not the issue
here since our goal is merely to compare different
ways to evaluate word order. All metrics that we
tested have the same lexical component, get the
same permutation as their input and have the same
value for α.
</bodyText>
<page confidence="0.996843">
143
</page>
<table confidence="0.999878625">
English-Czech English-Spanish English-German English-Russian English-French
Kendall 0.16 0.170 0.183 0.193 0.218
Spearman 0.157 0.170 0.181 0.192 0.215
Hamming 0.150 0.163 0.168 0.187 0.196
FuzzyScore 0.155 0.166 0.178 0.189 0.215
Ulam 0.159 0.170 0.181 0.189 0.221
PEFs 0.156 0.173 0.185 0.196 0.219
PETs 0.157 0.165 0.182 0.195 0.216
</table>
<tableCaption confidence="0.932141333333333">
Table 1: Sentence level Kendall tau scores for
translation out of English with α = 0.5 and Q =
0.6
</tableCaption>
<table confidence="0.999942375">
Czech-English Spanish-English German-English Russian-English French-English
Kendall 0.196 0.265 0.235 0.173 0.223
Spearman 0.199 0.265 0.236 0.173 0.222
Hamming 0.172 0.239 0.215 0.157 0.206
FuzzyScore 0.184 0.263 0.228 0.169 0.216
Ulam 0.188 0.264 0.232 0.171 0.221
PEFs 0.201 0.265 0.237 0.181 0.228
PETs 0.200 0.264 0.234 0.174 0.221
</table>
<tableCaption confidence="0.9928395">
Table 2: Sentence level Kendall tau scores for
translation into English with α = 0.5 and Q = 0.6
</tableCaption>
<subsectionHeader confidence="0.91054">
5.1 Does hierarchical structure improve
evaluation?
</subsectionHeader>
<bodyText confidence="0.999970888888889">
The results in Tables 1, 2 and 3 suggest that the
PEFscore which uses hierarchy over permutations
outperforms the string based permutation metrics
in the majority of the language pairs. The main
exception is the English-Czech language pair in
which both PETs and PEFs based metric do not
give good results compared to some other met-
rics. For discussion about English-Czech look at
the section 6.1.
</bodyText>
<subsectionHeader confidence="0.999093">
5.2 Do PEFs help over one canonical PET?
</subsectionHeader>
<bodyText confidence="0.99857825">
From Figures 9 and 10 it is clear that using all
permutation trees instead of only canonical ones
makes the metric more stable in all language pairs.
Not only that it makes results more stable but it
</bodyText>
<table confidence="0.9969045">
metric avg rank avg Kendall
PEFs 1.6 0.2041
Kendall 2.65 0.2016
Spearman 3.4 0.201
PETs 3.55 0.2008
Ulam 4 0.1996
FuzzyScore 5.8 0.1963
Hamming 7 0.1853
</table>
<tableCaption confidence="0.952295">
Table 3: Average ranks and average Kendall
scores for each tested metrics over all language
pairs
</tableCaption>
<figureCaption confidence="0.9950475">
Figure 9: Plot of scaled Kendall tau correlation for
translation from English
</figureCaption>
<bodyText confidence="0.9993789">
also improves them in all cases except in English-
Czech where both PETs and PEFs perform badly.
The main reason why PEFs outperform PETs is
that they encode all possible phrase segmentations
of monotone and inverted sub-permutations. By
giving the score that considers all segmentations,
PEFs also include the right segmentation (the one
perceived by human evaluators as the right seg-
mentation), while PETs get the right segmentation
only if the right segmentation is the canonical one.
</bodyText>
<subsectionHeader confidence="0.966822">
5.3 Is improvement consistent over language
pairs?
</subsectionHeader>
<bodyText confidence="0.9999973">
Table 3 shows average rank (metric’s position af-
ter sorting all metrics by their correlation for each
language pair) and average Kendall tau correlation
coefficient over the ten language pairs. The table
shows clearly that the PEFs metric outperforms all
other metrics. To make it more visible how met-
rics perform on the different language pairs, Fig-
ures 9 and 10 show Kendall tau correlation co-
efficient scaled between the best scoring metric
for the given language (in most cases PEFs) and
</bodyText>
<page confidence="0.9979">
144
</page>
<figureCaption confidence="0.811505">
Figure 10: Plot of scaled Kendall tau correlation
for translation into English
</figureCaption>
<bodyText confidence="0.999206">
the worst scoring metric (in all cases Hamming
score). We can see that, except in English-Czech,
PEFs are consistently the best or second best (only
in English-French) metric in all language pairs.
PETs are not stable and do not give equally good
results in all language pairs. Hamming distance
is without exception the worst metric for evalua-
tion since it is very strict about positioning of the
words (it does not take relative ordering between
words into account). Kendall tau is the only string
based metric that gives relatively good scores in
all language pairs and in one (English-Czech) it is
the best scoring one.
</bodyText>
<sectionHeader confidence="0.989682" genericHeader="method">
6 Further experiments and analysis
</sectionHeader>
<bodyText confidence="0.999983833333333">
So far we have shown that PEFs outperform the
existing metrics over the majority of language
pairs. There are two pending issues to discuss.
Why is English-Czech seemingly so difficult?
And does preferring inversion over non-binary
branching correlate better with human judgement.
</bodyText>
<subsectionHeader confidence="0.998792">
6.1 The results on English-Czech
</subsectionHeader>
<bodyText confidence="0.9999755">
The English-Czech language pair turned out to
be the hardest one to evaluate for all metrics.
All metrics that were used in the meta-evaluation
that we conducted give much lower Kendall tau
correlation coefficient compared to the other lan-
guage pairs. The experiments conducted by other
researchers on the same dataset (Mach´aˇcek and
Bojar, 2013), using full evaluation metrics, also
get far lower Kendall tau correlation coefficient
for English-Czech than for other language pairs.
In the description of WMT13 data that we used
(Bojar et al., 2013), it is shown that annotator-
agreement for English-Czech is a few times lower
than for other languages. English-Russian, which
is linguistically similar to English-Czech, does
not show low numbers in these categories, and is
one of the language pairs where our metrics per-
form the best. The alignment ratio is equally high
between English-Czech and English-Russian (but
that does not rule out the possibility that the align-
ments are of different quality). One seemingly
unlikely explanation is that English-Czech might
be a harder task in general, and might require a
more sophisticated measure. However, the more
plausible explanation is that the WMT13 data for
English-Czech is not of the same quality as other
language pairs. It could be that data filtering, for
example by taking only judgments for which many
evaluators agree, could give more trustworthy re-
sults.
</bodyText>
<sectionHeader confidence="0.6811705" genericHeader="method">
6.2 Is inversion preferred over non-binary
branching?
</sectionHeader>
<bodyText confidence="0.999461285714286">
Since our original version of the scoring function
for PETs and PEFs on the operator level does not
discriminate between kinds of non-monotone op-
erators (all non-monotone get zero as a score) we
also tested whether discriminating between inver-
sion (binary) and non-binary operators make any
difference.
</bodyText>
<table confidence="0.97992175">
PEFs -y = 0.0 0.156 0.173 0.185 0.196 0.219
PEFs -y = 0.5 0.157 0.175 0.183 0.195 0.219
PETs -y = 0.0 0.157 0.165 0.182 0.195 0.216
PETs -y = 0.5 0.158 0.165 0.183 0.195 0.217
</table>
<tableCaption confidence="0.98129">
Table 4: Sentence level Kendall tau score for
</tableCaption>
<bodyText confidence="0.954146727272727">
translation out of English different -y with α = 0.5
and Q = 0.6
Intuitively, we might expect that inverted binary
operators are preferred by human evaluators over
non-binary ones. So instead of assigning zero as a
score to inverted nodes we give them 0.5, while for
non-binary nodes we remain with zero. The ex-
periments with the inverted operator scored with
0.5 (i.e., -y = 0.5) are shown in Tables 4 and 5.
The results show that there is no clear improve-
ment by distinguishing between the two kinds of
</bodyText>
<figure confidence="0.9907758">
English-Czech
English-French
English-Russian
English-Spanish
English-German
</figure>
<page confidence="0.991348">
145
</page>
<bodyText confidence="0.9926575">
members. We also thank Ivan Titov for helpful
comments on the ideas presented in this paper.
</bodyText>
<sectionHeader confidence="0.843139" genericHeader="method">
References
</sectionHeader>
<figure confidence="0.9259252">
Czech-English
German-English
Spanish-English
French-English
Russian-English
</figure>
<table confidence="0.66844575">
PEFs -y = 0.0 0.201 0.265 0.237 0.181 0.228
PEFs -y = 0.5 0.201 0.264 0.235 0.179 0.227
PETs -y = 0.0 0.200 0.264 0.234 0.174 0.221
PETs -y = 0.5 0.202 0.263 0.235 0.176 0.224
</table>
<tableCaption confidence="0.654666333333333">
Table 5: Sentence level Kendall tau score for
translation into English for different -y with α =
0.5 and Q = 0.6
</tableCaption>
<bodyText confidence="0.851282">
non-monotone operators on the nodes.
</bodyText>
<sectionHeader confidence="0.999638" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999579428571429">
Representing order differences as compact permu-
tation forests provides a good basis for develop-
ing evaluation measures of word order differences.
These hierarchical representations of permutations
bring together two crucial elements (1) grouping
words into blocks, and (2) factorizing reorder-
ing phenomena recursively over these groupings.
Earlier work on MT evaluation metrics has of-
ten stressed the importance of the first ingredient
(grouping into blocks) but employed it merely in a
flat (non-recursive) fashion. In this work we pre-
sented novel metrics based on permutation trees
and forests (the PETscore and PEFscore) where
the second ingredient (factorizing reordering phe-
nomena recursively) plays a major role. Permuta-
tion forests compactly represent all possible block
groupings for a given permutation, whereas per-
mutation trees select a single canonical grouping.
Our experiments with WMT13 data show that our
PEFscore metric outperforms the existing string-
based metrics on the large majority of language
pairs, and in the minority of cases where it is not
ranked first, it ranks high. Crucially, the PEFs-
core is by far the most stable reordering score over
ten language pairs, and works well also for lan-
guage pairs with long range reordering phenom-
ena (English-German, German-English, English-
Russian and Russian-English).
</bodyText>
<sectionHeader confidence="0.999369" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.889923">
This work is supported by STW grant nr. 12271
and NWO VICI grant nr. 277-89-002. We thank
TAUS and the other DatAptor project User Board
</bodyText>
<reference confidence="0.99919786">
Michael H. Albert and Mike D. Atkinson. 2005. Sim-
ple permutations and pattern restricted permutations.
Discrete Mathematics, 300(1-3):1–15.
Alexandra Birch and Miles Osborne. 2010. LRscore
for Evaluating Lexical and Reordering Quality in
MT. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 327–332, Uppsala, Sweden, July. Association
for Computational Linguistics.
Alexandra Birch and Miles Osborne. 2011. Reorder-
ing Metrics for MT. In Proceedings of the Associ-
ation for Computational Linguistics, Portland, Ore-
gon, USA. Association for Computational Linguis-
tics.
Alexandra Birch, Miles Osborne, and Phil Blunsom.
2010. Metrics for MT evaluation: evaluating re-
ordering. Machine Translation, pages 1–12.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. Association for
Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic Metric for Reliable Optimization
and Evaluation of Machine Translation Systems. In
Proceedings of the EMNLP 2011 Workshop on Sta-
tistical Machine Translation.
Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006.
Factoring Synchronous Grammars by Sorting. In
ACL.
Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito
Sudoh, and Hajime Tsukada. 2010. Automatic
evaluation of translation quality for distant language
pairs. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’10, pages 944–952, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mirella Lapata. 2006. Automatic Evaluation of In-
formation Ordering: Kendall’s Tau. Computational
Linguistics, 32(4):471–484.
</reference>
<page confidence="0.987475">
146
</page>
<reference confidence="0.999669">
Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-
man. 2004. The significance of recall in auto-
matic metrics for MT evaluation. In Proceedings of
the Sixth Conference of the Association for Machine
Translation in the Americas.
Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results
of the WMT13 Metrics Shared Task. In Proceed-
ings of the Eighth Workshop on Statistical Machine
Translation, pages 45–51, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Gideon Maillette de Buy Wenniger and Khalil Sima’an.
2011. Hierarchical Translation Equivalence over
Word Alignments. In ILLC Prepublication Series,
PP-2011-38. University of Amsterdam.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL’02, pages 311–318, Philadelphia, PA, USA.
Milo&amp;quot;s Stanojevi´c and Khalil Sima’an. 2014. BEER:
BEtter Evaluation as Ranking. In Proceedings of the
Ninth Workshop on Statistical Machine Translation,
pages 414–419, Baltimore, Maryland, USA, June.
Association for Computational Linguistics.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason
Katz-Brown, Masakazu Seno, and Franz Och. 2011.
A Lightweight Evaluation Framework for Machine
Translation Reordering. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
12–21, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 3(23):377–403.
Hao Zhang and Daniel Gildea. 2007. Factorization
of Synchronous Context-Free Grammars in Linear
Time. In NAACL Workshop on Syntax and Structure
in Statistical Translation (SSST), pages 25–32.
Hao Zhang, Daniel Gildea, and David Chiang. 2008.
Extracting synchronous grammar rules from word-
level alignments in linear time. In Proceedings
of the 22nd International Conference on Computa-
tional Linguistics-Volume 1, pages 1081–1088. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.9981">
147
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.747340">
<title confidence="0.999776">Evaluating Word Order Recursively over Permutation-Forests</title>
<author confidence="0.924798">Stanojevi´c</author>
<affiliation confidence="0.993147">Institute for Logic, Language and University of</affiliation>
<address confidence="0.821118">Science Park 107, 1098 XG Amsterdam, The</address>
<email confidence="0.979359">m.stanojevic@uva.nl</email>
<email confidence="0.979359">k.simaan@uva.nl</email>
<abstract confidence="0.99971584375">Automatically evaluating word order of MT system output at the sentence-level is challenging. At the sentence-level, ngram counts are rather sparse which makes it difficult to measure word order quality effectively using lexicalized units. Recent approaches abstract away from lexicalizaby assigning a score to the permutahow word positions in system output move around relative to a reference translation. Metrics over permutations exist (e.g., Kendal tau or Spearman Rho) and have been shown to be useful in earlier work. However, none of the existing metrics over permutations groups word positions recursively into larger phrase-like blocks, which makes it difficult to account for long-distance reordering phenomena. In this paper we exnovel metrics computed over Per- Forests packed charts of Permutation Trees (PETs), which are tree decompositions of a permutation into primitive ordering units. We empirically compare PEFs metric against five known reordering metrics on WMT13 data for ten language pairs. The PEFs metric shows better correlation with human ranking than the other metrics almost on all language pairs. None of the other metrics exhibits as stable behavior across language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael H Albert</author>
<author>Mike D Atkinson</author>
</authors>
<title>Simple permutations and pattern restricted permutations.</title>
<date>2005</date>
<journal>Discrete Mathematics,</journal>
<pages>300--1</pages>
<contexts>
<context position="5689" citStr="Albert and Atkinson, 2005" startWordPosition="886" endWordPosition="890">ET dominates a subtree whose fringe1 is itself a permutation over an integer sub-range of the original permutation. Every node is decorated with a permutation over the child positions (called operator). For example (4, 5, 6) constitutes a contiguous range of integers (corresponding to a phrase pair), and hence will be grouped into a subtree; 1Ordered sequence of leaf nodes. which in turn can be internally re-grouped into a binary branching subtree. Every node in a PET is minimum branching, i.e., the permutation factorizes into a minimum number of adjacent permutations over integer sub-ranges (Albert and Atkinson, 2005). The node operators in a PET are known to be the atomic building blocks of all permutations (called primal permutations). Because these are building atomic units of reordering, it makes sense to want to measure reordering as a function of the individual cost of these operators. In this work we propose to compute new reordering measures that aggregate over the individual node-permutations in these PETs. While PETs where exploited rather recently for extracting features used in the BEER metric system description (Stanojevi´c and Sima’an, 2014) in the official WMT 2014 competition, this work is </context>
</contexts>
<marker>Albert, Atkinson, 2005</marker>
<rawString>Michael H. Albert and Mike D. Atkinson. 2005. Simple permutations and pattern restricted permutations. Discrete Mathematics, 300(1-3):1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
</authors>
<title>LRscore for Evaluating Lexical and Reordering Quality in MT.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>327--332</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="7079" citStr="Birch and Osborne, 2010" startWordPosition="1115" endWordPosition="1118">at measures at the same time both fluency and adequacy). We empirically show that a PEF-based evaluation measure correlates better with human rankings than the string-based measures on eight of the ten language pairs in WMT13 data. For the 9th language pair it is close to best, and for the 10th (English-Czech) we find a likely explanation in the Findings of the 2013 WMT (Bojar et al., 2013). Crucially, the PEF-based measure shows more stable ranking across language pairs than any of the other measures. The metric is available online as free software2. 2 Measures on permutations: Baselines In (Birch and Osborne, 2010; Birch and Osborne, 2011) Kendall’s tau and Hamming distance are combined with unigram BLEU (BLEU-1) leading to LRscore showing better correlation with human judgment than BLEU-4. Birch et al. (2010) additionally tests Ulam distance (longest common subsequence – LCS – normalized by the permutation length) and the square root of Kendall’s tau. Isozaki et al. (2010) presents a similar approach to (Birch and Osborne, 2011) additionally testing Spearman rho as a distance measure. Talbot et al. (2011) extracts a reordering measure from METEOR (Denkowski and Lavie, 2011) dubbed Fuzzy Reordering Sco</context>
<context position="18813" citStr="Birch and Osborne (2010)" startWordPosition="3300" endWordPosition="3303"> the formula for Kendall tau rank correlation coefficient that is used in meta-evaluation is different from the Kendall tau similarity function used for evaluating permutations. The values that it returns are in the range [−1, 1], where −1 means that order is always opposite from the human judgment while the value 1 means that metric ranks the system translations in the same way as humans do. Ir = #concordant pairs−#discordant pairs (1) #concordant pairs+#discordant pairs Evaluating reordering Since system translations do not differ only in the word order but also in lexical choice, we follow Birch and Osborne (2010) and interpolate the score given by each reordering metric with the same lexical score. For lexical scoring we use unigram BLEU. The parameter that balances the weights for these two metrics α is chosen to be 0.5 so it would not underestimate the lexical differences between translations (α « 0.5) but also would not turn the whole metric into unigram BLEU (α » 0.5). The equation 142 for this interpolation is shown in Equation 2.4 FullMetric(ref, sys) = α lexical(ref, sys) + (1 − α) × bp(|ref|, |7r|) × ordering(7r) (2) Where 7r(ref,sys) is the permutation representing the word alignment from sys</context>
</contexts>
<marker>Birch, Osborne, 2010</marker>
<rawString>Alexandra Birch and Miles Osborne. 2010. LRscore for Evaluating Lexical and Reordering Quality in MT. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 327–332, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
</authors>
<title>Reordering Metrics for MT.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1822" citStr="Birch and Osborne, 2011" startWordPosition="265" endWordPosition="269"> units. We empirically compare PEFs metric against five known reordering metrics on WMT13 data for ten language pairs. The PEFs metric shows better correlation with human ranking than the other metrics almost on all language pairs. None of the other metrics exhibits as stable behavior across language pairs. 1 Introduction Evaluating word order (also reordering) in MT is one of the main ingredients in automatic MT evaluation, e.g., (Papineni et al., 2002; Denkowski and Lavie, 2011). To monitor progress on evaluating reordering, recent work explores dedicated reordering evaluation metrics, cf. (Birch and Osborne, 2011; Isozaki et al., 2010; Talbot et al., 2011). Existing work computes the correlation between the ranking of the outputs of different systems by an evaluation metric to human ranking, on e.g., the WMT evaluation data. For evaluating reordering, it is necessary to word align system output with the corresponding reference translation. For convenience, a 1:1 alignment (a permutation) is induced between the words on both sides (Birch and Osborne, 2011), possibly leaving words unaligned on either side. Existing work then concentrates on defining measures of reordering over permutations, cf. (Lapata,</context>
<context position="7105" citStr="Birch and Osborne, 2011" startWordPosition="1119" endWordPosition="1122">ime both fluency and adequacy). We empirically show that a PEF-based evaluation measure correlates better with human rankings than the string-based measures on eight of the ten language pairs in WMT13 data. For the 9th language pair it is close to best, and for the 10th (English-Czech) we find a likely explanation in the Findings of the 2013 WMT (Bojar et al., 2013). Crucially, the PEF-based measure shows more stable ranking across language pairs than any of the other measures. The metric is available online as free software2. 2 Measures on permutations: Baselines In (Birch and Osborne, 2010; Birch and Osborne, 2011) Kendall’s tau and Hamming distance are combined with unigram BLEU (BLEU-1) leading to LRscore showing better correlation with human judgment than BLEU-4. Birch et al. (2010) additionally tests Ulam distance (longest common subsequence – LCS – normalized by the permutation length) and the square root of Kendall’s tau. Isozaki et al. (2010) presents a similar approach to (Birch and Osborne, 2011) additionally testing Spearman rho as a distance measure. Talbot et al. (2011) extracts a reordering measure from METEOR (Denkowski and Lavie, 2011) dubbed Fuzzy Reordering Score and evaluates it on MT </context>
<context position="20439" citStr="Birch and Osborne, 2011" startWordPosition="3595" endWordPosition="3598">. Figure 7: Effect of α on German-English evaluation for Q = 0.6 Choice of word alignments The issue we did not discuss so far is how to find a permutation from system and reference translations. One way is to first get alignments between the source sentence and the system translation (from a decoder or by automatically aligning sentences), and also alignments between the source sentence and the reference translation (manually or automatically aligned). Subsequently we must make those alignments 1-to-1 and merge them into a permutation. That is the approach that was followed in previous work (Birch and Osborne, 2011; Talbot et al., 4Note that for reordering evaluation it does not make sense to tune α because that would blur the individual contributions of reordering and adequacy during meta evaluation, which is confirmed by Figure 7 showing that α » 0.5 leads to similar performance for all metrics. Figure 8: Effect of Q on German-English evaluation for α = 0.5 2011). Alternatively, we may align system and reference translations directly. One of the simplest ways to do that is by finding exact matches between words and bigrams between system and reference translation as done in (Isozaki et al., 2010). The</context>
</contexts>
<marker>Birch, Osborne, 2011</marker>
<rawString>Alexandra Birch and Miles Osborne. 2011. Reordering Metrics for MT. In Proceedings of the Association for Computational Linguistics, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
<author>Phil Blunsom</author>
</authors>
<title>Metrics for MT evaluation: evaluating reordering. Machine Translation,</title>
<date>2010</date>
<pages>1--12</pages>
<contexts>
<context position="7279" citStr="Birch et al. (2010)" startWordPosition="1145" endWordPosition="1148">nguage pairs in WMT13 data. For the 9th language pair it is close to best, and for the 10th (English-Czech) we find a likely explanation in the Findings of the 2013 WMT (Bojar et al., 2013). Crucially, the PEF-based measure shows more stable ranking across language pairs than any of the other measures. The metric is available online as free software2. 2 Measures on permutations: Baselines In (Birch and Osborne, 2010; Birch and Osborne, 2011) Kendall’s tau and Hamming distance are combined with unigram BLEU (BLEU-1) leading to LRscore showing better correlation with human judgment than BLEU-4. Birch et al. (2010) additionally tests Ulam distance (longest common subsequence – LCS – normalized by the permutation length) and the square root of Kendall’s tau. Isozaki et al. (2010) presents a similar approach to (Birch and Osborne, 2011) additionally testing Spearman rho as a distance measure. Talbot et al. (2011) extracts a reordering measure from METEOR (Denkowski and Lavie, 2011) dubbed Fuzzy Reordering Score and evaluates it on MT reordering quality. 2https://github.com/stanojevic/beer h2,4,1,3i 2 h1,2i 4 h1,2i 5 6 1 3 h2,1i A h2,1i because B h2,1i A because B h2,1i 139 For an evaluation metric we need</context>
</contexts>
<marker>Birch, Osborne, Blunsom, 2010</marker>
<rawString>Alexandra Birch, Miles Osborne, and Phil Blunsom. 2010. Metrics for MT evaluation: evaluating reordering. Machine Translation, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="6849" citStr="Bojar et al., 2013" startWordPosition="1078" endWordPosition="1082">, 2014) in the official WMT 2014 competition, this work is the first to propose integral recursive metrics over PETs and PEFs solely for measuring reordering (as opposed to individual non-recursive features in a full metric that measures at the same time both fluency and adequacy). We empirically show that a PEF-based evaluation measure correlates better with human rankings than the string-based measures on eight of the ten language pairs in WMT13 data. For the 9th language pair it is close to best, and for the 10th (English-Czech) we find a likely explanation in the Findings of the 2013 WMT (Bojar et al., 2013). Crucially, the PEF-based measure shows more stable ranking across language pairs than any of the other measures. The metric is available online as free software2. 2 Measures on permutations: Baselines In (Birch and Osborne, 2010; Birch and Osborne, 2011) Kendall’s tau and Hamming distance are combined with unigram BLEU (BLEU-1) leading to LRscore showing better correlation with human judgment than BLEU-4. Birch et al. (2010) additionally tests Ulam distance (longest common subsequence – LCS – normalized by the permutation length) and the square root of Kendall’s tau. Isozaki et al. (2010) pr</context>
<context position="16920" citStr="Bojar et al., 2013" startWordPosition="2980" endWordPosition="2984">he permutation forest. This score is the average inference score φinf over all inferences of this node. The score of an inference φinf interpolates (Q) between the op5core of the operator in the current span and (1 − Q) the scores of each child node. The interpolation parameter Q can be tuned on a development set. The PET-score (single PET) is a simplification of the PEF- score where the summation over all inferences of a node in φnode is replaced by “Select a canonical l E Zji ”. 4 Experimental setting Data The data that was used for experiments are human rankings of translations from WMT13 (Bojar et al., 2013). The data covers 10 language pairs with a diverse set of systems used for translation. Each human evaluator was presented with 5 different translations, source sentence and a reference translation and asked to rank system translations by their quality (ties were allowed).3 Meta-evaluation The standard way for doing meta-evaluation on the sentence level is with Kendall’s tau correlation coefficient (CallisonBurch et al., 2012) computed on the number of times an evaluation metric and a human evaluator agree (and disagree) on the rankings of pairs of 3We would like to extend our work also to Eng</context>
<context position="26940" citStr="Bojar et al., 2013" startWordPosition="4667" endWordPosition="4670">relate better with human judgement. 6.1 The results on English-Czech The English-Czech language pair turned out to be the hardest one to evaluate for all metrics. All metrics that were used in the meta-evaluation that we conducted give much lower Kendall tau correlation coefficient compared to the other language pairs. The experiments conducted by other researchers on the same dataset (Mach´aˇcek and Bojar, 2013), using full evaluation metrics, also get far lower Kendall tau correlation coefficient for English-Czech than for other language pairs. In the description of WMT13 data that we used (Bojar et al., 2013), it is shown that annotatoragreement for English-Czech is a few times lower than for other languages. English-Russian, which is linguistically similar to English-Czech, does not show low numbers in these categories, and is one of the language pairs where our metrics perform the best. The alignment ratio is equally high between English-Czech and English-Russian (but that does not rule out the possibility that the alignments are of different quality). One seemingly unlikely explanation is that English-Czech might be a harder task in general, and might require a more sophisticated measure. Howev</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1684" citStr="Denkowski and Lavie, 2011" startWordPosition="246" endWordPosition="249">Permutation Forests (PEFs), packed charts of Permutation Trees (PETs), which are tree decompositions of a permutation into primitive ordering units. We empirically compare PEFs metric against five known reordering metrics on WMT13 data for ten language pairs. The PEFs metric shows better correlation with human ranking than the other metrics almost on all language pairs. None of the other metrics exhibits as stable behavior across language pairs. 1 Introduction Evaluating word order (also reordering) in MT is one of the main ingredients in automatic MT evaluation, e.g., (Papineni et al., 2002; Denkowski and Lavie, 2011). To monitor progress on evaluating reordering, recent work explores dedicated reordering evaluation metrics, cf. (Birch and Osborne, 2011; Isozaki et al., 2010; Talbot et al., 2011). Existing work computes the correlation between the ranking of the outputs of different systems by an evaluation metric to human ranking, on e.g., the WMT evaluation data. For evaluating reordering, it is necessary to word align system output with the corresponding reference translation. For convenience, a 1:1 alignment (a permutation) is induced between the words on both sides (Birch and Osborne, 2011), possibly </context>
<context position="7651" citStr="Denkowski and Lavie, 2011" startWordPosition="1205" endWordPosition="1208">on permutations: Baselines In (Birch and Osborne, 2010; Birch and Osborne, 2011) Kendall’s tau and Hamming distance are combined with unigram BLEU (BLEU-1) leading to LRscore showing better correlation with human judgment than BLEU-4. Birch et al. (2010) additionally tests Ulam distance (longest common subsequence – LCS – normalized by the permutation length) and the square root of Kendall’s tau. Isozaki et al. (2010) presents a similar approach to (Birch and Osborne, 2011) additionally testing Spearman rho as a distance measure. Talbot et al. (2011) extracts a reordering measure from METEOR (Denkowski and Lavie, 2011) dubbed Fuzzy Reordering Score and evaluates it on MT reordering quality. 2https://github.com/stanojevic/beer h2,4,1,3i 2 h1,2i 4 h1,2i 5 6 1 3 h2,1i A h2,1i because B h2,1i A because B h2,1i 139 For an evaluation metric we need a function which would have the standard behaviour of evaluation metrics - the higher the score the better. Bellow we define the baseline metrics that were used in our experiments. Baselines A permutation over [1..n] (subrange of the positive integers where n &gt; 1) is a bijective function from [1..n] to itself. To represent permutations we will use angle brackets as in </context>
<context position="21159" citStr="Denkowski and Lavie, 2011" startWordPosition="3721" endWordPosition="3724"> that would blur the individual contributions of reordering and adequacy during meta evaluation, which is confirmed by Figure 7 showing that α » 0.5 leads to similar performance for all metrics. Figure 8: Effect of Q on German-English evaluation for α = 0.5 2011). Alternatively, we may align system and reference translations directly. One of the simplest ways to do that is by finding exact matches between words and bigrams between system and reference translation as done in (Isozaki et al., 2010). The way we align system and reference translations is by using the aligner supplied with METEOR (Denkowski and Lavie, 2011) for finding 1-to-1 alignments which are later converted to a permutation. The advantage of this method is that it can do non-exact matching by stemming or using additional sources for semantic similarity such as WordNets and paraphrase tables. Since we will not have a perfect permutation as input, because many words in the reference or system translations might not be aligned, we introduce a brevity penalty (bp(·, ·) in Equation 2) for the ordering component as in (Isozaki et al., 2010). The brevity penalty is the same as in BLEU with the small difference that instead of taking the length of </context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems. In Proceedings of the EMNLP 2011 Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Giorgio Satta</author>
<author>Hao Zhang</author>
</authors>
<title>Factoring Synchronous Grammars by Sorting.</title>
<date>2006</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="3470" citStr="Gildea et al., 2006" startWordPosition="524" endWordPosition="527">permutations also contain latent atomic units that govern the recursive reordering of phrase-like units. Accounting for these latent reorderings could actually be far simpler than the flat view of a permutation. Isozaki et al. (2010) argue that the conventional metrics cannot measure well the long distance reordering between an English reference sentence “A because B” and a Japanese-English hypothesis translation “B because A”, where A and B are blocks of any length with internal monotonic alignments. In this paper we explore the idea of factorizing permutations into permutation-trees (PETs) (Gildea et al., 2006) and defining new 138 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 138–147, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics Figure 1: A permutation tree for (2, 4, 5, 6, 1, 3) tree-based reordering metrics which aims at dealing with this type of long range reorderings. For the Isozaki et al. (2010) Japanese-English example, there are two PETs (when leaving A and B as encapsulated blocks): Our PET-based metrics interpolate the scores over the two inversion operators (2, 1) with the internal scores for</context>
<context position="9287" citStr="Gildea et al., 2006" startWordPosition="1503" endWordPosition="1506">est Common Subsequence, and Kronecker S[a] which is 1 if (a == true) else zero, and An1 = (1, · · · , n) which is the identity permutation over [1..n]. We note that all existing metrics spearman(7r) = 1 n(n2 − 1) LC5(7r, An 1) − 1 ulam(7r) = n − 1 c − 1 fuzzy(7r) = 1 − n − 1 where c is # of monotone sub-permutations Figure 2: Five commonly used metrics over permutations are defined directly over flat string-level permutations. In the next section we present an alternative view of permutations are compositional, recursive tree structures. 3 Measures on Permutation Forests Existing work, e.g., (Gildea et al., 2006), shows how to factorize any permutation 7r over [1..n] into a canonical permutation tree (PET). Here we will summarize the relevant aspects and extend PETs to permutation forests (PEFs). A non-empty sub-sequence 7rji of a permutation 7r is isomorphic with a permutation over [1..(j − i + 1)] iff the set {7ri, ... , 7rj} is a contiguous range of positive integers. We will use the term a sub-permutation of 7r to refer to a subsequence of 7r that is isomorphic with a permutation. Note that not every subsequence of a permutation 7r is necessarily isomorphic with a permutation, e.g., the subsequenc</context>
<context position="11008" citStr="Gildea et al., 2006" startWordPosition="1823" endWordPosition="1826">rity of 7r and denote it with a(7r) (or simply a when 7r is understood from the context). For example, the arity of 7r = (5, 7, 4, 6, 3, 1, 2) is a = 2 because it can be split into a minimum of two sub-permutations (Figure 3), e.g. (5, 7, 4, 6, 3) and (1, 2) (but alternatively also (5, 7, 4, 6) and (3, 1,2)). In contrast, 7r = (2, 4, 1, 3) (also known as the Wu (1997) permutation) cannot be split into less than four subpermutations, i.e., a = 4. Factorization can be applied recursively to the sub-permutations of 7r, resulting in a tree structure (see Figure 3) called a permutation tree (PET) (Gildea et al., 2006; Zhang and Gildea, 2007; Maillette de Buy Wenniger and Sima’an, 2011). Some permutations factorize into multiple alternative PETs. For 7r = (4, 3, 2,1) there are five PETs shown in Figure 3. The alternative PETs can be packed into an O(n2) permutation forest (PEF). For many computational purposes, a single canonical PET is sufficient, cf. (Gildea et al., 2006). However, while different PETs of 7r exhibit the same reordering pattern, their different binary branching structures might indicate important differences as we show in our experiments. A permutation forest (akin to a parse forest) F fo</context>
</contexts>
<marker>Gildea, Satta, Zhang, 2006</marker>
<rawString>Daniel Gildea, Giorgio Satta, and Hao Zhang. 2006. Factoring Synchronous Grammars by Sorting. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Tsutomu Hirao</author>
<author>Kevin Duh</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Automatic evaluation of translation quality for distant language pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>944--952</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1844" citStr="Isozaki et al., 2010" startWordPosition="270" endWordPosition="273">mpare PEFs metric against five known reordering metrics on WMT13 data for ten language pairs. The PEFs metric shows better correlation with human ranking than the other metrics almost on all language pairs. None of the other metrics exhibits as stable behavior across language pairs. 1 Introduction Evaluating word order (also reordering) in MT is one of the main ingredients in automatic MT evaluation, e.g., (Papineni et al., 2002; Denkowski and Lavie, 2011). To monitor progress on evaluating reordering, recent work explores dedicated reordering evaluation metrics, cf. (Birch and Osborne, 2011; Isozaki et al., 2010; Talbot et al., 2011). Existing work computes the correlation between the ranking of the outputs of different systems by an evaluation metric to human ranking, on e.g., the WMT evaluation data. For evaluating reordering, it is necessary to word align system output with the corresponding reference translation. For convenience, a 1:1 alignment (a permutation) is induced between the words on both sides (Birch and Osborne, 2011), possibly leaving words unaligned on either side. Existing work then concentrates on defining measures of reordering over permutations, cf. (Lapata, 2006; Birch and Osbor</context>
<context position="3083" citStr="Isozaki et al. (2010)" startWordPosition="466" endWordPosition="469">et al., 2010; Talbot et al., 2011). Popular metrics over permutations are: Kendall’s tau, Spearman, Hamming distance, Ulam and Fuzzy score. These metrics treat a permutation as a flat sequence of integers or blocks, disregarding the possibility of hierarchical grouping into phrase-like units, making it difficult to measure long-range order divergence. Next we will show by example that permutations also contain latent atomic units that govern the recursive reordering of phrase-like units. Accounting for these latent reorderings could actually be far simpler than the flat view of a permutation. Isozaki et al. (2010) argue that the conventional metrics cannot measure well the long distance reordering between an English reference sentence “A because B” and a Japanese-English hypothesis translation “B because A”, where A and B are blocks of any length with internal monotonic alignments. In this paper we explore the idea of factorizing permutations into permutation-trees (PETs) (Gildea et al., 2006) and defining new 138 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 138–147, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Lingui</context>
<context position="7446" citStr="Isozaki et al. (2010)" startWordPosition="1173" endWordPosition="1176">WMT (Bojar et al., 2013). Crucially, the PEF-based measure shows more stable ranking across language pairs than any of the other measures. The metric is available online as free software2. 2 Measures on permutations: Baselines In (Birch and Osborne, 2010; Birch and Osborne, 2011) Kendall’s tau and Hamming distance are combined with unigram BLEU (BLEU-1) leading to LRscore showing better correlation with human judgment than BLEU-4. Birch et al. (2010) additionally tests Ulam distance (longest common subsequence – LCS – normalized by the permutation length) and the square root of Kendall’s tau. Isozaki et al. (2010) presents a similar approach to (Birch and Osborne, 2011) additionally testing Spearman rho as a distance measure. Talbot et al. (2011) extracts a reordering measure from METEOR (Denkowski and Lavie, 2011) dubbed Fuzzy Reordering Score and evaluates it on MT reordering quality. 2https://github.com/stanojevic/beer h2,4,1,3i 2 h1,2i 4 h1,2i 5 6 1 3 h2,1i A h2,1i because B h2,1i A because B h2,1i 139 For an evaluation metric we need a function which would have the standard behaviour of evaluation metrics - the higher the score the better. Bellow we define the baseline metrics that were used in ou</context>
<context position="21034" citStr="Isozaki et al., 2010" startWordPosition="3699" endWordPosition="3702">k (Birch and Osborne, 2011; Talbot et al., 4Note that for reordering evaluation it does not make sense to tune α because that would blur the individual contributions of reordering and adequacy during meta evaluation, which is confirmed by Figure 7 showing that α » 0.5 leads to similar performance for all metrics. Figure 8: Effect of Q on German-English evaluation for α = 0.5 2011). Alternatively, we may align system and reference translations directly. One of the simplest ways to do that is by finding exact matches between words and bigrams between system and reference translation as done in (Isozaki et al., 2010). The way we align system and reference translations is by using the aligner supplied with METEOR (Denkowski and Lavie, 2011) for finding 1-to-1 alignments which are later converted to a permutation. The advantage of this method is that it can do non-exact matching by stemming or using additional sources for semantic similarity such as WordNets and paraphrase tables. Since we will not have a perfect permutation as input, because many words in the reference or system translations might not be aligned, we introduce a brevity penalty (bp(·, ·) in Equation 2) for the ordering component as in (Isoz</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, 2010</marker>
<rawString>Hideki Isozaki, Tsutomu Hirao, Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Automatic evaluation of translation quality for distant language pairs. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 944–952, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
</authors>
<title>Automatic Evaluation of Information Ordering: Kendall’s Tau.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="2427" citStr="Lapata, 2006" startWordPosition="363" endWordPosition="365">e, 2011; Isozaki et al., 2010; Talbot et al., 2011). Existing work computes the correlation between the ranking of the outputs of different systems by an evaluation metric to human ranking, on e.g., the WMT evaluation data. For evaluating reordering, it is necessary to word align system output with the corresponding reference translation. For convenience, a 1:1 alignment (a permutation) is induced between the words on both sides (Birch and Osborne, 2011), possibly leaving words unaligned on either side. Existing work then concentrates on defining measures of reordering over permutations, cf. (Lapata, 2006; Birch and Osborne, 2011; Isozaki et al., 2010; Talbot et al., 2011). Popular metrics over permutations are: Kendall’s tau, Spearman, Hamming distance, Ulam and Fuzzy score. These metrics treat a permutation as a flat sequence of integers or blocks, disregarding the possibility of hierarchical grouping into phrase-like units, making it difficult to measure long-range order divergence. Next we will show by example that permutations also contain latent atomic units that govern the recursive reordering of phrase-like units. Accounting for these latent reorderings could actually be far simpler th</context>
</contexts>
<marker>Lapata, 2006</marker>
<rawString>Mirella Lapata. 2006. Automatic Evaluation of Information Ordering: Kendall’s Tau. Computational Linguistics, 32(4):471–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Kenji Sagae</author>
<author>Shyamsundar Jayaraman</author>
</authors>
<title>The significance of recall in automatic metrics for MT evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="22168" citStr="Lavie et al., 2004" startWordPosition="3895" endWordPosition="3898">uce a brevity penalty (bp(·, ·) in Equation 2) for the ordering component as in (Isozaki et al., 2010). The brevity penalty is the same as in BLEU with the small difference that instead of taking the length of system and reference translation as its parameters, it takes the length of the system permutation and the length of the reference. 5 Empirical results The results are shown in Table 1 and Table 2. These scores could be much higher if we used some more sophisticated measure than unigram BLEU for the lexical part (for example recall is very useful in evaluation of the system translations (Lavie et al., 2004)). However, this is not the issue here since our goal is merely to compare different ways to evaluate word order. All metrics that we tested have the same lexical component, get the same permutation as their input and have the same value for α. 143 English-Czech English-Spanish English-German English-Russian English-French Kendall 0.16 0.170 0.183 0.193 0.218 Spearman 0.157 0.170 0.181 0.192 0.215 Hamming 0.150 0.163 0.168 0.187 0.196 FuzzyScore 0.155 0.166 0.178 0.189 0.215 Ulam 0.159 0.170 0.181 0.189 0.221 PEFs 0.156 0.173 0.185 0.196 0.219 PETs 0.157 0.165 0.182 0.195 0.216 Table 1: Senten</context>
</contexts>
<marker>Lavie, Sagae, Jayaraman, 2004</marker>
<rawString>Alon Lavie, Kenji Sagae, and Shyamsundar Jayaraman. 2004. The significance of recall in automatic metrics for MT evaluation. In Proceedings of the Sixth Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Results of the WMT13 Metrics Shared Task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>45--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Mach´acek, Bojar, 2013</marker>
<rawString>Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results of the WMT13 Metrics Shared Task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45–51, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon Maillette de Buy Wenniger</author>
<author>Khalil Sima’an</author>
</authors>
<title>Hierarchical Translation Equivalence over Word Alignments.</title>
<date>2011</date>
<booktitle>In ILLC Prepublication Series,</booktitle>
<pages>2011--38</pages>
<institution>University of Amsterdam.</institution>
<marker>Wenniger, Sima’an, 2011</marker>
<rawString>Gideon Maillette de Buy Wenniger and Khalil Sima’an. 2011. Hierarchical Translation Equivalence over Word Alignments. In ILLC Prepublication Series, PP-2011-38. University of Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL’02,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="1656" citStr="Papineni et al., 2002" startWordPosition="242" endWordPosition="245"> metrics computed over Permutation Forests (PEFs), packed charts of Permutation Trees (PETs), which are tree decompositions of a permutation into primitive ordering units. We empirically compare PEFs metric against five known reordering metrics on WMT13 data for ten language pairs. The PEFs metric shows better correlation with human ranking than the other metrics almost on all language pairs. None of the other metrics exhibits as stable behavior across language pairs. 1 Introduction Evaluating word order (also reordering) in MT is one of the main ingredients in automatic MT evaluation, e.g., (Papineni et al., 2002; Denkowski and Lavie, 2011). To monitor progress on evaluating reordering, recent work explores dedicated reordering evaluation metrics, cf. (Birch and Osborne, 2011; Isozaki et al., 2010; Talbot et al., 2011). Existing work computes the correlation between the ranking of the outputs of different systems by an evaluation metric to human ranking, on e.g., the WMT evaluation data. For evaluating reordering, it is necessary to word align system output with the corresponding reference translation. For convenience, a 1:1 alignment (a permutation) is induced between the words on both sides (Birch a</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL’02, pages 311–318, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milos Stanojevi´c</author>
<author>Khalil Sima’an</author>
</authors>
<title>BEER: BEtter Evaluation as Ranking.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>414--419</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<marker>Stanojevi´c, Sima’an, 2014</marker>
<rawString>Milo&amp;quot;s Stanojevi´c and Khalil Sima’an. 2014. BEER: BEtter Evaluation as Ranking. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 414–419, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Hideto Kazawa</author>
<author>Hiroshi Ichikawa</author>
<author>Jason Katz-Brown</author>
<author>Masakazu Seno</author>
<author>Franz Och</author>
</authors>
<title>A Lightweight Evaluation Framework for Machine Translation Reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>12--21</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="1866" citStr="Talbot et al., 2011" startWordPosition="274" endWordPosition="277">nst five known reordering metrics on WMT13 data for ten language pairs. The PEFs metric shows better correlation with human ranking than the other metrics almost on all language pairs. None of the other metrics exhibits as stable behavior across language pairs. 1 Introduction Evaluating word order (also reordering) in MT is one of the main ingredients in automatic MT evaluation, e.g., (Papineni et al., 2002; Denkowski and Lavie, 2011). To monitor progress on evaluating reordering, recent work explores dedicated reordering evaluation metrics, cf. (Birch and Osborne, 2011; Isozaki et al., 2010; Talbot et al., 2011). Existing work computes the correlation between the ranking of the outputs of different systems by an evaluation metric to human ranking, on e.g., the WMT evaluation data. For evaluating reordering, it is necessary to word align system output with the corresponding reference translation. For convenience, a 1:1 alignment (a permutation) is induced between the words on both sides (Birch and Osborne, 2011), possibly leaving words unaligned on either side. Existing work then concentrates on defining measures of reordering over permutations, cf. (Lapata, 2006; Birch and Osborne, 2011; Isozaki et a</context>
<context position="7581" citStr="Talbot et al. (2011)" startWordPosition="1195" endWordPosition="1198">s. The metric is available online as free software2. 2 Measures on permutations: Baselines In (Birch and Osborne, 2010; Birch and Osborne, 2011) Kendall’s tau and Hamming distance are combined with unigram BLEU (BLEU-1) leading to LRscore showing better correlation with human judgment than BLEU-4. Birch et al. (2010) additionally tests Ulam distance (longest common subsequence – LCS – normalized by the permutation length) and the square root of Kendall’s tau. Isozaki et al. (2010) presents a similar approach to (Birch and Osborne, 2011) additionally testing Spearman rho as a distance measure. Talbot et al. (2011) extracts a reordering measure from METEOR (Denkowski and Lavie, 2011) dubbed Fuzzy Reordering Score and evaluates it on MT reordering quality. 2https://github.com/stanojevic/beer h2,4,1,3i 2 h1,2i 4 h1,2i 5 6 1 3 h2,1i A h2,1i because B h2,1i A because B h2,1i 139 For an evaluation metric we need a function which would have the standard behaviour of evaluation metrics - the higher the score the better. Bellow we define the baseline metrics that were used in our experiments. Baselines A permutation over [1..n] (subrange of the positive integers where n &gt; 1) is a bijective function from [1..n] </context>
</contexts>
<marker>Talbot, Kazawa, Ichikawa, Katz-Brown, Seno, Och, 2011</marker>
<rawString>David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Jason Katz-Brown, Masakazu Seno, and Franz Och. 2011. A Lightweight Evaluation Framework for Machine Translation Reordering. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 12–21, Edinburgh, Scotland, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>3</volume>
<issue>23</issue>
<contexts>
<context position="10759" citStr="Wu (1997)" startWordPosition="1783" endWordPosition="1784">tions of 7r and rank them from the smallest to the largest. For every permutation 7r there is a minimum number of adjacent sub-permutations it can be factorized into (see e.g., (Gildea et al., 2006)). We will call this minimum number the arity of 7r and denote it with a(7r) (or simply a when 7r is understood from the context). For example, the arity of 7r = (5, 7, 4, 6, 3, 1, 2) is a = 2 because it can be split into a minimum of two sub-permutations (Figure 3), e.g. (5, 7, 4, 6, 3) and (1, 2) (but alternatively also (5, 7, 4, 6) and (3, 1,2)). In contrast, 7r = (2, 4, 1, 3) (also known as the Wu (1997) permutation) cannot be split into less than four subpermutations, i.e., a = 4. Factorization can be applied recursively to the sub-permutations of 7r, resulting in a tree structure (see Figure 3) called a permutation tree (PET) (Gildea et al., 2006; Zhang and Gildea, 2007; Maillette de Buy Wenniger and Sima’an, 2011). Some permutations factorize into multiple alternative PETs. For 7r = (4, 3, 2,1) there are five PETs shown in Figure 3. The alternative PETs can be packed into an O(n2) permutation forest (PEF). For many computational purposes, a single canonical PET is sufficient, cf. (Gildea e</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 3(23):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Factorization of Synchronous Context-Free Grammars in Linear Time.</title>
<date>2007</date>
<booktitle>In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST),</booktitle>
<pages>25--32</pages>
<contexts>
<context position="11032" citStr="Zhang and Gildea, 2007" startWordPosition="1827" endWordPosition="1830"> it with a(7r) (or simply a when 7r is understood from the context). For example, the arity of 7r = (5, 7, 4, 6, 3, 1, 2) is a = 2 because it can be split into a minimum of two sub-permutations (Figure 3), e.g. (5, 7, 4, 6, 3) and (1, 2) (but alternatively also (5, 7, 4, 6) and (3, 1,2)). In contrast, 7r = (2, 4, 1, 3) (also known as the Wu (1997) permutation) cannot be split into less than four subpermutations, i.e., a = 4. Factorization can be applied recursively to the sub-permutations of 7r, resulting in a tree structure (see Figure 3) called a permutation tree (PET) (Gildea et al., 2006; Zhang and Gildea, 2007; Maillette de Buy Wenniger and Sima’an, 2011). Some permutations factorize into multiple alternative PETs. For 7r = (4, 3, 2,1) there are five PETs shown in Figure 3. The alternative PETs can be packed into an O(n2) permutation forest (PEF). For many computational purposes, a single canonical PET is sufficient, cf. (Gildea et al., 2006). However, while different PETs of 7r exhibit the same reordering pattern, their different binary branching structures might indicate important differences as we show in our experiments. A permutation forest (akin to a parse forest) F for 7r (over [1..n]) is a </context>
<context position="14403" citStr="Zhang and Gildea, 2007" startWordPosition="2484" endWordPosition="2487">tion a(7r). If a = 1, a single leaf node is stored with an empty set of inferences. If a &gt; 1 then the algorithm computes all possible factorizations of 7r into a sub-permutations (a sequence of a −1 split points) and stores their inferences together as Iji and their operator Oji associated with a node in entry [[i, j, Iji , Oji ]]. Subsequently, the algorithm applies recursively to each sub-permutation. Efficiency is a topic beyond the scope of this paper, but this naive algorithm has worst case time complexity O(n3), and when computing only a single canonical PET this can be O(n) (see e.g., (Zhang and Gildea, 2007)). Function PEF (i, j, 7r, F); # Args: sub-perm. π over [i..j] and forest Y Output: Parse-Forest F(7r) for 7r; F := F ∪ {[[i, j,Iji ,Oji]]}; Return F; end; Figure 5: Pseudo-code of permutation-forest factorization algorithm. Function a(7r) returns the arity of 7r. Function RankListOf(r1, ... , rm) returns the list of rank positions (i.e., a permutation) of sub-permutations r1, ... , rm after sorting them smallest first. The top-level call to this algorithm uses 7r, i = 0, j = n and F = ∅. Our measure (PEFscore) uses a function opScore(p) which assigns a score to a given operator, which can be </context>
</contexts>
<marker>Zhang, Gildea, 2007</marker>
<rawString>Hao Zhang and Daniel Gildea. 2007. Factorization of Synchronous Context-Free Grammars in Linear Time. In NAACL Workshop on Syntax and Structure in Statistical Translation (SSST), pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
<author>David Chiang</author>
</authors>
<title>Extracting synchronous grammar rules from wordlevel alignments in linear time.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1,</booktitle>
<pages>1081--1088</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="12658" citStr="Zhang et al., 2008" startWordPosition="2158" endWordPosition="2161">(2,1) (2,4,1,3) 5 7 4 6 (1,2) 3 1 2 (2,1) 4 (2,1) 3 (2,1) 2 1 (2,1) 4 (2,1) (2,1) 3 2 (2,1) (2,1) (2,1) (2,1) (2,1) 1 4 3 2 1 (2,1) 4 3 (2,1) (2,1) 4 (2,1) 3 2 1 2 1 Figure 3: A PET for 7r = h5, 7, 4, 6, 3, 1, 2i. And five different PETs for 7r = h4, 3, 2, 1i. 1)), then each inference consists of a a − 1-tuple [l1, ... , la−1], where for each 1 ≤ x ≤ (a − 1), lx is a “split point” which is given by the index of the last integer in the xth sub-permutation in 7r. The permutation of the a sub-permutations (“children” of 7rji+1) is stored in Oji and it is the same for all inferences of that span (Zhang et al., 2008). Figure 4: The factorizations of 7r = h4, 3, 2, 1i. Let us exemplify the inferences on 7r = h4, 3, 2,1i (see Figure 4) which factorizes into pairs of sub-permutations (a = 2): a split point can be at positions with index l1 ∈ {1, 2, 3}. Each of these split points (factorizations) of 7r will be represented as an inference for the same root node which covers the whole of 7r (placed in entry [0, 4]); the operator of the inference here consists of the permutation h2, 1i (swapping the two ranges covered by the children sub-permutations) and inference consists of a − 1 indexes l1, ... , la−1 signif</context>
</contexts>
<marker>Zhang, Gildea, Chiang, 2008</marker>
<rawString>Hao Zhang, Daniel Gildea, and David Chiang. 2008. Extracting synchronous grammar rules from wordlevel alignments in linear time. In Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 1081–1088. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
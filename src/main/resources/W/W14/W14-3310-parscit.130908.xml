<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005347">
<title confidence="0.893169">
EU-BRIDGE MT: Combined Machine Translation
</title>
<author confidence="0.93403875">
*Markus Freitag, *Stephan Peitz, *Joern Wuebker, *Hermann Ney,
*Matthias Huck, *Rico Sennrich, *Nadir Durrani,
*Maria Nadejde, *Philip Williams, *Philipp Koehn,
†Teresa Herrmann, †Eunah Cho, †Alex Waibel
</author>
<affiliation confidence="0.999607666666667">
*RWTH Aachen University, Aachen, Germany
*University of Edinburgh, Edinburgh, Scotland
†Karlsruhe Institute of Technology, Karlsruhe, Germany
</affiliation>
<email confidence="0.961157">
*{freitag,peitz,wuebker,ney}@cs.rwth-aachen.de
*{mhuck,ndurrani,pkoehn}@inf.ed.ac.uk
*v1rsennr@staffmail.ed.ac.uk
*maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk
†{teresa.herrmann,eunah.cho,alex.waibel}@kit.edu
</email>
<sectionHeader confidence="0.995541" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999806259259259">
This paper describes one of the col-
laborative efforts within EU-BRIDGE to
further advance the state of the art in
machine translation between two Euro-
pean language pairs, German→English
and English→German. Three research
institutes involved in the EU-BRIDGE
project combined their individual machine
translation systems and participated with a
joint setup in the shared translation task of
the evaluation campaign at the ACL 2014
Eighth Workshop on Statistical Machine
Translation (WMT 2014).
We combined up to nine different machine
translation engines via system combina-
tion. RWTH Aachen University, the Uni-
versity of Edinburgh, and Karlsruhe In-
stitute of Technology developed several
individual systems which serve as sys-
tem combination input. We devoted spe-
cial attention to building syntax-based sys-
tems and combining them with the phrase-
based ones. The joint setups yield em-
pirical gains of up to 1.6 points in BLEU
and 1.0 points in TER on the WMT news-
test2013 test set compared to the best sin-
gle systems.
</bodyText>
<sectionHeader confidence="0.996708" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.811674333333333">
EU-BRIDGE1 is a European research project
which is aimed at developing innovative speech
translation technology. This paper describes a
</bodyText>
<footnote confidence="0.918365">
1http://www.eu-bridge.eu
</footnote>
<bodyText confidence="0.998793866666667">
joint WMT submission of three EU-BRIDGE
project partners. RWTH Aachen University
(RWTH), the University of Edinburgh (UEDIN)
and Karlsruhe Institute of Technology (KIT) all
provided several individual systems which were
combined by means of the RWTH Aachen system
combination approach (Freitag et al., 2014). As
distinguished from our EU-BRIDGE joint submis-
sion to the IWSLT 2013 evaluation campaign (Fre-
itag et al., 2013), we particularly focused on trans-
lation of news text (instead of talks) for WMT. Be-
sides, we put an emphasis on engineering syntax-
based systems in order to combine them with our
more established phrase-based engines. We built
combined system setups for translation from Ger-
man to English as well as from English to Ger-
man. This paper gives some insight into the tech-
nology behind the system combination framework
and the combined engines which have been used
to produce the joint EU-BRIDGE submission to
the WMT 2014 translation task.
The remainder of the paper is structured as fol-
lows: We first describe the individual systems by
RWTH Aachen University (Section 2), the Uni-
versity of Edinburgh (Section 3), and Karlsruhe
Institute of Technology (Section 4). We then
present the techniques for machine translation sys-
tem combination in Section 5. Experimental re-
sults are given in Section 6. We finally conclude
the paper with Section 7.
</bodyText>
<sectionHeader confidence="0.990848" genericHeader="method">
2 RWTH Aachen University
</sectionHeader>
<bodyText confidence="0.9981725">
RWTH (Peitz et al., 2014) employs both the
phrase-based (RWTH scss) and the hierarchical
(RWTH hiero) decoder implemented in RWTH’s
publicly available translation toolkit Jane (Vilar
</bodyText>
<page confidence="0.989131">
105
</page>
<note confidence="0.715411">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9983064">
et al., 2010; Wuebker et al., 2012). The model
weights of all systems have been tuned with stan-
dard Minimum Error Rate Training (Och, 2003)
on a concatenation of the newstest2011 and news-
test2012 sets. RWTH used BLEU as optimiza-
tion objective. Both for language model estima-
tion and querying at decoding, the KenLM toolkit
(Heafield et al., 2013) is used. All RWTH sys-
tems include the standard set of models provided
by Jane. Both systems have been augmented with
a hierarchical orientation model (Galley and Man-
ning, 2008; Huck et al., 2013) and a cluster lan-
guage model (Wuebker et al., 2013). The phrase-
based system (RWTH scss) has been further im-
proved by maximum expected BLEU training sim-
ilar to (He and Deng, 2012). The latter has been
performed on a selection from the News Commen-
tary, Europarl and Common Crawl corpora based
on language and translation model cross-entropies
(Mansour et al., 2011).
</bodyText>
<sectionHeader confidence="0.824008" genericHeader="method">
3 University of Edinburgh
</sectionHeader>
<bodyText confidence="0.994815666666667">
UEDIN contributed phrase-based and syntax-
based systems to both the German→English and
the English→German joint submission.
</bodyText>
<subsectionHeader confidence="0.997915">
3.1 Phrase-based Systems
</subsectionHeader>
<bodyText confidence="0.99984543902439">
UEDIN’s phrase-based systems (Durrani et al.,
2014) have been trained using the Moses toolkit
(Koehn et al., 2007), replicating the settings de-
scribed in (Durrani et al., 2013b). The features
include: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA++ align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, a lexically-driven 5-gram
operation sequence model (OSM) (Durrani et al.,
2013a), msd-bidirectional-fe lexicalized reorder-
ing, sparse lexical and domain features (Hasler
et al., 2012), a distortion limit of 6, a maxi-
mum phrase length of 5, 100-best translation op-
tions, Minimum Bayes Risk decoding (Kumar and
Byrne, 2004), cube pruning (Huang and Chiang,
2007), with a stack size of 1000 during tuning and
5000 during testing and the no-reordering-over-
punctuation heuristic. UEDIN uses POS and mor-
phological target sequence models built on the in-
domain subset of the parallel corpus using Kneser-
Ney smoothed 7-gram models as additional factors
in phrase translation models (Koehn and Hoang,
2007). UEDIN has furthermore built OSM mod-
els over POS and morph sequences following
Durrani et al. (2013c). The English→German
system additionally comprises a target-side LM
over automatically built word classes (Birch et
al., 2013). UEDIN has applied syntactic pre-
reordering (Collins et al., 2005) and compound
splitting (Koehn and Knight, 2003) of the source
side for the German→English system. The sys-
tems have been tuned on a very large tuning set
consisting of the test sets from 2008-2012, with
a total of 13,071 sentences. UEDIN used news-
test2013 as held-out test set. On top of UEDIN
phrase-based 1 system, UEDIN phrase-based 2
augments word classes as additional factor and
learns an interpolated target sequence model over
cluster IDs. Furthermore, it learns OSM models
over POS, morph and word classes.
</bodyText>
<subsectionHeader confidence="0.998448">
3.2 Syntax-based Systems
</subsectionHeader>
<bodyText confidence="0.97309803125">
UEDIN’s syntax-based systems (Williams et al.,
2014) follow the GHKM syntax approach as pro-
posed by Galley, Hopkins, Knight, and Marcu
(Galley et al., 2004). The open source Moses
implementation has been employed to extract
GHKM rules (Williams and Koehn, 2012). Com-
posed rules (Galley et al., 2006) are extracted in
addition to minimal rules, but only up to the fol-
lowing limits: at most twenty tree nodes per rule,
a maximum depth of five, and a maximum size of
five. Singleton hierarchical rules are dropped.
The features for the syntax-based systems com-
prise Good-Turing-smoothed phrase translation
probabilities, lexical translation probabilities in
both directions, word and phrase penalty, a rule
rareness penalty, a monolingual PCFG probability,
and a 5-gram language model. UEDIN has used
the SRILM toolkit (Stolcke, 2002) to train the lan-
guage model and relies on KenLM for language
model scoring during decoding. Model weights
are optimized to maximize BLEU. 2000 sentences
from the newstest2008-2012 sets have been se-
lected as a development set. The selected sen-
tences obtained high sentence-level BLEU scores
when being translated with a baseline phrase-
based system, and each contain less than 30 words
for more rapid tuning. Decoding for the syntax-
based systems is carried out with cube pruning
using Moses’ hierarchical decoder (Hoang et al.,
2009).
UEDIN’s German→English syntax-based setup
is a string-to-tree system with compound splitting
</bodyText>
<page confidence="0.997532">
106
</page>
<bodyText confidence="0.978907857142857">
on the German source-language side and syntactic
annotation from the Berkeley Parser (Petrov et al.,
2006) on the English target-language side.
For English—*German, UEDIN has trained var-
ious string-to-tree GHKM syntax systems which
differ with respect to the syntactic annotation. A
tree-to-string system and a string-to-string system
(with rules that are not syntactically decorated)
have been trained as well. The English—*German
UEDIN GHKM system names in Table 3 denote:
UEDIN GHKM S2T (ParZu): A string-to-tree
system trained with target-side syntactic an-
notation obtained with ParZu (Sennrich et
al., 2013). It uses a modified syntactic label
set, target-side compound splitting, and addi-
tional syntactic constraints.
UEDIN GHKM S2T (BitPar): A string-to-tree
system trained with target-side syntactic
annotation obtained with BitPar (Schmid,
2004).
UEDIN GHKM S2T (Stanford): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Stan-
ford Parser (Rafferty and Manning, 2008a).
UEDIN GHKM S2T (Berkeley): A string-to-
tree system trained with target-side syntactic
annotation obtained with the German Berke-
ley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
UEDIN GHKM T2S (Berkeley): A tree-to-
string system trained with source-side syn-
tactic annotation obtained with the English
Berkeley Parser (Petrov et al., 2006).
UEDIN GHKM S2S (Berkeley): A string-to-
string system. The extraction is GHKM-
based with syntactic target-side annotation
from the German Berkeley Parser, but we
strip off the syntactic labels. The final gram-
mar contains rules with a single generic non-
terminal instead of syntactic ones, plus rules
that have been added from plain phrase-based
extraction (Huck et al., 2014).
</bodyText>
<sectionHeader confidence="0.991552" genericHeader="method">
4 Karlsruhe Institute of Technology
</sectionHeader>
<bodyText confidence="0.999975418181819">
The KIT translations (Herrmann et al., 2014) are
generated by an in-house phrase-based transla-
tions system (Vogel, 2003). The provided News
Commentary, Europarl, and Common Crawl par-
allel corpora are used for training the translation
model. The monolingual part of those parallel
corpora, the News Shuffle corpus for both direc-
tions and additionally the Gigaword corpus for
German—*English are used as monolingual train-
ing data for the different language models. Opti-
mization is done with Minimum Error Rate Train-
ing as described in (Venugopal et al., 2005), using
newstest2012 and newstest2013 as development
and test data respectively.
Compound splitting (Koehn and Knight, 2003)
is performed on the source side of the corpus for
German—*English translation before training. In
order to improve the quality of the web-crawled
Common Crawl corpus, noisy sentence pairs are
filtered out using an SVM classifier as described
by Mediani et al. (2011).
The word alignment for German—*English is
generated using the GIZA++ toolkit (Och and Ney,
2003). For English—*German, KIT uses discrimi-
native word alignment (Niehues and Vogel, 2008).
Phrase extraction and scoring is done using the
Moses toolkit (Koehn et al., 2007). Phrase pair
probabilities are computed using modified Kneser-
Ney smoothing as in (Foster et al., 2006).
In both systems KIT applies short-range re-
orderings (Rottmann and Vogel, 2007) and long-
range reorderings (Niehues and Kolss, 2009)
based on POS tags (Schmid, 1994) to perform
source sentence reordering according to the target
language word order. The long-range reordering
rules are applied to the training corpus to create
reordering lattices to extract the phrases for the
translation model. In addition, a tree-based re-
ordering model (Herrmann et al., 2013) trained
on syntactic parse trees (Rafferty and Manning,
2008b; Klein and Manning, 2003) as well as a lex-
icalized reordering model (Koehn et al., 2005) are
applied.
Language models are trained with the SRILM
toolkit (Stolcke, 2002) and use modified Kneser-
Ney smoothing. Both systems utilize a lan-
guage model based on automatically learned
word classes using the MKCLS algorithm (Och,
1999). The English—*German system comprises
language models based on fine-grained part-of-
speech tags (Schmid and Laws, 2008). In addi-
tion, a bilingual language model (Niehues et al.,
2011) is used as well as a discriminative word lex-
icon (Mauser et al., 2009) using source context to
guide the word choices in the target sentence.
</bodyText>
<page confidence="0.994133">
107
</page>
<bodyText confidence="0.999913266666667">
In total, the English→German system uses the
following language models: two 4-gram word-
based language models trained on the parallel data
and the filtered Common Crawl data separately,
two 5-gram POS-based language models trained
on the same data as the word-based language mod-
els, and a 4-gram cluster-based language model
trained on 1,000 MKCLS word classes.
The German→English system uses a 4-gram
word-based language model trained on all mono-
lingual data and an additional language model
trained on automatically selected data (Moore and
Lewis, 2010). Again, a 4-gram cluster-based
language model trained on 1000 MKCLS word
classes is applied.
</bodyText>
<sectionHeader confidence="0.989073" genericHeader="method">
5 System Combination
</sectionHeader>
<bodyText confidence="0.999961931034483">
System combination is used to produce consen-
sus translations from multiple hypotheses which
are outputs of different translation engines. The
consensus translations can be better in terms of
translation quality than any of the individual hy-
potheses. To combine the engines of the project
partners for the EU-BRIDGE joint setups, we ap-
ply a system combination implementation that has
been developed at RWTH Aachen University.
The implementation of RWTH’s approach to
machine translation system combination is de-
scribed in (Freitag et al., 2014). This approach
includes an enhanced alignment and reordering
framework. Alignments between the system out-
puts are learned using METEOR (Banerjee and
Lavie, 2005). A confusion network is then built
using one of the hypotheses as “primary” hypoth-
esis. We do not make a hard decision on which
of the hypotheses to use for that, but instead com-
bine all possible confusion networks into a single
lattice. Majority voting on the generated lattice
is performed using the prior probabilities for each
system as well as other statistical models, e.g. a
special n-gram language model which is learned
on the input hypotheses. Scaling factors of the
models are optimized using the Minimum Error
Rate Training algorithm. The translation with the
best total score within the lattice is selected as con-
sensus translation.
</bodyText>
<sectionHeader confidence="0.999969" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999706818181818">
In this section, we present our experimental results
on the two translation tasks, German→English
and English→German. The weights of the in-
dividual system engines have been optimized on
different test sets which partially or fully include
newstest2011 or newstest2012. System combina-
tion weights are either optimized on newstest2011
or newstest2012. We kept newstest2013 as an un-
seen test set which has not been used for tuning
the system combination or any of the individual
systems.
</bodyText>
<subsectionHeader confidence="0.99711">
6.1 German→English
</subsectionHeader>
<bodyText confidence="0.999984297297297">
The automatic scores of all individual systems
as well as of our final system combination sub-
mission are given in Table 1. KIT, UEDIN and
RWTH are each providing one individual phrase-
based system output. RWTH (hiero) and UEDIN
(GHKM) are providing additional systems based
on the hierarchical translation model and a string-
to-tree syntax model. The pairwise difference
of the single system performances is up to 1.3
points in BLEU and 2.5 points in TER. For
German→English, our system combination pa-
rameters are optimized on newstest2012. System
combination gives us a gain of 1.6 points in BLEU
and 1.0 points in TER for newstest2013 compared
to the best single system.
In Table 2 the pairwise BLEU scores for all in-
dividual systems as well as for the system combi-
nation output are given. The pairwise BLEU score
of both RWTH systems (taking one as hypothesis
and the other one as reference) is the highest for all
pairs of individual system outputs. A high BLEU
score means similar hypotheses. The syntax-based
system of UEDIN and RWTH scss differ mostly,
which can be observed from the fact of the low-
est pairwise BLEU score. Furthermore, we can
see that better performing individual systems have
higher BLEU scores when evaluating against the
system combination output.
In Figure 1 system combination output is com-
pared to the best single system KIT. We distribute
the sentence-level BLEU scores of all sentences of
newstest2013. To allow for sentence-wise evalu-
ation, all bi-, tri-, and four-gram counts are ini-
tialized with 1 instead of 0. Many sentences have
been improved by system combination. Neverthe-
less, some sentences fall off in quality compared
to the individual system output of KIT.
</bodyText>
<subsectionHeader confidence="0.99714">
6.2 English→German
</subsectionHeader>
<bodyText confidence="0.996258">
The results of all English→German system setups
are given in Table 3. For the English→German
translation task, only UEDIN and KIT are con-
</bodyText>
<page confidence="0.98386">
108
</page>
<table confidence="0.999953375">
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
KIT 25.0 57.6 25.2 57.4 27.5 54.4
UEDIN 23.9 59.2 24.7 58.3 27.4 55.0
RWTH scss 23.6 59.5 24.2 58.5 27.0 55.0
RWTH hiero 23.3 59.9 24.1 59.0 26.7 55.9
UEDIN GHKM S2T (Berkeley) 23.0 60.1 23.2 60.8 26.2 56.9
syscom 25.6 57.1 26.4 56.5 29.1 53.4
</table>
<tableCaption confidence="0.8314105">
Table 1: Results for the German→English translation task. The system combination is tuned on news-
test2012, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly better than the best single system
with p &lt; 0.05.
</tableCaption>
<table confidence="0.999969285714286">
KIT UEDIN RWTH scss RWTH hiero UEDIN S2T syscom
KIT 59.07 57.60 57.91 55.62 77.68
UEDIN 59.17 56.96 57.84 59.89 72.89
RWTH scss 57.64 56.90 64.94 53.10 71.16
RWTH hiero 57.98 57.80 64.97 55.73 70.87
UEDIN S2T 55.75 59.95 53.19 55.82 65.35
syscom 77.76 72.83 71.17 70.85 65.24
</table>
<tableCaption confidence="0.857239">
Table 2: Cross BLEU scores for the German→English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as hypothesis and the other one as reference.)
</tableCaption>
<table confidence="0.999977583333334">
system newstest2011 newstest2012 newstest2013
BLEU TER BLEU TER BLEU TER
UEDIN phrase-based 1 17.5 67.3 18.2 65.0 20.5 62.7
UEDIN phrase-based 2 17.8 66.9 18.5 64.6 20.8 62.3
UEDIN GHKM S2T (ParZu) 17.2 67.6 18.0 65.5 20.2 62.8
UEDIN GHKM S2T (BitPar) 16.3 69.0 17.3 66.6 19.5 63.9
UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2
UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8
UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8
UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3
KIT 17.1 67.0 17.8 64.8 20.2 62.2
syscom 18.4 65.0 18.7 63.4 21.3 60.6
</table>
<tableCaption confidence="0.992614">
Table 3: Results for the English→German translation task. The system combination is tuned on news-
</tableCaption>
<bodyText confidence="0.928930090909091">
test2011, newstest2013 is used as held-out test set for all individual systems and system combination.
Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than
the best single system with p &lt; 0.05. Italic font indicates system combination results that are significantly
better than the best single system with p &lt; 0.1.
tributing individual systems. KIT is providing a
phrase-based system output, UEDIN is providing
two phrase-based system outputs and six syntax-
based ones (GHKM). For English→German, our
system combination parameters are optimized on
newstest2011. Combining all nine different sys-
tem outputs yields an improvement of 0.5 points
in BLEU and 1.7 points in TER over the best sin-
gle system performance.
In Table 4 the cross BLEU scores for all
English→German systems are given. The individ-
ual system of KIT and the syntax-based ParZu sys-
tem of UEDIN have the lowest BLEU score when
scored against each other. Both approaches are
quite different and both are coming from differ-
ent institutes. In contrast, both phrase-based sys-
tems pbt 1 and pbt 2 from UEDIN are very sim-
ilar and hence have a high pairwise BLEU score.
</bodyText>
<page confidence="0.995889">
109
</page>
<table confidence="0.989849090909091">
pbt 1 pbt 2 ParZu BitPar Stanford S2T T2S S2S KIT syscom
pbt 1 75.84 51.61 53.93 55.32 54.79 54.52 60.92 54.80 70.12
pbt 2 75.84 51.96 53.39 53.93 53.97 53.10 57.32 54.04 73.75
ParZu 51.57 51.91 56.67 55.11 56.05 52.13 51.22 48.14 68.39
BitPar 54.00 53.45 56.78 64.59 65.67 56.33 56.62 49.23 62.08
Stanford 55.37 53.98 55.19 64.56 69.22 58.81 61.19 50.50 61.51
S2T 54.83 54.02 56.14 65.64 69.21 59.32 60.16 50.07 62.81
T2S 54.57 53.15 52.21 56.30 58.81 59.32 59.34 50.01 63.13
S2S 60.96 57.36 51.29 56.59 61.18 60.15 59.33 53.68 60.46
KIT 54.75 53.98 48.13 49.13 50.41 49.98 49.93 53.59 63.33
syscom 70.01 73.63 68.32 61.92 61.37 62.67 62.99 60.32 63.27
</table>
<tableCaption confidence="0.995452">
Table 4: Cross BLEU scores for the German-*English newstest2013 test set. (Pairwise BLEU scores:
each entry is taking the horizontal system as reference and the other one as hypothesis.)
</tableCaption>
<figure confidence="0.999552866666666">
amount sentences
400
350
300
250
200
150
100
50
0
better
same
worse
amount sentences
400
350
300
250
200
150
100
50
0
better
same
worse
0 20 40 60 80 100
sBLEU
0 20 40 60 80 100
sBLEU
</figure>
<figureCaption confidence="0.9940855">
Figure 1: Sentence distribution for the
German-*English newstest2013 test set compar-
ing system combination output against the best
individual system.
</figureCaption>
<bodyText confidence="0.999953166666667">
As for the German-*English translation direction,
the best performing individual system outputs are
also having the highest BLEU scores when evalu-
ated against the final system combination output.
In Figure 2 system combination output is com-
pared to the best single system pbt 2. We distribute
the sentence-level BLEU scores of all sentences
of newstest2013. Many sentences have been im-
proved by system combination. But there is still
room for improvement as some sentences are still
better in terms of sentence-level BLEU in the indi-
vidual best system pbt 2.
</bodyText>
<sectionHeader confidence="0.997694" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.99160075">
We achieved significantly better translation perfor-
mance with gains of up to +1.6 points in BLEU
and -1.0 points in TER by combining up to nine
different machine translation systems. Three dif-
ferent research institutes (RWTH Aachen Univer-
sity, University of Edinburgh, Karlsruhe Institute
of Technology) provided machine translation en-
gines based on different approaches like phrase-
</bodyText>
<figureCaption confidence="0.9874395">
Figure 2: Sentence distribution for the
English-*German newstest2013 test set compar-
ing system combination output against the best
individual system.
</figureCaption>
<bodyText confidence="0.9998791">
based, hierarchical phrase-based, and syntax-
based. For English-*German, we included six
different syntax-based systems, which were com-
bined to our final combined translation. The au-
tomatic scores of all submitted system outputs for
the actual 2014 evaluation set are presented on the
WMT submission page.2 Our joint submission is
the best submission in terms of BLEU and TER for
both translation directions German-*English and
English-*German without adding any new data.
</bodyText>
<sectionHeader confidence="0.989524" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99893125">
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement no 287658.
</bodyText>
<footnote confidence="0.75280875">
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1 148717.
2http://matrix.statmt.org/
</footnote>
<page confidence="0.996062">
110
</page>
<sectionHeader confidence="0.802334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983131733944954">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Intrinsic and Extrin-
sic Evaluation Measures for MT and/or Summariza-
tion, pages 65–72, Ann Arbor, MI, USA, June.
Alexandra Birch, Nadir Durrani, and Philipp Koehn.
2013. Edinburgh SLT and MT System Description
for the IWSLT 2013 Evaluation. In Proceedings
of the 10th International Workshop on Spoken Lan-
guage Translation, pages 40–48, Heidelberg, Ger-
many, December.
Maximilian Bisani and Hermann Ney. 2004. Bootstrap
Estimates for Confidence Intervals in ASR Perfor-
mance Evaluation. In IEEE International Confer-
ence on Acoustics, Speech, and Signal Processing,
volume 1, pages 409–412, Montr´eal, Canada, May.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Ma-
chine Translation. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL’05), pages 531–540, Ann Arbor,
Michigan, June.
Nadir Durrani, Alexander Fraser, Helmut Schmid,
Hieu Hoang, and Philipp Koehn. 2013a. Can
Markov Models Over Minimal Translation Units
Help Phrase-Based SMT? In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, Sofia, Bulgaria, August.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013b. Edinburgh’s Machine Trans-
lation Systems for European Language Pairs. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria, August.
Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-
san Sajjad, and Richard Farkas. 2013c. Munich-
Edinburgh-Stuttgart Submissions of OSM Systems
at WMT13. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, Sofia, Bulgaria.
Nadir Durrani, Barry Haddow, Philipp Koehn, and
Kenneth Heafield. 2014. Edinburgh’s Phrase-based
Machine Translation Systems for WMT-14. In Pro-
ceedings of the ACL 2014 Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, MD, USA,
June.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrasetable Smoothing for Statistical Ma-
chine Translation. In EMNLP, pages 53–61.
M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Dur-
rani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues,
M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi,
M. Cettolo, and M. Federico. 2013. EU-BRIDGE
MT: Text Translation of Talks in the EU-BRIDGE
Project. In International Workshop on Spoken Lan-
guage Translation, Heidelberg, Germany, Decem-
ber.
Markus Freitag, Matthias Huck, and Hermann Ney.
2014. Jane: Open Source Machine Translation Sys-
tem Combination. In Conference of the European
Chapter of the Association for Computational Lin-
guistics, Gothenburg, Sweden, April.
Michel Galley and Christopher D. Manning. 2008. A
Simple and Effective Hierarchical Phrase Reorder-
ing Model. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 847–855, Honolulu, HI, USA, Octo-
ber.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proc. of the Human Language Technology Conf.
/North American Chapter of the Assoc. for Compu-
tational Linguistics (HLT-NAACL), pages 273–280,
Boston, MA, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable Inference and Training
of Context-Rich Syntactic Translation Models. In
Proc. of the 21st International Conf. on Computa-
tional Linguistics and 44th Annual Meeting of the
Assoc. for Computational Linguistics, pages 961–
968, Sydney, Australia, July.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised features and Topic Adaptation
for SMT. In Proceedings of the seventh Interna-
tional Workshop on Spoken Language Translation
(IWSLT), pages 268–275.
Xiaodong He and Li Deng. 2012. Maximum Expected
BLEU Training of Phrase and Lexicon Translation
Models. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 292–301, Jeju, Republic of Korea,
July.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics, pages 690–696,
Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187–197, Edinburgh, Scotland, UK, July.
Teresa Herrmann, Jan Niehues, and Alex Waibel.
2013. Combining Word Reordering Methods on
different Linguistic Abstraction Levels for Statisti-
cal Machine Translation. In Proceedings of the Sev-
enth Workshop on Syntax, Semantics and Structure
in Statistical Translation, Atlanta, GA, USA, June.
</reference>
<page confidence="0.994138">
111
</page>
<reference confidence="0.997111513761468">
Teresa Herrmann, Mohammed Mediani, Eunah Cho,
Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi
Zhang, and Alex Waibel. 2014. The Karlsruhe In-
stitute of Technology Translation Systems for the
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009.
A Unified Framework for Phrase-Based, Hierarchi-
cal, and Syntax-Based Statistical Machine Transla-
tion. pages 152–159, Tokyo, Japan, December.
Liang Huang and David Chiang. 2007. Forest Rescor-
ing: Faster Decoding with Integrated Language
Models. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 144–151, Prague, Czech Republic, June.
Matthias Huck, Joern Wuebker, Felix Rietig, and Her-
mann Ney. 2013. A Phrase Orientation Model
for Hierarchical Machine Translation. In ACL 2013
Eighth Workshop on Statistical Machine Transla-
tion, pages 452–463, Sofia, Bulgaria, August.
Matthias Huck, Hieu Hoang, and Philipp Koehn.
2014. Augmenting String-to-Tree and Tree-to-
String Translation with Non-Syntactic Phrases. In
Proceedings of the ACL 2014 Ninth Workshop on
Statistical Machine Translation, Baltimore, MD,
USA, June.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of ACL
2003.
Philipp Koehn and Hieu Hoang. 2007. Factored Trans-
lation Models. In EMNLP-CoNLL, pages 868–876,
Prague, Czech Republic, June.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne,
Chris Callison-Burch, Miles Osborne, and David
Talbot. 2005. Edinburgh System Description for
the 2005 IWSLT Speech Translation Evaluation. In
Proceedings of the International Workshop on Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, pages 177–180,
Prague, Czech Republic, June.
Shankar Kumar and William Byrne. 2004. Mini-
mum Bayes-Risk Decoding for Statistical Machine
Translation. In Proc. Human Language Technol-
ogy Conf. /North American Chapter of the Associa-
tion for Computational Linguistics Annual Meeting
(HLT-NAACL), pages 169–176, Boston, MA, USA,
May.
Saab Mansour, Joern Wuebker, and Hermann Ney.
2011. Combining Translation and Language Model
Scoring for Domain-Specific Data Filtering. In Pro-
ceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 222–229, San
Francisco, CA, USA, December.
Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009.
Extending Statistical Machine Translation with Dis-
criminative and Trigger-Based Lexicon Models. In
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 210–217, Singapore, Au-
gust.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT
English-French Translation systems for IWSLT
2011. In Proceedings of the Eight Interna-
tional Workshop on Spoken Language Translation
(IWSLT), San Francisco, CA, USA.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Pro-
ceedings of the ACL 2010 Conference Short Papers,
pages 220–224, Uppsala, Sweden, July.
Jan Niehues and Muntsin Kolss. 2009. A POS-Based
Model for Long-Range Reorderings in SMT. In
Fourth Workshop on Statistical Machine Translation
(WMT 2009), Athens, Greece.
Jan Niehues and Stephan Vogel. 2008. Discriminative
Word Alignment via Alignment Matrix Modeling.
In Proceedings of Third ACL Workshop on Statisti-
cal Machine Translation, Columbus, USA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and
Alex Waibel. 2011. Wider Context by Using Bilin-
gual Language Models in Machine Translation. In
Sixth Workshop on Statistical Machine Translation
(WMT 2011), Edinburgh, UK.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 1999. An Efficient Method for De-
termining Bilingual Word Classes. In EACL’99.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of the
41th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 160–167, Sapporo,
Japan, July.
Stephan Peitz, Joern Wuebker, Markus Freitag, and
Hermann Ney. 2014. The RWTH Aachen German-
English Machine Translation System for WMT
2014. In Proceedings of the ACL 2014 Ninth Work-
shop on Statistical Machine Translation, Baltimore,
MD, USA, June.
</reference>
<page confidence="0.98222">
112
</page>
<reference confidence="0.99985934375">
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ’08, pages
33–39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning Accurate, Compact, and In-
terpretable Tree Annotation. In Proc. of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Assoc. for
Computational Linguistics, pages 433–440, Sydney,
Australia, July.
Anna N. Rafferty and Christopher D. Manning. 2008a.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German atACL ’08, pages 40–
46, Columbus, OH, USA, June.
Anna N. Rafferty and Christopher D. Manning. 2008b.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
Workshop on Parsing German.
Kay Rottmann and Stephan Vogel. 2007. Word Re-
ordering in Statistical Machine Translation with a
POS-Based Distortion Model. In Proceedings of
the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation
(TMI), Sk¨ovde, Sweden.
Helmut Schmid and Florian Laws. 2008. Estimation
of Conditional Probabilities with Decision Trees and
an Application to Fine-Grained POS Tagging. In
COLING 2008, Manchester, UK.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In International
Conference on New Methods in Language Process-
ing, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601–609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM – An Extensible Lan-
guage Modeling Toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901–904, Denver, CO, USA, Septem-
ber.
Ashish Venugopal, Andreas Zollman, and Alex Waibel.
2005. Training and Evaluation Error Minimization
Rules for Statistical Machine Translation. In Work-
shop on Data-drive Machine Translation and Be-
yond (WPT-05), Ann Arbor, Michigan, USA.
David Vilar, Daniel Stein, Matthias Huck, and Her-
mann Ney. 2010. Jane: Open Source Hierarchi-
cal Translation, Extended with Reordering and Lex-
icon Models. In ACL 2010 Joint Fifth Workshop on
Statistical Machine Translation and Metrics MATR,
pages 262–270, Uppsala, Sweden, July.
Stephan Vogel. 2003. SMT Decoder Dissected: Word
Reordering. In International Conference on Natural
Language Processing and Knowledge Engineering,
Beijing, China.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation (WMT), pages 388–394,
Montr´eal, Canada, June.
Philip Williams, Rico Sennrich, Maria Nadejde,
Matthias Huck, Eva Hasler, and Philipp Koehn.
2014. Edinburgh’s Syntax-Based Systems at
WMT 2014. In Proceedings of the ACL 2014 Ninth
Workshop on Statistical Machine Translation, Balti-
more, MD, USA, June.
Joern Wuebker, Matthias Huck, Stephan Peitz, Malte
Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab
Mansour, and Hermann Ney. 2012. Jane 2:
Open Source Phrase-based and Hierarchical Statisti-
cal Machine Translation. In COLING ’12: The 24th
Int. Conf. on Computational Linguistics, pages 483–
491, Mumbai, India, December.
Joern Wuebker, Stephan Peitz, Felix Rietig, and Her-
mann Ney. 2013. Improving Statistical Machine
Translation with Word Class Models. In Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1377–1381, Seattle, WA, USA, Oc-
tober.
</reference>
<page confidence="0.999317">
113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.195652">
<title confidence="0.940654">EU-BRIDGE MT: Combined Machine Translation</title>
<author confidence="0.708304">Peitz Freitag</author>
<author confidence="0.708304">Wuebker</author>
<affiliation confidence="0.436991">Huck, Sennrich,</affiliation>
<address confidence="0.9127275">Nadejde, Williams, Herrmann, Cho,</address>
<affiliation confidence="0.847617">Aachen University, Aachen, of Edinburgh, Edinburgh, Institute of Technology, Karlsruhe,</affiliation>
<abstract confidence="0.99806725">This paper describes one of the collaborative efforts within EU-BRIDGE to further advance the state of the art in machine translation between two Eurolanguage pairs, Three research institutes involved in the EU-BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of evaluation campaign at the 2014 Eighth Workshop on Statistical Machine 2014). We combined up to nine different machine translation engines via system combination. RWTH Aachen University, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield emgains of up to 1.6 points in 1.0 points in the WMT newstest2013 test set compared to the best single systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, MI, USA,</location>
<contexts>
<context position="13680" citStr="Banerjee and Lavie, 2005" startWordPosition="2066" endWordPosition="2069">heses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we apply a system combination implementation that has been developed at RWTH Aachen University. The implementation of RWTH’s approach to machine translation system combination is described in (Freitag et al., 2014). This approach includes an enhanced alignment and reordering framework. Alignments between the system outputs are learned using METEOR (Banerjee and Lavie, 2005). A confusion network is then built using one of the hypotheses as “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible confusion networks into a single lattice. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models, e.g. a special n-gram language model which is learned on the input hypotheses. Scaling factors of the models are optimized using the Minimum Error Rate Training algorithm. The translation with the best total score within the</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In 43rd Annual Meeting of the Assoc. for Computational Linguistics: Proc. Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization, pages 65–72, Ann Arbor, MI, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Nadir Durrani</author>
<author>Philipp Koehn</author>
</authors>
<date>2013</date>
<booktitle>Edinburgh SLT and MT System Description for the IWSLT 2013 Evaluation. In Proceedings of the 10th International Workshop on Spoken Language Translation,</booktitle>
<pages>40--48</pages>
<location>Heidelberg, Germany,</location>
<contexts>
<context position="5955" citStr="Birch et al., 2013" startWordPosition="882" endWordPosition="885">and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM models over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. 3.2 Syntax-based S</context>
</contexts>
<marker>Birch, Durrani, Koehn, 2013</marker>
<rawString>Alexandra Birch, Nadir Durrani, and Philipp Koehn. 2013. Edinburgh SLT and MT System Description for the IWSLT 2013 Evaluation. In Proceedings of the 10th International Workshop on Spoken Language Translation, pages 40–48, Heidelberg, Germany, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Bisani</author>
<author>Hermann Ney</author>
</authors>
<title>Bootstrap Estimates for Confidence Intervals in ASR Performance Evaluation.</title>
<date>2004</date>
<booktitle>In IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<volume>1</volume>
<pages>409--412</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="18670" citStr="Bisani and Ney, 2004" startWordPosition="2889" endWordPosition="2892">ar) 16.3 69.0 17.3 66.6 19.5 63.9 UEDIN GHKM S2T (Stanford) 16.1 69.2 17.2 67.0 19.0 64.2 UEDIN GHKM S2T (Berkeley) 16.3 68.9 17.2 66.7 19.3 63.8 UEDIN GHKM T2S (Berkeley) 16.7 68.9 17.5 66.9 19.5 63.8 UEDIN GHKM S2S (Berkeley) 16.3 69.2 17.3 66.8 19.1 64.3 KIT 17.1 67.0 17.8 64.8 20.2 62.2 syscom 18.4 65.0 18.7 63.4 21.3 60.6 Table 3: Results for the English→German translation task. The system combination is tuned on newstest2011, newstest2013 is used as held-out test set for all individual systems and system combination. Bold font indicates system combination results that are significantly (Bisani and Ney, 2004) better than the best single system with p &lt; 0.05. Italic font indicates system combination results that are significantly better than the best single system with p &lt; 0.1. tributing individual systems. KIT is providing a phrase-based system output, UEDIN is providing two phrase-based system outputs and six syntaxbased ones (GHKM). For English→German, our system combination parameters are optimized on newstest2011. Combining all nine different system outputs yields an improvement of 0.5 points in BLEU and 1.7 points in TER over the best single system performance. In Table 4 the cross BLEU score</context>
</contexts>
<marker>Bisani, Ney, 2004</marker>
<rawString>Maximilian Bisani and Hermann Ney. 2004. Bootstrap Estimates for Confidence Intervals in ASR Performance Evaluation. In IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 1, pages 409–412, Montr´eal, Canada, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kucerova</author>
</authors>
<title>Clause Restructuring for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>531--540</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="6021" citStr="Collins et al., 2005" startWordPosition="892" endWordPosition="895">stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM models over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. 3.2 Syntax-based Systems UEDIN’s syntax-based systems (Williams et al., 2014) follow</context>
</contexts>
<marker>Collins, Koehn, Kucerova, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause Restructuring for Statistical Machine Translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 531–540, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT?</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4812" citStr="Durrani et al., 2013" startWordPosition="710" endWordPosition="713">en further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size o</context>
</contexts>
<marker>Durrani, Fraser, Schmid, Hoang, Koehn, 2013</marker>
<rawString>Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn. 2013a. Can Markov Models Over Minimal Translation Units Help Phrase-Based SMT? In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Barry Haddow</author>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s Machine Translation Systems for European Language Pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4812" citStr="Durrani et al., 2013" startWordPosition="710" endWordPosition="713">en further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size o</context>
</contexts>
<marker>Durrani, Haddow, Heafield, Koehn, 2013</marker>
<rawString>Nadir Durrani, Barry Haddow, Kenneth Heafield, and Philipp Koehn. 2013b. Edinburgh’s Machine Translation Systems for European Language Pairs. In Proceedings of the Eighth Workshop on Statistical Machine Translation, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nadir Durrani</author>
<author>Helmut Schmid</author>
<author>Alexander Fraser</author>
<author>Hassan Sajjad</author>
<author>Richard Farkas</author>
</authors>
<booktitle>2013c. MunichEdinburgh-Stuttgart Submissions of OSM Systems at WMT13. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<location>Sofia, Bulgaria.</location>
<marker>Durrani, Schmid, Fraser, Sajjad, Farkas, </marker>
<rawString>Nadir Durrani, Helmut Schmid, Alexander Fraser, Hassan Sajjad, and Richard Farkas. 2013c. MunichEdinburgh-Stuttgart Submissions of OSM Systems at WMT13. In Proceedings of the Eighth Workshop on Statistical Machine Translation, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Kenneth Heafield</author>
</authors>
<title>Edinburgh’s Phrase-based Machine Translation Systems for WMT-14.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="4688" citStr="Durrani et al., 2014" startWordPosition="689" endWordPosition="692">ning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translat</context>
</contexts>
<marker>Durrani, Haddow, Koehn, Heafield, 2014</marker>
<rawString>Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield. 2014. Edinburgh’s Phrase-based Machine Translation Systems for WMT-14. In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
<author>Howard Johnson</author>
</authors>
<title>Phrasetable Smoothing for Statistical Machine Translation. In</title>
<date>2006</date>
<booktitle>EMNLP,</booktitle>
<pages>53--61</pages>
<contexts>
<context position="11125" citStr="Foster et al., 2006" startWordPosition="1671" endWordPosition="1674">e source side of the corpus for German—*English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). The word alignment for German—*English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English—*German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn </context>
</contexts>
<marker>Foster, Kuhn, Johnson, 2006</marker>
<rawString>George Foster, Roland Kuhn, and Howard Johnson. 2006. Phrasetable Smoothing for Statistical Machine Translation. In EMNLP, pages 53–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Freitag</author>
<author>S Peitz</author>
<author>J Wuebker</author>
<author>H Ney</author>
<author>N Durrani</author>
<author>M Huck</author>
<author>P Koehn</author>
<author>T-L Ha</author>
<author>J Niehues</author>
<author>M Mediani</author>
<author>T Herrmann</author>
<author>A Waibel</author>
<author>N Bertoldi</author>
<author>M Cettolo</author>
<author>M Federico</author>
</authors>
<date>2013</date>
<booktitle>EU-BRIDGE MT: Text Translation of Talks in the EU-BRIDGE Project. In International Workshop on Spoken Language Translation,</booktitle>
<location>Heidelberg, Germany,</location>
<contexts>
<context position="2226" citStr="Freitag et al., 2013" startWordPosition="294" endWordPosition="298">e best single systems. 1 Introduction EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a 1http://www.eu-bridge.eu joint WMT submission of three EU-BRIDGE project partners. RWTH Aachen University (RWTH), the University of Edinburgh (UEDIN) and Karlsruhe Institute of Technology (KIT) all provided several individual systems which were combined by means of the RWTH Aachen system combination approach (Freitag et al., 2014). As distinguished from our EU-BRIDGE joint submission to the IWSLT 2013 evaluation campaign (Freitag et al., 2013), we particularly focused on translation of news text (instead of talks) for WMT. Besides, we put an emphasis on engineering syntaxbased systems in order to combine them with our more established phrase-based engines. We built combined system setups for translation from German to English as well as from English to German. This paper gives some insight into the technology behind the system combination framework and the combined engines which have been used to produce the joint EU-BRIDGE submission to the WMT 2014 translation task. The remainder of the paper is structured as follows: We first de</context>
</contexts>
<marker>Freitag, Peitz, Wuebker, Ney, Durrani, Huck, Koehn, Ha, Niehues, Mediani, Herrmann, Waibel, Bertoldi, Cettolo, Federico, 2013</marker>
<rawString>M. Freitag, S. Peitz, J. Wuebker, H. Ney, N. Durrani, M. Huck, P. Koehn, T.-L. Ha, J. Niehues, M. Mediani, T. Herrmann, A. Waibel, N. Bertoldi, M. Cettolo, and M. Federico. 2013. EU-BRIDGE MT: Text Translation of Talks in the EU-BRIDGE Project. In International Workshop on Spoken Language Translation, Heidelberg, Germany, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Freitag</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open Source Machine Translation System Combination.</title>
<date>2014</date>
<booktitle>In Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<location>Gothenburg, Sweden,</location>
<contexts>
<context position="2111" citStr="Freitag et al., 2014" startWordPosition="276" endWordPosition="279">d empirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT newstest2013 test set compared to the best single systems. 1 Introduction EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a 1http://www.eu-bridge.eu joint WMT submission of three EU-BRIDGE project partners. RWTH Aachen University (RWTH), the University of Edinburgh (UEDIN) and Karlsruhe Institute of Technology (KIT) all provided several individual systems which were combined by means of the RWTH Aachen system combination approach (Freitag et al., 2014). As distinguished from our EU-BRIDGE joint submission to the IWSLT 2013 evaluation campaign (Freitag et al., 2013), we particularly focused on translation of news text (instead of talks) for WMT. Besides, we put an emphasis on engineering syntaxbased systems in order to combine them with our more established phrase-based engines. We built combined system setups for translation from German to English as well as from English to German. This paper gives some insight into the technology behind the system combination framework and the combined engines which have been used to produce the joint EU-B</context>
<context position="13518" citStr="Freitag et al., 2014" startWordPosition="2043" endWordPosition="2046">age model trained on 1000 MKCLS word classes is applied. 5 System Combination System combination is used to produce consensus translations from multiple hypotheses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we apply a system combination implementation that has been developed at RWTH Aachen University. The implementation of RWTH’s approach to machine translation system combination is described in (Freitag et al., 2014). This approach includes an enhanced alignment and reordering framework. Alignments between the system outputs are learned using METEOR (Banerjee and Lavie, 2005). A confusion network is then built using one of the hypotheses as “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible confusion networks into a single lattice. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models, e.g. a special n-gram language model which is learned on the i</context>
</contexts>
<marker>Freitag, Huck, Ney, 2014</marker>
<rawString>Markus Freitag, Matthias Huck, and Hermann Ney. 2014. Jane: Open Source Machine Translation System Combination. In Conference of the European Chapter of the Association for Computational Linguistics, Gothenburg, Sweden, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A Simple and Effective Hierarchical Phrase Reordering Model.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>847--855</pages>
<location>Honolulu, HI, USA,</location>
<contexts>
<context position="4077" citStr="Galley and Manning, 2008" startWordPosition="592" endWordPosition="596"> 105–113, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used BLEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A Simple and Effective Hierarchical Phrase Reordering Model. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 847–855, Honolulu, HI, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proc. of the Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL),</booktitle>
<pages>273--280</pages>
<location>Boston, MA, USA,</location>
<contexts>
<context position="6718" citStr="Galley et al., 2004" startWordPosition="1005" endWordPosition="1008">erman→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. 3.2 Syntax-based Systems UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, an</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proc. of the Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics (HLT-NAACL), pages 273–280, Boston, MA, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable Inference and Training of Context-Rich Syntactic Translation Models.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st International Conf. on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6861" citStr="Galley et al., 2006" startWordPosition="1028" endWordPosition="1031"> sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. 3.2 Syntax-based Systems UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model </context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable Inference and Training of Context-Rich Syntactic Translation Models. In Proc. of the 21st International Conf. on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics, pages 961– 968, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hasler</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Sparse Lexicalised features and Topic Adaptation for SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>268--275</pages>
<contexts>
<context position="5214" citStr="Hasler et al., 2012" startWordPosition="765" endWordPosition="768">joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM models over POS and morph sequences following Durrani</context>
</contexts>
<marker>Hasler, Haddow, Koehn, 2012</marker>
<rawString>Eva Hasler, Barry Haddow, and Philipp Koehn. 2012. Sparse Lexicalised features and Topic Adaptation for SMT. In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages 268–275.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Li Deng</author>
</authors>
<title>Maximum Expected BLEU Training of Phrase and Lexicon Translation Models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>292--301</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="4276" citStr="He and Deng, 2012" startWordPosition="630" endWordPosition="633">imum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used BLEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growd</context>
</contexts>
<marker>He, Deng, 2012</marker>
<rawString>Xiaodong He and Li Deng. 2012. Maximum Expected BLEU Training of Phrase and Lexicon Translation Models. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 292–301, Jeju, Republic of Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable modified Kneser-Ney language model estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>690--696</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3901" citStr="Heafield et al., 2013" startWordPosition="563" endWordPosition="566"> (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used BLEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contri</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable modified Kneser-Ney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 690–696, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, Scotland, UK,</location>
<contexts>
<context position="5013" citStr="Heafield, 2011" startWordPosition="740" endWordPosition="741">ge and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, UK, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teresa Herrmann</author>
<author>Jan Niehues</author>
<author>Alex Waibel</author>
</authors>
<title>Combining Word Reordering Methods on different Linguistic Abstraction Levels for Statistical Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<location>Atlanta, GA, USA,</location>
<contexts>
<context position="11587" citStr="Herrmann et al., 2013" startWordPosition="1742" endWordPosition="1745">ring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English—*German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discri</context>
</contexts>
<marker>Herrmann, Niehues, Waibel, 2013</marker>
<rawString>Teresa Herrmann, Jan Niehues, and Alex Waibel. 2013. Combining Word Reordering Methods on different Linguistic Abstraction Levels for Statistical Machine Translation. In Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation, Atlanta, GA, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teresa Herrmann</author>
<author>Mohammed Mediani</author>
<author>Eunah Cho</author>
<author>Jan Niehues Thanh-Le Ha</author>
<author>Isabel Slawik</author>
<author>Yuqi Zhang</author>
<author>Alex Waibel</author>
</authors>
<date>2014</date>
<booktitle>The Karlsruhe Institute of Technology Translation Systems for the WMT</booktitle>
<location>Baltimore, MD, USA,</location>
<marker>Herrmann, Mediani, Cho, Thanh-Le Ha, Slawik, Zhang, Waibel, 2014</marker>
<rawString>Teresa Herrmann, Mohammed Mediani, Eunah Cho, Thanh-Le Ha, Jan Niehues, Isabel Slawik, Yuqi Zhang, and Alex Waibel. 2014. The Karlsruhe Institute of Technology Translation Systems for the WMT 2014. In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
<author>Adam Lopez</author>
</authors>
<title>A Unified Framework for Phrase-Based, Hierarchical, and Syntax-Based Statistical Machine Translation.</title>
<date>2009</date>
<pages>152--159</pages>
<location>Tokyo, Japan,</location>
<contexts>
<context position="7925" citStr="Hoang et al., 2009" startWordPosition="1195" endWordPosition="1198">bability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model scoring during decoding. Model weights are optimized to maximize BLEU. 2000 sentences from the newstest2008-2012 sets have been selected as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting 106 on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English—*German, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English—*German UEDIN GHKM system names in Table 3 denote: UEDIN GHKM S2T (ParZu): A </context>
</contexts>
<marker>Hoang, Koehn, Lopez, 2009</marker>
<rawString>Hieu Hoang, Philipp Koehn, and Adam Lopez. 2009. A Unified Framework for Phrase-Based, Hierarchical, and Syntax-Based Statistical Machine Translation. pages 152–159, Tokyo, Japan, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Forest Rescoring: Faster Decoding with Integrated Language Models.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>144--151</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5391" citStr="Huang and Chiang, 2007" startWordPosition="795" endWordPosition="798"> settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM models over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereo</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>Liang Huang and David Chiang. 2007. Forest Rescoring: Faster Decoding with Integrated Language Models. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144–151, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Joern Wuebker</author>
<author>Felix Rietig</author>
<author>Hermann Ney</author>
</authors>
<title>A Phrase Orientation Model for Hierarchical Machine Translation.</title>
<date>2013</date>
<booktitle>In ACL 2013 Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>452--463</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="4097" citStr="Huck et al., 2013" startWordPosition="597" endWordPosition="600">and USA, June 26–27, 2014. c�2014 Association for Computational Linguistics et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used BLEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have bee</context>
</contexts>
<marker>Huck, Wuebker, Rietig, Ney, 2013</marker>
<rawString>Matthias Huck, Joern Wuebker, Felix Rietig, and Hermann Ney. 2013. A Phrase Orientation Model for Hierarchical Machine Translation. In ACL 2013 Eighth Workshop on Statistical Machine Translation, pages 452–463, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Huck</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>Augmenting String-to-Tree and Tree-toString Translation with Non-Syntactic Phrases.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="9767" citStr="Huck et al., 2014" startWordPosition="1463" endWordPosition="1466">ned with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German—*English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), usi</context>
</contexts>
<marker>Huck, Hoang, Koehn, 2014</marker>
<rawString>Matthias Huck, Hieu Hoang, and Philipp Koehn. 2014. Augmenting String-to-Tree and Tree-toString Translation with Non-Syntactic Phrases. In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="11675" citStr="Klein and Manning, 2003" startWordPosition="1755" endWordPosition="1758">re computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English—*German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choic</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
</authors>
<title>Factored Translation Models.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>868--876</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5727" citStr="Koehn and Hoang, 2007" startWordPosition="848" endWordPosition="851">2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM models over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based</context>
</contexts>
<marker>Koehn, Hoang, 2007</marker>
<rawString>Philipp Koehn and Hieu Hoang. 2007. Factored Translation Models. In EMNLP-CoNLL, pages 868–876, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical Methods for Compound Splitting. In EACL,</title>
<date>2003</date>
<location>Budapest, Hungary.</location>
<contexts>
<context position="6069" citStr="Koehn and Knight, 2003" startWordPosition="899" endWordPosition="902">ng testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM models over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. 3.2 Syntax-based Systems UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley,</context>
<context position="10486" citStr="Koehn and Knight, 2003" startWordPosition="1571" endWordPosition="1574"> by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German—*English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German—*English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). The word alignment for German—*English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English—*German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNe</context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound Splitting. In EACL, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra B Mayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="11738" citStr="Koehn et al., 2005" startWordPosition="1767" endWordPosition="1770"> 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English—*German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. 107 In total, the English→German sys</context>
</contexts>
<marker>Koehn, Axelrod, Mayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra B. Mayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4751" citStr="Koehn et al., 2007" startWordPosition="700" endWordPosition="703">ker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004</context>
<context position="11022" citStr="Koehn et al., 2007" startWordPosition="1655" endWordPosition="1658">development and test data respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German—*English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). The word alignment for German—*English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English—*German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum Bayes-Risk Decoding for Statistical Machine Translation. In</title>
<date>2004</date>
<booktitle>Proc. Human Language Technology Conf. /North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL),</booktitle>
<pages>169--176</pages>
<location>Boston, MA, USA,</location>
<contexts>
<context position="5352" citStr="Kumar and Byrne, 2004" startWordPosition="789" endWordPosition="792"> (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM models over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 20</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum Bayes-Risk Decoding for Statistical Machine Translation. In Proc. Human Language Technology Conf. /North American Chapter of the Association for Computational Linguistics Annual Meeting (HLT-NAACL), pages 169–176, Boston, MA, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saab Mansour</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
</authors>
<title>Combining Translation and Language Model Scoring for Domain-Specific Data Filtering.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>222--229</pages>
<location>San Francisco, CA, USA,</location>
<contexts>
<context position="4461" citStr="Mansour et al., 2011" startWordPosition="659" endWordPosition="662">erying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5- gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram oper</context>
</contexts>
<marker>Mansour, Wuebker, Ney, 2011</marker>
<rawString>Saab Mansour, Joern Wuebker, and Hermann Ney. 2011. Combining Translation and Language Model Scoring for Domain-Specific Data Filtering. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 222–229, San Francisco, CA, USA, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Mauser</author>
<author>Saˇsa Hasan</author>
<author>Hermann Ney</author>
</authors>
<title>Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models.</title>
<date>2009</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>210--217</pages>
<location>Singapore,</location>
<contexts>
<context position="12230" citStr="Mauser et al., 2009" startWordPosition="1845" endWordPosition="1848">arse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English—*German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. 107 In total, the English→German system uses the following language models: two 4-gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based language models, and a 4-gram cluster-based language model trained on 1,000 MKCLS word classes. The German→English system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selecte</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>Arne Mauser, Saˇsa Hasan, and Hermann Ney. 2009. Extending Statistical Machine Translation with Discriminative and Trigger-Based Lexicon Models. In Conference on Empirical Methods in Natural Language Processing, pages 210–217, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Mediani</author>
<author>Eunah Cho</author>
<author>Jan Niehues</author>
<author>Teresa Herrmann</author>
<author>Alex Waibel</author>
</authors>
<title>The KIT English-French Translation systems for IWSLT</title>
<date>2011</date>
<booktitle>In Proceedings of the Eight International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="10753" citStr="Mediani et al. (2011)" startWordPosition="1614" endWordPosition="1617">tions and additionally the Gigaword corpus for German—*English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German—*English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). The word alignment for German—*English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English—*German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target </context>
</contexts>
<marker>Mediani, Cho, Niehues, Herrmann, Waibel, 2011</marker>
<rawString>Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa Herrmann, and Alex Waibel. 2011. The KIT English-French Translation systems for IWSLT 2011. In Proceedings of the Eight International Workshop on Spoken Language Translation (IWSLT), San Francisco, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>220--224</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="12860" citStr="Moore and Lewis, 2010" startWordPosition="1943" endWordPosition="1946">ource context to guide the word choices in the target sentence. 107 In total, the English→German system uses the following language models: two 4-gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based language models, and a 4-gram cluster-based language model trained on 1,000 MKCLS word classes. The German→English system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). Again, a 4-gram cluster-based language model trained on 1000 MKCLS word classes is applied. 5 System Combination System combination is used to produce consensus translations from multiple hypotheses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we apply a system combination implementation that has been developed at RWTH Aachen University. The implementation of RWTH’s approach to machine translation</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C. Moore and William Lewis. 2010. Intelligent selection of language model training data. In Proceedings of the ACL 2010 Conference Short Papers, pages 220–224, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Muntsin Kolss</author>
</authors>
<title>A POS-Based Model for Long-Range Reorderings in SMT.</title>
<date>2009</date>
<booktitle>In Fourth Workshop on Statistical Machine Translation (WMT</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="11257" citStr="Niehues and Kolss, 2009" startWordPosition="1691" endWordPosition="1694">Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). The word alignment for German—*English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English—*German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. </context>
</contexts>
<marker>Niehues, Kolss, 2009</marker>
<rawString>Jan Niehues and Muntsin Kolss. 2009. A POS-Based Model for Long-Range Reorderings in SMT. In Fourth Workshop on Statistical Machine Translation (WMT 2009), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Stephan Vogel</author>
</authors>
<title>Discriminative Word Alignment via Alignment Matrix Modeling.</title>
<date>2008</date>
<booktitle>In Proceedings of Third ACL Workshop on Statistical Machine Translation,</booktitle>
<location>Columbus, USA.</location>
<contexts>
<context position="10938" citStr="Niehues and Vogel, 2008" startWordPosition="1641" endWordPosition="1644">raining as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German—*English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). The word alignment for German—*English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English—*German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tr</context>
</contexts>
<marker>Niehues, Vogel, 2008</marker>
<rawString>Jan Niehues and Stephan Vogel. 2008. Discriminative Word Alignment via Alignment Matrix Modeling. In Proceedings of Third ACL Workshop on Statistical Machine Translation, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Niehues</author>
<author>Teresa Herrmann</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Wider Context by Using Bilingual Language Models in Machine Translation.</title>
<date>2011</date>
<booktitle>In Sixth Workshop on Statistical Machine Translation (WMT 2011),</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="12159" citStr="Niehues et al., 2011" startWordPosition="1831" endWordPosition="1834">ee-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English—*German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. 107 In total, the English→German system uses the following language models: two 4-gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based language models, and a 4-gram cluster-based language model trained on 1,000 MKCLS word classes. The German→English system uses a 4-gram word-based language model trained on all monolingual</context>
</contexts>
<marker>Niehues, Herrmann, Vogel, Waibel, 2011</marker>
<rawString>Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel. 2011. Wider Context by Using Bilingual Language Models in Machine Translation. In Sixth Workshop on Statistical Machine Translation (WMT 2011), Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10851" citStr="Och and Ney, 2003" startWordPosition="1629" endWordPosition="1632">for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German—*English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). The word alignment for German—*English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English—*German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>An Efficient Method for Determining Bilingual Word Classes.</title>
<date>1999</date>
<booktitle>In EACL’99.</booktitle>
<contexts>
<context position="11976" citStr="Och, 1999" startWordPosition="1806" endWordPosition="1807"> order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English—*German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. 107 In total, the English→German system uses the following language models: two 4-gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based langu</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz Josef Och. 1999. An Efficient Method for Determining Bilingual Word Classes. In EACL’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan,</location>
<contexts>
<context position="3694" citStr="Och, 2003" startWordPosition="531" endWordPosition="532">tal results are given in Section 6. We finally conclude the paper with Section 7. 2 RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used BLEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has b</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proc. of the 41th Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167, Sapporo, Japan, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Peitz</author>
<author>Joern Wuebker</author>
<author>Markus Freitag</author>
<author>Hermann Ney</author>
</authors>
<title>The RWTH Aachen GermanEnglish Machine Translation System for WMT</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="3216" citStr="Peitz et al., 2014" startWordPosition="460" endWordPosition="463">gy behind the system combination framework and the combined engines which have been used to produce the joint EU-BRIDGE submission to the WMT 2014 translation task. The remainder of the paper is structured as follows: We first describe the individual systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), and Karlsruhe Institute of Technology (Section 4). We then present the techniques for machine translation system combination in Section 5. Experimental results are given in Section 6. We finally conclude the paper with Section 7. 2 RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used BLEU as optimization objective. Both for language</context>
</contexts>
<marker>Peitz, Wuebker, Freitag, Ney, 2014</marker>
<rawString>Stephan Peitz, Joern Wuebker, Markus Freitag, and Hermann Ney. 2014. The RWTH Aachen GermanEnglish Machine Translation System for WMT 2014. In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="9208" citStr="Petrov and Klein, 2007" startWordPosition="1376" endWordPosition="1379">tation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Karlsruhe Institute of Technology The</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Parsing German with Latent Variable Grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Parsing German at ACL ’08,</booktitle>
<pages>33--39</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="9233" citStr="Petrov and Klein, 2008" startWordPosition="1380" endWordPosition="1383">Zu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Karlsruhe Institute of Technology The KIT translations (Herrma</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008. Parsing German with Latent Variable Grammars. In Proceedings of the Workshop on Parsing German at ACL ’08, pages 33–39, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning Accurate, Compact, and Interpretable Tree Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="8130" citStr="Petrov et al., 2006" startWordPosition="1223" endWordPosition="1226">zed to maximize BLEU. 2000 sentences from the newstest2008-2012 sets have been selected as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting 106 on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English—*German, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English—*German UEDIN GHKM system names in Table 3 denote: UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional synta</context>
<context position="9394" citStr="Petrov et al., 2006" startWordPosition="1404" endWordPosition="1407"> string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel c</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning Accurate, Compact, and Interpretable Tree Annotation. In Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Assoc. for Computational Linguistics, pages 433–440, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Parsing German atACL ’08,</booktitle>
<pages>40--46</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="9044" citStr="Rafferty and Manning, 2008" startWordPosition="1352" endWordPosition="1355"> trained as well. The English—*German UEDIN GHKM system names in Table 3 denote: UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic no</context>
<context position="11648" citStr="Rafferty and Manning, 2008" startWordPosition="1751" endWordPosition="1754">. Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English—*German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source cont</context>
</contexts>
<marker>Rafferty, Manning, 2008</marker>
<rawString>Anna N. Rafferty and Christopher D. Manning. 2008a. Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines. In Proceedings of the Workshop on Parsing German atACL ’08, pages 40– 46, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Parsing German.</booktitle>
<contexts>
<context position="9044" citStr="Rafferty and Manning, 2008" startWordPosition="1352" endWordPosition="1355"> trained as well. The English—*German UEDIN GHKM system names in Table 3 denote: UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic no</context>
<context position="11648" citStr="Rafferty and Manning, 2008" startWordPosition="1751" endWordPosition="1754">. Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English—*German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source cont</context>
</contexts>
<marker>Rafferty, Manning, 2008</marker>
<rawString>Anna N. Rafferty and Christopher D. Manning. 2008b. Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines. In Proceedings of the Workshop on Parsing German.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kay Rottmann</author>
<author>Stephan Vogel</author>
</authors>
<title>Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Sk¨ovde,</booktitle>
<contexts>
<context position="11205" citStr="Rottmann and Vogel, 2007" startWordPosition="1683" endWordPosition="1686">. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). The word alignment for German—*English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English—*German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (S</context>
</contexts>
<marker>Rottmann, Vogel, 2007</marker>
<rawString>Kay Rottmann and Stephan Vogel. 2007. Word Reordering in Statistical Machine Translation with a POS-Based Distortion Model. In Proceedings of the 11th International Conference on Theoretical and Methodological Issues in Machine Translation (TMI), Sk¨ovde, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Florian Laws</author>
</authors>
<title>Estimation of Conditional Probabilities with Decision Trees and an Application to Fine-Grained POS Tagging.</title>
<date>2008</date>
<booktitle>In COLING 2008,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="12095" citStr="Schmid and Laws, 2008" startWordPosition="1820" endWordPosition="1823"> extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English—*German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. 107 In total, the English→German system uses the following language models: two 4-gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based language models, and a 4-gram cluster-based language model trained on 1,000 MKCLS word classes. The German→English system us</context>
</contexts>
<marker>Schmid, Laws, 2008</marker>
<rawString>Helmut Schmid and Florian Laws. 2008. Estimation of Conditional Probabilities with Decision Trees and an Application to Fine-Grained POS Tagging. In COLING 2008, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="11290" citStr="Schmid, 1994" startWordPosition="1699" endWordPosition="1700">e filtered out using an SVM classifier as described by Mediani et al. (2011). The word alignment for German—*English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English—*German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language m</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors.</title>
<date>2004</date>
<booktitle>In Proc. of the Int. Conf. on Computational Linguistics (COLING),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="8878" citStr="Schmid, 2004" startWordPosition="1330" endWordPosition="1331">th respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English—*German UEDIN GHKM system names in Table 3 denote: UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased w</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors. In Proc. of the Int. Conf. on Computational Linguistics (COLING), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
<author>Martin Volk</author>
<author>Gerold Schneider</author>
</authors>
<title>Exploiting Synergies Between Open Resources for German Dependency Parsing, POStagging, and Morphological Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference Recent Advances in Natural Language Processing</booktitle>
<pages>601--609</pages>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="8636" citStr="Sennrich et al., 2013" startWordPosition="1296" endWordPosition="1299">tting 106 on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English—*German, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English—*German UEDIN GHKM system names in Table 3 denote: UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). U</context>
</contexts>
<marker>Sennrich, Volk, Schneider, 2013</marker>
<rawString>Rico Sennrich, Martin Volk, and Gerold Schneider. 2013. Exploiting Synergies Between Open Resources for German Dependency Parsing, POStagging, and Morphological Analysis. In Proceedings of the International Conference Recent Advances in Natural Language Processing 2013, pages 601–609, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP),</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<location>Denver, CO, USA,</location>
<contexts>
<context position="7393" citStr="Stolcke, 2002" startWordPosition="1111" endWordPosition="1112">xtract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model scoring during decoding. Model weights are optimized to maximize BLEU. 2000 sentences from the newstest2008-2012 sets have been selected as a development set. The selected sentences obtained high sentence-level BLEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree syst</context>
<context position="11818" citStr="Stolcke, 2002" startWordPosition="1781" endWordPosition="1782">) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English—*German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. 107 In total, the English→German system uses the following language models: two 4-gram wordbased language models tra</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An Extensible Language Modeling Toolkit. In Proc. of the Int. Conf. on Speech and Language Processing (ICSLP), volume 2, pages 901–904, Denver, CO, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Venugopal</author>
<author>Andreas Zollman</author>
<author>Alex Waibel</author>
</authors>
<title>Training and Evaluation Error Minimization Rules for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Workshop on Data-drive Machine Translation and Beyond (WPT-05),</booktitle>
<location>Ann Arbor, Michigan, USA.</location>
<contexts>
<context position="10362" citStr="Venugopal et al., 2005" startWordPosition="1555" endWordPosition="1558">traction (Huck et al., 2014). 4 Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German—*English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German—*English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). The word alignment for German—*English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English—*German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and </context>
</contexts>
<marker>Venugopal, Zollman, Waibel, 2005</marker>
<rawString>Ashish Venugopal, Andreas Zollman, and Alex Waibel. 2005. Training and Evaluation Error Minimization Rules for Statistical Machine Translation. In Workshop on Data-drive Machine Translation and Beyond (WPT-05), Ann Arbor, Michigan, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Daniel Stein</author>
<author>Matthias Huck</author>
<author>Hermann Ney</author>
</authors>
<title>Jane: Open Source Hierarchical Translation, Extended with Reordering and Lexicon Models.</title>
<date>2010</date>
<booktitle>In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR,</booktitle>
<pages>262--270</pages>
<location>Uppsala, Sweden,</location>
<marker>Vilar, Stein, Huck, Ney, 2010</marker>
<rawString>David Vilar, Daniel Stein, Matthias Huck, and Hermann Ney. 2010. Jane: Open Source Hierarchical Translation, Extended with Reordering and Lexicon Models. In ACL 2010 Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR, pages 262–270, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
</authors>
<title>SMT Decoder Dissected: Word Reordering.</title>
<date>2003</date>
<booktitle>In International Conference on Natural Language Processing and Knowledge Engineering,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="9925" citStr="Vogel, 2003" startWordPosition="1488" endWordPosition="1489">tactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German—*English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the</context>
</contexts>
<marker>Vogel, 2003</marker>
<rawString>Stephan Vogel. 2003. SMT Decoder Dissected: Word Reordering. In International Conference on Natural Language Processing and Knowledge Engineering, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>GHKM Rule Extraction and Scope-3 Parsing in Moses.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation (WMT),</booktitle>
<pages>388--394</pages>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="6823" citStr="Williams and Koehn, 2012" startWordPosition="1021" endWordPosition="1024">sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. 3.2 Syntax-based Systems UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model a</context>
</contexts>
<marker>Williams, Koehn, 2012</marker>
<rawString>Philip Williams and Philipp Koehn. 2012. GHKM Rule Extraction and Scope-3 Parsing in Moses. In Proceedings of the Seventh Workshop on Statistical Machine Translation (WMT), pages 388–394, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Rico Sennrich</author>
<author>Maria Nadejde</author>
<author>Matthias Huck</author>
<author>Eva Hasler</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s Syntax-Based Systems at WMT</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="6614" citStr="Williams et al., 2014" startWordPosition="987" endWordPosition="990">dering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. 3.2 Syntax-based Systems UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities </context>
</contexts>
<marker>Williams, Sennrich, Nadejde, Huck, Hasler, Koehn, 2014</marker>
<rawString>Philip Williams, Rico Sennrich, Maria Nadejde, Matthias Huck, Eva Hasler, and Philipp Koehn. 2014. Edinburgh’s Syntax-Based Systems at WMT 2014. In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Matthias Huck</author>
<author>Stephan Peitz</author>
<author>Malte Nuhn</author>
<author>Markus Freitag</author>
<author>Jan-Thorsten Peter</author>
<author>Saab Mansour</author>
<author>Hermann Ney</author>
</authors>
<title>Jane 2: Open Source Phrase-based and Hierarchical Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In COLING ’12: The 24th Int. Conf. on Computational Linguistics,</booktitle>
<pages>483--491</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="3590" citStr="Wuebker et al., 2012" startWordPosition="511" endWordPosition="514">logy (Section 4). We then present the techniques for machine translation system combination in Section 5. Experimental results are given in Section 6. We finally conclude the paper with Section 7. 2 RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used BLEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has </context>
</contexts>
<marker>Wuebker, Huck, Peitz, Nuhn, Freitag, Peter, Mansour, Ney, 2012</marker>
<rawString>Joern Wuebker, Matthias Huck, Stephan Peitz, Malte Nuhn, Markus Freitag, Jan-Thorsten Peter, Saab Mansour, and Hermann Ney. 2012. Jane 2: Open Source Phrase-based and Hierarchical Statistical Machine Translation. In COLING ’12: The 24th Int. Conf. on Computational Linguistics, pages 483– 491, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joern Wuebker</author>
<author>Stephan Peitz</author>
<author>Felix Rietig</author>
<author>Hermann Ney</author>
</authors>
<title>Improving Statistical Machine Translation with Word Class Models.</title>
<date>2013</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1377--1381</pages>
<location>Seattle, WA, USA,</location>
<contexts>
<context position="4149" citStr="Wuebker et al., 2013" startWordPosition="607" endWordPosition="610"> Computational Linguistics et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used BLEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected BLEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 200</context>
</contexts>
<marker>Wuebker, Peitz, Rietig, Ney, 2013</marker>
<rawString>Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney. 2013. Improving Statistical Machine Translation with Word Class Models. In Conference on Empirical Methods in Natural Language Processing, pages 1377–1381, Seattle, WA, USA, October.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
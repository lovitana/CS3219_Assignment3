<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010161">
<title confidence="0.995299">
Combining strategies for tagging and parsing Arabic
</title>
<author confidence="0.982618">
Maytham Alabbas
</author>
<affiliation confidence="0.841833">
Department of Computer Science
University of Basrah
Basrah, Iraq
</affiliation>
<email confidence="0.996022">
maytham.alabbas@gmail.com
</email>
<author confidence="0.99337">
Allan Ramsay
</author>
<affiliation confidence="0.9973835">
School of Computer Science
University of Manchester
</affiliation>
<address confidence="0.97798">
Manchester M13 9PL, UK
</address>
<email confidence="0.987374">
Allan.Ramsay@manchester.ac.uk
</email>
<bodyText confidence="0.999596333333333">
We describe a simple method for com-
bining taggers which produces substan-
tially better performance than any of the
contributing tools. The method is very
simple, but it leads to considerable im-
provements in performance: given three
taggers for Arabic whose individual ac-
curacies range from 0.956 to 0.967, the
combined tagger scores 0.995–a seven-
fold reduction in the error rate when
compared to the best of the contributing
tools.
Given the effectiveness of this approach
to combining taggers, we have investi-
gated its applicability to parsing. For
parsing, it seems better to take pairs of
similar parsers and back off to a third if
they disagree.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913058823529">
If you have several systems that perform the same
task, it seems reasonable to suppose that you can
obtain better performance by using some judicious
combination of them than can be obtained by any
of them in isolation. A large number of combin-
ing strategies have been proposed, with majority
voting being particularly popular (Stefano et al.,
2002). We have investigated a range of such strate-
gies for combining taggers and parsers for Ara-
bic: the best strategy we have found for tagging
involves asking each of the contributing taggers
how confident it is, and accepting the answer given
by the most confident one. We hypothesise that
the reason for the effectiveness of this strategy for
tagging arises from the fact that the contributing
taggers work in essentially different ways (differ-
ent training data, different underlying algorithms),
and hence if they make systematic mistakes these
will tend to be different. This means, in turn, that
the places where they don’t make mistakes will be
different.
This strategy is less effective for parsing. We
have tried combining two members of the MALT-
Parser family (Nivre et al., 2006; Nivre et al.,
2007; Nivre et al., 2010) with MSTParser (Mc-
Donald et al., 2006a; McDonald et al., 2006b).
The best strategy here seems to be to accept the
output of the two versions of MALTParser when
they agree, but to switch to MSTParser if the
MALTParser versions disagree. It may be that this
is because the MALTParser versions are very sim-
ilar, so that when they disagree this suggests that
there is something anomalous about the input text,
and that neither of them can be trusted at this point.
</bodyText>
<sectionHeader confidence="0.966064" genericHeader="keywords">
2 Tagging
</sectionHeader>
<bodyText confidence="0.999743857142857">
We present a very simple strategy for combin-
ing part-of-speech (POS) taggers which leads to
substantial improvements in accuracy. A num-
ber of combination strategies have been proposed
in the literature (Zeman and ˇZabokrtsk`y, 2005).
In experiments with combining three Arabic tag-
gers (AMIRA (Diab, 2009), MADA (Habash et
al., 2009) and a simple affix-based maximum-
likelihood Arabic tagger (MXL) (Ramsay and
Sabtan, 2009)) the current strategy significantly
outperformed voting-based strategies.
We used the Penn Arabic Treebank (PATB) Part
1 v3.0 as a resource for our experiments. The
words in the PATB are already tagged, which thus
provides us with a widely-accepted Gold standard.
Even PATB tagging is not guaranteed to be 100%
accurate, but it nonetheless provides as good a ref-
erence set as can be found.1
The PATB uses the tags provided by the Buck-
walter morphological analyser (Buckwalter, 2004;
Buckwalter, 2007), which carry a great deal
</bodyText>
<footnote confidence="0.993179">
1The PATB is the largest easily available tagged Arabic
corpus, with about 165K words in the section we are us-
ing. Thus for each fold of our 10-fold testing regime we are
training on 150K words and testing on 15K, which should be
enough to provide robust results.
</footnote>
<page confidence="0.986081">
73
</page>
<bodyText confidence="0.960512333333333">
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 73–77,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
of syntactically relevant information (particularly
case-marking). This tagset contains 305 tags, with
for instance 47 tags for different kinds of verb
and 44 for different kinds of noun. The very fine
distinctions between different kinds of nouns and
verbs (e.g. between subject and object case nouns)
in the absence of visible markers make this an ex-
tremely difficult tagset to work with. It is in gen-
eral virtually impossible to decide the case of an
Arabic noun until its overall syntactic role is de-
termined, and it is similarly difficult to decide the
form of a verb until the overall syntactic structure
of the sentence is determined. For this reason tag-
gers often work with a coarser set of tags, of which
the ‘Bies tagset’ (Maamouri and Bies, 2004) is
widely used (see for instance the Stanford Arabic
parser (Green and Manning, 2010)). We carried
out our experiments with a variant of the origi-
nal fine-grained tagset, and also with a variant of
the coarser-grained Bies set obtained by deleting
details such as case- and agreement-markers. We
carried out two sets of experiments, with a coarse-
grained set of tags (a superset of the Bies tagset
with 39 tags, shown in Figure 1) and the original
fine-grained one with 305 tags.
</bodyText>
<sectionHeader confidence="0.95056025" genericHeader="introduction">
ABBREV EXCEPT PART PART
ADJ FOCUS PART POSS PRON
ADV FUT+IV PREP
CONJ INTERJ PRON
</sectionHeader>
<table confidence="0.939375333333333">
CV INTERROG PART PUNC
CVSUFF DO IV PV
DEM PRON IVSUFF DO PVSUFF DO
DET LATIN RC PART
DET+ADJ NEG PART REL ADV
DET+NOUN NOUN REL PRON
DET+NOUN PROP NOUN PROP SUB
DET+NUM NO FUNC SUB CONJ
EMPH PART NUM VERB PART
</table>
<tableCaption confidence="0.999865">
Table 1: Coarse-grained tagset
</tableCaption>
<bodyText confidence="0.999743083333334">
The accuracy of a tagger clearly depends on
the granularity of the tagset: the contributing tag-
gers produced scores from 0.955 to 0.967 on the
coarse-grained tagset, and from 0.888 to 0.936 on
the fine-grained one. We applied transformation-
based retagging (TBR) (Brill, 1995; Lager, 1999)
to the output of the basic taggers, which produced
a small improvement in the results for MADA
and MXL and a more substantial improvement
for AMIRA. Table 2 shows the performance of
the three taggers using the two tagsets with and
without TBR. The improvement obtained by using
</bodyText>
<table confidence="0.9753134">
POS TBR AMIRA MXL MADA
Coarse × 0.896 0.952 0.941
√ 0.953 0.956 0.967
Fine × 0.843 0.897 0.917
√ 0.888 0.912 0.936
</table>
<tableCaption confidence="0.8962185">
Table 2: Tagger accuracies in isolation, with and
without TBR
</tableCaption>
<bodyText confidence="0.993348948717949">
TBR for AMIRA arises largely from the fact that
in some cases AMIRA uses tags similar to those
used in the English Penn Treebank rather than the
ones in the the tags in the PATB, e.g. JJ for
adjectives where the PATB uses ADJ. TBR pro-
vides a simple and reliable mechanism for discov-
ering and patching systematic renamings of this
kind, and hence is extremely useful when working
with different tagsets. A significant component of
the remaining errors produced by AMIRA arise
because AMIRA has a much coarser classifica-
tion of particles than the classification provided by
the Buckwalter tagset. Since AMIRA assigns the
same tag to a variety of different particles, TBR
cannot easily recover the correct fine-grained tags,
and hence AMIRA makes a substantial number of
errors on these items.
The key to the proposed combining strategy is
that each of the contributing taggers is likely to
make systematic mistakes; and that if they are
based on different principles they are likely to
make different systematic mistakes. If we clas-
sify the mistakes that a tagger makes, we should be
able to avoid believing it in cases where it is likely
to be wrong. So long as the taggers are based
on sufficiently different principles, they should be
wrong in different places.
We therefore collected confusion matrices for
each of the individual taggers showing how likely
they were to be right for each category of item–
how likely, for instance, was MADA to be right
when it proposed to tag some item as a noun (very
likely–accuracy of MADA when it proposes NN
is 0.98), how likely was AMIRA to be right when
it proposed the tag RP (very unlikely–accuracy of
0.08 in this case)? Given these tables, we simply
took the tagger whose prediction was most likely
to be right.2
Table 3 shows an excerpt from the output of the
</bodyText>
<footnote confidence="0.59892725">
2All the tagging results reported below were obtained by
using 10-fold cross validation, i.e. carrying out 10 experi-
ments each of which involved removing 10% of the data for
testing and training on the remaining 90%.
</footnote>
<page confidence="0.971928">
74
</page>
<table confidence="0.9981556">
Word Gold standard MADA MXL AMIRA TAG
... ... ... ... ... ...
gyr NEG PART NOUN (0.979) NEG PART (0.982) RP (0.081) NEG PART
&lt;lA EXCEPT PART EXCEPT PART (1.00) SUB CONJ (0.965) RP (0.790) EXCEPT PART
... ... ... ... ... ...
</table>
<tableCaption confidence="0.999876">
Table 3: Confidence levels for individual tags
</tableCaption>
<bodyText confidence="0.999965636363636">
three individual taggers looking at a string con-
taining the two words gyr and &lt;lA, with the tags
annotated with the accuracy of each tagger on the
given tag, e.g. in this sequence MADA has tagged
gyr as a noun, and MXL has tagged it as a neg-
ative particle and AMIRA has tagged it as RP;
and when MADA suggests NOUN as the tag it is
right 97.9% of the time, whereas when MXL sug-
gests NEG PART it is right 98.2% of the time and
AMIRA is right just 8.1% of the time when it sug-
gests RP. It is important to note that the tags are
assigned to words in context, but the confidence
levels are calculated across the entire training data.
The fact that MADA is right 97.9% of the time
when it assigns the tag NOUN is not restricted to
the word gyr, and certainly not to this occurrence
of this word.
We compared the results of this simple strategy,
which is similar to a strategy proposed for image
classification by Woods at el. (1997), with a strat-
egy proposed by (2005), in which you accept the
majority view if at least two of the taggers agree,
and you back off to one of them if they all dis-
agree, and with a variation on that where you ac-
cept the majority view if two agree and back off to
the most confident if they all disagree. The results
are given in Table 4.
All four strategies produce an improvement
over the individual taggers. The fact that ma-
jority voting works better when backing off to
MXL than to MADA, despite the fact that MADA
works better in isolation, is thought-provoking. It
seems likely to be that this arises from the fact that
MADA and AMIRA are based on similar princi-
ples, and hence are likely to agree even when they
are wrong. This hypothesis suggested that looking
at the likely accuracy of each tagger on each case
might be a good backoff strategy. It turns out that
it is not just a good backoff strategy, as shown in
the third column of Table 4: it is even better when
used as the main strategy (column 5). The differ-
ences between columns 4 and 5 are not huge,3 but
that should not be too surprising, since these two
strategies will agree in every case where all three
of the contributing taggers agree, so the only place
where these two will disagree is when one of the
taggers disagrees with the others and the isolated
tagger is more confident than either of the others.
The idea reported here is very simple, but it is
also very effective. We have reduced the error in
tagging with fairly coarse-grained tags to 0.05%,
and we have also produced a substantial improve-
ment for the fine grained tags, from 0.936 for the
best of the individual taggers to 0.96 for the com-
bination.
</bodyText>
<sectionHeader confidence="0.993791" genericHeader="method">
3 Parsing
</sectionHeader>
<bodyText confidence="0.918709173913043">
Given the success of the approach outlined above
for tagging, it seemed worth investigating whether
the same idea could be applied to parsing. We
therefore tried using it with a combination of de-
pendency parsers, for which we used MSTParser
(McDonald et al., 2006a; McDonald et al., 2006b)
and two variants from the MALTParser family
(Nivre et al., 2006; Nivre et al., 2007; Nivre et
al., 2010), namely Nivre arc-eager, which we will
refer to as MALTParser1, and stack-eager, which
we will refer to as MALTParser2. The results in
Table 5 include (i) the three parsers in isolation;
(ii) a strategy in which we select a pair and trust
their proposals wherever they agree, and back-off
3In terms of error rate the difference looks more substan-
tial, since the error rate, 0.005, for column 5 for the fine-
grained set is 62.5% of that for column 4, 0.008; and for the
coarse-grained set the error rate for column 5, 0.04, is 73%
of that for column 4, 0.055
Tagset Majority voting Majority voting Majority voting Majority voting Just most
(back off to MXL) (back off to MADA) (back off to AMIRA) (most confident) confident
Coarse-grained 0.982 0.979 0.975 0.992 0.995
Fine-grained 0.918 0.915 0.906 0.945 0.96
</bodyText>
<tableCaption confidence="0.997292">
Table 4: Modified majority voting vs proposed strategy
</tableCaption>
<page confidence="0.96472">
75
</page>
<table confidence="0.999691769230769">
Parser LA
MSTParser 0.816
MALTParser1 0.797
MALTParser2 0.796
Use MSTParser &amp; MALTParser1 if they agree, backoff to MALTParser2 0.838
Use MSTParser &amp; MALTParser2 if they agree, backoff to MALTParser2 0.837
Use MALTParser1 &amp; MALTParser2 if they agree, backoff to MSTParser 0.848
Use MSTParse &amp; MALTParser1 if they agree, backoff to most confident 0.801
Use MSTParser &amp; MALTParser2 if they agree, backoff to most confident 0.799
Use MALTParser1&amp; MALTParser2 if they agree, backoff to most confident 0.814
If at least two agree use their proposal, backoff to most confident 0.819
If all three agree use their proposal, backoff to most confident 0.797
Most confident parser only 0.789
</table>
<tableCaption confidence="0.9924">
Table 5: Labelled accuracy (LA) for various combinations of MSTParser, MALTParser1 and
MALTParser2 five fold cross-validation with 4000 training sentences and 1000 testing
</tableCaption>
<bodyText confidence="0.999869193548387">
to the other one when they do not; (iii) a strategy
in which we select a pair and trust them whenever
they agree and backoff to the parser which is most
confident (which may be one of these or may be
the other one) when they do not; (iv) strategies
where we either just use the most confident one,
or where we take either a unanimous vote or a ma-
jority vote and backoff to the most confident one
if this is inconclusive. All these experiments were
carried using fivefold cross-validation over a set of
5000 sentences from the PATB (i.e. each fold has
4000 sentences for training and 1000 for testing).
These results indicate that for parsing, simply
relying on the parser which is most likely to be
right when choosing the head for a specific depen-
dent in isolation does not produce the best over-
all result, and indeed does not even surpass the
individual parsers in isolation. For these exper-
iments, the best results were obtained by asking
a predefined pair of parsers whether they agree
on the head for a given item, and backing off to
the other one when they do not. This fits with
Henderson and Brill (2000)’s observations about
a similar strategy for dependency parsing for En-
glish. It seems likely that the problem with rely-
ing on the most confident parser for each individ-
ual daughter-head relation is that this will tend to
ignore the big picture, so that a collection of rela-
tions that are individually plausible, but which do
not add up to a coherent overall analysis, will be
picked.
</bodyText>
<sectionHeader confidence="0.998648" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999976454545455">
It seems that the success of the proposed method
for tagging depends crucially on having taggers
that exploit different principles, since under those
circumstances the systematic errors that the dif-
ferent taggers make will be different; and on the
fact that POS tags can be assigned largely inde-
pendently (though of course each of the individual
taggers makes use of information about the local
context, and in particular about the tags that have
been assigned to neighbouring items). The rea-
son why simply taking the most likely proposals
in isolation is ineffective when parsing may be that
global constraints such as Henderson and Brill’s
‘no crossing brackets’ requirement are likely to
be violated. Interestingly, the most effective of
our strategies for combining parsers takes two that
use the same learning algorithm and same feature
sets but different parsing strategies (MALTParser1
and MALTParser2), and relies on them when they
agree; and backs off to MSTParser, which ex-
ploits fundamentally different machinery, when
these two disagree. In other words, it makes use
of two parsers that depend on very similar under-
lying principles, and hence are likely to make the
same systematic errors, and backs off to one that
exploits different principles when they disagree.
We have not carried out a parallel set of exper-
iments on taggers for languages other than Arabic
because we do not have access to taggers where
we have reason to believe that the underlying prin-
ciples are different for anything other than Ara-
bic. In situations where three (or more) distinct
approaches to a problem of this kind are available,
</bodyText>
<page confidence="0.945497">
76
</page>
<bodyText confidence="0.998961">
it seems at least worthwhile investigating whether
the proposed method of combination will work.
</bodyText>
<sectionHeader confidence="0.987482" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998715333333333">
Maytham Alabbas owes his deepest gratitude to
Iraqi Ministry of Higher Education and Scientific
Research for financial support in his PhD study.
Allan Ramsay’s contribution to this work was par-
tially supported by the Qatar National Research
Fund (grant NPRP 09-046-6-001).
</bodyText>
<sectionHeader confidence="0.992641" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.985899369047619">
E Brill. 1995. Transformation-based error-driven
learning and natural language processing: a case
study in part of speech tagging. Computational Lin-
guistics, 23(4):543–565.
T Buckwalter. 2004. Buckwalter Arabic morpholog-
ical analyzer version 2.0. Linguistic Data Consor-
tium.
T Buckwalter. 2007. Issues in Arabic morphological
analysis. ARabic computational morphology, pages
23–41.
M. Diab. 2009. Second Generation Tools (AMIRA
2.0): Fast and Robust Tokenization, POS Tagging,
and Base Phrase Chunking. In Proceedings of the
Second International Conference on Arabic Lan-
guage Resources and Tools, pages 285–288, Cairo,
Eygpt, April. The MEDAR Consortium.
S Green and C D Manning. 2010. Better arabic
parsing: Baselines, evaluations, and analysis. In
Proceedings of the 23rd International Conference
on Computational Linguistics, COLING ’10, pages
394–402, Stroudsburg, PA, USA. Association for
Computational Linguistics.
N. Habash, O. Rambow, and R. Roth. 2009.
MADA+TOKAN: A Toolkit for Arabic Tokeniza-
tion, Diacritization, Morphological Disambiguation,
POS Tagging, Stemming and Lemmatization. In
Proceedings of the Second International Conference
on Arabic Language Resources and Tools, Cairo.
The MEDAR Consortium.
J C Henderson and E Brill. 2000. Exploiting diversity
in natural language processing: Combining parsers.
CoRR, cs.CL/0006003.
T Lager. 1999. µ-tbl lite: a small, extendible
transformation-based learner. In Proceedings of the
9th European Conference on Computational Lin-
guistics (EACL-99), pages 279–280, Bergen. Asso-
ciation for Computational Linguistics.
M Maamouri and A Bies. 2004. Developing an Ara-
bic treebank: methods, guidelines, procedures, and
tools. In Proceedings of the Workshop on Com-
putational Approaches to Arabic Script-based Lan-
guages, pages 2–9, Geneva.
R McDonald, K Lerman, and F Pereira. 2006a. Mul-
tilingual dependency parsing with a two-stage dis-
criminative parser. In Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
New York.
R McDonald, K Lerman, and F Pereira. 2006b. Mul-
tilingual dependency parsing with a two-stage dis-
criminative parser. In Tenth Conference on Com-
putational Natural Language Learning (CoNLL-X),
New York.
J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser:
A data-driven parser-generator for dependency pars-
ing. In Proceedings of the International Conference
on Language Resources and Evaluation (LREC),
volume 6, pages 2216–2219.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. K¨ubler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95–135.
J Nivre, L Rimell, R McDonald, and C G´omez-
Rodriguez. 2010. Evaluation of dependency parsers
on unbounded dependencies. In Proceedings of the
23rd International Conference on Computational
Linguistics, COLING ’10, pages 833–841, Beijing.
A. Ramsay and Y. Sabtan. 2009. Bootstrapping a
lexicon-free tagger for Arabic. In Proceedings of
the 9th Conference on Language Engineering, pages
202–215, Cairo, Egypt, December.
Claudio De Stefano, Antonio Della Cioppa, and An-
gelo Marcelli. 2002. An adaptive weighted major-
ity vote rule for combining multiple classifiers. In
ICPR (2), pages 192–195.
Kevin Woods, W. Philip Kegelmeyer, Jr., and Kevin
Bowyer. 1997. Combination of multiple classifiers
using local accuracy estimates. IEEE Trans. Pattern
Anal. Mach. Intell., 19(4):405–410, April.
D. Zeman and Z. ˇZabokrtsk`y. 2005. Improving
parsing accuracy by combining diverse dependency
parsers. In Proceedings of the Ninth International
Workshop on Parsing Technology, pages 171–178.
Association for Computational Linguistics.
</reference>
<page confidence="0.999125">
77
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.283850">
<title confidence="0.988644">Combining strategies for tagging and parsing Arabic</title>
<author confidence="0.69557">Maytham</author>
<affiliation confidence="0.980599">Department of Computer University of Basrah,</affiliation>
<email confidence="0.998056">maytham.alabbas@gmail.com</email>
<author confidence="0.993835">Allan</author>
<affiliation confidence="0.9995165">School of Computer University of</affiliation>
<address confidence="0.620557">Manchester M13 9PL,</address>
<email confidence="0.516325">Allan.Ramsay@manchester.ac.uk</email>
<abstract confidence="0.998505111111111">We describe a simple method for combining taggers which produces substantially better performance than any of the contributing tools. The method is very simple, but it leads to considerable improvements in performance: given three taggers for Arabic whose individual accuracies range from 0.956 to 0.967, the combined tagger scores 0.995–a sevenfold reduction in the error rate when compared to the best of the contributing tools. Given the effectiveness of this approach to combining taggers, we have investigated its applicability to parsing. For parsing, it seems better to take pairs of similar parsers and back off to a third if they disagree.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>4</issue>
<contexts>
<context position="5815" citStr="Brill, 1995" startWordPosition="957" endWordPosition="958">ith 305 tags. ABBREV EXCEPT PART PART ADJ FOCUS PART POSS PRON ADV FUT+IV PREP CONJ INTERJ PRON CV INTERROG PART PUNC CVSUFF DO IV PV DEM PRON IVSUFF DO PVSUFF DO DET LATIN RC PART DET+ADJ NEG PART REL ADV DET+NOUN NOUN REL PRON DET+NOUN PROP NOUN PROP SUB DET+NUM NO FUNC SUB CONJ EMPH PART NUM VERB PART Table 1: Coarse-grained tagset The accuracy of a tagger clearly depends on the granularity of the tagset: the contributing taggers produced scores from 0.955 to 0.967 on the coarse-grained tagset, and from 0.888 to 0.936 on the fine-grained one. We applied transformationbased retagging (TBR) (Brill, 1995; Lager, 1999) to the output of the basic taggers, which produced a small improvement in the results for MADA and MXL and a more substantial improvement for AMIRA. Table 2 shows the performance of the three taggers using the two tagsets with and without TBR. The improvement obtained by using POS TBR AMIRA MXL MADA Coarse × 0.896 0.952 0.941 √ 0.953 0.956 0.967 Fine × 0.843 0.897 0.917 √ 0.888 0.912 0.936 Table 2: Tagger accuracies in isolation, with and without TBR TBR for AMIRA arises largely from the fact that in some cases AMIRA uses tags similar to those used in the English Penn Treebank r</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>E Brill. 1995. Transformation-based error-driven learning and natural language processing: a case study in part of speech tagging. Computational Linguistics, 23(4):543–565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<title>Buckwalter Arabic morphological analyzer version 2.0. Linguistic Data Consortium.</title>
<date>2004</date>
<contexts>
<context position="3490" citStr="Buckwalter, 2004" startWordPosition="561" endWordPosition="562">MIRA (Diab, 2009), MADA (Habash et al., 2009) and a simple affix-based maximumlikelihood Arabic tagger (MXL) (Ramsay and Sabtan, 2009)) the current strategy significantly outperformed voting-based strategies. We used the Penn Arabic Treebank (PATB) Part 1 v3.0 as a resource for our experiments. The words in the PATB are already tagged, which thus provides us with a widely-accepted Gold standard. Even PATB tagging is not guaranteed to be 100% accurate, but it nonetheless provides as good a reference set as can be found.1 The PATB uses the tags provided by the Buckwalter morphological analyser (Buckwalter, 2004; Buckwalter, 2007), which carry a great deal 1The PATB is the largest easily available tagged Arabic corpus, with about 165K words in the section we are using. Thus for each fold of our 10-fold testing regime we are training on 150K words and testing on 15K, which should be enough to provide robust results. 73 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 73–77, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics of syntactically relevant information (particularly case-marking). This tagset contains 305 tags, with for in</context>
</contexts>
<marker>Buckwalter, 2004</marker>
<rawString>T Buckwalter. 2004. Buckwalter Arabic morphological analyzer version 2.0. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<title>Issues in Arabic morphological analysis. ARabic computational morphology,</title>
<date>2007</date>
<pages>23--41</pages>
<contexts>
<context position="3509" citStr="Buckwalter, 2007" startWordPosition="563" endWordPosition="564"> MADA (Habash et al., 2009) and a simple affix-based maximumlikelihood Arabic tagger (MXL) (Ramsay and Sabtan, 2009)) the current strategy significantly outperformed voting-based strategies. We used the Penn Arabic Treebank (PATB) Part 1 v3.0 as a resource for our experiments. The words in the PATB are already tagged, which thus provides us with a widely-accepted Gold standard. Even PATB tagging is not guaranteed to be 100% accurate, but it nonetheless provides as good a reference set as can be found.1 The PATB uses the tags provided by the Buckwalter morphological analyser (Buckwalter, 2004; Buckwalter, 2007), which carry a great deal 1The PATB is the largest easily available tagged Arabic corpus, with about 165K words in the section we are using. Thus for each fold of our 10-fold testing regime we are training on 150K words and testing on 15K, which should be enough to provide robust results. 73 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 73–77, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics of syntactically relevant information (particularly case-marking). This tagset contains 305 tags, with for instance 47 tags for </context>
</contexts>
<marker>Buckwalter, 2007</marker>
<rawString>T Buckwalter. 2007. Issues in Arabic morphological analysis. ARabic computational morphology, pages 23–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
</authors>
<title>Second Generation Tools (AMIRA 2.0): Fast and Robust Tokenization, POS Tagging, and Base Phrase Chunking.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second International Conference on Arabic Language Resources and Tools,</booktitle>
<pages>285--288</pages>
<publisher>The MEDAR Consortium.</publisher>
<location>Cairo, Eygpt,</location>
<contexts>
<context position="2891" citStr="Diab, 2009" startWordPosition="465" endWordPosition="466"> agree, but to switch to MSTParser if the MALTParser versions disagree. It may be that this is because the MALTParser versions are very similar, so that when they disagree this suggests that there is something anomalous about the input text, and that neither of them can be trusted at this point. 2 Tagging We present a very simple strategy for combining part-of-speech (POS) taggers which leads to substantial improvements in accuracy. A number of combination strategies have been proposed in the literature (Zeman and ˇZabokrtsk`y, 2005). In experiments with combining three Arabic taggers (AMIRA (Diab, 2009), MADA (Habash et al., 2009) and a simple affix-based maximumlikelihood Arabic tagger (MXL) (Ramsay and Sabtan, 2009)) the current strategy significantly outperformed voting-based strategies. We used the Penn Arabic Treebank (PATB) Part 1 v3.0 as a resource for our experiments. The words in the PATB are already tagged, which thus provides us with a widely-accepted Gold standard. Even PATB tagging is not guaranteed to be 100% accurate, but it nonetheless provides as good a reference set as can be found.1 The PATB uses the tags provided by the Buckwalter morphological analyser (Buckwalter, 2004;</context>
</contexts>
<marker>Diab, 2009</marker>
<rawString>M. Diab. 2009. Second Generation Tools (AMIRA 2.0): Fast and Robust Tokenization, POS Tagging, and Base Phrase Chunking. In Proceedings of the Second International Conference on Arabic Language Resources and Tools, pages 285–288, Cairo, Eygpt, April. The MEDAR Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Green</author>
<author>C D Manning</author>
</authors>
<title>Better arabic parsing: Baselines, evaluations, and analysis.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>394--402</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4824" citStr="Green and Manning, 2010" startWordPosition="782" endWordPosition="785">een different kinds of nouns and verbs (e.g. between subject and object case nouns) in the absence of visible markers make this an extremely difficult tagset to work with. It is in general virtually impossible to decide the case of an Arabic noun until its overall syntactic role is determined, and it is similarly difficult to decide the form of a verb until the overall syntactic structure of the sentence is determined. For this reason taggers often work with a coarser set of tags, of which the ‘Bies tagset’ (Maamouri and Bies, 2004) is widely used (see for instance the Stanford Arabic parser (Green and Manning, 2010)). We carried out our experiments with a variant of the original fine-grained tagset, and also with a variant of the coarser-grained Bies set obtained by deleting details such as case- and agreement-markers. We carried out two sets of experiments, with a coarsegrained set of tags (a superset of the Bies tagset with 39 tags, shown in Figure 1) and the original fine-grained one with 305 tags. ABBREV EXCEPT PART PART ADJ FOCUS PART POSS PRON ADV FUT+IV PREP CONJ INTERJ PRON CV INTERROG PART PUNC CVSUFF DO IV PV DEM PRON IVSUFF DO PVSUFF DO DET LATIN RC PART DET+ADJ NEG PART REL ADV DET+NOUN NOUN </context>
</contexts>
<marker>Green, Manning, 2010</marker>
<rawString>S Green and C D Manning. 2010. Better arabic parsing: Baselines, evaluations, and analysis. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 394–402, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
<author>R Roth</author>
</authors>
<title>MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization.</title>
<date>2009</date>
<booktitle>In Proceedings of the Second International Conference on Arabic Language Resources and Tools, Cairo. The MEDAR Consortium.</booktitle>
<contexts>
<context position="2919" citStr="Habash et al., 2009" startWordPosition="468" endWordPosition="471">ch to MSTParser if the MALTParser versions disagree. It may be that this is because the MALTParser versions are very similar, so that when they disagree this suggests that there is something anomalous about the input text, and that neither of them can be trusted at this point. 2 Tagging We present a very simple strategy for combining part-of-speech (POS) taggers which leads to substantial improvements in accuracy. A number of combination strategies have been proposed in the literature (Zeman and ˇZabokrtsk`y, 2005). In experiments with combining three Arabic taggers (AMIRA (Diab, 2009), MADA (Habash et al., 2009) and a simple affix-based maximumlikelihood Arabic tagger (MXL) (Ramsay and Sabtan, 2009)) the current strategy significantly outperformed voting-based strategies. We used the Penn Arabic Treebank (PATB) Part 1 v3.0 as a resource for our experiments. The words in the PATB are already tagged, which thus provides us with a widely-accepted Gold standard. Even PATB tagging is not guaranteed to be 100% accurate, but it nonetheless provides as good a reference set as can be found.1 The PATB uses the tags provided by the Buckwalter morphological analyser (Buckwalter, 2004; Buckwalter, 2007), which ca</context>
</contexts>
<marker>Habash, Rambow, Roth, 2009</marker>
<rawString>N. Habash, O. Rambow, and R. Roth. 2009. MADA+TOKAN: A Toolkit for Arabic Tokenization, Diacritization, Morphological Disambiguation, POS Tagging, Stemming and Lemmatization. In Proceedings of the Second International Conference on Arabic Language Resources and Tools, Cairo. The MEDAR Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Henderson</author>
<author>E Brill</author>
</authors>
<title>Exploiting diversity in natural language processing: Combining parsers.</title>
<date>2000</date>
<tech>CoRR, cs.CL/0006003.</tech>
<contexts>
<context position="14433" citStr="Henderson and Brill (2000)" startWordPosition="2483" endWordPosition="2486">n over a set of 5000 sentences from the PATB (i.e. each fold has 4000 sentences for training and 1000 for testing). These results indicate that for parsing, simply relying on the parser which is most likely to be right when choosing the head for a specific dependent in isolation does not produce the best overall result, and indeed does not even surpass the individual parsers in isolation. For these experiments, the best results were obtained by asking a predefined pair of parsers whether they agree on the head for a given item, and backing off to the other one when they do not. This fits with Henderson and Brill (2000)’s observations about a similar strategy for dependency parsing for English. It seems likely that the problem with relying on the most confident parser for each individual daughter-head relation is that this will tend to ignore the big picture, so that a collection of relations that are individually plausible, but which do not add up to a coherent overall analysis, will be picked. 4 Conclusions It seems that the success of the proposed method for tagging depends crucially on having taggers that exploit different principles, since under those circumstances the systematic errors that the differe</context>
</contexts>
<marker>Henderson, Brill, 2000</marker>
<rawString>J C Henderson and E Brill. 2000. Exploiting diversity in natural language processing: Combining parsers. CoRR, cs.CL/0006003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Lager</author>
</authors>
<title>µ-tbl lite: a small, extendible transformation-based learner.</title>
<date>1999</date>
<booktitle>In Proceedings of the 9th European Conference on Computational Linguistics (EACL-99),</booktitle>
<pages>279--280</pages>
<institution>Bergen. Association for Computational Linguistics.</institution>
<contexts>
<context position="5829" citStr="Lager, 1999" startWordPosition="959" endWordPosition="960"> ABBREV EXCEPT PART PART ADJ FOCUS PART POSS PRON ADV FUT+IV PREP CONJ INTERJ PRON CV INTERROG PART PUNC CVSUFF DO IV PV DEM PRON IVSUFF DO PVSUFF DO DET LATIN RC PART DET+ADJ NEG PART REL ADV DET+NOUN NOUN REL PRON DET+NOUN PROP NOUN PROP SUB DET+NUM NO FUNC SUB CONJ EMPH PART NUM VERB PART Table 1: Coarse-grained tagset The accuracy of a tagger clearly depends on the granularity of the tagset: the contributing taggers produced scores from 0.955 to 0.967 on the coarse-grained tagset, and from 0.888 to 0.936 on the fine-grained one. We applied transformationbased retagging (TBR) (Brill, 1995; Lager, 1999) to the output of the basic taggers, which produced a small improvement in the results for MADA and MXL and a more substantial improvement for AMIRA. Table 2 shows the performance of the three taggers using the two tagsets with and without TBR. The improvement obtained by using POS TBR AMIRA MXL MADA Coarse × 0.896 0.952 0.941 √ 0.953 0.956 0.967 Fine × 0.843 0.897 0.917 √ 0.888 0.912 0.936 Table 2: Tagger accuracies in isolation, with and without TBR TBR for AMIRA arises largely from the fact that in some cases AMIRA uses tags similar to those used in the English Penn Treebank rather than the</context>
</contexts>
<marker>Lager, 1999</marker>
<rawString>T Lager. 1999. µ-tbl lite: a small, extendible transformation-based learner. In Proceedings of the 9th European Conference on Computational Linguistics (EACL-99), pages 279–280, Bergen. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Maamouri</author>
<author>A Bies</author>
</authors>
<title>Developing an Arabic treebank: methods, guidelines, procedures, and tools.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages,</booktitle>
<pages>2--9</pages>
<location>Geneva.</location>
<contexts>
<context position="4738" citStr="Maamouri and Bies, 2004" startWordPosition="768" endWordPosition="771">rent kinds of verb and 44 for different kinds of noun. The very fine distinctions between different kinds of nouns and verbs (e.g. between subject and object case nouns) in the absence of visible markers make this an extremely difficult tagset to work with. It is in general virtually impossible to decide the case of an Arabic noun until its overall syntactic role is determined, and it is similarly difficult to decide the form of a verb until the overall syntactic structure of the sentence is determined. For this reason taggers often work with a coarser set of tags, of which the ‘Bies tagset’ (Maamouri and Bies, 2004) is widely used (see for instance the Stanford Arabic parser (Green and Manning, 2010)). We carried out our experiments with a variant of the original fine-grained tagset, and also with a variant of the coarser-grained Bies set obtained by deleting details such as case- and agreement-markers. We carried out two sets of experiments, with a coarsegrained set of tags (a superset of the Bies tagset with 39 tags, shown in Figure 1) and the original fine-grained one with 305 tags. ABBREV EXCEPT PART PART ADJ FOCUS PART POSS PRON ADV FUT+IV PREP CONJ INTERJ PRON CV INTERROG PART PUNC CVSUFF DO IV PV </context>
</contexts>
<marker>Maamouri, Bies, 2004</marker>
<rawString>M Maamouri and A Bies. 2004. Developing an Arabic treebank: methods, guidelines, procedures, and tools. In Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages, pages 2–9, Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency parsing with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<location>New York.</location>
<contexts>
<context position="2153" citStr="McDonald et al., 2006" startWordPosition="339" endWordPosition="343">y the most confident one. We hypothesise that the reason for the effectiveness of this strategy for tagging arises from the fact that the contributing taggers work in essentially different ways (different training data, different underlying algorithms), and hence if they make systematic mistakes these will tend to be different. This means, in turn, that the places where they don’t make mistakes will be different. This strategy is less effective for parsing. We have tried combining two members of the MALTParser family (Nivre et al., 2006; Nivre et al., 2007; Nivre et al., 2010) with MSTParser (McDonald et al., 2006a; McDonald et al., 2006b). The best strategy here seems to be to accept the output of the two versions of MALTParser when they agree, but to switch to MSTParser if the MALTParser versions disagree. It may be that this is because the MALTParser versions are very similar, so that when they disagree this suggests that there is something anomalous about the input text, and that neither of them can be trusted at this point. 2 Tagging We present a very simple strategy for combining part-of-speech (POS) taggers which leads to substantial improvements in accuracy. A number of combination strategies h</context>
<context position="11469" citStr="McDonald et al., 2006" startWordPosition="1979" endWordPosition="1982">ore confident than either of the others. The idea reported here is very simple, but it is also very effective. We have reduced the error in tagging with fairly coarse-grained tags to 0.05%, and we have also produced a substantial improvement for the fine grained tags, from 0.936 for the best of the individual taggers to 0.96 for the combination. 3 Parsing Given the success of the approach outlined above for tagging, it seemed worth investigating whether the same idea could be applied to parsing. We therefore tried using it with a combination of dependency parsers, for which we used MSTParser (McDonald et al., 2006a; McDonald et al., 2006b) and two variants from the MALTParser family (Nivre et al., 2006; Nivre et al., 2007; Nivre et al., 2010), namely Nivre arc-eager, which we will refer to as MALTParser1, and stack-eager, which we will refer to as MALTParser2. The results in Table 5 include (i) the three parsers in isolation; (ii) a strategy in which we select a pair and trust their proposals wherever they agree, and back-off 3In terms of error rate the difference looks more substantial, since the error rate, 0.005, for column 5 for the finegrained set is 62.5% of that for column 4, 0.008; and for the </context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R McDonald, K Lerman, and F Pereira. 2006a. Multilingual dependency parsing with a two-stage discriminative parser. In Tenth Conference on Computational Natural Language Learning (CoNLL-X), New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency parsing with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Tenth Conference on Computational Natural Language Learning (CoNLL-X),</booktitle>
<location>New York.</location>
<contexts>
<context position="2153" citStr="McDonald et al., 2006" startWordPosition="339" endWordPosition="343">y the most confident one. We hypothesise that the reason for the effectiveness of this strategy for tagging arises from the fact that the contributing taggers work in essentially different ways (different training data, different underlying algorithms), and hence if they make systematic mistakes these will tend to be different. This means, in turn, that the places where they don’t make mistakes will be different. This strategy is less effective for parsing. We have tried combining two members of the MALTParser family (Nivre et al., 2006; Nivre et al., 2007; Nivre et al., 2010) with MSTParser (McDonald et al., 2006a; McDonald et al., 2006b). The best strategy here seems to be to accept the output of the two versions of MALTParser when they agree, but to switch to MSTParser if the MALTParser versions disagree. It may be that this is because the MALTParser versions are very similar, so that when they disagree this suggests that there is something anomalous about the input text, and that neither of them can be trusted at this point. 2 Tagging We present a very simple strategy for combining part-of-speech (POS) taggers which leads to substantial improvements in accuracy. A number of combination strategies h</context>
<context position="11469" citStr="McDonald et al., 2006" startWordPosition="1979" endWordPosition="1982">ore confident than either of the others. The idea reported here is very simple, but it is also very effective. We have reduced the error in tagging with fairly coarse-grained tags to 0.05%, and we have also produced a substantial improvement for the fine grained tags, from 0.936 for the best of the individual taggers to 0.96 for the combination. 3 Parsing Given the success of the approach outlined above for tagging, it seemed worth investigating whether the same idea could be applied to parsing. We therefore tried using it with a combination of dependency parsers, for which we used MSTParser (McDonald et al., 2006a; McDonald et al., 2006b) and two variants from the MALTParser family (Nivre et al., 2006; Nivre et al., 2007; Nivre et al., 2010), namely Nivre arc-eager, which we will refer to as MALTParser1, and stack-eager, which we will refer to as MALTParser2. The results in Table 5 include (i) the three parsers in isolation; (ii) a strategy in which we select a pair and trust their proposals wherever they agree, and back-off 3In terms of error rate the difference looks more substantial, since the error rate, 0.005, for column 5 for the finegrained set is 62.5% of that for column 4, 0.008; and for the </context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R McDonald, K Lerman, and F Pereira. 2006b. Multilingual dependency parsing with a two-stage discriminative parser. In Tenth Conference on Computational Natural Language Learning (CoNLL-X), New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>MaltParser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC),</booktitle>
<volume>6</volume>
<pages>2216--2219</pages>
<contexts>
<context position="2074" citStr="Nivre et al., 2006" startWordPosition="325" endWordPosition="328">e contributing taggers how confident it is, and accepting the answer given by the most confident one. We hypothesise that the reason for the effectiveness of this strategy for tagging arises from the fact that the contributing taggers work in essentially different ways (different training data, different underlying algorithms), and hence if they make systematic mistakes these will tend to be different. This means, in turn, that the places where they don’t make mistakes will be different. This strategy is less effective for parsing. We have tried combining two members of the MALTParser family (Nivre et al., 2006; Nivre et al., 2007; Nivre et al., 2010) with MSTParser (McDonald et al., 2006a; McDonald et al., 2006b). The best strategy here seems to be to accept the output of the two versions of MALTParser when they agree, but to switch to MSTParser if the MALTParser versions disagree. It may be that this is because the MALTParser versions are very similar, so that when they disagree this suggests that there is something anomalous about the input text, and that neither of them can be trusted at this point. 2 Tagging We present a very simple strategy for combining part-of-speech (POS) taggers which lead</context>
<context position="11559" citStr="Nivre et al., 2006" startWordPosition="1994" endWordPosition="1997">o very effective. We have reduced the error in tagging with fairly coarse-grained tags to 0.05%, and we have also produced a substantial improvement for the fine grained tags, from 0.936 for the best of the individual taggers to 0.96 for the combination. 3 Parsing Given the success of the approach outlined above for tagging, it seemed worth investigating whether the same idea could be applied to parsing. We therefore tried using it with a combination of dependency parsers, for which we used MSTParser (McDonald et al., 2006a; McDonald et al., 2006b) and two variants from the MALTParser family (Nivre et al., 2006; Nivre et al., 2007; Nivre et al., 2010), namely Nivre arc-eager, which we will refer to as MALTParser1, and stack-eager, which we will refer to as MALTParser2. The results in Table 5 include (i) the three parsers in isolation; (ii) a strategy in which we select a pair and trust their proposals wherever they agree, and back-off 3In terms of error rate the difference looks more substantial, since the error rate, 0.005, for column 5 for the finegrained set is 62.5% of that for column 4, 0.008; and for the coarse-grained set the error rate for column 5, 0.04, is 73% of that for column 4, 0.055 T</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A data-driven parser-generator for dependency parsing. In Proceedings of the International Conference on Language Resources and Evaluation (LREC), volume 6, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S K¨ubler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>MaltParser: A language-independent system for datadriven dependency parsing.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>02</issue>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, K¨ubler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. K¨ubler, S. Marinov, and E. Marsi. 2007. MaltParser: A language-independent system for datadriven dependency parsing. Natural Language Engineering, 13(02):95–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>L Rimell</author>
<author>R McDonald</author>
<author>C G´omezRodriguez</author>
</authors>
<title>Evaluation of dependency parsers on unbounded dependencies.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10,</booktitle>
<pages>833--841</pages>
<location>Beijing.</location>
<marker>Nivre, Rimell, McDonald, G´omezRodriguez, 2010</marker>
<rawString>J Nivre, L Rimell, R McDonald, and C G´omezRodriguez. 2010. Evaluation of dependency parsers on unbounded dependencies. In Proceedings of the 23rd International Conference on Computational Linguistics, COLING ’10, pages 833–841, Beijing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ramsay</author>
<author>Y Sabtan</author>
</authors>
<title>Bootstrapping a lexicon-free tagger for Arabic.</title>
<date>2009</date>
<booktitle>In Proceedings of the 9th Conference on Language Engineering,</booktitle>
<pages>202--215</pages>
<location>Cairo, Egypt,</location>
<contexts>
<context position="3008" citStr="Ramsay and Sabtan, 2009" startWordPosition="481" endWordPosition="484">he MALTParser versions are very similar, so that when they disagree this suggests that there is something anomalous about the input text, and that neither of them can be trusted at this point. 2 Tagging We present a very simple strategy for combining part-of-speech (POS) taggers which leads to substantial improvements in accuracy. A number of combination strategies have been proposed in the literature (Zeman and ˇZabokrtsk`y, 2005). In experiments with combining three Arabic taggers (AMIRA (Diab, 2009), MADA (Habash et al., 2009) and a simple affix-based maximumlikelihood Arabic tagger (MXL) (Ramsay and Sabtan, 2009)) the current strategy significantly outperformed voting-based strategies. We used the Penn Arabic Treebank (PATB) Part 1 v3.0 as a resource for our experiments. The words in the PATB are already tagged, which thus provides us with a widely-accepted Gold standard. Even PATB tagging is not guaranteed to be 100% accurate, but it nonetheless provides as good a reference set as can be found.1 The PATB uses the tags provided by the Buckwalter morphological analyser (Buckwalter, 2004; Buckwalter, 2007), which carry a great deal 1The PATB is the largest easily available tagged Arabic corpus, with abo</context>
</contexts>
<marker>Ramsay, Sabtan, 2009</marker>
<rawString>A. Ramsay and Y. Sabtan. 2009. Bootstrapping a lexicon-free tagger for Arabic. In Proceedings of the 9th Conference on Language Engineering, pages 202–215, Cairo, Egypt, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio De Stefano</author>
<author>Antonio Della Cioppa</author>
<author>Angelo Marcelli</author>
</authors>
<title>An adaptive weighted majority vote rule for combining multiple classifiers.</title>
<date>2002</date>
<booktitle>In ICPR (2),</booktitle>
<pages>192--195</pages>
<marker>De Stefano, Cioppa, Marcelli, 2002</marker>
<rawString>Claudio De Stefano, Antonio Della Cioppa, and Angelo Marcelli. 2002. An adaptive weighted majority vote rule for combining multiple classifiers. In ICPR (2), pages 192–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Woods</author>
<author>W Philip Kegelmeyer</author>
<author>Kevin Bowyer</author>
</authors>
<title>Combination of multiple classifiers using local accuracy estimates.</title>
<date>1997</date>
<journal>IEEE Trans. Pattern Anal. Mach. Intell.,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Woods, Kegelmeyer, Bowyer, 1997</marker>
<rawString>Kevin Woods, W. Philip Kegelmeyer, Jr., and Kevin Bowyer. 1997. Combination of multiple classifiers using local accuracy estimates. IEEE Trans. Pattern Anal. Mach. Intell., 19(4):405–410, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zeman</author>
<author>Z ˇZabokrtsk`y</author>
</authors>
<title>Improving parsing accuracy by combining diverse dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth International Workshop on Parsing Technology,</booktitle>
<pages>171--178</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Zeman, ˇZabokrtsk`y, 2005</marker>
<rawString>D. Zeman and Z. ˇZabokrtsk`y. 2005. Improving parsing accuracy by combining diverse dependency parsers. In Proceedings of the Ninth International Workshop on Parsing Technology, pages 171–178. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
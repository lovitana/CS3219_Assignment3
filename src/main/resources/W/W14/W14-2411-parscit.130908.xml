<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001264">
<title confidence="0.9832">
From Treebank Parses to Episodic Logic and Commonsense Inference
</title>
<author confidence="0.935752">
Lenhart Schubert
</author>
<affiliation confidence="0.895175">
Univ. of Rochester
</affiliation>
<email confidence="0.989761">
schubert@cs.rochester.edu
</email>
<sectionHeader confidence="0.984853" genericHeader="abstract">
1. Introduction and overview
</sectionHeader>
<bodyText confidence="0.999765">
We have developed an approach to broad-coverage
semantic parsing that starts with Treebank parses
and yields scoped, deindexed formulas in Episodic
Logic (EL) that are directly usable for knowledge-
based inference. Distinctive properties of our ap-
proach are
</bodyText>
<listItem confidence="0.943498666666666">
• the use of a tree transduction language, TTT,
to partially disambiguate, refine (and some-
times repair) raw Treebank parses, and also to
perform many deindexing and logical canon-
icalization tasks;
• the use of EL, a Montague-inspired logical
framework for semantic representation and
knowledge representation;
• allowance for nonclassical restricted quanti-
fiers, several forms of modification and reifi-
cation, quasi-quotes and syntactic closures;
• an event semantics that directly represents
events with complex characterizations;
• a scoping algorithm that heuristically scopes
quantifiers, logical connectives, and tense;
• a compositional approach to tense deindexing
making use of tense trees; and
• the use of an inference engine, EPILOG, that
supports input-driven and goal-driven infer-
ence in EL, in a style similar to (but more
general than) Natural Logic.
</listItem>
<bodyText confidence="0.999881733333334">
We have applied this framework to general
knowledge acquisition from text corpora and the
web (though with tense meaning and many other
semantic details stripped away) (e.g., Schubert &amp;
Hwang 2000, Van Durme &amp; Schubert 2008), and
more recently to caption interpretation for family
photos, enabling alignment of names and other de-
scriptors with human faces in the photos, and to in-
terpreting sentences in simple first-reader stories.
Ongoing projects are aimed at full interpretation
of lexical glosses and other sources of explicitly
expressed general knowledge.
We now elaborate some of the themes in the pre-
ceding overview, concluding with comments on
related work and important remaining challenges.
</bodyText>
<sectionHeader confidence="0.904732" genericHeader="method">
2. Refinement of Treebank parses using TTT
</sectionHeader>
<bodyText confidence="0.999961787878788">
We generate initial logical forms by compositional
interpretation of Treebank parses produced by the
Charniak parser.1 This mapping is encumbered by
a number of difficulties. One is that current Tree-
bank parsers produce many thousands of distinct
expansions of phrasal categories, especially VPs,
into sequences of constituents. We have overcome
this difficulty through use of enhanced regular-
expression patterns applied to sequences of con-
stituent types, where our interpretive rules are as-
sociated directly with these patterns. About 100
patterns and corresponding semantic rules cover
most of English.
Two other difficulties are that parsers still intro-
duce about one phrasal error for every 10 words,
and these can render interpretations nonsensical;
and even when parses are deemed correct accord-
ing to “gold standard&amp;quot; annotated corpora, they
often conflate semantically disparate word and
phrase types. For example, prepositional phrases
(PPs) functioning as predicates are not distin-
guished from ones functioning as adverbial modi-
fiers; the roles of wh-words that form questions,
relative clauses, or wh-nominals are not distin-
guished; and constituents parsed as SBARs (sub-
ordinate clauses) can be relative clauses, adver-
bials, question clauses, or clausal nominals. Our
approach to these problems makes use of a new
tree transduction language, TTT (Purtee &amp; Schu-
bert 2012) that allows concise, modular, declara-
tive representation of tree transductions. (As in-
dicated below, TTT also plays a key role in log-
ical form postprocessing.) While we cannot ex-
</bodyText>
<footnote confidence="0.513941">
1ftp://ftp.cs.brown.edu/pub/nlparser/
</footnote>
<page confidence="0.983548">
55
</page>
<bodyText confidence="0.958118857142857">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 55–60,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
pect to correct the majority of parse errors in gen-
eral texts, we have found it easy to use TTT for
correction of certain systematic errors in particu-
lar domains. In addition, we use TTT to subclas-
sify many function words and phrase types, and to
partially disambiguate the role of PPs and SBARs,
among other phrase types, allowing more reliable
semantic interpretation.
3. EL as a semantic representation and
knowledge representation
From a compositional perspective, the semantics
of natural language is intensional and richly ex-
pressive, allowing for nonclassical quantifiers and
several types of modification and reification. Yet
many approaches to semantic interpretation rely
on first-order logic (FOL) or some subset thereof
as their target semantic representation. This is jus-
tifiable in certain restricted applications, grounded
in extensional domains such as databases. How-
ever, FOL or description logics are often chosen
as the semantic target even for broad-coverage se-
mantic parsing, because of their well-understood
semantics and proof theory and well-developed in-
ference technology and, in some cases, by a pu-
tative expressiveness-tractability tradeoff. We re-
ject such motivations – tools should be made to fit
the phenomenon rather than the other way around.
The tractability argument, for example, is simply
mistaken: Efficient inference algorithms for sub-
sets of an expressive representation can also be
implemented within a more comprehensive infer-
ence framework, without forfeiting the advantages
of expressiveness. Moreover, recent work in Nat-
ural Logic, which uses phrase-structured NL di-
rectly for inference, indicates that the richness of
language is no obstacle to rapid inference of many
obvious lexical entailments (e.g., MacCartney &amp;
Manning 2009).
Thus our target representation, EL, taking its
cue from Montague allows directly for the kinds
of quantification, intensionality, modification, and
reification found in all natural languages (e.g.,
Schubert &amp; Hwang 2000, Schubert, to appear).
In addition, EL associates episodes (events, sit-
uations, processes) directly with arbitrarily com-
plex sentences, rather than just with atomic pred-
ications, as in Davidsonian event semantics. For
example, the initial sentence in each of the follow-
ing pairs is interpreted as directly characterizing
an episode, which then serves as antecedent for a
pronoun or definite:
For many months, no rain fell;
this totally dried out the topsoil.
Each superpower menaced the other with its nuclear
arsenal; this situation persisted for decades.
Also, since NL allows for discussion of linguis-
tic and other symbolic entities, so does EL, via
quasi-quotation and substitutional quantification
(closures). These can also express axiom schemas,
and autocognitive reasoning (see further com-
ments in Section 5).
</bodyText>
<sectionHeader confidence="0.638743" genericHeader="method">
4. Comprehensive scoping and tense
deindexing
</sectionHeader>
<bodyText confidence="0.999974567567567">
Though EL is Montague-inspired, one difference
from a Montague-style intensional logic is that
we treat noun phrase (NP) interpretations as un-
scoped elements, rather than second-order predi-
cates. These elements are heuristically scoped to
the sentence level in LF postprocessing, as pro-
posed in (Schubert &amp; Pelletier 1982). The latter
proposal also covered scoping of logical connec-
tives, which exhibit the same scope ambiguities
as quantifiers. Our current heuristic scoping al-
gorithm handles these phenomena as well as tense
scope, allowing for such factors as syntactic or-
dering, island constraints, and differences in wide-
scoping tendencies among different operators.
Episodes characterized by sentences remain im-
plicit until application of a “deindexing&amp;quot; algo-
rithm. This algorithm makes use of a contextual
element called a tense tree which is built and tra-
versed in accordance with simple recursive rules
applied to indexical LFs. A tense tree contains
branches corresponding to tense and aspect op-
erators, and in the course of processing one or
more sentences, sequences of episode tokens cor-
responding to clauses are deposited at the nodes by
the deindexing rules, and adjacent tokens are used
by these same rules to posit temporal or causal re-
lations among “evoked&amp;quot; episodes. A comprehen-
sive set of rules covering all tenses, aspects, and
temporal adverbials was specified in (Hwang &amp;
Schubert 1994); the current semantic parsing ma-
chinery incorporates the tense and aspect rules but
not yet the temporal adverbial rules.
Further processing steps, many implemented
through TTT rules, further transform the LFs so
as to Skolemize top-level existentials and defi-
nite NPs (in effect accommodating their presup-
positions), separate top-level conjuncts, narrow
</bodyText>
<page confidence="0.980612">
56
</page>
<bodyText confidence="0.999704">
the scopes of certain negations, widen quantifier
scopes out of episodic operator scopes where pos-
sible, resolve intrasentential coreference, perform
lambda and equality reductions, and also gener-
ate some immediate inferences (e.g., inferring that
Mrs. Smith refers to a married woman).
The following example, for the first sentence
above, illustrates the kind of LF generated by our
semantic parser (first in unscoped, indexical form,
then the resulting set of scoped, deindexed, and
canonicalized formulas). Note that EL uses pred-
icate infixing at the sentence level, for readabil-
ity; so for example we have (E0 BEFORE NOW0)
rather than (BEFORE E0 NOW0). ‘**’ is the op-
erator linking a sentential formula to the episode
it characterizes (Schubert 2000). ADV-S is a type-
shifting operator, L stands for λ, and PLUR is a
predicate modifer that converts a predicate over in-
dividuals into a predicate over sets of individuals.
For many months, no rain fell;
</bodyText>
<equation confidence="0.974625909090909">
Refined Treebank parse:
(S (PP-FOR (IN for) (NP (CD many) (NNS months))) (|, ||,|)
(NP (DT no) (NN rain)) (VP (VBD fell)) (|: ||;|))
Unscoped, indexical LF (keys :F, :P, etc., are dropped later):
(:F (:F ADV-S (:P FOR.P (:Q MANY.DET (:F PLUR MONTH.N))))
(:I (:Q NO.DET RAIN.N) (:O PAST FALL.V)))
Canonicalized LFs (without adverbial-modifier deindexing):
(MONTHS0.SK (PLUR MONTH.N)), (MONTHS0.SK MANY.A),
((ADV-S (L Y (Y FOR.P MONTHS0.SK)))
((NO Z (Z RAIN.N) (SOME E0 (E0 BEFORE NOW0)
(Z FALL.V))) ** E0)
</equation>
<bodyText confidence="0.9999434">
With adverbial deindexing, the prefixed adver-
bial modifier would become a predication (E0
LASTS-FOR.V MONTHS0.SK); E0 is the episode of
no rain falling and MONTHS0.SK is the Skolem
name generated for the set of many months.
</bodyText>
<sectionHeader confidence="0.534611" genericHeader="method">
5. Inference using the EPILOG inference engine
</sectionHeader>
<bodyText confidence="0.999966796875">
Semantic parsers that employ FOL or a subset of
FOL (such as a description logic) as the target rep-
resentation often employ an initial “abstract&amp;quot; rep-
resentation mirroring some of the expressive de-
vices of natural languages, which is then mapped
to the target representation enabling inference. An
important feature of our approach is that (scoped,
deindexed) LFs expressed in EL are directly us-
able for inference in conjunction with lexical and
world knowledge by our EPILOG inference en-
gine. This has the advantages of not sacrificing
any of the expressiveness of language, of linking
inference more directly to surface form (in prin-
ciple enabling incremental entailment inference),
and of being easier to understand and edit than rep-
resentations remote from language.
EPILOG’s two main inference rules, for
input-driven (forward-chaining) and goal-driven
(backward-chaining) inference, substitute conse-
quences or anti-consequences for subformulas as a
function of polarity, much as in Natural Logic. But
substitutions can be based on world knowledge as
well as lexical knowledge, and to assure first-order
completeness the chaining rules are supplemented
with natural deduction rules such as proof by con-
tradiction and proof of conditional formulas by as-
sumption of the antecedent.
Moreover, EPILOG can reason with the ex-
pressive devices of EL mentioned in Sections 1
and 3 that lie beyond FOL, including general-
ized quantifiers, and reified predicates and propo-
sitions. (Schubert, to appear) contains relevant ex-
amples, such as the inference from Most of the
heavy Monroe resources are located in Monroe-
east, and background knowledge, to the conclu-
sion Few heavy resources are located in Monroe-
west; and inference of an answer to the modally
complex question Can the small crane be used
to hoist rubble from the collapsed building on
Penfield Rd onto a truck? Also, the ability to
use axiom schemas that involve quasi-quotes and
syntactic closures allows lexical inferences based
on knowledge about syntactic classes of lexical
items (i.e., meaning postulates), as well as vari-
ous forms of metareasoning, including reasoning
about the system’s own knowledge and percep-
tions (Morbini &amp; Schubert 2011). Significantly,
the expressiveness of EL/EPILOG does not pre-
vent competitive performance on first-order com-
monsense knowledge bases (derived from Doug
Lenat’s Cyc), especially as the number of KB for-
mulas grows into the thousands (Morbini &amp; Schu-
bert 2009).
In the various inference tasks to which EPI-
LOG was applied in the past, the LFs used for
natural language sentences were based on pre-
sumed compositional rules, without the machin-
ery to derive them automatically (e.g., Schubert &amp;
Hwang 2000, Morbini &amp; Schubert 2011, Stratos
et al. 2011). Starting in 2001, in developing
our KNEXT system for knowledge extraction from
text, we used broad-coverage compositional inter-
pretion into EL for the first time, but since our
goal was to obtain simple general “factoids&amp;quot;–such
</bodyText>
<page confidence="0.992913">
57
</page>
<bodyText confidence="0.999959916666667">
as that a person may believe a proposition, peo-
ple may wish to get rid of a dictator, clothes can
be washed, etc. (expressed logically)–our interpre-
tive rules ignored tense, many modifiers, and other
subtleties (e.g., Van Durme &amp; Schubert 2008).
Factoids like the ones mentioned are uncondi-
tional and as such not directly usable for inference,
but many millions of the factoids have been auto-
matically strengthened into quantified, inference-
enabling commonsense axioms (Gordon &amp; Schu-
bert 2010), and allow EPILOG to draw conclusions
from short sentences (Gordon 2014, chapter 6).
An example is the inference from Tremblay is a
singer to the conclusion Quite possibly Tremblay
occasionally performs (or performed) a song (au-
tomatically verbalized from an EL formula). Here
the modal and frequency modification would not
easily be captured within an FOL framework.
Recently, we have begun to apply much more
complete compositional semantic rules to sen-
tences “in the wild&amp;quot;, choosing two settings where
sentences tend to be short (to minimize the impact
of parse errors on semantic interpretation): deriva-
tion and integration of caption-derived knowledge
and image-derived knowledge in a family photo
domain, and interpretation of sentences in first-
reader stories. In the family photo domain, we
have fully interpreted the captions in a small de-
velopment set, and used an EPILOG knowledge
base to derive implicit attributes of the individuals
mentioned in the captions (by name or other des-
ignations). These attributes then served to align
the caption-derived individuals with individuals
detected in the images, and were subsequently
merged with image-derived attributes (with al-
lowance for uncertainty). For example, for the
caption Tanya and Grandma Lillian at her high
school graduation party, after correct interpreta-
tion of her as referring to Tanya, Tanya was in-
ferred to be a teenager (from the knowledge that
a high school graduation party is generally held
for a recent high school graduate, and a recent
high school graduate is likely to be a teenager);
while Grandma Lillian was inferred to be a grand-
mother, hence probably a senior, hence quite pos-
sibly gray-haired, and this enabled correct align-
ment of the names with the persons detected in the
image, determined via image processing to be a
young dark-haired female and a senior gray-haired
female respectively.
In the first-reader domain (where we are using
McGuffey (2005)), we found that we could obtain
correct or nearly correct interpretations for most
simple declaratives (and some of the stories con-
sist entirely of such sentences). At the time of
writing, we are still working on discourse phe-
nomena, especially in stories involving dialogues.
For example, our semantic parser correctly derived
and canonicalized the logical content of the open-
ing line of one of the stories under consideration,
</bodyText>
<subsubsectionHeader confidence="0.46224">
Oh Rosie! Do you see that nest in the apple tree?
</subsubsectionHeader>
<bodyText confidence="0.999972838709677">
The interpretation includes separate speech acts
for the initial interjection and the question. Our
goal in this work is integration of symbolic infer-
ence with inferences from imagistic modeling (for
which we are using the Blender open source soft-
ware), where the latter provides spatial inferences
such as that the contents of a nest in a tree are not
likely to be visible to children on the ground (set-
ting the stage for the continuation of the story).
Phenomena not handled well at this point in-
clude intersentential anaphora, questions with
gaps, imperatives, interjections, and direct ad-
dress (Look, Lucy, ...). We are making progress
on these, by using TTT repair rules for phenom-
ena where Treebank parsers tend to falter, and by
adding LF-level and discourse-level interpretive
rules for the resulting phrasal patterns. Ongoing
projects are aimed at full interpretation of lexical
glosses and other sources of explicitly expressed
general knowledge. However, as we explain in
the concluding section, we do not believe that full-
fledged, deep story understanding will be possible
until we have large amounts of general knowledge,
including not only the kinds of “if-then&amp;quot; knowl-
edge (about word meanings and the world) we
and others have been deriving and are continuing
to derive, but also large amounts of pattern-like,
schematic knowledge encoding our expectations
about typical object configurations and event se-
quences (especially ones directed towards agents’
goals) in the world and in dialogue.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="method">
6. Related work
</sectionHeader>
<bodyText confidence="0.9990065">
Most current projects in semantic parsing either
single out domains that assure highly restricted
natural language usage, or greatly limit the seman-
tic content that is extracted from text. For exam-
ple, projects may be aimed at question-answering
over relational databases, with themes such as
geography, air travel planning, or robocup (e.g.,
Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011,
</bodyText>
<page confidence="0.995714">
58
</page>
<bodyText confidence="0.999389162162162">
Kwiatkowski et al. 2011, Liang et al. 2011, Poon
2013). Impressive thematic scope is achieved in
(Berant et al. 2013, Kwiatkowski et al. 2013), but
the target semantic language (for Freebase access)
is still restricted to database operations such as
join, intersection, and set cardinality. Another
popular domain is command execution by robots
(e.g., Tellex 2011, Howard et al. 2013, Artzi &amp;
Zettlemoyer 2013).
Examples of work aimed at broader lin-
guistic coverage are Johan Bos’ Boxer project
(Bos 2008), Lewis &amp; Steedman’s (2013) CCG-
Distributional system, James Allen et al.’s (2013)
work on extracting an OWL-DL verb ontology
from WordNet, and Draicchio et al.’s (2013)
FRED system for mapping from NL to OWL on-
tology. Boxer2 is highly developed, but inter-
pretations are limited to FOL, so that the kinds
of general quantification, reification and modifi-
cation that pervade ordinary language cannot be
adequately captured. The CCG-Distributional ap-
proach combines logical and distributional seman-
tics in an interesting way, but apart from the FOL
limitation, the induced cluster-based predicates
lose distinctions such as that between town and
country or between elected to and ran for. As
such, the system is applicable to (soft) entailment
verification, but probably not to reasoning. A
major limitation of mapping natural language to
OWL-DL is that the assertion component of the
latter is essentially limited to atomic predications
and their negations, so that ordinary statements
such as Most students who passed the AI exam
also passed the theory exam, or If Kim and Sandy
get divorced, then Kim will probably get custody
of their children, cannot be represented, let alone
reasoned with.
</bodyText>
<subsectionHeader confidence="0.489745">
7. Concluding thoughts
</subsectionHeader>
<bodyText confidence="0.999340363636363">
The history of research in natural language under-
standing shows two seemingly divergent trends:
One is the attempt to faithfully capture the log-
ical form of natural language sentences, and to
study entailment relations based on such forms.
The other is the effort to map language onto
preexisting, schematic knowledge structures of
some sort, intended as a basis for understand-
ing and inference – these might be FrameNet-like
or Minsky-like frames, concepts in a description
logic, Schankian scripts, general plans as under-
</bodyText>
<footnote confidence="0.75603">
2www.meaningfactory.com/bos/pubs/Bos2008STEP2.pdf
</footnote>
<bodyText confidence="0.999371428571428">
stood in AI, Pustejovskyan telic event schemas,
or something similar. Both perspectives seem to
have compelling merits, and this leads us to sup-
pose that deep understanding may indeed require
both surface representations and schematic repre-
sentations, where surface representations can be
viewed as concise abstractions from, or summaries
of, schema instances or (for generic statements) of
the schemas themselves. But where we differ from
most approaches is that we would want both levels
of representation to support inference. The surface
level should support at least Natural-Logic-like
entailment inference, along with inference chain-
ing – for which EL and EPILOG are well-suited.
The schematic level would support “reasonable&amp;quot;
(or default) expectations based on familiar patterns
of events, actions, or relationships. Further, the
schematic level should itself allow for language-
like expressiveness in the specification of roles,
steps, goals, or other components, which might
again be abstractions from more basic schemas.
In other words, we envisage hierarchically orga-
nized schemas whose constituents are expressed
in a language like EL and allow for EPILOG-like
inference. We see the acquisition of such schemas
as the most pressing need in machine understand-
ing. Without them, we are limited to either narrow
or shallow understanding.
</bodyText>
<sectionHeader confidence="0.993807" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998665">
This work was supported by NSF grant IIS-
0916599 and a subcontract to ONR STTR
N00014-10-M-0297. The comments of the anony-
mous referees helped to improve the paper.
</bodyText>
<sectionHeader confidence="0.998505" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9971438">
J. Allen, J. Orfan, W. de Beaumont, C. M. Teng, L.
Galescu, M. Swift, 2013. Automatically deriv-
ing event ontologies for a commonsense knowledge
base. 10th Int. Conf. on Computational Semantics
(IWCS 2013), Potsdam, Germany, March 19-22.
Y. Artzi and L. Zettlemoyer, 2011. Bootstrap-
ping Semantic Parsers from Conversations. Em-
pirical Methods in Natural Language Processing
(EMNLP), Edinburgh, UK.
Y. Artzi and L. Zettlemoyer, 2013. Weakly supervised
learning of semantic parsers for mapping instruc-
tions to actions. In Trans. of the Assoc. for Com-
putational Linguistics (TACL).
J. Berant, A. Chou, R. Frostig, P. Liang, 2013. Seman-
tic parsing on Freebase from question-answer pairs.
</reference>
<page confidence="0.993302">
59
</page>
<reference confidence="0.999393691588785">
Empirical Methods in Natural Language Processing
(EMNLP), Seattle, WA.
J. Bos, 2008. Wide-coverage semantic analysis with
Boxer. In J. Bos and R. Delmonte (eds.), Seman-
tics in Text Processing. STEP 2008 Conference Pro-
ceedings. Research in Computational Semantics,
College Publications, 277–286.
F. Draicchio, A. Gangemi, V. Presutti, and A.G. Nuz-
zolese, 2013. FRED: From natural language text
to RDF and OWL in one click. In P. Cimiano et al.
(eds.). ESWC 2013, LNCS 7955, Springer, 263-267.
R. Ge and R. J. Mooney, 2009. Learning a Compo-
sitional Semantic Parser using an Existing Syntactic
Parser, In Proceedings of the ACL-IJCNLP 2009,
Suntec, Singapore.
J. Gordon, 2014. Inferential Commonsense
Knowledge from Text. Ph.D. Thesis, De-
partment of Computer Science, University
of Rochester, Rochester, NY. Available at
http://www.cs.rochester.edu/u/jgordon/.
J. Gordon and L. K. Schubert, 2010. Quantificational
sharpening of commonsense knowledge. Com-
mon Sense Knowledge Symposium (CSK-10), AAAI
2010 Fall Symposium Series, Arlington, VA.
T. M. Howard, S. Tellex, and N. Roy, 2013. A Natu-
ral Language Planner Interface for Mobile Manipu-
lators. 30th Int. Conf. on Machine Learning, Atlanta,
GA, JMLR: W&amp;CP volume 28.
C. H. Hwang and L. K. Schubert, 1994. Interpreting
tense, aspect, and time adverbials: a compositional,
unified approach. D. M. Gabbay and H. J. Ohlbach
(eds.), Proc. of the 1st Int. Conf. on Temporal Logic,
Bonn, Germany, Springer, 238–264.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer,
2013. Scaling semantic parsers with on-the-fly on-
tology matching. Empirical Methods in Natural
Language Processing (EMNLP), Seattle, WA.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman, 2011. Lexical generalization in
CCG grammar induction for semantic parsing. Em-
pirical Methods in Natural Language Processing
(EMNLP), Edinburgh, UK.
M. Lewis and M. Steedman, 2013. Combined distri-
butional and logical semantics. Trans. of the Assoc.
for Computational Linguistics 1, 179–192.
P. Liang, M. Jordan, and D. Klein, 2011. Learning
dependency-based compositional semantics. Conf.
of the Assoc. for Computational Linguistics (ACL),
Portland, OR.
B. MacCartney and C. D. Manning, 2009. An ex-
tended model of natural logic. 8th Int. Conf. on
Computational Semantics (IWCS-8), Tilburg Uni-
versity, Netherlands. Assoc. for Computational Lin-
guistics (ACL), Stroudsberg, PA.
W. H. McGuffey, 2005 (original edition 1879).
McGuffey’s First Eclectic Reader. John Wiley and
Sons, New York.
F. Morbini and L. K. Schubert, 2011. Metareasoning
as an integral part of commonsense and autocog-
nitive reasoning. In M.T. Cox and A. Raja (eds.),
Metareasoning: Thinking about Thinking, Ch. 17.
MIT Press, Cambridge, MA, 267–282.
F. Morbini &amp; Schubert, 2009. Evaluation of
Epilog: A reasoner for Episodic Logic. Common-
sense’09, June 1-3, Toronto, Canada. Available at
http://commonsensereasoning.org/2009/papers.html.
H. Poon, 2013. Grounded unsupervised semantic pars-
ing. 51st Ann. Meet. of the Assoc. for Computational
Linguistics (ACL), Sofia, Bulgaria.
A. Purtee and L. K. Schubert, 2012. TTT: A tree trans-
duction language for syntactic and semantic process-
ing. EACL 2012 Workshop on Applications of Tree
Automata Techniques in Natural Language Process-
ing (ATANLP 2012), Avignon, France.
L. K. Schubert, to appear. NLog-like inference and
commonsense reasoning. In A. Zaenen, V. de
Paiva and C. Condoravdi (eds.), Semantics for
Textual Inference, CSLI Publications. Available at
http://www.cs.rochester.edu/u/schubert/papers/nlog-
like-inference12.pdf.
L. K. Schubert, 2000. The situations we talk about. In
J. Minker (ed.), Logic-Based Artificial Intelligence,
Kluwer, Dortrecht, 407–439.
L. K. Schubert and C. H. Hwang, 2000. Episodic
Logic meets Little Red Riding Hood: A comprehen-
sive, natural representation for language understand-
ing. In L. Iwanska and S.C. Shapiro (eds.), Natural
Language Processing and Knowledge Representa-
tion: Language for Knowledge and Knowledge for
Language. MIT/AAAI Press, Menlo Park, CA, and
Cambridge, MA, 111–174.
L. K. Schubert and F.J. Pelletier, 1982. From En-
glish to logic: Context-free computation of ‘conven-
tional’ logical translations. Am. J. of Computational
Linguistics 8, 27–44. Reprinted in B.J. Grosz, K.
Sparck Jones, and B.L. Webber (eds.), Readings in
Natural Language Processing, Morgan Kaufmann,
Los Altos, CA, 1986, 293-311.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.
Banerjee, S. Teller, and N. Roy, 2011. Understand-
ing natural language commands for robotic naviga-
tion and mobile manipulation. Nat. Conf. on Artifi-
cial Intelligence (AAAI 2011), San Francisco, CA.
B. Van Durme and L. K. Schubert, 2008. Open
knowledge extraction through compositional lan-
guage processing. Symposium on Semantics in Sys-
tems for Text Processing (STEP 2008), Venice, Italy.
</reference>
<page confidence="0.998408">
60
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.732170">
<title confidence="0.898576">From Treebank Parses to Episodic Logic and Commonsense Inference</title>
<author confidence="0.760513">Lenhart</author>
<affiliation confidence="0.997853">Univ. of Rochester</affiliation>
<email confidence="0.979009">schubert@cs.rochester.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>J Orfan</author>
<author>W de Beaumont</author>
<author>C M Teng</author>
<author>L Galescu</author>
<author>M Swift</author>
</authors>
<title>Automatically deriving event ontologies for a commonsense knowledge base.</title>
<date>2013</date>
<booktitle>10th Int. Conf. on Computational Semantics (IWCS 2013),</booktitle>
<location>Potsdam, Germany,</location>
<marker>Allen, Orfan, de Beaumont, Teng, Galescu, Swift, 2013</marker>
<rawString>J. Allen, J. Orfan, W. de Beaumont, C. M. Teng, L. Galescu, M. Swift, 2013. Automatically deriving event ontologies for a commonsense knowledge base. 10th Int. Conf. on Computational Semantics (IWCS 2013), Potsdam, Germany, March 19-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Bootstrapping Semantic Parsers from Conversations.</title>
<date>2011</date>
<booktitle>Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="17947" citStr="Artzi &amp; Zettlemoyer 2011" startWordPosition="2763" endWordPosition="2766">, but also large amounts of pattern-like, schematic knowledge encoding our expectations about typical object configurations and event sequences (especially ones directed towards agents’ goals) in the world and in dialogue. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011, 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi &amp; Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis &amp; Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on </context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Y. Artzi and L. Zettlemoyer, 2011. Bootstrapping Semantic Parsers from Conversations. Empirical Methods in Natural Language Processing (EMNLP), Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<booktitle>In Trans. of the Assoc. for Computational Linguistics (TACL).</booktitle>
<contexts>
<context position="18362" citStr="Artzi &amp; Zettlemoyer 2013" startWordPosition="2828" endWordPosition="2831">ed from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011, 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi &amp; Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis &amp; Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicchio et al.’s (2013) FRED system for mapping from NL to OWL ontology. Boxer2 is highly developed, but interpretations are limited to FOL, so that the kinds of general quantification, reification and modification that pervade ordinary language cannot be adequately captured. The CCG-Distributional approach combines logical and distributional semantics in an</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Y. Artzi and L. Zettlemoyer, 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. In Trans. of the Assoc. for Computational Linguistics (TACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berant</author>
<author>A Chou</author>
<author>R Frostig</author>
<author>P Liang</author>
</authors>
<title>Semantic parsing on Freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="18068" citStr="Berant et al. 2013" startWordPosition="2784" endWordPosition="2787"> event sequences (especially ones directed towards agents’ goals) in the world and in dialogue. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011, 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi &amp; Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis &amp; Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicchio et al.’s (2013) FRED system for mapping from NL to OWL ont</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>J. Berant, A. Chou, R. Frostig, P. Liang, 2013. Semantic parsing on Freebase from question-answer pairs. Empirical Methods in Natural Language Processing (EMNLP), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
</authors>
<title>Wide-coverage semantic analysis with Boxer.</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings. Research in Computational Semantics, College Publications,</booktitle>
<pages>277--286</pages>
<editor>In J. Bos and R. Delmonte (eds.),</editor>
<contexts>
<context position="18457" citStr="Bos 2008" startWordPosition="2846" endWordPosition="2847">uch as geography, air travel planning, or robocup (e.g., Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011, 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi &amp; Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis &amp; Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicchio et al.’s (2013) FRED system for mapping from NL to OWL ontology. Boxer2 is highly developed, but interpretations are limited to FOL, so that the kinds of general quantification, reification and modification that pervade ordinary language cannot be adequately captured. The CCG-Distributional approach combines logical and distributional semantics in an interesting way, but apart from the FOL limitation, the induced cluster-based predicates lose </context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>J. Bos, 2008. Wide-coverage semantic analysis with Boxer. In J. Bos and R. Delmonte (eds.), Semantics in Text Processing. STEP 2008 Conference Proceedings. Research in Computational Semantics, College Publications, 277–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Draicchio</author>
<author>A Gangemi</author>
<author>V Presutti</author>
<author>A G Nuzzolese</author>
</authors>
<title>FRED: From natural language text to RDF and OWL in one click.</title>
<date>2013</date>
<booktitle>ESWC 2013, LNCS</booktitle>
<pages>7955</pages>
<editor>In P. Cimiano et al. (eds.).</editor>
<publisher>Springer,</publisher>
<marker>Draicchio, Gangemi, Presutti, Nuzzolese, 2013</marker>
<rawString>F. Draicchio, A. Gangemi, V. Presutti, and A.G. Nuzzolese, 2013. FRED: From natural language text to RDF and OWL in one click. In P. Cimiano et al. (eds.). ESWC 2013, LNCS 7955, Springer, 263-267.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ge</author>
<author>R J Mooney</author>
</authors>
<title>Learning a Compositional Semantic Parser using an Existing Syntactic Parser,</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP</booktitle>
<location>Suntec, Singapore.</location>
<contexts>
<context position="17921" citStr="Ge &amp; Mooney 2009" startWordPosition="2759" endWordPosition="2762">ntinuing to derive, but also large amounts of pattern-like, schematic knowledge encoding our expectations about typical object configurations and event sequences (especially ones directed towards agents’ goals) in the world and in dialogue. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011, 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi &amp; Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis &amp; Steedman’s (2013) CCGDistributional system, James Alle</context>
</contexts>
<marker>Ge, Mooney, 2009</marker>
<rawString>R. Ge and R. J. Mooney, 2009. Learning a Compositional Semantic Parser using an Existing Syntactic Parser, In Proceedings of the ACL-IJCNLP 2009, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gordon</author>
</authors>
<title>Inferential Commonsense Knowledge from Text.</title>
<date>2014</date>
<tech>Ph.D. Thesis,</tech>
<institution>Department of Computer Science, University of Rochester,</institution>
<location>Rochester, NY.</location>
<note>Available at http://www.cs.rochester.edu/u/jgordon/.</note>
<contexts>
<context position="13687" citStr="Gordon 2014" startWordPosition="2088" endWordPosition="2089">s to obtain simple general “factoids&amp;quot;–such 57 as that a person may believe a proposition, people may wish to get rid of a dictator, clothes can be washed, etc. (expressed logically)–our interpretive rules ignored tense, many modifiers, and other subtleties (e.g., Van Durme &amp; Schubert 2008). Factoids like the ones mentioned are unconditional and as such not directly usable for inference, but many millions of the factoids have been automatically strengthened into quantified, inferenceenabling commonsense axioms (Gordon &amp; Schubert 2010), and allow EPILOG to draw conclusions from short sentences (Gordon 2014, chapter 6). An example is the inference from Tremblay is a singer to the conclusion Quite possibly Tremblay occasionally performs (or performed) a song (automatically verbalized from an EL formula). Here the modal and frequency modification would not easily be captured within an FOL framework. Recently, we have begun to apply much more complete compositional semantic rules to sentences “in the wild&amp;quot;, choosing two settings where sentences tend to be short (to minimize the impact of parse errors on semantic interpretation): derivation and integration of caption-derived knowledge and image-deri</context>
</contexts>
<marker>Gordon, 2014</marker>
<rawString>J. Gordon, 2014. Inferential Commonsense Knowledge from Text. Ph.D. Thesis, Department of Computer Science, University of Rochester, Rochester, NY. Available at http://www.cs.rochester.edu/u/jgordon/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gordon</author>
<author>L K Schubert</author>
</authors>
<title>Quantificational sharpening of commonsense knowledge.</title>
<date>2010</date>
<booktitle>Common Sense Knowledge Symposium (CSK-10), AAAI 2010 Fall Symposium Series,</booktitle>
<location>Arlington, VA.</location>
<contexts>
<context position="13615" citStr="Gordon &amp; Schubert 2010" startWordPosition="2074" endWordPosition="2078">overage compositional interpretion into EL for the first time, but since our goal was to obtain simple general “factoids&amp;quot;–such 57 as that a person may believe a proposition, people may wish to get rid of a dictator, clothes can be washed, etc. (expressed logically)–our interpretive rules ignored tense, many modifiers, and other subtleties (e.g., Van Durme &amp; Schubert 2008). Factoids like the ones mentioned are unconditional and as such not directly usable for inference, but many millions of the factoids have been automatically strengthened into quantified, inferenceenabling commonsense axioms (Gordon &amp; Schubert 2010), and allow EPILOG to draw conclusions from short sentences (Gordon 2014, chapter 6). An example is the inference from Tremblay is a singer to the conclusion Quite possibly Tremblay occasionally performs (or performed) a song (automatically verbalized from an EL formula). Here the modal and frequency modification would not easily be captured within an FOL framework. Recently, we have begun to apply much more complete compositional semantic rules to sentences “in the wild&amp;quot;, choosing two settings where sentences tend to be short (to minimize the impact of parse errors on semantic interpretation)</context>
</contexts>
<marker>Gordon, Schubert, 2010</marker>
<rawString>J. Gordon and L. K. Schubert, 2010. Quantificational sharpening of commonsense knowledge. Common Sense Knowledge Symposium (CSK-10), AAAI 2010 Fall Symposium Series, Arlington, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T M Howard</author>
<author>S Tellex</author>
<author>N Roy</author>
</authors>
<title>A Natural Language Planner Interface for Mobile Manipulators.</title>
<date>2013</date>
<journal>W&amp;CP</journal>
<booktitle>30th Int. Conf. on Machine Learning,</booktitle>
<volume>28</volume>
<location>Atlanta, GA, JMLR:</location>
<contexts>
<context position="18335" citStr="Howard et al. 2013" startWordPosition="2824" endWordPosition="2827">tent that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011, 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi &amp; Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis &amp; Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicchio et al.’s (2013) FRED system for mapping from NL to OWL ontology. Boxer2 is highly developed, but interpretations are limited to FOL, so that the kinds of general quantification, reification and modification that pervade ordinary language cannot be adequately captured. The CCG-Distributional approach combines logical and dis</context>
</contexts>
<marker>Howard, Tellex, Roy, 2013</marker>
<rawString>T. M. Howard, S. Tellex, and N. Roy, 2013. A Natural Language Planner Interface for Mobile Manipulators. 30th Int. Conf. on Machine Learning, Atlanta, GA, JMLR: W&amp;CP volume 28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Hwang</author>
<author>L K Schubert</author>
</authors>
<title>Interpreting tense, aspect, and time adverbials: a compositional, unified approach.</title>
<date>1994</date>
<booktitle>Proc. of the 1st Int. Conf. on Temporal Logic,</booktitle>
<pages>238--264</pages>
<editor>D. M. Gabbay and H. J. Ohlbach (eds.),</editor>
<publisher>Springer,</publisher>
<location>Bonn, Germany,</location>
<contexts>
<context position="8054" citStr="Hwang &amp; Schubert 1994" startWordPosition="1201" endWordPosition="1204">makes use of a contextual element called a tense tree which is built and traversed in accordance with simple recursive rules applied to indexical LFs. A tense tree contains branches corresponding to tense and aspect operators, and in the course of processing one or more sentences, sequences of episode tokens corresponding to clauses are deposited at the nodes by the deindexing rules, and adjacent tokens are used by these same rules to posit temporal or causal relations among “evoked&amp;quot; episodes. A comprehensive set of rules covering all tenses, aspects, and temporal adverbials was specified in (Hwang &amp; Schubert 1994); the current semantic parsing machinery incorporates the tense and aspect rules but not yet the temporal adverbial rules. Further processing steps, many implemented through TTT rules, further transform the LFs so as to Skolemize top-level existentials and definite NPs (in effect accommodating their presuppositions), separate top-level conjuncts, narrow 56 the scopes of certain negations, widen quantifier scopes out of episodic operator scopes where possible, resolve intrasentential coreference, perform lambda and equality reductions, and also generate some immediate inferences (e.g., inferrin</context>
</contexts>
<marker>Hwang, Schubert, 1994</marker>
<rawString>C. H. Hwang and L. K. Schubert, 1994. Interpreting tense, aspect, and time adverbials: a compositional, unified approach. D. M. Gabbay and H. J. Ohlbach (eds.), Proc. of the 1st Int. Conf. on Temporal Logic, Bonn, Germany, Springer, 238–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>E Choi</author>
<author>Y Artzi</author>
<author>L Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="18094" citStr="Kwiatkowski et al. 2013" startWordPosition="2788" endWordPosition="2791">pecially ones directed towards agents’ goals) in the world and in dialogue. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011, 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi &amp; Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis &amp; Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicchio et al.’s (2013) FRED system for mapping from NL to OWL ontology. Boxer2 is highly de</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer, 2013. Scaling semantic parsers with on-the-fly ontology matching. Empirical Methods in Natural Language Processing (EMNLP), Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kwiatkowski</author>
<author>L Zettlemoyer</author>
<author>S Goldwater</author>
<author>M Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Edinburgh, UK.</location>
<contexts>
<context position="17975" citStr="Kwiatkowski et al. 2011" startWordPosition="2768" endWordPosition="2771">attern-like, schematic knowledge encoding our expectations about typical object configurations and event sequences (especially ones directed towards agents’ goals) in the world and in dialogue. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011, 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi &amp; Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis &amp; Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb on</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and M. Steedman, 2011. Lexical generalization in CCG grammar induction for semantic parsing. Empirical Methods in Natural Language Processing (EMNLP), Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lewis</author>
<author>M Steedman</author>
</authors>
<title>Combined distributional and logical semantics.</title>
<date>2013</date>
<journal>Trans. of the Assoc. for Computational Linguistics</journal>
<volume>1</volume>
<pages>179--192</pages>
<marker>Lewis, Steedman, 2013</marker>
<rawString>M. Lewis and M. Steedman, 2013. Combined distributional and logical semantics. Trans. of the Assoc. for Computational Linguistics 1, 179–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>M Jordan</author>
<author>D Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>Conf. of the Assoc. for Computational Linguistics (ACL),</booktitle>
<location>Portland, OR.</location>
<contexts>
<context position="17994" citStr="Liang et al. 2011" startWordPosition="2772" endWordPosition="2775">owledge encoding our expectations about typical object configurations and event sequences (especially ones directed towards agents’ goals) in the world and in dialogue. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011, 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi &amp; Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis &amp; Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>P. Liang, M. Jordan, and D. Klein, 2011. Learning dependency-based compositional semantics. Conf. of the Assoc. for Computational Linguistics (ACL), Portland, OR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacCartney</author>
<author>C D Manning</author>
</authors>
<title>An extended model of natural logic.</title>
<date>2009</date>
<booktitle>8th Int. Conf. on Computational Semantics (IWCS-8),</booktitle>
<institution>Tilburg University, Netherlands. Assoc. for Computational Linguistics (ACL),</institution>
<location>Stroudsberg, PA.</location>
<contexts>
<context position="5560" citStr="MacCartney &amp; Manning 2009" startWordPosition="822" endWordPosition="825">ctability tradeoff. We reject such motivations – tools should be made to fit the phenomenon rather than the other way around. The tractability argument, for example, is simply mistaken: Efficient inference algorithms for subsets of an expressive representation can also be implemented within a more comprehensive inference framework, without forfeiting the advantages of expressiveness. Moreover, recent work in Natural Logic, which uses phrase-structured NL directly for inference, indicates that the richness of language is no obstacle to rapid inference of many obvious lexical entailments (e.g., MacCartney &amp; Manning 2009). Thus our target representation, EL, taking its cue from Montague allows directly for the kinds of quantification, intensionality, modification, and reification found in all natural languages (e.g., Schubert &amp; Hwang 2000, Schubert, to appear). In addition, EL associates episodes (events, situations, processes) directly with arbitrarily complex sentences, rather than just with atomic predications, as in Davidsonian event semantics. For example, the initial sentence in each of the following pairs is interpreted as directly characterizing an episode, which then serves as antecedent for a pronoun</context>
</contexts>
<marker>MacCartney, Manning, 2009</marker>
<rawString>B. MacCartney and C. D. Manning, 2009. An extended model of natural logic. 8th Int. Conf. on Computational Semantics (IWCS-8), Tilburg University, Netherlands. Assoc. for Computational Linguistics (ACL), Stroudsberg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W H McGuffey</author>
</authors>
<title>(original edition 1879). McGuffey’s First Eclectic Reader.</title>
<date>2005</date>
<publisher>John Wiley and Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="15559" citStr="McGuffey (2005)" startWordPosition="2385" endWordPosition="2386"> of her as referring to Tanya, Tanya was inferred to be a teenager (from the knowledge that a high school graduation party is generally held for a recent high school graduate, and a recent high school graduate is likely to be a teenager); while Grandma Lillian was inferred to be a grandmother, hence probably a senior, hence quite possibly gray-haired, and this enabled correct alignment of the names with the persons detected in the image, determined via image processing to be a young dark-haired female and a senior gray-haired female respectively. In the first-reader domain (where we are using McGuffey (2005)), we found that we could obtain correct or nearly correct interpretations for most simple declaratives (and some of the stories consist entirely of such sentences). At the time of writing, we are still working on discourse phenomena, especially in stories involving dialogues. For example, our semantic parser correctly derived and canonicalized the logical content of the opening line of one of the stories under consideration, Oh Rosie! Do you see that nest in the apple tree? The interpretation includes separate speech acts for the initial interjection and the question. Our goal in this work is</context>
</contexts>
<marker>McGuffey, 2005</marker>
<rawString>W. H. McGuffey, 2005 (original edition 1879). McGuffey’s First Eclectic Reader. John Wiley and Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Morbini</author>
<author>L K Schubert</author>
</authors>
<title>Metareasoning as an integral part of commonsense and autocognitive reasoning.</title>
<date>2011</date>
<booktitle>Metareasoning: Thinking about Thinking, Ch. 17.</booktitle>
<pages>267--282</pages>
<editor>In M.T. Cox and A. Raja (eds.),</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="12346" citStr="Morbini &amp; Schubert 2011" startWordPosition="1872" endWordPosition="1875">ources are located in Monroeeast, and background knowledge, to the conclusion Few heavy resources are located in Monroewest; and inference of an answer to the modally complex question Can the small crane be used to hoist rubble from the collapsed building on Penfield Rd onto a truck? Also, the ability to use axiom schemas that involve quasi-quotes and syntactic closures allows lexical inferences based on knowledge about syntactic classes of lexical items (i.e., meaning postulates), as well as various forms of metareasoning, including reasoning about the system’s own knowledge and perceptions (Morbini &amp; Schubert 2011). Significantly, the expressiveness of EL/EPILOG does not prevent competitive performance on first-order commonsense knowledge bases (derived from Doug Lenat’s Cyc), especially as the number of KB formulas grows into the thousands (Morbini &amp; Schubert 2009). In the various inference tasks to which EPILOG was applied in the past, the LFs used for natural language sentences were based on presumed compositional rules, without the machinery to derive them automatically (e.g., Schubert &amp; Hwang 2000, Morbini &amp; Schubert 2011, Stratos et al. 2011). Starting in 2001, in developing our KNEXT system for k</context>
</contexts>
<marker>Morbini, Schubert, 2011</marker>
<rawString>F. Morbini and L. K. Schubert, 2011. Metareasoning as an integral part of commonsense and autocognitive reasoning. In M.T. Cox and A. Raja (eds.), Metareasoning: Thinking about Thinking, Ch. 17. MIT Press, Cambridge, MA, 267–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Morbini</author>
<author>Schubert</author>
</authors>
<title>Evaluation of Epilog: A reasoner for Episodic Logic.</title>
<date>2009</date>
<location>Commonsense’09,</location>
<note>Available at http://commonsensereasoning.org/2009/papers.html.</note>
<contexts>
<context position="12602" citStr="Morbini &amp; Schubert 2009" startWordPosition="1910" endWordPosition="1914">on Penfield Rd onto a truck? Also, the ability to use axiom schemas that involve quasi-quotes and syntactic closures allows lexical inferences based on knowledge about syntactic classes of lexical items (i.e., meaning postulates), as well as various forms of metareasoning, including reasoning about the system’s own knowledge and perceptions (Morbini &amp; Schubert 2011). Significantly, the expressiveness of EL/EPILOG does not prevent competitive performance on first-order commonsense knowledge bases (derived from Doug Lenat’s Cyc), especially as the number of KB formulas grows into the thousands (Morbini &amp; Schubert 2009). In the various inference tasks to which EPILOG was applied in the past, the LFs used for natural language sentences were based on presumed compositional rules, without the machinery to derive them automatically (e.g., Schubert &amp; Hwang 2000, Morbini &amp; Schubert 2011, Stratos et al. 2011). Starting in 2001, in developing our KNEXT system for knowledge extraction from text, we used broad-coverage compositional interpretion into EL for the first time, but since our goal was to obtain simple general “factoids&amp;quot;–such 57 as that a person may believe a proposition, people may wish to get rid of a dict</context>
</contexts>
<marker>Morbini, Schubert, 2009</marker>
<rawString>F. Morbini &amp; Schubert, 2009. Evaluation of Epilog: A reasoner for Episodic Logic. Commonsense’09, June 1-3, Toronto, Canada. Available at http://commonsensereasoning.org/2009/papers.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Poon</author>
</authors>
<title>Grounded unsupervised semantic parsing.</title>
<date>2013</date>
<booktitle>51st Ann. Meet. of the Assoc. for Computational Linguistics (ACL),</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="18006" citStr="Poon 2013" startWordPosition="2776" endWordPosition="2777">r expectations about typical object configurations and event sequences (especially ones directed towards agents’ goals) in the world and in dialogue. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge &amp; Mooney 2009, Artzi &amp; Zettlemoyer 2011, 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi &amp; Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis &amp; Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicc</context>
</contexts>
<marker>Poon, 2013</marker>
<rawString>H. Poon, 2013. Grounded unsupervised semantic parsing. 51st Ann. Meet. of the Assoc. for Computational Linguistics (ACL), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Purtee</author>
<author>L K Schubert</author>
</authors>
<title>TTT: A tree transduction language for syntactic and semantic processing.</title>
<date>2012</date>
<booktitle>EACL 2012 Workshop on Applications of Tree Automata Techniques in Natural Language Processing (ATANLP 2012),</booktitle>
<location>Avignon, France.</location>
<contexts>
<context position="3406" citStr="Purtee &amp; Schubert 2012" startWordPosition="501" endWordPosition="505">d even when parses are deemed correct according to “gold standard&amp;quot; annotated corpora, they often conflate semantically disparate word and phrase types. For example, prepositional phrases (PPs) functioning as predicates are not distinguished from ones functioning as adverbial modifiers; the roles of wh-words that form questions, relative clauses, or wh-nominals are not distinguished; and constituents parsed as SBARs (subordinate clauses) can be relative clauses, adverbials, question clauses, or clausal nominals. Our approach to these problems makes use of a new tree transduction language, TTT (Purtee &amp; Schubert 2012) that allows concise, modular, declarative representation of tree transductions. (As indicated below, TTT also plays a key role in logical form postprocessing.) While we cannot ex1ftp://ftp.cs.brown.edu/pub/nlparser/ 55 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 55–60, Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics pect to correct the majority of parse errors in general texts, we have found it easy to use TTT for correction of certain systematic errors in particular domains. In addition, we use TTT to subclassify many function words</context>
</contexts>
<marker>Purtee, Schubert, 2012</marker>
<rawString>A. Purtee and L. K. Schubert, 2012. TTT: A tree transduction language for syntactic and semantic processing. EACL 2012 Workshop on Applications of Tree Automata Techniques in Natural Language Processing (ATANLP 2012), Avignon, France.</rawString>
</citation>
<citation valid="false">
<authors>
<author>L K Schubert</author>
</authors>
<title>to appear. NLog-like inference and commonsense reasoning.</title>
<booktitle>Semantics for Textual Inference, CSLI Publications. Available at http://www.cs.rochester.edu/u/schubert/papers/nloglike-inference12.pdf.</booktitle>
<editor>In A. Zaenen, V. de Paiva and C. Condoravdi (eds.),</editor>
<marker>Schubert, </marker>
<rawString>L. K. Schubert, to appear. NLog-like inference and commonsense reasoning. In A. Zaenen, V. de Paiva and C. Condoravdi (eds.), Semantics for Textual Inference, CSLI Publications. Available at http://www.cs.rochester.edu/u/schubert/papers/nloglike-inference12.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L K Schubert</author>
</authors>
<title>The situations we talk about.</title>
<date>2000</date>
<booktitle>Logic-Based Artificial Intelligence, Kluwer, Dortrecht,</booktitle>
<pages>407--439</pages>
<editor>In J. Minker (ed.),</editor>
<contexts>
<context position="9167" citStr="Schubert 2000" startWordPosition="1372" endWordPosition="1373">, perform lambda and equality reductions, and also generate some immediate inferences (e.g., inferring that Mrs. Smith refers to a married woman). The following example, for the first sentence above, illustrates the kind of LF generated by our semantic parser (first in unscoped, indexical form, then the resulting set of scoped, deindexed, and canonicalized formulas). Note that EL uses predicate infixing at the sentence level, for readability; so for example we have (E0 BEFORE NOW0) rather than (BEFORE E0 NOW0). ‘**’ is the operator linking a sentential formula to the episode it characterizes (Schubert 2000). ADV-S is a typeshifting operator, L stands for λ, and PLUR is a predicate modifer that converts a predicate over individuals into a predicate over sets of individuals. For many months, no rain fell; Refined Treebank parse: (S (PP-FOR (IN for) (NP (CD many) (NNS months))) (|, ||,|) (NP (DT no) (NN rain)) (VP (VBD fell)) (|: ||;|)) Unscoped, indexical LF (keys :F, :P, etc., are dropped later): (:F (:F ADV-S (:P FOR.P (:Q MANY.DET (:F PLUR MONTH.N)))) (:I (:Q NO.DET RAIN.N) (:O PAST FALL.V))) Canonicalized LFs (without adverbial-modifier deindexing): (MONTHS0.SK (PLUR MONTH.N)), (MONTHS0.SK MAN</context>
</contexts>
<marker>Schubert, 2000</marker>
<rawString>L. K. Schubert, 2000. The situations we talk about. In J. Minker (ed.), Logic-Based Artificial Intelligence, Kluwer, Dortrecht, 407–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L K Schubert</author>
<author>C H Hwang</author>
</authors>
<title>Episodic Logic meets Little Red Riding Hood: A comprehensive, natural representation for language understanding.</title>
<date>2000</date>
<booktitle>Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language.</booktitle>
<pages>111--174</pages>
<editor>In L. Iwanska and S.C. Shapiro (eds.),</editor>
<publisher>MIT/AAAI Press,</publisher>
<location>Menlo Park, CA, and Cambridge, MA,</location>
<contexts>
<context position="1476" citStr="Schubert &amp; Hwang 2000" startWordPosition="212" endWordPosition="215">ures; • an event semantics that directly represents events with complex characterizations; • a scoping algorithm that heuristically scopes quantifiers, logical connectives, and tense; • a compositional approach to tense deindexing making use of tense trees; and • the use of an inference engine, EPILOG, that supports input-driven and goal-driven inference in EL, in a style similar to (but more general than) Natural Logic. We have applied this framework to general knowledge acquisition from text corpora and the web (though with tense meaning and many other semantic details stripped away) (e.g., Schubert &amp; Hwang 2000, Van Durme &amp; Schubert 2008), and more recently to caption interpretation for family photos, enabling alignment of names and other descriptors with human faces in the photos, and to interpreting sentences in simple first-reader stories. Ongoing projects are aimed at full interpretation of lexical glosses and other sources of explicitly expressed general knowledge. We now elaborate some of the themes in the preceding overview, concluding with comments on related work and important remaining challenges. 2. Refinement of Treebank parses using TTT We generate initial logical forms by compositional</context>
<context position="5781" citStr="Schubert &amp; Hwang 2000" startWordPosition="853" endWordPosition="856">ts of an expressive representation can also be implemented within a more comprehensive inference framework, without forfeiting the advantages of expressiveness. Moreover, recent work in Natural Logic, which uses phrase-structured NL directly for inference, indicates that the richness of language is no obstacle to rapid inference of many obvious lexical entailments (e.g., MacCartney &amp; Manning 2009). Thus our target representation, EL, taking its cue from Montague allows directly for the kinds of quantification, intensionality, modification, and reification found in all natural languages (e.g., Schubert &amp; Hwang 2000, Schubert, to appear). In addition, EL associates episodes (events, situations, processes) directly with arbitrarily complex sentences, rather than just with atomic predications, as in Davidsonian event semantics. For example, the initial sentence in each of the following pairs is interpreted as directly characterizing an episode, which then serves as antecedent for a pronoun or definite: For many months, no rain fell; this totally dried out the topsoil. Each superpower menaced the other with its nuclear arsenal; this situation persisted for decades. Also, since NL allows for discussion of li</context>
<context position="12843" citStr="Schubert &amp; Hwang 2000" startWordPosition="1952" endWordPosition="1955">rious forms of metareasoning, including reasoning about the system’s own knowledge and perceptions (Morbini &amp; Schubert 2011). Significantly, the expressiveness of EL/EPILOG does not prevent competitive performance on first-order commonsense knowledge bases (derived from Doug Lenat’s Cyc), especially as the number of KB formulas grows into the thousands (Morbini &amp; Schubert 2009). In the various inference tasks to which EPILOG was applied in the past, the LFs used for natural language sentences were based on presumed compositional rules, without the machinery to derive them automatically (e.g., Schubert &amp; Hwang 2000, Morbini &amp; Schubert 2011, Stratos et al. 2011). Starting in 2001, in developing our KNEXT system for knowledge extraction from text, we used broad-coverage compositional interpretion into EL for the first time, but since our goal was to obtain simple general “factoids&amp;quot;–such 57 as that a person may believe a proposition, people may wish to get rid of a dictator, clothes can be washed, etc. (expressed logically)–our interpretive rules ignored tense, many modifiers, and other subtleties (e.g., Van Durme &amp; Schubert 2008). Factoids like the ones mentioned are unconditional and as such not directly</context>
</contexts>
<marker>Schubert, Hwang, 2000</marker>
<rawString>L. K. Schubert and C. H. Hwang, 2000. Episodic Logic meets Little Red Riding Hood: A comprehensive, natural representation for language understanding. In L. Iwanska and S.C. Shapiro (eds.), Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language. MIT/AAAI Press, Menlo Park, CA, and Cambridge, MA, 111–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L K Schubert</author>
<author>F J Pelletier</author>
</authors>
<title>From English to logic: Context-free computation of ‘conventional’ logical translations.</title>
<date>1982</date>
<journal>Am. J. of Computational Linguistics</journal>
<booktitle>Readings in Natural Language Processing,</booktitle>
<volume>8</volume>
<pages>27--44</pages>
<editor>B.J. Grosz, K. Sparck Jones, and B.L. Webber (eds.),</editor>
<publisher>Morgan Kaufmann,</publisher>
<location>Los Altos, CA,</location>
<note>Reprinted in</note>
<contexts>
<context position="6968" citStr="Schubert &amp; Pelletier 1982" startWordPosition="1030" endWordPosition="1033">o, since NL allows for discussion of linguistic and other symbolic entities, so does EL, via quasi-quotation and substitutional quantification (closures). These can also express axiom schemas, and autocognitive reasoning (see further comments in Section 5). 4. Comprehensive scoping and tense deindexing Though EL is Montague-inspired, one difference from a Montague-style intensional logic is that we treat noun phrase (NP) interpretations as unscoped elements, rather than second-order predicates. These elements are heuristically scoped to the sentence level in LF postprocessing, as proposed in (Schubert &amp; Pelletier 1982). The latter proposal also covered scoping of logical connectives, which exhibit the same scope ambiguities as quantifiers. Our current heuristic scoping algorithm handles these phenomena as well as tense scope, allowing for such factors as syntactic ordering, island constraints, and differences in widescoping tendencies among different operators. Episodes characterized by sentences remain implicit until application of a “deindexing&amp;quot; algorithm. This algorithm makes use of a contextual element called a tense tree which is built and traversed in accordance with simple recursive rules applied to </context>
</contexts>
<marker>Schubert, Pelletier, 1982</marker>
<rawString>L. K. Schubert and F.J. Pelletier, 1982. From English to logic: Context-free computation of ‘conventional’ logical translations. Am. J. of Computational Linguistics 8, 27–44. Reprinted in B.J. Grosz, K. Sparck Jones, and B.L. Webber (eds.), Readings in Natural Language Processing, Morgan Kaufmann, Los Altos, CA, 1986, 293-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Tellex</author>
<author>T Kollar</author>
<author>S Dickerson</author>
<author>M R Walter</author>
<author>A G Banerjee</author>
<author>S Teller</author>
<author>N Roy</author>
</authors>
<title>Understanding natural language commands for robotic navigation and mobile manipulation.</title>
<date>2011</date>
<booktitle>Nat. Conf. on Artificial Intelligence (AAAI 2011),</booktitle>
<location>San Francisco, CA.</location>
<marker>Tellex, Kollar, Dickerson, Walter, Banerjee, Teller, Roy, 2011</marker>
<rawString>S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G. Banerjee, S. Teller, and N. Roy, 2011. Understanding natural language commands for robotic navigation and mobile manipulation. Nat. Conf. on Artificial Intelligence (AAAI 2011), San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Van Durme</author>
<author>L K Schubert</author>
</authors>
<title>Open knowledge extraction through compositional language processing.</title>
<date>2008</date>
<booktitle>Symposium on Semantics in Systems for Text Processing (STEP 2008),</booktitle>
<location>Venice, Italy.</location>
<marker>Van Durme, Schubert, 2008</marker>
<rawString>B. Van Durme and L. K. Schubert, 2008. Open knowledge extraction through compositional language processing. Symposium on Semantics in Systems for Text Processing (STEP 2008), Venice, Italy.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000623">
<title confidence="0.997439">
Application of Prize based on Sentence Length in Chunk-based
Automatic Evaluation of Machine Translation
</title>
<author confidence="0.992992">
Hiroshi Echizen’ya
</author>
<affiliation confidence="0.974281">
Hokkai-Gakuen University
</affiliation>
<address confidence="0.7349115">
S26-Jo, W11-Chome, Chuo-ku,
Sapporo 064-0926 Japan
</address>
<email confidence="0.997443">
echi@lst.hokkai-s-u.ac.jp
</email>
<author confidence="0.996093">
Kenji Araki
</author>
<affiliation confidence="0.998805">
Hokkaido University
</affiliation>
<address confidence="0.97242">
N 14-Jo, W 9-Chome, Kita-ku,
Sapporo 060-0814 Japan
</address>
<email confidence="0.999362">
araki@ist.hokudai.ac.jp
</email>
<sectionHeader confidence="0.983209" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999195625">
As described in this paper, we pro-
pose a new automatic evaluation met-
ric for machine translation. Our met-
ric is based on chunking between the
reference and candidate translation.
Moreover, we apply a prize based on
sentence-length to the metric, dissim-
ilar from penalties in BLEU or NIST.
We designate this metric as Automatic
Evaluation of Machine Translation in
which the Prize is Applied to a Chunk-
based metric (APAC). Through meta-
evaluation experiments and compari-
son with several metrics, we confirmed
that our metric shows stable correla-
tion with human judgment.
</bodyText>
<sectionHeader confidence="0.996304" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999215352941176">
In the field of machine translation, various
automatic evaluation metrics have been pro-
posed. Among them, chunk-based metrics
such as METEOR(A. Lavie and A. Agarwal,
2007), ROUGE-L(Lin and Och, 2004), and
IMPACT(H. Echizen-ya and K. Araki, 2007)
are effective. In general, BLEU(K. Papineni et
al., 2002), NIST(NIST, 2002), and RIBES(H.
Isozaki et al., 2010) use a penalty for calcula-
tion of scores because the high score is often
given extremely when the candidate transla-
tion is short. Therefore, the penalty is effective
to obtain high correlation with human judg-
ment. On the other hand, almost all chunk-
based metrics use the F-measure based on a
precision by candidate translation and a re-
call by reference. Moreover, they assign a
</bodyText>
<author confidence="0.922019">
Eduard Hovy
</author>
<affiliation confidence="0.957092">
Carnegie Mellon University
</affiliation>
<address confidence="0.6418915">
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
</address>
<email confidence="0.995623">
hovy@cmu.edu
</email>
<bodyText confidence="0.999923566666667">
penalty for the difference of chunk order be-
tween the candidate translation and the refer-
ence, not the penalty for the difference of sen-
tence length. Nevertheless, it is also impor-
tant for chunk-based metrics to examine the
sentence length. In chunk-based metrics, each
word’s weight depends on the sentence length.
For example, the weight of each word is 0.2
(=1/5) when the number of words in a sen-
tence is 5; it is 0.1 (=1/10) when the number
of words in a sentence is 10. Therefore, the
weight of the non-matched word in the short
sentence is large.
To resolve this problem, it is effective for
short sentences to give a prize based on the
sentence length in the chunk-based metrics.
Therefore, we propose a new metric using a
prize based on the sentence length. We des-
ignate this metric as Automatic Evaluation
of Machine Translation in which the Prize is
Applied to a Chunk-based metric (APAC). In
our metric, the weight of a non-matched word
becomes small for the short sentence by award-
ing of the prize. It is almost identical to that
for a long sentence by awarding of the prize.
Therefore, our metric does not depend heavily
on sentence length because the weight of non-
matched words is constantly small. We con-
firmed the effectiveness of APAC using meta-
evaluation experiments.
</bodyText>
<sectionHeader confidence="0.420247" genericHeader="introduction">
2 Score calculation in APAC
</sectionHeader>
<bodyText confidence="0.99387125">
The APAC score is calculated in two phases.
In the first phase, the chunk sequence is
determined between a candidate translation
and the reference. The chunk sequence
</bodyText>
<page confidence="0.981716">
381
</page>
<bodyText confidence="0.963198028571429">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 381–386,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
is determined using the Longest Common
Subsequence (LCS). Generally, several chunk
sequences are obtained using LCS. In that
case, APAC determines only one chunk se-
quence using the number of words in each
chunk and the position of each chunk.
For example, in between the candidate
translation “In this case, the system power
supply is accessory battery 86.” and “In this
case, the system power supply is the accessory
power supply battery 86.”, the chunk sequence
is “in this case, the system power supply is”,
“accessory” and “battery 86.”, and the chunk
sequence is ony one in these sentences. Only
one chunk sequence is determined using the
number of words in each chunk and the po-
sition of each chunk when several chunk se-
quences are obtained.
The second phase is calculation of the score
based on the determined chunk sequence. The
Ch score in Eq. (3) is calculated using the de-
termined chunk sequence. In Eq. (3), ch de-
notes each chunk and ch num represents the
number of chunks. Moreover, length(ch) is the
word number of each chunk. β is the weight
parameter for the length of each chunk. For
example, in between the candidate translation
“In this case, the system power supply is ac-
cessory battery 86.” and “In this case, the
system power supply is the accessory power
supply battery 86.”, ch num is 3 (“in this
case, the system power supply is”, “accessory”
and “battery 86.”). Therefore, Ch score is 91
</bodyText>
<equation confidence="0.938819857142857">
(=92.0 + 12.0 + 32.0) when β is 2.0.
log(m) + 1
1 (4)
1 (5)
log(n) + 1
APAC score = (1 + γ2)RP (6)
R + γ2P
</equation>
<bodyText confidence="0.99983725">
The P and R in Eqs. (1) and (2) re-
spectively denote precision by candidate
translation and recall by reference. These
are calculated using the Ch score obtained
using Eq. (3). Therein, m and n respectively
represent the word numbers of the candidate
translation and the reference. Moreover,
the chunk sequence determination process is
repeated recursively to all common words.
The number of determination processes of
the chunk sequence is high when the word
order of the candidate translation differs
from that of the reference. The RN is the
number of determination processes of the
chunk sequence. Here, α is the parameter for
the chunk order. It is less than 1.0. The value
of the Ch score is small when the chunk order
between the candidate translation and refer-
ences differs because the value of length(ch)
in each chunk becomes small. For example,
in between the candidate translation “In this
case, the system power supply is accessory
battery 86.” and “In this case, the system
power supply is the accessory power sup-
</bodyText>
<equation confidence="0.964303125">
1
ply battery 86. ,mβ
„ CY-i=0−1(αixCh score) β
Prize m =
Prize n =
1 +0.5 x Prize m /2.0 (1)
R = nβ
(RN−1 i
EZ=o (α x Ch score) β
1
1 +0.5 x Prize n /2.0 (2)
�Ch score = length(ch)β (3)
chEch num
is 0.773 _ Ei=o(0.10x 91)
(_ ✓ 91 169— 132.0 )
C =_&apos; (αi xCh score )1 β
</equation>
<bodyText confidence="0.936168866666667">
and i-0 nβ J is 0.596
(=✓91 _ �z-o (0.10 x91)
256— 162.0 ) when α and β
respectively stand for 0.1 and 2.0. The
value of RN is 1 because there is no more
matching words after the determined chunks
(“in this case, the system power supply is”,
“accessory” and “battery 86.”) are removed
from the candidate translation “In this case,
the system power supply is accessory battery
86.” and “In this case, the system power
supply is the accessory power supply battery
86.”.
Moreover, Prize m and Prize n in Eqs. (1)
and (2) are calculated respectively using Eqs.
</bodyText>
<equation confidence="0.98734525">
P = \ mβ
i
ERN−1
αi
=0 (
x Ch score)
1
β
</equation>
<page confidence="0.971865">
382
</page>
<bodyText confidence="0.991817142857143">
(4) and (5). Each is less than 1.0. For ex-
ample, in the candidate translation “In this
case, the system power supply is accessory
battery 86.” and “In this case, the system
power supply is the accessory power supply
battery 86.”, Prize m and Prize n respec-
tively stand for 0.473 (= 1
</bodyText>
<equation confidence="0.990029">
1.114+1= 1
log(13)+1) and
0.454 (= 1
1.204+1= 1
</equation>
<bodyText confidence="0.985870235294118">
log(16)+1). These values be-
come large in the short sentences. They be-
come small in the long sentences. Therefore,
the weight of each non-matched word is small
in the short sentences. It is kept small in
the long sentences. Finally, the score is cal-
culated using Eq. (6). This equation shows
the f-measure based on P and R. In Eq. (6),
γ is determined as P/R(C. J. V. Rijsbergen,
1979). The APAC score is between 0.0 and
1.0. For example, in the candidate transla-
tion “In this case, the system power supply is
accessory battery 86.” and “In this case, the
system power supply is the accessory power
supply battery 86.”, P and R respectively
stand for 0.505 (=0.773+0.5×0.473
2.0 ) and 0.412
</bodyText>
<equation confidence="0.6489416">
(=0.596+0.5×0.454
2.0 ). Therefore, APAC score is
0.445 (=0.521 (1+1.503)×0.412×0.505
1 0
.171= .412+1.503×0.505 ) and γ is
</equation>
<bodyText confidence="0.999938">
by human judgments at the system level and
the segment level, respectively. Spearman’s
rank correlation coefficient is used at the sys-
tem level. The Kendall tau rank correlation
coefficient is used in the segment level.
Moreover, we used BLEU (ver. 13a),
NIST (ver. 13a), METEOR (ver. 1.4), and
APAC with no prize (APAC no p) as the
automatic evaluation metrics for comparison
with APAC as shown in Eqs. (4) and (5).
</bodyText>
<equation confidence="0.83222">
1
C =_&apos; (αi ×Ch score)1
and z-0 J
mβ
spectively in Eqs. (1) and (2).
</equation>
<subsectionHeader confidence="0.986156">
3.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.99991404">
Tables 1 and 2 respectively present Spear-
man’s rank correlation coefficients of system-
level and Kendall tau rank correlation coef-
ficients of segment-level in WMT2012 data.
Tables 3 and 4 respectively show Spearman’s
rank correlation coefficients of the system-level
and Kendall tau rank correlation coefficients of
segment-level in WMT2013 data. Moreover,
Tables 5 and 6 respectively present Spear-
man’s rank correlation coefficients of system-
level and Kendall tau rank correlation coeffi-
cients of segment-level in NTCIR-7 data. Ta-
bles 7 and 8 respectively show Spearman’s
rank correlation coefficients of system-level
and Kendall tau rank correlation coefficients
of the segment level in NTCIR-9 data.
In APAC, 0.1 and 1.2 were used as the values
of parameters α and Q by the preliminarily ex-
perimentally obtained results. In Tables 1–8,
“Rank” denotes the ranking based on “Avg.”
The value of “()” denotes the number of MT
systems in Tables 1, 3, 5, and 7. The value of
“()” represents the number of sentence pairs
in Tables 2, 4, 6, and 8. These values depend
on the data.
</bodyText>
<subsectionHeader confidence="0.939685">
3.3 Discussion
</subsectionHeader>
<bodyText confidence="0.999922142857143">
The results presented in Tables 1–8 indicate
that APAC can obtain the most stable corre-
lation coefficients among some metrics. The
ranking of APAC is No. 1 through NTCIR
data in Tables 5–8. In WMT data of Ta-
bles 1–4, the ranking of APAC is the lowest
except for Table 3. However, the difference
</bodyText>
<figure confidence="0.781375666666667">
1.226 (=0.505
0.412)
3 Experiments
</figure>
<subsectionHeader confidence="0.728264">
3.1 Experimental Procedure
</subsectionHeader>
<bodyText confidence="0.996266095238095">
Meta-evaluation experiments are performed
using WMT2012(C. Callison-Burch et al.,
2012) data and WMT2013(O. Bojar et al.,
2013) data, and NTCIR-7(A. Fujii et al., 2008)
data and NTCIR-9(A. Goto et al., 2011) data.
All sentences by NTCIR data are English
patent sentences obtained through Japanese-
to-English translation. The number of refer-
ences is 1. In NTICR-7 data, the average
value in the evaluation results of three hu-
man judgments is used as the scores of 1–
5 from the perspective of adequacy and flu-
ency. In NTCIR-9 data, the evaluation results
of one human judgment is used as the scores
of 1–5 from the view of adequacy and accep-
tance. For this meta-evaluation, we used only
English and Japanese candidate translations
because we can evaluate them in comparison
with other languages correctly.
We calculated the correlation between the
scores by automatic evaluation and the scores
</bodyText>
<figure confidence="0.7884478">
=_&apos; (αi ×Ch score )1 β
In APAC no p, z-0 mβ) as P
1
β
as R are used re-
</figure>
<page confidence="0.993626">
383
</page>
<table confidence="0.9998245">
cs-en(6) de-en(16) es-en(12) fr-en(15) Avg. Rank
APAC 0.886 0.650 0.958 0.811 0.826 5
APAC no p 0.886 0.676 0.958 0.807 0.832 3
METEOR 0.943 0.841 0.979 0.818 0.895 1
BLEU 0.886 0.674 0.958 0.796 0.828 4
NIST 0.943 0.700 0.944 0.779 0.841 2
</table>
<tableCaption confidence="0.99459">
Table 1: Spearman’s rank correlation coefficient of system-level in WMT2012 data.
</tableCaption>
<table confidence="0.9999045">
cs-en(11,155) de-en(12,042) es-en(9,880) fr-en(11,682) Avg. Rank
APAC 0.185 0.204 0.209 0.226 0.206 3
APAC no p 0.189 0.207 0.208 0.226 0.207 2
METEOR 0.223 0.279 0.248 0.243 0.248 1
</table>
<tableCaption confidence="0.999535">
Table 2: Kendall tau rank correlation coefficient of the segment level in WMT2012 data.
</tableCaption>
<bodyText confidence="0.999966387096774">
between the ranking of METEOR, which is
the highest, and that of APAC is not larger
in WMT data. The correlation coefficients of
APAC in NTCIR data of Tables 5–8 are higher
than those of METEOR. In Tables 5 and 6,
underlining in APAC signifies that the differ-
ences between correlation coefficients obtained
using APAC and METEOR are statistically
significant at the 5% significance level. In Ta-
ble 7, the correlation coefficients of METEOR,
BLEU, and NIST are extremely low. Only one
human judgment was used in NTCIR-9 data.
As a result, APAC is fundamentally effective
for various languages independent of the differ-
ences in the grammatical structures between
languages: these experimentally obtained re-
sults indicate that APAC is the most stable
metric.
Moreover, in APAC, the correlation coeffi-
cients of the segment level in NTCIR data were
increased using the prize of Eqs. (4) and (5).
In WMT data, the correlation coefficients are
almost identical using the prize. Therefore,
use of the prize was fundamentally effective
at the segment level. The evaluation quality
of segment level is generally very low in the
automatic evaluation metrics. Therefore, it is
extremely important to improve the correla-
tion coefficient of segment level. Application
of the prize is effective to improve the evalua-
tion quality of the segment level.
</bodyText>
<sectionHeader confidence="0.997706" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99994025">
As described in this paper, we proposed a new
chunk-based automatic evaluation metric us-
ing the prize based on the sentence length.
The experimentally obtained results indicate
that APAC is the most stable metric.
We will improve APAC to obtain higher
correlation coefficients in future studies.
Particularly, we will strive to improve
the correlation coefficients at the segment
level. The APAC software will be re-
leased by http://www.lst.hokkai-s-u.ac.
jp/~echi/automatic_evaluation_mt.html.
</bodyText>
<sectionHeader confidence="0.998549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998322">
This work was done as research under the
AAMT/JAPIO Special Interest Group on
Patent Translation. The Japan Patent In-
formation Organization (JAPIO) and the Na-
tional Institute of Information (NII) provided
corpora used in this work. The author grate-
fully acknowledges support from JAPIO and
NII.
</bodyText>
<sectionHeader confidence="0.999053" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999020307692308">
O. Bojar, C. Buck, C. Callison-Burch, C. Feder-
mann, B. Haddow, P. Koehn, C. Monz, M. Post,
R. Sortcut and L. Specia. 2013. Findings of the
2013 Workshop on Statistical Machine Transla-
tion. Proceedings of the Eighth Workshop on
Statistical Machine Translation. pp.1–44.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Sortcut and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Transla-
tion. Proceedings of the Seventh Workshop on
Statistical Machine Translation. pp.10–51.
H. Echizen-ya and K. Araki. 2007. Automatic
Evaluation of Machine Translation based on
</reference>
<page confidence="0.995855">
384
</page>
<table confidence="0.999848666666667">
cs-en(11) de-en(17) es-en(12) fr-en(13) ru-en(19) Avg. Rank
APAC 0.900 0.904 0.916 0.934 0.709 0.873 3
APAC no p 0.909 0.909 0.937 0.934 0.721 0.882 2
METEOR 0.982 0.946 0.923 0.967 0.889 0.941 1
BLEU 0.945 0.897 0.853 0.951 0.614 0.852 4
NIST 0.900 0.828 0.804 0.786 0.465 0.757 5
</table>
<tableCaption confidence="0.989757">
Table 3: Spearman’s rank correlation coefficient of the system level in WMT2013 data.
</tableCaption>
<table confidence="0.9998388">
Metrics cs-en de-en es-en fr-en ru-en Avg. Rank
(85,469) (128,668) (67,832) (80,741) (151,422)
APAC 0.144 0.163 0.169 0.139 0.121 0.147 3
APAC no p 0.148 0.167 0.176 0.142 0.123 0.151 2
METEOR 0.222 0.236 0.241 0.194 0.226 0.224 1
</table>
<tableCaption confidence="0.999004">
Table 4: Kendall tau rank correlation coefficient of the segment level in WMT2013 data.
</tableCaption>
<reference confidence="0.996608093023256">
Recursive Acquisition of an Intuitive Common
Parts Continuum. Proceedings of the Eleventh
Machine Translation Summit. pp.151–158.
A. Fujii, M. Utiyama, M. Yamamoto and T. Ut-
suro. 2008. Overview of the Patent Translation
Task at the NTCIR-7 Workshop. Proceedings
of the Seventh NTCIR Workshop Meeting on
Evaluation of Information Access Technologies:
Information Retrieval, Question Answering and
Cross-lingual Information Access. pp.389–400.
I. Goto, B. Lu, K. P. Chow, E. Sumita and B. K.
Tsou. 2011. Overview of the Patent Translation
Task at the NTCIR-9 Workshop. Proceedings of
the Ninth NTCIR Workshop Meeting. pp.559–
578.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh and
H. Tsukada. 2010. Automatic Evaluation of
Translation Quality for Distant Language Pairs.
Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing.
pp.944–952.
A. Lavie and A. Agarwal. 2007. Meteor: An Auto-
matic Metric for MT Evaluation with High Lev-
els of Correlation with Human Judgments. Pro-
ceedings of the Second Workshop on Statistical
Machine Translation.
Chin-Yew Lin and F. J. Och. 2004. Automatic
Evaluation of Machine Translation Quality Us-
ing the Longest Common Subsequence and Skip-
Bigram Statistics. In Proc. of ACL’04, 606–613.
NIST. 2002. Automatic Evaluation
of Machine Translation Quality Us-
ing N-gram Co-Occurrence Statistics.
http://www.nist.gov/speech/tests/mt/doc/
ngram-study.pdf.
K. Papineni, S. Roukos, T. Ward, and Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL). pp.311–
318.
C. J. Van Rijsbergen. 1979. Information Retrieval
(2nd ed.), Butterworths.
</reference>
<page confidence="0.998116">
385
</page>
<table confidence="0.999545833333333">
Adequacy(15) Fluency(15) Avg. Rank
APAC 0.872 0.805 0.839 1
APAC no p 0.872 0.805 0.839 1
METEOR 0.424 0.380 0.402 5
BLEU 0.582 0.586 0.584 3
NIST 0.578 0.568 0.573 4
</table>
<tableCaption confidence="0.993342">
Table 5: Spearman’s rank correlation coefficient of the system level in NTCIR-7 data.
</tableCaption>
<table confidence="0.99985475">
Adequacy (1,500) Fluency (1,500) Avg. Rank
APAC 0.494 0.489 0.491 1
APAC no p 0.482 0.476 0.479 2
METEOR 0.366 0.383 0.375 3
</table>
<tableCaption confidence="0.977824">
Table 6: Kendall tau rank correlation coefficient of the segment level in NTCIR-7 data.
</tableCaption>
<table confidence="0.981358333333333">
Adequacy (19) Acceptance (14) Avg. Rank
APAC 0.182 0.298 0.240 1
APAC no p 0.182 0.298 0.240 1
METEOR -0.081 0.015 -0.033 4
BLEU -0.123 0.059 -0.032 3
NIST -0.344 -0.275 -0.309 5
</table>
<tableCaption confidence="0.992042">
Table 7: Spearman’s rank correlation coefficient of the system level in NTCIR-9 data.
</tableCaption>
<table confidence="0.99941225">
Adequacy (5,700) Acceptance (5,700) Avg. Rank
APAC 0.250 0.261 0.256 1
APAC no p 0.242 0.250 0.246 2
METEOR 0.167 0.217 0.192 3
</table>
<tableCaption confidence="0.999433">
Table 8: Kendall tau rank correlation coefficient of segment-level in NTCIR-9 data.
</tableCaption>
<page confidence="0.99777">
386
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.037105">
<title confidence="0.953581666666667">Application of Prize based on Sentence Length in Chunk-based Automatic Evaluation of Machine Translation Hiroshi</title>
<author confidence="0.656727">Hokkai-Gakuen</author>
<email confidence="0.347345">S26-Jo,W11-Chome,</email>
<address confidence="0.887896">Sapporo 064-0926</address>
<email confidence="0.956875">echi@lst.hokkai-s-u.ac.jp</email>
<note confidence="0.412531">Kenji Hokkaido N 14-Jo, W 9-Chome,</note>
<address confidence="0.746013">Sapporo 060-0814</address>
<email confidence="0.928311">araki@ist.hokudai.ac.jp</email>
<abstract confidence="0.996687411764706">As described in this paper, we propose a new automatic evaluation metric for machine translation. Our metric is based on chunking between the reference and candidate translation. Moreover, we apply a prize based on sentence-length to the metric, dissimilar from penalties in BLEU or NIST. designate this metric as Evaluation of Machine Translation in the is to a based metric (APAC). Through metaevaluation experiments and comparison with several metrics, we confirmed that our metric shows stable correlation with human judgment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>C Buck</author>
<author>C Callison-Burch</author>
<author>C Federmann</author>
<author>B Haddow</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Sortcut</author>
<author>L Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. Proceedings of the Eighth Workshop on Statistical Machine Translation.</booktitle>
<pages>1--44</pages>
<contexts>
<context position="10070" citStr="Bojar et al., 2013" startWordPosition="1720" endWordPosition="1723">7. The value of “()” represents the number of sentence pairs in Tables 2, 4, 6, and 8. These values depend on the data. 3.3 Discussion The results presented in Tables 1–8 indicate that APAC can obtain the most stable correlation coefficients among some metrics. The ranking of APAC is No. 1 through NTCIR data in Tables 5–8. In WMT data of Tables 1–4, the ranking of APAC is the lowest except for Table 3. However, the difference 1.226 (=0.505 0.412) 3 Experiments 3.1 Experimental Procedure Meta-evaluation experiments are performed using WMT2012(C. Callison-Burch et al., 2012) data and WMT2013(O. Bojar et al., 2013) data, and NTCIR-7(A. Fujii et al., 2008) data and NTCIR-9(A. Goto et al., 2011) data. All sentences by NTCIR data are English patent sentences obtained through Japaneseto-English translation. The number of references is 1. In NTICR-7 data, the average value in the evaluation results of three human judgments is used as the scores of 1– 5 from the perspective of adequacy and fluency. In NTCIR-9 data, the evaluation results of one human judgment is used as the scores of 1–5 from the view of adequacy and acceptance. For this meta-evaluation, we used only English and Japanese candidate translation</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Sortcut, Specia, 2013</marker>
<rawString>O. Bojar, C. Buck, C. Callison-Burch, C. Federmann, B. Haddow, P. Koehn, C. Monz, M. Post, R. Sortcut and L. Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. Proceedings of the Eighth Workshop on Statistical Machine Translation. pp.1–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Sortcut</author>
<author>L Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. Proceedings of the Seventh Workshop on Statistical Machine Translation.</booktitle>
<pages>10--51</pages>
<contexts>
<context position="10030" citStr="Callison-Burch et al., 2012" startWordPosition="1713" endWordPosition="1716"> the number of MT systems in Tables 1, 3, 5, and 7. The value of “()” represents the number of sentence pairs in Tables 2, 4, 6, and 8. These values depend on the data. 3.3 Discussion The results presented in Tables 1–8 indicate that APAC can obtain the most stable correlation coefficients among some metrics. The ranking of APAC is No. 1 through NTCIR data in Tables 5–8. In WMT data of Tables 1–4, the ranking of APAC is the lowest except for Table 3. However, the difference 1.226 (=0.505 0.412) 3 Experiments 3.1 Experimental Procedure Meta-evaluation experiments are performed using WMT2012(C. Callison-Burch et al., 2012) data and WMT2013(O. Bojar et al., 2013) data, and NTCIR-7(A. Fujii et al., 2008) data and NTCIR-9(A. Goto et al., 2011) data. All sentences by NTCIR data are English patent sentences obtained through Japaneseto-English translation. The number of references is 1. In NTICR-7 data, the average value in the evaluation results of three human judgments is used as the scores of 1– 5 from the perspective of adequacy and fluency. In NTCIR-9 data, the evaluation results of one human judgment is used as the scores of 1–5 from the view of adequacy and acceptance. For this meta-evaluation, we used only En</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Sortcut, Specia, 2012</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sortcut and L. Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. Proceedings of the Seventh Workshop on Statistical Machine Translation. pp.10–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Echizen-ya</author>
<author>K Araki</author>
</authors>
<title>Automatic Evaluation of Machine Translation based on Recursive Acquisition of an Intuitive Common Parts Continuum.</title>
<date>2007</date>
<booktitle>Proceedings of the Eleventh Machine Translation Summit. pp.151–158.</booktitle>
<marker>Echizen-ya, Araki, 2007</marker>
<rawString>H. Echizen-ya and K. Araki. 2007. Automatic Evaluation of Machine Translation based on Recursive Acquisition of an Intuitive Common Parts Continuum. Proceedings of the Eleventh Machine Translation Summit. pp.151–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fujii</author>
<author>M Utiyama</author>
<author>M Yamamoto</author>
<author>T Utsuro</author>
</authors>
<title>Overview of the Patent Translation Task at the NTCIR-7 Workshop.</title>
<date>2008</date>
<booktitle>Proceedings of the Seventh NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-lingual Information Access.</booktitle>
<pages>389--400</pages>
<contexts>
<context position="10111" citStr="Fujii et al., 2008" startWordPosition="1727" endWordPosition="1730">r of sentence pairs in Tables 2, 4, 6, and 8. These values depend on the data. 3.3 Discussion The results presented in Tables 1–8 indicate that APAC can obtain the most stable correlation coefficients among some metrics. The ranking of APAC is No. 1 through NTCIR data in Tables 5–8. In WMT data of Tables 1–4, the ranking of APAC is the lowest except for Table 3. However, the difference 1.226 (=0.505 0.412) 3 Experiments 3.1 Experimental Procedure Meta-evaluation experiments are performed using WMT2012(C. Callison-Burch et al., 2012) data and WMT2013(O. Bojar et al., 2013) data, and NTCIR-7(A. Fujii et al., 2008) data and NTCIR-9(A. Goto et al., 2011) data. All sentences by NTCIR data are English patent sentences obtained through Japaneseto-English translation. The number of references is 1. In NTICR-7 data, the average value in the evaluation results of three human judgments is used as the scores of 1– 5 from the perspective of adequacy and fluency. In NTCIR-9 data, the evaluation results of one human judgment is used as the scores of 1–5 from the view of adequacy and acceptance. For this meta-evaluation, we used only English and Japanese candidate translations because we can evaluate them in compari</context>
</contexts>
<marker>Fujii, Utiyama, Yamamoto, Utsuro, 2008</marker>
<rawString>A. Fujii, M. Utiyama, M. Yamamoto and T. Utsuro. 2008. Overview of the Patent Translation Task at the NTCIR-7 Workshop. Proceedings of the Seventh NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-lingual Information Access. pp.389–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Goto</author>
<author>B Lu</author>
<author>K P Chow</author>
<author>E Sumita</author>
<author>B K Tsou</author>
</authors>
<date>2011</date>
<booktitle>Overview of the Patent Translation Task at the NTCIR-9 Workshop. Proceedings of the Ninth NTCIR Workshop Meeting. pp.559–</booktitle>
<pages>578</pages>
<contexts>
<context position="10150" citStr="Goto et al., 2011" startWordPosition="1734" endWordPosition="1737">nd 8. These values depend on the data. 3.3 Discussion The results presented in Tables 1–8 indicate that APAC can obtain the most stable correlation coefficients among some metrics. The ranking of APAC is No. 1 through NTCIR data in Tables 5–8. In WMT data of Tables 1–4, the ranking of APAC is the lowest except for Table 3. However, the difference 1.226 (=0.505 0.412) 3 Experiments 3.1 Experimental Procedure Meta-evaluation experiments are performed using WMT2012(C. Callison-Burch et al., 2012) data and WMT2013(O. Bojar et al., 2013) data, and NTCIR-7(A. Fujii et al., 2008) data and NTCIR-9(A. Goto et al., 2011) data. All sentences by NTCIR data are English patent sentences obtained through Japaneseto-English translation. The number of references is 1. In NTICR-7 data, the average value in the evaluation results of three human judgments is used as the scores of 1– 5 from the perspective of adequacy and fluency. In NTCIR-9 data, the evaluation results of one human judgment is used as the scores of 1–5 from the view of adequacy and acceptance. For this meta-evaluation, we used only English and Japanese candidate translations because we can evaluate them in comparison with other languages correctly. We </context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>I. Goto, B. Lu, K. P. Chow, E. Sumita and B. K. Tsou. 2011. Overview of the Patent Translation Task at the NTCIR-9 Workshop. Proceedings of the Ninth NTCIR Workshop Meeting. pp.559– 578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Isozaki</author>
<author>T Hirao</author>
<author>K Duh</author>
<author>K Sudoh</author>
<author>H Tsukada</author>
</authors>
<title>Automatic Evaluation of Translation Quality for Distant Language Pairs.</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<pages>944--952</pages>
<contexts>
<context position="1284" citStr="Isozaki et al., 2010" startWordPosition="184" endWordPosition="187">omatic Evaluation of Machine Translation in which the Prize is Applied to a Chunkbased metric (APAC). Through metaevaluation experiments and comparison with several metrics, we confirmed that our metric shows stable correlation with human judgment. 1 Introduction In the field of machine translation, various automatic evaluation metrics have been proposed. Among them, chunk-based metrics such as METEOR(A. Lavie and A. Agarwal, 2007), ROUGE-L(Lin and Och, 2004), and IMPACT(H. Echizen-ya and K. Araki, 2007) are effective. In general, BLEU(K. Papineni et al., 2002), NIST(NIST, 2002), and RIBES(H. Isozaki et al., 2010) use a penalty for calculation of scores because the high score is often given extremely when the candidate translation is short. Therefore, the penalty is effective to obtain high correlation with human judgment. On the other hand, almost all chunkbased metrics use the F-measure based on a precision by candidate translation and a recall by reference. Moreover, they assign a Eduard Hovy Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 USA hovy@cmu.edu penalty for the difference of chunk order between the candidate translation and the reference, not the penalty for the differe</context>
</contexts>
<marker>Isozaki, Hirao, Duh, Sudoh, Tsukada, 2010</marker>
<rawString>H. Isozaki, T. Hirao, K. Duh, K. Sudoh and H. Tsukada. 2010. Automatic Evaluation of Translation Quality for Distant Language Pairs. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing. pp.944–952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>A Agarwal</author>
</authors>
<title>Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments.</title>
<date>2007</date>
<booktitle>Proceedings of the Second Workshop on Statistical Machine Translation.</booktitle>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>A. Lavie and A. Agarwal. 2007. Meteor: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments. Proceedings of the Second Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>F J Och</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using the Longest Common Subsequence and SkipBigram Statistics.</title>
<date>2004</date>
<booktitle>In Proc. of ACL’04,</booktitle>
<pages>606--613</pages>
<contexts>
<context position="1126" citStr="Lin and Och, 2004" startWordPosition="160" endWordPosition="163"> translation. Moreover, we apply a prize based on sentence-length to the metric, dissimilar from penalties in BLEU or NIST. We designate this metric as Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunkbased metric (APAC). Through metaevaluation experiments and comparison with several metrics, we confirmed that our metric shows stable correlation with human judgment. 1 Introduction In the field of machine translation, various automatic evaluation metrics have been proposed. Among them, chunk-based metrics such as METEOR(A. Lavie and A. Agarwal, 2007), ROUGE-L(Lin and Och, 2004), and IMPACT(H. Echizen-ya and K. Araki, 2007) are effective. In general, BLEU(K. Papineni et al., 2002), NIST(NIST, 2002), and RIBES(H. Isozaki et al., 2010) use a penalty for calculation of scores because the high score is often given extremely when the candidate translation is short. Therefore, the penalty is effective to obtain high correlation with human judgment. On the other hand, almost all chunkbased metrics use the F-measure based on a precision by candidate translation and a recall by reference. Moreover, they assign a Eduard Hovy Carnegie Mellon University 5000 Forbes Avenue Pittsb</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and F. J. Och. 2004. Automatic Evaluation of Machine Translation Quality Using the Longest Common Subsequence and SkipBigram Statistics. In Proc. of ACL’04, 606–613.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics.</title>
<date>2002</date>
<note>http://www.nist.gov/speech/tests/mt/doc/ ngram-study.pdf.</note>
<contexts>
<context position="1248" citStr="NIST, 2002" startWordPosition="180" endWordPosition="181">signate this metric as Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunkbased metric (APAC). Through metaevaluation experiments and comparison with several metrics, we confirmed that our metric shows stable correlation with human judgment. 1 Introduction In the field of machine translation, various automatic evaluation metrics have been proposed. Among them, chunk-based metrics such as METEOR(A. Lavie and A. Agarwal, 2007), ROUGE-L(Lin and Och, 2004), and IMPACT(H. Echizen-ya and K. Araki, 2007) are effective. In general, BLEU(K. Papineni et al., 2002), NIST(NIST, 2002), and RIBES(H. Isozaki et al., 2010) use a penalty for calculation of scores because the high score is often given extremely when the candidate translation is short. Therefore, the penalty is effective to obtain high correlation with human judgment. On the other hand, almost all chunkbased metrics use the F-measure based on a precision by candidate translation and a recall by reference. Moreover, they assign a Eduard Hovy Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 USA hovy@cmu.edu penalty for the difference of chunk order between the candidate translation and the refere</context>
</contexts>
<marker>NIST, 2002</marker>
<rawString>NIST. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram Co-Occurrence Statistics. http://www.nist.gov/speech/tests/mt/doc/ ngram-study.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL). pp.311–</booktitle>
<pages>318</pages>
<contexts>
<context position="1230" citStr="Papineni et al., 2002" startWordPosition="176" endWordPosition="179">alties in BLEU or NIST. We designate this metric as Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunkbased metric (APAC). Through metaevaluation experiments and comparison with several metrics, we confirmed that our metric shows stable correlation with human judgment. 1 Introduction In the field of machine translation, various automatic evaluation metrics have been proposed. Among them, chunk-based metrics such as METEOR(A. Lavie and A. Agarwal, 2007), ROUGE-L(Lin and Och, 2004), and IMPACT(H. Echizen-ya and K. Araki, 2007) are effective. In general, BLEU(K. Papineni et al., 2002), NIST(NIST, 2002), and RIBES(H. Isozaki et al., 2010) use a penalty for calculation of scores because the high score is often given extremely when the candidate translation is short. Therefore, the penalty is effective to obtain high correlation with human judgment. On the other hand, almost all chunkbased metrics use the F-measure based on a precision by candidate translation and a recall by reference. Moreover, they assign a Eduard Hovy Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 USA hovy@cmu.edu penalty for the difference of chunk order between the candidate translat</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL). pp.311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Van Rijsbergen</author>
</authors>
<date>1979</date>
<journal>Information Retrieval</journal>
<editor>(2nd ed.),</editor>
<location>Butterworths.</location>
<marker>Van Rijsbergen, 1979</marker>
<rawString>C. J. Van Rijsbergen. 1979. Information Retrieval (2nd ed.), Butterworths.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
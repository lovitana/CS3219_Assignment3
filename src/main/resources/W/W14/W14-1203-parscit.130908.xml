<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007605">
<title confidence="0.999480666666667">
Exploring Measures of “Readability” for Spoken Language:
Analyzing linguistic features of subtitles
to identify age-specific TV programs
</title>
<author confidence="0.966617">
Sowmya Vajjala and Detmar Meurers
</author>
<affiliation confidence="0.982068">
LEAD Graduate School, Department of Linguistics
University of T¨ubingen
</affiliation>
<email confidence="0.998003">
{sowmya,dm}@sfs.uni-tuebingen.de
</email>
<sectionHeader confidence="0.997376" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999974473684211">
We investigate whether measures of read-
ability can be used to identify age-specific
TV programs. Based on a corpus of BBC
TV subtitles, we employ a range of lin-
guistic readability features motivated by
Second Language Acquisition and Psy-
cholinguistics research.
Our hypothesis that such readability fea-
tures can successfully distinguish between
spoken language targeting different age
groups is fully confirmed. The classifiers
we trained on the basis of these readability
features achieve a classification accuracy
of 95.9%. Investigating several feature
subsets, we show that the authentic mate-
rial targeting specific age groups exhibits
a broad range of linguistics and psycholin-
guistic characteristics that are indicative of
the complexity of the language used.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999665866666667">
Reading, listening, and watching television pro-
grams are all ways to obtain information partly en-
coded in language. Just like books are written for
different target groups, current TV programs target
particular audiences, which differ in their interests
and ability to understand language. For books and
text in general, a wide range of readability mea-
sures have been developed to determine for which
audience the information encoded in the language
used is accessible. Different audiences are com-
monly distinguished in terms of the age or school
level targeted by a given text.
While for TV programs the nature of the inter-
action between the audio-visual presentation and
the language used is a relevant factor, in this pa-
per we want to explore whether the language by
itself is equally characteristic of the particular age
groups targeted by a given TV program. We thus
focused on the language content of the program
as encoded in TV subtitles and explored the role
of text complexity in predicting the intended age
group of the different programs.
The paper is organized as follows. Section 2
introduces the corpus we used, and section 3 the
readability features employed and their motiva-
tion. Section 4 discusses the experimental setup,
the experiments we conducted and their results.
Section 5 puts our research into the context of re-
lated work, before section 6 concludes and pro-
vides pointers to future research directions.
</bodyText>
<sectionHeader confidence="0.995961" genericHeader="introduction">
2 Corpus
</sectionHeader>
<bodyText confidence="0.99980744">
The BBC started subtitling all the scheduled pro-
grams on its main channels in 2008, implement-
ing UK regulations designed to help the hearing
impaired. Van Heuven et al. (2014) constructed a
corpus of subtitles from the programs run by nine
TV channels of the BBC, collected over a period
of three years, January 2010 to December 2012.
They used this corpus to compile an English word
frequencies database SUBTLEX-UK1, as a part of
the British Lexicon Project (Keuleers et al., 2012).
The subtitles of four channels (CBeebies, CBBC,
BBC News and BBC Parliament) were annotated
with the channel names.
While CBeebies targets children aged under 6
years, CBBC telecasts programs for children 6–12
years old. The other two channels (News, Parlia-
ment) are not assigned to a specific age-group, but
it seems safe to assume that they target a broader,
adult audience. In sum, we used the BBC subtitle
corpus with a three-way categorization: CBeebies,
CBBC, Adults.
Table 1 shows the basic statistics for the overall
corpus. For our machine learning experiments, we
use a balanced subcorpus with 3776 instances for
each class. As shown in the table, the programs for
</bodyText>
<footnote confidence="0.980753">
1http://crr.ugent.be/archives/1423
</footnote>
<page confidence="0.987851">
21
</page>
<note confidence="0.9959985">
Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 21–29,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<table confidence="0.9638504">
Program Category Age group # texts avg. tokens avg. sentence length
per text (in words)
CBEEBIES &lt; 6 years 4846 1144 4.9
CBBC 6–12 years 4840 2710 6.7
Adults (News + Parliament) &gt; 12 years 3776 4182 12.9
</table>
<tableCaption confidence="0.999225">
Table 1: BBC Subtitles Corpus Description
</tableCaption>
<bodyText confidence="0.999803888888889">
the older age-groups tend to be longer (i.e., more
words per text) and have longer sentences. While
text length and sentence length seem to constitute
informative features for predicting the age-group,
we hypothesized that other linguistic properties of
the language used may be at least as informative as
those superficial (and easily manipulated) proper-
ties. Hence, we explored a broad linguistic feature
set encoding various aspects of complexity.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.9969553">
The feature set we experimented with consists of
152 lexical and syntactic features that are primar-
ily derived from the research on text complexity
in Second Language Acquisition (SLA) and Psy-
cholinguistics. There are four types of features:
Lexical richness features (LEX): This group
consists of various part-of-speech (POS) tag den-
sities, lexical richness features from SLA research,
and the average number of senses per word.
Concretely, the POS tag features are: the pro-
portion of words belonging to different parts of
speech (nouns, proper nouns, pronouns, determin-
ers, adjectives, verbs, adverbs, conjunctions, in-
terjections, and prepositions) and different verb
forms (VBG, VBD, VBN, VBP in the Penn Tree-
bank tagset; Santorini 1990) per document.
The SLA-based lexical richness features we
used are: type-token ratio and corrected type-
token ratio, lexical density, ratio of nouns, verbs,
adjectives and adverbs to the number of lexical
words in a document, as described in Lu (2012).
The POS information required to extract these
features was obtained using Stanford Tagger
(Toutanova et al., 2003). The average number of
senses for a non-function word was obtained by
using the MIT WordNet API2 (Finlayson, 2014).
Syntactic complexity features (SYNTAX): This
group of features encodes the syntactic complex-
ity of a text derived from the constituent struc-
ture of the sentences. Some of these features are
</bodyText>
<footnote confidence="0.756448">
2http://projects.csail.mit.edu/jwi
</footnote>
<bodyText confidence="0.9978204">
derived from SLA research (Lu, 2010), specif-
ically: mean lengths of production units (sen-
tence, clause, t-unit), sentence complexity ratio
(# clauses/sentence), subordination in a sentence
(# clauses per t-unit, # complex t-units per t-unit,
# dependent clauses per clause and t-unit), co-
ordination in a sentence (# co-ordinate phrases
per clause and t-unit, # t-units/sentence), and spe-
cific syntactic structures (# complex nominals per
clause and t-unit, # VP per t-unit). Other syntactic
complexity features we made use of are the num-
ber of NPs, VPs, PPs, and SBARs per sentence
and their average length (in terms of # words), the
average parse tree height and the average number
of constituents per sub-tree.
All of these features were extracted using the
Berkeley Parser (Petrov and Klein, 2007) and the
Tregex pattern matcher (Levy and Andrew, 2006).
While the selection of features for these two
classes is based on Vajjala and Meurers (2012), for
the following two sets of features, we explored fur-
ther information available through psycholinguis-
tic resources.
Psycholinguistic features (PSYCH): This group
of features includes an encoding of the average
Age-of-acquisition (AoA) of words according to
different norms as provided by Kuperman et al.
(2012), including their own AoA rating obtained
through crowd sourcing. It also includes mea-
sures of word familiarity, concreteness, imageabil-
ity, meaningfulness and AoA as assigned in the
MRC Psycholinguistic database3 (Wilson, 1988).
For each feature, the value per text we computed
is the average of the values for all the words in the
text that had an entry in the database.
While these measures were not developed with
readability analysis in mind, we came across one
paper using such features as measures of word
difficulty in an approach to lexical simplification
(Jauhar and Specia, 2012).
</bodyText>
<footnote confidence="0.969102">
3http://www.psych.rl.ac.uk/
</footnote>
<page confidence="0.999128">
22
</page>
<bodyText confidence="0.999854740740741">
Celex features (CELEX): The Celex lexical
database (Baayen et al., 1995) for English con-
sists of annotations for the morphological, syntac-
tic, orthographic and phonological properties for
more than 50k words and lemmas. We included
all the morphological and syntactic properties that
were encoded using character or numeric codes in
our feature set. We did not use frequency informa-
tion from this database.
In all, this feature set consists of 35 morpholog-
ical and 49 syntactic properties per lemma. The
set includes: proportion of morphologically com-
plex words, attributive nouns, predicative adjec-
tives, etc. in the text. A detailed description of
all the properties of the words and lemmas in this
database can be found in the Celex English Lin-
guistic Guide4.
For both the PSYCH and CELEX features,
we encode the average value for a given text.
Words which were not included in the respec-
tive databases were ignored for this computation.
On average, around 40% of the words from texts
for covered by CELEX, 75% by Kuperman et al.
(2012) and 77% by the MRC database.
We do not use any features encoding the occur-
rence or frequency of specific words or n-grams in
a document.
</bodyText>
<sectionHeader confidence="0.997202" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<subsectionHeader confidence="0.967496">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999527555555556">
We used the WEKA toolkit (Hall et al., 2009) to
perform our classification experiments and evalu-
ated the classification accuracy using 10-fold cross
validation. As classification algorithm, we used
the Sequential Minimal Optimization (SMO) im-
plementation in WEKA, which marginally outper-
formed (1–1.5%) some other classification algo-
rithms (J48 Decision tree, Logistic Regression and
Random Forest) we tried in initial experiments.
</bodyText>
<subsectionHeader confidence="0.9769645">
4.2 Classification accuracy with various
feature groups
</subsectionHeader>
<bodyText confidence="0.999968142857143">
We discussed in the context of Table 1 that sen-
tence length may be a good surface indicator of
the age-group. So, we first constructed a classifi-
cation model with only one feature. This yielded
a classification accuracy of 71.4%, which we con-
sider as our baseline (instead of a basic random
baseline of 33%).
</bodyText>
<footnote confidence="0.967492">
4http://catalog.ldc.upenn.edu/docs/
LDC96L14/eug_a4.pdf
</footnote>
<bodyText confidence="0.9972603">
We then constructed a model with all the fea-
tures we introduced in section 3. This model
achieves a classification accuracy of 95.9%, which
is a 23.7% improvement over the sentence length
baseline in terms of classification accuracy.
In order to understand what features contribute
the most to classification accuracy, we applied fea-
ture selection on the entire set, using two algo-
rithms available in WEKA, which differ in the way
they select feature subsets:
</bodyText>
<listItem confidence="0.939472833333333">
• InfoGainAttributeEval evaluates the features
individually based on their Information Gain
(IG) with respect to the class.
• CfsSubsetEval (Hall, 1999) chooses a feature
subset considering the correlations between
features in addition to their predictive power.
</listItem>
<bodyText confidence="0.7844698">
Both feature selection algorithms use methods
that are independent of the classification algorithm
as such to select the feature subsets.
Information Gain-based feature selection re-
sults in a ranked list of features, which are inde-
pendent of each other. The Top-10 features ac-
cording to this algorithm are listed in Table 2.
Feature Group
avg. AoA (Kuperman et al., 2012) PSYCH
avg. # PPs in a sentence SYNTAX
avg. # instances where the lemma CELEX
has stem and affix
– avg. parse tree height SYNTAX
– avg. # NPs in a sentence SYNTAX
avg. # instances of affix substitution CELEX
– avg. # prep. in a sentence LEX
avg. # instances where a lemma is CELEX
not a count noun
avg. # clauses per sentence SYNTAX
– sentence length SYNTAX
</bodyText>
<tableCaption confidence="0.987688">
Table 2: Ranked list of Top-10 features using IG
</tableCaption>
<bodyText confidence="0.9999405">
As is clear from their description, all Top-10
features encode different linguistic aspects of a
text. While there are more syntactic features fol-
lowed by Celex features in these Top-10 features,
the most predictive feature is a psycholinguistic
feature encoding the average age of acquisition of
words. A classifier using only the Top-10 IG fea-
tures achieves an accuracy of 84.5%.
Applying CfsSubsetEval to these Top-10 fea-
tures set selects the six features not prefixed by a
</bodyText>
<page confidence="0.988175">
23
</page>
<bodyText confidence="0.998726285714286">
hyphen in the table, indicating that these features
do not correlate with each other (much). A clas-
sifier using only this subset of 6 features achieves
an accuracy of 84.1%.
We also explored the use of CfsSubsetEval fea-
ture selection on the entire feature set instead of
using only the Top 10 features. From the total of
152 features, CfsSubsetEval selected a set of 41
features. Building a classification model with only
these features resulted in a classification accuracy
of 93.9% which is only 2% less than the model
including all the features.
Table 3 shows the specific feature subset se-
lected by the CfsSubsetEval method, including
</bodyText>
<table confidence="0.706791585365854">
# preposition phrases
# t-units
# co-ordinate phrases per t-unit
# lexical words in total words
# interjections
# conjunctive phrases
# word senses
# verbs
# verbs, past participle (VBN)
# proper nouns
# plural nouns
avg. corrected type-token ratio
avg. AoA acc. to ratings of Kuperman et al. (2012)
avg. AoA acc. to ratings of Cortese and Khanna (2008)
avg. word imageability rating (MRC)
avg. AoA according to MRC
# morph. complex words (e.g., sandbank)
# morph. conversion (e.g., abandon)
# morph. irrelevant (e.g., meow)
# morph. obscure (e.g., dedicate)
# morph. may include root (e.g., imprimatur)
# foreign words (e.g., eureka)
# words with multiple analyses (e.g., treasurer)
# noun verb affix compounds (e.g., stockholder)
# lemmas with stem and affix (e.g., abundant=abound+ant)
# flectional forms (e.g., bagpipes)
# clipping allomorphy (e.g., phone vs. telephone)
# deriv. allomorphy (e.g., clarify–clarification)
# flectional allomorphy (e.g., verb bear → adjective born)
# conversion allomorphy (e.g., halve–half)
# lemmas with affix substitution (e.g., active=action+ive)
# words with reversion (e.g., downpour)
# uncountable nouns
# collective, countable nouns
# collective, uncountable nouns
# post positive nouns.
# verb, expression (e.g., bell the cat)
# adverb, expression (e.g., run amok)
# reflexive pronouns
# wh pronouns
# determinative pronouns
</table>
<tableCaption confidence="0.999752">
Table 3: CfsSubsetEval feature subset
</tableCaption>
<bodyText confidence="0.992308571428572">
some examples illustrating the morphological fea-
tures. The method does not provide a ranked list,
so the features here simply appear in the order in
which they are included in the feature vector.
All of these features except for the psycholinguis-
tic features encode the number of occurrences av-
eraged across the text (e.g., average number of
prepositions/sentence in a text) unless explicitly
stated otherwise. The psycholinguistic features
encode the average ratings of words for a given
property (e.g., average AoA of words in a text).
Table 4 summarizes the classification accura-
cies with the different feature subsets seen so far,
with the feature count shown in parentheses.
</bodyText>
<table confidence="0.9996646">
Feature Subset (#) Accuracy SD
All Features (152) 95.9% 0.37
Cfs on all features (41) 93.9% 0.59
Top-10 IG features (10) 84.5% 0.70
Cfs on IG (6) 84.1% 0.55
</table>
<tableCaption confidence="0.999961">
Table 4: Accuracy with various feature subsets
</tableCaption>
<bodyText confidence="0.998243">
We performed statistical significance tests be-
tween the feature subsets using the Paired T-tester
(corrected), provided with WEKA and all the dif-
ferences in accuracy were found to be statistically
significant at p &lt; 0.001. We also provide the Stan-
dard Deviation (SD) of the test set accuracy in the
10 folds of CV per dataset, to make it possible to
compare these experiments with future research on
this dataset in terms of statistical significance.
Table 5 presents the classification accuracies of
individual features from the Top-10 features list
(introduced in Table 2).
</bodyText>
<table confidence="0.999609181818182">
Feature Accuracy
AoA Kup Lem 82.4%
# pp 74.0%
# stem &amp; affix 77.7%
avg. parse tree height 73.4%
# np 73.0%
# substitution 74.3%
# prep 72.0%
# uncountable nouns 68.3%
# clauses 72.5%
sentence length 71.4%
</table>
<tableCaption confidence="0.9998">
Table 5: Accuracies of Top-10 individual features
</tableCaption>
<bodyText confidence="0.998841">
The table shows that all but one of the features
individually achieves a classification accuracy
above 70%. The first feature (AoA Kup Lem)
</bodyText>
<page confidence="0.994022">
24
</page>
<bodyText confidence="0.998353090909091">
alone resulted in an accuracy of 82.4%, which is
quite close to the accuracy obtained by all the Top-
10 features together (84.5%).
To obtain a fuller picture of the impact of dif-
ferent feature groups, we also performed ablation
tests removing some groups of features at a time.
Table 6 shows the results of these tests along with
the SD of the 10 fold CV. All the results that are
statistically different at p &lt; 0.001 from the model
with all features (95.9% accuracy, 0.37 SD) are in-
dicated with a *.
</bodyText>
<table confidence="0.999662545454545">
Features Acc. SD
All − AoA Kup Lem 95.9% 0.37
All − All AoA Features 95.6% 0.58
All − PSYCH 95.8% 0.31
All − CELEX 94.7%* 0.51
All − CELEX−PSYCH 93.6%* 0.66
All − CELEX−PSYCH−LEX 77.5%* 0.99
(= SYNTAX only)
LEX 93.1%* 0.70
CELEX 90.0%* 0.79
PSYCH 84.5%* 1.12
</table>
<tableCaption confidence="0.999836">
Table 6: Ablation test accuracies
</tableCaption>
<bodyText confidence="0.999907269230769">
Interestingly, removing the most predictive in-
dividual feature (AoA Kup Lem) from the feature
set did not change the overall classification accu-
racy at all. Removing all of the AoA features or
all of the psycholinguistic features also resulted in
only a very small drop. The combination of the
linguistic features, covering lexical and syntactic
characteristics as well as the morphological, syn-
tactic, orthographic, and phonological properties
from Celex, thus seem to be equally characteristic
of the texts targeting different age-groups as the
psycholinguistic properties, even though the fea-
tures are quite different in nature.
In terms of separate groups of features, syntac-
tic features alone performed the worst (77.5%) and
lexical richness features the best (93.1%).
To investigate which classes were mixed up by
the classifier, consider Table 7 showing the con-
fusion matrix for the model with all features on a
10-fold CV experiment.
We find that CBeebies is more often con-
fused with the CBBC program for older chil-
dren (156+214) and very rarely with the program
for adults (1+2). The older children programs
(CBBC) are more commonly confused with pro-
grams for adults (36+58) compared to CBeebies
</bodyText>
<table confidence="0.996708">
classified as → CBeebies CBBC Adults
CBeebies (0–6) 3619 156 1
CBBC (6–12) 214 3526 36
Adults (12+) 2 58 3716
</table>
<tableCaption confidence="0.998988">
Table 7: Confusion Matrix
</tableCaption>
<bodyText confidence="0.999682846153846">
(1+2), which is expected given that the CBBC au-
dience is closer in age to adults than the CBeebies
audience.
Summing up, we can conclude from these ex-
periments that the classification of transcripts into
age groups can be informed by a wide range of lin-
guistics and psycholinguistic features. While for
some practical tasks a few features may be enough
to obtain a classification of sufficient accuracy, the
more general take-home message is that authentic
texts targeting specific age groups exhibit a broad
range of linguistics characteristics that are indica-
tive of the complexity of the language used.
</bodyText>
<subsectionHeader confidence="0.998956">
4.3 Effect of text size and training data size
</subsectionHeader>
<bodyText confidence="0.999931">
When we first introduced the properties of the cor-
pus in Table 1, it appeared that sentence length
and the overall text length could be important pre-
dictors of the target age-groups. However, the list
of Top-10 features based on information gain was
dominated by more linguistically oriented syntac-
tic and psycholinguistic features.
Sentence length was only the tenth best feature
by information gain and did not figure at all in the
43 features chosen by the CfsSubsetEval method
selecting features that are highly correlated with
the class prediction while having low correlation
between themselves. As mentioned above, sen-
tence length as an individual feature only achieved
a classification accuracy of 71.4%.
The text length is not a part of any feature set we
used, but considering the global corpus properties
we wanted to verify how well it would perform
and thus trained a model with only text length
(#sentences per text) as a feature. This achieved
a classification accuracy of only 56.7%.
The corpus consists of transcripts of whole TV
programs and hence an individual transcript text
typically is longer than the texts commonly used in
readability classification experiments. This raises
the question whether the high classification accu-
racies we obtained are the consequences of the
larger text size.
As a second issue, the training size available for
the 10-fold cross-validation experiments is com-
</bodyText>
<page confidence="0.995588">
25
</page>
<bodyText confidence="0.9982025625">
paratively large, given the 3776 text per level
available in the overall corpus. We thus also
wanted to study the impact of the training size on
the classification accuracy achieved.
Pulling these threads together, we compared
the classification accuracy against text length and
training set size to better understand their impact.
For this, we trained models with different text
sizes (by considering the first 25%, 50%, 75% or
100% of the sentences from each text) and with
different training set sizes (from 10% to 100%).
Figure 1 presents the resulting classification ac-
curacy in relation to training set size for the dif-
ferent text sizes. All models were trained with the
full feature set (152 features), using 10-fold cross-
validation as before.
</bodyText>
<figure confidence="0.695588">
10 20 30 40 50 60 70 80 90 100
training set size (in percent)
</figure>
<figureCaption confidence="0.9940925">
Figure 1: Classification accuracy for different text
sizes and training set sizes
</figureCaption>
<bodyText confidence="0.999969756097561">
As expected, both the training set size and the
text size affect the classification accuracy. How-
ever, the classification accuracy even for the small-
est text and training set size is always above 90%,
which means that the unusually large text and
training size is not the main factor behind the very
high accuracy rates.
In all four cases of text size, there was a small
effect of training set size on the classification ac-
curacy. But the effect reduced as the text size in-
creased. At 25% text size, for example, the clas-
sification accuracy ranged 90–93% (mean 92.1%,
SD 0.9) as the training set size increased from 10%
to 100%. However, at 100% text size, the range
was only 94.8–96% (mean 95.6%, SD 0.4).
Comparing the results in terms of text size
alone, larger text size resulted in better classifica-
tion accuracy in all cases, irrespective of the train-
ing set size. A longer text will simply provide
more information for the various linguistic fea-
tures, enabling the model to deliver better judg-
ments about the text. However, despite the text
length being reduced to one fourth of its size, the
models built with our feature set always collect
enough information to ensure a classification ac-
curacy of at least 90%.
In the above experiments, we varied the text size
from 10% to 100%. But since these are percent-
ages, texts from CBBC and Adults on average still
are longer than CBEEBIES texts. While this re-
flects the fact that TV transcripts in real life are of
different length, we also wanted to see what hap-
pens when we eliminate such length differences.
We thus trained classification models fixing the
length of all documents to a concrete absolute
length, starting from 100 words (rounded off to the
nearest sentence boundary) increasing the text size
until we achieve the best overall performance. Fig-
ure 2 displays the classification accuracy we ob-
tained for the different (maximum) text sizes, for
all features and feature subsets.
</bodyText>
<figure confidence="0.446077">
100 200 300 400 500 600 700 800 900
max. text size (in number of words)
</figure>
<figureCaption confidence="0.910049">
Figure 2: Classification accuracy for different ab-
solute text sizes (in words)
</figureCaption>
<bodyText confidence="0.999651727272727">
The plot shows that the classification accuracy
already reaches 80% accuracy for short texts, 100
words in length, for the model with all features. It
rises to above 90% for texts which are 300 words
long and reaches the best overall accuracy of al-
most 96% for texts which are 900 words in length.
All the feature subsets too follow the same trend,
with varying degrees of accuracy that is always
lower than the model with all features.
While in this paper, we focus on documents,
the issue whether the data can be reduced further
</bodyText>
<figure confidence="0.995659076923077">
classification accuracy (in percent)
96
95
94
93
92
91
90
25% text size
50% text size
75% text size
100% text size
classification accuracy (in percent)
100
95
90
85
80
75
70
65
All Features
PSYCH
LEX
SYN
CELEX
</figure>
<page confidence="0.991994">
26
</page>
<bodyText confidence="0.99270075">
to perform readability at the sentence level is dis-
cussed in Vajjala and Meurers (2014a).
artificially shortened transcripts, confirm the use-
fulness of connecting these research threads.
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99990525">
Analyzing the complexity of written texts and
choosing suitable texts for various target groups
including children is widely studied in computa-
tional linguistics. Some of the popular approaches
include the use of language models and machine
learning approaches (e.g., Collins-Thompson and
Callan, 2005; Feng, 2010). Web-based tools such
as REAP5 and TextEvaluator6 are some examples
of real-life applications for selecting English texts
by grade level.
In terms of analyzing spoken language, research
in language assessment has analyzed spoken tran-
scripts in terms of syntactic complexity (Chen and
Zechner, 2011) and other textual characteristics
(Crossley and McNamara, 2013).
In the domain of readability assessment,
the Common Core Standards (http://www.
corestandards.org) guideline texts were
used as a standard test set in the recent past (Nel-
son et al., 2012; Flor et al., 2013). This test set
contains some transcribed speech. However, to
the best of our knowledge, the process of select-
ing suitable TV programs for children as explored
in this paper has not been considered as a case of
readability assessment of spoken language before.
Subtitle corpora have been created and used
in computational linguistics for various pur-
poses. Some of them include video classifica-
tion (Katsiouli et al., 2007), machine translation
(Petukhova et al., 2012), and simplification for
deaf people (Daelemans et al., 2004). But, we are
not aware of any such subtitle research studying
the problem of automatically identifying TV pro-
grams for various age-groups.
This paper thus can be seen as connecting sev-
eral threads of research, from the analysis of text
complexity and readability, via the research on
measuring SLA proficiency that many of the lin-
guistic features we used stem from, to the com-
putational analysis of speech as encoded in subti-
tles. The range of linguistic characteristics which
turn out to be relevant and the very high preci-
sion with which the age-group classification can
be performed, even when restricting the input to
</bodyText>
<footnote confidence="0.995932">
5http://reap.cs.cmu.edu
6https://texteval-pilot.ets.org/
TextEvaluator
</footnote>
<sectionHeader confidence="0.992531" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999969191489362">
In this paper, we described a classification ap-
proach identifying TV programs for different
age-groups based on a range of linguistically-
motivated features derived from research on text
readability, proficiency in SLA, and psycholin-
guistic research. Using a collection of subtitle
documents classified into three groups based on
the targeted age-group, we explored different clas-
sification models with our feature set.
The experiments showed that our linguistically
motivated features perform very well, achieving
a classification accuracy of 95.9% (section 4.2).
Apart from the entire feature set, we also exper-
imented with small groups of features by apply-
ing feature selection algorithms. As it turns out,
the single most predictive feature was the age-
of-acquisition feature of Kuperman et al. (2012),
with an accuracy of 82.4%. Yet when this fea-
ture is removed from the overall feature set, the
classification accuracy is not reduced, highlighting
that such age-group classification is informed by a
range of different characteristics, not just a single,
dominating one. Authentic texts targeting specific
age groups exhibit a broad range of linguistics and
psycholinguistic characteristics that are indicative
of the complexity of the language used.
While an information gain-based feature subset
consisting of 10 features resulted in an accuracy of
84.5%, a feature set chosen using the CfsSubsetE-
val method in WEKA gave an accuracy of 93.9%.
Any of the feature groups we tested exceeded the
random baseline (33%) and a baseline using the
popular sentence length feature (71.4%) by a large
margin. Individual feature groups also performed
well at over 90% accurately in most of the cases.
The analysis thus supports multiple, equally valid
perspectives on a given text, each view encoding a
different linguistic aspect.
Apart from the features explored, we also stud-
ied the effect of the training set size and the length
of the text considered for feature extraction on
classification accuracy (Section 4.3). The size of
training set mattered more when the text size was
smaller. Text size, which did not work well as an
individual feature, clearly influences classification
accuracy by providing more information for model
building and testing.
</bodyText>
<page confidence="0.992675">
27
</page>
<bodyText confidence="0.9999617">
In terms of the practical relevance of the re-
sults, one question that needs some attention is
how well the features and trained models gener-
alize across different type of TV programs or lan-
guages. While we have not yet investigated this
for TV subtitles, in experiments investigating the
cross-corpus performance of a model using the
same feature set, we found that the approach per-
forms well for a range of corpora composed of
reading materials for language learners (Vajjala
and Meurers, 2014b). The very high classification
accuracies of the experiments we presented in the
current paper thus seem to support the assumption
that the approach can be useful in practice for au-
tomatically identifying TV programs for viewers
of different age groups.
Regarding the three class distinctions and the
classifier setup we used in this paper, the approach
can also be generalized to other scales and a re-
gression setup (Vajjala and Meurers, 2013).
</bodyText>
<subsectionHeader confidence="0.9584">
6.1 Outlook
</subsectionHeader>
<bodyText confidence="0.999900913043478">
The current work focused mostly on modeling and
studying different feature groups in terms of their
classification accuracy. Performing error analysis
and looking at the texts where the approach failed
may yield further insights into the problem. Some
aspects of the text that we did not consider in-
clude discourse coherence or topic effects. Study-
ing these two aspects can provide more insights
into the nature of the language used in TV pro-
grams directed at viewers of different ages. A
cross-genre evaluation between written and spo-
ken language complexity across age-groups could
also be insightful.
On the technical side, it would also be useful
to explore the possibility of using a parser tuned
to spoken language, to check if this helps improve
the classification accuracy of syntactic features.
While in this paper we focused on English, a
related readability model also performed well for
German (Hancke et al., 2012) so that we expect
the general approach to be applicable to other lan-
guages, subject to the availability of the relevant
resources and tools.
</bodyText>
<sectionHeader confidence="0.997734" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999952875">
We would like to thank Marc Brysbaert and his
colleagues for making their excellent resources
available to the research community. We also
thank the anonymous reviewers for their useful
feedback. This research was funded by LEAD
Graduate School (GSC 1028, http://purl.org/
lead), a project of the Excellence Initiative of the
German federal and state governments.
</bodyText>
<sectionHeader confidence="0.995533" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997825488888889">
Harald R. Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1995. The CELEX lexical database. http:
//catalog.ldc.upenn.edu/LDC96L14.
Maio Chen and Klaus Zechner. 2011. Computing and
evaluating syntactic complexity features for auto-
mated scoring of spontaneous non-native speech. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
722–731, Portland, Oregon, June.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Predicting reading difficulty with statistical lan-
guage models. Journal of the American Society for
Information Science and Technology, 56(13):1448–
1462.
Michael J. Cortese and Maya M. Khanna. 2008. Age
of acquisition ratings for 3,000 monosyllabic words.
Behavior Research Methods, 43:791–794.
Scott Crossley and Danielle McNamara. 2013. Ap-
plications of text analysis tools for spoken re-
sponse grading. Language Learning &amp; Technology,
17:171–192.
Walter Daelemans, Anja Hoethker, and Erik F.
Tjong Kim Sang. 2004. Automatic sentence sim-
plification for subtitling in Dutch and English. In
Fourth International Conference on Language Re-
sources And Evaluation (LREC), pages 1045–1048.
Lijun Feng. 2010. Automatic Readability Assessment.
Ph.D. thesis, City University of New York (CUNY).
Mark Alan Finlayson. 2014. Java libraries for access-
ing the princeton wordnet: Comparison and evalua-
tion. In Proceedings of the 7th Global Wordnet Con-
ference, pages 78–85.
Michael Flor, Beata Beigman Klebanov, and Kath-
leen M. Sheehan. 2013. Lexical tightness and text
complexity. In Proceedings of the Second Workshop
on Natural Language Processing for Improving Tex-
tual Accessibility (PITR) held at ACL, pages 29–38,
Sofia, Bulgaria. ACL.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
The SIGKDD Explorations, 11:10–18.
Mark A. Hall. 1999. Correlation-based Feature Selec-
tion for Machine Learning. Ph.D. thesis, The Uni-
versity of Waikato, Hamilton, NewZealand.
</reference>
<page confidence="0.977635">
28
</page>
<reference confidence="0.999890705882353">
Julia Hancke, Detmar Meurers, and Sowmya Vajjala.
2012. Readability classification for german using
lexical, syntactic, and morphological features. In
Proceedings of the 24th International Conference
on Computational Linguistics (COLING): Technical
Papers, pages 1063–1080, Mumbai, India.
Sujay Kumar Jauhar and Lucia Specia. 2012. Uow-
shef: Simplex – lexical simplicity ranking based on
contextual and psycholinguistic features. In In pro-
ceedings of the First Joint Conference on Lexical
and Computational Semantics (SEM).
Polyxeni Katsiouli, Vassileios Tsetsos, and Stathes
Hadjiefthymiades. 2007. Semantic video classifi-
cation based on subtitles and domain terminologies.
In Proceedings of the 1st International Workshop
on Knowledge Acquisition from Multimedia Content
(KAMC).
Emmanuel Keuleers, Paula Lacey, Kathleen Rastle,
and Marc Brysbaert. 2012. The british lexicon
project: Lexical decision data for 28,730 monosyl-
labic and disyllabic english words. Behavior Re-
search Methods, 44:287–304.
Victor Kuperman, Hans Stadthagen-Gonzalez, and
Marc Brysbaert. 2012. Age-of-acquisition ratings
for 30,000 english words. Behavior Research Meth-
ods, 44(4):978–990.
Roger Levy and Galen Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In 5th International Conference on Lan-
guage Resources and Evaluation, pages 2231–2234,
Genoa, Italy. European Language Resources Asso-
ciation (ELRA).
Xiaofei Lu. 2010. Automatic analysis of syntac-
tic complexity in second language writing. Inter-
national Journal of Corpus Linguistics, 15(4):474–
496.
Xiaofei Lu. 2012. The relationship of lexical richness
to the quality of ESL learners’ oral narratives. The
Modern Languages Journal, pages 190–208.
Jessica Nelson, Charles Perfetti, David Liben, and
Meredith Liben. 2012. Measures of text difficulty:
Testing their predictive value for grade levels and
student performance. Technical report, The Coun-
cil of Chief State School Officers.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Volha Petukhova, Rodrigo Agerri, Mark Fishel, Yota
Georgakopoulou, Sergio Penkale, Arantza del Pozo,
Mirjam Sepesy Maucec, Martin Volk, and Andy
Way. 2012. Sumat: Data collection and parallel
corpus compilation for machine translation of sub-
titles. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation
(LREC-2012), pages 21–28, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
Beatrice Santorini. 1990. Part-of-speech tagging
guidelines for the Penn Treebank, 3rd revision, 2nd
printing. Technical report, Department of Computer
Science, University of Pennsylvania.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-
ofspeech tagging with a cyclic dependency net-
work. In HLT-NAACL, pages 252–259, Edmonton,
Canada.
Sowmya Vajjala and Detmar Meurers. 2012. On im-
proving the accuracy of readability classification us-
ing insights from second language acquisition. In
In Proceedings of the 7th Workshop on Innovative
Use of NLP for Building Educational Applications
(BEA) atNAACL-HLT, pages 163—-173, Montr´eal,
Canada. ACL.
Sowmya Vajjala and Detmar Meurers. 2013. On
the applicability of readability models to web texts.
In Proceedings of the Second Workshop on Natural
Language Processing for Improving Textual Acces-
sibility (PITR) held at ACL, pages 59—-68, Sofia,
Bulgaria. ACL.
Sowmya Vajjala and Detmar Meurers. 2014a. Assess-
ing the relative reading level of sentence pairs for
text simplification. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics (EACL). ACL.
Sowmya Vajjala and Detmar Meurers. 2014b. Read-
ability assessment for text simplification: From an-
alyzing documents to identifying sentential simplifi-
cations. International Journal of Applied Linguis-
tics, Special Issue on Current Research in Read-
ability and Text Simplification, edited by Thomas
Franc¸ois and Delphine Bernhard.
Walter J.B. Van Heuven, Pawel Mandera, Emmanuel
Keuleers, and Marc Brysbaert. 2014. Subtlex-UK:
A new and improved word frequency database for
British English. The Quarterly Journal of Experi-
mental Psychology, pages 1–15.
Michael D. Wilson. 1988. The mrc psycholinguis-
tic database: Machine readable dictionary, version
2. Behavioural Research Methods, Instruments and
Computers, 20(1):6–11.
</reference>
<page confidence="0.999114">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.919986">
<title confidence="0.997128333333333">Exploring Measures of “Readability” for Spoken Analyzing linguistic features of to identify age-specific TV programs</title>
<author confidence="0.977348">Vajjala</author>
<affiliation confidence="0.972621">LEAD Graduate School, Department of University of</affiliation>
<abstract confidence="0.99935415">We investigate whether measures of readability can be used to identify age-specific TV programs. Based on a corpus of BBC TV subtitles, we employ a range of linguistic readability features motivated by Second Language Acquisition and Psycholinguistics research. Our hypothesis that such readability features can successfully distinguish between spoken language targeting different age groups is fully confirmed. The classifiers we trained on the basis of these readability features achieve a classification accuracy of 95.9%. Investigating several feature subsets, we show that the authentic material targeting specific age groups exhibits a broad range of linguistics and psycholinguistic characteristics that are indicative of the complexity of the language used.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Harald R Baayen</author>
<author>Richard Piepenbrock</author>
<author>Leon Gulikers</author>
</authors>
<date>1995</date>
<note>The CELEX lexical database. http: //catalog.ldc.upenn.edu/LDC96L14.</note>
<contexts>
<context position="8044" citStr="Baayen et al., 1995" startWordPosition="1245" endWordPosition="1248">ncludes measures of word familiarity, concreteness, imageability, meaningfulness and AoA as assigned in the MRC Psycholinguistic database3 (Wilson, 1988). For each feature, the value per text we computed is the average of the values for all the words in the text that had an entry in the database. While these measures were not developed with readability analysis in mind, we came across one paper using such features as measures of word difficulty in an approach to lexical simplification (Jauhar and Specia, 2012). 3http://www.psych.rl.ac.uk/ 22 Celex features (CELEX): The Celex lexical database (Baayen et al., 1995) for English consists of annotations for the morphological, syntactic, orthographic and phonological properties for more than 50k words and lemmas. We included all the morphological and syntactic properties that were encoded using character or numeric codes in our feature set. We did not use frequency information from this database. In all, this feature set consists of 35 morphological and 49 syntactic properties per lemma. The set includes: proportion of morphologically complex words, attributive nouns, predicative adjectives, etc. in the text. A detailed description of all the properties of </context>
</contexts>
<marker>Baayen, Piepenbrock, Gulikers, 1995</marker>
<rawString>Harald R. Baayen, Richard Piepenbrock, and Leon Gulikers. 1995. The CELEX lexical database. http: //catalog.ldc.upenn.edu/LDC96L14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maio Chen</author>
<author>Klaus Zechner</author>
</authors>
<title>Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>722--731</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="24689" citStr="Chen and Zechner, 2011" startWordPosition="3982" endWordPosition="3985">ated Work Analyzing the complexity of written texts and choosing suitable texts for various target groups including children is widely studied in computational linguistics. Some of the popular approaches include the use of language models and machine learning approaches (e.g., Collins-Thompson and Callan, 2005; Feng, 2010). Web-based tools such as REAP5 and TextEvaluator6 are some examples of real-life applications for selecting English texts by grade level. In terms of analyzing spoken language, research in language assessment has analyzed spoken transcripts in terms of syntactic complexity (Chen and Zechner, 2011) and other textual characteristics (Crossley and McNamara, 2013). In the domain of readability assessment, the Common Core Standards (http://www. corestandards.org) guideline texts were used as a standard test set in the recent past (Nelson et al., 2012; Flor et al., 2013). This test set contains some transcribed speech. However, to the best of our knowledge, the process of selecting suitable TV programs for children as explored in this paper has not been considered as a case of readability assessment of spoken language before. Subtitle corpora have been created and used in computational lingu</context>
</contexts>
<marker>Chen, Zechner, 2011</marker>
<rawString>Maio Chen and Klaus Zechner. 2011. Computing and evaluating syntactic complexity features for automated scoring of spontaneous non-native speech. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 722–731, Portland, Oregon, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevyn Collins-Thompson</author>
<author>Jamie Callan</author>
</authors>
<title>Predicting reading difficulty with statistical language models.</title>
<date>2005</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>56</volume>
<issue>13</issue>
<pages>1462</pages>
<contexts>
<context position="24377" citStr="Collins-Thompson and Callan, 2005" startWordPosition="3936" endWordPosition="3939">ze 75% text size 100% text size classification accuracy (in percent) 100 95 90 85 80 75 70 65 All Features PSYCH LEX SYN CELEX 26 to perform readability at the sentence level is discussed in Vajjala and Meurers (2014a). artificially shortened transcripts, confirm the usefulness of connecting these research threads. 5 Related Work Analyzing the complexity of written texts and choosing suitable texts for various target groups including children is widely studied in computational linguistics. Some of the popular approaches include the use of language models and machine learning approaches (e.g., Collins-Thompson and Callan, 2005; Feng, 2010). Web-based tools such as REAP5 and TextEvaluator6 are some examples of real-life applications for selecting English texts by grade level. In terms of analyzing spoken language, research in language assessment has analyzed spoken transcripts in terms of syntactic complexity (Chen and Zechner, 2011) and other textual characteristics (Crossley and McNamara, 2013). In the domain of readability assessment, the Common Core Standards (http://www. corestandards.org) guideline texts were used as a standard test set in the recent past (Nelson et al., 2012; Flor et al., 2013). This test set</context>
</contexts>
<marker>Collins-Thompson, Callan, 2005</marker>
<rawString>Kevyn Collins-Thompson and Jamie Callan. 2005. Predicting reading difficulty with statistical language models. Journal of the American Society for Information Science and Technology, 56(13):1448– 1462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Cortese</author>
<author>Maya M Khanna</author>
</authors>
<title>Age of acquisition ratings for 3,000 monosyllabic words. Behavior Research Methods,</title>
<date>2008</date>
<pages>43--791</pages>
<contexts>
<context position="13024" citStr="Cortese and Khanna (2008)" startWordPosition="2060" endWordPosition="2063">41 features. Building a classification model with only these features resulted in a classification accuracy of 93.9% which is only 2% less than the model including all the features. Table 3 shows the specific feature subset selected by the CfsSubsetEval method, including # preposition phrases # t-units # co-ordinate phrases per t-unit # lexical words in total words # interjections # conjunctive phrases # word senses # verbs # verbs, past participle (VBN) # proper nouns # plural nouns avg. corrected type-token ratio avg. AoA acc. to ratings of Kuperman et al. (2012) avg. AoA acc. to ratings of Cortese and Khanna (2008) avg. word imageability rating (MRC) avg. AoA according to MRC # morph. complex words (e.g., sandbank) # morph. conversion (e.g., abandon) # morph. irrelevant (e.g., meow) # morph. obscure (e.g., dedicate) # morph. may include root (e.g., imprimatur) # foreign words (e.g., eureka) # words with multiple analyses (e.g., treasurer) # noun verb affix compounds (e.g., stockholder) # lemmas with stem and affix (e.g., abundant=abound+ant) # flectional forms (e.g., bagpipes) # clipping allomorphy (e.g., phone vs. telephone) # deriv. allomorphy (e.g., clarify–clarification) # flectional allomorphy (e.g</context>
</contexts>
<marker>Cortese, Khanna, 2008</marker>
<rawString>Michael J. Cortese and Maya M. Khanna. 2008. Age of acquisition ratings for 3,000 monosyllabic words. Behavior Research Methods, 43:791–794.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Crossley</author>
<author>Danielle McNamara</author>
</authors>
<title>Applications of text analysis tools for spoken response grading.</title>
<date>2013</date>
<journal>Language Learning &amp; Technology,</journal>
<pages>17--171</pages>
<contexts>
<context position="24753" citStr="Crossley and McNamara, 2013" startWordPosition="3990" endWordPosition="3993">osing suitable texts for various target groups including children is widely studied in computational linguistics. Some of the popular approaches include the use of language models and machine learning approaches (e.g., Collins-Thompson and Callan, 2005; Feng, 2010). Web-based tools such as REAP5 and TextEvaluator6 are some examples of real-life applications for selecting English texts by grade level. In terms of analyzing spoken language, research in language assessment has analyzed spoken transcripts in terms of syntactic complexity (Chen and Zechner, 2011) and other textual characteristics (Crossley and McNamara, 2013). In the domain of readability assessment, the Common Core Standards (http://www. corestandards.org) guideline texts were used as a standard test set in the recent past (Nelson et al., 2012; Flor et al., 2013). This test set contains some transcribed speech. However, to the best of our knowledge, the process of selecting suitable TV programs for children as explored in this paper has not been considered as a case of readability assessment of spoken language before. Subtitle corpora have been created and used in computational linguistics for various purposes. Some of them include video classifi</context>
</contexts>
<marker>Crossley, McNamara, 2013</marker>
<rawString>Scott Crossley and Danielle McNamara. 2013. Applications of text analysis tools for spoken response grading. Language Learning &amp; Technology, 17:171–192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Anja Hoethker</author>
<author>Erik F Tjong Kim Sang</author>
</authors>
<title>Automatic sentence simplification for subtitling in Dutch and English.</title>
<date>2004</date>
<booktitle>In Fourth International Conference on Language Resources And Evaluation (LREC),</booktitle>
<pages>1045--1048</pages>
<contexts>
<context position="25491" citStr="Daelemans et al., 2004" startWordPosition="4107" endWordPosition="4110">s were used as a standard test set in the recent past (Nelson et al., 2012; Flor et al., 2013). This test set contains some transcribed speech. However, to the best of our knowledge, the process of selecting suitable TV programs for children as explored in this paper has not been considered as a case of readability assessment of spoken language before. Subtitle corpora have been created and used in computational linguistics for various purposes. Some of them include video classification (Katsiouli et al., 2007), machine translation (Petukhova et al., 2012), and simplification for deaf people (Daelemans et al., 2004). But, we are not aware of any such subtitle research studying the problem of automatically identifying TV programs for various age-groups. This paper thus can be seen as connecting several threads of research, from the analysis of text complexity and readability, via the research on measuring SLA proficiency that many of the linguistic features we used stem from, to the computational analysis of speech as encoded in subtitles. The range of linguistic characteristics which turn out to be relevant and the very high precision with which the age-group classification can be performed, even when re</context>
</contexts>
<marker>Daelemans, Hoethker, Sang, 2004</marker>
<rawString>Walter Daelemans, Anja Hoethker, and Erik F. Tjong Kim Sang. 2004. Automatic sentence simplification for subtitling in Dutch and English. In Fourth International Conference on Language Resources And Evaluation (LREC), pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lijun Feng</author>
</authors>
<title>Automatic Readability Assessment.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>City University of New</institution>
<location>York (CUNY).</location>
<contexts>
<context position="24390" citStr="Feng, 2010" startWordPosition="3940" endWordPosition="3941">ssification accuracy (in percent) 100 95 90 85 80 75 70 65 All Features PSYCH LEX SYN CELEX 26 to perform readability at the sentence level is discussed in Vajjala and Meurers (2014a). artificially shortened transcripts, confirm the usefulness of connecting these research threads. 5 Related Work Analyzing the complexity of written texts and choosing suitable texts for various target groups including children is widely studied in computational linguistics. Some of the popular approaches include the use of language models and machine learning approaches (e.g., Collins-Thompson and Callan, 2005; Feng, 2010). Web-based tools such as REAP5 and TextEvaluator6 are some examples of real-life applications for selecting English texts by grade level. In terms of analyzing spoken language, research in language assessment has analyzed spoken transcripts in terms of syntactic complexity (Chen and Zechner, 2011) and other textual characteristics (Crossley and McNamara, 2013). In the domain of readability assessment, the Common Core Standards (http://www. corestandards.org) guideline texts were used as a standard test set in the recent past (Nelson et al., 2012; Flor et al., 2013). This test set contains som</context>
</contexts>
<marker>Feng, 2010</marker>
<rawString>Lijun Feng. 2010. Automatic Readability Assessment. Ph.D. thesis, City University of New York (CUNY).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Alan Finlayson</author>
</authors>
<title>Java libraries for accessing the princeton wordnet: Comparison and evaluation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 7th Global Wordnet Conference,</booktitle>
<pages>78--85</pages>
<contexts>
<context position="5859" citStr="Finlayson, 2014" startWordPosition="910" endWordPosition="911">onjunctions, interjections, and prepositions) and different verb forms (VBG, VBD, VBN, VBP in the Penn Treebank tagset; Santorini 1990) per document. The SLA-based lexical richness features we used are: type-token ratio and corrected typetoken ratio, lexical density, ratio of nouns, verbs, adjectives and adverbs to the number of lexical words in a document, as described in Lu (2012). The POS information required to extract these features was obtained using Stanford Tagger (Toutanova et al., 2003). The average number of senses for a non-function word was obtained by using the MIT WordNet API2 (Finlayson, 2014). Syntactic complexity features (SYNTAX): This group of features encodes the syntactic complexity of a text derived from the constituent structure of the sentences. Some of these features are 2http://projects.csail.mit.edu/jwi derived from SLA research (Lu, 2010), specifically: mean lengths of production units (sentence, clause, t-unit), sentence complexity ratio (# clauses/sentence), subordination in a sentence (# clauses per t-unit, # complex t-units per t-unit, # dependent clauses per clause and t-unit), coordination in a sentence (# co-ordinate phrases per clause and t-unit, # t-units/sent</context>
</contexts>
<marker>Finlayson, 2014</marker>
<rawString>Mark Alan Finlayson. 2014. Java libraries for accessing the princeton wordnet: Comparison and evaluation. In Proceedings of the 7th Global Wordnet Conference, pages 78–85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Flor</author>
<author>Beata Beigman Klebanov</author>
<author>Kathleen M Sheehan</author>
</authors>
<title>Lexical tightness and text complexity.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Workshop on Natural Language Processing for Improving Textual Accessibility (PITR) held at ACL,</booktitle>
<pages>29--38</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="24962" citStr="Flor et al., 2013" startWordPosition="4024" endWordPosition="4027">Collins-Thompson and Callan, 2005; Feng, 2010). Web-based tools such as REAP5 and TextEvaluator6 are some examples of real-life applications for selecting English texts by grade level. In terms of analyzing spoken language, research in language assessment has analyzed spoken transcripts in terms of syntactic complexity (Chen and Zechner, 2011) and other textual characteristics (Crossley and McNamara, 2013). In the domain of readability assessment, the Common Core Standards (http://www. corestandards.org) guideline texts were used as a standard test set in the recent past (Nelson et al., 2012; Flor et al., 2013). This test set contains some transcribed speech. However, to the best of our knowledge, the process of selecting suitable TV programs for children as explored in this paper has not been considered as a case of readability assessment of spoken language before. Subtitle corpora have been created and used in computational linguistics for various purposes. Some of them include video classification (Katsiouli et al., 2007), machine translation (Petukhova et al., 2012), and simplification for deaf people (Daelemans et al., 2004). But, we are not aware of any such subtitle research studying the prob</context>
</contexts>
<marker>Flor, Klebanov, Sheehan, 2013</marker>
<rawString>Michael Flor, Beata Beigman Klebanov, and Kathleen M. Sheehan. 2013. Lexical tightness and text complexity. In Proceedings of the Second Workshop on Natural Language Processing for Improving Textual Accessibility (PITR) held at ACL, pages 29–38, Sofia, Bulgaria. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update. The SIGKDD Explorations,</title>
<date>2009</date>
<pages>11--10</pages>
<contexts>
<context position="9242" citStr="Hall et al., 2009" startWordPosition="1448" endWordPosition="1451">l the properties of the words and lemmas in this database can be found in the Celex English Linguistic Guide4. For both the PSYCH and CELEX features, we encode the average value for a given text. Words which were not included in the respective databases were ignored for this computation. On average, around 40% of the words from texts for covered by CELEX, 75% by Kuperman et al. (2012) and 77% by the MRC database. We do not use any features encoding the occurrence or frequency of specific words or n-grams in a document. 4 Experiments and Results 4.1 Experimental Setup We used the WEKA toolkit (Hall et al., 2009) to perform our classification experiments and evaluated the classification accuracy using 10-fold cross validation. As classification algorithm, we used the Sequential Minimal Optimization (SMO) implementation in WEKA, which marginally outperformed (1–1.5%) some other classification algorithms (J48 Decision tree, Logistic Regression and Random Forest) we tried in initial experiments. 4.2 Classification accuracy with various feature groups We discussed in the context of Table 1 that sentence length may be a good surface indicator of the age-group. So, we first constructed a classification mode</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. The SIGKDD Explorations, 11:10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Hall</author>
</authors>
<title>Correlation-based Feature Selection for Machine Learning.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>The University of Waikato,</institution>
<location>Hamilton, NewZealand.</location>
<contexts>
<context position="10663" citStr="Hall, 1999" startWordPosition="1662" endWordPosition="1663">e then constructed a model with all the features we introduced in section 3. This model achieves a classification accuracy of 95.9%, which is a 23.7% improvement over the sentence length baseline in terms of classification accuracy. In order to understand what features contribute the most to classification accuracy, we applied feature selection on the entire set, using two algorithms available in WEKA, which differ in the way they select feature subsets: • InfoGainAttributeEval evaluates the features individually based on their Information Gain (IG) with respect to the class. • CfsSubsetEval (Hall, 1999) chooses a feature subset considering the correlations between features in addition to their predictive power. Both feature selection algorithms use methods that are independent of the classification algorithm as such to select the feature subsets. Information Gain-based feature selection results in a ranked list of features, which are independent of each other. The Top-10 features according to this algorithm are listed in Table 2. Feature Group avg. AoA (Kuperman et al., 2012) PSYCH avg. # PPs in a sentence SYNTAX avg. # instances where the lemma CELEX has stem and affix – avg. parse tree hei</context>
</contexts>
<marker>Hall, 1999</marker>
<rawString>Mark A. Hall. 1999. Correlation-based Feature Selection for Machine Learning. Ph.D. thesis, The University of Waikato, Hamilton, NewZealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hancke</author>
<author>Detmar Meurers</author>
<author>Sowmya Vajjala</author>
</authors>
<title>Readability classification for german using lexical, syntactic, and morphological features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING): Technical Papers,</booktitle>
<pages>1063--1080</pages>
<location>Mumbai, India.</location>
<marker>Hancke, Meurers, Vajjala, 2012</marker>
<rawString>Julia Hancke, Detmar Meurers, and Sowmya Vajjala. 2012. Readability classification for german using lexical, syntactic, and morphological features. In Proceedings of the 24th International Conference on Computational Linguistics (COLING): Technical Papers, pages 1063–1080, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sujay Kumar Jauhar</author>
<author>Lucia Specia</author>
</authors>
<title>Uowshef: Simplex – lexical simplicity ranking based on contextual and psycholinguistic features.</title>
<date>2012</date>
<booktitle>In In proceedings of the First Joint Conference on Lexical and Computational Semantics (SEM).</booktitle>
<contexts>
<context position="7939" citStr="Jauhar and Specia, 2012" startWordPosition="1232" endWordPosition="1235">provided by Kuperman et al. (2012), including their own AoA rating obtained through crowd sourcing. It also includes measures of word familiarity, concreteness, imageability, meaningfulness and AoA as assigned in the MRC Psycholinguistic database3 (Wilson, 1988). For each feature, the value per text we computed is the average of the values for all the words in the text that had an entry in the database. While these measures were not developed with readability analysis in mind, we came across one paper using such features as measures of word difficulty in an approach to lexical simplification (Jauhar and Specia, 2012). 3http://www.psych.rl.ac.uk/ 22 Celex features (CELEX): The Celex lexical database (Baayen et al., 1995) for English consists of annotations for the morphological, syntactic, orthographic and phonological properties for more than 50k words and lemmas. We included all the morphological and syntactic properties that were encoded using character or numeric codes in our feature set. We did not use frequency information from this database. In all, this feature set consists of 35 morphological and 49 syntactic properties per lemma. The set includes: proportion of morphologically complex words, attr</context>
</contexts>
<marker>Jauhar, Specia, 2012</marker>
<rawString>Sujay Kumar Jauhar and Lucia Specia. 2012. Uowshef: Simplex – lexical simplicity ranking based on contextual and psycholinguistic features. In In proceedings of the First Joint Conference on Lexical and Computational Semantics (SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Polyxeni Katsiouli</author>
</authors>
<title>Vassileios Tsetsos, and Stathes Hadjiefthymiades.</title>
<date>2007</date>
<booktitle>In Proceedings of the 1st International Workshop on Knowledge Acquisition from Multimedia Content (KAMC).</booktitle>
<marker>Katsiouli, 2007</marker>
<rawString>Polyxeni Katsiouli, Vassileios Tsetsos, and Stathes Hadjiefthymiades. 2007. Semantic video classification based on subtitles and domain terminologies. In Proceedings of the 1st International Workshop on Knowledge Acquisition from Multimedia Content (KAMC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Keuleers</author>
<author>Paula Lacey</author>
<author>Kathleen Rastle</author>
<author>Marc Brysbaert</author>
</authors>
<title>The british lexicon project: Lexical decision data for 28,730 monosyllabic and disyllabic english words. Behavior Research Methods,</title>
<date>2012</date>
<pages>44--287</pages>
<contexts>
<context position="2988" citStr="Keuleers et al., 2012" startWordPosition="462" endWordPosition="465">uts our research into the context of related work, before section 6 concludes and provides pointers to future research directions. 2 Corpus The BBC started subtitling all the scheduled programs on its main channels in 2008, implementing UK regulations designed to help the hearing impaired. Van Heuven et al. (2014) constructed a corpus of subtitles from the programs run by nine TV channels of the BBC, collected over a period of three years, January 2010 to December 2012. They used this corpus to compile an English word frequencies database SUBTLEX-UK1, as a part of the British Lexicon Project (Keuleers et al., 2012). The subtitles of four channels (CBeebies, CBBC, BBC News and BBC Parliament) were annotated with the channel names. While CBeebies targets children aged under 6 years, CBBC telecasts programs for children 6–12 years old. The other two channels (News, Parliament) are not assigned to a specific age-group, but it seems safe to assume that they target a broader, adult audience. In sum, we used the BBC subtitle corpus with a three-way categorization: CBeebies, CBBC, Adults. Table 1 shows the basic statistics for the overall corpus. For our machine learning experiments, we use a balanced subcorpus</context>
</contexts>
<marker>Keuleers, Lacey, Rastle, Brysbaert, 2012</marker>
<rawString>Emmanuel Keuleers, Paula Lacey, Kathleen Rastle, and Marc Brysbaert. 2012. The british lexicon project: Lexical decision data for 28,730 monosyllabic and disyllabic english words. Behavior Research Methods, 44:287–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Kuperman</author>
<author>Hans Stadthagen-Gonzalez</author>
<author>Marc Brysbaert</author>
</authors>
<title>Age-of-acquisition ratings for 30,000 english words.</title>
<date>2012</date>
<journal>Behavior Research Methods,</journal>
<volume>44</volume>
<issue>4</issue>
<contexts>
<context position="7349" citStr="Kuperman et al. (2012)" startWordPosition="1137" endWordPosition="1140"> parse tree height and the average number of constituents per sub-tree. All of these features were extracted using the Berkeley Parser (Petrov and Klein, 2007) and the Tregex pattern matcher (Levy and Andrew, 2006). While the selection of features for these two classes is based on Vajjala and Meurers (2012), for the following two sets of features, we explored further information available through psycholinguistic resources. Psycholinguistic features (PSYCH): This group of features includes an encoding of the average Age-of-acquisition (AoA) of words according to different norms as provided by Kuperman et al. (2012), including their own AoA rating obtained through crowd sourcing. It also includes measures of word familiarity, concreteness, imageability, meaningfulness and AoA as assigned in the MRC Psycholinguistic database3 (Wilson, 1988). For each feature, the value per text we computed is the average of the values for all the words in the text that had an entry in the database. While these measures were not developed with readability analysis in mind, we came across one paper using such features as measures of word difficulty in an approach to lexical simplification (Jauhar and Specia, 2012). 3http://</context>
<context position="9011" citStr="Kuperman et al. (2012)" startWordPosition="1406" endWordPosition="1409">is feature set consists of 35 morphological and 49 syntactic properties per lemma. The set includes: proportion of morphologically complex words, attributive nouns, predicative adjectives, etc. in the text. A detailed description of all the properties of the words and lemmas in this database can be found in the Celex English Linguistic Guide4. For both the PSYCH and CELEX features, we encode the average value for a given text. Words which were not included in the respective databases were ignored for this computation. On average, around 40% of the words from texts for covered by CELEX, 75% by Kuperman et al. (2012) and 77% by the MRC database. We do not use any features encoding the occurrence or frequency of specific words or n-grams in a document. 4 Experiments and Results 4.1 Experimental Setup We used the WEKA toolkit (Hall et al., 2009) to perform our classification experiments and evaluated the classification accuracy using 10-fold cross validation. As classification algorithm, we used the Sequential Minimal Optimization (SMO) implementation in WEKA, which marginally outperformed (1–1.5%) some other classification algorithms (J48 Decision tree, Logistic Regression and Random Forest) we tried in in</context>
<context position="11145" citStr="Kuperman et al., 2012" startWordPosition="1735" endWordPosition="1738">tributeEval evaluates the features individually based on their Information Gain (IG) with respect to the class. • CfsSubsetEval (Hall, 1999) chooses a feature subset considering the correlations between features in addition to their predictive power. Both feature selection algorithms use methods that are independent of the classification algorithm as such to select the feature subsets. Information Gain-based feature selection results in a ranked list of features, which are independent of each other. The Top-10 features according to this algorithm are listed in Table 2. Feature Group avg. AoA (Kuperman et al., 2012) PSYCH avg. # PPs in a sentence SYNTAX avg. # instances where the lemma CELEX has stem and affix – avg. parse tree height SYNTAX – avg. # NPs in a sentence SYNTAX avg. # instances of affix substitution CELEX – avg. # prep. in a sentence LEX avg. # instances where a lemma is CELEX not a count noun avg. # clauses per sentence SYNTAX – sentence length SYNTAX Table 2: Ranked list of Top-10 features using IG As is clear from their description, all Top-10 features encode different linguistic aspects of a text. While there are more syntactic features followed by Celex features in these Top-10 feature</context>
<context position="12970" citStr="Kuperman et al. (2012)" startWordPosition="2050" endWordPosition="2053">l of 152 features, CfsSubsetEval selected a set of 41 features. Building a classification model with only these features resulted in a classification accuracy of 93.9% which is only 2% less than the model including all the features. Table 3 shows the specific feature subset selected by the CfsSubsetEval method, including # preposition phrases # t-units # co-ordinate phrases per t-unit # lexical words in total words # interjections # conjunctive phrases # word senses # verbs # verbs, past participle (VBN) # proper nouns # plural nouns avg. corrected type-token ratio avg. AoA acc. to ratings of Kuperman et al. (2012) avg. AoA acc. to ratings of Cortese and Khanna (2008) avg. word imageability rating (MRC) avg. AoA according to MRC # morph. complex words (e.g., sandbank) # morph. conversion (e.g., abandon) # morph. irrelevant (e.g., meow) # morph. obscure (e.g., dedicate) # morph. may include root (e.g., imprimatur) # foreign words (e.g., eureka) # words with multiple analyses (e.g., treasurer) # noun verb affix compounds (e.g., stockholder) # lemmas with stem and affix (e.g., abundant=abound+ant) # flectional forms (e.g., bagpipes) # clipping allomorphy (e.g., phone vs. telephone) # deriv. allomorphy (e.g</context>
<context position="27002" citStr="Kuperman et al. (2012)" startWordPosition="4336" endWordPosition="4339">text readability, proficiency in SLA, and psycholinguistic research. Using a collection of subtitle documents classified into three groups based on the targeted age-group, we explored different classification models with our feature set. The experiments showed that our linguistically motivated features perform very well, achieving a classification accuracy of 95.9% (section 4.2). Apart from the entire feature set, we also experimented with small groups of features by applying feature selection algorithms. As it turns out, the single most predictive feature was the ageof-acquisition feature of Kuperman et al. (2012), with an accuracy of 82.4%. Yet when this feature is removed from the overall feature set, the classification accuracy is not reduced, highlighting that such age-group classification is informed by a range of different characteristics, not just a single, dominating one. Authentic texts targeting specific age groups exhibit a broad range of linguistics and psycholinguistic characteristics that are indicative of the complexity of the language used. While an information gain-based feature subset consisting of 10 features resulted in an accuracy of 84.5%, a feature set chosen using the CfsSubsetE</context>
</contexts>
<marker>Kuperman, Stadthagen-Gonzalez, Brysbaert, 2012</marker>
<rawString>Victor Kuperman, Hans Stadthagen-Gonzalez, and Marc Brysbaert. 2012. Age-of-acquisition ratings for 30,000 english words. Behavior Research Methods, 44(4):978–990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Galen Andrew</author>
</authors>
<title>Tregex and tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In 5th International Conference on Language Resources and Evaluation,</booktitle>
<pages>2231--2234</pages>
<location>Genoa,</location>
<contexts>
<context position="6941" citStr="Levy and Andrew, 2006" startWordPosition="1076" endWordPosition="1079"> t-unit, # dependent clauses per clause and t-unit), coordination in a sentence (# co-ordinate phrases per clause and t-unit, # t-units/sentence), and specific syntactic structures (# complex nominals per clause and t-unit, # VP per t-unit). Other syntactic complexity features we made use of are the number of NPs, VPs, PPs, and SBARs per sentence and their average length (in terms of # words), the average parse tree height and the average number of constituents per sub-tree. All of these features were extracted using the Berkeley Parser (Petrov and Klein, 2007) and the Tregex pattern matcher (Levy and Andrew, 2006). While the selection of features for these two classes is based on Vajjala and Meurers (2012), for the following two sets of features, we explored further information available through psycholinguistic resources. Psycholinguistic features (PSYCH): This group of features includes an encoding of the average Age-of-acquisition (AoA) of words according to different norms as provided by Kuperman et al. (2012), including their own AoA rating obtained through crowd sourcing. It also includes measures of word familiarity, concreteness, imageability, meaningfulness and AoA as assigned in the MRC Psych</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>Roger Levy and Galen Andrew. 2006. Tregex and tsurgeon: tools for querying and manipulating tree data structures. In 5th International Conference on Language Resources and Evaluation, pages 2231–2234, Genoa, Italy. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei Lu</author>
</authors>
<title>Automatic analysis of syntactic complexity in second language writing.</title>
<date>2010</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>15</volume>
<issue>4</issue>
<pages>496</pages>
<contexts>
<context position="6122" citStr="Lu, 2010" startWordPosition="947" endWordPosition="948">tio of nouns, verbs, adjectives and adverbs to the number of lexical words in a document, as described in Lu (2012). The POS information required to extract these features was obtained using Stanford Tagger (Toutanova et al., 2003). The average number of senses for a non-function word was obtained by using the MIT WordNet API2 (Finlayson, 2014). Syntactic complexity features (SYNTAX): This group of features encodes the syntactic complexity of a text derived from the constituent structure of the sentences. Some of these features are 2http://projects.csail.mit.edu/jwi derived from SLA research (Lu, 2010), specifically: mean lengths of production units (sentence, clause, t-unit), sentence complexity ratio (# clauses/sentence), subordination in a sentence (# clauses per t-unit, # complex t-units per t-unit, # dependent clauses per clause and t-unit), coordination in a sentence (# co-ordinate phrases per clause and t-unit, # t-units/sentence), and specific syntactic structures (# complex nominals per clause and t-unit, # VP per t-unit). Other syntactic complexity features we made use of are the number of NPs, VPs, PPs, and SBARs per sentence and their average length (in terms of # words), the av</context>
</contexts>
<marker>Lu, 2010</marker>
<rawString>Xiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4):474– 496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei Lu</author>
</authors>
<title>The relationship of lexical richness to the quality of ESL learners’ oral narratives. The Modern Languages Journal,</title>
<date>2012</date>
<pages>190--208</pages>
<contexts>
<context position="5628" citStr="Lu (2012)" startWordPosition="874" endWordPosition="875">arch, and the average number of senses per word. Concretely, the POS tag features are: the proportion of words belonging to different parts of speech (nouns, proper nouns, pronouns, determiners, adjectives, verbs, adverbs, conjunctions, interjections, and prepositions) and different verb forms (VBG, VBD, VBN, VBP in the Penn Treebank tagset; Santorini 1990) per document. The SLA-based lexical richness features we used are: type-token ratio and corrected typetoken ratio, lexical density, ratio of nouns, verbs, adjectives and adverbs to the number of lexical words in a document, as described in Lu (2012). The POS information required to extract these features was obtained using Stanford Tagger (Toutanova et al., 2003). The average number of senses for a non-function word was obtained by using the MIT WordNet API2 (Finlayson, 2014). Syntactic complexity features (SYNTAX): This group of features encodes the syntactic complexity of a text derived from the constituent structure of the sentences. Some of these features are 2http://projects.csail.mit.edu/jwi derived from SLA research (Lu, 2010), specifically: mean lengths of production units (sentence, clause, t-unit), sentence complexity ratio (# </context>
</contexts>
<marker>Lu, 2012</marker>
<rawString>Xiaofei Lu. 2012. The relationship of lexical richness to the quality of ESL learners’ oral narratives. The Modern Languages Journal, pages 190–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jessica Nelson</author>
<author>Charles Perfetti</author>
<author>David Liben</author>
<author>Meredith Liben</author>
</authors>
<title>Measures of text difficulty: Testing their predictive value for grade levels and student performance. Technical report, The Council of Chief State School Officers.</title>
<date>2012</date>
<contexts>
<context position="24942" citStr="Nelson et al., 2012" startWordPosition="4019" endWordPosition="4023">ng approaches (e.g., Collins-Thompson and Callan, 2005; Feng, 2010). Web-based tools such as REAP5 and TextEvaluator6 are some examples of real-life applications for selecting English texts by grade level. In terms of analyzing spoken language, research in language assessment has analyzed spoken transcripts in terms of syntactic complexity (Chen and Zechner, 2011) and other textual characteristics (Crossley and McNamara, 2013). In the domain of readability assessment, the Common Core Standards (http://www. corestandards.org) guideline texts were used as a standard test set in the recent past (Nelson et al., 2012; Flor et al., 2013). This test set contains some transcribed speech. However, to the best of our knowledge, the process of selecting suitable TV programs for children as explored in this paper has not been considered as a case of readability assessment of spoken language before. Subtitle corpora have been created and used in computational linguistics for various purposes. Some of them include video classification (Katsiouli et al., 2007), machine translation (Petukhova et al., 2012), and simplification for deaf people (Daelemans et al., 2004). But, we are not aware of any such subtitle resear</context>
</contexts>
<marker>Nelson, Perfetti, Liben, Liben, 2012</marker>
<rawString>Jessica Nelson, Charles Perfetti, David Liben, and Meredith Liben. 2012. Measures of text difficulty: Testing their predictive value for grade levels and student performance. Technical report, The Council of Chief State School Officers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="6886" citStr="Petrov and Klein, 2007" startWordPosition="1067" endWordPosition="1070"> a sentence (# clauses per t-unit, # complex t-units per t-unit, # dependent clauses per clause and t-unit), coordination in a sentence (# co-ordinate phrases per clause and t-unit, # t-units/sentence), and specific syntactic structures (# complex nominals per clause and t-unit, # VP per t-unit). Other syntactic complexity features we made use of are the number of NPs, VPs, PPs, and SBARs per sentence and their average length (in terms of # words), the average parse tree height and the average number of constituents per sub-tree. All of these features were extracted using the Berkeley Parser (Petrov and Klein, 2007) and the Tregex pattern matcher (Levy and Andrew, 2006). While the selection of features for these two classes is based on Vajjala and Meurers (2012), for the following two sets of features, we explored further information available through psycholinguistic resources. Psycholinguistic features (PSYCH): This group of features includes an encoding of the average Age-of-acquisition (AoA) of words according to different norms as provided by Kuperman et al. (2012), including their own AoA rating obtained through crowd sourcing. It also includes measures of word familiarity, concreteness, imageabili</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Volha Petukhova</author>
<author>Rodrigo Agerri</author>
<author>Mark Fishel</author>
</authors>
<title>Yota Georgakopoulou, Sergio Penkale, Arantza del Pozo, Mirjam Sepesy Maucec,</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012),</booktitle>
<pages>21--28</pages>
<location>Martin</location>
<contexts>
<context position="25430" citStr="Petukhova et al., 2012" startWordPosition="4098" endWordPosition="4101">Core Standards (http://www. corestandards.org) guideline texts were used as a standard test set in the recent past (Nelson et al., 2012; Flor et al., 2013). This test set contains some transcribed speech. However, to the best of our knowledge, the process of selecting suitable TV programs for children as explored in this paper has not been considered as a case of readability assessment of spoken language before. Subtitle corpora have been created and used in computational linguistics for various purposes. Some of them include video classification (Katsiouli et al., 2007), machine translation (Petukhova et al., 2012), and simplification for deaf people (Daelemans et al., 2004). But, we are not aware of any such subtitle research studying the problem of automatically identifying TV programs for various age-groups. This paper thus can be seen as connecting several threads of research, from the analysis of text complexity and readability, via the research on measuring SLA proficiency that many of the linguistic features we used stem from, to the computational analysis of speech as encoded in subtitles. The range of linguistic characteristics which turn out to be relevant and the very high precision with whic</context>
</contexts>
<marker>Petukhova, Agerri, Fishel, 2012</marker>
<rawString>Volha Petukhova, Rodrigo Agerri, Mark Fishel, Yota Georgakopoulou, Sergio Penkale, Arantza del Pozo, Mirjam Sepesy Maucec, Martin Volk, and Andy Way. 2012. Sumat: Data collection and parallel corpus compilation for machine translation of subtitles. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012), pages 21–28, Istanbul, Turkey. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Santorini</author>
</authors>
<title>Part-of-speech tagging guidelines for the Penn Treebank, 3rd revision, 2nd printing.</title>
<date>1990</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, University of Pennsylvania.</institution>
<contexts>
<context position="5378" citStr="Santorini 1990" startWordPosition="834" endWordPosition="835">rch on text complexity in Second Language Acquisition (SLA) and Psycholinguistics. There are four types of features: Lexical richness features (LEX): This group consists of various part-of-speech (POS) tag densities, lexical richness features from SLA research, and the average number of senses per word. Concretely, the POS tag features are: the proportion of words belonging to different parts of speech (nouns, proper nouns, pronouns, determiners, adjectives, verbs, adverbs, conjunctions, interjections, and prepositions) and different verb forms (VBG, VBD, VBN, VBP in the Penn Treebank tagset; Santorini 1990) per document. The SLA-based lexical richness features we used are: type-token ratio and corrected typetoken ratio, lexical density, ratio of nouns, verbs, adjectives and adverbs to the number of lexical words in a document, as described in Lu (2012). The POS information required to extract these features was obtained using Stanford Tagger (Toutanova et al., 2003). The average number of senses for a non-function word was obtained by using the MIT WordNet API2 (Finlayson, 2014). Syntactic complexity features (SYNTAX): This group of features encodes the syntactic complexity of a text derived fro</context>
</contexts>
<marker>Santorini, 1990</marker>
<rawString>Beatrice Santorini. 1990. Part-of-speech tagging guidelines for the Penn Treebank, 3rd revision, 2nd printing. Technical report, Department of Computer Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich partofspeech tagging with a cyclic dependency network. In</title>
<date>2003</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>252--259</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="5744" citStr="Toutanova et al., 2003" startWordPosition="889" endWordPosition="892">of words belonging to different parts of speech (nouns, proper nouns, pronouns, determiners, adjectives, verbs, adverbs, conjunctions, interjections, and prepositions) and different verb forms (VBG, VBD, VBN, VBP in the Penn Treebank tagset; Santorini 1990) per document. The SLA-based lexical richness features we used are: type-token ratio and corrected typetoken ratio, lexical density, ratio of nouns, verbs, adjectives and adverbs to the number of lexical words in a document, as described in Lu (2012). The POS information required to extract these features was obtained using Stanford Tagger (Toutanova et al., 2003). The average number of senses for a non-function word was obtained by using the MIT WordNet API2 (Finlayson, 2014). Syntactic complexity features (SYNTAX): This group of features encodes the syntactic complexity of a text derived from the constituent structure of the sentences. Some of these features are 2http://projects.csail.mit.edu/jwi derived from SLA research (Lu, 2010), specifically: mean lengths of production units (sentence, clause, t-unit), sentence complexity ratio (# clauses/sentence), subordination in a sentence (# clauses per t-unit, # complex t-units per t-unit, # dependent clau</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich partofspeech tagging with a cyclic dependency network. In HLT-NAACL, pages 252–259, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sowmya Vajjala</author>
<author>Detmar Meurers</author>
</authors>
<title>On improving the accuracy of readability classification using insights from second language acquisition. In</title>
<date>2012</date>
<booktitle>In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications (BEA) atNAACL-HLT,</booktitle>
<pages>163--173</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="7035" citStr="Vajjala and Meurers (2012)" startWordPosition="1092" endWordPosition="1095">nate phrases per clause and t-unit, # t-units/sentence), and specific syntactic structures (# complex nominals per clause and t-unit, # VP per t-unit). Other syntactic complexity features we made use of are the number of NPs, VPs, PPs, and SBARs per sentence and their average length (in terms of # words), the average parse tree height and the average number of constituents per sub-tree. All of these features were extracted using the Berkeley Parser (Petrov and Klein, 2007) and the Tregex pattern matcher (Levy and Andrew, 2006). While the selection of features for these two classes is based on Vajjala and Meurers (2012), for the following two sets of features, we explored further information available through psycholinguistic resources. Psycholinguistic features (PSYCH): This group of features includes an encoding of the average Age-of-acquisition (AoA) of words according to different norms as provided by Kuperman et al. (2012), including their own AoA rating obtained through crowd sourcing. It also includes measures of word familiarity, concreteness, imageability, meaningfulness and AoA as assigned in the MRC Psycholinguistic database3 (Wilson, 1988). For each feature, the value per text we computed is the </context>
</contexts>
<marker>Vajjala, Meurers, 2012</marker>
<rawString>Sowmya Vajjala and Detmar Meurers. 2012. On improving the accuracy of readability classification using insights from second language acquisition. In In Proceedings of the 7th Workshop on Innovative Use of NLP for Building Educational Applications (BEA) atNAACL-HLT, pages 163—-173, Montr´eal, Canada. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sowmya Vajjala</author>
<author>Detmar Meurers</author>
</authors>
<title>On the applicability of readability models to web texts.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Workshop on Natural Language Processing for Improving Textual Accessibility (PITR) held at ACL,</booktitle>
<pages>59--68</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="29394" citStr="Vajjala and Meurers, 2013" startWordPosition="4719" endWordPosition="4722">f a model using the same feature set, we found that the approach performs well for a range of corpora composed of reading materials for language learners (Vajjala and Meurers, 2014b). The very high classification accuracies of the experiments we presented in the current paper thus seem to support the assumption that the approach can be useful in practice for automatically identifying TV programs for viewers of different age groups. Regarding the three class distinctions and the classifier setup we used in this paper, the approach can also be generalized to other scales and a regression setup (Vajjala and Meurers, 2013). 6.1 Outlook The current work focused mostly on modeling and studying different feature groups in terms of their classification accuracy. Performing error analysis and looking at the texts where the approach failed may yield further insights into the problem. Some aspects of the text that we did not consider include discourse coherence or topic effects. Studying these two aspects can provide more insights into the nature of the language used in TV programs directed at viewers of different ages. A cross-genre evaluation between written and spoken language complexity across age-groups could als</context>
</contexts>
<marker>Vajjala, Meurers, 2013</marker>
<rawString>Sowmya Vajjala and Detmar Meurers. 2013. On the applicability of readability models to web texts. In Proceedings of the Second Workshop on Natural Language Processing for Improving Textual Accessibility (PITR) held at ACL, pages 59—-68, Sofia, Bulgaria. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sowmya Vajjala</author>
<author>Detmar Meurers</author>
</authors>
<title>Assessing the relative reading level of sentence pairs for text simplification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<publisher>ACL.</publisher>
<contexts>
<context position="23960" citStr="Vajjala and Meurers (2014" startWordPosition="3878" endWordPosition="3881">and reaches the best overall accuracy of almost 96% for texts which are 900 words in length. All the feature subsets too follow the same trend, with varying degrees of accuracy that is always lower than the model with all features. While in this paper, we focus on documents, the issue whether the data can be reduced further classification accuracy (in percent) 96 95 94 93 92 91 90 25% text size 50% text size 75% text size 100% text size classification accuracy (in percent) 100 95 90 85 80 75 70 65 All Features PSYCH LEX SYN CELEX 26 to perform readability at the sentence level is discussed in Vajjala and Meurers (2014a). artificially shortened transcripts, confirm the usefulness of connecting these research threads. 5 Related Work Analyzing the complexity of written texts and choosing suitable texts for various target groups including children is widely studied in computational linguistics. Some of the popular approaches include the use of language models and machine learning approaches (e.g., Collins-Thompson and Callan, 2005; Feng, 2010). Web-based tools such as REAP5 and TextEvaluator6 are some examples of real-life applications for selecting English texts by grade level. In terms of analyzing spoken la</context>
<context position="28948" citStr="Vajjala and Meurers, 2014" startWordPosition="4647" endWordPosition="4650">an individual feature, clearly influences classification accuracy by providing more information for model building and testing. 27 In terms of the practical relevance of the results, one question that needs some attention is how well the features and trained models generalize across different type of TV programs or languages. While we have not yet investigated this for TV subtitles, in experiments investigating the cross-corpus performance of a model using the same feature set, we found that the approach performs well for a range of corpora composed of reading materials for language learners (Vajjala and Meurers, 2014b). The very high classification accuracies of the experiments we presented in the current paper thus seem to support the assumption that the approach can be useful in practice for automatically identifying TV programs for viewers of different age groups. Regarding the three class distinctions and the classifier setup we used in this paper, the approach can also be generalized to other scales and a regression setup (Vajjala and Meurers, 2013). 6.1 Outlook The current work focused mostly on modeling and studying different feature groups in terms of their classification accuracy. Performing erro</context>
</contexts>
<marker>Vajjala, Meurers, 2014</marker>
<rawString>Sowmya Vajjala and Detmar Meurers. 2014a. Assessing the relative reading level of sentence pairs for text simplification. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL). ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sowmya Vajjala</author>
<author>Detmar Meurers</author>
</authors>
<title>Readability assessment for text simplification: From analyzing documents to identifying sentential simplifications.</title>
<date>2014</date>
<journal>International Journal of Applied Linguistics, Special Issue</journal>
<note>on Current Research in Readability and Text Simplification, edited by</note>
<contexts>
<context position="23960" citStr="Vajjala and Meurers (2014" startWordPosition="3878" endWordPosition="3881">and reaches the best overall accuracy of almost 96% for texts which are 900 words in length. All the feature subsets too follow the same trend, with varying degrees of accuracy that is always lower than the model with all features. While in this paper, we focus on documents, the issue whether the data can be reduced further classification accuracy (in percent) 96 95 94 93 92 91 90 25% text size 50% text size 75% text size 100% text size classification accuracy (in percent) 100 95 90 85 80 75 70 65 All Features PSYCH LEX SYN CELEX 26 to perform readability at the sentence level is discussed in Vajjala and Meurers (2014a). artificially shortened transcripts, confirm the usefulness of connecting these research threads. 5 Related Work Analyzing the complexity of written texts and choosing suitable texts for various target groups including children is widely studied in computational linguistics. Some of the popular approaches include the use of language models and machine learning approaches (e.g., Collins-Thompson and Callan, 2005; Feng, 2010). Web-based tools such as REAP5 and TextEvaluator6 are some examples of real-life applications for selecting English texts by grade level. In terms of analyzing spoken la</context>
<context position="28948" citStr="Vajjala and Meurers, 2014" startWordPosition="4647" endWordPosition="4650">an individual feature, clearly influences classification accuracy by providing more information for model building and testing. 27 In terms of the practical relevance of the results, one question that needs some attention is how well the features and trained models generalize across different type of TV programs or languages. While we have not yet investigated this for TV subtitles, in experiments investigating the cross-corpus performance of a model using the same feature set, we found that the approach performs well for a range of corpora composed of reading materials for language learners (Vajjala and Meurers, 2014b). The very high classification accuracies of the experiments we presented in the current paper thus seem to support the assumption that the approach can be useful in practice for automatically identifying TV programs for viewers of different age groups. Regarding the three class distinctions and the classifier setup we used in this paper, the approach can also be generalized to other scales and a regression setup (Vajjala and Meurers, 2013). 6.1 Outlook The current work focused mostly on modeling and studying different feature groups in terms of their classification accuracy. Performing erro</context>
</contexts>
<marker>Vajjala, Meurers, 2014</marker>
<rawString>Sowmya Vajjala and Detmar Meurers. 2014b. Readability assessment for text simplification: From analyzing documents to identifying sentential simplifications. International Journal of Applied Linguistics, Special Issue on Current Research in Readability and Text Simplification, edited by Thomas Franc¸ois and Delphine Bernhard.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter J B Van Heuven</author>
<author>Pawel Mandera</author>
<author>Emmanuel Keuleers</author>
<author>Marc Brysbaert</author>
</authors>
<title>Subtlex-UK: A new and improved word frequency database for British English. The Quarterly Journal of Experimental Psychology,</title>
<date>2014</date>
<pages>1--15</pages>
<marker>Van Heuven, Mandera, Keuleers, Brysbaert, 2014</marker>
<rawString>Walter J.B. Van Heuven, Pawel Mandera, Emmanuel Keuleers, and Marc Brysbaert. 2014. Subtlex-UK: A new and improved word frequency database for British English. The Quarterly Journal of Experimental Psychology, pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael D Wilson</author>
</authors>
<title>The mrc psycholinguistic database: Machine readable dictionary, version 2.</title>
<date>1988</date>
<journal>Behavioural Research Methods, Instruments and Computers,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="7577" citStr="Wilson, 1988" startWordPosition="1171" endWordPosition="1172">f features for these two classes is based on Vajjala and Meurers (2012), for the following two sets of features, we explored further information available through psycholinguistic resources. Psycholinguistic features (PSYCH): This group of features includes an encoding of the average Age-of-acquisition (AoA) of words according to different norms as provided by Kuperman et al. (2012), including their own AoA rating obtained through crowd sourcing. It also includes measures of word familiarity, concreteness, imageability, meaningfulness and AoA as assigned in the MRC Psycholinguistic database3 (Wilson, 1988). For each feature, the value per text we computed is the average of the values for all the words in the text that had an entry in the database. While these measures were not developed with readability analysis in mind, we came across one paper using such features as measures of word difficulty in an approach to lexical simplification (Jauhar and Specia, 2012). 3http://www.psych.rl.ac.uk/ 22 Celex features (CELEX): The Celex lexical database (Baayen et al., 1995) for English consists of annotations for the morphological, syntactic, orthographic and phonological properties for more than 50k wor</context>
</contexts>
<marker>Wilson, 1988</marker>
<rawString>Michael D. Wilson. 1988. The mrc psycholinguistic database: Machine readable dictionary, version 2. Behavioural Research Methods, Instruments and Computers, 20(1):6–11.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.102381">
<title confidence="0.985517">
Classifying Negative Findings in Biomedical Publications
</title>
<author confidence="0.999132">
Bei Yu Daniele Fanelli
</author>
<affiliation confidence="0.9997015">
School of Information Studies School of Library and Information Science
Syracuse University University of Montreal
</affiliation>
<email confidence="0.999209">
byu@syr.edu email@danielefanelli.com
</email>
<sectionHeader confidence="0.993906" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999700666666667">
Publication bias refers to the phenome-
non that statistically significant, “posi-
tive” results are more likely to be pub-
lished than non-significant, “negative”
results. Currently, researchers have to
manually identify negative results in a
large number of publications in order to
examine publication biases. This paper
proposes an NLP approach for automati-
cally classifying negated sentences in bi-
omedical abstracts as either reporting
negative findings or not. Using multino-
mial naïve Bayes algorithm and bag-of-
words features enriched by parts-of-
speeches and constituents, we built a
classifier that reached 84% accuracy
based on 5-fold cross validation on a bal-
anced data set.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999909916666667">
Publication bias refers to the phenomenon that
statistically significant, “positive” results are
more likely to be published than non-significant,
“negative” results (Estabrook et al., 1991). Due
to the “file-drawer” effect (Rosenthal, 1979),
negative results are more likely to be “filed
away” privately than to be published publicly.
Publication bias poses challenge for an accu-
rate review of current research progress. It
threatens the quality of meta-analyses and sys-
tematic reviews that rely on published research
results (e.g., the Cochrane Review). Publication
bias may be further spread through citation net-
work, and amplified by citation bias, a phenome-
non that positive results are more likely to be
cited than negative results (Greenberg, 2009).
To address the publication bias problem, some
new journals were launched and dedicated to
publishing negative results, such as the Journal
of Negative Results in Biomedicine, Journal of
Pharmaceutical Negative Results, Journal of
Negative Results in Ecology and Evolutionary
Biology, and All Results Journals: Chem. Some
quantitative methods like the funnel plot (Egger
et al., 1997) were used to measure publication
bias in publications retrieved for a certain topic.
A key step in such manual analysis is to exam-
ine the article abstracts or full-texts to see wheth-
er the findings are negative or not. For example,
Hebert et al. (2002) examined the full text of
1,038 biomedical articles whose primary out-
comes were hypothesis testing results, and found
234 (23%) negative articles. Apparently, such
manual analysis approach is time consuming. An
accurate, automated classifier would be ideal to
actively track positive and negative publications.
This paper proposes an NLP approach for au-
tomatically identifying negative results in bio-
medical abstracts. Because one publication may
have multiple findings, we currently focus on
classifying negative findings at sentence level:
for a sentence that contains the negation cues
“no” and/or “not”, we predict whether the sen-
tence reported negative finding or not. We con-
structed a training data set using manual annota-
tion and convenience samples. Two widely-used
text classification algorithms, Multinomial naive
Bayes (MNB) and Support Vector Machines
(SVM), were compared in this study. A few text
representation approaches were also compared
by their effectiveness in building the classifier.
The approaches include (1) bag-of-words
(BOW), (2) BOW with PoS tagging and shallow
parsing, and (3) local contexts of the negation
cues “no” and ‘not”, including the words, PoS
tags, and constituents. The best classifier was
built using MNB and bag-of-words features en-
riched with PoS tags and constituent markers.
The best performance is 84% accuracy based on
5-fold cross validation on a balanced data set.
</bodyText>
<page confidence="0.991151">
19
</page>
<note confidence="0.3478595">
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 19–23,
Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.99861" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999989825">
The problem of identifying negative results is
related to several other BioNLP problems, espe-
cially on negation and scientific claim identifica-
tion.
The first relevant task is to identify negation
signals and their scopes (e.g., Morante and
Daelemans, 2008;2009; Farkas et al., 2010;
Agarwal et al., 2011). Manually-annotated cor-
pora like BioScope (Szarvas et al., 2008) were
created to annotate negations and their scopes in
biomedical abstracts in support of automated
identification. This task targets a wide range of
negation types, such as the presence or absence
of clinical observations in narrative clinical re-
ports (Chapman et al., 2001). In comparison, our
task focuses on identifying negative findings on-
ly. Although not all negations report negative
results, negation signals are important rhetorical
device for authors to make negative claims.
Therefore, in this study we also examine preci-
sion and recall of using negation signals as pre-
dictors of negative findings.
The second relevant task is to identify the
strength and types of scientific claims. Light et
al. (2004) developed a classifier to predict the
level of speculations in sentences in biomedical
abstracts. Blake (2010) proposed a “claim
framework” that differentiates explicit claims,
observations, correlations, comparisons, and im-
plicit claims, based on the certainty of the causal
relationship that was presented. Blake also found
that abstracts contained only 7.84% of all scien-
tific claims, indicating the need for full-text
analysis. Currently, our preliminary study exam-
ines abstracts only, assuming the most important
findings are reported there. We also focus on
coarse-grained classification of positive vs. nega-
tive findings at this stage, and leave for future
work the task of differentiating negative claims
in finer-granularity.
</bodyText>
<sectionHeader confidence="0.997949" genericHeader="method">
3 The NLP approach
</sectionHeader>
<subsectionHeader confidence="0.99946">
3.1 The definition of negative results
</subsectionHeader>
<bodyText confidence="0.999982642857143">
When deciding what kinds of results count as
“negative”, some prior studies used “non-
significant” results as an equivalent for “negative
results” (e.g. Hebert et al., 2002; Fanelli, 2012).
However, in practice, the definition of “negative
results” is actually broader. For example, the
Journal of Negative Results in Biomedicine
(JNRBM), launched in 2002, was devoted to
publishing “unexpected, controversial, provoca-
tive and/or negative results,” according to the
journal’s website. This broader definition has its
pros and cons. The added ambiguity poses chal-
lenge for manual and automated identification.
At the same time, the broader definition allows
the inclusion of descriptive studies, such as the
first JNRBM article (Hebert et al., 2002).
Interestingly, Hebert et al. (2002) defined
“negative results” as “non-significant outcomes”
and drew a negative conclusion that “prominent
medical journals often provide insufficient in-
formation to assess the validity of studies with
negative results”, based on descriptive statistics,
not hypothesis testing. This finding would not be
counted as “negative” unless the broader defini-
tion is adopted.
In our study, we utilized the JNRBM articles
as a convenience sample of negative results, and
thus inherit its broader definition.
</bodyText>
<subsectionHeader confidence="0.8453525">
3.2 The effectiveness of negation cues as
predictors
</subsectionHeader>
<bodyText confidence="0.999750870967742">
The Bioscope corpus marked a number of nega-
tion cues in the abstracts of research articles,
such as “not”, “no”, “without”, etc. It is so far the
most comprehensive negation cue collection we
can find for biomedical publications. However,
some challenges arise when applying these nega-
tion cues to the task of identifying negative re-
sults.
First, instead of focusing on negative results,
the Bioscope corpus was annotated with cues
expressing general negation and speculations.
Consequently, some negation cues such as “un-
likely” was annotated as a speculation cue, not a
negation cue, although “unlikely” was used to
report negative results like
“These data indicate that changes in Wnt ex-
pression per se are unlikely to be the cause of
the observed dysregulation of β-catenin ex-
pression in DD” (PMC1564412).
Therefore, the Bioscope negation cues may
not have captured all negation cues for reporting
negative findings. To test this hypothesis, we
used the JNRBM abstracts (N=90) as a conven-
ience sample of negative results, and found that
81 abstracts (88.9%) contain at least one Bio-
scope negation cue. Note that because the
JNRBM abstracts consist of multiple subsections
“background”, “objective”, “method”, “result”,
and “conclusion”, we used the “result” and “con-
clusions” subsections only to narrow down the
search range.
</bodyText>
<page confidence="0.973749">
20
</page>
<bodyText confidence="0.999199481481481">
Among the 9 missed abstracts, 5 used cues not
captured in Bioscope negation cues: “insuffi-
cient”, “unlikely”, “setbacks”, “worsening”, and
“underestimates”. However, the authors’ writing
style might be affected by the fact that JNRBM
is dedicated to negative results. One hypothesis
is that the authors would feel less pressure to use
negative tones, and thus used more variety of
negation words. Hence we leave it as an open
question whether the new-found negation cues
and their synonyms are generalizable to other
biomedical journal articles.
The rest 4 abstracts (PMC 1863432, 1865554,
1839113, and 2746800) did not report explicit
negation results, indicating that sometimes ab-
stracts alone are not enough to decide whether
negative results were reported, although the per-
centage is relatively low (4.4%). Hence, we de-
cided that missing target is not a major concern
for our task, and thus would classify a research
finding as positive if no negation cues were
found.
Second, some positive research results may be
mistaken as negative just because they used ne-
gation cues. For example, “without” is marked as
a negation cue in Bioscope, but it can be used in
many contexts that do not indicate negative re-
sults, such as
“The effects are consistent with or without the
presence of hypertension and other comorbidi-
ties and across a range of drug classes.”
(PMC2659734)
To measure the percentage of false alarm, we
applied the aforementioned trivial classifier to a
corpus of 600 abstracts in 4 biomedical disci-
plines, which were manually annotated by Fan-
elli (2012). This corpus will be referred to as
“Corpus-600” hereafter. Each abstract is marked
as “positive”, “negative”, “partially positive”, or
“n/a”, based on hypothesis testing results. The
latter two types were excluded in our study. The
trivial classifier predicted an abstract as “posi-
tive” if no negation cues were found. Table 1
reported the prediction results, including the pre-
cision and recall in identifying negative results.
This result corroborates with our previous find-
ing that the inclusiveness of negation cues is not
the major problem since high recalls have been
observed in both experiments. However, the low
precision is the major problem in that the false
negative predictions are far more than the true
negative predictions. Hence, weeding out the
negations that did not report negative results be-
came the main purpose of this preliminary study.
</bodyText>
<table confidence="0.999625571428571">
Discipline #abstracts Precision Recall
Psychiatry 140 .11 .92
Clinical 127 .16 .94
Medicine
Neuroscience 144 .20 .95
Immunology 140 .18 .95
Total 551 .16 .94
</table>
<tableCaption confidence="0.999929">
Table 1: results of cue-based trivial classifier
</tableCaption>
<subsectionHeader confidence="0.989945">
3.3 Classification task definition
</subsectionHeader>
<bodyText confidence="0.999991346153846">
This preliminary study focuses on separating
negations that reported negative results and those
not. We limit our study to abstracts at this time.
Because a paper may report multiple findings,
we performed the prediction at sentence level,
and leave for future work the task of aggregating
sentence-level predictions to abstract-level or
article-level. By this definition, we will classify
each sentence as reporting negative finding or
not. A sentence that includes mixed findings will
be categorized as reporting negative finding.
“Not” and “no” are the most frequent negation
cues in the Bioscope corpus, accounting for more
than 85% of all occurrences of negation cues. In
this study we also examined whether local con-
text, such as the words, parts-of-speeches, and
constituents surrounding the negation cues,
would be useful for predicting negative findings.
Considering that different negation cues may be
used in different contexts to report negative find-
ings, we built a classifier based on the local con-
texts of “no” and “not”. Contexts for other nega-
tion cues will be studied in the future.
Therefore, our goal is to extract sentences con-
taining “no” or “not” from abstracts, and predict
whether they report negative findings or not.
</bodyText>
<subsectionHeader confidence="0.995496">
3.4 Training data
</subsectionHeader>
<bodyText confidence="0.9998586">
We obtained a set of “positive examples”,
which are negative-finding sentences, and a set
of “negative examples” that did not report nega-
tive findings. The examples were obtained in the
following way.
Positive examples. These are sentences that
used “no” or “not” to report negative findings.
We extracted all sentences that contain “no” or
“not” in JNRBM abstracts, and manually marked
each sentence as reporting negative findings or
</bodyText>
<page confidence="0.998063">
21
</page>
<bodyText confidence="0.999264681818182">
not. Finally we obtained 158 sentences reporting
negative findings.
To increase the number of negative-finding
examples and add variety to writing styles, we
repeat the above annotations to all Lancet ab-
stracts (“result” and “finding” subsections only)
in the PubMed Central open access subset, and
obtained 55 more such sentences. Now we have
obtained 213 negative-finding examples in total.
Negative examples. To reduce the workload
for manual labeling, we utilized the heuristic rule
that a “no” or “not” does not report negative re-
sult if it occurs in a positive abstract, therefore
we extracted such sentences from positive ab-
stracts in “Corpus-600”. These are the negative
examples we will use. To balance the number of
positive and negative examples, we used a total
of 231 negative examples in two domains (132 in
clinical medicine and 99 in neuroscience) instead
of all four domains, because there are not enough
positive examples.
Now the training data is ready for use.
</bodyText>
<subsectionHeader confidence="0.976627">
3.5 Feature extraction
</subsectionHeader>
<bodyText confidence="0.999910181818182">
We compared three text representation methods
by their effectiveness in building the classifier.
The approaches are (1) BOW: simple bag-of-
words, (2) E-BOW: bag-of-words enriched with
PoS tagging and shallow parsing, and (3) LCE-
BOW: local contexts of the negation cues “no”
and ‘not”, including the words, PoS tags, and
constituents. For (2) and (3), we ran the
OpenNLP chunker through all sentences in the
training data. For (3), we extracted the following
features for each sentence:
</bodyText>
<listItem confidence="0.996236333333333">
• The type of chunk (constituent) where
“no/not” is in (e.g. verb phrase “VP”);
• The types of two chunks before and after the
chunk where “not” is in;
• All words or punctuations in these chunks;
• The parts-of-speech of all these words.
</listItem>
<bodyText confidence="0.997788375">
See Table 2 below for an example of negative
finding: row 1 is the original sentence; row 2 is
the chunked sentence, and row 3 is the extracted
local context of the negation cue “not”. These
three representations were then converted to fea-
ture vectors using the “bag-of-words” representa-
tion. To reduce vocabulary size, we removed
words that occurred only once.
</bodyText>
<table confidence="0.9993183">
Vascular mortality did not differ signifi-
cantly (0.19% vs 0.19% per year, p=0.7).
&amp;quot;[NP Vascular/JJ mortality/NN ] [VP
did/VBD not/RB differ/VB ] [ADVP sig-
nificantly/RB ] [PP (/-LRB- ] [NP 019/CD
%/NN ] [PP vs/IN ] [NP 019/CD %/NN ]
[PP per/IN ] [NP year/NN ] ,/, [NP
p=07/NNS ] [VP )/-RRB- ] ./.&amp;quot;
“na na VP ADVP PP did not differ signif-
icantly VBD RB VB RB”
</table>
<tableCaption confidence="0.995307">
Table 2: text representations
</tableCaption>
<subsectionHeader confidence="0.988761">
3.6 Classification result
</subsectionHeader>
<bodyText confidence="0.99815775">
We applied two supervised learning algorithms,
multinomial naïve Bayes (MNB), and Support
Vector Machines (Liblinear) to the unigram fea-
ture vectors. We used the Sci-kit Learn toolkit to
carry out the experiment, and compared the algo-
rithms’ performance using 5-fold cross valida-
tion. All algorithms were set to the default pa-
rameter setting.
</bodyText>
<table confidence="0.996830666666667">
Representation MNB SVM
Presence BOW .82 .79
vs.
absence
E-BOW .82 .79
LCE-BOW .72 .72
tf BOW .82 .79
E-BOW .84 .79
LCE-BOW .72 .72
Tfidf BOW .82 .75
E-BOW .84 .73
LCE-BOW .72 .75
</table>
<tableCaption confidence="0.999799">
Table 3: classification accuracy
</tableCaption>
<bodyText confidence="0.9961953125">
Table 3 reports the classification accuracy. Be-
cause the data set contains 213 positive and 231
negative examples, the majority vote baseline is
.52. Both algorithms combined with any text rep-
resentation methods outperformed the majority
baseline significantly. Among them the best clas-
sifier is a MNB classifier based on enriched bag-
of-words representation and tfidf weighting. Alt-
hough LCE-BOW reached as high as .75 accura-
cy using SVM and tfidf weighting, it did not per-
form as well as the other text representation
methods, indicating that the local context with
+/- 2 window did not capture all relevant indica-
tors for negative findings.
Tuning the regularization parameter C in
SVM did not improve the accuracy. Adding bi-
</bodyText>
<page confidence="0.986713">
22
</page>
<bodyText confidence="0.9944915">
grams to the feature set resulted in slightly lower
accuracy.
</bodyText>
<sectionHeader confidence="0.998655" genericHeader="method">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999964705882353">
In this study we aimed for building a classifier to
predict whether a sentence containing the words
“no” or “not” reported negative findings. Built
with MNB algorithms and enriched bag-of-
words features with tfidf weighting, the best
classifier reached .84 accuracy on a balanced
data set.
This preliminary study shows promising re-
sults for automatically identifying negative find-
ings for the purpose of tracking publication bias.
To reach this goal, we will have to aggregate the
sentence-level predictions on individual findings
to abstract- or article-level negative results. The
aggregation strategy is dependent on the decision
of which finding is the primary outcome when
multiple findings are present. We leave this as
our future work.
</bodyText>
<sectionHeader confidence="0.983222" genericHeader="method">
Reference
</sectionHeader>
<reference confidence="0.999218210526316">
S. Agarwal, H. Yu, and I. Kohane, I. 2011. BioNOT:
A searchable database of biomedical negated sen-
tences. BMC bioinformatics, 12: 420.
C. Blake. 2010. Beyond genes, proteins, and ab-
stracts: Identifying scientific claims from full-text
biomedical articles. Journal of biomedical infor-
matics, 43(2): 173-189.
W. W. Chapman, W. Bridewell, P. Hanbury, G. F.
Cooper, and B. G. Buchanan. 2001. Evaluation of
negation phrases in narrative clinical reports. Pro-
ceedings of the AMIA Symposium, 105.
P. J. Easterbrook, R. Gopalan, J. A. Berlin, and D. R.
Matthews. 1991. Publication bias in clinical re-
search. Lancet, 337(8746): 867-872.
M. Egger, G. D. Smith, M. Schneider, and C. Minder.
1997. Bias in meta-analysis detected by a simple,
graphical test. BMJ 315(7109): 629-634.
D. Fanelli. 2012. Negative results are disappearing
from most disciplines and coun-
tries. Scientometrics 90(3): 891-904.
R. Farkas, V. Vincze, G. Móra, J. Csirik, and G. Szar-
vas. 2010. The CoNLL-2010 shared task: learning
to detect hedges and their scope in natural language
text. Proceedings of the Fourteenth Conference on
Computational Natural Language Learning---
Shared Task, 1-12.
S. A. Greenberg. 2009. How citation distortions create
unfounded authority: analysis of a citation net-
work. BMJ 339, b2680.
R. S. Hebert, S. M. Wright, R. S. Dittus, and T. A.
Elasy. 2002. Prominent medical journals often pro-
vide insufficient information to assess the validity
of studies with negative results. Journal of Nega-
tive Results in Biomedicine 1(1):1.
M. Light, X-Y Qiu, and P. Srinivasan. 2004. The lan-
guage of bioscience: Facts, speculations, and
statements in between. In Proceedings of BioLink
2004 workshop on linking biological literature, on-
tologies and databases: tools for users, pp. 17-24.
R. Morante, A. Liekens, and W. Daelemans. 2008.
Learning the scope of negation in biomedical texts.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pp. 715-
724.
R. Morante, and W. Daelemans. 2009. A metalearn-
ing approach to processing the scope of negation.
Proceedings of the Thirteenth Conference on Com-
putational Natural Language Learning, 21-29.
R. Rosenthal. 1979. The file drawer problem and tol-
erance for null results. Psychological Bulle-
tin, 86(3): 638.
G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008.
The BioScope corpus: annotation for negation, un-
certainty and their scope in biomedical texts.
In Proceedings of the Workshop on Current Trends
in Biomedical Natural Language Processing, pp.
38-45.
</reference>
<page confidence="0.998915">
23
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.961029">
<title confidence="0.999726">Classifying Negative Findings in Biomedical Publications</title>
<author confidence="0.999914">Bei Yu Daniele Fanelli</author>
<affiliation confidence="0.999904">School of Information Studies School of Library and Information Science Syracuse University University of Montreal</affiliation>
<email confidence="0.997908">byu@syr.eduemail@danielefanelli.com</email>
<abstract confidence="0.998037684210526">Publication bias refers to the phenomenon that statistically significant, “positive” results are more likely to be published than non-significant, “negative” results. Currently, researchers have to manually identify negative results in a large number of publications in order to examine publication biases. This paper proposes an NLP approach for automatically classifying negated sentences in biomedical abstracts as either reporting negative findings or not. Using multinomial naïve Bayes algorithm and bag-ofwords features enriched by parts-ofspeeches and constituents, we built a classifier that reached 84% accuracy based on 5-fold cross validation on a balanced data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Agarwal</author>
<author>H Yu</author>
<author>I Kohane</author>
<author>I</author>
</authors>
<title>BioNOT: A searchable database of biomedical negated sentences.</title>
<date>2011</date>
<journal>BMC bioinformatics,</journal>
<volume>12</volume>
<pages>420</pages>
<contexts>
<context position="4262" citStr="Agarwal et al., 2011" startWordPosition="630" endWordPosition="633"> markers. The best performance is 84% accuracy based on 5-fold cross validation on a balanced data set. 19 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 19–23, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 2 Related work The problem of identifying negative results is related to several other BioNLP problems, especially on negation and scientific claim identification. The first relevant task is to identify negation signals and their scopes (e.g., Morante and Daelemans, 2008;2009; Farkas et al., 2010; Agarwal et al., 2011). Manually-annotated corpora like BioScope (Szarvas et al., 2008) were created to annotate negations and their scopes in biomedical abstracts in support of automated identification. This task targets a wide range of negation types, such as the presence or absence of clinical observations in narrative clinical reports (Chapman et al., 2001). In comparison, our task focuses on identifying negative findings only. Although not all negations report negative results, negation signals are important rhetorical device for authors to make negative claims. Therefore, in this study we also examine precisi</context>
</contexts>
<marker>Agarwal, Yu, Kohane, I, 2011</marker>
<rawString>S. Agarwal, H. Yu, and I. Kohane, I. 2011. BioNOT: A searchable database of biomedical negated sentences. BMC bioinformatics, 12: 420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Blake</author>
</authors>
<title>Beyond genes, proteins, and abstracts: Identifying scientific claims from full-text biomedical articles.</title>
<date>2010</date>
<journal>Journal of biomedical informatics,</journal>
<volume>43</volume>
<issue>2</issue>
<pages>173--189</pages>
<contexts>
<context position="5153" citStr="Blake (2010)" startWordPosition="769" endWordPosition="770">ervations in narrative clinical reports (Chapman et al., 2001). In comparison, our task focuses on identifying negative findings only. Although not all negations report negative results, negation signals are important rhetorical device for authors to make negative claims. Therefore, in this study we also examine precision and recall of using negation signals as predictors of negative findings. The second relevant task is to identify the strength and types of scientific claims. Light et al. (2004) developed a classifier to predict the level of speculations in sentences in biomedical abstracts. Blake (2010) proposed a “claim framework” that differentiates explicit claims, observations, correlations, comparisons, and implicit claims, based on the certainty of the causal relationship that was presented. Blake also found that abstracts contained only 7.84% of all scientific claims, indicating the need for full-text analysis. Currently, our preliminary study examines abstracts only, assuming the most important findings are reported there. We also focus on coarse-grained classification of positive vs. negative findings at this stage, and leave for future work the task of differentiating negative clai</context>
</contexts>
<marker>Blake, 2010</marker>
<rawString>C. Blake. 2010. Beyond genes, proteins, and abstracts: Identifying scientific claims from full-text biomedical articles. Journal of biomedical informatics, 43(2): 173-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W W Chapman</author>
<author>W Bridewell</author>
<author>P Hanbury</author>
<author>G F Cooper</author>
<author>B G Buchanan</author>
</authors>
<title>Evaluation of negation phrases in narrative clinical reports.</title>
<date>2001</date>
<booktitle>Proceedings of the AMIA Symposium,</booktitle>
<pages>105</pages>
<contexts>
<context position="4603" citStr="Chapman et al., 2001" startWordPosition="682" endWordPosition="685">g negative results is related to several other BioNLP problems, especially on negation and scientific claim identification. The first relevant task is to identify negation signals and their scopes (e.g., Morante and Daelemans, 2008;2009; Farkas et al., 2010; Agarwal et al., 2011). Manually-annotated corpora like BioScope (Szarvas et al., 2008) were created to annotate negations and their scopes in biomedical abstracts in support of automated identification. This task targets a wide range of negation types, such as the presence or absence of clinical observations in narrative clinical reports (Chapman et al., 2001). In comparison, our task focuses on identifying negative findings only. Although not all negations report negative results, negation signals are important rhetorical device for authors to make negative claims. Therefore, in this study we also examine precision and recall of using negation signals as predictors of negative findings. The second relevant task is to identify the strength and types of scientific claims. Light et al. (2004) developed a classifier to predict the level of speculations in sentences in biomedical abstracts. Blake (2010) proposed a “claim framework” that differentiates </context>
</contexts>
<marker>Chapman, Bridewell, Hanbury, Cooper, Buchanan, 2001</marker>
<rawString>W. W. Chapman, W. Bridewell, P. Hanbury, G. F. Cooper, and B. G. Buchanan. 2001. Evaluation of negation phrases in narrative clinical reports. Proceedings of the AMIA Symposium, 105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Easterbrook</author>
<author>R Gopalan</author>
<author>J A Berlin</author>
<author>D R Matthews</author>
</authors>
<title>Publication bias in clinical research.</title>
<date>1991</date>
<journal>Lancet,</journal>
<volume>337</volume>
<issue>8746</issue>
<pages>867--872</pages>
<marker>Easterbrook, Gopalan, Berlin, Matthews, 1991</marker>
<rawString>P. J. Easterbrook, R. Gopalan, J. A. Berlin, and D. R. Matthews. 1991. Publication bias in clinical research. Lancet, 337(8746): 867-872.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Egger</author>
<author>G D Smith</author>
<author>M Schneider</author>
<author>C Minder</author>
</authors>
<title>Bias in meta-analysis detected by a simple, graphical test.</title>
<date>1997</date>
<journal>BMJ</journal>
<volume>315</volume>
<issue>7109</issue>
<pages>629--634</pages>
<contexts>
<context position="2074" citStr="Egger et al., 1997" startWordPosition="296" endWordPosition="299">sults (e.g., the Cochrane Review). Publication bias may be further spread through citation network, and amplified by citation bias, a phenomenon that positive results are more likely to be cited than negative results (Greenberg, 2009). To address the publication bias problem, some new journals were launched and dedicated to publishing negative results, such as the Journal of Negative Results in Biomedicine, Journal of Pharmaceutical Negative Results, Journal of Negative Results in Ecology and Evolutionary Biology, and All Results Journals: Chem. Some quantitative methods like the funnel plot (Egger et al., 1997) were used to measure publication bias in publications retrieved for a certain topic. A key step in such manual analysis is to examine the article abstracts or full-texts to see whether the findings are negative or not. For example, Hebert et al. (2002) examined the full text of 1,038 biomedical articles whose primary outcomes were hypothesis testing results, and found 234 (23%) negative articles. Apparently, such manual analysis approach is time consuming. An accurate, automated classifier would be ideal to actively track positive and negative publications. This paper proposes an NLP approach</context>
</contexts>
<marker>Egger, Smith, Schneider, Minder, 1997</marker>
<rawString>M. Egger, G. D. Smith, M. Schneider, and C. Minder. 1997. Bias in meta-analysis detected by a simple, graphical test. BMJ 315(7109): 629-634.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Fanelli</author>
</authors>
<title>Negative results are disappearing from most disciplines and countries.</title>
<date>2012</date>
<journal>Scientometrics</journal>
<volume>90</volume>
<issue>3</issue>
<pages>891--904</pages>
<contexts>
<context position="6023" citStr="Fanelli, 2012" startWordPosition="897" endWordPosition="898">ll scientific claims, indicating the need for full-text analysis. Currently, our preliminary study examines abstracts only, assuming the most important findings are reported there. We also focus on coarse-grained classification of positive vs. negative findings at this stage, and leave for future work the task of differentiating negative claims in finer-granularity. 3 The NLP approach 3.1 The definition of negative results When deciding what kinds of results count as “negative”, some prior studies used “nonsignificant” results as an equivalent for “negative results” (e.g. Hebert et al., 2002; Fanelli, 2012). However, in practice, the definition of “negative results” is actually broader. For example, the Journal of Negative Results in Biomedicine (JNRBM), launched in 2002, was devoted to publishing “unexpected, controversial, provocative and/or negative results,” according to the journal’s website. This broader definition has its pros and cons. The added ambiguity poses challenge for manual and automated identification. At the same time, the broader definition allows the inclusion of descriptive studies, such as the first JNRBM article (Hebert et al., 2002). Interestingly, Hebert et al. (2002) de</context>
<context position="10071" citStr="Fanelli (2012)" startWordPosition="1527" endWordPosition="1529">n cues were found. Second, some positive research results may be mistaken as negative just because they used negation cues. For example, “without” is marked as a negation cue in Bioscope, but it can be used in many contexts that do not indicate negative results, such as “The effects are consistent with or without the presence of hypertension and other comorbidities and across a range of drug classes.” (PMC2659734) To measure the percentage of false alarm, we applied the aforementioned trivial classifier to a corpus of 600 abstracts in 4 biomedical disciplines, which were manually annotated by Fanelli (2012). This corpus will be referred to as “Corpus-600” hereafter. Each abstract is marked as “positive”, “negative”, “partially positive”, or “n/a”, based on hypothesis testing results. The latter two types were excluded in our study. The trivial classifier predicted an abstract as “positive” if no negation cues were found. Table 1 reported the prediction results, including the precision and recall in identifying negative results. This result corroborates with our previous finding that the inclusiveness of negation cues is not the major problem since high recalls have been observed in both experime</context>
</contexts>
<marker>Fanelli, 2012</marker>
<rawString>D. Fanelli. 2012. Negative results are disappearing from most disciplines and countries. Scientometrics 90(3): 891-904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Farkas</author>
<author>V Vincze</author>
<author>G Móra</author>
<author>J Csirik</author>
<author>G Szarvas</author>
</authors>
<title>The CoNLL-2010 shared task: learning to detect hedges and their scope in natural language text.</title>
<date>2010</date>
<booktitle>Proceedings of the Fourteenth Conference on Computational Natural Language Learning---Shared Task,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="4239" citStr="Farkas et al., 2010" startWordPosition="626" endWordPosition="629"> tags and constituent markers. The best performance is 84% accuracy based on 5-fold cross validation on a balanced data set. 19 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 19–23, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 2 Related work The problem of identifying negative results is related to several other BioNLP problems, especially on negation and scientific claim identification. The first relevant task is to identify negation signals and their scopes (e.g., Morante and Daelemans, 2008;2009; Farkas et al., 2010; Agarwal et al., 2011). Manually-annotated corpora like BioScope (Szarvas et al., 2008) were created to annotate negations and their scopes in biomedical abstracts in support of automated identification. This task targets a wide range of negation types, such as the presence or absence of clinical observations in narrative clinical reports (Chapman et al., 2001). In comparison, our task focuses on identifying negative findings only. Although not all negations report negative results, negation signals are important rhetorical device for authors to make negative claims. Therefore, in this study </context>
</contexts>
<marker>Farkas, Vincze, Móra, Csirik, Szarvas, 2010</marker>
<rawString>R. Farkas, V. Vincze, G. Móra, J. Csirik, and G. Szarvas. 2010. The CoNLL-2010 shared task: learning to detect hedges and their scope in natural language text. Proceedings of the Fourteenth Conference on Computational Natural Language Learning---Shared Task, 1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Greenberg</author>
</authors>
<title>How citation distortions create unfounded authority: analysis of a citation network.</title>
<date>2009</date>
<journal>BMJ</journal>
<volume>339</volume>
<pages>2680</pages>
<contexts>
<context position="1689" citStr="Greenberg, 2009" startWordPosition="242" endWordPosition="243">ficant, “negative” results (Estabrook et al., 1991). Due to the “file-drawer” effect (Rosenthal, 1979), negative results are more likely to be “filed away” privately than to be published publicly. Publication bias poses challenge for an accurate review of current research progress. It threatens the quality of meta-analyses and systematic reviews that rely on published research results (e.g., the Cochrane Review). Publication bias may be further spread through citation network, and amplified by citation bias, a phenomenon that positive results are more likely to be cited than negative results (Greenberg, 2009). To address the publication bias problem, some new journals were launched and dedicated to publishing negative results, such as the Journal of Negative Results in Biomedicine, Journal of Pharmaceutical Negative Results, Journal of Negative Results in Ecology and Evolutionary Biology, and All Results Journals: Chem. Some quantitative methods like the funnel plot (Egger et al., 1997) were used to measure publication bias in publications retrieved for a certain topic. A key step in such manual analysis is to examine the article abstracts or full-texts to see whether the findings are negative or </context>
</contexts>
<marker>Greenberg, 2009</marker>
<rawString>S. A. Greenberg. 2009. How citation distortions create unfounded authority: analysis of a citation network. BMJ 339, b2680.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R S Hebert</author>
<author>S M Wright</author>
<author>R S Dittus</author>
<author>T A Elasy</author>
</authors>
<title>Prominent medical journals often provide insufficient information to assess the validity of studies with negative results.</title>
<date>2002</date>
<journal>Journal of Negative Results in Biomedicine</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="2327" citStr="Hebert et al. (2002)" startWordPosition="341" endWordPosition="344">ublication bias problem, some new journals were launched and dedicated to publishing negative results, such as the Journal of Negative Results in Biomedicine, Journal of Pharmaceutical Negative Results, Journal of Negative Results in Ecology and Evolutionary Biology, and All Results Journals: Chem. Some quantitative methods like the funnel plot (Egger et al., 1997) were used to measure publication bias in publications retrieved for a certain topic. A key step in such manual analysis is to examine the article abstracts or full-texts to see whether the findings are negative or not. For example, Hebert et al. (2002) examined the full text of 1,038 biomedical articles whose primary outcomes were hypothesis testing results, and found 234 (23%) negative articles. Apparently, such manual analysis approach is time consuming. An accurate, automated classifier would be ideal to actively track positive and negative publications. This paper proposes an NLP approach for automatically identifying negative results in biomedical abstracts. Because one publication may have multiple findings, we currently focus on classifying negative findings at sentence level: for a sentence that contains the negation cues “no” and/o</context>
<context position="6007" citStr="Hebert et al., 2002" startWordPosition="893" endWordPosition="896">ained only 7.84% of all scientific claims, indicating the need for full-text analysis. Currently, our preliminary study examines abstracts only, assuming the most important findings are reported there. We also focus on coarse-grained classification of positive vs. negative findings at this stage, and leave for future work the task of differentiating negative claims in finer-granularity. 3 The NLP approach 3.1 The definition of negative results When deciding what kinds of results count as “negative”, some prior studies used “nonsignificant” results as an equivalent for “negative results” (e.g. Hebert et al., 2002; Fanelli, 2012). However, in practice, the definition of “negative results” is actually broader. For example, the Journal of Negative Results in Biomedicine (JNRBM), launched in 2002, was devoted to publishing “unexpected, controversial, provocative and/or negative results,” according to the journal’s website. This broader definition has its pros and cons. The added ambiguity poses challenge for manual and automated identification. At the same time, the broader definition allows the inclusion of descriptive studies, such as the first JNRBM article (Hebert et al., 2002). Interestingly, Hebert </context>
</contexts>
<marker>Hebert, Wright, Dittus, Elasy, 2002</marker>
<rawString>R. S. Hebert, S. M. Wright, R. S. Dittus, and T. A. Elasy. 2002. Prominent medical journals often provide insufficient information to assess the validity of studies with negative results. Journal of Negative Results in Biomedicine 1(1):1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Light</author>
<author>X-Y Qiu</author>
<author>P Srinivasan</author>
</authors>
<title>The language of bioscience: Facts, speculations, and statements in between.</title>
<date>2004</date>
<booktitle>In Proceedings of BioLink</booktitle>
<pages>17--24</pages>
<contexts>
<context position="5042" citStr="Light et al. (2004)" startWordPosition="751" endWordPosition="754">ated identification. This task targets a wide range of negation types, such as the presence or absence of clinical observations in narrative clinical reports (Chapman et al., 2001). In comparison, our task focuses on identifying negative findings only. Although not all negations report negative results, negation signals are important rhetorical device for authors to make negative claims. Therefore, in this study we also examine precision and recall of using negation signals as predictors of negative findings. The second relevant task is to identify the strength and types of scientific claims. Light et al. (2004) developed a classifier to predict the level of speculations in sentences in biomedical abstracts. Blake (2010) proposed a “claim framework” that differentiates explicit claims, observations, correlations, comparisons, and implicit claims, based on the certainty of the causal relationship that was presented. Blake also found that abstracts contained only 7.84% of all scientific claims, indicating the need for full-text analysis. Currently, our preliminary study examines abstracts only, assuming the most important findings are reported there. We also focus on coarse-grained classification of po</context>
</contexts>
<marker>Light, Qiu, Srinivasan, 2004</marker>
<rawString>M. Light, X-Y Qiu, and P. Srinivasan. 2004. The language of bioscience: Facts, speculations, and statements in between. In Proceedings of BioLink 2004 workshop on linking biological literature, ontologies and databases: tools for users, pp. 17-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Morante</author>
<author>A Liekens</author>
<author>W Daelemans</author>
</authors>
<title>Learning the scope of negation in biomedical texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>715--724</pages>
<marker>Morante, Liekens, Daelemans, 2008</marker>
<rawString>R. Morante, A. Liekens, and W. Daelemans. 2008. Learning the scope of negation in biomedical texts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 715-724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Morante</author>
<author>W Daelemans</author>
</authors>
<title>A metalearning approach to processing the scope of negation.</title>
<date>2009</date>
<booktitle>Proceedings of the Thirteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>21--29</pages>
<marker>Morante, Daelemans, 2009</marker>
<rawString>R. Morante, and W. Daelemans. 2009. A metalearning approach to processing the scope of negation. Proceedings of the Thirteenth Conference on Computational Natural Language Learning, 21-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenthal</author>
</authors>
<title>The file drawer problem and tolerance for null results.</title>
<date>1979</date>
<journal>Psychological Bulletin,</journal>
<volume>86</volume>
<issue>3</issue>
<pages>638</pages>
<contexts>
<context position="1175" citStr="Rosenthal, 1979" startWordPosition="162" endWordPosition="163">proposes an NLP approach for automatically classifying negated sentences in biomedical abstracts as either reporting negative findings or not. Using multinomial naïve Bayes algorithm and bag-ofwords features enriched by parts-ofspeeches and constituents, we built a classifier that reached 84% accuracy based on 5-fold cross validation on a balanced data set. 1 Introduction Publication bias refers to the phenomenon that statistically significant, “positive” results are more likely to be published than non-significant, “negative” results (Estabrook et al., 1991). Due to the “file-drawer” effect (Rosenthal, 1979), negative results are more likely to be “filed away” privately than to be published publicly. Publication bias poses challenge for an accurate review of current research progress. It threatens the quality of meta-analyses and systematic reviews that rely on published research results (e.g., the Cochrane Review). Publication bias may be further spread through citation network, and amplified by citation bias, a phenomenon that positive results are more likely to be cited than negative results (Greenberg, 2009). To address the publication bias problem, some new journals were launched and dedicat</context>
</contexts>
<marker>Rosenthal, 1979</marker>
<rawString>R. Rosenthal. 1979. The file drawer problem and tolerance for null results. Psychological Bulletin, 86(3): 638.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Szarvas</author>
<author>V Vincze</author>
<author>R Farkas</author>
<author>J Csirik</author>
</authors>
<title>The BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,</booktitle>
<pages>38--45</pages>
<contexts>
<context position="4327" citStr="Szarvas et al., 2008" startWordPosition="639" endWordPosition="642">oss validation on a balanced data set. 19 Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 19–23, Baltimore, Maryland USA, June 26-27 2014. c�2014 Association for Computational Linguistics 2 Related work The problem of identifying negative results is related to several other BioNLP problems, especially on negation and scientific claim identification. The first relevant task is to identify negation signals and their scopes (e.g., Morante and Daelemans, 2008;2009; Farkas et al., 2010; Agarwal et al., 2011). Manually-annotated corpora like BioScope (Szarvas et al., 2008) were created to annotate negations and their scopes in biomedical abstracts in support of automated identification. This task targets a wide range of negation types, such as the presence or absence of clinical observations in narrative clinical reports (Chapman et al., 2001). In comparison, our task focuses on identifying negative findings only. Although not all negations report negative results, negation signals are important rhetorical device for authors to make negative claims. Therefore, in this study we also examine precision and recall of using negation signals as predictors of negative</context>
</contexts>
<marker>Szarvas, Vincze, Farkas, Csirik, 2008</marker>
<rawString>G. Szarvas, V. Vincze, R. Farkas, and J. Csirik. 2008. The BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts. In Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pp. 38-45.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
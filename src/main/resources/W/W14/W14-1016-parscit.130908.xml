<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001570">
<title confidence="0.996323">
Extracting Multiword Translations
from Aligned Comparable Documents
</title>
<note confidence="0.866214">
Reinhard Rapp Serge Sharoff
Aix-Marseille Université, Laboratoire University of Leeds
d&apos;Informatique Fondamentale Centre for Translation Studies
F-13288 Marseille, France Leeds, LS2 9JT, UK
</note>
<email confidence="0.970582">
reinhardrapp@gmx.de S.Sharoff@leeds.ac.uk
</email>
<sectionHeader confidence="0.993215" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913869565217">
Most previous attempts to identify trans-
lations of multiword expressions using
comparable corpora relied on dictionaries
of single words. The translation of a mul-
tiword was then constructed from the
translations of its components. In con-
trast, in this work we try to determine the
translation of a multiword unit by analyz-
ing its contextual behaviour in aligned
comparable documents, thereby not pre-
supposing any given dictionary. Whereas
with this method translation results for
single words are rather good, the results
for multiword units are considerably
worse. This is an indication that the type
of multiword expressions considered here
is too infrequent to provide a sufficient
amount of contextual information. Thus
indirectly it is confirmed that it should
make sense to look at the contextual be-
haviour of the components of a multi-
word expression individually, and to
combine the results.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926903225807">
The task of identifying word translations from
comparable text has received considerable atten-
tion. Some early papers include Fung (1995) and
Rapp (1995). Fung (1995) utilized a context het-
erogeneity measure, thereby assuming that words
with productive context in one language translate
to words with productive context in another lan-
guage, and words with rigid context translate into
words with rigid context. In contrast, the under-
lying assumption in Rapp (1995) was that words
which are translations of each other show similar
co-occurrence patterns across languages. This
assumption is effectively an extension of Harris&apos;
(1954) distributional hypotheses to the multilin-
gual case.
This work was further elaborated in some by
now classical papers, such as Fung &amp; Yee (1998)
and Rapp (1999). Based on these papers, the
standard approach is to start from a dictionary of
seed words, and to assume that the words occur-
ring in the context of a source language word
have similar meanings as the words occurring in
the context of its target language translation.
There have been suggestions to eliminate the
need for the seed dictionary. However, most at-
tempts, such as Rapp (1995), Diab &amp; Finch
(2000) and Haghighi et al. (2008) did not work to
an extent that the results would be useful for
practical purposes. Only recently a more pro-
mising approach has been investigated: Schafer
&amp; Yarowsky (2002), Hassan &amp; Mihalcea (2009),
Prochasson &amp; Fung (2011) and Rapp et al.
(2012) look at aligned comparable documents
and deal with them in analogy to the treatment of
aligned parallel sentences, i.e. effectively doing a
word alignment in a very noisy environment.
This approach has been rather successful and it
was possible to improve on previous results. This
is therefore the approach which we will pursue in
the current paper.
However, in contrast to the above mentioned
papers the focus of our work is on multiword
expressions, and we will compare the perform-
ance of our algorithm when applied to multiword
expressions and when applied to single words.
There has been some previous work on identi-
fying the translations of multiword units using
comparable corpora, such as Robitaille et al.
(2006), Babych et al. (2007), Daille &amp; Morin
(2012); Delpech et al. (2012). However, none of
this work utilizes aligned comparable documents,
and the underlying assumption is that the transla-
tion of a multiword unit can be determined by
looking at its components individually, and by
merging the results.
In contrast, we try to explore whether the
translation of a multiword unit can be determined
solely by looking at its contextual behavior, i.e.
whether it is possible to also apply the standard
approach as successfully used for single words.
The underlying fundamental question is whether
the meaning of a multiword unit is determined by
</bodyText>
<page confidence="0.991481">
87
</page>
<note confidence="0.990823">
Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 87–95,
Gothenburg, Sweden, April 27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999474272727273">
the contextual behavior of the full unit, or by the
contextual behavior of its components (or by a
mix of both). But multiword expressions are of
complex nature, as expressed e.g. by Moon
(1998): &amp;quot;there is no unified phenomenon to de-
scribe but rather a complex of features that inter-
act in various, often untidy, ways and represent a
broad continuum between non-compositional (or
idiomatic) and compositional groups of words.&amp;quot;
The current paper is an attempt to systematically
approach one aspect of this complexity.
</bodyText>
<sectionHeader confidence="0.992809" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.998405">
Our approach is based on the usual assumption
that there is a correlation between the patterns of
word-co-occurrence across languages. However,
instead of presupposing a bilingual dictionary it
only requires pre-aligned comparable documents,
i.e. small or medium sized documents aligned
across languages which are known to deal with
similar topics. This could be, for example, news-
paper articles, scientific papers, contributions to
discussion groups, or encyclopaedic articles. As
Wikipedia is a large resource and readily avail-
able for many languages, we decided to base our
study on this encyclopaedia. The Wikipedias
have the so-called interlanguage links which are
manually inserted by the authors and connect
articles referring to the same headword in differ-
ent languages.
Given that each Wikipedia community con-
tributes in its own language, only occasionally an
article connected in this way will be an exact
translation of a foreign language article, and in
most cases the contents will be rather different.
On the positive side, the link structure of the in-
terlanguage links tends to be quite dense. For
example, of the 1,114,696 German Wikipedia
articles, 603,437 have a link to the corresponding
English Wikipedia article.
</bodyText>
<subsectionHeader confidence="0.999251">
2.1 Pre-processing and MWE extraction
</subsectionHeader>
<bodyText confidence="0.999816575">
We used the same versions of Wikipedia as in
Rapp et al. (2012) and used the same processing.
After download, each Wikipedia was minimally
processed to extract the plain text contents of the
articles. In this process all templates, e.g.
&apos;infoboxes&apos;, as well as tables were removed, and
we kept only the webpages with more than 500
characters of running text (including white
space). Linguistic processing steps included to-
kenisation, tagging and lemmatisation using the
default UTF-8 versions of the respective Tree-
Tagger resources (Schmid, 1994).
From the pre-processed English and German
Wikipedia, we extracted the multiword expres-
sions using two simple principles, a negative
POS filter and a containment filter. The negative
POS filter operates in a rule-based fashion on the
complete list of n-grams by removing the un-
likely candidates according to a set of con-
straints, such as the presence of determiners or
prepositions at the edges of expressions, see a
similar method used by (Justeson &amp; Katz, 1995).
With some further extensions this was also used
to produce the multiword lists for the dictionary
of translation equivalents (Babych et al., 2007).
We did not use positive shallow filters. These
would need to capture the relatively complex
structure of the noun, verb and prepositional
phrases, while avoiding noise. This can often
lead to a lack of recall when more complex con-
structions cannot be captured. In contrast, nega-
tive shallow filters simply avoid obvious noise,
while passing other multiword expressions
(MWEs) through, which are very often legiti-
mate syntactic constructions in a language in
question. For example, the following English
filters1 rejected personal pronouns (PP) and con-
junctions (CC) at the edges of expressions (using
the Penn Treebank tagset as implemented by
Treetagger):
</bodyText>
<sectionHeader confidence="0.5363445" genericHeader="method">
^[^ ]+~~PP |~~PP$
^[^ ]+~~CC |~~CC$
</sectionHeader>
<bodyText confidence="0.9779515">
Similarly, any MWE candidates including proper
nouns (NP) and numerals (CD) were discarded:
~~NP
~~CD
In the end, this helps in improving the recall rate
while using a relatively small number of pat-
terns: 18 patterns were used for English, 11 for
German.
The containment filter further rejects MWEs
by removing those that regularly occur as a part
of a longer acceptable MWE. For example,
graphical user is an acceptable expression pass-
ing through the POS filter, but it is rejected by
the containment filter since the overwhelming
majority of its uses are in the containing MWE
graphical user interface (1507 vs 1304 uses in
Wikipedia, since MWEs are still possible, e.g.,
graphical user environment).
</bodyText>
<footnote confidence="0.662087">
1 We use here the standard notation for regular ex-
pressions as implemented in Perl (Friedl, 2002). For
example, &apos;^&apos; means &apos;beginning of line&apos; and &apos;$&apos; means
&apos;end of line&apos;.
</footnote>
<page confidence="0.99852">
88
</page>
<table confidence="0.999065471698113">
English keyterms for &apos;Airbus 320 family&apos;
Score f Keyterm
34.88 4 final_JJ assembly_NN
31.22 3 firm_NN order_NN
30.73 3 series_NN aircraft_NN
29.07 4 flight_NN control_NN
27.38 3 wing_NN area_NN
23.26 3 final_JJ approach_NN
22.19 2 lose_VV life_NN
20.63 6 passenger_NN and_CC crew_NN
17.54 2 first_JJ derivative_NN
17.34 2 fly-by-wire_NN flight_NN control_NN
16.63 3 flight_NN deck_NN
16.41 2 crew_NN die_VV
15.08 2 pilot_NN error_NN
14.98 2 passenger_NN capacity_NN
14.38 2 turbofan_NN engine_NN
14.03 2 development_NN cost_NN
12.30 2 maiden_JJ flight_NN
11.54 2 direct_JJ competition_NN
10.75 2 overall_JJ length_NN
10.39 2 overrun_VV the_DT runway_NN
9.54 2 flight_NN control_NN system_NN
9.31 2 fuel_NN consumption_NN
8.63 2 roll_VV out_RP
7.86 3 crew_NN member_NN
7.54 2 crew_NN on_IN board_NN
7.33 2 bad_JJ weather_NN
6.63 2 landing_NN gear_NN
German keyterms for &apos;Airbus-A320-Familie&apos;
Score f Keyterm
155.25 20 Triebwerk
62.88 4 Fly-by-Wire-System
59.76 8 Erstflug
57.67 8 Absturz
43.79 4 Endmontage
43.70 4 Hauptfahrwerk
41.77 4 Tragflügel
36.52 8 Unfall
35.90 6 Unglück
33.25 3 Abfluggewicht
33.10 5 Auslieferung
30.01 3 Treibstoffverbrauch
29.00 2 Triebwerkstyp
28.51 2 Zwillingsreifen
18.20 2 Absturz_NN verursachen_VV
16.28 3 Passagier_NN Platz_NN
16.23 2 Triebwerk_NN antreiben_VV
13.41 2 Steuerung_NN d_AR Flugzeug_NN
12.52 2 Absturz_NN führen_VV
11.68 2 Rumpf_NN befinden_VV
8.59 2 Insasse_NN ums_AP Leben_NN
8.55 2 Zeitpunkt_NN d_AR Unglück_NN
</table>
<tableCaption confidence="0.994022">
Table 1. English and German keyterms for &apos;Airbus 320 fam-
ily&apos; (lists truncated). Score = log-likelihood score; f = occur-
rence frequency of keyterm; NN = noun; VV = verb; AR =
article; AP = article+preposition; JJ = adjective; CC = con-
junction; RP = preposition.
</tableCaption>
<subsectionHeader confidence="0.995111">
2.2 Keyterm extraction
</subsectionHeader>
<bodyText confidence="0.99249923255814">
As the aligned English and German Wikipedia
documents are typically not translations of each
other, we cannot apply the usual procedures and
tools as available for parallel texts (e.g. the Gale
&amp; Church sentence aligner and the Giza++ word
alignment tool). Instead we conduct a two step
procedure:
1. We first extract salient terms (single word or
multiword) from each of the documents.
2. We then align these terms across languages
using an approach inspired by a connectionist
(Rumelhart &amp; McClelland, 1987) Winner-
Takes-It-All Network. The respective algo-
rithm is called WINTIAN and is described in
Rapp et al. (2012) and in Rapp (1996).
For term extraction, the occurrence frequency of
a term in a particular document is compared to
its average occurrence frequency in all Wikipe-
dia documents, whereby a high discrepancy indi-
cates a strong keyness. Following Rayson &amp;
Garside (2000), we use the log-likelihood score
to measure keyness, since it has been shown to
be robust to small numbers of instances. This
robustness is important as many Wikipedia arti-
cles are rather short.
This procedure leads to multiword keyterms as
exemplified in Table 1 for the Wikipedia entry
Airbus A320 family. Because of compounding in
German, many single-word German expressions
are translated into multiword expressions in Eng-
lish. So we chose to include single-word expres-
sions into the German candidate list for align-
ment with English multiwords.
One of the problems in obtaining multiword
keyterms from the Wikipedia articles is relative
data sparseness. Usually, the frequency of an
individual multiword expression within a Wiki-
pedia article is between 2 and 4. Therefore we
had to use a less conservative threshold of 6.63
(1% significance level) rather than the more
standard 15.13 (0.01% significance level) for the
log-likelihood score (see Rayson &amp; Garside,
2000, and http://ucrel. lancs.ac.uk/llwizard.html).
</bodyText>
<subsectionHeader confidence="0.997564">
2.3 Term alignment
</subsectionHeader>
<bodyText confidence="0.999994375">
The WINTIAN algorithm is used for establishing
term alignments across languages. As a more
detailed technical description is given in Rapp et
al. (2012) and in Rapp (1996), we only briefly
describe this algorithm here, thereby focusing on
the neural network analogy. The algorithm can
be considered as an artificial neural network
where the nodes are all English and German
</bodyText>
<page confidence="0.998839">
89
</page>
<bodyText confidence="0.999916325">
terms occurring in the keyterm lists. Each Eng-
lish term has connections to all German terms.
The connections are all initialized with values of
one when the algorithm is started, but will serve
as a measure of the translation probabilities after
the completion of the algorithm. One after the
other, the network is fed with the pairs of corre-
sponding keyterm lists. Each German term acti-
vates the corresponding German node with an
activity of one. This activity is then propagated
to all English terms occurring in the correspond-
ing list of keyterms. The distribution of the activ-
ity is not equal, but in proportion to the connect-
ing weights. This unequal distribution has no
effect at the beginning when all weights are one,
but later on leads to rapid activity increases for
pairs of terms which often occur in correspond-
ing keyterm lists. The assumption is that these
are translations of each other. Using Hebbian
learning (Rumelhart &amp; McClelland, 1987) the
activity changes are stored in the connections.
We use a heuristic to avoid the effect that fre-
quent keyterms dominate the network: When
more than 50 of the connections to a particular
English node have weights higher than one, the
weakest 20 of them are reset to one. This way
only translations which are frequently confirmed
can build up high weights.
It turned out that the algorithm shows a robust
behaviour in practice, which is important as the
corresponding keyterm lists tend to be very noisy
and, especially for multiword expressions, in
many cases may contain hardly any terms that
are actually translations of each other. Reasons
are that corresponding Wikipedia articles are of-
ten written from different perspectives, that the
variation in length can be considerable across
languages, and that multiword expressions tend
to show more variability with regard to their
translations than single words.
</bodyText>
<sectionHeader confidence="0.9276105" genericHeader="method">
3 Results and evaluation
3.1 Results for single words
</sectionHeader>
<bodyText confidence="0.999951842105263">
In this subsection we report on our previous re-
sults for single words (Rapp et al., 2012) as these
serve as a baseline for our new results concern-
ing multiword units.
The WINTIAN algorithm requires as input
vocabularies of the source and the target lan-
guage. For both English and German, we con-
structed these as follows: Based on the keyword
lists for the respective Wikipedia, we counted the
number of occurrences of each keyword, and
then applied a threshold of five, i.e. all keywords
with a lower frequency were eliminated. The rea-
soning behind this is that rare keywords are of
not much use due to data sparseness. This re-
sulted in a vocabulary size of 133,806 for Eng-
lish, and of 144,251 for German.
Using the WINTIAN algorithm, the English
translations for all 144,251 words occurring in
the German vocabulary were computed. Table 2
shows the results for the German word Straße
(which means street).
For a quantitative evaluation we used the
ML1000 test set comprising 1000 English-
German translations (see Rapp et al., 2012). We
verified in how many cases our algorithm had
assigned the expected translation (as provided by
the gold standard) the top rank among all
133,806 translation candidates. (Candidates are
all words occurring in the English vocabulary.)
This was the case for 381 of the 1000 items,
which gives us an accuracy of 38.1%. Let us
mention that this result refers to exact matches
with the word equations in the gold standard. As
in reality due to word ambiguity other transla-
tions might also be acceptable (e.g. for Straße
not only street but also road would be accept-
able), these figures are conservative and can be
seen as a lower bound of the actual performance.
</bodyText>
<table confidence="0.969684636363636">
GIVEN GERMAN Straße
WORD
EXPECTED street
TRANSLATION
LL-SCORE TRANSLATION
1 215.3 road
2 148.2 street
3 66.0 traffic
4 46.0 Road
5 42.6 route
6 34.6 building
</table>
<tableCaption confidence="0.999611">
Table 2. Computed translations for Straße.
</tableCaption>
<sectionHeader confidence="0.617701" genericHeader="evaluation">
3.2 Results for multiword expressions
</sectionHeader>
<bodyText confidence="0.999981769230769">
In analogy to the procedure for single words, for
the WINTIAN algorithm we also needed to de-
fine English and German vocabularies of multi-
word terms. For English, we selected all multi-
word terms which occurred at least three times in
the lists of English key terms, and for German
those which occurred at least four times in the
lists of German key terms. This resulted in simi-
lar sized vocabularies of 114,796 terms for Eng-
lish, and 131,170 for German. Note that the
threshold for German had to be selected higher
not because German has more inflectional vari-
ants (which does not matter as we are working
</bodyText>
<page confidence="0.990506">
90
</page>
<bodyText confidence="0.996357666666667">
with lemmatized data), but because — other than
the English — the German vocabulary also in-
cludes unigrams. The reason for this is that Ger-
man is highly compositional, so that English
multiword units are often translated by German
unigrams.
Using the WINTIAN algorithm, the English
translations for all 131,170 words occurring in
the German multiword vocabulary were com-
puted, and in another run the German translations
for all 114,796 English words. Table 3 shows
some sample results.
For a quantitative evaluation, we did not have
a gold standard at hand. As multiword expres-
sions show a high degree of variability with re-
gard to their translations, so that it is hard to
come up with all possibilities, we first decided
not to construct a gold standard, but instead did a
manual evaluation. For this purpose, we ran-
domly selected 100 of the German multiword
expressions with an occurrence frequency above
nine, and verified their computed translations
(i.e. the top ranked item for each) manually. We
distinguished three categories: 1) Acceptable
translation; 2) Associatively related to an accept-
able translation; 3) Unrelated to an acceptable
translation.
</bodyText>
<table confidence="0.911736409090909">
English → German
husband_NN and_CC wife_NN
Rank Aktivity Translation
1 2.98 Eheleute
2 1.09 Voraussetzung
3 1.08 Kirchenrecht
4 0.76 Trennung
5 0.35 Mann
6 0.24 Kirche
7 0.08 Mischehe
8 0.08 Diakon
German → English
Eheleute
Rank Aktivity Translation
1 3.01 husband_NN_and_CC_wife_NN
2 1.26 married_JJ_couple_NN
3 1.02 civil_JJ_law_NN
4 1.02 equitable_JJ_distribution_NN
5 1.02 community_NN_property_NN
6 0.52 law_NN_jurisdiction_NN
7 0.05 racing_NN_history_NN
8 0.05 great_JJ_female_JJ
</table>
<tableCaption confidence="0.9770665">
Table 3. Sample results for translation directions EN → DE
and DE → EN.
</tableCaption>
<bodyText confidence="0.999816857142857">
We also did the same computation for the reverse
language direction, i.e. for English to German.
The results are listed in Table 4. These results
indicate that our procedure, although currently
state of the art for single words, does not work
well for multiword units. We investigated the
data and located the following problems:
</bodyText>
<listItem confidence="0.877149375">
• The problem of data sparseness is, on average,
considerably more severe for multiword ex-
pressions than it is for single words.
• Although the English and the German vocabu-
lary each contain more than 100,000 items,
their overlap is still limited. The reason is that
the number of possible multiword units is very
high, far higher than the number of words in a
language.
• We considered only multiword units up to
length three, but in some cases this may not
suffice for an acceptable translation.
• In the aligned keyterm lists, only rarely correct
translations of the source language terms oc-
cur. Apparently the reason is the high variabil-
ity of multiword translations.
</listItem>
<bodyText confidence="0.999452857142857">
Hereby he last point seems to have a particularly
severe negative effect on translation quality.
However, all of these findings are of fundamen-
tal nature and contribute to the insight that at
least for our set of multiword expressions com-
positionality seems to be more important than
contextuality.
</bodyText>
<table confidence="0.999491578947368">
German → English
Judgment Num- Example taken from actual
ber data
Acceptable 5 Jugendherberge →
youth_NN hostel_NN
Association 38 Maischegärung →
oak_NN barrel_NN
Unacceptable 57 Stachelbeere →
horror_NN film_NN
English → German
Judgment Num- Example taken from actual
ber data
Acceptable 6 amino_NN acid_NN →
Aminosäure
Association 52 iron_NN mine_NN → Ei-
senerz
Unacceptable 42 kill_VV more_JJ → Welt-
meistertitel_NN im_AP
Schwergewicht_NN
</table>
<tableCaption confidence="0.999876">
Table 4. Quantitative results involving MWEs.
</tableCaption>
<page confidence="0.997948">
91
</page>
<subsectionHeader confidence="0.997604">
3.3 Large scale evaluation
</subsectionHeader>
<bodyText confidence="0.999978979591837">
As a manual evaluation like the one described
above is time consuming and subjective, we
thought about how we could efficiently come up
with a gold standard for multiword expressions
with the aim of conducting a large scale auto-
matic evaluation. We had the idea to determine
the correspondences between our English and
German MWEs via translation information as
extracted from a word-aligned parallel corpus.
Such data we had readily at hand from a pre-
vious project called COMTRANS. During this
project we had constructed a large bilingual dic-
tionary of bigrams, i.e. of pairs of adjacent words
in the source language. For constructing the dic-
tionary, we word-aligned the English and Ger-
man parts of the Europarl corpus. For this pur-
pose, using Moses default settings, we combined
two symmetric runs of Giza++, which considera-
bly improves alignment quality. Then we deter-
mined and extracted for each English bigram the
German word or word sequence which had been
used for its translation. Discontinuities of one or
several word positions were allowed and were
indicated by the wildcard ‘*’. As the above me-
thod for word alignment produces many unjusti-
fied empty assignments (i.e. assignments where a
source language word pair is erroneously as-
sumed to have no equivalent in the target lan-
guage sentence), so that the majority of these is
incorrect, all empty assignments were removed
from the dictionary.
In the dictionary, for each source language
word pair its absolute frequency and the absolute
and relative frequencies of its translation(s) are
given. To filter out spurious assignments, thresh-
olds of 2 for the absolute and 10% for the rela-
tive frequency of a translation were used. The
resulting dictionary is available online.2 Table 5
shows a small extract of the altogether 371,590
dictionary entries. Alternatively, we could have
started from a Moses phrase table, but it was eas-
ier for us to use our own data.
Although the quality of our bigram dictionary
seems reasonably good, it contains a lot of items
which are not really interesting multiword ex-
pressions (e.g. arbitrary word sequences such as
credible if or the discontinuous word sequences
on the target language side). For this reason we
filtered the dictionary using the lists of Wikipe-
</bodyText>
<footnote confidence="0.942677666666667">
2 http://www.ftsk.uni-mainz.de/user/rapp/comtrans/
There click on &amp;quot;Dictionaries of word pairs&amp;quot; and then
download &amp;quot;English - German&amp;quot;.
</footnote>
<bodyText confidence="0.997915631578947">
dia-derived multiword expressions as described
in section 2.1. These contained 418,627 items for
English and 1,212,341 candidate items for Ger-
man (the latter included unigram compounds).
That is, in the dictionary those items were re-
moved where either the English side did not
match any of the English MWEs, or where the
German side did not match any of the German
candidates.
This intersection resulted in a reduction of our
bigram dictionary from 371,590 items to 137,701
items. Table 6 shows the results after filtering the
items listed in Table 5. Note that occasionally
reasonable MWEs are eliminated if they happen
not to occur in Wikipedia, or if the algorithm for
extracting the MWEs does not identify them.
The reduced dictionary we considered as an
appropriate gold standard for the automatic eval-
uation of our system.
</bodyText>
<table confidence="0.998184954545454">
ENGLISH BIGRAM GERMAN TRANSLATION
credible if dann glaubwürdig * wenn
credible if glaubhaft * wenn
credible if glaubwürdig * wenn
credible in in * Glaubwürdigkeit
credible in in * glaubwürdig
credible investigation glaubwürdige Untersuchung
credible labelling glaubwürdige Kennzeichnung
credible manner glaubwürdig
credible military glaubwürdige militärische
credible military glaubwürdigen militärischen
credible only nur dann glaubwürdig
credible partner glaubwürdiger Partner
credible policy Politik * glaubwürdig
credible policy glaubwürdige Politik
credible reports glaubwürdige Berichte
credible response glaubwürdige Antwort
credible solution glaubwürdige Lösung
credible system glaubwürdiges System
credible threat glaubhafte Androhung
credible to für * glaubwürdig
credible to glaubwürdig
</table>
<tableCaption confidence="0.998088">
Table 5. Extract from the COMTRANS bigram dictionary.
</tableCaption>
<table confidence="0.998696222222222">
ENGLISH BIGRAM GERMAN TRANSLATION
credible investigation glaubwürdige Untersuchung
credible only nur dann glaubwürdig
credible policy glaubwürdige Politik
credible response glaubwürdige Antwort
credible solution glaubwürdige Lösung
credible system glaubwürdiges System
credible threat glaubhafte Androhung
credible to glaubwürdig
</table>
<tableCaption confidence="0.996733">
Table 6. Extract from the bigram dictionary after filtering.
</tableCaption>
<page confidence="0.993947">
92
</page>
<bodyText confidence="0.988195185185185">
As in section 3.2, the next step was to apply
the keyword extraction algorithm to the English
and the German Wikipedia documents. Hereby
only terms occurring in the gold standard dic-
tionary were taken into account. But it turned out
that, when using the same log-likelihood thresh-
old as in section 3.2, only few keyterms were
assigned: on average less than one per document.
This had already been a problem in 3.2, but it
was now considerably more severe as this time
the MWE lists had been filtered, and as the filter-
ing had been on the basis of another type of cor-
pus (Europarl rather than Wikipedia).
This is why, after some preliminary experi-
ments with various thresholds, we finally de-
cided to disable the log-likelihood threshold. In-
stead, on the English side, all keyterms from the
gold standard were used if they occurred at least
once in the respective Wikipedia document. On
the German side, as here we had many unigram
compounds which tend to be more stable and
therefore more repetitive than MWEs, we used
the keyterms if the occurred at least twice. This
way for most documents we obtained at least a
few keyterms.
When running the WINTIAN algorithm on the
parallel keyword lists, in some cases reasonable
results were obtained. For example, for the direc-
tion English to German, the system translates
information society with Informationsgesell-
schaft, and education policy with Bildungs-
politik. As WINTIAN is symmetric and can
likewise produce a dictionary in the opposite di-
rection, we also generated the results for German
to English. Here, among the good examples, are
Telekommunikationsmarkt, which is translated as
telecommunications market, and Werbekam-
pagne, which is translated as advertising cam-
paign. However, these are selected examples
showing that the algorithm works in principle.
Of more interest is the quantitative evaluation
which is based on thousands of test words and
uses the gold standard dictionary. For English to
German we obtained an accuracy of 0.77% if
only the top ranked word is taken into account,
i.e. if this word matches the expected translation.
This improves to 1.6% if it suffices that the ex-
pected translation is ranked among the top ten
words. The respective figures for German to
English are 1.41% and 2.04%.
The finding that German to English performs
better can be explained by the fact that other than
English German is a highly inflectional lan-
guage. That is, when generating translations it is
more likely for German that an inflectional vari-
ant not matching the gold standard translation is
ranked first, thus adversely affecting perform-
ance.
A question more difficult to answer is why the
results based on the gold standard are considera-
bly worse than the ones reported in section 3.2
which were based on human judgment. We see
the following reasons:
• The evaluation in section 3.2 used only a
small sample so might be not very reliable.
Also, other than here, it considered only
source language words with frequencies
above nine.
• Unlike the candidate expressions, the gold
standard data is not lemmatized on the target
language side.
• The hard string matching used for the gold-
standard-based evaluation does not allow for
inflectional variants.
• The gold-standard-based evaluation used
terms resulting from the intersection of term
lists based on Wikipedia and Europarl. It is
clear that this led to a reduction of average
term frequency (if measured on the basis of
Wikipedia), thus increasing the problem of
data sparseness.
</bodyText>
<listItem confidence="0.8608285">
• As for the same reason the log-likelihood
threshold had to be abandoned, on average
less salient terms had to be used. This is
likely to additionally reduce accuracy.
• For many terms the gold standard lists sev-
eral possible translations. In the current im-
</listItem>
<bodyText confidence="0.93230425">
plementation of the evaluation algorithm
only one of them is counted as correct.3
However, in the human evaluation any rea-
sonable translation was accepted.
</bodyText>
<listItem confidence="0.6373975">
• Some reasonable MWE candidates extracted
from Wikipedia are not present in the gold
</listItem>
<bodyText confidence="0.996515555555556">
standard, for example credible evidence,
credible source, and credible witness are not
frequent enough in Europarl to be selected
for alignment.
We should perhaps mention that it would be pos-
sible to come up with better looking accuracies
by presenting results for selected subsets of the
source language terms. For example, one could
concentrate on terms with particularly good cov-
</bodyText>
<tableCaption confidence="0.730691">
3 This can be justified because an optimal algorithm
should provide all possible translations of a term. If
only some translations are provided, only partial
credit should be given. But this is likely to average
out over large numbers, so the simple version seems
acceptable.
</tableCaption>
<page confidence="0.997645">
93
</page>
<bodyText confidence="0.99736">
erage. Another possibility would be to consider
MWEs consisting of nouns only. This we actu-
ally did by limiting source and target language
vocabulary (of MWEs) to compound nouns. The
results were as follows:
English to German (top 1): 1.81%
English to German (top 10): 3.75%
German to English (top 1): 2.03%
German to English (top 10): 3.16%
As can be seen, these results look somewhat bet-
ter. But this is only for the reason that translating
compound nouns appears to be a comparatively
easier task on average.
</bodyText>
<sectionHeader confidence="0.996166" genericHeader="conclusions">
4 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999976804878049">
We have presented a method for identifying term
translations using aligned comparable docu-
ments. Although it is based on a knowledge poor
approach and does not presuppose a seed lexi-
con, it delivers competitive results for single
words.
A disadvantage of our method is that it pre-
supposes that the alignments of the comparable
documents are known. On the other hand, there
are methods for finding such alignments auto-
matically not only in special cases such as
Wikipedia and newspaper texts, but also in the
case of unstructured texts (although these meth-
ods may require a seed lexicon).
Concerning the question from the introduc-
tion, namely whether the translation (and conse-
quently also the meaning) of a multiword unit is
determined compositionally or contextually, our
answer is as follows: For the type of multiword
units we were investigating, namely automati-
cally extracted collocations, our results indicate
that looking at their contextual behavior usually
does not suffice. The reasons seem to be that
their contextual behavior shows a high degree of
variability, that their translations tend to be less
salient than those of single words, and that the
problem of data sparseness is considerably more
severe.
It must be seen, however, that there are many
types of multiword expressions, such as idioms,
metaphorical expressions, named entities, fixed
phrases, noun compounds, compound verbs,
compound adjectives, and so on, so that our re-
sults are not automatically applicable to all of
them. Therefore, in future work we intend to
compare the behavior of different types of mul-
tiword expressions (e.g. multiword named enti-
ties and short phrases such as those used in
phrase-based machine translations) and to quan-
tify in how far their behavior is compositional or
contextual.
</bodyText>
<sectionHeader confidence="0.967829" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.643322333333333">
This research was supported by a Marie Curie
Intra European Fellowship within the 7th Euro-
pean Community Framework Programme.
</bodyText>
<sectionHeader confidence="0.971787" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999935333333334">
Babych, B., Sharoff, S., Hartley, A., and Mudraya, O.
(2007). Assisting Translators in Indirect Lexical
Transfer. Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics
ACL 2007, Prague, Czech Republic.
Daille, B.; Morin, E. (2012). Revising the composi-
tional method for terminology acquisition from
comparable corpora. Proceedings of Coling 2012,
Mumbai.
Delpech, E.; Daille, B.; Morin, E., Lemaire, C.
(2012). Extraction of domain-specific bilingual
lexicon from comparable corpora: compositional
translation and ranking. Proceedings of Coling
2012, Mumbai.
Diab, M., Finch, S. (2000): A statistical wordlevel
translation model for comparable corpora. In: Pro-
ceedings of the Conference on Content-Based Mul-
timedia Information Access (RIAO).
Friedl, J. (2002). Mastering Regular Expressions.
O&apos;Reilly.
Fung, P. (1995). Compiling bilingual lexicon entries
from a non-parallel English-Chinese corpus. In:
Proceedings of the Third Annual Workshop on Ve-
ry Large Corpora, Boston, Massachusetts. 173-
183.
Fung, P.; Yee, L. Y. (1998). An IR approach for
translating new words from nonparallel, compara-
ble texts. Proceedings of COLING/ACL 1998,
Montreal, Canada. 414-420.
Haghighi, A., Liang, P., Berg-Kirkpatrick, T., Klein,
D. (2008): Learning bilingual lexicons from mono-
lingual corpora. In: Proceedings of ACL-HLT
2008, Columbus, Ohio. 771-779.
Harris, Z.S. (1954). Distributional structure. Word,
10(23), 146–162.
Hassan, S., Mihalcea, R. (2009): Cross-lingual seman-
tic relatedness using encyclopedic knowledge. In:
Proceedings of EMNLP.
Justeson, J.S.; Katz, S.M. (1995). Techninal terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural Language Engineer-
ing, 1(1): 9–27.
Moon, R.E. 1998. Fixed Expressions and Idioms in
English: A Corpus-based Approach. Oxford: Clar-
endon Press.
</reference>
<page confidence="0.990635">
94
</page>
<reference confidence="0.999640111111111">
Prochasson, E., Fung, P. (2011). Rare word transla-
tion extraction from aligned comparable docu-
ments. In: Proceedings of ACL-HLT. Portland .
Rapp, R. (1995). Identifying word translations in non-
parallel texts. In: Proceedings of the 33rd Annual
Meeting of the ACL. Cambridge, MA, 320-322.
Rapp, R. (1996). Die Berechnung von Assoziationen.
Hildesheim: Olms.
Rapp, R. (1999). Automatic identification of word
translations from unrelated English and German
corpora. Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, Col-
lege Park, Maryland. 519–526.
Rapp, R., Sharoff, S., Babych, B. (2012). Identifying
word translations from comparable documents
without a seed lexicon. In: Proceedings of the 8th
Language Resources and Evaluation Conference,
LREC 2012, Istanbul.
Rayson, P.; Garside, R. (2000). Comparing corpora
using frequency profiling. Proceedings of the
Workshop on Comparing Corpora (WCC &apos;00 ), Vol-
ume 9, 1–6.
Robitaille, X., Sasaki, Y., Tonoike, M., Sato, S., Utsu-
ro, T. (2006). Compiling French-Japanese termi-
nologies from the web. In: Proceedings of the 11th
Conference of EACL, Trento, Italy, 225-232.
Rumelhart, D.E.; McClelland, J.L. (1987). Parallel
Distributed Processing. Explorations in the Micro-
structure of Cognition. Volume 1: Foundations.
MIT Press.
Schafer, C., Yarowsky, D (2002).: Inducing transla-
tion lexicons via diverse similarity measures and
bridge languages. In: Proceedings of CoNLL.
Schmid, H. (1994). Probabilistic part-of-speech tag-
ging using decision trees. International Conference
on New Methods in Language Processing, 44–49.
</reference>
<page confidence="0.999067">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941189">
<title confidence="0.9992365">Extracting Multiword from Aligned Comparable Documents</title>
<author confidence="0.999186">Reinhard Rapp Serge Sharoff</author>
<affiliation confidence="0.9853315">Aix-Marseille Université, Laboratoire University of Leeds d&apos;Informatique Fondamentale Centre for Translation Studies</affiliation>
<address confidence="0.999177">F-13288 Marseille, France Leeds, LS2 9JT, UK</address>
<email confidence="0.97535">reinhardrapp@gmx.deS.Sharoff@leeds.ac.uk</email>
<abstract confidence="0.999722083333333">Most previous attempts to identify translations of multiword expressions using comparable corpora relied on dictionaries of single words. The translation of a multiword was then constructed from the translations of its components. In contrast, in this work we try to determine the translation of a multiword unit by analyzing its contextual behaviour in aligned comparable documents, thereby not presupposing any given dictionary. Whereas with this method translation results for single words are rather good, the results for multiword units are considerably worse. This is an indication that the type of multiword expressions considered here is too infrequent to provide a sufficient amount of contextual information. Thus indirectly it is confirmed that it should make sense to look at the contextual behaviour of the components of a multiword expression individually, and to combine the results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Babych</author>
<author>S Sharoff</author>
<author>A Hartley</author>
<author>O Mudraya</author>
</authors>
<title>Assisting Translators in Indirect Lexical Transfer.</title>
<date>2007</date>
<booktitle>Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics ACL 2007,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3440" citStr="Babych et al. (2007)" startWordPosition="534" endWordPosition="537">ectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some previous work on identifying the translations of multiword units using comparable corpora, such as Robitaille et al. (2006), Babych et al. (2007), Daille &amp; Morin (2012); Delpech et al. (2012). However, none of this work utilizes aligned comparable documents, and the underlying assumption is that the translation of a multiword unit can be determined by looking at its components individually, and by merging the results. In contrast, we try to explore whether the translation of a multiword unit can be determined solely by looking at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning of a multiword unit is</context>
<context position="7203" citStr="Babych et al., 2007" startWordPosition="1125" endWordPosition="1128">d, 1994). From the pre-processed English and German Wikipedia, we extracted the multiword expressions using two simple principles, a negative POS filter and a containment filter. The negative POS filter operates in a rule-based fashion on the complete list of n-grams by removing the unlikely candidates according to a set of constraints, such as the presence of determiners or prepositions at the edges of expressions, see a similar method used by (Justeson &amp; Katz, 1995). With some further extensions this was also used to produce the multiword lists for the dictionary of translation equivalents (Babych et al., 2007). We did not use positive shallow filters. These would need to capture the relatively complex structure of the noun, verb and prepositional phrases, while avoiding noise. This can often lead to a lack of recall when more complex constructions cannot be captured. In contrast, negative shallow filters simply avoid obvious noise, while passing other multiword expressions (MWEs) through, which are very often legitimate syntactic constructions in a language in question. For example, the following English filters1 rejected personal pronouns (PP) and conjunctions (CC) at the edges of expressions (usi</context>
</contexts>
<marker>Babych, Sharoff, Hartley, Mudraya, 2007</marker>
<rawString>Babych, B., Sharoff, S., Hartley, A., and Mudraya, O. (2007). Assisting Translators in Indirect Lexical Transfer. Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics ACL 2007, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daille</author>
<author>E Morin</author>
</authors>
<title>Revising the compositional method for terminology acquisition from comparable corpora.</title>
<date>2012</date>
<booktitle>Proceedings of Coling 2012,</booktitle>
<location>Mumbai.</location>
<contexts>
<context position="3463" citStr="Daille &amp; Morin (2012)" startWordPosition="538" endWordPosition="541">alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some previous work on identifying the translations of multiword units using comparable corpora, such as Robitaille et al. (2006), Babych et al. (2007), Daille &amp; Morin (2012); Delpech et al. (2012). However, none of this work utilizes aligned comparable documents, and the underlying assumption is that the translation of a multiword unit can be determined by looking at its components individually, and by merging the results. In contrast, we try to explore whether the translation of a multiword unit can be determined solely by looking at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning of a multiword unit is determined by 87 Proce</context>
</contexts>
<marker>Daille, Morin, 2012</marker>
<rawString>Daille, B.; Morin, E. (2012). Revising the compositional method for terminology acquisition from comparable corpora. Proceedings of Coling 2012, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Delpech</author>
<author>B Daille</author>
<author>E Morin</author>
<author>C Lemaire</author>
</authors>
<title>Extraction of domain-specific bilingual lexicon from comparable corpora: compositional translation and ranking.</title>
<date>2012</date>
<booktitle>Proceedings of Coling 2012,</booktitle>
<location>Mumbai.</location>
<contexts>
<context position="3486" citStr="Delpech et al. (2012)" startWordPosition="542" endWordPosition="545">sy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some previous work on identifying the translations of multiword units using comparable corpora, such as Robitaille et al. (2006), Babych et al. (2007), Daille &amp; Morin (2012); Delpech et al. (2012). However, none of this work utilizes aligned comparable documents, and the underlying assumption is that the translation of a multiword unit can be determined by looking at its components individually, and by merging the results. In contrast, we try to explore whether the translation of a multiword unit can be determined solely by looking at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning of a multiword unit is determined by 87 Proceedings of the 3rd Works</context>
</contexts>
<marker>Delpech, Daille, Morin, Lemaire, 2012</marker>
<rawString>Delpech, E.; Daille, B.; Morin, E., Lemaire, C. (2012). Extraction of domain-specific bilingual lexicon from comparable corpora: compositional translation and ranking. Proceedings of Coling 2012, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>S Finch</author>
</authors>
<title>A statistical wordlevel translation model for comparable corpora. In:</title>
<date>2000</date>
<booktitle>Proceedings of the Conference on Content-Based Multimedia Information Access (RIAO).</booktitle>
<contexts>
<context position="2422" citStr="Diab &amp; Finch (2000)" startWordPosition="365" endWordPosition="368">mption is effectively an extension of Harris&apos; (1954) distributional hypotheses to the multilingual case. This work was further elaborated in some by now classical papers, such as Fung &amp; Yee (1998) and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab &amp; Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer &amp; Yarowsky (2002), Hassan &amp; Mihalcea (2009), Prochasson &amp; Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pur</context>
</contexts>
<marker>Diab, Finch, 2000</marker>
<rawString>Diab, M., Finch, S. (2000): A statistical wordlevel translation model for comparable corpora. In: Proceedings of the Conference on Content-Based Multimedia Information Access (RIAO).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Friedl</author>
</authors>
<title>Mastering Regular Expressions.</title>
<date>2002</date>
<tech>O&apos;Reilly.</tech>
<contexts>
<context position="8698" citStr="Friedl, 2002" startWordPosition="1366" endWordPosition="1367">mber of patterns: 18 patterns were used for English, 11 for German. The containment filter further rejects MWEs by removing those that regularly occur as a part of a longer acceptable MWE. For example, graphical user is an acceptable expression passing through the POS filter, but it is rejected by the containment filter since the overwhelming majority of its uses are in the containing MWE graphical user interface (1507 vs 1304 uses in Wikipedia, since MWEs are still possible, e.g., graphical user environment). 1 We use here the standard notation for regular expressions as implemented in Perl (Friedl, 2002). For example, &apos;^&apos; means &apos;beginning of line&apos; and &apos;$&apos; means &apos;end of line&apos;. 88 English keyterms for &apos;Airbus 320 family&apos; Score f Keyterm 34.88 4 final_JJ assembly_NN 31.22 3 firm_NN order_NN 30.73 3 series_NN aircraft_NN 29.07 4 flight_NN control_NN 27.38 3 wing_NN area_NN 23.26 3 final_JJ approach_NN 22.19 2 lose_VV life_NN 20.63 6 passenger_NN and_CC crew_NN 17.54 2 first_JJ derivative_NN 17.34 2 fly-by-wire_NN flight_NN control_NN 16.63 3 flight_NN deck_NN 16.41 2 crew_NN die_VV 15.08 2 pilot_NN error_NN 14.98 2 passenger_NN capacity_NN 14.38 2 turbofan_NN engine_NN 14.03 2 development_NN cost</context>
</contexts>
<marker>Friedl, 2002</marker>
<rawString>Friedl, J. (2002). Mastering Regular Expressions. O&apos;Reilly.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
</authors>
<title>Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus. In:</title>
<date>1995</date>
<booktitle>Proceedings of the Third Annual Workshop on Very Large Corpora,</booktitle>
<pages>173--183</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="1360" citStr="Fung (1995)" startWordPosition="194" endWordPosition="195">ith this method translation results for single words are rather good, the results for multiword units are considerably worse. This is an indication that the type of multiword expressions considered here is too infrequent to provide a sufficient amount of contextual information. Thus indirectly it is confirmed that it should make sense to look at the contextual behaviour of the components of a multiword expression individually, and to combine the results. 1 Introduction The task of identifying word translations from comparable text has received considerable attention. Some early papers include Fung (1995) and Rapp (1995). Fung (1995) utilized a context heterogeneity measure, thereby assuming that words with productive context in one language translate to words with productive context in another language, and words with rigid context translate into words with rigid context. In contrast, the underlying assumption in Rapp (1995) was that words which are translations of each other show similar co-occurrence patterns across languages. This assumption is effectively an extension of Harris&apos; (1954) distributional hypotheses to the multilingual case. This work was further elaborated in some by now clas</context>
</contexts>
<marker>Fung, 1995</marker>
<rawString>Fung, P. (1995). Compiling bilingual lexicon entries from a non-parallel English-Chinese corpus. In: Proceedings of the Third Annual Workshop on Very Large Corpora, Boston, Massachusetts. 173-183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>L Y Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>Proceedings of COLING/ACL</booktitle>
<pages>414--420</pages>
<location>Montreal,</location>
<contexts>
<context position="1999" citStr="Fung &amp; Yee (1998)" startWordPosition="291" endWordPosition="294"> (1995) utilized a context heterogeneity measure, thereby assuming that words with productive context in one language translate to words with productive context in another language, and words with rigid context translate into words with rigid context. In contrast, the underlying assumption in Rapp (1995) was that words which are translations of each other show similar co-occurrence patterns across languages. This assumption is effectively an extension of Harris&apos; (1954) distributional hypotheses to the multilingual case. This work was further elaborated in some by now classical papers, such as Fung &amp; Yee (1998) and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab &amp; Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Sch</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Fung, P.; Yee, L. Y. (1998). An IR approach for translating new words from nonparallel, comparable texts. Proceedings of COLING/ACL 1998, Montreal, Canada. 414-420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Haghighi</author>
<author>P Liang</author>
<author>T Berg-Kirkpatrick</author>
<author>D Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora. In:</title>
<date>2008</date>
<booktitle>Proceedings of ACL-HLT 2008,</booktitle>
<pages>771--779</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2449" citStr="Haghighi et al. (2008)" startWordPosition="370" endWordPosition="373"> extension of Harris&apos; (1954) distributional hypotheses to the multilingual case. This work was further elaborated in some by now classical papers, such as Fung &amp; Yee (1998) and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab &amp; Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer &amp; Yarowsky (2002), Hassan &amp; Mihalcea (2009), Prochasson &amp; Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. H</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Haghighi, A., Liang, P., Berg-Kirkpatrick, T., Klein, D. (2008): Learning bilingual lexicons from monolingual corpora. In: Proceedings of ACL-HLT 2008, Columbus, Ohio. 771-779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>Distributional structure.</title>
<date>1954</date>
<journal>Word,</journal>
<volume>10</volume>
<issue>23</issue>
<pages>146--162</pages>
<marker>Harris, 1954</marker>
<rawString>Harris, Z.S. (1954). Distributional structure. Word, 10(23), 146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hassan</author>
<author>R Mihalcea</author>
</authors>
<title>Cross-lingual semantic relatedness using encyclopedic knowledge. In:</title>
<date>2009</date>
<booktitle>Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2647" citStr="Hassan &amp; Mihalcea (2009)" startWordPosition="403" endWordPosition="406">on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab &amp; Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer &amp; Yarowsky (2002), Hassan &amp; Mihalcea (2009), Prochasson &amp; Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and wh</context>
</contexts>
<marker>Hassan, Mihalcea, 2009</marker>
<rawString>Hassan, S., Mihalcea, R. (2009): Cross-lingual semantic relatedness using encyclopedic knowledge. In: Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Justeson</author>
<author>S M Katz</author>
</authors>
<title>Techninal terminology: some linguistic properties and an algorithm for identification in text.</title>
<date>1995</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>9--27</pages>
<contexts>
<context position="7055" citStr="Justeson &amp; Katz, 1995" startWordPosition="1102" endWordPosition="1105">istic processing steps included tokenisation, tagging and lemmatisation using the default UTF-8 versions of the respective TreeTagger resources (Schmid, 1994). From the pre-processed English and German Wikipedia, we extracted the multiword expressions using two simple principles, a negative POS filter and a containment filter. The negative POS filter operates in a rule-based fashion on the complete list of n-grams by removing the unlikely candidates according to a set of constraints, such as the presence of determiners or prepositions at the edges of expressions, see a similar method used by (Justeson &amp; Katz, 1995). With some further extensions this was also used to produce the multiword lists for the dictionary of translation equivalents (Babych et al., 2007). We did not use positive shallow filters. These would need to capture the relatively complex structure of the noun, verb and prepositional phrases, while avoiding noise. This can often lead to a lack of recall when more complex constructions cannot be captured. In contrast, negative shallow filters simply avoid obvious noise, while passing other multiword expressions (MWEs) through, which are very often legitimate syntactic constructions in a lang</context>
</contexts>
<marker>Justeson, Katz, 1995</marker>
<rawString>Justeson, J.S.; Katz, S.M. (1995). Techninal terminology: some linguistic properties and an algorithm for identification in text. Natural Language Engineering, 1(1): 9–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Moon</author>
</authors>
<title>Fixed Expressions and Idioms in English: A Corpus-based Approach.</title>
<date>1998</date>
<publisher>Clarendon Press.</publisher>
<location>Oxford:</location>
<contexts>
<context position="4439" citStr="Moon (1998)" startWordPosition="696" endWordPosition="697">at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning of a multiword unit is determined by 87 Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 87–95, Gothenburg, Sweden, April 27, 2014. c�2014 Association for Computational Linguistics the contextual behavior of the full unit, or by the contextual behavior of its components (or by a mix of both). But multiword expressions are of complex nature, as expressed e.g. by Moon (1998): &amp;quot;there is no unified phenomenon to describe but rather a complex of features that interact in various, often untidy, ways and represent a broad continuum between non-compositional (or idiomatic) and compositional groups of words.&amp;quot; The current paper is an attempt to systematically approach one aspect of this complexity. 2 Approach Our approach is based on the usual assumption that there is a correlation between the patterns of word-co-occurrence across languages. However, instead of presupposing a bilingual dictionary it only requires pre-aligned comparable documents, i.e. small or medium siz</context>
</contexts>
<marker>Moon, 1998</marker>
<rawString>Moon, R.E. 1998. Fixed Expressions and Idioms in English: A Corpus-based Approach. Oxford: Clarendon Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Prochasson</author>
<author>P Fung</author>
</authors>
<title>Rare word translation extraction from aligned comparable documents. In:</title>
<date>2011</date>
<booktitle>Proceedings of ACL-HLT. Portland .</booktitle>
<contexts>
<context position="2673" citStr="Prochasson &amp; Fung (2011)" startWordPosition="407" endWordPosition="410">ard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab &amp; Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer &amp; Yarowsky (2002), Hassan &amp; Mihalcea (2009), Prochasson &amp; Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words</context>
</contexts>
<marker>Prochasson, Fung, 2011</marker>
<rawString>Prochasson, E., Fung, P. (2011). Rare word translation extraction from aligned comparable documents. In: Proceedings of ACL-HLT. Portland .</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Identifying word translations in nonparallel texts. In:</title>
<date>1995</date>
<booktitle>Proceedings of the 33rd Annual Meeting of the ACL.</booktitle>
<pages>320--322</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="1376" citStr="Rapp (1995)" startWordPosition="197" endWordPosition="198">translation results for single words are rather good, the results for multiword units are considerably worse. This is an indication that the type of multiword expressions considered here is too infrequent to provide a sufficient amount of contextual information. Thus indirectly it is confirmed that it should make sense to look at the contextual behaviour of the components of a multiword expression individually, and to combine the results. 1 Introduction The task of identifying word translations from comparable text has received considerable attention. Some early papers include Fung (1995) and Rapp (1995). Fung (1995) utilized a context heterogeneity measure, thereby assuming that words with productive context in one language translate to words with productive context in another language, and words with rigid context translate into words with rigid context. In contrast, the underlying assumption in Rapp (1995) was that words which are translations of each other show similar co-occurrence patterns across languages. This assumption is effectively an extension of Harris&apos; (1954) distributional hypotheses to the multilingual case. This work was further elaborated in some by now classical papers, su</context>
</contexts>
<marker>Rapp, 1995</marker>
<rawString>Rapp, R. (1995). Identifying word translations in nonparallel texts. In: Proceedings of the 33rd Annual Meeting of the ACL. Cambridge, MA, 320-322.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Die Berechnung von Assoziationen.</title>
<date>1996</date>
<location>Hildesheim: Olms.</location>
<contexts>
<context position="11156" citStr="Rapp (1996)" startWordPosition="1739" endWordPosition="1740">German Wikipedia documents are typically not translations of each other, we cannot apply the usual procedures and tools as available for parallel texts (e.g. the Gale &amp; Church sentence aligner and the Giza++ word alignment tool). Instead we conduct a two step procedure: 1. We first extract salient terms (single word or multiword) from each of the documents. 2. We then align these terms across languages using an approach inspired by a connectionist (Rumelhart &amp; McClelland, 1987) WinnerTakes-It-All Network. The respective algorithm is called WINTIAN and is described in Rapp et al. (2012) and in Rapp (1996). For term extraction, the occurrence frequency of a term in a particular document is compared to its average occurrence frequency in all Wikipedia documents, whereby a high discrepancy indicates a strong keyness. Following Rayson &amp; Garside (2000), we use the log-likelihood score to measure keyness, since it has been shown to be robust to small numbers of instances. This robustness is important as many Wikipedia articles are rather short. This procedure leads to multiword keyterms as exemplified in Table 1 for the Wikipedia entry Airbus A320 family. Because of compounding in German, many singl</context>
<context position="12613" citStr="Rapp (1996)" startWordPosition="1965" endWordPosition="1966">ms from the Wikipedia articles is relative data sparseness. Usually, the frequency of an individual multiword expression within a Wikipedia article is between 2 and 4. Therefore we had to use a less conservative threshold of 6.63 (1% significance level) rather than the more standard 15.13 (0.01% significance level) for the log-likelihood score (see Rayson &amp; Garside, 2000, and http://ucrel. lancs.ac.uk/llwizard.html). 2.3 Term alignment The WINTIAN algorithm is used for establishing term alignments across languages. As a more detailed technical description is given in Rapp et al. (2012) and in Rapp (1996), we only briefly describe this algorithm here, thereby focusing on the neural network analogy. The algorithm can be considered as an artificial neural network where the nodes are all English and German 89 terms occurring in the keyterm lists. Each English term has connections to all German terms. The connections are all initialized with values of one when the algorithm is started, but will serve as a measure of the translation probabilities after the completion of the algorithm. One after the other, the network is fed with the pairs of corresponding keyterm lists. Each German term activates t</context>
</contexts>
<marker>Rapp, 1996</marker>
<rawString>Rapp, R. (1996). Die Berechnung von Assoziationen. Hildesheim: Olms.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>519--526</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="2015" citStr="Rapp (1999)" startWordPosition="296" endWordPosition="297">text heterogeneity measure, thereby assuming that words with productive context in one language translate to words with productive context in another language, and words with rigid context translate into words with rigid context. In contrast, the underlying assumption in Rapp (1995) was that words which are translations of each other show similar co-occurrence patterns across languages. This assumption is effectively an extension of Harris&apos; (1954) distributional hypotheses to the multilingual case. This work was further elaborated in some by now classical papers, such as Fung &amp; Yee (1998) and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab &amp; Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer &amp; Yarowsky </context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Rapp, R. (1999). Automatic identification of word translations from unrelated English and German corpora. Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, College Park, Maryland. 519–526.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rapp</author>
<author>S Sharoff</author>
<author>B Babych</author>
</authors>
<title>Identifying word translations from comparable documents without a seed lexicon. In:</title>
<date>2012</date>
<booktitle>Proceedings of the 8th Language Resources and Evaluation Conference, LREC 2012,</booktitle>
<location>Istanbul.</location>
<contexts>
<context position="2696" citStr="Rapp et al. (2012)" startWordPosition="412" endWordPosition="415"> a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab &amp; Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer &amp; Yarowsky (2002), Hassan &amp; Mihalcea (2009), Prochasson &amp; Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some p</context>
<context position="6108" citStr="Rapp et al. (2012)" startWordPosition="953" endWordPosition="956">ticles referring to the same headword in different languages. Given that each Wikipedia community contributes in its own language, only occasionally an article connected in this way will be an exact translation of a foreign language article, and in most cases the contents will be rather different. On the positive side, the link structure of the interlanguage links tends to be quite dense. For example, of the 1,114,696 German Wikipedia articles, 603,437 have a link to the corresponding English Wikipedia article. 2.1 Pre-processing and MWE extraction We used the same versions of Wikipedia as in Rapp et al. (2012) and used the same processing. After download, each Wikipedia was minimally processed to extract the plain text contents of the articles. In this process all templates, e.g. &apos;infoboxes&apos;, as well as tables were removed, and we kept only the webpages with more than 500 characters of running text (including white space). Linguistic processing steps included tokenisation, tagging and lemmatisation using the default UTF-8 versions of the respective TreeTagger resources (Schmid, 1994). From the pre-processed English and German Wikipedia, we extracted the multiword expressions using two simple princi</context>
<context position="11137" citStr="Rapp et al. (2012)" startWordPosition="1733" endWordPosition="1736">s the aligned English and German Wikipedia documents are typically not translations of each other, we cannot apply the usual procedures and tools as available for parallel texts (e.g. the Gale &amp; Church sentence aligner and the Giza++ word alignment tool). Instead we conduct a two step procedure: 1. We first extract salient terms (single word or multiword) from each of the documents. 2. We then align these terms across languages using an approach inspired by a connectionist (Rumelhart &amp; McClelland, 1987) WinnerTakes-It-All Network. The respective algorithm is called WINTIAN and is described in Rapp et al. (2012) and in Rapp (1996). For term extraction, the occurrence frequency of a term in a particular document is compared to its average occurrence frequency in all Wikipedia documents, whereby a high discrepancy indicates a strong keyness. Following Rayson &amp; Garside (2000), we use the log-likelihood score to measure keyness, since it has been shown to be robust to small numbers of instances. This robustness is important as many Wikipedia articles are rather short. This procedure leads to multiword keyterms as exemplified in Table 1 for the Wikipedia entry Airbus A320 family. Because of compounding in</context>
<context position="12594" citStr="Rapp et al. (2012)" startWordPosition="1959" endWordPosition="1962">obtaining multiword keyterms from the Wikipedia articles is relative data sparseness. Usually, the frequency of an individual multiword expression within a Wikipedia article is between 2 and 4. Therefore we had to use a less conservative threshold of 6.63 (1% significance level) rather than the more standard 15.13 (0.01% significance level) for the log-likelihood score (see Rayson &amp; Garside, 2000, and http://ucrel. lancs.ac.uk/llwizard.html). 2.3 Term alignment The WINTIAN algorithm is used for establishing term alignments across languages. As a more detailed technical description is given in Rapp et al. (2012) and in Rapp (1996), we only briefly describe this algorithm here, thereby focusing on the neural network analogy. The algorithm can be considered as an artificial neural network where the nodes are all English and German 89 terms occurring in the keyterm lists. Each English term has connections to all German terms. The connections are all initialized with values of one when the algorithm is started, but will serve as a measure of the translation probabilities after the completion of the algorithm. One after the other, the network is fed with the pairs of corresponding keyterm lists. Each Germ</context>
<context position="14834" citStr="Rapp et al., 2012" startWordPosition="2330" endWordPosition="2333">portant as the corresponding keyterm lists tend to be very noisy and, especially for multiword expressions, in many cases may contain hardly any terms that are actually translations of each other. Reasons are that corresponding Wikipedia articles are often written from different perspectives, that the variation in length can be considerable across languages, and that multiword expressions tend to show more variability with regard to their translations than single words. 3 Results and evaluation 3.1 Results for single words In this subsection we report on our previous results for single words (Rapp et al., 2012) as these serve as a baseline for our new results concerning multiword units. The WINTIAN algorithm requires as input vocabularies of the source and the target language. For both English and German, we constructed these as follows: Based on the keyword lists for the respective Wikipedia, we counted the number of occurrences of each keyword, and then applied a threshold of five, i.e. all keywords with a lower frequency were eliminated. The reasoning behind this is that rare keywords are of not much use due to data sparseness. This resulted in a vocabulary size of 133,806 for English, and of 144</context>
</contexts>
<marker>Rapp, Sharoff, Babych, 2012</marker>
<rawString>Rapp, R., Sharoff, S., Babych, B. (2012). Identifying word translations from comparable documents without a seed lexicon. In: Proceedings of the 8th Language Resources and Evaluation Conference, LREC 2012, Istanbul.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Rayson</author>
<author>R Garside</author>
</authors>
<title>Comparing corpora using frequency profiling.</title>
<date>2000</date>
<booktitle>Proceedings of the Workshop on Comparing Corpora (WCC &apos;00 ), Volume 9,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="11403" citStr="Rayson &amp; Garside (2000)" startWordPosition="1776" endWordPosition="1779">ad we conduct a two step procedure: 1. We first extract salient terms (single word or multiword) from each of the documents. 2. We then align these terms across languages using an approach inspired by a connectionist (Rumelhart &amp; McClelland, 1987) WinnerTakes-It-All Network. The respective algorithm is called WINTIAN and is described in Rapp et al. (2012) and in Rapp (1996). For term extraction, the occurrence frequency of a term in a particular document is compared to its average occurrence frequency in all Wikipedia documents, whereby a high discrepancy indicates a strong keyness. Following Rayson &amp; Garside (2000), we use the log-likelihood score to measure keyness, since it has been shown to be robust to small numbers of instances. This robustness is important as many Wikipedia articles are rather short. This procedure leads to multiword keyterms as exemplified in Table 1 for the Wikipedia entry Airbus A320 family. Because of compounding in German, many single-word German expressions are translated into multiword expressions in English. So we chose to include single-word expressions into the German candidate list for alignment with English multiwords. One of the problems in obtaining multiword keyterm</context>
</contexts>
<marker>Rayson, Garside, 2000</marker>
<rawString>Rayson, P.; Garside, R. (2000). Comparing corpora using frequency profiling. Proceedings of the Workshop on Comparing Corpora (WCC &apos;00 ), Volume 9, 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Robitaille</author>
<author>Y Sasaki</author>
<author>M Tonoike</author>
<author>S Sato</author>
<author>T Utsuro</author>
</authors>
<title>Compiling French-Japanese terminologies from the web. In:</title>
<date>2006</date>
<booktitle>Proceedings of the 11th Conference of EACL,</booktitle>
<pages>225--232</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="3418" citStr="Robitaille et al. (2006)" startWordPosition="530" endWordPosition="533">rallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to multiword expressions and when applied to single words. There has been some previous work on identifying the translations of multiword units using comparable corpora, such as Robitaille et al. (2006), Babych et al. (2007), Daille &amp; Morin (2012); Delpech et al. (2012). However, none of this work utilizes aligned comparable documents, and the underlying assumption is that the translation of a multiword unit can be determined by looking at its components individually, and by merging the results. In contrast, we try to explore whether the translation of a multiword unit can be determined solely by looking at its contextual behavior, i.e. whether it is possible to also apply the standard approach as successfully used for single words. The underlying fundamental question is whether the meaning </context>
</contexts>
<marker>Robitaille, Sasaki, Tonoike, Sato, Utsuro, 2006</marker>
<rawString>Robitaille, X., Sasaki, Y., Tonoike, M., Sato, S., Utsuro, T. (2006). Compiling French-Japanese terminologies from the web. In: Proceedings of the 11th Conference of EACL, Trento, Italy, 225-232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>J L McClelland</author>
</authors>
<date>1987</date>
<booktitle>Parallel Distributed Processing. Explorations in the Microstructure of Cognition. Volume 1: Foundations.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="11027" citStr="Rumelhart &amp; McClelland, 1987" startWordPosition="1715" endWordPosition="1718">erb; AR = article; AP = article+preposition; JJ = adjective; CC = conjunction; RP = preposition. 2.2 Keyterm extraction As the aligned English and German Wikipedia documents are typically not translations of each other, we cannot apply the usual procedures and tools as available for parallel texts (e.g. the Gale &amp; Church sentence aligner and the Giza++ word alignment tool). Instead we conduct a two step procedure: 1. We first extract salient terms (single word or multiword) from each of the documents. 2. We then align these terms across languages using an approach inspired by a connectionist (Rumelhart &amp; McClelland, 1987) WinnerTakes-It-All Network. The respective algorithm is called WINTIAN and is described in Rapp et al. (2012) and in Rapp (1996). For term extraction, the occurrence frequency of a term in a particular document is compared to its average occurrence frequency in all Wikipedia documents, whereby a high discrepancy indicates a strong keyness. Following Rayson &amp; Garside (2000), we use the log-likelihood score to measure keyness, since it has been shown to be robust to small numbers of instances. This robustness is important as many Wikipedia articles are rather short. This procedure leads to mult</context>
<context position="13775" citStr="Rumelhart &amp; McClelland, 1987" startWordPosition="2157" endWordPosition="2160"> pairs of corresponding keyterm lists. Each German term activates the corresponding German node with an activity of one. This activity is then propagated to all English terms occurring in the corresponding list of keyterms. The distribution of the activity is not equal, but in proportion to the connecting weights. This unequal distribution has no effect at the beginning when all weights are one, but later on leads to rapid activity increases for pairs of terms which often occur in corresponding keyterm lists. The assumption is that these are translations of each other. Using Hebbian learning (Rumelhart &amp; McClelland, 1987) the activity changes are stored in the connections. We use a heuristic to avoid the effect that frequent keyterms dominate the network: When more than 50 of the connections to a particular English node have weights higher than one, the weakest 20 of them are reset to one. This way only translations which are frequently confirmed can build up high weights. It turned out that the algorithm shows a robust behaviour in practice, which is important as the corresponding keyterm lists tend to be very noisy and, especially for multiword expressions, in many cases may contain hardly any terms that are</context>
</contexts>
<marker>Rumelhart, McClelland, 1987</marker>
<rawString>Rumelhart, D.E.; McClelland, J.L. (1987). Parallel Distributed Processing. Explorations in the Microstructure of Cognition. Volume 1: Foundations. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Schafer</author>
<author>D Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages. In:</title>
<date>2002</date>
<booktitle>Proceedings of CoNLL.</booktitle>
<contexts>
<context position="2621" citStr="Schafer &amp; Yarowsky (2002)" startWordPosition="399" endWordPosition="402">98) and Rapp (1999). Based on these papers, the standard approach is to start from a dictionary of seed words, and to assume that the words occurring in the context of a source language word have similar meanings as the words occurring in the context of its target language translation. There have been suggestions to eliminate the need for the seed dictionary. However, most attempts, such as Rapp (1995), Diab &amp; Finch (2000) and Haghighi et al. (2008) did not work to an extent that the results would be useful for practical purposes. Only recently a more promising approach has been investigated: Schafer &amp; Yarowsky (2002), Hassan &amp; Mihalcea (2009), Prochasson &amp; Fung (2011) and Rapp et al. (2012) look at aligned comparable documents and deal with them in analogy to the treatment of aligned parallel sentences, i.e. effectively doing a word alignment in a very noisy environment. This approach has been rather successful and it was possible to improve on previous results. This is therefore the approach which we will pursue in the current paper. However, in contrast to the above mentioned papers the focus of our work is on multiword expressions, and we will compare the performance of our algorithm when applied to mu</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Schafer, C., Yarowsky, D (2002).: Inducing translation lexicons via diverse similarity measures and bridge languages. In: Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="6591" citStr="Schmid, 1994" startWordPosition="1028" endWordPosition="1029">g English Wikipedia article. 2.1 Pre-processing and MWE extraction We used the same versions of Wikipedia as in Rapp et al. (2012) and used the same processing. After download, each Wikipedia was minimally processed to extract the plain text contents of the articles. In this process all templates, e.g. &apos;infoboxes&apos;, as well as tables were removed, and we kept only the webpages with more than 500 characters of running text (including white space). Linguistic processing steps included tokenisation, tagging and lemmatisation using the default UTF-8 versions of the respective TreeTagger resources (Schmid, 1994). From the pre-processed English and German Wikipedia, we extracted the multiword expressions using two simple principles, a negative POS filter and a containment filter. The negative POS filter operates in a rule-based fashion on the complete list of n-grams by removing the unlikely candidates according to a set of constraints, such as the presence of determiners or prepositions at the edges of expressions, see a similar method used by (Justeson &amp; Katz, 1995). With some further extensions this was also used to produce the multiword lists for the dictionary of translation equivalents (Babych e</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, H. (1994). Probabilistic part-of-speech tagging using decision trees. International Conference on New Methods in Language Processing, 44–49.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
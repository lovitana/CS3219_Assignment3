<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051385">
<title confidence="0.991912">
Extraction of Nominal Multiword Expressions in French
</title>
<author confidence="0.992636">
Marie Dubremetz and Joakim Nivre
</author>
<affiliation confidence="0.957289666666667">
Uppsala university
Department of Linguistics and Philology
Uppsala, Sweden
</affiliation>
<sectionHeader confidence="0.974651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999834611111111">
Multiword expressions (MWEs) can be
extracted automatically from large corpora
using association measures, and tools like
mwetoolkit allow researchers to generate
training data for MWE extraction given a
tagged corpus and a lexicon. We use mwe-
toolkit on a sample of the French Europarl
corpus together with the French lexicon
Dela, and use Weka to train classifiers for
MWE extraction on the generated training
data. A manual evaluation shows that the
classifiers achieve 60–75% precision and
that about half of the MWEs found are
novel and not listed in the lexicon. We also
investigate the impact of the patterns used
to generate the training data and find that
this can affect the trade-off between preci-
sion and novelty.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988315">
In alphabetic languages, words are delimited by
spaces. Some words can combine to create a new
unit of meaning that we call a multiword expres-
sion (MWE). However, MWEs such as kick the
bucket must be distinguished from free combina-
tions of words such as kick the ball. A sequence of
several words is an MWE if “at least one of its syn-
tactic, distributional or semantic properties cannot
be deduced from the properties of its component”
(Silberztein and L.A.D.L., 1990). So how can we
extract them?
Statistical association measures have long been
used for MWE extraction (Pecina, 2010), and by
training supervised classifiers that use association
measures as features we can further improve the
quality of the extraction process. However, super-
vised machine learning requires annotated data,
which creates a bottleneck in the absence of large
corpora annotated for MWEs. In order to cir-
cumvent this bottleneck, mwetoolkit (Ramisch et
</bodyText>
<page confidence="0.992411">
72
</page>
<bodyText confidence="0.999874538461539">
al., 2010b) generates training instances by first ex-
tracting candidates that fit a certain part-of-speech
pattern, such as Noun-Noun or Noun-Adjective,
and then marking the candidates as positive or
negative instances depending on whether they can
be found in a given lexicon or not. Such a train-
ing set will presumably not contain any false pos-
itives (that is, candidates marked as positive in-
stances that are not real MWEs), but depending on
the coverage of the lexicon there will be a smaller
or larger proportion of false negatives. The ques-
tion is what quality can be obtained using such a
noisy training set. To the best of our knowledge,
we cannot find the answer for French in literature.
Indeed, Ramisch et al. (2012) compares the perfor-
mance of mwetoolkit with another toolkit on En-
glish and French corpora, but they never use the
data generated by mwetoolkit to train a model. In
contrast, Zilio et al. (2011) make a study involving
training a model but use it only on English and use
extra lexical resources to complement the machine
learning method, so their study does not focus just
on classifier evaluation.
This paper presents the first evaluation of mwe-
toolkit on French together with two resources very
commonly used by the French NLP community:
the tagger TreeTagger (Schmid, 1994) and the dic-
tionary Dela.1 Training and test data are taken
from the French Europarl corpus (Koehn, 2005)
and classifiers are trained using the Weka machine
learning toolkit (Hall et al., 2009). The primary
goal is to evaluate what level of precision can be
achieved for nominal MWEs, using a manual eval-
uation of MWEs extracted, and to what extent the
MWEs extracted are novel and can be used to en-
rich the lexicon. In addition, we will investigate
what effect the choice of part-of-speech patterns
used to generate the training data has on precision
and novelty. Our results indicate that classifiers
</bodyText>
<footnote confidence="0.999108">
1http://www-igm.univ-mlv.fr/˜unitex/
index.php?page=5&amp;html=bibliography.html
</footnote>
<note confidence="0.7250415">
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 72–76,
Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9842952">
achieve precision in the 60–75% range and that
about half of the MWEs found are novel ones. In
addition, it seems that the choice of patterns used
to generate the training data can affect the trade-
off between precision and novelty.
</bodyText>
<sectionHeader confidence="0.999776" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999766">
2.1 Extraction Techniques
</subsectionHeader>
<bodyText confidence="0.999965">
There is no unique definition of MWEs (Ramisch,
2012). In the literature on the subject, we no-
tice that manual MWE extraction often requires
several annotators native of the studied language.
Nevertheless, some techniques exist for selecting
automatically candidates that are more likely to be
the true ones. Candidates can be validated against
an external resource, such as a lexicon. It is pos-
sible also to check the frequency of candidates in
another corpus like the web. Villaviciencio (2005),
for example, uses number of hits on Google for
validating the likelihood of particle verbs.
However, as Ramisch (2012) states in his
introduction, MWE is an institutionalised phe-
nomenon. This means that an MWE is fre-
quently used and is part of the vocabulary of a
speaker as well as the simple words. It means
also that MWEs have specific statistical proper-
ties that have been studied. The results of those
studies are statistical measures such as dice score,
maximum likelihood estimate, pointwise mutual
information, T-score. As Islam et al. (2012) re-
mark in a study of Google Ngram, those measures
of association are language independent. And it
is demonstrated by Pecina (2008) that combining
different collocation measures using standard sta-
tistical classification methods improves over using
a single collocation measure. However, nowadays,
using only lexical association measures for extrac-
tion and validation of MWE is not considered the
most effective method. The tendency these last
years is to combine association measures with lin-
guistic features (Ramisch et al., 2010a; Pecina,
2008; Tsvetkov and Wintner, 2011).
</bodyText>
<subsectionHeader confidence="0.997848">
2.2 Mwetoolkit
</subsectionHeader>
<bodyText confidence="0.999993151515152">
Among the tools developed for extracting MWEs,
mwetoolkit is one of the most recent. Developed
by Ramisch et al. (2010b) it aims not only at ex-
tracting candidates for potential MWEs, but also
at extracting their association measures. Provided
that a lexicon of MWEs is available and provided
a preprocessed corpus, mwetoolkit makes it pos-
sible to train a machine learning system with the
association measures as features with a minimum
of implementation.
Ramisch et al. (2010b) provide experiments on
Portuguese, English and Greek. Zilio et al. (2011)
provide experiments with this tool as well. In the
latter study, after having trained a machine on bi-
gram MWEs, they try to extract full n-gram ex-
pressions from the Europarl corpus. They then
reuse the model obtained on bigrams for extraction
of full n-gram MWEs. Finally, they apply a second
filter for getting back the false negatives by check-
ing every MWE annotated as False by the algo-
rithm against a online dictionary. This method gets
a very good precision (over 87%) and recall (over
84%). However, we do not really know if this re-
sult is mostly due to the coverage of the dictionary
online. What is the contribution of machine learn-
ing in itself? Another question raised by this study
is the ability of a machine trained on one kind of
pattern (e.g., Noun-Adjective) to extract correctly
another kind of MWE pattern (e.g., Noun-Noun).
That is the reason why we will run three experi-
ments close to the one of Zilio et al. (2011) but
were the only changing parameter is the pattern
that we train our classifiers on.
</bodyText>
<sectionHeader confidence="0.913569" genericHeader="method">
3 Generating Training Data
</sectionHeader>
<subsectionHeader confidence="0.999959">
3.1 Choice of Patterns
</subsectionHeader>
<bodyText confidence="0.999992266666667">
In contrast to Zilio et al. (2011) we run our ex-
periment on French. The choice of a differ-
ent language requires an adaptation of the pat-
terns. French indeed, as a latin language, does
not show the same characteristic patterns as En-
glish. We know that there is a strong recurrence
of the pattern Noun-Adjective in bigram MWEs
in our lexicon (Silberztein and L.A.D.L., 1990,
p.82), and the next most frequent pattern is Noun-
Noun. Therefore we extract only candidates that
correspond to these patterns. And, since we have
two patterns, we will run two extra experiments
where our models will be trained only on one of
the patterns. In this way, we will discover how
sensitive the method is to the choice of pattern.
</bodyText>
<subsectionHeader confidence="0.997862">
3.2 Corpus
</subsectionHeader>
<bodyText confidence="0.9999658">
As Ramisch et al. (2012) we work on the French
Europarl corpus. We took the three first million
words of Europarl and divided it into three equal
parts (one million words each) for running our ex-
periments. The first part will be devoted at 80% to
</bodyText>
<page confidence="0.994728">
73
</page>
<bodyText confidence="0.999965166666667">
training and 20% to development test set, when
training classifiers on Noun-Adjective or Noun-
Noun patterns, or both. We use the second million
as a secondary development set that is not used in
this study. The third million is used as a final test
set and we will present results on this set.
</bodyText>
<subsectionHeader confidence="0.998747">
3.3 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999342">
For preprocessing we used the same processes as
described in Zilio et al. (2011). First we ran the
sentence splitter and the tokenizer provided with
the Europarl corpus. Then we ran TreeTagger
(Schmid, 1994) to obtain the tags and the lemmas.
</bodyText>
<subsectionHeader confidence="0.999782">
3.4 Extracting Data and Features
</subsectionHeader>
<bodyText confidence="0.999998153846154">
The mwetoolkit takes as input a preprocessed cor-
pus plus a lexicon and gives two main outputs: an
arff file which is a format adapted to the machine
learning framework Weka, and an XML file. At
the end of the process we obtain, for each candi-
date, a binary classification as an MWE (True) or
not (False) depending on whether it is contained
in the lexicon. For each candidate, we also ob-
tain the following features: maximum likelihood
estimate, pointwise mutual information, T-score,
dice coefficient, log-likelihood ratio. The machine
learning task is then to predict the class (True or
False) given the features of a candidate.
</bodyText>
<subsectionHeader confidence="0.998095">
3.5 Choice of a Lexicon in French
</subsectionHeader>
<bodyText confidence="0.99989275">
The evaluation part of mwetoolkit is furnished
with an internal English lexicon as a gold stan-
dard for evaluating bigram MWEs, but for French
it is necessary to provide an external resource.
We used as our gold standard the French dictio-
nary Dela (Silberztein and L.A.D.L., 1990), the
MWE part of which is called Delac. It is a gen-
eral purpose dictionary for NLP and it includes
100,000 MWE expressions, which is a reasonable
size for leading an experiment on the Europarl
corpus. Also the technical documentation of the
Delac (Silberztein and L.A.D.L., 1990, p.72) says
that this dictionary has been constructed by lin-
guists with reference to several dictionaries. So it
is a manually built resource that contains MWEs
only referenced in official lexicographical books.
</bodyText>
<subsectionHeader confidence="0.994827">
3.6 Processing
</subsectionHeader>
<bodyText confidence="0.999921666666667">
Thanks to mwetoolkit we extracted all the bi-
grams that correspond to the patterns Noun-
Adjective (NA), Noun-Noun (NN) and to both
Noun-Adjective and Noun-Noun (NANN) in our
three data sets and let mwetoolkit make an auto-
matic annotation by checking the presence of the
MWE candidates in the Delac. Note that the auto-
matic annotation was used only for training. The
final evaluation was done manually.
</bodyText>
<sectionHeader confidence="0.99041" genericHeader="method">
4 Training Classifiers
</sectionHeader>
<bodyText confidence="0.999986363636364">
For finding the best model we think that we have
to favour the recall of the positive candidates. In-
deed, when an MWE candidate is annotated as
True, it means that it is listed in the Dela, which
means that it is an officially listed MWE. How-
ever, if an MWE is not in the Dela, it does not
mean that the candidate does not fulfil all the cri-
teria for being an MWE. For this reason, obtaining
a good recall is much more difficult than getting a
good precision, but it is also the most important if
we stay on a lexicographical purpose.
</bodyText>
<subsectionHeader confidence="0.997893">
4.1 Training on NA
</subsectionHeader>
<bodyText confidence="0.9999258">
We tested several algorithms offered by Weka as
well as the training options suggested by Zilio et
al. (2011). We also tried to remove some features
and to keep only the most informative ones (MLE,
T-score and log-likelihood according to informa-
tion gain ratio) but we noticed each time a loss in
the recall. At the end with all the features kept and
for the purpose of evaluating NA MWE candidates
the best classification algorithm was the Bayesian
network.
</bodyText>
<subsectionHeader confidence="0.996564">
4.2 Training on NN
</subsectionHeader>
<bodyText confidence="0.999995375">
When training a model on NN MWEs, our aim
was to keep as much as possible the same condi-
tion for our three experiments. However, the NN
training set has definitely not the same properties
as the NA and NANN ones. The NN training set
is twenty-four times smaller than NA training set.
Most of the algorithms offered by Weka therefore
ended up with a dummy systematic classification
to the majority class False. The only exceptions
were ibk, ib1, hyperpipes, random trees and ran-
dom forest. We kept random forest because it gave
the best recall with a very good precision. We tried
several options and obtained the optimum results
with 8 trees each constructed while considering 3
random features, one seed, and unlimited depth of
trees. As well as for NA we kept all features.
</bodyText>
<subsectionHeader confidence="0.999759">
4.3 Training on NA+NN
</subsectionHeader>
<bodyText confidence="0.9649725">
For the training on NANN candidates we tried the
same models as for NN and for NA candidates.
</bodyText>
<page confidence="0.996836">
74
</page>
<bodyText confidence="0.999186">
The best result was obtained with the same algo-
rithm as for NA: Bayesian network.
</bodyText>
<sectionHeader confidence="0.990962" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999754714285714">
The data automatically annotated by mwetoolkit
could be used for training, but to properly evalu-
ate the precision of MWE extraction on new data
and not penalize the system for ‘false positives’
that are due to lack of coverage of the lexicon, we
needed to perform a manual annotation. To do so,
we randomly picked 100 candidates annotated as
True by each model (regardless if they were in the
Delac or not). We then annotated all such candi-
dates as True if they were found in Delac (without
further inspection) and otherwise classified them
manually following the definition of Silberztein
and L.A.D.L. (1990) and the intuition of a native
French speaker. The results are in Table 1.
</bodyText>
<table confidence="0.9998408">
Extracting NA NN NANN
NANN model model model
In Delac 40 f9.4 18 f7.2 28 f8.6
Not in Delac 34 f9.0 41 f9.2 38 f9.3
Precision 74 f8.4 59 f9.2 66 f9.0
</table>
<tableCaption confidence="0.998694">
Table 1: Performance of three different models
</tableCaption>
<bodyText confidence="0.98016747826087">
on the same corpus of Noun-Adjective and Noun-
Noun candidates. Percentages with 95% confi-
dence intervals, sample size = 100.
As we see in Table 1, the experiment reveals a pre-
cision ranging from almost 60% up to 74%. The
results of our comparative manual annotation indi-
cate that the model trained on NN candidates has
the capacity to find more MWEs not listed in our
lexicon (41 out of 59) even if it is the least pre-
cise model. On the other hand, we notice that the
model based on Noun-Adjective patterns is more
precise but at the same time extracts fewer MWEs
that are not already in the lexicon (34 out of 74).
Our mixed model confirms these two tendencies
with a performance in between (38 new MWEs out
of 66). Thus, the method appears to be sensitive to
the patterns used for training.
We notice during evaluation different kinds of
MWEs that are successfully extracted by models
but that are not listed in the Delac. Most of them
are the MWEs specific to Europarl (e.g., ‘dimen-
sion communautaire’, ‘l´egislation europ´eenne’2).
Another category are those MWEs that became
</bodyText>
<footnote confidence="0.329472">
2‘community scale’, ‘European legislation’
</footnote>
<bodyText confidence="0.999968870967742">
popular in the French language after the years
2000’s and therefore could not be included in the
Delac, released in 1997. Indeed by reading the
first paragraph of the French version of Europarl
we notice that the texts have been written after
1999. Of course, they are not the majority of the
successfully extracted MWEs but we still manage
to find up to 3 of them in a sample of 100 that we
checked (‘d´eveloppement durable’, ‘radiophonie
num´erique’, ‘site internet’3). Furthermore the cor-
pus in itself is already more than ten years old,
so in a text of 2014 we can expect to find even
more of them. Finally, there are MWEs that are
not in French (e.g., ‘Partido popular’), these, how-
ever, did not appear systematically in our samples.
It is tricky to learn statistical properties of
MWEs when, actually, we do not have all the in-
formation necessary for extracting the MWEs in
the corpus. Indeed, for this purpose the corpus
should ideally be read and annotated by humans.
However, we still managed to train models with
decent performance, even if it is likely that a lot of
candidates pre-annotated as False in the training
data were probably perfect MWEs. This means
that the Delac has covered enough MWEs for the
features to not appear as completely meaningless
and arbitrary. The final precision would never be
as good as it is, if the coverage had been not suffi-
cient enough. This shows that the method of auto-
matic annotation offered by mwetoolkit is reliable
given a lexicon as large as Delac.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999964846153846">
We wanted to know if the method of automatic
extraction and evaluation offered by mwetoolkit
could have a decent precision in French. We an-
notated automatically part of the Europarl corpus
given the lexical resource Dela as a gold stan-
dard and generated in this way annotated training
sets. Classifiers trained on this data using Weka
achieved a maximum precision of 74%, with about
half of the extracted MWEs being novel compared
to the lexicon. In addition, we found that the fi-
nal precision and novelty scores were sensitive to
the choice of patterns used to generate the training
data.
</bodyText>
<footnote confidence="0.751823">
3‘sustainable development’, ‘digital radio’,‘website’
</footnote>
<page confidence="0.99342">
75
</page>
<table confidence="0.7515838">
Yulia Tsvetkov and Shuly Wintner. 2011. Identifica-
tion of Multi-word Expressions by Combining Mul-
tiple Linguistic Information Sources. In Empiri-
cal Methods in Natural Language Processing, pages
836–845.
References
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA data mining software: an update.
SIGKDD Exploration Newsletter, 11(1):10–18.
</table>
<reference confidence="0.999418862068966">
Aminul Islam, Evangelos E Milios, and Vlado Ke-
selj. 2012. Comparing Word Relatedness Mea-
sures Based on Google n-grams. In COLING, Inter-
national Conference on Computational Linguistics
(Posters), pages 495–506, Mumbai, India.
Philipp Koehn. 2005. Europarl: A Parallel Corpus
for Statistical Machine Translation. In The Tenth
Machine Translation Summit, pages 79–86, Phuket,
Thailand.
Pavel Pecina. 2008. A Machine Learning Approach
to Multiword Expression Extraction. In Proceed-
ings of the LREC 2008 Workshop Towards a Shared
Task for Multiword Expressions, pages 54–57, Mar-
rakech, Morocco.
Pavel Pecina. 2010. Lexical association measures
and collocation extraction. Language Resources and
Evaluation, 44(1-2):137–158.
Carlos Ramisch, Helena de Medeiros Caseli, Aline
Villavicencio, Andr´e Machado, and Maria Jos´e Fi-
natto. 2010a. A Hybrid Approach for Multiword
Expression Identification. In Proceedings of the 9th
International Conference on Computational Pro-
cessing of Portuguese Language (PROPOR), pages
65–74.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010b. Multiword Expressions in the wild?
The mwetoolkit comes in handy. In COLING, Inter-
national Conference on Computational Linguistics
(Demos), pages 57–60.
Carlos Ramisch, Vitor De Araujo, and Aline Villavi-
cencio. 2012. A Broad Evaluation of Techniques for
Automatic Acquisition of Multiword Expressions.
In Proceedings ofACL 2012 Student Research Work-
shop, pages 1–6, Jeju Island, Korea. Association for
Computational Linguistics.
Carlos Ramisch. 2012. Une plate-forme g´en´erique et
ouverte pour l’acquisition des expressions polylexi-
cales. In Actes de la 14e Rencontres des ´Etudiants
Chercheurs en Informatique pour le Traitement Au-
tomatique des Langues, pages 137–149, Grenoble,
France.
Helmut Schmid. 1994. Probabilistic Part-of-Speech
Tagging Using Decision Trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, pages 44–49, Manchester, Great
Britain.
Max Silberztein and L.A.D.L. 1990. Le dictionnaire
´electronique des mots compos´es. Langue franc¸aise,
87(1):71–83.
Aline Villavicencio. 2005. The availability of
verb–particle constructions in lexical resources:
How much is enough? Computer Speech &amp; Lan-
guage, 19(4):415–432.
Leonardo Zilio, Luiz Svoboda, Luiz Henrique Longhi
Rossi, and Rafael Martins Feitosa. 2011. Automatic
extraction and evaluation of MWE. In 8th Brazilian
Symposium in Information and Human Language
Technology, pages 214–218, Cuiab´a, Brazil.
</reference>
<page confidence="0.991686">
76
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.680233">
<title confidence="0.967013">Extraction of Nominal Multiword Expressions in French</title>
<author confidence="0.788345">Dubremetz Nivre</author>
<affiliation confidence="0.99895">Uppsala university Department of Linguistics and Philology</affiliation>
<address confidence="0.957943">Uppsala, Sweden</address>
<abstract confidence="0.994587789473684">Multiword expressions (MWEs) can be extracted automatically from large corpora using association measures, and tools like mwetoolkit allow researchers to generate training data for MWE extraction given a tagged corpus and a lexicon. We use mwetoolkit on a sample of the French Europarl corpus together with the French lexicon Dela, and use Weka to train classifiers for MWE extraction on the generated training data. A manual evaluation shows that the classifiers achieve 60–75% precision and that about half of the MWEs found are novel and not listed in the lexicon. We also investigate the impact of the patterns used to generate the training data and find that this can affect the trade-off between precision and novelty.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Evangelos E Milios</author>
<author>Vlado Keselj</author>
</authors>
<title>Comparing Word Relatedness Measures Based on Google n-grams.</title>
<date>2012</date>
<booktitle>In COLING, International Conference on Computational Linguistics (Posters),</booktitle>
<pages>495--506</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="5319" citStr="Islam et al. (2012)" startWordPosition="853" endWordPosition="856">of candidates in another corpus like the web. Villaviciencio (2005), for example, uses number of hits on Google for validating the likelihood of particle verbs. However, as Ramisch (2012) states in his introduction, MWE is an institutionalised phenomenon. This means that an MWE is frequently used and is part of the vocabulary of a speaker as well as the simple words. It means also that MWEs have specific statistical properties that have been studied. The results of those studies are statistical measures such as dice score, maximum likelihood estimate, pointwise mutual information, T-score. As Islam et al. (2012) remark in a study of Google Ngram, those measures of association are language independent. And it is demonstrated by Pecina (2008) that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. However, nowadays, using only lexical association measures for extraction and validation of MWE is not considered the most effective method. The tendency these last years is to combine association measures with linguistic features (Ramisch et al., 2010a; Pecina, 2008; Tsvetkov and Wintner, 2011). 2.2 Mwetoolkit Among the</context>
</contexts>
<marker>Islam, Milios, Keselj, 2012</marker>
<rawString>Aminul Islam, Evangelos E Milios, and Vlado Keselj. 2012. Comparing Word Relatedness Measures Based on Google n-grams. In COLING, International Conference on Computational Linguistics (Posters), pages 495–506, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In The Tenth Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="3253" citStr="Koehn, 2005" startWordPosition="530" endWordPosition="531">ish and French corpora, but they never use the data generated by mwetoolkit to train a model. In contrast, Zilio et al. (2011) make a study involving training a model but use it only on English and use extra lexical resources to complement the machine learning method, so their study does not focus just on classifier evaluation. This paper presents the first evaluation of mwetoolkit on French together with two resources very commonly used by the French NLP community: the tagger TreeTagger (Schmid, 1994) and the dictionary Dela.1 Training and test data are taken from the French Europarl corpus (Koehn, 2005) and classifiers are trained using the Weka machine learning toolkit (Hall et al., 2009). The primary goal is to evaluate what level of precision can be achieved for nominal MWEs, using a manual evaluation of MWEs extracted, and to what extent the MWEs extracted are novel and can be used to enrich the lexicon. In addition, we will investigate what effect the choice of part-of-speech patterns used to generate the training data has on precision and novelty. Our results indicate that classifiers 1http://www-igm.univ-mlv.fr/˜unitex/ index.php?page=5&amp;html=bibliography.html Proceedings of the 10th W</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In The Tenth Machine Translation Summit, pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
</authors>
<title>A Machine Learning Approach to Multiword Expression Extraction.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC 2008 Workshop Towards a Shared Task for Multiword Expressions,</booktitle>
<pages>54--57</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="5450" citStr="Pecina (2008)" startWordPosition="877" endWordPosition="878">hood of particle verbs. However, as Ramisch (2012) states in his introduction, MWE is an institutionalised phenomenon. This means that an MWE is frequently used and is part of the vocabulary of a speaker as well as the simple words. It means also that MWEs have specific statistical properties that have been studied. The results of those studies are statistical measures such as dice score, maximum likelihood estimate, pointwise mutual information, T-score. As Islam et al. (2012) remark in a study of Google Ngram, those measures of association are language independent. And it is demonstrated by Pecina (2008) that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. However, nowadays, using only lexical association measures for extraction and validation of MWE is not considered the most effective method. The tendency these last years is to combine association measures with linguistic features (Ramisch et al., 2010a; Pecina, 2008; Tsvetkov and Wintner, 2011). 2.2 Mwetoolkit Among the tools developed for extracting MWEs, mwetoolkit is one of the most recent. Developed by Ramisch et al. (2010b) it aims not only at</context>
</contexts>
<marker>Pecina, 2008</marker>
<rawString>Pavel Pecina. 2008. A Machine Learning Approach to Multiword Expression Extraction. In Proceedings of the LREC 2008 Workshop Towards a Shared Task for Multiword Expressions, pages 54–57, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
</authors>
<title>Lexical association measures and collocation extraction. Language Resources and Evaluation,</title>
<date>2010</date>
<pages>44--1</pages>
<contexts>
<context position="1495" citStr="Pecina, 2010" startWordPosition="239" endWordPosition="240"> and novelty. 1 Introduction In alphabetic languages, words are delimited by spaces. Some words can combine to create a new unit of meaning that we call a multiword expression (MWE). However, MWEs such as kick the bucket must be distinguished from free combinations of words such as kick the ball. A sequence of several words is an MWE if “at least one of its syntactic, distributional or semantic properties cannot be deduced from the properties of its component” (Silberztein and L.A.D.L., 1990). So how can we extract them? Statistical association measures have long been used for MWE extraction (Pecina, 2010), and by training supervised classifiers that use association measures as features we can further improve the quality of the extraction process. However, supervised machine learning requires annotated data, which creates a bottleneck in the absence of large corpora annotated for MWEs. In order to circumvent this bottleneck, mwetoolkit (Ramisch et 72 al., 2010b) generates training instances by first extracting candidates that fit a certain part-of-speech pattern, such as Noun-Noun or Noun-Adjective, and then marking the candidates as positive or negative instances depending on whether they can </context>
</contexts>
<marker>Pecina, 2010</marker>
<rawString>Pavel Pecina. 2010. Lexical association measures and collocation extraction. Language Resources and Evaluation, 44(1-2):137–158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Helena de Medeiros Caseli</author>
<author>Aline Villavicencio</author>
<author>Andr´e Machado</author>
<author>Maria Jos´e Finatto</author>
</authors>
<title>A Hybrid Approach for Multiword Expression Identification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th International Conference on Computational Processing of Portuguese Language (PROPOR),</booktitle>
<pages>65--74</pages>
<contexts>
<context position="5849" citStr="Ramisch et al., 2010" startWordPosition="932" endWordPosition="935">maximum likelihood estimate, pointwise mutual information, T-score. As Islam et al. (2012) remark in a study of Google Ngram, those measures of association are language independent. And it is demonstrated by Pecina (2008) that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. However, nowadays, using only lexical association measures for extraction and validation of MWE is not considered the most effective method. The tendency these last years is to combine association measures with linguistic features (Ramisch et al., 2010a; Pecina, 2008; Tsvetkov and Wintner, 2011). 2.2 Mwetoolkit Among the tools developed for extracting MWEs, mwetoolkit is one of the most recent. Developed by Ramisch et al. (2010b) it aims not only at extracting candidates for potential MWEs, but also at extracting their association measures. Provided that a lexicon of MWEs is available and provided a preprocessed corpus, mwetoolkit makes it possible to train a machine learning system with the association measures as features with a minimum of implementation. Ramisch et al. (2010b) provide experiments on Portuguese, English and Greek. Zilio e</context>
</contexts>
<marker>Ramisch, Caseli, Villavicencio, Machado, Finatto, 2010</marker>
<rawString>Carlos Ramisch, Helena de Medeiros Caseli, Aline Villavicencio, Andr´e Machado, and Maria Jos´e Finatto. 2010a. A Hybrid Approach for Multiword Expression Identification. In Proceedings of the 9th International Conference on Computational Processing of Portuguese Language (PROPOR), pages 65–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Aline Villavicencio</author>
<author>Christian Boitet</author>
</authors>
<title>Multiword Expressions in the wild? The mwetoolkit comes in handy.</title>
<date>2010</date>
<booktitle>In COLING, International Conference on Computational Linguistics (Demos),</booktitle>
<pages>57--60</pages>
<contexts>
<context position="5849" citStr="Ramisch et al., 2010" startWordPosition="932" endWordPosition="935">maximum likelihood estimate, pointwise mutual information, T-score. As Islam et al. (2012) remark in a study of Google Ngram, those measures of association are language independent. And it is demonstrated by Pecina (2008) that combining different collocation measures using standard statistical classification methods improves over using a single collocation measure. However, nowadays, using only lexical association measures for extraction and validation of MWE is not considered the most effective method. The tendency these last years is to combine association measures with linguistic features (Ramisch et al., 2010a; Pecina, 2008; Tsvetkov and Wintner, 2011). 2.2 Mwetoolkit Among the tools developed for extracting MWEs, mwetoolkit is one of the most recent. Developed by Ramisch et al. (2010b) it aims not only at extracting candidates for potential MWEs, but also at extracting their association measures. Provided that a lexicon of MWEs is available and provided a preprocessed corpus, mwetoolkit makes it possible to train a machine learning system with the association measures as features with a minimum of implementation. Ramisch et al. (2010b) provide experiments on Portuguese, English and Greek. Zilio e</context>
</contexts>
<marker>Ramisch, Villavicencio, Boitet, 2010</marker>
<rawString>Carlos Ramisch, Aline Villavicencio, and Christian Boitet. 2010b. Multiword Expressions in the wild? The mwetoolkit comes in handy. In COLING, International Conference on Computational Linguistics (Demos), pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Vitor De Araujo</author>
<author>Aline Villavicencio</author>
</authors>
<title>A Broad Evaluation of Techniques for Automatic Acquisition of Multiword Expressions.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL 2012 Student Research Workshop,</booktitle>
<pages>1--6</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<marker>Ramisch, De Araujo, Villavicencio, 2012</marker>
<rawString>Carlos Ramisch, Vitor De Araujo, and Aline Villavicencio. 2012. A Broad Evaluation of Techniques for Automatic Acquisition of Multiword Expressions. In Proceedings ofACL 2012 Student Research Workshop, pages 1–6, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
</authors>
<title>Une plate-forme g´en´erique et ouverte pour l’acquisition des expressions polylexicales.</title>
<date>2012</date>
<booktitle>In Actes de la 14e Rencontres des ´Etudiants Chercheurs en Informatique pour le Traitement Automatique des Langues,</booktitle>
<pages>137--149</pages>
<location>Grenoble, France.</location>
<contexts>
<context position="4324" citStr="Ramisch, 2012" startWordPosition="694" endWordPosition="695">y. Our results indicate that classifiers 1http://www-igm.univ-mlv.fr/˜unitex/ index.php?page=5&amp;html=bibliography.html Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 72–76, Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics achieve precision in the 60–75% range and that about half of the MWEs found are novel ones. In addition, it seems that the choice of patterns used to generate the training data can affect the tradeoff between precision and novelty. 2 Related Work 2.1 Extraction Techniques There is no unique definition of MWEs (Ramisch, 2012). In the literature on the subject, we notice that manual MWE extraction often requires several annotators native of the studied language. Nevertheless, some techniques exist for selecting automatically candidates that are more likely to be the true ones. Candidates can be validated against an external resource, such as a lexicon. It is possible also to check the frequency of candidates in another corpus like the web. Villaviciencio (2005), for example, uses number of hits on Google for validating the likelihood of particle verbs. However, as Ramisch (2012) states in his introduction, MWE is a</context>
</contexts>
<marker>Ramisch, 2012</marker>
<rawString>Carlos Ramisch. 2012. Une plate-forme g´en´erique et ouverte pour l’acquisition des expressions polylexicales. In Actes de la 14e Rencontres des ´Etudiants Chercheurs en Informatique pour le Traitement Automatique des Langues, pages 137–149, Grenoble, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In Proceedings of International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, Great Britain.</location>
<contexts>
<context position="3148" citStr="Schmid, 1994" startWordPosition="512" endWordPosition="513">erature. Indeed, Ramisch et al. (2012) compares the performance of mwetoolkit with another toolkit on English and French corpora, but they never use the data generated by mwetoolkit to train a model. In contrast, Zilio et al. (2011) make a study involving training a model but use it only on English and use extra lexical resources to complement the machine learning method, so their study does not focus just on classifier evaluation. This paper presents the first evaluation of mwetoolkit on French together with two resources very commonly used by the French NLP community: the tagger TreeTagger (Schmid, 1994) and the dictionary Dela.1 Training and test data are taken from the French Europarl corpus (Koehn, 2005) and classifiers are trained using the Weka machine learning toolkit (Hall et al., 2009). The primary goal is to evaluate what level of precision can be achieved for nominal MWEs, using a manual evaluation of MWEs extracted, and to what extent the MWEs extracted are novel and can be used to enrich the lexicon. In addition, we will investigate what effect the choice of part-of-speech patterns used to generate the training data has on precision and novelty. Our results indicate that classifie</context>
<context position="9026" citStr="Schmid, 1994" startWordPosition="1484" endWordPosition="1485">e million words each) for running our experiments. The first part will be devoted at 80% to 73 training and 20% to development test set, when training classifiers on Noun-Adjective or NounNoun patterns, or both. We use the second million as a secondary development set that is not used in this study. The third million is used as a final test set and we will present results on this set. 3.3 Preprocessing For preprocessing we used the same processes as described in Zilio et al. (2011). First we ran the sentence splitter and the tokenizer provided with the Europarl corpus. Then we ran TreeTagger (Schmid, 1994) to obtain the tags and the lemmas. 3.4 Extracting Data and Features The mwetoolkit takes as input a preprocessed corpus plus a lexicon and gives two main outputs: an arff file which is a format adapted to the machine learning framework Weka, and an XML file. At the end of the process we obtain, for each candidate, a binary classification as an MWE (True) or not (False) depending on whether it is contained in the lexicon. For each candidate, we also obtain the following features: maximum likelihood estimate, pointwise mutual information, T-score, dice coefficient, log-likelihood ratio. The mac</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In Proceedings of International Conference on New Methods in Language Processing, pages 44–49, Manchester, Great Britain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max Silberztein</author>
<author>L A D L</author>
</authors>
<title>Le dictionnaire ´electronique des mots compos´es. Langue franc¸aise,</title>
<date>1990</date>
<pages>87--1</pages>
<marker>Silberztein, L, 1990</marker>
<rawString>Max Silberztein and L.A.D.L. 1990. Le dictionnaire ´electronique des mots compos´es. Langue franc¸aise, 87(1):71–83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
</authors>
<title>The availability of verb–particle constructions in lexical resources: How much is enough?</title>
<date>2005</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>19</volume>
<issue>4</issue>
<marker>Villavicencio, 2005</marker>
<rawString>Aline Villavicencio. 2005. The availability of verb–particle constructions in lexical resources: How much is enough? Computer Speech &amp; Language, 19(4):415–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonardo Zilio</author>
<author>Luiz Svoboda</author>
<author>Luiz Henrique Longhi Rossi</author>
<author>Rafael Martins Feitosa</author>
</authors>
<title>Automatic extraction and evaluation of MWE.</title>
<date>2011</date>
<booktitle>In 8th Brazilian Symposium in Information and Human Language Technology,</booktitle>
<pages>214--218</pages>
<location>Cuiab´a, Brazil.</location>
<contexts>
<context position="2767" citStr="Zilio et al. (2011)" startWordPosition="448" endWordPosition="451">et will presumably not contain any false positives (that is, candidates marked as positive instances that are not real MWEs), but depending on the coverage of the lexicon there will be a smaller or larger proportion of false negatives. The question is what quality can be obtained using such a noisy training set. To the best of our knowledge, we cannot find the answer for French in literature. Indeed, Ramisch et al. (2012) compares the performance of mwetoolkit with another toolkit on English and French corpora, but they never use the data generated by mwetoolkit to train a model. In contrast, Zilio et al. (2011) make a study involving training a model but use it only on English and use extra lexical resources to complement the machine learning method, so their study does not focus just on classifier evaluation. This paper presents the first evaluation of mwetoolkit on French together with two resources very commonly used by the French NLP community: the tagger TreeTagger (Schmid, 1994) and the dictionary Dela.1 Training and test data are taken from the French Europarl corpus (Koehn, 2005) and classifiers are trained using the Weka machine learning toolkit (Hall et al., 2009). The primary goal is to e</context>
<context position="6461" citStr="Zilio et al. (2011)" startWordPosition="1028" endWordPosition="1031">., 2010a; Pecina, 2008; Tsvetkov and Wintner, 2011). 2.2 Mwetoolkit Among the tools developed for extracting MWEs, mwetoolkit is one of the most recent. Developed by Ramisch et al. (2010b) it aims not only at extracting candidates for potential MWEs, but also at extracting their association measures. Provided that a lexicon of MWEs is available and provided a preprocessed corpus, mwetoolkit makes it possible to train a machine learning system with the association measures as features with a minimum of implementation. Ramisch et al. (2010b) provide experiments on Portuguese, English and Greek. Zilio et al. (2011) provide experiments with this tool as well. In the latter study, after having trained a machine on bigram MWEs, they try to extract full n-gram expressions from the Europarl corpus. They then reuse the model obtained on bigrams for extraction of full n-gram MWEs. Finally, they apply a second filter for getting back the false negatives by checking every MWE annotated as False by the algorithm against a online dictionary. This method gets a very good precision (over 87%) and recall (over 84%). However, we do not really know if this result is mostly due to the coverage of the dictionary online. </context>
<context position="8899" citStr="Zilio et al. (2011)" startWordPosition="1462" endWordPosition="1465">2) we work on the French Europarl corpus. We took the three first million words of Europarl and divided it into three equal parts (one million words each) for running our experiments. The first part will be devoted at 80% to 73 training and 20% to development test set, when training classifiers on Noun-Adjective or NounNoun patterns, or both. We use the second million as a secondary development set that is not used in this study. The third million is used as a final test set and we will present results on this set. 3.3 Preprocessing For preprocessing we used the same processes as described in Zilio et al. (2011). First we ran the sentence splitter and the tokenizer provided with the Europarl corpus. Then we ran TreeTagger (Schmid, 1994) to obtain the tags and the lemmas. 3.4 Extracting Data and Features The mwetoolkit takes as input a preprocessed corpus plus a lexicon and gives two main outputs: an arff file which is a format adapted to the machine learning framework Weka, and an XML file. At the end of the process we obtain, for each candidate, a binary classification as an MWE (True) or not (False) depending on whether it is contained in the lexicon. For each candidate, we also obtain the followin</context>
<context position="11631" citStr="Zilio et al. (2011)" startWordPosition="1932" endWordPosition="1935">have to favour the recall of the positive candidates. Indeed, when an MWE candidate is annotated as True, it means that it is listed in the Dela, which means that it is an officially listed MWE. However, if an MWE is not in the Dela, it does not mean that the candidate does not fulfil all the criteria for being an MWE. For this reason, obtaining a good recall is much more difficult than getting a good precision, but it is also the most important if we stay on a lexicographical purpose. 4.1 Training on NA We tested several algorithms offered by Weka as well as the training options suggested by Zilio et al. (2011). We also tried to remove some features and to keep only the most informative ones (MLE, T-score and log-likelihood according to information gain ratio) but we noticed each time a loss in the recall. At the end with all the features kept and for the purpose of evaluating NA MWE candidates the best classification algorithm was the Bayesian network. 4.2 Training on NN When training a model on NN MWEs, our aim was to keep as much as possible the same condition for our three experiments. However, the NN training set has definitely not the same properties as the NA and NANN ones. The NN training se</context>
</contexts>
<marker>Zilio, Svoboda, Rossi, Feitosa, 2011</marker>
<rawString>Leonardo Zilio, Luiz Svoboda, Luiz Henrique Longhi Rossi, and Rafael Martins Feitosa. 2011. Automatic extraction and evaluation of MWE. In 8th Brazilian Symposium in Information and Human Language Technology, pages 214–218, Cuiab´a, Brazil.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
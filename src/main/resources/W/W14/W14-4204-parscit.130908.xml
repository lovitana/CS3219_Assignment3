<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002604">
<title confidence="0.981694">
Language variety identification in Spanish tweets
</title>
<author confidence="0.999247">
Wolfgang Maier
</author>
<affiliation confidence="0.9961515">
Institute for Language and Information
University of D¨usseldorf
</affiliation>
<address confidence="0.549977">
D¨usseldorf, Germany
</address>
<email confidence="0.997742">
maierw@hhu.de
</email>
<sectionHeader confidence="0.993873" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999914">
We study the problem of language vari-
ant identification, approximated by the
problem of labeling tweets from Spanish
speaking countries by the country from
which they were posted. While this task
is closely related to “pure” language iden-
tification, it comes with additional com-
plications. We build a balanced collec-
tion of tweets and apply techniques from
language modeling. A simplified version
of the task is also solved by human test
subjects, who are outperformed by the
automatic classification. Our best auto-
matic system achieves an overall F-score
of 67.7% on 5-class classification.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995225">
Spanish (or castellano), a descendant of Latin,
is currently the language with the second largest
number of native speakers after Mandarin Chi-
nese, namely around 414 million people (Lewis
et al., 2014). Spanish has a large number of re-
gional varieties across Spain and the Americas
(Lipski, 1994).1 They diverge in spoken language
and vocabulary and also, albeit to a lesser extent,
in syntax. Between different American varieties
of Spanish, there are important differences; how-
ever, the largest differences can be found between
American and European (“Peninsular”) Spanish.
Language identification, the task of automati-
cally identifying the natural language used in a
given text segment, is a relatively well understood
problem (see Section 2). To our knowledge, how-
ever, there is little previous work on the identifica-
tion of the varieties of a single language, such as
the regional varieties of Spanish. This task is espe-
cially challenging because the differences between
</bodyText>
<footnote confidence="0.989626666666667">
1We are aware that there are natively Spanish-speaking
communities elsewhere, such as on the Philippines, but we
do not consider them in this study.
</footnote>
<note confidence="0.97926375">
Carlos G´omez-Rodriguez
Depto. de Computaci´on
Universidade da Coru˜na
A Coru˜na, Spain
</note>
<email confidence="0.947058">
cgomezr@udc.es
</email>
<bodyText confidence="0.999970756097561">
variants are subtle, making it difficult to discern
between them. This is evidenced by the fact that
humans that are native speakers of the varieties
are often unable to solve the problem, particularly
when given short, noisy text segments (which are
the focus of this work) where the amount of avail-
able information is limited.
In this paper, we approximate the problem of
language variety identification by the problem
of classifying status messages from the micro-
blogging service Twitter (“tweets”) from Span-
ish speaking countries by the country from which
they were sent. With the tweet, the location of
the device from which the tweet was sent can be
recorded (depending on the Twitter users’ permis-
sion) and can then be retrieved from the metadata
of the tweet. The tweet location information does
not always correlate with the actual language va-
riety used in the tweet: it is conceivable, e.g., that
migrants do not use the prevalent language vari-
ety of the country in which they live, but rather
their native variety. Nevertheless, Twitter can give
a realistic picture of actual language use in a cer-
tain region, which, additionally, is closer to spoken
than to standard written language. Eventually and
more importantly, Twitter data is available from
almost all Spanish speaking countries.
We proceed as follows. We build a balanced
collection of tweets sent by Twitter users from
five countries, namely Argentina, Chile, Colom-
bia, Mexico, and Spain. Applying different meth-
ods, we perform an automatic classification be-
tween all countries. In order to obtain a more de-
tailed view of the difficulty of our task, we also
investigate human performance. For this purpose,
we build a smaller sample of tweets from Ar-
gentina, Chile and Spain and have them classified
by both our system and three native human evalua-
tors. The results show that automatic classification
outperforms human annotators. The best variant
of our system, using a meta-classifier with voting,
</bodyText>
<page confidence="0.975968">
25
</page>
<note confidence="0.6675495">
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 25–35,
October 29, 2014, Doha, Qatar. (c 2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998238777777778">
reaches an overall F-score of 67.72 on the five-
class problem. On the two-class problem, human
classification is outperformed by a large margin.
The remainder of this paper is structured as fol-
lows. In the following section, we present related
work. Section 3 presents our data collection. Sec-
tions 4 and 5 present our classification methodol-
ogy and the experiments. Section 7 discusses the
results, and Section 8 concludes the article.
</bodyText>
<sectionHeader confidence="0.99978" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999973815789474">
Research on language identification has seen a va-
riety of methods. A well established technique is
the use of character n-gram models. Cavnar and
Trenkle (1994) build n-gram frequency “profiles”
for several languages and classify text by match-
ing it to the profiles. Dunning (1994) uses lan-
guage modeling. This technique is general and
not limited to language identification; it has also
been successfully employed in other areas, e.g., in
authorship attribution (Keˇselj et al., 2003) and au-
thor native language identification (Gyawali et al.,
2013). Other language identification systems use
non-textual methods, exploiting optical properties
of text such as stroke geometry (Muir and Thomas,
2000), or using compression methods which rely
on the assumption that natural languages differ
by their entropy, and consequently by the rate
to which they can be compressed (Teahan, 2000;
Benedetto et al., 2002). Two newer approaches
are Brown (2013), who uses character n-grams,
and ˇReh˚uˇrek and Kolkus (2009), who treat “noisy”
web text and therefore consider the particular in-
fluence of single words in discriminating between
languages.
Language identification is harder the shorter the
text segments whose language is to be identified
(Baldwin and Lui, 2010). Especially due to the
rise of Twitter, this particular problem has recently
received attention. Several solutions have been
proposed. Vatanen et al. (2010) compare character
n-gram language models with elaborate smooth-
ing techniques to the approach of Cavnar and
Trenkle and the Google Language ID API, on the
basis of different versions of the Universal Decla-
ration of Human Rights. Other researchers work
on Twitter. Bergsma et al. (2012) use language
identification to create language specific tweet col-
lections, thereby facilitating more high-quality re-
sults with supervised techniques. Lui and Baldwin
(2014) review a wide range of off-the-shelf tools
for Twitter language identification, and achieve
their best results with a voting over three individ-
ual systems, one of them being langid.py (Lui
and Baldwin, 2012). Carter et al. (2013) exploit
particular characteristics of Twitter (such as user
profile data and relations between Twitter users)
to improve language identification on this genre.
Bush (2014) successfully uses LZW compression
for Twitter language identification.
Within the field of natural language processing,
the problem of language variant identification has
only begun to be studied very recently. Zampieri
et al. (2013) have addressed the task for Spanish
newspaper texts, using character and word n-gram
models as well as POS and morphological infor-
mation. Very recently, the Discriminating between
Similar Languages (DSL) Shared Task (Zampieri
et al., 2014) proposed the problem of identify-
ing between pairs of similar languages and lan-
guage variants on sentences from newspaper cor-
pora, one of the pairs being Peninsular vs. Argen-
tine Spanish. However, all these approaches are
tailored to the standard language found in news
sources, very different from the colloquial, noisy
language of tweets, which presents distinct chal-
lenges for NLP (Derczynski et al., 2013; Vilares et
al., 2013). Lui and Cook (2013) evaluate various
approaches to classify documents into Australian,
British and Canadian English, including a corpus
of tweets, but we are not aware of any previous
work on variant identification in Spanish tweets.
A review of research on Spanish varieties from
a linguistics point of view is beyond the scope of
this article. Recommended further literature in this
area is Lipski (1994), Quesada Pacheco (2002)
and Alvar (1996b; 1996a).
</bodyText>
<sectionHeader confidence="0.989923" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.998407636363636">
We first built a collection of tweets using the
Twitter streaming API,2 requesting all tweets sent
within the geographic areas given by the coordi-
nates -120°, -55° and -29°, 30° (roughly delimit-
ing Latin America), as well as -10°, 35° and 3°,
46° (roughly delimiting Spain). The download ran
from July 2 to July 4, 2014. In a second step, we
sorted the tweets according to the respective coun-
tries.
Twitter is not used to the same extent in all
countries where Spanish is spoken. In the time
</bodyText>
<footnote confidence="0.9943755">
2https://dev.twitter.com/docs/api/
streaming
</footnote>
<page confidence="0.993483">
26
</page>
<bodyText confidence="0.984417368421053">
#
it took to collect 2,400 tweets from Bolivia,
we could collect over 700,000 tweets from Ar-
gentina.3 To ensure homogeneous conditions for
our experiments, our final tweet collection com-
prises exactly 100,000 tweets from each of the five
countries from which most tweets were collected,
that is, Argentina, Chile, Colombia, Mexico, and
Spain.
At this stage, we do not perform any cleanup
or normalization operations such as, e.g., deleting
forwarded tweets (“re-tweets”), deleting tweets
which are sent by robots, or tweets not written in
Spanish (some tweets use code switching, or are
entirely written in a different language, mostly in
English or in regional and minority languages that
coexist with Spanish in the focus countries). Our
reasoning behind this is that the tweet production
in a certain country captures the variant of Spanish
that is spoken.
We mark the start and end of single tweets by
&lt;s&gt; and &lt;/s&gt;, respectively. We use 80% of the
tweets of each language for training, and 10% for
development and testing, respectively. The data
is split in a round-robin fashion, i.e., every ninth
tweet is put into the development set and every
tenth tweet is put in the test set, all other tweets
are put in the training set.
In order to help with the interpretation of clas-
sification results, we investigate the distribution of
tweet lengths on the development set, as shown in
Figure 1. We see that in all countries, tweets tend
to be either short, or take advantage of all available
characters. Lengths around 100 to 110 characters
are the rarest. The clearest further trend is that the
tweets from Colombia and, especially, Argentina
tend to be shorter than the tweets from the other
countries.
</bodyText>
<sectionHeader confidence="0.991195" genericHeader="method">
4 Automatic Tweet Classification
</sectionHeader>
<bodyText confidence="0.999819444444444">
The classification task we envisage is similar to
the task of language identification in short text
segments. We explore three methods that have
been used before for that task, namely character
n-gram frequency profiles (Cavnar and Trenkle,
1994; Vatanen et al., 2010), character n-gram lan-
guage models (Vatanen et al., 2010), as well as
LZW compression (Bush, 2014). Furthermore, we
explore the usability of syllable-based language
</bodyText>
<footnote confidence="0.993045333333333">
3We are aware that the Twitter API does not make all sent
tweets available. However, we still assume that this huge dif-
ference reflects a variance in the number of Twitter users.
</footnote>
<figure confidence="0.984514">
0 20 40 60 80 100 120 140
Tweet length
</figure>
<figureCaption confidence="0.999219">
Figure 1: Tweet length distribution
</figureCaption>
<table confidence="0.999863333333333">
50 100 500 1k 10k
AR 31.68 29.72 43.93 31.77 18.42
CO 24.29 21.36 26.14 19.68 19.03
MX 31.86 28.97 32.58 30.28 22.27
ES 20.19 25.22 22.08 21.25 16.15
CL 22.95 29.74 35.67 26.01 16.69
</table>
<tableCaption confidence="0.9367045">
Table 1: Results (F1): n-gram frequency profiles
(classes/profile sizes)
</tableCaption>
<bodyText confidence="0.9925368">
models. For all four approaches, we train mod-
els for binary classification for each class, i.e., five
models that decide for each tweet if it belongs to a
single class. As final label, we take the output of
the one of the five classifiers that has the highest
score.
We finally use a meta-classifier on the basis of
voting. All methods are tested on the development
set. For evaluation, we compute precision, recall
and F1 overall as well as for single classes.
Note that we decided to rely on the tweet text
only. An exploration of the benefit of, e.g., directly
exploiting Twitter-specific information (such as
user mentions or hash tags) is out of the scope of
this paper.
</bodyText>
<subsectionHeader confidence="0.993297">
4.1 Character n-gram frequency profiles
</subsectionHeader>
<bodyText confidence="0.999842125">
We first investigate the n-gram frequency ap-
proach of Cavnar and Trenkle (1994). We use the
well-known implementation TextCat.4 The re-
sults for all classes with different profile sizes are
shown in Table 1. Table 2 shows precision and re-
call for the best setting, a profile with a maximal
size of 500 entries.
The results obtained with a profile size of 500
</bodyText>
<footnote confidence="0.6723265">
4As available from http://odur.let.rug.nl/
˜vannoord/TextCat/.
</footnote>
<figure confidence="0.947088176470588">
200
150
100
50
0
ES
CL
CO
AR
MX
27
class precision recall F1
AR 32.60 67.33 43.93
CO 31.66 22.26 26.14
MX 51.52 23.82 32.58
ES 32.83 16.63 22.08
CL 31.96 40.36 35.67
overall 34.08 34.08 34.08
Table 2: Results: n-gram frequency profile with
500 n-grams
F-score
70
65
60
55
50
45
40
no pruning
0.01 pruning
0.1 pruning
1 pruning
2 3 4 5 6
n-gram order
</figure>
<table confidence="0.948139166666667">
AR CO MX ES CL
AR 6,733 949 384 610 1,324
CO 4,207 2,226 720 803 2,044
MX 2,547 1,342 2,382 1,051 2,678
ES 3,781 1,361 649 1,663 2,546
CL 3,384 1,153 488 939 4,036
</table>
<tableCaption confidence="0.9193465">
Table 3: Confusion matrix (n-gram freq. profiles,
500 n-grams)
</tableCaption>
<bodyText confidence="0.998300285714286">
entries for Colombia align with the results for
Spain and Mexico in that the precision is higher
than the recall. The results for Chile align with
those for Argentina with the recall being higher
than the precision. For Mexico and Argentina the
differences between recall and precision are par-
ticularly large (28 and 35 points, respectively).
The confusion matrix in Table 3 reveals that tweets
from all classes are likely to be mislabeled as
coming from Argentina, while, on the other hand,
Mexican tweets are mislabeled most frequently as
coming from other countries.
Overall, the n-gram frequency profiles are not
very good at our task, achieving an maximal over-
all F-score of only 34.08 with a profile size of 500
entries. However, this performance is still well
above the 20.00 F-score we would obtain with
a random baseline. Larger profile sizes deterio-
rate results: with 10,000 entries, we only have
an overall F-score of 18.23. As observed before
(Vatanen et al., 2010), the weak performance can
most likely be attributed to the shortness of the
tweets and the resulting lack of frequent n-grams
that hinders a successful profile matching. While
Vatanen et al. alleviate this problem to some ex-
tent, they have more success with character-level
n-gram language models, the approach which we
explore next.
</bodyText>
<figureCaption confidence="0.908026">
Figure 2: Character n-gram lm: Pruning vs. n-
gram order
</figureCaption>
<subsectionHeader confidence="0.990074">
4.2 Character n-gram language models
</subsectionHeader>
<bodyText confidence="0.999722363636364">
We recur to n-gram language models as avail-
able in variKN (Siivola et al., 2007).5 We run
variKN with absolute discounting and the cross-
product of four different pruning settings (no prun-
ing, and thresholds 0.01, 0.1 and 1) and five differ-
ent n-gram lengths (2 to 6).
Figure 2 contrasts the effect of different pruning
settings with different n-gram lengths. While ex-
cessive pruning is detrimental to the result, slight
pruning has barely any effect on the results, while
reducing look-up time immensely. The order of
the n-grams, however, does have an important in-
fluence. We confirm that also for this problem, we
do not benefit from increasing it beyond n = 6,
like Vatanen et al. (2010).
We now check if some countries are more dif-
ficult to identify than others and how they bene-
fit from different n-gram orders. Figure 3 visual-
izes the corresponding results. Not all countries
profit equally from longer n-grams. When com-
paring the 3- and 6-gram models without pruning,
we see that the F1 for Argentina is just 8 points
higher, while the difference is more than 14 points
for Mexico.
Table 4 shows all results including precision and
recall for all classes, in the setting with 6-grams
and no pruning. We can see that this approach
works noticeably better than the frequency pro-
files, achieving an overall F-score of 66.96. The
behavior of the classes is not uniform: Argentina
shows the largest difference between precision and
recall, and is furthermore the only class in which
precision is higher than recall. Note also that in
</bodyText>
<footnote confidence="0.947975">
5https://github.com/vsiivola/variKN
</footnote>
<page confidence="0.996025">
28
</page>
<figure confidence="0.99496175">
F-score
F-score
2 3 4 5 6
n-gram order
</figure>
<figureCaption confidence="0.9931805">
Figure 3: Character n-gram lm: Classes vs. n-
gram order (no pruning)
</figureCaption>
<table confidence="0.997214857142857">
class precision recall Fi
AR 70.67 66.22 68.37
CO 62.56 62.77 62.66
MX 65.23 65.74 65.48
ES 68.75 69.36 69.06
CL 67.81 70.73 69.24
overall 66.96 66.96 66.96
</table>
<tableCaption confidence="0.999362">
Table 4: Results: 6-grams without pruning
</tableCaption>
<bodyText confidence="0.991847176470588">
general, the differences between precision and re-
call are lower than for the n-gram frequency pro-
file approach. The confusion matrix shown in Ta-
ble 5 reveals that the Colombia class is the one
with the highest confusion, particularly in com-
bination with the Mexican class. This could in-
dicate that those classes are more heterogeneous
than the others, possibly showing more Twitter-
specific noise, such as tweets consisting only of
URLs, etc.
We finally investigate how tweet length influ-
ences classification performance in the 6-gram
model. Figure 4 shows the F-scores for intervals
of length 20 for all classes. The graph confirms
that longer tweets are easier to classify. This cor-
relates with findings from previous work. Over
82 points Fi are achieved for tweets from Chile
</bodyText>
<table confidence="0.997673166666667">
AR CO MX ES CL
AR 6,622 1,036 702 740 900
CO 800 6,277 1,151 875 897
MX 509 1,237 6,574 847 833
ES 630 850 857 6,936 727
CL 809 634 794 690 7,073
</table>
<tableCaption confidence="0.999806">
Table 5: Confusion matrix (6-grams, no pruning)
</tableCaption>
<figure confidence="0.951896">
20 40 60 80 100 120 140
length
</figure>
<figureCaption confidence="0.988842">
Figure 4: Character n-grams: Results (Fi) for
tweet length intervals
</figureCaption>
<figure confidence="0.9936935">
20 40 60 80 100 120 140
length
</figure>
<figureCaption confidence="0.9683765">
Figure 5: Character n-grams: Precision/recall for
AR and CL
</figureCaption>
<bodyText confidence="0.999891333333333">
longer than 120 characters, while for those con-
taining up to 20 characters, Fi is almost 30 points
lower. We investigate precision and recall sepa-
rately. Figure 5 shows the corresponding curves
for the best and worst performing classes, namely,
CL and CO. For Chile, both precision and recall
develop in parallel to the F-score (i.e., the longer
the tweets, the higher the scores). For Colombia,
the curves confirm that the low Fi is rather due to
a low precision than a low recall, particularly for
tweets longer than 40 characters. This correlates
with the counts in the confusion table (Tab. 5).
</bodyText>
<subsectionHeader confidence="0.994608">
4.3 Syllable n-gram language models
</subsectionHeader>
<bodyText confidence="0.999853285714286">
Since varieties of Spanish exhibit differences in
vocabulary, we may think that models based on
word n-grams can be more useful than character
n-grams to discriminate between varieties. How-
ever, the larger diversity of word n-grams means
that such models run into sparsity problems. An
intermediate family of models can be built by us-
</bodyText>
<figure confidence="0.997288885714286">
70
65
60
55
50
45
40
35
ES
CL
CO
AR
MX
90
80
70
60
50
40
ES
CL
CO
AR
MX
precision/recall
90
80
70
60
50
40
CO precision
CO recall
CL precision
CL recall
</figure>
<page confidence="0.985969">
29
</page>
<table confidence="0.998928714285714">
class precision recall F1
AR 55.94 61.11 58.41
CO 53.23 53.03 53.13
MX 59.10 56.17 57.60
ES 62.35 56.96 59.53
CL 59.31 62.12 60.68
overall 57.88 57.88 57.88
</table>
<tableCaption confidence="0.993311">
Table 6: Results (F1): Syllable 4-gram lm
</tableCaption>
<figure confidence="0.998094923076923">
F-score
65
60
55
50
45
ES
CL
CO
AR
MX
2 3 4
n-gram order
</figure>
<figureCaption confidence="0.960962">
Figure 6: Syllable n-gram lm: pruning vs. n-gram
order
</figureCaption>
<bodyText confidence="0.999933191489362">
ing syllable n-grams, taking advantage of the fact
that Spanish variants do not differ in the criteria
for syllabification of written words. Note that this
property does not hold in general for the language
identification problem, as different languages typ-
ically have different syllabification rules, which is
a likely reason why syllable n-gram models have
not been used for this problem.
To perform the splitting of Spanish words into
syllables, we use the TIP syllabifier (Hern´andez-
Figeroa et al., 2012), which applies an algorithm
implementing the general syllabification rules de-
scribed by the Royal Spanish Academy of Lan-
guage and outlined in standard Spanish dictionar-
ies and grammars. These rules are enough to cor-
rectly split the vast majority of Spanish words, ex-
cluding only a few corner cases related with word
prefixes (Hern´andez-Figueroa et al., 2013). While
accurate syllabification requires texts to be written
correctly with accented characters, and this is of-
ten not the case in informal online environments
(Vilares et al., 2014); we assume that this need not
cause problems because the errors originated by
unaccented words will follow a uniform pattern,
producing a viable model for the purposes of clas-
sification.
We train n-gram language models with
variKN as described in the last section, using
absolute discounting. Due to the larger vocabulary
size, we limit ourselves to 0.01 pruning, and to
n-gram orders 2 to 4. Figure 6 shows the results
(F1) of all classes for the different n-gram orders,
and Table 6 shows the results for all classes for
the 4-gram language model.
As expected, shorter n-grams are more effective
for syllable than for character language models.
For the Chilean tweets, e.g., the F-score for the 2-
gram language model is around 11 points higher
than for the character 2-gram language model.
Furthermore, the performance seems to converge
earlier, given that the results change only slightly
when raising the n-gram order from 3 to 4. The
overall F-score for the 4-gram language model is
around 6 points lower than for character 4-grams.
However, the behavior of the classes is similar:
again, Mexico and Colombia have slightly lower
results than the other classes.
</bodyText>
<subsectionHeader confidence="0.989515">
4.4 Compression
</subsectionHeader>
<bodyText confidence="0.999908678571429">
We eventually test the applicability of
compression-based classification using the
approach of Bush (2014). As mentioned ear-
lier, the assumption behind compression-based
strategies for text categorization is that different
text categories have a different entropy. Clas-
sification is possible because the effectivity of
compression algorithms depends on the entropy
of the data to be compressed (less entropy ≈ more
compression).
A simple classification algorithm is Lempel-
Ziv-Welch (LZW) (Welch, 1984). It is based on
a dictionary which maps sequences of symbols to
unique indices. Compression is achieved by re-
placing sequences of input symbols with the re-
spective dictionary indices. More precisely, com-
pression works as follows. First, the dictionary
is initialized with the inventory of symbols (i.e.,
with all possible 1-grams). Then, until the input is
fully consumed, we repeat the following steps. We
search the dictionary for the longest sequence of
symbols s that matches the current input, we out-
put the dictionary entry for s, remove s from the
input and add s followed by the next input symbol
to the dictionary.
For our experiments, we use our own imple-
mentation of LZW. We first build LZW dictionar-
ies by compressing our training sets as described
</bodyText>
<page confidence="0.99694">
30
</page>
<table confidence="0.999205333333333">
1k 8k 25k 50k
AR 28.42 38.78 46.92 51.89
CO 19.81 28.27 32.81 36.05
MX 22.07 33.90 43.10 45.06
ES 22.08 29.48 35.15 38.61
CL 27.08 28.22 33.59 36.68
</table>
<tableCaption confidence="0.996169">
Table 7: Results (F1): LZW without ties
</tableCaption>
<table confidence="0.999573285714286">
class precision recall F1
AR 70.96 68.36 69.64
CO 62.44 64.22 63.32
MX 66.37 65.67 66.02
ES 70.10 69.64 69.87
CL 68.97 70.72 69.83
overall 67.72 67.72 67.72
</table>
<tableCaption confidence="0.999419">
Table 8: Results: Voting
</tableCaption>
<bodyText confidence="0.999976894736842">
above, using different limits on dictionary lengths.
As symbol inventory, we use bytes, not unicode
symbols. Then we use these dictionaries to com-
press all tweets from all test sets, skipping the ini-
tialization stage. The country assigned to each
tweet is the one whose dictionary yields the high-
est compression. We run LZW with different max-
imal dictionary sizes.
The problem with the evaluation of the results
is that the compression produced many ties, i.e.,
the compression of a single tweet with dictionaries
from different languages resulted in identical com-
pression rates. On the concatenated dev sets (50k
tweets, i.e., 10k per country) with a maximal dic-
tionary size of 1k, 8k, 25k and 50k entries, we got
14.867, 20,166, 22,031, and 23,652 ties, respec-
tively. In 3,515 (7%), 4,839 (10%), 5,455 (11%)
and 6,102 (12%) cases, respectively, the correct re-
sult was hidden in a tie. If we replace the labels
of all tied instances with a new label TIE, we ob-
tain the F-scores shown in Table 7. While they are
higher than the scores for n-gram frequency pro-
files, they still lie well below the results for both
syllable and character language models.
While previous literature mentions an ideal size
limit on the dictionary of 8k entries (Bush, 2014),
we obtain better results the larger the dictionaries.
Note that already with a dictionary of size 1000,
even without including the ties, we are above the
20.00 F-score of a random baseline. The high
rate of ties constitutes a major problem of this ap-
proach, and remains even if we would find im-
provements to the approach (one possibility could
be to use unicode characters instead of bytes for
dictionary initialization). It cannot easily be alle-
viated, because if the compression rate is taken as
the score, particularly the scores for short tweets
are likely to coincide.
</bodyText>
<subsectionHeader confidence="0.976028">
4.5 Voting
</subsectionHeader>
<bodyText confidence="0.9999786">
Voting is a simple meta-classifying technique
which takes the output of different classifiers and
decides based on a predefined method on one of
them, thereby combining their strengths and level-
ing out their weaknesses. It has been successfully
used to improve language identification on Twitter
data by Lui and Baldwin (2014).
We utilize the character 5-gram and 6-gram lan-
guage models without pruning, as well as the syl-
lable 3-gram and 4-gram models. We decide as
follows. All instances for which the output of the
5-gram model coincides with the output of at least
one of the syllable models are labeled with the out-
put of the 5-gram model. For all other instances,
the output of the 6-gram model is used. The corre-
sponding results for all classes are shown in Table
8.
We obtain a slightly higher F-score than for
the 6-gram character language model (0.8 points).
In other words, even though the 6-gram language
model leads to the highest overall results among
individual models, in some instances it is out-
performed by the lower-order character language
model and by the syllable language models, which
have a lower overall score.
</bodyText>
<sectionHeader confidence="0.968703" genericHeader="method">
5 Human Tweet Classification
</sectionHeader>
<bodyText confidence="0.9999905625">
In order to get a better idea of the difficulty of the
task of classifying tweets by the country of their
authors, we have tweets classified by humans.
Generally, speakers of Spanish have limited
contact with speakers of other varieties, simply
due to geographical separation of varieties. We
therefore recur to a simplified version of our task,
in which the test subjects only have to distinguish
their own variety from one other variety, i.e., per-
form a binary classification. We randomly draw
two times 150 tweets from the Argentinian test and
150 tweets from the Chilean and Spanish test sets,
respectively. We then build shuffled concatena-
tions of the first 150 Argentinian and the Chilean
tweets, as well as of the remaining 150 Argen-
tinian and the Spanish tweets. Then we let three
</bodyText>
<page confidence="0.999734">
31
</page>
<table confidence="0.999763307692308">
data subject class prec. rec. F1
AR-ES AR AR 68.5 76.7 72.3
ES 73.5 64.7 68.8
ES AR 71.5 62.0 66.4
ES 66.5 75.3 70.6
n-gram AR 92.3 87.3 89.7
ES 88.0 92.7 90.3
AR-CL AR AR 61.0 77.3 68.2
CL 69.1 50.7 58.5
CL AR 70.0 70.0 70.0
CL 70.0 70.0 70.0
n-gram AR 93.4 84.7 88.8
CL 86.0 94.0 89.8
</table>
<tableCaption confidence="0.876235">
Table 9: Results: Human vs. automatic classifica-
tion
</tableCaption>
<table confidence="0.999903769230769">
data subject class prec. rec. F1
AR-ES AR AR 71.8 80.0 75.7
ES 74.8 65.4 69.7
ES AR 74.6 62.9 68.2
ES 65.1 76.3 70.2
n-gram AR 93.2 88.6 90.8
ES 88.1 92.9 90.4
AR-CL AR AR 61.1 78.6 68.8
CL 68.8 48.5 56.9
CL AR 73.0 71.4 72.2
CL 71.2 72.8 72.0
n-gram AR 95.3 87.1 91.0
CL 87.8 95.6 91.5
</table>
<tableCaption confidence="0.9708515">
Table 10: Results: Human vs. automatic classifi-
cation (filtered)
</tableCaption>
<bodyText confidence="0.999847970588236">
natives classify them. The test subjects are not
given any other training data samples or similar re-
sources before the task, and they are instructed not
to look up on the Internet any information within
the tweet that might reveal the country of its author
(such as hyperlinks, user mentions or hash tags).
Table 9 shows the results, together with the re-
sults on the same task of the character 6-gram
model without pruning. Note that with 300 test
instances out of 20,000, there is a sampling er-
ror of ± 4.7% (confidence interval 95%). The re-
sults confirm our intuition in the light of the good
performance achieved by the n-gram approach in
the 5-class case: when reducing the classification
problem from five classes to two, human classi-
fication performance is much below the perfor-
mance of automatic classification, by between 17
and 31 F-score points. In terms of error rate, the
human annotators made between 3 and 4 times
more classification errors than the automatic sys-
tem. One can observe a tendency among the hu-
man test subjects that more errors come from la-
beling too many tweets as coming from their na-
tive country than vice versa (cf. the recall values).
In order to better understand the large result dif-
ference, we ask the test subjects for the strategies
they used to label tweets. They stated that the eas-
iest tweets where those specifying a location (“Es-
toy en Madrid”), or referencing local named en-
tities (TV programs, public figures, etc.). In case
of absence of such information, other clues were
used that tend to occur in only one variety. They
include the use of different words (such as en-
fadado (Spain) vs. enojado (America) (“angry”)),
a different distribution of the same word (such as
the filler pues), and different inflection, such as the
second person plural verb forms, which in Amer-
ican Spanish, albeit sometimes not in Chile, is re-
placed by the identical third person plural forms
(for the verb hacer (“do”), the peninsular form
would be hac´eis instead of hacen), and the per-
sonal pronoun vos (“you”), which is rarely used
in Chile, and not used in Spain. To sum up, the
test subjects generally relied on lexical cues on
the surface, and were therefore bound to miss non-
obvious information captured by the character n-
gram model.
Since the test subjects also stated that some
tweets were impossible to assign to a country be-
cause they contained only URLs, emoticons, or
similar, in Table 10 we show a reevaluation of a
second version of the two shuffled concatenated
samples in which we remove all tweets which con-
tain only emoticons, URLs, or numbers; tweets
which are entirely written in a language other than
Spanish; and tweets which are only two or one
words long (i.e., tweets with zero or one spaces).
For the AR-ES data, we remove 23 Spanish and
10 Argentinian tweets, while for the AR-CL data,
we remove 10 Argentinian and 14 Chilean tweets.
As for the human classification on the AR/ES
data, the results for Spain do not change much. For
Argentina, there is an increase in performance (2
to 3 points). On the AR/CL data, there is a slight
improvement on all sets except for the Chilean
data classified.
As for the automatic classification, the filter-
ing gives better result on all data sets. However,
</bodyText>
<page confidence="0.997678">
32
</page>
<table confidence="0.998465666666667">
training dev test
AR 57,546 (71.9%) 7,174 7,196
CO 58,068 (72.6%) 7,249 7,289
MX 48,527 (60.7%) 6,117 6,061
ES 53,199 (66.5%) 6,699 6,657
CL 56,865 (71.1%) 6,998 7,071
</table>
<tableCaption confidence="0.921532">
Table 11: Data sizes (filtered by langid.py)
</tableCaption>
<table confidence="0.999679714285714">
class precision recall F1
AR 70.32 66.09 68.14
CO 63.76 62.22 62.98
MX 61.52 61.11 61.31
ES 69.13 69.20 69.17
CL 67.12 73.29 70.07
overall 66.45 66.45 66.45
</table>
<tableCaption confidence="0.99968">
Table 12: Results: Filtered by langid.py
</tableCaption>
<bodyText confidence="0.9999418">
the difference between the F1 of the filtered and
unfiltered data is larger on the AR/CL data set.
This can be explained with the fact that among
the tweets removed from the AR/ES data set, there
were more longer tweets (not written in Spanish)
than among the tweets removed from the CL/AR
data set, the longer tweets being easier to iden-
tify. Note that the filtering of tweets does not cause
much change in the difference between human and
automatic classification.
</bodyText>
<sectionHeader confidence="0.99405" genericHeader="method">
6 Language Filtering
</sectionHeader>
<bodyText confidence="0.999925">
As mentioned before, our data has not been
cleaned up or normalized. In particular, the data
set contains tweets written in languages other than
Spanish. We have reasoned that those can be seen
as belonging to the “natural” language production
of a country. However, in order to see what im-
pact they have on our classification results, we
perform an additional experiment on a version of
the data were we only include the tweets that the
state-of-the-art language identifier langid.py
labels Spanish (Lui and Baldwin, 2012).6 Table
11 shows the sizes of all data sets after filtering.
Note that many of the excluded tweets are in fact
written in Spanish, but are very noisy, due to or-
thography, Twitter hash tags, etc. The next most
frequent labels across all tweets is English (9%).
Note that in the data from Spain, 2% of the tweets
are labeled as Catalan, 1.2% as Galician, and only
0.3% as Basque.
Table 12 finally shows the classification re-
sults for character 6-gram language models with-
out pruning.
The changes in F1 are minor, i.e., below one
point, except for the Mexican tweets, which lose
around 4 points. The previous experiments have
already indicated that the Mexican data set is the
most heterogeneous one which also resulted in the
largest number of tweets being filtered out. In
general, we see that the character n-gram method
</bodyText>
<footnote confidence="0.795861">
6https://github.com/saffsd/langid.py.
</footnote>
<bodyText confidence="0.999496333333333">
seems to be relatively stable with respect to a dif-
ferent number of non-Spanish tweets in the data.
More insight could be obtained by performing ex-
periments with advanced methods of tweet nor-
malization, such as those of Han and Baldwin
(2011). We leave this for future work.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999801121212121">
Human classification of language varieties was
judged by our test subjects to be considerably
more difficult that differentiating between lan-
guages. Additionally, the test subjects were only
able to differentiate between two classes. While
the automatic classification results lie below the
results which one would expect for language iden-
tification, n-gram classification still achieves good
performance.
Our experiments touch on the more general
question of how a language variety is defined. In
order to take advantage of the metadata provided
by Twitter, we had to restrict the classification
problem to identifying varieties associated with
countries were tweets were sent. In reality, the
boundaries between variants are often blurred, and
there can also be variance within the same country
(e.g., the Spanish spoken in the southern Spanish
region of Andalusia is different from that of As-
turias, even if they both share features common
to Peninsular Spanish and larger differences with
American Spanish). However, it would be diffi-
cult to obtain a reliable corpus with this kind of
fine-grained distinctions.
It is also worth noting that not all the classifica-
tion criteria used by the human test subjects were
purely linguistic – for example, a subject could
guess a tweet as being from Chile by recogniz-
ing a mention to a Chilean city, public figure or
TV show. Note that this factor intuitively seems to
benefit humans – who have a wealth of knowledge
about entities, events and trending topics from
their country – over the automatic system. In spite
</bodyText>
<page confidence="0.997016">
33
</page>
<bodyText confidence="0.99982125">
of this, automatic classification still vastly outper-
formed human classification, suggesting that the
language models are capturing linguistic patterns
that are not obvious to humans.
</bodyText>
<sectionHeader confidence="0.994296" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999986416666667">
We have studied different approaches to the task
of classifying tweets from Spanish-speaking coun-
tries according to the country from which they
were sent. To the best of our knowledge, these are
the first results for this problem. On the problem
of assigning one of five classes (Argentina, Mex-
ico, Chile, Colombia, Spain) to 10,000 tweets, the
best performance, an overall F-score of 67.72, was
obtained with a voting meta-classifier approach
that recombines the results for four single clas-
sifiers, the 6-gram (66.96 Fi) and 5-gram (66.75
Fi) character-based language models, and the 4-
gram (57.87 Fi) and 3-gram (57.24 Fi) syllable-
based language models. For a simplified version
of the problem that only required a decision be-
tween two classes (Argentina vs. Chile and Spain
vs. Argentina), given a sample of 150 tweets from
each class, human classification was outperformed
by automatic classification by up to 31 points.
In future work, we want to investigate the ef-
fect of tweet normalization on our problem, and
furthermore, how the techniques we have used can
be applied to classify text from other social media
sources, such as Facebook.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999125">
The first author has been funded by Deutsche
Forschungsgemeinschaft (DFG). The second au-
thor has been partially funded by Ministerio
de Economia y Competitividad/FEDER (Grant
TIN2010-18552-C03-02) and by Xunta de Galicia
(Grant CN2012/008).
</bodyText>
<sectionHeader confidence="0.998581" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999461530303031">
Manuel Alvar, editor. 1996a. Manual de dialectologia
hisp´anica. El espa˜nol de Am´erica. Ariel, Barcelona.
Manuel Alvar, editor. 1996b. Manual de dialectologia
hisp´anica. El espa˜nol de Espa˜na. Ariel, Barcelona.
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229–237, Los Angeles, CA.
Dario Benedetto, Emanuele Caglioti, and Vittorio
Loreto. 2002. Language trees and zipping. Phys-
ical Review Letters, 88(4).
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, LSM ’12, pages 65–
74, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Ralph D. Brown. 2013. Selecting and weighting n-
grams to identify 1100 languages. In Springer, ed-
itor, Proceedings of the 16th International Confer-
ence on Text, Speech, and Dialogue, volume 8082 of
LNCS, pages 475–483, Pilsen, Czech Republic.
Brian O. Bush. 2014. Language identication of tweets
using LZW compression. In 3rd Pacific Northwest
Regional NLP Workshop: NW-NLP 2014, Redmond,
WA.
Simon Carter, Wouter Weerkamp, and Manos
Tsagkias. 2013. Microblog language identification:
overcoming the limitations of short, unedited and id-
iomatic text. Language Resources and Evaluation,
47(1):195–215.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In In Proceedings
of SDAIR-94, 3rd Annual Symposium on Document
Analysis and Information Retrieval, pages 161–175,
Las Vegas, NV.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging
for all: Overcoming sparse and noisy data. In Pro-
ceedings of the International Conference on Recent
Advances in Natural Language Processing, pages
198–206. Association for Computational Linguis-
tics.
Ted Dunning. 1994. Statistical identification of lan-
guage. Technical Report MCCS-94-273, Comput-
ing Research Lab, New Mexico State University.
Binod Gyawali, Gabriela Ramirez, and Thamar
Solorio. 2013. Native language identification: a
simple n-gram based approach. In Proceedings of
the Eighth Workshop on Innovative Use of NLP for
Building Educational Applications, pages 224–231,
Atlanta, Georgia, June. Association for Computa-
tional Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, pages 368–378, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Zen´on Hern´andez-Figeroa, Gustavo Rodr´ıguez-
Rodr´ıguez, and Francisco J. Carreras-
Riudavets. 2012. Separador de s´ılabas
</reference>
<page confidence="0.990952">
34
</page>
<reference confidence="0.999103916666667">
del espa˜nol - silabeador TIP. Available at
http://tip.dis.ulpgc.es.
Zen´on Hern´andez-Figueroa, Francisco J. Carreras-
Riudavets, and Gustavo Rodr´ıguez-Rodr´ıguez.
2013. Automatic syllabification for Spanish
using lemmatization and derivation to solve the
prefix’s prominence issue. Expert Syst. Appl.,
40(17):7122–7131.
Vlado Kes&amp;quot;elj, Fuchun Peng, Nick Cercone, and Calvin
Thomas. 2003. N-gram-based author profiles for
authorship attribution. In Proceedings of PACLING,
pages 255–264.
M. Paul Lewis, Gary F. Simons, and Charles D. Fen-
nig, editors. 2014. Ethnologue: Languages of
the World. SIL International, Dallas, Texas, sev-
enteenth edition edition. Online version: http:
//www.ethnologue.com.
John M. Lipski. 1994. Latin American Spanish. Long-
man, London.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the ACL 2012 System Demonstrations,
pages 25–30, Jeju Island, Korea, July. Association
for Computational Linguistics.
Marco Lui and Timothy Baldwin. 2014. Accurate
language identification of twitter messages. In Pro-
ceedings of the 5th Workshop on Language Analysis
for Social Media (LASM), pages 17–25, Gothenburg,
Sweden.
Marco Lui and Paul Cook. 2013. Classifying english
documents by national dialect. In Proceedings of
the Australasian Language Technology Association
Workshop 2013 (ALTA 2013), pages 5–15, Brisbane,
Australia, December.
Douglas W. Muir and Timothy R. Thomas. 2000. Au-
tomatic language identification by stroke geometry
analysis, May 16. US Patent 6,064,767.
Miguel ´Angel Quesada Pacheco. 2002. El Espa˜nol
de Am´erica. Editorial Tecnol´ogica de Costa Rica,
Cartago, 2a edition.
Vesa Siivola, Teemu Hirsim¨aki, and Sami Virpi-
oja. 2007. On growing and pruning kneser-
ney smoothed n-gram models. IEEE Transac-
tions on Speech, Audio and Language Processing,
15(5):1617–1624.
William J. Teahan. 2000. Text classification and seg-
mentation using minimum cross-entropy. In Pro-
ceedings of RIAO’00, pages 943–961.
Tommi Vatanen, Jaakko J. Vyrynen, and Sami Virpi-
oja. 2010. Language identification of short text
segments with n-gram models. In Proceedings
of the Seventh International Conference on Lan-
guage Resources and Evaluation (LREC’10), Val-
letta, Malta. European Language Resources Associ-
ation (ELRA).
David Vilares, Miguel A. Alonso, and Carlos G´omez-
Rodr´ıguez. 2013. Supervised polarity classification
of spanish tweets based on linguistic knowledge. In
Proceedings of 13th ACM Symposium on Document
Engineering (DocEng 2013), pages 169–172, Flo-
rence, Italy.
David Vilares, Miguel A. Alonso, and Carlos G´omez-
Rodr´ıguez. 2014. A syntactic approach for opinion
mining on Spanish reviews. Natural Language En-
gineering, FirstView:1–25, 6.
Radim &amp;quot;Reh˚u&amp;quot;rek and Milan Kolkus. 2009. Language
identification on the web: Extending the dictionary
method. In Proceedings of CICLing, pages 357–
368.
Terry A. Welch. 1984. A technique for high-
performance data compression. Computer, 17(6):8–
19, June.
Marcos Zampieri, Binyam Gebrekidan Gebre, and
Sascha Diwersy. 2013. N-gram language models
and pos distribution for the identification of spanish
varieties. In Proceedings of TALN2013, pages 580–
587.
Marcos Zampieri, Liling Tan, Nikola Ljube&amp;quot;si´c, and
J¨org Tiedemann. 2014. A report on the dsl shared
task 2014. In Proceedings of the First Workshop
on Applying NLP Tools to Similar Languages, Va-
rieties and Dialects, pages 58–67, Dublin, Ireland,
August. Association for Computational Linguistics
and Dublin City University.
</reference>
<page confidence="0.999329">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.727631">
<title confidence="0.998844">Language variety identification in Spanish tweets</title>
<author confidence="0.95984">Wolfgang</author>
<affiliation confidence="0.943897">Institute for Language and University of D¨usseldorf,</affiliation>
<email confidence="0.983283">maierw@hhu.de</email>
<abstract confidence="0.994909375">study the problem of variapproximated by the problem of labeling tweets from Spanish speaking countries by the country from which they were posted. While this task is closely related to “pure” language identification, it comes with additional complications. We build a balanced collection of tweets and apply techniques from language modeling. A simplified version of the task is also solved by human test subjects, who are outperformed by the automatic classification. Our best automatic system achieves an overall F-score of 67.7% on 5-class classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>1996a. Manual de dialectologia hisp´anica. El espa˜nol de Am´erica.</booktitle>
<editor>Manuel Alvar, editor.</editor>
<location>Ariel, Barcelona.</location>
<marker></marker>
<rawString>Manuel Alvar, editor. 1996a. Manual de dialectologia hisp´anica. El espa˜nol de Am´erica. Ariel, Barcelona.</rawString>
</citation>
<citation valid="false">
<editor>Manuel Alvar, editor. 1996b. Manual de dialectologia hisp´anica. El espa˜nol de Espa˜na. Ariel,</editor>
<location>Barcelona.</location>
<marker></marker>
<rawString>Manuel Alvar, editor. 1996b. Manual de dialectologia hisp´anica. El espa˜nol de Espa˜na. Ariel, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Marco Lui</author>
</authors>
<title>Language identification: The long and the short of the matter.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>229--237</pages>
<location>Los Angeles, CA.</location>
<contexts>
<context position="5877" citStr="Baldwin and Lui, 2010" startWordPosition="911" endWordPosition="914">erties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, and ˇReh˚uˇrek and Kolkus (2009), who treat “noisy” web text and therefore consider the particular influence of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and</context>
</contexts>
<marker>Baldwin, Lui, 2010</marker>
<rawString>Timothy Baldwin and Marco Lui. 2010. Language identification: The long and the short of the matter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 229–237, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dario Benedetto</author>
<author>Emanuele Caglioti</author>
<author>Vittorio Loreto</author>
</authors>
<title>Language trees and zipping.</title>
<date>2002</date>
<journal>Physical Review Letters,</journal>
<volume>88</volume>
<issue>4</issue>
<contexts>
<context position="5524" citStr="Benedetto et al., 2002" startWordPosition="858" endWordPosition="861">es language modeling. This technique is general and not limited to language identification; it has also been successfully employed in other areas, e.g., in authorship attribution (Keˇselj et al., 2003) and author native language identification (Gyawali et al., 2013). Other language identification systems use non-textual methods, exploiting optical properties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, and ˇReh˚uˇrek and Kolkus (2009), who treat “noisy” web text and therefore consider the particular influence of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the appro</context>
</contexts>
<marker>Benedetto, Caglioti, Loreto, 2002</marker>
<rawString>Dario Benedetto, Emanuele Caglioti, and Vittorio Loreto. 2002. Language trees and zipping. Physical Review Letters, 88(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Paul McNamee</author>
<author>Mossaab Bagdouri</author>
<author>Clayton Fink</author>
<author>Theresa Wilson</author>
</authors>
<title>Language identification for creating language-specific twitter collections.</title>
<date>2012</date>
<booktitle>In Proceedings of the Second Workshop on Language in Social Media, LSM ’12,</booktitle>
<pages>65--74</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6319" citStr="Bergsma et al. (2012)" startWordPosition="980" endWordPosition="983">nce of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools for Twitter language identification, and achieve their best results with a voting over three individual systems, one of them being langid.py (Lui and Baldwin, 2012). Carter et al. (2013) exploit particular characteristics of Twitter (such as user profile data and relations between Twitter users) to improve language identification on this genre. Bush (2014) successfully uses LZW co</context>
</contexts>
<marker>Bergsma, McNamee, Bagdouri, Fink, Wilson, 2012</marker>
<rawString>Shane Bergsma, Paul McNamee, Mossaab Bagdouri, Clayton Fink, and Theresa Wilson. 2012. Language identification for creating language-specific twitter collections. In Proceedings of the Second Workshop on Language in Social Media, LSM ’12, pages 65– 74, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph D Brown</author>
</authors>
<title>Selecting and weighting ngrams to identify 1100 languages.</title>
<date>2013</date>
<booktitle>Proceedings of the 16th International Conference on Text, Speech, and Dialogue,</booktitle>
<volume>8082</volume>
<pages>475--483</pages>
<editor>In Springer, editor,</editor>
<location>Pilsen, Czech Republic.</location>
<contexts>
<context position="5563" citStr="Brown (2013)" startWordPosition="866" endWordPosition="867">d not limited to language identification; it has also been successfully employed in other areas, e.g., in authorship attribution (Keˇselj et al., 2003) and author native language identification (Gyawali et al., 2013). Other language identification systems use non-textual methods, exploiting optical properties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, and ˇReh˚uˇrek and Kolkus (2009), who treat “noisy” web text and therefore consider the particular influence of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Googl</context>
</contexts>
<marker>Brown, 2013</marker>
<rawString>Ralph D. Brown. 2013. Selecting and weighting ngrams to identify 1100 languages. In Springer, editor, Proceedings of the 16th International Conference on Text, Speech, and Dialogue, volume 8082 of LNCS, pages 475–483, Pilsen, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian O Bush</author>
</authors>
<title>Language identication of tweets using LZW compression.</title>
<date>2014</date>
<booktitle>In 3rd Pacific Northwest Regional NLP Workshop: NW-NLP 2014,</booktitle>
<location>Redmond, WA.</location>
<contexts>
<context position="6894" citStr="Bush (2014)" startWordPosition="1066" endWordPosition="1067">rk on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools for Twitter language identification, and achieve their best results with a voting over three individual systems, one of them being langid.py (Lui and Baldwin, 2012). Carter et al. (2013) exploit particular characteristics of Twitter (such as user profile data and relations between Twitter users) to improve language identification on this genre. Bush (2014) successfully uses LZW compression for Twitter language identification. Within the field of natural language processing, the problem of language variant identification has only begun to be studied very recently. Zampieri et al. (2013) have addressed the task for Spanish newspaper texts, using character and word n-gram models as well as POS and morphological information. Very recently, the Discriminating between Similar Languages (DSL) Shared Task (Zampieri et al., 2014) proposed the problem of identifying between pairs of similar languages and language variants on sentences from newspaper corp</context>
<context position="10921" citStr="Bush, 2014" startWordPosition="1717" endWordPosition="1718">haracters. Lengths around 100 to 110 characters are the rarest. The clearest further trend is that the tweets from Colombia and, especially, Argentina tend to be shorter than the tweets from the other countries. 4 Automatic Tweet Classification The classification task we envisage is similar to the task of language identification in short text segments. We explore three methods that have been used before for that task, namely character n-gram frequency profiles (Cavnar and Trenkle, 1994; Vatanen et al., 2010), character n-gram language models (Vatanen et al., 2010), as well as LZW compression (Bush, 2014). Furthermore, we explore the usability of syllable-based language 3We are aware that the Twitter API does not make all sent tweets available. However, we still assume that this huge difference reflects a variance in the number of Twitter users. 0 20 40 60 80 100 120 140 Tweet length Figure 1: Tweet length distribution 50 100 500 1k 10k AR 31.68 29.72 43.93 31.77 18.42 CO 24.29 21.36 26.14 19.68 19.03 MX 31.86 28.97 32.58 30.28 22.27 ES 20.19 25.22 22.08 21.25 16.15 CL 22.95 29.74 35.67 26.01 16.69 Table 1: Results (F1): n-gram frequency profiles (classes/profile sizes) models. For all four ap</context>
<context position="21437" citStr="Bush (2014)" startWordPosition="3512" endWordPosition="3513">he F-score for the 2- gram language model is around 11 points higher than for the character 2-gram language model. Furthermore, the performance seems to converge earlier, given that the results change only slightly when raising the n-gram order from 3 to 4. The overall F-score for the 4-gram language model is around 6 points lower than for character 4-grams. However, the behavior of the classes is similar: again, Mexico and Colombia have slightly lower results than the other classes. 4.4 Compression We eventually test the applicability of compression-based classification using the approach of Bush (2014). As mentioned earlier, the assumption behind compression-based strategies for text categorization is that different text categories have a different entropy. Classification is possible because the effectivity of compression algorithms depends on the entropy of the data to be compressed (less entropy ≈ more compression). A simple classification algorithm is LempelZiv-Welch (LZW) (Welch, 1984). It is based on a dictionary which maps sequences of symbols to unique indices. Compression is achieved by replacing sequences of input symbols with the respective dictionary indices. More precisely, comp</context>
<context position="24220" citStr="Bush, 2014" startWordPosition="3979" endWordPosition="3980">ry) with a maximal dictionary size of 1k, 8k, 25k and 50k entries, we got 14.867, 20,166, 22,031, and 23,652 ties, respectively. In 3,515 (7%), 4,839 (10%), 5,455 (11%) and 6,102 (12%) cases, respectively, the correct result was hidden in a tie. If we replace the labels of all tied instances with a new label TIE, we obtain the F-scores shown in Table 7. While they are higher than the scores for n-gram frequency profiles, they still lie well below the results for both syllable and character language models. While previous literature mentions an ideal size limit on the dictionary of 8k entries (Bush, 2014), we obtain better results the larger the dictionaries. Note that already with a dictionary of size 1000, even without including the ties, we are above the 20.00 F-score of a random baseline. The high rate of ties constitutes a major problem of this approach, and remains even if we would find improvements to the approach (one possibility could be to use unicode characters instead of bytes for dictionary initialization). It cannot easily be alleviated, because if the compression rate is taken as the score, particularly the scores for short tweets are likely to coincide. 4.5 Voting Voting is a s</context>
</contexts>
<marker>Bush, 2014</marker>
<rawString>Brian O. Bush. 2014. Language identication of tweets using LZW compression. In 3rd Pacific Northwest Regional NLP Workshop: NW-NLP 2014, Redmond, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Carter</author>
<author>Wouter Weerkamp</author>
<author>Manos Tsagkias</author>
</authors>
<title>Microblog language identification: overcoming the limitations of short, unedited and idiomatic text.</title>
<date>2013</date>
<journal>Language Resources and Evaluation,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="6722" citStr="Carter et al. (2013)" startWordPosition="1040" endWordPosition="1043">echniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools for Twitter language identification, and achieve their best results with a voting over three individual systems, one of them being langid.py (Lui and Baldwin, 2012). Carter et al. (2013) exploit particular characteristics of Twitter (such as user profile data and relations between Twitter users) to improve language identification on this genre. Bush (2014) successfully uses LZW compression for Twitter language identification. Within the field of natural language processing, the problem of language variant identification has only begun to be studied very recently. Zampieri et al. (2013) have addressed the task for Spanish newspaper texts, using character and word n-gram models as well as POS and morphological information. Very recently, the Discriminating between Similar Langu</context>
</contexts>
<marker>Carter, Weerkamp, Tsagkias, 2013</marker>
<rawString>Simon Carter, Wouter Weerkamp, and Manos Tsagkias. 2013. Microblog language identification: overcoming the limitations of short, unedited and idiomatic text. Language Resources and Evaluation, 47(1):195–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Cavnar</author>
<author>John M Trenkle</author>
</authors>
<title>Ngram-based text categorization. In</title>
<date>1994</date>
<booktitle>In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,</booktitle>
<pages>161--175</pages>
<location>Las Vegas, NV.</location>
<contexts>
<context position="4777" citStr="Cavnar and Trenkle (1994)" startWordPosition="745" endWordPosition="748">l Linguistics reaches an overall F-score of 67.72 on the fiveclass problem. On the two-class problem, human classification is outperformed by a large margin. The remainder of this paper is structured as follows. In the following section, we present related work. Section 3 presents our data collection. Sections 4 and 5 present our classification methodology and the experiments. Section 7 discusses the results, and Section 8 concludes the article. 2 Related Work Research on language identification has seen a variety of methods. A well established technique is the use of character n-gram models. Cavnar and Trenkle (1994) build n-gram frequency “profiles” for several languages and classify text by matching it to the profiles. Dunning (1994) uses language modeling. This technique is general and not limited to language identification; it has also been successfully employed in other areas, e.g., in authorship attribution (Keˇselj et al., 2003) and author native language identification (Gyawali et al., 2013). Other language identification systems use non-textual methods, exploiting optical properties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption </context>
<context position="10800" citStr="Cavnar and Trenkle, 1994" startWordPosition="1695" endWordPosition="1698">elopment set, as shown in Figure 1. We see that in all countries, tweets tend to be either short, or take advantage of all available characters. Lengths around 100 to 110 characters are the rarest. The clearest further trend is that the tweets from Colombia and, especially, Argentina tend to be shorter than the tweets from the other countries. 4 Automatic Tweet Classification The classification task we envisage is similar to the task of language identification in short text segments. We explore three methods that have been used before for that task, namely character n-gram frequency profiles (Cavnar and Trenkle, 1994; Vatanen et al., 2010), character n-gram language models (Vatanen et al., 2010), as well as LZW compression (Bush, 2014). Furthermore, we explore the usability of syllable-based language 3We are aware that the Twitter API does not make all sent tweets available. However, we still assume that this huge difference reflects a variance in the number of Twitter users. 0 20 40 60 80 100 120 140 Tweet length Figure 1: Tweet length distribution 50 100 500 1k 10k AR 31.68 29.72 43.93 31.77 18.42 CO 24.29 21.36 26.14 19.68 19.03 MX 31.86 28.97 32.58 30.28 22.27 ES 20.19 25.22 22.08 21.25 16.15 CL 22.95</context>
<context position="12293" citStr="Cavnar and Trenkle (1994)" startWordPosition="1951" endWordPosition="1954">s final label, we take the output of the one of the five classifiers that has the highest score. We finally use a meta-classifier on the basis of voting. All methods are tested on the development set. For evaluation, we compute precision, recall and F1 overall as well as for single classes. Note that we decided to rely on the tweet text only. An exploration of the benefit of, e.g., directly exploiting Twitter-specific information (such as user mentions or hash tags) is out of the scope of this paper. 4.1 Character n-gram frequency profiles We first investigate the n-gram frequency approach of Cavnar and Trenkle (1994). We use the well-known implementation TextCat.4 The results for all classes with different profile sizes are shown in Table 1. Table 2 shows precision and recall for the best setting, a profile with a maximal size of 500 entries. The results obtained with a profile size of 500 4As available from http://odur.let.rug.nl/ ˜vannoord/TextCat/. 200 150 100 50 0 ES CL CO AR MX 27 class precision recall F1 AR 32.60 67.33 43.93 CO 31.66 22.26 26.14 MX 51.52 23.82 32.58 ES 32.83 16.63 22.08 CL 31.96 40.36 35.67 overall 34.08 34.08 34.08 Table 2: Results: n-gram frequency profile with 500 n-grams F-scor</context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>William B. Cavnar and John M. Trenkle. 1994. Ngram-based text categorization. In In Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval, pages 161–175, Las Vegas, NV.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Twitter part-of-speech tagging for all: Overcoming sparse and noisy data.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>198--206</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7776" citStr="Derczynski et al., 2013" startWordPosition="1198" endWordPosition="1201">for Spanish newspaper texts, using character and word n-gram models as well as POS and morphological information. Very recently, the Discriminating between Similar Languages (DSL) Shared Task (Zampieri et al., 2014) proposed the problem of identifying between pairs of similar languages and language variants on sentences from newspaper corpora, one of the pairs being Peninsular vs. Argentine Spanish. However, all these approaches are tailored to the standard language found in news sources, very different from the colloquial, noisy language of tweets, which presents distinct challenges for NLP (Derczynski et al., 2013; Vilares et al., 2013). Lui and Cook (2013) evaluate various approaches to classify documents into Australian, British and Canadian English, including a corpus of tweets, but we are not aware of any previous work on variant identification in Spanish tweets. A review of research on Spanish varieties from a linguistics point of view is beyond the scope of this article. Recommended further literature in this area is Lipski (1994), Quesada Pacheco (2002) and Alvar (1996b; 1996a). 3 Data Collection We first built a collection of tweets using the Twitter streaming API,2 requesting all tweets sent w</context>
</contexts>
<marker>Derczynski, Ritter, Clark, Bontcheva, 2013</marker>
<rawString>Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all: Overcoming sparse and noisy data. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, pages 198–206. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Statistical identification of language.</title>
<date>1994</date>
<tech>Technical Report MCCS-94-273,</tech>
<institution>Computing Research Lab, New Mexico State University.</institution>
<contexts>
<context position="4898" citStr="Dunning (1994)" startWordPosition="766" endWordPosition="767">formed by a large margin. The remainder of this paper is structured as follows. In the following section, we present related work. Section 3 presents our data collection. Sections 4 and 5 present our classification methodology and the experiments. Section 7 discusses the results, and Section 8 concludes the article. 2 Related Work Research on language identification has seen a variety of methods. A well established technique is the use of character n-gram models. Cavnar and Trenkle (1994) build n-gram frequency “profiles” for several languages and classify text by matching it to the profiles. Dunning (1994) uses language modeling. This technique is general and not limited to language identification; it has also been successfully employed in other areas, e.g., in authorship attribution (Keˇselj et al., 2003) and author native language identification (Gyawali et al., 2013). Other language identification systems use non-textual methods, exploiting optical properties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 200</context>
</contexts>
<marker>Dunning, 1994</marker>
<rawString>Ted Dunning. 1994. Statistical identification of language. Technical Report MCCS-94-273, Computing Research Lab, New Mexico State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binod Gyawali</author>
<author>Gabriela Ramirez</author>
<author>Thamar Solorio</author>
</authors>
<title>Native language identification: a simple n-gram based approach.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>224--231</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="5167" citStr="Gyawali et al., 2013" startWordPosition="805" endWordPosition="808">usses the results, and Section 8 concludes the article. 2 Related Work Research on language identification has seen a variety of methods. A well established technique is the use of character n-gram models. Cavnar and Trenkle (1994) build n-gram frequency “profiles” for several languages and classify text by matching it to the profiles. Dunning (1994) uses language modeling. This technique is general and not limited to language identification; it has also been successfully employed in other areas, e.g., in authorship attribution (Keˇselj et al., 2003) and author native language identification (Gyawali et al., 2013). Other language identification systems use non-textual methods, exploiting optical properties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, and ˇReh˚uˇrek and Kolkus (2009), who treat “noisy” web text and therefore consider the particular influence of single words in discriminating between languages. Language ide</context>
</contexts>
<marker>Gyawali, Ramirez, Solorio, 2013</marker>
<rawString>Binod Gyawali, Gabriela Ramirez, and Thamar Solorio. 2013. Native language identification: a simple n-gram based approach. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 224–231, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalisation of short text messages: Makn sens a #twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>368--378</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="33231" citStr="Han and Baldwin (2011)" startWordPosition="5538" endWordPosition="5541">g. The changes in F1 are minor, i.e., below one point, except for the Mexican tweets, which lose around 4 points. The previous experiments have already indicated that the Mexican data set is the most heterogeneous one which also resulted in the largest number of tweets being filtered out. In general, we see that the character n-gram method 6https://github.com/saffsd/langid.py. seems to be relatively stable with respect to a different number of non-Spanish tweets in the data. More insight could be obtained by performing experiments with advanced methods of tweet normalization, such as those of Han and Baldwin (2011). We leave this for future work. 7 Discussion Human classification of language varieties was judged by our test subjects to be considerably more difficult that differentiating between languages. Additionally, the test subjects were only able to differentiate between two classes. While the automatic classification results lie below the results which one would expect for language identification, n-gram classification still achieves good performance. Our experiments touch on the more general question of how a language variety is defined. In order to take advantage of the metadata provided by Twit</context>
</contexts>
<marker>Han, Baldwin, 2011</marker>
<rawString>Bo Han and Timothy Baldwin. 2011. Lexical normalisation of short text messages: Makn sens a #twitter. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 368–378, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zen´on Hern´andez-Figeroa</author>
<author>Gustavo Rodr´ıguezRodr´ıguez</author>
<author>Francisco J CarrerasRiudavets</author>
</authors>
<title>Separador de s´ılabas del espa˜nol - silabeador TIP. Available at http://tip.dis.ulpgc.es.</title>
<date>2012</date>
<marker>Hern´andez-Figeroa, Rodr´ıguezRodr´ıguez, CarrerasRiudavets, 2012</marker>
<rawString>Zen´on Hern´andez-Figeroa, Gustavo Rodr´ıguezRodr´ıguez, and Francisco J. CarrerasRiudavets. 2012. Separador de s´ılabas del espa˜nol - silabeador TIP. Available at http://tip.dis.ulpgc.es.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zen´on Hern´andez-Figueroa</author>
<author>Francisco J CarrerasRiudavets</author>
<author>Gustavo Rodr´ıguez-Rodr´ıguez</author>
</authors>
<title>Automatic syllabification for Spanish using lemmatization and derivation to solve the prefix’s prominence issue. Expert Syst.</title>
<date>2013</date>
<journal>Appl.,</journal>
<volume>40</volume>
<issue>17</issue>
<marker>Hern´andez-Figueroa, CarrerasRiudavets, Rodr´ıguez-Rodr´ıguez, 2013</marker>
<rawString>Zen´on Hern´andez-Figueroa, Francisco J. CarrerasRiudavets, and Gustavo Rodr´ıguez-Rodr´ıguez. 2013. Automatic syllabification for Spanish using lemmatization and derivation to solve the prefix’s prominence issue. Expert Syst. Appl., 40(17):7122–7131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vlado Keselj</author>
<author>Fuchun Peng</author>
<author>Nick Cercone</author>
<author>Calvin Thomas</author>
</authors>
<title>N-gram-based author profiles for authorship attribution.</title>
<date>2003</date>
<booktitle>In Proceedings of PACLING,</booktitle>
<pages>255--264</pages>
<marker>Keselj, Peng, Cercone, Thomas, 2003</marker>
<rawString>Vlado Kes&amp;quot;elj, Fuchun Peng, Nick Cercone, and Calvin Thomas. 2003. N-gram-based author profiles for authorship attribution. In Proceedings of PACLING, pages 255–264.</rawString>
</citation>
<citation valid="true">
<title>edition edition. Online version: http: //www.ethnologue.com.</title>
<date>2014</date>
<booktitle>Ethnologue: Languages of the World. SIL International,</booktitle>
<editor>M. Paul Lewis, Gary F. Simons, and Charles D. Fennig, editors.</editor>
<location>Dallas, Texas, seventeenth</location>
<contexts>
<context position="6492" citStr="(2014)" startWordPosition="1006" endWordPosition="1006">lly due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools for Twitter language identification, and achieve their best results with a voting over three individual systems, one of them being langid.py (Lui and Baldwin, 2012). Carter et al. (2013) exploit particular characteristics of Twitter (such as user profile data and relations between Twitter users) to improve language identification on this genre. Bush (2014) successfully uses LZW compression for Twitter language identification. Within the field of natural language processing, the problem of language variant identification has only begun to be studied v</context>
<context position="21437" citStr="(2014)" startWordPosition="3513" endWordPosition="3513">score for the 2- gram language model is around 11 points higher than for the character 2-gram language model. Furthermore, the performance seems to converge earlier, given that the results change only slightly when raising the n-gram order from 3 to 4. The overall F-score for the 4-gram language model is around 6 points lower than for character 4-grams. However, the behavior of the classes is similar: again, Mexico and Colombia have slightly lower results than the other classes. 4.4 Compression We eventually test the applicability of compression-based classification using the approach of Bush (2014). As mentioned earlier, the assumption behind compression-based strategies for text categorization is that different text categories have a different entropy. Classification is possible because the effectivity of compression algorithms depends on the entropy of the data to be compressed (less entropy ≈ more compression). A simple classification algorithm is LempelZiv-Welch (LZW) (Welch, 1984). It is based on a dictionary which maps sequences of symbols to unique indices. Compression is achieved by replacing sequences of input symbols with the respective dictionary indices. More precisely, comp</context>
<context position="25133" citStr="(2014)" startWordPosition="4130" endWordPosition="4130">to the approach (one possibility could be to use unicode characters instead of bytes for dictionary initialization). It cannot easily be alleviated, because if the compression rate is taken as the score, particularly the scores for short tweets are likely to coincide. 4.5 Voting Voting is a simple meta-classifying technique which takes the output of different classifiers and decides based on a predefined method on one of them, thereby combining their strengths and leveling out their weaknesses. It has been successfully used to improve language identification on Twitter data by Lui and Baldwin (2014). We utilize the character 5-gram and 6-gram language models without pruning, as well as the syllable 3-gram and 4-gram models. We decide as follows. All instances for which the output of the 5-gram model coincides with the output of at least one of the syllable models are labeled with the output of the 5-gram model. For all other instances, the output of the 6-gram model is used. The corresponding results for all classes are shown in Table 8. We obtain a slightly higher F-score than for the 6-gram character language model (0.8 points). In other words, even though the 6-gram language model lea</context>
</contexts>
<marker>2014</marker>
<rawString>M. Paul Lewis, Gary F. Simons, and Charles D. Fennig, editors. 2014. Ethnologue: Languages of the World. SIL International, Dallas, Texas, seventeenth edition edition. Online version: http: //www.ethnologue.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Lipski</author>
</authors>
<date>1994</date>
<publisher>Latin American Spanish. Longman,</publisher>
<location>London.</location>
<contexts>
<context position="1077" citStr="Lipski, 1994" startWordPosition="161" endWordPosition="162">We build a balanced collection of tweets and apply techniques from language modeling. A simplified version of the task is also solved by human test subjects, who are outperformed by the automatic classification. Our best automatic system achieves an overall F-score of 67.7% on 5-class classification. 1 Introduction Spanish (or castellano), a descendant of Latin, is currently the language with the second largest number of native speakers after Mandarin Chinese, namely around 414 million people (Lewis et al., 2014). Spanish has a large number of regional varieties across Spain and the Americas (Lipski, 1994).1 They diverge in spoken language and vocabulary and also, albeit to a lesser extent, in syntax. Between different American varieties of Spanish, there are important differences; however, the largest differences can be found between American and European (“Peninsular”) Spanish. Language identification, the task of automatically identifying the natural language used in a given text segment, is a relatively well understood problem (see Section 2). To our knowledge, however, there is little previous work on the identification of the varieties of a single language, such as the regional varieties </context>
<context position="8207" citStr="Lipski (1994)" startWordPosition="1269" endWordPosition="1270">ored to the standard language found in news sources, very different from the colloquial, noisy language of tweets, which presents distinct challenges for NLP (Derczynski et al., 2013; Vilares et al., 2013). Lui and Cook (2013) evaluate various approaches to classify documents into Australian, British and Canadian English, including a corpus of tweets, but we are not aware of any previous work on variant identification in Spanish tweets. A review of research on Spanish varieties from a linguistics point of view is beyond the scope of this article. Recommended further literature in this area is Lipski (1994), Quesada Pacheco (2002) and Alvar (1996b; 1996a). 3 Data Collection We first built a collection of tweets using the Twitter streaming API,2 requesting all tweets sent within the geographic areas given by the coordinates -120°, -55° and -29°, 30° (roughly delimiting Latin America), as well as -10°, 35° and 3°, 46° (roughly delimiting Spain). The download ran from July 2 to July 4, 2014. In a second step, we sorted the tweets according to the respective countries. Twitter is not used to the same extent in all countries where Spanish is spoken. In the time 2https://dev.twitter.com/docs/api/ stre</context>
</contexts>
<marker>Lipski, 1994</marker>
<rawString>John M. Lipski. 1994. Latin American Spanish. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>langid.py: An off-the-shelf language identification tool.</title>
<date>2012</date>
<booktitle>In Proceedings of the ACL 2012 System Demonstrations,</booktitle>
<pages>25--30</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="6700" citStr="Lui and Baldwin, 2012" startWordPosition="1036" endWordPosition="1039">th elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools for Twitter language identification, and achieve their best results with a voting over three individual systems, one of them being langid.py (Lui and Baldwin, 2012). Carter et al. (2013) exploit particular characteristics of Twitter (such as user profile data and relations between Twitter users) to improve language identification on this genre. Bush (2014) successfully uses LZW compression for Twitter language identification. Within the field of natural language processing, the problem of language variant identification has only begun to be studied very recently. Zampieri et al. (2013) have addressed the task for Spanish newspaper texts, using character and word n-gram models as well as POS and morphological information. Very recently, the Discriminating</context>
<context position="32129" citStr="Lui and Baldwin, 2012" startWordPosition="5352" endWordPosition="5355">use much change in the difference between human and automatic classification. 6 Language Filtering As mentioned before, our data has not been cleaned up or normalized. In particular, the data set contains tweets written in languages other than Spanish. We have reasoned that those can be seen as belonging to the “natural” language production of a country. However, in order to see what impact they have on our classification results, we perform an additional experiment on a version of the data were we only include the tweets that the state-of-the-art language identifier langid.py labels Spanish (Lui and Baldwin, 2012).6 Table 11 shows the sizes of all data sets after filtering. Note that many of the excluded tweets are in fact written in Spanish, but are very noisy, due to orthography, Twitter hash tags, etc. The next most frequent labels across all tweets is English (9%). Note that in the data from Spain, 2% of the tweets are labeled as Catalan, 1.2% as Galician, and only 0.3% as Basque. Table 12 finally shows the classification results for character 6-gram language models without pruning. The changes in F1 are minor, i.e., below one point, except for the Mexican tweets, which lose around 4 points. The pr</context>
</contexts>
<marker>Lui, Baldwin, 2012</marker>
<rawString>Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings of the ACL 2012 System Demonstrations, pages 25–30, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Timothy Baldwin</author>
</authors>
<title>Accurate language identification of twitter messages.</title>
<date>2014</date>
<booktitle>In Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM),</booktitle>
<pages>17--25</pages>
<location>Gothenburg,</location>
<contexts>
<context position="6492" citStr="Lui and Baldwin (2014)" startWordPosition="1003" endWordPosition="1006">, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools for Twitter language identification, and achieve their best results with a voting over three individual systems, one of them being langid.py (Lui and Baldwin, 2012). Carter et al. (2013) exploit particular characteristics of Twitter (such as user profile data and relations between Twitter users) to improve language identification on this genre. Bush (2014) successfully uses LZW compression for Twitter language identification. Within the field of natural language processing, the problem of language variant identification has only begun to be studied v</context>
<context position="25133" citStr="Lui and Baldwin (2014)" startWordPosition="4127" endWordPosition="4130">nd improvements to the approach (one possibility could be to use unicode characters instead of bytes for dictionary initialization). It cannot easily be alleviated, because if the compression rate is taken as the score, particularly the scores for short tweets are likely to coincide. 4.5 Voting Voting is a simple meta-classifying technique which takes the output of different classifiers and decides based on a predefined method on one of them, thereby combining their strengths and leveling out their weaknesses. It has been successfully used to improve language identification on Twitter data by Lui and Baldwin (2014). We utilize the character 5-gram and 6-gram language models without pruning, as well as the syllable 3-gram and 4-gram models. We decide as follows. All instances for which the output of the 5-gram model coincides with the output of at least one of the syllable models are labeled with the output of the 5-gram model. For all other instances, the output of the 6-gram model is used. The corresponding results for all classes are shown in Table 8. We obtain a slightly higher F-score than for the 6-gram character language model (0.8 points). In other words, even though the 6-gram language model lea</context>
</contexts>
<marker>Lui, Baldwin, 2014</marker>
<rawString>Marco Lui and Timothy Baldwin. 2014. Accurate language identification of twitter messages. In Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM), pages 17–25, Gothenburg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Lui</author>
<author>Paul Cook</author>
</authors>
<title>Classifying english documents by national dialect.</title>
<date>2013</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>5--15</pages>
<location>Brisbane, Australia,</location>
<contexts>
<context position="7820" citStr="Lui and Cook (2013)" startWordPosition="1206" endWordPosition="1209"> word n-gram models as well as POS and morphological information. Very recently, the Discriminating between Similar Languages (DSL) Shared Task (Zampieri et al., 2014) proposed the problem of identifying between pairs of similar languages and language variants on sentences from newspaper corpora, one of the pairs being Peninsular vs. Argentine Spanish. However, all these approaches are tailored to the standard language found in news sources, very different from the colloquial, noisy language of tweets, which presents distinct challenges for NLP (Derczynski et al., 2013; Vilares et al., 2013). Lui and Cook (2013) evaluate various approaches to classify documents into Australian, British and Canadian English, including a corpus of tweets, but we are not aware of any previous work on variant identification in Spanish tweets. A review of research on Spanish varieties from a linguistics point of view is beyond the scope of this article. Recommended further literature in this area is Lipski (1994), Quesada Pacheco (2002) and Alvar (1996b; 1996a). 3 Data Collection We first built a collection of tweets using the Twitter streaming API,2 requesting all tweets sent within the geographic areas given by the coor</context>
</contexts>
<marker>Lui, Cook, 2013</marker>
<rawString>Marco Lui and Paul Cook. 2013. Classifying english documents by national dialect. In Proceedings of the Australasian Language Technology Association Workshop 2013 (ALTA 2013), pages 5–15, Brisbane, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas W Muir</author>
<author>Timothy R Thomas</author>
</authors>
<title>Automatic language identification by stroke geometry analysis,</title>
<date>2000</date>
<tech>US Patent 6,064,767.</tech>
<contexts>
<context position="5317" citStr="Muir and Thomas, 2000" startWordPosition="825" endWordPosition="828">blished technique is the use of character n-gram models. Cavnar and Trenkle (1994) build n-gram frequency “profiles” for several languages and classify text by matching it to the profiles. Dunning (1994) uses language modeling. This technique is general and not limited to language identification; it has also been successfully employed in other areas, e.g., in authorship attribution (Keˇselj et al., 2003) and author native language identification (Gyawali et al., 2013). Other language identification systems use non-textual methods, exploiting optical properties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, and ˇReh˚uˇrek and Kolkus (2009), who treat “noisy” web text and therefore consider the particular influence of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter,</context>
</contexts>
<marker>Muir, Thomas, 2000</marker>
<rawString>Douglas W. Muir and Timothy R. Thomas. 2000. Automatic language identification by stroke geometry analysis, May 16. US Patent 6,064,767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel ´Angel Quesada Pacheco</author>
</authors>
<title>edition.</title>
<date>2002</date>
<booktitle>El Espa˜nol de Am´erica. Editorial Tecnol´ogica de Costa Rica,</booktitle>
<location>Cartago,</location>
<contexts>
<context position="8231" citStr="Pacheco (2002)" startWordPosition="1272" endWordPosition="1273">nguage found in news sources, very different from the colloquial, noisy language of tweets, which presents distinct challenges for NLP (Derczynski et al., 2013; Vilares et al., 2013). Lui and Cook (2013) evaluate various approaches to classify documents into Australian, British and Canadian English, including a corpus of tweets, but we are not aware of any previous work on variant identification in Spanish tweets. A review of research on Spanish varieties from a linguistics point of view is beyond the scope of this article. Recommended further literature in this area is Lipski (1994), Quesada Pacheco (2002) and Alvar (1996b; 1996a). 3 Data Collection We first built a collection of tweets using the Twitter streaming API,2 requesting all tweets sent within the geographic areas given by the coordinates -120°, -55° and -29°, 30° (roughly delimiting Latin America), as well as -10°, 35° and 3°, 46° (roughly delimiting Spain). The download ran from July 2 to July 4, 2014. In a second step, we sorted the tweets according to the respective countries. Twitter is not used to the same extent in all countries where Spanish is spoken. In the time 2https://dev.twitter.com/docs/api/ streaming 26 # it took to co</context>
</contexts>
<marker>Pacheco, 2002</marker>
<rawString>Miguel ´Angel Quesada Pacheco. 2002. El Espa˜nol de Am´erica. Editorial Tecnol´ogica de Costa Rica, Cartago, 2a edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vesa Siivola</author>
<author>Teemu Hirsim¨aki</author>
<author>Sami Virpioja</author>
</authors>
<title>On growing and pruning kneserney smoothed n-gram models.</title>
<date>2007</date>
<journal>IEEE Transactions on Speech, Audio and Language Processing,</journal>
<volume>15</volume>
<issue>5</issue>
<marker>Siivola, Hirsim¨aki, Virpioja, 2007</marker>
<rawString>Vesa Siivola, Teemu Hirsim¨aki, and Sami Virpioja. 2007. On growing and pruning kneserney smoothed n-gram models. IEEE Transactions on Speech, Audio and Language Processing, 15(5):1617–1624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William J Teahan</author>
</authors>
<title>Text classification and segmentation using minimum cross-entropy.</title>
<date>2000</date>
<booktitle>In Proceedings of RIAO’00,</booktitle>
<pages>943--961</pages>
<contexts>
<context position="5499" citStr="Teahan, 2000" startWordPosition="856" endWordPosition="857">ning (1994) uses language modeling. This technique is general and not limited to language identification; it has also been successfully employed in other areas, e.g., in authorship attribution (Keˇselj et al., 2003) and author native language identification (Gyawali et al., 2013). Other language identification systems use non-textual methods, exploiting optical properties of text such as stroke geometry (Muir and Thomas, 2000), or using compression methods which rely on the assumption that natural languages differ by their entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, and ˇReh˚uˇrek and Kolkus (2009), who treat “noisy” web text and therefore consider the particular influence of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothin</context>
</contexts>
<marker>Teahan, 2000</marker>
<rawString>William J. Teahan. 2000. Text classification and segmentation using minimum cross-entropy. In Proceedings of RIAO’00, pages 943–961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommi Vatanen</author>
<author>Jaakko J Vyrynen</author>
<author>Sami Virpioja</author>
</authors>
<title>Language identification of short text segments with n-gram models.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="6034" citStr="Vatanen et al. (2010)" startWordPosition="934" endWordPosition="937">ir entropy, and consequently by the rate to which they can be compressed (Teahan, 2000; Benedetto et al., 2002). Two newer approaches are Brown (2013), who uses character n-grams, and ˇReh˚uˇrek and Kolkus (2009), who treat “noisy” web text and therefore consider the particular influence of single words in discriminating between languages. Language identification is harder the shorter the text segments whose language is to be identified (Baldwin and Lui, 2010). Especially due to the rise of Twitter, this particular problem has recently received attention. Several solutions have been proposed. Vatanen et al. (2010) compare character n-gram language models with elaborate smoothing techniques to the approach of Cavnar and Trenkle and the Google Language ID API, on the basis of different versions of the Universal Declaration of Human Rights. Other researchers work on Twitter. Bergsma et al. (2012) use language identification to create language specific tweet collections, thereby facilitating more high-quality results with supervised techniques. Lui and Baldwin (2014) review a wide range of off-the-shelf tools for Twitter language identification, and achieve their best results with a voting over three indiv</context>
<context position="10823" citStr="Vatanen et al., 2010" startWordPosition="1699" endWordPosition="1702">Figure 1. We see that in all countries, tweets tend to be either short, or take advantage of all available characters. Lengths around 100 to 110 characters are the rarest. The clearest further trend is that the tweets from Colombia and, especially, Argentina tend to be shorter than the tweets from the other countries. 4 Automatic Tweet Classification The classification task we envisage is similar to the task of language identification in short text segments. We explore three methods that have been used before for that task, namely character n-gram frequency profiles (Cavnar and Trenkle, 1994; Vatanen et al., 2010), character n-gram language models (Vatanen et al., 2010), as well as LZW compression (Bush, 2014). Furthermore, we explore the usability of syllable-based language 3We are aware that the Twitter API does not make all sent tweets available. However, we still assume that this huge difference reflects a variance in the number of Twitter users. 0 20 40 60 80 100 120 140 Tweet length Figure 1: Tweet length distribution 50 100 500 1k 10k AR 31.68 29.72 43.93 31.77 18.42 CO 24.29 21.36 26.14 19.68 19.03 MX 31.86 28.97 32.58 30.28 22.27 ES 20.19 25.22 22.08 21.25 16.15 CL 22.95 29.74 35.67 26.01 16.6</context>
<context position="14189" citStr="Vatanen et al., 2010" startWordPosition="2283" endWordPosition="2286"> matrix in Table 3 reveals that tweets from all classes are likely to be mislabeled as coming from Argentina, while, on the other hand, Mexican tweets are mislabeled most frequently as coming from other countries. Overall, the n-gram frequency profiles are not very good at our task, achieving an maximal overall F-score of only 34.08 with a profile size of 500 entries. However, this performance is still well above the 20.00 F-score we would obtain with a random baseline. Larger profile sizes deteriorate results: with 10,000 entries, we only have an overall F-score of 18.23. As observed before (Vatanen et al., 2010), the weak performance can most likely be attributed to the shortness of the tweets and the resulting lack of frequent n-grams that hinders a successful profile matching. While Vatanen et al. alleviate this problem to some extent, they have more success with character-level n-gram language models, the approach which we explore next. Figure 2: Character n-gram lm: Pruning vs. ngram order 4.2 Character n-gram language models We recur to n-gram language models as available in variKN (Siivola et al., 2007).5 We run variKN with absolute discounting and the crossproduct of four different pruning set</context>
</contexts>
<marker>Vatanen, Vyrynen, Virpioja, 2010</marker>
<rawString>Tommi Vatanen, Jaakko J. Vyrynen, and Sami Virpioja. 2010. Language identification of short text segments with n-gram models. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilares</author>
<author>Miguel A Alonso</author>
<author>Carlos G´omezRodr´ıguez</author>
</authors>
<title>Supervised polarity classification of spanish tweets based on linguistic knowledge.</title>
<date>2013</date>
<booktitle>In Proceedings of 13th ACM Symposium on Document Engineering (DocEng</booktitle>
<pages>169--172</pages>
<location>Florence, Italy.</location>
<marker>Vilares, Alonso, G´omezRodr´ıguez, 2013</marker>
<rawString>David Vilares, Miguel A. Alonso, and Carlos G´omezRodr´ıguez. 2013. Supervised polarity classification of spanish tweets based on linguistic knowledge. In Proceedings of 13th ACM Symposium on Document Engineering (DocEng 2013), pages 169–172, Florence, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilares</author>
<author>Miguel A Alonso</author>
<author>Carlos G´omezRodr´ıguez</author>
</authors>
<title>A syntactic approach for opinion mining on Spanish reviews.</title>
<date>2014</date>
<journal>Natural Language Engineering, FirstView:1–25,</journal>
<volume>6</volume>
<marker>Vilares, Alonso, G´omezRodr´ıguez, 2014</marker>
<rawString>David Vilares, Miguel A. Alonso, and Carlos G´omezRodr´ıguez. 2014. A syntactic approach for opinion mining on Spanish reviews. Natural Language Engineering, FirstView:1–25, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim Reh˚urek</author>
<author>Milan Kolkus</author>
</authors>
<title>Language identification on the web: Extending the dictionary method.</title>
<date>2009</date>
<booktitle>In Proceedings of CICLing,</booktitle>
<pages>357--368</pages>
<marker>Reh˚urek, Kolkus, 2009</marker>
<rawString>Radim &amp;quot;Reh˚u&amp;quot;rek and Milan Kolkus. 2009. Language identification on the web: Extending the dictionary method. In Proceedings of CICLing, pages 357– 368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry A Welch</author>
</authors>
<title>A technique for highperformance data compression.</title>
<date>1984</date>
<journal>Computer,</journal>
<volume>17</volume>
<issue>6</issue>
<contexts>
<context position="21832" citStr="Welch, 1984" startWordPosition="3568" endWordPosition="3569">ses is similar: again, Mexico and Colombia have slightly lower results than the other classes. 4.4 Compression We eventually test the applicability of compression-based classification using the approach of Bush (2014). As mentioned earlier, the assumption behind compression-based strategies for text categorization is that different text categories have a different entropy. Classification is possible because the effectivity of compression algorithms depends on the entropy of the data to be compressed (less entropy ≈ more compression). A simple classification algorithm is LempelZiv-Welch (LZW) (Welch, 1984). It is based on a dictionary which maps sequences of symbols to unique indices. Compression is achieved by replacing sequences of input symbols with the respective dictionary indices. More precisely, compression works as follows. First, the dictionary is initialized with the inventory of symbols (i.e., with all possible 1-grams). Then, until the input is fully consumed, we repeat the following steps. We search the dictionary for the longest sequence of symbols s that matches the current input, we output the dictionary entry for s, remove s from the input and add s followed by the next input s</context>
</contexts>
<marker>Welch, 1984</marker>
<rawString>Terry A. Welch. 1984. A technique for highperformance data compression. Computer, 17(6):8– 19, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Binyam Gebrekidan Gebre</author>
<author>Sascha Diwersy</author>
</authors>
<title>N-gram language models and pos distribution for the identification of spanish varieties.</title>
<date>2013</date>
<booktitle>In Proceedings of TALN2013,</booktitle>
<pages>580--587</pages>
<contexts>
<context position="7128" citStr="Zampieri et al. (2013)" startWordPosition="1097" endWordPosition="1100"> range of off-the-shelf tools for Twitter language identification, and achieve their best results with a voting over three individual systems, one of them being langid.py (Lui and Baldwin, 2012). Carter et al. (2013) exploit particular characteristics of Twitter (such as user profile data and relations between Twitter users) to improve language identification on this genre. Bush (2014) successfully uses LZW compression for Twitter language identification. Within the field of natural language processing, the problem of language variant identification has only begun to be studied very recently. Zampieri et al. (2013) have addressed the task for Spanish newspaper texts, using character and word n-gram models as well as POS and morphological information. Very recently, the Discriminating between Similar Languages (DSL) Shared Task (Zampieri et al., 2014) proposed the problem of identifying between pairs of similar languages and language variants on sentences from newspaper corpora, one of the pairs being Peninsular vs. Argentine Spanish. However, all these approaches are tailored to the standard language found in news sources, very different from the colloquial, noisy language of tweets, which presents dist</context>
</contexts>
<marker>Zampieri, Gebre, Diwersy, 2013</marker>
<rawString>Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and pos distribution for the identification of spanish varieties. In Proceedings of TALN2013, pages 580– 587.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcos Zampieri</author>
<author>Liling Tan</author>
<author>Nikola Ljubesi´c</author>
<author>J¨org Tiedemann</author>
</authors>
<title>A report on the dsl shared task 2014.</title>
<date>2014</date>
<booktitle>In Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects,</booktitle>
<pages>58--67</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<marker>Zampieri, Tan, Ljubesi´c, Tiedemann, 2014</marker>
<rawString>Marcos Zampieri, Liling Tan, Nikola Ljube&amp;quot;si´c, and J¨org Tiedemann. 2014. A report on the dsl shared task 2014. In Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58–67, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
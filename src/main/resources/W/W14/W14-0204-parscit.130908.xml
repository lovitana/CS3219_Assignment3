<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016763">
<title confidence="0.948321">
Recipes for building voice search UIs for automotive
</title>
<author confidence="0.99289">
Martin Labsky, Ladislav Kunc, Tomas Macek, Jan Kleindienst, Jan Vystrcil
</author>
<affiliation confidence="0.918252">
IBM Prague Research and Development Lab
</affiliation>
<address confidence="0.654751666666667">
V Parku 2294/4, 148 00 Prague 4
Czech Republic
{martin.labsky, ladislav kunc1, tomas macek,
</address>
<email confidence="0.743777">
jankle, jan vystrcil}@cz.ibm.com
</email>
<sectionHeader confidence="0.961029" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999720058823529">
In this paper we describe a set of tech-
niques we found suitable for building
multi-modal search applications for au-
tomotive environments. As these ap-
plications often search across different
topical domains, such as maps, weather
or Wikipedia, we discuss the problem
of switching focus between different do-
mains. Also, we propose techniques use-
ful for minimizing the response time of the
search system in mobile environment. We
evaluate some of the proposed techniques
by means of usability tests with 10 novice
test subjects who drove a simulated lane
change test on a driving simulator. We re-
port results describing the induced driving
distraction and user acceptance.
</bodyText>
<sectionHeader confidence="0.995133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941214285714">
The task of designing mobile search user inter-
faces (UIs) that combine multiple application do-
mains (such as navigation, POI and web search)
is significantly harder than just placing all sin-
gle domain solutions adjacent to one another. We
propose and evaluate a set of UI techniques use-
ful for implementing such systems. The tech-
niques are exemplified using a prototype multi-
modal search assistant tailored for in-car use. The
prototype supports several application domains in-
cluding navigation and POI search, Wikipedia,
weather forecasts and car owner’s manual. Fi-
nally, we report usability evaluation results using
this prototype.
</bodyText>
<sectionHeader confidence="0.999102" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9942155">
Two examples of multi-modal search UIs for au-
tomotive are the Toyota Entune1 and the Honda
</bodyText>
<footnote confidence="0.65055">
1http://www.toyota.com/entune/
</footnote>
<bodyText confidence="0.998796461538462">
Link2. Both infotainment systems integrate a
set of dedicated mobile applications including
a browser, navigation, music services, stocks,
weather or traffic information. Both use a tablet or
a smartphone to run the mobile applications which
brings the advantage of faster upgrades of the in-
car infotainment suite. Home screens of these sys-
tems consist of a matrix of square tiles that corre-
spond to individual applications.
The answers presented to the user should only
contain highly relevant information, e.g. present-
ing only points of interest that are near the cur-
rent location. This is called conversational maxim
of relevance (Paul, 1975). Many other lessons
learned by evaluating in-car infotainment systems
are discussed in (Green, 2013).
In recent years, personal assistant systems like
Siri (Aron, 2011), Google Now! (Google, 2013)
and the Dragon Mobile Assistant (Nuance, 2013)
started to penetrate the automotive environment.
Most of these applications are being enhanced
with driving modes to enable safer usage while
driving. Dragon Mobile Assistant can detect
whether the user is in a moving car and auto-
matically switches to “Driver Mode” that relies
on speech recognition and text-to-speech feed-
back. Siri recently added spoken presentation
of incoming text messages and voice mail, and
it also allows to dictate responses. Besides the
speech-activated assistant functionality, Google
Now! tries to exploit various context variables
(e.g. location history, user’s calendar, search his-
tory). Context is used for pro-active reminders that
pop-up in the right time and place. Speech recog-
nition of Google Now! has an interesting feature
that tries to act upon incomplete/interim recogni-
tion results; sometimes the first answer is however
not the right one which is later detected and the
answer is replaced when results are refined.
</bodyText>
<footnote confidence="0.813437">
2http://owners.honda.com/hondalink/
nextgeneration
</footnote>
<page confidence="0.669337">
28
</page>
<note confidence="0.9715705">
Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 28–32,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.67094" genericHeader="method">
3 UI techniques to support search while
driving
</sectionHeader>
<bodyText confidence="0.99996">
Below we present selected techniques we found
useful while designing and testing prototype
search UIs for automotive.
</bodyText>
<subsectionHeader confidence="0.998309">
3.1 Nearly stateless VUI
</subsectionHeader>
<bodyText confidence="0.999994535714286">
While driving and interacting with an application
UI, it often happens that the driver must interrupt
interaction with the system due to a sudden in-
crease of cognitive load associated with the pri-
mary task of driving. The interaction is either
postponed or even abandoned. The UI activity
may later be resumed but often the driver will
not remember the context where s/he left off. In
heavily state-based systems such as those based
on hierarchical menus, reconstruction of applica-
tion context in the driver’s mind may be costly and
associated with multiple glances at the display.
In order to minimize the need for memorizing
or reconstructing the application context, we ad-
vocate UIs that are as stateless as possible from
the user’s point of view. In the context of spoken
input, this means the UI should be able to process
all voice input regardless of its state.
This is important so that the driver does not need
to recall the application state before s/he utters a
request. For instance, being able to ask “Where
can we get a pizza” only after changing screen to
“POI search” can be problematic as the driver (1)
needs to change screens, (2) needs to remember
what the current screen is, and (3) may need to
look at the display to check the screen state. All
of these issues may increase driver distraction (its
haptic, visual and mental components).
</bodyText>
<subsectionHeader confidence="0.999951">
3.2 Self-sufficient auditory channel
</subsectionHeader>
<bodyText confidence="0.999981125">
According to the subjective results of usability
tests described in Section 6 and according to ear-
lier work on automotive dictation (Macek et al.,
2013), many drivers were observed to rely primar-
ily on the audio-out channel to convey information
from the UI while driving and they also preferred
it to looking at a display. A similar observation
was made also for test drivers who listened to and
navigated news articles and short stories (Kunc et
al., 2014).
Two recommendations could be abstracted from
the above user tests. First, the UI should produce
verbose audio output that fully describes what
happens with the system (in cases when the driver
controls the UI while driving). This includes spo-
ken output as well as earcons indicating important
micro-states of the system such as “listening” or
“processing”. Second, the UI should enable the
user to easily replay what has been said by the
system, e.g. by pressing a button, to offset the se-
rial character of spoken output. These steps should
make it possible for selected applications to run in
a display-less mode while driving or at least mini-
mize the number of gazes at the display.
</bodyText>
<subsectionHeader confidence="0.996383">
3.3 Distinguish domain transition types
</subsectionHeader>
<bodyText confidence="0.999965846153846">
By observing users accessing functions of mul-
tiple applications through a common UI, we ob-
served several characteristic transition types.
Hierarchical. The user navigates a menu tree,
often guided by GUI hints.
Within domain. Users often perform multiple
interactions within one application, such as per-
forming several Wikipedia queries, refining them
and browsing the retrieved results.
Application switching. Aware of the namings
of the applications supported by the system, users
often switch explicitly to a chosen domain before
uttering a domain-specific command.
Direct task invocation. Especially in case of UIs
having a unifying persona like Siri (Aron, 2011),
users do not view the system as a set of appli-
cations and instead directly request app-specific
functions, regardless of their past interaction.
Subdialog. The user requests functionality out
of the current application domain. The corre-
sponding application is invoked to handle the re-
quest and then the focus returns automatically to
the original domain. Examples include taking a
note or checking the weather forecast while in the
middle of another task.
Undo. A combined “undo” or “go back” fea-
ture accessible globally at a key press proved use-
ful during our usability testing to negate any un-
wanted actions accidentally triggered.
Figure 1 shows samples for the above transi-
tion types using an example multi-domain search
assistant further described in Section 4. Similar
lists of transition types ware described previously,
e.g. (Milward et al., 2006). Based on observing
human interactions with our prototype system, we
built a simple probabilistic model to control the
likelihood of the system taking each of the above
transition types, and used it to rescore the results
of the ASR and NLU systems.
</bodyText>
<page confidence="0.811992">
29
</page>
<figureCaption confidence="0.9999706">
Figure 2: Sample incremental prompt graph. Seg-
ments are annotated with durations in round brack-
ets and min/max times before an unknown slot
value has to be spoken (ms).
Figure 1: Transitions in a multi-domain system.
</figureCaption>
<bodyText confidence="0.990281948717949">
3.4 Early and incremental feedback about
the application state
Mobile search UIs often depend both on local and
remote resources such as ASR and NLU services
and various data providers. In mobile environ-
ments, availability and response times of remote
services may vary significantly. Most mobile UIs
address this problem by responding with a beep
and displaying a “processing” sign until the fi-
nal answer is rendered. We describe a UI tech-
nique that combines redundant local and remote
resources (ASR and NLU) to quickly come up
with a partial meaningful response that addresses
the user’s request. Chances are that the first re-
sponse based on partial understanding is wrong
and the following prompt must correct it.
Figure 2 shows a template definition for a sys-
tem prompt that starts playing once the system is
confident enough about the user’s intent being a
weather forecast question. The system provides
forecasts for the current location by default but
can switch to other locations if specified by the
user. Supposing the system is equipped with real-
time ASR and NLU that quickly determine the
high-level intent of the user, such as “weather fore-
cast”, the initial part of the prompt can start play-
ing almost immediately after the user has stopped
speaking. While a prefix of this prompt is play-
ing, more advanced ASR and NLU models de-
liver a finer-grained and more precise interpreta-
tion of the input, including any slot-value pairs
like “location=London”. Once this final interpre-
tation is known, the playback can be directed via
the shortest path to the identified variable prompt
segments like &lt;location&gt;. Further, the selec-
tion of prompt prefix to be played can be guided
by a current estimate of service delays to mini-
mize chances of potential pauses before speaking
prompt segments whose values are not yet known.
</bodyText>
<sectionHeader confidence="0.962835" genericHeader="method">
4 Voice search assistant prototype
</sectionHeader>
<bodyText confidence="0.999942666666667">
In this section we briefly present a voice search in-
terface that was developed by incrementaly imple-
menting the four UI techniques presented above.
While interim versions of this system were only
evaluated subjectively, formal evaluation results
are presented for the final version in Section 6.
The voice search assistant covers six applica-
tion domains shown in Figure 3. Navigation ser-
vices include spoken route guidance together with
unified destination entry by voice (addresses and
POIs). Some POIs are accompanied by user re-
views that can be read out as part of POI details.
</bodyText>
<figureCaption confidence="0.988711">
Figure 3: Prototype home screen (apps as tiles).
</figureCaption>
<bodyText confidence="0.999959923076923">
Further, the user can search various knowledge
sources like Wikipedia, Wolfram Alpha and the
web. The retrieved results are pre-processed and
the first one is played back to the user with the
possibility of navigating the result list.
To simulate asynchronous events, the system
reads out Skype text messages. The driver can also
create location and time based reminders that pop
up during the journey.
Finally, the system supports full-text search
over the car owner’s manual. Relevant text pas-
sages are read out and displayed based on a prob-
lem description or question uttered by the driver.
</bodyText>
<page confidence="0.773589">
30
</page>
<sectionHeader confidence="0.713531" genericHeader="method">
5 Usability testing setup and procedure
</sectionHeader>
<bodyText confidence="0.999931269230769">
A low-fidelity driving simulator setup similar to
the one described in (Curin et al., 2011) was
used to conduct lane change tests using (Mattes,
2003). Tests were conducted with 10 novice sub-
jects and took approximately 1 hour and 20 min-
utes per participant. At the beginning and at the
end of the test, subjects filled in pre-test and post-
test questionnaires. Before the actual test, each
participant practised both driving and using the
prototype for up to 20 minutes. The evaluated
test consisted of four tasks: an initial undistracted
drive (used to adapt a custom LCT ideal path for
each participant), two distracted driving trips in
counter-balanced order, and a final undistracted
drive (used for evaluation). Each of the four drives
was performed at constant speed of 60km/h and
took about 3.5 minutes. During the distracted
driving tasks, the users were instructed verbally
to perform several search tasks using the proto-
type. During task 1, subjects had to set destina-
tion to “office”, then find a pharmacy along the
route, check the weather forecast and take a note
about the forecast conditions. Task 2 only dif-
fered slightly by having a different destination and
POI, and by the user searching Wikipedia instead
of asking about weather.
</bodyText>
<sectionHeader confidence="0.75081" genericHeader="method">
6 Usability testing results
</sectionHeader>
<bodyText confidence="0.9888125">
Objective distraction was measured using mean
deviation (MDev) and standard deviation
(SDLP) of the vehicle’s lateral position (Mattes,
2003). Two versions of both statistics were
obtained: overall (computed over the whole trip)
and using lane-keeping segments only. The graph
in Figure 4 shows averaged results for the final
undistracted drive and for the first and second
distracted driving tasks (reflecting the order of the
tasks, not their types). We observe that using the
search UI led to significant distraction during lane
change segments but not during lane keeping.
Also, the distraction results for the first trip show
higher variance which we attribute to the users
still adapting to the driving simulator and to
using the UI. The observed distraction levels are
comparable to our earlier results obtained for a
text dictation UI when used with a GUI display
(Curin et al., 2011).
Several observations came out of the subjec-
tive feedback collected using forms. The users re-
ported extensive use of the auditory channel (both
Figure 4: Driving distraction while using a multi-
modal search UI.
in and out) only with occasional glimpses at the
screen (we however observed that objectively they
looked at the display more often than they reported
subjectively). Users also missed some informa-
tion in the voice output channel such as audio indi-
cation of route calculation progress (which could
take several seconds). Reading any text from the
screen was found difficult, and users requested that
playback be improved; see related follow-up study
(Kunc et al., 2014). Interestingly, multiple partic-
ipants requested voice commands that would du-
plicate buttons like “next” and “previous”, even in
cases where speech would be less efficient. This
may show a tendency to stick with a single modal-
ity as described by (Suhm et al., 2001). Addi-
tionally, the users requested better synchronization
of navigation announcements like “take exit 4 in
200 metres” with the output of other applications.
The baseline behaviour utilized in the test was
that high-priority navigation prompts interrupted
the output of other applications. Navigation, POI
search, simple note-taking and constrained search
domains like weather and Wikipedia were found
most useful (in this order). Open web search
and browsing an original car owner’s manual were
considered too distracting to use while driving.
</bodyText>
<sectionHeader confidence="0.996742" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999825285714286">
We described several recipes for building spoken
search applications for automotive and exempli-
fied them on a prototype search UI. Early us-
ability testing results for the prototype were pre-
sented. Our future work focuses on improving the
introduced techniques and exploring alternative UI
paradigms (Macek et al., 2013).
</bodyText>
<sectionHeader confidence="0.932559" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.949607">
The presented work is part of an IBM and Nuance
joint research project.
</bodyText>
<figure confidence="0.997248">
Overa MDev MDev
Overa SDLP SDLP
Lane k
Lane k
1
0,9
0,8
0,7
0,6
0,5
0,4
0,3
0,2
0,1
0
</figure>
<page confidence="0.969123">
31
</page>
<sectionHeader confidence="0.993281" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999903644444445">
Jacob Aron. 2011. How innovative is apple’s new
voice assistant, siri? New Scientist, 212(2836):24.
J. Curin, M. Labsky, T. Macek, J. Kleindienst,
H. Young, A. Thyme-Gobbel, H. Quast, and
L. Koenig. 2011. Dictating and editing short texts
while driving: distraction and task completion. In
Proceedings of the 3rd International Conference on
Automotive User Interfaces and Interactive Vehicu-
lar Applications.
Google. 2013. Google now assistant. Available at
http://www.google.com/landing/now/.
Paul A Green. 2013. Development and evaluation
of automotive speech interfaces: useful information
from the human factors and the related literature. In-
ternational Journal of Vehicular Technology, 2013.
L. Kunc, M. Labsky, T. Macek, J. Vystrcil, J. Klein-
dienst, T. Kasparova, D. Luksch, and Z. Medenica.
2014. Long text reading in a car. In Proceedings
of the 16th International Conference on Human-
Computer Interaction Conference (HCII).
Tom´a&amp;quot;s Macek, Tereza Ka&amp;quot;sparov´a, Jan Kleindienst,
Ladislav Kunc, Martin Labsk´y, and Jan Vystr&amp;quot;cil.
2013. Mostly passive information delivery in a
car. In Proceedings of the 5th International Confer-
ence on Automotive User Interfaces and Interactive
Vehicular Applications, AutomotiveUI ’13, pages
250–253, New York, NY, USA. ACM.
Stefan Mattes. 2003. The lane-change-task as a tool
for driver distraction evaluation. In Proceedings of
the Annual Spring Conference of the GFA/ISOES,
volume 2003.
David Milward, Gabriel Amores, Nate Blaylock,
Staffan Larsson, Peter Ljunglof, Pilar Manchon, and
Guillermo Perez. 2006. D2.2: Dynamic multimodal
interface reconfiguration. In Talk and Look: Tools
for Ambient Linguistic Knowledge IST-507802 De-
liverable D2.2.
Nuance. 2013. Dragon mobile assistant. Available at
http://www.dragonmobileapps.com.
Grice H Paul. 1975. Logic and conversation. Syntax
and semantics, 3:41–58.
Bernhard Suhm, Brad Myers, and Alex Waibel. 2001.
Multimodal error correction for speech user inter-
faces. ACM Transactions on Computer-Human In-
teraction (TOCHI), 8(1):60–98.
</reference>
<page confidence="0.945624">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.200721">
<title confidence="0.997229">Recipes for building voice search UIs for automotive</title>
<author confidence="0.99975">Martin Labsky</author>
<author confidence="0.99975">Ladislav Kunc</author>
<author confidence="0.99975">Tomas Macek</author>
<author confidence="0.99975">Jan Kleindienst</author>
<author confidence="0.99975">Jan</author>
<affiliation confidence="0.7535945">IBM Prague Research and Development V Parku 2294/4, 148 00 Prague</affiliation>
<abstract confidence="0.926951095238095">Czech ladislav kunc1, tomas jan Abstract In this paper we describe a set of techniques we found suitable for building multi-modal search applications for automotive environments. As these applications often search across different topical domains, such as maps, weather or Wikipedia, we discuss the problem of switching focus between different domains. Also, we propose techniques useful for minimizing the response time of the search system in mobile environment. We evaluate some of the proposed techniques by means of usability tests with 10 novice test subjects who drove a simulated lane change test on a driving simulator. We report results describing the induced driving distraction and user acceptance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacob Aron</author>
</authors>
<title>How innovative is apple’s new voice assistant, siri?</title>
<date>2011</date>
<journal>New Scientist,</journal>
<volume>212</volume>
<issue>2836</issue>
<contexts>
<context position="2570" citStr="Aron, 2011" startWordPosition="392" endWordPosition="393">e to run the mobile applications which brings the advantage of faster upgrades of the incar infotainment suite. Home screens of these systems consist of a matrix of square tiles that correspond to individual applications. The answers presented to the user should only contain highly relevant information, e.g. presenting only points of interest that are near the current location. This is called conversational maxim of relevance (Paul, 1975). Many other lessons learned by evaluating in-car infotainment systems are discussed in (Green, 2013). In recent years, personal assistant systems like Siri (Aron, 2011), Google Now! (Google, 2013) and the Dragon Mobile Assistant (Nuance, 2013) started to penetrate the automotive environment. Most of these applications are being enhanced with driving modes to enable safer usage while driving. Dragon Mobile Assistant can detect whether the user is in a moving car and automatically switches to “Driver Mode” that relies on speech recognition and text-to-speech feedback. Siri recently added spoken presentation of incoming text messages and voice mail, and it also allows to dictate responses. Besides the speech-activated assistant functionality, Google Now! tries </context>
<context position="7269" citStr="Aron, 2011" startWordPosition="1142" endWordPosition="1143">cations through a common UI, we observed several characteristic transition types. Hierarchical. The user navigates a menu tree, often guided by GUI hints. Within domain. Users often perform multiple interactions within one application, such as performing several Wikipedia queries, refining them and browsing the retrieved results. Application switching. Aware of the namings of the applications supported by the system, users often switch explicitly to a chosen domain before uttering a domain-specific command. Direct task invocation. Especially in case of UIs having a unifying persona like Siri (Aron, 2011), users do not view the system as a set of applications and instead directly request app-specific functions, regardless of their past interaction. Subdialog. The user requests functionality out of the current application domain. The corresponding application is invoked to handle the request and then the focus returns automatically to the original domain. Examples include taking a note or checking the weather forecast while in the middle of another task. Undo. A combined “undo” or “go back” feature accessible globally at a key press proved useful during our usability testing to negate any unwan</context>
</contexts>
<marker>Aron, 2011</marker>
<rawString>Jacob Aron. 2011. How innovative is apple’s new voice assistant, siri? New Scientist, 212(2836):24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Curin</author>
<author>M Labsky</author>
<author>T Macek</author>
<author>J Kleindienst</author>
<author>H Young</author>
<author>A Thyme-Gobbel</author>
<author>H Quast</author>
<author>L Koenig</author>
</authors>
<title>Dictating and editing short texts while driving: distraction and task completion.</title>
<date>2011</date>
<booktitle>In Proceedings of the 3rd International Conference on Automotive User Interfaces and Interactive Vehicular Applications.</booktitle>
<contexts>
<context position="11819" citStr="Curin et al., 2011" startWordPosition="1885" endWordPosition="1888">ed results are pre-processed and the first one is played back to the user with the possibility of navigating the result list. To simulate asynchronous events, the system reads out Skype text messages. The driver can also create location and time based reminders that pop up during the journey. Finally, the system supports full-text search over the car owner’s manual. Relevant text passages are read out and displayed based on a problem description or question uttered by the driver. 30 5 Usability testing setup and procedure A low-fidelity driving simulator setup similar to the one described in (Curin et al., 2011) was used to conduct lane change tests using (Mattes, 2003). Tests were conducted with 10 novice subjects and took approximately 1 hour and 20 minutes per participant. At the beginning and at the end of the test, subjects filled in pre-test and posttest questionnaires. Before the actual test, each participant practised both driving and using the prototype for up to 20 minutes. The evaluated test consisted of four tasks: an initial undistracted drive (used to adapt a custom LCT ideal path for each participant), two distracted driving trips in counter-balanced order, and a final undistracted dri</context>
<context position="13898" citStr="Curin et al., 2011" startWordPosition="2223" endWordPosition="2226">aph in Figure 4 shows averaged results for the final undistracted drive and for the first and second distracted driving tasks (reflecting the order of the tasks, not their types). We observe that using the search UI led to significant distraction during lane change segments but not during lane keeping. Also, the distraction results for the first trip show higher variance which we attribute to the users still adapting to the driving simulator and to using the UI. The observed distraction levels are comparable to our earlier results obtained for a text dictation UI when used with a GUI display (Curin et al., 2011). Several observations came out of the subjective feedback collected using forms. The users reported extensive use of the auditory channel (both Figure 4: Driving distraction while using a multimodal search UI. in and out) only with occasional glimpses at the screen (we however observed that objectively they looked at the display more often than they reported subjectively). Users also missed some information in the voice output channel such as audio indication of route calculation progress (which could take several seconds). Reading any text from the screen was found difficult, and users reque</context>
</contexts>
<marker>Curin, Labsky, Macek, Kleindienst, Young, Thyme-Gobbel, Quast, Koenig, 2011</marker>
<rawString>J. Curin, M. Labsky, T. Macek, J. Kleindienst, H. Young, A. Thyme-Gobbel, H. Quast, and L. Koenig. 2011. Dictating and editing short texts while driving: distraction and task completion. In Proceedings of the 3rd International Conference on Automotive User Interfaces and Interactive Vehicular Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Google</author>
</authors>
<title>Google now assistant. Available at http://www.google.com/landing/now/.</title>
<date>2013</date>
<contexts>
<context position="2598" citStr="Google, 2013" startWordPosition="396" endWordPosition="397">ations which brings the advantage of faster upgrades of the incar infotainment suite. Home screens of these systems consist of a matrix of square tiles that correspond to individual applications. The answers presented to the user should only contain highly relevant information, e.g. presenting only points of interest that are near the current location. This is called conversational maxim of relevance (Paul, 1975). Many other lessons learned by evaluating in-car infotainment systems are discussed in (Green, 2013). In recent years, personal assistant systems like Siri (Aron, 2011), Google Now! (Google, 2013) and the Dragon Mobile Assistant (Nuance, 2013) started to penetrate the automotive environment. Most of these applications are being enhanced with driving modes to enable safer usage while driving. Dragon Mobile Assistant can detect whether the user is in a moving car and automatically switches to “Driver Mode” that relies on speech recognition and text-to-speech feedback. Siri recently added spoken presentation of incoming text messages and voice mail, and it also allows to dictate responses. Besides the speech-activated assistant functionality, Google Now! tries to exploit various context v</context>
</contexts>
<marker>Google, 2013</marker>
<rawString>Google. 2013. Google now assistant. Available at http://www.google.com/landing/now/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul A Green</author>
</authors>
<title>Development and evaluation of automotive speech interfaces: useful information from the human factors and the related literature.</title>
<date>2013</date>
<journal>International Journal of Vehicular Technology,</journal>
<contexts>
<context position="2502" citStr="Green, 2013" startWordPosition="382" endWordPosition="383">cks, weather or traffic information. Both use a tablet or a smartphone to run the mobile applications which brings the advantage of faster upgrades of the incar infotainment suite. Home screens of these systems consist of a matrix of square tiles that correspond to individual applications. The answers presented to the user should only contain highly relevant information, e.g. presenting only points of interest that are near the current location. This is called conversational maxim of relevance (Paul, 1975). Many other lessons learned by evaluating in-car infotainment systems are discussed in (Green, 2013). In recent years, personal assistant systems like Siri (Aron, 2011), Google Now! (Google, 2013) and the Dragon Mobile Assistant (Nuance, 2013) started to penetrate the automotive environment. Most of these applications are being enhanced with driving modes to enable safer usage while driving. Dragon Mobile Assistant can detect whether the user is in a moving car and automatically switches to “Driver Mode” that relies on speech recognition and text-to-speech feedback. Siri recently added spoken presentation of incoming text messages and voice mail, and it also allows to dictate responses. Besi</context>
</contexts>
<marker>Green, 2013</marker>
<rawString>Paul A Green. 2013. Development and evaluation of automotive speech interfaces: useful information from the human factors and the related literature. International Journal of Vehicular Technology, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kunc</author>
<author>M Labsky</author>
<author>T Macek</author>
<author>J Vystrcil</author>
<author>J Kleindienst</author>
<author>T Kasparova</author>
<author>D Luksch</author>
<author>Z Medenica</author>
</authors>
<title>Long text reading in a car.</title>
<date>2014</date>
<booktitle>In Proceedings of the 16th International Conference on HumanComputer Interaction Conference (HCII).</booktitle>
<contexts>
<context position="5875" citStr="Kunc et al., 2014" startWordPosition="921" endWordPosition="924">to check the screen state. All of these issues may increase driver distraction (its haptic, visual and mental components). 3.2 Self-sufficient auditory channel According to the subjective results of usability tests described in Section 6 and according to earlier work on automotive dictation (Macek et al., 2013), many drivers were observed to rely primarily on the audio-out channel to convey information from the UI while driving and they also preferred it to looking at a display. A similar observation was made also for test drivers who listened to and navigated news articles and short stories (Kunc et al., 2014). Two recommendations could be abstracted from the above user tests. First, the UI should produce verbose audio output that fully describes what happens with the system (in cases when the driver controls the UI while driving). This includes spoken output as well as earcons indicating important micro-states of the system such as “listening” or “processing”. Second, the UI should enable the user to easily replay what has been said by the system, e.g. by pressing a button, to offset the serial character of spoken output. These steps should make it possible for selected applications to run in a di</context>
<context position="14577" citStr="Kunc et al., 2014" startWordPosition="2332" endWordPosition="2335">llected using forms. The users reported extensive use of the auditory channel (both Figure 4: Driving distraction while using a multimodal search UI. in and out) only with occasional glimpses at the screen (we however observed that objectively they looked at the display more often than they reported subjectively). Users also missed some information in the voice output channel such as audio indication of route calculation progress (which could take several seconds). Reading any text from the screen was found difficult, and users requested that playback be improved; see related follow-up study (Kunc et al., 2014). Interestingly, multiple participants requested voice commands that would duplicate buttons like “next” and “previous”, even in cases where speech would be less efficient. This may show a tendency to stick with a single modality as described by (Suhm et al., 2001). Additionally, the users requested better synchronization of navigation announcements like “take exit 4 in 200 metres” with the output of other applications. The baseline behaviour utilized in the test was that high-priority navigation prompts interrupted the output of other applications. Navigation, POI search, simple note-taking a</context>
</contexts>
<marker>Kunc, Labsky, Macek, Vystrcil, Kleindienst, Kasparova, Luksch, Medenica, 2014</marker>
<rawString>L. Kunc, M. Labsky, T. Macek, J. Vystrcil, J. Kleindienst, T. Kasparova, D. Luksch, and Z. Medenica. 2014. Long text reading in a car. In Proceedings of the 16th International Conference on HumanComputer Interaction Conference (HCII).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom´as Macek</author>
<author>Tereza Kasparov´a</author>
<author>Jan Kleindienst</author>
<author>Ladislav Kunc</author>
<author>Martin Labsk´y</author>
<author>Jan Vystrcil</author>
</authors>
<title>Mostly passive information delivery in a car.</title>
<date>2013</date>
<booktitle>In Proceedings of the 5th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI ’13,</booktitle>
<pages>250--253</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Macek, Kasparov´a, Kleindienst, Kunc, Labsk´y, Vystrcil, 2013</marker>
<rawString>Tom´a&amp;quot;s Macek, Tereza Ka&amp;quot;sparov´a, Jan Kleindienst, Ladislav Kunc, Martin Labsk´y, and Jan Vystr&amp;quot;cil. 2013. Mostly passive information delivery in a car. In Proceedings of the 5th International Conference on Automotive User Interfaces and Interactive Vehicular Applications, AutomotiveUI ’13, pages 250–253, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Mattes</author>
</authors>
<title>The lane-change-task as a tool for driver distraction evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Annual Spring Conference of the GFA/ISOES,</booktitle>
<volume>volume</volume>
<contexts>
<context position="11878" citStr="Mattes, 2003" startWordPosition="1897" endWordPosition="1898">the user with the possibility of navigating the result list. To simulate asynchronous events, the system reads out Skype text messages. The driver can also create location and time based reminders that pop up during the journey. Finally, the system supports full-text search over the car owner’s manual. Relevant text passages are read out and displayed based on a problem description or question uttered by the driver. 30 5 Usability testing setup and procedure A low-fidelity driving simulator setup similar to the one described in (Curin et al., 2011) was used to conduct lane change tests using (Mattes, 2003). Tests were conducted with 10 novice subjects and took approximately 1 hour and 20 minutes per participant. At the beginning and at the end of the test, subjects filled in pre-test and posttest questionnaires. Before the actual test, each participant practised both driving and using the prototype for up to 20 minutes. The evaluated test consisted of four tasks: an initial undistracted drive (used to adapt a custom LCT ideal path for each participant), two distracted driving trips in counter-balanced order, and a final undistracted drive (used for evaluation). Each of the four drives was perfo</context>
<context position="13147" citStr="Mattes, 2003" startWordPosition="2102" endWordPosition="2103">tes. During the distracted driving tasks, the users were instructed verbally to perform several search tasks using the prototype. During task 1, subjects had to set destination to “office”, then find a pharmacy along the route, check the weather forecast and take a note about the forecast conditions. Task 2 only differed slightly by having a different destination and POI, and by the user searching Wikipedia instead of asking about weather. 6 Usability testing results Objective distraction was measured using mean deviation (MDev) and standard deviation (SDLP) of the vehicle’s lateral position (Mattes, 2003). Two versions of both statistics were obtained: overall (computed over the whole trip) and using lane-keeping segments only. The graph in Figure 4 shows averaged results for the final undistracted drive and for the first and second distracted driving tasks (reflecting the order of the tasks, not their types). We observe that using the search UI led to significant distraction during lane change segments but not during lane keeping. Also, the distraction results for the first trip show higher variance which we attribute to the users still adapting to the driving simulator and to using the UI. T</context>
</contexts>
<marker>Mattes, 2003</marker>
<rawString>Stefan Mattes. 2003. The lane-change-task as a tool for driver distraction evaluation. In Proceedings of the Annual Spring Conference of the GFA/ISOES, volume 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Milward</author>
<author>Gabriel Amores</author>
<author>Nate Blaylock</author>
<author>Staffan Larsson</author>
<author>Peter Ljunglof</author>
<author>Pilar Manchon</author>
<author>Guillermo Perez</author>
</authors>
<title>D2.2: Dynamic multimodal interface reconfiguration.</title>
<date>2006</date>
<booktitle>In Talk and Look: Tools for Ambient Linguistic Knowledge IST-507802 Deliverable D2.2.</booktitle>
<contexts>
<context position="8126" citStr="Milward et al., 2006" startWordPosition="1276" endWordPosition="1279">responding application is invoked to handle the request and then the focus returns automatically to the original domain. Examples include taking a note or checking the weather forecast while in the middle of another task. Undo. A combined “undo” or “go back” feature accessible globally at a key press proved useful during our usability testing to negate any unwanted actions accidentally triggered. Figure 1 shows samples for the above transition types using an example multi-domain search assistant further described in Section 4. Similar lists of transition types ware described previously, e.g. (Milward et al., 2006). Based on observing human interactions with our prototype system, we built a simple probabilistic model to control the likelihood of the system taking each of the above transition types, and used it to rescore the results of the ASR and NLU systems. 29 Figure 2: Sample incremental prompt graph. Segments are annotated with durations in round brackets and min/max times before an unknown slot value has to be spoken (ms). Figure 1: Transitions in a multi-domain system. 3.4 Early and incremental feedback about the application state Mobile search UIs often depend both on local and remote resources </context>
</contexts>
<marker>Milward, Amores, Blaylock, Larsson, Ljunglof, Manchon, Perez, 2006</marker>
<rawString>David Milward, Gabriel Amores, Nate Blaylock, Staffan Larsson, Peter Ljunglof, Pilar Manchon, and Guillermo Perez. 2006. D2.2: Dynamic multimodal interface reconfiguration. In Talk and Look: Tools for Ambient Linguistic Knowledge IST-507802 Deliverable D2.2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nuance</author>
</authors>
<title>Dragon mobile assistant. Available at http://www.dragonmobileapps.com.</title>
<date>2013</date>
<contexts>
<context position="2645" citStr="Nuance, 2013" startWordPosition="403" endWordPosition="404">rades of the incar infotainment suite. Home screens of these systems consist of a matrix of square tiles that correspond to individual applications. The answers presented to the user should only contain highly relevant information, e.g. presenting only points of interest that are near the current location. This is called conversational maxim of relevance (Paul, 1975). Many other lessons learned by evaluating in-car infotainment systems are discussed in (Green, 2013). In recent years, personal assistant systems like Siri (Aron, 2011), Google Now! (Google, 2013) and the Dragon Mobile Assistant (Nuance, 2013) started to penetrate the automotive environment. Most of these applications are being enhanced with driving modes to enable safer usage while driving. Dragon Mobile Assistant can detect whether the user is in a moving car and automatically switches to “Driver Mode” that relies on speech recognition and text-to-speech feedback. Siri recently added spoken presentation of incoming text messages and voice mail, and it also allows to dictate responses. Besides the speech-activated assistant functionality, Google Now! tries to exploit various context variables (e.g. location history, user’s calenda</context>
</contexts>
<marker>Nuance, 2013</marker>
<rawString>Nuance. 2013. Dragon mobile assistant. Available at http://www.dragonmobileapps.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grice H Paul</author>
</authors>
<title>Logic and conversation. Syntax and semantics,</title>
<date>1975</date>
<pages>3--41</pages>
<contexts>
<context position="2401" citStr="Paul, 1975" startWordPosition="368" endWordPosition="369">ntegrate a set of dedicated mobile applications including a browser, navigation, music services, stocks, weather or traffic information. Both use a tablet or a smartphone to run the mobile applications which brings the advantage of faster upgrades of the incar infotainment suite. Home screens of these systems consist of a matrix of square tiles that correspond to individual applications. The answers presented to the user should only contain highly relevant information, e.g. presenting only points of interest that are near the current location. This is called conversational maxim of relevance (Paul, 1975). Many other lessons learned by evaluating in-car infotainment systems are discussed in (Green, 2013). In recent years, personal assistant systems like Siri (Aron, 2011), Google Now! (Google, 2013) and the Dragon Mobile Assistant (Nuance, 2013) started to penetrate the automotive environment. Most of these applications are being enhanced with driving modes to enable safer usage while driving. Dragon Mobile Assistant can detect whether the user is in a moving car and automatically switches to “Driver Mode” that relies on speech recognition and text-to-speech feedback. Siri recently added spoken</context>
</contexts>
<marker>Paul, 1975</marker>
<rawString>Grice H Paul. 1975. Logic and conversation. Syntax and semantics, 3:41–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Suhm</author>
<author>Brad Myers</author>
<author>Alex Waibel</author>
</authors>
<title>Multimodal error correction for speech user interfaces.</title>
<date>2001</date>
<journal>ACM Transactions on Computer-Human Interaction (TOCHI),</journal>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="14842" citStr="Suhm et al., 2001" startWordPosition="2376" endWordPosition="2379">splay more often than they reported subjectively). Users also missed some information in the voice output channel such as audio indication of route calculation progress (which could take several seconds). Reading any text from the screen was found difficult, and users requested that playback be improved; see related follow-up study (Kunc et al., 2014). Interestingly, multiple participants requested voice commands that would duplicate buttons like “next” and “previous”, even in cases where speech would be less efficient. This may show a tendency to stick with a single modality as described by (Suhm et al., 2001). Additionally, the users requested better synchronization of navigation announcements like “take exit 4 in 200 metres” with the output of other applications. The baseline behaviour utilized in the test was that high-priority navigation prompts interrupted the output of other applications. Navigation, POI search, simple note-taking and constrained search domains like weather and Wikipedia were found most useful (in this order). Open web search and browsing an original car owner’s manual were considered too distracting to use while driving. 7 Conclusion We described several recipes for building</context>
</contexts>
<marker>Suhm, Myers, Waibel, 2001</marker>
<rawString>Bernhard Suhm, Brad Myers, and Alex Waibel. 2001. Multimodal error correction for speech user interfaces. ACM Transactions on Computer-Human Interaction (TOCHI), 8(1):60–98.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
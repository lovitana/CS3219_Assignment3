<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016623">
<title confidence="0.986093">
CimS – The CIS and IMS joint submission to WMT 2014
translating from English into German
</title>
<author confidence="0.858077">
Fabienne Cap✿, Marion Weller✿�, Anita Rammp, Alexander Fraser✿
</author>
<affiliation confidence="0.6898435">
✿ CIS, Ludwig-Maximilian University of Munich – (cap|fraser)@cis.uni-muenchen.de
A IMS, University of Stuttgart – (wellermn|ramm)@ims.uni-stuttgart.de
</affiliation>
<sectionHeader confidence="0.977711" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996736">
We present the CimS submissions to the
2014 Shared Task for the language pair
EN→DE. We address the major problems
that arise when translating into German:
complex nominal and verbal morphol-
ogy, productive compounding and flex-
ible word ordering. Our morphology-
aware translation systems handle word
formation issues on different levels of
morpho-syntactic modeling.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906142857143">
In our shared task submissions, we focus on the
English to German translation direction: we ad-
dress different levels of productivity of the Ger-
man language, i.e., nominal and verbal inflec-
tion and productive word formation, which lead
to data sparsity and thus confuse classical SMT
systems.
Our basic goal is to make the two languages
as morphosyntactically similar as possible. We
use a parser and a morphological analyser to re-
move linguistic features from German that are
not present in English and reorder the English
input to make it more similar to the German sen-
tence structure. Prior to training, all words are
lemmatised and compounds are split into single
words. This is not only beneficial for word align-
ment, but it also allows us to generalise over in-
flectional variants of the same lexemes and over
single words which could occur in one place as a
standalone word and in another place as part of
a compound. Translation happens in two steps:
first, we translate from English into split, lemma-
tised German and then, we perform compound
merging and generation of inflection as a post-
processing step. This way, we are able to cre-
ate German compounds and inflectional vari-
ants that have not been seen in the parallel train-
ing data.
In this paper, we investigate the performance of
well-established source-side reordering, nomi-
nal re-inflection and compound processing sys-
tems on an up-to-date shared task. In addition,
we present experimental results on a verbal in-
flection component and a syntax-based variant
including source-side reordering.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998074375">
Re-Inflection The two-step translation ap-
proach we use was described by e.g. Toutanova
et al. (2008) and Jeong et al. (2010), who use
a number of morphological and syntactic
features derived from both source and target
language. More recently, Fraser et al. (2012)
describe a similar approach for German using
different CRF-based feature prediction models,
one for each of the four grammatical features
to be predicted for German words in noun
phrases, namely number, gender, case and
definiteness. This approach also handles word-
formation issues such as portmanteau splitting
and compounding. Weller et al. (2013) added
subcategorization information in combination
with source-side syntactic features in order to
improve the prediction of case.
De Gispert and Mariño (2008) generate verbal
inflection for translation from English into Span-
ish. They use classifiers trained not only on tar-
get language but also on source language fea-
tures, which is even more crucial for the predic-
tion of verbs than it is for nominal inflection.
More recently, Williams and Koehn (2011)
translate directly into target language surface
forms. Agreement within NPs and PPs, and also
between subject and verb is considered during
the decoding process: they use string-to-tree
translation, where the target language (German)
morphology is expressed as a set of unification
constraints automatically learned from a mor-
phologically annotated German corpus.
</bodyText>
<page confidence="0.986698">
71
</page>
<note confidence="0.719775">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 71–78,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.995897">
Compound Processing Compound splitting
for SMT has been addressed by numerous dif-
ferent groups, for translation from German
to English, e.g. using corpus-based frequen-
cies (Koehn and Knight, 2003), using POS-
constraints (Stymne et al., 2008), a lattice-based
approach propagating the splitting decision to
the decoder (Dyer, 2009), a rule-based morpho-
logical analyser (Fritzinger and Fraser, 2010) or
unsupervised, language-independent segmen-
tation (Macherey et al., 2011).
Compound processing in the other translation
direction, however, has been much less investi-
gated. Popovi´c et al. (2006) describe a list-based
approach, in which words are only re-combined
if they have been seen as compounds in a huge
corpus. However this approach is limited to
the list’s coverage. The approach of Stymne
(2009) overcomes this coverage issue by mak-
ing use of a POS-markup which distinguishes
former compound modifiers from former heads
and thus allows for their adequate recombina-
tion after translation. An extension of this ap-
proach is reported in Stymne and Cancedda
(2011) where a CRF-model is used for compound
prediction. In Cap et al. (2014) their approach
is extended through using source-language fea-
tures and lemmatisation, allowing for maximal
generalisation over compound parts.
Source-side Reordering One major problem in
English to German translation is the divergent
clausal ordering: in particular, German verbs
tend to occur at the very end of clauses, whereas
English sticks to a rigid SVO order in most cases.
Collins et al. (2005), Fraser (2009) and Gojun
and Fraser (2012) showed that restructuring the
source language so that it corresponds to the ex-
pected structure of the target language is helpful
for SMT.
</bodyText>
<sectionHeader confidence="0.997901" genericHeader="method">
3 Inflection Prediction
</sectionHeader>
<bodyText confidence="0.999978833333333">
German has a rich morphology, both for nom-
inal and verbal inflection. It requires differ-
ent forms of agreement, e.g., for adjectives and
nouns or verbs and their subjects. Traditional
phrase-based SMT systems often get such agree-
ments wrong. In our systems, we explicitly
model agreement using a two-step approach:
first we translate from English into lemmatised
German and then generate fully inflected forms
in a second step. In this section, we describe our
nominal inflection component and first experi-
mental steps towards verbal re-inflection.
</bodyText>
<subsectionHeader confidence="0.999186">
3.1 Noun Phrase Inflection
</subsectionHeader>
<bodyText confidence="0.999977666666667">
Prior to training, the German data is re-
duced to a lemmatised representation contain-
ing translation-relevant morphological features.
For nominal inflection, the lemmas are marked
with number and gender: gender is considered
as part of the lemma, whereas number is indi-
rectly determined by the source-side, as we ex-
pect nouns to be translated with their appro-
priate number value. We use a linear chain
CRF (Lafferty et al., 2001) to predict the mor-
phological features (number, gender, case and
strong/weak). The features that are part of the
lemma of nouns (number, gender) are propa-
gated over the rest of the linguistic phrase. In
contrast, case depends on the role of the NP in
the sentence (e.g. subject or direct/indirect ob-
ject) and is thus to be determined entirely from
the respective context in the sentence. The value
for strong/weak depends on the combination of
the other features. Based on the lemma and the
predicted features, inflected forms are then gen-
erated using the rule-based morphological anal-
yser SMOR (Schmid et al., 2004). This system is
described in more detail in Fraser et al. (2012).
</bodyText>
<subsectionHeader confidence="0.997596">
3.2 Verbal Inflection
</subsectionHeader>
<bodyText confidence="0.999896083333333">
German verbs agree in number and person with
their subjects. We thus have to derive this in-
formation from a noun phrase in nominative
case (= the subject) near the verb. This informa-
tion comes from the nominal inflection predic-
tion described in section 3.1. We predict tense
and mode of the verb using a maximum-entropy
classifier which is trained on English and Ger-
man contextual information. After deriving all
information needed for the generation of the
verbs, the inflected forms are generated with
SMOR.
</bodyText>
<sectionHeader confidence="0.994825" genericHeader="method">
4 Compound Processing
</sectionHeader>
<bodyText confidence="0.999884625">
In English to German translation, compound
processing is more difficult than in the oppo-
site direction. Not only do compounds have to
be split accurately, but they also have to be put
together correctly after decoding. The disflu-
ency of MT output and the difficulty of deciding
which single words should be merged into com-
pounds make this task even more challenging.
</bodyText>
<page confidence="0.975852">
72
</page>
<figure confidence="0.99993343902439">
English text German text
fruit trade
Target Training Data
Parallel Training Data
tool box
....
....
....
Werkzeugkiste
German text
Obsthandel
....
....
....
....
....
Pre−Processing
Werkzeugkiste
....
Obsthandel
....
....
split
lemmatise
lemmatise
split
English text
fruit trade
Parallel Training Data
tool
Target Training Data
....
....
....
....
....
Obst Handel
....
Werkzeug Kiste
....
....
....
box
German text
(split+lem.)
(split+lem.
....
Obst Handel
....
Werkzeug Kiste
....
German text
(split+lem.)
Training
Translation
Model
Language
Model
German
output
(split+lem.)
....
Obst Handel
....
Obst Kiste
....
fruit trade
....
fruit box
Testing
Decoder
English
input
Recombine
Re−inflect
Post−processing
....
Obsthandel
....
Obstkiste
German
(fluent)
</figure>
<figureCaption confidence="0.999994">
Figure 1: Pipeline overview of our primary CimS-CoRI system.
</figureCaption>
<bodyText confidence="0.999998423076923">
We combine compound processing with in-
flection prediction (see Section 3) and thus ex-
tend the two-step approach respectively: com-
pounds are split and lemmatised simultane-
ously, again using SMOR. This allows for maxi-
mal generalisation over former compound parts
and independently occurring simple words. We
use this split representation for training. Af-
ter decoding, we re-combine words into com-
pounds again, using our extended CRF-based
approach, which is based on Stymne and Can-
cedda (2011), but includes source-language fea-
tures and allows for maximal generalisation
through lemmatisation. More details can be
found in Cap et al. (2014). We then use SMOR
to generate sound German compounds (includ-
ing morphological transformations such as in-
troduction or deletion of filler letters). Finally,
the whole text including the newly-created com-
pounds, is re-inflected using the nominal in-
flection prediction models as described in Sec-
tion 3.1 above. This procedure allows us to create
compounds that have not been seen in the par-
allel training data, and also inflectional variants
of seen compounds. See Figure 1 for an overview
of our compound processing pipeline.
</bodyText>
<subsectionHeader confidence="0.96046">
4.1 Portmanteaus
</subsectionHeader>
<bodyText confidence="0.999970333333333">
Portmanteaus are a special kind of compound.
They are a fusion of a preposition and a defi-
nite article (thus not productive) and their case
must agree with the case of the noun. For ex-
ample, “zum” can be split into “zu” + “dem” =
to+theDative. They introduce additional spar-
sity to the training data: imagine a noun oc-
curred with its definite article in the training
data, but not with the portemanteau required at
testing time. Splitting portemanteaus allows a
phrase-based SMT system to access phrases cov-
ering nouns and their corresponding definite ar-
ticles. In a post-processing step, definite articles
are then re-merged with their preceding prepo-
sitions to restore the original portmanteau, see
(Fraser et al., 2012) for details. This generalisa-
tion effect is even larger as we not only split port-
manteaus, but also lemmatise the articles.
</bodyText>
<sectionHeader confidence="0.989623" genericHeader="method">
5 System descriptions
</sectionHeader>
<bodyText confidence="0.999942466666667">
Our shared task submissions include different
combinations of the inflection and compound
processing procedures as described in the pre-
vious two sections. We give an overview of all
our systems in Table 1. Note that we did not
re-train the compound processing CRFs on the
new dataset, but used our models trained on the
2009 training data instead. However, this does
not hurt performance, as the CRF we use is not
trained on surface forms, but only frequencies
and source-side features instead. See (Fraser et
al., 2012) and (Cap et al., 2014) for more details
on how we trained the respective CRFs. In con-
trast, the verbal classifier has been trained on
WMT 2014 data.
</bodyText>
<sectionHeader confidence="0.998802" genericHeader="method">
6 Experimental Settings
</sectionHeader>
<bodyText confidence="0.999144714285714">
In all our systems, we only used data distributed
for the shared task. All available German data
was morphologically analysed with SMOR. For
lemmatisation of the German training data, we
disambiguated SMOR using POS tags we ob-
tained through parsing the German section of
the parallel training data with BitPar (Schmid,
</bodyText>
<page confidence="0.994969">
73
</page>
<table confidence="0.996975142857143">
No. apprart nominal compound verbal source-side
splitting inflection processing inflection reordering
CimS-RI X X
CimS-CoRIP X X X
CimS-RIVe X X X
CimS-CoRIVe X X X X
CimS-Syntax-RORI X X X
</table>
<tableCaption confidence="0.709696666666667">
Table 1: Overview of our submission systems.RI = nominal Re-Inflection, Co = Compound process-
ing, Ve = Verbal inflection, RO = source-side Re-Ordering. Syntax = syntax-based SMT P = primary
submission.
</tableCaption>
<bodyText confidence="0.999671722222222">
2004) and tagging the big monolingual training
data using RFTagger (Schmid and Laws, 2008)1.
Note that we did not normalise German lan-
guage e.g. with respect to old vs. new writing
convention etc. as we did in previous submis-
sions (e.g. (Fraser, 2009)).
For the compound prediction CRFs using syn-
tactic features derived from the source language,
we parsed the English section of the parallel
data using EGRET, a re-implementation of the
Berkeley-Parser by Hui Zhang2. Before training
our models on the English data, we normalised
all occurrences of British vs. American English
variants to British English. We did so for train-
ing, tuning and testing input.
Language Model We trained 5-gram language
models based on all available German monolin-
gual training data from the shared task (roughly
1.5 billion words) using the SRILM toolkit (Stol-
cke, 2002) with Kneser-Ney smoothing. We then
used KenLM (Heafield, 2011) for faster process-
ing. For each of our experiments, we trained
a separate language model on the whole data
set, corresponding to the different underspeci-
fied representations of German used in our ex-
periments, e.g. lemmatised for CimS-RI, lemma-
tised with split compounds for CimS-CoRI, etc.
Phrase-based Translation model We per-
formed word alignment using the multithreaded
GIZA++ toolkit (Och and Ney, 2003; Gao and
Vogel, 2008). For translation model training and
decoding, we used the Moses toolkit (Koehn
et al., 2007) to build phrase-based statistical
machine translation systems, following the
instructions for the baseline system for the
shared task, using only default settings.
</bodyText>
<footnote confidence="0.992571">
1We could not parse the whole monolingual dataset due
to time-constraints and thus used RFTagger as a substitute.
2available from https://sites.google.com/
site/zhangh1982/egret.
</footnote>
<bodyText confidence="0.99968125">
Syntax-based Translation model As a variant
to the phrase-based systems, we applied the in-
flection prediction system to a string-to-tree sys-
tem with GHKM extraction (Galley et al. (2004),
Williams and Koehn (2012)). We used the same
data-sets as for the phrase-based systems, and
applied BitPar (Schmid, 2004) to obtain target-
side trees. For this system, we used source-side
reordering according to Gojun and Fraser (2012)
relying on parses obtained with EGRET3.
Tuning For tuning of feature weights, we used
batch-mira with ’safe-hope’ (Cherry and Foster,
2012) until convergence (or maximal 25 runs).
We used the 3,000 sentences of newstest2012 for
tuning. Each experiment was tuned separately,
optimising Bleu scores (Papineni et al., 2002)
against a lemmatised version of the tuning ref-
erence. In the compound processing systems we
integrated the CRF-based prediction and merg-
ing procedure into each tuning iteration and
scored each output against the same unsplit and
lemmatised reference as the other systems.
Testing After decoding, the underspecified
representation has to be retransformed into
fluent German text, i.e., compounds need to
be re-combined and all words have to be re-
inflected. The whole procedure can be divided
into the following steps:
</bodyText>
<listItem confidence="0.959181444444445">
1a) translation into lemmatised German
representation (RI, RIVe)
1b) translation into split and lemmatised
German (CoRi, CoRIVe)
2) compound merging (CoRI, CoRIVe):
3) nominal inflection prediction and gen-
eration of full forms using SMOR (all)
4) verbal re-inflection (RIVe, CoRIVe)
5) merging of portmanteaus (all)
</listItem>
<footnote confidence="0.992289666666667">
3Note that we observed some data-related issues on the
Syntax-RORI experiments that we hope to resolve in the
near future.
</footnote>
<page confidence="0.996696">
74
</page>
<table confidence="0.998706">
Experiment mert.log Bleu ci Bleu cs Bleu ci Bleu cs
news2012 news2013 news2013 news2014 news2014
raw 16.52 18.62 17.61 17.80 17.25
CimS-RI 18.51 19.23 18.38 18.33 17.75
CimS-CoRIP 18.36 19.13 18.25 18.51 17.87
CimS-RIVe 19.08 18.89 18.06 17.86 17.31
CimS-CoRIVe 18.69 18.60 17.77 17.38 16.78
CimS-Syntax-RORI 18.26 19.04 18.17 18.15 17.59
</table>
<tableCaption confidence="0.888829">
Table 2: Bleu scores for all CimS-submissions of the 2014 shared task. ci = case-insensitive, cs = case-
sensitive; P = primary submission.
</tableCaption>
<bodyText confidence="0.999437333333333">
After these post-processing steps, the text was
automatically recapitalised and detokenised, us-
ing the tools provided by the shared task, which
we trained on the whole German dataset. We cal-
culated Bleu (Papineni et al., 2002) scores using
the NIST script version 13a.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999817178571428">
We evaluated our systems with the 3,000 sen-
tences of last year’s newstest2013 and also the
2,737 sentences of the 2014 blind test set for the
German-English language pair. The Bleu scores
of our systems are given in Table 2, where raw
denotes our baseline system which we ran with-
out any pre- or postprocessing whatsoever. Note
that the big gap in mert.log scores between raw
and the CimS-systems comes from the fact that
raw is scored against the original (i.e. fully in-
flected) version of the tuning reference, while the
CimS-systems are scored against the stemmed
tuning reference.
As for the Bleu scores of the test sets, we ob-
serve similar improvements for the CimS-RI and
CimS-CoRI systems of +0.5/0.6 with respect to
the raw baseline as we did in previous experi-
ments (Cap et al., 2014)4. In contrast, our sys-
tems incorporating verbal prediction inflection
(CimS-RIVe/CoRIVe) cannot yet catch up with
the performance of the well-investigated nom-
inal inflection and compound processing sys-
tems (CimS-RI/CoRI). We attribute this partly to
the positive influence we assume fully inflected
verbs to have in nominal inflection prediction
models, but as the verb processing systems are
still under development, there might be other is-
sues we have not discovered yet. We plan to re-
</bodyText>
<footnote confidence="0.96309">
4We will have a closer look at the data from a compound
processing view in Section 7.1 below.
</footnote>
<bodyText confidence="0.995061428571429">
visit these systems and improve them.
Finally, the syntax-based reordering system
yields scores that are competitive to those of
CimS-RI/CoRI. While Syntax-RORI so far onlyin-
corporates source-side reordering and nominal
re-inflection, we plan to investigate further ex-
tensions of this approach in the future.
</bodyText>
<subsectionHeader confidence="0.997216">
7.1 Additional Evaluation
</subsectionHeader>
<bodyText confidence="0.99667425">
We manually screened the filtered 2014 test set
and identified 3,456 German compound tokens,
whereof 862 did not occur in the parallel training
data and thereof, 244 did not even occur in the
monolingual training data. For each of our sys-
tems, we calculated the number of compound
reference matches they produced. The results
are given in Table 3.
</bodyText>
<table confidence="0.990844428571429">
system ref new
raw 827 0
CimS-RI. 864 5
CimS-CoRIP 1,064 109
CimS-RIVe 853 5
CimS-CoRIVe 1,070 122
CimS-Syntax-RORI 900 20
</table>
<tableCaption confidence="0.992688">
Table 3: Numbers of compounds produced by
</tableCaption>
<bodyText confidence="0.964263153846154">
the systems that matched the reference (ref) and
did not occur in the parallel training data (new).
The compound processing systems (with Co
in the name) generate many more correct com-
pounds than comparable systems without com-
pound handling. Compared to the raw base-
line, CoRI/CoRIVe did not only produce 237/243
more reference matches, but also 109/122 com-
pounds that matched the reference but did not
occur in the parallel training data. A lookup of
those 109/122 compounds in the monolingual
training data (consisting of roughly 1.5 billion
words) revealed, that 8/6 of them did not oc-
</bodyText>
<page confidence="0.99726">
75
</page>
<bodyText confidence="0.999990090909091">
cur there either5. These were thus not accessi-
ble to a list-based compound merging approach
either. This result also shows that despite the
fact that CoRIVe does not yield a competitive
translation quality performance yet, the com-
pound processing component seems to bene-
fit from the verbal inflection and it is definitely
worth more investigation in the future.
Moreover, it can be seen from Table 3 that
the re-inflection systems (*RI*) produce more
reference matches than the raw baseline. In-
terestingly, they even produce some reference
matches that have not been seen in the par-
allel training data due to inflectional variation,
and in the case of the syntax-based system due
to a naive list-based compound merging: even
though it has not been trained on a split repre-
sentation of German text, it might occasionally
occur that two German nouns occur next to each
other in the MT output. If so, these two words are
merged into a compound, using a list-based ap-
proach, similar to Popovi´c et al. (2006).
</bodyText>
<sectionHeader confidence="0.979008" genericHeader="evaluation">
8 Reordering
</sectionHeader>
<bodyText confidence="0.999973045454546">
For the system CimS-Syntax-RORI, English data
parsed with EGRET was reordered using scripts
written for parse trees produced by the con-
stituent parser (Charniak and Johnson, 2005),
using a model we trained on the standard Penn
Treebank sections. Unfortunately, the reorder-
ing scripts could not be straightforwardly ap-
plied to EGRET parses and require more signifi-
cant modifications than we first expected.
We thus decided to parse the Europarl data
(v7) with (Charniak and Johnson, 2005) instead
and run our reordering scripts on it (CimS-RO).
For evaluation purposes, we build a baseline sys-
tem raw’ which has been trained only on Eu-
roparl. Tuning and testing setup is the same as
for the systems described in Section 6 with the
difference that the weights have been tuned on
newstest2013. The evaluation results are shown
in Table 4. Similarly to previous results reported
in (Gojun and Fraser, 2012), the CimS-RO system
shows an improvement of 0.5 Bleu points when
compared to the raw’ baseline .
</bodyText>
<footnote confidence="0.467520285714286">
5Namely: Testflugzeugen (test airplanes), Medientri-
bunal (media tribunal), RBS-Mitarbeiter (RBS worker),
Schulmauersanierung (school wall renovation), Anti-
Terror-Organisationen (anti-terror organisations), and
Tabakimpfstoffe (tobacco-plant-created vaccines) in both
and in CoRI also Hand-gepäckgebühr (hand luggage fee)
and Haftungsstreitigkeiten (liability litigation).
</footnote>
<table confidence="0.9929145">
Experiment mert.log Bleu ci Bleu cs
news2013 news2014 news2014
raw’ 16.87 16.25 15.31
CimS-RO 17.76 16.81 15.81
</table>
<tableCaption confidence="0.989188">
Table 4: Evaluation of the reordering system
trained on Europarl v7.
</tableCaption>
<sectionHeader confidence="0.97936" genericHeader="conclusions">
9 Summary
</sectionHeader>
<bodyText confidence="0.999985166666666">
We presented the CimS systems, a set of
morphology-aware translation systems cus-
tomised for translation from English to German.
Each system operates on a different level of
morphological description, be it nominal inflec-
tion, verbal inflection, compound processing
or source-side reordering. Some of the systems
are well-established (RI, CoRI and RO), others
are still under developement (RIVe, CoRIVe and
Syntax-RORI). However, all of them, with the ex-
ception of CoRIVe, lead to improved translation
quality when evaluated against a contrastive
baseline without linguistic processing. In an
additional evaluation, we could show that the
compound processing systems are able to create
a considerable number of compounds unseen
in the parallel training data.
In the future, we will investigate further com-
binations and extensions of our morphological
components, including reordering, compound
processing and verbal inflection. There are still
many many interesting challenges to be solved
in all of these areas, and this is especially true for
verbal inflection.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999856266666667">
This work was supported by Deutsche For-
schungsgemeinschaft grants Models of Mor-
phosyntax for Statistical Machine Translation
(Phase 2) and Distributional Approaches to Se-
mantic Relatedness. We would like to thank
Daniel Quernheim for sharing the workload of
preprocessing the data with us.
Moreover, we thank Edgar Hoch from the IMS
system administration for generously providing
us with disk space and all our colleagues at IMS,
especially Fabienne Braune, Junfei Guo, Nina
Seemann and Jason Utt for postponing their ex-
periments to let us use most of IMS’ computing
facilities for a whole week. Thank you each beau-
coup!
</bodyText>
<page confidence="0.978861">
76
</page>
<sectionHeader confidence="0.9896" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999711980952381">
Fabienne Cap, Alexander Fraser, Marion Weller, and
Aoife Cahill. 2014. How to Produce Unseen
Teddy Bears: Improved Morphological Processing
of Compounds in SMT. In Proceedings of EACL
2014.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics
(ACL), Ann Arbor, Michigan.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation.
In Proceedings ofHLT-NAACL 2012.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerová.
2005. Clause restructuring for statistical machine
translation. In Proceedings ACL 2005.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for MT. In Proceed-
ings of HLT-NAACL 2009.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word
Formation in SMT. In Proceedings of EACL 2012.
Alexander Fraser. 2009. Experiments in Morphosyn-
tactic Processing for Translation to and from Ger-
man. In Proceedings of WMT 2009.
Fabienne Fritzinger and Alexander Fraser. 2010.
How to Avoid Burning Ducks: Combining Lin-
guistic Analysis and Corpus Statistics for Ger-
man Compound Processing. In Proceedings of
WMT@ACL2010.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a Translation Rule?
In Proceedings ofHLT-NAACL 2004.
Qin Gao and Stephan Vogel. 2008. Parallel imple-
mentations of word alignment tool. In ACL 2008:
Proceedings of the Workshop on Software Engineer-
ing, Testing, and Quality Assurance for Natural
Language Processing.
Adrià De Gispert and José B. Mariño. 2008. On the
impact of morphology in English to Spanish statis-
tical MT. Speech Communication.
Anita Gojun and Alexander Fraser. 2012. Determin-
ing the placement of German verbs in English-to-
German SMT. In Proceedings of EACL 2012.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of WMT
2011.
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki,
and Chris Quirk. 2010. A discriminative lexicon
model for complex morphology. In Proceedings of
AMTA 2010.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In Proceedings
of EACL 2003.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of ACL 2007
(Demo Session).
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In ICML’01.
Klaus Macherey, Andrew M. Dai, David Talbot,
Ashok C. Popat, and Franz Och. 2011. Language-
independent Compound Splitting with Morpho-
logical Operations. In Proceedings ofACL 2011.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51,.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
ofACL 2002.
Maja Popovi´c, Daniel Stein, and Hermann Ney. 2006.
Statistical Machine Translation of German Com-
pound Words. In Proceedings of FinTAL 2006.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained pos tagging. In Pro-
ceedings of COLING 2008.
Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004. SMOR: A German Computational Morphol-
ogy Covering Derivation, Composition and Inflec-
tion. In Proceedings ofLREC 2004.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proceedings of Coling2004.
Andreas Stolcke. 2002. SRILM – an Extensible Lan-
guage Modelling Toolkit. In Proceedings of ICSLN
2002.
Sara Stymne and Nicola Cancedda. 2011. Pro-
ductive Generation of Compound Words in Sta-
tistical Machine Translation. In Proceedings of
WMT@EMNLP’11.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2008. Effects of Morphological Analysis in Transla-
tion between German and English. In Proceedings
of WMT 2008.
Sara Stymne. 2009. A Comparison of Merging Strate-
gies for Translation of German Compounds. In
Proceedings of EACL 2009 (Student Workshop).
</reference>
<page confidence="0.979624">
77
</page>
<reference confidence="0.999369142857143">
Kristina Toutanova, Hisami Suzuki, and Achim
Ruopp. 2008. Applying Morphology Generation
Models to Machine Translation. In Proceedings of
HLT-ACL 2008.
Marion Weller, Alexander Fraser, and Sabine
Schulte im Walde. 2013. Using Subcatego-
rization Knowledge to Improve Case Prediction for
Translation to German. In Proceedings ofACL’13.
Philip Williams and Philipp Koehn. 2011. Agreement
constraints for statistical machine translation into
German. In Proceedings of WMT 2011.
Philip Williams and Phillipp Koehn. 2012. GHKM-
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of WMT 2007.
</reference>
<page confidence="0.99882">
78
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801057">
<title confidence="0.894131">CimS – The CIS and IMS joint submission to WMT translating from English into German</title>
<author confidence="0.998281">Marion Anita Alexander</author>
<affiliation confidence="0.997257">Ludwig-Maximilian University of Munich – University of Stuttgart –</affiliation>
<abstract confidence="0.999106454545455">We present the CimS submissions to the 2014 Shared Task for the language pair We address the major problems that arise when translating into German: complex nominal and verbal morphology, productive compounding and flexible word ordering. Our morphologyaware translation systems handle word formation issues on different levels of morpho-syntactic modeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fabienne Cap</author>
<author>Alexander Fraser</author>
<author>Marion Weller</author>
<author>Aoife Cahill</author>
</authors>
<title>How to Produce Unseen Teddy Bears: Improved Morphological Processing of Compounds in SMT.</title>
<date>2014</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="5014" citStr="Cap et al. (2014)" startWordPosition="767" endWordPosition="770">ection, however, has been much less investigated. Popovi´c et al. (2006) describe a list-based approach, in which words are only re-combined if they have been seen as compounds in a huge corpus. However this approach is limited to the list’s coverage. The approach of Stymne (2009) overcomes this coverage issue by making use of a POS-markup which distinguishes former compound modifiers from former heads and thus allows for their adequate recombination after translation. An extension of this approach is reported in Stymne and Cancedda (2011) where a CRF-model is used for compound prediction. In Cap et al. (2014) their approach is extended through using source-language features and lemmatisation, allowing for maximal generalisation over compound parts. Source-side Reordering One major problem in English to German translation is the divergent clausal ordering: in particular, German verbs tend to occur at the very end of clauses, whereas English sticks to a rigid SVO order in most cases. Collins et al. (2005), Fraser (2009) and Gojun and Fraser (2012) showed that restructuring the source language so that it corresponds to the expected structure of the target language is helpful for SMT. 3 Inflection Pre</context>
<context position="9688" citStr="Cap et al. (2014)" startWordPosition="1504" endWordPosition="1507">rocessing with inflection prediction (see Section 3) and thus extend the two-step approach respectively: compounds are split and lemmatised simultaneously, again using SMOR. This allows for maximal generalisation over former compound parts and independently occurring simple words. We use this split representation for training. After decoding, we re-combine words into compounds again, using our extended CRF-based approach, which is based on Stymne and Cancedda (2011), but includes source-language features and allows for maximal generalisation through lemmatisation. More details can be found in Cap et al. (2014). We then use SMOR to generate sound German compounds (including morphological transformations such as introduction or deletion of filler letters). Finally, the whole text including the newly-created compounds, is re-inflected using the nominal inflection prediction models as described in Section 3.1 above. This procedure allows us to create compounds that have not been seen in the parallel training data, and also inflectional variants of seen compounds. See Figure 1 for an overview of our compound processing pipeline. 4.1 Portmanteaus Portmanteaus are a special kind of compound. They are a fu</context>
<context position="11641" citStr="Cap et al., 2014" startWordPosition="1825" endWordPosition="1828">plit portmanteaus, but also lemmatise the articles. 5 System descriptions Our shared task submissions include different combinations of the inflection and compound processing procedures as described in the previous two sections. We give an overview of all our systems in Table 1. Note that we did not re-train the compound processing CRFs on the new dataset, but used our models trained on the 2009 training data instead. However, this does not hurt performance, as the CRF we use is not trained on surface forms, but only frequencies and source-side features instead. See (Fraser et al., 2012) and (Cap et al., 2014) for more details on how we trained the respective CRFs. In contrast, the verbal classifier has been trained on WMT 2014 data. 6 Experimental Settings In all our systems, we only used data distributed for the shared task. All available German data was morphologically analysed with SMOR. For lemmatisation of the German training data, we disambiguated SMOR using POS tags we obtained through parsing the German section of the parallel training data with BitPar (Schmid, 73 No. apprart nominal compound verbal source-side splitting inflection processing inflection reordering CimS-RI X X CimS-CoRIP X </context>
<context position="17538" citStr="Cap et al., 2014" startWordPosition="2755" endWordPosition="2758">Bleu scores of our systems are given in Table 2, where raw denotes our baseline system which we ran without any pre- or postprocessing whatsoever. Note that the big gap in mert.log scores between raw and the CimS-systems comes from the fact that raw is scored against the original (i.e. fully inflected) version of the tuning reference, while the CimS-systems are scored against the stemmed tuning reference. As for the Bleu scores of the test sets, we observe similar improvements for the CimS-RI and CimS-CoRI systems of +0.5/0.6 with respect to the raw baseline as we did in previous experiments (Cap et al., 2014)4. In contrast, our systems incorporating verbal prediction inflection (CimS-RIVe/CoRIVe) cannot yet catch up with the performance of the well-investigated nominal inflection and compound processing systems (CimS-RI/CoRI). We attribute this partly to the positive influence we assume fully inflected verbs to have in nominal inflection prediction models, but as the verb processing systems are still under development, there might be other issues we have not discovered yet. We plan to re4We will have a closer look at the data from a compound processing view in Section 7.1 below. visit these system</context>
</contexts>
<marker>Cap, Fraser, Weller, Cahill, 2014</marker>
<rawString>Fabienne Cap, Alexander Fraser, Marion Weller, and Aoife Cahill. 2014. How to Produce Unseen Teddy Bears: Improved Morphological Processing of Compounds in SMT. In Proceedings of EACL 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL),</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="20757" citStr="Charniak and Johnson, 2005" startWordPosition="3278" endWordPosition="3281"> in the parallel training data due to inflectional variation, and in the case of the syntax-based system due to a naive list-based compound merging: even though it has not been trained on a split representation of German text, it might occasionally occur that two German nouns occur next to each other in the MT output. If so, these two words are merged into a compound, using a list-based approach, similar to Popovi´c et al. (2006). 8 Reordering For the system CimS-Syntax-RORI, English data parsed with EGRET was reordered using scripts written for parse trees produced by the constituent parser (Charniak and Johnson, 2005), using a model we trained on the standard Penn Treebank sections. Unfortunately, the reordering scripts could not be straightforwardly applied to EGRET parses and require more significant modifications than we first expected. We thus decided to parse the Europarl data (v7) with (Charniak and Johnson, 2005) instead and run our reordering scripts on it (CimS-RO). For evaluation purposes, we build a baseline system raw’ which has been trained only on Europarl. Tuning and testing setup is the same as for the systems described in Section 6 with the difference that the weights have been tuned on ne</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL), Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<contexts>
<context position="14848" citStr="Cherry and Foster, 2012" startWordPosition="2325" endWordPosition="2328">ilable from https://sites.google.com/ site/zhangh1982/egret. Syntax-based Translation model As a variant to the phrase-based systems, we applied the inflection prediction system to a string-to-tree system with GHKM extraction (Galley et al. (2004), Williams and Koehn (2012)). We used the same data-sets as for the phrase-based systems, and applied BitPar (Schmid, 2004) to obtain targetside trees. For this system, we used source-side reordering according to Gojun and Fraser (2012) relying on parses obtained with EGRET3. Tuning For tuning of feature weights, we used batch-mira with ’safe-hope’ (Cherry and Foster, 2012) until convergence (or maximal 25 runs). We used the 3,000 sentences of newstest2012 for tuning. Each experiment was tuned separately, optimising Bleu scores (Papineni et al., 2002) against a lemmatised version of the tuning reference. In the compound processing systems we integrated the CRF-based prediction and merging procedure into each tuning iteration and scored each output against the same unsplit and lemmatised reference as the other systems. Testing After decoding, the underspecified representation has to be retransformed into fluent German text, i.e., compounds need to be re-combined </context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings ofHLT-NAACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerová</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ACL</booktitle>
<marker>Collins, Koehn, Kuˇcerová, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerová. 2005. Clause restructuring for statistical machine translation. In Proceedings ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
</authors>
<title>Using a maximum entropy model to build segmentation lattices for MT.</title>
<date>2009</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<contexts>
<context position="4205" citStr="Dyer, 2009" startWordPosition="643" endWordPosition="644"> a set of unification constraints automatically learned from a morphologically annotated German corpus. 71 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 71–78, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Compound Processing Compound splitting for SMT has been addressed by numerous different groups, for translation from German to English, e.g. using corpus-based frequencies (Koehn and Knight, 2003), using POSconstraints (Stymne et al., 2008), a lattice-based approach propagating the splitting decision to the decoder (Dyer, 2009), a rule-based morphological analyser (Fritzinger and Fraser, 2010) or unsupervised, language-independent segmentation (Macherey et al., 2011). Compound processing in the other translation direction, however, has been much less investigated. Popovi´c et al. (2006) describe a list-based approach, in which words are only re-combined if they have been seen as compounds in a huge corpus. However this approach is limited to the list’s coverage. The approach of Stymne (2009) overcomes this coverage issue by making use of a POS-markup which distinguishes former compound modifiers from former heads an</context>
</contexts>
<marker>Dyer, 2009</marker>
<rawString>Chris Dyer. 2009. Using a maximum entropy model to build segmentation lattices for MT. In Proceedings of HLT-NAACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Marion Weller</author>
<author>Aoife Cahill</author>
<author>Fabienne Cap</author>
</authors>
<title>Modeling Inflection and Word Formation in SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="2527" citStr="Fraser et al. (2012)" startWordPosition="395" endWordPosition="398">the parallel training data. In this paper, we investigate the performance of well-established source-side reordering, nominal re-inflection and compound processing systems on an up-to-date shared task. In addition, we present experimental results on a verbal inflection component and a syntax-based variant including source-side reordering. 2 Related Work Re-Inflection The two-step translation approach we use was described by e.g. Toutanova et al. (2008) and Jeong et al. (2010), who use a number of morphological and syntactic features derived from both source and target language. More recently, Fraser et al. (2012) describe a similar approach for German using different CRF-based feature prediction models, one for each of the four grammatical features to be predicted for German words in noun phrases, namely number, gender, case and definiteness. This approach also handles wordformation issues such as portmanteau splitting and compounding. Weller et al. (2013) added subcategorization information in combination with source-side syntactic features in order to improve the prediction of case. De Gispert and Mariño (2008) generate verbal inflection for translation from English into Spanish. They use classifier</context>
<context position="7307" citStr="Fraser et al. (2012)" startWordPosition="1135" endWordPosition="1138">and strong/weak). The features that are part of the lemma of nouns (number, gender) are propagated over the rest of the linguistic phrase. In contrast, case depends on the role of the NP in the sentence (e.g. subject or direct/indirect object) and is thus to be determined entirely from the respective context in the sentence. The value for strong/weak depends on the combination of the other features. Based on the lemma and the predicted features, inflected forms are then generated using the rule-based morphological analyser SMOR (Schmid et al., 2004). This system is described in more detail in Fraser et al. (2012). 3.2 Verbal Inflection German verbs agree in number and person with their subjects. We thus have to derive this information from a noun phrase in nominative case (= the subject) near the verb. This information comes from the nominal inflection prediction described in section 3.1. We predict tense and mode of the verb using a maximum-entropy classifier which is trained on English and German contextual information. After deriving all information needed for the generation of the verbs, the inflected forms are generated with SMOR. 4 Compound Processing In English to German translation, compound p</context>
<context position="10952" citStr="Fraser et al., 2012" startWordPosition="1709" endWordPosition="1712"> (thus not productive) and their case must agree with the case of the noun. For example, “zum” can be split into “zu” + “dem” = to+theDative. They introduce additional sparsity to the training data: imagine a noun occurred with its definite article in the training data, but not with the portemanteau required at testing time. Splitting portemanteaus allows a phrase-based SMT system to access phrases covering nouns and their corresponding definite articles. In a post-processing step, definite articles are then re-merged with their preceding prepositions to restore the original portmanteau, see (Fraser et al., 2012) for details. This generalisation effect is even larger as we not only split portmanteaus, but also lemmatise the articles. 5 System descriptions Our shared task submissions include different combinations of the inflection and compound processing procedures as described in the previous two sections. We give an overview of all our systems in Table 1. Note that we did not re-train the compound processing CRFs on the new dataset, but used our models trained on the 2009 training data instead. However, this does not hurt performance, as the CRF we use is not trained on surface forms, but only frequ</context>
</contexts>
<marker>Fraser, Weller, Cahill, Cap, 2012</marker>
<rawString>Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap. 2012. Modeling Inflection and Word Formation in SMT. In Proceedings of EACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
</authors>
<title>Experiments in Morphosyntactic Processing for Translation to and from German.</title>
<date>2009</date>
<booktitle>In Proceedings of WMT</booktitle>
<contexts>
<context position="5431" citStr="Fraser (2009)" startWordPosition="832" endWordPosition="833"> for their adequate recombination after translation. An extension of this approach is reported in Stymne and Cancedda (2011) where a CRF-model is used for compound prediction. In Cap et al. (2014) their approach is extended through using source-language features and lemmatisation, allowing for maximal generalisation over compound parts. Source-side Reordering One major problem in English to German translation is the divergent clausal ordering: in particular, German verbs tend to occur at the very end of clauses, whereas English sticks to a rigid SVO order in most cases. Collins et al. (2005), Fraser (2009) and Gojun and Fraser (2012) showed that restructuring the source language so that it corresponds to the expected structure of the target language is helpful for SMT. 3 Inflection Prediction German has a rich morphology, both for nominal and verbal inflection. It requires different forms of agreement, e.g., for adjectives and nouns or verbs and their subjects. Traditional phrase-based SMT systems often get such agreements wrong. In our systems, we explicitly model agreement using a two-step approach: first we translate from English into lemmatised German and then generate fully inflected forms</context>
<context position="12757" citStr="Fraser, 2009" startWordPosition="2010" endWordPosition="2011">erbal source-side splitting inflection processing inflection reordering CimS-RI X X CimS-CoRIP X X X CimS-RIVe X X X CimS-CoRIVe X X X X CimS-Syntax-RORI X X X Table 1: Overview of our submission systems.RI = nominal Re-Inflection, Co = Compound processing, Ve = Verbal inflection, RO = source-side Re-Ordering. Syntax = syntax-based SMT P = primary submission. 2004) and tagging the big monolingual training data using RFTagger (Schmid and Laws, 2008)1. Note that we did not normalise German language e.g. with respect to old vs. new writing convention etc. as we did in previous submissions (e.g. (Fraser, 2009)). For the compound prediction CRFs using syntactic features derived from the source language, we parsed the English section of the parallel data using EGRET, a re-implementation of the Berkeley-Parser by Hui Zhang2. Before training our models on the English data, we normalised all occurrences of British vs. American English variants to British English. We did so for training, tuning and testing input. Language Model We trained 5-gram language models based on all available German monolingual training data from the shared task (roughly 1.5 billion words) using the SRILM toolkit (Stolcke, 2002) </context>
</contexts>
<marker>Fraser, 2009</marker>
<rawString>Alexander Fraser. 2009. Experiments in Morphosyntactic Processing for Translation to and from German. In Proceedings of WMT 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Fritzinger</author>
<author>Alexander Fraser</author>
</authors>
<title>How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing.</title>
<date>2010</date>
<booktitle>In Proceedings of WMT@ACL2010.</booktitle>
<contexts>
<context position="4272" citStr="Fritzinger and Fraser, 2010" startWordPosition="650" endWordPosition="653">arned from a morphologically annotated German corpus. 71 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 71–78, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Compound Processing Compound splitting for SMT has been addressed by numerous different groups, for translation from German to English, e.g. using corpus-based frequencies (Koehn and Knight, 2003), using POSconstraints (Stymne et al., 2008), a lattice-based approach propagating the splitting decision to the decoder (Dyer, 2009), a rule-based morphological analyser (Fritzinger and Fraser, 2010) or unsupervised, language-independent segmentation (Macherey et al., 2011). Compound processing in the other translation direction, however, has been much less investigated. Popovi´c et al. (2006) describe a list-based approach, in which words are only re-combined if they have been seen as compounds in a huge corpus. However this approach is limited to the list’s coverage. The approach of Stymne (2009) overcomes this coverage issue by making use of a POS-markup which distinguishes former compound modifiers from former heads and thus allows for their adequate recombination after translation. A</context>
</contexts>
<marker>Fritzinger, Fraser, 2010</marker>
<rawString>Fabienne Fritzinger and Alexander Fraser. 2010. How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing. In Proceedings of WMT@ACL2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a Translation Rule?</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL</booktitle>
<contexts>
<context position="14471" citStr="Galley et al. (2004)" startWordPosition="2267" endWordPosition="2270">or translation model training and decoding, we used the Moses toolkit (Koehn et al., 2007) to build phrase-based statistical machine translation systems, following the instructions for the baseline system for the shared task, using only default settings. 1We could not parse the whole monolingual dataset due to time-constraints and thus used RFTagger as a substitute. 2available from https://sites.google.com/ site/zhangh1982/egret. Syntax-based Translation model As a variant to the phrase-based systems, we applied the inflection prediction system to a string-to-tree system with GHKM extraction (Galley et al. (2004), Williams and Koehn (2012)). We used the same data-sets as for the phrase-based systems, and applied BitPar (Schmid, 2004) to obtain targetside trees. For this system, we used source-side reordering according to Gojun and Fraser (2012) relying on parses obtained with EGRET3. Tuning For tuning of feature weights, we used batch-mira with ’safe-hope’ (Cherry and Foster, 2012) until convergence (or maximal 25 runs). We used the 3,000 sentences of newstest2012 for tuning. Each experiment was tuned separately, optimising Bleu scores (Papineni et al., 2002) against a lemmatised version of the tunin</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a Translation Rule? In Proceedings ofHLT-NAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel implementations of word alignment tool.</title>
<date>2008</date>
<booktitle>In ACL 2008: Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing.</booktitle>
<contexts>
<context position="13848" citStr="Gao and Vogel, 2008" startWordPosition="2179" endWordPosition="2182">ilable German monolingual training data from the shared task (roughly 1.5 billion words) using the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We then used KenLM (Heafield, 2011) for faster processing. For each of our experiments, we trained a separate language model on the whole data set, corresponding to the different underspecified representations of German used in our experiments, e.g. lemmatised for CimS-RI, lemmatised with split compounds for CimS-CoRI, etc. Phrase-based Translation model We performed word alignment using the multithreaded GIZA++ toolkit (Och and Ney, 2003; Gao and Vogel, 2008). For translation model training and decoding, we used the Moses toolkit (Koehn et al., 2007) to build phrase-based statistical machine translation systems, following the instructions for the baseline system for the shared task, using only default settings. 1We could not parse the whole monolingual dataset due to time-constraints and thus used RFTagger as a substitute. 2available from https://sites.google.com/ site/zhangh1982/egret. Syntax-based Translation model As a variant to the phrase-based systems, we applied the inflection prediction system to a string-to-tree system with GHKM extractio</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In ACL 2008: Proceedings of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adrià De Gispert</author>
<author>José B Mariño</author>
</authors>
<title>On the impact of morphology in English to Spanish statistical MT. Speech Communication.</title>
<date>2008</date>
<marker>De Gispert, Mariño, 2008</marker>
<rawString>Adrià De Gispert and José B. Mariño. 2008. On the impact of morphology in English to Spanish statistical MT. Speech Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anita Gojun</author>
<author>Alexander Fraser</author>
</authors>
<title>Determining the placement of German verbs in English-toGerman SMT.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="5459" citStr="Gojun and Fraser (2012)" startWordPosition="835" endWordPosition="838">e recombination after translation. An extension of this approach is reported in Stymne and Cancedda (2011) where a CRF-model is used for compound prediction. In Cap et al. (2014) their approach is extended through using source-language features and lemmatisation, allowing for maximal generalisation over compound parts. Source-side Reordering One major problem in English to German translation is the divergent clausal ordering: in particular, German verbs tend to occur at the very end of clauses, whereas English sticks to a rigid SVO order in most cases. Collins et al. (2005), Fraser (2009) and Gojun and Fraser (2012) showed that restructuring the source language so that it corresponds to the expected structure of the target language is helpful for SMT. 3 Inflection Prediction German has a rich morphology, both for nominal and verbal inflection. It requires different forms of agreement, e.g., for adjectives and nouns or verbs and their subjects. Traditional phrase-based SMT systems often get such agreements wrong. In our systems, we explicitly model agreement using a two-step approach: first we translate from English into lemmatised German and then generate fully inflected forms in a second step. In this s</context>
<context position="14707" citStr="Gojun and Fraser (2012)" startWordPosition="2304" endWordPosition="2307">nly default settings. 1We could not parse the whole monolingual dataset due to time-constraints and thus used RFTagger as a substitute. 2available from https://sites.google.com/ site/zhangh1982/egret. Syntax-based Translation model As a variant to the phrase-based systems, we applied the inflection prediction system to a string-to-tree system with GHKM extraction (Galley et al. (2004), Williams and Koehn (2012)). We used the same data-sets as for the phrase-based systems, and applied BitPar (Schmid, 2004) to obtain targetside trees. For this system, we used source-side reordering according to Gojun and Fraser (2012) relying on parses obtained with EGRET3. Tuning For tuning of feature weights, we used batch-mira with ’safe-hope’ (Cherry and Foster, 2012) until convergence (or maximal 25 runs). We used the 3,000 sentences of newstest2012 for tuning. Each experiment was tuned separately, optimising Bleu scores (Papineni et al., 2002) against a lemmatised version of the tuning reference. In the compound processing systems we integrated the CRF-based prediction and merging procedure into each tuning iteration and scored each output against the same unsplit and lemmatised reference as the other systems. Testi</context>
<context position="21480" citStr="Gojun and Fraser, 2012" startWordPosition="3398" endWordPosition="3401">ipts could not be straightforwardly applied to EGRET parses and require more significant modifications than we first expected. We thus decided to parse the Europarl data (v7) with (Charniak and Johnson, 2005) instead and run our reordering scripts on it (CimS-RO). For evaluation purposes, we build a baseline system raw’ which has been trained only on Europarl. Tuning and testing setup is the same as for the systems described in Section 6 with the difference that the weights have been tuned on newstest2013. The evaluation results are shown in Table 4. Similarly to previous results reported in (Gojun and Fraser, 2012), the CimS-RO system shows an improvement of 0.5 Bleu points when compared to the raw’ baseline . 5Namely: Testflugzeugen (test airplanes), Medientribunal (media tribunal), RBS-Mitarbeiter (RBS worker), Schulmauersanierung (school wall renovation), AntiTerror-Organisationen (anti-terror organisations), and Tabakimpfstoffe (tobacco-plant-created vaccines) in both and in CoRI also Hand-gepäckgebühr (hand luggage fee) and Haftungsstreitigkeiten (liability litigation). Experiment mert.log Bleu ci Bleu cs news2013 news2014 news2014 raw’ 16.87 16.25 15.31 CimS-RO 17.76 16.81 15.81 Table 4: Evaluatio</context>
</contexts>
<marker>Gojun, Fraser, 2012</marker>
<rawString>Anita Gojun and Alexander Fraser. 2012. Determining the placement of German verbs in English-toGerman SMT. In Proceedings of EACL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and Smaller Language Model Queries.</title>
<date>2011</date>
<booktitle>In Proceedings of WMT</booktitle>
<contexts>
<context position="13419" citStr="Heafield, 2011" startWordPosition="2114" endWordPosition="2115">ctic features derived from the source language, we parsed the English section of the parallel data using EGRET, a re-implementation of the Berkeley-Parser by Hui Zhang2. Before training our models on the English data, we normalised all occurrences of British vs. American English variants to British English. We did so for training, tuning and testing input. Language Model We trained 5-gram language models based on all available German monolingual training data from the shared task (roughly 1.5 billion words) using the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We then used KenLM (Heafield, 2011) for faster processing. For each of our experiments, we trained a separate language model on the whole data set, corresponding to the different underspecified representations of German used in our experiments, e.g. lemmatised for CimS-RI, lemmatised with split compounds for CimS-CoRI, etc. Phrase-based Translation model We performed word alignment using the multithreaded GIZA++ toolkit (Och and Ney, 2003; Gao and Vogel, 2008). For translation model training and decoding, we used the Moses toolkit (Koehn et al., 2007) to build phrase-based statistical machine translation systems, following the </context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of WMT 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
<author>Chris Quirk</author>
</authors>
<title>A discriminative lexicon model for complex morphology.</title>
<date>2010</date>
<booktitle>In Proceedings of AMTA</booktitle>
<contexts>
<context position="2387" citStr="Jeong et al. (2010)" startWordPosition="373" endWordPosition="376">inflection as a postprocessing step. This way, we are able to create German compounds and inflectional variants that have not been seen in the parallel training data. In this paper, we investigate the performance of well-established source-side reordering, nominal re-inflection and compound processing systems on an up-to-date shared task. In addition, we present experimental results on a verbal inflection component and a syntax-based variant including source-side reordering. 2 Related Work Re-Inflection The two-step translation approach we use was described by e.g. Toutanova et al. (2008) and Jeong et al. (2010), who use a number of morphological and syntactic features derived from both source and target language. More recently, Fraser et al. (2012) describe a similar approach for German using different CRF-based feature prediction models, one for each of the four grammatical features to be predicted for German words in noun phrases, namely number, gender, case and definiteness. This approach also handles wordformation issues such as portmanteau splitting and compounding. Weller et al. (2013) added subcategorization information in combination with source-side syntactic features in order to improve th</context>
</contexts>
<marker>Jeong, Toutanova, Suzuki, Quirk, 2010</marker>
<rawString>Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and Chris Quirk. 2010. A discriminative lexicon model for complex morphology. In Proceedings of AMTA 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Empirical Methods for Compound Splitting.</title>
<date>2003</date>
<booktitle>In Proceedings of EACL</booktitle>
<contexts>
<context position="4072" citStr="Koehn and Knight, 2003" startWordPosition="622" endWordPosition="625">erb is considered during the decoding process: they use string-to-tree translation, where the target language (German) morphology is expressed as a set of unification constraints automatically learned from a morphologically annotated German corpus. 71 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 71–78, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Compound Processing Compound splitting for SMT has been addressed by numerous different groups, for translation from German to English, e.g. using corpus-based frequencies (Koehn and Knight, 2003), using POSconstraints (Stymne et al., 2008), a lattice-based approach propagating the splitting decision to the decoder (Dyer, 2009), a rule-based morphological analyser (Fritzinger and Fraser, 2010) or unsupervised, language-independent segmentation (Macherey et al., 2011). Compound processing in the other translation direction, however, has been much less investigated. Popovi´c et al. (2006) describe a list-based approach, in which words are only re-combined if they have been seen as compounds in a huge corpus. However this approach is limited to the list’s coverage. The approach of Stymne </context>
</contexts>
<marker>Koehn, Knight, 2003</marker>
<rawString>Philipp Koehn and Kevin Knight. 2003. Empirical Methods for Compound Splitting. In Proceedings of EACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="13941" citStr="Koehn et al., 2007" startWordPosition="2194" endWordPosition="2197"> the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We then used KenLM (Heafield, 2011) for faster processing. For each of our experiments, we trained a separate language model on the whole data set, corresponding to the different underspecified representations of German used in our experiments, e.g. lemmatised for CimS-RI, lemmatised with split compounds for CimS-CoRI, etc. Phrase-based Translation model We performed word alignment using the multithreaded GIZA++ toolkit (Och and Ney, 2003; Gao and Vogel, 2008). For translation model training and decoding, we used the Moses toolkit (Koehn et al., 2007) to build phrase-based statistical machine translation systems, following the instructions for the baseline system for the shared task, using only default settings. 1We could not parse the whole monolingual dataset due to time-constraints and thus used RFTagger as a substitute. 2available from https://sites.google.com/ site/zhangh1982/egret. Syntax-based Translation model As a variant to the phrase-based systems, we applied the inflection prediction system to a string-to-tree system with GHKM extraction (Galley et al. (2004), Williams and Koehn (2012)). We used the same data-sets as for the ph</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of ACL 2007 (Demo Session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In ICML’01.</booktitle>
<contexts>
<context position="6626" citStr="Lafferty et al., 2001" startWordPosition="1020" endWordPosition="1023">erate fully inflected forms in a second step. In this section, we describe our nominal inflection component and first experimental steps towards verbal re-inflection. 3.1 Noun Phrase Inflection Prior to training, the German data is reduced to a lemmatised representation containing translation-relevant morphological features. For nominal inflection, the lemmas are marked with number and gender: gender is considered as part of the lemma, whereas number is indirectly determined by the source-side, as we expect nouns to be translated with their appropriate number value. We use a linear chain CRF (Lafferty et al., 2001) to predict the morphological features (number, gender, case and strong/weak). The features that are part of the lemma of nouns (number, gender) are propagated over the rest of the linguistic phrase. In contrast, case depends on the role of the NP in the sentence (e.g. subject or direct/indirect object) and is thus to be determined entirely from the respective context in the sentence. The value for strong/weak depends on the combination of the other features. Based on the lemma and the predicted features, inflected forms are then generated using the rule-based morphological analyser SMOR (Schm</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In ICML’01.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Macherey</author>
<author>Andrew M Dai</author>
<author>David Talbot</author>
<author>Ashok C Popat</author>
<author>Franz Och</author>
</authors>
<title>Languageindependent Compound Splitting with Morphological Operations.</title>
<date>2011</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="4347" citStr="Macherey et al., 2011" startWordPosition="659" endWordPosition="662"> Workshop on Statistical Machine Translation, pages 71–78, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Compound Processing Compound splitting for SMT has been addressed by numerous different groups, for translation from German to English, e.g. using corpus-based frequencies (Koehn and Knight, 2003), using POSconstraints (Stymne et al., 2008), a lattice-based approach propagating the splitting decision to the decoder (Dyer, 2009), a rule-based morphological analyser (Fritzinger and Fraser, 2010) or unsupervised, language-independent segmentation (Macherey et al., 2011). Compound processing in the other translation direction, however, has been much less investigated. Popovi´c et al. (2006) describe a list-based approach, in which words are only re-combined if they have been seen as compounds in a huge corpus. However this approach is limited to the list’s coverage. The approach of Stymne (2009) overcomes this coverage issue by making use of a POS-markup which distinguishes former compound modifiers from former heads and thus allows for their adequate recombination after translation. An extension of this approach is reported in Stymne and Cancedda (2011) wher</context>
</contexts>
<marker>Macherey, Dai, Talbot, Popat, Och, 2011</marker>
<rawString>Klaus Macherey, Andrew M. Dai, David Talbot, Ashok C. Popat, and Franz Och. 2011. Languageindependent Compound Splitting with Morphological Operations. In Proceedings ofACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="13826" citStr="Och and Ney, 2003" startWordPosition="2175" endWordPosition="2178">ls based on all available German monolingual training data from the shared task (roughly 1.5 billion words) using the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We then used KenLM (Heafield, 2011) for faster processing. For each of our experiments, we trained a separate language model on the whole data set, corresponding to the different underspecified representations of German used in our experiments, e.g. lemmatised for CimS-RI, lemmatised with split compounds for CimS-CoRI, etc. Phrase-based Translation model We performed word alignment using the multithreaded GIZA++ toolkit (Och and Ney, 2003; Gao and Vogel, 2008). For translation model training and decoding, we used the Moses toolkit (Koehn et al., 2007) to build phrase-based statistical machine translation systems, following the instructions for the baseline system for the shared task, using only default settings. 1We could not parse the whole monolingual dataset due to time-constraints and thus used RFTagger as a substitute. 2available from https://sites.google.com/ site/zhangh1982/egret. Syntax-based Translation model As a variant to the phrase-based systems, we applied the inflection prediction system to a string-to-tree syst</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51,.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL</booktitle>
<contexts>
<context position="15029" citStr="Papineni et al., 2002" startWordPosition="2352" endWordPosition="2355">string-to-tree system with GHKM extraction (Galley et al. (2004), Williams and Koehn (2012)). We used the same data-sets as for the phrase-based systems, and applied BitPar (Schmid, 2004) to obtain targetside trees. For this system, we used source-side reordering according to Gojun and Fraser (2012) relying on parses obtained with EGRET3. Tuning For tuning of feature weights, we used batch-mira with ’safe-hope’ (Cherry and Foster, 2012) until convergence (or maximal 25 runs). We used the 3,000 sentences of newstest2012 for tuning. Each experiment was tuned separately, optimising Bleu scores (Papineni et al., 2002) against a lemmatised version of the tuning reference. In the compound processing systems we integrated the CRF-based prediction and merging procedure into each tuning iteration and scored each output against the same unsplit and lemmatised reference as the other systems. Testing After decoding, the underspecified representation has to be retransformed into fluent German text, i.e., compounds need to be re-combined and all words have to be reinflected. The whole procedure can be divided into the following steps: 1a) translation into lemmatised German representation (RI, RIVe) 1b) translation i</context>
<context position="16692" citStr="Papineni et al., 2002" startWordPosition="2608" endWordPosition="2611">ws2014 news2014 raw 16.52 18.62 17.61 17.80 17.25 CimS-RI 18.51 19.23 18.38 18.33 17.75 CimS-CoRIP 18.36 19.13 18.25 18.51 17.87 CimS-RIVe 19.08 18.89 18.06 17.86 17.31 CimS-CoRIVe 18.69 18.60 17.77 17.38 16.78 CimS-Syntax-RORI 18.26 19.04 18.17 18.15 17.59 Table 2: Bleu scores for all CimS-submissions of the 2014 shared task. ci = case-insensitive, cs = casesensitive; P = primary submission. After these post-processing steps, the text was automatically recapitalised and detokenised, using the tools provided by the shared task, which we trained on the whole German dataset. We calculated Bleu (Papineni et al., 2002) scores using the NIST script version 13a. 7 Results We evaluated our systems with the 3,000 sentences of last year’s newstest2013 and also the 2,737 sentences of the 2014 blind test set for the German-English language pair. The Bleu scores of our systems are given in Table 2, where raw denotes our baseline system which we ran without any pre- or postprocessing whatsoever. Note that the big gap in mert.log scores between raw and the CimS-systems comes from the fact that raw is scored against the original (i.e. fully inflected) version of the tuning reference, while the CimS-systems are scored </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: A Method for Automatic Evaluation of Machine Translation. In Proceedings ofACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovi´c</author>
<author>Daniel Stein</author>
<author>Hermann Ney</author>
</authors>
<title>Statistical Machine Translation of German Compound Words.</title>
<date>2006</date>
<booktitle>In Proceedings of FinTAL</booktitle>
<marker>Popovi´c, Stein, Ney, 2006</marker>
<rawString>Maja Popovi´c, Daniel Stein, and Hermann Ney. 2006. Statistical Machine Translation of German Compound Words. In Proceedings of FinTAL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Florian Laws</author>
</authors>
<title>Estimation of conditional probabilities with decision trees and an application to fine-grained pos tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="12596" citStr="Schmid and Laws, 2008" startWordPosition="1979" endWordPosition="1982">, we disambiguated SMOR using POS tags we obtained through parsing the German section of the parallel training data with BitPar (Schmid, 73 No. apprart nominal compound verbal source-side splitting inflection processing inflection reordering CimS-RI X X CimS-CoRIP X X X CimS-RIVe X X X CimS-CoRIVe X X X X CimS-Syntax-RORI X X X Table 1: Overview of our submission systems.RI = nominal Re-Inflection, Co = Compound processing, Ve = Verbal inflection, RO = source-side Re-Ordering. Syntax = syntax-based SMT P = primary submission. 2004) and tagging the big monolingual training data using RFTagger (Schmid and Laws, 2008)1. Note that we did not normalise German language e.g. with respect to old vs. new writing convention etc. as we did in previous submissions (e.g. (Fraser, 2009)). For the compound prediction CRFs using syntactic features derived from the source language, we parsed the English section of the parallel data using EGRET, a re-implementation of the Berkeley-Parser by Hui Zhang2. Before training our models on the English data, we normalised all occurrences of British vs. American English variants to British English. We did so for training, tuning and testing input. Language Model We trained 5-gram </context>
</contexts>
<marker>Schmid, Laws, 2008</marker>
<rawString>Helmut Schmid and Florian Laws. 2008. Estimation of conditional probabilities with decision trees and an application to fine-grained pos tagging. In Proceedings of COLING 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
<author>Arne Fitschen</author>
<author>Ulrich Heid</author>
</authors>
<title>SMOR: A German Computational Morphology Covering Derivation, Composition and Inflection.</title>
<date>2004</date>
<booktitle>In Proceedings ofLREC</booktitle>
<contexts>
<context position="7242" citStr="Schmid et al., 2004" startWordPosition="1123" endWordPosition="1126">001) to predict the morphological features (number, gender, case and strong/weak). The features that are part of the lemma of nouns (number, gender) are propagated over the rest of the linguistic phrase. In contrast, case depends on the role of the NP in the sentence (e.g. subject or direct/indirect object) and is thus to be determined entirely from the respective context in the sentence. The value for strong/weak depends on the combination of the other features. Based on the lemma and the predicted features, inflected forms are then generated using the rule-based morphological analyser SMOR (Schmid et al., 2004). This system is described in more detail in Fraser et al. (2012). 3.2 Verbal Inflection German verbs agree in number and person with their subjects. We thus have to derive this information from a noun phrase in nominative case (= the subject) near the verb. This information comes from the nominal inflection prediction described in section 3.1. We predict tense and mode of the verb using a maximum-entropy classifier which is trained on English and German contextual information. After deriving all information needed for the generation of the verbs, the inflected forms are generated with SMOR. 4</context>
</contexts>
<marker>Schmid, Fitschen, Heid, 2004</marker>
<rawString>Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004. SMOR: A German Computational Morphology Covering Derivation, Composition and Inflection. In Proceedings ofLREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling2004.</booktitle>
<contexts>
<context position="14594" citStr="Schmid, 2004" startWordPosition="2288" endWordPosition="2289">ne translation systems, following the instructions for the baseline system for the shared task, using only default settings. 1We could not parse the whole monolingual dataset due to time-constraints and thus used RFTagger as a substitute. 2available from https://sites.google.com/ site/zhangh1982/egret. Syntax-based Translation model As a variant to the phrase-based systems, we applied the inflection prediction system to a string-to-tree system with GHKM extraction (Galley et al. (2004), Williams and Koehn (2012)). We used the same data-sets as for the phrase-based systems, and applied BitPar (Schmid, 2004) to obtain targetside trees. For this system, we used source-side reordering according to Gojun and Fraser (2012) relying on parses obtained with EGRET3. Tuning For tuning of feature weights, we used batch-mira with ’safe-hope’ (Cherry and Foster, 2012) until convergence (or maximal 25 runs). We used the 3,000 sentences of newstest2012 for tuning. Each experiment was tuned separately, optimising Bleu scores (Papineni et al., 2002) against a lemmatised version of the tuning reference. In the compound processing systems we integrated the CRF-based prediction and merging procedure into each tuni</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors. In Proceedings of Coling2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – an Extensible Language Modelling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of ICSLN</booktitle>
<contexts>
<context position="13356" citStr="Stolcke, 2002" startWordPosition="2104" endWordPosition="2106"> (Fraser, 2009)). For the compound prediction CRFs using syntactic features derived from the source language, we parsed the English section of the parallel data using EGRET, a re-implementation of the Berkeley-Parser by Hui Zhang2. Before training our models on the English data, we normalised all occurrences of British vs. American English variants to British English. We did so for training, tuning and testing input. Language Model We trained 5-gram language models based on all available German monolingual training data from the shared task (roughly 1.5 billion words) using the SRILM toolkit (Stolcke, 2002) with Kneser-Ney smoothing. We then used KenLM (Heafield, 2011) for faster processing. For each of our experiments, we trained a separate language model on the whole data set, corresponding to the different underspecified representations of German used in our experiments, e.g. lemmatised for CimS-RI, lemmatised with split compounds for CimS-CoRI, etc. Phrase-based Translation model We performed word alignment using the multithreaded GIZA++ toolkit (Och and Ney, 2003; Gao and Vogel, 2008). For translation model training and decoding, we used the Moses toolkit (Koehn et al., 2007) to build phras</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – an Extensible Language Modelling Toolkit. In Proceedings of ICSLN 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Nicola Cancedda</author>
</authors>
<date>2011</date>
<booktitle>Productive Generation of Compound Words in Statistical Machine Translation. In Proceedings of WMT@EMNLP’11.</booktitle>
<contexts>
<context position="4942" citStr="Stymne and Cancedda (2011)" startWordPosition="754" endWordPosition="757">ntation (Macherey et al., 2011). Compound processing in the other translation direction, however, has been much less investigated. Popovi´c et al. (2006) describe a list-based approach, in which words are only re-combined if they have been seen as compounds in a huge corpus. However this approach is limited to the list’s coverage. The approach of Stymne (2009) overcomes this coverage issue by making use of a POS-markup which distinguishes former compound modifiers from former heads and thus allows for their adequate recombination after translation. An extension of this approach is reported in Stymne and Cancedda (2011) where a CRF-model is used for compound prediction. In Cap et al. (2014) their approach is extended through using source-language features and lemmatisation, allowing for maximal generalisation over compound parts. Source-side Reordering One major problem in English to German translation is the divergent clausal ordering: in particular, German verbs tend to occur at the very end of clauses, whereas English sticks to a rigid SVO order in most cases. Collins et al. (2005), Fraser (2009) and Gojun and Fraser (2012) showed that restructuring the source language so that it corresponds to the expect</context>
<context position="9541" citStr="Stymne and Cancedda (2011)" startWordPosition="1481" endWordPosition="1485">Re−inflect Post−processing .... Obsthandel .... Obstkiste German (fluent) Figure 1: Pipeline overview of our primary CimS-CoRI system. We combine compound processing with inflection prediction (see Section 3) and thus extend the two-step approach respectively: compounds are split and lemmatised simultaneously, again using SMOR. This allows for maximal generalisation over former compound parts and independently occurring simple words. We use this split representation for training. After decoding, we re-combine words into compounds again, using our extended CRF-based approach, which is based on Stymne and Cancedda (2011), but includes source-language features and allows for maximal generalisation through lemmatisation. More details can be found in Cap et al. (2014). We then use SMOR to generate sound German compounds (including morphological transformations such as introduction or deletion of filler letters). Finally, the whole text including the newly-created compounds, is re-inflected using the nominal inflection prediction models as described in Section 3.1 above. This procedure allows us to create compounds that have not been seen in the parallel training data, and also inflectional variants of seen compo</context>
</contexts>
<marker>Stymne, Cancedda, 2011</marker>
<rawString>Sara Stymne and Nicola Cancedda. 2011. Productive Generation of Compound Words in Statistical Machine Translation. In Proceedings of WMT@EMNLP’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
<author>Maria Holmqvist</author>
<author>Lars Ahrenberg</author>
</authors>
<title>Effects of Morphological Analysis in Translation between German and English.</title>
<date>2008</date>
<booktitle>In Proceedings of WMT</booktitle>
<contexts>
<context position="4116" citStr="Stymne et al., 2008" startWordPosition="629" endWordPosition="632">they use string-to-tree translation, where the target language (German) morphology is expressed as a set of unification constraints automatically learned from a morphologically annotated German corpus. 71 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 71–78, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Compound Processing Compound splitting for SMT has been addressed by numerous different groups, for translation from German to English, e.g. using corpus-based frequencies (Koehn and Knight, 2003), using POSconstraints (Stymne et al., 2008), a lattice-based approach propagating the splitting decision to the decoder (Dyer, 2009), a rule-based morphological analyser (Fritzinger and Fraser, 2010) or unsupervised, language-independent segmentation (Macherey et al., 2011). Compound processing in the other translation direction, however, has been much less investigated. Popovi´c et al. (2006) describe a list-based approach, in which words are only re-combined if they have been seen as compounds in a huge corpus. However this approach is limited to the list’s coverage. The approach of Stymne (2009) overcomes this coverage issue by maki</context>
</contexts>
<marker>Stymne, Holmqvist, Ahrenberg, 2008</marker>
<rawString>Sara Stymne, Maria Holmqvist, and Lars Ahrenberg. 2008. Effects of Morphological Analysis in Translation between German and English. In Proceedings of WMT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Stymne</author>
</authors>
<title>A Comparison of Merging Strategies for Translation of German Compounds.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL</booktitle>
<note>(Student Workshop).</note>
<contexts>
<context position="4678" citStr="Stymne (2009)" startWordPosition="714" endWordPosition="715">, 2003), using POSconstraints (Stymne et al., 2008), a lattice-based approach propagating the splitting decision to the decoder (Dyer, 2009), a rule-based morphological analyser (Fritzinger and Fraser, 2010) or unsupervised, language-independent segmentation (Macherey et al., 2011). Compound processing in the other translation direction, however, has been much less investigated. Popovi´c et al. (2006) describe a list-based approach, in which words are only re-combined if they have been seen as compounds in a huge corpus. However this approach is limited to the list’s coverage. The approach of Stymne (2009) overcomes this coverage issue by making use of a POS-markup which distinguishes former compound modifiers from former heads and thus allows for their adequate recombination after translation. An extension of this approach is reported in Stymne and Cancedda (2011) where a CRF-model is used for compound prediction. In Cap et al. (2014) their approach is extended through using source-language features and lemmatisation, allowing for maximal generalisation over compound parts. Source-side Reordering One major problem in English to German translation is the divergent clausal ordering: in particula</context>
</contexts>
<marker>Stymne, 2009</marker>
<rawString>Sara Stymne. 2009. A Comparison of Merging Strategies for Translation of German Compounds. In Proceedings of EACL 2009 (Student Workshop).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Hisami Suzuki</author>
<author>Achim Ruopp</author>
</authors>
<title>Applying Morphology Generation Models to Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of HLT-ACL</booktitle>
<contexts>
<context position="2363" citStr="Toutanova et al. (2008)" startWordPosition="368" endWordPosition="371">d merging and generation of inflection as a postprocessing step. This way, we are able to create German compounds and inflectional variants that have not been seen in the parallel training data. In this paper, we investigate the performance of well-established source-side reordering, nominal re-inflection and compound processing systems on an up-to-date shared task. In addition, we present experimental results on a verbal inflection component and a syntax-based variant including source-side reordering. 2 Related Work Re-Inflection The two-step translation approach we use was described by e.g. Toutanova et al. (2008) and Jeong et al. (2010), who use a number of morphological and syntactic features derived from both source and target language. More recently, Fraser et al. (2012) describe a similar approach for German using different CRF-based feature prediction models, one for each of the four grammatical features to be predicted for German words in noun phrases, namely number, gender, case and definiteness. This approach also handles wordformation issues such as portmanteau splitting and compounding. Weller et al. (2013) added subcategorization information in combination with source-side syntactic feature</context>
</contexts>
<marker>Toutanova, Suzuki, Ruopp, 2008</marker>
<rawString>Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. 2008. Applying Morphology Generation Models to Machine Translation. In Proceedings of HLT-ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Weller</author>
<author>Alexander Fraser</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>Using Subcategorization Knowledge to Improve Case Prediction for Translation to German.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL’13.</booktitle>
<contexts>
<context position="2877" citStr="Weller et al. (2013)" startWordPosition="447" endWordPosition="450"> Work Re-Inflection The two-step translation approach we use was described by e.g. Toutanova et al. (2008) and Jeong et al. (2010), who use a number of morphological and syntactic features derived from both source and target language. More recently, Fraser et al. (2012) describe a similar approach for German using different CRF-based feature prediction models, one for each of the four grammatical features to be predicted for German words in noun phrases, namely number, gender, case and definiteness. This approach also handles wordformation issues such as portmanteau splitting and compounding. Weller et al. (2013) added subcategorization information in combination with source-side syntactic features in order to improve the prediction of case. De Gispert and Mariño (2008) generate verbal inflection for translation from English into Spanish. They use classifiers trained not only on target language but also on source language features, which is even more crucial for the prediction of verbs than it is for nominal inflection. More recently, Williams and Koehn (2011) translate directly into target language surface forms. Agreement within NPs and PPs, and also between subject and verb is considered during the</context>
</contexts>
<marker>Weller, Fraser, Walde, 2013</marker>
<rawString>Marion Weller, Alexander Fraser, and Sabine Schulte im Walde. 2013. Using Subcategorization Knowledge to Improve Case Prediction for Translation to German. In Proceedings ofACL’13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>Agreement constraints for statistical machine translation into German.</title>
<date>2011</date>
<booktitle>In Proceedings of WMT</booktitle>
<contexts>
<context position="3333" citStr="Williams and Koehn (2011)" startWordPosition="519" endWordPosition="522">hrases, namely number, gender, case and definiteness. This approach also handles wordformation issues such as portmanteau splitting and compounding. Weller et al. (2013) added subcategorization information in combination with source-side syntactic features in order to improve the prediction of case. De Gispert and Mariño (2008) generate verbal inflection for translation from English into Spanish. They use classifiers trained not only on target language but also on source language features, which is even more crucial for the prediction of verbs than it is for nominal inflection. More recently, Williams and Koehn (2011) translate directly into target language surface forms. Agreement within NPs and PPs, and also between subject and verb is considered during the decoding process: they use string-to-tree translation, where the target language (German) morphology is expressed as a set of unification constraints automatically learned from a morphologically annotated German corpus. 71 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 71–78, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics Compound Processing Compound splitting for SMT has been a</context>
</contexts>
<marker>Williams, Koehn, 2011</marker>
<rawString>Philip Williams and Philipp Koehn. 2011. Agreement constraints for statistical machine translation into German. In Proceedings of WMT 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Phillipp Koehn</author>
</authors>
<title>GHKMRule Extraction and Scope-3 Parsing in Moses.</title>
<date>2012</date>
<booktitle>In Proceedings of WMT</booktitle>
<contexts>
<context position="14498" citStr="Williams and Koehn (2012)" startWordPosition="2271" endWordPosition="2274">raining and decoding, we used the Moses toolkit (Koehn et al., 2007) to build phrase-based statistical machine translation systems, following the instructions for the baseline system for the shared task, using only default settings. 1We could not parse the whole monolingual dataset due to time-constraints and thus used RFTagger as a substitute. 2available from https://sites.google.com/ site/zhangh1982/egret. Syntax-based Translation model As a variant to the phrase-based systems, we applied the inflection prediction system to a string-to-tree system with GHKM extraction (Galley et al. (2004), Williams and Koehn (2012)). We used the same data-sets as for the phrase-based systems, and applied BitPar (Schmid, 2004) to obtain targetside trees. For this system, we used source-side reordering according to Gojun and Fraser (2012) relying on parses obtained with EGRET3. Tuning For tuning of feature weights, we used batch-mira with ’safe-hope’ (Cherry and Foster, 2012) until convergence (or maximal 25 runs). We used the 3,000 sentences of newstest2012 for tuning. Each experiment was tuned separately, optimising Bleu scores (Papineni et al., 2002) against a lemmatised version of the tuning reference. In the compoun</context>
</contexts>
<marker>Williams, Koehn, 2012</marker>
<rawString>Philip Williams and Phillipp Koehn. 2012. GHKMRule Extraction and Scope-3 Parsing in Moses. In Proceedings of WMT 2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023242">
<title confidence="0.946298">
Comparing Czech and English AMRs
</title>
<author confidence="0.989072">
Zdefnka Urefsov´a Jan Hajifc Ondfrej Bojar
</author>
<affiliation confidence="0.975023333333333">
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
</affiliation>
<address confidence="0.824188">
Malostransk´e n´amˇest´ı 25, 11800 Prague 1, Czech Republic
</address>
<email confidence="0.998227">
{uresova,hajic,bojar}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.997374" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999555">
This paper describes in detail the differences between Czech and English annotation us-
ing the Abstract Meaning Representation scheme, which stresses the use of ontologies (and
semantically-oriented verbal lexicons) and relations based on meaning or ontological content
rather than semantics or syntax. The basic “slogan” of the AMR specification clearly states that
AMR is not an interlingua, yet it is expected that many relations as well as structures constructed
from these relations will be similar or even identical across languages. In our study, we have
investigated 100 sentences in English and their translations into Czech, annotated manually by
AMRs, with the goal to describe the differences and if possible, to classify them into two main
categories: those which are merely convention differences and thus can be unified by changing
such conventions in the AMR annotation guidelines, and those which are so deeply rooted in the
language structure that the level of abstraction which is inherent in the current AMR scheme does
not allow for such unification.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991225">
In this paper, we follow on a previous first exploratory investigation of differences in AMR annotation
among different languages (Xue et al., 2014), which has classified the similarities and differences into
four categories: (a) no difference, (b) local difference only (such as multiword expressions vs. single
word terms), (c) reconcilable difference due to AMR conventions, and (d) deep differences which cannot
be unified in the AMR guidelines. In this paper, we would like to elaborate especially on the (b) and (c)
types, which have been only exemplified in the previous work. In this paper, we would like to not only
go deeper, but also present quantitative comparison on 100 parallel sentences, for all the aforementioned
categories and some of their subtypes.
We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building
also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study
(Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English
and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work.
</bodyText>
<sectionHeader confidence="0.990381" genericHeader="method">
2 Abstract Meaning Representation (AMR)
</sectionHeader>
<bodyText confidence="0.884484">
Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005)
and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004),
TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse
TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP
tasks. These annotated resources enabled substantial amounts of research in different areas of semantic
analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak,
2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the
PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013)
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0
</bodyText>
<page confidence="0.976929">
55
</page>
<note confidence="0.745472">
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64,
Coling 2014, Dublin, Ireland, August 24 2014.
</note>
<bodyText confidence="0.999328333333333">
and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in
the area of temporal analysis.
There have been efforts to create a unified representation which would cover at least a whole sentence,
or even a continuous text (Hajiˇc et al., 2003; Srikumar and Roth, 2013), and currently the Abstract
Meaning Representation represents an attempt to provide a common ground for truly semantic and fully
covering annotation representation.
An Abstract Meaning Representation is a rooted, directional and labeled graph that represents the
meaning of a sentence and it abstracts away from such syntactic notions as word category (verbs and
nouns), word order, morphological variation etc. Instead, it focuses on semantic relations between con-
cepts and makes heavy use of predicate-argument structures as defined in PropBank (for English). As
a result, the word order in the sentence is considered to be of little relevance to the meaning represen-
tation and is not necessarily maintained in the AMR. In addition, many function words (determiners,
prepositions) that do not contribute to meaning and are not explicitly represented in AMR, except for the
semantic relations they express. Readers are referred to Baranescu et al. (2013) for a complete descrip-
tion of AMR.1
</bodyText>
<figureCaption confidence="0.999095">
Figure 1: AMR annotation of the sentence “This infatuation with city living truly baffles me.”
</figureCaption>
<bodyText confidence="0.985497555555555">
An example of an AMR-annotated sentence can be seen in Fig. 1. The predicate of the sentence
(baffle) becomes the root of the annotation graph, with a reference to the correct sense baffle-01 as
found in PropBank frame files for baffle; PropBank frame files play the role of an ontology of events.
Arguments of predicates, again as described in the PropBank frames, become the substitutes for roles of
the “who did what to whom” interpretation - in the example sentence, infatuation - marked as ARG0 - is
the thing that baffles someone (the ARG1), i.e. me (the author of the text) in this case. This “baffling” is
further modified by “truly”, and marked simply as a modifier, the semantics of which is fully represented
by the word true itself. The agent (infatuation) has to be further restricted - it is the “infatuation with
city living” which baffles the author - not just any infatuation. This is represented by the relation topic
assigned to the edge between infatuation and live-01 in the AMR graph, and the “living” (sense
live-01) is further restricted by the location mentioned in the sentence, namely city. Finally,
the modifier this is kept in, since it is needed for reference to previous text, where the “infatuation”
has been first mentioned.
While the graphical representation in Fig. 1 is simplified in that it does not show the AMR’s crucial
instance-of relations explicitly as edges in the AMR graph, Fig. 2 shows the native underlying
“bracketed” textual representation of the same tree, where the main nodes (i.e. those shown visibly in
Fig. 1) are mentions, and the labels baffle-01, true, live-01, city etc. represent links to
external ontologies. These links are currently represented only by these strings, or by links to PropBank
</bodyText>
<footnote confidence="0.999556333333333">
1This paragraph as well as the two preceding ones are taken over (and slightly adapted) from the introductory sections of a
previous paper on this topic presented at LREC and co-authored by us (Xue et al., 2014); we share the same AMR formalism
and data.
</footnote>
<page confidence="0.998784">
56
</page>
<bodyText confidence="0.99874175">
files for events. In the future, these links will be wikified, i.e. for concepts described in an external
ontology, such as Wikipedia, they will be linked to it. The single- or two-letter “indexes” are in fact the
labels (IDs) of the mentions, and they also serve for (co-)reference purposes; the slash (‘/’) is a shortcut
for the instance-of relation.
</bodyText>
<equation confidence="0.999114571428572">
(b / baffle-01
:ARG0 (i / infatuation
:topic (l / live-01
:location (c / city))
:mod (t / this))
:ARG1 (i2 / i)
:mod (t2 / true))
</equation>
<figureCaption confidence="0.9656195">
Figure 2: Textual form of the AMR annotation of the sentence “This infatuation with city living truly
baffles me.”
</figureCaption>
<bodyText confidence="0.996389">
In Czech, the event ontology has been approximated by the Czech valency dictionary, PDT-Vallex
(Hajiˇc et al., 2003), (Ureˇsov´a, 2009), (Ureˇsov´a and Pajas, 2009), (Ureˇsov´a, 2006). No wikification of
non-event nodes has been attempted yet; this is a continuing work, as it is for English.
</bodyText>
<sectionHeader confidence="0.990077" genericHeader="method">
3 The Data
</sectionHeader>
<bodyText confidence="0.999990272727273">
We have drawn on a blog on Virginia road construction, taken from the WB part of the Penn Treebank.
These sentences have already been annotated using AMRs, and also translated to Czech2 and subse-
quently AMR-annotated. The English text has 1676 word and punctuation tokens (using the Penn Tree-
bank style tokenization), and its annotated AMR representation contains 1231 nodes (not counting the
instance-of nodes as separate nodes). The Czech version is a result of manually doubly-translated
English original, which has been mutually checked and then one (slightly corrected) translation has been
used for annotation. The Czech text has a total of 1563 tokens and its AMR representation contains 1215
nodes (again, not counting the instance-of nodes as separate nodes).
The data, once annotated, have been converted to a graph and in such a form presented to a linguist
familiar with the AMR style annotation, to study and extract statistics for this comparison study. Fig. 3
shows such a side-by-side graphs for English and Czech AMR for the example parallel sentences.
</bodyText>
<sectionHeader confidence="0.998823" genericHeader="method">
4 The Comparison
</sectionHeader>
<subsectionHeader confidence="0.999425">
4.1 Quantitative Comparison
</subsectionHeader>
<bodyText confidence="0.999949">
In the first pass, we have concentrated on marking and counting the following phenomena:
</bodyText>
<listItem confidence="0.9990586">
• structural identity: sentences with identical structure have been marked as being structurally the
same, even if some relation (edge) labels have been different
• structural differences: no. of structural differences have been noted in cases where one or more
(sub)parts of the AMR graph differ between the two languages
• local difference only: out of the above, certain differences have been marked as “local only” - for
example, if a multiword expression annotated as several nodes in one language corresponds to a
single node in the other language
• relation differences: for each sentence, number of differences in relational labels has been counted
• reference differences: number of different references to an external ontology (or assumed differ-
ences in case no link to such an ontology was actually present in the annotation).
</listItem>
<footnote confidence="0.710473">
2...and Chinese, but that was not used for this study.
</footnote>
<page confidence="0.9975">
57
</page>
<figureCaption confidence="0.891296333333333">
Figure 3: AMR annotation of the sentence “This infatuation with city living truly baffles me.” and its
translation to Czech (“Tohle/this poblouznˇen´ı/infatuation bydlen´ım/living-INSTR v/in mˇestsk´em/city-like
stylu/style mˇe/me poˇr´ad/still mate/baffles.”)
</figureCaption>
<bodyText confidence="0.977543666666667">
It is obvious that we could have observed also other types of differences, but at this point, we wanted
to have at least an idea how many differences exist in our approx. 1500-token sample. The resulting
figures are summarized in Table 1.3
</bodyText>
<table confidence="0.9981454">
Same Different Local difference Relation Reference
structure substructures only differences differences
29 (sents) 193 (subgraphs) 92 (subgraphs) 331 (nodes) 37 (nodes)
of 100 of approx. 8002 of 193 (all diffs) of 1215 Cz nodes of 1215 Cz nodes
29 % approx. 25 %2 47.7 % 27.2 % 3.0 %
</table>
<tableCaption confidence="0.99976">
Table 1: Number and percentages of differences in the annotated data
</tableCaption>
<bodyText confidence="0.999789">
The number of truly identically annotated sentences (including relation labeling) was only four, two
of which has been interjective “sentences” at the beginning of the document (“Braaawk!”). On the other
hand, 18 additional sentences would be structurally identical (on top of the 29) if local differences were
disregarded, bringing the (unlabeled sentence identity) total to 47, or almost half of the data (47=29+18).
</bodyText>
<subsectionHeader confidence="0.999782">
4.2 Analysis of Differences
</subsectionHeader>
<bodyText confidence="0.999788125">
The main goal of this study is to analyze differences in the annotation for the two languages, Czech
and English, and determine if a reconciliation of the annotation is possible or not (and for what reason
it is / it is not). Based on the above quantitative analysis, we have concentrated on relation labeling
differences due their high proportion, and on structural differences due to their heterogeneous nature.
The differences in reference annotation are small, but this is due to the lack of full referential annotation
(it has been done for events, but only assumed for other types of entities due to the lack of ontology,
or better to say due to the lack of “wikification” annotation in both languages), rather than due to high
agreement. We will come back to this once wikification of the annotation is finished.
</bodyText>
<footnote confidence="0.693545">
3Structural differences are hard to quantify exactly, since the base is difficult to define; it is part of future work.
</footnote>
<page confidence="0.994489">
58
</page>
<subsectionHeader confidence="0.980353">
4.3 Differences in Relation Labeling
</subsectionHeader>
<bodyText confidence="0.99944975">
The differences in function labeling should be taken with a grain of salt. The crucial question is what
should count as a difference in relation labeling if the structure differs - should this be automatically
counted as a difference, or not at all? In the figures summarized in Table 1, we have taken a middle
ground: if the structural difference implied a change in labeling by itself, we have not counted that
difference in order not to “penalize” the sentence annotation twice.
More detailed inspection of relation labeling differences, which appear to be relatively frequent at
more than 1/4th of all nodes in the annotation, revealed that the by far most frequent mismatch is caused
by different argument labeling for events.4 While for most purely transitive verbs there is a complete
match, for most other there is a discrepancy due to the attempted semanticization of PDT-Vallex argu-
ment labels ADDR (addressee), EFF (effect) and ORIG (origin), while PropBank simply continues to
number arguments of corresponding verbs consecutively (for example, I thought there is.ARG1 ... vs.
Myslel/I-thought jsem, ˇze/that tam/there je.ARG3←EFF/is ...). The concept of “shifting” in PDT-Vallex,
which compulsorily fills the first two arguments on syntactic grounds as ACT(ARG0) and PAT(ARG1)
is another source of differences. Furthermore, PropBank leaves out ARG0 e.g. for unaccusative verbs
(for example The window.ARG1 broke vs. Okno.ARG0←ACT/window se/itself rozbilo/broke.). Finally,
some differences are due to some arguments not being considered arguments at all in the other language,
in which case some other AMR label is used instead (for example, We could have spent 400M.ARG3 ...
elsewhere vs.... mohli/could utratit/spend 400M.extent ... jinde/elsewhere).
These differences could possibly be consolidated (only) by carefully linking the two lexicons (with
AMR guidelines intact). This is in fact being performed in another project (Sindlerova et al., 2014), but
it is a daunting manual task, since the underlying theories behind PropBank and PDT-Vallex/EngVallex
differ. However, one has to ask if it does make sense to do so, because with enough parallel data available,
the mappings can be learned relatively easily: in most cases, no structural differences are involved and
there will be a simple one-to-one mapping between the labels (conditioned on the particular verb sense).
</bodyText>
<subsectionHeader confidence="0.994632">
4.4 Structural Differences
</subsectionHeader>
<bodyText confidence="0.9999266">
Local differences can be safely ignored, since they will be in most cases resolved during the assumed
process of wikification, i.e., linking to an ontology concept. For example, the abbreviation VDOT (Vir-
ginia Department of Transportation), which has to be (and was) translated into Czech in an explanatory
way (otherwise the sentence would become not quite understandable, if only because of the real-world
context). Without wikification, it could not be linked as a whole, and thus a subgraph has been created
with the AMR-appropriate internal semantic relations in the translation (e.g. Virginia.location, etc.).
Certain differences, albeit “localized” into a small subtree (or subgraph) corresponding to a single
node or another small subtree (subgraph), cannot be resolved by wikification of a different event ontology
(than PropBank or PDT-Vallex). For example, light verb constructions or even certain modal or aspectual
constructions could have a single verb equivalent resulting in two node vs. single node annotation: get
close vs. piibl´ıˇzit-se, make worse vs. zhorˇsit, take position (for sb) vs. zast´avat-se or causing sprawl vs.
roztahuje-se.
Looking at the true structural differences, we have found that there are actually quite a few reasons for
them to appear in the annotation. We will describe them in more detail below.
Non-literal translation is the primary reason for such differences.5 For example, destination vs.
kam/where-to lid´e/people jezd´ı/drive (Fig. 4), or job center vs. misto/place, kde/where pracuje/work
hodnˇe/many lid´ı/people; these cases cannot be unified neither by changing the translation to a more literal
one (because it would be strongly misleading in the given context, despite the fact that literal translation
of both destination as well as job center does exist in Czech), neither by changing the guidelines, since
the level of abstraction of AMR does not call for a unification of such concepts. Sometimes, non-literal
</bodyText>
<footnote confidence="0.997702">
4The Czech PDT-Vallex argument labels have been mapped to PropBank labels as follows: ACT → ARG0, PAT → ARG1,
ADDR → ARG2, EFF → ARG3 and ORIG → ARG4.
5This includes also cases of truly wrong translation, stemming of translator’s misunderstanding of the facts behind the
sentence. This has been found fairly often only after we studied the differences in depth, since a superficial reading and
standard translation revision procedure did not help.
</footnote>
<page confidence="0.999124">
59
</page>
<bodyText confidence="0.849795666666667">
translation is forced upon the translation because no word-for-word translation exists, such as in in the
aggregate, which has to be translated using an extra clause z/from celkov´eho/overall pohledu/view to/it
je/is tak/so, ˇze/that ... (Fig. 5).
</bodyText>
<figureCaption confidence="0.993956">
Figure 4: AMR structural difference: destination vs. kam/where-to lid´e/people jezd´ı/drive
</figureCaption>
<bodyText confidence="0.999936636363636">
Phraseological differences and idioms form another large group of differences between the two lan-
guages. The possibility of changing the translation is even more remote than in the above case, even if we
had the chance: the provided translation is actually the correct and perfect one. The reason for different
annotation lies in the AMR scheme, which does not go that far to require “unified” annotation in such
cases where the idiom or specific phrase cannot be linked to the external ontology as a single unit. For
example, English “I don’t see any point” is translated as “nem´a/not-have smysl/purpose”, and despite the
fact that have-purpose-91 is a specific event reference in English (and has been used in the anno-
tation), the verb “see” still remains annotated as a separate event node, which is not the case in Czech,
since no “seeing” is expressed in the sentence and it could hardly be asked for in the guidelines to be
inserted. Similarly, I commute back and forth has been translated simply as doj´ıˇzdim/commute, which is
semantically perfectly equivalent but the back and forth has been kept in the English annotation, because
</bodyText>
<page confidence="0.9963">
60
</page>
<figureCaption confidence="0.9694465">
Figure 5: AMR structural difference: in the aggregate vs. z/from celkov´eho/overall pohledu/view to/it
je/is tak/so, ˇze/that ...
</figureCaption>
<bodyText confidence="0.999483727272727">
deleting it was (probably) considered loss of information. It is only the confrontation with the translation
to a different language when one realizes that with just a little more abstraction, the annotation could
have been structurally the same (by keeping only the commute node in in the English annotation).6
Translation by interpretation is typically discouraged in translation school education, but sometimes
it is necessary to use it for smooth understanding of the translated text. Often, such interpretation results
in different AMR annotation. For example, Virginia centrist has been translated as stˇredov´y/centrist
voliˇc/voter [z/from Virginie/Virginia], because without the extra word voliˇc, the literal translation of
centrist would not be understandable correctly in this context (Fig. 6). Similarly, a SSmph zone vs.
z´ona/zone s/with omezenim/restriction na/to SS mph (added word omezenim/restriction), or traffic vs.
dopravn´ı/traffic z´acpa/jam.
Convention differences are inherent in many annotation schemes, and we have found them in AMR
</bodyText>
<footnote confidence="0.958963666666667">
6One could perhaps also argue that adding the equivalent of back and forth to the Czech translation would unify the trans-
lation, too; however, adding its literal equivalent in Czech tam a zpˇet would be considered superfluous and unnatural by Czech
speakers.
</footnote>
<page confidence="0.998145">
61
</page>
<figureCaption confidence="0.974222">
Figure 6: AMR structural difference: Virginia Centrist vs.
stˇredov´y/centrist voliˇc/voter [z/from Virginie/Virginia]
Figure 7: AMR convention difference:
</figureCaption>
<bodyText confidence="0.916883">
auditor as a single node vs. person, who
audits
guidelines, too. Often, they were related to the use of ARG-of vs. keeping the nominalization as a
single node. For example, for auditor, translated quite literally as auditor into Czech, has been annotated
as “a person, who audits” in English while in the Czech AMR structure, there is a single node (Fig. 7)
labeled as auditor (which undoubtedly will be correctly linked to some ontology entry after such link-
age/wikification is complete). These differences might be harder to consolidate, since such conventions
are very difficult to create proper guidelines for, especially across languages. No ontology (whether for
events or objects) will be complete either (to base the decisions on a particular ontology content).
</bodyText>
<sectionHeader confidence="0.995987" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999991333333333">
We have investigated differences in the annotation of parallel texts using the Abstract Meaning Rep-
resentation scheme, on approx. 1500 words of English-Czech corpus (100 sentences). We found and
counted the number of identities and four types of differences (structural, structural local, relational, and
referential), and exemplified them to see if a reconciliation (either by possibly changing the translation,
the guidelines, or the annotation itself) is possible.
This is a work in progress. Substantial amount of work remains. We will have to use larger data,
multiple annotation (interannotator agreement on English was relatively low and we expect to be the
case on Czech, too, once two annotators start annotating the same sentences), and we would also have
to actually suggest changes in the guidelines or their conventions, and to test them also on substantial
amounts of data.
The immediate extension of this work will cover wikification, i.e. the linking of all nodes in the AMR
representation of our dataset to some ontology: events are already covered, internally defined relations
are already annotated, too (such as named entity types, dates, quantities, etc.), but external links remain to
be added. We will not only use Wikipedia (as the term “wikification” might suggest), but we will extend
this idea also to other sources, such as DBpedia or BabelNet, keeping all links in parallel if possible.
This should allow for deep comparison of the two languages also content-wise. We should then be able
to better answer the question of annotation unification which does depend on these links rather than on
the annotation guidelines themselves.
Parallel AMR-annotated data will be used at the JHU 2014 Summer Workshop, where technology for
AMR-based parsing, generation and possibly also MT will be developed, allowing also technological
insight into the AMR scheme across languages.
</bodyText>
<sectionHeader confidence="0.997751" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999136">
This work was supported by the grant GP13-03351P of the Grant Agency of the Czech Republic, projects
LH12093 and LM2010013 of the Ministry of Education, Youth and Sports of the Czech Republic, and
the EU FP7 project 610516 “QTLeap”. It has been using language resources distributed by the LIN-
DAT/CLARIN7 project of the Ministry of Education, Youth and Sports of the Czech Republic (project
LM2010013).
</bodyText>
<footnote confidence="0.970516">
7http://lindat.cz, resource used: http://hdl.handle.net/11858/00-097C-0000-0023-4338-F,
alsoathttp://lindat.mff.cuni.cz/services/PDT-Vallex
</footnote>
<page confidence="0.998773">
62
</page>
<sectionHeader confidence="0.996392" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998529833333333">
L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and
N. Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic
Annotation Workshop, Sophia, Bulgaria.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richard Farkas, Filip Ginter, and Jan Hajiˇc. 2013. Joint morpho-
logical and syntactic analysis for richly inflected languages. Transactions of the Association for Computational
Linguistics, 1:415–428.
Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter
of the Association for Computational Linguistics conference, pages 132–139. Association for Computational
Linguistics.
Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics,
28(3):245–288.
Jan Hajiˇc, Jarmila Panevov´a, Zdeˇnka Ureˇsov´a, Alevtina B´emov´a, Veronika Kol´aˇrov´a, and Petr Pajas. 2003. PDT-
VALLEX: Creating a Large-coverage Valency Lexicon for Treebank Annotation. In Proceedings of The Second
Workshop on Treebanks and Linguistic Theories, volume 9 of Mathematical Modeling in Physics, Engineering
and Cognitive Sciences, pages 57–68. V¨axj¨o University Press, November 14–15, 2003.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Martf, Llufs M`arquez, Adam
Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Lan-
guages. In Jan Hajiˇc, editor, Proceedings of the Thirteenth Conference on Computational Natural Language
Learning (CoNLL): Shared Task, pages 1–18, Boulder, CO, USA. Association for Computational Linguistics.
Jan Hajiˇc, Alena B¨ohmov´a, Eva Hajicov´a, and Barbora Hladk´a. 2003. The Prague Dependency Treebank: A
Three Level Annotation Scenario. In Anne Abeill´e, editor, Treebanks: Building and Using Annotated Corpora.
Kluwer Academic Publishers.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of
english: The penn treebank. Computational Linguistics, 19(2):313–330.
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank
Project: An Interim Report. In Proceedings of the NAACL/HLT Workshop on Frontiers in Corpus Annotation,
pages 24–31, Boston, Massachusetts.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL, pages 404–411.
Sameer S Pradhan, Wayne Ward, Kadri Hacioglu, James H Martin, and Daniel Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In HLT-NAACL, pages 233–240.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language
Resources and Evaluation (LREC 2008), Marrakech, Morocco.
James Pustejovsky, Patrick Hanks, Roser Saur´ı, Andrew See, David Day, Lisa Ferro, Robert Gaizauskas, Marcia
Lazo, Andrea Setzer, and Beth Sundheim. 2003. The TimeBank Corpus. Corpus Linguistics, pages 647–656.
Roser Saur´ı and James Pustejovsky. 2009. Factbank: a corpus annotated with event factuality. Language resources
and evaluation, 43(3):227–268.
Jana Sindlerova, Zdenka Uresova, and Eva Fucikova. 2014. Resources in Conflict: A Bilingual Valency Lexicon
vs. a Bilingual Treebank vs. a Linguistic Theory. In Nicoletta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Ste-
lios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evalua-
tion (LREC’14), pages 2490–2494, Reykjavik, Iceland, May 26-31. European Language Resources Association
(ELRA).
</reference>
<page confidence="0.987487">
63
</page>
<reference confidence="0.99930195">
Vivek Srikumar and Dan Roth. 2013. Modeling semantic relations expressed by prepositions. Transactions of the
Association for Computational Linguistics, 1:231–242.
Zdeˇnka Ureˇsov´a and Petr Pajas. 2009. Diatheses in the Czech Valency Lexicon PDT-Vallex. In Jana Levick´a
and Radovan Garabik, editors, Slovko 2009, NLP, Corpus Linguistics, Corpus Based Grammar Research, pages
358–376, Bratislava, Slovakia. Jazykovedn´y ´ustav udovita ˇSt´ura Slovenskej akad´emie vied, Slovensk´a akad´emia
vied.
Zdeˇnka Ureˇsov´a, 2006. Verbal Valency in the Prague Dependency Treebank from the Annotator’s Viewpoint, pages
93–112. Veda, Bratislava, Bratislava, Slovensko.
Zdeˇnka Ureˇsov´a. 2009. Building the PDT-VALLEX valency lexicon. In Catherine Smith
Michaela Mahlberg, Victorina Gonzalez-Diaz, editor, Proceedings of the Corpus Linguistics Conference),
http://ucrel.lancs.ac.uk/publications/cl2009/100 FullPaper.doc, July 20-23. University of Liverpool, UK.
Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In EMNLP, pages 88–94.
Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase Structure
Annotation of a Large Corpus. Natural Language Engineering, 11(2):207–238.
Nianwen Xue, Ondrej Bojar, Jan Hajic, Martha Palmer, Zdenka Uresova, and Xiuhong Zhang. 2014. Not an In-
terlingua, But Close: Comparison of English AMRs to Chinese and Czech. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno,
Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Re-
sources and Evaluation (LREC’14), pages 1765–1772, Reykjavik, Iceland, May 26-31. European Language
Resources Association (ELRA).
</reference>
<page confidence="0.999412">
64
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.502356">
<title confidence="0.998604">Comparing Czech and English AMRs</title>
<author confidence="0.945105">Zdefnka Urefsov´a Jan Hajifc Ondfrej</author>
<affiliation confidence="0.918128333333333">Charles University in Faculty of Mathematics and Institute of Formal and Applied</affiliation>
<address confidence="0.540923">Malostransk´e n´amˇest´ı 25, 11800 Prague 1, Czech</address>
<abstract confidence="0.999356769230769">This paper describes in detail the differences between Czech and English annotation using the Abstract Meaning Representation scheme, which stresses the use of ontologies (and semantically-oriented verbal lexicons) and relations based on meaning or ontological content rather than semantics or syntax. The basic “slogan” of the AMR specification clearly states that AMR is not an interlingua, yet it is expected that many relations as well as structures constructed from these relations will be similar or even identical across languages. In our study, we have investigated 100 sentences in English and their translations into Czech, annotated manually by AMRs, with the goal to describe the differences and if possible, to classify them into two main categories: those which are merely convention differences and thus can be unified by changing such conventions in the AMR annotation guidelines, and those which are so deeply rooted in the language structure that the level of abstraction which is inherent in the current AMR scheme does not allow for such unification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Banarescu</author>
<author>C Bonial</author>
<author>S Cai</author>
<author>M Georgescu</author>
<author>K Griffitt</author>
<author>U Hermjakob</author>
<author>K Knight</author>
<author>P Koehn</author>
<author>M Palmer</author>
<author>N Schneider</author>
</authors>
<title>Abstract Meaning Representation for Sembanking.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop,</booktitle>
<location>Sophia, Bulgaria.</location>
<contexts>
<context position="2234" citStr="Banarescu et al., 2013" startWordPosition="335" endWordPosition="338">difference, (b) local difference only (such as multiword expressions vs. single word terms), (c) reconcilable difference due to AMR conventions, and (d) deep differences which cannot be unified in the AMR guidelines. In this paper, we would like to elaborate especially on the (b) and (c) types, which have been only exemplified in the previous work. In this paper, we would like to not only go deeper, but also present quantitative comparison on 100 parallel sentences, for all the aforementioned categories and some of their subtypes. We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı an</context>
</contexts>
<marker>Banarescu, Bonial, Cai, Georgescu, Griffitt, Hermjakob, Knight, Koehn, Palmer, Schneider, 2013</marker>
<rawString>L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K. Griffitt, U. Hermjakob, K. Knight, P. Koehn, M. Palmer, and N. Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic Annotation Workshop, Sophia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
<author>Igor Boguslavsky</author>
<author>Richard Farkas</author>
<author>Filip Ginter</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Joint morphological and syntactic analysis for richly inflected languages.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--415</pages>
<marker>Bohnet, Nivre, Boguslavsky, Farkas, Ginter, Hajiˇc, 2013</marker>
<rawString>Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Richard Farkas, Filip Ginter, and Jan Hajiˇc. 2013. Joint morphological and syntactic analysis for richly inflected languages. Transactions of the Association for Computational Linguistics, 1:415–428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>132--139</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3188" citStr="Charniak, 2000" startWordPosition="483" endWordPosition="484"> Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 132–139. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="3172" citStr="Collins, 1999" startWordPosition="481" endWordPosition="482">sentation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in o</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational linguistics,</journal>
<pages>28--3</pages>
<contexts>
<context position="3315" citStr="Gildea and Jurafsky, 2002" startWordPosition="502" endWordPosition="505">nnotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
<author>Zdeˇnka Ureˇsov´a</author>
<author>Alevtina B´emov´a</author>
<author>Veronika Kol´aˇrov´a</author>
<author>Petr Pajas</author>
</authors>
<title>PDTVALLEX: Creating a Large-coverage Valency Lexicon for Treebank Annotation.</title>
<date>2003</date>
<booktitle>In Proceedings of The Second Workshop on Treebanks and Linguistic Theories,</booktitle>
<volume>9</volume>
<pages>57--68</pages>
<publisher>V¨axj¨o University Press,</publisher>
<marker>Hajiˇc, Panevov´a, Ureˇsov´a, B´emov´a, Kol´aˇrov´a, Pajas, 2003</marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Zdeˇnka Ureˇsov´a, Alevtina B´emov´a, Veronika Kol´aˇrov´a, and Petr Pajas. 2003. PDTVALLEX: Creating a Large-coverage Valency Lexicon for Treebank Annotation. In Proceedings of The Second Workshop on Treebanks and Linguistic Theories, volume 9 of Mathematical Modeling in Physics, Engineering and Cognitive Sciences, pages 57–68. V¨axj¨o University Press, November 14–15, 2003.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Zhang</author>
</authors>
<date>2009</date>
<booktitle>The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. In</booktitle>
<pages>1--18</pages>
<editor>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Martf, Llufs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Boulder, CO, USA.</location>
<marker>Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Martf, Llufs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. In Jan Hajiˇc, editor, Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 1–18, Boulder, CO, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
</authors>
<title>Alena B¨ohmov´a, Eva Hajicov´a, and Barbora Hladk´a.</title>
<date>2003</date>
<editor>In Anne Abeill´e, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Hajiˇc, 2003</marker>
<rawString>Jan Hajiˇc, Alena B¨ohmov´a, Eva Hajicov´a, and Barbora Hladk´a. 2003. The Prague Dependency Treebank: A Three Level Annotation Scenario. In Anne Abeill´e, editor, Treebanks: Building and Using Annotated Corpora. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2636" citStr="Marcus et al., 1993" startWordPosition="399" endWordPosition="402"> also present quantitative comparison on 100 parallel sentences, for all the aforementioned categories and some of their subtypes. We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Ro</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Meyers</author>
<author>R Reeves</author>
<author>C Macleod</author>
<author>R Szekely</author>
<author>V Zielinska</author>
<author>B Young</author>
<author>R Grishman</author>
</authors>
<title>The NomBank Project: An Interim Report.</title>
<date>2004</date>
<booktitle>In Proceedings of the NAACL/HLT Workshop on Frontiers in Corpus Annotation,</booktitle>
<pages>24--31</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="2776" citStr="Meyers et al., 2004" startWordPosition="423" endWordPosition="426">first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., </context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The NomBank Project: An Interim Report. In Proceedings of the NAACL/HLT Workshop on Frontiers in Corpus Annotation, pages 24–31, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="2745" citStr="Palmer et al., 2005" startWordPosition="418" endWordPosition="421">ome of their subtypes. We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue an</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing. In</title>
<date>2007</date>
<booktitle>HLT-NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="3213" citStr="Petrov and Klein, 2007" startWordPosition="485" endWordPosition="488">anks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved inference for unlexicalized parsing. In HLT-NAACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>233--240</pages>
<contexts>
<context position="3337" citStr="Pradhan et al., 2004" startWordPosition="506" endWordPosition="509">opBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create a unified representat</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer S Pradhan, Wayne Ward, Kadri Hacioglu, James H Martin, and Daniel Jurafsky. 2004. Shallow semantic parsing using support vector machines. In HLT-NAACL, pages 233–240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind Joshi</author>
<author>Bonnie Webber</author>
</authors>
<title>The Penn Discourse Treebank 2.0.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="2909" citStr="Prasad et al., 2008" startWordPosition="442" endWordPosition="445">resent the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of the PropBank (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004; Bohnet et al., 2013) This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are a</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse Treebank 2.0. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 2008), Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Saur´ı</author>
<author>Andrew See</author>
<author>David Day</author>
<author>Lisa Ferro</author>
<author>Robert Gaizauskas</author>
<author>Marcia Lazo</author>
<author>Andrea Setzer</author>
<author>Beth Sundheim</author>
</authors>
<title>The TimeBank Corpus. Corpus Linguistics,</title>
<date>2003</date>
<pages>647--656</pages>
<marker>Pustejovsky, Hanks, Saur´ı, See, Day, Ferro, Gaizauskas, Lazo, Setzer, Sundheim, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Saur´ı, Andrew See, David Day, Lisa Ferro, Robert Gaizauskas, Marcia Lazo, Andrea Setzer, and Beth Sundheim. 2003. The TimeBank Corpus. Corpus Linguistics, pages 647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Saur´ı</author>
<author>James Pustejovsky</author>
</authors>
<title>Factbank: a corpus annotated with event factuality. Language resources and evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<marker>Saur´ı, Pustejovsky, 2009</marker>
<rawString>Roser Saur´ı and James Pustejovsky. 2009. Factbank: a corpus annotated with event factuality. Language resources and evaluation, 43(3):227–268.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jana Sindlerova</author>
<author>Zdenka Uresova</author>
<author>Eva Fucikova</author>
</authors>
<title>Resources in Conflict: A Bilingual Valency Lexicon vs. a Bilingual Treebank vs. a Linguistic Theory.</title>
<date>2014</date>
<booktitle>Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<pages>2490--2494</pages>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Reykjavik, Iceland,</location>
<contexts>
<context position="14409" citStr="Sindlerova et al., 2014" startWordPosition="2268" endWordPosition="2271">ank leaves out ARG0 e.g. for unaccusative verbs (for example The window.ARG1 broke vs. Okno.ARG0←ACT/window se/itself rozbilo/broke.). Finally, some differences are due to some arguments not being considered arguments at all in the other language, in which case some other AMR label is used instead (for example, We could have spent 400M.ARG3 ... elsewhere vs.... mohli/could utratit/spend 400M.extent ... jinde/elsewhere). These differences could possibly be consolidated (only) by carefully linking the two lexicons (with AMR guidelines intact). This is in fact being performed in another project (Sindlerova et al., 2014), but it is a daunting manual task, since the underlying theories behind PropBank and PDT-Vallex/EngVallex differ. However, one has to ask if it does make sense to do so, because with enough parallel data available, the mappings can be learned relatively easily: in most cases, no structural differences are involved and there will be a simple one-to-one mapping between the labels (conditioned on the particular verb sense). 4.4 Structural Differences Local differences can be safely ignored, since they will be in most cases resolved during the assumed process of wikification, i.e., linking to an </context>
</contexts>
<marker>Sindlerova, Uresova, Fucikova, 2014</marker>
<rawString>Jana Sindlerova, Zdenka Uresova, and Eva Fucikova. 2014. Resources in Conflict: A Bilingual Valency Lexicon vs. a Bilingual Treebank vs. a Linguistic Theory. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 2490–2494, Reykjavik, Iceland, May 26-31. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivek Srikumar</author>
<author>Dan Roth</author>
</authors>
<title>Modeling semantic relations expressed by prepositions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>1--231</pages>
<contexts>
<context position="4058" citStr="Srikumar and Roth, 2013" startWordPosition="614" endWordPosition="617">bution 4.0 International Licence. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0 55 Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 55–64, Coling 2014, Dublin, Ireland, August 24 2014. and similar resources in other languages (Hajiˇc et al., 2009), and TimeBank has fueled much research in the area of temporal analysis. There have been efforts to create a unified representation which would cover at least a whole sentence, or even a continuous text (Hajiˇc et al., 2003; Srikumar and Roth, 2013), and currently the Abstract Meaning Representation represents an attempt to provide a common ground for truly semantic and fully covering annotation representation. An Abstract Meaning Representation is a rooted, directional and labeled graph that represents the meaning of a sentence and it abstracts away from such syntactic notions as word category (verbs and nouns), word order, morphological variation etc. Instead, it focuses on semantic relations between concepts and makes heavy use of predicate-argument structures as defined in PropBank (for English). As a result, the word order in the se</context>
</contexts>
<marker>Srikumar, Roth, 2013</marker>
<rawString>Vivek Srikumar and Dan Roth. 2013. Modeling semantic relations expressed by prepositions. Transactions of the Association for Computational Linguistics, 1:231–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdeˇnka Ureˇsov´a</author>
<author>Petr Pajas</author>
</authors>
<title>Diatheses in the Czech Valency Lexicon PDT-Vallex.</title>
<date>2009</date>
<booktitle>In Jana Levick´a and Radovan Garabik, editors, Slovko 2009, NLP, Corpus Linguistics, Corpus Based Grammar Research,</booktitle>
<pages>358--376</pages>
<location>Bratislava,</location>
<marker>Ureˇsov´a, Pajas, 2009</marker>
<rawString>Zdeˇnka Ureˇsov´a and Petr Pajas. 2009. Diatheses in the Czech Valency Lexicon PDT-Vallex. In Jana Levick´a and Radovan Garabik, editors, Slovko 2009, NLP, Corpus Linguistics, Corpus Based Grammar Research, pages 358–376, Bratislava, Slovakia. Jazykovedn´y ´ustav udovita ˇSt´ura Slovenskej akad´emie vied, Slovensk´a akad´emia vied.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zdeˇnka Ureˇsov´a</author>
</authors>
<title>Verbal Valency in the Prague Dependency Treebank from the Annotator’s Viewpoint,</title>
<date>2006</date>
<pages>93--112</pages>
<location>Veda, Bratislava, Bratislava, Slovensko.</location>
<marker>Ureˇsov´a, 2006</marker>
<rawString>Zdeˇnka Ureˇsov´a, 2006. Verbal Valency in the Prague Dependency Treebank from the Annotator’s Viewpoint, pages 93–112. Veda, Bratislava, Bratislava, Slovensko.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Zdeˇnka Ureˇsov´a</author>
</authors>
<title>Building the PDT-VALLEX valency lexicon.</title>
<date>2009</date>
<booktitle>Proceedings of the Corpus Linguistics Conference), http://ucrel.lancs.ac.uk/publications/cl2009/100 FullPaper.doc,</booktitle>
<pages>88--94</pages>
<editor>In Catherine Smith Michaela Mahlberg, Victorina Gonzalez-Diaz, editor,</editor>
<publisher>University of</publisher>
<marker>Ureˇsov´a, 2009</marker>
<rawString>Zdeˇnka Ureˇsov´a. 2009. Building the PDT-VALLEX valency lexicon. In Catherine Smith Michaela Mahlberg, Victorina Gonzalez-Diaz, editor, Proceedings of the Corpus Linguistics Conference), http://ucrel.lancs.ac.uk/publications/cl2009/100 FullPaper.doc, July 20-23. University of Liverpool, UK. Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In EMNLP, pages 88–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Fei Xia</author>
<author>Fu dong Chiou</author>
<author>Martha Palmer</author>
</authors>
<title>The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="2676" citStr="Xue et al., 2005" startWordPosition="407" endWordPosition="410">00 parallel sentences, for all the aforementioned categories and some of their subtypes. We will first describe the basic principles of AMR annotation (Banarescu et al., 2013) (Sect. 2, building also on (Xue et al., 2014)), then present the data (parallel texts) which we have used for this study (Sect. 3), and describe the quantitative and qualitative comparison between AMR annotation of English and Czech (Sect. 4). In Sect. 5, we will summarize and discuss further work. 2 Abstract Meaning Representation (AMR) Syntactic treebanks in several languages (Marcus et al., 1993; Hajiˇc et al., 2003; Xue et al., 2005) and related annotated corpora such as PropBank (Palmer et al., 2005), Nombank (Meyers et al., 2004), TimeBank (Pustejovsky et al., 2003), FactBank (Saur´ı and Pustejovsky, 2009), and the Penn Discourse TreeBank (Prasad et al., 2008), coupled with machine learning techniques, have been used in many NLP tasks. These annotated resources enabled substantial amounts of research in different areas of semantic analysis. There had already been tremendous progress in syntactic parsing (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) and now in Semantic Role Labeling because of the existence of </context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer. 2005. The Penn Chinese TreeBank: Phrase Structure Annotation of a Large Corpus. Natural Language Engineering, 11(2):207–238.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Nianwen Xue</author>
<author>Ondrej Bojar</author>
<author>Jan Hajic</author>
<author>Martha Palmer</author>
<author>Zdenka Uresova</author>
<author>Xiuhong Zhang</author>
</authors>
<title>Not an Interlingua, But Close: Comparison of English AMRs to Chinese and Czech.</title>
<date>2014</date>
<booktitle>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson,</booktitle>
<pages>1765--1772</pages>
<location>Bente Maegaard, Joseph Mariani, Asuncion Moreno,</location>
<contexts>
<context position="1526" citStr="Xue et al., 2014" startWordPosition="223" endWordPosition="226">ions into Czech, annotated manually by AMRs, with the goal to describe the differences and if possible, to classify them into two main categories: those which are merely convention differences and thus can be unified by changing such conventions in the AMR annotation guidelines, and those which are so deeply rooted in the language structure that the level of abstraction which is inherent in the current AMR scheme does not allow for such unification. 1 Introduction In this paper, we follow on a previous first exploratory investigation of differences in AMR annotation among different languages (Xue et al., 2014), which has classified the similarities and differences into four categories: (a) no difference, (b) local difference only (such as multiword expressions vs. single word terms), (c) reconcilable difference due to AMR conventions, and (d) deep differences which cannot be unified in the AMR guidelines. In this paper, we would like to elaborate especially on the (b) and (c) types, which have been only exemplified in the previous work. In this paper, we would like to not only go deeper, but also present quantitative comparison on 100 parallel sentences, for all the aforementioned categories and so</context>
<context position="7113" citStr="Xue et al., 2014" startWordPosition="1111" endWordPosition="1114">s crucial instance-of relations explicitly as edges in the AMR graph, Fig. 2 shows the native underlying “bracketed” textual representation of the same tree, where the main nodes (i.e. those shown visibly in Fig. 1) are mentions, and the labels baffle-01, true, live-01, city etc. represent links to external ontologies. These links are currently represented only by these strings, or by links to PropBank 1This paragraph as well as the two preceding ones are taken over (and slightly adapted) from the introductory sections of a previous paper on this topic presented at LREC and co-authored by us (Xue et al., 2014); we share the same AMR formalism and data. 56 files for events. In the future, these links will be wikified, i.e. for concepts described in an external ontology, such as Wikipedia, they will be linked to it. The single- or two-letter “indexes” are in fact the labels (IDs) of the mentions, and they also serve for (co-)reference purposes; the slash (‘/’) is a shortcut for the instance-of relation. (b / baffle-01 :ARG0 (i / infatuation :topic (l / live-01 :location (c / city)) :mod (t / this)) :ARG1 (i2 / i) :mod (t2 / true)) Figure 2: Textual form of the AMR annotation of the sentence “This inf</context>
</contexts>
<marker>Xue, Bojar, Hajic, Palmer, Uresova, Zhang, 2014</marker>
<rawString>Nianwen Xue, Ondrej Bojar, Jan Hajic, Martha Palmer, Zdenka Uresova, and Xiuhong Zhang. 2014. Not an Interlingua, But Close: Comparison of English AMRs to Chinese and Czech. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 1765–1772, Reykjavik, Iceland, May 26-31. European Language Resources Association (ELRA).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.918811">
Aspectual Properties of Conversational Activities
</title>
<note confidence="0.801085666666667">
Rebecca J. Passonneau and Boxuan Guan and Cho Ho Yeung
becky@ccls.columbia.edu and bg2469@columbia.edu and cy2277@columbia.edu
Columbia University, New York, NY, USA
</note>
<author confidence="0.540755">
Yuan Du
</author>
<email confidence="0.973398">
ydu@fb.com
</email>
<author confidence="0.51199">
Facebook, New York, NY, USA
</author>
<sectionHeader confidence="0.962564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960736842105">
Segmentation of spoken discourse into
distinct conversational activities has been
applied to broadcast news, meetings,
monologs, and two-party dialogs. This
paper considers the aspectual properties
of discourse segments, meaning how they
transpire in time. Classifiers were con-
structed to distinguish between segment
boundaries and non-boundaries, where the
sizes of utterance spans to represent data
instances were varied, and the locations
of segment boundaries relative to these in-
stances. Classifier performance was better
for representations that included the end of
one discourse segment combined with the
beginning of the next. In addition, classi-
fication accuracy was better for segments
in which speakers accomplish goals with
distinctive start and end points.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999664444444444">
People engage in dialogue to address a wide range
of goals. It has long been observed that discourse
can be structured into units that correspond to dis-
tinct goals and activities (Grosz and Sidner, 1986;
Passonneau and Litman, 1997). This is concep-
tually distinct from structuring discourse into the
topical units addressed in (Hearst, 1997). The
ability to recognize where distinct activities oc-
cur in spoken discourse could support offline ap-
plications to spoken corpora such as search (Ward
and Werner, 2013), summarization (Murray et al.,
2005), and question answering. Further, a deeper
understanding of the relation of conversational
activities to observable features of utterance se-
quences could inform the design of interactive sys-
tems for online applications such as information
gathering, service requests, tutoring, and compan-
ionship. Automatic identification of such units,
</bodyText>
<note confidence="0.394727">
Emma Conner
</note>
<email confidence="0.95027">
econner@oberlin.edu
</email>
<author confidence="0.667044">
Oberlin College, Oberlin, OH, USA
</author>
<bodyText confidence="0.999966365853659">
however, has been difficult to achieve. This pa-
per considers the aspectual properties of speak-
ers’ conversational activities, meaning how they
transpire in time. We hypothesize that recognition
of a transition to a new conversational activity de-
pends on recognizing not only the start of a new
activity but also the end of the preceding one, on
the grounds that the relative contrast between end-
ings and beginnings might matter as much or more
than absolute characteristics consistent across all
beginnings or all endings. We further hypothesize
that transitions to certain kinds of conversational
activity may be easier to detect than others.
Following Austin’s view that speech constitutes
action of different kinds (Austin, 1962), we as-
sume that different kinds of communicative ac-
tion have different ways of transpiring in time,
just as other actions do. Conversational activities
that address objective goals, for example, can have
very well-demarcated beginnings and endings, as
when two people choose a restaurant to go to
for dinner. Conversational participants can, how-
ever, address goals that need not have a specific
resolution, such as shared complaints about the
lack of good Chinese restaurants. This distinction
between different kinds of actions that speakers
perform through their communicative behavior is
analogous to the distinction in linguistic semantics
pertaining to verbal aspect, between states, pro-
cesses and transition events (or accomplishments
and achievements) (Vendler, 1957) (Dowty, 1986).
States (e.g., being at a standstill) have no percep-
tible change from moment to moment; processes
(e.g., walking) have detectable differences in state
from moment to moment with no clearly demar-
cated change of state during the process; transition
events (e.g., starting to walk; walking to the end
of the block) involve a transition from one state or
process to another.
To investigate the aspectual properties of dis-
course segments, we constructed classifiers to de-
</bodyText>
<page confidence="0.969905">
228
</page>
<note confidence="0.7312935">
Proceedings of the SIGDIAL 2014 Conference, pages 228–237,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999921871794872">
tect discourse segment boundaries based on fea-
tures of utterances. We considered the aspec-
tual properties of discourse segments in two ways.
First, to investigate the relative contribution of
features from segment endings versus beginnings,
we experimented with different sizes of utter-
ance sequences, and different locations of seg-
ment boundaries relative to these sequences. Sec-
ond, we considered different categories of seg-
ments, based on the speculation that segment tran-
sitions that are easier to recognize would be as-
sociated with conversational activities that have
a well-demarcated event structure, in constrast to
activities that involve goals to maintain or sustain
aspects of interaction.
The following section describes related work in
this area, as well as the difficulties in achieving
good performance. Most work on identification of
discourse segments (or other forms of discourse
structure in spoken interaction) depends on a prior
phase of annotation (e.g., (Galley et al., 2003; Pas-
sonneau and Litman, 1997)). We studied a corpus
of eighty-two transcribed and annotated telephone
dialogues between library patrons and librarians
that had been annotated with units analogous to
speech acts, and subsequently annotated with dis-
course segments comprised of these units. The an-
notation yielded eight distinct kinds of discourse
segment, where a segment results from a linear
segmentation of a discourse into strictly sequential
units. (While the segmentation is sequential, the
units can have hierarchical relations.) We found
that classifiers to detect segment boundaries per-
formed best with boundaries represented by fea-
tures of sequences of utterances that spanned the
end of one segment and the beginning of the next.
Error analysis indicated that performance was bet-
ter for boundaries that initiate conversational ac-
tivities with clear beginnings and endings.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999978333333333">
Segmentation of spoken language interaction into
distinct discourse units has been applied to meet-
ings as well as to two-party discourse using acous-
tic features, lexical features, and very heteroge-
neous features. In our previous work, we used
a very heterogeneous set of features to segment
monologues into units that had been identified
by annotators as corresonding to distinct inten-
tional units (Passonneau and Litman, 1997). Text
tiling (Hearst, 1997) has been applied to segmen-
tation of meetings into distinct agenda segments
using both prior and following context (Baner-
jee and Rudnicky, 2006). Results had high pre-
cision and low recall. We also find that recall is
more challenging than precision. Topic modeling
methods have also been applied to the identifica-
tion of topical segments in speech (Purver et al.,
2006) (Eisenstein and Barzilay, 2008), with im-
provements over earlier work on the ICSI meeting
corpus (Galley et al., 2003) (Malioutov and Barzi-
lay, 2006).
An analog of text tiling that uses acoustic pat-
terns rather than lexical items has been applied to
the segmentation of speech into stories using seg-
mental dynamic time warping (SDTW) (Park and
Glass, 2008). The method is based on the intuition
of aligning utterances by similar acoustic patterns,
possibly representing common words and phrases.
Results on TDT2 Mandarin Broadcast News cor-
pus were moderately good for short episodes with
F=0.71 beating the baseline for lexical text tiling
of 0.66, but poor on long episodes.
An alternative method of relying solely on
acoustic information has been applied to impor-
tance prediction at a very fine granularity (Ward
and Richart-Ruiz, 2013). Four basic classes
of prosodic features derived from PCA were
used (Ward and Vega, 2012): volume, pitch
height, pitch range and speaking rate cross various
widths of time intervals. The data was labeled by
annotators using an importance scale of 1 to 5, and
linear regression was used to predict the label for
instances consisting of frames. The method per-
formed well with a correlation of 0.82 and mean
average error of 0.75 (5-fold cross validation).
The identification of different kinds of units in
discourse is somewhat related to the notion of
genre identification, e.g. (Obin et al., 2010) (Ries
et al., 2000). Results from this area have been ap-
plied to segmentation of conversation by a combi-
nation of topic and style (Ries, 2002).
</bodyText>
<sectionHeader confidence="0.985643" genericHeader="method">
3 Data and Annotations
</sectionHeader>
<bodyText confidence="0.9921136">
The corpus consists of recordings, transcripts and
annotations on the transcripts of a set of 82 calls
recorded in 2005 between patrons of the Andrew
Heiskell Braille and Talking Book Library of New
York City.1 An annotation for dialog acts with a
</bodyText>
<footnote confidence="0.977303333333333">
1The audio files and transcripts are available for download
from the Columbia University Data Commons. The annota-
tions and raw features will be released in the near future.
</footnote>
<page confidence="0.998789">
229
</page>
<bodyText confidence="0.999779090909091">
reduced set of dialog act types and adjacency pair
relations (Dialogue Function Units, DFUs) was
developed, originally for comparison of dialogues
across modalities (Hu et al., 2009). A subsequent
phase of annotation at the discourse level that
makes use of the dialog act annotation was later
applied. This later annotation, referred to as Task
Success and Cost Annotation (TSCA), was aimed
at identifying individual dialog tasks analogous to
those carried out by spoken dialog systems, to fa-
cilitate comparison of human-human dialog with
human-machine dialog. Interannotator reliability
of both annotations was measured using Krippen-
dorff’s alpha (Krippendorff, 1980) at levels of 0.66
and above for individual dialogues (Passonneau et
al., 2011). The corpus consists of 24,760 words,
or 302 words per dialog.
Briefly, the second phase of annotation involved
grouping DFUs into larger sequences in which
the participants continued to pursue a single co-
ordinated activity, and labeling the large discourse
units for their discourse function. The human an-
notation instructions avoided reference to overt
signals of dialog structure. Rather, annotators
were asked to judge the semantic and pragmatic
functions of utterances. The annotations have been
described in previous work (Hu et al., 2009; Pas-
sonneau et al., 2011); the annotation guidelines are
available online.2
The location of a transition between one con-
versational activity and the next is represented as
occurring between adjacent utterances. There are
9,340 utterance in the corpus, or 114 per dialog.
About 10.6 percent of the utterances (994) start a
new discourse unit. Within each unit, the speak-
ers establish a conversational goal explicitly or im-
plicitly, and continue to address the goal until it
is achieved, suspended, or abandoned. The dis-
course segments were of the following seven cate-
gories, with an additional Other category for none
of the above (examples from the corpus are shown
after each segment category description; words
in brackets represent overlapping talk of the two
speakers):
</bodyText>
<listItem confidence="0.99996275">
• Conventional: The participants engage in
conventionalized behavior, e.g., greetings (at
the beginning of the call) or goodbyes (at the
end of the call).
</listItem>
<tableCaption confidence="0.383528666666667">
2See links at http://www1.ccls.columbia.
edu/˜Loqui/resources.html for transcription
guidelines, and annotation manuals.
</tableCaption>
<bodyText confidence="0.6872855">
Librarian: andrew heiskell library
Librarian: how are you
Patron: good morning
Librarian: good morning
</bodyText>
<listItem confidence="0.855989730769231">
• Book-Request: The participants address a pa-
tron’s request for a book, which can be a spe-
cific book that first needs to be identified,
or which can be a non-specific request for a
book fitting some criterion (e.g., a mystery
the patron has not read before).
Patron: do you have any fannie flagg stories
Librarian: flag
Patron: yeah
Patron: F L A &lt;Pause&gt;
Patron: A G G I think it is
• Inform: One of the participants provides the
other with general information that does not
support a Book Request, e.g., the patron pro-
vides identifying information so the librarian
can pull up the patron’s record.
Patron: well I’ll call him again then
Patron: and I’ll get the name [today]
Librarian [talk] to him and call me back
Patron: &lt;pause&gt; i- i’ll call him
Patron: and then i’ll call you okay
Librarian: okay
• Librarian-Proposal: The participants address
the librarian’s suggestion of a specific book
or a kind of book that might meet the patron’s
desires.
</listItem>
<bodyText confidence="0.959437166666667">
Librarian: I have ellis but not bret
Patron: ah wa wa what do you have by him
Librarian: by cose
Librarian: C O S E
Librarian: I have the rage of a privileged class
Patron: that’s all right
</bodyText>
<listItem confidence="0.84781">
• Request-Action: One of the participants asks
the other to perform an action, e.g., the pa-
tron asks that certain authors be added to the
patron’s list of preferences
Patron: also &lt;pause&gt; uh
Patron: &lt;pause&gt; of the favorite author list
Librarian: mmhm
</listItem>
<bodyText confidence="0.958069333333333">
Patron: would you um
Patron: remove t jefferson parker
Librarian: okay
</bodyText>
<page confidence="0.94168">
230
</page>
<listItem confidence="0.777341333333333">
• Information-Request: One of the participants
seeks information from the other, e.g., the pa-
tron wants to know if
</listItem>
<bodyText confidence="0.718149333333333">
Patron: this is the talking books right
Librarian: yes
Librarian: this is the library for the blind
</bodyText>
<listItem confidence="0.969540857142857">
• Sidebar: The librarian temporarily takes a
call from another Patron only long enough to
place the new caller on hold
Librarian: hold on one second
Librarian: Andrew Heiskell Library
Librarian: please hold
• Other
</listItem>
<bodyText confidence="0.9999659375">
Of these seven kinds of discourse units, Book-
Requests and Librarian-Proposals are the most
clearly delimited by beginning and ending points.
At the beginning of a Book-Request, the patron
establishes that she wants a book, and the end is
identified by the mutual achievement of the librar-
ian and patron of either a successful resolution,
meaning the identification of a particular book in
the library’s collection that the patron will accept,
or a failure of the current attempt, which often
leads to a new revised book request. Librarian-
Proposals are very parallel to Book-Requests; the
difference is that the librarian makes a suggestion
of a specific book or kind of book which must be
identified for the patron, and which the patron then
accepts or rejects.
</bodyText>
<sectionHeader confidence="0.999355" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999807">
The experiments to automatically identify the lo-
cations of the annotated discourse units apply ma-
chine learning to instances consisting of utterance
sequences that represent the two classes, presence
versus absence of a boundary. We hyothesize
that the enormous challenges for identifying dis-
course structure in human-machine dialogue can
be better addressed through complementary re-
liance on semantics and interaction structure (be-
havioral cues), and each can reinforce the other.
The main focus of the experiments reported here
is on data representation to address the questions,
what features of the context support the ability
to segment a dialogue into conversational activity
units, and how much context is necessary?
A disadvantage of the dataset is its relatively
small size, especially given the extreme skew with
</bodyText>
<table confidence="0.98056425">
+ First utterance of segment
- Any other utterance
+ Last utterance of segment
- Any other utterance
</table>
<figure confidence="0.717478">
0
</figure>
<figureCaption confidence="0.568162666666667">
Figure 1: Schematic representation of instance
spans and labels. Bars on the left show the num-
ber of utterances (size) and position of segment
boundary (position) for five of the fourteen types
of instances. Positive and negative labels are
shown on the descriptions at the right.
</figureCaption>
<bodyText confidence="0.999878533333333">
the positive class consisting of only 10% of the in-
stances. On the other hand, the small size made
detailed annotation feasible, and the corpus is
well-suited to our research question in that it rep-
resents naturally occurring, spontaneous human-
human telephone discourse. Therefore. the man-
ner in which the dialogs evolve over time is en-
tirely natural. Our major question of interest is
how much of the time-course of the discourse is
required for a machine learner to identify the start
of a new discourse unit. To examine this question,
we vary two dimensions of the representation of
the instances for learning. The first is the number
of utterances around the location of the start of a
new discourse unit. The second is the set of fea-
tures to represent each instance, which as we will
see below, affects to some degree how many utter-
ances to include before and after the start of a new
discourse unit.
Four machine learning methods were tested us-
ing the Weka toolkit (Hall et al., 2009): Naive
Bayes, J48 Decision Trees, Logistic Regression
and Multilayer Perceptron. Of these, J48 had the
best and most consistent performance, which we
speculate is due to a combination of the small size
of the dataset, and non-linearity of the data. Be-
cause J48 is doing feature selection while building
the tree, it can identify different threshholds for
the same features, depending on the location in the
tree. All results reported here are for J48.
</bodyText>
<subsectionHeader confidence="0.997832">
4.1 Labels and Instance Spans
</subsectionHeader>
<bodyText confidence="0.999923333333333">
We refer to a sequence of utterances, and a poten-
tial location of the onset of a discourse unit relative
to that sequence, as a span. We varied the num-
</bodyText>
<figure confidence="0.929684416666667">
2
S2P2
S2P0 + First 2 utterances of segment
- Any other sequence of 2 utterances
1
S2P1 + Last utterance of one segment, first of next
- Any other sequence of 2 utterances
+ Last 2 utterances of segment
- Any other sequence of 2 utterances
0 S1P0
1
S1P1
</figure>
<page confidence="0.987499">
231
</page>
<bodyText confidence="0.999960052631579">
ber of utterances for each span from 1 to 4, and
the location of the start of a new unit to be at the
beginning of the first utterance, at the end of the
last utterance, or between any pair of utterances in
the span. For a single utterance, there will be two
types of instances, as shown in Figure 1. Each in-
stance type is represented as S&lt;N&gt;P&lt;M&gt; where
N is the number of utterances in the span and M is
how many utterances there are before the bound-
ary. S1P0 denotes size 1 spans with the boundary
at position 0; positively labeled instances repre-
sent the first utterance of a segment. S1P1 denotes
size 1 spans with the boundary at position 1; posi-
tively labeled instances represent the last utterance
of a segment. The experiments used all labelings
for spans from size 1 to 4, yielding 14 types of
instances. For multi-utterance spans that occur at
the beginning or end of a discourse, dummy utter-
ances are used to fill out the spans.
</bodyText>
<subsectionHeader confidence="0.576253">
4.2 Features
</subsectionHeader>
<bodyText confidence="0.99999262962963">
We use three sets of features. A set we refer to
as discourse features consists of a mixed set of
acoustic features and lexicogrammatical features
that have been associated with discourse structure,
such as discourse cue words (Hirschberg and Lit-
man, 1993). Table 1 lists the 35 discourse features.
The second set is a bag-of-words (BOW) vector
representation, and the third is the combination of
the discourse and BOW features. We used alterna-
tive sets of features on the assumption that the per-
formance of a machine learner across the differ-
ent instance spans will vary, depending on the as-
pects of the utterance that the features capture. We
see some expected differences in performance be-
tween the discourse features and BOW, with BOW
benefitting more than the discourse features from
longer spans. Unexpectedly, we see no gain in
performance from the combination of both feature
sets.
The discourse features consist of acoustic fea-
tures, pause features, word and utterance length
features, proper noun features and speaker change.
The acoustic features and the (unfilled) pause lo-
cation and duration features were extracted using
Praat, a cross-platform tool for speech analysis.
The features pertaining to filled pauses (e.g., um,
uh) were extracted from the transcripts.
</bodyText>
<subsectionHeader confidence="0.996453">
4.3 Conditions and Evaluation
</subsectionHeader>
<bodyText confidence="0.999968024390244">
The experimental conditions varied the feature set,
the selection of training data versus testing data,
and the fourteen kinds of instance spans and la-
bels. Three feature sets consisted of the discourse
features from Table 1 (discourse), bag-of-words
(bow), and the combination of the two (combo).
In all experiments, the data was randomly split
into 75% for training, and 25% for testing, us-
ing two methods to select instances. In random-
ization by dialog, all utterances from a single di-
alog were kept together and 75% of the dialogs
were selected for training. In randomization by
utterance, 75% of all utterances were randomly
selected for training, without regard to which di-
alog they came from. This was done to test the
hypothesis that the bow representation would be
more sensitive to changes of vocabulary across di-
alogs. The three feature sets, fourteen data rep-
resentations and two randomization methods yield
84 experimental conditions.
While N-fold cross-validation is a popular
method to estimate a classifier’s prediction error,
it is not a perfect substitute for isolating the train-
ing data from the test data (Ng, 1997). The cross-
validation estimate of prediction error is relatively
unbiased, but it can be highly variable (Efron and
Tibshirani, 1997)(Rodriguez et al., 2010). To
avoid the inherent risk of overfitting (Ng, 1997),
one recommendation is to use cross-validation to
compare models, and to reserve a test set to verify
that a selected classifier has superior generaliza-
tion (Rao and Fung, 2008). To assess whether per-
formance measures of different models are gen-
uinely different requires error bounds on the result,
which is not done with cross-validation. We per-
form train-test splits of the data to minimize over-
fitting, and bootstrap confidence intervals for each
classifier’s accuracy (and other metrics) in order to
measure the variance, and thereby assess whether
the performance error bounds of two conditions
are distinct.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999516">
Given that for this data, the rate of segment
boundary instances (positive labels) is about 10%,
a baseline classifier that always predicts a non-
segment will have about 90% overall accuracy.
The baseline column in Table 2 shows the aver-
age accuracy that would be achieved by this sim-
ple baseline on the test data for a given run, along
with the bootstrapped confidence interval for this
baseline over the 50 runs. In the 84 experiments,
the baseline ranged from 90% (+/- 1%) to 89% (+/-
</bodyText>
<page confidence="0.97751">
232
</page>
<table confidence="0.839221631578947">
Interaction feature
1 Speaker whether there is a speaker switch between preceding utterance and current utterance
Acoustic features
2 Pitch MIN Minimum pitch of the utterance
3 Pitch MAX Maximum pitch of the utterance
4 Pitch MEAN Mean pitch of the utterance
5 Pitch STDV Standard deviation of the pitch of the utterance
6 Pitch RANGE Maximim pitch of the utterance less the minimum pitch
7 Pitch CHANGE Pitch MEAN of the current utterance less the Pitch MEAN of the preceding utterance
8 Intensity MIN Minimum intensity of the utterance
9 Intensity MAX Maximum intensity of the utterance
10 Intensity MEAN Mean intensity of the utterance
11 Intensity STDV Standard deviation of the intensity of the utterance
12 Intensity RANGE Intensity MAX less Intensity MIN
13 Intensity CHANGE Intensity MEAN of the current utterance less Intensity MEAN of preceding utterance
14 LR1 Utterance duration
15 LR1 Normalized Utterance duration normalized by each speaker independently
Lexical features
16 LR2 1 Word count
17 LR2 2 Word count normalized by speaker
18 LR3 1 Words per second
19 LR3 2 Words per second by speaker
20 LR4 Average word length
21 LR5 Maximum word length
22 LR6 1 Average frequency of characters in the utterance
23 LR6 2 Number of low frequency characters
24 IR Number of content words
25 PN 1 Number of named entities
26 PN 2 Whether the utterance contains a new named entity
Pause features
27 Pause DURT total duration of all pauses
28 Pause RATIO proportion utterance consisting of pauses
29 FP1 Presence of a filled pause at the beginning of an utterance
30 FP2 Presence of a filled pause at the end of an utterance
31 FP3 Presence of a filled pause in the middle of an utterance
32 P1 Presence of a pause tag at the beginning of an utterance
33 P2 Presence of a pause tag at the end of an utterance
34 P3 Presence of a pause tag in the middle of an utterance
</table>
<tableCaption confidence="0.998463">
Table 1: Discourse Features
</tableCaption>
<bodyText confidence="0.99985895">
1%). Crucially, however, the simple baseline will
fail to identify any of the members of the positive
class. Though it is difficult to beat the baseline
on overall accuracy, the question addressed here
is what level of accuracy is achieved on the pos-
itive class, while remaining relatively consistent
with the baseline on overall accuracy. It should
be noted that accuracy on the positive class is the
same as recall, or sensitivity (the term used in the
epidemiological literature). The worst perform-
ing classifier among the 84 (disc/utterance/ S1P4)
achieves 83% (+/- 1%) accuracy overall, or below
the baseline by 6%, with 11% accuracy on the pos-
itive class, 100% of which is a gain over the base-
line. By this standard, the best classifier of the 84
conditions (bow/dial/S4P1) matches the baseline
on overall accuracy, and achieves 50% (+/- 5%)
accuracy on the positive class, which far exceeds
the baseline. About half of the experimental con-
ditions meet the baseline and achieve at least 25%
accuracy on the positive class.
Overall accuracy, and accuracy on the positive
class, measure prediction error, but can be supple-
mented with additional metrics that facilitate anal-
ysis of the nature and cost of error types. As a sup-
plementary metric, we report average F-measure,
the harmonic mean of recall and precision, due to
its familiarity, and because it provides a sense of
how often a classifier incorrectly predicts the pos-
itive class. An F-measure close to accuracy on the
positive class indicates that precision is about the
same as recall, while a relatively higher F-measure
indicates that the precision is even higher than the
F-measure, and the converse is true when the F-
measure is lower than accuracy on the positive
class. Table 2 shows 32 classifiers with the high-
est measures of accuracy, accuracy on the positive
class, and F-measure. The confidence intervals for
accuracy on the positive class and F-measure are
rather wide, compared to those for overall accu-
</bodyText>
<page confidence="0.996906">
233
</page>
<figure confidence="0.991806212121212">
Exp.
bow/dial/S4P1
bow/dial/S4P2
bow/utterance/S1P0
bow/utterance/S4P0
disc/dial/S2P1
bow/utterance/S4P3
combo/dial/S3P2
disc/dial/S4P3
combo/dial/S3P1
combo/dial/S4P2
combo/dial/S2P1
combo/dial/S4P3
disc/dial/S3P2
bow/utterance/S4P1
bow/dial/S4P3
disc/dial/S4P2
bow/dial/S1P0
combo/dial/S4P1
disc/dial/S3P1
bow/dial/S4P0
disc/dial/S4P1
bow/utterance/S4P2
combo/utterance/S2P0
disc/dial/S2P0
disc/utterance/S2P0
combo/utterance/S1P0
combo/utterance/S3P0
disc/utterance/S4P3
combo/utterance/S2P1
disc/utterance/S2P1
combo/utterance/S4P1
disc/utterance/S4P0
</figure>
<table confidence="0.997865235294118">
F (sd) &gt;Acc,_ &gt; F
Baseline (sd) Acc (sd) AccPos(Recall) (sd)
0.89 (+/-0.010) 0.89 (+/-0.009) 0.42 (+/-0.082) 0.28 (+/- 0.054) 22 11
0.90 (+/-0.013) 0.89 (+/-0.010) 0.39 (+/-0.071) 0.26 (+/- 0.064) 22 3
0.90 (+/-0.004) 0.90 (+/-0.005) 0.51 (+/-0.051) 0.26 (+/- 0.034) 30 11
0.89 (+/-0.005) 0.88 (+/-0.006) 0.43 (+/-0.049) 0.26 (+/- 0.040) 23 10
0.90 (+/-0.009) 0.87 (+/-0.009) 0.32 (+/-0.059) 0.26 (+/- 0.037) 4 10
0.89 (+/-0.006) 0.88 (+/-0.005) 0.41 (+/-0.050) 0.25 (+/- 0.027) 22 11
0.89 (+/-0.011) 0.86 (+/-0.010) 0.31 (+/-0.048) 0.25 (+/- 0.031) 7 10
0.90 (+/-0.008) 0.86 (+/-0.009) 0.30 (+/-0.041) 0.25 (+/- 0.030) 4 10
0.89 (+/-0.010) 0.86 (+/-0.011) 0.31 (+/-0.059) 0.25 (+/- 0.038) 3 10
0.89 (+/-0.013) 0.86 (+/-0.012) 0.30 (+/-0.044) 0.25 (+/- 0.031) 4 10
0.89 (+/-0.012) 0.87 (+/-0.010) 0.32 (+/-0.054) 0.25 (+/- 0.033) 7 10
0.90 (+/-0.007) 0.87 (+/-0.008) 0.29 (+/-0.044) 0.25 (+/- 0.035) 4 10
0.90 (+/-0.008) 0.87 (+/-0.008) 0.29 (+/-0.047) 0.25 (+/- 0.040) 3 10
0.90 (+/-0.005) 0.89 (+/-0.004) 0.40 (+/-0.053) 0.25 (+/- 0.020) 22 10
0.90 (+/-0.007) 0.89 (+/-0.009) 0.39 (+/-0.072) 0.25 (+/- 0.035) 22 10
0.90 (+/-0.009) 0.86 (+/-0.009) 0.28 (+/-0.042) 0.25 (+/- 0.030) 0 10
0.90 (+/-0.009) 0.89 (+/-0.009) 0.48 (+/-0.065) 0.24 (+/- 0.045) 28 0
0.90 (+/-0.010) 0.86 (+/-0.010) 0.28 (+/-0.045) 0.24 (+/- 0.034) 0 9
0.89 (+/-0.011) 0.86 (+/-0.010) 0.29 (+/-0.046) 0.24 (+/- 0.033) 2 9
0.90 (+/-0.009) 0.88 (+/-0.011) 0.37 (+/-0.031) 0.24 (+/- 0.040) 22 0
0.90 (+/-0.009) 0.86 (+/-0.008) 0.27 (+/-0.041) 0.23 (+/- 0.032) 0 3
0.89 (+/-0.007) 0.88 (+/-0.010) 0.39 (+/-0.044) 0.23 (+/- 0.033) 22 0
0.89 (+/-0.005) 0.86 (+/-0.009) 0.27 (+/-0.041) 0.21 (+/- 0.029) 0 0
0.89 (+/-0.010) 0.86 (+/-0.009) 0.27 (+/-0.047) 0.20 (+/- 0.027) 0 0
0.90 (+/-0.006) 0.86 (+/-0.008) 0.26 (+/-0.032) 0.20 (+/- 0.024) 0 0
0.89 (+/-0.005) 0.88 (+/-0.006) 0.31 (+/-0.041) 0.20 (+/- 0.026) 10 0
0.90 (+/-0.005) 0.86 (+/-0.008) 0.25 (+/-0.038) 0.20 (+/- 0.033) 0 0
0.89 (+/-0.006) 0.86 (+/-0.009) 0.24 (+/-0.043) 0.20 (+/- 0.033) 0 0
0.89 (+/-0.006) 0.86 (+/-0.008) 0.26 (+/-0.036) 0.20 (+/- 0.023) 0 0
0.89 (+/-0.005) 0.86 (+/-0.007) 0.26 (+/-0.032) 0.20 (+/- 0.022) 0 0
0.89 (+/-0.006) 0.85 (+/-0.008) 0.24 (+/-0.033) 0.20 (+/- 0.027) 0 0
0.89 (+/-0.006) 0.85 (+/-0.009) 0.24 (+/-0.034) 0.20 (+/- 0.024) 0 0
</table>
<tableCaption confidence="0.900702333333333">
Table 2: Classification performance (with standard deviations in parentheses) of the best 40% of 84
J48 models trained on 75% of the data and tested on the remaining 25%, with bootstrapped confidence
intervals from 50 trials each.
</tableCaption>
<bodyText confidence="0.999856977272727">
racy. To draw comparisons among the classifiers
that take into account this variance, the two right-
most columns of the table indicate for each clas-
sifier how many other classifiers in the same ta-
ble the current classifier surpasses on mean accu-
racy of the positive class, or on mean F-measure.
Here, to surpass another classifier means the lower
bound of its confidence interval surpasses the up-
per bounds of other classifiers’ confidence inter-
vals.
Table 2 shows that there is no one classifier that
surpasses all others on all measures. There are,
however, some clear trends. Regarding the num-
ber of utterances spanned by each data instance,
the table shows that of the 32 best performing clas-
sifiers, the majority (seventeen) have size 4 spans,
and all but three have spans longer than a single
utterance. This trend indicates that more context
leads to better accuracy overall and better accuracy
on the positive class. Regarding where the seg-
ment boundary is located relative to the span, the
majority of cases (twenty-two) locate the bound-
ary within the span, meaning that the span includes
one or more of the final utterances of a segment
and one or more of the initial utterances of the next
segment. The remaining cases involve spans that
include utterances only from the beginning of the
segment. There are no cases of higher perform-
ing classifiers that use spans from segment end-
ings. Among the classifiers in the top half of the
table, the best performing bow classifiers surpass
a larger number of the other classifiers on accu-
racy of the positive class. The best performing dis-
course or combination classifiers surpass a larger
number of other classifiers on F-measure. This
suggests that in general, the bow classifiers do bet-
ter on recall and the classifiers with discourse fea-
tures have higher precision.
The combination of BOW and discourse fea-
tures has a performance that differs little from the
discourse features alone, and does not do as well
as BOW S4P1. This result was unexpected, and
suggests that the bow and discourse feature sets
often identify nearly the same set of discourse
</bodyText>
<page confidence="0.995151">
234
</page>
<table confidence="0.998867368421053">
Discourse, Rand Dial, S4P3
Activity Type TP % FN %
Inform 7 (0.11) 56 (0.89)
Book Request 18 (0.32) 40 (0.68)
Librarian Proposal 4 (0.27) 11 (0.73)
Request-Action 0 (0.00) 6 (1.00)
Information-Request 6 (0.11) 47 (0.89)
Sidebar 1 (0.08) 11 (0.92)
Conventional 5 (0.17) 25 (0.83)
Total 37 (0.14) 230 (0.86)
BOW, Rand Dial, S4P2
Inform 7 (0.10) 70 (0.90)
Book Request 14 (0.20) 57 (0.80)
Librarian Proposal 1 (0.05) 20 (0.95)
Request-Action 0 (0.00) 5 (1.00)
Information-Request 8 (0.16) 42 (0.84)
Sidebar 0 (0.00) 13 (1.00)
Conventional 6 (0.23) 29 (0.77)
Total 37 (0.14) 230 (0.86)
</table>
<tableCaption confidence="0.999856">
Table 3: Error Analysis of the Positive Class
</tableCaption>
<bodyText confidence="0.999977270833333">
boundaries. Since the initial utterances of a seg-
ment seem to have features with greater predictive
power than the final utterances of a segment, and
since discourse cue words tend to occur in the first
utterance or so of a segment, it could be that dis-
course cue words explain the good performance
of both sets of features. This could be tested in fu-
ture work by restricting a BOW representation to
words other than discourse cue words.
To pursue in more detail the factors that influ-
ence accuracy on the positive class (recall), we
now turn to an error analysis of the kinds of dis-
course units associated with true positives versus
false negatives of the classifier’s confusion matrix.
Table 3 presents the results of an error analysis of
the two cells of the confusion matrix for a clas-
sifier’s results on the positive class, the true pos-
itives and the false negatives. We looked at the
breakdown of the seven kinds of discourse units
to see whether there were differences in the like-
lihood of a correct identification of a boundary,
depending on the kind of discourse unit in ques-
tion. Results are drawn from classifiers learned
under two conditions, S4P3 spans with discourse
features randomized by dialogue (disc/dial/S4P3)
and S4P3 spans with BOW features, randomized
by dialogue (bow/dial/S4P3). (Results from other
classifiers are very similar.) In both cases, Book-
Requests have a much higher probability of be-
ing among the true positives (32% for discourse,
20% for BOW) than for the positive class over-
all (14%). Conventional discourse units, where
the participants first make their greetings, or make
their final good byes, are also correctly identified
more often than the overall TP rate. Librarian Pro-
posals are identified well by the model using the
discourse features, but not by the one using the
BOW features. We speculate that this is because
Librarian Proposals typically present information
that is new to the discourse: often, the librarian
is making a suggestion to the patron based on in-
formation the librarian can see in the preference
field of the patron’s record, or in the patron’s past
borrowing behavior. We speculate that the vocab-
ulary in Librarian Proposals may be too variable
to be predictive. Information-Request units and
Inform units are also relatively difficult to identify
correctly.
</bodyText>
<sectionHeader confidence="0.999561" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999913">
The problem of identification of conversational ac-
tivities is a difficult one for machine processing for
many reasons. Like vision and speech, segmenta-
tion of the units is difficult because the units are
not discrete, objective, components of perception,
but instead are the result of abstraction. The exper-
iments presented here consider a novel explana-
tion for the difficulty of the task, which is that dis-
course units differ from each other regarding the
manner in which they evolve in time. The results
show that a data representation that includes utter-
ances from both the end of one unit and the begin-
ning of another improves performance. The tran-
sition between one conversational activity and an-
other takes place over the course of several utter-
ances, rather than occurring at an instant in time.
Error analysis indicates further that discourse units
that correspond to conversational activities with
clear end points that can be achieved have a higher
probability of being recognized correctly.
</bodyText>
<sectionHeader confidence="0.998587" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999104692307692">
John L. Austin. 1962. How to do Things with Words:
The William James Lectures delivered at Harvard
University in 1955. Clarendon Press, Oxford.
Satanjeev Banerjee and Alexander I. Rudnicky. 2006.
A texttiling based approach to topic boundary detec-
tion in meetings. Technical report, Department of
Computer Science, Carnegie Mellon University.
David R. Dowty. 1986. The effects of aspectual class
on the temporal structure of discourse: semantics or
pragmatics? Linguistics and Philosophy, 9(1):37–
61.
Bradley Efron and Robert Tibshirani. 1997. Im-
provements on cross-validation: The .632+ boot-
</reference>
<page confidence="0.975465">
235
</page>
<reference confidence="0.99963429245283">
strap method. Journal of the American Statistical
Association, 92(438):548–560, June.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP ’08), pages 334–
343. Association for Computational Linguistics.
Michel Galley, Kathleen McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse
segmentation of multi-party conversation. In Pro-
ceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 562–569,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175–204, July.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: An update.
SIGKDD Explorations, 11(1):10–18.
Marti A. Hearst. 1997. Texttiling: segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1).
Julia Hirschberg and Diane Litman. 1993. Empirical
studies on the disambiguation of cue phrases. Com-
putational Linguistics, 19:501–530.
Jun Hu, Rebecca J. Passonneau, and Owen Rambow.
2009. Contrasting the interaction structure of an
email and a telephone corpus: A machine learning
approach to annotation of dialogue function units.
In Proceedings of the SIGDIAL 2009 Conference,
pages 357–366, London, UK, September. Associa-
tion for Computational Linguistics.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications,
Beverly Hills, CA.
Igor Malioutov and Regina Barzilay. 2006. Min-
imum cut model for spoken lecture segmentation.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Lin-
guistics, ACL-44, pages 25–32, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Gabriel Murray, Steve Renals, and Jean Carletta. 2005.
Extractive summarization of meeting recordings. In
Proceedings of the 9th European Conference on
Speech Communication and Technology, pages 593–
596.
Andrew Y. Ng. 1997. Preventing overfitting of
cross-validation data. In Proceedings of the Four-
teenth International Conference on Machine Learn-
ing, ICML ’97, pages 245–253.
Nicolas Obin, Volker Dellwo, Anne Lacheret, and
Xavier Rodet. 2010. Expectations for discourse
genre identification: aprosodic study. In Inter-
speech, pages 3070–3073.
A.S. Park and J.R. Glass. 2008. Unsupervised pattern
discovery in speech. Audio, Speech, and Language
Processing, IEEE Transactions on, 16(1):186–197,
Jan.
Rebecca J. Passonneau and Diane J. Litman. 1997.
Discourse segmentation by human and automated
means. Computational Linguistics, 23(1):103–139,
March.
Rebecca J. Passonneau, Irene Alvarado, Phil Crone,
and Simon Jerome. 2011. Paradise-style evaluation
of a human-human library corpus. In Proceedings
of the SIGDIAL 2011 Conference, pages 325–331,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Matthew Purver, Thomas L. Griffiths, and Joshua B.
Kording, Konrad P. andTenenbaum. 2006. Unsu-
pervised topic modelling for multi-party spoken dis-
course. In Proceedings of the 44th annual meet-
ing of the Association for Computational Linguistics
(ACL-44), pages 17–24. Association for Computa-
tional Linguistics.
R. Bharat Rao and Glenn Fung. 2008. On the dangers
of cross-validation. an experimental evaluation. In
SDM, pages 588–596. SIAM.
Klaus Ries, Lori Levin, Liza Valle, Alon Lavie, and
Alex Waibel. 2000. Shallow discourse genre an-
notation in callhome spanish. In Proceedings of In-
ternational Conference on Language Resources and
Evaluation (LREC). European Language Resources
and Evaluation (ELRA).
Klaus Ries. 2002. Segmenting conversations by
topic, initiative, and style. In AnniR. Coden, EricW.
Brown, and Savitha Srinivasan, editors, Information
Retrieval Techniques for Speech Applications, vol-
ume 2273 of Lecture Notes in Computer Science,
pages 51–66. Springer Berlin Heidelberg.
J.D. Rodriguez, A. Perez, and J.A. Lozano. 2010. Sen-
sitivity analysis of k-fold cross validation in predic-
tion error estimation. Pattern Analysis and Machine
Intelligence, IEEE Transactions on, 32(3):569–575,
March.
Zeno Vendler. 1957. Verbs and times. Philosophical
Review, 66(2):143–160.
Nigel G. Ward and Karen A. Richart-Ruiz. 2013. Pat-
terns of importance variation in spoken dialog. In
SigDial.
Nigel G. Ward and Alejandro Vega. 2012. A bottom-
up exploration of the dimensions of dialog state in
spoken interaction. In SigDial.
</reference>
<page confidence="0.977396">
236
</page>
<reference confidence="0.980663666666667">
Nigel G. Ward and Steven D. Werner. 2013. Using
dialog-activity similarity for spoken information re-
trieval. In Interspeech.
</reference>
<page confidence="0.997369">
237
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.848923">
<title confidence="0.99928">Aspectual Properties of Conversational Activities</title>
<author confidence="0.99904">J Passonneau Guan Ho</author>
<affiliation confidence="0.873885">Columbia University, New York, NY, USA</affiliation>
<author confidence="0.9952">Yuan Du</author>
<email confidence="0.993955">ydu@fb.com</email>
<address confidence="0.992686">Facebook, New York, NY, USA</address>
<abstract confidence="0.99952645">Segmentation of spoken discourse into distinct conversational activities has been applied to broadcast news, meetings, monologs, and two-party dialogs. This paper considers the aspectual properties of discourse segments, meaning how they transpire in time. Classifiers were constructed to distinguish between segment boundaries and non-boundaries, where the sizes of utterance spans to represent data instances were varied, and the locations of segment boundaries relative to these instances. Classifier performance was better for representations that included the end of one discourse segment combined with the beginning of the next. In addition, classification accuracy was better for segments in which speakers accomplish goals with distinctive start and end points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John L Austin</author>
</authors>
<title>How to do Things with Words: The William James Lectures delivered at Harvard University in</title>
<date>1962</date>
<publisher>Clarendon Press,</publisher>
<location>Oxford.</location>
<contexts>
<context position="2741" citStr="Austin, 1962" startWordPosition="399" endWordPosition="400">ities, meaning how they transpire in time. We hypothesize that recognition of a transition to a new conversational activity depends on recognizing not only the start of a new activity but also the end of the preceding one, on the grounds that the relative contrast between endings and beginnings might matter as much or more than absolute characteristics consistent across all beginnings or all endings. We further hypothesize that transitions to certain kinds of conversational activity may be easier to detect than others. Following Austin’s view that speech constitutes action of different kinds (Austin, 1962), we assume that different kinds of communicative action have different ways of transpiring in time, just as other actions do. Conversational activities that address objective goals, for example, can have very well-demarcated beginnings and endings, as when two people choose a restaurant to go to for dinner. Conversational participants can, however, address goals that need not have a specific resolution, such as shared complaints about the lack of good Chinese restaurants. This distinction between different kinds of actions that speakers perform through their communicative behavior is analogou</context>
</contexts>
<marker>Austin, 1962</marker>
<rawString>John L. Austin. 1962. How to do Things with Words: The William James Lectures delivered at Harvard University in 1955. Clarendon Press, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alexander I Rudnicky</author>
</authors>
<title>A texttiling based approach to topic boundary detection in meetings.</title>
<date>2006</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="6643" citStr="Banerjee and Rudnicky, 2006" startWordPosition="983" endWordPosition="987"> and endings. 2 Related Work Segmentation of spoken language interaction into distinct discourse units has been applied to meetings as well as to two-party discourse using acoustic features, lexical features, and very heterogeneous features. In our previous work, we used a very heterogeneous set of features to segment monologues into units that had been identified by annotators as corresonding to distinct intentional units (Passonneau and Litman, 1997). Text tiling (Hearst, 1997) has been applied to segmentation of meetings into distinct agenda segments using both prior and following context (Banerjee and Rudnicky, 2006). Results had high precision and low recall. We also find that recall is more challenging than precision. Topic modeling methods have also been applied to the identification of topical segments in speech (Purver et al., 2006) (Eisenstein and Barzilay, 2008), with improvements over earlier work on the ICSI meeting corpus (Galley et al., 2003) (Malioutov and Barzilay, 2006). An analog of text tiling that uses acoustic patterns rather than lexical items has been applied to the segmentation of speech into stories using segmental dynamic time warping (SDTW) (Park and Glass, 2008). The method is bas</context>
</contexts>
<marker>Banerjee, Rudnicky, 2006</marker>
<rawString>Satanjeev Banerjee and Alexander I. Rudnicky. 2006. A texttiling based approach to topic boundary detection in meetings. Technical report, Department of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Dowty</author>
</authors>
<title>The effects of aspectual class on the temporal structure of discourse: semantics or pragmatics? Linguistics and Philosophy,</title>
<date>1986</date>
<volume>9</volume>
<issue>1</issue>
<pages>61</pages>
<contexts>
<context position="3530" citStr="Dowty, 1986" startWordPosition="514" endWordPosition="515">oals, for example, can have very well-demarcated beginnings and endings, as when two people choose a restaurant to go to for dinner. Conversational participants can, however, address goals that need not have a specific resolution, such as shared complaints about the lack of good Chinese restaurants. This distinction between different kinds of actions that speakers perform through their communicative behavior is analogous to the distinction in linguistic semantics pertaining to verbal aspect, between states, processes and transition events (or accomplishments and achievements) (Vendler, 1957) (Dowty, 1986). States (e.g., being at a standstill) have no perceptible change from moment to moment; processes (e.g., walking) have detectable differences in state from moment to moment with no clearly demarcated change of state during the process; transition events (e.g., starting to walk; walking to the end of the block) involve a transition from one state or process to another. To investigate the aspectual properties of discourse segments, we constructed classifiers to de228 Proceedings of the SIGDIAL 2014 Conference, pages 228–237, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computat</context>
</contexts>
<marker>Dowty, 1986</marker>
<rawString>David R. Dowty. 1986. The effects of aspectual class on the temporal structure of discourse: semantics or pragmatics? Linguistics and Philosophy, 9(1):37– 61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert Tibshirani</author>
</authors>
<title>Improvements on cross-validation: The .632+ bootstrap method.</title>
<date>1997</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>92</volume>
<issue>438</issue>
<contexts>
<context position="20633" citStr="Efron and Tibshirani, 1997" startWordPosition="3293" endWordPosition="3296">r training, without regard to which dialog they came from. This was done to test the hypothesis that the bow representation would be more sensitive to changes of vocabulary across dialogs. The three feature sets, fourteen data representations and two randomization methods yield 84 experimental conditions. While N-fold cross-validation is a popular method to estimate a classifier’s prediction error, it is not a perfect substitute for isolating the training data from the test data (Ng, 1997). The crossvalidation estimate of prediction error is relatively unbiased, but it can be highly variable (Efron and Tibshirani, 1997)(Rodriguez et al., 2010). To avoid the inherent risk of overfitting (Ng, 1997), one recommendation is to use cross-validation to compare models, and to reserve a test set to verify that a selected classifier has superior generalization (Rao and Fung, 2008). To assess whether performance measures of different models are genuinely different requires error bounds on the result, which is not done with cross-validation. We perform train-test splits of the data to minimize overfitting, and bootstrap confidence intervals for each classifier’s accuracy (and other metrics) in order to measure the varia</context>
</contexts>
<marker>Efron, Tibshirani, 1997</marker>
<rawString>Bradley Efron and Robert Tibshirani. 1997. Improvements on cross-validation: The .632+ bootstrap method. Journal of the American Statistical Association, 92(438):548–560, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>Regina Barzilay</author>
</authors>
<title>Bayesian unsupervised topic segmentation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’08),</booktitle>
<pages>334--343</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6900" citStr="Eisenstein and Barzilay, 2008" startWordPosition="1026" endWordPosition="1029">us work, we used a very heterogeneous set of features to segment monologues into units that had been identified by annotators as corresonding to distinct intentional units (Passonneau and Litman, 1997). Text tiling (Hearst, 1997) has been applied to segmentation of meetings into distinct agenda segments using both prior and following context (Banerjee and Rudnicky, 2006). Results had high precision and low recall. We also find that recall is more challenging than precision. Topic modeling methods have also been applied to the identification of topical segments in speech (Purver et al., 2006) (Eisenstein and Barzilay, 2008), with improvements over earlier work on the ICSI meeting corpus (Galley et al., 2003) (Malioutov and Barzilay, 2006). An analog of text tiling that uses acoustic patterns rather than lexical items has been applied to the segmentation of speech into stories using segmental dynamic time warping (SDTW) (Park and Glass, 2008). The method is based on the intuition of aligning utterances by similar acoustic patterns, possibly representing common words and phrases. Results on TDT2 Mandarin Broadcast News corpus were moderately good for short episodes with F=0.71 beating the baseline for lexical text</context>
</contexts>
<marker>Eisenstein, Barzilay, 2008</marker>
<rawString>Jacob Eisenstein and Regina Barzilay. 2008. Bayesian unsupervised topic segmentation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP ’08), pages 334– 343. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Eric FoslerLussier</author>
<author>Hongyan Jing</author>
</authors>
<title>Discourse segmentation of multi-party conversation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>562--569</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5145" citStr="Galley et al., 2003" startWordPosition="756" endWordPosition="759">econd, we considered different categories of segments, based on the speculation that segment transitions that are easier to recognize would be associated with conversational activities that have a well-demarcated event structure, in constrast to activities that involve goals to maintain or sustain aspects of interaction. The following section describes related work in this area, as well as the difficulties in achieving good performance. Most work on identification of discourse segments (or other forms of discourse structure in spoken interaction) depends on a prior phase of annotation (e.g., (Galley et al., 2003; Passonneau and Litman, 1997)). We studied a corpus of eighty-two transcribed and annotated telephone dialogues between library patrons and librarians that had been annotated with units analogous to speech acts, and subsequently annotated with discourse segments comprised of these units. The annotation yielded eight distinct kinds of discourse segment, where a segment results from a linear segmentation of a discourse into strictly sequential units. (While the segmentation is sequential, the units can have hierarchical relations.) We found that classifiers to detect segment boundaries performe</context>
<context position="6986" citStr="Galley et al., 2003" startWordPosition="1041" endWordPosition="1044">been identified by annotators as corresonding to distinct intentional units (Passonneau and Litman, 1997). Text tiling (Hearst, 1997) has been applied to segmentation of meetings into distinct agenda segments using both prior and following context (Banerjee and Rudnicky, 2006). Results had high precision and low recall. We also find that recall is more challenging than precision. Topic modeling methods have also been applied to the identification of topical segments in speech (Purver et al., 2006) (Eisenstein and Barzilay, 2008), with improvements over earlier work on the ICSI meeting corpus (Galley et al., 2003) (Malioutov and Barzilay, 2006). An analog of text tiling that uses acoustic patterns rather than lexical items has been applied to the segmentation of speech into stories using segmental dynamic time warping (SDTW) (Park and Glass, 2008). The method is based on the intuition of aligning utterances by similar acoustic patterns, possibly representing common words and phrases. Results on TDT2 Mandarin Broadcast News corpus were moderately good for short episodes with F=0.71 beating the baseline for lexical text tiling of 0.66, but poor on long episodes. An alternative method of relying solely on</context>
</contexts>
<marker>Galley, McKeown, FoslerLussier, Jing, 2003</marker>
<rawString>Michel Galley, Kathleen McKeown, Eric FoslerLussier, and Hongyan Jing. 2003. Discourse segmentation of multi-party conversation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 562–569, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara J Grosz</author>
<author>Candace L Sidner</author>
</authors>
<title>Attention, intentions, and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<volume>12</volume>
<issue>3</issue>
<contexts>
<context position="1259" citStr="Grosz and Sidner, 1986" startWordPosition="177" endWordPosition="180">f utterance spans to represent data instances were varied, and the locations of segment boundaries relative to these instances. Classifier performance was better for representations that included the end of one discourse segment combined with the beginning of the next. In addition, classification accuracy was better for segments in which speakers accomplish goals with distinctive start and end points. 1 Introduction People engage in dialogue to address a wide range of goals. It has long been observed that discourse can be structured into units that correspond to distinct goals and activities (Grosz and Sidner, 1986; Passonneau and Litman, 1997). This is conceptually distinct from structuring discourse into the topical units addressed in (Hearst, 1997). The ability to recognize where distinct activities occur in spoken discourse could support offline applications to spoken corpora such as search (Ward and Werner, 2013), summarization (Murray et al., 2005), and question answering. Further, a deeper understanding of the relation of conversational activities to observable features of utterance sequences could inform the design of interactive systems for online applications such as information gathering, ser</context>
</contexts>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara J. Grosz and Candace L. Sidner. 1986. Attention, intentions, and the structure of discourse. Computational Linguistics, 12(3):175–204, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: An update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="16213" citStr="Hall et al., 2009" startWordPosition="2543" endWordPosition="2546">f interest is how much of the time-course of the discourse is required for a machine learner to identify the start of a new discourse unit. To examine this question, we vary two dimensions of the representation of the instances for learning. The first is the number of utterances around the location of the start of a new discourse unit. The second is the set of features to represent each instance, which as we will see below, affects to some degree how many utterances to include before and after the start of a new discourse unit. Four machine learning methods were tested using the Weka toolkit (Hall et al., 2009): Naive Bayes, J48 Decision Trees, Logistic Regression and Multilayer Perceptron. Of these, J48 had the best and most consistent performance, which we speculate is due to a combination of the small size of the dataset, and non-linearity of the data. Because J48 is doing feature selection while building the tree, it can identify different threshholds for the same features, depending on the location in the tree. All results reported here are for J48. 4.1 Labels and Instance Spans We refer to a sequence of utterances, and a potential location of the onset of a discourse unit relative to that sequ</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The weka data mining software: An update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Texttiling: segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1398" citStr="Hearst, 1997" startWordPosition="199" endWordPosition="200">ce was better for representations that included the end of one discourse segment combined with the beginning of the next. In addition, classification accuracy was better for segments in which speakers accomplish goals with distinctive start and end points. 1 Introduction People engage in dialogue to address a wide range of goals. It has long been observed that discourse can be structured into units that correspond to distinct goals and activities (Grosz and Sidner, 1986; Passonneau and Litman, 1997). This is conceptually distinct from structuring discourse into the topical units addressed in (Hearst, 1997). The ability to recognize where distinct activities occur in spoken discourse could support offline applications to spoken corpora such as search (Ward and Werner, 2013), summarization (Murray et al., 2005), and question answering. Further, a deeper understanding of the relation of conversational activities to observable features of utterance sequences could inform the design of interactive systems for online applications such as information gathering, service requests, tutoring, and companionship. Automatic identification of such units, Emma Conner econner@oberlin.edu Oberlin College, Oberli</context>
<context position="6499" citStr="Hearst, 1997" startWordPosition="963" endWordPosition="964">Error analysis indicated that performance was better for boundaries that initiate conversational activities with clear beginnings and endings. 2 Related Work Segmentation of spoken language interaction into distinct discourse units has been applied to meetings as well as to two-party discourse using acoustic features, lexical features, and very heterogeneous features. In our previous work, we used a very heterogeneous set of features to segment monologues into units that had been identified by annotators as corresonding to distinct intentional units (Passonneau and Litman, 1997). Text tiling (Hearst, 1997) has been applied to segmentation of meetings into distinct agenda segments using both prior and following context (Banerjee and Rudnicky, 2006). Results had high precision and low recall. We also find that recall is more challenging than precision. Topic modeling methods have also been applied to the identification of topical segments in speech (Purver et al., 2006) (Eisenstein and Barzilay, 2008), with improvements over earlier work on the ICSI meeting corpus (Galley et al., 2003) (Malioutov and Barzilay, 2006). An analog of text tiling that uses acoustic patterns rather than lexical items h</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. Texttiling: segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hirschberg</author>
<author>Diane Litman</author>
</authors>
<title>Empirical studies on the disambiguation of cue phrases.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--501</pages>
<contexts>
<context position="18313" citStr="Hirschberg and Litman, 1993" startWordPosition="2922" endWordPosition="2926">gment. S1P1 denotes size 1 spans with the boundary at position 1; positively labeled instances represent the last utterance of a segment. The experiments used all labelings for spans from size 1 to 4, yielding 14 types of instances. For multi-utterance spans that occur at the beginning or end of a discourse, dummy utterances are used to fill out the spans. 4.2 Features We use three sets of features. A set we refer to as discourse features consists of a mixed set of acoustic features and lexicogrammatical features that have been associated with discourse structure, such as discourse cue words (Hirschberg and Litman, 1993). Table 1 lists the 35 discourse features. The second set is a bag-of-words (BOW) vector representation, and the third is the combination of the discourse and BOW features. We used alternative sets of features on the assumption that the performance of a machine learner across the different instance spans will vary, depending on the aspects of the utterance that the features capture. We see some expected differences in performance between the discourse features and BOW, with BOW benefitting more than the discourse features from longer spans. Unexpectedly, we see no gain in performance from the </context>
</contexts>
<marker>Hirschberg, Litman, 1993</marker>
<rawString>Julia Hirschberg and Diane Litman. 1993. Empirical studies on the disambiguation of cue phrases. Computational Linguistics, 19:501–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hu</author>
<author>Rebecca J Passonneau</author>
<author>Owen Rambow</author>
</authors>
<title>Contrasting the interaction structure of an email and a telephone corpus: A machine learning approach to annotation of dialogue function units.</title>
<date>2009</date>
<booktitle>In Proceedings of the SIGDIAL 2009 Conference,</booktitle>
<pages>357--366</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>London, UK,</location>
<contexts>
<context position="9079" citStr="Hu et al., 2009" startWordPosition="1383" endWordPosition="1386">otations The corpus consists of recordings, transcripts and annotations on the transcripts of a set of 82 calls recorded in 2005 between patrons of the Andrew Heiskell Braille and Talking Book Library of New York City.1 An annotation for dialog acts with a 1The audio files and transcripts are available for download from the Columbia University Data Commons. The annotations and raw features will be released in the near future. 229 reduced set of dialog act types and adjacency pair relations (Dialogue Function Units, DFUs) was developed, originally for comparison of dialogues across modalities (Hu et al., 2009). A subsequent phase of annotation at the discourse level that makes use of the dialog act annotation was later applied. This later annotation, referred to as Task Success and Cost Annotation (TSCA), was aimed at identifying individual dialog tasks analogous to those carried out by spoken dialog systems, to facilitate comparison of human-human dialog with human-machine dialog. Interannotator reliability of both annotations was measured using Krippendorff’s alpha (Krippendorff, 1980) at levels of 0.66 and above for individual dialogues (Passonneau et al., 2011). The corpus consists of 24,760 wo</context>
</contexts>
<marker>Hu, Passonneau, Rambow, 2009</marker>
<rawString>Jun Hu, Rebecca J. Passonneau, and Owen Rambow. 2009. Contrasting the interaction structure of an email and a telephone corpus: A machine learning approach to annotation of dialogue function units. In Proceedings of the SIGDIAL 2009 Conference, pages 357–366, London, UK, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage Publications,</title>
<date>1980</date>
<location>Beverly Hills, CA.</location>
<contexts>
<context position="9566" citStr="Krippendorff, 1980" startWordPosition="1456" endWordPosition="1457"> relations (Dialogue Function Units, DFUs) was developed, originally for comparison of dialogues across modalities (Hu et al., 2009). A subsequent phase of annotation at the discourse level that makes use of the dialog act annotation was later applied. This later annotation, referred to as Task Success and Cost Annotation (TSCA), was aimed at identifying individual dialog tasks analogous to those carried out by spoken dialog systems, to facilitate comparison of human-human dialog with human-machine dialog. Interannotator reliability of both annotations was measured using Krippendorff’s alpha (Krippendorff, 1980) at levels of 0.66 and above for individual dialogues (Passonneau et al., 2011). The corpus consists of 24,760 words, or 302 words per dialog. Briefly, the second phase of annotation involved grouping DFUs into larger sequences in which the participants continued to pursue a single coordinated activity, and labeling the large discourse units for their discourse function. The human annotation instructions avoided reference to overt signals of dialog structure. Rather, annotators were asked to judge the semantic and pragmatic functions of utterances. The annotations have been described in previo</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Klaus Krippendorff. 1980. Content Analysis: An Introduction to Its Methodology. Sage Publications, Beverly Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Malioutov</author>
<author>Regina Barzilay</author>
</authors>
<title>Minimum cut model for spoken lecture segmentation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7017" citStr="Malioutov and Barzilay, 2006" startWordPosition="1045" endWordPosition="1049">otators as corresonding to distinct intentional units (Passonneau and Litman, 1997). Text tiling (Hearst, 1997) has been applied to segmentation of meetings into distinct agenda segments using both prior and following context (Banerjee and Rudnicky, 2006). Results had high precision and low recall. We also find that recall is more challenging than precision. Topic modeling methods have also been applied to the identification of topical segments in speech (Purver et al., 2006) (Eisenstein and Barzilay, 2008), with improvements over earlier work on the ICSI meeting corpus (Galley et al., 2003) (Malioutov and Barzilay, 2006). An analog of text tiling that uses acoustic patterns rather than lexical items has been applied to the segmentation of speech into stories using segmental dynamic time warping (SDTW) (Park and Glass, 2008). The method is based on the intuition of aligning utterances by similar acoustic patterns, possibly representing common words and phrases. Results on TDT2 Mandarin Broadcast News corpus were moderately good for short episodes with F=0.71 beating the baseline for lexical text tiling of 0.66, but poor on long episodes. An alternative method of relying solely on acoustic information has been </context>
</contexts>
<marker>Malioutov, Barzilay, 2006</marker>
<rawString>Igor Malioutov and Regina Barzilay. 2006. Minimum cut model for spoken lecture segmentation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, ACL-44, pages 25–32, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Steve Renals</author>
<author>Jean Carletta</author>
</authors>
<title>Extractive summarization of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proceedings of the 9th European Conference on Speech Communication and Technology,</booktitle>
<pages>593--596</pages>
<contexts>
<context position="1605" citStr="Murray et al., 2005" startWordPosition="229" endWordPosition="232"> accomplish goals with distinctive start and end points. 1 Introduction People engage in dialogue to address a wide range of goals. It has long been observed that discourse can be structured into units that correspond to distinct goals and activities (Grosz and Sidner, 1986; Passonneau and Litman, 1997). This is conceptually distinct from structuring discourse into the topical units addressed in (Hearst, 1997). The ability to recognize where distinct activities occur in spoken discourse could support offline applications to spoken corpora such as search (Ward and Werner, 2013), summarization (Murray et al., 2005), and question answering. Further, a deeper understanding of the relation of conversational activities to observable features of utterance sequences could inform the design of interactive systems for online applications such as information gathering, service requests, tutoring, and companionship. Automatic identification of such units, Emma Conner econner@oberlin.edu Oberlin College, Oberlin, OH, USA however, has been difficult to achieve. This paper considers the aspectual properties of speakers’ conversational activities, meaning how they transpire in time. We hypothesize that recognition of</context>
</contexts>
<marker>Murray, Renals, Carletta, 2005</marker>
<rawString>Gabriel Murray, Steve Renals, and Jean Carletta. 2005. Extractive summarization of meeting recordings. In Proceedings of the 9th European Conference on Speech Communication and Technology, pages 593– 596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
</authors>
<title>Preventing overfitting of cross-validation data.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97,</booktitle>
<pages>245--253</pages>
<contexts>
<context position="20500" citStr="Ng, 1997" startWordPosition="3275" endWordPosition="3276"> dialogs were selected for training. In randomization by utterance, 75% of all utterances were randomly selected for training, without regard to which dialog they came from. This was done to test the hypothesis that the bow representation would be more sensitive to changes of vocabulary across dialogs. The three feature sets, fourteen data representations and two randomization methods yield 84 experimental conditions. While N-fold cross-validation is a popular method to estimate a classifier’s prediction error, it is not a perfect substitute for isolating the training data from the test data (Ng, 1997). The crossvalidation estimate of prediction error is relatively unbiased, but it can be highly variable (Efron and Tibshirani, 1997)(Rodriguez et al., 2010). To avoid the inherent risk of overfitting (Ng, 1997), one recommendation is to use cross-validation to compare models, and to reserve a test set to verify that a selected classifier has superior generalization (Rao and Fung, 2008). To assess whether performance measures of different models are genuinely different requires error bounds on the result, which is not done with cross-validation. We perform train-test splits of the data to mini</context>
</contexts>
<marker>Ng, 1997</marker>
<rawString>Andrew Y. Ng. 1997. Preventing overfitting of cross-validation data. In Proceedings of the Fourteenth International Conference on Machine Learning, ICML ’97, pages 245–253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Obin</author>
<author>Volker Dellwo</author>
<author>Anne Lacheret</author>
<author>Xavier Rodet</author>
</authors>
<title>Expectations for discourse genre identification: aprosodic study.</title>
<date>2010</date>
<booktitle>In Interspeech,</booktitle>
<pages>3070--3073</pages>
<contexts>
<context position="8304" citStr="Obin et al., 2010" startWordPosition="1255" endWordPosition="1258"> Richart-Ruiz, 2013). Four basic classes of prosodic features derived from PCA were used (Ward and Vega, 2012): volume, pitch height, pitch range and speaking rate cross various widths of time intervals. The data was labeled by annotators using an importance scale of 1 to 5, and linear regression was used to predict the label for instances consisting of frames. The method performed well with a correlation of 0.82 and mean average error of 0.75 (5-fold cross validation). The identification of different kinds of units in discourse is somewhat related to the notion of genre identification, e.g. (Obin et al., 2010) (Ries et al., 2000). Results from this area have been applied to segmentation of conversation by a combination of topic and style (Ries, 2002). 3 Data and Annotations The corpus consists of recordings, transcripts and annotations on the transcripts of a set of 82 calls recorded in 2005 between patrons of the Andrew Heiskell Braille and Talking Book Library of New York City.1 An annotation for dialog acts with a 1The audio files and transcripts are available for download from the Columbia University Data Commons. The annotations and raw features will be released in the near future. 229 reduced</context>
</contexts>
<marker>Obin, Dellwo, Lacheret, Rodet, 2010</marker>
<rawString>Nicolas Obin, Volker Dellwo, Anne Lacheret, and Xavier Rodet. 2010. Expectations for discourse genre identification: aprosodic study. In Interspeech, pages 3070–3073.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Park</author>
<author>J R Glass</author>
</authors>
<title>Unsupervised pattern discovery in speech.</title>
<date>2008</date>
<journal>Audio, Speech, and Language Processing, IEEE Transactions on,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="7224" citStr="Park and Glass, 2008" startWordPosition="1081" endWordPosition="1084">ng context (Banerjee and Rudnicky, 2006). Results had high precision and low recall. We also find that recall is more challenging than precision. Topic modeling methods have also been applied to the identification of topical segments in speech (Purver et al., 2006) (Eisenstein and Barzilay, 2008), with improvements over earlier work on the ICSI meeting corpus (Galley et al., 2003) (Malioutov and Barzilay, 2006). An analog of text tiling that uses acoustic patterns rather than lexical items has been applied to the segmentation of speech into stories using segmental dynamic time warping (SDTW) (Park and Glass, 2008). The method is based on the intuition of aligning utterances by similar acoustic patterns, possibly representing common words and phrases. Results on TDT2 Mandarin Broadcast News corpus were moderately good for short episodes with F=0.71 beating the baseline for lexical text tiling of 0.66, but poor on long episodes. An alternative method of relying solely on acoustic information has been applied to importance prediction at a very fine granularity (Ward and Richart-Ruiz, 2013). Four basic classes of prosodic features derived from PCA were used (Ward and Vega, 2012): volume, pitch height, pitc</context>
</contexts>
<marker>Park, Glass, 2008</marker>
<rawString>A.S. Park and J.R. Glass. 2008. Unsupervised pattern discovery in speech. Audio, Speech, and Language Processing, IEEE Transactions on, 16(1):186–197, Jan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Discourse segmentation by human and automated means.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="1289" citStr="Passonneau and Litman, 1997" startWordPosition="181" endWordPosition="184">resent data instances were varied, and the locations of segment boundaries relative to these instances. Classifier performance was better for representations that included the end of one discourse segment combined with the beginning of the next. In addition, classification accuracy was better for segments in which speakers accomplish goals with distinctive start and end points. 1 Introduction People engage in dialogue to address a wide range of goals. It has long been observed that discourse can be structured into units that correspond to distinct goals and activities (Grosz and Sidner, 1986; Passonneau and Litman, 1997). This is conceptually distinct from structuring discourse into the topical units addressed in (Hearst, 1997). The ability to recognize where distinct activities occur in spoken discourse could support offline applications to spoken corpora such as search (Ward and Werner, 2013), summarization (Murray et al., 2005), and question answering. Further, a deeper understanding of the relation of conversational activities to observable features of utterance sequences could inform the design of interactive systems for online applications such as information gathering, service requests, tutoring, and c</context>
<context position="5175" citStr="Passonneau and Litman, 1997" startWordPosition="760" endWordPosition="764">different categories of segments, based on the speculation that segment transitions that are easier to recognize would be associated with conversational activities that have a well-demarcated event structure, in constrast to activities that involve goals to maintain or sustain aspects of interaction. The following section describes related work in this area, as well as the difficulties in achieving good performance. Most work on identification of discourse segments (or other forms of discourse structure in spoken interaction) depends on a prior phase of annotation (e.g., (Galley et al., 2003; Passonneau and Litman, 1997)). We studied a corpus of eighty-two transcribed and annotated telephone dialogues between library patrons and librarians that had been annotated with units analogous to speech acts, and subsequently annotated with discourse segments comprised of these units. The annotation yielded eight distinct kinds of discourse segment, where a segment results from a linear segmentation of a discourse into strictly sequential units. (While the segmentation is sequential, the units can have hierarchical relations.) We found that classifiers to detect segment boundaries performed best with boundaries represe</context>
<context position="6471" citStr="Passonneau and Litman, 1997" startWordPosition="957" endWordPosition="960">one segment and the beginning of the next. Error analysis indicated that performance was better for boundaries that initiate conversational activities with clear beginnings and endings. 2 Related Work Segmentation of spoken language interaction into distinct discourse units has been applied to meetings as well as to two-party discourse using acoustic features, lexical features, and very heterogeneous features. In our previous work, we used a very heterogeneous set of features to segment monologues into units that had been identified by annotators as corresonding to distinct intentional units (Passonneau and Litman, 1997). Text tiling (Hearst, 1997) has been applied to segmentation of meetings into distinct agenda segments using both prior and following context (Banerjee and Rudnicky, 2006). Results had high precision and low recall. We also find that recall is more challenging than precision. Topic modeling methods have also been applied to the identification of topical segments in speech (Purver et al., 2006) (Eisenstein and Barzilay, 2008), with improvements over earlier work on the ICSI meeting corpus (Galley et al., 2003) (Malioutov and Barzilay, 2006). An analog of text tiling that uses acoustic patterns</context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>Rebecca J. Passonneau and Diane J. Litman. 1997. Discourse segmentation by human and automated means. Computational Linguistics, 23(1):103–139, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Irene Alvarado</author>
<author>Phil Crone</author>
<author>Simon Jerome</author>
</authors>
<title>Paradise-style evaluation of a human-human library corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the SIGDIAL 2011 Conference,</booktitle>
<pages>325--331</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon,</location>
<contexts>
<context position="9645" citStr="Passonneau et al., 2011" startWordPosition="1467" endWordPosition="1470">omparison of dialogues across modalities (Hu et al., 2009). A subsequent phase of annotation at the discourse level that makes use of the dialog act annotation was later applied. This later annotation, referred to as Task Success and Cost Annotation (TSCA), was aimed at identifying individual dialog tasks analogous to those carried out by spoken dialog systems, to facilitate comparison of human-human dialog with human-machine dialog. Interannotator reliability of both annotations was measured using Krippendorff’s alpha (Krippendorff, 1980) at levels of 0.66 and above for individual dialogues (Passonneau et al., 2011). The corpus consists of 24,760 words, or 302 words per dialog. Briefly, the second phase of annotation involved grouping DFUs into larger sequences in which the participants continued to pursue a single coordinated activity, and labeling the large discourse units for their discourse function. The human annotation instructions avoided reference to overt signals of dialog structure. Rather, annotators were asked to judge the semantic and pragmatic functions of utterances. The annotations have been described in previous work (Hu et al., 2009; Passonneau et al., 2011); the annotation guidelines a</context>
</contexts>
<marker>Passonneau, Alvarado, Crone, Jerome, 2011</marker>
<rawString>Rebecca J. Passonneau, Irene Alvarado, Phil Crone, and Simon Jerome. 2011. Paradise-style evaluation of a human-human library corpus. In Proceedings of the SIGDIAL 2011 Conference, pages 325–331, Portland, Oregon, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Thomas L Griffiths</author>
<author>Joshua B Kording</author>
<author>Konrad P andTenenbaum</author>
</authors>
<title>Unsupervised topic modelling for multi-party spoken discourse.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th annual meeting of the Association for Computational Linguistics (ACL-44),</booktitle>
<pages>17--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6868" citStr="Purver et al., 2006" startWordPosition="1022" endWordPosition="1025">eatures. In our previous work, we used a very heterogeneous set of features to segment monologues into units that had been identified by annotators as corresonding to distinct intentional units (Passonneau and Litman, 1997). Text tiling (Hearst, 1997) has been applied to segmentation of meetings into distinct agenda segments using both prior and following context (Banerjee and Rudnicky, 2006). Results had high precision and low recall. We also find that recall is more challenging than precision. Topic modeling methods have also been applied to the identification of topical segments in speech (Purver et al., 2006) (Eisenstein and Barzilay, 2008), with improvements over earlier work on the ICSI meeting corpus (Galley et al., 2003) (Malioutov and Barzilay, 2006). An analog of text tiling that uses acoustic patterns rather than lexical items has been applied to the segmentation of speech into stories using segmental dynamic time warping (SDTW) (Park and Glass, 2008). The method is based on the intuition of aligning utterances by similar acoustic patterns, possibly representing common words and phrases. Results on TDT2 Mandarin Broadcast News corpus were moderately good for short episodes with F=0.71 beati</context>
</contexts>
<marker>Purver, Griffiths, Kording, andTenenbaum, 2006</marker>
<rawString>Matthew Purver, Thomas L. Griffiths, and Joshua B. Kording, Konrad P. andTenenbaum. 2006. Unsupervised topic modelling for multi-party spoken discourse. In Proceedings of the 44th annual meeting of the Association for Computational Linguistics (ACL-44), pages 17–24. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bharat Rao</author>
<author>Glenn Fung</author>
</authors>
<title>On the dangers of cross-validation. an experimental evaluation.</title>
<date>2008</date>
<booktitle>In SDM,</booktitle>
<pages>588--596</pages>
<publisher>SIAM.</publisher>
<contexts>
<context position="20889" citStr="Rao and Fung, 2008" startWordPosition="3334" endWordPosition="3337"> methods yield 84 experimental conditions. While N-fold cross-validation is a popular method to estimate a classifier’s prediction error, it is not a perfect substitute for isolating the training data from the test data (Ng, 1997). The crossvalidation estimate of prediction error is relatively unbiased, but it can be highly variable (Efron and Tibshirani, 1997)(Rodriguez et al., 2010). To avoid the inherent risk of overfitting (Ng, 1997), one recommendation is to use cross-validation to compare models, and to reserve a test set to verify that a selected classifier has superior generalization (Rao and Fung, 2008). To assess whether performance measures of different models are genuinely different requires error bounds on the result, which is not done with cross-validation. We perform train-test splits of the data to minimize overfitting, and bootstrap confidence intervals for each classifier’s accuracy (and other metrics) in order to measure the variance, and thereby assess whether the performance error bounds of two conditions are distinct. 5 Results Given that for this data, the rate of segment boundary instances (positive labels) is about 10%, a baseline classifier that always predicts a nonsegment </context>
</contexts>
<marker>Rao, Fung, 2008</marker>
<rawString>R. Bharat Rao and Glenn Fung. 2008. On the dangers of cross-validation. an experimental evaluation. In SDM, pages 588–596. SIAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Ries</author>
<author>Lori Levin</author>
<author>Liza Valle</author>
<author>Alon Lavie</author>
<author>Alex Waibel</author>
</authors>
<title>Shallow discourse genre annotation in callhome spanish.</title>
<date>2000</date>
<booktitle>In Proceedings of International Conference on Language Resources and Evaluation (LREC). European Language Resources and Evaluation (ELRA).</booktitle>
<contexts>
<context position="8324" citStr="Ries et al., 2000" startWordPosition="1259" endWordPosition="1262">. Four basic classes of prosodic features derived from PCA were used (Ward and Vega, 2012): volume, pitch height, pitch range and speaking rate cross various widths of time intervals. The data was labeled by annotators using an importance scale of 1 to 5, and linear regression was used to predict the label for instances consisting of frames. The method performed well with a correlation of 0.82 and mean average error of 0.75 (5-fold cross validation). The identification of different kinds of units in discourse is somewhat related to the notion of genre identification, e.g. (Obin et al., 2010) (Ries et al., 2000). Results from this area have been applied to segmentation of conversation by a combination of topic and style (Ries, 2002). 3 Data and Annotations The corpus consists of recordings, transcripts and annotations on the transcripts of a set of 82 calls recorded in 2005 between patrons of the Andrew Heiskell Braille and Talking Book Library of New York City.1 An annotation for dialog acts with a 1The audio files and transcripts are available for download from the Columbia University Data Commons. The annotations and raw features will be released in the near future. 229 reduced set of dialog act t</context>
</contexts>
<marker>Ries, Levin, Valle, Lavie, Waibel, 2000</marker>
<rawString>Klaus Ries, Lori Levin, Liza Valle, Alon Lavie, and Alex Waibel. 2000. Shallow discourse genre annotation in callhome spanish. In Proceedings of International Conference on Language Resources and Evaluation (LREC). European Language Resources and Evaluation (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Ries</author>
</authors>
<title>Segmenting conversations by topic, initiative, and style.</title>
<date>2002</date>
<booktitle>Information Retrieval Techniques for Speech Applications,</booktitle>
<volume>2273</volume>
<pages>51--66</pages>
<editor>In AnniR. Coden, EricW. Brown, and Savitha Srinivasan, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="8447" citStr="Ries, 2002" startWordPosition="1283" endWordPosition="1284">speaking rate cross various widths of time intervals. The data was labeled by annotators using an importance scale of 1 to 5, and linear regression was used to predict the label for instances consisting of frames. The method performed well with a correlation of 0.82 and mean average error of 0.75 (5-fold cross validation). The identification of different kinds of units in discourse is somewhat related to the notion of genre identification, e.g. (Obin et al., 2010) (Ries et al., 2000). Results from this area have been applied to segmentation of conversation by a combination of topic and style (Ries, 2002). 3 Data and Annotations The corpus consists of recordings, transcripts and annotations on the transcripts of a set of 82 calls recorded in 2005 between patrons of the Andrew Heiskell Braille and Talking Book Library of New York City.1 An annotation for dialog acts with a 1The audio files and transcripts are available for download from the Columbia University Data Commons. The annotations and raw features will be released in the near future. 229 reduced set of dialog act types and adjacency pair relations (Dialogue Function Units, DFUs) was developed, originally for comparison of dialogues acr</context>
</contexts>
<marker>Ries, 2002</marker>
<rawString>Klaus Ries. 2002. Segmenting conversations by topic, initiative, and style. In AnniR. Coden, EricW. Brown, and Savitha Srinivasan, editors, Information Retrieval Techniques for Speech Applications, volume 2273 of Lecture Notes in Computer Science, pages 51–66. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Rodriguez</author>
<author>A Perez</author>
<author>J A Lozano</author>
</authors>
<title>Sensitivity analysis of k-fold cross validation in prediction error estimation. Pattern Analysis and Machine Intelligence,</title>
<date>2010</date>
<journal>IEEE Transactions on,</journal>
<volume>32</volume>
<issue>3</issue>
<contexts>
<context position="20657" citStr="Rodriguez et al., 2010" startWordPosition="3296" endWordPosition="3299">o which dialog they came from. This was done to test the hypothesis that the bow representation would be more sensitive to changes of vocabulary across dialogs. The three feature sets, fourteen data representations and two randomization methods yield 84 experimental conditions. While N-fold cross-validation is a popular method to estimate a classifier’s prediction error, it is not a perfect substitute for isolating the training data from the test data (Ng, 1997). The crossvalidation estimate of prediction error is relatively unbiased, but it can be highly variable (Efron and Tibshirani, 1997)(Rodriguez et al., 2010). To avoid the inherent risk of overfitting (Ng, 1997), one recommendation is to use cross-validation to compare models, and to reserve a test set to verify that a selected classifier has superior generalization (Rao and Fung, 2008). To assess whether performance measures of different models are genuinely different requires error bounds on the result, which is not done with cross-validation. We perform train-test splits of the data to minimize overfitting, and bootstrap confidence intervals for each classifier’s accuracy (and other metrics) in order to measure the variance, and thereby assess </context>
</contexts>
<marker>Rodriguez, Perez, Lozano, 2010</marker>
<rawString>J.D. Rodriguez, A. Perez, and J.A. Lozano. 2010. Sensitivity analysis of k-fold cross validation in prediction error estimation. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 32(3):569–575, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Vendler</author>
</authors>
<title>Verbs and times.</title>
<date>1957</date>
<journal>Philosophical Review,</journal>
<volume>66</volume>
<issue>2</issue>
<contexts>
<context position="3516" citStr="Vendler, 1957" startWordPosition="512" endWordPosition="513">ress objective goals, for example, can have very well-demarcated beginnings and endings, as when two people choose a restaurant to go to for dinner. Conversational participants can, however, address goals that need not have a specific resolution, such as shared complaints about the lack of good Chinese restaurants. This distinction between different kinds of actions that speakers perform through their communicative behavior is analogous to the distinction in linguistic semantics pertaining to verbal aspect, between states, processes and transition events (or accomplishments and achievements) (Vendler, 1957) (Dowty, 1986). States (e.g., being at a standstill) have no perceptible change from moment to moment; processes (e.g., walking) have detectable differences in state from moment to moment with no clearly demarcated change of state during the process; transition events (e.g., starting to walk; walking to the end of the block) involve a transition from one state or process to another. To investigate the aspectual properties of discourse segments, we constructed classifiers to de228 Proceedings of the SIGDIAL 2014 Conference, pages 228–237, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Associatio</context>
</contexts>
<marker>Vendler, 1957</marker>
<rawString>Zeno Vendler. 1957. Verbs and times. Philosophical Review, 66(2):143–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel G Ward</author>
<author>Karen A Richart-Ruiz</author>
</authors>
<title>Patterns of importance variation in spoken dialog.</title>
<date>2013</date>
<booktitle>In SigDial.</booktitle>
<contexts>
<context position="7706" citStr="Ward and Richart-Ruiz, 2013" startWordPosition="1156" endWordPosition="1159"> than lexical items has been applied to the segmentation of speech into stories using segmental dynamic time warping (SDTW) (Park and Glass, 2008). The method is based on the intuition of aligning utterances by similar acoustic patterns, possibly representing common words and phrases. Results on TDT2 Mandarin Broadcast News corpus were moderately good for short episodes with F=0.71 beating the baseline for lexical text tiling of 0.66, but poor on long episodes. An alternative method of relying solely on acoustic information has been applied to importance prediction at a very fine granularity (Ward and Richart-Ruiz, 2013). Four basic classes of prosodic features derived from PCA were used (Ward and Vega, 2012): volume, pitch height, pitch range and speaking rate cross various widths of time intervals. The data was labeled by annotators using an importance scale of 1 to 5, and linear regression was used to predict the label for instances consisting of frames. The method performed well with a correlation of 0.82 and mean average error of 0.75 (5-fold cross validation). The identification of different kinds of units in discourse is somewhat related to the notion of genre identification, e.g. (Obin et al., 2010) (</context>
</contexts>
<marker>Ward, Richart-Ruiz, 2013</marker>
<rawString>Nigel G. Ward and Karen A. Richart-Ruiz. 2013. Patterns of importance variation in spoken dialog. In SigDial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel G Ward</author>
<author>Alejandro Vega</author>
</authors>
<title>A bottomup exploration of the dimensions of dialog state in spoken interaction.</title>
<date>2012</date>
<booktitle>In SigDial.</booktitle>
<contexts>
<context position="7796" citStr="Ward and Vega, 2012" startWordPosition="1171" endWordPosition="1174">namic time warping (SDTW) (Park and Glass, 2008). The method is based on the intuition of aligning utterances by similar acoustic patterns, possibly representing common words and phrases. Results on TDT2 Mandarin Broadcast News corpus were moderately good for short episodes with F=0.71 beating the baseline for lexical text tiling of 0.66, but poor on long episodes. An alternative method of relying solely on acoustic information has been applied to importance prediction at a very fine granularity (Ward and Richart-Ruiz, 2013). Four basic classes of prosodic features derived from PCA were used (Ward and Vega, 2012): volume, pitch height, pitch range and speaking rate cross various widths of time intervals. The data was labeled by annotators using an importance scale of 1 to 5, and linear regression was used to predict the label for instances consisting of frames. The method performed well with a correlation of 0.82 and mean average error of 0.75 (5-fold cross validation). The identification of different kinds of units in discourse is somewhat related to the notion of genre identification, e.g. (Obin et al., 2010) (Ries et al., 2000). Results from this area have been applied to segmentation of conversati</context>
</contexts>
<marker>Ward, Vega, 2012</marker>
<rawString>Nigel G. Ward and Alejandro Vega. 2012. A bottomup exploration of the dimensions of dialog state in spoken interaction. In SigDial.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nigel G Ward</author>
<author>Steven D Werner</author>
</authors>
<title>Using dialog-activity similarity for spoken information retrieval.</title>
<date>2013</date>
<booktitle>In Interspeech.</booktitle>
<contexts>
<context position="1568" citStr="Ward and Werner, 2013" startWordPosition="224" endWordPosition="227">s better for segments in which speakers accomplish goals with distinctive start and end points. 1 Introduction People engage in dialogue to address a wide range of goals. It has long been observed that discourse can be structured into units that correspond to distinct goals and activities (Grosz and Sidner, 1986; Passonneau and Litman, 1997). This is conceptually distinct from structuring discourse into the topical units addressed in (Hearst, 1997). The ability to recognize where distinct activities occur in spoken discourse could support offline applications to spoken corpora such as search (Ward and Werner, 2013), summarization (Murray et al., 2005), and question answering. Further, a deeper understanding of the relation of conversational activities to observable features of utterance sequences could inform the design of interactive systems for online applications such as information gathering, service requests, tutoring, and companionship. Automatic identification of such units, Emma Conner econner@oberlin.edu Oberlin College, Oberlin, OH, USA however, has been difficult to achieve. This paper considers the aspectual properties of speakers’ conversational activities, meaning how they transpire in tim</context>
</contexts>
<marker>Ward, Werner, 2013</marker>
<rawString>Nigel G. Ward and Steven D. Werner. 2013. Using dialog-activity similarity for spoken information retrieval. In Interspeech.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
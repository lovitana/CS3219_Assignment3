<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001841">
<title confidence="0.9968955">
Leveraging Rich Linguistic Features for Cross-domain Chinese
Segmentation
</title>
<author confidence="0.999194">
Guohua Wu, Dezhu He, Keli Zhong, Xue Zhou and Caixia Yuan
</author>
<affiliation confidence="0.999414">
School of Computer,
Beijing University of Posts and Telecommunications,
</affiliation>
<address confidence="0.952951">
China, 100876
</address>
<email confidence="0.974716">
trustwugh@gmail.com, hedezhu@bupt.edu.cn, zhk@126.com
bupt.zhouxue@gmail.com, yuancx@bupt.edu.cn
</email>
<sectionHeader confidence="0.993852" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999482590909091">
This paper describes the system that we
use for Chinese segmentation task in the
3rd CIPS-SIGHAN bakeoff. We use char-
acter sequence labeling method for seg-
mentation, and in order to improve seg-
mentation accuracy over multi-domain,
we present a CRF-based Chinese segmen-
tation system integrating supervised, un-
supervised and lexical features. We firstly
preliminarily segment the target data us-
ing CRF model trained over three types
of features mentioned above, from the re-
sult of which new words are detected and
absorbed into the lexicon. To generalize
across different domains, we then execute
the second segment with the updated lexi-
con. The OOV recognition is further pro-
moted with refined post processing. All
the features we used share a unified fea-
ture template trained by CRF. Our system
achieves a competitive F score of 0.9730
for this bakeoff.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993675">
Word is the fundamental unit in natural language
understanding. Since people do not retain the
boundary information between words in practi-
cal use, Chinese Word Segmentation (CWS) is
the very first step in Chinese information process-
ing. A considerable amount of research has shown
that using character sequence labeling is a sim-
ple but effective formulation of Chinese word seg-
mentation task (Xue and others, 2003; Peng et al.,
2004; Low et al., 2005; Zhao et al., 2006a), among
which the method using sequence labeling based
on CRF (Lafferty et al., 2001) is widely used with
attractive performance. However, most of the ex-
isting segmentation systems greatly rely on data
that the model was trained over. The segmentation
performance tends to would reduce significantly
when the test data differs greatly from the training
data in phraseology and vocabulary. Exploiting
corpora in multi-domain for model learning can
solve the problem above directly, whereas labeling
corpora manually costs a lot, so that it is unrealis-
tic to label mass corpora.
So far there are two ways to improve the per-
formance of cross-domain word segmentation sys-
tem. The first way is proposed in (Zhao and Kit,
2007; Zhao and Kit, 2008; Zhao and Kit, 2011),
in which they put forward a unified framework
that integrated supervised and unsupervised seg-
mentation together, where they could take full ad-
vantage of unsupervised segmentation to discover
new word from untagged corpora and obtain the
ability of supervised segmentation to recognize
the known words at the same time. The segmen-
tation system is generalized to some extent. The
second way is to build a segmentation system with
multi-layers. The first layer is a set of distinctive
word segmentation subsystems, who might has an
outstanding performance on specific domain. And
the second layer combines all the outputs of these
subsystems, determining the most possible seg-
mentation boundaries on test dataset. Gao and Vo-
gel (2010) used this method achieved top perfor-
mance in three test domains out of the four during
Bakeoff-2010 (Zhao and Liu, 2010). In this paper
we follow the first method to improve the perfor-
mance of cross-domain segmentation, meanwhile
add some of the effective features that mentioned
in method two. And the performance of handling
OOV is improved by adding lexical feature and
new words discovery.
In Section 2, we describe the features we
adopted in our system. Section 3 represents how
we discover new words from preliminary segmen-
tation results and how we expand the lexicon to
update lexical feature before we segment test data
again to improve the segmentation performance.
</bodyText>
<page confidence="0.98471">
101
</page>
<note confidence="0.9065355">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 101–107,
Wuhan, China, 20-21 October 2014
</note>
<tableCaption confidence="0.999062">
Table 1: Illustration of character tagging
</tableCaption>
<bodyText confidence="0.9996395">
The experimental result that tested on Bakeoff
dataset compared with the best official result is
provided in Section 4. Section 5 leads to the con-
clusion.
</bodyText>
<sectionHeader confidence="0.985193" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999925181818182">
We formulate Chinese word segmentation task
into a sequence labeling problem and use CRF
to train the segmentation model. Our imple-
mentation of CRF-based CWS system uses the
CRF++1 package by Taku Kudo. We regard
“, ”, “。 ”,“?”,“!”,“�” as the boundary
of a sentence and both the training and testing cor-
pora are segmented by these boundaries.
Zhao et al. (2006b) prove that CRF segmenta-
tion performance using 6-tag set for training is bet-
ter than other tag set, so we adopt 6-tag (B,B2,
B3,M,E,S) set labeling the characters in
words. Table 1 explains how to label the charac-
ters in words with different length. We follow six
n-gram character features that are used in (Zhao et
al., 2006b; Zhao and Kit, 2008), as C−1, C0, C1,
C−1C0, C0C1 and C−1C1 respectively, in which
C represents the character, subscript -1, 0 and 1
means the previous character, the current charac-
ter and the next character. With respect to the other
features in our system, the similar six n-gram fea-
ture template is also applied to them.
</bodyText>
<subsectionHeader confidence="0.99449">
2.1 Character Type Features
</subsectionHeader>
<bodyText confidence="0.980630428571429">
We simply classify all the characters by its Uni-
code code point into 5 classes: Chinese char-
acter (C), English character (E), number2 (N),
punctuation (P) and others (O). Denote character
type feature as CTF, and define the feature tem-
plate as CTF−1, CTF0, CTF1, CTF−1CTF0,
CTF0CTF1 and CTF−1CTF1.
</bodyText>
<footnote confidence="0.99193025">
1http://crfpp.googlecode.com/svn/
trunk/doc/index.html
2Numbers including Arabic numerals and its Chinese ver-
sion accordingly.
</footnote>
<subsectionHeader confidence="0.998098">
2.2 Conditional Entropy Feature
</subsectionHeader>
<bodyText confidence="0.999861862068966">
Gao and Vogel (2010) improve the segmentation
performance on 2010 Bakeoff (Zhao and Liu,
2010) dataset by using conditional entropy feature.
The forward conditional entropy for specific char-
acter C is the entropy that combines all the entropy
of characters which might appear in the following
position after C throughout the corpora, recorded
as Hf(C), while the backward conditional entropy
consists of all the entropy of characters that might
appear in the next position after C throughout the
corpora, denoted as Hb(C). We could mix un-
labeled corpora in multi-domain to calculate for-
ward and backword conditional entropy, which
makes this feature more domain adaptive. For-
ward and backward conditional entropy can be ef-
ficiently carried out with the aid of Statistical bi-
gram matrixes.
Continuous values of conditional entropy can
be mapped into discrete numeric values by means
of the method proposed by Gao and Vogel (2010)
as following: [0, 1.0) H 0, [1.0, 2.0) H 1,
[2.0,3.5) H 2, [3.5,5.0) H 4, [5.0,7.0) H
5, [7.0, +oo) H 6. The template is simi-
lar to character feature template, and forward
conditional entropy template is in accordance
with the backward one. Here, the forward
conditional entropy feature templates are given:
Hf(C−1), Hf(C0), Hf(C1), Hf(C−1)Hf(C0),
Hf(C0)Hf(C1), Hf(C−1)Hf(C1).
</bodyText>
<subsectionHeader confidence="0.999665">
2.3 Lexical Feature
</subsectionHeader>
<bodyText confidence="0.999630842105263">
Appropriately using of lexical feature has
shown some improvement in Segmentation, and
hence we adopt the definition of lexical feature
from (Gao and Vogel, 2010). Feature Lbegin(C)
represents the maximum length of words begin
with character C in the lexicon via forward
maximum matching from character C in the
current sentence, and Lend(C) represents the
maximum length of words end with character C
in the lexicon via backward maximum matching
from character C. When processing forward and
backward maximum matching, we only deal with
the word with length equal or greater than 2,
furthermore, the lexical feature value will be 0
where matching failed. Especially when feature
value is equal or greater than 6, we set these
feature values to 6. We hope to increase the
performance by using a large-scale cross-domain
lexicon. Six feature templates are defined for
</bodyText>
<figure confidence="0.997412571428572">
1
2
S
BE
3
4
5
&gt; 6
Word length
Tag sequence for a word
BB2E
BB2B3E
BB2B3ME
BB2B3M··· ME
</figure>
<page confidence="0.844989">
102
</page>
<construct confidence="0.7175835">
Lbegin(C): Lbegin(C−1), Lbegin(C0), Lbegin(C1),
Lbegin(C−1)Lbegin(C0), Lbegin(C0)Lbegin(C1)
</construct>
<listItem confidence="0.374548">
and Lbegin(C−1)Lbegin(C1). As six feature
templates of Lend(C) could be inferred from
above.
</listItem>
<subsectionHeader confidence="0.983797">
2.4 Accessor variety feature
</subsectionHeader>
<bodyText confidence="0.999970941176471">
Accessor variety (AV) proposed by Feng et al.
(2004) could be used to measure the possibility of
whether a substring is a Chinese word. Zhao and
Kit (2007) thought that the method above is agreed
with the method proposed by Harris (1970), in
which morpheme could be found in unfamiliar
language. Zhao and Kit (2008)’s experiments
proved that AV feature improves the performance
of CRF segmentation model on dataset in Bakeoff-
2003, Bakeoff-2005 and Bakeoff-2006 (Sproat
and Emerson, 2003; Emerson, 2005; Levow,
2006) while achieved the best performance on
close test in Bakeoff-2008 (Chen and Jin, 2008).
Therefore in this paper, AV feature is employed
and we make further improvement of the perfor-
mance by making better use of AV feature method.
As to substring s, AV feature is defined as follow:
</bodyText>
<equation confidence="0.907189">
AV (s) = min{L,,,,,(s),R,,,,,(s)}
</equation>
<bodyText confidence="0.9999346">
in which L,,,,,(s) and R,,,,,(s) represent the number
of different characters before s and after s respec-
tively, while the sign in the begin or the end of
sentence would be double counted.
How we use AV is similar to (Zhao and Kit,
2008; Yang et al., 2011), considering the AV value
of substrings with length is equal or less than 5 in
sentence and designing several feature templates
accordingly. We used the formula below to dis-
crete AV value of substring s:
</bodyText>
<equation confidence="0.976522">
f(s) = t, if 2t ≤ AV (s) &lt; 2t+1
</equation>
<bodyText confidence="0.998820615384615">
Discrete value t is regarded as the feature value.
The difference between our method and the
method above is that for substring s, we marked
the feature value of s on the first character of
s, not on every character of s. Representation
of lexical feature mentioned in Section 2.3 was
used for reference because we believed labeling
this way could highlight boundary information be-
tween words. Table 2 shows the differences in de-
tail. For instance, consider all the substring con-
sist of 4 characters. In this case, we have a sub-
string “在我,b中 (in the middle of my heart)”
with AV feature value t = 1. So that we updated
</bodyText>
<table confidence="0.996849222222222">
Accessor Variety Feature Selection
In 2 char 3 char 4 char T
1 char 5 char
而 9 9 5 5 2 2 0 0 0 0 S
在 10 10 5 5 2 1 1 1 1 1 S
我 9 9 5 3 2 2 1 0 1 0 S
,b 8 8 5 5 2 2 1 0 1 0 B
中 9 9 8 8 2 0 1 0 1 0 E
, 11 11 8 0 2 0 0 0 1 0 S
</table>
<tableCaption confidence="0.999889">
Table 2: Comparison of how to use AV feature
</tableCaption>
<bodyText confidence="0.999912515151515">
feature values in “4 char” row. The left row in-
dicates that for every character “在”, “我”, “,b”,
“中”, feature values should be set to 1 according to
method (Zhao and Kit, 2008; Yang et al., 2011).
The right row indicates the feature values in our
method, in which only the first character “在” is
given feature value of 1. We created 6 templates
similar to character feature template for each row
in Table 2.
In order to prove the effectivity of improved AV
feature in our method, we continued to use the ex-
periment setting of (Zhao and Kit, 2008; Yang et
al., 2011) and and had experiment on the dataset of
Bakeoff-2005 (Emerson, 2005) and the simplified
Chinese dataset of Bakeoff-2010 (Zhao and Liu,
2010). OldAV stands for their AV feature while
our feature named as NewAV. 6 n-gram character
features and character type feature mentioned in
Section 2.1 were used in each experiment. Evalu-
ation indicator F score equals F = 2RP/(R+P),
in which R is the recall and P stands for preci-
sion. After combined corresponding training and
test dataset of Bakeoff-2005 together without seg-
mentation marks, statistical AV features were cre-
ated. Then the training corpus, unlabeled corpus
and test corpus of Bakeoff-2010 were combined
together without segmentation marks to count AV
features. The experiment results in Table 3 indi-
cates that our improvement in AV feature is effec-
tive due to the performance is better than other old
methods. These experiment results were not post-
processed so as to compare segmentation perfor-
mance easily.
</bodyText>
<subsectionHeader confidence="0.997531">
2.5 Post-processing
</subsectionHeader>
<bodyText confidence="0.9998688">
Post-processing aimed at handling segmentation
error in English word, Arabic numeric string and
URL. Faced with this situation, these characters
should be regarded as a whole segment unit, but
out system might make segmentation errors. In
</bodyText>
<page confidence="0.989214">
103
</page>
<table confidence="0.9977926">
Bakeoff-2005 AS CityU MSRA PKU
Baseline F 0.954 0.955 0.971 0.950
ROOV 1 0.700 0.798 0.772 0.778
OldAV F 0.957 0.961 0.973 0.952
ROOV 0.688 0.807 0.747 0.770
NewAV F 0.957 0.964 0.973 0.954
ROOV 0.688 0.822 0.743 0.773
Bakeoff-2010 A B C D
Baseline F 0.921 0.93 0.918 0.953
ROOV 0.629 0.773 0.72 0.853
OldAV F 0.933 0.94 0.935 0.956
ROOV 0.656 0.784 0.77 0.848
NewAV F 0.935 0.945 0.936 0.956
ROOV 0.659 0.807 0.763 0.843
1 Recall of out-of-vocabulary (OOV) words.
</table>
<tableCaption confidence="0.999511">
Table 3: Comparion experiment on AV feature, n-
</tableCaption>
<bodyText confidence="0.984358266666667">
gram feature and character type feature were used
for each experiment
Table 4 we have an example of URL segmented
incorrectly, and raw represents the original sen-
tence; result shows the result after segmentation;
final stands for the result after post-processing. To
deal with this kind of problem, we have to make
sure that when we take gaps away from the seg-
mented sentence, it should be in correspondences
with original characters in sentence. Here is a
quick procedure of how we restored URL segmen-
tation error. First, we put the original sentence in a
string; then saved the segmented result in to a list.
Every element in the list is a word with subscript
starts from 0.
</bodyText>
<listItem confidence="0.9974743">
1. Use regular expression to find the start and
the end position of the original sentence. In
case http://t.cn/aBPxzO, the start and
end index is 4 and 22 respectively.
2. Accumulating word length in the word list
from left to right, we can get the start index
of URL is 2 and end index is 3 according to
word list.
3. Combine the 2nd and 3rd word in the word
list as one word.
</listItem>
<bodyText confidence="0.8886155">
English word and Arabic numeric string can be
handled in the same way.
</bodyText>
<table confidence="0.90385">
raw 点击网址http://t.cn/aBPxzO
result 点击 网址 http://t.cn/aB PxzO
final 点击 网址 http://t.cn/aBPxzO
</table>
<tableCaption confidence="0.997565">
Table 4: Post-processing of particular
</tableCaption>
<bodyText confidence="0.735053">
string (URL)
</bodyText>
<sectionHeader confidence="0.984287" genericHeader="method">
3 Improve The Segmentation
</sectionHeader>
<subsectionHeader confidence="0.563286">
Performance of New Words
</subsectionHeader>
<bodyText confidence="0.999956434782609">
The segmentation system that we described in
Section 2 was not very stable when it comes to
new words. New words with some sort of con-
text can be segmented correctly while other con-
text might lead to mistake. For example, the word
“涅维拉济莫夫 (涅维拉济莫夫)” with context
“文官涅维拉济 莫夫在起草一封贺信 (civil offi-
cer Nie Vilage is making a draft of congratulatory
letter)” can be segmented correctly, but the sen-
tence “于是涅维拉济莫夫开始绞尽脑汁 (hence
Nie Vialge began to rack his brain)” was wrongly
segmented. To solve this sort of problem, we tried
to find these new words by rules, then added new
words to the lexicon, re-calculated the lexical fea-
tures of test corpora, segmented test corpora again
in the end. Let ’s mark the lexicon used for extract-
ing lexical features when training segmentation
model as Lexicontrain, and count the Bigram sta-
tistical information on segmented corpora of Peo-
ple’s Daily 1998 and 2000 as PKUbigram without
smoothing. For the preliminary segmentation re-
sult, if word w meets the following conditions, we
deemed w as a new word:
</bodyText>
<listItem confidence="0.995526428571429">
1. (w with length between 2 to 6) or (w with
length greater than 6 and w is a foreign name
at the same time (en dash • exists in w)),
2. w does not exist in Lexicontrain,
3. w is not a Chinese name,
4. w can not be the concatenation of w_1 and
w0 for ∀(w_1, w0) ∈ PKUbigram.
</listItem>
<bodyText confidence="0.9997867">
We checked every word in result after segmen-
tation so that we have a new version of new
words list named Lexicontest. If Lexicontest
has two words with inclusion relation, we only
reserved the word with longer length. Combine
Lexicontrain and Lexicontest together then we
have a new word list named Lexiconnew. This
new word list could be used for calculating lexical
feature of the test corpora to update segmentation
result.
</bodyText>
<page confidence="0.995755">
104
</page>
<table confidence="0.9725274">
Name Features Lexicon
Baseline CF,CTF None
Closed CF,CTF,EF,AV None
Open1 CF,CTF,EF,AV Webdict
Refined2 CF,CTF,EF,AV Webdict
</table>
<tableCaption confidence="0.7717365">
1 Webdict were used to calculate lexi-
cal feature for both testing and train-
ing.
2 Webdict were used to calculate lex-
ical feature for training, then the
method mentioned in Section 3 was
used for performance improvement.
Table 5: Feature combination: CF represents 6 n-
</tableCaption>
<bodyText confidence="0.842063333333333">
gram features of character, CTF represents charac-
ter type feature, EF represents conditional entropy
feature and AV represents Accessor variety feature
</bodyText>
<sectionHeader confidence="0.999219" genericHeader="method">
4 Experiment
</sectionHeader>
<bodyText confidence="0.9999543">
In order to prove the performance of our method,
we considered four kinds of feature combination
demonstrated in Table 5, in which Closed means
closed test, Open means open test in which we
used a cross-domain lexicon — Webdict3. Re-
fined represents that we added new words’ pro-
cess proposed in Section 3 on the basis of Open.
For Refined, we needed corpora to create statis-
tical Bigram information and a lexicon for train-
ing. Because of the limited scale of labeled data
and we have merely sufficient simplified Chinese
training data and lexicon, we didn’t process both
the AS and CityU of Bakeoff-2005 for Refined.
All the experiments in this section were linked
to post-processing mentioned in Section 2.5. We
tested our system on Bakeoff-2005 and Bakeoff-
2010 dataset with major measure index F score.
Table 6 shows the experiment result on Bakeoff-
2005. When computing conditional entropy fea-
ture and AV feature, corresponding test corpus and
training corpus should be mixed together, wiping
off of the segmentation boundaries before the fea-
ture extraction. “Best closed” indicates the best
result on closed test of Bakeoff-2005 and “Best
open” stands for the best open test of official out-
come. Our closed test outcome fully exceeded the
“Best closed”, and open test outcome exist a slight
achieves a slightly lower F scores compared with
“Best open” only on PKU test set, which might
due to the deficiency of corpora and might be im-
</bodyText>
<footnote confidence="0.932122">
3https://github.com/ling0322/webdict
</footnote>
<table confidence="0.999608230769231">
Bakeoff-2005 AS CityU MSRA PKU
Best closed F 0.952 0.943 0.964 0.95
ROOV 0.696 0.698 0.717 0.636
Baseline F 0.955 0.956 0.971 0.950
ROOV 0.708 0.806 0.772 0.779
Closed F 0.957 0.963 0.974 0.954
ROOV 0.705 0.817 0.739 0.770
Open F 0.958 0.965 0.977 0.962
ROOV 0.700 0.811 0.751 0.765
Refined F - - 0.976 0.962
ROOV - - 0.751 0.766
Best open F 0.956 0.962 0.972 0.969
ROOV 0.684 0.806 0.59 0.838
</table>
<tableCaption confidence="0.999031">
Table 6: Test result on Bakeoff-2005 dataset
</tableCaption>
<bodyText confidence="0.999576885714286">
proved only by enlarging the amount of training
corpora.
Table 7 shows the test result on Bakeoff-2010
simplified Chinese dataset. When computing con-
ditional entropy feature and AV feature, we needed
to combine all of the simplified Chinese cor-
pus together without segmentation boundaries of
Bakeoff-2010 corpora to create the statistical fea-
ture values. “Best closed” and “Best open” shows
the best result on official closed test and open test.
Our closed test result on test set A differs greatly
from “Best closed”, yet the result is closer to “Best
closed” on other test sets. The performance on
Closed improves a lot comparing to the baseline.
In addition, our method exceeded “Best open” on
dataset C, D in open test, while slightly poorer re-
sults than the best on dataset A and B but the dif-
ferences are not significant.
From the Refined results of both Table 6 and Ta-
ble 7, we can observe that our strategy on detect-
ing new words provide improvements over all the
ROOV compared to all the Open system in gen-
eral. Meanwhile, our Refined model provide more
balanced F scores among all the dataset.
It is proved on two Bakeoff datasets that our
Open feature combination and Refined feature
combination are effective. On account of lacking
training corpus of this Bakeoff, Open data test is
required. Hence we used Open and Refined fea-
ture combination in Table 5. With purpose of mak-
ing model to be more cross-domain adaptive, we
made use of a large number of unlabeled corpora
to extract conditional entropy feature and AV fea-
ture. Web crawler was used to get totally 1.5G
corpora in 5 domains, including finance, literature,
</bodyText>
<page confidence="0.995061">
105
</page>
<table confidence="0.999708615384615">
Bakeoff-2010 A B C D
Best closed F 0.946 0.951 0.939 0.959
ROOV 0.816 0.827 0.75 0.827
Baseline F 0.921 0.933 0.918 0.954
ROOV 0.629 0.781 0.72 0.86
Closed F 0.935 0.949 0.936 0.958
ROOV 0.658 0.819 0.763 0.853
Open F 0.95 0.949 0.943 0.963
ROOV 0.509 0.766 0.571 0.879
Refined F 0.95 0.949 0.943 0.963
ROOV 0.519 0.768 0.572 0.883
Best open F 0.955 0.95 0.938 0.96
ROOV 0.655 0.82 0.768 0.847
</table>
<tableCaption confidence="0.999537">
Table 7: Test result on Bakeoff-2010 dataset
</tableCaption>
<bodyText confidence="0.9717345">
news, microblog and novel. The data we used is
explained as followed:
</bodyText>
<listItem confidence="0.960102333333333">
• PKU-Corpus: labeled People’s Daily corpus
in year 1998 and 2000.
• PKU-Raw: PKU-Corpus without segmenta-
tion boundaries.
• Web-Corpus: combines all the unlabeled cor-
pora from web crawler.
• Sample-Corpus: randomly select 15% from
Web-Corpus.
• Entropy-Corpus: PKU-Raw together with
Web-Corpus.
• AV-Corpus: PKU-Raw together with
Sample-Corpus.
</listItem>
<bodyText confidence="0.999920583333334">
Finally we used PKU-Corpus as training data, and
extracted from Entropy-Corpus to extract condi-
tional entropy feature while making use of AV-
Corpus to extract AV features, together with char-
acter feature and character type feature to train
CRF word segmentation model. Our results on
this bakeoff are showed in Table 8, which achieves
a competitive F score of 0.9730. From this table,
we can catch that Refined feature combination out-
performs Open, which further confirms that the
new word detection is critical for cross-domain
Chinese segmentation.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.688522">
In this paper we attempted to implement a word
segmentation system with the ability to handle
</bodyText>
<table confidence="0.999920666666667">
Precision Recall F Score
Open 0.9673 0.9776 0.9724
Refined 0.9681 0.9779 0.9730
</table>
<tableCaption confidence="0.999639">
Table 8: Results on Bakeoff-2014 dataset
</tableCaption>
<bodyText confidence="0.999949842105263">
the situation of cross domain. We combined
supervised and unsupervised global features to-
gether and improved the ability to recognize OOV
through adding cross-domain lexical feature. Dis-
covering new words from target test set then re-
computing the lexical feature to refine the segmen-
tation results makes the model more domain adap-
tive.
Yet our system still have many deficiencies
which can be improved from three aspects. First of
all, we only used one kind of unsupervised feature
and there might be other unsupervised features or
feature combination that could achieve better per-
formance. Next, we coined all the feature into one
set of template mainly due to its simplicity in prac-
tice. However, there might exist a more fitting fea-
ture template for different features. At last, our
rule-based method to discover new words could be
changed into automatic discovery.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999876666666667">
This work was partially supported by National
Natural Science Foundation of China Grant
NO.61202248. Many thanks to our colleagues
participating in this work. We also thank Huiming
Duan and Zhifang Sui for their excellent organiza-
tion.
</bodyText>
<sectionHeader confidence="0.97286" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.865425">
Xiao Chen and Guangjin Jin. 2008. The fourth inter-
national chinese language processing bakeoff: Chi-
nese word segmentation named entity recognition
and chinese pos tagging. In Proceedings of the Sixth
</bodyText>
<reference confidence="0.930853846153846">
SIGHAN Workshop on Chinese Language Process-
ing.
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, volume 133.
Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin
Zheng. 2004. Accessor variety criteria for chi-
nese word extraction. Computational Linguistics,
30(1):75–93.
Qin Gao and Stephan Vogel. 2010. A multi-layer chi-
nese word segmentation system optimized for out-
of-domain tasks. In Proceedings of CIPS-SIGHAN
</reference>
<page confidence="0.935342">
106
</page>
<reference confidence="0.999751083333333">
Joint Conference on Chinese Language Processing
(CLP2010), pages 210–215.
Zellig S Harris. 1970. Morpheme boundaries within
words: Report on a computer test. Springer.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Gina-Anne Levow. 2006. The third international chi-
nese language processing bakeoff: Word segmen-
tation and named entity recognition. In Proceed-
ings of the Fifth SIGHAN Workshop on Chinese Lan-
guage Processing, pages 108–117, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005.
A maximum entropy approach to chinese word seg-
mentation. In Proceedings of the Fourth SIGHAN
Workshop on Chinese Language Processing, volume
1612164.
Fuchun Peng, Fangfang Feng, and Andrew McCallum.
2004. Chinese segmentation and new word detec-
tion using conditional random fields. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, page 562. Association for Com-
putational Linguistics.
Richard Sproat and Thomas Emerson. 2003. The
first international chinese word segmentation bake-
off. In Proceedings of the Second SIGHAN Work-
shop on Chinese Language Processing, pages 133–
143, Sapporo, Japan, July. Association for Compu-
tational Linguistics.
Nianwen Xue et al. 2003. Chinese word segmentation
as character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29–48.
Ting-hao Yang, Tian-Jian Jiang, Chan-hung Kuo,
Richard Tzong-han Tsai, and Wen-lian Hsu. 2011.
Unsupervised overlapping feature selection for con-
ditional random fields learning in chinese word seg-
mentation. In Proceedings of the 23rd Conference
on Computational Linguistics and Speech Process-
ing, pages 109–122. Association for Computational
Linguistics.
Hai Zhao and Chunyu Kit. 2007. Incorporating
global information into supervised learning for chi-
nese word segmentation. In 10th Conference of the
Pacific Association for Computational Linguistics,
pages 66–74.
Hai Zhao and Chunyu Kit. 2008. Unsupervised
segmentation helps supervised learning of charac-
ter tagging for word segmentation and named en-
tity recognition. In The Sixth SIGHAN Workshop on
Chinese Language Processing, pages 106–111.
Hai Zhao and Chunyu Kit. 2011. Integrating unsu-
pervised and supervised word segmentation: The
role of goodness measures. Information Sciences,
181(1):163–183.
Hongmei Zhao and Qun Liu. 2010. The cips-sighan
clp 2010 chinese word segmentation bakeoff. In
Proceedings of the First CPS-SIGHANJoint Confer-
ence on Chinese Language Processing, pages 199–
209.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006a. An
improved chinese word segmentation system with
conditional random field. In Proceedings of the Fifth
SIGHAN Workshop on Chinese Language Process-
ing, volume 1082117. Sydney: July.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006b. Effective tag set selection in chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, volume 20,
pages 87–94.
</reference>
<page confidence="0.9987">
107
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.411499">
<title confidence="0.9991295">Leveraging Rich Linguistic Features for Cross-domain Segmentation</title>
<author confidence="0.973524">Guohua Wu</author>
<author confidence="0.973524">Dezhu He</author>
<author confidence="0.973524">Keli Zhong</author>
<author confidence="0.973524">Xue Zhou</author>
<author confidence="0.973524">Caixia</author>
<affiliation confidence="0.997687">School of Beijing University of Posts and</affiliation>
<address confidence="0.548037">China,</address>
<email confidence="0.8970095">trustwugh@gmail.com,hedezhu@bupt.edu.cn,bupt.zhouxue@gmail.com,yuancx@bupt.edu.cn</email>
<abstract confidence="0.995205565217392">This paper describes the system that we use for Chinese segmentation task in the 3rd CIPS-SIGHAN bakeoff. We use character sequence labeling method for segmentation, and in order to improve segmentation accuracy over multi-domain, we present a CRF-based Chinese segmentation system integrating supervised, unsupervised and lexical features. We firstly preliminarily segment the target data using CRF model trained over three types of features mentioned above, from the result of which new words are detected and absorbed into the lexicon. To generalize across different domains, we then execute the second segment with the updated lexicon. The OOV recognition is further promoted with refined post processing. All the features we used share a unified feature template trained by CRF. Our system achieves a competitive F score of 0.9730 for this bakeoff.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>SIGHAN Workshop on Chinese Language Processing.</booktitle>
<marker></marker>
<rawString>SIGHAN Workshop on Chinese Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international chinese word segmentation bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>133</volume>
<contexts>
<context position="8690" citStr="Emerson, 2005" startWordPosition="1379" endWordPosition="1380">(C−1)Lbegin(C1). As six feature templates of Lend(C) could be inferred from above. 2.4 Accessor variety feature Accessor variety (AV) proposed by Feng et al. (2004) could be used to measure the possibility of whether a substring is a Chinese word. Zhao and Kit (2007) thought that the method above is agreed with the method proposed by Harris (1970), in which morpheme could be found in unfamiliar language. Zhao and Kit (2008)’s experiments proved that AV feature improves the performance of CRF segmentation model on dataset in Bakeoff2003, Bakeoff-2005 and Bakeoff-2006 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006) while achieved the best performance on close test in Bakeoff-2008 (Chen and Jin, 2008). Therefore in this paper, AV feature is employed and we make further improvement of the performance by making better use of AV feature method. As to substring s, AV feature is defined as follow: AV (s) = min{L,,,,,(s),R,,,,,(s)} in which L,,,,,(s) and R,,,,,(s) represent the number of different characters before s and after s respectively, while the sign in the begin or the end of sentence would be double counted. How we use AV is similar to (Zhao and Kit, 2008; Yang et al., 2011), considering</context>
<context position="11036" citStr="Emerson, 2005" startWordPosition="1852" endWordPosition="1853"> char” row. The left row indicates that for every character “在”, “我”, “,b”, “中”, feature values should be set to 1 according to method (Zhao and Kit, 2008; Yang et al., 2011). The right row indicates the feature values in our method, in which only the first character “在” is given feature value of 1. We created 6 templates similar to character feature template for each row in Table 2. In order to prove the effectivity of improved AV feature in our method, we continued to use the experiment setting of (Zhao and Kit, 2008; Yang et al., 2011) and and had experiment on the dataset of Bakeoff-2005 (Emerson, 2005) and the simplified Chinese dataset of Bakeoff-2010 (Zhao and Liu, 2010). OldAV stands for their AV feature while our feature named as NewAV. 6 n-gram character features and character type feature mentioned in Section 2.1 were used in each experiment. Evaluation indicator F score equals F = 2RP/(R+P), in which R is the recall and P stands for precision. After combined corresponding training and test dataset of Bakeoff-2005 together without segmentation marks, statistical AV features were created. Then the training corpus, unlabeled corpus and test corpus of Bakeoff-2010 were combined together </context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international chinese word segmentation bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haodi Feng</author>
<author>Kang Chen</author>
<author>Xiaotie Deng</author>
<author>Weimin Zheng</author>
</authors>
<title>Accessor variety criteria for chinese word extraction.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="8241" citStr="Feng et al. (2004)" startWordPosition="1305" endWordPosition="1308">l feature value will be 0 where matching failed. Especially when feature value is equal or greater than 6, we set these feature values to 6. We hope to increase the performance by using a large-scale cross-domain lexicon. Six feature templates are defined for 1 2 S BE 3 4 5 &gt; 6 Word length Tag sequence for a word BB2E BB2B3E BB2B3ME BB2B3M··· ME 102 Lbegin(C): Lbegin(C−1), Lbegin(C0), Lbegin(C1), Lbegin(C−1)Lbegin(C0), Lbegin(C0)Lbegin(C1) and Lbegin(C−1)Lbegin(C1). As six feature templates of Lend(C) could be inferred from above. 2.4 Accessor variety feature Accessor variety (AV) proposed by Feng et al. (2004) could be used to measure the possibility of whether a substring is a Chinese word. Zhao and Kit (2007) thought that the method above is agreed with the method proposed by Harris (1970), in which morpheme could be found in unfamiliar language. Zhao and Kit (2008)’s experiments proved that AV feature improves the performance of CRF segmentation model on dataset in Bakeoff2003, Bakeoff-2005 and Bakeoff-2006 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006) while achieved the best performance on close test in Bakeoff-2008 (Chen and Jin, 2008). Therefore in this paper, AV feature is employed </context>
</contexts>
<marker>Feng, Chen, Deng, Zheng, 2004</marker>
<rawString>Haodi Feng, Kang Chen, Xiaotie Deng, and Weimin Zheng. 2004. Accessor variety criteria for chinese word extraction. Computational Linguistics, 30(1):75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>A multi-layer chinese word segmentation system optimized for outof-domain tasks.</title>
<date>2010</date>
<booktitle>In Proceedings of CIPS-SIGHAN Joint Conference on Chinese Language Processing (CLP2010),</booktitle>
<pages>210--215</pages>
<contexts>
<context position="3159" citStr="Gao and Vogel (2010)" startWordPosition="490" endWordPosition="494">, where they could take full advantage of unsupervised segmentation to discover new word from untagged corpora and obtain the ability of supervised segmentation to recognize the known words at the same time. The segmentation system is generalized to some extent. The second way is to build a segmentation system with multi-layers. The first layer is a set of distinctive word segmentation subsystems, who might has an outstanding performance on specific domain. And the second layer combines all the outputs of these subsystems, determining the most possible segmentation boundaries on test dataset. Gao and Vogel (2010) used this method achieved top performance in three test domains out of the four during Bakeoff-2010 (Zhao and Liu, 2010). In this paper we follow the first method to improve the performance of cross-domain segmentation, meanwhile add some of the effective features that mentioned in method two. And the performance of handling OOV is improved by adding lexical feature and new words discovery. In Section 2, we describe the features we adopted in our system. Section 3 represents how we discover new words from preliminary segmentation results and how we expand the lexicon to update lexical feature</context>
<context position="5710" citStr="Gao and Vogel (2010)" startWordPosition="906" endWordPosition="909">With respect to the other features in our system, the similar six n-gram feature template is also applied to them. 2.1 Character Type Features We simply classify all the characters by its Unicode code point into 5 classes: Chinese character (C), English character (E), number2 (N), punctuation (P) and others (O). Denote character type feature as CTF, and define the feature template as CTF−1, CTF0, CTF1, CTF−1CTF0, CTF0CTF1 and CTF−1CTF1. 1http://crfpp.googlecode.com/svn/ trunk/doc/index.html 2Numbers including Arabic numerals and its Chinese version accordingly. 2.2 Conditional Entropy Feature Gao and Vogel (2010) improve the segmentation performance on 2010 Bakeoff (Zhao and Liu, 2010) dataset by using conditional entropy feature. The forward conditional entropy for specific character C is the entropy that combines all the entropy of characters which might appear in the following position after C throughout the corpora, recorded as Hf(C), while the backward conditional entropy consists of all the entropy of characters that might appear in the next position after C throughout the corpora, denoted as Hb(C). We could mix unlabeled corpora in multi-domain to calculate forward and backword conditional entr</context>
<context position="7178" citStr="Gao and Vogel, 2010" startWordPosition="1137" endWordPosition="1140">lues by means of the method proposed by Gao and Vogel (2010) as following: [0, 1.0) H 0, [1.0, 2.0) H 1, [2.0,3.5) H 2, [3.5,5.0) H 4, [5.0,7.0) H 5, [7.0, +oo) H 6. The template is similar to character feature template, and forward conditional entropy template is in accordance with the backward one. Here, the forward conditional entropy feature templates are given: Hf(C−1), Hf(C0), Hf(C1), Hf(C−1)Hf(C0), Hf(C0)Hf(C1), Hf(C−1)Hf(C1). 2.3 Lexical Feature Appropriately using of lexical feature has shown some improvement in Segmentation, and hence we adopt the definition of lexical feature from (Gao and Vogel, 2010). Feature Lbegin(C) represents the maximum length of words begin with character C in the lexicon via forward maximum matching from character C in the current sentence, and Lend(C) represents the maximum length of words end with character C in the lexicon via backward maximum matching from character C. When processing forward and backward maximum matching, we only deal with the word with length equal or greater than 2, furthermore, the lexical feature value will be 0 where matching failed. Especially when feature value is equal or greater than 6, we set these feature values to 6. We hope to inc</context>
</contexts>
<marker>Gao, Vogel, 2010</marker>
<rawString>Qin Gao and Stephan Vogel. 2010. A multi-layer chinese word segmentation system optimized for outof-domain tasks. In Proceedings of CIPS-SIGHAN Joint Conference on Chinese Language Processing (CLP2010), pages 210–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig S Harris</author>
</authors>
<title>Morpheme boundaries within words: Report on a computer test.</title>
<date>1970</date>
<publisher>Springer.</publisher>
<contexts>
<context position="8426" citStr="Harris (1970)" startWordPosition="1340" endWordPosition="1341">rge-scale cross-domain lexicon. Six feature templates are defined for 1 2 S BE 3 4 5 &gt; 6 Word length Tag sequence for a word BB2E BB2B3E BB2B3ME BB2B3M··· ME 102 Lbegin(C): Lbegin(C−1), Lbegin(C0), Lbegin(C1), Lbegin(C−1)Lbegin(C0), Lbegin(C0)Lbegin(C1) and Lbegin(C−1)Lbegin(C1). As six feature templates of Lend(C) could be inferred from above. 2.4 Accessor variety feature Accessor variety (AV) proposed by Feng et al. (2004) could be used to measure the possibility of whether a substring is a Chinese word. Zhao and Kit (2007) thought that the method above is agreed with the method proposed by Harris (1970), in which morpheme could be found in unfamiliar language. Zhao and Kit (2008)’s experiments proved that AV feature improves the performance of CRF segmentation model on dataset in Bakeoff2003, Bakeoff-2005 and Bakeoff-2006 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006) while achieved the best performance on close test in Bakeoff-2008 (Chen and Jin, 2008). Therefore in this paper, AV feature is employed and we make further improvement of the performance by making better use of AV feature method. As to substring s, AV feature is defined as follow: AV (s) = min{L,,,,,(s),R,,,,,(s)} in wh</context>
</contexts>
<marker>Harris, 1970</marker>
<rawString>Zellig S Harris. 1970. Morpheme boundaries within words: Report on a computer test. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<contexts>
<context position="1750" citStr="Lafferty et al., 2001" startWordPosition="265" endWordPosition="268">a competitive F score of 0.9730 for this bakeoff. 1 Introduction Word is the fundamental unit in natural language understanding. Since people do not retain the boundary information between words in practical use, Chinese Word Segmentation (CWS) is the very first step in Chinese information processing. A considerable amount of research has shown that using character sequence labeling is a simple but effective formulation of Chinese word segmentation task (Xue and others, 2003; Peng et al., 2004; Low et al., 2005; Zhao et al., 2006a), among which the method using sequence labeling based on CRF (Lafferty et al., 2001) is widely used with attractive performance. However, most of the existing segmentation systems greatly rely on data that the model was trained over. The segmentation performance tends to would reduce significantly when the test data differs greatly from the training data in phraseology and vocabulary. Exploiting corpora in multi-domain for model learning can solve the problem above directly, whereas labeling corpora manually costs a lot, so that it is unrealistic to label mass corpora. So far there are two ways to improve the performance of cross-domain word segmentation system. The first way</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
</authors>
<title>The third international chinese language processing bakeoff: Word segmentation and named entity recognition.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>108--117</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="8704" citStr="Levow, 2006" startWordPosition="1381" endWordPosition="1382">. As six feature templates of Lend(C) could be inferred from above. 2.4 Accessor variety feature Accessor variety (AV) proposed by Feng et al. (2004) could be used to measure the possibility of whether a substring is a Chinese word. Zhao and Kit (2007) thought that the method above is agreed with the method proposed by Harris (1970), in which morpheme could be found in unfamiliar language. Zhao and Kit (2008)’s experiments proved that AV feature improves the performance of CRF segmentation model on dataset in Bakeoff2003, Bakeoff-2005 and Bakeoff-2006 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006) while achieved the best performance on close test in Bakeoff-2008 (Chen and Jin, 2008). Therefore in this paper, AV feature is employed and we make further improvement of the performance by making better use of AV feature method. As to substring s, AV feature is defined as follow: AV (s) = min{L,,,,,(s),R,,,,,(s)} in which L,,,,,(s) and R,,,,,(s) represent the number of different characters before s and after s respectively, while the sign in the begin or the end of sentence would be double counted. How we use AV is similar to (Zhao and Kit, 2008; Yang et al., 2011), considering the AV value </context>
</contexts>
<marker>Levow, 2006</marker>
<rawString>Gina-Anne Levow. 2006. The third international chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 108–117, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Kiat Low</author>
<author>Hwee Tou Ng</author>
<author>Wenyuan Guo</author>
</authors>
<title>A maximum entropy approach to chinese word segmentation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>volume</volume>
<pages>1612164</pages>
<contexts>
<context position="1644" citStr="Low et al., 2005" startWordPosition="247" endWordPosition="250">sing. All the features we used share a unified feature template trained by CRF. Our system achieves a competitive F score of 0.9730 for this bakeoff. 1 Introduction Word is the fundamental unit in natural language understanding. Since people do not retain the boundary information between words in practical use, Chinese Word Segmentation (CWS) is the very first step in Chinese information processing. A considerable amount of research has shown that using character sequence labeling is a simple but effective formulation of Chinese word segmentation task (Xue and others, 2003; Peng et al., 2004; Low et al., 2005; Zhao et al., 2006a), among which the method using sequence labeling based on CRF (Lafferty et al., 2001) is widely used with attractive performance. However, most of the existing segmentation systems greatly rely on data that the model was trained over. The segmentation performance tends to would reduce significantly when the test data differs greatly from the training data in phraseology and vocabulary. Exploiting corpora in multi-domain for model learning can solve the problem above directly, whereas labeling corpora manually costs a lot, so that it is unrealistic to label mass corpora. So</context>
</contexts>
<marker>Low, Ng, Guo, 2005</marker>
<rawString>Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo. 2005. A maximum entropy approach to chinese word segmentation. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, volume 1612164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuchun Peng</author>
<author>Fangfang Feng</author>
<author>Andrew McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>562</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1626" citStr="Peng et al., 2004" startWordPosition="243" endWordPosition="246">refined post processing. All the features we used share a unified feature template trained by CRF. Our system achieves a competitive F score of 0.9730 for this bakeoff. 1 Introduction Word is the fundamental unit in natural language understanding. Since people do not retain the boundary information between words in practical use, Chinese Word Segmentation (CWS) is the very first step in Chinese information processing. A considerable amount of research has shown that using character sequence labeling is a simple but effective formulation of Chinese word segmentation task (Xue and others, 2003; Peng et al., 2004; Low et al., 2005; Zhao et al., 2006a), among which the method using sequence labeling based on CRF (Lafferty et al., 2001) is widely used with attractive performance. However, most of the existing segmentation systems greatly rely on data that the model was trained over. The segmentation performance tends to would reduce significantly when the test data differs greatly from the training data in phraseology and vocabulary. Exploiting corpora in multi-domain for model learning can solve the problem above directly, whereas labeling corpora manually costs a lot, so that it is unrealistic to labe</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of the 20th international conference on Computational Linguistics, page 562. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Thomas Emerson</author>
</authors>
<title>The first international chinese word segmentation bakeoff.</title>
<date>2003</date>
<booktitle>In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>133--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sapporo, Japan,</location>
<contexts>
<context position="8675" citStr="Sproat and Emerson, 2003" startWordPosition="1375" endWordPosition="1378">n(C0)Lbegin(C1) and Lbegin(C−1)Lbegin(C1). As six feature templates of Lend(C) could be inferred from above. 2.4 Accessor variety feature Accessor variety (AV) proposed by Feng et al. (2004) could be used to measure the possibility of whether a substring is a Chinese word. Zhao and Kit (2007) thought that the method above is agreed with the method proposed by Harris (1970), in which morpheme could be found in unfamiliar language. Zhao and Kit (2008)’s experiments proved that AV feature improves the performance of CRF segmentation model on dataset in Bakeoff2003, Bakeoff-2005 and Bakeoff-2006 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006) while achieved the best performance on close test in Bakeoff-2008 (Chen and Jin, 2008). Therefore in this paper, AV feature is employed and we make further improvement of the performance by making better use of AV feature method. As to substring s, AV feature is defined as follow: AV (s) = min{L,,,,,(s),R,,,,,(s)} in which L,,,,,(s) and R,,,,,(s) represent the number of different characters before s and after s respectively, while the sign in the begin or the end of sentence would be double counted. How we use AV is similar to (Zhao and Kit, 2008; Yang et al., 201</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Thomas Emerson. 2003. The first international chinese word segmentation bakeoff. In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing, pages 133– 143, Sapporo, Japan, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue et al. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting-hao Yang</author>
<author>Tian-Jian Jiang</author>
<author>Chan-hung Kuo</author>
<author>Richard Tzong-han Tsai</author>
<author>Wen-lian Hsu</author>
</authors>
<title>Unsupervised overlapping feature selection for conditional random fields learning in chinese word segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 23rd Conference on Computational Linguistics and Speech Processing,</booktitle>
<pages>109--122</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9277" citStr="Yang et al., 2011" startWordPosition="1480" endWordPosition="1483">nd Emerson, 2003; Emerson, 2005; Levow, 2006) while achieved the best performance on close test in Bakeoff-2008 (Chen and Jin, 2008). Therefore in this paper, AV feature is employed and we make further improvement of the performance by making better use of AV feature method. As to substring s, AV feature is defined as follow: AV (s) = min{L,,,,,(s),R,,,,,(s)} in which L,,,,,(s) and R,,,,,(s) represent the number of different characters before s and after s respectively, while the sign in the begin or the end of sentence would be double counted. How we use AV is similar to (Zhao and Kit, 2008; Yang et al., 2011), considering the AV value of substrings with length is equal or less than 5 in sentence and designing several feature templates accordingly. We used the formula below to discrete AV value of substring s: f(s) = t, if 2t ≤ AV (s) &lt; 2t+1 Discrete value t is regarded as the feature value. The difference between our method and the method above is that for substring s, we marked the feature value of s on the first character of s, not on every character of s. Representation of lexical feature mentioned in Section 2.3 was used for reference because we believed labeling this way could highlight bound</context>
<context position="10596" citStr="Yang et al., 2011" startWordPosition="1771" endWordPosition="1774">e substring consist of 4 characters. In this case, we have a substring “在我,b中 (in the middle of my heart)” with AV feature value t = 1. So that we updated Accessor Variety Feature Selection In 2 char 3 char 4 char T 1 char 5 char 而 9 9 5 5 2 2 0 0 0 0 S 在 10 10 5 5 2 1 1 1 1 1 S 我 9 9 5 3 2 2 1 0 1 0 S ,b 8 8 5 5 2 2 1 0 1 0 B 中 9 9 8 8 2 0 1 0 1 0 E , 11 11 8 0 2 0 0 0 1 0 S Table 2: Comparison of how to use AV feature feature values in “4 char” row. The left row indicates that for every character “在”, “我”, “,b”, “中”, feature values should be set to 1 according to method (Zhao and Kit, 2008; Yang et al., 2011). The right row indicates the feature values in our method, in which only the first character “在” is given feature value of 1. We created 6 templates similar to character feature template for each row in Table 2. In order to prove the effectivity of improved AV feature in our method, we continued to use the experiment setting of (Zhao and Kit, 2008; Yang et al., 2011) and and had experiment on the dataset of Bakeoff-2005 (Emerson, 2005) and the simplified Chinese dataset of Bakeoff-2010 (Zhao and Liu, 2010). OldAV stands for their AV feature while our feature named as NewAV. 6 n-gram character</context>
</contexts>
<marker>Yang, Jiang, Kuo, Tsai, Hsu, 2011</marker>
<rawString>Ting-hao Yang, Tian-Jian Jiang, Chan-hung Kuo, Richard Tzong-han Tsai, and Wen-lian Hsu. 2011. Unsupervised overlapping feature selection for conditional random fields learning in chinese word segmentation. In Proceedings of the 23rd Conference on Computational Linguistics and Speech Processing, pages 109–122. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Incorporating global information into supervised learning for chinese word segmentation.</title>
<date>2007</date>
<booktitle>In 10th Conference of the Pacific Association for Computational Linguistics,</booktitle>
<pages>66--74</pages>
<contexts>
<context position="2385" citStr="Zhao and Kit, 2007" startWordPosition="368" endWordPosition="371">ith attractive performance. However, most of the existing segmentation systems greatly rely on data that the model was trained over. The segmentation performance tends to would reduce significantly when the test data differs greatly from the training data in phraseology and vocabulary. Exploiting corpora in multi-domain for model learning can solve the problem above directly, whereas labeling corpora manually costs a lot, so that it is unrealistic to label mass corpora. So far there are two ways to improve the performance of cross-domain word segmentation system. The first way is proposed in (Zhao and Kit, 2007; Zhao and Kit, 2008; Zhao and Kit, 2011), in which they put forward a unified framework that integrated supervised and unsupervised segmentation together, where they could take full advantage of unsupervised segmentation to discover new word from untagged corpora and obtain the ability of supervised segmentation to recognize the known words at the same time. The segmentation system is generalized to some extent. The second way is to build a segmentation system with multi-layers. The first layer is a set of distinctive word segmentation subsystems, who might has an outstanding performance on s</context>
<context position="8344" citStr="Zhao and Kit (2007)" startWordPosition="1324" endWordPosition="1327">n 6, we set these feature values to 6. We hope to increase the performance by using a large-scale cross-domain lexicon. Six feature templates are defined for 1 2 S BE 3 4 5 &gt; 6 Word length Tag sequence for a word BB2E BB2B3E BB2B3ME BB2B3M··· ME 102 Lbegin(C): Lbegin(C−1), Lbegin(C0), Lbegin(C1), Lbegin(C−1)Lbegin(C0), Lbegin(C0)Lbegin(C1) and Lbegin(C−1)Lbegin(C1). As six feature templates of Lend(C) could be inferred from above. 2.4 Accessor variety feature Accessor variety (AV) proposed by Feng et al. (2004) could be used to measure the possibility of whether a substring is a Chinese word. Zhao and Kit (2007) thought that the method above is agreed with the method proposed by Harris (1970), in which morpheme could be found in unfamiliar language. Zhao and Kit (2008)’s experiments proved that AV feature improves the performance of CRF segmentation model on dataset in Bakeoff2003, Bakeoff-2005 and Bakeoff-2006 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006) while achieved the best performance on close test in Bakeoff-2008 (Chen and Jin, 2008). Therefore in this paper, AV feature is employed and we make further improvement of the performance by making better use of AV feature method. As to sub</context>
</contexts>
<marker>Zhao, Kit, 2007</marker>
<rawString>Hai Zhao and Chunyu Kit. 2007. Incorporating global information into supervised learning for chinese word segmentation. In 10th Conference of the Pacific Association for Computational Linguistics, pages 66–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition.</title>
<date>2008</date>
<booktitle>In The Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>106--111</pages>
<contexts>
<context position="2405" citStr="Zhao and Kit, 2008" startWordPosition="372" endWordPosition="375">rmance. However, most of the existing segmentation systems greatly rely on data that the model was trained over. The segmentation performance tends to would reduce significantly when the test data differs greatly from the training data in phraseology and vocabulary. Exploiting corpora in multi-domain for model learning can solve the problem above directly, whereas labeling corpora manually costs a lot, so that it is unrealistic to label mass corpora. So far there are two ways to improve the performance of cross-domain word segmentation system. The first way is proposed in (Zhao and Kit, 2007; Zhao and Kit, 2008; Zhao and Kit, 2011), in which they put forward a unified framework that integrated supervised and unsupervised segmentation together, where they could take full advantage of unsupervised segmentation to discover new word from untagged corpora and obtain the ability of supervised segmentation to recognize the known words at the same time. The segmentation system is generalized to some extent. The second way is to build a segmentation system with multi-layers. The first layer is a set of distinctive word segmentation subsystems, who might has an outstanding performance on specific domain. And </context>
<context position="4901" citStr="Zhao and Kit, 2008" startWordPosition="780" endWordPosition="783">o train the segmentation model. Our implementation of CRF-based CWS system uses the CRF++1 package by Taku Kudo. We regard “, ”, “。 ”,“?”,“!”,“�” as the boundary of a sentence and both the training and testing corpora are segmented by these boundaries. Zhao et al. (2006b) prove that CRF segmentation performance using 6-tag set for training is better than other tag set, so we adopt 6-tag (B,B2, B3,M,E,S) set labeling the characters in words. Table 1 explains how to label the characters in words with different length. We follow six n-gram character features that are used in (Zhao et al., 2006b; Zhao and Kit, 2008), as C−1, C0, C1, C−1C0, C0C1 and C−1C1 respectively, in which C represents the character, subscript -1, 0 and 1 means the previous character, the current character and the next character. With respect to the other features in our system, the similar six n-gram feature template is also applied to them. 2.1 Character Type Features We simply classify all the characters by its Unicode code point into 5 classes: Chinese character (C), English character (E), number2 (N), punctuation (P) and others (O). Denote character type feature as CTF, and define the feature template as CTF−1, CTF0, CTF1, CTF−1</context>
<context position="8504" citStr="Zhao and Kit (2008)" startWordPosition="1351" endWordPosition="1354">2 S BE 3 4 5 &gt; 6 Word length Tag sequence for a word BB2E BB2B3E BB2B3ME BB2B3M··· ME 102 Lbegin(C): Lbegin(C−1), Lbegin(C0), Lbegin(C1), Lbegin(C−1)Lbegin(C0), Lbegin(C0)Lbegin(C1) and Lbegin(C−1)Lbegin(C1). As six feature templates of Lend(C) could be inferred from above. 2.4 Accessor variety feature Accessor variety (AV) proposed by Feng et al. (2004) could be used to measure the possibility of whether a substring is a Chinese word. Zhao and Kit (2007) thought that the method above is agreed with the method proposed by Harris (1970), in which morpheme could be found in unfamiliar language. Zhao and Kit (2008)’s experiments proved that AV feature improves the performance of CRF segmentation model on dataset in Bakeoff2003, Bakeoff-2005 and Bakeoff-2006 (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006) while achieved the best performance on close test in Bakeoff-2008 (Chen and Jin, 2008). Therefore in this paper, AV feature is employed and we make further improvement of the performance by making better use of AV feature method. As to substring s, AV feature is defined as follow: AV (s) = min{L,,,,,(s),R,,,,,(s)} in which L,,,,,(s) and R,,,,,(s) represent the number of different characters befor</context>
<context position="10576" citStr="Zhao and Kit, 2008" startWordPosition="1767" endWordPosition="1770">nce, consider all the substring consist of 4 characters. In this case, we have a substring “在我,b中 (in the middle of my heart)” with AV feature value t = 1. So that we updated Accessor Variety Feature Selection In 2 char 3 char 4 char T 1 char 5 char 而 9 9 5 5 2 2 0 0 0 0 S 在 10 10 5 5 2 1 1 1 1 1 S 我 9 9 5 3 2 2 1 0 1 0 S ,b 8 8 5 5 2 2 1 0 1 0 B 中 9 9 8 8 2 0 1 0 1 0 E , 11 11 8 0 2 0 0 0 1 0 S Table 2: Comparison of how to use AV feature feature values in “4 char” row. The left row indicates that for every character “在”, “我”, “,b”, “中”, feature values should be set to 1 according to method (Zhao and Kit, 2008; Yang et al., 2011). The right row indicates the feature values in our method, in which only the first character “在” is given feature value of 1. We created 6 templates similar to character feature template for each row in Table 2. In order to prove the effectivity of improved AV feature in our method, we continued to use the experiment setting of (Zhao and Kit, 2008; Yang et al., 2011) and and had experiment on the dataset of Bakeoff-2005 (Emerson, 2005) and the simplified Chinese dataset of Bakeoff-2010 (Zhao and Liu, 2010). OldAV stands for their AV feature while our feature named as NewAV</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition. In The Sixth SIGHAN Workshop on Chinese Language Processing, pages 106–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Integrating unsupervised and supervised word segmentation: The role of goodness measures.</title>
<date>2011</date>
<journal>Information Sciences,</journal>
<volume>181</volume>
<issue>1</issue>
<contexts>
<context position="2426" citStr="Zhao and Kit, 2011" startWordPosition="376" endWordPosition="379">t of the existing segmentation systems greatly rely on data that the model was trained over. The segmentation performance tends to would reduce significantly when the test data differs greatly from the training data in phraseology and vocabulary. Exploiting corpora in multi-domain for model learning can solve the problem above directly, whereas labeling corpora manually costs a lot, so that it is unrealistic to label mass corpora. So far there are two ways to improve the performance of cross-domain word segmentation system. The first way is proposed in (Zhao and Kit, 2007; Zhao and Kit, 2008; Zhao and Kit, 2011), in which they put forward a unified framework that integrated supervised and unsupervised segmentation together, where they could take full advantage of unsupervised segmentation to discover new word from untagged corpora and obtain the ability of supervised segmentation to recognize the known words at the same time. The segmentation system is generalized to some extent. The second way is to build a segmentation system with multi-layers. The first layer is a set of distinctive word segmentation subsystems, who might has an outstanding performance on specific domain. And the second layer comb</context>
</contexts>
<marker>Zhao, Kit, 2011</marker>
<rawString>Hai Zhao and Chunyu Kit. 2011. Integrating unsupervised and supervised word segmentation: The role of goodness measures. Information Sciences, 181(1):163–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongmei Zhao</author>
<author>Qun Liu</author>
</authors>
<title>The cips-sighan clp 2010 chinese word segmentation bakeoff.</title>
<date>2010</date>
<booktitle>In Proceedings of the First CPS-SIGHANJoint Conference on Chinese Language Processing,</booktitle>
<pages>199--209</pages>
<contexts>
<context position="3280" citStr="Zhao and Liu, 2010" startWordPosition="512" endWordPosition="515">the ability of supervised segmentation to recognize the known words at the same time. The segmentation system is generalized to some extent. The second way is to build a segmentation system with multi-layers. The first layer is a set of distinctive word segmentation subsystems, who might has an outstanding performance on specific domain. And the second layer combines all the outputs of these subsystems, determining the most possible segmentation boundaries on test dataset. Gao and Vogel (2010) used this method achieved top performance in three test domains out of the four during Bakeoff-2010 (Zhao and Liu, 2010). In this paper we follow the first method to improve the performance of cross-domain segmentation, meanwhile add some of the effective features that mentioned in method two. And the performance of handling OOV is improved by adding lexical feature and new words discovery. In Section 2, we describe the features we adopted in our system. Section 3 represents how we discover new words from preliminary segmentation results and how we expand the lexicon to update lexical feature before we segment test data again to improve the segmentation performance. 101 Proceedings of the Third CIPS-SIGHAN Join</context>
<context position="5784" citStr="Zhao and Liu, 2010" startWordPosition="917" endWordPosition="920">ature template is also applied to them. 2.1 Character Type Features We simply classify all the characters by its Unicode code point into 5 classes: Chinese character (C), English character (E), number2 (N), punctuation (P) and others (O). Denote character type feature as CTF, and define the feature template as CTF−1, CTF0, CTF1, CTF−1CTF0, CTF0CTF1 and CTF−1CTF1. 1http://crfpp.googlecode.com/svn/ trunk/doc/index.html 2Numbers including Arabic numerals and its Chinese version accordingly. 2.2 Conditional Entropy Feature Gao and Vogel (2010) improve the segmentation performance on 2010 Bakeoff (Zhao and Liu, 2010) dataset by using conditional entropy feature. The forward conditional entropy for specific character C is the entropy that combines all the entropy of characters which might appear in the following position after C throughout the corpora, recorded as Hf(C), while the backward conditional entropy consists of all the entropy of characters that might appear in the next position after C throughout the corpora, denoted as Hb(C). We could mix unlabeled corpora in multi-domain to calculate forward and backword conditional entropy, which makes this feature more domain adaptive. Forward and backward c</context>
<context position="11108" citStr="Zhao and Liu, 2010" startWordPosition="1861" endWordPosition="1864">”, “,b”, “中”, feature values should be set to 1 according to method (Zhao and Kit, 2008; Yang et al., 2011). The right row indicates the feature values in our method, in which only the first character “在” is given feature value of 1. We created 6 templates similar to character feature template for each row in Table 2. In order to prove the effectivity of improved AV feature in our method, we continued to use the experiment setting of (Zhao and Kit, 2008; Yang et al., 2011) and and had experiment on the dataset of Bakeoff-2005 (Emerson, 2005) and the simplified Chinese dataset of Bakeoff-2010 (Zhao and Liu, 2010). OldAV stands for their AV feature while our feature named as NewAV. 6 n-gram character features and character type feature mentioned in Section 2.1 were used in each experiment. Evaluation indicator F score equals F = 2RP/(R+P), in which R is the recall and P stands for precision. After combined corresponding training and test dataset of Bakeoff-2005 together without segmentation marks, statistical AV features were created. Then the training corpus, unlabeled corpus and test corpus of Bakeoff-2010 were combined together without segmentation marks to count AV features. The experiment results </context>
</contexts>
<marker>Zhao, Liu, 2010</marker>
<rawString>Hongmei Zhao and Qun Liu. 2010. The cips-sighan clp 2010 chinese word segmentation bakeoff. In Proceedings of the First CPS-SIGHANJoint Conference on Chinese Language Processing, pages 199– 209.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>volume</volume>
<pages>1082117</pages>
<location>Sydney:</location>
<contexts>
<context position="1663" citStr="Zhao et al., 2006" startWordPosition="251" endWordPosition="254">ures we used share a unified feature template trained by CRF. Our system achieves a competitive F score of 0.9730 for this bakeoff. 1 Introduction Word is the fundamental unit in natural language understanding. Since people do not retain the boundary information between words in practical use, Chinese Word Segmentation (CWS) is the very first step in Chinese information processing. A considerable amount of research has shown that using character sequence labeling is a simple but effective formulation of Chinese word segmentation task (Xue and others, 2003; Peng et al., 2004; Low et al., 2005; Zhao et al., 2006a), among which the method using sequence labeling based on CRF (Lafferty et al., 2001) is widely used with attractive performance. However, most of the existing segmentation systems greatly rely on data that the model was trained over. The segmentation performance tends to would reduce significantly when the test data differs greatly from the training data in phraseology and vocabulary. Exploiting corpora in multi-domain for model learning can solve the problem above directly, whereas labeling corpora manually costs a lot, so that it is unrealistic to label mass corpora. So far there are two </context>
<context position="4552" citStr="Zhao et al. (2006" startWordPosition="718" endWordPosition="721">107, Wuhan, China, 20-21 October 2014 Table 1: Illustration of character tagging The experimental result that tested on Bakeoff dataset compared with the best official result is provided in Section 4. Section 5 leads to the conclusion. 2 System Description We formulate Chinese word segmentation task into a sequence labeling problem and use CRF to train the segmentation model. Our implementation of CRF-based CWS system uses the CRF++1 package by Taku Kudo. We regard “, ”, “。 ”,“?”,“!”,“�” as the boundary of a sentence and both the training and testing corpora are segmented by these boundaries. Zhao et al. (2006b) prove that CRF segmentation performance using 6-tag set for training is better than other tag set, so we adopt 6-tag (B,B2, B3,M,E,S) set labeling the characters in words. Table 1 explains how to label the characters in words with different length. We follow six n-gram character features that are used in (Zhao et al., 2006b; Zhao and Kit, 2008), as C−1, C0, C1, C−1C0, C0C1 and C−1C1 respectively, in which C represents the character, subscript -1, 0 and 1 means the previous character, the current character and the next character. With respect to the other features in our system, the similar </context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, and Mu Li. 2006a. An improved chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, volume 1082117. Sydney: July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Effective tag set selection in chinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of PACLIC,</booktitle>
<volume>20</volume>
<pages>87--94</pages>
<contexts>
<context position="1663" citStr="Zhao et al., 2006" startWordPosition="251" endWordPosition="254">ures we used share a unified feature template trained by CRF. Our system achieves a competitive F score of 0.9730 for this bakeoff. 1 Introduction Word is the fundamental unit in natural language understanding. Since people do not retain the boundary information between words in practical use, Chinese Word Segmentation (CWS) is the very first step in Chinese information processing. A considerable amount of research has shown that using character sequence labeling is a simple but effective formulation of Chinese word segmentation task (Xue and others, 2003; Peng et al., 2004; Low et al., 2005; Zhao et al., 2006a), among which the method using sequence labeling based on CRF (Lafferty et al., 2001) is widely used with attractive performance. However, most of the existing segmentation systems greatly rely on data that the model was trained over. The segmentation performance tends to would reduce significantly when the test data differs greatly from the training data in phraseology and vocabulary. Exploiting corpora in multi-domain for model learning can solve the problem above directly, whereas labeling corpora manually costs a lot, so that it is unrealistic to label mass corpora. So far there are two </context>
<context position="4552" citStr="Zhao et al. (2006" startWordPosition="718" endWordPosition="721">107, Wuhan, China, 20-21 October 2014 Table 1: Illustration of character tagging The experimental result that tested on Bakeoff dataset compared with the best official result is provided in Section 4. Section 5 leads to the conclusion. 2 System Description We formulate Chinese word segmentation task into a sequence labeling problem and use CRF to train the segmentation model. Our implementation of CRF-based CWS system uses the CRF++1 package by Taku Kudo. We regard “, ”, “。 ”,“?”,“!”,“�” as the boundary of a sentence and both the training and testing corpora are segmented by these boundaries. Zhao et al. (2006b) prove that CRF segmentation performance using 6-tag set for training is better than other tag set, so we adopt 6-tag (B,B2, B3,M,E,S) set labeling the characters in words. Table 1 explains how to label the characters in words with different length. We follow six n-gram character features that are used in (Zhao et al., 2006b; Zhao and Kit, 2008), as C−1, C0, C1, C−1C0, C0C1 and C−1C1 respectively, in which C represents the character, subscript -1, 0 and 1 means the previous character, the current character and the next character. With respect to the other features in our system, the similar </context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2006b. Effective tag set selection in chinese word segmentation via conditional random field modeling. In Proceedings of PACLIC, volume 20, pages 87–94.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
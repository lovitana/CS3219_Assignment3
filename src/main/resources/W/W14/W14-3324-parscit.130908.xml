<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005798">
<title confidence="0.993214">
Edinburgh’s Syntax-Based Systems at WMT 2014
</title>
<author confidence="0.998567">
Philip Williams1, Rico Sennrich1, Maria Nadejde1,
Matthias Huck1, Eva Hasler1, Philipp Koehn1,2
</author>
<affiliation confidence="0.945457">
1School of Informatics, University of Edinburgh
2Center for Speech and Language Processing, The Johns Hopkins University
</affiliation>
<sectionHeader confidence="0.977227" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995849888888889">
This paper describes the string-to-tree sys-
tems built at the University of Edin-
burgh for the WMT 2014 shared trans-
lation task. We developed systems for
English-German, Czech-English, French-
English, German-English, Hindi-English,
and Russian-English. This year we
improved our English-German system
through target-side compound splitting,
morphosyntactic constraints, and refine-
ments to parse tree annotation; we ad-
dressed the out-of-vocabulary problem us-
ing transliteration for Hindi and Rus-
sian and using morphological reduction
for Russian; we improved our German-
English system through tree binarization;
and we reduced system development time
by filtering the tuning sets.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993703">
For this year’s WMT shared translation task we
built syntax-based systems for six language pairs:
</bodyText>
<listItem confidence="0.999901666666667">
• English-German • German-English
• Czech-English • Hindi-English
• French-English • Russian-English
</listItem>
<bodyText confidence="0.999905885714286">
As last year (Nadejde et al., 2013), our systems are
based on the string-to-tree pipeline implemented
in the Moses toolkit (Koehn et al., 2007).
We paid particular attention to the production of
grammatical German, trying various parsers and
incorporating target-side compound splitting and
morphosyntactic constraints; for Hindi and Rus-
sian, we employed the new Moses transliteration
model to handle out-of-vocabulary words; and for
German to English, we experimented with tree bi-
narization, obtaining good results from right bina-
rization.
We also present our first syntax-based results
for French-English, the scale of which defeated us
last year. This year we were able to train a sys-
tem using all available training data, a task that
was made considerably easier through principled
filtering of the tuning set. Although our system
was not ready in time for human evaluation, we
present BLEU scores in this paper.
In addition to the five single-system submis-
sions described here, we also contributed our
English-German and German-English systems for
use in the collaborative EU-BRIDGE system com-
bination effort (Freitag et al., 2014).
This paper is organised as follows. In Sec-
tion 2 we describe the core setup that is com-
mon to all systems. In subsequent sections we de-
scribe language-pair specific variations and exten-
sions. For each language pair, we present results
for both the development test set (newstest2013
in most cases) and for the filtered test set (new-
stest2014) that was provided after the system sub-
mission deadline. We refer to these as ‘devtest’
and ‘test’, respectively.
</bodyText>
<sectionHeader confidence="0.978222" genericHeader="method">
2 System Overview
</sectionHeader>
<subsectionHeader confidence="0.993309">
2.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999869375">
The training data was normalized using the WMT
normalize-punctuation.perl script then
tokenized and truecased. Where the target lan-
guage was English, we used the Moses tokenizer’s
-penn option, which uses a tokenization scheme
that more closely matches that of the parser. For
the English-German system we used the default
Moses tokenization scheme, which is similar to
that of the German parsers.
For the systems that translate into English, we
used the Berkeley parser (Petrov et al., 2006;
Petrov and Klein, 2007) to parse the target-side of
the training corpus. As we will describe in Sec-
tion 3, we tried a variety of parsers for German.
We did not perform any corpus filtering other
than the standard Moses method, which removes
</bodyText>
<page confidence="0.967009">
207
</page>
<bodyText confidence="0.7403526">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207–214,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
sentence pairs with dubious length ratios and sen-
tence pairs where parsing fails for the target-side
sentence.
</bodyText>
<subsectionHeader confidence="0.986861">
2.2 Translation Model
</subsectionHeader>
<bodyText confidence="0.998716263157895">
Our translation grammar is a synchronous context-
free grammar (SCFG) with phrase-structure labels
on the target side and the generic non-terminal la-
bel X on the source side.
The grammar was extracted from the word-
aligned parallel data using the Moses implemen-
tation (Williams and Koehn, 2012) of the GHKM
algorithm (Galley et al., 2004; Galley et al., 2006).
For word alignment we used MGIZA++ (Gao and
Vogel, 2008), a multi-threaded implementation of
GIZA++ (Och and Ney, 2003).
Minimal GHKM rules were composed into
larger rules subject to parameterized restrictions
on size defined in terms of the resulting target tree
fragment. A good choice of parameter settings
depends on the annotation style of the target-side
parse trees. We used the settings shown in Table 1,
which were chosen empirically during the devel-
opment of last years’ systems:
</bodyText>
<figure confidence="0.86289475">
Parameter Value
Rule depth 5
Node count 20
Rule size 5
</figure>
<tableCaption confidence="0.99646">
Table 1: Parameter settings for rule composition.
</tableCaption>
<bodyText confidence="0.999889875">
Further to the restrictions on rule composition,
fully non-lexical unary rules were eliminated us-
ing the method described in Chung et al. (2011)
and rules with scope greater than 3 (Hopkins and
Langmead, 2010) were pruned from the trans-
lation grammar. Scope pruning makes parsing
tractable without the need for grammar binariza-
tion.
</bodyText>
<subsectionHeader confidence="0.992522">
2.3 Language Model
</subsectionHeader>
<bodyText confidence="0.999969285714286">
We used all available monolingual data to train
5-gram language models. Language models
for each monolingual corpus were trained using
the SRILM toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) and then interpolated using weights tuned to
minimize perplexity on the development set.
</bodyText>
<subsectionHeader confidence="0.991817">
2.4 Feature Functions
</subsectionHeader>
<bodyText confidence="0.999731142857143">
Our feature functions are unchanged from the pre-
vious two years. They include the n-gram lan-
guage model probability of the derivation’s target
yield, its word count, and various scores for the
synchronous derivation.
Each grammar rule has a number of pre-
computed scores. For a grammar rule r of the form
</bodyText>
<equation confidence="0.968453">
C — (α, Q, -)
</equation>
<bodyText confidence="0.995604">
where C is a target-side non-terminal label, α is a
string of source terminals and non-terminals, Q is
a string of target terminals and non-terminals, and
— is a one-to-one correspondence between source
and target non-terminals, we score the rule accord-
ing to the following functions:
</bodyText>
<listItem confidence="0.998527090909091">
• p (C, Q  |α, —) and p (α  |C, Q, —), the direct
and indirect translation probabilities.
• plex (Q  |α) and plex (α  |Q), the direct and
indirect lexical weights (Koehn et al., 2003).
• ppcfg (7r), the monolingual PCFG probability
of the tree fragment 7r from which the rule
was extracted.
• exp(—1/count(r)), a rule rareness penalty.
• exp(1), a rule penalty. The main grammar
and glue grammars have distinct penalty fea-
tures.
</listItem>
<subsectionHeader confidence="0.991851">
2.5 Tuning
</subsectionHeader>
<bodyText confidence="0.999978142857143">
The feature weights were tuned using the Moses
implementation of MERT (Och, 2003) for all sys-
tems except English-to-German, for which we
used k-best MIRA (Cherry and Foster, 2012) due
to the larger number of features.
We used tuning sentences drawn from all of
the previous years’ test sets (except newstest2013,
which was used as the development test set). In
order to speed up the tuning process, we used sub-
sets of the full tuning sets with sentence pairs up
to length 30 (Max-30) and further applied a fil-
tering technique to reduce the tuning set size to
2,000 sentence pairs for the language pairs involv-
ing German, French and Czech1. We also experi-
mented with random subsets of size 2,000.
For the filtering technique, we make the as-
sumption that finding suitable weights for all the
feature functions requires the optimizer to see a
range of feature values and to see hypotheses that
can partially match the reference translations in
order to rank the hypotheses. For example, if a
</bodyText>
<footnote confidence="0.969556">
1For Russian and Hindi, the development sets are smaller
and no filtering was applied.
</footnote>
<page confidence="0.996175">
208
</page>
<bodyText confidence="0.999922541666667">
tuning example contains many out-of-vocabulary
words or is difficult to translate for other reasons,
this will result in low quality translation hypothe-
ses and provide the system with little evidence for
which features are useful to produce good transla-
tions. Therefore, we select high quality examples
using a smooth version of sentence-BLEU com-
puted on the 1-best output of a single decoder run
on the development set. Standard sentence-BLEU
tends to select short examples because they are
more likely to have perfect n-gram matches with
the reference translation. Very short sentence pairs
are less informative for tuning but also tend to have
more extreme source-target length ratios which
can affect the weight of the word penalty. Thus,
we penalize short examples by padding the de-
coder output with a fixed number of non-matching
tokens2 to the left and right before computing
sentence-BLEU. This has the effect of reducing
the precision of short sentences against the refer-
ence translation while affecting longer sentences
proportionally less. Experiments on phrase-based
systems have shown that the resulting tuning sets
are of comparable diversity as randomly selected
sets in terms of their feature vectors and maintain
BLEU scores in comparison with tuning on the en-
tire development set.
Table 2 shows the size of the full tuning sets
and the size of the subsets with up to length 30,
Table 3 shows the results of tuning with different
sets. Reducing the tuning sets to Max-30 results
in a speed-up in tuning time but affects the per-
formance on some of the devtest/test sets (mostly
for Czech-English). However, tuning on the full
set took more than 18 days using 12 cores for
German-English which is not feasible when try-
ing out several model variations. Further filter-
ing these subsets to a size of 2,000 sentence pairs
as described above maintains the BLEU scores in
most cases and even improves the scores in some
cases. This indicates that the quality of the se-
lected examples is more important than the total
number of tuning examples. However, the exper-
iments with random subsets from Max-30 show
that random selection also yields results which im-
prove over the results with Max-30 in most cases,
though are not always as good as with the filtered
sets.3 The filtered tuning sets yield reasonable per-
</bodyText>
<footnote confidence="0.97452925">
2These can be arbitrary tokens that do not match any ref-
erence token.
3For random subsets from the full tuning set the perfor-
mance was similar but resulted in standard deviations of up
</footnote>
<bodyText confidence="0.999684">
formance compared to the full tuning sets except
for the German-English devtest set where perfor-
mance drops by 0.5 BLEU4.
</bodyText>
<table confidence="0.988203666666667">
Tuning set Cs-En En-De De-En
Full 13,055 13,071 13,071
Max-30 10,392 9,151 10,610
</table>
<tableCaption confidence="0.999513">
Table 2: Size of full tuning sets and with sentence
length up to 30.
</tableCaption>
<table confidence="0.99973975">
Tuning set Cs-En devtest De-En
En-De
Full 25.1 19.9 26.7
Max-30 24.7 19.8 26.2
Filtered 24.9 19.8 26.2
Random 24.8 19.7 26.4
test
Tuning set Cs-En En-De De-En
Full 27.5 19.2 26.9
Max-30 27.2 19.2 27.0
Filtered 27.5 19.1 27.2
Random 27.3 19.4 27.0
</table>
<tableCaption confidence="0.9839">
Table 3: BLEU results on devtest and test sets with
</tableCaption>
<bodyText confidence="0.825398333333333">
different tuning sets: Full, Max-30, filtered subsets
of Max-30 and average of three random subsets of
Max-30 (size of filtered/random subsets: 2,000).
</bodyText>
<sectionHeader confidence="0.9741" genericHeader="method">
3 English to German
</sectionHeader>
<bodyText confidence="0.980947882352941">
We use the projective output of the dependency
parser ParZu (Sennrich et al., 2013) for the syn-
tactic annotation of our primary submission. Con-
trastive systems were built with other parsers: Bit-
Par (Schmid, 2004), the German Stanford Parser
(Rafferty and Manning, 2008), and the German
Berkeley Parser (Petrov and Klein, 2007; Petrov
and Klein, 2008).
The set of syntactic labels provided by ParZu
has been refined to reduce overgeneralization phe-
nomena. Specifically, we disambiguate the labels
ROOT (used for the root of a sentence, but also
commas, punctuation marks, and sentence frag-
ments), KON and CJ (coordinations of different
constituents), and GMOD (pre- or postmodifying
genitive modifier).
to 0.36 across three random sets.
</bodyText>
<footnote confidence="0.993915">
4Note however that due to the long tuning times, we are
reporting single tuning runs.
</footnote>
<page confidence="0.993178">
209
</page>
<figure confidence="0.9912871">
NN
COMP
SEGMENT
SEGMENT
COMP
JUNC
@s@
berufung
gericht
JUN
</figure>
<page confidence="0.930764">
210
</page>
<table confidence="0.794432">
system devtest test
baseline 86,341,766 88,657,327
</table>
<tableCaption confidence="0.915936333333333">
Table 7: Grammar sizes of the French to En-
glish system after filtering for the devtest (new-
stest2013) and test (newstest2014) sets.
</tableCaption>
<sectionHeader confidence="0.918206" genericHeader="method">
6 German to English
</sectionHeader>
<bodyText confidence="0.99958595">
German compounds were split using the script
provided with Moses.
For training the primary system, the target parse
trees were restructured before rule extraction by
right binarization. Since binarization strategies
increase the tree depth and number of nodes by
adding virtual non-terminals, we increased the ex-
traction parameters to: Rule Depth = 7, Node
Count = 100, Rule Size = 7. A thorough in-
vestigation of binarization methods for restructur-
ing Penn Treebank style trees was carried out by
Wang et al. (2007).
Table 8 shows BLEU scores for the baseline
system and two systems employing different bi-
narization strategies. Table 9 shows the result-
ing grammar sizes after filtering for the evaluation
sets. Results on the development set showed no
improvement when left binarization was used for
restructuring the trees, although the grammar size
increased significantly.
</bodyText>
<table confidence="0.961542">
BLEU
system devtest test
baseline 26.2 27.2
+ right binarization (primary) 26.8 28.2
+ left binarization 26.3 -
</table>
<tableCaption confidence="0.977555">
Table 8: German to English results on the devtest
(newsdev2013) and test (newstest2014) sets.
</tableCaption>
<table confidence="0.991638">
system devtest test
baseline 11,462,976 13,811,304
+ right binarization 24,851,982 29,133,910
+ left binarization 21,387,976 -
</table>
<tableCaption confidence="0.919628">
Table 9: Grammar sizes of the German to En-
glish systems after filtering for the devtest (new-
stest2013) and test (newstest2014) sets.
</tableCaption>
<sectionHeader confidence="0.925548" genericHeader="method">
7 Hindi to English
</sectionHeader>
<bodyText confidence="0.995497441860465">
English-Hindi has the least parallel training data
of this year’s language pairs. Out-of-vocabulary
(OOV) input words are therefore a comparatively
large source of translation error: in the devtest set
(newsdev2014) and filtered test set (newstest2014)
the average OOV rates are 1.08 and 1.16 unknown
words per sentence, respectively.
Assuming a significant fraction of OOV words
to be named entities and thus amenable to translit-
eration, we applied the post-processing translitera-
tion method described in Durrani et al. (2014) and
implemented in Moses. In brief, this is an unsuper-
vised method that i) uses EM to induce a corpus of
transliteration examples from the parallel training
data; ii) learns a monotone character-level phrase-
based SMT model from the transliteration corpus;
and iii) substitutes transliterations for OOVs in the
system output by using the monolingual language
model and other features to select between translit-
eration candidates.5
Table 10 shows BLEU scores with and without
transliteration on the devtest and filtered test sets.
Due to a bug in the submitted system, the language
model trained on the HindEnCorp corpus was used
for transliteration candidate selection rather than
the full interpolated language model. This was
fixed subsequent to submission.
BLEU
system devtest test
baseline
+ transliteration (submission)
+ transliteration (fixed)
Table 10: Hindi to English results with and with-
out transliteration on the devtest (newsdev2014)
and test (newstest2014) sets.
Transliteration increased 1-gram precision from
48.1% to 49.4% for devtest and from 49.1% to
50.6% for test. Of the 2,913 OOV words in test,
938 (32.2%) of transliterations exactly match the
reference. Manual inspection reveals that there are
also many near matches. For instance, translitera-
tion produces Bernat Jackie where the reference is
Jacqui Barnat.
</bodyText>
<sectionHeader confidence="0.905737" genericHeader="method">
8 Russian to English
</sectionHeader>
<bodyText confidence="0.98654">
Compared to Hindi-English, the Russian-English
language pair has over six times as much parallel
data. Nonetheless, OOVs remain a problem: the
average OOV rates are approximately half those
</bodyText>
<footnote confidence="0.9190755">
5This is the variant referred to as Method 2 in Dur-
rani et al. (2014).
</footnote>
<figure confidence="0.68524">
12.9 14.7
13.3 15.1
13.6 15.5
</figure>
<page confidence="0.996951">
211
</page>
<bodyText confidence="0.99966437254902">
of Hindi-English, at 0.47 and 0.51 unknown words
per sentence for the devtest (newstest2013) and fil-
tered test (newstest2014) sets, respectively. We
address this in part using the same transliteration
method as for Hindi-English.
Data sparsity issues for this language pair are
exacerbated by the rich inflectional morphology of
Russian. Many Russian word forms express gram-
matical distinctions that are either absent from En-
glish translations (like grammatical gender) or are
expressed by different means (like grammatical
function being expressed through syntactic config-
uration rather than case). We adopt the widely-
used approach of simplifying morphologically-
complex source forms to remove distinctions that
we believe to be redundant. Our method is simi-
lar to that of Weller et al. (2013) except that ours
is much more conservative (in their experiments,
Weller et al. (2013) found morphological reduc-
tion to harm translation indicating that useful in-
formation was likely to have been discarded).
We used TreeTagger (Schmid, 1994) to obtain
a lemma-tag pair for each Russian word. The tag
specifies the word class and various morphosyn-
tactic feature values. For example, the adjective
peciiy6m4Kaacxax (‘republican’) gets the lemma-
tag pair peciiy6m4Kaacx41rI + Afpfsnf, where
the code A indicates the word class and the re-
maining codes indicate values for the type, degree,
gender, number, case, and definiteness features.
Like Weller et al. (2013), we selectively re-
placed surface forms with their lemmas and re-
duced tags, reducing tags through feature dele-
tion. We restricted morphological reduction to ad-
jectives and verbs, leaving all other word forms
unchanged. Table 11 shows the features that
were deleted. We focused on contextual inflec-
tion, making the assumption that inflectional dis-
tinctions required by agreement alone were the
least likely to be useful for translation (since the
same information was marked elsewhere in the
sentence) and also the most likely to be the source
of ‘spurious’ variation.
Table 12 shows the BLEU scores for Russian-
English with transliteration and morphological re-
duction. The effect of transliteration was smaller
than for Hindi-English, as might be expected from
the lower baseline OOV rate. 1-gram precision in-
creased from 57.1% to 57.6% for devtest and from
62.9% to 63.6% for test. Morphological reduction
decreased the initial OOV rates by 3.5% and 4.1%
</bodyText>
<table confidence="0.999234181818182">
Adjective Verb
Type X Type X
Degree ✓ VForm ✓
Gender X Tense ✓
Number X Person ✓
Case X Number ✓
Definiteness X Gender X
Voice ✓
Definiteness X
Aspect ✓
Case ✓
</table>
<tableCaption confidence="0.949386666666667">
Table 11: Feature values that are retained (✓)
or deleted (X) during morphological reduction of
Russian.
</tableCaption>
<table confidence="0.9946588">
BLEU
system devtest test
baseline 23.3 29.7
+ transliteration 23.7 30.3
+ morphological reduction 23.8 30.3
</table>
<tableCaption confidence="0.9570855">
Table 12: Russian to English results on the devtest
(newstest2013) and test (newstest2014) sets.
</tableCaption>
<bodyText confidence="0.993363">
on the devtest and filtered test sets. After both
morphological and transliteration the 1-gram pre-
cisions for devtest and test were 57.7% and 63.8%.
</bodyText>
<sectionHeader confidence="0.979004" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999916461538462">
We have described Edinburgh’s syntax-based sys-
tems in the WMT 2014 shared translation task.
Building upon the already-strong string-to-tree
systems developed for previous years’ shared
translation tasks, we have achieved substantial im-
provements over our baseline setup: we improved
translation into German through target-side com-
pound splitting, morphosyntactic constraints, and
refinements to parse tree annotation; we have ad-
dressed unknown words using transliteration (for
Hindi and Russian) and morphological reduction
(for Russian); and we have improved our German-
English system through tree binarization.
</bodyText>
<sectionHeader confidence="0.980158" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9973955">
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement no 287658 (EU-BRIDGE).
</bodyText>
<footnote confidence="0.739889">
Rico Sennrich has received funding from the
Swiss National Science Foundation under grant
P2ZHP1_148717.
</footnote>
<page confidence="0.996503">
212
</page>
<sectionHeader confidence="0.958595" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999513954954955">
Stanley F. Chen and Joshua Goodman. 1998. An em-
pirical study of smoothing techniques for language
modeling. Technical report, Harvard University.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427–436, Montréal, Canada, June. Association for
Computational Linguistics.
Tagyoung Chung, Licheng Fang, and Daniel Gildea.
2011. Issues concerning decoding with synchronous
context-free grammar. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 413–417, Portland, Oregon, USA, June.
Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp
Koehn. 2014. Integrating an Unsupervised Translit-
eration Model into Statistical Machine Translation.
In Proceedings of the 15th Conference of the Euro-
pean Chapter of the ACL (EACL 2014), Gothenburg,
Sweden, April. To appear.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In In Proc. NAACL/HLT 2013,
pages 644–648.
Markus Freitag, Stephan Peitz, Joern Wuebker, Her-
mann Ney, Matthias Huck, Rico Sennrich, Nadir
Durrani, Maria Nadejde, Philip Williams, Philipp
Koehn, Teresa Herrmann, Eunah Cho, and Alex
Waibel. 2014. EU-BRIDGE MT: Combined Ma-
chine Translation. In Proceedings of the ACL 2014
Ninth Workshop on Statistical Machine Translation,
Baltimore, MD, USA, June.
Fabienne Fritzinger and Alexander Fraser. 2010. How
to Avoid Burning Ducks: Combining Linguistic
Analysis and Corpus Statistics for German Com-
pound Processing. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, WMT ’10, pages 224–234, Uppsala,
Sweden.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a Translation Rule?
In HLT-NAACL ’04.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In ACL-
44: Proceedings of the 21st International Confer-
ence on Computational Linguistics and the 44th an-
nual meeting of the Association for Computational
Linguistics, pages 961–968, Morristown, NJ, USA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ’08, pages 49–
57, Stroudsburg, PA, USA.
Mark Hopkins and Greg Langmead. 2010. SCFG de-
coding without binarization. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 646–655, Cambridge,
MA, October.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ’03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48–54, Morristown, NJ, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ’07, pages 177–180, Morristown, NJ, USA.
Association for Computational Linguistics.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh’s Syntax-Based Machine Transla-
tion Systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 170–
176, Sofia, Bulgaria, August.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19–51, March.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, ACL ’03, pages
160–167, Morristown, NJ, USA.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 404–411, Rochester, New York, April.
Slav Petrov and Dan Klein. 2008. Parsing German
with Latent Variable Grammars. In Proceedings of
the Workshop on Parsing German at ACL ’08, pages
33–39, Columbus, OH, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433–440.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing Three German Treebanks: Lexicalized and
Unlexicalized Baselines. In Proceedings of the
</reference>
<page confidence="0.988959">
213
</page>
<reference confidence="0.999887941176471">
Workshop on Parsing German atACL ’08, pages 40–
46, Columbus, OH, USA, June.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In International Con-
ference on New Methods in Language Processing,
pages 44–49, Manchester, UK.
Helmut Schmid. 2004. Efficient Parsing of Highly
Ambiguous Context-Free Grammars with Bit Vec-
tors. In Proc. of the Int. Conf. on Computational
Linguistics (COLING), Geneva, Switzerland, Au-
gust.
Rico Sennrich and Beat Kunz. 2014. Zmorge: A Ger-
man Morphological Lexicon Extracted from Wik-
tionary. In Proceedings of the 9th International
Conference on Language Resources and Evaluation
(LREC 2014), Reykjavik, Iceland, May.
Rico Sennrich, Martin Volk, and Gerold Schneider.
2013. Exploiting Synergies Between Open Re-
sources for German Dependency Parsing, POS-
tagging, and Morphological Analysis. In Proceed-
ings of the International Conference Recent Ad-
vances in Natural Language Processing 2013, pages
601–609, Hissar, Bulgaria.
Andreas Stolcke. 2002. SRILM - an extensible
language modeling toolkit. In Intl. Conf. Spoken
Language Processing, Denver, Colorado, September
2002.
Wei Wang, Kevin Knight, Daniel Marcu, and Marina
Rey. 2007. Binarizing Syntax Trees to Improve
Syntax-Based Machine Translation Accuracy. In
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning, pages 746–754.
Marion Weller, Max Kisselew, Svetlana Smekalova,
Alexander Fraser, Helmut Schmid, Nadir Durrani,
Hassan Sajjad, and Richárd Farkas. 2013. Munich-
Edinburgh-Stuttgart submissions at WMT13: Mor-
phological and syntactic processing for SMT. In
Proceedings of the Eighth Workshop on Statistical
Machine Translation, pages 232–239, Sofia, Bul-
garia, August.
Philip Williams and Philipp Koehn. 2011. Agreement
Constraints for Statistical Machine Translation into
German. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 217–226, Ed-
inburgh, Scotland, July.
Philip Williams and Philipp Koehn. 2012. GHKM
Rule Extraction and Scope-3 Parsing in Moses. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 388–394, Montréal,
Canada, June.
</reference>
<page confidence="0.998953">
214
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.472478">
<title confidence="0.682477">Edinburgh’s Syntax-Based Systems at WMT 2014</title>
<author confidence="0.9303165">Rico Maria Eva Philipp</author>
<affiliation confidence="0.972709">of Informatics, University of for Speech and Language Processing, The Johns Hopkins University</affiliation>
<abstract confidence="0.981412842105263">This paper describes the string-to-tree systems built at the University of Edinburgh for the WMT 2014 shared translation task. We developed systems for English-German, Czech-English, French- English, German-English, Hindi-English, and Russian-English. This year improved our English-German system through target-side compound splitting, morphosyntactic constraints, and refinements to parse tree annotation; we addressed the out-of-vocabulary problem using transliteration for Hindi and Russian and using morphological reduction for Russian; we improved our German- English system through tree binarization; and we reduced system development time by filtering the tuning sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>Harvard University.</institution>
<contexts>
<context position="5373" citStr="Chen and Goodman, 1998" startWordPosition="819" endWordPosition="822">: Parameter settings for rule composition. Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 2.3 Language Model We used all available monolingual data to train 5-gram language models. Language models for each monolingual corpus were trained using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) and then interpolated using weights tuned to minimize perplexity on the development set. 2.4 Feature Functions Our feature functions are unchanged from the previous two years. They include the n-gram language model probability of the derivation’s target yield, its word count, and various scores for the synchronous derivation. Each grammar rule has a number of precomputed scores. For a grammar rule r of the form C — (α, Q, -) where C is a target-side non-terminal label, α is a string of source terminals and non-terminals, Q is a string of target terminals and non-terminals, and — is a one-to-o</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>Stanley F. Chen and Joshua Goodman. 1998. An empirical study of smoothing techniques for language modeling. Technical report, Harvard University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch tuning strategies for statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>427--436</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montréal, Canada,</location>
<contexts>
<context position="6703" citStr="Cherry and Foster, 2012" startWordPosition="1045" endWordPosition="1048">nctions: • p (C, Q |α, —) and p (α |C, Q, —), the direct and indirect translation probabilities. • plex (Q |α) and plex (α |Q), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (7r), the monolingual PCFG probability of the tree fragment 7r from which the rule was extracted. • exp(—1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. 2.5 Tuning The feature weights were tuned using the Moses implementation of MERT (Och, 2003) for all systems except English-to-German, for which we used k-best MIRA (Cherry and Foster, 2012) due to the larger number of features. We used tuning sentences drawn from all of the previous years’ test sets (except newstest2013, which was used as the development test set). In order to speed up the tuning process, we used subsets of the full tuning sets with sentence pairs up to length 30 (Max-30) and further applied a filtering technique to reduce the tuning set size to 2,000 sentence pairs for the language pairs involving German, French and Czech1. We also experimented with random subsets of size 2,000. For the filtering technique, we make the assumption that finding suitable weights f</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch tuning strategies for statistical machine translation. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 427–436, Montréal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tagyoung Chung</author>
<author>Licheng Fang</author>
<author>Daniel Gildea</author>
</authors>
<title>Issues concerning decoding with synchronous context-free grammar.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>413--417</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="4937" citStr="Chung et al. (2011)" startWordPosition="752" endWordPosition="755"> Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Value Rule depth 5 Node count 20 Rule size 5 Table 1: Parameter settings for rule composition. Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 2.3 Language Model We used all available monolingual data to train 5-gram language models. Language models for each monolingual corpus were trained using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) and then interpolated using weights tuned to minimize perplexity on the development set. 2.4 Feature Functions Our feature functions are unchanged from the previou</context>
</contexts>
<marker>Chung, Fang, Gildea, 2011</marker>
<rawString>Tagyoung Chung, Licheng Fang, and Daniel Gildea. 2011. Issues concerning decoding with synchronous context-free grammar. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 413–417, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadir Durrani</author>
<author>Hassan Sajjad</author>
<author>Hieu Hoang</author>
<author>Philipp Koehn</author>
</authors>
<title>Integrating an Unsupervised Transliteration Model into Statistical Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 15th Conference of the European Chapter of the ACL (EACL 2014),</booktitle>
<location>Gothenburg, Sweden,</location>
<note>To appear.</note>
<contexts>
<context position="13831" citStr="Durrani et al. (2014)" startWordPosition="2206" endWordPosition="2209">ter filtering for the devtest (newstest2013) and test (newstest2014) sets. 7 Hindi to English English-Hindi has the least parallel training data of this year’s language pairs. Out-of-vocabulary (OOV) input words are therefore a comparatively large source of translation error: in the devtest set (newsdev2014) and filtered test set (newstest2014) the average OOV rates are 1.08 and 1.16 unknown words per sentence, respectively. Assuming a significant fraction of OOV words to be named entities and thus amenable to transliteration, we applied the post-processing transliteration method described in Durrani et al. (2014) and implemented in Moses. In brief, this is an unsupervised method that i) uses EM to induce a corpus of transliteration examples from the parallel training data; ii) learns a monotone character-level phrasebased SMT model from the transliteration corpus; and iii) substitutes transliterations for OOVs in the system output by using the monolingual language model and other features to select between transliteration candidates.5 Table 10 shows BLEU scores with and without transliteration on the devtest and filtered test sets. Due to a bug in the submitted system, the language model trained on th</context>
<context position="15447" citStr="Durrani et al. (2014)" startWordPosition="2454" endWordPosition="2458">ncreased 1-gram precision from 48.1% to 49.4% for devtest and from 49.1% to 50.6% for test. Of the 2,913 OOV words in test, 938 (32.2%) of transliterations exactly match the reference. Manual inspection reveals that there are also many near matches. For instance, transliteration produces Bernat Jackie where the reference is Jacqui Barnat. 8 Russian to English Compared to Hindi-English, the Russian-English language pair has over six times as much parallel data. Nonetheless, OOVs remain a problem: the average OOV rates are approximately half those 5This is the variant referred to as Method 2 in Durrani et al. (2014). 12.9 14.7 13.3 15.1 13.6 15.5 211 of Hindi-English, at 0.47 and 0.51 unknown words per sentence for the devtest (newstest2013) and filtered test (newstest2014) sets, respectively. We address this in part using the same transliteration method as for Hindi-English. Data sparsity issues for this language pair are exacerbated by the rich inflectional morphology of Russian. Many Russian word forms express grammatical distinctions that are either absent from English translations (like grammatical gender) or are expressed by different means (like grammatical function being expressed through syntact</context>
</contexts>
<marker>Durrani, Sajjad, Hoang, Koehn, 2014</marker>
<rawString>Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp Koehn. 2014. Integrating an Unsupervised Transliteration Model into Statistical Machine Translation. In Proceedings of the 15th Conference of the European Chapter of the ACL (EACL 2014), Gothenburg, Sweden, April. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of ibm model 2. In</title>
<date>2013</date>
<booktitle>In Proc. NAACL/HLT 2013,</booktitle>
<pages>644--648</pages>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A. Smith. 2013. A simple, fast, and effective reparameterization of ibm model 2. In In Proc. NAACL/HLT 2013, pages 644–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Freitag</author>
<author>Stephan Peitz</author>
<author>Joern Wuebker</author>
<author>Hermann Ney</author>
<author>Matthias Huck</author>
<author>Rico Sennrich</author>
<author>Nadir Durrani</author>
</authors>
<title>EU-BRIDGE MT: Combined Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation,</booktitle>
<location>Maria Nadejde, Philip Williams, Philipp Koehn, Teresa</location>
<contexts>
<context position="2297" citStr="Freitag et al., 2014" startWordPosition="329" endWordPosition="332"> from right binarization. We also present our first syntax-based results for French-English, the scale of which defeated us last year. This year we were able to train a system using all available training data, a task that was made considerably easier through principled filtering of the tuning set. Although our system was not ready in time for human evaluation, we present BLEU scores in this paper. In addition to the five single-system submissions described here, we also contributed our English-German and German-English systems for use in the collaborative EU-BRIDGE system combination effort (Freitag et al., 2014). This paper is organised as follows. In Section 2 we describe the core setup that is common to all systems. In subsequent sections we describe language-pair specific variations and extensions. For each language pair, we present results for both the development test set (newstest2013 in most cases) and for the filtered test set (newstest2014) that was provided after the system submission deadline. We refer to these as ‘devtest’ and ‘test’, respectively. 2 System Overview 2.1 Pre-processing The training data was normalized using the WMT normalize-punctuation.perl script then tokenized and truec</context>
</contexts>
<marker>Freitag, Peitz, Wuebker, Ney, Huck, Sennrich, Durrani, 2014</marker>
<rawString>Markus Freitag, Stephan Peitz, Joern Wuebker, Hermann Ney, Matthias Huck, Rico Sennrich, Nadir Durrani, Maria Nadejde, Philip Williams, Philipp Koehn, Teresa Herrmann, Eunah Cho, and Alex Waibel. 2014. EU-BRIDGE MT: Combined Machine Translation. In Proceedings of the ACL 2014 Ninth Workshop on Statistical Machine Translation, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabienne Fritzinger</author>
<author>Alexander Fraser</author>
</authors>
<title>How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10,</booktitle>
<pages>224--234</pages>
<location>Uppsala,</location>
<marker>Fritzinger, Fraser, 2010</marker>
<rawString>Fabienne Fritzinger and Alexander Fraser. 2010. How to Avoid Burning Ducks: Combining Linguistic Analysis and Corpus Statistics for German Compound Processing. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, WMT ’10, pages 224–234, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a Translation Rule? In</title>
<date>2004</date>
<booktitle>HLT-NAACL ’04.</booktitle>
<contexts>
<context position="4174" citStr="Galley et al., 2004" startWordPosition="629" endWordPosition="632">Workshop on Statistical Machine Translation, pages 207–214, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Value Rule depth 5 Node count 20 Rule size 5 Table 1: Parameter settings for</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a Translation Rule? In HLT-NAACL ’04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In ACL44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>961--968</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="4196" citStr="Galley et al., 2006" startWordPosition="633" endWordPosition="636">al Machine Translation, pages 207–214, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Value Rule depth 5 Node count 20 Rule size 5 Table 1: Parameter settings for rule composition. Fur</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In ACL44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, pages 961–968, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel implementations of word alignment tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP ’08,</booktitle>
<pages>49--57</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4254" citStr="Gao and Vogel, 2008" startWordPosition="643" endWordPosition="646"> USA, June 26–27, 2014. c�2014 Association for Computational Linguistics sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Value Rule depth 5 Node count 20 Rule size 5 Table 1: Parameter settings for rule composition. Further to the restrictions on rule composition, fully non-le</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, SETQA-NLP ’08, pages 49– 57, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Greg Langmead</author>
</authors>
<title>SCFG decoding without binarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>646--655</pages>
<location>Cambridge, MA,</location>
<contexts>
<context position="5002" citStr="Hopkins and Langmead, 2010" startWordPosition="763" endWordPosition="766">ct to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Value Rule depth 5 Node count 20 Rule size 5 Table 1: Parameter settings for rule composition. Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 2.3 Language Model We used all available monolingual data to train 5-gram language models. Language models for each monolingual corpus were trained using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) and then interpolated using weights tuned to minimize perplexity on the development set. 2.4 Feature Functions Our feature functions are unchanged from the previous two years. They include the n-gram language model probability o</context>
</contexts>
<marker>Hopkins, Langmead, 2010</marker>
<rawString>Mark Hopkins and Greg Langmead. 2010. SCFG decoding without binarization. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 646–655, Cambridge, MA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>48--54</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6267" citStr="Koehn et al., 2003" startWordPosition="974" endWordPosition="977">t, and various scores for the synchronous derivation. Each grammar rule has a number of precomputed scores. For a grammar rule r of the form C — (α, Q, -) where C is a target-side non-terminal label, α is a string of source terminals and non-terminals, Q is a string of target terminals and non-terminals, and — is a one-to-one correspondence between source and target non-terminals, we score the rule according to the following functions: • p (C, Q |α, —) and p (α |C, Q, —), the direct and indirect translation probabilities. • plex (Q |α) and plex (α |Q), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (7r), the monolingual PCFG probability of the tree fragment 7r from which the rule was extracted. • exp(—1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. 2.5 Tuning The feature weights were tuned using the Moses implementation of MERT (Och, 2003) for all systems except English-to-German, for which we used k-best MIRA (Cherry and Foster, 2012) due to the larger number of features. We used tuning sentences drawn from all of the previous years’ test sets (except newstest2013, which was used as the developm</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 48–54, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07,</booktitle>
<pages>177--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1304" citStr="Koehn et al., 2007" startWordPosition="178" endWordPosition="181">dressed the out-of-vocabulary problem using transliteration for Hindi and Russian and using morphological reduction for Russian; we improved our GermanEnglish system through tree binarization; and we reduced system development time by filtering the tuning sets. 1 Introduction For this year’s WMT shared translation task we built syntax-based systems for six language pairs: • English-German • German-English • Czech-English • Hindi-English • French-English • Russian-English As last year (Nadejde et al., 2013), our systems are based on the string-to-tree pipeline implemented in the Moses toolkit (Koehn et al., 2007). We paid particular attention to the production of grammatical German, trying various parsers and incorporating target-side compound splitting and morphosyntactic constraints; for Hindi and Russian, we employed the new Moses transliteration model to handle out-of-vocabulary words; and for German to English, we experimented with tree binarization, obtaining good results from right binarization. We also present our first syntax-based results for French-English, the scale of which defeated us last year. This year we were able to train a system using all available training data, a task that was m</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, ACL ’07, pages 177–180, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Nadejde</author>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>Edinburgh’s Syntax-Based Machine Translation Systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>170--176</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1196" citStr="Nadejde et al., 2013" startWordPosition="161" endWordPosition="164">h target-side compound splitting, morphosyntactic constraints, and refinements to parse tree annotation; we addressed the out-of-vocabulary problem using transliteration for Hindi and Russian and using morphological reduction for Russian; we improved our GermanEnglish system through tree binarization; and we reduced system development time by filtering the tuning sets. 1 Introduction For this year’s WMT shared translation task we built syntax-based systems for six language pairs: • English-German • German-English • Czech-English • Hindi-English • French-English • Russian-English As last year (Nadejde et al., 2013), our systems are based on the string-to-tree pipeline implemented in the Moses toolkit (Koehn et al., 2007). We paid particular attention to the production of grammatical German, trying various parsers and incorporating target-side compound splitting and morphosyntactic constraints; for Hindi and Russian, we employed the new Moses transliteration model to handle out-of-vocabulary words; and for German to English, we experimented with tree binarization, obtaining good results from right binarization. We also present our first syntax-based results for French-English, the scale of which defeated</context>
</contexts>
<marker>Nadejde, Williams, Koehn, 2013</marker>
<rawString>Maria Nadejde, Philip Williams, and Philipp Koehn. 2013. Edinburgh’s Syntax-Based Machine Translation Systems. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 170– 176, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Comput. Linguist.,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4317" citStr="Och and Ney, 2003" startWordPosition="652" endWordPosition="655">uistics sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Value Rule depth 5 Node count 20 Rule size 5 Table 1: Parameter settings for rule composition. Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Comput. Linguist., 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>160--167</pages>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6605" citStr="Och, 2003" startWordPosition="1031" endWordPosition="1032">een source and target non-terminals, we score the rule according to the following functions: • p (C, Q |α, —) and p (α |C, Q, —), the direct and indirect translation probabilities. • plex (Q |α) and plex (α |Q), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (7r), the monolingual PCFG probability of the tree fragment 7r from which the rule was extracted. • exp(—1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. 2.5 Tuning The feature weights were tuned using the Moses implementation of MERT (Och, 2003) for all systems except English-to-German, for which we used k-best MIRA (Cherry and Foster, 2012) due to the larger number of features. We used tuning sentences drawn from all of the previous years’ test sets (except newstest2013, which was used as the development test set). In order to speed up the tuning process, we used subsets of the full tuning sets with sentence pairs up to length 30 (Max-30) and further applied a filtering technique to reduce the tuning set size to 2,000 sentence pairs for the language pairs involving German, French and Czech1. We also experimented with random subsets </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 160–167, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Improved Inference for Unlexicalized Parsing. In Human Language Technologies</title>
<date>2007</date>
<booktitle>The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>404--411</pages>
<location>Rochester, New York,</location>
<contexts>
<context position="3308" citStr="Petrov and Klein, 2007" startWordPosition="492" endWordPosition="495">on deadline. We refer to these as ‘devtest’ and ‘test’, respectively. 2 System Overview 2.1 Pre-processing The training data was normalized using the WMT normalize-punctuation.perl script then tokenized and truecased. Where the target language was English, we used the Moses tokenizer’s -penn option, which uses a tokenization scheme that more closely matches that of the parser. For the English-German system we used the default Moses tokenization scheme, which is similar to that of the German parsers. For the systems that translate into English, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to parse the target-side of the training corpus. As we will describe in Section 3, we tried a variety of parsers for German. We did not perform any corpus filtering other than the standard Moses method, which removes 207 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207–214, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) </context>
<context position="11176" citStr="Petrov and Klein, 2007" startWordPosition="1796" endWordPosition="1799">26.9 Max-30 27.2 19.2 27.0 Filtered 27.5 19.1 27.2 Random 27.3 19.4 27.0 Table 3: BLEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying genitive modifier). to 0.36 across three random sets. 4Note however that due to the long tuning times, we are reporting single tuning runs. 209 NN COMP SEGMENT SEGMENT COMP JUNC @s@ berufung gericht JUN 210 system devtest test baseline 86,341,</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Improved Inference for Unlexicalized Parsing. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 404–411, Rochester, New York, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Parsing German with Latent Variable Grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Parsing German at ACL ’08,</booktitle>
<pages>33--39</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="11201" citStr="Petrov and Klein, 2008" startWordPosition="1800" endWordPosition="1803">.0 Filtered 27.5 19.1 27.2 Random 27.3 19.4 27.0 Table 3: BLEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying genitive modifier). to 0.36 across three random sets. 4Note however that due to the long tuning times, we are reporting single tuning runs. 209 NN COMP SEGMENT SEGMENT COMP JUNC @s@ berufung gericht JUN 210 system devtest test baseline 86,341,766 88,657,327 Table 7: G</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Slav Petrov and Dan Klein. 2008. Parsing German with Latent Variable Grammars. In Proceedings of the Workshop on Parsing German at ACL ’08, pages 33–39, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="3283" citStr="Petrov et al., 2006" startWordPosition="488" endWordPosition="491">r the system submission deadline. We refer to these as ‘devtest’ and ‘test’, respectively. 2 System Overview 2.1 Pre-processing The training data was normalized using the WMT normalize-punctuation.perl script then tokenized and truecased. Where the target language was English, we used the Moses tokenizer’s -penn option, which uses a tokenization scheme that more closely matches that of the parser. For the English-German system we used the default Moses tokenization scheme, which is similar to that of the German parsers. For the systems that translate into English, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to parse the target-side of the training corpus. As we will describe in Section 3, we tried a variety of parsers for German. We did not perform any corpus filtering other than the standard Moses method, which removes 207 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207–214, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous co</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna N Rafferty</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines.</title>
<date>2008</date>
<booktitle>In Proceedings of the Workshop on Parsing German atACL ’08,</booktitle>
<pages>40--46</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="11120" citStr="Rafferty and Manning, 2008" startWordPosition="1787" endWordPosition="1790">8 19.7 26.4 test Tuning set Cs-En En-De De-En Full 27.5 19.2 26.9 Max-30 27.2 19.2 27.0 Filtered 27.5 19.1 27.2 Random 27.3 19.4 27.0 Table 3: BLEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying genitive modifier). to 0.36 across three random sets. 4Note however that due to the long tuning times, we are reporting single tuning runs. 209 NN COMP SEGMENT SEGMENT COMP JUNC @s@ beruf</context>
</contexts>
<marker>Rafferty, Manning, 2008</marker>
<rawString>Anna N. Rafferty and Christopher D. Manning. 2008. Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines. In Proceedings of the Workshop on Parsing German atACL ’08, pages 40– 46, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="16518" citStr="Schmid, 1994" startWordPosition="2621" endWordPosition="2622"> English translations (like grammatical gender) or are expressed by different means (like grammatical function being expressed through syntactic configuration rather than case). We adopt the widelyused approach of simplifying morphologicallycomplex source forms to remove distinctions that we believe to be redundant. Our method is similar to that of Weller et al. (2013) except that ours is much more conservative (in their experiments, Weller et al. (2013) found morphological reduction to harm translation indicating that useful information was likely to have been discarded). We used TreeTagger (Schmid, 1994) to obtain a lemma-tag pair for each Russian word. The tag specifies the word class and various morphosyntactic feature values. For example, the adjective peciiy6m4Kaacxax (‘republican’) gets the lemmatag pair peciiy6m4Kaacx41rI + Afpfsnf, where the code A indicates the word class and the remaining codes indicate values for the type, degree, gender, number, case, and definiteness features. Like Weller et al. (2013), we selectively replaced surface forms with their lemmas and reduced tags, reducing tags through feature deletion. We restricted morphological reduction to adjectives and verbs, lea</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In International Conference on New Methods in Language Processing, pages 44–49, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors.</title>
<date>2004</date>
<booktitle>In Proc. of the Int. Conf. on Computational Linguistics (COLING),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="11063" citStr="Schmid, 2004" startWordPosition="1781" endWordPosition="1782">9.8 26.2 Filtered 24.9 19.8 26.2 Random 24.8 19.7 26.4 test Tuning set Cs-En En-De De-En Full 27.5 19.2 26.9 Max-30 27.2 19.2 27.0 Filtered 27.5 19.1 27.2 Random 27.3 19.4 27.0 Table 3: BLEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying genitive modifier). to 0.36 across three random sets. 4Note however that due to the long tuning times, we are reporting single tun</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Helmut Schmid. 2004. Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors. In Proc. of the Int. Conf. on Computational Linguistics (COLING), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
<author>Beat Kunz</author>
</authors>
<title>Zmorge: A German Morphological Lexicon Extracted from Wiktionary.</title>
<date>2014</date>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014),</booktitle>
<location>Reykjavik, Iceland,</location>
<marker>Sennrich, Kunz, 2014</marker>
<rawString>Rico Sennrich and Beat Kunz. 2014. Zmorge: A German Morphological Lexicon Extracted from Wiktionary. In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC 2014), Reykjavik, Iceland, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
<author>Martin Volk</author>
<author>Gerold Schneider</author>
</authors>
<title>Exploiting Synergies Between Open Resources for German Dependency Parsing, POStagging, and Morphological Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference Recent Advances in Natural Language Processing</booktitle>
<pages>601--609</pages>
<location>Hissar, Bulgaria.</location>
<contexts>
<context position="10934" citStr="Sennrich et al., 2013" startWordPosition="1758" endWordPosition="1761">able 2: Size of full tuning sets and with sentence length up to 30. Tuning set Cs-En devtest De-En En-De Full 25.1 19.9 26.7 Max-30 24.7 19.8 26.2 Filtered 24.9 19.8 26.2 Random 24.8 19.7 26.4 test Tuning set Cs-En En-De De-En Full 27.5 19.2 26.9 Max-30 27.2 19.2 27.0 Filtered 27.5 19.1 27.2 Random 27.3 19.4 27.0 Table 3: BLEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying g</context>
</contexts>
<marker>Sennrich, Volk, Schneider, 2013</marker>
<rawString>Rico Sennrich, Martin Volk, and Gerold Schneider. 2013. Exploiting Synergies Between Open Resources for German Dependency Parsing, POStagging, and Morphological Analysis. In Proceedings of the International Conference Recent Advances in Natural Language Processing 2013, pages 601–609, Hissar, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Intl. Conf. Spoken Language Processing,</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="5313" citStr="Stolcke, 2002" startWordPosition="812" endWordPosition="813">alue Rule depth 5 Node count 20 Rule size 5 Table 1: Parameter settings for rule composition. Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 2.3 Language Model We used all available monolingual data to train 5-gram language models. Language models for each monolingual corpus were trained using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) and then interpolated using weights tuned to minimize perplexity on the development set. 2.4 Feature Functions Our feature functions are unchanged from the previous two years. They include the n-gram language model probability of the derivation’s target yield, its word count, and various scores for the synchronous derivation. Each grammar rule has a number of precomputed scores. For a grammar rule r of the form C — (α, Q, -) where C is a target-side non-terminal label, α is a string of source terminals and non-terminals, Q is a strin</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an extensible language modeling toolkit. In Intl. Conf. Spoken Language Processing, Denver, Colorado, September 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Marina Rey</author>
</authors>
<title>Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy.</title>
<date>2007</date>
<booktitle>In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>746--754</pages>
<contexts>
<context position="12458" citStr="Wang et al. (2007)" startWordPosition="2000" endWordPosition="2003">ystem after filtering for the devtest (newstest2013) and test (newstest2014) sets. 6 German to English German compounds were split using the script provided with Moses. For training the primary system, the target parse trees were restructured before rule extraction by right binarization. Since binarization strategies increase the tree depth and number of nodes by adding virtual non-terminals, we increased the extraction parameters to: Rule Depth = 7, Node Count = 100, Rule Size = 7. A thorough investigation of binarization methods for restructuring Penn Treebank style trees was carried out by Wang et al. (2007). Table 8 shows BLEU scores for the baseline system and two systems employing different binarization strategies. Table 9 shows the resulting grammar sizes after filtering for the evaluation sets. Results on the development set showed no improvement when left binarization was used for restructuring the trees, although the grammar size increased significantly. BLEU system devtest test baseline 26.2 27.2 + right binarization (primary) 26.8 28.2 + left binarization 26.3 - Table 8: German to English results on the devtest (newsdev2013) and test (newstest2014) sets. system devtest test baseline 11,4</context>
</contexts>
<marker>Wang, Knight, Marcu, Rey, 2007</marker>
<rawString>Wei Wang, Kevin Knight, Daniel Marcu, and Marina Rey. 2007. Binarizing Syntax Trees to Improve Syntax-Based Machine Translation Accuracy. In Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 746–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Weller</author>
<author>Max Kisselew</author>
<author>Svetlana Smekalova</author>
<author>Alexander Fraser</author>
<author>Helmut Schmid</author>
<author>Nadir Durrani</author>
<author>Hassan Sajjad</author>
<author>Richárd Farkas</author>
</authors>
<title>MunichEdinburgh-Stuttgart submissions at WMT13: Morphological and syntactic processing for SMT.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>232--239</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="16276" citStr="Weller et al. (2013)" startWordPosition="2582" endWordPosition="2585">ng the same transliteration method as for Hindi-English. Data sparsity issues for this language pair are exacerbated by the rich inflectional morphology of Russian. Many Russian word forms express grammatical distinctions that are either absent from English translations (like grammatical gender) or are expressed by different means (like grammatical function being expressed through syntactic configuration rather than case). We adopt the widelyused approach of simplifying morphologicallycomplex source forms to remove distinctions that we believe to be redundant. Our method is similar to that of Weller et al. (2013) except that ours is much more conservative (in their experiments, Weller et al. (2013) found morphological reduction to harm translation indicating that useful information was likely to have been discarded). We used TreeTagger (Schmid, 1994) to obtain a lemma-tag pair for each Russian word. The tag specifies the word class and various morphosyntactic feature values. For example, the adjective peciiy6m4Kaacxax (‘republican’) gets the lemmatag pair peciiy6m4Kaacx41rI + Afpfsnf, where the code A indicates the word class and the remaining codes indicate values for the type, degree, gender, number</context>
</contexts>
<marker>Weller, Kisselew, Smekalova, Fraser, Schmid, Durrani, Sajjad, Farkas, 2013</marker>
<rawString>Marion Weller, Max Kisselew, Svetlana Smekalova, Alexander Fraser, Helmut Schmid, Nadir Durrani, Hassan Sajjad, and Richárd Farkas. 2013. MunichEdinburgh-Stuttgart submissions at WMT13: Morphological and syntactic processing for SMT. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232–239, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>Agreement Constraints for Statistical Machine Translation into German.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>217--226</pages>
<location>Edinburgh, Scotland,</location>
<marker>Williams, Koehn, 2011</marker>
<rawString>Philip Williams and Philipp Koehn. 2011. Agreement Constraints for Statistical Machine Translation into German. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 217–226, Edinburgh, Scotland, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Williams</author>
<author>Philipp Koehn</author>
</authors>
<title>GHKM Rule Extraction and Scope-3 Parsing in Moses.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>388--394</pages>
<location>Montréal, Canada,</location>
<contexts>
<context position="4131" citStr="Williams and Koehn, 2012" startWordPosition="621" endWordPosition="624">thod, which removes 207 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207–214, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Value Rule depth 5 Node count 20 </context>
</contexts>
<marker>Williams, Koehn, 2012</marker>
<rawString>Philip Williams and Philipp Koehn. 2012. GHKM Rule Extraction and Scope-3 Parsing in Moses. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 388–394, Montréal, Canada, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
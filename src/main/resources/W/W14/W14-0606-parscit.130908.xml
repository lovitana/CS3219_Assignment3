<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.037360">
<title confidence="0.9971865">
Enhancing the possibilities of corpus-based investigations: Word sense
disambiguation on query results of large text corpora
</title>
<author confidence="0.992523">
Christian Poelitz
</author>
<affiliation confidence="0.905332">
Technical University Dortmund
Artificial Intelligence Group
</affiliation>
<address confidence="0.634836">
44227 Dortmund, Germany
</address>
<email confidence="0.738071">
poelitz@tu-dortmund.de
</email>
<author confidence="0.995438">
Thomas Bartz
</author>
<affiliation confidence="0.868138333333333">
Technical University Dortmund
Institute of German Language and Literature
44227 Dortmund, Germany
</affiliation>
<email confidence="0.770988">
bartz@tu-dortmund.de
</email>
<sectionHeader confidence="0.988705" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999874571428571">
Common large digital text corpora do not
distinguish between different meanings of
word forms, intense manual effort has to
be done for disambiguation tasks when
querying for homonyms or polysemes. To
improve this situation, we ran experiments
with automatic word sense disambiguation
methods operating directly on the output
of the corpus query. In this paper, we
present experiments with topic models to
cluster search result snippets in order to
separate occurrences of homonymous or
polysemous queried words by their mean-
ings.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999810553846154">
Large digital text corpora contain text documents
from different sources, genres and periods of
time as well as often structural and linguistic
markups. Nowadays, they provide novel and en-
hanced possibilities of exploring research ques-
tions at the basis of authentic language usage not
only in the field of linguistics, but for human-
ities and social sciences in general. But even
though tools for query and analysis are getting
more and more flexible and sophisticated (not
least thanks to the efforts been done in infras-
tructure projects like CLARIN), automatically ob-
tained data have to be reviewed manually in most
cases because of false positives. Depending on
the amount of data, intense manual effort has to
be done for cleaning, classification or disambigua-
tion tasks. Hence, many research questions cannot
be addressed because of time constraints (Storrer,
2011). A project funded by the German BMBF
(Bundesministerium f¨ur Bildung und Forschung,
”Federal Ministry of Education and Research”),
therefore, is investigating benefits and issues of
using machine learning technology in order to per-
form these tasks automatically. In this paper, we
focus on the disambiguation task, which is an issue
known for a long time in the field of corpus-based
lexicography (Engelberg and Lemnitzer, 2009),
but has not been satisfactorily solved, yet, and is
still highly relevant also to social scientists or his-
torians. In the humanities, researchers usually are
not examining word forms, but terms represent-
ing relations of word forms and their meanings.
While the common large corpora do not distin-
guish between different meanings of word forms,
the disambiguation task has to be carried out man-
ually most of the times. To improve this situa-
tion, we ran experiments with word sense disam-
biguation methods operating directly on the output
of the corpus queries, i.e. search result lists con-
taining small snippets with the occurrences of the
search keyword, each in a context of about only
three sentences. In particular, we used topic mod-
elling to automatically detect clusters of keyword
occurrences with similar contexts, that we con-
sider corresponding to a certain meaning of the
keyword. In the following, we report our findings
from experiments with the German terms Leiter
and zeitnah, both supposed to provide interest-
ing insights into processes of language change.
Der Leiter ”chief”, ”director” and die Leiter ”lad-
der” are homonyms with possible further senses
Energieleiter ”conducting medium” and Tonleiter
”scale” (in music), whereby der Leiter competes
against borrowings like Boss or Chef. Zeitnah, a
polyseme meaning zeitgenssisch ”contemporary”,
zeitkritisch ”critical of the times” as well as un-
verzglich ”prompt”, seems to have acquired the
latter meaning as a new sense not until the sec-
ond half of the last century. The basis of our ex-
periments are search result lists derived from the
DWDS Kernkorpus core corpus of the 20th cen-
tury (for Leiter) and, in addition, from the ZEIT
corpus (for zeitnah). The DWDS Kernkorpus,
constructed at the Berlin-Brandenburg Academy
of Sciences (BBAW), contains approximately 100
</bodyText>
<page confidence="0.990799">
42
</page>
<note confidence="0.617796">
Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 42–46,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99988975">
million running words, balanced chronologically
(over the decades of the 20th century) and by text
genre (over the genres journalism, literary texts,
scientific literature and other nonfiction; (Geyken,
2007)). The ZEIT corpus covers all the issues of
the German weekly newspaper Die Zeit from 1946
to 2009, approximately 460 million running words
(http://www.dwds.de/ressourcen/korpora).
</bodyText>
<sectionHeader confidence="0.999723" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999936565217391">
Word sense disambiguation is a well studied prob-
lem in Machine Learning and Natural Language
Processing. For a given word, later mentioned as
word of interest, we expect that there exist several
meanings. The differences in the meanings are re-
flected by different words occurring and frequen-
cies together with the word to be disambiguated.
A very early statistical approach was proposed by
(Brown et al., 1991). The authors proposed to
estimate the probability distribution of senses for
given words from annotated examples. A general
survey about the topic can be found in (Navigli,
2009). Latent Dirichlet Allocation (LDA) intro-
duced by (Blei et al., 2003) can be used to esti-
mate topic distributions for a given document cor-
pus. Each topic represent a sense in which the
documents, respectively the words, appear. (Grif-
fiths and Steyvers, 2004) proposed efficient train-
ing for LDA using Monte Carlo sampling. They
used Gibbs sampling to estimate the topic distribu-
tion. The authors in (Brody and Lapata, 2009) ex-
tend the generative model by LDA by many paral-
lel feature representations. Hence, beside the pure
words, additional features like part of speech tags
can be used. Further, the authors perform analy-
sis with different context sizes. Investigations of
word sense disambiguation on small snippets have
been done before on search engine results. The
snippets retrieved after a query has been sent to
a search engine are used for disambiguation. In
(Navigli and Crisafulli, 2010) for instance, the au-
thors search for word senses of web search results
using retrieved snippets.
Our approach differs from these previous ones
since we concentrate on snippets from a text cor-
pus for linguistic and lexicographic research pur-
poses (see Section 1). Unlike results from search
engines, that refer to documents whose topics are
strongly related to the search keyword, result lists
from text corpora contain snippets with occur-
rences of the keyword in each document of the
corpus, irrespective of the document topic. That
is why keywords can occur in less typical, se-
mantically less definite contexts. In the liter-
ary documents, they are not infrequently used as
metaphors.
</bodyText>
<sectionHeader confidence="0.970701" genericHeader="method">
3 Snippet Representation
</sectionHeader>
<bodyText confidence="0.99996204">
In order to properly apply Machine Learning
methods for word sense disambiguation we need
to encode the snippets in an appropriate way.
Therefore, we represent each snippet as bag-of-
words. This means we build a large vector that
contains at the component i the number of times
word i - from the overall vocabulary of the docu-
ment corpus - appears in the snippet. These vec-
tors are very sparse and can be efficiently saved as
hash tables.
Since we want to investigate different context
information for the disambiguation, we generate
for each snippet many different bag-of-words rep-
resentations. First, we use only those words that
appear in close proximity to the word we want to
disambiguate. This means, we place a window on
the text, that contains a certain number of words
that appear before and after the word of interest.
Next, we filter out words that are not immediate
constituents (or immediate constituents of the 1st,
2nd, nth superordinate node) of the word of inter-
est. In this case the proximity is not crucial but the
syntactical relatedness to the word of interest.
These word vectors are used for the word sense
disambiguation.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="method">
4 Disambiguation
</sectionHeader>
<bodyText confidence="0.999872529411765">
For the word sense disambiguation we use Latent
Dirichlet Allocation (LDA) as introduced by (Blei
et al., 2003). LDA estimates the probability dis-
tributions of words and documents, respectively
snippet, over a number of different topics. The
topics will be used to disambiguate the word of in-
terest. These distributions are drawn from Dirich-
let distributions that depend on given meta param-
eters α and β.
The probability of a topic, given a snippet
is modelled as Multinomial distribution that de-
pends on a Dirichlet distributed distribution of
the snippets over the topics. Formally we have:
φ — Dirichlet(β) the probability distribution of
a snippet and p(zi|φ(j)) — Multi(φ(j)) the prob-
ability of topic zi for a given snippet j.
To estimate the distributions we use a Gibbs
</bodyText>
<page confidence="0.998866">
43
</page>
<table confidence="0.99984175">
Leiter w10 w40 w80 all syntax
NMI 0.2086 0.2579 0.2414 0.2573 0.1944
zeitnah w10 w40 w80 all syntax
NMI 0.1012 0.1926 0.1656 0.2230 0.0456
</table>
<tableCaption confidence="0.958136">
Table 1: NMI of the extracted senses with respect
to the given annotations of the text snippets.
</tableCaption>
<table confidence="0.99993125">
Leiter w10 w40 w80 all syntax
F1 0.7271 0.7487 0.7405 0.7416 0.6904
zeitnah w10 w40 w80 all syntax
F1 0.7773 0.6919 0.7630 0.7488 0.4584
</table>
<tableCaption confidence="0.8846845">
Table 2: F1 score of the extracted senses with re-
spect to the given annotations of the text snippets.
</tableCaption>
<bodyText confidence="0.999898896551724">
sampler as proposed by (Griffiths and Steyvers,
2004). The Gibbs sampler models the probabil-
ity distributions of a given topic zi, depending on
all other topics and the words in the snippet as
Markov chain. This Markov chain converges to
the posterior distribution of the topics given the
words in a certain snippet. This posterior can be
used to estimate the most likely topic for a given
snippet.
Further, we use the author topic model as in-
troduced by (Steyvers et al., 2004). This model
integrates additional indications about the author
for each snippet into the topic modelling process.
This method can also be used to model the text
categories instead of authors. We simply treat the
categories as the authors. Now, the probability dis-
tribution of the topics additionally depends on the
random variable c over the categories. This can be
leveraged to estimate the probability of category c
for a given topic zi, hence p(c|zi).
Using the author topic model, we estimate
the topic distribution over words and categories.
Based on these distributions the stochastic process
of generating topics is simulated. Depending on
the number of times a topic is drawn for a given
snippet and category, we extract the most likely
words and categories for the topics. The topics
represent the different senses of the word of inter-
est.
</bodyText>
<sectionHeader confidence="0.999609" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999989070175439">
We performed experiments on two data sets that
consist of short snippets retrieved by corpus
queries for the words Leiter and zeitnah in the
DWDS Kernkorpus www.dwds.de and the ZEIT
corpus (see Section 1). Each snippet consists of
the three sentences, whereby the second sentence
contains the search keyword (the word to disam-
biguate) in each case. The snippets belong to the
different text categories covered by the mentioned
corpora: journalism, literary texts, scientific lit-
erature and other nonfiction (see Section 1). For
each snippet, we have information to which cate-
gory it belongs to. This information is used only
for validation, not for the topic extraction. For
each data set, 30 percent of snippets were disam-
biguated manually by two independent annotators,
whereby doubtful cases were clarified by a third
person. The annotations are not used for disam-
biguation, but for the validation of the method.
For each snippet we generate bag-of-words vec-
tors using contexts of 10, 40, 80 or all words
around the word of interest. Hence, for context
size 10 we use the ten words before the token, the
token itself and the ten following tokens, as repre-
sentation of the snippet. For further experiments
we used the Stanford Constituent Parser (Klein
and Manning, 2003) to get only the words that
syntactically depend on the words of interest. For
the extraction of the topics and distribution over
the text categories we used the Gibbs sampler for
LDA and the author topic model from the Mat-
lab library Topictoolbox (Griffiths and Steyvers,
2004).
Based on the annotation mentioned above we
can estimate the Normalized Mutual Information
(NMI) as score for the goodness of the method.
NMI measures how many snippets that are anno-
tated as being from different topics are placed into
the same topic based on the extracted topics from
LDA. It is defined as the fraction of the sum of
the entropies of the distributions of the annotations
and the disambiguation results, and the entropy
of the joint distribution of annotations and results
(Manning et al., 2008) (p. 357f). Further, we use
one of the standard measures to estimate the good-
ness of a word sense disambiguation result, the F1
score. The F1 score is the weighted average of
the precision and recall of the disambiguation re-
sults for the given annotations. This and further
evaluation methods are described in (Navigli and
Vannella, 2013).
In the Tables 1 and 2 we show the NMI and F1
score for the extracted topics, respectively senses,
by LDA. We tested different context sizes from 10
to 80 words around the word of interest. Com-
pared to the results when we use the whole snip-
pets, we see that a context size of 40 results in the
</bodyText>
<page confidence="0.996901">
44
</page>
<table confidence="0.998978">
Sense 1 Sense 2 Sense 3 Sense 4
music standing GDR 1 government
Berlin saw SED 2 got
Prof up party Berlin
Comp above political ZK 3
</table>
<tableCaption confidence="0.881987">
Table 3: Translation of the most frequent words
for each of the extracted senses for the word
Leiter.
</tableCaption>
<table confidence="0.9995916">
Sense 1 Sense 2 Sense 3 Sense 4
question society German publisher
DM 4 just time book
years examples film literature
music questions Berlin year
</table>
<tableCaption confidence="0.995544">
Table 4: Translation of the most frequent words
</tableCaption>
<bodyText confidence="0.992650333333333">
for each of the extracted senses for the word zeit-
nah.
best performance. Less context decrease the per-
formance and the filtering by constituencies give
the worst results. The experiments show that a
windowing approach is well suited to represent
documents for a word sense disambiguation task.
The size of the window seems to be crucial and
must be chosen a priori. Optimal window size
could be found by cross validation techniques us-
ing annotated snippets.
Next, we investigated the distribution of the top-
ics over the text categories. We used the author
topic model as described above to estimate how
the categories distribute over the sense. Tables 3
and 4 show the most likely words to appear in the
corresponding senses translated into English for
four extracted topics. In the Tables 5 and 6 the
distribution of the senses over the given categories
are presented. Based on the posterior distribution
of the categories, we simulated the process of as-
signing topics to categories for each word in the
snippets. In the tables we present the number of
times we assign sense i to category c.
For the word Leiter in Table 5, we see that in
each category always one certain sense for the
word is prominent. For instance sense 2, here
</bodyText>
<table confidence="0.9902146">
Leiter Sense 1 Sense 2 Sense 3 Sense 4
Literature 597 23818 7464 6718
Non-fiction 3031 5295 63708 8733
Science 41564 3269 1216 1046
Journalism 5527 8845 23104 78645
</table>
<tableCaption confidence="0.973404333333333">
Table 5: The distribution of the senses among the
text categories during the simulation for the word
Leiter.
</tableCaption>
<table confidence="0.999882">
zeitnah Sense 1 Sense 2 Sense 3 Sense 4
Literature 23 0 12 6
Non-fiction 1 0 574 10
Science 211 0 478 1
Journalism 2150 2438 1691 2924
</table>
<tableCaption confidence="0.972109">
Table 6: The distribution of the senses among the
</tableCaption>
<bodyText confidence="0.990873">
text categories during the simulation for the word
zeitnah.
Leiter appears in the context of a ladder. In this
context, the word is more likely to appear in a fic-
tional text than in the other categories. For zeit-
nah in Table 6 the results are not very clear. First,
the word is most likely to appear in news papers
rather than in literature or science articles. This is
due to the fact that we have much more snippets
from news papers. Only in sense 3, the word is
also likely to appear in other categories. This con-
text seems to be German films. In contrast, we see
sense 2 that is about social questions appears only
in news papers.
</bodyText>
<sectionHeader confidence="0.996167" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999988947368421">
We used topic models to cluster search result
snippets received by queries in two large digi-
tal text corpora in order to separate occurrences
of homonymous or polysemous queried words by
their meanings. We showed that LDA performs
well in extracting the senses in which the words
appear. Finally, we found that the author topic
model can be used to estimate how the extracted
senses distribute over document categories.
For the future, we want to further investigate the
distribution of the topics over different categories
and time periods, as first experiments showed po-
tential benefit of the author topic model. An im-
portant point for future work is, moreover, the in-
tegration of syntactic features not only for filtering
important words but also for enhancement of our
simple bag-of-words representation. Especially,
the integration of constituency and dependency in-
formation will be further investigated.
</bodyText>
<sectionHeader confidence="0.86972" genericHeader="acknowledgments">
Aknowledgements
</sectionHeader>
<bodyText confidence="0.989393333333333">
The authors are supported by the Bundesminis-
terium f¨ur Bildung und Forschung (BMBF) in
project KobRA5.
</bodyText>
<footnote confidence="0.979389">
5http://www.kobra.tu-dortmund.de/
</footnote>
<page confidence="0.999155">
45
</page>
<sectionHeader confidence="0.990259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999034264705882">
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. J. Mach. Learn.
Res., 3:993–1022, March.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In Proceedings of the 12th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL ’09,
pages 103–111, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1991. Word-
sense disambiguation using statistical methods. In
Proceedings of the 29th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ’91, pages
264–270, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stefan Engelberg and Lothar Lemnitzer. 2009. Lexiko-
graphie und Woerterbuchbenutzung. Stauffenburg,
Tuebingen.
Alexander Geyken. 2007. The DWDS corpus. A ref-
erence corpus for the German language of the twen-
tieth century. In Christiane Fellbaum, editor, Idioms
and collocations. corpus-based linguistic and lexi-
cographic studies, pages 23–40. Continuum, Lon-
don.
T. L. Griffiths and M. Steyvers. 2004. Finding scien-
tific topics. Proceedings of the National Academy of
Sciences, 101(Suppl. 1):5228–5235, April.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ’03, pages 423–
430, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch¨utze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Roberto Navigli and Giuseppe Crisafulli. 2010. Induc-
ing word senses to improve web search result clus-
tering. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’10, pages 116–126, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Roberto Navigli and Daniele Vannella. 2013.
Semeval-2013 task 11: Word sense induction and
disambiguation within an end-user application. In
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM), Volume 2: Proceedings
of the Seventh International Workshop on Seman-
tic Evaluation (SemEval 2013), pages 193–201, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Roberto Navigli. 2009. Word sense disambiguation:
A survey. ACM Comput. Surv., 41(2):10:1–10:69,
February.
Mark Steyvers, Padhraic Smyth, Michal Rosen-Zvi,
and Thomas Griffiths. 2004. Probabilistic author-
topic models for information discovery. In Proceed-
ings of the Tenth ACM SIGKDD International Con-
ference on Knowledge Discovery and Data Mining,
KDD ’04, pages 306–315, New York, NY, USA.
ACM.
Angelika Storrer. 2011. Korpusgesttzte sprachanal-
yse in lexikographie und phraseologie. In Karl-
fried Knapp et al., editor, Angewandte Linguistik.
Ein Lehrbuch, pages 216–239. Francke Verlag, Tue-
bingen.
</reference>
<page confidence="0.999607">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.768834">
<title confidence="0.997291">Enhancing the possibilities of corpus-based investigations: Word disambiguation on query results of large text corpora</title>
<author confidence="0.999951">Christian Poelitz</author>
<affiliation confidence="0.999889">Technical University Dortmund Artificial Intelligence Group</affiliation>
<address confidence="0.999972">44227 Dortmund, Germany</address>
<email confidence="0.994258">poelitz@tu-dortmund.de</email>
<author confidence="0.943045">Thomas</author>
<affiliation confidence="0.9953055">Technical University Institute of German Language and</affiliation>
<address confidence="0.991087">44227 Dortmund,</address>
<email confidence="0.999168">bartz@tu-dortmund.de</email>
<abstract confidence="0.989241133333333">Common large digital text corpora do not distinguish between different meanings of word forms, intense manual effort has to be done for disambiguation tasks when querying for homonyms or polysemes. To improve this situation, we ran experiments with automatic word sense disambiguation methods operating directly on the output of the corpus query. In this paper, we present experiments with topic models to cluster search result snippets in order to separate occurrences of homonymous or polysemous queried words by their meanings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--993</pages>
<contexts>
<context position="5363" citStr="Blei et al., 2003" startWordPosition="811" endWordPosition="814">ied problem in Machine Learning and Natural Language Processing. For a given word, later mentioned as word of interest, we expect that there exist several meanings. The differences in the meanings are reflected by different words occurring and frequencies together with the word to be disambiguated. A very early statistical approach was proposed by (Brown et al., 1991). The authors proposed to estimate the probability distribution of senses for given words from annotated examples. A general survey about the topic can be found in (Navigli, 2009). Latent Dirichlet Allocation (LDA) introduced by (Blei et al., 2003) can be used to estimate topic distributions for a given document corpus. Each topic represent a sense in which the documents, respectively the words, appear. (Griffiths and Steyvers, 2004) proposed efficient training for LDA using Monte Carlo sampling. They used Gibbs sampling to estimate the topic distribution. The authors in (Brody and Lapata, 2009) extend the generative model by LDA by many parallel feature representations. Hence, beside the pure words, additional features like part of speech tags can be used. Further, the authors perform analysis with different context sizes. Investigatio</context>
<context position="8177" citStr="Blei et al., 2003" startWordPosition="1275" endWordPosition="1278">y to the word we want to disambiguate. This means, we place a window on the text, that contains a certain number of words that appear before and after the word of interest. Next, we filter out words that are not immediate constituents (or immediate constituents of the 1st, 2nd, nth superordinate node) of the word of interest. In this case the proximity is not crucial but the syntactical relatedness to the word of interest. These word vectors are used for the word sense disambiguation. 4 Disambiguation For the word sense disambiguation we use Latent Dirichlet Allocation (LDA) as introduced by (Blei et al., 2003). LDA estimates the probability distributions of words and documents, respectively snippet, over a number of different topics. The topics will be used to disambiguate the word of interest. These distributions are drawn from Dirichlet distributions that depend on given meta parameters α and β. The probability of a topic, given a snippet is modelled as Multinomial distribution that depends on a Dirichlet distributed distribution of the snippets over the topics. Formally we have: φ — Dirichlet(β) the probability distribution of a snippet and p(zi|φ(j)) — Multi(φ(j)) the probability of topic zi fo</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res., 3:993–1022, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09,</booktitle>
<pages>103--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5717" citStr="Brody and Lapata, 2009" startWordPosition="870" endWordPosition="873"> (Brown et al., 1991). The authors proposed to estimate the probability distribution of senses for given words from annotated examples. A general survey about the topic can be found in (Navigli, 2009). Latent Dirichlet Allocation (LDA) introduced by (Blei et al., 2003) can be used to estimate topic distributions for a given document corpus. Each topic represent a sense in which the documents, respectively the words, appear. (Griffiths and Steyvers, 2004) proposed efficient training for LDA using Monte Carlo sampling. They used Gibbs sampling to estimate the topic distribution. The authors in (Brody and Lapata, 2009) extend the generative model by LDA by many parallel feature representations. Hence, beside the pure words, additional features like part of speech tags can be used. Further, the authors perform analysis with different context sizes. Investigations of word sense disambiguation on small snippets have been done before on search engine results. The snippets retrieved after a query has been sent to a search engine are used for disambiguation. In (Navigli and Crisafulli, 2010) for instance, the authors search for word senses of web search results using retrieved snippets. Our approach differs from </context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’09, pages 103–111, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>Wordsense disambiguation using statistical methods.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Meeting on Association for Computational Linguistics, ACL ’91,</booktitle>
<pages>264--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5115" citStr="Brown et al., 1991" startWordPosition="772" endWordPosition="775">Geyken, 2007)). The ZEIT corpus covers all the issues of the German weekly newspaper Die Zeit from 1946 to 2009, approximately 460 million running words (http://www.dwds.de/ressourcen/korpora). 2 Related Work Word sense disambiguation is a well studied problem in Machine Learning and Natural Language Processing. For a given word, later mentioned as word of interest, we expect that there exist several meanings. The differences in the meanings are reflected by different words occurring and frequencies together with the word to be disambiguated. A very early statistical approach was proposed by (Brown et al., 1991). The authors proposed to estimate the probability distribution of senses for given words from annotated examples. A general survey about the topic can be found in (Navigli, 2009). Latent Dirichlet Allocation (LDA) introduced by (Blei et al., 2003) can be used to estimate topic distributions for a given document corpus. Each topic represent a sense in which the documents, respectively the words, appear. (Griffiths and Steyvers, 2004) proposed efficient training for LDA using Monte Carlo sampling. They used Gibbs sampling to estimate the topic distribution. The authors in (Brody and Lapata, 200</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1991. Wordsense disambiguation using statistical methods. In Proceedings of the 29th Annual Meeting on Association for Computational Linguistics, ACL ’91, pages 264–270, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Engelberg</author>
<author>Lothar Lemnitzer</author>
</authors>
<date>2009</date>
<booktitle>Lexikographie und Woerterbuchbenutzung.</booktitle>
<location>Stauffenburg, Tuebingen.</location>
<contexts>
<context position="2236" citStr="Engelberg and Lemnitzer, 2009" startWordPosition="326" endWordPosition="329">amount of data, intense manual effort has to be done for cleaning, classification or disambiguation tasks. Hence, many research questions cannot be addressed because of time constraints (Storrer, 2011). A project funded by the German BMBF (Bundesministerium f¨ur Bildung und Forschung, ”Federal Ministry of Education and Research”), therefore, is investigating benefits and issues of using machine learning technology in order to perform these tasks automatically. In this paper, we focus on the disambiguation task, which is an issue known for a long time in the field of corpus-based lexicography (Engelberg and Lemnitzer, 2009), but has not been satisfactorily solved, yet, and is still highly relevant also to social scientists or historians. In the humanities, researchers usually are not examining word forms, but terms representing relations of word forms and their meanings. While the common large corpora do not distinguish between different meanings of word forms, the disambiguation task has to be carried out manually most of the times. To improve this situation, we ran experiments with word sense disambiguation methods operating directly on the output of the corpus queries, i.e. search result lists containing smal</context>
</contexts>
<marker>Engelberg, Lemnitzer, 2009</marker>
<rawString>Stefan Engelberg and Lothar Lemnitzer. 2009. Lexikographie und Woerterbuchbenutzung. Stauffenburg, Tuebingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Geyken</author>
</authors>
<title>The DWDS corpus. A reference corpus for the German language of the twentieth century.</title>
<date>2007</date>
<pages>23--40</pages>
<editor>In Christiane Fellbaum, editor,</editor>
<publisher>Continuum,</publisher>
<location>London.</location>
<contexts>
<context position="4509" citStr="Geyken, 2007" startWordPosition="680" endWordPosition="681">ter) and, in addition, from the ZEIT corpus (for zeitnah). The DWDS Kernkorpus, constructed at the Berlin-Brandenburg Academy of Sciences (BBAW), contains approximately 100 42 Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH) @ EACL 2014, pages 42–46, Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics million running words, balanced chronologically (over the decades of the 20th century) and by text genre (over the genres journalism, literary texts, scientific literature and other nonfiction; (Geyken, 2007)). The ZEIT corpus covers all the issues of the German weekly newspaper Die Zeit from 1946 to 2009, approximately 460 million running words (http://www.dwds.de/ressourcen/korpora). 2 Related Work Word sense disambiguation is a well studied problem in Machine Learning and Natural Language Processing. For a given word, later mentioned as word of interest, we expect that there exist several meanings. The differences in the meanings are reflected by different words occurring and frequencies together with the word to be disambiguated. A very early statistical approach was proposed by (Brown et al.,</context>
</contexts>
<marker>Geyken, 2007</marker>
<rawString>Alexander Geyken. 2007. The DWDS corpus. A reference corpus for the German language of the twentieth century. In Christiane Fellbaum, editor, Idioms and collocations. corpus-based linguistic and lexicographic studies, pages 23–40. Continuum, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T L Griffiths</author>
<author>M Steyvers</author>
</authors>
<title>Finding scientific topics.</title>
<date>2004</date>
<booktitle>Proceedings of the National Academy of Sciences,</booktitle>
<volume>101</volume>
<pages>1--5228</pages>
<contexts>
<context position="5552" citStr="Griffiths and Steyvers, 2004" startWordPosition="842" endWordPosition="846">es in the meanings are reflected by different words occurring and frequencies together with the word to be disambiguated. A very early statistical approach was proposed by (Brown et al., 1991). The authors proposed to estimate the probability distribution of senses for given words from annotated examples. A general survey about the topic can be found in (Navigli, 2009). Latent Dirichlet Allocation (LDA) introduced by (Blei et al., 2003) can be used to estimate topic distributions for a given document corpus. Each topic represent a sense in which the documents, respectively the words, appear. (Griffiths and Steyvers, 2004) proposed efficient training for LDA using Monte Carlo sampling. They used Gibbs sampling to estimate the topic distribution. The authors in (Brody and Lapata, 2009) extend the generative model by LDA by many parallel feature representations. Hence, beside the pure words, additional features like part of speech tags can be used. Further, the authors perform analysis with different context sizes. Investigations of word sense disambiguation on small snippets have been done before on search engine results. The snippets retrieved after a query has been sent to a search engine are used for disambig</context>
<context position="9374" citStr="Griffiths and Steyvers, 2004" startWordPosition="1478" endWordPosition="1481">) the probability of topic zi for a given snippet j. To estimate the distributions we use a Gibbs 43 Leiter w10 w40 w80 all syntax NMI 0.2086 0.2579 0.2414 0.2573 0.1944 zeitnah w10 w40 w80 all syntax NMI 0.1012 0.1926 0.1656 0.2230 0.0456 Table 1: NMI of the extracted senses with respect to the given annotations of the text snippets. Leiter w10 w40 w80 all syntax F1 0.7271 0.7487 0.7405 0.7416 0.6904 zeitnah w10 w40 w80 all syntax F1 0.7773 0.6919 0.7630 0.7488 0.4584 Table 2: F1 score of the extracted senses with respect to the given annotations of the text snippets. sampler as proposed by (Griffiths and Steyvers, 2004). The Gibbs sampler models the probability distributions of a given topic zi, depending on all other topics and the words in the snippet as Markov chain. This Markov chain converges to the posterior distribution of the topics given the words in a certain snippet. This posterior can be used to estimate the most likely topic for a given snippet. Further, we use the author topic model as introduced by (Steyvers et al., 2004). This model integrates additional indications about the author for each snippet into the topic modelling process. This method can also be used to model the text categories in</context>
<context position="12214" citStr="Griffiths and Steyvers, 2004" startWordPosition="1950" endWordPosition="1953">ch snippet we generate bag-of-words vectors using contexts of 10, 40, 80 or all words around the word of interest. Hence, for context size 10 we use the ten words before the token, the token itself and the ten following tokens, as representation of the snippet. For further experiments we used the Stanford Constituent Parser (Klein and Manning, 2003) to get only the words that syntactically depend on the words of interest. For the extraction of the topics and distribution over the text categories we used the Gibbs sampler for LDA and the author topic model from the Matlab library Topictoolbox (Griffiths and Steyvers, 2004). Based on the annotation mentioned above we can estimate the Normalized Mutual Information (NMI) as score for the goodness of the method. NMI measures how many snippets that are annotated as being from different topics are placed into the same topic based on the extracted topics from LDA. It is defined as the fraction of the sum of the entropies of the distributions of the annotations and the disambiguation results, and the entropy of the joint distribution of annotations and results (Manning et al., 2008) (p. 357f). Further, we use one of the standard measures to estimate the goodness of a w</context>
</contexts>
<marker>Griffiths, Steyvers, 2004</marker>
<rawString>T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228–5235, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11936" citStr="Klein and Manning, 2003" startWordPosition="1903" endWordPosition="1906">the topic extraction. For each data set, 30 percent of snippets were disambiguated manually by two independent annotators, whereby doubtful cases were clarified by a third person. The annotations are not used for disambiguation, but for the validation of the method. For each snippet we generate bag-of-words vectors using contexts of 10, 40, 80 or all words around the word of interest. Hence, for context size 10 we use the ten words before the token, the token itself and the ten following tokens, as representation of the snippet. For further experiments we used the Stanford Constituent Parser (Klein and Manning, 2003) to get only the words that syntactically depend on the words of interest. For the extraction of the topics and distribution over the text categories we used the Gibbs sampler for LDA and the author topic model from the Matlab library Topictoolbox (Griffiths and Steyvers, 2004). Based on the annotation mentioned above we can estimate the Normalized Mutual Information (NMI) as score for the goodness of the method. NMI measures how many snippets that are annotated as being from different topics are placed into the same topic based on the extracted topics from LDA. It is defined as the fraction o</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics - Volume 1, ACL ’03, pages 423– 430, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Prabhakar Raghavan</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Introduction to Information Retrieval.</title>
<date>2008</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<marker>Manning, Raghavan, Sch¨utze, 2008</marker>
<rawString>Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch¨utze. 2008. Introduction to Information Retrieval. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Giuseppe Crisafulli</author>
</authors>
<title>Inducing word senses to improve web search result clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>116--126</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6193" citStr="Navigli and Crisafulli, 2010" startWordPosition="946" endWordPosition="949">ficient training for LDA using Monte Carlo sampling. They used Gibbs sampling to estimate the topic distribution. The authors in (Brody and Lapata, 2009) extend the generative model by LDA by many parallel feature representations. Hence, beside the pure words, additional features like part of speech tags can be used. Further, the authors perform analysis with different context sizes. Investigations of word sense disambiguation on small snippets have been done before on search engine results. The snippets retrieved after a query has been sent to a search engine are used for disambiguation. In (Navigli and Crisafulli, 2010) for instance, the authors search for word senses of web search results using retrieved snippets. Our approach differs from these previous ones since we concentrate on snippets from a text corpus for linguistic and lexicographic research purposes (see Section 1). Unlike results from search engines, that refer to documents whose topics are strongly related to the search keyword, result lists from text corpora contain snippets with occurrences of the keyword in each document of the corpus, irrespective of the document topic. That is why keywords can occur in less typical, semantically less defin</context>
</contexts>
<marker>Navigli, Crisafulli, 2010</marker>
<rawString>Roberto Navigli and Giuseppe Crisafulli. 2010. Inducing word senses to improve web search result clustering. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 116–126, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Daniele Vannella</author>
</authors>
<title>Semeval-2013 task 11: Word sense induction and disambiguation within an end-user application.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>193--201</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="13064" citStr="Navigli and Vannella, 2013" startWordPosition="2093" endWordPosition="2096"> placed into the same topic based on the extracted topics from LDA. It is defined as the fraction of the sum of the entropies of the distributions of the annotations and the disambiguation results, and the entropy of the joint distribution of annotations and results (Manning et al., 2008) (p. 357f). Further, we use one of the standard measures to estimate the goodness of a word sense disambiguation result, the F1 score. The F1 score is the weighted average of the precision and recall of the disambiguation results for the given annotations. This and further evaluation methods are described in (Navigli and Vannella, 2013). In the Tables 1 and 2 we show the NMI and F1 score for the extracted topics, respectively senses, by LDA. We tested different context sizes from 10 to 80 words around the word of interest. Compared to the results when we use the whole snippets, we see that a context size of 40 results in the 44 Sense 1 Sense 2 Sense 3 Sense 4 music standing GDR 1 government Berlin saw SED 2 got Prof up party Berlin Comp above political ZK 3 Table 3: Translation of the most frequent words for each of the extracted senses for the word Leiter. Sense 1 Sense 2 Sense 3 Sense 4 question society German publisher DM</context>
</contexts>
<marker>Navigli, Vannella, 2013</marker>
<rawString>Roberto Navigli and Daniele Vannella. 2013. Semeval-2013 task 11: Word sense induction and disambiguation within an end-user application. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 193–201, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word sense disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Comput. Surv.,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="5294" citStr="Navigli, 2009" startWordPosition="802" endWordPosition="803">korpora). 2 Related Work Word sense disambiguation is a well studied problem in Machine Learning and Natural Language Processing. For a given word, later mentioned as word of interest, we expect that there exist several meanings. The differences in the meanings are reflected by different words occurring and frequencies together with the word to be disambiguated. A very early statistical approach was proposed by (Brown et al., 1991). The authors proposed to estimate the probability distribution of senses for given words from annotated examples. A general survey about the topic can be found in (Navigli, 2009). Latent Dirichlet Allocation (LDA) introduced by (Blei et al., 2003) can be used to estimate topic distributions for a given document corpus. Each topic represent a sense in which the documents, respectively the words, appear. (Griffiths and Steyvers, 2004) proposed efficient training for LDA using Monte Carlo sampling. They used Gibbs sampling to estimate the topic distribution. The authors in (Brody and Lapata, 2009) extend the generative model by LDA by many parallel feature representations. Hence, beside the pure words, additional features like part of speech tags can be used. Further, th</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word sense disambiguation: A survey. ACM Comput. Surv., 41(2):10:1–10:69, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steyvers</author>
<author>Padhraic Smyth</author>
<author>Michal Rosen-Zvi</author>
<author>Thomas Griffiths</author>
</authors>
<title>Probabilistic authortopic models for information discovery.</title>
<date>2004</date>
<booktitle>In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04,</booktitle>
<pages>306--315</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9799" citStr="Steyvers et al., 2004" startWordPosition="1553" endWordPosition="1556">tax F1 0.7773 0.6919 0.7630 0.7488 0.4584 Table 2: F1 score of the extracted senses with respect to the given annotations of the text snippets. sampler as proposed by (Griffiths and Steyvers, 2004). The Gibbs sampler models the probability distributions of a given topic zi, depending on all other topics and the words in the snippet as Markov chain. This Markov chain converges to the posterior distribution of the topics given the words in a certain snippet. This posterior can be used to estimate the most likely topic for a given snippet. Further, we use the author topic model as introduced by (Steyvers et al., 2004). This model integrates additional indications about the author for each snippet into the topic modelling process. This method can also be used to model the text categories instead of authors. We simply treat the categories as the authors. Now, the probability distribution of the topics additionally depends on the random variable c over the categories. This can be leveraged to estimate the probability of category c for a given topic zi, hence p(c|zi). Using the author topic model, we estimate the topic distribution over words and categories. Based on these distributions the stochastic process </context>
</contexts>
<marker>Steyvers, Smyth, Rosen-Zvi, Griffiths, 2004</marker>
<rawString>Mark Steyvers, Padhraic Smyth, Michal Rosen-Zvi, and Thomas Griffiths. 2004. Probabilistic authortopic models for information discovery. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’04, pages 306–315, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelika Storrer</author>
</authors>
<title>Korpusgesttzte sprachanalyse in lexikographie und phraseologie.</title>
<date>2011</date>
<booktitle>Angewandte Linguistik. Ein Lehrbuch,</booktitle>
<pages>216--239</pages>
<editor>In Karlfried Knapp et al., editor,</editor>
<publisher>Francke Verlag, Tuebingen.</publisher>
<contexts>
<context position="1807" citStr="Storrer, 2011" startWordPosition="263" endWordPosition="264"> authentic language usage not only in the field of linguistics, but for humanities and social sciences in general. But even though tools for query and analysis are getting more and more flexible and sophisticated (not least thanks to the efforts been done in infrastructure projects like CLARIN), automatically obtained data have to be reviewed manually in most cases because of false positives. Depending on the amount of data, intense manual effort has to be done for cleaning, classification or disambiguation tasks. Hence, many research questions cannot be addressed because of time constraints (Storrer, 2011). A project funded by the German BMBF (Bundesministerium f¨ur Bildung und Forschung, ”Federal Ministry of Education and Research”), therefore, is investigating benefits and issues of using machine learning technology in order to perform these tasks automatically. In this paper, we focus on the disambiguation task, which is an issue known for a long time in the field of corpus-based lexicography (Engelberg and Lemnitzer, 2009), but has not been satisfactorily solved, yet, and is still highly relevant also to social scientists or historians. In the humanities, researchers usually are not examini</context>
</contexts>
<marker>Storrer, 2011</marker>
<rawString>Angelika Storrer. 2011. Korpusgesttzte sprachanalyse in lexikographie und phraseologie. In Karlfried Knapp et al., editor, Angewandte Linguistik. Ein Lehrbuch, pages 216–239. Francke Verlag, Tuebingen.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
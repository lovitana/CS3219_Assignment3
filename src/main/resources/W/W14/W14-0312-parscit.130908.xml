<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.037050">
<title confidence="0.996101">
Confidence-based Active Learning Methods for Machine Translation
</title>
<author confidence="0.997121">
Varvara Logacheva Lucia Specia
</author>
<affiliation confidence="0.7622505">
University of Sheffield University of Sheffield
Sheffield, United Kingdom Sheffield, United Kingdom
</affiliation>
<email confidence="0.979903">
v.logacheva@sheffield.ac.uk l.specia@sheffield.ac.uk
</email>
<sectionHeader confidence="0.993455" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999976833333333">
The paper presents experiments with ac-
tive learning methods for the acquisition
of training data in the context of machine
translation. We propose a confidence-
based method which is superior to the
state-of-the-art method both in terms of
quality and complexity. Additionally,
we discovered that oracle selection tech-
niques that use real quality scores lead to
poor results, making the effectiveness of
confidence-driven methods of active learn-
ing for machine translation questionable.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999690253731343">
Active learning (AL) is a technique for the auto-
matic selection of data which is most useful for
model building. In the context of machine trans-
lation (MT), AL is particularly important as the
acquisition of data often has a high cost, i.e. new
source texts need to be translated manually. Thus
it is beneficial to select for manual translation sen-
tences which can lead to better translation quality.
The majority of AL methods for MT is based
on the (dis)similarity of sentences with respect to
the training data, with particular focus on domain
adaptation. Eck et al. (2005) suggest a TF-IDF
metric to choose sentences with words absent in
the training corpus. Ambati et al. (2010) propose
a metric of informativeness relying on unseen n-
grams.
Bloodgood and Callison-Burch (2010) use n-
gram frequency and coverage of the additional
data as selection criteria. Their technique solic-
its translations for phrases instead of entire sen-
tences, which saves user effort and leads to quality
improvements even if the initial dataset is already
sizeable.
A recent trend is to select source sentences
based on an estimate of the quality of their trans-
lation by a baseline MT system. It is assumed
that if a sentence has been translated well with the
existing data, it will not contribute to improving
the translation quality. If however a sentence has
been translated erroneously, it might have words
or phrases that are absent or incorrectly repre-
sented. Haffari et al. (2009) train a classifier to
define the sentences to select. The classifier uses
a set of features of the source sentences and their
automatic translations: n-grams and phrases fre-
quency, MT model score, etc. Ananthakrishnan et
al. (2010) build a pairwise classifier that ranks sen-
tences according to the proportion of n-grams they
contain that can cause errors. For quality estima-
tion, Banerjee et al. (2013) train language models
of well and badly translated sentences. The use-
fulness of a sentence is measured as the difference
of its perplexities in these two language models.
In this research we also explore a quality-based
AL technique. Compared to its predecessors, our
method is based on a more complex and therefore
potentially more reliable quality estimation frame-
work. It uses wider range of features, which go
beyond those used in previous work, covering in-
formation from both source and target sentences.
Another important novel feature in our work is
the addition of real post-editions to the MT train-
ing data, as opposed to simulated post-editions
(human reference translations) as in previous work
on AL for MT. As we show in section 3.2, adding
post-editions leads to superior translation quality
improvements. Additionally, this is a suitable so-
lution for “human in the loop” settings, as post-
editing automatically translated sentences tends to
be faster and easier than translation from scratch
(Koehn and Haddow, 2009). Also, different from
previous work, we do not focus on domain adapta-
tion: our experiments involve only in-domain data.
Compared to previous work on confidence-
driven AL, our approach has led to better results,
but these proved to be highly dependent on a sen-
tence length bias. However, an oracle-based selec-
</bodyText>
<page confidence="0.977513">
78
</page>
<bodyText confidence="0.841655333333334">
Workshop on Humans and Computer-assisted Translation, pages 78–83,
Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics
tion using true quality scores has not been shown
to perform well. This indicates that the usefulness
of quality scores as AL selection criterion in the
context of MT needs to be further investigated.
</bodyText>
<sectionHeader confidence="0.882367" genericHeader="method">
2 Active selection strategy
</sectionHeader>
<bodyText confidence="0.999792342857143">
Our AL sentence selection strategy relies on qual-
ity estimation (QE). QE is aimed at predicting the
quality of a translated text (in this case, a sen-
tence) without resorting to reference translations.
It considers features of the source and machine
translated texts, and an often small number (a few
hundreds) of examples of translations labelled for
quality by humans to train a machine learning al-
gorithm to predict such quality labels for new data.
We use the open source QE framework QuEst
(Specia et al., 2013). In our settings it was trained
to predict an HTER score (Snover et al., 2006) for
each sentence, i.e., the edit distance between the
automatic translation and its human post-edited
version. QuEst can extract a wide range of fea-
tures. In our experiments we use only the 17 so-
called baseline features, which have been shown
to perform well in evaluation campaigns (Bojar
et al., 2013): number of tokens in sentences, av-
erage token length, language model probabilities
for source and target sentences, average number of
translations per source word, percentage of higher
and lower frequency n-grams in source sentence
based on MT training corpus, number of punctua-
tion marks in source and target sentences.
Similarly to Ananthakrishnan et al. (2010), we
assume that the most useful sentences are those
that lead to larger translation errors. However,
instead of looking at the n-grams that caused er-
rors — a very sparse indicator requiring signifi-
cantly larger amounts of training data, we account
for errors in a more general way: the (QuEst pre-
dicted) percentage of edits (HTER) that would be
necessary to transform the MT output into a cor-
rect sentence.
</bodyText>
<sectionHeader confidence="0.993094" genericHeader="evaluation">
3 Experiments and results
</sectionHeader>
<subsectionHeader confidence="0.996897">
3.1 Datasets and MT settings
</subsectionHeader>
<bodyText confidence="0.999955096153846">
For the AL data selection experiment, two datasets
are necessary: parallel sentences to train an ini-
tial, baseline MT system, and an additional pool
of parallel sentences to select from. Our goal
was to study potential improvements in the base-
line MT system in a realistic “human in the loop”
scenario, where source sentences are translated by
the baseline system and post-edited by humans be-
fore they are added to the system. As it has been
shown in (Potet et al., 2012), post-editions tend to
be closer to source sentences than freely created
translations. One of our research questions was to
investigate whether they would be more useful to
improve MT quality.
We chose the biggest corpus with machine
translations and post-editions available to date: the
LIG French–English post-editions corpus (Potet
et al., 2012). It contains 10,881 quadruples of
the type: &lt;source sentence, reference transla-
tion, automatic translation, post-edited automatic
translation&gt;. Out of these, we selected 9,000 as
the pool to be added to be baseline MT system,
and the remaining 1,881 to train the QE system for
the experiments with AL. For QE training, we use
the HTER scores between MT and its post-edited
version as computed by the TERp tool.1
We use the Moses toolkit with standard set-
tings2 to build the (baseline) statistical MT sys-
tems. As training data, we use the French-
English News Commentary corpus released by the
WMT13 shared task (Bojar et al., 2013). For the
AL experiments, the size of the pool of additional
data (10,000) poses a limitation. To examine im-
provements obtained by adding fractions of up to
only 9,000 sentences, we took a small random sub-
set of the WMT13 data for these experiments (Ta-
ble 1). Although these figures may seem small, the
settings are realistic for many language pairs and
text domains where larger data sets are simply not
available.
We should also note that all the data used in our
experiments belongs to the same domain: the LIG
SMT system which produced sentences for the
post-editions corpus was trained on Europarl and
News commentary datasets (Potet et al., 2010), but
the post-edited sentences themselves were taken
from news test sets released for WMT shared tasks
in different years. Our baseline system is trained
on a fraction of the news commentary corpus. Fi-
nally, we tune and test all our systems on WMT
shared task news news datasets (those which do
not overlap with the post-editions corpus).
</bodyText>
<footnote confidence="0.9981315">
1http://www.umiacs.umd.edu/˜snover/
terp/
2http://www.statmt.org/moses/?n=Moses.
Baseline
</footnote>
<page confidence="0.995391">
79
</page>
<table confidence="0.998408545454545">
Corpora Size
(sentences)
Initial data (baseline MT system)
Training - subset of 10,000
News Commentary corpus
Tuning - WMT newstest-2012 3,000
Test - WMT newstest-2013 3,000
Additional data (AL data)
Post-editions corpus: 10,881
- Training QE system 1,881
- AL pool 9,000
</table>
<tableCaption confidence="0.999181">
Table 1: Datasets
</tableCaption>
<subsectionHeader confidence="0.999346">
3.2 Post-editions versus references
</subsectionHeader>
<bodyText confidence="0.999927952380952">
In order to compare the impact of post-editions
and reference translations on MT quality, we
added these two variants of translations to base-
line MT systems of different sizes, including the
entire News Commentary corpus. The figures for
BLEU (Papineni et al., 2002) scores in Table 2
show that adding post-editions results in signifi-
cantly better quality than adding the same number
of reference translations3. This effect can be seen
even when the additional data corresponds to only
a small fraction of the training data.
In addition, it does not seem to matter which
MT system produced the translations which were
then post-edited in the post-edition corpus. Even if
the output of a third-party system was used (as in
our case), it improves the quality of machine trans-
lations for unseen data. We assume that since post-
editions tend to be closer to original sentences than
free translations (Potet et al., 2012), they gener-
ally help produce better source-target alignments,
leading to the extraction of good quality phrases.
</bodyText>
<table confidence="0.999765">
Baseline corpus Results (BLEU)
(sentences) Baseline Ref PE
150,000 22.41 22.95 23.21
50,000 20.22 20.91 22.01
10,000 15.09 18.65 20.44
</table>
<tableCaption confidence="0.742784">
Table 2: Influence of post-edited and reference
translations on MT quality. Ref: baseline system
with added free references, PE: baseline system
with added post-editions.
</tableCaption>
<footnote confidence="0.93382925">
3These systems use the whole post-editions set (10,881
sentences) as opposed to 9,000-sentence subset which we use
further in our AL experiments. Therefore the figures reported
in this table are higher than those in subsequent sections.
</footnote>
<subsectionHeader confidence="0.988293">
3.3 AL settings
</subsectionHeader>
<bodyText confidence="0.998999666666667">
The experimental settings for all methods are as
follows. First, a baseline MT system is trained.
Then a batch of 1,000 sentences is selected from
the data pool with an AL strategy, and the selected
data is removed from the pool. The MT system is
rebuilt using a concatenation of the initial training
data and the new batch. The process is repeated
until the pool is empty, with subsequent steps us-
ing the MT system trained on the previous step as
a baseline. The performance of each MT system
is measured in terms of BLEU scores. We use the
following AL strategies:
</bodyText>
<listItem confidence="0.999528857142857">
• QuEst: our method described in section 2.
• Random: random selection of sentences.
• HTER: oracle-based selection based on true
HTER scores of sentences in the pool, instead
of the QuEst estimated HTER scores.
• Ranking: AL strategy described in (Anan-
thakrishnan et al., 2010) for comparison.
</listItem>
<subsectionHeader confidence="0.979249">
3.4 AL results
</subsectionHeader>
<bodyText confidence="0.980990666666667">
Our initial results in Figure 1 show that our selec-
tion strategy (QuEst) consistently outperforms the
Random selection baseline.
</bodyText>
<figureCaption confidence="0.900580666666667">
Figure 1: Performance of MT systems enhanced
with data selected by different AL strategies
In comparison with previous work, we found
that the error-based Ranking strategy performs
closely to Random selection, although (Anan-
thakrishnan et al., 2010) reports it to be better.
</figureCaption>
<page confidence="0.991099">
80
</page>
<bodyText confidence="0.999860377777778">
Compared to QuEst, we believe the lower figures
of the Ranking strategy are due to the fact that the
latter considers features of only one type (source
n-grams), whereas QuEst uses a range of different
features of the source and translation sentences.
Interestingly, the Oracle method under-
performs our QE-based method, although we
expected the use of real HTER scores to be more
effective. In order to understand the reasons
behind such behaviour, we examined the batches
selected by QuEst and Oracle strategies more
closely. We found that the distribution of sentence
lengths in batches by the two strategies is very
different (see Figure 2). While in batches selected
by QuEst the average sentence length steadily
decreases as more data is added, in Oracle
batches the average length was almost uniform for
all batches, except the first one, which contains
shorter sentences.
This is explained by HTER formulation: HTER
is computed as the number of edits over the sen-
tence length, and therefore in shorter sentences ev-
ery edit is given more weight. For example, the
HTER score of a 5-word sentence with one error
is 0.2, whereas a sentence of 20 words with the
same single error has a score of 0.05. However, it
is doubtful that the former sentence will be more
useful for an MT system than the latter. Regarding
the nature of length bias in the predictions done by
QuEst system, sentence length is used there as a
feature, and longer sentences tend to be estimated
as having higher HTER scores (i.e., lower transla-
tion quality).
Therefore, sentences with the highest HTER
may not actually be the most useful, which makes
the Oracle strategy inferior to QuEst. Moreover,
longer sentences chosen by our strategy simply
provide more data, so their addition might be more
useful even regardless of the amount of errors.
This seems to indicate that the success of our
strategy might not be related to the quality of the
translations only, but to their length. Another pos-
sibility is that sentences selected by QuEst might
have more errors, which means that they can con-
tribute more to the MT system.
</bodyText>
<subsectionHeader confidence="0.95253">
3.5 Additional experiments
</subsectionHeader>
<bodyText confidence="0.998737">
In order to check the two hypotheses put forward
in the previous section, we conduct two other sets
of AL experiments: (i) a selection strategy that
chooses longer sentences first (denoted as Length)
</bodyText>
<figureCaption confidence="0.971668">
Figure 2: Number of words in batches selected by
different AL strategies
</figureCaption>
<bodyText confidence="0.984087904761905">
and (ii) a selection strategy that chooses sentences
with larger numbers of errors first (Errors).
Figure 3 shows that a simple length-based strat-
egy yields better results than any of the other
tested strategies. Therefore, in cases when the
corpus has sufficient variation in sentence length,
length-based selection might perform at least as
well as other more sophisticated criteria. The
experiments with confidence-based selection de-
scribed in (Ananthakrishnan et al., 2010) were free
of this length bias, as sentences much longer or
shorter than average were deliberately filtered out.
Interestingly, results for the Errors strategy are
slightly worse than those for QuEst, although the
former is guaranteed to choose sentences with the
largest number of errors and has even stronger
length bias than QuEst (see figure 2). Therefore,
the reasons hypothesised to be behind the superi-
ority of QuEst over Oracle (longer sentences and
larger number of errors) are actually not the only
factors that influence the quality of an AL strategy.
</bodyText>
<subsectionHeader confidence="0.996753">
3.6 Length-independent results
</subsectionHeader>
<bodyText confidence="0.999956363636364">
Despite the success of the length-based strategy,
we do not believe that it is enough for an effective
AL technique. First of all, the experiment with
the Errors strategy demonstrated that more data
does not always lead to better results. Further-
more, our aim is to reduce the translator’s effort in
cases when the additional data needs to be trans-
lated or post-edited manually. However, longer
sentences usually take more time to translate or
edit, so choosing the longest sentences from a pool
of sentences will not reduce translator’s effort.
</bodyText>
<page confidence="0.998614">
81
</page>
<figureCaption confidence="0.9954705">
Figure 3: Comparison of our QuEst-based selec-
tion with a length-based selection
</figureCaption>
<bodyText confidence="0.999656384615385">
Therefore, we would like to study the effec-
tiveness of our strategy by isolating the sentence
length bias. One option is to filter out long sen-
tences, as it was done in (Ananthakrishnan et al.,
2010). However, our pool is already too small.
Therefore, we plot the performance improvements
with respect to training data size in words, in-
stead of sentences. As it was already noted by
Bloodgood and Callison-Burch (2010), measuring
the amount of added data in sentences can signifi-
cantly contort the real annotation cost (the cost of
acquisition of new translations). So we switch to
length-independent representation.
</bodyText>
<figureCaption confidence="0.9939392">
Figure 4: Active learning quality plotted with re-
spect to data size in words: QuEst vs Oracle
strategies.
Figure 4 shows that the Oracle strategy in
Figure 5: AL quality plotted with respect to data
</figureCaption>
<bodyText confidence="0.967909481481481">
size in words: QuEst vs Length and Errors
strategies.
length-independent representation can still be seen
to perform worse than both our strategy and ran-
dom selection. Results of Length and Error
strategies (plotted separately in figure 5 for read-
ability) are very close and both underperform our
QuEst-based strategy and random selection of
data.
Here our experience echoes the results of (Mo-
hit and Hwa, 2007), where the authors propose the
idea of difficult to translate phrases. It is assumed
that extending an MT system with phrases that can
cause difficulties during translation is more effec-
tive than simply adding new data and re-building
the system. Due to the lack of time and human
annotators, the authors extracted difficult phrases
automatically using a set of features: alignment
features, syntactic features, model score, etc. Con-
versely, we had the human-generated information
on what segments have been translated incorrectly.
We assumed that the use of this knowledge as part
of our AL strategy would give us an upper bound
for our AL method results. However, it turned out
that prediction based on multiple features is more
reliable than precise information on quality, which
accounts for only one aspect of data.
</bodyText>
<sectionHeader confidence="0.999726" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999953">
We presented experiments with an active learning
strategy for machine translation based on quality
predictions. This strategy performs well compared
to another quality-driven strategy and a random
baseline. However, we found that it was success-
</bodyText>
<page confidence="0.995527">
82
</page>
<bodyText confidence="0.9999959">
ful mostly due to its tendency to rate long sen-
tences as having lower quality. Consequently, the
AL application that chooses the longest sentences
is not less successful when selecting from corpora
with large variation in sentence length. A length-
independent representation of the results showed
that an oracle selection is less effective than our
quality-based strategy, which we believe to be due
to the nature of corrections and small size of the
post-edition corpus. In addition to that, another
oracle selection based on the amount of errors and
length-based selection show poor results when dis-
played in length-independent mode.
We believe that the quality estimation strategy
benefits from other features that reflect the useful-
ness of a sentence better than its HTER score and
the amount of user corrections. In future work we
will examine the influence of individual features
of the quality estimation model (such as language
model scores) as active learning selection strategy.
</bodyText>
<sectionHeader confidence="0.998048" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999311790697675">
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active Learning and Crowd-Sourcing for Ma-
chine Translation. LREC 2010: Proceedings of the
seventh international conference on Language Re-
sources and Evaluation, 17-23 May 2010, Valletta,
Malta, pages 2169–2174.
Sankaranarayanan Ananthakrishnan, Rohit Prasad,
David Stallard, and Prem Natarajan. 2010. Dis-
criminative Sample Selection for Statistical Machine
Translation. EMNLP-2010: Proceedings of the
2010 Conference on Empirical Methods in Natu-
ral Language Processing, October 9-11, 2010, MIT,
Massachusetts, USA, (October):626–635.
Pratyush Banerjee, Raphael Rubino, Johann Roturier,
and Josef van Genabith. 2013. Quality Estimation-
guided Data Selection for Domain Adaptation of
SMT. MT Summit XIV: proceedings of the four-
teenth Machine Translation Summit, September 2-6,
2013, Nice, France, pages 101–108.
Michael Bloodgood and Chris Callison-Burch. 2010.
Bucking the Trend: Large-Scale Cost-Focused Ac-
tive Learning for Statistical Machine Translation.
ACL 2010: the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, Uppsala, Swe-
den, July 11-16, 2010, pages 854–864.
Ondˇrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1–44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Matthias Eck, Stephan Vogel, and Alex Waibel. 2005.
Low Cost Portability for Statistical Machine Trans-
lation based on N-gram Frequency and TF-IDF.
IWSLT 2005: Proceedings of the International
Workshop on Spoken Language Translation. Octo-
ber 24-25, 2005, Pittsburgh, PA.
Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.
2009. Active learning for statistical phrase-based
machine translation. Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics on - NAACL ’09.
Philipp Koehn and Barry Haddow. 2009. Interactive
Assistance to Human Translators using Statistical
Machine Translation Methods. MT Summit XII: pro-
ceedings of the twelfth Machine Translation Sum-
mit, August 26-30, 2009, Ottawa, Ontario, Canada,
pages 73–80.
Behrang Mohit and Rebecca Hwa. 2007. Localiza-
tion of Difficult-to-Translate Phrases. ACL 2007:
proceedings of the Second Workshop on Statistical
Machine Translation, June 23, 2007, Prague, Czech
Republic, pages 248–255.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. ACL 2002: 40th
Annual Meeting of the Association for Computa-
tional Linguistics, July 2002, Philadelphia, pages
311–318.
Marion Potet, Laurent Besacier, and Herv´e Blanchon.
2010. The LIG machine translation system for
WMT 2010. ACL 2010: Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 161–166.
Marion Potet, Emmanuelle Esperanc¸a-Rodier, Laurent
Besacier, and Herv´e Blanchon. 2012. Collection
of a Large Database of French-English SMT Output
Corrections. LREC 2012: Eighth international con-
ference on Language Resources and Evaluation, 21-
27 May 2012, Istanbul, Turkey, pages 4043–4048.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. AMTA 2006: Proceedings of the 7th Con-
ference of the Association for Machine Translation
in the Americas, Visions for the Future of Machine
Translation, August 8-12, 2006, Cambridge, Mas-
sachusetts, USA, pages 223–231.
Lucia Specia, Kashif Shah, Jose G C de Souza, and
Trevor Cohn. 2013. QuEst - A translation quality
estimation framework. ACL 2013: Annual Meet-
ing of the Association for Computational Linguis-
tics, Demo session, August 2013, Sofia, Bulgaria.
</reference>
<page confidence="0.999312">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.749551">
<title confidence="0.999638">Confidence-based Active Learning Methods for Machine Translation</title>
<author confidence="0.999036">Varvara Logacheva Lucia Specia</author>
<affiliation confidence="0.997424">University of Sheffield University of Sheffield</affiliation>
<address confidence="0.802815">Sheffield, United Kingdom Sheffield, United</address>
<email confidence="0.92297">v.logacheva@sheffield.ac.ukl.specia@sheffield.ac.uk</email>
<abstract confidence="0.998455923076923">The paper presents experiments with active learning methods for the acquisition of training data in the context of machine translation. We propose a confidencebased method which is superior to the state-of-the-art method both in terms of quality and complexity. Additionally, we discovered that oracle selection techniques that use real quality scores lead to poor results, making the effectiveness of confidence-driven methods of active learning for machine translation questionable.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Vamshi Ambati</author>
<author>Stephan Vogel</author>
<author>Jaime Carbonell</author>
</authors>
<title>Active Learning and Crowd-Sourcing for Machine Translation.</title>
<date>2010</date>
<booktitle>LREC 2010: Proceedings of the seventh international conference on Language Resources and Evaluation,</booktitle>
<pages>17--23</pages>
<location>Valletta, Malta,</location>
<contexts>
<context position="1441" citStr="Ambati et al. (2010)" startWordPosition="213" endWordPosition="216"> of data which is most useful for model building. In the context of machine translation (MT), AL is particularly important as the acquisition of data often has a high cost, i.e. new source texts need to be translated manually. Thus it is beneficial to select for manual translation sentences which can lead to better translation quality. The majority of AL methods for MT is based on the (dis)similarity of sentences with respect to the training data, with particular focus on domain adaptation. Eck et al. (2005) suggest a TF-IDF metric to choose sentences with words absent in the training corpus. Ambati et al. (2010) propose a metric of informativeness relying on unseen ngrams. Bloodgood and Callison-Burch (2010) use ngram frequency and coverage of the additional data as selection criteria. Their technique solicits translations for phrases instead of entire sentences, which saves user effort and leads to quality improvements even if the initial dataset is already sizeable. A recent trend is to select source sentences based on an estimate of the quality of their translation by a baseline MT system. It is assumed that if a sentence has been translated well with the existing data, it will not contribute to i</context>
</contexts>
<marker>Ambati, Vogel, Carbonell, 2010</marker>
<rawString>Vamshi Ambati, Stephan Vogel, and Jaime Carbonell. 2010. Active Learning and Crowd-Sourcing for Machine Translation. LREC 2010: Proceedings of the seventh international conference on Language Resources and Evaluation, 17-23 May 2010, Valletta, Malta, pages 2169–2174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sankaranarayanan Ananthakrishnan</author>
<author>Rohit Prasad</author>
<author>David Stallard</author>
<author>Prem Natarajan</author>
</authors>
<title>Discriminative Sample Selection for Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>EMNLP-2010: Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>MIT, Massachusetts, USA, (October):626–635.</location>
<contexts>
<context position="2458" citStr="Ananthakrishnan et al. (2010)" startWordPosition="379" endWordPosition="382">ect source sentences based on an estimate of the quality of their translation by a baseline MT system. It is assumed that if a sentence has been translated well with the existing data, it will not contribute to improving the translation quality. If however a sentence has been translated erroneously, it might have words or phrases that are absent or incorrectly represented. Haffari et al. (2009) train a classifier to define the sentences to select. The classifier uses a set of features of the source sentences and their automatic translations: n-grams and phrases frequency, MT model score, etc. Ananthakrishnan et al. (2010) build a pairwise classifier that ranks sentences according to the proportion of n-grams they contain that can cause errors. For quality estimation, Banerjee et al. (2013) train language models of well and badly translated sentences. The usefulness of a sentence is measured as the difference of its perplexities in these two language models. In this research we also explore a quality-based AL technique. Compared to its predecessors, our method is based on a more complex and therefore potentially more reliable quality estimation framework. It uses wider range of features, which go beyond those u</context>
<context position="5614" citStr="Ananthakrishnan et al. (2010)" startWordPosition="887" endWordPosition="890">nce between the automatic translation and its human post-edited version. QuEst can extract a wide range of features. In our experiments we use only the 17 socalled baseline features, which have been shown to perform well in evaluation campaigns (Bojar et al., 2013): number of tokens in sentences, average token length, language model probabilities for source and target sentences, average number of translations per source word, percentage of higher and lower frequency n-grams in source sentence based on MT training corpus, number of punctuation marks in source and target sentences. Similarly to Ananthakrishnan et al. (2010), we assume that the most useful sentences are those that lead to larger translation errors. However, instead of looking at the n-grams that caused errors — a very sparse indicator requiring significantly larger amounts of training data, we account for errors in a more general way: the (QuEst predicted) percentage of edits (HTER) that would be necessary to transform the MT output into a correct sentence. 3 Experiments and results 3.1 Datasets and MT settings For the AL data selection experiment, two datasets are necessary: parallel sentences to train an initial, baseline MT system, and an addi</context>
<context position="11371" citStr="Ananthakrishnan et al., 2010" startWordPosition="1827" endWordPosition="1831">rom the pool. The MT system is rebuilt using a concatenation of the initial training data and the new batch. The process is repeated until the pool is empty, with subsequent steps using the MT system trained on the previous step as a baseline. The performance of each MT system is measured in terms of BLEU scores. We use the following AL strategies: • QuEst: our method described in section 2. • Random: random selection of sentences. • HTER: oracle-based selection based on true HTER scores of sentences in the pool, instead of the QuEst estimated HTER scores. • Ranking: AL strategy described in (Ananthakrishnan et al., 2010) for comparison. 3.4 AL results Our initial results in Figure 1 show that our selection strategy (QuEst) consistently outperforms the Random selection baseline. Figure 1: Performance of MT systems enhanced with data selected by different AL strategies In comparison with previous work, we found that the error-based Ranking strategy performs closely to Random selection, although (Ananthakrishnan et al., 2010) reports it to be better. 80 Compared to QuEst, we believe the lower figures of the Ranking strategy are due to the fact that the latter considers features of only one type (source n-grams),</context>
<context position="14686" citStr="Ananthakrishnan et al., 2010" startWordPosition="2370" endWordPosition="2373">) a selection strategy that chooses longer sentences first (denoted as Length) Figure 2: Number of words in batches selected by different AL strategies and (ii) a selection strategy that chooses sentences with larger numbers of errors first (Errors). Figure 3 shows that a simple length-based strategy yields better results than any of the other tested strategies. Therefore, in cases when the corpus has sufficient variation in sentence length, length-based selection might perform at least as well as other more sophisticated criteria. The experiments with confidence-based selection described in (Ananthakrishnan et al., 2010) were free of this length bias, as sentences much longer or shorter than average were deliberately filtered out. Interestingly, results for the Errors strategy are slightly worse than those for QuEst, although the former is guaranteed to choose sentences with the largest number of errors and has even stronger length bias than QuEst (see figure 2). Therefore, the reasons hypothesised to be behind the superiority of QuEst over Oracle (longer sentences and larger number of errors) are actually not the only factors that influence the quality of an AL strategy. 3.6 Length-independent results Despit</context>
<context position="16108" citStr="Ananthakrishnan et al., 2010" startWordPosition="2603" endWordPosition="2606">does not always lead to better results. Furthermore, our aim is to reduce the translator’s effort in cases when the additional data needs to be translated or post-edited manually. However, longer sentences usually take more time to translate or edit, so choosing the longest sentences from a pool of sentences will not reduce translator’s effort. 81 Figure 3: Comparison of our QuEst-based selection with a length-based selection Therefore, we would like to study the effectiveness of our strategy by isolating the sentence length bias. One option is to filter out long sentences, as it was done in (Ananthakrishnan et al., 2010). However, our pool is already too small. Therefore, we plot the performance improvements with respect to training data size in words, instead of sentences. As it was already noted by Bloodgood and Callison-Burch (2010), measuring the amount of added data in sentences can significantly contort the real annotation cost (the cost of acquisition of new translations). So we switch to length-independent representation. Figure 4: Active learning quality plotted with respect to data size in words: QuEst vs Oracle strategies. Figure 4 shows that the Oracle strategy in Figure 5: AL quality plotted with</context>
</contexts>
<marker>Ananthakrishnan, Prasad, Stallard, Natarajan, 2010</marker>
<rawString>Sankaranarayanan Ananthakrishnan, Rohit Prasad, David Stallard, and Prem Natarajan. 2010. Discriminative Sample Selection for Statistical Machine Translation. EMNLP-2010: Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, October 9-11, 2010, MIT, Massachusetts, USA, (October):626–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pratyush Banerjee</author>
<author>Raphael Rubino</author>
<author>Johann Roturier</author>
<author>Josef van Genabith</author>
</authors>
<title>Quality Estimationguided Data Selection for Domain Adaptation of SMT.</title>
<date>2013</date>
<booktitle>MT Summit XIV: proceedings of the fourteenth Machine Translation</booktitle>
<pages>101--108</pages>
<location>Summit,</location>
<marker>Banerjee, Rubino, Roturier, van Genabith, 2013</marker>
<rawString>Pratyush Banerjee, Raphael Rubino, Johann Roturier, and Josef van Genabith. 2013. Quality Estimationguided Data Selection for Domain Adaptation of SMT. MT Summit XIV: proceedings of the fourteenth Machine Translation Summit, September 2-6, 2013, Nice, France, pages 101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bloodgood</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation.</title>
<date>2010</date>
<booktitle>ACL 2010: the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>854--864</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1539" citStr="Bloodgood and Callison-Burch (2010)" startWordPosition="227" endWordPosition="230">tion (MT), AL is particularly important as the acquisition of data often has a high cost, i.e. new source texts need to be translated manually. Thus it is beneficial to select for manual translation sentences which can lead to better translation quality. The majority of AL methods for MT is based on the (dis)similarity of sentences with respect to the training data, with particular focus on domain adaptation. Eck et al. (2005) suggest a TF-IDF metric to choose sentences with words absent in the training corpus. Ambati et al. (2010) propose a metric of informativeness relying on unseen ngrams. Bloodgood and Callison-Burch (2010) use ngram frequency and coverage of the additional data as selection criteria. Their technique solicits translations for phrases instead of entire sentences, which saves user effort and leads to quality improvements even if the initial dataset is already sizeable. A recent trend is to select source sentences based on an estimate of the quality of their translation by a baseline MT system. It is assumed that if a sentence has been translated well with the existing data, it will not contribute to improving the translation quality. If however a sentence has been translated erroneously, it might </context>
<context position="16327" citStr="Bloodgood and Callison-Burch (2010)" startWordPosition="2638" endWordPosition="2641"> take more time to translate or edit, so choosing the longest sentences from a pool of sentences will not reduce translator’s effort. 81 Figure 3: Comparison of our QuEst-based selection with a length-based selection Therefore, we would like to study the effectiveness of our strategy by isolating the sentence length bias. One option is to filter out long sentences, as it was done in (Ananthakrishnan et al., 2010). However, our pool is already too small. Therefore, we plot the performance improvements with respect to training data size in words, instead of sentences. As it was already noted by Bloodgood and Callison-Burch (2010), measuring the amount of added data in sentences can significantly contort the real annotation cost (the cost of acquisition of new translations). So we switch to length-independent representation. Figure 4: Active learning quality plotted with respect to data size in words: QuEst vs Oracle strategies. Figure 4 shows that the Oracle strategy in Figure 5: AL quality plotted with respect to data size in words: QuEst vs Length and Errors strategies. length-independent representation can still be seen to perform worse than both our strategy and random selection. Results of Length and Error strate</context>
</contexts>
<marker>Bloodgood, Callison-Burch, 2010</marker>
<rawString>Michael Bloodgood and Chris Callison-Burch. 2010. Bucking the Trend: Large-Scale Cost-Focused Active Learning for Statistical Machine Translation. ACL 2010: the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden, July 11-16, 2010, pages 854–864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ondˇrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5250" citStr="Bojar et al., 2013" startWordPosition="832" endWordPosition="835">small number (a few hundreds) of examples of translations labelled for quality by humans to train a machine learning algorithm to predict such quality labels for new data. We use the open source QE framework QuEst (Specia et al., 2013). In our settings it was trained to predict an HTER score (Snover et al., 2006) for each sentence, i.e., the edit distance between the automatic translation and its human post-edited version. QuEst can extract a wide range of features. In our experiments we use only the 17 socalled baseline features, which have been shown to perform well in evaluation campaigns (Bojar et al., 2013): number of tokens in sentences, average token length, language model probabilities for source and target sentences, average number of translations per source word, percentage of higher and lower frequency n-grams in source sentence based on MT training corpus, number of punctuation marks in source and target sentences. Similarly to Ananthakrishnan et al. (2010), we assume that the most useful sentences are those that lead to larger translation errors. However, instead of looking at the n-grams that caused errors — a very sparse indicator requiring significantly larger amounts of training data</context>
<context position="7527" citStr="Bojar et al., 2013" startWordPosition="1206" endWordPosition="1209">ns 10,881 quadruples of the type: &lt;source sentence, reference translation, automatic translation, post-edited automatic translation&gt;. Out of these, we selected 9,000 as the pool to be added to be baseline MT system, and the remaining 1,881 to train the QE system for the experiments with AL. For QE training, we use the HTER scores between MT and its post-edited version as computed by the TERp tool.1 We use the Moses toolkit with standard settings2 to build the (baseline) statistical MT systems. As training data, we use the FrenchEnglish News Commentary corpus released by the WMT13 shared task (Bojar et al., 2013). For the AL experiments, the size of the pool of additional data (10,000) poses a limitation. To examine improvements obtained by adding fractions of up to only 9,000 sentences, we took a small random subset of the WMT13 data for these experiments (Table 1). Although these figures may seem small, the settings are realistic for many language pairs and text domains where larger data sets are simply not available. We should also note that all the data used in our experiments belongs to the same domain: the LIG SMT system which produced sentences for the post-editions corpus was trained on Europa</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondˇrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1–44, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Low Cost Portability for Statistical Machine Translation based on N-gram Frequency and TF-IDF.</title>
<date>2005</date>
<booktitle>IWSLT 2005: Proceedings of the International Workshop on Spoken Language Translation.</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1334" citStr="Eck et al. (2005)" startWordPosition="195" endWordPosition="198">translation questionable. 1 Introduction Active learning (AL) is a technique for the automatic selection of data which is most useful for model building. In the context of machine translation (MT), AL is particularly important as the acquisition of data often has a high cost, i.e. new source texts need to be translated manually. Thus it is beneficial to select for manual translation sentences which can lead to better translation quality. The majority of AL methods for MT is based on the (dis)similarity of sentences with respect to the training data, with particular focus on domain adaptation. Eck et al. (2005) suggest a TF-IDF metric to choose sentences with words absent in the training corpus. Ambati et al. (2010) propose a metric of informativeness relying on unseen ngrams. Bloodgood and Callison-Burch (2010) use ngram frequency and coverage of the additional data as selection criteria. Their technique solicits translations for phrases instead of entire sentences, which saves user effort and leads to quality improvements even if the initial dataset is already sizeable. A recent trend is to select source sentences based on an estimate of the quality of their translation by a baseline MT system. It</context>
</contexts>
<marker>Eck, Vogel, Waibel, 2005</marker>
<rawString>Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Low Cost Portability for Statistical Machine Translation based on N-gram Frequency and TF-IDF. IWSLT 2005: Proceedings of the International Workshop on Spoken Language Translation. October 24-25, 2005, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gholamreza Haffari</author>
<author>Maxim Roy</author>
<author>Anoop Sarkar</author>
</authors>
<title>Active learning for statistical phrase-based machine translation.</title>
<date>2009</date>
<booktitle>Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on - NAACL ’09.</booktitle>
<contexts>
<context position="2226" citStr="Haffari et al. (2009)" startWordPosition="342" endWordPosition="345">ion criteria. Their technique solicits translations for phrases instead of entire sentences, which saves user effort and leads to quality improvements even if the initial dataset is already sizeable. A recent trend is to select source sentences based on an estimate of the quality of their translation by a baseline MT system. It is assumed that if a sentence has been translated well with the existing data, it will not contribute to improving the translation quality. If however a sentence has been translated erroneously, it might have words or phrases that are absent or incorrectly represented. Haffari et al. (2009) train a classifier to define the sentences to select. The classifier uses a set of features of the source sentences and their automatic translations: n-grams and phrases frequency, MT model score, etc. Ananthakrishnan et al. (2010) build a pairwise classifier that ranks sentences according to the proportion of n-grams they contain that can cause errors. For quality estimation, Banerjee et al. (2013) train language models of well and badly translated sentences. The usefulness of a sentence is measured as the difference of its perplexities in these two language models. In this research we also </context>
</contexts>
<marker>Haffari, Roy, Sarkar, 2009</marker>
<rawString>Gholamreza Haffari, Maxim Roy, and Anoop Sarkar. 2009. Active learning for statistical phrase-based machine translation. Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics on - NAACL ’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Barry Haddow</author>
</authors>
<title>Interactive Assistance to Human Translators using Statistical Machine Translation Methods.</title>
<date>2009</date>
<booktitle>MT Summit XII: proceedings of the twelfth Machine Translation Summit,</booktitle>
<pages>73--80</pages>
<location>Ottawa, Ontario, Canada,</location>
<contexts>
<context position="3660" citStr="Koehn and Haddow, 2009" startWordPosition="571" endWordPosition="574">ich go beyond those used in previous work, covering information from both source and target sentences. Another important novel feature in our work is the addition of real post-editions to the MT training data, as opposed to simulated post-editions (human reference translations) as in previous work on AL for MT. As we show in section 3.2, adding post-editions leads to superior translation quality improvements. Additionally, this is a suitable solution for “human in the loop” settings, as postediting automatically translated sentences tends to be faster and easier than translation from scratch (Koehn and Haddow, 2009). Also, different from previous work, we do not focus on domain adaptation: our experiments involve only in-domain data. Compared to previous work on confidencedriven AL, our approach has led to better results, but these proved to be highly dependent on a sentence length bias. However, an oracle-based selec78 Workshop on Humans and Computer-assisted Translation, pages 78–83, Gothenburg, Sweden, 26 April 2014. c�2014 Association for Computational Linguistics tion using true quality scores has not been shown to perform well. This indicates that the usefulness of quality scores as AL selection cr</context>
</contexts>
<marker>Koehn, Haddow, 2009</marker>
<rawString>Philipp Koehn and Barry Haddow. 2009. Interactive Assistance to Human Translators using Statistical Machine Translation Methods. MT Summit XII: proceedings of the twelfth Machine Translation Summit, August 26-30, 2009, Ottawa, Ontario, Canada, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Rebecca Hwa</author>
</authors>
<title>Localization of Difficult-to-Translate Phrases.</title>
<date>2007</date>
<booktitle>ACL 2007: proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>248--255</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="17136" citStr="Mohit and Hwa, 2007" startWordPosition="2767" endWordPosition="2771">entation. Figure 4: Active learning quality plotted with respect to data size in words: QuEst vs Oracle strategies. Figure 4 shows that the Oracle strategy in Figure 5: AL quality plotted with respect to data size in words: QuEst vs Length and Errors strategies. length-independent representation can still be seen to perform worse than both our strategy and random selection. Results of Length and Error strategies (plotted separately in figure 5 for readability) are very close and both underperform our QuEst-based strategy and random selection of data. Here our experience echoes the results of (Mohit and Hwa, 2007), where the authors propose the idea of difficult to translate phrases. It is assumed that extending an MT system with phrases that can cause difficulties during translation is more effective than simply adding new data and re-building the system. Due to the lack of time and human annotators, the authors extracted difficult phrases automatically using a set of features: alignment features, syntactic features, model score, etc. Conversely, we had the human-generated information on what segments have been translated incorrectly. We assumed that the use of this knowledge as part of our AL strateg</context>
</contexts>
<marker>Mohit, Hwa, 2007</marker>
<rawString>Behrang Mohit and Rebecca Hwa. 2007. Localization of Difficult-to-Translate Phrases. ACL 2007: proceedings of the Second Workshop on Statistical Machine Translation, June 23, 2007, Prague, Czech Republic, pages 248–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>ACL 2002: 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia,</location>
<contexts>
<context position="9204" citStr="Papineni et al., 2002" startWordPosition="1472" endWordPosition="1475">g/moses/?n=Moses. Baseline 79 Corpora Size (sentences) Initial data (baseline MT system) Training - subset of 10,000 News Commentary corpus Tuning - WMT newstest-2012 3,000 Test - WMT newstest-2013 3,000 Additional data (AL data) Post-editions corpus: 10,881 - Training QE system 1,881 - AL pool 9,000 Table 1: Datasets 3.2 Post-editions versus references In order to compare the impact of post-editions and reference translations on MT quality, we added these two variants of translations to baseline MT systems of different sizes, including the entire News Commentary corpus. The figures for BLEU (Papineni et al., 2002) scores in Table 2 show that adding post-editions results in significantly better quality than adding the same number of reference translations3. This effect can be seen even when the additional data corresponds to only a small fraction of the training data. In addition, it does not seem to matter which MT system produced the translations which were then post-edited in the post-edition corpus. Even if the output of a third-party system was used (as in our case), it improves the quality of machine translations for unseen data. We assume that since posteditions tend to be closer to original sent</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. ACL 2002: 40th Annual Meeting of the Association for Computational Linguistics, July 2002, Philadelphia, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Potet</author>
<author>Laurent Besacier</author>
<author>Herv´e Blanchon</author>
</authors>
<title>The LIG machine translation system for WMT</title>
<date>2010</date>
<booktitle>ACL 2010: Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>161--166</pages>
<contexts>
<context position="8179" citStr="Potet et al., 2010" startWordPosition="1318" endWordPosition="1321">e of the pool of additional data (10,000) poses a limitation. To examine improvements obtained by adding fractions of up to only 9,000 sentences, we took a small random subset of the WMT13 data for these experiments (Table 1). Although these figures may seem small, the settings are realistic for many language pairs and text domains where larger data sets are simply not available. We should also note that all the data used in our experiments belongs to the same domain: the LIG SMT system which produced sentences for the post-editions corpus was trained on Europarl and News commentary datasets (Potet et al., 2010), but the post-edited sentences themselves were taken from news test sets released for WMT shared tasks in different years. Our baseline system is trained on a fraction of the news commentary corpus. Finally, we tune and test all our systems on WMT shared task news news datasets (those which do not overlap with the post-editions corpus). 1http://www.umiacs.umd.edu/˜snover/ terp/ 2http://www.statmt.org/moses/?n=Moses. Baseline 79 Corpora Size (sentences) Initial data (baseline MT system) Training - subset of 10,000 News Commentary corpus Tuning - WMT newstest-2012 3,000 Test - WMT newstest-2013</context>
</contexts>
<marker>Potet, Besacier, Blanchon, 2010</marker>
<rawString>Marion Potet, Laurent Besacier, and Herv´e Blanchon. 2010. The LIG machine translation system for WMT 2010. ACL 2010: Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 161–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Potet</author>
<author>Emmanuelle Esperanc¸a-Rodier</author>
<author>Laurent Besacier</author>
<author>Herv´e Blanchon</author>
</authors>
<title>Collection of a Large Database of French-English SMT Output Corrections.</title>
<date>2012</date>
<booktitle>LREC 2012: Eighth international conference on Language Resources and Evaluation,</booktitle>
<pages>21--27</pages>
<location>Istanbul, Turkey,</location>
<marker>Potet, Esperanc¸a-Rodier, Besacier, Blanchon, 2012</marker>
<rawString>Marion Potet, Emmanuelle Esperanc¸a-Rodier, Laurent Besacier, and Herv´e Blanchon. 2012. Collection of a Large Database of French-English SMT Output Corrections. LREC 2012: Eighth international conference on Language Resources and Evaluation, 21-27 May 2012, Istanbul, Turkey, pages 4043–4048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>AMTA 2006: Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, Visions for the Future of Machine Translation,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA,</location>
<contexts>
<context position="4945" citStr="Snover et al., 2006" startWordPosition="781" endWordPosition="784">Active selection strategy Our AL sentence selection strategy relies on quality estimation (QE). QE is aimed at predicting the quality of a translated text (in this case, a sentence) without resorting to reference translations. It considers features of the source and machine translated texts, and an often small number (a few hundreds) of examples of translations labelled for quality by humans to train a machine learning algorithm to predict such quality labels for new data. We use the open source QE framework QuEst (Specia et al., 2013). In our settings it was trained to predict an HTER score (Snover et al., 2006) for each sentence, i.e., the edit distance between the automatic translation and its human post-edited version. QuEst can extract a wide range of features. In our experiments we use only the 17 socalled baseline features, which have been shown to perform well in evaluation campaigns (Bojar et al., 2013): number of tokens in sentences, average token length, language model probabilities for source and target sentences, average number of translations per source word, percentage of higher and lower frequency n-grams in source sentence based on MT training corpus, number of punctuation marks in so</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. AMTA 2006: Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, Visions for the Future of Machine Translation, August 8-12, 2006, Cambridge, Massachusetts, USA, pages 223–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Kashif Shah</author>
<author>Jose G C de Souza</author>
<author>Trevor Cohn</author>
</authors>
<title>QuEst - A translation quality estimation framework.</title>
<date>2013</date>
<booktitle>ACL 2013: Annual Meeting of the Association for Computational Linguistics, Demo session,</booktitle>
<location>Sofia, Bulgaria.</location>
<marker>Specia, Shah, de Souza, Cohn, 2013</marker>
<rawString>Lucia Specia, Kashif Shah, Jose G C de Souza, and Trevor Cohn. 2013. QuEst - A translation quality estimation framework. ACL 2013: Annual Meeting of the Association for Computational Linguistics, Demo session, August 2013, Sofia, Bulgaria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
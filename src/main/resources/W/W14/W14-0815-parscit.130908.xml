<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001527">
<title confidence="0.630812">
Detecting change and emergence for multiword expressions
</title>
<author confidence="0.993564">
Martin Emms
</author>
<affiliation confidence="0.882653666666667">
Department of Computer Science
Trinity College, Dublin
Ireland
</affiliation>
<email confidence="0.978337">
Martin.Emms@tcd.ie
</email>
<sectionHeader confidence="0.997048" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998904">
This work looks at a temporal aspect of
multiword expressions (MWEs), namely
that the behaviour of a given n-gram and
its status as a MWE change over time. We
propose a model in which context words
have particular probabilities given a us-
age choice for an n-gram, and those us-
age choices have time dependent probabil-
ities, and we put forward an expectation-
maximisation technique for estimating the
parameters from data with no annotation
of usage choice. For a range of MWE
usages of recent coinage, we evaluate
whether the technique is able to detect the
emerging usage.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9997957">
When an n-gram is designated a ’multiword ex-
pression’, or MWE, its because it possesses prop-
erties which are not straightforwardly predictable
given the component words of the n-gram – that
red tape can refer to bureaucratic regulation would
be a simple example. A further aspect is that while
some tokens of the n-gram type may be examples
of the irregular MWE usage, others may not be –
so red tape can certainly also be used in a fash-
ion which is transparent relative to its parts. A
further aspect is temporal: that tokens of the n-
gram can be sought in language samples from dif-
ferent times. It seems reasonable to assume that
the irregular MWE usage of red tape at some time
emerged, and was predated by the more transpar-
ent usage. This paper concerns the possibility of
finding automatic, unsupervised means to detect
the emergence of a MWE usage of a given n-gram.
To illustrate further, consider the following ex-
amples (these are all taken from the data set on
</bodyText>
<page confidence="0.998863">
89
</page>
<author confidence="0.471034">
Arun Jayapal
</author>
<affiliation confidence="0.933458">
Department of Computer Science
Trinity College, Dublin
</affiliation>
<figure confidence="0.862973583333333">
Ireland
jayapala@tcd.ie
which we worked)
(a) the wind lifted his three-car garage and (1)
smashed it to the ground. (1995)
(a′) sensational group CEO, totally smashed
it in the BGT (Britain Got Talent) (2013)
(b) my schedule gave me time to get ad-
justed (1990)
(b′) it’s important to set time out and enjoy
some me time (2013)
(a) and (a′) feature the n-gram smashed it. (a)
</figure>
<bodyText confidence="0.996816862068965">
uses the standard destructive sense of smashed,
and it refers to an object undergoing the destruc-
tive transformation. In (a′) the n-gram is used dif-
ferently and is roughly replaceable by ’excelled’,
a usage not via the standard sense of smashed, nor
one where it refers to any object at all. Where in
both (a) and (a′) the n-gram would be regarded as
a phrase, (b) and (b′) involving the n-gram me time
show another possibility. In (b), me and time are
straightforward dependants of gave. In (b′), the
two words form a noun-phrase, meaning some-
thing like ’personal time’. The usage is arguably
more acceptable than would be the case with other
object pronouns, and if addressed to a particular
person, the me would refer to the addressee, which
is not the usual function of a first-person pronoun.
For smashed it and me time, the second (primed)
example illustrates an irregular usage-variant of
the n-gram, whilst the first illustrates a regular
usage-variant, and the irregular example is drawn
from a later time than the regular usage. Lan-
guage is a dynamic phenomenon, with the range
of ways a given n-gram might contribute subject
to change over time, and for these n-grams, it
would seem to be the case that the availability
of the ’me time’ = ’personal time’ and ’smashed
it = ’excelled’ usage-variants is a relatively re-
cent innovation1, predated by the regular usage-
variants. It seems that in work on multiword ex-
</bodyText>
<footnote confidence="0.98372">
1That is to say, recent in British English according to the
</footnote>
<note confidence="0.718905">
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 89–93,
Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999829125">
pressions, there has been little attention paid to
this dynamic aspect, whereby a particular multi-
word usage starts to play a role in a language at a
particular point in time. Building on earlier work
(Emms, 2013), we present some work concerning
unsupervised means to detect this. Section 2 de-
scribes our data, section 3 our EM-based method
and section 4 discusses the results obtained.
</bodyText>
<sectionHeader confidence="0.991377" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999964857142857">
To investigate such emergence phenomena some
kind of time-stamped corpus is required. The ap-
proach we took to this was to exploit a search fa-
cility that Google has offered for some time – cus-
tom date range – whereby it is possible to specify
a time period for text matching the searched item.
To obtain data for a given n-gram, we repeatedly
set different year-long time spans and saved the
first 100 returned ’hits’ as potential examples of
the n-gram’s use. Each ’hit’ has a text snippet and
an anchor text for a link to the online source from
which the snippet comes. If the text snippet or an-
chor string contains the n-gram it can furnish an
example of its use, and the longer of the two is
taken if both feature the n-gram.
A number of n-grams were chosen having the
properties that they have an irregular, MWE usage
alongside a regular one, with the MWE usage a re-
cent innovation. These were smashed it, me time
(illustrated in (1)) and going forward, and biolog-
ical clock, illustrated below.
</bodyText>
<listItem confidence="0.98325525">
(c) Going forward from the entrance, (2)
you’ll come to a large room. (1995)
(c′) Going forward BJP should engage in
people’s movements (2009)
(d) A biological clock present in most eu-
karyotes imposes daily rhythms (1995)
(d′) How To Stop Worrying About Your Bi-
ological Clock ... Pressure to have a
</listItem>
<bodyText confidence="0.97732575">
baby before 35 (2009)
Alongside the plain movement usage-variant seen
in (c), going forward has the more opaque usage-
variant in which it is roughly replaceable by ’in
the future’, seen in (c′). Alongside a technical use
in biology seen in (d), biological clock has come
to be used in a wider context to refer to a sense of
expiring time within which people may be able to
have a child, seen in (d′).
first author’s intuitions. It is not easy to find sources to cor-
roborate such intuitions
For each n-gram data was downloaded for suc-
cessive year-long time-spans from 1990 to 2013,
retaining the first 100 hits for each year. For some
of the earlier years there are less than 100 hits, but
mostly there are more than 100. This gives on the
order of 2000 examples for each n-gram, each with
a date stamp, but otherwise with no other annota-
tion. See Section 4 for some discussion of this
method of obtaining data.
</bodyText>
<sectionHeader confidence="0.995931" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.990706448275862">
For an n-gram with usage variants (as illustrated
by (1) and (2)), we take the Bayesian approach
that each variant gives different probabilities to
the words in its immediate vicinity, as has been
done in unsupervised word-sense disambiguation
(Manning and Sch¨utze, 2003; de Marneffe and
Dupont, 2004). In those approaches, which ignore
any temporal dimension, it is also assumed that
there are prior probabilities on the usage-variants.
We bring in language change by having a succes-
sion of priors, one for each time period.
To make this more precise, where T is an oc-
currence of a particular n-gram, with W the se-
quence of words around T, let Y represent its
time-stamp. If we suppose there are k different
usage-variants of the n-gram, we simply model
this with a discrete variable S which can take on k
values. So S can be thought of as ranging over po-
sitions in an enumeration of the different ways that
the n-gram can contribute to the semantics. With
these variables we can say that we are consider-
ing a probability model for p(Y, S, W ). Apply-
ing the chain-rule this may be re-expresssed with-
out loss of generality as p(Y )p(S|Y )p(W |S, Y ).
We then make some assumptions: (i) that W
is conditionally independent of Y given S, so
p(W |S, Y ) = p(W |S), (ii) that p(W |S) may be
treated as HZ(p(W Z|S) , and (iii) that p(Y ) is uni-
form. This then gives
</bodyText>
<equation confidence="0.995623">
p(Y, S,W) = p(Y )p(S|Y ) � (p(W Z|S) (3)
Z
</equation>
<bodyText confidence="0.985266833333333">
The term p(S|Y ) directly models the fact that a
usage variant can vary its likelihood over time,
possibly having zero probability on some early
range of times. While (i) make context words
and times indepedent given a usage variant, con-
text words are still time-dependent: the sum
</bodyText>
<equation confidence="0.539002">
E
</equation>
<bodyText confidence="0.827815">
S[p(S|Y )p(W|S)] varies with time Y due to
</bodyText>
<page confidence="0.981915">
90
</page>
<bodyText confidence="0.9966245625">
p(S|Y ). Assumption (i) reflects a plausibile idea
that given a concept being conveyed, the expected
accompanying vocabulary is substantially time-
independent. Moreover (i) drastically reduces the
number of parameters to be estimated: with 20
time spans and a 2-way usage choice, the word
probabilities are conditioned on 2 settings rather
than 40.
The parameters of the model in (3) have to be
estimated from data which is labelled only for
time – the usage-variant variable is a hidden vari-
able – and we tackle this with an EM procedure
(Dempster et al., 1977). Space precludes giving
the derivations of the update formulae but in out-
line there is an iteration of an E and an M step, as
follows:
</bodyText>
<equation confidence="0.894692818181818">
(E step) based on current parameters, a table, γ,
is populated, such that for each data point d, and
possible S value s, γ[d][s] stores P(S = s|Y =
yd, W = wd).
(M step) based on γ, fresh parameter values are
re-estimated according to:
Ed(if Y d=y then γ[d][s] else 0)
P(S = s|Y = y) =
Ed(if Yd=y then 1 else 0)
P(w |S = s) = Ed(γ[d][s]×freq(w∈Wd))
1 Ed(γ[d][s]×length(Wd))
</equation>
<bodyText confidence="0.999805333333333">
These updates can be shown to increase the
data probability, where the usage variable S is
summed-out.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999935557142857">
Running the above-outlined EM procedure on the
downloaded data for a particular n-gram gener-
ates unsupervised estimates for p(S|Y ) – inferred
usage distributions for each time span. To ob-
tain a reference with which to compare these in-
ferred distributions, approximately 10% of the
data per time-span was manually annotated and
used to give simple relative-frequency estimates of
p(S|Y ) – which we will call empirical estimates.
Although the data was downloaded for year-long
time spans, it was decided to group the data into
successive spans of 3 year duration. This was to
make the empirical p(S|Y ) less brittle as they are
otherwise based on too small a quantity of data.
Figure 1 shows the outcomes, as usage-variant
probabilities in a succession of time spans, both
the empirical estimates obtained on a subset, and
the unsupervised estimates obtained on all the
data. The EM method can seek any number
of usage variants, and the results show the case
where 2 variants were sought. Where the man-
ually annotated subset used more variants these
were grouped to facilitate a comparison.
For smashed it, biological clock and going for-
ward, the o line in the empirical plot is for the
MWE usage, and for me time it is the 0 line, and
it has an upward trend. In the unsupervised case,
there is inevitable indeterminacy about which S
values may come to be associated with any objec-
tively real usage. Modulo this the unsupervised
and supervised graphs broadly concur.
One can also inspect the context-words which
come to have high probability in one semantic
variant relative to their probability in another. For
example, for smashed it, for the semantic usage
which is inferred to have an increasing proba-
bility in recent years, a selection from the most
favoured tokens includes !!, guys, really, com-
pletely, They, !, whilst for the other usage they in-
clude smithereens, bits, bottle, onto, phone. For
biological clock, a similar exercise gives for the
apparently increasing usage, tokens such as Ticks,
Ticking?, Health, Fertility and for the other usage
running, 24-hour, controlled, mammalian, mecha-
nisms. These associations would seem to be con-
sistent with the inferred semantic-usages being in
broad correspondence with the annotated usages.
As noted in section 2, as a means to obtain
data on relatively recent n-gram usages, we used
the custom date range search facility of Google.
One of the issues with such data is the potential
for the time-stamping (inferred by Google) to be
innaccurate. Though its not possible to exhaus-
tively verify the time-stamping, some inspection
was done, which revealed that although there are
some cases of documents which were incorrectly
stamped, this was tolerably infrequent. Then there
is the question of the representativeness of the
sample obtained. The mechanism we used gives
the first 100 from the at most 1000 ’hits’ which
Google will return from amongst all index docu-
ments which match the n-gram and the date range,
so an uncontrollable factor is the ranking mech-
anism according to which these hits are selected
and ordered. The fact that the empirical usage
distributions accord reasonably well with prior in-
tuition is a modest indicator that the data is not
unusably unrepresentative. One could also argue
that for an initial test of the algorithms it suf-
fices for the methods to recover an apparent trend
</bodyText>
<page confidence="0.991627">
91
</page>
<figure confidence="0.992489357142857">
biological clock (empirical and unsupervised)
bio clock
1995 2005
Year
Sense Prop
0.0 0.4 0.8
Sense Prop
0.0 0.4 0.8
1995 2005
Year
smashed it (empirical and unsupervised)
smashed it
0
0
Sense Prop 0.0 0.2 0.4 0.6 0.8 1 . Sense Prop 0.0 0.2 0.4 0.6 0.8 1 .
2000 2005 2010
Year
2000 2004 2008 2012
Year
going forward (empirical and unsupervised)
me time (empirical and unsupervised)
i
me t
1995 2005
Year
me t
1995 2005
Year
0.0 0.4 0.8
Sense Prop
0.0 0.4 0.8
Sense Prop
0.0 0.4 0.8
Sense Prop
Sense Prop
1995 2005
0.0 0.4 0.8
1995 2005
Year
Year
going forward
going forward
</figure>
<figureCaption confidence="0.9702185">
Figure 1: For each n-gram the plots show the empirical usage-variant distributions per time-period in the
labelled subset and unsupervised usage-varaint distributions per time-period in the entire data set
</figureCaption>
<bodyText confidence="0.999951816326531">
in the downloaded data, even if the data is un-
representative. This being said, one direction for
further work will be to consider other sources of
time-stamped language use, such as the Google n-
grams corpus (Brants and Franz, 2012), or various
newswire corpora (Graff et al., 2007).
There does not seem to have been that much
work on unsupervised means to identify emer-
gence of new usage of a given expression – there
is more work which groups all tokens of a type
together and uses change of context words to indi-
cate an evolving single meaning (Sagi et al., 2008;
Gulordava and Baroni, 2011). Lau et al. (2012)
though they do not address MWEs do look at the
emergence of new word senses, applying a word-
sense induction technique. Their testing was be-
tween two corpora taken to represent two different
time periods, the BNC and ukWac corpus, taken
to represent the late 20th century and 2007, re-
spectively, and they reported promising results on
5 words. The unsupervised method they used is
based on a Hierarchical Dirichlet Process model
(Yao and Van Durme, 2011), and a direction for
future work will be a closer comparison of the al-
gorithm presented here to that algorithm and other
related LDA-based methods in word sense induc-
tion (Brody and Lapata, 2009). Also the bag-
of-tokens model of the context words which we
adopted is a very simple one, and we wish to con-
sider more sophisticated models involving for ex-
ample part-of-tagging or syntactic structures.
The results are indicative at least that MWE
usage of an n-gram can be detected by unsuper-
vised means to be preceded by the other usages of
the n-gram. There has been some work on algo-
rithms which seek to quantify the degree of com-
positionality of particular n-grams (Maldonado-
Guerra and Emms, 2011; Biemann and Gies-
brecht, 2011) and it is hoped in future work to
consider the possible integration of some of these
techniques with those reported here. For a given
n-gram, it would be interesting to know if the col-
lection of its occurrences which the techniques of
the current paper suggest to belong to a more re-
cently emerging usage, are also a corpus of occur-
rences relative to which a compositionality mea-
sure would report the n-gram as being of low com-
positionality, and conversely for the apparently
less recent usage.
</bodyText>
<sectionHeader confidence="0.997782" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<footnote confidence="0.54823975">
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
the Centre for Next Generation Localisation
(www.cngl.ie) at Trinity College Dublin.
</footnote>
<page confidence="0.992018">
92
</page>
<sectionHeader confidence="0.9958" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999565515625001">
Chris Biemann and Eugenie Giesbrecht, editors. 2011.
Proceedings of the Workshop on Distributional Se-
mantics and Compositionality.
Thorsten Brants and Alex Franz. 2012. Google books
n-grams. ngrams.googlelabs.com.
Samuel Brody and Mirella Lapata. 2009. Bayesian
word sense induction. In EACL 09: Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 103–111, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Marie-Catharine de Marneffe and Pierre Dupont. 2004.
Comparative study of statistical word sense discrim-
ination. In G´erald Purnelle, C´edric Fairon, and
Anne Dister, editors, Proceedings ofJADT2004 7th
International Conference on the Statistical Analysis
of Textual Data, pages 270–281. UCL Presses Uni-
versitaire de Louvain.
A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977.
Maximum likelihood from incomplete data via the
em algorithm. J. Royal Statistical Society, B 39:1–
38.
Martin Emms. 2013. Dynamic EM in neologism evo-
lution. In Hujun Yin, Ke Tang, Yang Gao, Frank
Klawonn, Minho Lee, Thomas Weise, Bin Li, and
Xin Yao, editors, Proceedings of IDEAL 2013, vol-
ume 8206 of Lecture Notes in Computer Science,
pages 286–293. Springer.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English gigaword corpus. Linguis-
tic Data Consortium.
Kristina Gulordava and Marco Baroni. 2011. A distri-
butional similarity approach to the detection of se-
mantic change in the google books ngram corpus. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 67–71, Edinburgh, UK, July. Association for
Computational Linguistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense in-
duction for novel sense detection. In Proceedings of
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, EACL
’12, pages 591–601, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Alfredo Maldonado-Guerra and Martin Emms. 2011.
Measuring the compositionality of collocations via
word co-occurrence vectors: Shared task system de-
scription. In Proceedings of the Workshop on Dis-
tributional Semantics and Compositionality, pages
48–53, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Christopher Manning and Hinrich Sch¨utze, 2003.
Foundations of Statistical Language Processing,
chapter Word Sense Disambiguation, pages 229–
264. MIT Press, 6 edition.
Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2008.
Tracing semantic change with latent semantic anal-
ysis. In Proceedings ofICEHL 2008.
Xuchen Yao and Benjamin Van Durme. 2011. Non-
parametric bayesian word sense induction. In Pro-
ceedings of TextGraphs-6: Graph-based Methods
for Natural Language Processing, pages 10–14. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.999173">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.863238">
<title confidence="0.999357">Detecting change and emergence for multiword expressions</title>
<author confidence="0.999011">Martin</author>
<affiliation confidence="0.9927225">Department of Computer Trinity College,</affiliation>
<email confidence="0.889831">Martin.Emms@tcd.ie</email>
<abstract confidence="0.9992061875">This work looks at a temporal aspect of multiword expressions (MWEs), namely that the behaviour of a given n-gram and its status as a MWE change over time. We propose a model in which context words have particular probabilities given a usage choice for an n-gram, and those usage choices have time dependent probabilities, and we put forward an expectationmaximisation technique for estimating the parameters from data with no annotation of usage choice. For a range of MWE usages of recent coinage, we evaluate whether the technique is able to detect the emerging usage.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2011</date>
<booktitle>Proceedings of the Workshop on Distributional Semantics and Compositionality.</booktitle>
<editor>Chris Biemann and Eugenie Giesbrecht, editors.</editor>
<marker>2011</marker>
<rawString>Chris Biemann and Eugenie Giesbrecht, editors. 2011. Proceedings of the Workshop on Distributional Semantics and Compositionality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<date>2012</date>
<note>Google books n-grams. ngrams.googlelabs.com.</note>
<contexts>
<context position="13738" citStr="Brants and Franz, 2012" startWordPosition="2380" endWordPosition="2383"> 1995 2005 Year me t 1995 2005 Year 0.0 0.4 0.8 Sense Prop 0.0 0.4 0.8 Sense Prop 0.0 0.4 0.8 Sense Prop Sense Prop 1995 2005 0.0 0.4 0.8 1995 2005 Year Year going forward going forward Figure 1: For each n-gram the plots show the empirical usage-variant distributions per time-period in the labelled subset and unsupervised usage-varaint distributions per time-period in the entire data set in the downloaded data, even if the data is unrepresentative. This being said, one direction for further work will be to consider other sources of time-stamped language use, such as the Google ngrams corpus (Brants and Franz, 2012), or various newswire corpora (Graff et al., 2007). There does not seem to have been that much work on unsupervised means to identify emergence of new usage of a given expression – there is more work which groups all tokens of a type together and uses change of context words to indicate an evolving single meaning (Sagi et al., 2008; Gulordava and Baroni, 2011). Lau et al. (2012) though they do not address MWEs do look at the emergence of new word senses, applying a wordsense induction technique. Their testing was between two corpora taken to represent two different time periods, the BNC and uk</context>
</contexts>
<marker>Brants, Franz, 2012</marker>
<rawString>Thorsten Brants and Alex Franz. 2012. Google books n-grams. ngrams.googlelabs.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Mirella Lapata</author>
</authors>
<title>Bayesian word sense induction.</title>
<date>2009</date>
<booktitle>In EACL 09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>103--111</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14769" citStr="Brody and Lapata, 2009" startWordPosition="2561" endWordPosition="2564"> MWEs do look at the emergence of new word senses, applying a wordsense induction technique. Their testing was between two corpora taken to represent two different time periods, the BNC and ukWac corpus, taken to represent the late 20th century and 2007, respectively, and they reported promising results on 5 words. The unsupervised method they used is based on a Hierarchical Dirichlet Process model (Yao and Van Durme, 2011), and a direction for future work will be a closer comparison of the algorithm presented here to that algorithm and other related LDA-based methods in word sense induction (Brody and Lapata, 2009). Also the bagof-tokens model of the context words which we adopted is a very simple one, and we wish to consider more sophisticated models involving for example part-of-tagging or syntactic structures. The results are indicative at least that MWE usage of an n-gram can be detected by unsupervised means to be preceded by the other usages of the n-gram. There has been some work on algorithms which seek to quantify the degree of compositionality of particular n-grams (MaldonadoGuerra and Emms, 2011; Biemann and Giesbrecht, 2011) and it is hoped in future work to consider the possible integration</context>
</contexts>
<marker>Brody, Lapata, 2009</marker>
<rawString>Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In EACL 09: Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 103–111, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catharine de Marneffe</author>
<author>Pierre Dupont</author>
</authors>
<title>Comparative study of statistical word sense discrimination.</title>
<date>2004</date>
<booktitle>Proceedings ofJADT2004 7th International Conference on the Statistical Analysis of Textual Data,</booktitle>
<pages>270--281</pages>
<editor>In G´erald Purnelle, C´edric Fairon, and Anne Dister, editors,</editor>
<marker>de Marneffe, Dupont, 2004</marker>
<rawString>Marie-Catharine de Marneffe and Pierre Dupont. 2004. Comparative study of statistical word sense discrimination. In G´erald Purnelle, C´edric Fairon, and Anne Dister, editors, Proceedings ofJADT2004 7th International Conference on the Statistical Analysis of Textual Data, pages 270–281. UCL Presses Universitaire de Louvain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the em algorithm.</title>
<date>1977</date>
<journal>J. Royal Statistical Society, B</journal>
<volume>39</volume>
<pages>38</pages>
<contexts>
<context position="8725" citStr="Dempster et al., 1977" startWordPosition="1524" endWordPosition="1527">ent: the sum E S[p(S|Y )p(W|S)] varies with time Y due to 90 p(S|Y ). Assumption (i) reflects a plausibile idea that given a concept being conveyed, the expected accompanying vocabulary is substantially timeindependent. Moreover (i) drastically reduces the number of parameters to be estimated: with 20 time spans and a 2-way usage choice, the word probabilities are conditioned on 2 settings rather than 40. The parameters of the model in (3) have to be estimated from data which is labelled only for time – the usage-variant variable is a hidden variable – and we tackle this with an EM procedure (Dempster et al., 1977). Space precludes giving the derivations of the update formulae but in outline there is an iteration of an E and an M step, as follows: (E step) based on current parameters, a table, γ, is populated, such that for each data point d, and possible S value s, γ[d][s] stores P(S = s|Y = yd, W = wd). (M step) based on γ, fresh parameter values are re-estimated according to: Ed(if Y d=y then γ[d][s] else 0) P(S = s|Y = y) = Ed(if Yd=y then 1 else 0) P(w |S = s) = Ed(γ[d][s]×freq(w∈Wd)) 1 Ed(γ[d][s]×length(Wd)) These updates can be shown to increase the data probability, where the usage variable S is</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A.P. Dempster, N.M. Laird, and D.B. Rubin. 1977. Maximum likelihood from incomplete data via the em algorithm. J. Royal Statistical Society, B 39:1– 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Emms</author>
</authors>
<title>Dynamic EM in neologism evolution.</title>
<date>2013</date>
<booktitle>Proceedings of IDEAL 2013,</booktitle>
<volume>8206</volume>
<pages>286--293</pages>
<editor>In Hujun Yin, Ke Tang, Yang Gao, Frank Klawonn, Minho Lee, Thomas Weise, Bin Li, and Xin Yao, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="4027" citStr="Emms, 2013" startWordPosition="682" endWordPosition="683">sonal time’ and ’smashed it = ’excelled’ usage-variants is a relatively recent innovation1, predated by the regular usagevariants. It seems that in work on multiword ex1That is to say, recent in British English according to the Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 89–93, Gothenburg, Sweden, 26-27 April 2014. c�2014 Association for Computational Linguistics pressions, there has been little attention paid to this dynamic aspect, whereby a particular multiword usage starts to play a role in a language at a particular point in time. Building on earlier work (Emms, 2013), we present some work concerning unsupervised means to detect this. Section 2 describes our data, section 3 our EM-based method and section 4 discusses the results obtained. 2 Data To investigate such emergence phenomena some kind of time-stamped corpus is required. The approach we took to this was to exploit a search facility that Google has offered for some time – custom date range – whereby it is possible to specify a time period for text matching the searched item. To obtain data for a given n-gram, we repeatedly set different year-long time spans and saved the first 100 returned ’hits’ a</context>
</contexts>
<marker>Emms, 2013</marker>
<rawString>Martin Emms. 2013. Dynamic EM in neologism evolution. In Hujun Yin, Ke Tang, Yang Gao, Frank Klawonn, Minho Lee, Thomas Weise, Bin Li, and Xin Yao, editors, Proceedings of IDEAL 2013, volume 8206 of Lecture Notes in Computer Science, pages 286–293. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<title>English gigaword corpus. Linguistic Data Consortium.</title>
<date>2007</date>
<contexts>
<context position="13788" citStr="Graff et al., 2007" startWordPosition="2388" endWordPosition="2391">Prop 0.0 0.4 0.8 Sense Prop 0.0 0.4 0.8 Sense Prop Sense Prop 1995 2005 0.0 0.4 0.8 1995 2005 Year Year going forward going forward Figure 1: For each n-gram the plots show the empirical usage-variant distributions per time-period in the labelled subset and unsupervised usage-varaint distributions per time-period in the entire data set in the downloaded data, even if the data is unrepresentative. This being said, one direction for further work will be to consider other sources of time-stamped language use, such as the Google ngrams corpus (Brants and Franz, 2012), or various newswire corpora (Graff et al., 2007). There does not seem to have been that much work on unsupervised means to identify emergence of new usage of a given expression – there is more work which groups all tokens of a type together and uses change of context words to indicate an evolving single meaning (Sagi et al., 2008; Gulordava and Baroni, 2011). Lau et al. (2012) though they do not address MWEs do look at the emergence of new word senses, applying a wordsense induction technique. Their testing was between two corpora taken to represent two different time periods, the BNC and ukWac corpus, taken to represent the late 20th centu</context>
</contexts>
<marker>Graff, Kong, Chen, Maeda, 2007</marker>
<rawString>David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2007. English gigaword corpus. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Gulordava</author>
<author>Marco Baroni</author>
</authors>
<title>A distributional similarity approach to the detection of semantic change in the google books ngram corpus.</title>
<date>2011</date>
<booktitle>In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,</booktitle>
<pages>67--71</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, UK,</location>
<contexts>
<context position="14100" citStr="Gulordava and Baroni, 2011" startWordPosition="2446" endWordPosition="2449">ime-period in the entire data set in the downloaded data, even if the data is unrepresentative. This being said, one direction for further work will be to consider other sources of time-stamped language use, such as the Google ngrams corpus (Brants and Franz, 2012), or various newswire corpora (Graff et al., 2007). There does not seem to have been that much work on unsupervised means to identify emergence of new usage of a given expression – there is more work which groups all tokens of a type together and uses change of context words to indicate an evolving single meaning (Sagi et al., 2008; Gulordava and Baroni, 2011). Lau et al. (2012) though they do not address MWEs do look at the emergence of new word senses, applying a wordsense induction technique. Their testing was between two corpora taken to represent two different time periods, the BNC and ukWac corpus, taken to represent the late 20th century and 2007, respectively, and they reported promising results on 5 words. The unsupervised method they used is based on a Hierarchical Dirichlet Process model (Yao and Van Durme, 2011), and a direction for future work will be a closer comparison of the algorithm presented here to that algorithm and other relat</context>
</contexts>
<marker>Gulordava, Baroni, 2011</marker>
<rawString>Kristina Gulordava and Marco Baroni. 2011. A distributional similarity approach to the detection of semantic change in the google books ngram corpus. In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, pages 67–71, Edinburgh, UK, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jey Han Lau</author>
<author>Paul Cook</author>
<author>Diana McCarthy</author>
<author>David Newman</author>
<author>Timothy Baldwin</author>
</authors>
<title>Word sense induction for novel sense detection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12,</booktitle>
<pages>591--601</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="14119" citStr="Lau et al. (2012)" startWordPosition="2450" endWordPosition="2453"> set in the downloaded data, even if the data is unrepresentative. This being said, one direction for further work will be to consider other sources of time-stamped language use, such as the Google ngrams corpus (Brants and Franz, 2012), or various newswire corpora (Graff et al., 2007). There does not seem to have been that much work on unsupervised means to identify emergence of new usage of a given expression – there is more work which groups all tokens of a type together and uses change of context words to indicate an evolving single meaning (Sagi et al., 2008; Gulordava and Baroni, 2011). Lau et al. (2012) though they do not address MWEs do look at the emergence of new word senses, applying a wordsense induction technique. Their testing was between two corpora taken to represent two different time periods, the BNC and ukWac corpus, taken to represent the late 20th century and 2007, respectively, and they reported promising results on 5 words. The unsupervised method they used is based on a Hierarchical Dirichlet Process model (Yao and Van Durme, 2011), and a direction for future work will be a closer comparison of the algorithm presented here to that algorithm and other related LDA-based method</context>
</contexts>
<marker>Lau, Cook, McCarthy, Newman, Baldwin, 2012</marker>
<rawString>Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense induction for novel sense detection. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL ’12, pages 591–601, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfredo Maldonado-Guerra</author>
<author>Martin Emms</author>
</authors>
<title>Measuring the compositionality of collocations via word co-occurrence vectors: Shared task system description.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Distributional Semantics and Compositionality,</booktitle>
<pages>48--53</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<marker>Maldonado-Guerra, Emms, 2011</marker>
<rawString>Alfredo Maldonado-Guerra and Martin Emms. 2011. Measuring the compositionality of collocations via word co-occurrence vectors: Shared task system description. In Proceedings of the Workshop on Distributional Semantics and Compositionality, pages 48–53, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<date>2003</date>
<booktitle>Foundations of Statistical Language Processing, chapter Word Sense Disambiguation,</booktitle>
<volume>6</volume>
<pages>229--264</pages>
<publisher>MIT Press,</publisher>
<marker>Manning, Sch¨utze, 2003</marker>
<rawString>Christopher Manning and Hinrich Sch¨utze, 2003. Foundations of Statistical Language Processing, chapter Word Sense Disambiguation, pages 229– 264. MIT Press, 6 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Sagi</author>
<author>Stefan Kaufmann</author>
<author>Brady Clark</author>
</authors>
<title>Tracing semantic change with latent semantic analysis.</title>
<date>2008</date>
<booktitle>In Proceedings ofICEHL</booktitle>
<contexts>
<context position="14071" citStr="Sagi et al., 2008" startWordPosition="2442" endWordPosition="2445">distributions per time-period in the entire data set in the downloaded data, even if the data is unrepresentative. This being said, one direction for further work will be to consider other sources of time-stamped language use, such as the Google ngrams corpus (Brants and Franz, 2012), or various newswire corpora (Graff et al., 2007). There does not seem to have been that much work on unsupervised means to identify emergence of new usage of a given expression – there is more work which groups all tokens of a type together and uses change of context words to indicate an evolving single meaning (Sagi et al., 2008; Gulordava and Baroni, 2011). Lau et al. (2012) though they do not address MWEs do look at the emergence of new word senses, applying a wordsense induction technique. Their testing was between two corpora taken to represent two different time periods, the BNC and ukWac corpus, taken to represent the late 20th century and 2007, respectively, and they reported promising results on 5 words. The unsupervised method they used is based on a Hierarchical Dirichlet Process model (Yao and Van Durme, 2011), and a direction for future work will be a closer comparison of the algorithm presented here to t</context>
</contexts>
<marker>Sagi, Kaufmann, Clark, 2008</marker>
<rawString>Eyal Sagi, Stefan Kaufmann, and Brady Clark. 2008. Tracing semantic change with latent semantic analysis. In Proceedings ofICEHL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuchen Yao</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Nonparametric bayesian word sense induction.</title>
<date>2011</date>
<booktitle>In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing,</booktitle>
<pages>10--14</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yao, Van Durme, 2011</marker>
<rawString>Xuchen Yao and Benjamin Van Durme. 2011. Nonparametric bayesian word sense induction. In Proceedings of TextGraphs-6: Graph-based Methods for Natural Language Processing, pages 10–14. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
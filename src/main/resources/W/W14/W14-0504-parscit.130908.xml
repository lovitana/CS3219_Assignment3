<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025277">
<title confidence="0.990422">
Learning the hyperparameters to learn morphology
</title>
<author confidence="0.996324">
Stella Frank
</author>
<affiliation confidence="0.9965875">
ILCC, School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.965869">
Edinburgh, EH8 9AB, UK
</address>
<email confidence="0.997841">
sfrank@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993843" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999566714285714">
We perform hyperparameter inference
within a model of morphology learn-
ing (Goldwater et al., 2011) and find
that it affects model behaviour drastically.
Changing the model structure successfully
avoids the unsegmented solution, but re-
sults in oversegmentation instead.
</bodyText>
<sectionHeader confidence="0.999265" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974136363636">
Bayesian models provide a sound statistical
framework in which to explore aspects of language
acquisition. Explicitly specifying the causal and
computational structure of a model enables the in-
vestigation of hypotheses such as the feasibility of
learning linguistic structure from the available in-
put (Perfors et al., 2011), or the interaction of dif-
ferent linguistic levels (Johnson, 2008a). However,
these models can be sensitive to small changes in
(hyper-)parameters settings. Robustness in this re-
spect is important, since positing specific parame-
ter values is cognitively implausible.
In this paper we revisit a model of morphol-
ogy learning presented by Goldwater and col-
leagues in Goldwater et al. (2006) and Goldwa-
ter et al. (2011) (henceforth GGJ). This model
demonstrated the effectiveness of non-parametric
stochastic processes, specifically the Pitman-Yor
Process, for interpolating between types and to-
kens. Language learners are exposed to tokens,
but many aspects of linguistic structure are lexical;
identifying which tokens belong to the same lexi-
cal type is crucial. Surface form is not always suf-
ficient, as in the case of ambiguous words. More-
over, morphology in particular is influenced by
vocabulary-level type statistics (Bybee, 1995), so
it is important for a model to operate on both lev-
els: token statistics from realistic (child-directed)
input, and type-level statistics based on the token
analyses.
The GGJ model learns successfully given fixed
hyperparameter values in the Pitman-Yor Process.
However, we show that when these hyperparam-
eters are inferred, it collapses to a token-based
model with a trivial morphology. In this paper
we discuss the reasons for this problematic be-
haviour, which are relevant for other models based
on Pitman-Yor Processes with discrete base dis-
tributions, common in natural language tasks. We
investigate some potential solutions, by chang-
ing the way morphemes are generated within the
model. Our results are mixed; we avoid the hyper-
parameter problem, but learn overly compact mor-
pheme lexicons.
</bodyText>
<sectionHeader confidence="0.832161" genericHeader="method">
2 The Pitman-Yor Process
</sectionHeader>
<bodyText confidence="0.99931992">
The Pitman-Yor Process G ∼ PYP(a, b, H0) (Pit-
man and Yor, 1997; Teh, 2006) generates distribu-
tions over the space of the base distribution H0,
with the hyperparameters a and b governing the
extent of the shift from H0. Draws from G have
values from H0, but with probabilities given by
the PYP. For example, in a unigram PYP language
model with observed words, H0 may be a uni-
form distribution over the vocabulary, U(1). The
PYP shifts this distribution to the power-law dis-
tribution over tokens found in natural language,
allowing words to have much higher (and lower)
than uniform probability. We will continue using
the language model example in this section, since
the subsequent morphology model is effectively a
complex unigram language model in which word
types correspond to morphological analyses. In
our presentation, we pay particular attention to the
role of the hyperparameter a, since this value gov-
erns the power-law behaviour of the PYP (Buntine
and Hutter, 2010).
When G is marginalised out, the result is the
PYP Chinese Restaurant Process, which is a use-
ful representation of the distribution of observa-
tions (word tokens) to values from H0 (types). In
</bodyText>
<page confidence="0.987482">
14
</page>
<bodyText confidence="0.981738136363636">
Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 14–18,
Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics
this restaurant, customers (tokens) arrive and are
seated at one of a potentially infinite number of
tables. Each table receives a dish (type) from the
base distribution when the first customer is seated
there; thereafter all subsequent customers adopt
the same dish. The probability of customer zz be-
ing seated at a table k depends on the number of
customers already seated at that table nk. Popu-
lar tables will attract more customers, generating a
Zipfian distribution over customers at tables.
This Zipfian/power-law behaviour can be simi-
lar to that of the natural language data, and is the
principal motivation behind using the PYP. How-
ever, it is only valid for the distribution of cus-
tomers to tables. When the base distribution is dis-
crete — as in our language model example and
the morphology model — the same dish may be
served at multiple tables. In most cases, the dis-
tribution of interest is generally that of customers
(tokens) to dishes (types), rather than to tables,
suggesting a preference for a setting in which each
dish appears at few tables. This is dependent on a
(constrained to be 0 ≤ a &lt; 1), and to a lesser ex-
tent on b: If a is small, each dish will be served at
a single table, resulting in the type-token and the
table-customer power-laws matching. If a is near
1, however, the probability of more than a single
customer being seated at a table is small, and the
distribution of dishes being eaten by the customers
will match the base distribution, rather than being
adapted by the caching mechanism of the PYP.
The expected number of tables K grows as
O(Na) (see Buntine and Hutter (2010) for an ex-
act formulation). The number of word types in
the data gives us a minimum number of tables,
K ≥ T. When a is small (less than 0.5), the num-
ber of expected tables is significantly less than the
number of types in a non-trivial dataset, suggest-
ing a lower bound for values of a.
In our language model, the posterior probability
of assigning a word wz to a table k with dish `k and
nk previous customers is:
</bodyText>
<equation confidence="0.9930755">
p(wz = k|w1 ... wz−1, a, b) ∝ (1)
�
(nk − a)I(wz = `k) if 1 ≤ k ≤ K
(Ka + b)H0(wz) if k = K + 1
</equation>
<bodyText confidence="0.99930875">
where I(wz = `k) returns 1 if the token and the
dish match, and 0 otherwise. We see that in order
to prefer assigning customers to already occupied
tables, we need H0(w)(Ka + b) &lt; nk − a. Given
K ≥ T, and setting H0 = 1T, we can approxi-
mate this with T 1 (T a + b) &lt; nk − a. From this
we obtain a &lt; 12(nk − Tb ), which indicates that in
order for tables with a single customer (nk = 1) to
attract further customers, a must be smaller than
0.5. Thus, there is a tension between the number
of tables required by the data and our desire to
reuse tables. One solution is to fix a to an arbi-
trary, sufficiently small value, as GGJ do in their
experiments. In contrast, in this paper we infer a
and b along with the other parameters, and change
the other free variable, the base distribution H0.
</bodyText>
<sectionHeader confidence="0.996657" genericHeader="method">
3 Morphology
</sectionHeader>
<bodyText confidence="0.99993975">
The morphology model introduced by GGJ has a
base distribution that generates not simply word
types, as in the language model example, but mor-
phological analyses. These are relatively simple,
consisting of stem+suffix segmentation and a clus-
ter membership. The probability of a word is the
sum of the probability of all cluster c, stem s, suf-
fix f tuples:
</bodyText>
<equation confidence="0.997337">
�H0(w) = p(c)p(s|c)p(f|c)I(w = s.f)
(c,s,f)
</equation>
<bodyText confidence="0.9953839375">
(2)
with the stems and the suffixes being gener-
ated from cluster-specific distributions. In the
GGJ model, all three distributions (cluster, stem,
suffix) are finite conjugate symmetric Dirichlet-
Multinomial (DirMult) distributions. We retain the
DirMult over clusters, but change the morpheme-
generating distributions.
The DirMult is equivalent to a Dirichlet Process
prior (DP) with a finite base distribution; we use
this representation because it allows us to replace
the base distributions flexibly. A DP(α, H0) is also
equivalent to a PYP with a = 0, and thus also can
be represented with a Chinese Restaurant Process,
but in this case we sum over all tables to obtain the
predictive probability of a (say) stem:
</bodyText>
<equation confidence="0.997973">
p(s|αs ms + αsHS, Hs) _ (3)
Es&apos; ms&apos; + αs
</equation>
<bodyText confidence="0.999964">
Note that the counts ms refer to stems generated
within the base distribution, not to token counts
within the PYP.
The original GGJ model, ORIG, is equivalent to
setting HS for stems to U( 1S ), and likewise HF =
U(1F), where S and F are the number of possible
stems and suffixes in the dataset (i.e., all possible
prefix and suffix strings, including a null string).
</bodyText>
<page confidence="0.989708">
15
</page>
<bodyText confidence="0.999932696969697">
There are two difficulties with this model.
Firstly, it assumes a closed vocabulary and re-
quires setting S and F in advance, by looking at
the data. As a cognitive model, this is awkward,
since it assumes a fixed, relatively small number
of possible morphemes.
Secondly, when the PYP hyperparameters are
inferred, a is set to be nearly 1, resulting in a model
with as many tokens as tables. This behaviour is
due to the interaction between vocabulary size and
base distribution probabilities outlined in the pre-
vious section: this base distribution assigns rel-
atively high probability to words, so new tables
have high probability; as the number of tables in-
creases (from its fairly large minimum), the op-
timal a for this table configuration also increases,
resulting in convergence at the token-based model.
We investigate two alternate base distribution
over stems and suffixes, both of which extend the
space of possible morphemes, thereby lowering
the overall probability of the observed words.
DP-CHAR generates morphemes by first gener-
ating a length l — Poisson(A). Characters
are then drawn from a uniform distribution,
c0...l — U(1/jCharsj). A morpheme’s prob-
ability decreases exponentially by length, re-
sulting in a strong preference for shorter mor-
phemes.
DP-UNI simply extends the original uniform dis-
tribution to s and f — U(1/1e6), in effect
moving probability mass to a large number
of unseen morphemes. It is thus similar to
DP-CHAR without the length preference.
</bodyText>
<sectionHeader confidence="0.999848" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.9999909">
We follow the same inference procedure as GGJ,
using Gibbs sampling. The sampler iterates be-
tween inferring each token’s table assignment and
resampling the table labels (see GGJ for details).
Within the morphology base distribution, the
prior for the DirMult over clusters is set to αk =
0.5. To replicate the original DirMult model1, we
set αs = 0.001S and αf = 0.001F. In the other
models, αs = αf = 1. Within DP-CHAR, A = 6
for stems, 0.5 for suffixes.
</bodyText>
<footnote confidence="0.929707">
1In this model, the predictive posterior is defined as
</footnote>
<table confidence="0.762405857142857">
p(s|α, S) = Ms+α
M.+Sα, using an alternate definition of α.
Eve (Orth.) Ornat (Orth.)
a Tables/Type a Tables/Type
ORIG 0.96 21.2 0.97 10.64
DP-CHAR 0.46 1.4 0.56 1.17
DP-UNI 0.81 7.3 0.70 2.33
</table>
<tableCaption confidence="0.998308">
Table 1: Final values for a on the orthographic En-
</tableCaption>
<bodyText confidence="0.95106925">
glish and Spanish datasets, as well as the average
number of tables for each word type. The 95%
confidence interval across three runs is &lt; 0.01.
(Phonological Eve is similar to Orthographic Eve.)
</bodyText>
<subsectionHeader confidence="0.999435">
4.1 Sampling Hyperparameters
</subsectionHeader>
<bodyText confidence="0.999900727272727">
We sample PYP a and b hyperparameters using
a slice sampler2. Previous work with this model
has always fixed these values, generally finding
small a to be optimal and b to have little effect.
In experiments with fixed hyperparameters, we set
a = b = 0.1.
To sample the hyperparameters, we place vague
priors over them: a — Beta(1,1) and b —
Gamma(10, 0.1). The slice sampler samples anew
value for a and b after every 10 iterations of Gibbs
sampling.
</bodyText>
<sectionHeader confidence="0.999321" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.830803">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.999973157894737">
Our datasets consist of the adult utterances
from two morphologically annotated corpora from
CHILDES, an English corpus, Eve (Brown, 1973),
and a Spanish corpus, Ornat (Ornat, 1994). Mor-
phology is marked by a grammatical suffix on the
stem, e.g. doggy-PL. Words marked with irregular
morphology are unsegmented.
The two languages, while related, have differ-
ing degrees of affixation: the English Eve corpus
consists of 63 315 tokens (5% suffixed) and 1988
types (28% suffixed); the Ornat corpus has 43 796
tokens (23% suffixed) and 3157 types (50% suf-
fixed). The English corpus has 17 gold suffix
types, while Spanish has 72.
We also use the phonologically encoded Eve
dataset used by GGJ. This dataset does not ex-
actly correspond to the orthographic version, due
to discrepancies in tokenisation, so we are unable
to evaluate this dataset quantitatively.
</bodyText>
<footnote confidence="0.996068">
2Mark Johnson’s implementation, available at
http://web.science.mq.edu.au/˜mjohnson/
Software.htm
</footnote>
<page confidence="0.985805">
16
</page>
<table confidence="0.996684666666666">
Gold Eve (Orth.) % Seg Ornat (Orth.) Eve (Phon.)
% Seg |L |VM |L |VM % Seg |L|
5 23 (5)
ORIG Fix 7 1680 46.42(10.8) 14 2488 46.63(2.7) 17 1619
ORIG Inf 1 1893 9.94(1.0) 4 2769 17.80(3.7) 1 1984
DP-CHAR Fix 52 1331 15.33(0.3) 83 1828 35.76(1.1) 47 1289
DP-CHAR Inf 50 1330 16.15(0.4) 85 1824 36.47(0.5) 33 1317
DP-UNI Fix 38 1394 17.28(1.7) 51 1874 39.58(0.8) 36 1392
DP-UNI Inf 15 1574 31.54(3.1) 31 1983 42.48(1.1) 21 1500
</table>
<tableCaption confidence="0.952421666666667">
Table 2: Final morphology results. ‘Fix’ refers to models with fixed PYP hyperparameters (a = b = 0.1),
while ‘Inf’ models have inferred hyperparameters. % Seg shows the percentage of tokens that have a non-
null suffix, while |L |is the size of the morpheme lexicon. VM is shown with 95% confidence intervals.
</tableCaption>
<subsectionHeader confidence="0.878257">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.9998642">
For each setting, we report the average over three
runs of 1000 iterations of Gibbs sampling without
annealing, using the last iteration for evaluation.
Table 1 shows what happens when hyperpa-
rameters are inferred: ORIG finds a token-based
solution, with as many tables as tokens, while
DP-CHAR is the opposite, with a small a allowing
for just over one table for each word type. DP-UNI
is between these two extremes. b is consistently
between 1 and 3, confirming it has little effect.
The effect of the hyperparameters can be seen
in the morphology results, shown in Table 2.
DP-CHAR is robust across hyperparameter val-
ues, finding the same type-based solution with
fixed and inferred hyperparameters, while the
other models have very different results depending
on the hyperparameter settings. ORIG with fixed
hyperparameters performs best, with the highest
VM score (a clustering measure, Rosenberg and
Hirschberg (2007)) and a level of segmentation
close to the correct one. However, with inferred
hyperparameters, this model severely underseg-
ments: it finds the unsegmented maximum likeli-
hood solution, where all tokens are generated from
the stem distribution (Goldwater, 2007).
The models with alternate base distributions go
to the other extreme, oversegmenting the corpus.
As generating new morphemes becomes less prob-
able, the pressure to find the most compact mor-
pheme lexicon grows. This leads to oversegmen-
tation due to many spurious suffixes. The length
penalty in DP-CHAR exacerbates this problem, but
it can be seen in the DP-UNI solutions as well,
particularly when hyperparameters are fixed to en-
courage a type-based solution.
</bodyText>
<sectionHeader confidence="0.997472" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999974121212121">
The base distribution in the original GGJ model
assigned a relatively high probability to unseen
morphemes, allowing the model to generate new
analyses for seen words instead of reusing old
analyses and leading to undersegented token-
based solutions. The alternative base distributions
proposed here were effective in finding type-based
solutions. However, these over-segmented solu-
tions clearly do not match the true morphology,
indicating that the model structure is inadequate.
One reason may be that the model structure
is overly simple. The model is faced with an ar-
guably more difficult task than a human learner,
who has access to semantic, syntactic, and phono-
logical cues. Adding these types of information
has been shown to help morphology learning in
similar models (Johnson, 2008b; Sirts and Gold-
water, 2013; Frank et al., 2013).
Similarly, the morphological ambiguity that is
captured by a model operating over tokens (and
ignored in better-performing models that allow
only a single analysis for each word type: Poon
et al. (2009); Lee et al. (2011); Sirts and Alum¨ae
(2012)) can often be disambiguated using seman-
tic and syntactic information. A model that gener-
ates a single analysis per meaningful (semantically
and syntactically distinct) word-form could avoid
the potential problems of spurious re-generation
seen in the original GGJ model as well as the
converse problem of under-generation in our al-
ternatives. Such a model might also map onto the
human lexicon (which demonstrably avoids both
problems) in a more realistic way.
</bodyText>
<page confidence="0.998798">
17
</page>
<sectionHeader confidence="0.989154" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.984645215189873">
Roger Brown. A first language: The early
stages. Harvard University Press, Cambridge,
MA, 1973.
Wray Buntine and Marcus Hutter. A Bayesian
view of the Poisson-Dirichlet process. 2010.
URL arXiv:1007.0296.
Joan Bybee. Regular morphology and the lexi-
con. Language and Cognitive Processes, 10:
425–455, 1995.
Stella Frank, Frank Keller, and Sharon Goldwater.
Exploring the utility of joint morphological and
syntactic learning from child-directed speech.
In Proceedings of the 18th Conference on Em-
pirical Methods in Natural Language Process-
ing (EMNLP), 2013.
Sharon Goldwater. Nonparametric Bayesian Mod-
els of Lexical Acquisition. PhD thesis, Brown
University, 2007.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. Interpolating between types and to-
kens by estimating power-law generators. In
Advances in Neural Information Processing
Systems 18, 2006.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. Producing power-law distributions
and damping word frequencies with two-stage
language models. Journal of Machine Learning
Research, 12:2335–2382, 2011.
Mark Johnson. Using Adaptor Grammars to iden-
tify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), 2008a.
Mark Johnson. Unsupervised word segmentation
for Sesotho using Adaptor Grammars. In Pro-
ceedings of the Tenth Meeting of ACL Special
Interest Group on Computational Morphology
and Phonology, pages 20–27, June 2008b.
Yoong Keok Lee, Aria Haghighi, and Regina
Barzilay. Modeling syntactic context improves
morphological segmentation. In Proceedings of
Fifteenth Conference on Computational Natural
Language Learning (CONLL), 2011.
S. Lopez Ornat. La adquisicion de la lengua es-
pagnola. Siglo XXI, Madrid, 1994.
Amy Perfors, Joshua B. Tenenbaum, and Terry
Regier. The learnability of abstract syntactic
principles. Cognition, 118(3):306 – 338, 2011.
Jim Pitman and Marc Yor. The two-parameter
Poisson-Dirichlet distribution derived from a
stable subordinator. Annals of Probability, 25
(2):855–900, 1997.
Hoifung Poon, Colin Cherry, and Kristina
Toutanova. Unsupervised morphological
segmentation with log-linear models. In
Proceedings of the Conference of the North
American Chapter of the Association for
Computational Linguistics (NAACL), 2009.
Andrew Rosenberg and Julia Hirschberg. V-
measure: A conditional entropy-based external
cluster evaluation measure. In Proceedings of
the 12th Conference on Empirical Methods in
Natural Language Processing (EMNLP), 2007.
Kairit Sirts and Tanel Alum¨ae. A hierarchical
Dirichlet process model for joint part-of-speech
and morphology induction. In Proceedings of
the Conference of the North American Chapter
of the Association for Computational Linguis-
tics (NAACL), 2012.
Kairit Sirts and Sharon Goldwater. Minimally-
supervised morphological segmentation using
adaptor grammars. Transactions of the Associa-
tion for Computational Linguistics, 1:231–242,
2013.
Yee Whye Teh. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Pro-
ceedings of the 44th Annual Meeting of the As-
sociation for Computational Linguistics (ACL),
Sydney, 2006.
</reference>
<page confidence="0.999267">
18
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.665150">
<title confidence="0.999811">Learning the hyperparameters to learn morphology</title>
<author confidence="0.994287">Stella</author>
<affiliation confidence="0.9855025">ILCC, School of University of</affiliation>
<address confidence="0.709004">Edinburgh, EH8 9AB,</address>
<email confidence="0.997984">sfrank@inf.ed.ac.uk</email>
<abstract confidence="0.9961915">We perform hyperparameter inference within a model of morphology learning (Goldwater et al., 2011) and find that it affects model behaviour drastically. Changing the model structure successfully avoids the unsegmented solution, but results in oversegmentation instead.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roger Brown</author>
</authors>
<title>A first language: The early stages.</title>
<date>1973</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="11453" citStr="Brown, 1973" startWordPosition="1936" endWordPosition="1937">ample PYP a and b hyperparameters using a slice sampler2. Previous work with this model has always fixed these values, generally finding small a to be optimal and b to have little effect. In experiments with fixed hyperparameters, we set a = b = 0.1. To sample the hyperparameters, we place vague priors over them: a — Beta(1,1) and b — Gamma(10, 0.1). The slice sampler samples anew value for a and b after every 10 iterations of Gibbs sampling. 5 Experiments 5.1 Datasets Our datasets consist of the adult utterances from two morphologically annotated corpora from CHILDES, an English corpus, Eve (Brown, 1973), and a Spanish corpus, Ornat (Ornat, 1994). Morphology is marked by a grammatical suffix on the stem, e.g. doggy-PL. Words marked with irregular morphology are unsegmented. The two languages, while related, have differing degrees of affixation: the English Eve corpus consists of 63 315 tokens (5% suffixed) and 1988 types (28% suffixed); the Ornat corpus has 43 796 tokens (23% suffixed) and 3157 types (50% suffixed). The English corpus has 17 gold suffix types, while Spanish has 72. We also use the phonologically encoded Eve dataset used by GGJ. This dataset does not exactly correspond to the </context>
</contexts>
<marker>Brown, 1973</marker>
<rawString>Roger Brown. A first language: The early stages. Harvard University Press, Cambridge, MA, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wray Buntine</author>
<author>Marcus Hutter</author>
</authors>
<title>A Bayesian view of the Poisson-Dirichlet process.</title>
<date>2010</date>
<note>URL arXiv:1007.0296.</note>
<contexts>
<context position="3512" citStr="Buntine and Hutter, 2010" startWordPosition="538" endWordPosition="541">words, H0 may be a uniform distribution over the vocabulary, U(1). The PYP shifts this distribution to the power-law distribution over tokens found in natural language, allowing words to have much higher (and lower) than uniform probability. We will continue using the language model example in this section, since the subsequent morphology model is effectively a complex unigram language model in which word types correspond to morphological analyses. In our presentation, we pay particular attention to the role of the hyperparameter a, since this value governs the power-law behaviour of the PYP (Buntine and Hutter, 2010). When G is marginalised out, the result is the PYP Chinese Restaurant Process, which is a useful representation of the distribution of observations (word tokens) to values from H0 (types). In 14 Proc. of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 14–18, Gothenburg, Sweden, April 26 2014. c�2014 Association for Computational Linguistics this restaurant, customers (tokens) arrive and are seated at one of a potentially infinite number of tables. Each table receives a dish (type) from the base distribution when the first customer is seated th</context>
<context position="5523" citStr="Buntine and Hutter (2010)" startWordPosition="884" endWordPosition="887">g a preference for a setting in which each dish appears at few tables. This is dependent on a (constrained to be 0 ≤ a &lt; 1), and to a lesser extent on b: If a is small, each dish will be served at a single table, resulting in the type-token and the table-customer power-laws matching. If a is near 1, however, the probability of more than a single customer being seated at a table is small, and the distribution of dishes being eaten by the customers will match the base distribution, rather than being adapted by the caching mechanism of the PYP. The expected number of tables K grows as O(Na) (see Buntine and Hutter (2010) for an exact formulation). The number of word types in the data gives us a minimum number of tables, K ≥ T. When a is small (less than 0.5), the number of expected tables is significantly less than the number of types in a non-trivial dataset, suggesting a lower bound for values of a. In our language model, the posterior probability of assigning a word wz to a table k with dish `k and nk previous customers is: p(wz = k|w1 ... wz−1, a, b) ∝ (1) � (nk − a)I(wz = `k) if 1 ≤ k ≤ K (Ka + b)H0(wz) if k = K + 1 where I(wz = `k) returns 1 if the token and the dish match, and 0 otherwise. We see that </context>
</contexts>
<marker>Buntine, Hutter, 2010</marker>
<rawString>Wray Buntine and Marcus Hutter. A Bayesian view of the Poisson-Dirichlet process. 2010. URL arXiv:1007.0296.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Bybee</author>
</authors>
<title>Regular morphology and the lexicon.</title>
<date>1995</date>
<journal>Language and Cognitive Processes,</journal>
<volume>10</volume>
<pages>425--455</pages>
<contexts>
<context position="1707" citStr="Bybee, 1995" startWordPosition="248" endWordPosition="249">g presented by Goldwater and colleagues in Goldwater et al. (2006) and Goldwater et al. (2011) (henceforth GGJ). This model demonstrated the effectiveness of non-parametric stochastic processes, specifically the Pitman-Yor Process, for interpolating between types and tokens. Language learners are exposed to tokens, but many aspects of linguistic structure are lexical; identifying which tokens belong to the same lexical type is crucial. Surface form is not always sufficient, as in the case of ambiguous words. Moreover, morphology in particular is influenced by vocabulary-level type statistics (Bybee, 1995), so it is important for a model to operate on both levels: token statistics from realistic (child-directed) input, and type-level statistics based on the token analyses. The GGJ model learns successfully given fixed hyperparameter values in the Pitman-Yor Process. However, we show that when these hyperparameters are inferred, it collapses to a token-based model with a trivial morphology. In this paper we discuss the reasons for this problematic behaviour, which are relevant for other models based on Pitman-Yor Processes with discrete base distributions, common in natural language tasks. We in</context>
</contexts>
<marker>Bybee, 1995</marker>
<rawString>Joan Bybee. Regular morphology and the lexicon. Language and Cognitive Processes, 10: 425–455, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stella Frank</author>
<author>Frank Keller</author>
<author>Sharon Goldwater</author>
</authors>
<title>Exploring the utility of joint morphological and syntactic learning from child-directed speech.</title>
<date>2013</date>
<booktitle>In Proceedings of the 18th Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="15514" citStr="Frank et al., 2013" startWordPosition="2585" endWordPosition="2588">egented tokenbased solutions. The alternative base distributions proposed here were effective in finding type-based solutions. However, these over-segmented solutions clearly do not match the true morphology, indicating that the model structure is inadequate. One reason may be that the model structure is overly simple. The model is faced with an arguably more difficult task than a human learner, who has access to semantic, syntactic, and phonological cues. Adding these types of information has been shown to help morphology learning in similar models (Johnson, 2008b; Sirts and Goldwater, 2013; Frank et al., 2013). Similarly, the morphological ambiguity that is captured by a model operating over tokens (and ignored in better-performing models that allow only a single analysis for each word type: Poon et al. (2009); Lee et al. (2011); Sirts and Alum¨ae (2012)) can often be disambiguated using semantic and syntactic information. A model that generates a single analysis per meaningful (semantically and syntactically distinct) word-form could avoid the potential problems of spurious re-generation seen in the original GGJ model as well as the converse problem of under-generation in our alternatives. Such a </context>
</contexts>
<marker>Frank, Keller, Goldwater, 2013</marker>
<rawString>Stella Frank, Frank Keller, and Sharon Goldwater. Exploring the utility of joint morphological and syntactic learning from child-directed speech. In Proceedings of the 18th Conference on Empirical Methods in Natural Language Processing (EMNLP), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
</authors>
<title>Nonparametric Bayesian Models of Lexical Acquisition.</title>
<date>2007</date>
<tech>PhD thesis,</tech>
<institution>Brown University,</institution>
<contexts>
<context position="14200" citStr="Goldwater, 2007" startWordPosition="2381" endWordPosition="2382"> Table 2. DP-CHAR is robust across hyperparameter values, finding the same type-based solution with fixed and inferred hyperparameters, while the other models have very different results depending on the hyperparameter settings. ORIG with fixed hyperparameters performs best, with the highest VM score (a clustering measure, Rosenberg and Hirschberg (2007)) and a level of segmentation close to the correct one. However, with inferred hyperparameters, this model severely undersegments: it finds the unsegmented maximum likelihood solution, where all tokens are generated from the stem distribution (Goldwater, 2007). The models with alternate base distributions go to the other extreme, oversegmenting the corpus. As generating new morphemes becomes less probable, the pressure to find the most compact morpheme lexicon grows. This leads to oversegmentation due to many spurious suffixes. The length penalty in DP-CHAR exacerbates this problem, but it can be seen in the DP-UNI solutions as well, particularly when hyperparameters are fixed to encourage a type-based solution. 6 Conclusion The base distribution in the original GGJ model assigned a relatively high probability to unseen morphemes, allowing the mode</context>
</contexts>
<marker>Goldwater, 2007</marker>
<rawString>Sharon Goldwater. Nonparametric Bayesian Models of Lexical Acquisition. PhD thesis, Brown University, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Interpolating between types and tokens by estimating power-law generators.</title>
<date>2006</date>
<booktitle>In Advances in Neural Information Processing Systems 18,</booktitle>
<contexts>
<context position="1161" citStr="Goldwater et al. (2006)" startWordPosition="165" endWordPosition="168">guage acquisition. Explicitly specifying the causal and computational structure of a model enables the investigation of hypotheses such as the feasibility of learning linguistic structure from the available input (Perfors et al., 2011), or the interaction of different linguistic levels (Johnson, 2008a). However, these models can be sensitive to small changes in (hyper-)parameters settings. Robustness in this respect is important, since positing specific parameter values is cognitively implausible. In this paper we revisit a model of morphology learning presented by Goldwater and colleagues in Goldwater et al. (2006) and Goldwater et al. (2011) (henceforth GGJ). This model demonstrated the effectiveness of non-parametric stochastic processes, specifically the Pitman-Yor Process, for interpolating between types and tokens. Language learners are exposed to tokens, but many aspects of linguistic structure are lexical; identifying which tokens belong to the same lexical type is crucial. Surface form is not always sufficient, as in the case of ambiguous words. Moreover, morphology in particular is influenced by vocabulary-level type statistics (Bybee, 1995), so it is important for a model to operate on both le</context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2006</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. Interpolating between types and tokens by estimating power-law generators. In Advances in Neural Information Processing Systems 18, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>Producing power-law distributions and damping word frequencies with two-stage language models.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<contexts>
<context position="1189" citStr="Goldwater et al. (2011)" startWordPosition="170" endWordPosition="174">y specifying the causal and computational structure of a model enables the investigation of hypotheses such as the feasibility of learning linguistic structure from the available input (Perfors et al., 2011), or the interaction of different linguistic levels (Johnson, 2008a). However, these models can be sensitive to small changes in (hyper-)parameters settings. Robustness in this respect is important, since positing specific parameter values is cognitively implausible. In this paper we revisit a model of morphology learning presented by Goldwater and colleagues in Goldwater et al. (2006) and Goldwater et al. (2011) (henceforth GGJ). This model demonstrated the effectiveness of non-parametric stochastic processes, specifically the Pitman-Yor Process, for interpolating between types and tokens. Language learners are exposed to tokens, but many aspects of linguistic structure are lexical; identifying which tokens belong to the same lexical type is crucial. Surface form is not always sufficient, as in the case of ambiguous words. Moreover, morphology in particular is influenced by vocabulary-level type statistics (Bybee, 1995), so it is important for a model to operate on both levels: token statistics from </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2011</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. Producing power-law distributions and damping word frequencies with two-stage language models. Journal of Machine Learning Research, 12:2335–2382, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using Adaptor Grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<contexts>
<context position="839" citStr="Johnson, 2008" startWordPosition="117" endWordPosition="118">logy learning (Goldwater et al., 2011) and find that it affects model behaviour drastically. Changing the model structure successfully avoids the unsegmented solution, but results in oversegmentation instead. 1 Introduction Bayesian models provide a sound statistical framework in which to explore aspects of language acquisition. Explicitly specifying the causal and computational structure of a model enables the investigation of hypotheses such as the feasibility of learning linguistic structure from the available input (Perfors et al., 2011), or the interaction of different linguistic levels (Johnson, 2008a). However, these models can be sensitive to small changes in (hyper-)parameters settings. Robustness in this respect is important, since positing specific parameter values is cognitively implausible. In this paper we revisit a model of morphology learning presented by Goldwater and colleagues in Goldwater et al. (2006) and Goldwater et al. (2011) (henceforth GGJ). This model demonstrated the effectiveness of non-parametric stochastic processes, specifically the Pitman-Yor Process, for interpolating between types and tokens. Language learners are exposed to tokens, but many aspects of linguis</context>
<context position="15465" citStr="Johnson, 2008" startWordPosition="2578" endWordPosition="2579"> reusing old analyses and leading to undersegented tokenbased solutions. The alternative base distributions proposed here were effective in finding type-based solutions. However, these over-segmented solutions clearly do not match the true morphology, indicating that the model structure is inadequate. One reason may be that the model structure is overly simple. The model is faced with an arguably more difficult task than a human learner, who has access to semantic, syntactic, and phonological cues. Adding these types of information has been shown to help morphology learning in similar models (Johnson, 2008b; Sirts and Goldwater, 2013; Frank et al., 2013). Similarly, the morphological ambiguity that is captured by a model operating over tokens (and ignored in better-performing models that allow only a single analysis for each word type: Poon et al. (2009); Lee et al. (2011); Sirts and Alum¨ae (2012)) can often be disambiguated using semantic and syntactic information. A model that generates a single analysis per meaningful (semantically and syntactically distinct) word-form could avoid the potential problems of spurious re-generation seen in the original GGJ model as well as the converse problem</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. Using Adaptor Grammars to identify synergies in the unsupervised acquisition of linguistic structure. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), 2008a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Unsupervised word segmentation for Sesotho using Adaptor Grammars.</title>
<date>2008</date>
<booktitle>In Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology,</booktitle>
<pages>20--27</pages>
<contexts>
<context position="839" citStr="Johnson, 2008" startWordPosition="117" endWordPosition="118">logy learning (Goldwater et al., 2011) and find that it affects model behaviour drastically. Changing the model structure successfully avoids the unsegmented solution, but results in oversegmentation instead. 1 Introduction Bayesian models provide a sound statistical framework in which to explore aspects of language acquisition. Explicitly specifying the causal and computational structure of a model enables the investigation of hypotheses such as the feasibility of learning linguistic structure from the available input (Perfors et al., 2011), or the interaction of different linguistic levels (Johnson, 2008a). However, these models can be sensitive to small changes in (hyper-)parameters settings. Robustness in this respect is important, since positing specific parameter values is cognitively implausible. In this paper we revisit a model of morphology learning presented by Goldwater and colleagues in Goldwater et al. (2006) and Goldwater et al. (2011) (henceforth GGJ). This model demonstrated the effectiveness of non-parametric stochastic processes, specifically the Pitman-Yor Process, for interpolating between types and tokens. Language learners are exposed to tokens, but many aspects of linguis</context>
<context position="15465" citStr="Johnson, 2008" startWordPosition="2578" endWordPosition="2579"> reusing old analyses and leading to undersegented tokenbased solutions. The alternative base distributions proposed here were effective in finding type-based solutions. However, these over-segmented solutions clearly do not match the true morphology, indicating that the model structure is inadequate. One reason may be that the model structure is overly simple. The model is faced with an arguably more difficult task than a human learner, who has access to semantic, syntactic, and phonological cues. Adding these types of information has been shown to help morphology learning in similar models (Johnson, 2008b; Sirts and Goldwater, 2013; Frank et al., 2013). Similarly, the morphological ambiguity that is captured by a model operating over tokens (and ignored in better-performing models that allow only a single analysis for each word type: Poon et al. (2009); Lee et al. (2011); Sirts and Alum¨ae (2012)) can often be disambiguated using semantic and syntactic information. A model that generates a single analysis per meaningful (semantically and syntactically distinct) word-form could avoid the potential problems of spurious re-generation seen in the original GGJ model as well as the converse problem</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. Unsupervised word segmentation for Sesotho using Adaptor Grammars. In Proceedings of the Tenth Meeting of ACL Special Interest Group on Computational Morphology and Phonology, pages 20–27, June 2008b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Aria Haghighi</author>
<author>Regina Barzilay</author>
</authors>
<title>Modeling syntactic context improves morphological segmentation.</title>
<date>2011</date>
<booktitle>In Proceedings of Fifteenth Conference on Computational Natural Language Learning (CONLL),</booktitle>
<marker>Lee, Haghighi, Barzilay, 2011</marker>
<rawString>Yoong Keok Lee, Aria Haghighi, and Regina Barzilay. Modeling syntactic context improves morphological segmentation. In Proceedings of Fifteenth Conference on Computational Natural Language Learning (CONLL), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lopez Ornat</author>
</authors>
<title>La adquisicion de la lengua espagnola. Siglo XXI,</title>
<date>1994</date>
<location>Madrid,</location>
<contexts>
<context position="11496" citStr="Ornat, 1994" startWordPosition="1943" endWordPosition="1944">lice sampler2. Previous work with this model has always fixed these values, generally finding small a to be optimal and b to have little effect. In experiments with fixed hyperparameters, we set a = b = 0.1. To sample the hyperparameters, we place vague priors over them: a — Beta(1,1) and b — Gamma(10, 0.1). The slice sampler samples anew value for a and b after every 10 iterations of Gibbs sampling. 5 Experiments 5.1 Datasets Our datasets consist of the adult utterances from two morphologically annotated corpora from CHILDES, an English corpus, Eve (Brown, 1973), and a Spanish corpus, Ornat (Ornat, 1994). Morphology is marked by a grammatical suffix on the stem, e.g. doggy-PL. Words marked with irregular morphology are unsegmented. The two languages, while related, have differing degrees of affixation: the English Eve corpus consists of 63 315 tokens (5% suffixed) and 1988 types (28% suffixed); the Ornat corpus has 43 796 tokens (23% suffixed) and 3157 types (50% suffixed). The English corpus has 17 gold suffix types, while Spanish has 72. We also use the phonologically encoded Eve dataset used by GGJ. This dataset does not exactly correspond to the orthographic version, due to discrepancies </context>
</contexts>
<marker>Ornat, 1994</marker>
<rawString>S. Lopez Ornat. La adquisicion de la lengua espagnola. Siglo XXI, Madrid, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amy Perfors</author>
<author>Joshua B Tenenbaum</author>
<author>Terry Regier</author>
</authors>
<title>The learnability of abstract syntactic principles.</title>
<date>2011</date>
<journal>Cognition,</journal>
<volume>118</volume>
<issue>3</issue>
<contexts>
<context position="773" citStr="Perfors et al., 2011" startWordPosition="105" endWordPosition="108">c.uk Abstract We perform hyperparameter inference within a model of morphology learning (Goldwater et al., 2011) and find that it affects model behaviour drastically. Changing the model structure successfully avoids the unsegmented solution, but results in oversegmentation instead. 1 Introduction Bayesian models provide a sound statistical framework in which to explore aspects of language acquisition. Explicitly specifying the causal and computational structure of a model enables the investigation of hypotheses such as the feasibility of learning linguistic structure from the available input (Perfors et al., 2011), or the interaction of different linguistic levels (Johnson, 2008a). However, these models can be sensitive to small changes in (hyper-)parameters settings. Robustness in this respect is important, since positing specific parameter values is cognitively implausible. In this paper we revisit a model of morphology learning presented by Goldwater and colleagues in Goldwater et al. (2006) and Goldwater et al. (2011) (henceforth GGJ). This model demonstrated the effectiveness of non-parametric stochastic processes, specifically the Pitman-Yor Process, for interpolating between types and tokens. La</context>
</contexts>
<marker>Perfors, Tenenbaum, Regier, 2011</marker>
<rawString>Amy Perfors, Joshua B. Tenenbaum, and Terry Regier. The learnability of abstract syntactic principles. Cognition, 118(3):306 – 338, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Pitman</author>
<author>Marc Yor</author>
</authors>
<title>The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator.</title>
<date>1997</date>
<journal>Annals of Probability,</journal>
<volume>25</volume>
<pages>2--855</pages>
<contexts>
<context position="2596" citStr="Pitman and Yor, 1997" startWordPosition="387" endWordPosition="391">ever, we show that when these hyperparameters are inferred, it collapses to a token-based model with a trivial morphology. In this paper we discuss the reasons for this problematic behaviour, which are relevant for other models based on Pitman-Yor Processes with discrete base distributions, common in natural language tasks. We investigate some potential solutions, by changing the way morphemes are generated within the model. Our results are mixed; we avoid the hyperparameter problem, but learn overly compact morpheme lexicons. 2 The Pitman-Yor Process The Pitman-Yor Process G ∼ PYP(a, b, H0) (Pitman and Yor, 1997; Teh, 2006) generates distributions over the space of the base distribution H0, with the hyperparameters a and b governing the extent of the shift from H0. Draws from G have values from H0, but with probabilities given by the PYP. For example, in a unigram PYP language model with observed words, H0 may be a uniform distribution over the vocabulary, U(1). The PYP shifts this distribution to the power-law distribution over tokens found in natural language, allowing words to have much higher (and lower) than uniform probability. We will continue using the language model example in this section, </context>
</contexts>
<marker>Pitman, Yor, 1997</marker>
<rawString>Jim Pitman and Marc Yor. The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator. Annals of Probability, 25 (2):855–900, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoifung Poon</author>
<author>Colin Cherry</author>
<author>Kristina Toutanova</author>
</authors>
<title>Unsupervised morphological segmentation with log-linear models.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<marker>Poon, Cherry, Toutanova, 2009</marker>
<rawString>Hoifung Poon, Colin Cherry, and Kristina Toutanova. Unsupervised morphological segmentation with log-linear models. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Julia Hirschberg</author>
</authors>
<title>Vmeasure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 12th Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<contexts>
<context position="13940" citStr="Rosenberg and Hirschberg (2007)" startWordPosition="2341" endWordPosition="2344">AR is the opposite, with a small a allowing for just over one table for each word type. DP-UNI is between these two extremes. b is consistently between 1 and 3, confirming it has little effect. The effect of the hyperparameters can be seen in the morphology results, shown in Table 2. DP-CHAR is robust across hyperparameter values, finding the same type-based solution with fixed and inferred hyperparameters, while the other models have very different results depending on the hyperparameter settings. ORIG with fixed hyperparameters performs best, with the highest VM score (a clustering measure, Rosenberg and Hirschberg (2007)) and a level of segmentation close to the correct one. However, with inferred hyperparameters, this model severely undersegments: it finds the unsegmented maximum likelihood solution, where all tokens are generated from the stem distribution (Goldwater, 2007). The models with alternate base distributions go to the other extreme, oversegmenting the corpus. As generating new morphemes becomes less probable, the pressure to find the most compact morpheme lexicon grows. This leads to oversegmentation due to many spurious suffixes. The length penalty in DP-CHAR exacerbates this problem, but it can</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>Andrew Rosenberg and Julia Hirschberg. Vmeasure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 12th Conference on Empirical Methods in Natural Language Processing (EMNLP), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kairit Sirts</author>
<author>Tanel Alum¨ae</author>
</authors>
<title>A hierarchical Dirichlet process model for joint part-of-speech and morphology induction.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<marker>Sirts, Alum¨ae, 2012</marker>
<rawString>Kairit Sirts and Tanel Alum¨ae. A hierarchical Dirichlet process model for joint part-of-speech and morphology induction. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kairit Sirts</author>
<author>Sharon Goldwater</author>
</authors>
<title>Minimallysupervised morphological segmentation using adaptor grammars.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<contexts>
<context position="15493" citStr="Sirts and Goldwater, 2013" startWordPosition="2580" endWordPosition="2584">lyses and leading to undersegented tokenbased solutions. The alternative base distributions proposed here were effective in finding type-based solutions. However, these over-segmented solutions clearly do not match the true morphology, indicating that the model structure is inadequate. One reason may be that the model structure is overly simple. The model is faced with an arguably more difficult task than a human learner, who has access to semantic, syntactic, and phonological cues. Adding these types of information has been shown to help morphology learning in similar models (Johnson, 2008b; Sirts and Goldwater, 2013; Frank et al., 2013). Similarly, the morphological ambiguity that is captured by a model operating over tokens (and ignored in better-performing models that allow only a single analysis for each word type: Poon et al. (2009); Lee et al. (2011); Sirts and Alum¨ae (2012)) can often be disambiguated using semantic and syntactic information. A model that generates a single analysis per meaningful (semantically and syntactically distinct) word-form could avoid the potential problems of spurious re-generation seen in the original GGJ model as well as the converse problem of under-generation in our </context>
</contexts>
<marker>Sirts, Goldwater, 2013</marker>
<rawString>Kairit Sirts and Sharon Goldwater. Minimallysupervised morphological segmentation using adaptor grammars. Transactions of the Association for Computational Linguistics, 1:231–242, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
</authors>
<title>A hierarchical Bayesian language model based on Pitman-Yor processes.</title>
<date>2006</date>
<booktitle>In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Sydney,</location>
<contexts>
<context position="2608" citStr="Teh, 2006" startWordPosition="392" endWordPosition="393">n these hyperparameters are inferred, it collapses to a token-based model with a trivial morphology. In this paper we discuss the reasons for this problematic behaviour, which are relevant for other models based on Pitman-Yor Processes with discrete base distributions, common in natural language tasks. We investigate some potential solutions, by changing the way morphemes are generated within the model. Our results are mixed; we avoid the hyperparameter problem, but learn overly compact morpheme lexicons. 2 The Pitman-Yor Process The Pitman-Yor Process G ∼ PYP(a, b, H0) (Pitman and Yor, 1997; Teh, 2006) generates distributions over the space of the base distribution H0, with the hyperparameters a and b governing the extent of the shift from H0. Draws from G have values from H0, but with probabilities given by the PYP. For example, in a unigram PYP language model with observed words, H0 may be a uniform distribution over the vocabulary, U(1). The PYP shifts this distribution to the power-law distribution over tokens found in natural language, allowing words to have much higher (and lower) than uniform probability. We will continue using the language model example in this section, since the su</context>
</contexts>
<marker>Teh, 2006</marker>
<rawString>Yee Whye Teh. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings of the 44th Annual Meeting of the Association for Computational Linguistics (ACL), Sydney, 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
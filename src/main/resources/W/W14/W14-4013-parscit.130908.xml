<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005120">
<title confidence="0.993248666666667">
Transduction Recursive Auto-Associative Memory:
Learning Bilingual Compositional Distributed Vector Representations of
Inversion Transduction Grammars
</title>
<author confidence="0.835005">
Karteek ADDANKi Dekai Wu
</author>
<affiliation confidence="0.94459825">
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
</affiliation>
<email confidence="0.99626">
{vskaddanki|dekai}@cs.ust.hk
</email>
<sectionHeader confidence="0.993854" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999975375">
We introduce TRAAM, or Transduction
RAAM, a fully bilingual generalization
of Pollack’s (1990) monolingual Recur-
sive Auto-Associative Memory neural net-
work model, in which each distributed vec-
tor represents a bilingual constituent—i.e.,
an instance of a transduction rule, which
specifies a relation between two monolin-
gual constituents and how their subcon-
stituents should be permuted. Bilingual
terminals are special cases of bilingual
constituents, where a vector represents ei-
ther (1) a bilingual token —a token-to-
token or “word-to-word” translation rule
—or (2) a bilingual segment—a segment-
to-segment or “phrase-to-phrase” transla-
tion rule. TRAAMs have properties that
appear attractive for bilingual grammar in-
duction and statistical machine translation
applications. Training of TRAAM drives
both the autoencoder weights and the vec-
tor representations to evolve, such that
similar bilingual constituents tend to have
more similar vectors.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952314814815">
We introduce Transduction RAAM—or TRAAM
for short—a recurrent neural network model that
generalizes the monolingual RAAM model of Pol-
lack (1990) to a distributed vector representation
of compositionally structured transduction gram-
mars (Aho and Ullman, 1972) that is fully bilingual
from top to bottom. In RAAM, which stands for
Recursive Auto-Associative Memory, using fea-
ture vectors to characterize constituents at every
level of a parse tree has the advantages that (1)
the entire context of all subtrees inside the con-
stituent can be efficiently captured in the feature
vectors, (2) the learned representations generalize
well because similar feature vectors represent sim-
ilar constituents or segments, and (3) representa-
tions can be automatically learned so as to max-
imize prediction accuracy for various tasks using
semi-supervised learning. We argue that different,
but analogous, properties are desirable for bilin-
gual structured translation models.
Unlike RAAM, where each distributed vector
represents a monolingual token or constituent,
each distributed vector in TRAAM represents a
bilingual constituent or biconstituent—that is, an
instance of a transduction rule, which asserts a re-
lation between two monolingual constituents, as
well as specifying how to permute their subcon-
stituents in translation. Bilingual terminals, or
biterminals, are special cases of biconstituents
where a vector represents either (1) a bitoken—a
token-to-token or “word-to-word” translation rule
—or (2) a bisegment—a segment-to-segment or
“phrase-to-phrase” translation rule.
The properties of TRAAMs are attractive for
machine translation applications. As with RAAM,
TRAAMs can be trained via backpropagation
training, which simultaneously evolves both the
autoencoder weights and the biconstituent vector
representations. As with RAAM, the evolution
of the vector representations within the hidden
layer performs automatic feature induction, and for
many applications can obviate the need for man-
ual feature engineering. However, the result is
that similar vectors tend to represent similar bicon-
stituents, rather than monolingual constituents.
The learned vector representations thus tend to
form clusters of similar translation relations in-
stead of merely similar strings. That is, TRAAM
clusters represent soft nonterminal categories of
cross-lingual relations and translation patterns, as
opposed to soft nonterminal categories of mono-
lingual strings as in RAAM.
Also, TRAAMs inherently make full simulta-
neous use of both input and output language fea-
</bodyText>
<page confidence="0.97229">
112
</page>
<note confidence="0.7826">
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 112–121,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999889803921568">
tures, recursively, in an elegant integrated fash-
ion. TRAAM does not make restrictive a pri-
ori assumptions of conditional independence be-
tween input and output language features. When
evolving the biconstituent vector representations,
generalization occurs over similar input and out-
put structural characteristics simultaneously. In
most recurrent neural network applications to ma-
chine translation to date, only input side features
or only output language features are used. Even in
the few previous cases where recurrent neural net-
works have employed both input and output lan-
guage features for machine translation, the models
have typically been factored so that their recursive
portion is applied only to either the input or output
language, but not both.
As with RAAM, the objective criteria for train-
ing can be adjusted to reflect accuracy on nu-
merous different kinds of tasks, biasing the di-
rection that vector representations evolve toward.
But again, TRAAM’s learned vector representa-
tions support making predictions that simultane-
ously make use of both input and output struc-
tural characteristics. For example, TRAAM has
the ability to take into account the structure of
both input and output subtree characteristics while
making predictions on reordering them. Similarly,
for specific cross-lingual tasks such as word align-
ment, sense disambiguation, or machine transla-
tion, classifiers can simultaneously be trained in
conjunction with evolving the vector representa-
tions to optimize task-specific accuracy (Chris-
man, 1991).
In this paper we use as examples binary bi-
parse trees consistent with transduction grammars
in a 2-normal form, which by definition are in-
version transduction grammars (Wu, 1997) since
they are binary rank. This is not a requirement
for TRAAM, which in general can be formed for
transduction grammars of any rank. Moreover,
with distributed vector representations, the notion
of nonterminal categories in TRAAM is that of soft
membership, unlike in symbolically represented
transduction grammars. We start with bracketed
training data that contains no bilingual category
labels (like training data for Bracketing ITGs or
BITGs). Training results in self-organizing clus-
ters that have been automatically induced, repre-
senting soft nonterminal categories (unlike BITGs,
which do not have differentiated nonterminal cat-
egories).
</bodyText>
<sectionHeader confidence="0.998696" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.99988675">
TRAAM builds on different aspects of a spec-
trum of previous work. A large body of work ex-
ists on various different types of self-organizing
recurrent neural network approaches to model-
ing recursive structure, but mostly in monolin-
gual modeling. Even in applications to ma-
chine translation or cross-lingual modeling, the
typical practice has been to insert neural net-
work scoring components while still maintain-
ing older SMT modeling assumptions like bags-
of-words/phrases, “shake’n’bake” translation that
relies heavily on strong monolingual language
models, and log-linear models —in contrast to
TRAAM’s fully integrated bilingual approach.
Here we survey representative work across the
spectrum.
</bodyText>
<subsectionHeader confidence="0.992903">
2.1 Monolingual related work
</subsectionHeader>
<bodyText confidence="0.99986290625">
Distributed vector representations have long
been used for n-gram language modeling; these
continuous-valued models exploit the general-
ization capabilities of neural networks, although
there is no hidden contextual or hierarchical
structure as in RAAM. Schwenk (2010) applies
one such language model within an SMT system.
In the simple recurrent neural networks (RNNs
or SRNs) of Elman (1990), hidden layer represen-
tations are fed back to the input to dynamically rep-
resent an aggregate of the immediate contextual
history. More recently, the probabilistic NNLMs
of Bengio et al. (2003) and Bengio et al. (2009)
follow in this vein.
To represent hierarchical tree structure using
vector representations, one simple family of ap-
proaches employs convolutional networks, as in
Lee et al. (2009) for example. Collobert and We-
ston (2008) use a convolution neural network layer
quite effectively to learn vector representations for
words which are then used in a host of NLP tasks
such as POS tagging, chunking, and semantic role
labeling.
RAAM approaches, and related recursive au-
toencoder approaches, can be more flexible than
convolutional networks. Like SRNs, they can be
extended in numerous ways. The URAAM (Uni-
fication RAAM) model of Stolcke and Wu (1992)
extended RAAM to demonstrate the possibility of
using neural networks to perform more sophisti-
cated operations like unification directly upon the
distributed vector representations of hierarchical
</bodyText>
<page confidence="0.995653">
113
</page>
<bodyText confidence="0.999984909090909">
feature structures. Socher et al. (2011) used mono-
lingual recursive autoencoders for sentiment pre-
diction, with or without parse tree information; this
was perhaps the first use of a RAAM style ap-
proach on a large scale NLP task, albeit mono-
lingual. Scheible and Schütze (2013) automat-
ically simplified the monolingual tree structures
generated by recursive autoencoders, validated the
simplified structures via manual evaluation, and
showed that sentiment classification accuracy is
not affected.
</bodyText>
<subsectionHeader confidence="0.999564">
2.2 Bilingual related work
</subsectionHeader>
<bodyText confidence="0.999984888888889">
The majority of work on learning bilingual dis-
tributed vector representations has not made use of
recursive approaches or hidden contextual or com-
positional structure, as in the bilingual word em-
bedding learning of Klementiev et al. (2012) or the
bilingual phrase embedding learning of Gao et al.
(2014). Schwenk (2012) uses a non-recursive neu-
ral network to predict phrase translation probabil-
ities in conventional phrase-based SMT.
Attempts have been made to generalize the dis-
tributed vector representations of monolingual n-
gram language models, avoiding any hidden con-
textual or hierarchical structure. Working within
the framework of n-gram translation models, Son
et al. (2012) generalize left-to-right monolingual
n-gram models to bilingual n-grams, and study
bilingual variants of class-based n-grams. How-
ever, their model does not allow tackling the chal-
lenge of modeling cross-lingual constituent order,
as TRAAM does; instead it relies on the assump-
tion that some other preprocessor has already man-
aged to accurately re-order the words of the input
sentence into exactly the order of words in the out-
put sentence.
Similarly, generalizations of monolingual SRNs
to the bilingual case have been studied. Zou
et al. (2013) generalize the monolingual recur-
rent NNLM model of Bengio et al. (2009) to
learn bilingual word embeddings using conven-
tional SMT word alignments, and demonstrate that
the resulting embeddings outperform the baselines
in word semantic similarity. They also add a sin-
gle semantic similarity feature induced with bilin-
gual embeddings to a phrase-based SMT log-linear
model, and report improvements in BLEU. Com-
pared to TRAAM, however, they only learn non-
compositional features, with distributed vectors
only representing biterminals (as opposed to bi-
constituents or bilingual subtrees), and so other
mechanisms for combining biterminal scores still
need to be used to handle hierarchical structure,
as opposed to seamlessly being integrated into
the distributed vector representation model. De-
vlin et al. (2014) obtain translation accuracy im-
provements by extending the probabilistic NNLMs
of Bengio et al. (2003), which are used for the
output language, by adding input language con-
text features. Unlike TRAAM, neither of these
approaches symmetrically models the recursive
structure of both the input and output language
sides.
For convolutional network approaches, Kalch-
brenner and Blunsom (2013) use a recurrent prob-
abilistic model to generate a representation of the
source sentence and then generate the target sen-
tence from this representation. This use of in-
put language context to bias translation choices
is in some sense a neural network analogy to
the PSD (phrase sense disambiguation) approach
for context-dependent translation probabilities of
Carpuat and Wu (2007). Unlike TRAAM, the
model does not contain structural constraints, and
permutation of phrases must still be done in con-
ventional PBSMT “shake’n’bake” style by rely-
ing mostly on a language model (in their case, a
NNLM).
A few applications of monolingual RAAM-style
recursive autoencoders to bilingual tasks have also
appeared. For cross-lingual document classifica-
tion, Hermann and Blunsom (2014) use two sep-
arate monolingual fixed vector composition net-
works, one for each language. One provides the
training signal for the other, and training is only
on the embeddings.
Li et al. (2013) described a use of monolingual
recursive autoencoders within maximum entropy
ITGs. They replace their earlier model for pre-
dicting reordering based on the first and the last
tokens in a constituent, by instead using the con-
text vector generated using the recursive autoen-
coder. Only input language context is used, unlike
TRAAM which can use the input and output lan-
guage contexts equally.
Autoencoders have also been applied to SMT in
a very different way by Zhao et al. (2014) but with-
out recursion and not for learning distributed vec-
tor representations of words; rather, they used non-
recursive autoencoders to compress very high-
dimensional bilingual sparse features down to low-
dimensional feature vectors, so that MIRA or PRO
</bodyText>
<page confidence="0.994587">
114
</page>
<bodyText confidence="0.991422">
could be used to optimize the log-linear model
weights.
</bodyText>
<sectionHeader confidence="0.968762" genericHeader="method">
3 Representing transduction grammars
with TRAAM
</sectionHeader>
<bodyText confidence="0.999927">
As a recurrent neural network representation of a
transduction grammar, TRAAM learns bilingual
distributed representations that parallel the struc-
tural composition of a transduction grammar. As
with transduction grammars, the learned represen-
tations are symmetric and model structured rela-
tional correlations between the input and output
languages. The induced feature vectors in effect
represent soft categories of cross-lingual relations
and translations. The TRAAM model integrates
elegantly with the transduction grammar formal-
ism and aims to model the compositional struc-
ture of the transduction grammar as opposed to
incorporating external alignment information. It
is straightforward to formulate TRAAMs for arbi-
trary syntax directed transduction grammars; here
we shall describe an example of a TRAAM model
for an inversion transduction grammar (ITG).
Formally, an ITG is a tuple (N, E, ∆, R, S),
where N is a finite nonempty set of nonterminal
symbols, E is a finite set of terminal symbols in L0,
∆ is a finite set of terminal symbols in L1, R is a
finite nonempty set of inversion transduction rules
and S E N is a designated start symbol. A normal-
form ITG consists of rules in one of the following
four forms:
</bodyText>
<equation confidence="0.582631">
S —* A, A —* [BC] , A —* (BC), A —* e/f
</equation>
<bodyText confidence="0.999954681159421">
where S E N is the start symbol, A, B, C E
N are nonterminal symbols and e/f is a biter-
minal. A biterminal is a pair of symbol strings:
E* x ∆% where at least one of the strings have to
be nonempty. The square and angled brackets sig-
nal straight and inverted order respectively. With
straight order, both the L0 and the L1 productions
are generated left-to-right, but with inverted order,
the L1 production is generated right-to-left.
In the distributed TRAAM representation of the
ITG, we represent each bispan, using a feature vec-
tor v of dimension d that represents a fuzzy encod-
ing of all the nonterminals that could generate it.
This is in contrast to the ITG model where each
nonterminal that generates a bispan has to be enu-
merated separately. Feature vectors correspond-
ing to larger bispans are compositionally generated
from smaller bispans using a compressor network
which takes two feature vectors of dimension d,
corresponding to the smaller bispans and gener-
ates the feature vector of dimension d correspond-
ing to the larger bispan. A single bit correspond-
ing to straight or inverted order is also fed as an
input to the compressor network. The compres-
sor network in TRAAM serves a similar role as
the syntactic rules in the symbolic ITG, but keeps
the encoding fuzzy. Figure 2 shows the straight
and inverted syntactic rules and the correspond-
ing inputs to the compressor network. Modeling
of unary rules (with start symbol on the left hand
side) although similar, is beyond the scope of this
paper.
It is easy to demonstrate that TRAAM mod-
els are capable of representing any symbolic ITG
model. All the nonterminals representing a bispan
can be encoded as a bit vector in the feature vector
of the bispan. Using the universal approximation
theorem of neural networks (Hornik et al., 1989),
an encoder with a single hidden layer can represent
any set of syntactic rules. Similarly, all TRAAM
models can be represented using a symbolic ITG
by assuming a unique nonterminal label for every
feature vector. Therefore, TRAAM and ITGs rep-
resent two equivalent classes of models for repre-
senting compositional bilingual relations.
It is important to note that although both
TRAAM and ITG models might be equivalent, the
fuzzy encoding of nonterminals in TRAAM is suit-
able for modeling the generalizations in bilingual
relations without exploding the search space unlike
the symbolic models. This property of TRAAM
makes it attractive for bilingual category learning
and machine translation applications as long as ap-
propriate language bias and objective functions are
determined.
Given our objective of inducing categories of
bilingual relations in an unsupervised manner, we
bias our TRAAM model by using a simple non-
linear activation function to be our compressor,
similar to the monolingual recursive autoencoder
model proposed by Socher et al. (2011). Having a
single layer in our compressor provides the neces-
sary language bias by forcing the network to cap-
ture the generalizations while reducing the dimen-
sions of the input vectors. We use tanh as the non-
linear activation function and the compressor ac-
cepts two vectors c1 and c2 of dimension d corre-
sponding to the nonterminals of the smaller bis-
pans and a single bit o corresponding to the in-
</bodyText>
<page confidence="0.998854">
115
</page>
<figureCaption confidence="0.9999515">
Figure 1: Example of English-Telugu biparse trees where inversion depends on output language sense.
Figure 2: Architecture of TRAAM.
</figureCaption>
<figure confidence="0.7301845">
O2&apos; P1&apos; C3
&apos;
O1&apos; C1&apos; C2
&apos;
Reconstructor
P2
Reconstructor
P1
Compressor
Compressor
O2 = (1
O1 = 1 C1 C2 C3
</figure>
<figureCaption confidence="0.806929833333333">
version order and generates a vector p of dimen-
sion d corresponding to the larger bispan generated
by combining the two smaller bispans as shown in
Figure 2. The vector p then serves as the input for
the successive combinations of the larger bispan
with other bispans.
</figureCaption>
<equation confidence="0.988963">
p = tanh(W1[o; c1; c2] + b1) (1)
</equation>
<bodyText confidence="0.999979705882353">
where W1 and b1 are the weight matrix and the bias
vector of the encoder network.
To ensure that the computed vector p captures
the fuzzy encodings of its children and the inver-
sion order, we use a reconstructor network which
attempts to reconstruct the inversion order and the
feature vectors corresponding of its children. We
use the error in reconstruction as our objective
function and train our model to minimize the re-
construction error over all the nodes in the biparse
tree. The reconstructor network in our TRAAM
model can be replaced by any other network that
enables the computed feature vector representa-
tions to be optimized for the given task. In our
current implementation, we reconstruct the inver-
sion order o′ and the child vectors c′1 and c′2 using
another nonlinear activation function as follows:
</bodyText>
<equation confidence="0.995582">
[o′; c′1; c′2] = tanh(W2p + b2) (2)
</equation>
<bodyText confidence="0.999987">
where W2 and b2 are the weight matrix and the bias
vector of the reconstructor network.
</bodyText>
<sectionHeader confidence="0.97902" genericHeader="method">
4 Bilingual training
</sectionHeader>
<subsectionHeader confidence="0.871794">
4.1 Initialization
</subsectionHeader>
<bodyText confidence="0.999818">
The weights and the biases of the compressor and
the reconstructor networks of the TRAAM model
are randomly initialized. Bisegment embeddings
</bodyText>
<page confidence="0.997954">
116
</page>
<bodyText confidence="0.999925153846154">
corresponding to the leaf nodes (biterminals in the
symbolic ITG notation) in the biparse trees are also
initialized randomly. These constitute the model
parameters and are optimized to minimize our ob-
jective function of reconstruction error. The parse
trees for providing the structural constraints are
generated by a bracketing inversion transduction
grammar (BITG) induced in a purely unsupervised
fashion, according to the algorithm in Saers et al.
(2009). Due to constraints on the training time, we
consider only the Viterbi biparse trees according
to the BITG instead of all the biparse trees in the
forest.
</bodyText>
<subsectionHeader confidence="0.995417">
4.2 Computing feature vectors
</subsectionHeader>
<bodyText confidence="0.992287666666667">
We compute the feature vectors at each internal
node in the biparse tree, similar to the feedforward
pass in a neural network. We topologically sort all
the nodes in the biparse tree and set the feature vec-
tor of each node in the topologically sorted order
as follows:
</bodyText>
<listItem confidence="0.988198">
• If the node is a leaf node, the feature vector is
the corresponding bisegment embedding.
• Else, the biconstituent embedding corre-
</listItem>
<bodyText confidence="0.902739142857143">
sponding to the internal node is generated us-
ing the feature vectors of the children and the
inversion order using Equation 1. We also
normalize the length of the computed fea-
ture vector so as to prevent the network from
making the biconstituent embedding arbitrar-
ily small in magnitude (Socher et al., 2011).
</bodyText>
<subsectionHeader confidence="0.996675">
4.3 Feature vector optimization
</subsectionHeader>
<bodyText confidence="0.995012222222222">
We train our current implementation of TRAAM,
by optimizing the model parameters to minimize
an objective function based on the reconstruction
error over all the nodes in the biparse trees. The
objective function is defined as a linear combina-
tion of the l2 norm of the reconstruction error of
the children and the cross-entropy loss of recon-
structing the inversion order. We define the error
at each internal node n as follows:
</bodyText>
<equation confidence="0.946294333333333">
α
En = 2 |[c1; c2] − [c′1; c′2]|2 − (1 − α)
[(1 − o) log(1 − o′) + (1 + o) log(1 + o′)]
</equation>
<bodyText confidence="0.998176777777778">
where c1, c2, o correspond to the left child, right
child and inversion order, c′1, c′2, o′ are the respec-
tive reconstructions and α is the linear weighting
factor. The global objective function J is the sum
of the error function at all internal nodes n in the bi-
parse trees averaged over the total number of sen-
tences T in the corpus. A regularization parameter
A is used on the norm of the model parameters B to
avoid overfitting.
</bodyText>
<equation confidence="0.961226">
1 EnEn + A||B||2 (3)
J = T
</equation>
<bodyText confidence="0.999934625">
As the bisegment embeddings are also a part of
the model parameters, the optimization objective
is similar to a moving target training objective Ro-
hwer (1990). We use backpropagation with struc-
ture Goller and Kuchler (1996) to compute the gra-
dients efficiently. L-BFGS algorithm Liu and No-
cedal (1989) is used in order to minimize the loss
function.
</bodyText>
<sectionHeader confidence="0.958902" genericHeader="method">
5 Bilingual representation learning
</sectionHeader>
<bodyText confidence="0.999976222222222">
We expect the TRAAM model to generate clus-
ters over cross-lingual relations similar to RAAM
models on monolingual data. We test this hypoth-
esis by bilingually training our model using a par-
allel English-Telugu blocks world dataset. The
dataset is kept simple to better understand the na-
ture of clusters. Our dataset comprises of com-
mands which involves manipulating different col-
ored objects over different shapes.
</bodyText>
<subsectionHeader confidence="0.973233">
5.1 Example
</subsectionHeader>
<bodyText confidence="0.998369772727273">
Figure 1 shows the biparse trees for two English-
Telugu sentence pairs. The preposition on in En-
glish translates to bt�o;6�,(pinunna) and b;5(pina) re-
spectively in the first and second sentence pairs be-
cause in the first sentence block is described by its
position on the square, whereas in the second sen-
tence block is the subject and square is the object.
Since Telugu is a language with an SOV structure,
the verbs Eoi�(vunchu) and &amp;Wl�(teesuko) occur at
the end for both sentences.
The sentences in 1 illustrate the importance of
modeling bilingual relations simultaneously in-
stead of focusing only on the input or output lan-
guage as the cross-lingual structural relations are
sensitive to both the input and output language
context. For example, the constituent whose input
side is block on the square, the corresponding output
language tree structure is determined by whether
or not on is translated to bto6�, (pinunna) or b;(pina).
In symbolic frameworks such as ITGs, such
relations are encoded using different nontermi-
nal categories. However, inducing such cate-
</bodyText>
<page confidence="0.996268">
117
</page>
<figureCaption confidence="0.999805">
Figure 3: Clustering of biconstituents in the Telugu-English data.
</figureCaption>
<bodyText confidence="0.999977333333333">
gories within a symbolic framework in an un-
supervised manner creates extremely challenging
combinatorial scaling issues. TRAAM models
are a promising approach for tackling this prob-
lem, since the vector representations learned us-
ing the TRAAM model inherently yield soft syn-
tactic category membership properties, despite be-
ing trained only with the unlabeled structural con-
straints of simple BITG-style data.
</bodyText>
<subsectionHeader confidence="0.999831">
5.2 Biconstituent clustering
</subsectionHeader>
<bodyText confidence="0.99997576">
The soft membership properties of learned dis-
tributed vector representations can be explored
via cluster analysis. To illustrate, we trained a
TRAAM network bilingually using the algorithm
in Section 4, and obtained feature vector represen-
tations for each unique biconstituent. Clustering
the obtained feature vectors reveals emergence of
fuzzy nonterminal categories, as shown in Figure
3. It is important to note that each point in the
vector space corresponds to a tree-structured bi-
constituent as opposed to merely a flat bilingual
phrase, as same surface forms with different tree
structures will have different vectors.
As the full cluster tree is too unwieldy, Figure
4 zooms in to shows an enlarged version of a por-
tion of the clustering, along with the corresponding
bracketed bilingual structures. One can observe
that the cluster represents the biconstituents that
describe the object by its position on another ob-
ject. We can deduce this from the fact that only a
single sense of on/&apos;a;6o;6�,(pinnuna) seems to be occur-
ing in all the biconstituents of the cluster. Manual
inspection of other clusters reveals such similari-
ties despite noise expected to be introduced by the
sparsity of our dataset.
</bodyText>
<sectionHeader confidence="0.999234" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999977125">
We have introduced a fully bilingual generaliza-
tion of Pollack’s (1990) monolingual Recursive
Auto-Associative Memory neural network model,
TRAAM, in which each distributed vector repre-
sents a bilingual constituent—i.e., an instance of
a transduction rule, which specifies a relation be-
tween two monolingual constituents and how their
subconstituents should be permuted. Bilingual ter-
minals are special cases of bilingual constituents,
where a vector represents either (1) a bilingual to-
ken—a token-to-token or “word-to-word” transla-
tion rule—or (2) a bilingual segment—a segment-
to-segment or “phrase-to-phrase” translation rule.
TRAAMs can be used for arbitrary rank SDTGs
(syntax-directed transduction grammars, a.k.a.
synchronous context-free grammars). Although
our discussions in this paper have focused on bi-
parse trees from SDTGs in a 2-normal form, which
by definition are ITGs due to the binary rank,
nothing prevents TRAAMs from being applied to
higher-rank transduction grammars.
We believe TRAAMs are worth detailed ex-
ploration as their intrinsic properties address key
problems in bilingual grammar induction and sta-
</bodyText>
<page confidence="0.995942">
118
</page>
<figureCaption confidence="0.998383">
Figure 4: Typical zoomed view into the Telugu-English biconstituent clusters from Figure 3.
</figureCaption>
<bodyText confidence="0.999608666666667">
tistical machine translation —their sensitivity to
both input and output language context means that
the learned vector representations tend to reflect
the similarity of bilingual rather than monolingual
constituents, which is what is needed to induce dif-
ferentiated bilingual nonterminal categories.
</bodyText>
<sectionHeader confidence="0.998945" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999955615384615">
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT contract
nos. HR0011-12-C-0014 and HR0011-12-C-0016,
and GALE contract nos. HR0011-06-C-0022 and
HR0011-06-C-0023; by the European Union un-
der the FP7 grant agreement no. 287658; and by
the Hong Kong Research Grants Council (RGC)
research grants GRF620811, GRF621008, and
GRF612806. Any opinions, findings and conclu-
sions or recommendations expressed in this mate-
rial are those of the authors and do not necessarily
reflect the views of DARPA, the EU, or RGC.
</bodyText>
<sectionHeader confidence="0.994752" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.6692341">
Alfred V. Aho and Jeffrey D. Ullman. The The-
ory of Parsing, Translation, and Compiling.
Prentice-Halll, Englewood Cliffs, New Jersey,
1972.
Yoshua Bengio, Réjean Ducharme, Pascal Vin-
cent, and Christian Jauvin. A neural probabilis-
tic language model. Journal ofMachine Learn-
ing Research, 3:1137–1155, 2003.
Yoshua Bengio, Jérôme Louradour, Ronan Col-
lobert, and Jason Weston. Curriculum learning.
</reference>
<bodyText confidence="0.930369666666666">
In Proceedings of the 26th annual international
conference on machine learning, pages 41–48.
ACM, 2009.
Marine Carpuat and Dekai Wu. Context-
dependent phrasal translation lexicons for sta-
tistical machine translation. In 11th Machine
Translation Summit (MT Summit XI), pages 73–
80, 2007.
Lonnie Chrisman. Learning recursive distributed
representations for holistic computation. Con-
nection Science, 3(4):345–366, 1991.
Ronan Collobert and Jason Weston. A unified
architecture for natural language processing:
Deep neural networks with multitask learning.
In Proceedings of the 25th International Con-
</bodyText>
<reference confidence="0.728356166666667">
ference on Machine Learning, ICML ’08, pages
160–167, New York, NY, USA, 2008. ACM.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang,
Thomas Lamar, Richard Schwartz, and John
Makhoul. Fast and robust neural network joint
models for statistical machine translation. In
</reference>
<page confidence="0.996541">
119
</page>
<reference confidence="0.967527577319588">
52nd Annual Meeting of the Association for
Computational Linguistics, 2014.
Jeffrey L Elman. Finding structure in time. Cog-
nitive science, 14(2):179–211, 1990.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. Learning continuous phrase represen-
tations for translation modeling. In 52nd Annual
Meeting of the Association for Computational
Linguistics (Short Papers), 2014.
Christoph Goller and Andreas Kuchler. Learn-
ing task-dependent distributed representations
by backpropagation through structure. In Neu-
ral Networks, 1996., IEEE International Con-
ference on, volume 1, pages 347–352. IEEE,
1996.
Karl Moritz Hermann and Phil Blunsom. Multi-
lingual models for compositional distributed se-
mantics. In 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics, volume
abs/1404.4641, 2014.
Kurt Hornik, Maxwell Stinchcombe, and Hal-
bert White. Multilayer feedforward networks
are universal approximators. Neural networks,
2(5):359–366, 1989.
Nal Kalchbrenner and Phil Blunsom. Recurrent
continuous translation models. In EMNLP,
pages 1700–1709, 2013.
Alexandre Klementiev, Ivan Titov, and Binod
Bhattarai. Inducing crosslingual distributed
representations of words. In 24th Interna-
tional Conference on Computational Linguistics
(COLING 2012). Citeseer, 2012.
Honglak Lee, Roger Grosse, Rajesh Ranganath,
and Andrew Y Ng. Convolutional deep be-
lief networks for scalable unsupervised learning
of hierarchical representations. In Proceedings
of the 26th Annual International Conference
on Machine Learning, pages 609–616. ACM,
2009.
Peng Li, Yang Liu, and Maosong Sun. Recur-
sive autoencoders for itg-based translation. In
EMNLP, pages 567–577, 2013.
Dong C Liu and Jorge Nocedal. On the limited
memory bfgs method for large scale optimiza-
tion. Mathematical programming, 45(1-3):503–
528, 1989.
Jordan B Pollack. Recursive distributed represen-
tations. Artificial Intelligence, 46(1):77–105,
1990.
Richard Rohwer. The “moving targets”training al-
gorithm. In Neural Networks, pages 100–109.
Springer, 1990.
Markus Saers, Joakim Nivre, and Dekai Wu.
Learning stochastic bracketing inversion trans-
duction grammars with a cubic time biparsing
algorithm. In 11th International Conference on
Parsing Technologies (IWPT’09), pages 29–32,
Paris, France, October 2009.
Christian Scheible and Hinrich Schütze. Cutting
recursive autoencoder trees. In 1st International
Conference on Learning Representations (ICLR
2013), Scottsdale, Arizona, May 2013.
Holger Schwenk. Continuous-space language
models for statistical machine translation. In
The Prague Bulletin of Mathematical Linguis-
tics, volume 93, pages 137–146, 2010.
Holger Schwenk. Continuous space transla-
tion models for phrase-based statistical machine
translation. In Proceedings of COLING 2012:
Posters, pages 1071––1080. Citeseer, 2012.
Richard Socher, Jeffrey Pennington, Eric H
Huang, Andrew Y Ng, and Christopher D Man-
ning. Semi-supervised recursive autoencoders
for predicting sentiment distributions. In Pro-
ceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages
151–161. Association for Computational Lin-
guistics, 2011.
Le Hai Son, Alexandre Allauzen, and François
Yvon. Continuous space translation models with
neural networks. In Proceedings of the 2012
conference of the north american chapter of the
association for computational linguistics: Hu-
man language technologies, pages 39–48. As-
sociation for Computational Linguistics, 2012.
Andreas Stolcke and Dekai Wu. Tree match-
ing with recursive distributed representations.
In AAAI 1992 Workshop on Integrating Neu-
ral and Symbolic Processes —The Cognitive Di-
mension, 1992.
Dekai Wu. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377–
403, 1997.
Bing Zhao, Yik-Cheung Tam, and Jing Zheng.
An autoencoder with bilingual sparse features
for improved statistical machine translation. In
</reference>
<page confidence="0.972092">
120
</page>
<reference confidence="0.981653833333333">
IEEE International Conference on Acoustic,
Speech and Signal Processing (ICASSP), 2014.
Will Y Zou, Richard Socher, Daniel M Cer, and
Christopher D Manning. Bilingual word embed-
dings for phrase-based machine translation. In
EMNLP, pages 1393–1398, 2013.
</reference>
<page confidence="0.997885">
121
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.291587">
<title confidence="0.999780666666667">Transduction Recursive Auto-Associative Memory: Learning Bilingual Compositional Distributed Vector Representations of Inversion Transduction Grammars</title>
<author confidence="0.588049">Karteek ADDANKi Dekai</author>
<affiliation confidence="0.7054525">Human Language Technology Department of Computer Science and</affiliation>
<author confidence="0.776099">Hong Kong University of Science</author>
<author confidence="0.776099">Technology</author>
<email confidence="0.984811">{vskaddanki|dekai}@cs.ust.hk</email>
<abstract confidence="0.99961912">We introduce TRAAM, or Transduction RAAM, a fully bilingual generalization of Pollack’s (1990) monolingual Recursive Auto-Associative Memory neural network model, in which each distributed vecrepresents a bilingual an instance of a transduction rule, which specifies a relation between two monolingual constituents and how their subconstituents should be permuted. Bilingual terminals are special cases of bilingual constituents, where a vector represents ei- (1) a bilingual token token-totoken or “word-to-word” translation rule (2) a bilingual segmentto-segment or “phrase-to-phrase” translation rule. TRAAMs have properties that appear attractive for bilingual grammar induction and statistical machine translation applications. Training of TRAAM drives both the autoencoder weights and the vector representations to evolve, such that similar bilingual constituents tend to have more similar vectors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling. Prentice-Halll,</booktitle>
<location>Englewood Cliffs, New Jersey,</location>
<contexts>
<context position="1567" citStr="Aho and Ullman, 1972" startWordPosition="206" endWordPosition="209">segment or “phrase-to-phrase” translation rule. TRAAMs have properties that appear attractive for bilingual grammar induction and statistical machine translation applications. Training of TRAAM drives both the autoencoder weights and the vector representations to evolve, such that similar bilingual constituents tend to have more similar vectors. 1 Introduction We introduce Transduction RAAM—or TRAAM for short—a recurrent neural network model that generalizes the monolingual RAAM model of Pollack (1990) to a distributed vector representation of compositionally structured transduction grammars (Aho and Ullman, 1972) that is fully bilingual from top to bottom. In RAAM, which stands for Recursive Auto-Associative Memory, using feature vectors to characterize constituents at every level of a parse tree has the advantages that (1) the entire context of all subtrees inside the constituent can be efficiently captured in the feature vectors, (2) the learned representations generalize well because similar feature vectors represent similar constituents or segments, and (3) representations can be automatically learned so as to maximize prediction accuracy for various tasks using semi-supervised learning. We argue </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. The Theory of Parsing, Translation, and Compiling. Prentice-Halll, Englewood Cliffs, New Jersey, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Réjean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal ofMachine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="7751" citStr="Bengio et al. (2003)" startWordPosition="1107" endWordPosition="1110">rum. 2.1 Monolingual related work Distributed vector representations have long been used for n-gram language modeling; these continuous-valued models exploit the generalization capabilities of neural networks, although there is no hidden contextual or hierarchical structure as in RAAM. Schwenk (2010) applies one such language model within an SMT system. In the simple recurrent neural networks (RNNs or SRNs) of Elman (1990), hidden layer representations are fed back to the input to dynamically represent an aggregate of the immediate contextual history. More recently, the probabilistic NNLMs of Bengio et al. (2003) and Bengio et al. (2009) follow in this vein. To represent hierarchical tree structure using vector representations, one simple family of approaches employs convolutional networks, as in Lee et al. (2009) for example. Collobert and Weston (2008) use a convolution neural network layer quite effectively to learn vector representations for words which are then used in a host of NLP tasks such as POS tagging, chunking, and semantic role labeling. RAAM approaches, and related recursive autoencoder approaches, can be more flexible than convolutional networks. Like SRNs, they can be extended in nume</context>
<context position="11290" citStr="Bengio et al. (2003)" startWordPosition="1642" endWordPosition="1645">eature induced with bilingual embeddings to a phrase-based SMT log-linear model, and report improvements in BLEU. Compared to TRAAM, however, they only learn noncompositional features, with distributed vectors only representing biterminals (as opposed to biconstituents or bilingual subtrees), and so other mechanisms for combining biterminal scores still need to be used to handle hierarchical structure, as opposed to seamlessly being integrated into the distributed vector representation model. Devlin et al. (2014) obtain translation accuracy improvements by extending the probabilistic NNLMs of Bengio et al. (2003), which are used for the output language, by adding input language context features. Unlike TRAAM, neither of these approaches symmetrically models the recursive structure of both the input and output language sides. For convolutional network approaches, Kalchbrenner and Blunsom (2013) use a recurrent probabilistic model to generate a representation of the source sentence and then generate the target sentence from this representation. This use of input language context to bias translation choices is in some sense a neural network analogy to the PSD (phrase sense disambiguation) approach for co</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. A neural probabilistic language model. Journal ofMachine Learning Research, 3:1137–1155, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Jérôme Louradour</author>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>Curriculum learning. ference on</title>
<date>2008</date>
<booktitle>Machine Learning, ICML ’08,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA,</location>
<marker>Bengio, Louradour, Collobert, Weston, 2008</marker>
<rawString>Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. ference on Machine Learning, ICML ’08, pages 160–167, New York, NY, USA, 2008. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="11188" citStr="Devlin et al. (2014)" startWordPosition="1626" endWordPosition="1630">ngs outperform the baselines in word semantic similarity. They also add a single semantic similarity feature induced with bilingual embeddings to a phrase-based SMT log-linear model, and report improvements in BLEU. Compared to TRAAM, however, they only learn noncompositional features, with distributed vectors only representing biterminals (as opposed to biconstituents or bilingual subtrees), and so other mechanisms for combining biterminal scores still need to be used to handle hierarchical structure, as opposed to seamlessly being integrated into the distributed vector representation model. Devlin et al. (2014) obtain translation accuracy improvements by extending the probabilistic NNLMs of Bengio et al. (2003), which are used for the output language, by adding input language context features. Unlike TRAAM, neither of these approaches symmetrically models the recursive structure of both the input and output language sides. For convolutional network approaches, Kalchbrenner and Blunsom (2013) use a recurrent probabilistic model to generate a representation of the source sentence and then generate the target sentence from this representation. This use of input language context to bias translation choi</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. Fast and robust neural network joint models for statistical machine translation. In 52nd Annual Meeting of the Association for Computational Linguistics, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="7557" citStr="Elman (1990)" startWordPosition="1078" endWordPosition="1079">lies heavily on strong monolingual language models, and log-linear models —in contrast to TRAAM’s fully integrated bilingual approach. Here we survey representative work across the spectrum. 2.1 Monolingual related work Distributed vector representations have long been used for n-gram language modeling; these continuous-valued models exploit the generalization capabilities of neural networks, although there is no hidden contextual or hierarchical structure as in RAAM. Schwenk (2010) applies one such language model within an SMT system. In the simple recurrent neural networks (RNNs or SRNs) of Elman (1990), hidden layer representations are fed back to the input to dynamically represent an aggregate of the immediate contextual history. More recently, the probabilistic NNLMs of Bengio et al. (2003) and Bengio et al. (2009) follow in this vein. To represent hierarchical tree structure using vector representations, one simple family of approaches employs convolutional networks, as in Lee et al. (2009) for example. Collobert and Weston (2008) use a convolution neural network layer quite effectively to learn vector representations for words which are then used in a host of NLP tasks such as POS taggi</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179–211, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning continuous phrase representations for translation modeling.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers),</booktitle>
<contexts>
<context position="9451" citStr="Gao et al. (2014)" startWordPosition="1367" endWordPosition="1370">ge scale NLP task, albeit monolingual. Scheible and Schütze (2013) automatically simplified the monolingual tree structures generated by recursive autoencoders, validated the simplified structures via manual evaluation, and showed that sentiment classification accuracy is not affected. 2.2 Bilingual related work The majority of work on learning bilingual distributed vector representations has not made use of recursive approaches or hidden contextual or compositional structure, as in the bilingual word embedding learning of Klementiev et al. (2012) or the bilingual phrase embedding learning of Gao et al. (2014). Schwenk (2012) uses a non-recursive neural network to predict phrase translation probabilities in conventional phrase-based SMT. Attempts have been made to generalize the distributed vector representations of monolingual ngram language models, avoiding any hidden contextual or hierarchical structure. Working within the framework of n-gram translation models, Son et al. (2012) generalize left-to-right monolingual n-gram models to bilingual n-grams, and study bilingual variants of class-based n-grams. However, their model does not allow tackling the challenge of modeling cross-lingual constitu</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2014</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. Learning continuous phrase representations for translation modeling. In 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas Kuchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Neural Networks, 1996., IEEE International Conference on,</booktitle>
<volume>1</volume>
<pages>347--352</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="22163" citStr="Goller and Kuchler (1996)" startWordPosition="3457" endWordPosition="3460">and inversion order, c′1, c′2, o′ are the respective reconstructions and α is the linear weighting factor. The global objective function J is the sum of the error function at all internal nodes n in the biparse trees averaged over the total number of sentences T in the corpus. A regularization parameter A is used on the norm of the model parameters B to avoid overfitting. 1 EnEn + A||B||2 (3) J = T As the bisegment embeddings are also a part of the model parameters, the optimization objective is similar to a moving target training objective Rohwer (1990). We use backpropagation with structure Goller and Kuchler (1996) to compute the gradients efficiently. L-BFGS algorithm Liu and Nocedal (1989) is used in order to minimize the loss function. 5 Bilingual representation learning We expect the TRAAM model to generate clusters over cross-lingual relations similar to RAAM models on monolingual data. We test this hypothesis by bilingually training our model using a parallel English-Telugu blocks world dataset. The dataset is kept simple to better understand the nature of clusters. Our dataset comprises of commands which involves manipulating different colored objects over different shapes. 5.1 Example Figure 1 s</context>
</contexts>
<marker>Goller, Kuchler, 1996</marker>
<rawString>Christoph Goller and Andreas Kuchler. Learning task-dependent distributed representations by backpropagation through structure. In Neural Networks, 1996., IEEE International Conference on, volume 1, pages 347–352. IEEE, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual models for compositional distributed semantics.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1404</volume>
<contexts>
<context position="12351" citStr="Hermann and Blunsom (2014)" startWordPosition="1802" endWordPosition="1805">on. This use of input language context to bias translation choices is in some sense a neural network analogy to the PSD (phrase sense disambiguation) approach for context-dependent translation probabilities of Carpuat and Wu (2007). Unlike TRAAM, the model does not contain structural constraints, and permutation of phrases must still be done in conventional PBSMT “shake’n’bake” style by relying mostly on a language model (in their case, a NNLM). A few applications of monolingual RAAM-style recursive autoencoders to bilingual tasks have also appeared. For cross-lingual document classification, Hermann and Blunsom (2014) use two separate monolingual fixed vector composition networks, one for each language. One provides the training signal for the other, and training is only on the embeddings. Li et al. (2013) described a use of monolingual recursive autoencoders within maximum entropy ITGs. They replace their earlier model for predicting reordering based on the first and the last tokens in a constituent, by instead using the context vector generated using the recursive autoencoder. Only input language context is used, unlike TRAAM which can use the input and output language contexts equally. Autoencoders have</context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. Multilingual models for compositional distributed semantics. In 52nd Annual Meeting of the Association for Computational Linguistics, volume abs/1404.4641, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Hornik</author>
<author>Maxwell Stinchcombe</author>
<author>Halbert White</author>
</authors>
<title>Multilayer feedforward networks are universal approximators.</title>
<date>1989</date>
<journal>Neural networks,</journal>
<volume>2</volume>
<issue>5</issue>
<contexts>
<context position="16447" citStr="Hornik et al., 1989" startWordPosition="2488" endWordPosition="2491">AM serves a similar role as the syntactic rules in the symbolic ITG, but keeps the encoding fuzzy. Figure 2 shows the straight and inverted syntactic rules and the corresponding inputs to the compressor network. Modeling of unary rules (with start symbol on the left hand side) although similar, is beyond the scope of this paper. It is easy to demonstrate that TRAAM models are capable of representing any symbolic ITG model. All the nonterminals representing a bispan can be encoded as a bit vector in the feature vector of the bispan. Using the universal approximation theorem of neural networks (Hornik et al., 1989), an encoder with a single hidden layer can represent any set of syntactic rules. Similarly, all TRAAM models can be represented using a symbolic ITG by assuming a unique nonterminal label for every feature vector. Therefore, TRAAM and ITGs represent two equivalent classes of models for representing compositional bilingual relations. It is important to note that although both TRAAM and ITG models might be equivalent, the fuzzy encoding of nonterminals in TRAAM is suitable for modeling the generalizations in bilingual relations without exploding the search space unlike the symbolic models. This</context>
</contexts>
<marker>Hornik, Stinchcombe, White, 1989</marker>
<rawString>Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="11576" citStr="Kalchbrenner and Blunsom (2013)" startWordPosition="1683" endWordPosition="1687">lingual subtrees), and so other mechanisms for combining biterminal scores still need to be used to handle hierarchical structure, as opposed to seamlessly being integrated into the distributed vector representation model. Devlin et al. (2014) obtain translation accuracy improvements by extending the probabilistic NNLMs of Bengio et al. (2003), which are used for the output language, by adding input language context features. Unlike TRAAM, neither of these approaches symmetrically models the recursive structure of both the input and output language sides. For convolutional network approaches, Kalchbrenner and Blunsom (2013) use a recurrent probabilistic model to generate a representation of the source sentence and then generate the target sentence from this representation. This use of input language context to bias translation choices is in some sense a neural network analogy to the PSD (phrase sense disambiguation) approach for context-dependent translation probabilities of Carpuat and Wu (2007). Unlike TRAAM, the model does not contain structural constraints, and permutation of phrases must still be done in conventional PBSMT “shake’n’bake” style by relying mostly on a language model (in their case, a NNLM). A</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In EMNLP, pages 1700–1709, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ivan Titov</author>
<author>Binod Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In 24th International Conference on Computational Linguistics (COLING 2012). Citeseer,</booktitle>
<contexts>
<context position="9387" citStr="Klementiev et al. (2012)" startWordPosition="1356" endWordPosition="1359">ation; this was perhaps the first use of a RAAM style approach on a large scale NLP task, albeit monolingual. Scheible and Schütze (2013) automatically simplified the monolingual tree structures generated by recursive autoencoders, validated the simplified structures via manual evaluation, and showed that sentiment classification accuracy is not affected. 2.2 Bilingual related work The majority of work on learning bilingual distributed vector representations has not made use of recursive approaches or hidden contextual or compositional structure, as in the bilingual word embedding learning of Klementiev et al. (2012) or the bilingual phrase embedding learning of Gao et al. (2014). Schwenk (2012) uses a non-recursive neural network to predict phrase translation probabilities in conventional phrase-based SMT. Attempts have been made to generalize the distributed vector representations of monolingual ngram language models, avoiding any hidden contextual or hierarchical structure. Working within the framework of n-gram translation models, Son et al. (2012) generalize left-to-right monolingual n-gram models to bilingual n-grams, and study bilingual variants of class-based n-grams. However, their model does not</context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. Inducing crosslingual distributed representations of words. In 24th International Conference on Computational Linguistics (COLING 2012). Citeseer, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Honglak Lee</author>
<author>Roger Grosse</author>
<author>Rajesh Ranganath</author>
<author>Andrew Y Ng</author>
</authors>
<title>Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>609--616</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="7956" citStr="Lee et al. (2009)" startWordPosition="1139" endWordPosition="1142">, although there is no hidden contextual or hierarchical structure as in RAAM. Schwenk (2010) applies one such language model within an SMT system. In the simple recurrent neural networks (RNNs or SRNs) of Elman (1990), hidden layer representations are fed back to the input to dynamically represent an aggregate of the immediate contextual history. More recently, the probabilistic NNLMs of Bengio et al. (2003) and Bengio et al. (2009) follow in this vein. To represent hierarchical tree structure using vector representations, one simple family of approaches employs convolutional networks, as in Lee et al. (2009) for example. Collobert and Weston (2008) use a convolution neural network layer quite effectively to learn vector representations for words which are then used in a host of NLP tasks such as POS tagging, chunking, and semantic role labeling. RAAM approaches, and related recursive autoencoder approaches, can be more flexible than convolutional networks. Like SRNs, they can be extended in numerous ways. The URAAM (Unification RAAM) model of Stolcke and Wu (1992) extended RAAM to demonstrate the possibility of using neural networks to perform more sophisticated operations like unification direct</context>
</contexts>
<marker>Lee, Grosse, Ranganath, Ng, 2009</marker>
<rawString>Honglak Lee, Roger Grosse, Rajesh Ranganath, and Andrew Y Ng. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 609–616. ACM, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
</authors>
<title>Recursive autoencoders for itg-based translation. In</title>
<date>2013</date>
<booktitle>EMNLP,</booktitle>
<pages>567--577</pages>
<contexts>
<context position="12543" citStr="Li et al. (2013)" startWordPosition="1835" endWordPosition="1838">ties of Carpuat and Wu (2007). Unlike TRAAM, the model does not contain structural constraints, and permutation of phrases must still be done in conventional PBSMT “shake’n’bake” style by relying mostly on a language model (in their case, a NNLM). A few applications of monolingual RAAM-style recursive autoencoders to bilingual tasks have also appeared. For cross-lingual document classification, Hermann and Blunsom (2014) use two separate monolingual fixed vector composition networks, one for each language. One provides the training signal for the other, and training is only on the embeddings. Li et al. (2013) described a use of monolingual recursive autoencoders within maximum entropy ITGs. They replace their earlier model for predicting reordering based on the first and the last tokens in a constituent, by instead using the context vector generated using the recursive autoencoder. Only input language context is used, unlike TRAAM which can use the input and output language contexts equally. Autoencoders have also been applied to SMT in a very different way by Zhao et al. (2014) but without recursion and not for learning distributed vector representations of words; rather, they used nonrecursive a</context>
</contexts>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Peng Li, Yang Liu, and Maosong Sun. Recursive autoencoders for itg-based translation. In EMNLP, pages 567–577, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical programming,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="22241" citStr="Liu and Nocedal (1989)" startWordPosition="3469" endWordPosition="3473"> linear weighting factor. The global objective function J is the sum of the error function at all internal nodes n in the biparse trees averaged over the total number of sentences T in the corpus. A regularization parameter A is used on the norm of the model parameters B to avoid overfitting. 1 EnEn + A||B||2 (3) J = T As the bisegment embeddings are also a part of the model parameters, the optimization objective is similar to a moving target training objective Rohwer (1990). We use backpropagation with structure Goller and Kuchler (1996) to compute the gradients efficiently. L-BFGS algorithm Liu and Nocedal (1989) is used in order to minimize the loss function. 5 Bilingual representation learning We expect the TRAAM model to generate clusters over cross-lingual relations similar to RAAM models on monolingual data. We test this hypothesis by bilingually training our model using a parallel English-Telugu blocks world dataset. The dataset is kept simple to better understand the nature of clusters. Our dataset comprises of commands which involves manipulating different colored objects over different shapes. 5.1 Example Figure 1 shows the biparse trees for two EnglishTelugu sentence pairs. The preposition o</context>
</contexts>
<marker>Liu, Nocedal, 1989</marker>
<rawString>Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. Mathematical programming, 45(1-3):503– 528, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="1453" citStr="Pollack (1990)" startWordPosition="192" endWordPosition="194">ilingual token —a token-totoken or “word-to-word” translation rule —or (2) a bilingual segment—a segmentto-segment or “phrase-to-phrase” translation rule. TRAAMs have properties that appear attractive for bilingual grammar induction and statistical machine translation applications. Training of TRAAM drives both the autoencoder weights and the vector representations to evolve, such that similar bilingual constituents tend to have more similar vectors. 1 Introduction We introduce Transduction RAAM—or TRAAM for short—a recurrent neural network model that generalizes the monolingual RAAM model of Pollack (1990) to a distributed vector representation of compositionally structured transduction grammars (Aho and Ullman, 1972) that is fully bilingual from top to bottom. In RAAM, which stands for Recursive Auto-Associative Memory, using feature vectors to characterize constituents at every level of a parse tree has the advantages that (1) the entire context of all subtrees inside the constituent can be efficiently captured in the feature vectors, (2) the learned representations generalize well because similar feature vectors represent similar constituents or segments, and (3) representations can be autom</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Jordan B Pollack. Recursive distributed representations. Artificial Intelligence, 46(1):77–105, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Rohwer</author>
</authors>
<title>The “moving targets”training algorithm.</title>
<date>1990</date>
<journal>In Neural Networks,</journal>
<pages>100--109</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="22098" citStr="Rohwer (1990)" startWordPosition="3448" endWordPosition="3450"> c1, c2, o correspond to the left child, right child and inversion order, c′1, c′2, o′ are the respective reconstructions and α is the linear weighting factor. The global objective function J is the sum of the error function at all internal nodes n in the biparse trees averaged over the total number of sentences T in the corpus. A regularization parameter A is used on the norm of the model parameters B to avoid overfitting. 1 EnEn + A||B||2 (3) J = T As the bisegment embeddings are also a part of the model parameters, the optimization objective is similar to a moving target training objective Rohwer (1990). We use backpropagation with structure Goller and Kuchler (1996) to compute the gradients efficiently. L-BFGS algorithm Liu and Nocedal (1989) is used in order to minimize the loss function. 5 Bilingual representation learning We expect the TRAAM model to generate clusters over cross-lingual relations similar to RAAM models on monolingual data. We test this hypothesis by bilingually training our model using a parallel English-Telugu blocks world dataset. The dataset is kept simple to better understand the nature of clusters. Our dataset comprises of commands which involves manipulating differ</context>
</contexts>
<marker>Rohwer, 1990</marker>
<rawString>Richard Rohwer. The “moving targets”training algorithm. In Neural Networks, pages 100–109. Springer, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm.</title>
<date>2009</date>
<booktitle>In 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<pages>29--32</pages>
<location>Paris, France,</location>
<contexts>
<context position="20034" citStr="Saers et al. (2009)" startWordPosition="3077" endWordPosition="3080">nitialization The weights and the biases of the compressor and the reconstructor networks of the TRAAM model are randomly initialized. Bisegment embeddings 116 corresponding to the leaf nodes (biterminals in the symbolic ITG notation) in the biparse trees are also initialized randomly. These constitute the model parameters and are optimized to minimize our objective function of reconstruction error. The parse trees for providing the structural constraints are generated by a bracketing inversion transduction grammar (BITG) induced in a purely unsupervised fashion, according to the algorithm in Saers et al. (2009). Due to constraints on the training time, we consider only the Viterbi biparse trees according to the BITG instead of all the biparse trees in the forest. 4.2 Computing feature vectors We compute the feature vectors at each internal node in the biparse tree, similar to the feedforward pass in a neural network. We topologically sort all the nodes in the biparse tree and set the feature vector of each node in the topologically sorted order as follows: • If the node is a leaf node, the feature vector is the corresponding bisegment embedding. • Else, the biconstituent embedding corresponding to t</context>
</contexts>
<marker>Saers, Nivre, Wu, 2009</marker>
<rawString>Markus Saers, Joakim Nivre, and Dekai Wu. Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm. In 11th International Conference on Parsing Technologies (IWPT’09), pages 29–32, Paris, France, October 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Scheible</author>
<author>Hinrich Schütze</author>
</authors>
<title>Cutting recursive autoencoder trees.</title>
<date>2013</date>
<booktitle>In 1st International Conference on Learning Representations (ICLR 2013),</booktitle>
<location>Scottsdale, Arizona,</location>
<contexts>
<context position="8900" citStr="Scheible and Schütze (2013)" startWordPosition="1287" endWordPosition="1290"> flexible than convolutional networks. Like SRNs, they can be extended in numerous ways. The URAAM (Unification RAAM) model of Stolcke and Wu (1992) extended RAAM to demonstrate the possibility of using neural networks to perform more sophisticated operations like unification directly upon the distributed vector representations of hierarchical 113 feature structures. Socher et al. (2011) used monolingual recursive autoencoders for sentiment prediction, with or without parse tree information; this was perhaps the first use of a RAAM style approach on a large scale NLP task, albeit monolingual. Scheible and Schütze (2013) automatically simplified the monolingual tree structures generated by recursive autoencoders, validated the simplified structures via manual evaluation, and showed that sentiment classification accuracy is not affected. 2.2 Bilingual related work The majority of work on learning bilingual distributed vector representations has not made use of recursive approaches or hidden contextual or compositional structure, as in the bilingual word embedding learning of Klementiev et al. (2012) or the bilingual phrase embedding learning of Gao et al. (2014). Schwenk (2012) uses a non-recursive neural netw</context>
</contexts>
<marker>Scheible, Schütze, 2013</marker>
<rawString>Christian Scheible and Hinrich Schütze. Cutting recursive autoencoder trees. In 1st International Conference on Learning Representations (ICLR 2013), Scottsdale, Arizona, May 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous-space language models for statistical machine translation.</title>
<date>2010</date>
<booktitle>In The Prague Bulletin of Mathematical Linguistics,</booktitle>
<volume>93</volume>
<pages>137--146</pages>
<contexts>
<context position="7432" citStr="Schwenk (2010)" startWordPosition="1057" endWordPosition="1058">components while still maintaining older SMT modeling assumptions like bagsof-words/phrases, “shake’n’bake” translation that relies heavily on strong monolingual language models, and log-linear models —in contrast to TRAAM’s fully integrated bilingual approach. Here we survey representative work across the spectrum. 2.1 Monolingual related work Distributed vector representations have long been used for n-gram language modeling; these continuous-valued models exploit the generalization capabilities of neural networks, although there is no hidden contextual or hierarchical structure as in RAAM. Schwenk (2010) applies one such language model within an SMT system. In the simple recurrent neural networks (RNNs or SRNs) of Elman (1990), hidden layer representations are fed back to the input to dynamically represent an aggregate of the immediate contextual history. More recently, the probabilistic NNLMs of Bengio et al. (2003) and Bengio et al. (2009) follow in this vein. To represent hierarchical tree structure using vector representations, one simple family of approaches employs convolutional networks, as in Lee et al. (2009) for example. Collobert and Weston (2008) use a convolution neural network l</context>
</contexts>
<marker>Schwenk, 2010</marker>
<rawString>Holger Schwenk. Continuous-space language models for statistical machine translation. In The Prague Bulletin of Mathematical Linguistics, volume 93, pages 137–146, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>1071--1080</pages>
<publisher>Citeseer,</publisher>
<contexts>
<context position="9467" citStr="Schwenk (2012)" startWordPosition="1371" endWordPosition="1372">albeit monolingual. Scheible and Schütze (2013) automatically simplified the monolingual tree structures generated by recursive autoencoders, validated the simplified structures via manual evaluation, and showed that sentiment classification accuracy is not affected. 2.2 Bilingual related work The majority of work on learning bilingual distributed vector representations has not made use of recursive approaches or hidden contextual or compositional structure, as in the bilingual word embedding learning of Klementiev et al. (2012) or the bilingual phrase embedding learning of Gao et al. (2014). Schwenk (2012) uses a non-recursive neural network to predict phrase translation probabilities in conventional phrase-based SMT. Attempts have been made to generalize the distributed vector representations of monolingual ngram language models, avoiding any hidden contextual or hierarchical structure. Working within the framework of n-gram translation models, Son et al. (2012) generalize left-to-right monolingual n-gram models to bilingual n-grams, and study bilingual variants of class-based n-grams. However, their model does not allow tackling the challenge of modeling cross-lingual constituent order, as TR</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Holger Schwenk. Continuous space translation models for phrase-based statistical machine translation. In Proceedings of COLING 2012: Posters, pages 1071––1080. Citeseer, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="8663" citStr="Socher et al. (2011)" startWordPosition="1247" endWordPosition="1250">effectively to learn vector representations for words which are then used in a host of NLP tasks such as POS tagging, chunking, and semantic role labeling. RAAM approaches, and related recursive autoencoder approaches, can be more flexible than convolutional networks. Like SRNs, they can be extended in numerous ways. The URAAM (Unification RAAM) model of Stolcke and Wu (1992) extended RAAM to demonstrate the possibility of using neural networks to perform more sophisticated operations like unification directly upon the distributed vector representations of hierarchical 113 feature structures. Socher et al. (2011) used monolingual recursive autoencoders for sentiment prediction, with or without parse tree information; this was perhaps the first use of a RAAM style approach on a large scale NLP task, albeit monolingual. Scheible and Schütze (2013) automatically simplified the monolingual tree structures generated by recursive autoencoders, validated the simplified structures via manual evaluation, and showed that sentiment classification accuracy is not affected. 2.2 Bilingual related work The majority of work on learning bilingual distributed vector representations has not made use of recursive approac</context>
<context position="17506" citStr="Socher et al. (2011)" startWordPosition="2651" endWordPosition="2654"> of nonterminals in TRAAM is suitable for modeling the generalizations in bilingual relations without exploding the search space unlike the symbolic models. This property of TRAAM makes it attractive for bilingual category learning and machine translation applications as long as appropriate language bias and objective functions are determined. Given our objective of inducing categories of bilingual relations in an unsupervised manner, we bias our TRAAM model by using a simple nonlinear activation function to be our compressor, similar to the monolingual recursive autoencoder model proposed by Socher et al. (2011). Having a single layer in our compressor provides the necessary language bias by forcing the network to capture the generalizations while reducing the dimensions of the input vectors. We use tanh as the nonlinear activation function and the compressor accepts two vectors c1 and c2 of dimension d corresponding to the nonterminals of the smaller bispans and a single bit o corresponding to the in115 Figure 1: Example of English-Telugu biparse trees where inversion depends on output language sense. Figure 2: Architecture of TRAAM. O2&apos; P1&apos; C3 &apos; O1&apos; C1&apos; C2 &apos; Reconstructor P2 Reconstructor P1 Compre</context>
<context position="20929" citStr="Socher et al., 2011" startWordPosition="3231" endWordPosition="3234">feedforward pass in a neural network. We topologically sort all the nodes in the biparse tree and set the feature vector of each node in the topologically sorted order as follows: • If the node is a leaf node, the feature vector is the corresponding bisegment embedding. • Else, the biconstituent embedding corresponding to the internal node is generated using the feature vectors of the children and the inversion order using Equation 1. We also normalize the length of the computed feature vector so as to prevent the network from making the biconstituent embedding arbitrarily small in magnitude (Socher et al., 2011). 4.3 Feature vector optimization We train our current implementation of TRAAM, by optimizing the model parameters to minimize an objective function based on the reconstruction error over all the nodes in the biparse trees. The objective function is defined as a linear combination of the l2 norm of the reconstruction error of the children and the cross-entropy loss of reconstructing the inversion order. We define the error at each internal node n as follows: α En = 2 |[c1; c2] − [c′1; c′2]|2 − (1 − α) [(1 − o) log(1 − o′) + (1 + o) log(1 + o′)] where c1, c2, o correspond to the left child, rig</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Le Hai Son</author>
<author>Alexandre Allauzen</author>
<author>François Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the</booktitle>
<pages>39--48</pages>
<contexts>
<context position="9831" citStr="Son et al. (2012)" startWordPosition="1422" endWordPosition="1425">esentations has not made use of recursive approaches or hidden contextual or compositional structure, as in the bilingual word embedding learning of Klementiev et al. (2012) or the bilingual phrase embedding learning of Gao et al. (2014). Schwenk (2012) uses a non-recursive neural network to predict phrase translation probabilities in conventional phrase-based SMT. Attempts have been made to generalize the distributed vector representations of monolingual ngram language models, avoiding any hidden contextual or hierarchical structure. Working within the framework of n-gram translation models, Son et al. (2012) generalize left-to-right monolingual n-gram models to bilingual n-grams, and study bilingual variants of class-based n-grams. However, their model does not allow tackling the challenge of modeling cross-lingual constituent order, as TRAAM does; instead it relies on the assumption that some other preprocessor has already managed to accurately re-order the words of the input sentence into exactly the order of words in the output sentence. Similarly, generalizations of monolingual SRNs to the bilingual case have been studied. Zou et al. (2013) generalize the monolingual recurrent NNLM model of B</context>
</contexts>
<marker>Son, Allauzen, Yvon, 2012</marker>
<rawString>Le Hai Son, Alexandre Allauzen, and François Yvon. Continuous space translation models with neural networks. In Proceedings of the 2012 conference of the north american chapter of the association for computational linguistics: Human language technologies, pages 39–48. Association for Computational Linguistics, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Dekai Wu</author>
</authors>
<title>Tree matching with recursive distributed representations.</title>
<date>1992</date>
<booktitle>In AAAI 1992 Workshop on Integrating Neural and Symbolic Processes —The Cognitive Dimension,</booktitle>
<contexts>
<context position="8421" citStr="Stolcke and Wu (1992)" startWordPosition="1214" endWordPosition="1217"> represent hierarchical tree structure using vector representations, one simple family of approaches employs convolutional networks, as in Lee et al. (2009) for example. Collobert and Weston (2008) use a convolution neural network layer quite effectively to learn vector representations for words which are then used in a host of NLP tasks such as POS tagging, chunking, and semantic role labeling. RAAM approaches, and related recursive autoencoder approaches, can be more flexible than convolutional networks. Like SRNs, they can be extended in numerous ways. The URAAM (Unification RAAM) model of Stolcke and Wu (1992) extended RAAM to demonstrate the possibility of using neural networks to perform more sophisticated operations like unification directly upon the distributed vector representations of hierarchical 113 feature structures. Socher et al. (2011) used monolingual recursive autoencoders for sentiment prediction, with or without parse tree information; this was perhaps the first use of a RAAM style approach on a large scale NLP task, albeit monolingual. Scheible and Schütze (2013) automatically simplified the monolingual tree structures generated by recursive autoencoders, validated the simplified s</context>
</contexts>
<marker>Stolcke, Wu, 1992</marker>
<rawString>Andreas Stolcke and Dekai Wu. Tree matching with recursive distributed representations. In AAAI 1992 Workshop on Integrating Neural and Symbolic Processes —The Cognitive Dimension, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<pages>403</pages>
<contexts>
<context position="5776" citStr="Wu, 1997" startWordPosition="819" endWordPosition="820">. For example, TRAAM has the ability to take into account the structure of both input and output subtree characteristics while making predictions on reordering them. Similarly, for specific cross-lingual tasks such as word alignment, sense disambiguation, or machine translation, classifiers can simultaneously be trained in conjunction with evolving the vector representations to optimize task-specific accuracy (Chrisman, 1991). In this paper we use as examples binary biparse trees consistent with transduction grammars in a 2-normal form, which by definition are inversion transduction grammars (Wu, 1997) since they are binary rank. This is not a requirement for TRAAM, which in general can be formed for transduction grammars of any rank. Moreover, with distributed vector representations, the notion of nonterminal categories in TRAAM is that of soft membership, unlike in symbolically represented transduction grammars. We start with bracketed training data that contains no bilingual category labels (like training data for Bracketing ITGs or BITGs). Training results in self-organizing clusters that have been automatically induced, representing soft nonterminal categories (unlike BITGs, which do n</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377– 403, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Yik-Cheung Tam</author>
<author>Jing Zheng</author>
</authors>
<title>An autoencoder with bilingual sparse features for improved statistical machine translation.</title>
<date>2014</date>
<booktitle>In IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP),</booktitle>
<contexts>
<context position="13022" citStr="Zhao et al. (2014)" startWordPosition="1915" endWordPosition="1918">n networks, one for each language. One provides the training signal for the other, and training is only on the embeddings. Li et al. (2013) described a use of monolingual recursive autoencoders within maximum entropy ITGs. They replace their earlier model for predicting reordering based on the first and the last tokens in a constituent, by instead using the context vector generated using the recursive autoencoder. Only input language context is used, unlike TRAAM which can use the input and output language contexts equally. Autoencoders have also been applied to SMT in a very different way by Zhao et al. (2014) but without recursion and not for learning distributed vector representations of words; rather, they used nonrecursive autoencoders to compress very highdimensional bilingual sparse features down to lowdimensional feature vectors, so that MIRA or PRO 114 could be used to optimize the log-linear model weights. 3 Representing transduction grammars with TRAAM As a recurrent neural network representation of a transduction grammar, TRAAM learns bilingual distributed representations that parallel the structural composition of a transduction grammar. As with transduction grammars, the learned repres</context>
</contexts>
<marker>Zhao, Tam, Zheng, 2014</marker>
<rawString>Bing Zhao, Yik-Cheung Tam, and Jing Zheng. An autoencoder with bilingual sparse features for improved statistical machine translation. In IEEE International Conference on Acoustic, Speech and Signal Processing (ICASSP), 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel M Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1393--1398</pages>
<contexts>
<context position="10378" citStr="Zou et al. (2013)" startWordPosition="1507" endWordPosition="1510">g within the framework of n-gram translation models, Son et al. (2012) generalize left-to-right monolingual n-gram models to bilingual n-grams, and study bilingual variants of class-based n-grams. However, their model does not allow tackling the challenge of modeling cross-lingual constituent order, as TRAAM does; instead it relies on the assumption that some other preprocessor has already managed to accurately re-order the words of the input sentence into exactly the order of words in the output sentence. Similarly, generalizations of monolingual SRNs to the bilingual case have been studied. Zou et al. (2013) generalize the monolingual recurrent NNLM model of Bengio et al. (2009) to learn bilingual word embeddings using conventional SMT word alignments, and demonstrate that the resulting embeddings outperform the baselines in word semantic similarity. They also add a single semantic similarity feature induced with bilingual embeddings to a phrase-based SMT log-linear model, and report improvements in BLEU. Compared to TRAAM, however, they only learn noncompositional features, with distributed vectors only representing biterminals (as opposed to biconstituents or bilingual subtrees), and so other m</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y Zou, Richard Socher, Daniel M Cer, and Christopher D Manning. Bilingual word embeddings for phrase-based machine translation. In EMNLP, pages 1393–1398, 2013.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
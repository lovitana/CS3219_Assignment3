<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002348">
<title confidence="0.997865">
POSTECH Grammatical Error Correction System in the CoNLL-
2014 Shared Task
</title>
<author confidence="0.99729">
Kyusong Lee, Gary Geunbae Lee
</author>
<affiliation confidence="0.999959">
Department of Computer Science and Engineering
Pohang University of Science and Technology
</affiliation>
<address confidence="0.6643">
Pohang, Korea
</address>
<email confidence="0.998829">
{Kyusonglee,gblee}@postech.ac.kr
</email>
<sectionHeader confidence="0.995636" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943428571429">
This paper describes the POSTECH gram-
matical error correction system. Various
methods are proposed to correct errors
such as rule-based, probability n-gram
vector approaches and router-based ap-
proach. Google N-gram count corpus is
used mainly as the correction resource.
Correction candidates are extracted from
NUCLE training data and each candidate
is evaluated with development data to ex-
tract high precision rules and n-gram
frames. Out of 13 participating teams, our
system is ranked 4th on both the original
and revised annotation.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9992597">
Automatic grammar error correction (GEC) is
widely used by learners of English as a second
language (ESL) in written tasks. Many methods
have been proposed to correct grammatical errors;
these include methods based on rules (Naber,
2003), on statistical machine translation (Brockett
et al., 2006), on machine learning, and on n-grams
(Alam et al., 2006). Early research (Han et al.,
2006; De Felice, 2008; Knight &amp; Chander, 1994;
Nagata et al., 2006) on error correction for non-
native text was based on well-formed corpora.
Most recent work (Cahill et al., 2013;
Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has
used machine learning methods that rely on a GE-
tagged corpus such as NUCLE, Japanese English
Learner corpus, and Cambridge Learner Corpus
(Dahlmeier et al., 2013; Izumi et al., 2005;
Nicholls, 2003), because well-formed and GE-
tagged approaches are closely related to each
other, can be synergistically combined. Therefore,
research using both types of data has also been
conducted (Dahlmeier &amp; Ng, 2011). Moreover, a
meta-classification method using several GE-
tagged corpora and a native corpus has been pro-
posed to correct the grammatical errors (Seo et al.,
2012). A meta-classifier approach has been pro-
posed to combine a language model and error-spe-
cific classification for correction of article and
preposition errors (Gamon, 2010). Web-scale
well-formed corpora have been successfully ap-
plied to grammar error correction tasks instead of
using error-tagged data (Bergsma et al., 2009;
Gamon et al., 2009; Hermet et al., 2008). Espe-
cially in the CoNLL-2013 grammar error correc-
tion shared task, many of the high-ranked teams
(Kao et al., 2013; Mark &amp; Roth, 2013; Xing et al.,
2013) exploited the Google Web-1T n-gram cor-
pus. The major advantage of using these web-
scale corpora is that extremely large quantities of
data are publicly available at no additional costs;
thus fewer data sparseness problems arise com-
pared to previous approaches based on error-
tagged corpora.
We also use the Google Web-1T n-gram corpus.
We extract the candidate pairs (original erroneous
text and its correction) from NUCLE training data.
We use a router to choose the best frame to com-
pare the n-gram score difference between the orig-
inal and replacement in a given candidate pair.
The intuition of our grammar error correction
method is the following: First, if the uni-gram
count is less than some threshold, we assume that
the word is erroneous. Second, if the replacement
word n-gram has more frequent than the original
word n-gram, it presents strong evidence for cor-
rection. Third, depending on the candidate pair,
tailored n-gram frames help to correct errors ac-
curately. Fourth, only high precision method and
rules are applied. If correction precision on a can-
didate pair is less than 30% in development data,
</bodyText>
<page confidence="0.998245">
65
</page>
<note confidence="0.9887535">
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 65‚Äì73,
Baltimore, Maryland, 26-27 July 2014. cÔøΩ2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999419">
Figure 1. Overall Process of Router-based Correction
</figureCaption>
<bodyText confidence="0.998847769230769">
we do not make a correction for the candidate pair
at runtime.
In the CoNLL-Shared Task, objectives were
presented yearly. In 2012, the objective was to
correct article and preposition errors; in 2013, it
was to correct article, preposition, noun number,
verb form, and subject-verb agreement errors.
This year, the objective is to correct all errors.
Thus, our method should also correct prepro-
cessing and spelling errors. Detailed description
of the shared task set up, data set, and evaluation
about the CoNLL-2014 Shared Task is explained
in (Ng et al., 2014)
</bodyText>
<sectionHeader confidence="0.869947" genericHeader="introduction">
2 Data and Recourse
</sectionHeader>
<bodyText confidence="0.9997186">
The Google Web-1T corpus contains 1012 words
of running text and the counts for all 109 five-word
sequences that appear &gt; 40 times (Brants &amp; Franz,
2006). We used the NUS Corpus of Learner Eng-
lish (NUCLE) training data to extract the candi-
date pairs and CoNLL-2013 Official Shard Task
test data as development data. We used the Stan-
ford parser (De Marneffe &amp; Manning, 2008) to
extract part-of-speech, dependency, and constitu-
ency trees.
</bodyText>
<sectionHeader confidence="0.998203" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.997455">
3.1 Overall Process
</subsectionHeader>
<bodyText confidence="0.998682">
We correct the errors in the following order:
Tokenizing ‚Üí spelling error correction ‚Üí punc-
tuation error correction ‚Üí N-gram Vector Ap-
proach for Noun number (Nn) ‚Üí Router-based
Correction (Deletion Correction ‚Üí Insertion Cor-
rection ‚Üí Replacement) for various error types ‚Üí
Rule-based method for verb errors. Between each
pair of step, we parse, tag, and tokenize again us-
ing the Stanford parser because the previous cor-
rection affects parsing, tagging, and tokenizing re-
sults.
</bodyText>
<subsectionHeader confidence="0.999475">
3.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999924130434782">
Because the correction task is no longer restricted
to five error types, tokenizing and spelling error
correction have become critical for error correc-
tion. To detect tokenizing error such as ‚Äúciviliza-
tions.It‚Äù, a re-tokenzing process is necessary. If a
word contains a comma, punctuation (e.g., ‚Äò,‚Äô or
‚Äò.‚Äô) and the word count in Google n-gram is less
than some threshold (here, 1000), we tokenize the
word, e.g., as ‚Äúcivilizations . It‚Äù. We also correct
spelling errors by referring to the Google n-gram
word count. If the word uni-gram count is less
than a threshold (here, 60000) and the part-of-
speech (POS) tag is not NNP or NNPS, we assume
that the word has o ne or more errors. The thresh-
old is set based on the development set. We use
the Enchant Python Library to correct the spelling
errors1. However, using only one best result is not
very accurate. Thus, among the best results in the
Enchant Python Library, we select the one best
word, i.e. that word with the highest frequency in
the Google n-gram corpus. Using NUCLE train-
ing data, rules are constructed for comma, punc-
tuation, and other errors (Table 3).
</bodyText>
<footnote confidence="0.978155">
1 http://abisource.com/projects/enchant/
</footnote>
<page confidence="0.987596">
66
</page>
<subsectionHeader confidence="0.998349">
3.3 Candidate Generation
</subsectionHeader>
<bodyText confidence="0.992463552631579">
Selecting appropriate correction candidates is crit-
ical for the precision of the method. In article and
noun number correction, the number of candidates
is small: ‚Äòa‚Äô,‚Äôan‚Äô,‚Äôthe‚Äô in article correction, ‚Äòplural‚Äô
or ‚Äòsingular‚Äô in noun number correction. However,
the number of correction candidates can be unlim-
ited in wrong collocation/idiom errors. Reducing
the number of candidates is important in the gram-
mar error correction task.
In Correction Candidate: noun number correc-
tion has just one replacement candidate. If the
word is plural, its correction candidate is singular,
and vice versa. The language tool2 can perform
these changes.
Other Correction Candidate: for corrections
other than noun number, candidates are selected
from the GE-tagged corpus. A total of 4206 pairs
were extracted. We use the notation of candidate
pair (o‚Üír), which links the original word (o) and
its correction candidate (r). In the deletion correc-
tion step, we determine whether or not the word
should be deleted. In the insertion correction step,
we select the insertion position in a sentence as a
space between two words. If o is ‚àÖ, insertion cor-
rection is required; if r is ‚àÖ, the pair deletion cor-
rection is required. We use the Stanford constitu-
ency parser (De Marneffe &amp; Manning, 2008) to
extract a noun phrase; if it does not contain a de-
terminer or article, we insert one in front of the
noun phrase; if the noun in the noun phrase is sin-
gular, ‚Äòthe‚Äô, ‚Äòa‚Äô, and, ‚Äòan‚Äô are selected an insertion
candidates; if the noun is plural, only ‚Äòthe‚Äô is se-
lected as an insertion candidate. We only apply in-
sertion correction at ArtOrDet, comma errors, and
preposition; we skip insertion correction for other
error types because selecting an insertion position
is difficult and if every position is selected as in-
sertion position, precision decrease.
</bodyText>
<sectionHeader confidence="0.997503" genericHeader="method">
4 I-gram Approach
</sectionHeader>
<bodyText confidence="0.982159375">
We used the following notation.
N(o) n-gram vector in original sentence
N(r) n-gram vector in replacement sen-
tence
n(o)i i th element in N(o)
ùëõ (ùëü)ùëñ i th element in N(r)
N[i:j] n-gram vector from i th element to
j th element
</bodyText>
<footnote confidence="0.862639">
2http://www.languagetool.org
</footnote>
<bodyText confidence="0.9915177">
Web-scale data have also been used successfully
in many other research areas, such as lexical dis-
ambiguation (Bergsma et al., 2009). Most NLP
systems resolve ambiguities with the help of a
large corpus of text, e.g.:
‚Ä¢ The system tried to decide {among, between}
the two confusable words.
Disambiguation accuracy increases with the size
of the corpus. Many systems incorporate the web
count into their selection process. For the above
example, a typical web-based system would query
a search engine with the sequences ‚Äúdecide among
the‚Äù and ‚Äúdecide between the‚Äù and select the can-
didate that returns the most hits. Unfortunately,
this approach would fail when disambiguation re-
quires additional context. Bergsma (2009) sug-
gested using the context of samples of various
lengths and positions. For example, from the
above the example sentence, the following 5-gram
patterns can be extracted:
</bodyText>
<listItem confidence="0.9998914">
‚Ä¢ system tried to decide {among, between}
‚Ä¢ tried to decide {among, between} the
‚Ä¢ to decide {among, between} the two
‚Ä¢ decide {among, between} the two confusable
‚Ä¢ {among, between} the two confusable words
</listItem>
<bodyText confidence="0.999968125">
Similarly, four 4-gram patterns, three 3-gram pat-
terns and two 2-gram patterns are extracted by
spanning the target. A score for each pattern is cal-
culated by summing the log-counts. This method
was successfully applied in lexical disambigua-
tion. Web-scale data were used with the count in-
formation specified as features. Kao et al. (2013)
used a ‚Äúmoving window (MW)‚Äù :
</bodyText>
<equation confidence="0.963694">
ùëÄùëäùëñ,ùëò (w) = {ùë§ùëñ‚àíùëó, ... , ùë§ùëñ‚àíùëó+(ùëò‚àí1), J = 0, ùëò ‚àí 1} (1)
</equation>
<bodyText confidence="0.9977445">
where ùëñ denotes the position of the word, k the
window size and w the original or replacement
word at position ùëñ. The window size is set to 2 to
5 words. MW is the same concept as the SUMLM:
</bodyText>
<equation confidence="0.986729">
Sùëñ,ùëò (ùë§) = E cùëúuùëõt(ùëõùëîùëüùëéùëö) (2)
ùëõùëîùëüùëéùëöeùëÄùëäùëò(ùë§)
</equation>
<bodyText confidence="0.9997946">
Both approaches apply the sum of all MWs in (1).
Our approach is based on the MW method. The
difference is that instead of summing all the MWs,
we consider only one best MW which is referred
to here as a frame. The following sentences
</bodyText>
<page confidence="0.99698">
67
</page>
<bodyText confidence="0.9976065">
demonstrate the case when the following words
are the crucial features to correct errors:
</bodyText>
<listItem confidence="0.9973935">
‚Ä¢ I will do it (in‚Üíat) home.
‚Ä¢ We need (an‚Üí0) equipment to solve problems.
</listItem>
<bodyText confidence="0.991843333333333">
However, following sentences demonstrate the
case when preceding words is the crucial feature
to correct errors:
</bodyText>
<listItem confidence="0.9996715">
‚Ä¢ One (are‚Üíis) deemed to death at a later stage .
‚Ä¢ But data that (shows‚Üíshow) the rising of life
</listItem>
<bodyText confidence="0.9009045">
expectancies
We investigated which frame is the best based on
the development set, then router is trained to de-
cide on the frame depending on the candidate pair.
</bodyText>
<subsectionHeader confidence="0.961545">
4.1 Router-based I-gram Correction
</subsectionHeader>
<bodyText confidence="0.9994816">
A frame is a sequence of words around the target
position. A frame is divided into a preceding
frame and a following frame. The target position
can be either a position of a target word (Figure
2a) or a position in which a candidate word is
judged to be necessary (Figure 2b). Once the size
(i.e., number of words) of frames is chosen, sev-
eral forms of frames (n; m) with different sizes of
preceding (n) and following (m) words are possi-
ble.
</bodyText>
<figureCaption confidence="0.995698">
Figure 2. Frame for n-gram
</figureCaption>
<bodyText confidence="0.99992956">
The router is designed to take care of two stages
(training, run-time) error correction. During train-
ing, the router selects the best frame for each can-
didate pair. By testing each candidate pair with
each frame in the development data; the frame
with the best precision is selected as the best
frame among (1;1), (1;2), (1;3), (2;1),(2:2), etc.
At the end of the training stage, the router has
a list of pairs (x) which matches the best frame (y)
associated with it (Table 1) as a result of compar-
ing each candidate pair with one in the develop-
ment corpus.
During runtime, the router assigns each candi-
date pair to the best frame to produce the output
sentence (Figure 1). For example, for a sentence
‚ÄúThis ability is not seen 40 years back where the
technology advances were not as good as now .‚Äù
the candidate pair for correction (back‚Üí ago) is
suggested. The best frame assigned by the router
for this pair (1;1), which is ‚Äúyears back where‚Äù.
The best candidate frame for this is ‚Äúyear ago
where‚Äù. At this point, we query the count of
‚Äúyears back where‚Äù and ‚Äúyears ago where‚Äù from
the Google N-gram Count Corpus; these counts
are 46 and 1815 respectively. Because the count
</bodyText>
<tableCaption confidence="0.99281">
Table 1. Example of Trained Router
</tableCaption>
<equation confidence="0.991440625">
x (o‚Üír) y
(another‚Üíother) (1;3)
(less‚Üífewer) (1;3)
(rise‚Üíraise) (1;2)
(back‚Üíago) (1;1)
(could‚Üícan) (2;1)
(well‚Üígood) (2;1)
(near‚Üí0) No correction
</equation>
<bodyText confidence="0.999957307692308">
of ‚Äúyears ago where‚Äù is greater than that of ‚Äúyears
back where‚Äù, the former is selected as the correct
form. As a result, the sentence ‚ÄúThis ability is not
seen 40 years back where the technology ad-
vances were not as good as now.‚Äù is corrected to
‚ÄúThis ability is not seen 40 years ago where the
technology advances were not as good as now.‚Äù
Some words are allowed to have multiple best
frames; in all the best frames, if a candidate word
sequence is more frequent than an original word
sequence in the Google count, then correction is
made. The multiple frames are also trained from
the development data set.
</bodyText>
<subsectionHeader confidence="0.997507">
4.2 Probability n-gram Vector
</subsectionHeader>
<bodyText confidence="0.999968333333333">
We use the probability n-gram Vector approach to
correct Nn. Most errors are corrected using the
router-based method; however, training the router
for every noun is difficult because the number of
nouns is extremely large. Moreover, for noun
number, we found that rather than considering one
direction or one frame of n-gram, every direction
of n-gram should be considered for better perfor-
mance such as forward, backward, and two-way.
Thus, the probability n-gram vector algorithm is
applied only in the noun number error correction.
We propose the probability n-gram vector method
to correct grammatical errors to consider both di-
rections, forward and backward. In a forward n-
gram, the probability of each word is estimated
</bodyText>
<page confidence="0.997184">
68
</page>
<bodyText confidence="0.9997845">
depending on the preceding word. On the other
hand, in a backward n-gram the probability of
each word is estimated depending on the follow-
ing words. When the probability of a candidate
word is higher than original word, we replace the
original with the candidate word in the correction
step.
Probability n-gram vectors are generated from the
original word and a candidate word (Figure 3).
Rather than using a single sequence of n-gram
probability, we apply contexts of various lengths
and positions. We applied the probability infor-
mation using the Google n-gram count infor-
mation as in the following equation:
</bodyText>
<equation confidence="0.998074">
P(Wi|Wi‚àí2,Wi‚àí,) _ C(Wi‚àí2,Wi‚àí1Wi)
‚Äî
C(Wi‚àí2,Wi‚àí1)
</equation>
<bodyText confidence="0.942265166666667">
Moreover, rather than calculating one word‚Äôs
probability given n words such as
P(Wi|Wi‚àí,, Wi‚àí2, Wi‚àí3), our model calculates the
probability of m words given an n word sequence.
The following is an example 4-gram with forward
probability:
</bodyText>
<listItem confidence="0.998971333333333">
‚Ä¢ m = 3, n = 1 P(Wi‚àí2,Wi‚àí,Wi |Wi‚àí3)
‚Ä¢ m = 2, n = 2 P(Wi‚àí,, Wi |Wi‚àí3, Wi‚àí2)
‚Ä¢ m = 1, n = 3 P(Wi|Wi‚àí3, Wi‚àí2,Wi‚àí,).
</listItem>
<bodyText confidence="0.998692666666667">
We construct a 40-dimensional probability vector
with forward and backward probabilities consid-
ering of twenty 5-grams, twelve 4-grams, six 3-
</bodyText>
<tableCaption confidence="0.987449">
Table 2: The elements of n-gram vector
</tableCaption>
<equation confidence="0.989139823529412">
5-GRAM
no = P(Wi|Wi+,Wi+2Wi+3Wi+4) backward
n, = P(Wi |Wi‚àí4Wi‚àí3Wi‚àí2Wi‚àí,) forward
n2 = P (Wi Wi+,|Wi+2Wi+3Wi+4) backward
4-GRAM
n2o = P (Wi |Wi+,Wi+2Wi+3) backward
n2, = P(Wi|Wi‚àí3Wi‚àí2Wi‚àí,) forward
3-GRAM
n32 = P (Wi |Wi+,Wi+2) backward
n33 = P(Wi|Wi‚àí2Wi‚àí,) forward
n34 = P(WiWi+,|Wi+2) backward
n35 = P(Wi‚àí,Wi|Wi‚àí2) forward
n36 = P(Wi‚àí,Wi|Wi+,) backward
n37 = P(WiWi+,|Wi‚àí,) forward
2-GRAM
n38 = P(Wi |Wi+,) backward
n39 = P(Wi|Wi‚àí,) forward
</equation>
<figureCaption confidence="0.993751">
Figure 3. Overall process of Nn Correction
</figureCaption>
<bodyText confidence="0.9895416">
gram, and two 2-gram. Additionally, the elements
of the n-gram vector are detailed in Table 2.
Back-Off Model: A high-order n-gram is more
effective than a low-order n-gram. Thus, we ap-
plied back-off methods (Katz, 1987) to assign
higher priority to higher order probabilities. If all
elements in 5-gram vectors are 0 for both the orig-
inal and candidate sentence, which means
Zi=o{n(o)i + n(r)i} = 0, we consider 4-gram
vectors (N[20:31]). If 4-gram vectors are 0, we con-
sider 3-gram vectors. Moreover, when the pro-
posed method calculates each of the forward,
backward and two-way probabilities, the back-off
method is used to get each score.
Correction: Here, we explain the process of error
correction using n-gram vectors. First, we gener-
ate Nn error candidates. Second, we construct the
n-gram probability vector for each candidate. The
back-off method is applied in N(o)+N(r), The vec-
tor contains various directions and ranges of prob-
abilities of words given a sample sentence. We
then calculate forward n-gram score by summing
even elements in the vector. We calculate the
backward n-gram by summing odd elements in
Table 2. Next, the two-way n-gram is calculated
by summing all elements for both directions n-
gram. If forward, backward, and two-way n-
grams have higher probabilities for the candidate
word, we select the candidate as corrected word
(Figure 3).
</bodyText>
<page confidence="0.993185">
69
</page>
<figure confidence="0.8292415">
Algorithm Rule1-Comma
1: function rule1( toksent, tokpos)
2: for i ‚Üê 0 ... len(toksent) do
3: if toksent[i] in [ However‚Äô, ‚ÄòTherefore‚Äô, ‚ÄòThus‚Äô] and not toksent[i + 1] == ‚Äò,‚Äô then
4: toksent[i]= toksent[i] + ‚Äò ,‚Äô
Algorithm Rule2-preposition
1: function rule2( toksent, tokpos)
2: for i ‚Üê 0 ... len(toksent) do
3: if toksent[i] = ‚Äòaccording‚Äô and not toksent [i+1] _ ‚Äòto‚Äô
4: toksent [i+1] _ ‚Äòto ‚Äò+ toksent [i+1]
Algorithm Rule3-Subject Verb Agreement
1: function rule3( toksent, tokpos)
</figure>
<listItem confidence="0.851759727272727">
2: for i ‚Üê 0 ... len(toksent) do
3: if toksent[i] is ‚Äòwhich‚Äô
4: if tokpos[i ‚àí 1] == ‚ÄòNNS‚Äô and tokpos[i + 1] == ‚ÄòVBZ‚Äô then
5: toksent[i + 1]= changeWordForm (toksent[i + 1], ‚ÄòVBP‚Äô)
6: else if tokpos[i ‚àí 1] == ‚ÄòNNS‚Äô and tokpos[i + 1] == ‚ÄòNNS‚Äô then
7: toksent[i + 1]= =changeWordForm(toksent[i + 1], ‚ÄòVBP‚Äô)
8: else if tokpos[i ‚àí 1] == ‚ÄòNN‚Äô and tokpos[i + 1]== ‚Äòare‚Äô then
9: toksent[i + 1]= = is
10: else if tokpos[i ‚àí 1] == ‚ÄòNN‚Äô and tokpos[i + 1] in [‚ÄòVBP‚Äô,‚ÄôVB‚Äô,‚ÄôNN‚Äô] then
11: toksent[i + 1]= = makePlural(toksent[i + 1])
Algorithm Rule4-Subject Verb Agreement
1: function rule4( toksent, tokpos)
2: for i ‚Üê 0 ... len(toksent) do
3: if not ( tokpos[i]is ‚Ä≤VBZ‚Ä≤ and [‚ÄòNN‚Äô,‚Äôthis‚Äô,‚Äôit‚Äô,‚Äôone‚Äô,‚ÄôVBG‚Äô] in tokpos[i ‚àí 5: i]) then
4: tokcand‚ÜêchangeWordForm( tokword[i], ‚ÄòVBP‚Äô)
5: else if not ( tokpos[i]is ‚Ä≤VBP‚Ä≤ and [‚ÄòI‚Äô,‚Äôwe‚Äô,‚Äôthey‚Äô,‚Äôand‚Äô] in toksent[i ‚àí 5: i]) then
6: tokcand ‚ÜêchangeWordForm( tokword[i], ‚ÄòVBZ‚Äô)
7: else if not ( tokpos[i]is ‚Ä≤NN‚Ä≤ and [‚Äòbe‚Äô,‚Äôing‚Äô] in toksent[i ‚àí 5: i]) then
8: tokcand‚ÜêchangeWordForm( tokword[i], ‚ÄòVBN‚Äô)
9: original = ngramCount( toksent), candidate =ngramCount(tokcand)
10: If original &lt; candidate then
11: Return tokcand
</listItem>
<tableCaption confidence="0.997089">
Table 3. Examples of Rules
</tableCaption>
<sectionHeader confidence="0.965225" genericHeader="method">
5 Verb Correction (Rule-based)
</sectionHeader>
<bodyText confidence="0.99959565">
There are several types of verb errors in non-na-
tive text such as verb tense, verb modal, missing
verb, verb form, and subject-verb-agreement
(SVA). Among these errors, we attempt to correct
SVA errors using rule-based methods (Table 3).
In non-native text, parsing and tagging errors are
inevitable, and it may cause false alarm. Thus, in-
stead of dependency parsing to find subject and
verb, we consider the preceding five words be-
cause erroneous sentences often contain depend-
ency errors. Moreover, in erroneous sentences,
POS tagging accuracy is lower than native text.
Thus, NN and VB are misclassified, as are VBZ
and NNS. A rule is used that encodes the relevant
linguistic knowledge that these words or POSs
should not occur in the five positions preceding
the VBZ: ‚ÄòNN‚Äô, ‚Äôthis‚Äô, ‚Äôit‚Äô ,‚Äôone‚Äô, ‚ÄôVBG‚Äô. Moreover,
words that preceded and follow ‚Äòwhich‚Äô should
agree in verb form, as indicated in Rule3 and
Rule4.
</bodyText>
<sectionHeader confidence="0.993119" genericHeader="method">
6 Experiment
</sectionHeader>
<bodyText confidence="0.81488">
The CoNLL-2014 training data consist of 1,397
articles together with gold-standard annotation.
</bodyText>
<page confidence="0.998694">
70
</page>
<tableCaption confidence="0.995682">
Table 4. Performance on each error type Revised annotation
</tableCaption>
<table confidence="0.972536857142857">
Original annotation
Precision Recall F0.5 Precision Recall F0.5
N-gram (Nn) 31.0 6.55 17.75 42.28 9.0 24.31
Rule (Verb) 28.95 1.12 4.86 31.17 1.29 5.52
Rule (Mec) 49.34 5.47 18.94 52.16 6.17 20.93
Router (Others) 28.11 12.49 22.49 35.29 15.45 28.08
All 34.51 21.73 30.88 41.28 25.59 36.77
</table>
<bodyText confidence="0.999560295454545">
The documents are a subset of the NUS Corpus of
Learner English (NUCLE). We use the Max-
Match (M2) scorer provided by the CoNLL-2014
Shared Task. The M2 scorer works by using the
set that maximally matches the set of gold-stand-
ard edits specified by the annotator as being equal
to the set of system edits that are automatically
computed and used in scoring (Dahlmeier &amp; Ng,
2012). The official evaluation metric is F0.5,
weighting precision twice as much as recall. We
achieve F0.5 of 30.88; precision of 34.51; recall
of 21.73 in the original annotation (Table 4). After
original official annotations announced by organ-
izers (i.e., only based on the annotations of the two
annotators), another set of annotations is offered
based on including the additional answers pro-
posed by the 3 teams (CAMB, CUUI, UMC). The
improvement gap between the original annotation
and the revised annotation of our team (POST) is
5.89%. We obtain the highest improvement rate
except for the 3 proposed teams (Figure 4), F0.5
of 36.77; precision of 41.28; recall of 25.59 in the
revised annotation. Our system achieves the 4th
highest scores of 13 participating teams based on
both the original and revised annotations. To ana-
lyze the scores of each of the error types and mod-
ules, we apply the method of n-gram vector (Nn),
rule-based (Verb, Mec), and router-based (others)
separately in both the original and the revised an-
notation of all error types. We achieve high preci-
sion by rules at the Mec which indicates punctua-
tion, capitalization, spelling, and typos errors. Ad-
ditionally, the Nn type has the highest improve-
ment gap between the original and revised anno-
tation (17% ‚Üí 24.31 of F0.5). In order for our
team to improve the high precision in the rule-
based approach, we tested potential rules on the
development data and kept a rule only if its preci-
sion on that data set was 30% or greater. When we
trained router, the same strategy was conducted.
If a frame could not achieve 30% precision, we
assigned the candidate pair as ‚Äúno correction‚Äù in
the router. These constraints achieve precision of
30 % in most error types.
</bodyText>
<sectionHeader confidence="0.99712" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9999503">
Although preposition errors are frequently com-
mitted in non-native text, we mostly skip the cor-
rection of preposition error. This is because as-
signing prepositions correctly is extremely diffi-
cult, because (1) the preposition used can vary
(e.g., Canada: ‚Äòon the weekend‚Äô vs. Britain ‚Äòat the
weekend‚Äô); (2) in a given location, more than one
preposition may be possible, and the choice af-
fects the meaning (e.g., ‚Äòon the wall‚Äô, vs. ‚Äòat the
wall‚Äô). Verb errors can consist of many multi-
</bodyText>
<page confidence="0.807169">
10
8
6
4
2
0
</page>
<figureCaption confidence="0.996154">
Figure 4. Improvement gap between the original annotation and revised annotation of each team
</figureCaption>
<page confidence="0.995886">
71
</page>
<bodyText confidence="0.99991725">
word errors due to errors of usages of passive and
active voice. (e.g. release‚Äîbe released). Our cur-
rent system cannot correct these multi-words er-
rors, for three reasons. First, if the original exam-
ple consists of one word and the optimal replace-
ment consists of two words, n-gram scores cannot
be applied easily to compare probabilities be-
tween them. Second, the n-gram approach also
fails if the distance between subject and verb is
more than 5. Third, multiply dependent errors are
critical for verb error correction. For example,
noun number, determiner, and subject verb agree-
ment are often dependent upon each other: e.g.
‚ÄúAnd once this happens, privacy does not exist
any more and people&apos;s (life‚Äîlives) (is‚Äîare) un-
der great threaten.‚Äù The correction order will be
important when all error type must be corrected
simultaneously.
Grammar error correction is a challenging
problem. In CoNNL-2013, more than half of the
related teams obtained F-score &lt; 10.0. This low
performance in the grammar error correction can
be explained by several reasons, which indicate
the present limitations of grammar correction sys-
tems.
Among a total of 4206 pairs, we only use small
amount of candidate pairs, 215 pairs are used for
candidate pairs. The other 3991 pairs are dis-
carded in the router training step because these
pairs cannot be corrected by the n-gram approach.
Various classification methods and statistical ma-
chine translation based methods will be investi-
gated in the router-based approach to find the tai-
lored methods for the given word. A demonstra-
tion and progress of our grammar error correction
system is available to the public3.
</bodyText>
<sectionHeader confidence="0.997663" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999993142857143">
We have described the POSTECH grammatical
error correction system. We use the Google N-
gram count corpus to detect spelling errors, punc-
tuation, and comma errors. A rule-based method
is used to correct verb, punctuation, comma errors
and preposition errors. The Google corpus is also
used for an n-gram vector approach and a router-
based approaches. Currently we use the router to
select the best frame. In the future, we will train a
router to select the best method among classifica-
tion, n-gram approach, statistical machine transla-
tion-based method and pattern matching ap-
proaches. A machine learning method will be used
to train the router with various features.
</bodyText>
<sectionHeader confidence="0.990303" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.973871111111111">
This research was supported by the MSIP(The Ministry
of Science, ICT and Future Planning), Korea and Mi-
crosoft Research, under IT/SW Creative research pro-
gram supervised by the NIPA(National IT Industry
Promotion Agency) (NIPA-2013- H0503-13-1006)
and this research was supported by the Basic Science
Research Program through the National Research
Foundation of Korea(NRF) funded by the Ministry of
Education, Science and Technology(2010-0019523).
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.954104631578947">
Han, Na-Rae, Chodorow, Martin, &amp; Leacock, Claudia.
(2006). Detecting errors in English article
usage by non-native speakers.
Alam, Md Jahangir, UzZaman, Naushad, &amp; Khan,
Mumit. (2006). N-gram based statistical
grammar checker for Bangla and English.
Bergsma, Shane, Lin, Dekang, &amp; Goebel, Randy.
(2009). Web-Scale I-gram Models for
Lexical Disambiguation. Paper presented at
the IJCAI.
Brants, Thorsten, &amp; Franz, Alex. (2006). The Google
Web 1T 5-gram corpus version 1.1.
LDC2006T13.
Brockett, Chris, Dolan, William B, &amp; Gamon, Michael.
(2006). Correcting ESL errors using phrasal
SMT techniques. Paper presented at the
Proceedings of the 21st International
Conference on Computational Linguistics and
the 44th annual meeting of the Association for
Computational Linguistics.
Cahill, Aoife, Madnani, Nitin, Tetreault, Joel, &amp;
Napolitano, Diane. (2013). Robust Systems
for Preposition Error Correction Using
Wikipedia Revisions. Paper presented at the
Proceedings of NAACL-HLT.
Dahlmeier, Daniel, &amp; Ng, Hwee Tou. (2011).
Grammatical error correction with
alternating structure optimization. Paper
presented at the Proceedings of the 49th
Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies-Volume 1.
Dahlmeier, Daniel, &amp; Ng, Hwee Tou. (2012). Better
evaluation for grammatical error correction.
Paper presented at the Proceedings of the
2012 Conference of the North American
Chapter of the Association for Computational
Linguistics: Human Language Technologies.
</reference>
<footnote confidence="0.922188">
3 http://isoft.postech.ac.kr/grammar
</footnote>
<page confidence="0.99496">
72
</page>
<reference confidence="0.995428689320388">
Dahlmeier, Daniel, Ng, Hwee Tou, &amp; Wu, Siew Mei.
(2013). Building a large annotated corpus of
learner English: The NUS corpus of learner
English. Paper presented at the Proceedings of
the Eighth Workshop on Innovative Use of
NLP for Building Educational Applications.
De Felice, Rachele. (2008). Automatic error detection
in non-native English. University of Oxford.
De Marneffe, Marie-Catherine, &amp; Manning,
Christopher D. (2008). The Stanford typed
dependencies representation. Paper presented
at the Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-
Domain Parser Evaluation.
Gamon, Michael. (2010). Using mostly native data to
correct errors in learners&apos; writing: a meta-
classifier approach. Paper presented at the
Human Language Technologies: The 2010
Annual Conference of the North American
Chapter of the Association for Computational
Linguistics.
Gamon, Michael, Leacock, Claudia, Brockett, Chris,
Dolan, William B, Gao, Jianfeng, Belenko,
Dmitriy, &amp; Klementiev, Alexandre. (2009).
Using statistical techniques and web search to
correct ESL errors. Calico Journal, 26(3),
491-511.
Hermet, Matthieu, D√©silets, Alain, &amp; Szpakowicz, Stan.
(2008). Using the web as a linguistic resource
to automatically correct lexico-syntactic
errors.
Izumi, Emi, Uchimoto, Kiyotaka, &amp; Isahara, Hitoshi.
(2005). Error annotation for corpus of
Japanese learner English. Paper presented at
the Proceedings of the Sixth International
Workshop on Linguistically Interpreted
Corpora.
Kao, Ting-Hui, Chang, Yu-Wei, Chiu, Hsun-Wen, &amp;
Yen, Tzu-Hsi. (2013). CoNLL-2013 Shared
Task: Grammatical Error Correction NTHU
System Description. CoNLL-2013, 20.
Katz, Slava. (1987). Estimation of probabilities from
sparse data for the language model
component of a speech recognizer. Acoustics,
Speech and Signal Processing, IEEE
Transactions on, 35(3), 400-401.
Knight, Kevin, &amp; Chander, Ishwar. (1994). Automated
postediting of documents. Paper presented at
the AAAI.
Mark, Alla Rozovskaya Kai-Wei Chang, &amp; Roth,
Sammons Dan. (2013). The University of
Illinois System in the CoNLL-2013 Shared
Task. CoNLL-2013, 51, 13.
Naber, Daniel. (2003). A rule-based style and grammar
checker. Diploma Thesis
Nagata, Ryo, Morihiro, Koichiro, Kawai, Atsuo, &amp; Isu,
Naoki. (2006). A feedback-augmented method
for detecting errors in the writing of learners
of English. Paper presented at the Proceedings
of the 21st International Conference on
Computational Linguistics and the 44th
annual meeting of the Association for
Computational Linguistics.
Ng, Hwee Tou , Wu, Siew Mei , Briscoe, Ted ,
Hadiwinoto, Christian , Susanto, Raymond
Hendy, &amp; Bryant, Christopher (2014). The
CoNLL-2014 Shared Task on Grammatical
Error Correction. Paper presented at the the
Eighteenth Conference on Computational
Natural Language Learning: Shared Task
(CoNLL-2014 Shared Task), Baltimore,
Maryland, USA.
Nicholls, Diane. (2003). The Cambridge Learner
Corpus: Error coding and analysis for
lexicography and ELT. Paper presented at the
Proceedings of the Corpus Linguistics 2003
conference.
Rozovskaya, Alla, &amp; Roth, Dan. (2011). Algorithm
selection and model adaptation for ESL
correction tasks. Urbana, 51, 61801.
Seo, Hongsuck, Lee, Jonghoon, Kim, Seokhwan, Lee,
Kyusong, Kang, Sechun, &amp; Lee, Gary
Geunbae. (2012). A meta learning approach
to grammatical error correction. Paper
presented at the Proceedings of the 50th
Annual Meeting of the Association for
Computational Linguistics: Short Papers-
Volume 2.
Wu, Yuanbin, &amp; Ng, Hwee Tou. (2013). Grammatical
error correction using integer linear
programming. Paper presented at the
Proceedings of the 51st Annual Meeting of
the Association for Computational
Linguistics.
Xing, Junwen, Wang, Longyue, Wong, Derek F, Chao,
Lidia S, &amp; Zeng, Xiaodong. (2013). UM-
Checker: A Hybrid System for English
Grammatical Error Cor-rection. CoNLL-2013,
34.
Yannakoudakis, Helen, Briscoe, Ted, &amp; Medlock, Ben.
(2011). A New Dataset and Method for
Automatically Grading ESOL Texts. Paper
presented at the ACL.
</reference>
<page confidence="0.999273">
73
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.638650">
<title confidence="0.907878">Grammatical Error Correction System in the 2014 Shared Task</title>
<author confidence="0.998594">Kyusong Lee</author>
<author confidence="0.998594">Gary Geunbae</author>
<affiliation confidence="0.9999405">Department of Computer Science and Pohang University of Science and</affiliation>
<address confidence="0.852197">Pohang, Korea</address>
<email confidence="0.843493">Kyusonglee@postech.ac.kr</email>
<email confidence="0.843493">gblee@postech.ac.kr</email>
<abstract confidence="0.9990004">This paper describes the POSTECH grammatical error correction system. Various methods are proposed to correct errors such as rule-based, probability n-gram vector approaches and router-based approach. Google N-gram count corpus is used mainly as the correction resource. Correction candidates are extracted from NUCLE training data and each candidate is evaluated with development data to extract high precision rules and n-gram frames. Out of 13 participating teams, our is ranked on both the original and revised annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Na-Rae Han</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Detecting errors in English article usage by non-native speakers.</title>
<date>2006</date>
<contexts>
<context position="1191" citStr="Han et al., 2006" startWordPosition="172" endWordPosition="175">g data and each candidate is evaluated with development data to extract high precision rules and n-gram frames. Out of 13 participating teams, our system is ranked 4th on both the original and revised annotation. 1 Introduction Automatic grammar error correction (GEC) is widely used by learners of English as a second language (ESL) in written tasks. Many methods have been proposed to correct grammatical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>Han, Na-Rae, Chodorow, Martin, &amp; Leacock, Claudia. (2006). Detecting errors in English article usage by non-native speakers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahangir Alam</author>
<author>Naushad UzZaman</author>
<author>Mumit Khan</author>
</authors>
<title>N-gram based statistical grammar checker for Bangla and English.</title>
<date>2006</date>
<contexts>
<context position="1157" citStr="Alam et al., 2006" startWordPosition="166" endWordPosition="169">tes are extracted from NUCLE training data and each candidate is evaluated with development data to extract high precision rules and n-gram frames. Out of 13 participating teams, our system is ranked 4th on both the original and revised annotation. 1 Introduction Automatic grammar error correction (GEC) is widely used by learners of English as a second language (ESL) in written tasks. Many methods have been proposed to correct grammatical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both typ</context>
</contexts>
<marker>Alam, UzZaman, Khan, 2006</marker>
<rawString>Alam, Md Jahangir, UzZaman, Naushad, &amp; Khan, Mumit. (2006). N-gram based statistical grammar checker for Bangla and English.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Dekang Lin</author>
<author>Randy Goebel</author>
</authors>
<title>Web-Scale I-gram Models for Lexical Disambiguation. Paper presented at the IJCAI.</title>
<date>2009</date>
<contexts>
<context position="2300" citStr="Bergsma et al., 2009" startWordPosition="347" endWordPosition="350">ch other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been successfully applied to grammar error correction tasks instead of using error-tagged data (Bergsma et al., 2009; Gamon et al., 2009; Hermet et al., 2008). Especially in the CoNLL-2013 grammar error correction shared task, many of the high-ranked teams (Kao et al., 2013; Mark &amp; Roth, 2013; Xing et al., 2013) exploited the Google Web-1T n-gram corpus. The major advantage of using these webscale corpora is that extremely large quantities of data are publicly available at no additional costs; thus fewer data sparseness problems arise compared to previous approaches based on errortagged corpora. We also use the Google Web-1T n-gram corpus. We extract the candidate pairs (original erroneous text and its corr</context>
<context position="8839" citStr="Bergsma et al., 2009" startWordPosition="1419" endWordPosition="1422">rDet, comma errors, and preposition; we skip insertion correction for other error types because selecting an insertion position is difficult and if every position is selected as insertion position, precision decrease. 4 I-gram Approach We used the following notation. N(o) n-gram vector in original sentence N(r) n-gram vector in replacement sentence n(o)i i th element in N(o) ùëõ (ùëü)ùëñ i th element in N(r) N[i:j] n-gram vector from i th element to j th element 2http://www.languagetool.org Web-scale data have also been used successfully in many other research areas, such as lexical disambiguation (Bergsma et al., 2009). Most NLP systems resolve ambiguities with the help of a large corpus of text, e.g.: ‚Ä¢ The system tried to decide {among, between} the two confusable words. Disambiguation accuracy increases with the size of the corpus. Many systems incorporate the web count into their selection process. For the above example, a typical web-based system would query a search engine with the sequences ‚Äúdecide among the‚Äù and ‚Äúdecide between the‚Äù and select the candidate that returns the most hits. Unfortunately, this approach would fail when disambiguation requires additional context. Bergsma (2009) suggested us</context>
</contexts>
<marker>Bergsma, Lin, Goebel, 2009</marker>
<rawString>Bergsma, Shane, Lin, Dekang, &amp; Goebel, Randy. (2009). Web-Scale I-gram Models for Lexical Disambiguation. Paper presented at the IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>The Google Web 1T 5-gram corpus version 1.1.</title>
<date>2006</date>
<pages>2006--13</pages>
<contexts>
<context position="4606" citStr="Brants &amp; Franz, 2006" startWordPosition="721" endWordPosition="724">early. In 2012, the objective was to correct article and preposition errors; in 2013, it was to correct article, preposition, noun number, verb form, and subject-verb agreement errors. This year, the objective is to correct all errors. Thus, our method should also correct preprocessing and spelling errors. Detailed description of the shared task set up, data set, and evaluation about the CoNLL-2014 Shared Task is explained in (Ng et al., 2014) 2 Data and Recourse The Google Web-1T corpus contains 1012 words of running text and the counts for all 109 five-word sequences that appear &gt; 40 times (Brants &amp; Franz, 2006). We used the NUS Corpus of Learner English (NUCLE) training data to extract the candidate pairs and CoNLL-2013 Official Shard Task test data as development data. We used the Stanford parser (De Marneffe &amp; Manning, 2008) to extract part-of-speech, dependency, and constituency trees. 3 Method 3.1 Overall Process We correct the errors in the following order: Tokenizing ‚Üí spelling error correction ‚Üí punctuation error correction ‚Üí N-gram Vector Approach for Noun number (Nn) ‚Üí Router-based Correction (Deletion Correction ‚Üí Insertion Correction ‚Üí Replacement) for various error types ‚Üí Rule-based met</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Brants, Thorsten, &amp; Franz, Alex. (2006). The Google Web 1T 5-gram corpus version 1.1. LDC2006T13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Brockett</author>
<author>William B Dolan</author>
<author>Michael Gamon</author>
</authors>
<title>Correcting ESL errors using phrasal SMT techniques. Paper presented at the</title>
<date>2006</date>
<booktitle>Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1100" citStr="Brockett et al., 2006" startWordPosition="156" endWordPosition="159">is used mainly as the correction resource. Correction candidates are extracted from NUCLE training data and each candidate is evaluated with development data to extract high precision rules and n-gram frames. Out of 13 participating teams, our system is ranked 4th on both the original and revised annotation. 1 Introduction Automatic grammar error correction (GEC) is widely used by learners of English as a second language (ESL) in written tasks. Many methods have been proposed to correct grammatical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be syn</context>
</contexts>
<marker>Brockett, Dolan, Gamon, 2006</marker>
<rawString>Brockett, Chris, Dolan, William B, &amp; Gamon, Michael. (2006). Correcting ESL errors using phrasal SMT techniques. Paper presented at the Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aoife Cahill</author>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Diane Napolitano</author>
</authors>
<title>Robust Systems for Preposition Error Correction Using Wikipedia Revisions. Paper presented at the</title>
<date>2013</date>
<booktitle>Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="1365" citStr="Cahill et al., 2013" startWordPosition="202" endWordPosition="205">both the original and revised annotation. 1 Introduction Automatic grammar error correction (GEC) is widely used by learners of English as a second language (ESL) in written tasks. Many methods have been proposed to correct grammatical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et</context>
</contexts>
<marker>Cahill, Madnani, Tetreault, Napolitano, 2013</marker>
<rawString>Cahill, Aoife, Madnani, Nitin, Tetreault, Joel, &amp; Napolitano, Diane. (2013). Robust Systems for Preposition Error Correction Using Wikipedia Revisions. Paper presented at the Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Grammatical error correction with alternating structure optimization. Paper presented at the</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1.</booktitle>
<contexts>
<context position="1814" citStr="Dahlmeier &amp; Ng, 2011" startWordPosition="274" endWordPosition="277">De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been successfully applied to grammar error correction tasks instead of using error-tagged data (Bergsma et al., 2009; Gamon et al., 2009; Hermet et al., 2008). Especially in the CoNLL-2013 grammar error correction shared task, man</context>
</contexts>
<marker>Dahlmeier, Ng, 2011</marker>
<rawString>Dahlmeier, Daniel, &amp; Ng, Hwee Tou. (2011). Grammatical error correction with alternating structure optimization. Paper presented at the Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation for grammatical error correction. Paper presented at the</title>
<date>2012</date>
<booktitle>Proceedings of the 2012 Conference of the North American Chapter of</booktitle>
<contexts>
<context position="20958" citStr="Dahlmeier &amp; Ng, 2012" startWordPosition="3453" endWordPosition="3456">sion Recall F0.5 N-gram (Nn) 31.0 6.55 17.75 42.28 9.0 24.31 Rule (Verb) 28.95 1.12 4.86 31.17 1.29 5.52 Rule (Mec) 49.34 5.47 18.94 52.16 6.17 20.93 Router (Others) 28.11 12.49 22.49 35.29 15.45 28.08 All 34.51 21.73 30.88 41.28 25.59 36.77 The documents are a subset of the NUS Corpus of Learner English (NUCLE). We use the MaxMatch (M2) scorer provided by the CoNLL-2014 Shared Task. The M2 scorer works by using the set that maximally matches the set of gold-standard edits specified by the annotator as being equal to the set of system edits that are automatically computed and used in scoring (Dahlmeier &amp; Ng, 2012). The official evaluation metric is F0.5, weighting precision twice as much as recall. We achieve F0.5 of 30.88; precision of 34.51; recall of 21.73 in the original annotation (Table 4). After original official annotations announced by organizers (i.e., only based on the annotations of the two annotators), another set of annotations is offered based on including the additional answers proposed by the 3 teams (CAMB, CUUI, UMC). The improvement gap between the original annotation and the revised annotation of our team (POST) is 5.89%. We obtain the highest improvement rate except for the 3 propo</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Dahlmeier, Daniel, &amp; Ng, Hwee Tou. (2012). Better evaluation for grammatical error correction. Paper presented at the Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a large annotated corpus of learner English: The NUS corpus of learner English.</title>
<date>2013</date>
<booktitle>Paper presented at the Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.</booktitle>
<contexts>
<context position="1572" citStr="Dahlmeier et al., 2013" startWordPosition="237" endWordPosition="240">proposed to correct grammatical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed c</context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Dahlmeier, Daniel, Ng, Hwee Tou, &amp; Wu, Siew Mei. (2013). Building a large annotated corpus of learner English: The NUS corpus of learner English. Paper presented at the Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rachele De Felice</author>
</authors>
<title>Automatic error detection in non-native English.</title>
<date>2008</date>
<institution>University of Oxford.</institution>
<marker>De Felice, 2008</marker>
<rawString>De Felice, Rachele. (2008). Automatic error detection in non-native English. University of Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation. Paper presented at the Coling</title>
<date>2008</date>
<booktitle>Proceedings of the workshop on Cross-Framework and CrossDomain Parser Evaluation.</booktitle>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>De Marneffe, Marie-Catherine, &amp; Manning, Christopher D. (2008). The Stanford typed dependencies representation. Paper presented at the Coling 2008: Proceedings of the workshop on Cross-Framework and CrossDomain Parser Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>Using mostly native data to correct errors in learners&apos; writing: a metaclassifier approach. Paper presented at the Human Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2147" citStr="Gamon, 2010" startWordPosition="327" endWordPosition="328">earner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been successfully applied to grammar error correction tasks instead of using error-tagged data (Bergsma et al., 2009; Gamon et al., 2009; Hermet et al., 2008). Especially in the CoNLL-2013 grammar error correction shared task, many of the high-ranked teams (Kao et al., 2013; Mark &amp; Roth, 2013; Xing et al., 2013) exploited the Google Web-1T n-gram corpus. The major advantage of using these webscale corpora is that extremely large quantities of data are publicly available at no additional costs; thus fewer data sparseness problems arise compared to previous a</context>
</contexts>
<marker>Gamon, 2010</marker>
<rawString>Gamon, Michael. (2010). Using mostly native data to correct errors in learners&apos; writing: a metaclassifier approach. Paper presented at the Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
<author>Claudia Leacock</author>
<author>Chris Brockett</author>
<author>William B Dolan</author>
<author>Jianfeng Gao</author>
<author>Dmitriy Belenko</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Using statistical techniques and web search to correct ESL errors.</title>
<date>2009</date>
<journal>Calico Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>491--511</pages>
<contexts>
<context position="2320" citStr="Gamon et al., 2009" startWordPosition="351" endWordPosition="354">gistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been successfully applied to grammar error correction tasks instead of using error-tagged data (Bergsma et al., 2009; Gamon et al., 2009; Hermet et al., 2008). Especially in the CoNLL-2013 grammar error correction shared task, many of the high-ranked teams (Kao et al., 2013; Mark &amp; Roth, 2013; Xing et al., 2013) exploited the Google Web-1T n-gram corpus. The major advantage of using these webscale corpora is that extremely large quantities of data are publicly available at no additional costs; thus fewer data sparseness problems arise compared to previous approaches based on errortagged corpora. We also use the Google Web-1T n-gram corpus. We extract the candidate pairs (original erroneous text and its correction) from NUCLE t</context>
</contexts>
<marker>Gamon, Leacock, Brockett, Dolan, Gao, Belenko, Klementiev, 2009</marker>
<rawString>Gamon, Michael, Leacock, Claudia, Brockett, Chris, Dolan, William B, Gao, Jianfeng, Belenko, Dmitriy, &amp; Klementiev, Alexandre. (2009). Using statistical techniques and web search to correct ESL errors. Calico Journal, 26(3), 491-511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Hermet</author>
<author>Alain D√©silets</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Using the web as a linguistic resource to automatically correct lexico-syntactic errors.</title>
<date>2008</date>
<contexts>
<context position="2342" citStr="Hermet et al., 2008" startWordPosition="355" endWordPosition="358"> Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been successfully applied to grammar error correction tasks instead of using error-tagged data (Bergsma et al., 2009; Gamon et al., 2009; Hermet et al., 2008). Especially in the CoNLL-2013 grammar error correction shared task, many of the high-ranked teams (Kao et al., 2013; Mark &amp; Roth, 2013; Xing et al., 2013) exploited the Google Web-1T n-gram corpus. The major advantage of using these webscale corpora is that extremely large quantities of data are publicly available at no additional costs; thus fewer data sparseness problems arise compared to previous approaches based on errortagged corpora. We also use the Google Web-1T n-gram corpus. We extract the candidate pairs (original erroneous text and its correction) from NUCLE training data. We use a</context>
</contexts>
<marker>Hermet, D√©silets, Szpakowicz, 2008</marker>
<rawString>Hermet, Matthieu, D√©silets, Alain, &amp; Szpakowicz, Stan. (2008). Using the web as a linguistic resource to automatically correct lexico-syntactic errors.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emi Izumi</author>
<author>Kiyotaka Uchimoto</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Error annotation for corpus of Japanese learner English.</title>
<date>2005</date>
<booktitle>Paper presented at the Proceedings of the Sixth International Workshop on Linguistically Interpreted Corpora.</booktitle>
<contexts>
<context position="1592" citStr="Izumi et al., 2005" startWordPosition="241" endWordPosition="244">matical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been suc</context>
</contexts>
<marker>Izumi, Uchimoto, Isahara, 2005</marker>
<rawString>Izumi, Emi, Uchimoto, Kiyotaka, &amp; Isahara, Hitoshi. (2005). Error annotation for corpus of Japanese learner English. Paper presented at the Proceedings of the Sixth International Workshop on Linguistically Interpreted Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting-Hui Kao</author>
<author>Yu-Wei Chang</author>
<author>Hsun-Wen Chiu</author>
<author>Tzu-Hsi Yen</author>
</authors>
<date>2013</date>
<booktitle>CoNLL-2013 Shared Task: Grammatical Error Correction NTHU System Description. CoNLL-2013,</booktitle>
<pages>20</pages>
<contexts>
<context position="2458" citStr="Kao et al., 2013" startWordPosition="375" endWordPosition="378">ation method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been successfully applied to grammar error correction tasks instead of using error-tagged data (Bergsma et al., 2009; Gamon et al., 2009; Hermet et al., 2008). Especially in the CoNLL-2013 grammar error correction shared task, many of the high-ranked teams (Kao et al., 2013; Mark &amp; Roth, 2013; Xing et al., 2013) exploited the Google Web-1T n-gram corpus. The major advantage of using these webscale corpora is that extremely large quantities of data are publicly available at no additional costs; thus fewer data sparseness problems arise compared to previous approaches based on errortagged corpora. We also use the Google Web-1T n-gram corpus. We extract the candidate pairs (original erroneous text and its correction) from NUCLE training data. We use a router to choose the best frame to compare the n-gram score difference between the original and replacement in a gi</context>
<context position="10144" citStr="Kao et al. (2013)" startWordPosition="1628" endWordPosition="1631"> the example sentence, the following 5-gram patterns can be extracted: ‚Ä¢ system tried to decide {among, between} ‚Ä¢ tried to decide {among, between} the ‚Ä¢ to decide {among, between} the two ‚Ä¢ decide {among, between} the two confusable ‚Ä¢ {among, between} the two confusable words Similarly, four 4-gram patterns, three 3-gram patterns and two 2-gram patterns are extracted by spanning the target. A score for each pattern is calculated by summing the log-counts. This method was successfully applied in lexical disambiguation. Web-scale data were used with the count information specified as features. Kao et al. (2013) used a ‚Äúmoving window (MW)‚Äù : ùëÄùëäùëñ,ùëò (w) = {ùë§ùëñ‚àíùëó, ... , ùë§ùëñ‚àíùëó+(ùëò‚àí1), J = 0, ùëò ‚àí 1} (1) where ùëñ denotes the position of the word, k the window size and w the original or replacement word at position ùëñ. The window size is set to 2 to 5 words. MW is the same concept as the SUMLM: Sùëñ,ùëò (ùë§) = E cùëúuùëõt(ùëõùëîùëüùëéùëö) (2) ùëõùëîùëüùëéùëöeùëÄùëäùëò(ùë§) Both approaches apply the sum of all MWs in (1). Our approach is based on the MW method. The difference is that instead of summing all the MWs, we consider only one best MW which is referred to here as a frame. The following sentences 67 demonstrate the case when the following wo</context>
</contexts>
<marker>Kao, Chang, Chiu, Yen, 2013</marker>
<rawString>Kao, Ting-Hui, Chang, Yu-Wei, Chiu, Hsun-Wen, &amp; Yen, Tzu-Hsi. (2013). CoNLL-2013 Shared Task: Grammatical Error Correction NTHU System Description. CoNLL-2013, 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing,</title>
<date>1987</date>
<journal>IEEE Transactions on,</journal>
<volume>35</volume>
<issue>3</issue>
<pages>400--401</pages>
<contexts>
<context position="16369" citStr="Katz, 1987" startWordPosition="2698" endWordPosition="2699">i+,|Wi+2Wi+3Wi+4) backward 4-GRAM n2o = P (Wi |Wi+,Wi+2Wi+3) backward n2, = P(Wi|Wi‚àí3Wi‚àí2Wi‚àí,) forward 3-GRAM n32 = P (Wi |Wi+,Wi+2) backward n33 = P(Wi|Wi‚àí2Wi‚àí,) forward n34 = P(WiWi+,|Wi+2) backward n35 = P(Wi‚àí,Wi|Wi‚àí2) forward n36 = P(Wi‚àí,Wi|Wi+,) backward n37 = P(WiWi+,|Wi‚àí,) forward 2-GRAM n38 = P(Wi |Wi+,) backward n39 = P(Wi|Wi‚àí,) forward Figure 3. Overall process of Nn Correction gram, and two 2-gram. Additionally, the elements of the n-gram vector are detailed in Table 2. Back-Off Model: A high-order n-gram is more effective than a low-order n-gram. Thus, we applied back-off methods (Katz, 1987) to assign higher priority to higher order probabilities. If all elements in 5-gram vectors are 0 for both the original and candidate sentence, which means Zi=o{n(o)i + n(r)i} = 0, we consider 4-gram vectors (N[20:31]). If 4-gram vectors are 0, we consider 3-gram vectors. Moreover, when the proposed method calculates each of the forward, backward and two-way probabilities, the back-off method is used to get each score. Correction: Here, we explain the process of error correction using n-gram vectors. First, we generate Nn error candidates. Second, we construct the n-gram probability vector for</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, Slava. (1987). Estimation of probabilities from sparse data for the language model component of a speech recognizer. Acoustics, Speech and Signal Processing, IEEE Transactions on, 35(3), 400-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Ishwar Chander</author>
</authors>
<title>Automated postediting of documents. Paper presented at the AAAI.</title>
<date>1994</date>
<contexts>
<context position="1232" citStr="Knight &amp; Chander, 1994" startWordPosition="179" endWordPosition="182">ted with development data to extract high precision rules and n-gram frames. Out of 13 participating teams, our system is ranked 4th on both the original and revised annotation. 1 Introduction Automatic grammar error correction (GEC) is widely used by learners of English as a second language (ESL) in written tasks. Many methods have been proposed to correct grammatical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta</context>
</contexts>
<marker>Knight, Chander, 1994</marker>
<rawString>Knight, Kevin, &amp; Chander, Ishwar. (1994). Automated postediting of documents. Paper presented at the AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya Kai-Wei Chang Mark</author>
<author>Sammons Dan Roth</author>
</authors>
<date>2013</date>
<booktitle>The University of Illinois System in the CoNLL-2013 Shared Task. CoNLL-2013, 51,</booktitle>
<pages>13</pages>
<contexts>
<context position="2477" citStr="Mark &amp; Roth, 2013" startWordPosition="379" endWordPosition="382"> several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been successfully applied to grammar error correction tasks instead of using error-tagged data (Bergsma et al., 2009; Gamon et al., 2009; Hermet et al., 2008). Especially in the CoNLL-2013 grammar error correction shared task, many of the high-ranked teams (Kao et al., 2013; Mark &amp; Roth, 2013; Xing et al., 2013) exploited the Google Web-1T n-gram corpus. The major advantage of using these webscale corpora is that extremely large quantities of data are publicly available at no additional costs; thus fewer data sparseness problems arise compared to previous approaches based on errortagged corpora. We also use the Google Web-1T n-gram corpus. We extract the candidate pairs (original erroneous text and its correction) from NUCLE training data. We use a router to choose the best frame to compare the n-gram score difference between the original and replacement in a given candidate pair.</context>
</contexts>
<marker>Mark, Roth, 2013</marker>
<rawString>Mark, Alla Rozovskaya Kai-Wei Chang, &amp; Roth, Sammons Dan. (2013). The University of Illinois System in the CoNLL-2013 Shared Task. CoNLL-2013, 51, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Naber</author>
</authors>
<title>A rule-based style and grammar checker. Diploma Thesis</title>
<date>2003</date>
<contexts>
<context position="1040" citStr="Naber, 2003" startWordPosition="150" endWordPosition="151">router-based approach. Google N-gram count corpus is used mainly as the correction resource. Correction candidates are extracted from NUCLE training data and each candidate is evaluated with development data to extract high precision rules and n-gram frames. Out of 13 participating teams, our system is ranked 4th on both the original and revised annotation. 1 Introduction Automatic grammar error correction (GEC) is widely used by learners of English as a second language (ESL) in written tasks. Many methods have been proposed to correct grammatical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtag</context>
</contexts>
<marker>Naber, 2003</marker>
<rawString>Naber, Daniel. (2003). A rule-based style and grammar checker. Diploma Thesis</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryo Nagata</author>
<author>Koichiro Morihiro</author>
<author>Atsuo Kawai</author>
<author>Naoki Isu</author>
</authors>
<title>A feedback-augmented method for detecting errors in the writing of learners of English.</title>
<date>2006</date>
<booktitle>Paper presented at the Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1254" citStr="Nagata et al., 2006" startWordPosition="183" endWordPosition="186">a to extract high precision rules and n-gram frames. Out of 13 participating teams, our system is ranked 4th on both the original and revised annotation. 1 Introduction Automatic grammar error correction (GEC) is widely used by learners of English as a second language (ESL) in written tasks. Many methods have been proposed to correct grammatical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method</context>
</contexts>
<marker>Nagata, Morihiro, Kawai, Isu, 2006</marker>
<rawString>Nagata, Ryo, Morihiro, Koichiro, Kawai, Atsuo, &amp; Isu, Naoki. (2006). A feedback-augmented method for detecting errors in the writing of learners of English. Paper presented at the Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Briscoe</author>
</authors>
<date>2014</date>
<booktitle>The CoNLL-2014 Shared Task on Grammatical Error Correction. Paper presented at the the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task),</booktitle>
<location>Baltimore, Maryland, USA.</location>
<marker>Briscoe, 2014</marker>
<rawString>Ng, Hwee Tou , Wu, Siew Mei , Briscoe, Ted , Hadiwinoto, Christian , Susanto, Raymond Hendy, &amp; Bryant, Christopher (2014). The CoNLL-2014 Shared Task on Grammatical Error Correction. Paper presented at the the Eighteenth Conference on Computational Natural Language Learning: Shared Task (CoNLL-2014 Shared Task), Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane Nicholls</author>
</authors>
<title>The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT.</title>
<date>2003</date>
<booktitle>Paper presented at the Proceedings of the Corpus Linguistics</booktitle>
<pages>conference.</pages>
<contexts>
<context position="1609" citStr="Nicholls, 2003" startWordPosition="245" endWordPosition="246">e include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been successfully applied</context>
</contexts>
<marker>Nicholls, 2003</marker>
<rawString>Nicholls, Diane. (2003). The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT. Paper presented at the Proceedings of the Corpus Linguistics 2003 conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Algorithm selection and model adaptation for ESL correction tasks.</title>
<date>2011</date>
<journal>Urbana,</journal>
<volume>51</volume>
<pages>61801</pages>
<contexts>
<context position="1390" citStr="Rozovskaya &amp; Roth, 2011" startWordPosition="206" endWordPosition="209"> revised annotation. 1 Introduction Automatic grammar error correction (GEC) is widely used by learners of English as a second language (ESL) in written tasks. Many methods have been proposed to correct grammatical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-class</context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>Rozovskaya, Alla, &amp; Roth, Dan. (2011). Algorithm selection and model adaptation for ESL correction tasks. Urbana, 51, 61801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongsuck Seo</author>
<author>Jonghoon Lee</author>
<author>Seokhwan Kim</author>
<author>Kyusong Lee</author>
<author>Sechun Kang</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>A meta learning approach to grammatical error correction.</title>
<date>2012</date>
<booktitle>Paper presented at the Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short PapersVolume</booktitle>
<volume>2</volume>
<contexts>
<context position="1976" citStr="Seo et al., 2012" startWordPosition="300" endWordPosition="303">, 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been successfully applied to grammar error correction tasks instead of using error-tagged data (Bergsma et al., 2009; Gamon et al., 2009; Hermet et al., 2008). Especially in the CoNLL-2013 grammar error correction shared task, many of the high-ranked teams (Kao et al., 2013; Mark &amp; Roth, 2013; Xing et al., 2013) exploited the Google Web-1T n-gram corpus. The major advantage of using these </context>
</contexts>
<marker>Seo, Lee, Kim, Lee, Kang, Lee, 2012</marker>
<rawString>Seo, Hongsuck, Lee, Jonghoon, Kim, Seokhwan, Lee, Kyusong, Kang, Sechun, &amp; Lee, Gary Geunbae. (2012). A meta learning approach to grammatical error correction. Paper presented at the Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short PapersVolume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanbin Wu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Grammatical error correction using integer linear programming.</title>
<date>2013</date>
<booktitle>Paper presented at the Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1406" citStr="Wu &amp; Ng, 2013" startWordPosition="210" endWordPosition="213">troduction Automatic grammar error correction (GEC) is widely used by learners of English as a second language (ESL) in written tasks. Many methods have been proposed to correct grammatical errors; these include methods based on rules (Naber, 2003), on statistical machine translation (Brockett et al., 2006), on machine learning, and on n-grams (Alam et al., 2006). Early research (Han et al., 2006; De Felice, 2008; Knight &amp; Chander, 1994; Nagata et al., 2006) on error correction for nonnative text was based on well-formed corpora. Most recent work (Cahill et al., 2013; Rozovskaya &amp; Roth, 2011; Wu &amp; Ng, 2013) has used machine learning methods that rely on a GEtagged corpus such as NUCLE, Japanese English Learner corpus, and Cambridge Learner Corpus (Dahlmeier et al., 2013; Izumi et al., 2005; Nicholls, 2003), because well-formed and GEtagged approaches are closely related to each other, can be synergistically combined. Therefore, research using both types of data has also been conducted (Dahlmeier &amp; Ng, 2011). Moreover, a meta-classification method using several GEtagged corpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach h</context>
</contexts>
<marker>Wu, Ng, 2013</marker>
<rawString>Wu, Yuanbin, &amp; Ng, Hwee Tou. (2013). Grammatical error correction using integer linear programming. Paper presented at the Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junwen Xing</author>
<author>Longyue Wang</author>
<author>Derek F Wong</author>
<author>Lidia S Chao</author>
<author>Xiaodong Zeng</author>
</authors>
<title>UMChecker: A Hybrid System for English Grammatical Error Cor-rection.</title>
<date>2013</date>
<booktitle>CoNLL-2013,</booktitle>
<pages>34</pages>
<contexts>
<context position="2497" citStr="Xing et al., 2013" startWordPosition="383" endWordPosition="386">orpora and a native corpus has been proposed to correct the grammatical errors (Seo et al., 2012). A meta-classifier approach has been proposed to combine a language model and error-specific classification for correction of article and preposition errors (Gamon, 2010). Web-scale well-formed corpora have been successfully applied to grammar error correction tasks instead of using error-tagged data (Bergsma et al., 2009; Gamon et al., 2009; Hermet et al., 2008). Especially in the CoNLL-2013 grammar error correction shared task, many of the high-ranked teams (Kao et al., 2013; Mark &amp; Roth, 2013; Xing et al., 2013) exploited the Google Web-1T n-gram corpus. The major advantage of using these webscale corpora is that extremely large quantities of data are publicly available at no additional costs; thus fewer data sparseness problems arise compared to previous approaches based on errortagged corpora. We also use the Google Web-1T n-gram corpus. We extract the candidate pairs (original erroneous text and its correction) from NUCLE training data. We use a router to choose the best frame to compare the n-gram score difference between the original and replacement in a given candidate pair. The intuition of ou</context>
</contexts>
<marker>Xing, Wang, Wong, Chao, Zeng, 2013</marker>
<rawString>Xing, Junwen, Wang, Longyue, Wong, Derek F, Chao, Lidia S, &amp; Zeng, Xiaodong. (2013). UMChecker: A Hybrid System for English Grammatical Error Cor-rection. CoNLL-2013, 34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen Yannakoudakis</author>
<author>Ted Briscoe</author>
<author>Ben Medlock</author>
</authors>
<title>A New Dataset and Method for Automatically Grading ESOL Texts. Paper presented at the ACL.</title>
<date>2011</date>
<marker>Yannakoudakis, Briscoe, Medlock, 2011</marker>
<rawString>Yannakoudakis, Helen, Briscoe, Ted, &amp; Medlock, Ben. (2011). A New Dataset and Method for Automatically Grading ESOL Texts. Paper presented at the ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002134">
<title confidence="0.927019">
Coloring Objects: Adjective-Noun Visual Semantic Compositionality
</title>
<author confidence="0.56109">
Dat Tien Nguyen(1,2) Angeliki Lazaridou(2) Raffaella Bernardi(2)
</author>
<address confidence="0.487677">
(1)EM LCT, (2)University of Trento/ Italy
</address>
<email confidence="0.959825">
name.surname@unitn.it
</email>
<sectionHeader confidence="0.992891" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999636">
This paper reports preliminary experiments aiming at verifying the conjecture that semantic com-
positionality is a general process irrespective of the underlying modality. In particular, we model
compositionality of an attribute with an object in the visual modality as done in the case of an ad-
jective with a noun in the linguistic modality. Our experiments show that the concept topologies
in the two modalities share similarities, results that strengthen our conjecture.
</bodyText>
<sectionHeader confidence="0.904369" genericHeader="categories and subject descriptors">
1 Language and Vision
</sectionHeader>
<bodyText confidence="0.9996728">
Recently, fields like computational linguistics and computer vision have converged to a common way of
capturing and representing the linguistic and visual information of atomic concepts, through vector space
models. At the same time, advances in computational semantics have lead to effective and linguistically
inspired approaches of extending such methods from single concepts to arbitrary linguistic units (e.g.
phrases), through means of vector-based semantic composition (Mitchell and Lapata, 2010).
Compositionality is not to be considered only an important component from a linguistic perspective,
but also from a cognitive perspective and there has been efforts to validate it as a general cognitive
process. However, in computer vision so far compositionality has received limited attention. Thus, in
this work, we study the phenomenon of visual compositionality and we complement limited previous
literature that has focused on event compositionality (St¨ottinger et al., 2012) or general image struc-
ture (Socher et al., 2011), by studying models of attribute-object semantic composition.
In a nutshell, our work consists of learning vector representations of attribute-object (e.g., “red car”,
“cute dog” etc.) and objects (e.g., “car”, “dog”, “truck”, “cat” etc.) and by using those compute the
representation of new objects having similar attributes (“red truck”, “cute cat” etc.). This question has
both theoretical and applied impact. The possibility of developing a visual compositional model of
attribute-object, on the one hand, could shed light on the acquisition of such ability in humans; how we
learn attribute representation and compose them with different objects is still an open question within the
cognitive science community (Mintz and Gleitman, 2002). On the other hand, computer vision systems
could become generative and be able to recognize unseen attribute-object combinations, a component
especially useful for object recognition and image retrieval.
</bodyText>
<sectionHeader confidence="0.988616" genericHeader="method">
2 Visual Compositional Model
</sectionHeader>
<bodyText confidence="0.941919857142857">
As our source of inspiration regarding the type of compositionality, we use the Lexical Functional model
(LF) (Baroni and Zamparelli, 2010), under which adjectives, in linguistic compositionality, are repre-
sented as linear functions (i.e., matrix of weights). Concretely, each adjective function fWadj is induced
from corpus-observed vectors of adjective-noun phrases wi E Wphrase and noun wj E Wnoun, e.g.,
((wred car, wcar), (wred flag, wflag), . . .), by solving the least-squares regression problem:
arg min ||Wphrase − fWadjWnoun||
fWadj∈Rd×d
</bodyText>
<footnote confidence="0.842962">
This work is licensed under a Creative Commons Attribution 4.0 International Licence. License details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<page confidence="0.916202">
112
</page>
<note confidence="0.753006">
Proceedings of the 25th International Conference on Computational Linguistics, pages 112–114,
Dublin, Ireland, August 23-29 2014.
</note>
<bodyText confidence="0.9901214">
In this work, we propose to import the LF method in the visual modality, aiming at develop-
ing a Visual Compositional Model. Similarly to the case of linguistic compositionality, each at-
tribute function fVattr is induced from image-harvested vector representations of attribute-object vi E
Vphrase and object vj E Vobject, e.g. for training the function fVred the following data can be used
((vred car, vcar), (vred flag, vflag), . . .).
</bodyText>
<sectionHeader confidence="0.998762" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999974333333333">
The visual representations of attribute-objects and objects are created with the PHOW-color fea-
tures (Bosch et al., 2007) and SIFT color-agnostic features (Lowe, 2004) respectively. The linguistic
representations for the adjective-noun Wphrase and noun Wnoun are built with the word2vec toolkit1
using a corpus of 3 billion tokens.2 Both visual and linguistic representations consist of 300 dimensions.
In this work, we focus on attributes related to 10 colors (Russakovsky and Fei-Fei, 2012) for a
total number of 9699 images depicting 202 unique objects/nouns and 886 unique phrases (attribute-
object/adjective-noun). Our experiments are conducted with aggregated attribute-object representations
obtained by summing the visual vectors extracted from images representing the same attribute-object,
The same pipeline is followed for the objects to obtain aggregated object vectors.
This work aims at comparing the behavior of the semantically-driven compositionality process across
the two modalities. For this reason, we report results on the intersection of Vphrase and Wphrase, a
process that results in 266 attribute-object/adjective-noun items. Furthermore, although the training data
for the two modalities are different, the size of the training data is identical, i.e., the fVattr is trained using
the remaining 620 attribute-object items, whereas for the fWadj, we randomly sample 620 adjective-noun
items from the language space.
</bodyText>
<subsectionHeader confidence="0.999941">
3.1 Analysis of Language and Visual Semantic Spaces
</subsectionHeader>
<bodyText confidence="0.999887">
This experiment aims at assessing the degree to which language and vision share commonalities. To this
end, we compute the cosine similarities between all possible combination of objects (resp., nouns) and
perform a correlation analysis of the similarity of the corresponding pairs in the two lists resulting in 0.45
Spearman correlation – e.g., we correlate the similarity between vcat and vdog with that between wcat and
wdog. For instance, “goat” and “sheep” are highly similar in both spaces, whereas “whale” and “bird”
are similar only linguistically, whereas “blackboard” and “chair” are similar only visually. The same
experiment is performed between all possible combinations of attribute-object/adjective-noun items, e.g.
we correlate the similarity between vwhite cat and vblack dog with that between wwhite cat and wblack dog,
resulting in 0.33 Spearman correlation (see Table 1).
Overall, our results suggest that the topologies of the semantic spaces are similar in the two modalities.
Furthermore, since this phenomenon is also apparent in the cases of attribute-object and adjective-noun
pairs, this alludes to the possibility of transferring approaches of semantic compositionality from the
linguistic to the visual modality.
</bodyText>
<tableCaption confidence="0.986301">
Table 1: Similar and dissimilar concepts in the language and vision space.
</tableCaption>
<subsectionHeader confidence="0.999141">
3.2 Semantically-driven composition for attribute-object representations
</subsectionHeader>
<bodyText confidence="0.996296666666667">
The findings of the previous experiment suggest a high correlation between the visual attribute-attribute
representations and the corpus-harvested adjective-noun representations. An interesting question that
arises is whether we could approximate such visual representations of complex visual units, similarly to
</bodyText>
<footnote confidence="0.998951">
1https://code.google.com/p/word2vec/
2http://wacky.sslmit.unibo.it, http://www.natcorp.ox.ac.uk
</footnote>
<figure confidence="0.490237333333333">
High Visual Low Visual
High Linguistic goat-sheep, jaguar- lion baboon-transporter, bird-whale
black bag - brown bag, brown bear - yellow dog blue grass - blue van, gray whale - white deer
Low Linguistic ball-horse, blackboard-chair baboon-sofa, backboard-panda
red strawberry - white ball, white bear - yellow dog
black bag - green bridge, green table - yellow stick
</figure>
<page confidence="0.998243">
113
</page>
<bodyText confidence="0.931923142857143">
how is done in Computational Linguistics for approximating the text-based representations of adjective-
noun phrases. Thus, this experiment is designed in order to assess the validity of the semantically-driven
compositionality approach in the visual domain. Results are reported in Table 2. Since we expect that
the quality of the aggregated vectors depends on the numbers of available images, we report results for
subsets of the original data set that differ on the number of images per phrase.
By means of the LF composition method sketched in Section 2, we obtain the compositional represen-
tations of attribute-object (V comp
phrase) and adjective-noun (Wcomp
phrase) items. We then perform the correlation
analyses between the similarities obtained in the composed visual space V comp
phrase with: 1) the equiva-
lent image-harvested representations Vphrase,
Wphrase, 3) the equivalent compositionally-derived linguistic representations Wcomp
phrase.
Overall, the correlation between V comp
space and Vspace suggests that the visual compositionality of
attribute-object can account, to some extend, for the visual semantics of the respective image, and it
further improves with the number of images we consider for obtaining the aggregated vectors of the vi-
sual phrases. Finally, as expected, the correlations between V comp
space although lower than the ones reported
in Section 3.1, i.e., 0.22 vs 0.32, are still non negligible.
</bodyText>
<table confidence="0.99499525">
all phrases &gt; 10 images &gt; 20 images &gt; 30 images
Vcomp 0.24 0.40 0.53 0.58
phrase - Vphrase
VVcomp 0.10 0.22 0.19 0.23
phrase - Wphrase 0.04 0.05 0.18 0.10
comp
phrase - W comp
phrase
</table>
<tableCaption confidence="0.999593">
Table 2: Spearman correlations between the similarities in the V comp
</tableCaption>
<bodyText confidence="0.80568">
phrase and other semantic spaces.
</bodyText>
<sectionHeader confidence="0.999534" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999944625">
In this work, we have experimented with semantically-driven compositionality of attributes with objects
in the visual modality, by adopting an out-of-the-box composition method from the computational se-
mantics literature. Our preliminary results have shown that the visual representations of attribute-objects
when obtained compositionally reflect properties similar not only to the ones found in representations
harvested automatically from images, but also from those extracted from text corpora. These results
show that semantic compositionality might be a general process irrespective of the underlying modality.
We have just scratched the surface on this topic and in the future we plan to experiment with a larger
variety of attributes and use and design alternative visual compositional models.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999875666666667">
The second and third author acknowledge ERC 2011 Starting Independent Research Grant n. 283554
(COMPOSES). We thank the 3 anonymous reviewers for their comments, Marco Baroni and Elia Bruni
for their constant and useful feedback.
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.948789875">
[Baroni and Zamparelli2010] Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are
matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, 1183–1193.
[Bosch et al.2007] Anna Bosch, Andrew Zisserman, and Xavier Munoz. 2007. Image classification using random
forests and ferns. In Proceedings of ICCV, 1–8.
[Lowe2004] David G Lowe. 2004. Distinctive image features from scale-invariant keypoints. International
Journal of Computer Vision, 60:91–110.
[Mintz and Gleitman2002] Toben H. Mintz and Lila R. Gleitman. 2002. Adjectives really do modify nouns: the
incremental and restricted nature of early adjective acquisition. Cognition, 84:267–293.
[Mitchell and Lapata2010] Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of
semantics. Cognitive Science, 34(8):1388–1429.
[Russakovsky and Fei-Fei2012] Olga Russakovsky and Li Fei-Fei. 2012. Attribute learning in large-scale datasets.
In Trends and Topics in Computer Vision, 1–14. Springer.
[Socher et al.2011] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes
and natural language with recursive neural networks. In Proceedings of ICML, 129–136.
[St¨ottinger et al.2012] J. St¨ottinger, J.R.R. Uijlings, A.K. Pandey, N. Sebe, and F. Giunchiglia. 2012. (unseen)
event recognition via semantic compositionality. In CVPR.
</reference>
<figure confidence="0.266862">
2) the equivalent corpus-derived linguistic representations
</figure>
<page confidence="0.979313">
114
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.120927">
<title confidence="0.999646">Coloring Objects: Adjective-Noun Visual Semantic Compositionality</title>
<author confidence="0.998626">Tien Angeliki Raffaella</author>
<affiliation confidence="0.991965">LCT, of Trento/ Italy</affiliation>
<email confidence="0.995132">name.surname@unitn.it</email>
<abstract confidence="0.992831882352941">This paper reports preliminary experiments aiming at verifying the conjecture that semantic compositionality is a general process irrespective of the underlying modality. In particular, we model compositionality of an attribute with an object in the visual modality as done in the case of an adjective with a noun in the linguistic modality. Our experiments show that the concept topologies in the two modalities share similarities, results that strengthen our conjecture. 1 Language and Vision Recently, fields like computational linguistics and computer vision have converged to a common way of capturing and representing the linguistic and visual information of atomic concepts, through vector space models. At the same time, advances in computational semantics have lead to effective and linguistically inspired approaches of extending such methods from single concepts to arbitrary linguistic units (e.g. phrases), through means of vector-based semantic composition (Mitchell and Lapata, 2010). Compositionality is not to be considered only an important component from a linguistic perspective, but also from a cognitive perspective and there has been efforts to validate it as a general cognitive process. However, in computer vision so far compositionality has received limited attention. Thus, in work, we study the phenomenon of compositionality we complement limited previous literature that has focused on event compositionality (St¨ottinger et al., 2012) or general image structure (Socher et al., 2011), by studying models of attribute-object semantic composition. In a nutshell, our work consists of learning vector representations of attribute-object (e.g., “red car”, “cute dog” etc.) and objects (e.g., “car”, “dog”, “truck”, “cat” etc.) and by using those compute the representation of new objects having similar attributes (“red truck”, “cute cat” etc.). This question has both theoretical and applied impact. The possibility of developing a visual compositional model of attribute-object, on the one hand, could shed light on the acquisition of such ability in humans; how we learn attribute representation and compose them with different objects is still an open question within the cognitive science community (Mintz and Gleitman, 2002). On the other hand, computer vision systems could become generative and be able to recognize unseen attribute-object combinations, a component especially useful for object recognition and image retrieval. 2 Visual Compositional Model our source of inspiration regarding the type of compositionality, we use the Functional (LF) (Baroni and Zamparelli, 2010), under which adjectives, in linguistic compositionality, are repreas linear functions (i.e., matrix of weights). Concretely, each adjective function induced corpus-observed vectors of adjective-noun phrases noun e.g., . . by solving the least-squares regression problem: arg min</abstract>
<note confidence="0.995147">This work is licensed under a Creative Commons Attribution 4.0 International Licence. License details:</note>
<web confidence="0.63036">http://creativecommons.org/licenses/by/4.0/</web>
<abstract confidence="0.98138925882353">112 of the 25th International Conference on Computational pages Dublin, Ireland, August 23-29 2014. In this work, we propose to import the LF method in the visual modality, aiming at developa Compositional Similarly to the case of linguistic compositionality, each atfunction induced from image-harvested vector representations of attribute-object object e.g. for training the function following data can be used . . 3 Experiments The visual representations of attribute-objects and objects are created with the PHOW-color features (Bosch et al., 2007) and SIFT color-agnostic features (Lowe, 2004) respectively. The linguistic for the adjective-noun noun built with the word2vec a corpus of 3 billion Both visual and linguistic representations consist of 300 dimensions. In this work, we focus on attributes related to 10 colors (Russakovsky and Fei-Fei, 2012) for a total number of 9699 images depicting 202 unique objects/nouns and 886 unique phrases (attribute- Our experiments are conducted with attribute-object obtained by summing the visual vectors extracted from images representing the same attribute-object, same pipeline is followed for the objects to obtain object This work aims at comparing the behavior of the semantically-driven compositionality process across two modalities. For this reason, we report results on the intersection of anda process that results in 266 attribute-object/adjective-noun items. Furthermore, although the training data the two modalities are different, the size of the training data is identical, i.e., the trained using remaining 620 attribute-object items, whereas for the we randomly sample 620 adjective-noun items from the language space. 3.1 Analysis of Language and Visual Semantic Spaces This experiment aims at assessing the degree to which language and vision share commonalities. To this end, we compute the cosine similarities between all possible combination of objects (resp., nouns) and a correlation analysis of the similarity of the corresponding pairs in the two lists resulting in correlation – e.g., we correlate the similarity between and that between For instance, “goat” and “sheep” are highly similar in both spaces, whereas “whale” and “bird” are similar only linguistically, whereas “blackboard” and “chair” are similar only visually. The same experiment is performed between all possible combinations of attribute-object/adjective-noun items, e.g. correlate the similarity between cat dog that between cat in correlation (see Table 1). Overall, our results suggest that the topologies of the semantic spaces are similar in the two modalities. Furthermore, since this phenomenon is also apparent in the cases of attribute-object and adjective-noun pairs, this alludes to the possibility of transferring approaches of semantic compositionality from the linguistic to the visual modality. Table 1: Similar and dissimilar concepts in the language and vision space. 3.2 Semantically-driven composition for attribute-object representations The findings of the previous experiment suggest a high correlation between the visual attribute-attribute representations and the corpus-harvested adjective-noun representations. An interesting question that arises is whether we could approximate such visual representations of complex visual units, similarly to High Visual Low Visual Linguistic jaguarlion baboon-transporter, bird-whale black bag brown bag, brown bear yellow dog blue grass blue van, gray whale white deer Linguistic blackboard-chair baboon-sofa, backboard-panda red strawberry white ball, white bear yellow dog black bag green bridge, green table yellow stick 113 how is done in Computational Linguistics for approximating the text-based representations of adjectivenoun phrases. Thus, this experiment is designed in order to assess the validity of the semantically-driven compositionality approach in the visual domain. Results are reported in Table 2. Since we expect that the quality of the aggregated vectors depends on the numbers of available images, we report results for subsets of the original data set that differ on the number of images per phrase. By means of the LF composition method sketched in Section 2, we obtain the compositional represenof attribute-object and adjective-noun items. We then perform the correlation between the similarities obtained in the composed visual space 1) the equivaimage-harvested representations 3) the equivalent compositionally-derived linguistic representations the correlation between that the visual compositionality of attribute-object can account, to some extend, for the visual semantics of the respective image, and it improves with the number of images we consider for obtaining the vectors the viphrases. Finally, as expected, the correlations between lower than the ones reported in Section 3.1, i.e., 0.22 vs 0.32, are still non negligible. all phrases images images images 0.24 0.40 0.53 0.58 comp phrase 0.10 0.22 0.19 0.23 0.04 0.05 0.18 0.10 2: Spearman correlations between the similarities in the other semantic spaces. 4 Conclusions In this work, we have experimented with semantically-driven compositionality of attributes with objects in the visual modality, by adopting an out-of-the-box composition method from the computational semantics literature. Our preliminary results have shown that the visual representations of attribute-objects when obtained compositionally reflect properties similar not only to the ones found in representations harvested automatically from images, but also from those extracted from text corpora. These results show that semantic compositionality might be a general process irrespective of the underlying modality. We have just scratched the surface on this topic and in the future we plan to experiment with a larger variety of attributes and use and design alternative visual compositional models.</abstract>
<note confidence="0.899428043478261">Acknowledgements The second and third author acknowledge ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES). We thank the 3 anonymous reviewers for their comments, Marco Baroni and Elia Bruni for their constant and useful feedback. References [Baroni and Zamparelli2010] Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are Representing adjective-noun constructions in semantic space. In of 1183–1193. [Bosch et al.2007] Anna Bosch, Andrew Zisserman, and Xavier Munoz. 2007. Image classification using random and ferns. In of 1–8. David G Lowe. 2004. Distinctive image features from scale-invariant keypoints. of Computer 60:91–110. [Mintz and Gleitman2002] Toben H. Mintz and Lila R. Gleitman. 2002. Adjectives really do modify nouns: the and restricted nature of early adjective acquisition. 84:267–293. [Mitchell and Lapata2010] Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of 34(8):1388–1429. [Russakovsky and Fei-Fei2012] Olga Russakovsky and Li Fei-Fei. 2012. Attribute learning in large-scale datasets. and Topics in Computer 1–14. Springer. [Socher et al.2011] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes natural language with recursive neural networks. In of 129–136. [St¨ottinger et al.2012] J. St¨ottinger, J.R.R. Uijlings, A.K. Pandey, N. Sebe, and F. Giunchiglia. 2012. (unseen) recognition via semantic compositionality. In 2) the equivalent corpus-derived linguistic representations 114</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<marker>[Baroni and Zamparelli2010]</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Bosch</author>
<author>Andrew Zisserman</author>
<author>Xavier Munoz</author>
</authors>
<title>Image classification using random forests and ferns.</title>
<date>2007</date>
<booktitle>In Proceedings of ICCV,</booktitle>
<pages>1--8</pages>
<marker>[Bosch et al.2007]</marker>
<rawString>Anna Bosch, Andrew Zisserman, and Xavier Munoz. 2007. Image classification using random forests and ferns. In Proceedings of ICCV, 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Distinctive image features from scale-invariant keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<pages>60--91</pages>
<marker>[Lowe2004]</marker>
<rawString>David G Lowe. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60:91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toben H Mintz</author>
<author>Lila R Gleitman</author>
</authors>
<title>Adjectives really do modify nouns: the incremental and restricted nature of early adjective acquisition.</title>
<date>2002</date>
<journal>Cognition,</journal>
<pages>84--267</pages>
<marker>[Mintz and Gleitman2002]</marker>
<rawString>Toben H. Mintz and Lila R. Gleitman. 2002. Adjectives really do modify nouns: the incremental and restricted nature of early adjective acquisition. Cognition, 84:267–293.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<marker>[Mitchell and Lapata2010]</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Russakovsky</author>
<author>Li Fei-Fei</author>
</authors>
<title>Attribute learning in large-scale datasets.</title>
<date>2012</date>
<booktitle>In Trends and Topics in Computer Vision,</booktitle>
<pages>1--14</pages>
<publisher>Springer.</publisher>
<marker>[Russakovsky and Fei-Fei2012]</marker>
<rawString>Olga Russakovsky and Li Fei-Fei. 2012. Attribute learning in large-scale datasets. In Trends and Topics in Computer Vision, 1–14. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Cliff C Lin</author>
<author>Chris Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>129--136</pages>
<marker>[Socher et al.2011]</marker>
<rawString>Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of ICML, 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J St¨ottinger</author>
<author>J R R Uijlings</author>
<author>A K Pandey</author>
<author>N Sebe</author>
<author>F Giunchiglia</author>
</authors>
<title>(unseen) event recognition via semantic compositionality.</title>
<date>2012</date>
<booktitle>In CVPR.</booktitle>
<marker>[St¨ottinger et al.2012]</marker>
<rawString>J. St¨ottinger, J.R.R. Uijlings, A.K. Pandey, N. Sebe, and F. Giunchiglia. 2012. (unseen) event recognition via semantic compositionality. In CVPR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
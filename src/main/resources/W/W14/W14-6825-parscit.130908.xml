<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.853688">
An Improved Graph Model for Chinese Spell Checking*
</title>
<author confidence="0.996282">
Yang Xin&apos;,2, Hai Zhao&apos;,2,†, Yuzhu Wang&apos;,2 and Zhongye Jia&apos;,2
</author>
<affiliation confidence="0.9826016">
&apos;Center for Brain-Like Computing and Machine Intelligence,
Department of Computer Science and Engineering,
Shanghai Jiao Tong University, Shanghai 200240, China
2Key Laboratory of Shanghai Education Commission for Intelligent Interaction
and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240, China
</affiliation>
<email confidence="0.9778655">
xuechen.xy@gmail.com, zhaohai@cs.sjtu.edu.cn,
hfut0830@sjtu.edu.cn, jia.zhongye@gmail.com
</email>
<sectionHeader confidence="0.987426" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.898173631578948">
In this paper, we propose an improved
graph model for Chinese spell checking.
The model is based on a graph model for
generic errors and two independently-
trained models for specific errors. First, a
graph model represents a Chinese sentence
and a modified single source shortest path
algorithm is performed on the graph
to detect and correct generic spelling
errors. Then, we utilize conditional
random fields to solve two specific kinds
of common errors: the confusion of
“Al” (at) (pinyin is ‘zai’ in Chinese),
“44” (again, more, then) (pinyin: zai)
and “M” (of) (pinyin: de), “It” (-ly,
adverb-forming particle) (pinyin: de),
“1V (so that, have to) (pinyin: de).
Finally, a rule based system is exploited
to solve the pronoun usage confusions:
“t” (she) (pinyin: ta), “&apos;J” (he) (pinyin:
ta) and some others fixed collocation
errors. The proposed model is evaluated
on the standard data set released by the
SIGHAN Bake-off 2014 shared task, and
gives competitive result.
*This work was partially supported by the National
Natural Science Foundation of China (No. 60903119, No.
61170114, and No. 61272248), the National Basic Research
Program of China (No. 2013CB329401), the Science and
Technology Commission of Shanghai Municipality (No.
13511500200), the European Union Seventh Framework
Program (No. 247619), the Cai Yuanpei Program (CSC fund
201304490199 and 201304490171), and the art and science
interdiscipline funds of Shanghai Jiao Tong University (A
study on mobilization mechanism and alerting threshold set-
ting for online community, and media image and psychology
evaluation: a computational intelligence approach).
†Corresponding author.
</bodyText>
<sectionHeader confidence="0.995246" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97016340625">
Spell checking is a routine processing task for
every written language, which is an automatic
mechanism to detect and correct human spelling
errors. Given sentences, the goal of the task is to
return the locations of incorrect words and suggest
the correct words. However, Chinese spell check-
ing (CSC) is very different from that in English
or other alphabetical languages from the following
ways.
Usually, the object of spell checking is words,
but “word” is not a natural concept in Chinese,
since there are no word delimiters between words
in Chinese writing. An English “word” consists
of Latin letters. While a Chinese “word” consists
of characters, which also known as “&apos;�” (Chi-
nese character) (pinyin1 is ‘han zi’ in Chinese).
Thus, essentially, the object of CSC is misused
characters in a sentence. Meanwhile, sentences
for CSC task are meant to computer-typed but not
those handwritten Chinese. In handwritten Chi-
nese, there exist varies of spelling errors including
non-character errors which are probably caused by
stroke errors. While in computer-typed Chinese, a
non-character spelling error is impossible, because
any illegal Chinese characters will be filtered by
Chinese input method engine so that CSC nev-
er encounters “out-of-character (OOC)” problem.
Thus, the Chinese spelling errors come from the
misuse of characters, not characters themselves.
Spelling errors in alphabetical languages, such
as English, are always typically divided into two
categories:
</bodyText>
<listItem confidence="0.8453955">
• The misspelled word is a non-word, for ex-
ample “come” is misspelled into “cmoe”;
</listItem>
<footnote confidence="0.9685405">
1Pinyin is the official phonetic system for transcribing the
sound of Chinese characters into Latin script.
</footnote>
<page confidence="0.898924">
157
</page>
<note confidence="0.9826285">
Proceedings of the Third CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 157–166,
Wuhan, China, 20-21 October 2014
</note>
<listItem confidence="0.5246125">
• The misspelled word is still a legal word, for
example “come” is misspelled into “cone”.
</listItem>
<bodyText confidence="0.996479555555555">
While in Chinese, if the misspelled word is a non-
word, the word segmenter will not recognize it as
a word, but split it into two or more words with
fewer characters. For example, if “你好世界”
in Example 1 of Table 1 is misspelled into
“你好世節”, the word segmenter will segment it
into “你好/世/節” instead of “你好/世節”. For
non-word spelling error, the misspelled word will
be mis-segmented.
</bodyText>
<table confidence="0.725829333333333">
Name Example 1 Example 2
Golden 你好/世界 好好/地/出去/玩
Misspelled 你好/世/節 好好/的/出去/玩
</table>
<tableCaption confidence="0.673913">
Pinyin ni hao shi jie hao hao de chu qu wan
Translation hello the world enjoy yourself outside
Table 1: Two examples for Chinese spelling error.
</tableCaption>
<bodyText confidence="0.97742705">
Both examples have the same pinyin.
Thus CSC cannot be directly applied those edit
distance based methods which are commonly used
for alphabetical languages. CSC task has to deal
with word segmentation problem first, since mis-
spelled sentence could not be segmented properly
by word segmenter.
There also exist Chinese spelling errors which
are unrelated with word segmentation. For exam-
ple, “好好地出去玩” in Example 2 of Table 1 is
misspelled into “好好的出去玩”, but both of them
have the same segmentation. So it is necessary to
perform further specific process.
In this paper, based on our previous work (Jia
et al., 2013b) in SIGHAN Bake-off 2013, we de-
scribe an improved graph model to handle the CSC
task. The improved model includes a graph model
for generic spelling errors, conditional random
fields (CRF) for two special errors and a rule based
system for some collocation errors.
</bodyText>
<sectionHeader confidence="0.999641" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999881836065574">
Over the past few years, there were many methods
proposed for CSC task. (Sun et al., 2010) devel-
oped a phrase-based spelling error model from the
clickthrough data by means of measuring the edit
distance between an input query and the optimal
spelling correction. (Gao et al., 2010) explored
the ranker-based approach which included visual
similarity, phonological similarity, dictionary, and
frequency features for large scale web search. (Ah-
mad and Kondrak, 2005) proposed a spelling error
model from search query logs to improve the qual-
ity of query. (Han and Chang, 2013) employed
maximum entropy models for CSC. They trained a
maximum entropy model for each Chinese charac-
ter based on a large raw corpus and used the model
to detect the spelling errors.
Two key techniques, word segmentation (Zhao
et al., 2006a; Zhao and Kit, 2008b; Zhao et al.,
2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007;
Zhao and Kit, 2011; Zhao et al., 2010) and lan-
guage model (LM), are also popularly used for C-
SC. Most of those approaches can fall into four cat-
egories. The first category consists of the methods
that all the characters in a sentence are assumed to
be errors and an LM is used for correction (Chang,
1995; Yu et al., 2013). (Chang, 1995) proposed a
method that replaced each character in the sentence
based on a confusion set and computed the prob-
ability of the original sentence and all modified
sentences according to a bigram language model
generated from a newspaper corpus. The method
based on the motivation that all the typos were
caused by either visual similarity or phonological
similarity. So they manually built a confusion
set as a key factor in their system. Although the
method can detect misspelled words well, it was
very time consuming for detection, generated too
much false positive results and was not able to refer
to an entire paragraph. (Yu et al., 2013) developed
a joint error detection and correction system. The
method assumed that all characters in the sentence
may be errors and replaced every character using
a confusion set. Then they segmented all new
generated sentences and gave a score of the seg-
mentation using LM for every sentence. In fact,
this method did not always perform well according
to (Yu et al., 2013).
The second category includes the methods that
all single-character words are supposed to be errors
and an LM is used for correction, for example (Lin
and Chu, 2013) . They developed a system which
supposed that all single-character words may be
typos. They replaced all single-character words by
similar characters using a confusion set and seg-
mented the newly created sentences again. If a new
sentence resulted in a better word segmentation,
spelling error was reported. Their system gave
good detection recall but low false-alarm rate.
The third category utilizes more than one ap-
proaches for detection and an LM for correction.
(Hsieh et al., 2013) used two different systems for
</bodyText>
<page confidence="0.997254">
158
</page>
<bodyText confidence="0.999976552631579">
error detection. The first system detected error
characters based on unknown word detection and
LM verification. The second one solved error
detection based on a suggestion dictionary gener-
ated from a confusion set. Finally, two systems
were combined to obtain the final detection result.
(He and Fu, 2013) divided typos into three cate-
gories which were character-level errors (CLEs),
word-level errors (WLEs) and context-level errors
(CLEs), and three different methods were used to
detect the different errors respectively. In addition
to using the result of word segmentation for detec-
tion, (Yeh et al., 2013) also proposed a dictionary-
based method to detect spelling errors. The dic-
tionary contained similar pronunciation and shape
information for each Chinese character. (Yang et
al., 2013) proposed another method to improve the
candidate detections. They employed high confi-
dence pattern matching to strengthen the candidate
errors after word segmentation.
The last category is formed by the methods
which use word segmentation for detection and
different models for correction (Liu et al., 2013;
Chen et al., 2013; Chiu et al., 2013). (Liu et
al., 2013) used support vector machine (SVM) to
select the most probable sentence from multiple
candidates. They used word segmentation and ma-
chine translation model to generate the candidates
respectively. The SVM was used to rerank the
candidates. (Chen et al., 2013) not only applied
LM, but also used various topic models to cover
the shortage of LM. (Chiu et al., 2013) explored
statistical machine translation model to translate
the sentences containing typos into correct ones. In
their model, the sentence with the highest transla-
tion probability which indicated how likely a typo
was translated into its candidate correct word was
chosen as the final correction sentence.
</bodyText>
<sectionHeader confidence="0.984675" genericHeader="method">
3 The Revised Graph Model
</sectionHeader>
<bodyText confidence="0.986424666666667">
The graph model (Jia et al., 2013b) of SIGHAN
Bake-off 2013 is inspired by the idea of shortest
path word segmentation algorithm which is based
on the following assumption: a reasonable seg-
mentation should maximize the lengths of all seg-
ments or minimize the total number of segments
(Casey and Lecolinet, 1996). A directed acyclic
graph (DAG) is thus built from the input sentence
similar. The spelling error detection and correction
problem is transformed to a single source shortest
path (SSSP) problem on the DAG.
Given a dictionary IID and a similar characters C,
for a sentence 5 of m characters {c1, c2, ... , cm},
the original vertices V of the DAG in (Jia et al.,
2013b) are:
</bodyText>
<equation confidence="0.994405">
V ={wi,j|wi,j = ci ... cj E IID}
U {wki,j|wki,j = ci ...c′k ... cj EIID,
T &lt; j − i &lt; T,
c′k EC[ck],k=i,i+1,...,j}
U {w−,0, wn+1,−}.
</equation>
<bodyText confidence="0.9997874">
where w−,0 = “&lt;S&gt;” and wn+1,− = “&lt;/S&gt;” are
two special vertices represent the start and end of
the sentence.
However, the graph model cannot be applied
to continuous word errors. Take the following
sentence as an example, “健康” (health) (pinyin:
jian kang) is misspelled into “建缸” (pinyin: jian
gang). Because the substitution strategy does not
simultaneously substitute two continuous charac-
ters.
</bodyText>
<listItem confidence="0.99899">
• 然後,我是計劃我們到我家一個附近的
‘建缸’ (pinyin: jian gang) 中心去游泳。
</listItem>
<bodyText confidence="0.9316972">
Translation after correction: And then,
we plan to go swimming near my house.
For example, the substitution of “建缸” (pinyin:
jian gang) may be “碱缸” (pinyin: jian gang),
“建鋼” (pinyin: jian gang), “建行” (pinyin: jian
hang) and so on, none of which is the desired cor-
rection. So we revise the construction method of
the graph model. Considering efficiency, we only
deal with the continuous errors with 2 characters.
The revised V are:
</bodyText>
<equation confidence="0.994853285714286">
V ={wi,j|wi,j = ci ... cj E IID}
U {wki,j|wki,j = ci ... c′k ... cj E IID,
T &lt; j − i &lt; T,
c′k E C[ck],k = i,i + 1,...,j}
U {wl|wl = c′lc′l+1 E IID,
c′l, c′l+1 E C}
U {w−,0, wn+1,−}.
</equation>
<bodyText confidence="0.999895666666667">
With the modified DAG G, the “建缸” (pinyin:
jian gang) is substituted as “健康” (health) (pinyin:
jian kang), “峴港” (Danang) (pinyin: xian gang),
“潛航” (submerge) (pinyin: qian hang) and so on,
which have already contained the desired correc-
tion.
</bodyText>
<page confidence="0.997884">
159
</page>
<sectionHeader confidence="0.982068" genericHeader="method">
4 The Improved Graph Model
</sectionHeader>
<bodyText confidence="0.999575611111111">
The graph model based on word segmentation in
(Jia et al., 2013b) includes the revised graph model
in section 3 still has its limitations. For a sentence,
in the graph construction stage, the substitution
is only applied to the situation that the number
of words after segmenting has to be decreased,
which means there exists new longer word after
segmentation. In addition, if the segmentation
result of a sentence is a single character, the graph
model does not work, because a single charac-
ter will not be substituted. For example in the
following two sentences, the “他” (he) (pinyin:
ta) in the first sentence should be corrected into
“她” (she) (pinyin: ta) and the “的” (of)(pinyin:
de) in the second sentence should be corrected into
“地” (-ly, adverb-forming particle) (pinyin: de),
however, the graph model does not work for this
case.
</bodyText>
<listItem confidence="0.923348142857143">
• 雖然我不在我的國家,不能見到媽媽,可
是我要給‘他’ (him) (pinyin: ta)打電話!
Translation after correction: Though I’m
not in my country so that I cannot see my
mum, I would like to call her!
• 我們也不要想太多;我們來好好‘的’ (of)
(pinyin: de)出去玩吧!
</listItem>
<bodyText confidence="0.9965466">
Translation after correction: We would
not worry too much, just enjoy ourselves out-
side now!
The graph model is also powerless for the error sit-
uation that the wrong character was segmented into
a legal word. Take the following sentence as an ex-
ample, the word “心裡” (in mind, at heart) (pinyin:
xin li) will be not separated after the building the
graph, so “裡” (pinyin: li) could not be corrected
into “理” (pinyin: li).
</bodyText>
<listItem confidence="0.994168">
• 我對心‘裡’ (pinyin: li)研究有興趣。
</listItem>
<bodyText confidence="0.9722085">
Translation after correction: I’m inter-
ested in psychological research.
For the sake of alleviating the above limitations
of the graph model, we utilize CRF model to deal
with two kinds of errors, and a rule based system
is established to cope with the pronoun errors:
“她” (she) (pinyin: ta), “他” (he) (pinyin: ta) and
collocation errors.
</bodyText>
<subsectionHeader confidence="0.76146">
4.1 CRF Model
</subsectionHeader>
<bodyText confidence="0.949561038461538">
Two classifiers using CRF model are respective-
ly trained to tackle the common character usage
confusions: 在” (at) (pinyin: zai), 再” (again,
more, then) (pinyin: zai) and “的” (of)(pinyin: de),
“地” (-ly, adverb-forming particle) (pinyin: de),
“得”(so that, have to) (pinyin: de). We assume that
the correct character selection is related with its
neighboring two words and part-of-speech (POS)
tags. The classifiers are trained on a large five-
gram token set which is extracted from a large POS
tagged corpus. The feature selection algorithm is
according to (Zhao et al., 2013; Wang et al., 2014;
Jia et al., 2013a). The feature set for CRF model is
as follows:
wj,−2, posj,−2, wj,−1, posj,−1, wj,0, posj,0,
wj,1, posj,1, wj,2, posj,2
where j is the token index to indicate its position,
wj,0 is the current candidate character and posj,0
is its POS tag. ICTCLAS (Zhang et al., 2003) is
adopted for POS tagging.
A set of feature strings that we used are present-
ed in Table 2. The labels for “的” (of) (pinyin:
de), “地” (-ly, adverb-forming particle) (pinyin:
de), “得”(so that, have to) (pinyin: de) are 1, 2,
3 and “在” (at) (pinyin: zai), “再” (again, more,
then) (pinyin: zai) are 1, 2.
</bodyText>
<subsectionHeader confidence="0.95003">
4.2 The Rule Based System
</subsectionHeader>
<bodyText confidence="0.999831941176471">
To effectively handle pronoun usage errors for
“她” (she) (pinyin: ta), and “他” (he) (pinyin: ta)
and other collocation errors, we design a rule based
system extracted from the development set.
The Table 3 is the rules we set for solving the
pronoun usage errors, where the prefix[i] is the
current word w[i]’s prefix in a sentence. For the
others rules, we divide them into five categories,
which are presented in Table 4 – Table 8. In
Table 4, we only present several typical rules in
Rule 3. The negation symbol “�” in the Table 6
and Table 7 means that the word in corresponding
position is not the one in the brackets. Each rule in
the tables is verified by the Baidu2 search engine.
If the error situation is legally emerged in the
search result, we will not correct the error any
more.
</bodyText>
<footnote confidence="0.976921">
2http://www.baidu.com/
</footnote>
<page confidence="0.937463">
160
</page>
<table confidence="0.999943294117647">
Feature Example1 Example2
wj,−2 “來” “和”
wj,−1 “好好” “你”
wj,1 “出” “一起”
wj,−2,wj,−1 “來”,“好好” “和”,“你”
wj,−2,wj,−1,wj,1 “來”,“好好”,“出” “和”,“你”,“一起”
wj,1,wj,2 “出”,“去” “一起”,“。”
posj,−2 v p
posj,−1 z r
posj,1 v s
posj,−2,posj,−1 v,z p,r
posj,−1,posj,1 z,v r,s
posj,1,posj,2 v,v o s,w
posj,−2,posj,−1,posj,1 v,z,v p,r,s
wj,−1,posj,1 “好好”,v “你”,s
posj,−1,wj,1 z,“出” r,“一起”
posj,−2,posj,−1,wj,1 v,z,“出” p,r,“一起”
</table>
<tableCaption confidence="0.875048">
Table 2: Feature strings for sentences “我們來好好地出去玩吧!” and “我只要和你在一起。”.
</tableCaption>
<table confidence="0.958251857142857">
prefix[i] does not contain prefix[i] contains w[i] corrected w[i]
(媽 and 爸) or (她 and 他) or 她 or 媽 or 母 or 女 or 他 她
(母 and 父) or (女 and 男) or 妹 or 姊 or 姐 or 婆 or
(太太 and 先生) 阿姨 or 太太
她 or 媽 or 母 or 女 or 他 or 爸 or 父 or 男 or 她 他
妹 or 姊 or 姐 or 婆 or 哥 or 先生
阿姨 or 太太
</table>
<tableCaption confidence="0.956198">
Table 3: Specific rules for the pronouns “她、他” confusion.
</tableCaption>
<table confidence="0.9994074">
w[i] pos[i + 1] corrected w[i]
阿 w 啊
馬 or 碼門 w 嗎
把 r, n 們
r, n 吧
</table>
<tableCaption confidence="0.9678235">
Table 4: Rule 1. The correction related with right
neighbored POS tag.
</tableCaption>
<sectionHeader confidence="0.998563" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998063">
5.1 Data Sets and Resources
</subsectionHeader>
<bodyText confidence="0.999970333333333">
The proposed method is evaluated on the data
sets of SIGHAN Bake-off shared tasks in 2013
and 2014. In Bake-off 2013, the sentences were
collected from 13 to 14-year-old students’ essays
in formal written tests (Wu et al., 2013). In Bake-
off 2014, the sentences were collected from Chi-
nese as a foreign language (CFL) learners’ essays
selected from the National Taiwan Normal Univer-
sity (NTNU) learner corpus3. All the data sets are
in traditional Chinese.
In Bake-off 2013, the essays were manually an-
notated with different labels (see Figure 1). There
is at most one error in each sentence. However,
the development set in Bake-off 2014 is enlarged
and the error types (see Figure 2) are more diverse.
</bodyText>
<footnote confidence="0.977817666666667">
3http://www.cipsc.org.cn/clp2014/
webpage/en/four_bakeoffs/Bakeoff2014cfp_
ChtSpellingCheck_en.htm
</footnote>
<bodyText confidence="0.936601">
More than one error might be in each sentence.
And there exists continuous errors as in Figure 2.
</bodyText>
<figure confidence="0.994398888888889">
&lt;DOC Nid=&amp;quot;00001&amp;quot;&gt;
&lt;P&gt;RTrjA �A�N,k,fiFinm#rflh �J,%a ��R1�1 0&lt;/P&gt;
&lt;TEXT&gt;
&lt;MISTAKE wrong_position=13&gt;
&lt;WRONG&gt;m#r&lt;/WRONG&gt;
&lt;CORRECT&gt;�#r&lt;/CORRECT&gt;
&lt;/MISTAKE&gt;
&lt;/TEXT&gt;
&lt;/DOC&gt;
</figure>
<figureCaption confidence="0.960136">
Figure 1: A sample of annotated essay in Bake-off
2013.
</figureCaption>
<figure confidence="0.997077555555556">
&lt;ESSAY title=&amp;quot;ማ㎖ণሷࡍ⅑㾻䶒Ⲵㅶ৻Ⲵаሱؑ&amp;quot;&gt;
&lt;TEXT&gt;
&lt;PASSAGE id=&amp;quot;B1-0118-3&amp;quot;&gt;❦ᖼˈᡁᱟ䀸⮛ᡁفࡠᡁᇦаػ䱴䘁Ⲵᔪ㕨
ѝᗳ৫⑨⌣DŽᡁ〻㏃䐏ᡁ䅋䙾֐ᖸᴳ⑨⌣DŽ&lt;/PASSAGE&gt;
&lt;/TEXT&gt;
&lt;MISTAKE id=&amp;quot;B1-0118-3&amp;quot; location=&amp;quot;18&amp;quot;&gt;
&lt;WRONG&gt;ᔪ㕨ѝᗳ&lt;/WRONG&gt;
&lt;CORRECTION&gt;ڕᓧѝᗳ&lt;/CORRECTION&gt;
&lt;/MISTAKE&gt;
&lt;MISTAKE id=&amp;quot;B1-0118-3&amp;quot; location=&amp;quot;19&amp;quot;&gt;
&lt;WRONG&gt;ᔪ㕨ѝᗳ&lt;/WRONG&gt;
&lt;CORRECTION&gt;ڕᓧѝᗳ&lt;/CORRECTION&gt;
&lt;/MISTAKE&gt;
&lt;MISTAKE id=&amp;quot;B1-0118-3&amp;quot; location=&amp;quot;27&amp;quot;&gt;
&lt;WRONG&gt;〻㏃&lt;/WRONG&gt;
&lt;CORRECTION&gt;ᴮ㏃&lt;/CORRECTION&gt;
&lt;/MISTAKE&gt;
&lt;/ESSAY&gt;
</figure>
<figureCaption confidence="0.946768">
Figure 2: A sample of annotated essay in Bake-off
2014.
</figureCaption>
<tableCaption confidence="0.8608755">
Statistical information on data sets is shown in
Table 9. Three development sets are named as
</tableCaption>
<page confidence="0.948757">
161
</page>
<table confidence="0.9707405">
w[i] suffix[i] contains corrected w[i]
帶 帽, ORO, 皮帶, 手環 戴
負, 府 費, 錢, 經濟, 薪水 付
fit, 座 車, 巴士, 飛機, 捷運, 船, 高鐵 坐
</table>
<tableCaption confidence="0.991057">
Table 5: Rule 2. The correction related with the current word’s suffix.
</tableCaption>
<table confidence="0.993051125">
w[i − 1] w[i] w[i + 1] corrected w[i]
¬(一, o 到 ¬(部, –總於終 治, –髒 il
¬(IBJ, 肝, 腎) 臟 – 除 餓
– 俄 ¬(羅) 一
改 以 改 都
¬(Tk) 多 Tk 裡
心 理 ¬(学, 研) 跟
二, 這, 兩, 幾, 草, 壓) 根 本, 據, 源, 基,
</table>
<tableCaption confidence="0.973507">
Table 6: Rule 3. The correction related with neighbored words.
</tableCaption>
<table confidence="0.983837">
w[i − 2] w[i − 1] w[i] w[i + 1] w[i + 2] corrected w[i]
林依神––晨
鋼鐵依––衣
游泳 世 – – 池
星 期 路 – – 六
西 ri 丁 – – RT
– – Tk 不 N 恨
– – 仍 在 了 YJ
– – IT di 租 搭
– – 機 i 車 at
– – ¬(少) 子 化 少
</table>
<tableCaption confidence="0.993479">
Table 7: Rule 4. The correction related with two neighbored words.
</tableCaption>
<table confidence="0.999788882352941">
w[i − 1] w[i] w[i + 1] w[i + 2] w[i + 3] corrected w[i] and w[i + 1]
– H 到 – – oil
– 式 式 – – 試試
–
– 蘭 滿 – – 浪漫
41 41 – – 冷冷
– 44V 44V – – 拜拜
– 柏 柏 – – 1010
– 莎 增 – – 沙m
– 玈 V – – 旅館
– 棒 組 – – 幫助
– 想 心 – – I� .p.p..
*
– 名 性 – – 明星
– TA TA 大, 有 名 鼎鼎
– 白 花 商 Z 百貨
為 是 嗎 – – 什麼
</table>
<tableCaption confidence="0.999687">
Table 8: Rule 5. Two words are simultaneously corrected.
</tableCaption>
<bodyText confidence="0.9998391">
DEV13, DEV14C and DEV14B and the test set
is named as TEST14 respectively. In the DE-
V14B, there are 4624 errors, in which the statistics
information of the three common character usage
confusions in section 4 is shown in Table 10, so it
is necessary to deal with them respectively.
The dictionary IID used in SSSP algorithm is
SogouW4 dictionary from Sogou inc., which is in
simplified Chinese. The OpenCC5 converter is
used for simplified-to-traditional Chinese convert-
</bodyText>
<footnote confidence="0.9996965">
4http://www.sogou.com/labs/dl/w.html
5http://code.google.com/p/opencc/
</footnote>
<bodyText confidence="0.998998444444445">
ing. Similar character set C provided by (Liu et
al., 2010) is used to substitute the original words
in the graph construction stage. The LM is built
on the Academia Sinica corpus (Emerson, 2005)
with IRSTLM toolkit (Federico et al., 2008). The
CRF model is achieved by training and tuning
on the Academia Sinica corpus with the toolkit
CRF++ 0.586. For Chinese word segmentation,
the ICTCLAS20117 is exploited.
</bodyText>
<footnote confidence="0.9976">
6https://code.google.com/p/crfpp/downloads/list
7http://www.ictclas.org/ictclas_download.
aspx
</footnote>
<page confidence="0.98784">
162
</page>
<table confidence="0.999547">
Name Data Size (lines) Character number (k)
Development set Bake-off 2013 700 29
Bake-off 2014 C1 342 16
B1 3004 149
Test set 1062 53
</table>
<tableCaption confidence="0.984135">
Table 9: Statistical information of data sets.
</tableCaption>
<table confidence="0.905855166666667">
Error Type
Percent (%)
Number
在, 再 101 2.18
的, 地, 得 398 8.61
她, 他 101 3.98
</table>
<tableCaption confidence="0.9666555">
Table 10: Three common character usage confu-
sions in the DEV14B.
</tableCaption>
<subsectionHeader confidence="0.983477">
5.2 The Improved Graph Model
</subsectionHeader>
<bodyText confidence="0.999941">
We treat the graph model without filters in Bake-
off 2013 as our baseline in Bake-off 2014. The
edge function is the linear combination of similar-
ity and log conditional probability:
</bodyText>
<equation confidence="0.99248">
wL = ws � Q log P
</equation>
<bodyText confidence="0.930786833333333">
where wo - 0 which is omitted in the equation,
and ws for different kinds of characters are shown
in Table 11. The LM is set to bigram according to
(Yang et al., 2012). Improved Kneser-Ney method
is used for LM smoothing (Chen and Goodman,
1999).
Type w3
same pronunciation same tone 1
same pronunciation different tone 1
similar pronunciation same tone 2
similar pronunciation different tone 2
similar shape 2
</bodyText>
<tableCaption confidence="0.948003">
Table 11: ws used in wL.
</tableCaption>
<bodyText confidence="0.999369333333333">
We utilize the correction precision (P), correc-
tion recall (R) and F1 score (.T) as the metrics. The
computational formulas are as follows:
</bodyText>
<listItem confidence="0.798799">
• Correction precision:
number of correctly corrected characters
P =
• Correction recall:
</listItem>
<bodyText confidence="0.626933">
number of correctly corrected characters
</bodyText>
<equation confidence="0.9683186">
R =
• F1 macro:
2PR
� =
P + R,
</equation>
<bodyText confidence="0.9994477">
We firstly use the revised graph model in sec-
tion 3 to tackle the continuous word errors. The
results achieved by the graph model and its revi-
sion on DEV14B with different Q are shown in
Figure 3 respectively. We can see that the result
with the revised graph model is not improved,
and even worse than the baseline. Therefore,
for the improved graph model in Bake-off 2014,
we remain use the graph model in Bake-off 2013
without any modification.
</bodyText>
<figure confidence="0.997508">
(a) The graph model.
(b) The revised graph model.
</figure>
<figureCaption confidence="0.985252">
Figure 3: The results of the graph model and its
revision on DEV14B.
</figureCaption>
<bodyText confidence="0.99840875">
To observe the performance of the improved
graph model in detail, on the three development
sets: DEV13, DEV14C, DEV14B, we report the
results from the following settings:
</bodyText>
<listItem confidence="0.9875315">
1. CRF. We use the CRF model to process
the common character usage confusions:
“在” (at) (pinyin: zai), “再” (again, more,
then) (pinyin: zai) and “的” (of) (pinyin: de),
</listItem>
<figure confidence="0.992203027777778">
Precision
Recall
F1
0 2 4 6 8 10 12 14
β
1
Precision, Recall and F1 curves
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Precision
Recall
F1
0 2 4 6 8 10 12 14
β
1
Precision, Recall and F1 curves
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
number of all corrected characters &apos;
number of wrong characters of gold data&apos;
</figure>
<page confidence="0.994885">
163
</page>
<table confidence="0.9998">
DEV13 DEV14C DEV14B
Model P R F P R F P R F
Graph (baseline) 0.802 0.6 0.686 0.790 0.238 0.366 0.729 0.2 0.314
+CRF 0.623 0.6 0.611 0.75 0.38 0.504 0.631 0.282 0.389
+CRF+Rule_Post 0.512 0.614 0.558 0.723 0.421 0.532 0.699 0.461 0.555
+CRF+Rule_Pre 0.526 0.614 0.567 0.75 0.38 0.504 0.706 0.479 0.571
+CRF+Rule_Pre+Rule_Post 0.51 0.611 0.556 0.723 0.421 0.532 0.706 0.484 0.574
</table>
<tableCaption confidence="0.999365">
Table 14: The results with different models.
</tableCaption>
<bodyText confidence="0.83940725">
“地” (-ly, adverb-forming particle) (pinyin:
de), “得”(have to, get, obtain) (pinyin: de) on
all development sets. The results achieved
by the CRF model are shown in Table 12.
</bodyText>
<table confidence="0.77756575">
Development set P R F
DEV13 0.060 0.014 0.023
DEV14C 0.718 0.162 0.264
DEV14B 0.549 0.072 0.128
</table>
<bodyText confidence="0.999624833333333">
results of the improved graph model on DEV13
are relatively declined, the results both on the
DEV14C and DEV14B are improved. The results
in Table 14 prove that CRF model and the rule
based system are effective to cover the shortage of
the graph model.
</bodyText>
<sectionHeader confidence="0.644262" genericHeader="evaluation">
5.3 Results
</sectionHeader>
<tableCaption confidence="0.868734">
Table 12: The results of CRF model.
</tableCaption>
<bodyText confidence="0.7610865">
2. Rule. The rule based system is carried out
on the development sets to solve the fixed
collocation errors. The results achieved by
the rule based system are shown in Table 13.
</bodyText>
<table confidence="0.99854025">
Development set P R F
DEV13 0.111 0.034 0.052
DEV14C 0.583 0.076 0.135
DEV14B 0.766 0.253 0.380
</table>
<tableCaption confidence="0.997568">
Table 13: The results of the rule based system.
</tableCaption>
<listItem confidence="0.992676071428572">
3. Graph+CRF. In this setting, the graph model
with different Q in wL is performed on the
CRF results. For each development set, an
optimal Q could be found to obtain the opti-
mal performance.
4. CRF+Graph+Rule_Post. Based on the re-
sults of the Graph+CRF model, we add the
rule based system. Similarly, the optimal Q
could be found.
5. CRF+Rule_Pre+Graph. Different from the
third setting, we firstly utilize the rule based
system on the development sets, and then use
the graph model with different Q in wL.
6. CRF+Rule_Pre+Graph+Rule_Post. Based
</listItem>
<bodyText confidence="0.976449777777778">
on the results of CRF+Rule_Pre+Graph
model, we add the rule based system at last.
In Table 14, we compare different improved
graph models on the development sets, in which
we set Q as 6 in wL. We could find that though the
In Bake-off 2014, we submit 3 runs, using the CR-
F+Rule_Pre+Graph model and the weight func-
tion wL, of which the Q is set as 0, 6, and 10,
respectively. The results on TEST14 are listed in
</bodyText>
<tableCaption confidence="0.814322">
Table 15.
</tableCaption>
<table confidence="0.9999417">
Metric Run1 Run2 Run3
False Positive Rate 0.5951 0.2279 0.1921
Detection Accuracy 0.3117 0.5471 0.5367
Detection Precision 0.2685 0.5856 0.5802
Detection Recall 0.2185 0.322 0.2655
Detection F1-Score 0.2409 0.4156 0.3643
Correction Accuracy 0.2938 0.5377 0.5311
Correction Precision 0.2349 0.5709 0.5696
Correction Recall 0.1827 0.3032 0.2542
Correction F1-Score 0.2055 0.3961 0.3516
</table>
<tableCaption confidence="0.987725">
Table 15: Official results of Bake-off 2014.
</tableCaption>
<sectionHeader confidence="0.989952" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999570692307692">
In this paper we present an improved graph model
to deal with Chinese spell checking problem.
The model includes a graph model and two
independently-trained models. To begin with,
the graph model is utilized to solve generic spell
checking problem and SSSP algorithm is adopted
as the model implementation. Furthermore, a
CRF model and a rule based system are used
to cover the shortage of the graph model. The
effectiveness of the proposed model is verified on
the data released by the SIGHAN Bake-off 2014
shared task and our system gives competitive
results according to official evaluation..
</bodyText>
<sectionHeader confidence="0.99527" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.476232">
Farooq Ahmad and Grzegorz Kondrak. 2005.
Learning a spelling error model from search query
</reference>
<page confidence="0.995022">
164
</page>
<reference confidence="0.999254607142857">
logs. In Proceedings of Human Language Tech-
nology Conference and Conference on Empirical
Methods in Natural Language Processing, pages
pp. 955–962, Vancouver, British Columbia, Canada,
October.
Richard G Casey and Eric Lecolinet. 1996. A survey
of methods and strategies in character segmentation.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 18(7):690–706.
Chaohuang Chang. 1995. A new approach for
automatic Chinese spelling correction. In Proceed-
ings of Natural Language Processing Pacific Rim
Symposium, pages pp. 278–283, Seoul, Korea.
Stanley F Chen and Joshua Goodman. 1999.
An empirical study of smoothing techniques for
language modeling. Computer Speech &amp; Language,
13(4):359–393.
Kuanyu Chen, Hungshin Lee, Chunghan Lee, Hsinmin
Wang, and Hsinhsi Chen. 2013. A study of
language modeling for Chinese spelling check. In
Proceedings of the Seventh SIGHAN Workshop on
Chinese Language Processing, pages pp. 79–83,
Nagoya, Japan, October.
Hsunwen Chiu, Jiancheng Wu, and Jason S. Chang.
2013. Chinese spelling checker based on statistical
machine translation. In Proceedings of the Seventh
SIGHAN Workshop on Chinese Language Process-
ing, pages pp. 49–53, Nagoya, Japan, October.
Thomas Emerson. 2005. The second international
Chinese word segmentation Bakeoff. In Proceed-
ings of the Fourth SIGHAN Workshop on Chinese
Language Processing, pages pp. 123–133, Jeju
Island, Korea.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Proceedings of
9th Annual Conference of the International Speech
Communication Association, pages pp. 1618–1621,
Brisbane, Australia.
Jianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk,
and Xu Sun. 2010. A large scale ranker-based
system for search query spelling correction. In
Proceedings of the 23rd International Conference
on Computational Linguistics, pages pp. 358–366,
Beijing, China, August.
Dongxu Han and Baobao Chang. 2013. A maximum
entropy approach to Chinese spelling check. In
Proceedings of the Seventh SIGHAN Workshop on
Chinese Language Processing, pages pp. 74–78,
Nagoya, Japan, October.
Yu He and Guohong Fu. 2013. Description of HLJU
Chinese spelling checker for SIGHAN Bakeoff
2013. In Proceedings of the Seventh SIGHAN
Workshop on Chinese Language Processing, pages
pp. 84–87, Nagoya, Japan, October.
Yuming Hsieh, Minghong Bai, and Kehjiann Chen.
2013. Introduction to CKIP Chinese spelling check
system for SIGHAN Bakeoff 2013 evaluation. In
Proceedings of the Seventh SIGHAN Workshop on
Chinese Language Processing, pages pp. 59–63,
Nagoya, Japan, October.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013a.
Grammatical error correction as multiclass classi-
fication with single model. In Proceedings of the
Seventeenth Conference on Computational Natural
Language Learning: Shared Task, pages pp. 74–81,
Sofia, Bulgaria, August.
Zhongye Jia, Peilu Wang, and Hai Zhao. 2013b.
Graph model for Chinese spell checking. In
Proceedings of the Seventh SIGHAN Workshop on
Chinese Language Processing, pages pp. 88–92,
Nagoya, Japan, October.
Chuanjie Lin and Weicheng Chu. 2013. NTOU
Chinese spelling check system in SIGHAN Bake-
off 2013. In Proceedings of the Seventh SIGHAN
Workshop on Chinese Language Processing, pages
pp. 102–107, Nagoya, Japan, October.
Chaolin Liu, Minhua Lai, Yihsuan Chuang, and
Chiaying Lee. 2010. Visually and phonologically
similar characters in incorrect simplified Chinese
words. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
pages pp. 739–747, Beijing, China, August.
Xiaodong Liu, Kevin Cheng, Yanyan Luo, Kevin
Duh, and Yuji Matsumoto. 2013. A hybrid
Chinese spelling correction using language model
and statistical machine translation with reranking.
In Proceedings of the Seventh SIGHAN Workshop
on Chinese Language Processing, pages pp.54–58,
Nagoya, Japan, October.
Xu Sun, Jianfeng Gao, Daniel Micol, and Chris
Quirk. 2010. Learning phrase-based spelling error
models from clickthrough data. In Proceedings
of the 48th Annual Meeting of the Association
for Computational Linguistics, pages pp. 266–274,
Uppsala, Sweden, July.
Peilu Wang, Zhongye Jia, and Hai Zhao. 2014.
Grammatical error detection and correction using a
single maximum entropy model. In Proceedings
of the Eighteenth Conference on Computational
Natural Language Learning: Shared Task, pages pp.
74–82, Baltimore, Maryland, June.
Shihhung Wu, Chaolin Liu, and Lunghao Lee. 2013.
Chinese spelling check evaluation at SIGHAN Bake-
off 2013. In Proceedings of the Seventh SIGHAN
Workshop on Chinese Language Processing, pages
pp. 35–42, Nagoya, Japan, October.
Shaohua Yang, Hai Zhao, Xiaolin Wang, and Baoliang
Lu. 2012. Spell checking for Chinese. In
International Conference on Language Resources
and Evaluation, pages pp. 730–736, Istanbul,
Turkey, May.
</reference>
<page confidence="0.983992">
165
</page>
<reference confidence="0.999569417910447">
Tinghao Yang, Yulun Hsieh, Yuhsuan Chen, Michael
Tsang, Chengwei Shih, and Wenlian Hsu. 2013.
Sinica-IASL Chinese spelling check system at
SIGHAN-7. In Proceedings of the Seventh SIGHAN
Workshop on Chinese Language Processing, pages
pp. 93–96, Nagoya, Japan, October.
Juifeng Yeh, Shengfeng Li, Meirong Wu, Wenyi
Chen, and Maochuan Su. 2013. Chinese word
spelling correction based on N-gram ranked inverted
index list. In Proceedings of the Seventh SIGHAN
Workshop on Chinese Language Processing, pages
pp. 43–48, Nagoya, Japan, October.
Liangchih Yu, Chaohong Liu, and Chunghsien Wu.
2013. Candidate scoring using web-based measure
for Chinese spelling error correction. In Proceed-
ings of the Seventh SIGHAN Workshop on Chinese
Language Processing, pages pp. 108–112, Nagoya,
Japan, October.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun
Liu. 2003. HHMM-based Chinese lexical analyzer
ICTCLAS. In Proceedings of the second SIGHAN
workshop on Chinese language processing, pages
pp. 184–187, Sapporo,Japan.
Hai Zhao and Chunyu Kit. 2007. Incorporating
global information into supervised learning for
Chinese word segmentation. In Proceedings of
the 10th Conference of the Pacific Association
for Computational Linguistics, pages pp. 66–74,
Melbourne, Australia.
Hai Zhao and Chunyu Kit. 2008a. An empirical
comparison of goodness measures for unsupervised
Chinese word segmentation with a unified frame-
work. In Proceedings of the Third International
Joint Conference on Natural Language Processing,
pages pp. 9–16, Hyderabad, India.
Hai Zhao and Chunyu Kit. 2008b. Unsupervised
segmentation helps supervised learning of character
tagging for word segmentation and named entity
recognition. In Proceedings of the Third Inter-
national Joint Conference on Natural Language
Processing, pages pp. 106–111, Hyderabad, India.
Hai Zhao and Chunyu Kit. 2011. Integrating
unsupervised and supervised word segmentation:
The role of goodness measures. Information
Sciences, 181(1):163–183.
Hai Zhao, Chang-Ning Huang, and Mu Li. 2006a.
An improved Chinese word segmentation system
with conditional random field. In Proceedings of
the Fifth SIGHAN Workshop on Chinese Language
Processing, volume 1082117, pages pp. 162–165,
Sydney, Australia, July.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-
Liang Lu. 2006b. Effective tag set selection in
Chinese word segmentation via conditional random
field modeling. In Proceedings of the 20th Pacific
Asia Conference on Language, Information and
Computation, volume 20, pages pp. 87–94, Wuhan,
China.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-
Liang Lu. 2010. A unified character-based
tagging framework for Chinese word segmentation.
ACM Transactions on Asian Language Information
Processing, 9(2):1–32.
Hai Zhao, Xiaotian Zhang, and Chunyu Kit. 2013. In-
tegrative semantic dependency parsing via efficient
large-scale feature selection. Journal of Artificial
Intelligence Research, 46:203–233.
</reference>
<page confidence="0.998763">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.004002">
<title confidence="0.988923">Improved Graph Model for Chinese Spell</title>
<author confidence="0.4844305">Hai Yuzhu</author>
<author confidence="0.4844305">Zhongye for Brain-Like Computing</author>
<author confidence="0.4844305">Machine</author>
<affiliation confidence="0.797772333333333">Department of Computer Science and Shanghai Jiao Tong University, Shanghai 200240, Laboratory of Shanghai Education Commission for Intelligent</affiliation>
<address confidence="0.565601">and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, 200240,</address>
<email confidence="0.9451895">xuechen.xy@gmail.com,hfut0830@sjtu.edu.cn,jia.zhongye@gmail.com</email>
<abstract confidence="0.998756538461539">In this paper, we propose an improved graph model for Chinese spell checking. The model is based on a graph model for generic errors and two independentlytrained models for specific errors. First, a graph model represents a Chinese sentence and a modified single source shortest path algorithm is performed on the graph to detect and correct generic spelling errors. Then, we utilize random fields to solve two specific kinds of common errors: the confusion of (at) (pinyin is ‘zai’ in Chinese), (again, more, then) (pinyin: zai) (of) (pinyin: de), (-ly, adverb-forming particle) (pinyin: de), (so that, have to) (pinyin: de). Finally, a rule based system is exploited to solve the pronoun usage confusions: (she) (pinyin: ta), (he) (pinyin: ta) and some others fixed collocation errors. The proposed model is evaluated on the standard data set released by the SIGHAN Bake-off 2014 shared task, and gives competitive result.</abstract>
<note confidence="0.554444142857143">work was partially supported by the National Natural Science Foundation of China (No. 60903119, No. 61170114, and No. 61272248), the National Basic Research Program of China (No. 2013CB329401), the Science and Technology Commission of Shanghai Municipality (No. 13511500200), the European Union Seventh Framework Program (No. 247619), the Cai Yuanpei Program (CSC fund</note>
<abstract confidence="0.968812">201304490199 and 201304490171), and the art and science interdiscipline funds of Shanghai Jiao Tong University (A study on mobilization mechanism and alerting threshold setting for online community, and media image and psychology evaluation: a computational intelligence approach).</abstract>
<intro confidence="0.527646">author.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Farooq Ahmad</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Learning a spelling error model from search query logs.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>955--962</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="6061" citStr="Ahmad and Kondrak, 2005" startWordPosition="925" endWordPosition="929"> model for generic spelling errors, conditional random fields (CRF) for two special errors and a rule based system for some collocation errors. 2 Related Work Over the past few years, there were many methods proposed for CSC task. (Sun et al., 2010) developed a phrase-based spelling error model from the clickthrough data by means of measuring the edit distance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first ca</context>
</contexts>
<marker>Ahmad, Kondrak, 2005</marker>
<rawString>Farooq Ahmad and Grzegorz Kondrak. 2005. Learning a spelling error model from search query logs. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages pp. 955–962, Vancouver, British Columbia, Canada, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard G Casey</author>
<author>Eric Lecolinet</author>
</authors>
<title>A survey of methods and strategies in character segmentation.</title>
<date>1996</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>18</volume>
<issue>7</issue>
<contexts>
<context position="10705" citStr="Casey and Lecolinet, 1996" startWordPosition="1690" endWordPosition="1693">cal machine translation model to translate the sentences containing typos into correct ones. In their model, the sentence with the highest translation probability which indicated how likely a typo was translated into its candidate correct word was chosen as the final correction sentence. 3 The Revised Graph Model The graph model (Jia et al., 2013b) of SIGHAN Bake-off 2013 is inspired by the idea of shortest path word segmentation algorithm which is based on the following assumption: a reasonable segmentation should maximize the lengths of all segments or minimize the total number of segments (Casey and Lecolinet, 1996). A directed acyclic graph (DAG) is thus built from the input sentence similar. The spelling error detection and correction problem is transformed to a single source shortest path (SSSP) problem on the DAG. Given a dictionary IID and a similar characters C, for a sentence 5 of m characters {c1, c2, ... , cm}, the original vertices V of the DAG in (Jia et al., 2013b) are: V ={wi,j|wi,j = ci ... cj E IID} U {wki,j|wki,j = ci ...c′k ... cj EIID, T &lt; j − i &lt; T, c′k EC[ck],k=i,i+1,...,j} U {w−,0, wn+1,−}. where w−,0 = “&lt;S&gt;” and wn+1,− = “&lt;/S&gt;” are two special vertices represent the start and end of</context>
</contexts>
<marker>Casey, Lecolinet, 1996</marker>
<rawString>Richard G Casey and Eric Lecolinet. 1996. A survey of methods and strategies in character segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 18(7):690–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chaohuang Chang</author>
</authors>
<title>A new approach for automatic Chinese spelling correction.</title>
<date>1995</date>
<booktitle>In Proceedings of Natural Language Processing Pacific Rim Symposium,</booktitle>
<pages>278--283</pages>
<location>Seoul,</location>
<contexts>
<context position="6800" citStr="Chang, 1995" startWordPosition="1063" endWordPosition="1064">entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences according to a bigram language model generated from a newspaper corpus. The method based on the motivation that all the typos were caused by either visual similarity or phonological similarity. So they manually built a confusion set as a key factor in their system. Although the method can detect misspelled words well, it was very time consuming for detection, generated too much false positive r</context>
</contexts>
<marker>Chang, 1995</marker>
<rawString>Chaohuang Chang. 1995. A new approach for automatic Chinese spelling correction. In Proceedings of Natural Language Processing Pacific Rim Symposium, pages pp. 278–283, Seoul, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1999</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>13</volume>
<issue>4</issue>
<contexts>
<context position="21929" citStr="Chen and Goodman, 1999" startWordPosition="3682" endWordPosition="3685">ta sets. Error Type Percent (%) Number 在, 再 101 2.18 的, 地, 得 398 8.61 她, 他 101 3.98 Table 10: Three common character usage confusions in the DEV14B. 5.2 The Improved Graph Model We treat the graph model without filters in Bakeoff 2013 as our baseline in Bake-off 2014. The edge function is the linear combination of similarity and log conditional probability: wL = ws � Q log P where wo - 0 which is omitted in the equation, and ws for different kinds of characters are shown in Table 11. The LM is set to bigram according to (Yang et al., 2012). Improved Kneser-Ney method is used for LM smoothing (Chen and Goodman, 1999). Type w3 same pronunciation same tone 1 same pronunciation different tone 1 similar pronunciation same tone 2 similar pronunciation different tone 2 similar shape 2 Table 11: ws used in wL. We utilize the correction precision (P), correction recall (R) and F1 score (.T) as the metrics. The computational formulas are as follows: • Correction precision: number of correctly corrected characters P = • Correction recall: number of correctly corrected characters R = • F1 macro: 2PR � = P + R, We firstly use the revised graph model in section 3 to tackle the continuous word errors. The results achie</context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>Stanley F Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech &amp; Language, 13(4):359–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuanyu Chen</author>
<author>Hungshin Lee</author>
<author>Chunghan Lee</author>
<author>Hsinmin Wang</author>
<author>Hsinhsi Chen</author>
</authors>
<title>A study of language modeling for Chinese spelling check.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>79--83</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="9657" citStr="Chen et al., 2013" startWordPosition="1520" endWordPosition="1523"> errors respectively. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to strengthen the candidate errors after word segmentation. The last category is formed by the methods which use word segmentation for detection and different models for correction (Liu et al., 2013; Chen et al., 2013; Chiu et al., 2013). (Liu et al., 2013) used support vector machine (SVM) to select the most probable sentence from multiple candidates. They used word segmentation and machine translation model to generate the candidates respectively. The SVM was used to rerank the candidates. (Chen et al., 2013) not only applied LM, but also used various topic models to cover the shortage of LM. (Chiu et al., 2013) explored statistical machine translation model to translate the sentences containing typos into correct ones. In their model, the sentence with the highest translation probability which indicated</context>
</contexts>
<marker>Chen, Lee, Lee, Wang, Chen, 2013</marker>
<rawString>Kuanyu Chen, Hungshin Lee, Chunghan Lee, Hsinmin Wang, and Hsinhsi Chen. 2013. A study of language modeling for Chinese spelling check. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 79–83, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsunwen Chiu</author>
<author>Jiancheng Wu</author>
<author>Jason S Chang</author>
</authors>
<title>Chinese spelling checker based on statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>49--53</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="9677" citStr="Chiu et al., 2013" startWordPosition="1524" endWordPosition="1527">y. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to strengthen the candidate errors after word segmentation. The last category is formed by the methods which use word segmentation for detection and different models for correction (Liu et al., 2013; Chen et al., 2013; Chiu et al., 2013). (Liu et al., 2013) used support vector machine (SVM) to select the most probable sentence from multiple candidates. They used word segmentation and machine translation model to generate the candidates respectively. The SVM was used to rerank the candidates. (Chen et al., 2013) not only applied LM, but also used various topic models to cover the shortage of LM. (Chiu et al., 2013) explored statistical machine translation model to translate the sentences containing typos into correct ones. In their model, the sentence with the highest translation probability which indicated how likely a typo w</context>
</contexts>
<marker>Chiu, Wu, Chang, 2013</marker>
<rawString>Hsunwen Chiu, Jiancheng Wu, and Jason S. Chang. 2013. Chinese spelling checker based on statistical machine translation. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 49–53, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Emerson</author>
</authors>
<title>The second international Chinese word segmentation Bakeoff.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>123--133</pages>
<location>Jeju Island,</location>
<contexts>
<context position="20818" citStr="Emerson, 2005" startWordPosition="3496" endWordPosition="3497">the statistics information of the three common character usage confusions in section 4 is shown in Table 10, so it is necessary to deal with them respectively. The dictionary IID used in SSSP algorithm is SogouW4 dictionary from Sogou inc., which is in simplified Chinese. The OpenCC5 converter is used for simplified-to-traditional Chinese convert4http://www.sogou.com/labs/dl/w.html 5http://code.google.com/p/opencc/ ing. Similar character set C provided by (Liu et al., 2010) is used to substitute the original words in the graph construction stage. The LM is built on the Academia Sinica corpus (Emerson, 2005) with IRSTLM toolkit (Federico et al., 2008). The CRF model is achieved by training and tuning on the Academia Sinica corpus with the toolkit CRF++ 0.586. For Chinese word segmentation, the ICTCLAS20117 is exploited. 6https://code.google.com/p/crfpp/downloads/list 7http://www.ictclas.org/ictclas_download. aspx 162 Name Data Size (lines) Character number (k) Development set Bake-off 2013 700 29 Bake-off 2014 C1 342 16 B1 3004 149 Test set 1062 53 Table 9: Statistical information of data sets. Error Type Percent (%) Number 在, 再 101 2.18 的, 地, 得 398 8.61 她, 他 101 3.98 Table 10: Three common chara</context>
</contexts>
<marker>Emerson, 2005</marker>
<rawString>Thomas Emerson. 2005. The second international Chinese word segmentation Bakeoff. In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing, pages pp. 123–133, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
</authors>
<title>IRSTLM: an open source toolkit for handling large scale language models.</title>
<date>2008</date>
<booktitle>In Proceedings of 9th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1618--1621</pages>
<location>Brisbane, Australia.</location>
<contexts>
<context position="20862" citStr="Federico et al., 2008" startWordPosition="3501" endWordPosition="3504">ree common character usage confusions in section 4 is shown in Table 10, so it is necessary to deal with them respectively. The dictionary IID used in SSSP algorithm is SogouW4 dictionary from Sogou inc., which is in simplified Chinese. The OpenCC5 converter is used for simplified-to-traditional Chinese convert4http://www.sogou.com/labs/dl/w.html 5http://code.google.com/p/opencc/ ing. Similar character set C provided by (Liu et al., 2010) is used to substitute the original words in the graph construction stage. The LM is built on the Academia Sinica corpus (Emerson, 2005) with IRSTLM toolkit (Federico et al., 2008). The CRF model is achieved by training and tuning on the Academia Sinica corpus with the toolkit CRF++ 0.586. For Chinese word segmentation, the ICTCLAS20117 is exploited. 6https://code.google.com/p/crfpp/downloads/list 7http://www.ictclas.org/ictclas_download. aspx 162 Name Data Size (lines) Character number (k) Development set Bake-off 2013 700 29 Bake-off 2014 C1 342 16 B1 3004 149 Test set 1062 53 Table 9: Statistical information of data sets. Error Type Percent (%) Number 在, 再 101 2.18 的, 地, 得 398 8.61 她, 他 101 3.98 Table 10: Three common character usage confusions in the DEV14B. 5.2 The</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, 2008</marker>
<rawString>Marcello Federico, Nicola Bertoldi, and Mauro Cettolo. 2008. IRSTLM: an open source toolkit for handling large scale language models. In Proceedings of 9th Annual Conference of the International Speech Communication Association, pages pp. 1618–1621, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaolong Li</author>
<author>Daniel Micol</author>
<author>Chris Quirk</author>
<author>Xu Sun</author>
</authors>
<title>A large scale ranker-based system for search query spelling correction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>358--366</pages>
<location>Beijing, China,</location>
<contexts>
<context position="5878" citStr="Gao et al., 2010" startWordPosition="902" endWordPosition="905">s paper, based on our previous work (Jia et al., 2013b) in SIGHAN Bake-off 2013, we describe an improved graph model to handle the CSC task. The improved model includes a graph model for generic spelling errors, conditional random fields (CRF) for two special errors and a rule based system for some collocation errors. 2 Related Work Over the past few years, there were many methods proposed for CSC task. (Sun et al., 2010) developed a phrase-based spelling error model from the clickthrough data by means of measuring the edit distance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zh</context>
</contexts>
<marker>Gao, Li, Micol, Quirk, Sun, 2010</marker>
<rawString>Jianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk, and Xu Sun. 2010. A large scale ranker-based system for search query spelling correction. In Proceedings of the 23rd International Conference on Computational Linguistics, pages pp. 358–366, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dongxu Han</author>
<author>Baobao Chang</author>
</authors>
<title>A maximum entropy approach to Chinese spelling check.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>74--78</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="6171" citStr="Han and Chang, 2013" startWordPosition="946" endWordPosition="949"> for some collocation errors. 2 Related Work Over the past few years, there were many methods proposed for CSC task. (Sun et al., 2010) developed a phrase-based spelling error model from the clickthrough data by means of measuring the edit distance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is use</context>
</contexts>
<marker>Han, Chang, 2013</marker>
<rawString>Dongxu Han and Baobao Chang. 2013. A maximum entropy approach to Chinese spelling check. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 74–78, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yu He</author>
<author>Guohong Fu</author>
</authors>
<title>Description of HLJU Chinese spelling checker for SIGHAN Bakeoff</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>84--87</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="8842" citStr="He and Fu, 2013" startWordPosition="1397" endWordPosition="1400"> again. If a new sentence resulted in a better word segmentation, spelling error was reported. Their system gave good detection recall but low false-alarm rate. The third category utilizes more than one approaches for detection and an LM for correction. (Hsieh et al., 2013) used two different systems for 158 error detection. The first system detected error characters based on unknown word detection and LM verification. The second one solved error detection based on a suggestion dictionary generated from a confusion set. Finally, two systems were combined to obtain the final detection result. (He and Fu, 2013) divided typos into three categories which were character-level errors (CLEs), word-level errors (WLEs) and context-level errors (CLEs), and three different methods were used to detect the different errors respectively. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to</context>
</contexts>
<marker>He, Fu, 2013</marker>
<rawString>Yu He and Guohong Fu. 2013. Description of HLJU Chinese spelling checker for SIGHAN Bakeoff 2013. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 84–87, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuming Hsieh</author>
<author>Minghong Bai</author>
<author>Kehjiann Chen</author>
</authors>
<title>Introduction to CKIP Chinese spelling check system for SIGHAN Bakeoff 2013 evaluation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>59--63</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="8500" citStr="Hsieh et al., 2013" startWordPosition="1343" endWordPosition="1346">methods that all single-character words are supposed to be errors and an LM is used for correction, for example (Lin and Chu, 2013) . They developed a system which supposed that all single-character words may be typos. They replaced all single-character words by similar characters using a confusion set and segmented the newly created sentences again. If a new sentence resulted in a better word segmentation, spelling error was reported. Their system gave good detection recall but low false-alarm rate. The third category utilizes more than one approaches for detection and an LM for correction. (Hsieh et al., 2013) used two different systems for 158 error detection. The first system detected error characters based on unknown word detection and LM verification. The second one solved error detection based on a suggestion dictionary generated from a confusion set. Finally, two systems were combined to obtain the final detection result. (He and Fu, 2013) divided typos into three categories which were character-level errors (CLEs), word-level errors (WLEs) and context-level errors (CLEs), and three different methods were used to detect the different errors respectively. In addition to using the result of wor</context>
</contexts>
<marker>Hsieh, Bai, Chen, 2013</marker>
<rawString>Yuming Hsieh, Minghong Bai, and Kehjiann Chen. 2013. Introduction to CKIP Chinese spelling check system for SIGHAN Bakeoff 2013 evaluation. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 59–63, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongye Jia</author>
<author>Peilu Wang</author>
<author>Hai Zhao</author>
</authors>
<title>Grammatical error correction as multiclass classification with single model.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>74--81</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5314" citStr="Jia et al., 2013" startWordPosition="807" endWordPosition="810">. Both examples have the same pinyin. Thus CSC cannot be directly applied those edit distance based methods which are commonly used for alphabetical languages. CSC task has to deal with word segmentation problem first, since misspelled sentence could not be segmented properly by word segmenter. There also exist Chinese spelling errors which are unrelated with word segmentation. For example, “好好地出去玩” in Example 2 of Table 1 is misspelled into “好好的出去玩”, but both of them have the same segmentation. So it is necessary to perform further specific process. In this paper, based on our previous work (Jia et al., 2013b) in SIGHAN Bake-off 2013, we describe an improved graph model to handle the CSC task. The improved model includes a graph model for generic spelling errors, conditional random fields (CRF) for two special errors and a rule based system for some collocation errors. 2 Related Work Over the past few years, there were many methods proposed for CSC task. (Sun et al., 2010) developed a phrase-based spelling error model from the clickthrough data by means of measuring the edit distance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach </context>
<context position="10427" citStr="Jia et al., 2013" startWordPosition="1645" endWordPosition="1648">egmentation and machine translation model to generate the candidates respectively. The SVM was used to rerank the candidates. (Chen et al., 2013) not only applied LM, but also used various topic models to cover the shortage of LM. (Chiu et al., 2013) explored statistical machine translation model to translate the sentences containing typos into correct ones. In their model, the sentence with the highest translation probability which indicated how likely a typo was translated into its candidate correct word was chosen as the final correction sentence. 3 The Revised Graph Model The graph model (Jia et al., 2013b) of SIGHAN Bake-off 2013 is inspired by the idea of shortest path word segmentation algorithm which is based on the following assumption: a reasonable segmentation should maximize the lengths of all segments or minimize the total number of segments (Casey and Lecolinet, 1996). A directed acyclic graph (DAG) is thus built from the input sentence similar. The spelling error detection and correction problem is transformed to a single source shortest path (SSSP) problem on the DAG. Given a dictionary IID and a similar characters C, for a sentence 5 of m characters {c1, c2, ... , cm}, the origina</context>
<context position="12609" citStr="Jia et al., 2013" startWordPosition="2029" endWordPosition="2032">idering efficiency, we only deal with the continuous errors with 2 characters. The revised V are: V ={wi,j|wi,j = ci ... cj E IID} U {wki,j|wki,j = ci ... c′k ... cj E IID, T &lt; j − i &lt; T, c′k E C[ck],k = i,i + 1,...,j} U {wl|wl = c′lc′l+1 E IID, c′l, c′l+1 E C} U {w−,0, wn+1,−}. With the modified DAG G, the “建缸” (pinyin: jian gang) is substituted as “健康” (health) (pinyin: jian kang), “峴港” (Danang) (pinyin: xian gang), “潛航” (submerge) (pinyin: qian hang) and so on, which have already contained the desired correction. 159 4 The Improved Graph Model The graph model based on word segmentation in (Jia et al., 2013b) includes the revised graph model in section 3 still has its limitations. For a sentence, in the graph construction stage, the substitution is only applied to the situation that the number of words after segmenting has to be decreased, which means there exists new longer word after segmentation. In addition, if the segmentation result of a sentence is a single character, the graph model does not work, because a single character will not be substituted. For example in the following two sentences, the “他” (he) (pinyin: ta) in the first sentence should be corrected into “她” (she) (pinyin: ta) a</context>
<context position="15008" citStr="Jia et al., 2013" startWordPosition="2428" endWordPosition="2431"> Model Two classifiers using CRF model are respectively trained to tackle the common character usage confusions: 在” (at) (pinyin: zai), 再” (again, more, then) (pinyin: zai) and “的” (of)(pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de). We assume that the correct character selection is related with its neighboring two words and part-of-speech (POS) tags. The classifiers are trained on a large fivegram token set which is extracted from a large POS tagged corpus. The feature selection algorithm is according to (Zhao et al., 2013; Wang et al., 2014; Jia et al., 2013a). The feature set for CRF model is as follows: wj,−2, posj,−2, wj,−1, posj,−1, wj,0, posj,0, wj,1, posj,1, wj,2, posj,2 where j is the token index to indicate its position, wj,0 is the current candidate character and posj,0 is its POS tag. ICTCLAS (Zhang et al., 2003) is adopted for POS tagging. A set of feature strings that we used are presented in Table 2. The labels for “的” (of) (pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de) are 1, 2, 3 and “在” (at) (pinyin: zai), “再” (again, more, then) (pinyin: zai) are 1, 2. 4.2 The Rule Based System T</context>
</contexts>
<marker>Jia, Wang, Zhao, 2013</marker>
<rawString>Zhongye Jia, Peilu Wang, and Hai Zhao. 2013a. Grammatical error correction as multiclass classification with single model. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages pp. 74–81, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongye Jia</author>
<author>Peilu Wang</author>
<author>Hai Zhao</author>
</authors>
<title>Graph model for Chinese spell checking.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>88--92</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="5314" citStr="Jia et al., 2013" startWordPosition="807" endWordPosition="810">. Both examples have the same pinyin. Thus CSC cannot be directly applied those edit distance based methods which are commonly used for alphabetical languages. CSC task has to deal with word segmentation problem first, since misspelled sentence could not be segmented properly by word segmenter. There also exist Chinese spelling errors which are unrelated with word segmentation. For example, “好好地出去玩” in Example 2 of Table 1 is misspelled into “好好的出去玩”, but both of them have the same segmentation. So it is necessary to perform further specific process. In this paper, based on our previous work (Jia et al., 2013b) in SIGHAN Bake-off 2013, we describe an improved graph model to handle the CSC task. The improved model includes a graph model for generic spelling errors, conditional random fields (CRF) for two special errors and a rule based system for some collocation errors. 2 Related Work Over the past few years, there were many methods proposed for CSC task. (Sun et al., 2010) developed a phrase-based spelling error model from the clickthrough data by means of measuring the edit distance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach </context>
<context position="10427" citStr="Jia et al., 2013" startWordPosition="1645" endWordPosition="1648">egmentation and machine translation model to generate the candidates respectively. The SVM was used to rerank the candidates. (Chen et al., 2013) not only applied LM, but also used various topic models to cover the shortage of LM. (Chiu et al., 2013) explored statistical machine translation model to translate the sentences containing typos into correct ones. In their model, the sentence with the highest translation probability which indicated how likely a typo was translated into its candidate correct word was chosen as the final correction sentence. 3 The Revised Graph Model The graph model (Jia et al., 2013b) of SIGHAN Bake-off 2013 is inspired by the idea of shortest path word segmentation algorithm which is based on the following assumption: a reasonable segmentation should maximize the lengths of all segments or minimize the total number of segments (Casey and Lecolinet, 1996). A directed acyclic graph (DAG) is thus built from the input sentence similar. The spelling error detection and correction problem is transformed to a single source shortest path (SSSP) problem on the DAG. Given a dictionary IID and a similar characters C, for a sentence 5 of m characters {c1, c2, ... , cm}, the origina</context>
<context position="12609" citStr="Jia et al., 2013" startWordPosition="2029" endWordPosition="2032">idering efficiency, we only deal with the continuous errors with 2 characters. The revised V are: V ={wi,j|wi,j = ci ... cj E IID} U {wki,j|wki,j = ci ... c′k ... cj E IID, T &lt; j − i &lt; T, c′k E C[ck],k = i,i + 1,...,j} U {wl|wl = c′lc′l+1 E IID, c′l, c′l+1 E C} U {w−,0, wn+1,−}. With the modified DAG G, the “建缸” (pinyin: jian gang) is substituted as “健康” (health) (pinyin: jian kang), “峴港” (Danang) (pinyin: xian gang), “潛航” (submerge) (pinyin: qian hang) and so on, which have already contained the desired correction. 159 4 The Improved Graph Model The graph model based on word segmentation in (Jia et al., 2013b) includes the revised graph model in section 3 still has its limitations. For a sentence, in the graph construction stage, the substitution is only applied to the situation that the number of words after segmenting has to be decreased, which means there exists new longer word after segmentation. In addition, if the segmentation result of a sentence is a single character, the graph model does not work, because a single character will not be substituted. For example in the following two sentences, the “他” (he) (pinyin: ta) in the first sentence should be corrected into “她” (she) (pinyin: ta) a</context>
<context position="15008" citStr="Jia et al., 2013" startWordPosition="2428" endWordPosition="2431"> Model Two classifiers using CRF model are respectively trained to tackle the common character usage confusions: 在” (at) (pinyin: zai), 再” (again, more, then) (pinyin: zai) and “的” (of)(pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de). We assume that the correct character selection is related with its neighboring two words and part-of-speech (POS) tags. The classifiers are trained on a large fivegram token set which is extracted from a large POS tagged corpus. The feature selection algorithm is according to (Zhao et al., 2013; Wang et al., 2014; Jia et al., 2013a). The feature set for CRF model is as follows: wj,−2, posj,−2, wj,−1, posj,−1, wj,0, posj,0, wj,1, posj,1, wj,2, posj,2 where j is the token index to indicate its position, wj,0 is the current candidate character and posj,0 is its POS tag. ICTCLAS (Zhang et al., 2003) is adopted for POS tagging. A set of feature strings that we used are presented in Table 2. The labels for “的” (of) (pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de) are 1, 2, 3 and “在” (at) (pinyin: zai), “再” (again, more, then) (pinyin: zai) are 1, 2. 4.2 The Rule Based System T</context>
</contexts>
<marker>Jia, Wang, Zhao, 2013</marker>
<rawString>Zhongye Jia, Peilu Wang, and Hai Zhao. 2013b. Graph model for Chinese spell checking. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 88–92, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chuanjie Lin</author>
<author>Weicheng Chu</author>
</authors>
<title>NTOU Chinese spelling check system in SIGHAN Bakeoff</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>102--107</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="8012" citStr="Lin and Chu, 2013" startWordPosition="1265" endWordPosition="1268">tive results and was not able to refer to an entire paragraph. (Yu et al., 2013) developed a joint error detection and correction system. The method assumed that all characters in the sentence may be errors and replaced every character using a confusion set. Then they segmented all new generated sentences and gave a score of the segmentation using LM for every sentence. In fact, this method did not always perform well according to (Yu et al., 2013). The second category includes the methods that all single-character words are supposed to be errors and an LM is used for correction, for example (Lin and Chu, 2013) . They developed a system which supposed that all single-character words may be typos. They replaced all single-character words by similar characters using a confusion set and segmented the newly created sentences again. If a new sentence resulted in a better word segmentation, spelling error was reported. Their system gave good detection recall but low false-alarm rate. The third category utilizes more than one approaches for detection and an LM for correction. (Hsieh et al., 2013) used two different systems for 158 error detection. The first system detected error characters based on unknown</context>
</contexts>
<marker>Lin, Chu, 2013</marker>
<rawString>Chuanjie Lin and Weicheng Chu. 2013. NTOU Chinese spelling check system in SIGHAN Bakeoff 2013. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 102–107, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chaolin Liu</author>
<author>Minhua Lai</author>
<author>Yihsuan Chuang</author>
<author>Chiaying Lee</author>
</authors>
<title>Visually and phonologically similar characters in incorrect simplified Chinese words.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters,</booktitle>
<pages>739--747</pages>
<location>Beijing, China,</location>
<contexts>
<context position="20682" citStr="Liu et al., 2010" startWordPosition="3471" endWordPosition="3474">ously corrected. DEV13, DEV14C and DEV14B and the test set is named as TEST14 respectively. In the DEV14B, there are 4624 errors, in which the statistics information of the three common character usage confusions in section 4 is shown in Table 10, so it is necessary to deal with them respectively. The dictionary IID used in SSSP algorithm is SogouW4 dictionary from Sogou inc., which is in simplified Chinese. The OpenCC5 converter is used for simplified-to-traditional Chinese convert4http://www.sogou.com/labs/dl/w.html 5http://code.google.com/p/opencc/ ing. Similar character set C provided by (Liu et al., 2010) is used to substitute the original words in the graph construction stage. The LM is built on the Academia Sinica corpus (Emerson, 2005) with IRSTLM toolkit (Federico et al., 2008). The CRF model is achieved by training and tuning on the Academia Sinica corpus with the toolkit CRF++ 0.586. For Chinese word segmentation, the ICTCLAS20117 is exploited. 6https://code.google.com/p/crfpp/downloads/list 7http://www.ictclas.org/ictclas_download. aspx 162 Name Data Size (lines) Character number (k) Development set Bake-off 2013 700 29 Bake-off 2014 C1 342 16 B1 3004 149 Test set 1062 53 Table 9: Stati</context>
</contexts>
<marker>Liu, Lai, Chuang, Lee, 2010</marker>
<rawString>Chaolin Liu, Minhua Lai, Yihsuan Chuang, and Chiaying Lee. 2010. Visually and phonologically similar characters in incorrect simplified Chinese words. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages pp. 739–747, Beijing, China, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Liu</author>
<author>Kevin Cheng</author>
<author>Yanyan Luo</author>
<author>Kevin Duh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A hybrid Chinese spelling correction using language model and statistical machine translation with reranking.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>54--58</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="9638" citStr="Liu et al., 2013" startWordPosition="1516" endWordPosition="1519">tect the different errors respectively. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to strengthen the candidate errors after word segmentation. The last category is formed by the methods which use word segmentation for detection and different models for correction (Liu et al., 2013; Chen et al., 2013; Chiu et al., 2013). (Liu et al., 2013) used support vector machine (SVM) to select the most probable sentence from multiple candidates. They used word segmentation and machine translation model to generate the candidates respectively. The SVM was used to rerank the candidates. (Chen et al., 2013) not only applied LM, but also used various topic models to cover the shortage of LM. (Chiu et al., 2013) explored statistical machine translation model to translate the sentences containing typos into correct ones. In their model, the sentence with the highest translation probabil</context>
</contexts>
<marker>Liu, Cheng, Luo, Duh, Matsumoto, 2013</marker>
<rawString>Xiaodong Liu, Kevin Cheng, Yanyan Luo, Kevin Duh, and Yuji Matsumoto. 2013. A hybrid Chinese spelling correction using language model and statistical machine translation with reranking. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp.54–58, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Jianfeng Gao</author>
<author>Daniel Micol</author>
<author>Chris Quirk</author>
</authors>
<title>Learning phrase-based spelling error models from clickthrough data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>266--274</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="5686" citStr="Sun et al., 2010" startWordPosition="871" endWordPosition="874">entation. For example, “好好地出去玩” in Example 2 of Table 1 is misspelled into “好好的出去玩”, but both of them have the same segmentation. So it is necessary to perform further specific process. In this paper, based on our previous work (Jia et al., 2013b) in SIGHAN Bake-off 2013, we describe an improved graph model to handle the CSC task. The improved model includes a graph model for generic spelling errors, conditional random fields (CRF) for two special errors and a rule based system for some collocation errors. 2 Related Work Over the past few years, there were many methods proposed for CSC task. (Sun et al., 2010) developed a phrase-based spelling error model from the clickthrough data by means of measuring the edit distance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on </context>
</contexts>
<marker>Sun, Gao, Micol, Quirk, 2010</marker>
<rawString>Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk. 2010. Learning phrase-based spelling error models from clickthrough data. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages pp. 266–274, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peilu Wang</author>
<author>Zhongye Jia</author>
<author>Hai Zhao</author>
</authors>
<title>Grammatical error detection and correction using a single maximum entropy model.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>74--82</pages>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="14990" citStr="Wang et al., 2014" startWordPosition="2424" endWordPosition="2427">ion errors. 4.1 CRF Model Two classifiers using CRF model are respectively trained to tackle the common character usage confusions: 在” (at) (pinyin: zai), 再” (again, more, then) (pinyin: zai) and “的” (of)(pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de). We assume that the correct character selection is related with its neighboring two words and part-of-speech (POS) tags. The classifiers are trained on a large fivegram token set which is extracted from a large POS tagged corpus. The feature selection algorithm is according to (Zhao et al., 2013; Wang et al., 2014; Jia et al., 2013a). The feature set for CRF model is as follows: wj,−2, posj,−2, wj,−1, posj,−1, wj,0, posj,0, wj,1, posj,1, wj,2, posj,2 where j is the token index to indicate its position, wj,0 is the current candidate character and posj,0 is its POS tag. ICTCLAS (Zhang et al., 2003) is adopted for POS tagging. A set of feature strings that we used are presented in Table 2. The labels for “的” (of) (pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de) are 1, 2, 3 and “在” (at) (pinyin: zai), “再” (again, more, then) (pinyin: zai) are 1, 2. 4.2 The R</context>
</contexts>
<marker>Wang, Jia, Zhao, 2014</marker>
<rawString>Peilu Wang, Zhongye Jia, and Hai Zhao. 2014. Grammatical error detection and correction using a single maximum entropy model. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages pp. 74–82, Baltimore, Maryland, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shihhung Wu</author>
<author>Chaolin Liu</author>
<author>Lunghao Lee</author>
</authors>
<title>Chinese spelling check evaluation at SIGHAN Bakeoff</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>35--42</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="17623" citStr="Wu et al., 2013" startWordPosition="2911" endWordPosition="2914">他 她 (母 and 父) or (女 and 男) or 妹 or 姊 or 姐 or 婆 or (太太 and 先生) 阿姨 or 太太 她 or 媽 or 母 or 女 or 他 or 爸 or 父 or 男 or 她 他 妹 or 姊 or 姐 or 婆 or 哥 or 先生 阿姨 or 太太 Table 3: Specific rules for the pronouns “她、他” confusion. w[i] pos[i + 1] corrected w[i] 阿 w 啊 馬 or 碼門 w 嗎 把 r, n 們 r, n 吧 Table 4: Rule 1. The correction related with right neighbored POS tag. 5 Experiments 5.1 Data Sets and Resources The proposed method is evaluated on the data sets of SIGHAN Bake-off shared tasks in 2013 and 2014. In Bake-off 2013, the sentences were collected from 13 to 14-year-old students’ essays in formal written tests (Wu et al., 2013). In Bakeoff 2014, the sentences were collected from Chinese as a foreign language (CFL) learners’ essays selected from the National Taiwan Normal University (NTNU) learner corpus3. All the data sets are in traditional Chinese. In Bake-off 2013, the essays were manually annotated with different labels (see Figure 1). There is at most one error in each sentence. However, the development set in Bake-off 2014 is enlarged and the error types (see Figure 2) are more diverse. 3http://www.cipsc.org.cn/clp2014/ webpage/en/four_bakeoffs/Bakeoff2014cfp_ ChtSpellingCheck_en.htm More than one error might </context>
</contexts>
<marker>Wu, Liu, Lee, 2013</marker>
<rawString>Shihhung Wu, Chaolin Liu, and Lunghao Lee. 2013. Chinese spelling check evaluation at SIGHAN Bakeoff 2013. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 35–42, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shaohua Yang</author>
<author>Hai Zhao</author>
<author>Xiaolin Wang</author>
<author>Baoliang Lu</author>
</authors>
<title>Spell checking for Chinese.</title>
<date>2012</date>
<booktitle>In International Conference on Language Resources and Evaluation,</booktitle>
<pages>730--736</pages>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="21851" citStr="Yang et al., 2012" startWordPosition="3670" endWordPosition="3673">42 16 B1 3004 149 Test set 1062 53 Table 9: Statistical information of data sets. Error Type Percent (%) Number 在, 再 101 2.18 的, 地, 得 398 8.61 她, 他 101 3.98 Table 10: Three common character usage confusions in the DEV14B. 5.2 The Improved Graph Model We treat the graph model without filters in Bakeoff 2013 as our baseline in Bake-off 2014. The edge function is the linear combination of similarity and log conditional probability: wL = ws � Q log P where wo - 0 which is omitted in the equation, and ws for different kinds of characters are shown in Table 11. The LM is set to bigram according to (Yang et al., 2012). Improved Kneser-Ney method is used for LM smoothing (Chen and Goodman, 1999). Type w3 same pronunciation same tone 1 same pronunciation different tone 1 similar pronunciation same tone 2 similar pronunciation different tone 2 similar shape 2 Table 11: ws used in wL. We utilize the correction precision (P), correction recall (R) and F1 score (.T) as the metrics. The computational formulas are as follows: • Correction precision: number of correctly corrected characters P = • Correction recall: number of correctly corrected characters R = • F1 macro: 2PR � = P + R, We firstly use the revised gr</context>
</contexts>
<marker>Yang, Zhao, Wang, Lu, 2012</marker>
<rawString>Shaohua Yang, Hai Zhao, Xiaolin Wang, and Baoliang Lu. 2012. Spell checking for Chinese. In International Conference on Language Resources and Evaluation, pages pp. 730–736, Istanbul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tinghao Yang</author>
<author>Yulun Hsieh</author>
<author>Yuhsuan Chen</author>
<author>Michael Tsang</author>
<author>Chengwei Shih</author>
<author>Wenlian Hsu</author>
</authors>
<title>Sinica-IASL Chinese spelling check system at SIGHAN-7.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>93--96</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="9331" citStr="Yang et al., 2013" startWordPosition="1470" endWordPosition="1473">ionary generated from a confusion set. Finally, two systems were combined to obtain the final detection result. (He and Fu, 2013) divided typos into three categories which were character-level errors (CLEs), word-level errors (WLEs) and context-level errors (CLEs), and three different methods were used to detect the different errors respectively. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to strengthen the candidate errors after word segmentation. The last category is formed by the methods which use word segmentation for detection and different models for correction (Liu et al., 2013; Chen et al., 2013; Chiu et al., 2013). (Liu et al., 2013) used support vector machine (SVM) to select the most probable sentence from multiple candidates. They used word segmentation and machine translation model to generate the candidates respectively. The SVM was used to rerank the candid</context>
</contexts>
<marker>Yang, Hsieh, Chen, Tsang, Shih, Hsu, 2013</marker>
<rawString>Tinghao Yang, Yulun Hsieh, Yuhsuan Chen, Michael Tsang, Chengwei Shih, and Wenlian Hsu. 2013. Sinica-IASL Chinese spelling check system at SIGHAN-7. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 93–96, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juifeng Yeh</author>
<author>Shengfeng Li</author>
<author>Meirong Wu</author>
<author>Wenyi Chen</author>
<author>Maochuan Su</author>
</authors>
<title>Chinese word spelling correction based on N-gram ranked inverted index list.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>43--48</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="9148" citStr="Yeh et al., 2013" startWordPosition="1443" endWordPosition="1446"> 158 error detection. The first system detected error characters based on unknown word detection and LM verification. The second one solved error detection based on a suggestion dictionary generated from a confusion set. Finally, two systems were combined to obtain the final detection result. (He and Fu, 2013) divided typos into three categories which were character-level errors (CLEs), word-level errors (WLEs) and context-level errors (CLEs), and three different methods were used to detect the different errors respectively. In addition to using the result of word segmentation for detection, (Yeh et al., 2013) also proposed a dictionarybased method to detect spelling errors. The dictionary contained similar pronunciation and shape information for each Chinese character. (Yang et al., 2013) proposed another method to improve the candidate detections. They employed high confidence pattern matching to strengthen the candidate errors after word segmentation. The last category is formed by the methods which use word segmentation for detection and different models for correction (Liu et al., 2013; Chen et al., 2013; Chiu et al., 2013). (Liu et al., 2013) used support vector machine (SVM) to select the mo</context>
</contexts>
<marker>Yeh, Li, Wu, Chen, Su, 2013</marker>
<rawString>Juifeng Yeh, Shengfeng Li, Meirong Wu, Wenyi Chen, and Maochuan Su. 2013. Chinese word spelling correction based on N-gram ranked inverted index list. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 43–48, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liangchih Yu</author>
<author>Chaohong Liu</author>
<author>Chunghsien Wu</author>
</authors>
<title>Candidate scoring using web-based measure for Chinese spelling error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>108--112</pages>
<location>Nagoya, Japan,</location>
<contexts>
<context position="6818" citStr="Yu et al., 2013" startWordPosition="1065" endWordPosition="1068">s for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences according to a bigram language model generated from a newspaper corpus. The method based on the motivation that all the typos were caused by either visual similarity or phonological similarity. So they manually built a confusion set as a key factor in their system. Although the method can detect misspelled words well, it was very time consuming for detection, generated too much false positive results and was not</context>
</contexts>
<marker>Yu, Liu, Wu, 2013</marker>
<rawString>Liangchih Yu, Chaohong Liu, and Chunghsien Wu. 2013. Candidate scoring using web-based measure for Chinese spelling error correction. In Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing, pages pp. 108–112, Nagoya, Japan, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huaping Zhang</author>
<author>Hongkui Yu</author>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
</authors>
<title>HHMM-based Chinese lexical analyzer ICTCLAS.</title>
<date>2003</date>
<booktitle>In Proceedings of the second SIGHAN workshop on Chinese language processing,</booktitle>
<pages>184--187</pages>
<contexts>
<context position="15278" citStr="Zhang et al., 2003" startWordPosition="2474" endWordPosition="2477">o) (pinyin: de). We assume that the correct character selection is related with its neighboring two words and part-of-speech (POS) tags. The classifiers are trained on a large fivegram token set which is extracted from a large POS tagged corpus. The feature selection algorithm is according to (Zhao et al., 2013; Wang et al., 2014; Jia et al., 2013a). The feature set for CRF model is as follows: wj,−2, posj,−2, wj,−1, posj,−1, wj,0, posj,0, wj,1, posj,1, wj,2, posj,2 where j is the token index to indicate its position, wj,0 is the current candidate character and posj,0 is its POS tag. ICTCLAS (Zhang et al., 2003) is adopted for POS tagging. A set of feature strings that we used are presented in Table 2. The labels for “的” (of) (pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de) are 1, 2, 3 and “在” (at) (pinyin: zai), “再” (again, more, then) (pinyin: zai) are 1, 2. 4.2 The Rule Based System To effectively handle pronoun usage errors for “她” (she) (pinyin: ta), and “他” (he) (pinyin: ta) and other collocation errors, we design a rule based system extracted from the development set. The Table 3 is the rules we set for solving the pronoun usage errors, where th</context>
</contexts>
<marker>Zhang, Yu, Xiong, Liu, 2003</marker>
<rawString>Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Liu. 2003. HHMM-based Chinese lexical analyzer ICTCLAS. In Proceedings of the second SIGHAN workshop on Chinese language processing, pages pp. 184–187, Sapporo,Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Incorporating global information into supervised learning for Chinese word segmentation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics,</booktitle>
<pages>66--74</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="6494" citStr="Zhao and Kit, 2007" startWordPosition="1003" endWordPosition="1006">0) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences according to a bigram language model generated from a newspaper corpus. The method based on</context>
</contexts>
<marker>Zhao, Kit, 2007</marker>
<rawString>Hai Zhao and Chunyu Kit. 2007. Incorporating global information into supervised learning for Chinese word segmentation. In Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics, pages pp. 66–74, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>An empirical comparison of goodness measures for unsupervised Chinese word segmentation with a unified framework.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>9--16</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="6432" citStr="Zhao and Kit, 2008" startWordPosition="991" endWordPosition="994">ut query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences according to a bigram languag</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008a. An empirical comparison of goodness measures for unsupervised Chinese word segmentation with a unified framework. In Proceedings of the Third International Joint Conference on Natural Language Processing, pages pp. 9–16, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing,</booktitle>
<pages>106--111</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="6432" citStr="Zhao and Kit, 2008" startWordPosition="991" endWordPosition="994">ut query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences according to a bigram languag</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008b. Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition. In Proceedings of the Third International Joint Conference on Natural Language Processing, pages pp. 106–111, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Integrating unsupervised and supervised word segmentation: The role of goodness measures.</title>
<date>2011</date>
<journal>Information Sciences,</journal>
<volume>181</volume>
<issue>1</issue>
<contexts>
<context position="6514" citStr="Zhao and Kit, 2011" startWordPosition="1007" endWordPosition="1010">er-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences according to a bigram language model generated from a newspaper corpus. The method based on the motivation that</context>
</contexts>
<marker>Zhao, Kit, 2011</marker>
<rawString>Hai Zhao and Chunyu Kit. 2011. Integrating unsupervised and supervised word segmentation: The role of goodness measures. Information Sciences, 181(1):163–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
</authors>
<title>An improved Chinese word segmentation system with conditional random field.</title>
<date>2006</date>
<booktitle>In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<volume>1082117</volume>
<pages>162--165</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="6411" citStr="Zhao et al., 2006" startWordPosition="987" endWordPosition="990">tance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences accordin</context>
</contexts>
<marker>Zhao, Huang, Li, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, and Mu Li. 2006a. An improved Chinese word segmentation system with conditional random field. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, volume 1082117, pages pp. 162–165, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>BaoLiang Lu</author>
</authors>
<title>Effective tag set selection in Chinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<volume>20</volume>
<pages>87--94</pages>
<location>Wuhan,</location>
<contexts>
<context position="6411" citStr="Zhao et al., 2006" startWordPosition="987" endWordPosition="990">tance between an input query and the optimal spelling correction. (Gao et al., 2010) explored the ranker-based approach which included visual similarity, phonological similarity, dictionary, and frequency features for large scale web search. (Ahmad and Kondrak, 2005) proposed a spelling error model from search query logs to improve the quality of query. (Han and Chang, 2013) employed maximum entropy models for CSC. They trained a maximum entropy model for each Chinese character based on a large raw corpus and used the model to detect the spelling errors. Two key techniques, word segmentation (Zhao et al., 2006a; Zhao and Kit, 2008b; Zhao et al., 2006b; Zhao and Kit, 2008a; Zhao and Kit, 2007; Zhao and Kit, 2011; Zhao et al., 2010) and language model (LM), are also popularly used for CSC. Most of those approaches can fall into four categories. The first category consists of the methods that all the characters in a sentence are assumed to be errors and an LM is used for correction (Chang, 1995; Yu et al., 2013). (Chang, 1995) proposed a method that replaced each character in the sentence based on a confusion set and computed the probability of the original sentence and all modified sentences accordin</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and BaoLiang Lu. 2006b. Effective tag set selection in Chinese word segmentation via conditional random field modeling. In Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation, volume 20, pages pp. 87–94, Wuhan, China.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hai Zhao</author>
</authors>
<title>Chang-Ning Huang,</title>
<location>Mu Li, and Bao-</location>
<marker>Zhao, </marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Lu</author>
</authors>
<title>A unified character-based tagging framework for Chinese word segmentation.</title>
<date>2010</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>9</volume>
<issue>2</issue>
<marker>Lu, 2010</marker>
<rawString>Liang Lu. 2010. A unified character-based tagging framework for Chinese word segmentation. ACM Transactions on Asian Language Information Processing, 9(2):1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Xiaotian Zhang</author>
<author>Chunyu Kit</author>
</authors>
<title>Integrative semantic dependency parsing via efficient large-scale feature selection.</title>
<date>2013</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>46--203</pages>
<contexts>
<context position="14971" citStr="Zhao et al., 2013" startWordPosition="2420" endWordPosition="2423">n: ta) and collocation errors. 4.1 CRF Model Two classifiers using CRF model are respectively trained to tackle the common character usage confusions: 在” (at) (pinyin: zai), 再” (again, more, then) (pinyin: zai) and “的” (of)(pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de). We assume that the correct character selection is related with its neighboring two words and part-of-speech (POS) tags. The classifiers are trained on a large fivegram token set which is extracted from a large POS tagged corpus. The feature selection algorithm is according to (Zhao et al., 2013; Wang et al., 2014; Jia et al., 2013a). The feature set for CRF model is as follows: wj,−2, posj,−2, wj,−1, posj,−1, wj,0, posj,0, wj,1, posj,1, wj,2, posj,2 where j is the token index to indicate its position, wj,0 is the current candidate character and posj,0 is its POS tag. ICTCLAS (Zhang et al., 2003) is adopted for POS tagging. A set of feature strings that we used are presented in Table 2. The labels for “的” (of) (pinyin: de), “地” (-ly, adverb-forming particle) (pinyin: de), “得”(so that, have to) (pinyin: de) are 1, 2, 3 and “在” (at) (pinyin: zai), “再” (again, more, then) (pinyin: zai) </context>
</contexts>
<marker>Zhao, Zhang, Kit, 2013</marker>
<rawString>Hai Zhao, Xiaotian Zhang, and Chunyu Kit. 2013. Integrative semantic dependency parsing via efficient large-scale feature selection. Journal of Artificial Intelligence Research, 46:203–233.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
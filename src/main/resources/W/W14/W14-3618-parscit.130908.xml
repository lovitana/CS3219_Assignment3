<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002959">
<title confidence="0.992471">
CMUQ@QALB-2014: An SMT-based System
for Automatic Arabic Error Correction
</title>
<author confidence="0.993703">
Serena Jeblee1 , Houda Bouamor2, Wajdi Zaghouani2 and Kemal Oflazer2
</author>
<affiliation confidence="0.944963">
1Carnegie Mellon University
</affiliation>
<email confidence="0.986175">
sjeblee@cs.cmu.edu
</email>
<note confidence="0.635763">
2Carnegie Mellon University in Qatar
</note>
<email confidence="0.997812">
{hbouamor,wajdiz}@qatar.cmu.edu, ko@cs.cmu.edu
</email>
<sectionHeader confidence="0.993874" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999473">
In this paper, we describe the CMUQ sys-
tem we submitted to The ANLP-QALB 2014
Shared Task on Automatic Text Correction
for Arabic. Our system combines rule-based
linguistic techniques with statistical language
modeling techniques and machine translation-
based methods. Our system outperforms the
baseline and reaches an F-score of 65.42% on
the test set of QALB corpus. This ranks us 3rd
in the competition.
</bodyText>
<sectionHeader confidence="0.998792" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999584854545454">
The business of text creation and editing represents a
large market where NLP technologies might be applied
naturally (Dale, 1997). Today’s users of word proces-
sors get surprisingly little help in checking spelling,
and a small number of them use more sophisticated
tools such as grammar checkers, to provide help in en-
suring that a text remains grammatically accurate after
modification. For instance, in the Arabic version of Mi-
crosoft Word, the spelling checker for Arabic, does not
give reasonable and natural proposals for many real-
word errors and even for simple probable errors (Had-
dad and Yaseen, 2007).
With the increased usage of computers in the pro-
cessing of natural languages comes the need for cor-
recting errors introduced at different stages. Natu-
ral language errors are not only made by human op-
erators at the input stage but also by NLP systems
that produce natural language output. Machine trans-
lation (MT), or optical character recognition (OCR),
often produce incorrect output riddled with odd lexi-
cal choices, grammar errors, or incorrectly recognized
characters. Correcting human/machine-produced er-
rors, or post-editing, can be manual or automated. For
morphologically and syntactically complex languages,
such as Modern Standard Arabic (MSA), correcting
texts automatically requires complex human and ma-
chine processing which makes generation of correct
candidates a challenging task.
For instance, the Automatic Arabic Text Correction
Shared Task is an interesting testbed to develop and
evaluate spelling correction systems for Arabic trained
either on naturally occurring errors in texts written by
humans (e.g., non-native speakers), or machines (e.g.,
MT output). In such tasks, participants are asked to
implement a system that takes as input Modern Stan-
dard Arabic texts with various spelling errors and au-
tomatically correct them. In this paper, we describe
the CMUQ system we developed to participate in the
The First Shared Task on Automatic Text Correction
for Arabic (Mohit et al., 2014). Our system combines
rule-based linguistic techniques with statistical lan-
guage modeling techniques and machine translation-
based methods. Our system outperforms the baseline,
achieves a better correction quality and reaches an F-
score of 62.96% on the development set of QALB cor-
pus (Zaghouani et al., 2014) and 65.42% on the test set.
The remainder of this paper is organized as follows.
First, we review the main previous efforts for automatic
spelling correction, in Section 2. In Section 3, we de-
scribe our system, which consists of several modules.
We continue with our experiments on the shared task
2014 dev set (Section 4). Then, we give an analysis of
our system output in Section 5. Finally, we conclude
and hint towards future improvement of the system, in
Section 6.
</bodyText>
<sectionHeader confidence="0.999694" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999971318181818">
Automatic error detection and correction include auto-
matic spelling checking, grammar checking and post-
editing. Numerous approaches (both supervised and
unsupervised) have been explored to improve the flu-
ency of the text and reduce the percentage of out-
of-vocabulary words using NLP tools, resources, and
heuristics, e.g., morphological analyzers, language
models, and edit-distance measure (Kukich, 1992;
Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan
et al., 2003; Haddad and Yaseen, 2007; Hassan et al.,
2008; Habash, 2008; Shaalan et al., 2010). There has
been a lot of work on error correction for English (e.g.,
(Golding and Roth, 1999)). Other approaches learn
models of correction by training on paired examples
of errors and their corrections, which is the main goal
of this work.
For Arabic, this issue was studied in various direc-
tions and in different research work. In 2003, Shaalan
et al. (2003) presented work on the specification and
classification of spelling errors in Arabic. Later on,
Haddad and Yaseen (2007) presented a hybrid ap-
proach using morphological features and rules to fine
</bodyText>
<page confidence="0.990116">
137
</page>
<bodyText confidence="0.967797242424242">
Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 137–142,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
tune the word recognition and non-word correction
method. In order to build an Arabic spelling checker,
Attia et al. (2012) developed semi-automatically, a dic-
tionary of 9 million fully inflected Arabic words us-
ing a morphological transducer and a large corpus.
They then created an error model by analyzing error
types and by creating an edit distance ranker. Finally,
they analyzed the level of noise in different sources of
data and selected the optimal subset to train their sys-
tem. Alkanhal et al. (2012) presented a stochastic ap-
proach for spelling correction of Arabic text. They used
a context-based system to automatically correct mis-
spelled words. First of all, a list is generated with pos-
sible alternatives for each misspelled word using the
Damerau-Levenshtein edit distance, then the right al-
ternative for each misspelled word is selected stochas-
tically using a lattice search, and an n-gram method.
Shaalan et al. (2012) trained a Noisy Channel Model
on word-based unigrams to detect and correct spelling
errors. Dahlmeier and Ng (2012a) built specialized de-
coders for English grammatical error correction. More
recently, (Pasha et al., 2014) created MADAMIRA,
a system for morphological analysis and disambigua-
tion of Arabic, this system can be used to improve the
accuracy of spelling checking system especially with
Hamza spelling correction.
In contrast to the approaches described above, we
use a machine translation (MT) based method to train
an error correction system. To the best of our knowl-
edge, this is the first error correction system for Arabic
using an MT approach.
</bodyText>
<sectionHeader confidence="0.977962" genericHeader="method">
3 Our System
</sectionHeader>
<bodyText confidence="0.9999105">
Our system is a pipeline that consists of several dif-
ferent modules. The baseline system uses a spelling
checking module, and the final system uses a phrase-
based statistical machine translation system. To
preproces the text, we use the provided output of
MADAMIRA (Pasha et al., 2014) and a rule-based
correction. We then do a rule-based post-processing
to fix the punctuation.
</bodyText>
<subsectionHeader confidence="0.999339">
3.1 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.9998718">
For the baseline system, we try a common spelling
checking approach. We first pre-process the data us-
ing the features from MADAMIRA (see Feature 14
Replacement), then we use a noisy channel model for
spelling checking.
</bodyText>
<subsectionHeader confidence="0.79721">
Feature 14 Replacement
</subsectionHeader>
<bodyText confidence="0.9999157">
The first step in the pipeline is to extract
MADAMIRA’s 14th feature from the .column file
and replace each word in the input text with this form.
MADAMIRA uses morphological disambiguation and
SVM analysis to select the most likely fully diacritized
Arabic word for the input word. The 14th feature
represents the undiacritized form of the most likely
word. This step corrects many Hamza placement or
omission errors, which makes a good base for other
correction modules.
</bodyText>
<subsectionHeader confidence="0.972165">
Spelling Correction
</subsectionHeader>
<bodyText confidence="0.999989371428571">
The spelling checker is based on a noisy channel model
- we use a word list and language model to determine
the most probable correct Arabic word that could have
generated the incorrect form that we have in the text.
For detecting spelling errors we use the AraComLex
word list for spelling checking (Attia et al., 2012),
which contains about 9 million Arabic words.1 We
look up the word from the input sentence in this list,
and attempt to correct those that are not found in the
list. We also train a mapping of incorrect words and
possible corrections from the edits in the training data.
If the word is in this map, the list of possible correc-
tions from the training data becomes the candidate list.
If the word is not in the trained map, the candidate list
is created by generating a list of words with common
insertions, substitutions, and deletions, according to the
list in (Attia et al., 2012). Each candidate is generated
by performing these edits and has a weight according to
the edit distance weights in the list. We then prune the
candidate list by keeping only the lowest weight words,
and removing candidates that are not found in the word
list. The resulting sentence is scored with a 3-gram lan-
guage model built with KenLM (Heafield et al., 2013)
on the correct side of the training data. The top one
sentence is then kept and considerd as the ”corrected”
one.
This module handles spelling errors of individual
words; it does not handle split/merge errors or word
reordering. The spelling checker sometimes attempts
to correct words that were already correct, because
the list does not contain named entities or translitera-
tions, and it does not contain all possible correct Arabic
words. Because the spelling checker module decreased
the overall performance, it is not included in our final
system.
</bodyText>
<subsectionHeader confidence="0.9129525">
3.2 Final System
Feature 14 Replacement
</subsectionHeader>
<bodyText confidence="0.999962">
The first step in our final system is Feature 14 Replace-
ment, as described above.
</bodyText>
<subsectionHeader confidence="0.891854">
Rule-based Clitic Correction
</subsectionHeader>
<bodyText confidence="0.999668">
With the resulting data, we apply a set of rules to reat-
tach clitics that may have been split apart from the base
word. After examining the train dataset, we realized
that 95% of word merging cases involve “_3” attach-
ment. When found by themselves, the clitics are at-
tached to either the previous word or next word, based
on whether they generally appear as prefixes or suf-
fixes. The clitics handled by this module are specified
in Table 2.
We also remove extra characters by replacing a se-
quence of 3 or more of the same character with a single
</bodyText>
<footnote confidence="0.989375">
1http://sourceforge.net/projects/
arabic-wordlist/
</footnote>
<page confidence="0.983662">
138
</page>
<table confidence="0.99964">
Dev
Exact Match No Punct
Precision Recall F1 Precision Recall F1
Feature 14 0.7746 0.3210 0.4539 0.8100 0.5190 0.6326
Feature 14 + Spelling checker (baseline) 0.4241 0.3458 0.3810 0.4057 0.4765 0.4382
Feature 14 + Clitic Rules 0.7884 0.3642 0.4983 0.8149 0.5894 0.6841
Feature 14 + Phrase-based MT 0.7296 0.5043 0.5964 0.7797 0.6397 0.7028
Feature 14 + Clitic Rules + Phrase-based MT 0.7571 0.5389 0.6296 0.8220 0.6850 0.7473
Test
Feature 14 + Clitic Rules + Phrase-based MT 0.7797 0.5635 0.6542 0.7438 0.6855 0.7135
</table>
<tableCaption confidence="0.976523">
Table 1: System results on the dev set (upper part) and on the test set (lower part).
</tableCaption>
<table confidence="0.997585666666667">
Attach clitic to... Clitics
Beginning of next word {�, jI, y, v, &apos;_r}
End of previous word {J, La, L��, �� ��, �� , ��, I}
</table>
<tableCaption confidence="0.999297">
Table 2: Clitics handled by the rule-based module.
</tableCaption>
<bodyText confidence="0.644844">
instance of that character (e.g. ! !!!!!! would be replaced
with !).
</bodyText>
<sectionHeader confidence="0.439353" genericHeader="method">
Statistical Phrase-based Model
</sectionHeader>
<bodyText confidence="0.999972851851852">
We use the Moses toolkit (Koehn et al., 2007) to
create a statistical phrase-based machine translation
model built on the best pre-processed data, as described
above. We treat this last step as a translation prob-
lem, where the source language is pre-processed in-
correct Arabic text, and the reference is correct Ara-
bic. Feature 14 extraction, rule-based correction, and
character de-duplication are applied to both the train
and dev sets. All but the last 1,000 sentences of the
train data are used at the training set for the phrase-
based model, the last 1,000 sentences of the train data
are used as a tuning set, and the dev set is used for
testing and evaluation. We use fast align, the aligner
included with the cdec decoder (Dyer et al., 2010) as
the word aligner with grow-diag as the symmetrization
heuristic (Och and Ney, 2003), and build a 5-gram lan-
guage model from the correct Arabic training data with
KenLM (Heafield et al., 2013). The system is evaluated
with BLEU (Papineni et al., 2002) and then scored for
precision, recall, and F1 measure against the dev set
reference.
We tested several different reordering window sizes
since this is not a standard translation task, so we may
want shorter distance reordering. Although 7 is the de-
fault size, we tested 7, 5, 4, 3, and 0, and found that a
window of size 4 produces the best result according to
BLEU score and F1 measure.
</bodyText>
<sectionHeader confidence="0.998825" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.99988419047619">
We train and evaluate our system with the train-
ing and development datasets provided for the shared
task and the m2Scorer (Dahlmeier and Ng, 2012b).
These datasets are extracted from the QALB corpus
of human-edited Arabic text produced by native speak-
ers, non-native speakers and machines (Zaghouani et
al., 2014).
We conducted a small scale statistical study on the
950K tokens training set used to build our system. We
realized that 306K tokens are affected by a correction
action which could be a word edit, insertion, deletion,
split or merge. 169K tokens were edited to correct the
spelling errors and 99K tokens were inserted (mostly
punctuation marks). Furthermore, there is a total of
6,7K non necessary tokens deleted and 10.6K attached
tokens split and 18.2 tokens merged. Finally, there are
only 427 tokens moved in the sentence and 1563 mul-
tiple correction action.
We experiment with different configurations and
reach the sweet spot of performance when combining
the different modules.
</bodyText>
<sectionHeader confidence="0.681565" genericHeader="method">
4.1 Results
</sectionHeader>
<bodyText confidence="0.999982333333333">
To evaluate the performance of our system on the de-
velopment data, we compare its output to the reference
(gold annotation). We then compute the usual mea-
sures of precision, recall and f-measure. Results for
various system configurations on the dev and test sets
are given in Table 1. Using the baseline system con-
sisting in replacing words by their non diacritized form
(Feature 14), we could correct 51.9% of the errors oc-
curring in the dev set, when punctuation is not consid-
ered. This result drops when we consider the punctua-
tion errors which seem to be more complex to correct:
Only 32.1% of the errors are corrected in the dev set. It
is important to notice that adding the clitic rules to the
Feature 14 baseline yields an improvement of + 5.15 in
F-measure. We reach the best F-measure value when
using the phrase-based MT system after pre-processing
the data and applying the Feature 14 and clitic rules.
Using this combination we were able to correct 68.5%
of the errors (excluding punctuation) on the develop-
ment set with a precision of 82.2% and 74.38% on the
test set. When we consider the punctuation, 53.89%
of the errors of different types were corrected on the
dev set and 56.35% on the test set with a precision of
75.71% and 77.97%, respectively.
</bodyText>
<page confidence="0.999125">
139
</page>
<sectionHeader confidence="0.992544" genericHeader="method">
5 Error Analysis and Discussion
</sectionHeader>
<bodyText confidence="0.999812777777778">
When building error correction systems, minimizing
the number of cases where correct words are marked
as incorrect is often regarded as more important than
covering a high number of errors. Therefore, a higher
precision is often preferred over higher recall. In order
to understand what was affecting the performance, we
took a closer look at our system output and translation
tables to present some samples of errors that our system
makes on development set.
</bodyText>
<subsectionHeader confidence="0.793473">
5.1 Out-of-vocabulary Words
</subsectionHeader>
<bodyText confidence="0.999181166666667">
This category includes words that are not seen by our
system during the training which is a common problem
in machine translation systems. In our system, most of
out-of-vocabulary words were directly transferred un-
changed from source to target. For example the word
éJ��Ëð�ñ‚ÒÊ�¯@ was not corrected to &lt;J�Ëð y‚ÖÏ@.
</bodyText>
<subsectionHeader confidence="0.995925">
5.2 Unnecessary Edits
</subsectionHeader>
<bodyText confidence="0.998138">
In some cases, our system made some superfluous edits
such as adding the definite article in cases where it is
not required such as :
</bodyText>
<tableCaption confidence="0.8496335">
Table 3: An example of an unnecessary addition of the
definite article.
</tableCaption>
<subsectionHeader confidence="0.95663">
5.3 Number Normalization
</subsectionHeader>
<bodyText confidence="0.99990425">
We observed that in some cases, the system did not nor-
malize the numbers such as in the following case which
requires some knowledge of the real context to under-
stand that these numbers require normalization.
</bodyText>
<tableCaption confidence="0.991861">
Table 4: An example of number normalization.
</tableCaption>
<subsectionHeader confidence="0.994111">
5.4 Hamza Spelling
</subsectionHeader>
<bodyText confidence="0.9999865">
Even though our system corrected most of the Hamza
spelling errors, we noticed that in certain cases they
were not corrected, especially when the words without
the Hamza were valid entries in the dictionary. These
cases are not always easy to handle since only context
and semantic rules can handle them.
</bodyText>
<subsectionHeader confidence="0.957946">
5.5 Grammatical Errors
</subsectionHeader>
<bodyText confidence="0.998787">
In our error analysis we encountered many cases of un-
corrected grammatical errors. The most frequent type
</bodyText>
<figure confidence="0.702319857142857">
Source éJ��J£ñË@ X@ð
�
Hypothesis éJ��J£ñË@ X@ð
�
Reference éJ�
��
�J£ñË@ X@ð
</figure>
<tableCaption confidence="0.943639285714286">
Table 5: A sentence where the Hamza was not added
above the Alif in the first word because both versions
are valid dictionary entries.
is the case endings correction such as correcting the
verbs in jussive mode when there is a prohibition par-
ticle (negative imperative) like the (B) in the following
examples :
</tableCaption>
<table confidence="0.912356166666667">
�
Source ÑîE�XAK�@ úÎ« @ñK.Qå�”�� B
�
Hypothesis ÑîE�XAK�@ úÎ« @ñK.Qå�”&apos;» B
�
Reference ÑîE�XAK�@úÎ« B
</table>
<tableCaption confidence="0.99744">
Table 6: An example of a grammatical error.
</tableCaption>
<subsectionHeader confidence="0.939414">
5.6 Unnecessary Word Deletion
</subsectionHeader>
<bodyText confidence="0.999790285714286">
According to the QALB annotation guidelines, ex-
tra words causing semantic ambiguity in the sentence
should be deleted. The decision to delete a given word
is usually based on the meaning and the understanding
of the human annotator, unfortunately this kind of er-
rors is very hard to process and our system was not able
to delete most of the unnecessary words.
</bodyText>
<table confidence="0.9946445">
Source �
Q k ~@ A�J�� ~‚Ó AÒîE�YK�@ Aª “ð YîD�„�JƒÉë
Hypothesis �
Q k ~@ A�J�� ~‚Ó AÒîE�YK�@ Aª °ð YîD�„ Jƒ Éë
�
Reference Qk�@ A�JL. �‚Ó Aª �“ð YîD�„ Jƒ Éë
</table>
<tableCaption confidence="0.999735">
Table 7: An example of word deletion.
</tableCaption>
<subsectionHeader confidence="0.999396">
5.7 Adding Extra Words
</subsectionHeader>
<bodyText confidence="0.999902">
Our analysis revealed cases of extra words introduced
to some sentences, despite the fact that the words added
are coherent with the context and could even improve
the overall readability of the sentence, they are uncred-
ited correction since they are not included in the gold
standard. For example:
</bodyText>
<equation confidence="0.880228">
Source ø� Pñ‚Ë@ � @ �éªÖÞ... H. Qå�•
Hypothesis QmÌ @ ø� Pñ‚Ë@ �•~
m.Ì @ �éªÖÞ... H. Qå�•
Reference ø@ �éªÖÞ... H. Qå •
� Pñ‚Ë@ ~•~
m.Ì
</equation>
<tableCaption confidence="0.993279">
Table 8: An example of the addition of extra words.
</tableCaption>
<subsectionHeader confidence="0.948481">
5.8 Merge and Split Errors
</subsectionHeader>
<bodyText confidence="0.9971775">
In this category, we show some sample errors of neces-
sary word splits and merge not done by our system. The
</bodyText>
<figure confidence="0.997301055555555">
��
é�JK�YÖÏ@ � ¬AJ
£@
Source
��
é�JK�YÖÏ@ � ¬AJ�£B@
Hypothesis
Reference
�
(unchanged) �é�JK�YÖÏ@ � ¬AJ
£@
Source
u@ðAkÓ 450000
Hypothesis
u@ðAkÓ 450000
Reference
u@ðA;u
Ó 450
</figure>
<page confidence="0.990611">
140
</page>
<bodyText confidence="0.998641666666667">
word YªK.A“ñ’k should have been split as YªK. A“ñ’k
and the word YK. B should have been merged to appear
as one word as in YK.B.
</bodyText>
<subsectionHeader confidence="0.97763">
5.9 Dialectal Correction Errors
</subsectionHeader>
<bodyText confidence="0.990065375">
Dialectal words are usually converted to their Modern
Standard Arabic (MSA) equivalent in the QALB cor-
pus, since dialectal words are rare, our system is unable
to detect and translate the dialectal words to the MSA
as in the expression áK
P I. Ó that is translated in the
gold standard to áK
P Q��c.
</bodyText>
<sectionHeader confidence="0.994438" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999925142857143">
We presented our CMUQ system for automatic Ara-
bic text correction. Our system combines rule-based
linguistic techniques with statistical language model-
ing techniques and a phrase-based machine transla-
tion method. We experiment with different configu-
rations. Our experiments have shown that the system
we submitted outperforms the baseline and we reach
an F-score of 74.73% on the development set from
the QALB corpus when punctuation is excluded, and
65.42% on the test set when we consider the punctu-
ation errors . This placed us in the 3rd rank. We be-
lieve that our system could be improved in numerous
ways. In the future, we plan to finalize a current mod-
ule that we are developing to deal with merge and split
errors in a more specific way. We also want to focus in
a deeper way on the word movement as well as punc-
tuation problems, which can produce a more accurate
system. We will focus as well on learning further error
correction models from Arabic Wikipedia revision his-
tory, as it contains natural rewritings including spelling
corrections and other local text transformations.
</bodyText>
<sectionHeader confidence="0.994952" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999548">
This publication was made possible by grants NPRP-
09-1140-1-177 and NPRP-4-1058- 1-168 from the
Qatar National Research Fund (a member of the Qatar
Foundation). The statements made herein are solely the
responsibility of the authors.
</bodyText>
<sectionHeader confidence="0.998578" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999789205882353">
Mohamed I. Alkanhal, Mohamed Al-Badrashiny, Man-
sour M. Alghamdi, and Abdulaziz O. Al-Qabbany.
2012. Automatic Stochastic Arabic Spelling Correc-
tion With Emphasis on Space Insertions and Dele-
tions. IEEE Transactions on Audio, Speech &amp; Lan-
guage Processing, 20(7):2111–2122.
Mohammed Attia, Pavel Pecina, Younes Samih,
Khaled Shaalan, and Josef van Genabith. 2012. Im-
proved Spelling Error Detection and Correction for
Arabic. In Proceedings of COLING 2012: Posters,
pages 103–112, Mumbai, India.
Daniel Dahlmeier and Hwee Tou Ng. 2012a. A Beam-
Search Decoder for Grammatical Error Correction.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 568–578, Jeju Island, Korea.
Daniel Dahlmeier and Hwee Tou Ng. 2012b. Bet-
ter Evaluation for Grammatical Error Correction. In
NAACL HLT ’12 Proceedings of the 2012 Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 568–572.
Robert Dale. 1997. Computer Assistance in Text Cre-
ation and Editing. In Survey of the state of the art
in Human Language Technology, chapter 7, pages
235–237. Cambridge University Press.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A Decoder, Alignment, and Learning Frame-
work for Finite-state and Context-free Translation
Models. In Proceedings of the ACL 2010 System
Demonstrations, pages 7–12, Uppsala, Sweden.
A. R. Golding and D. Roth. 1999. A Winnow Based
Approach to Context-Sensitive Spelling Correction.
Machine Learning, 34(1-3):107–130.
Nizar Habash. 2008. Four Techniques for Online Han-
dling of Out-of-Vocabulary Words in Arabic-English
Statistical Machine Translation. In Proceedings of
ACL-08: HLT, Short Papers, pages 57–60, Colum-
bus, Ohio.
Bassam Haddad and Mustafa Yaseen. 2007. Detection
and Correction of Non-words in Arabic: a Hybrid
Approach. International Journal of Computer Pro-
cessing of Oriental Languages, 20(04):237–257.
Ahmed Hassan, Sara Noeman, and Hany Hassan.
2008. Language Independent Text Correction using
Finite State Automata. In Proceedings of the Third
International Joint Conference on Natural Language
Processing (IJCNLP 2008), pages 913–918, Hyder-
abad, India.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modfied
Kneser-Ney Language Model Estimation. In In Pro-
ceedings of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open Source Toolkit for Statistical Ma-
chine Translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177–180, Prague,
Czech Republic.
</reference>
<page confidence="0.979259">
141
</page>
<reference confidence="0.999901737704918">
Karen Kukich. 1992. Techniques for Automatically
Correcting Words in Text. ACM Computing Surveys
(CSUR), 24(4):377–439.
Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wa-
jdi Zaghouani, and Ossama Obeid. 2014. The First
QALB Shared Task on Automatic Text Correction
for Arabic. In Proceedings of EMNLP Workshop on
Arabic Natural Language Processing, Doha, Qatar,
October.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. In Computational Linguistics, page 1951.
Kemal Oflazer. 1996. Error-Tolerant Finite-State
Recognition with Applications to Morphological
Analysis and Spelling Correction. Computational
Linguistics, 22(1):73–89.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the Association for Computational Linguis-
tics, Philadelphia, Pennsylvania.
Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab,
Ahmed El Kholy, Ramy Eskander, Nizar Habash,
Manoj Pooleery, Owen Rambow, and Ryan Roth.
2014. MADAMIRA: A Fast, Comprehensive Tool
for Morphological Analysis and Disambiguation of
Arabic. In Proceedings of the Ninth International
Conference on Language Resources and Evaluation
(LREC’14), pages 1094–1101, Reykjavik, Iceland.
Khaled Shaalan, Amin Allam, and Abdallah Gomah.
2003. Towards Automatic Spell Checking for Ara-
bic. In Proceedings of the 4th Conference on Lan-
guage Engineering, Egyptian Society of Language
Engineering (ELSE), Cairo, Egypt.
Khaled Shaalan, Rana Aref, and Aly Fahmy. 2010. An
Approach for Analyzing and Correcting Spelling Er-
rors for Non-native Arabic Learners. In Proceedings
of The 7th International Conference on Informatics
and Systems, INFOS2010, the special track on Nat-
ural Language Processing and Knowledge Mining,
pages 28–30, Cairo, Egypt.
Khaled Shaalan, Mohammed Attia, Pavel Pecina,
Younes Samih, and Josef van Genabith. 2012.
Arabic Word Generation and Modelling for Spell
Checking. In Proceedings of the Eighth Inter-
national Conference on Language Resources and
Evaluation (LREC-2012), pages 719–725, Istanbul,
Turkey.
Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-
sama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura
Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014.
Large Scale Arabic Error Annotation: Guidelines
and Framework. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC’14), Reykjavik, Iceland.
Chiraz Zribi and Mohammed Ben Ahmed. 2003. Ef-
ficient Automatic Correction of Misspelled Arabic
Words Based on Contextual Information. In Pro-
ceedings of the Knowledge-Based Intelligent Infor-
mation and Engineering Systems Conference, pages
770–777, Oxford, UK.
</reference>
<page confidence="0.99773">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.479234">
<title confidence="0.915305">CMUQ@QALB-2014: An SMT-based for Automatic Arabic Error Correction , Houda Wajdi and Kemal</title>
<author confidence="0.552968">Mellon</author>
<affiliation confidence="0.93555">Mellon University in</affiliation>
<email confidence="0.997494">ko@cs.cmu.edu</email>
<abstract confidence="0.994989545454545">In this paper, we describe the CMUQ system we submitted to The ANLP-QALB 2014 Shared Task on Automatic Text Correction for Arabic. Our system combines rule-based linguistic techniques with statistical language modeling techniques and machine translationbased methods. Our system outperforms the baseline and reaches an F-score of 65.42% on the test set of QALB corpus. This ranks us 3rd in the competition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohamed I Alkanhal</author>
<author>Mohamed Al-Badrashiny</author>
<author>Mansour M Alghamdi</author>
<author>Abdulaziz O Al-Qabbany</author>
</authors>
<title>Automatic Stochastic Arabic Spelling Correction With Emphasis on Space Insertions and Deletions.</title>
<date>2012</date>
<journal>IEEE Transactions on Audio, Speech &amp; Language Processing,</journal>
<volume>20</volume>
<issue>7</issue>
<contexts>
<context position="5323" citStr="Alkanhal et al. (2012)" startWordPosition="819" endWordPosition="822">rocessing (ANLP), pages 137–142, October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics tune the word recognition and non-word correction method. In order to build an Arabic spelling checker, Attia et al. (2012) developed semi-automatically, a dictionary of 9 million fully inflected Arabic words using a morphological transducer and a large corpus. They then created an error model by analyzing error types and by creating an edit distance ranker. Finally, they analyzed the level of noise in different sources of data and selected the optimal subset to train their system. Alkanhal et al. (2012) presented a stochastic approach for spelling correction of Arabic text. They used a context-based system to automatically correct misspelled words. First of all, a list is generated with possible alternatives for each misspelled word using the Damerau-Levenshtein edit distance, then the right alternative for each misspelled word is selected stochastically using a lattice search, and an n-gram method. Shaalan et al. (2012) trained a Noisy Channel Model on word-based unigrams to detect and correct spelling errors. Dahlmeier and Ng (2012a) built specialized decoders for English grammatical error</context>
</contexts>
<marker>Alkanhal, Al-Badrashiny, Alghamdi, Al-Qabbany, 2012</marker>
<rawString>Mohamed I. Alkanhal, Mohamed Al-Badrashiny, Mansour M. Alghamdi, and Abdulaziz O. Al-Qabbany. 2012. Automatic Stochastic Arabic Spelling Correction With Emphasis on Space Insertions and Deletions. IEEE Transactions on Audio, Speech &amp; Language Processing, 20(7):2111–2122.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammed Attia</author>
<author>Pavel Pecina</author>
<author>Younes Samih</author>
<author>Khaled Shaalan</author>
<author>Josef van Genabith</author>
</authors>
<title>Improved Spelling Error Detection and Correction for Arabic.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>103--112</pages>
<location>Mumbai, India.</location>
<marker>Attia, Pecina, Samih, Shaalan, van Genabith, 2012</marker>
<rawString>Mohammed Attia, Pavel Pecina, Younes Samih, Khaled Shaalan, and Josef van Genabith. 2012. Improved Spelling Error Detection and Correction for Arabic. In Proceedings of COLING 2012: Posters, pages 103–112, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A BeamSearch Decoder for Grammatical Error Correction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>568--578</pages>
<location>Jeju Island,</location>
<contexts>
<context position="5864" citStr="Dahlmeier and Ng (2012" startWordPosition="904" endWordPosition="907">ata and selected the optimal subset to train their system. Alkanhal et al. (2012) presented a stochastic approach for spelling correction of Arabic text. They used a context-based system to automatically correct misspelled words. First of all, a list is generated with possible alternatives for each misspelled word using the Damerau-Levenshtein edit distance, then the right alternative for each misspelled word is selected stochastically using a lattice search, and an n-gram method. Shaalan et al. (2012) trained a Noisy Channel Model on word-based unigrams to detect and correct spelling errors. Dahlmeier and Ng (2012a) built specialized decoders for English grammatical error correction. More recently, (Pasha et al., 2014) created MADAMIRA, a system for morphological analysis and disambiguation of Arabic, this system can be used to improve the accuracy of spelling checking system especially with Hamza spelling correction. In contrast to the approaches described above, we use a machine translation (MT) based method to train an error correction system. To the best of our knowledge, this is the first error correction system for Arabic using an MT approach. 3 Our System Our system is a pipeline that consists o</context>
<context position="12567" citStr="Dahlmeier and Ng, 2012" startWordPosition="2036" endWordPosition="2039">13). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 Experiments and Results We train and evaluate our system with the training and development datasets provided for the shared task and the m2Scorer (Dahlmeier and Ng, 2012b). These datasets are extracted from the QALB corpus of human-edited Arabic text produced by native speakers, non-native speakers and machines (Zaghouani et al., 2014). We conducted a small scale statistical study on the 950K tokens training set used to build our system. We realized that 306K tokens are affected by a correction action which could be a word edit, insertion, deletion, split or merge. 169K tokens were edited to correct the spelling errors and 99K tokens were inserted (mostly punctuation marks). Furthermore, there is a total of 6,7K non necessary tokens deleted and 10.6K attached</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012a. A BeamSearch Decoder for Grammatical Error Correction. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 568–578, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better Evaluation for Grammatical Error Correction.</title>
<date>2012</date>
<booktitle>In NAACL HLT ’12 Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>568--572</pages>
<contexts>
<context position="5864" citStr="Dahlmeier and Ng (2012" startWordPosition="904" endWordPosition="907">ata and selected the optimal subset to train their system. Alkanhal et al. (2012) presented a stochastic approach for spelling correction of Arabic text. They used a context-based system to automatically correct misspelled words. First of all, a list is generated with possible alternatives for each misspelled word using the Damerau-Levenshtein edit distance, then the right alternative for each misspelled word is selected stochastically using a lattice search, and an n-gram method. Shaalan et al. (2012) trained a Noisy Channel Model on word-based unigrams to detect and correct spelling errors. Dahlmeier and Ng (2012a) built specialized decoders for English grammatical error correction. More recently, (Pasha et al., 2014) created MADAMIRA, a system for morphological analysis and disambiguation of Arabic, this system can be used to improve the accuracy of spelling checking system especially with Hamza spelling correction. In contrast to the approaches described above, we use a machine translation (MT) based method to train an error correction system. To the best of our knowledge, this is the first error correction system for Arabic using an MT approach. 3 Our System Our system is a pipeline that consists o</context>
<context position="12567" citStr="Dahlmeier and Ng, 2012" startWordPosition="2036" endWordPosition="2039">13). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 Experiments and Results We train and evaluate our system with the training and development datasets provided for the shared task and the m2Scorer (Dahlmeier and Ng, 2012b). These datasets are extracted from the QALB corpus of human-edited Arabic text produced by native speakers, non-native speakers and machines (Zaghouani et al., 2014). We conducted a small scale statistical study on the 950K tokens training set used to build our system. We realized that 306K tokens are affected by a correction action which could be a word edit, insertion, deletion, split or merge. 169K tokens were edited to correct the spelling errors and 99K tokens were inserted (mostly punctuation marks). Furthermore, there is a total of 6,7K non necessary tokens deleted and 10.6K attached</context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012b. Better Evaluation for Grammatical Error Correction. In NAACL HLT ’12 Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 568–572.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Computer Assistance in Text Creation and Editing. In Survey of the state of the art in Human Language Technology, chapter 7,</title>
<date>1997</date>
<pages>235--237</pages>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="835" citStr="Dale, 1997" startWordPosition="115" endWordPosition="116">ar {hbouamor,wajdiz}@qatar.cmu.edu, ko@cs.cmu.edu Abstract In this paper, we describe the CMUQ system we submitted to The ANLP-QALB 2014 Shared Task on Automatic Text Correction for Arabic. Our system combines rule-based linguistic techniques with statistical language modeling techniques and machine translationbased methods. Our system outperforms the baseline and reaches an F-score of 65.42% on the test set of QALB corpus. This ranks us 3rd in the competition. 1 Introduction The business of text creation and editing represents a large market where NLP technologies might be applied naturally (Dale, 1997). Today’s users of word processors get surprisingly little help in checking spelling, and a small number of them use more sophisticated tools such as grammar checkers, to provide help in ensuring that a text remains grammatically accurate after modification. For instance, in the Arabic version of Microsoft Word, the spelling checker for Arabic, does not give reasonable and natural proposals for many realword errors and even for simple probable errors (Haddad and Yaseen, 2007). With the increased usage of computers in the processing of natural languages comes the need for correcting errors intr</context>
</contexts>
<marker>Dale, 1997</marker>
<rawString>Robert Dale. 1997. Computer Assistance in Text Creation and Editing. In Survey of the state of the art in Human Language Technology, chapter 7, pages 235–237. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Jonathan Weese</author>
<author>Hendra Setiawan</author>
<author>Adam Lopez</author>
<author>Ferhan Ture</author>
<author>Vladimir Eidelman</author>
<author>Juri Ganitkevitch</author>
<author>Phil Blunsom</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A Decoder, Alignment, and Learning Framework for Finite-state and Context-free Translation Models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>7--12</pages>
<location>Uppsala,</location>
<contexts>
<context position="11753" citStr="Dyer et al., 2010" startWordPosition="1894" endWordPosition="1897">-processed data, as described above. We treat this last step as a translation problem, where the source language is pre-processed incorrect Arabic text, and the reference is correct Arabic. Feature 14 extraction, rule-based correction, and character de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for testing and evaluation. We use fast align, the aligner included with the cdec decoder (Dyer et al., 2010) as the word aligner with grow-diag as the symmetrization heuristic (Och and Ney, 2003), and build a 5-gram language model from the correct Arabic training data with KenLM (Heafield et al., 2013). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best resu</context>
</contexts>
<marker>Dyer, Weese, Setiawan, Lopez, Ture, Eidelman, Ganitkevitch, Blunsom, Resnik, 2010</marker>
<rawString>Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam Lopez, Ferhan Ture, Vladimir Eidelman, Juri Ganitkevitch, Phil Blunsom, and Philip Resnik. 2010. cdec: A Decoder, Alignment, and Learning Framework for Finite-state and Context-free Translation Models. In Proceedings of the ACL 2010 System Demonstrations, pages 7–12, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow Based Approach to Context-Sensitive Spelling Correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="4167" citStr="Golding and Roth, 1999" startWordPosition="635" endWordPosition="638">ion and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine 137 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 137–142, October 25, 2014, Doha, Qatar. c�</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth. 1999. A Winnow Based Approach to Context-Sensitive Spelling Correction. Machine Learning, 34(1-3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Four Techniques for Online Handling of Out-of-Vocabulary Words in Arabic-English Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT, Short Papers,</booktitle>
<pages>57--60</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="4050" citStr="Habash, 2008" startWordPosition="616" endWordPosition="617">ude and hint towards future improvement of the system, in Section 6. 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine 137 Proceedings of t</context>
</contexts>
<marker>Habash, 2008</marker>
<rawString>Nizar Habash. 2008. Four Techniques for Online Handling of Out-of-Vocabulary Words in Arabic-English Statistical Machine Translation. In Proceedings of ACL-08: HLT, Short Papers, pages 57–60, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bassam Haddad</author>
<author>Mustafa Yaseen</author>
</authors>
<title>Detection and Correction of Non-words in Arabic: a Hybrid Approach.</title>
<date>2007</date>
<journal>International Journal of Computer Processing of Oriental Languages,</journal>
<volume>20</volume>
<issue>04</issue>
<contexts>
<context position="1315" citStr="Haddad and Yaseen, 2007" startWordPosition="191" endWordPosition="195"> Introduction The business of text creation and editing represents a large market where NLP technologies might be applied naturally (Dale, 1997). Today’s users of word processors get surprisingly little help in checking spelling, and a small number of them use more sophisticated tools such as grammar checkers, to provide help in ensuring that a text remains grammatically accurate after modification. For instance, in the Arabic version of Microsoft Word, the spelling checker for Arabic, does not give reasonable and natural proposals for many realword errors and even for simple probable errors (Haddad and Yaseen, 2007). With the increased usage of computers in the processing of natural languages comes the need for correcting errors introduced at different stages. Natural language errors are not only made by human operators at the input stage but also by NLP systems that produce natural language output. Machine translation (MT), or optical character recognition (OCR), often produce incorrect output riddled with odd lexical choices, grammar errors, or incorrectly recognized characters. Correcting human/machine-produced errors, or post-editing, can be manual or automated. For morphologically and syntactically </context>
<context position="4015" citStr="Haddad and Yaseen, 2007" startWordPosition="608" endWordPosition="611"> system output in Section 5. Finally, we conclude and hint towards future improvement of the system, in Section 6. 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and</context>
</contexts>
<marker>Haddad, Yaseen, 2007</marker>
<rawString>Bassam Haddad and Mustafa Yaseen. 2007. Detection and Correction of Non-words in Arabic: a Hybrid Approach. International Journal of Computer Processing of Oriental Languages, 20(04):237–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmed Hassan</author>
<author>Sara Noeman</author>
<author>Hany Hassan</author>
</authors>
<title>Language Independent Text Correction using Finite State Automata.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<pages>913--918</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="4036" citStr="Hassan et al., 2008" startWordPosition="612" endWordPosition="615"> 5. Finally, we conclude and hint towards future improvement of the system, in Section 6. 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine 137 Pr</context>
</contexts>
<marker>Hassan, Noeman, Hassan, 2008</marker>
<rawString>Ahmed Hassan, Sara Noeman, and Hany Hassan. 2008. Language Independent Text Correction using Finite State Automata. In Proceedings of the Third International Joint Conference on Natural Language Processing (IJCNLP 2008), pages 913–918, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable Modfied Kneser-Ney Language Model Estimation. In</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="8822" citStr="Heafield et al., 2013" startWordPosition="1400" endWordPosition="1403">orrections from the training data becomes the candidate list. If the word is not in the trained map, the candidate list is created by generating a list of words with common insertions, substitutions, and deletions, according to the list in (Attia et al., 2012). Each candidate is generated by performing these edits and has a weight according to the edit distance weights in the list. We then prune the candidate list by keeping only the lowest weight words, and removing candidates that are not found in the word list. The resulting sentence is scored with a 3-gram language model built with KenLM (Heafield et al., 2013) on the correct side of the training data. The top one sentence is then kept and considerd as the ”corrected” one. This module handles spelling errors of individual words; it does not handle split/merge errors or word reordering. The spelling checker sometimes attempts to correct words that were already correct, because the list does not contain named entities or transliterations, and it does not contain all possible correct Arabic words. Because the spelling checker module decreased the overall performance, it is not included in our final system. 3.2 Final System Feature 14 Replacement The fi</context>
<context position="11948" citStr="Heafield et al., 2013" startWordPosition="1927" endWordPosition="1930">eature 14 extraction, rule-based correction, and character de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for testing and evaluation. We use fast align, the aligner included with the cdec decoder (Dyer et al., 2010) as the word aligner with grow-diag as the symmetrization heuristic (Och and Ney, 2003), and build a 5-gram language model from the correct Arabic training data with KenLM (Heafield et al., 2013). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 Experiments and Results We train and evaluate our system with the training and development datasets provided for the shared task and the m2Scorer (Dah</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H. Clark, and Philipp Koehn. 2013. Scalable Modfied Kneser-Ney Language Model Estimation. In In Proceedings of the Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Christopher Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Christopher Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="11050" citStr="Koehn et al., 2007" startWordPosition="1773" endWordPosition="1776">6 0.5043 0.5964 0.7797 0.6397 0.7028 Feature 14 + Clitic Rules + Phrase-based MT 0.7571 0.5389 0.6296 0.8220 0.6850 0.7473 Test Feature 14 + Clitic Rules + Phrase-based MT 0.7797 0.5635 0.6542 0.7438 0.6855 0.7135 Table 1: System results on the dev set (upper part) and on the test set (lower part). Attach clitic to... Clitics Beginning of next word {�, jI, y, v, &apos;_r} End of previous word {J, La, L��, �� ��, �� , ��, I} Table 2: Clitics handled by the rule-based module. instance of that character (e.g. ! !!!!!! would be replaced with !). Statistical Phrase-based Model We use the Moses toolkit (Koehn et al., 2007) to create a statistical phrase-based machine translation model built on the best pre-processed data, as described above. We treat this last step as a translation problem, where the source language is pre-processed incorrect Arabic text, and the reference is correct Arabic. Feature 14 extraction, rule-based correction, and character de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for te</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Christopher Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Christopher Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for Automatically Correcting Words in Text.</title>
<date>1992</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="3926" citStr="Kukich, 1992" startWordPosition="595" endWordPosition="596"> on the shared task 2014 dev set (Section 4). Then, we give an analysis of our system output in Section 5. Finally, we conclude and hint towards future improvement of the system, in Section 6. 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later </context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Techniques for Automatically Correcting Words in Text. ACM Computing Surveys (CSUR), 24(4):377–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Behrang Mohit</author>
<author>Alla Rozovskaya</author>
<author>Nizar Habash</author>
<author>Wajdi Zaghouani</author>
<author>Ossama Obeid</author>
</authors>
<title>The First QALB Shared Task on Automatic Text Correction for Arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP Workshop on Arabic Natural Language Processing,</booktitle>
<location>Doha, Qatar,</location>
<contexts>
<context position="2728" citStr="Mohit et al., 2014" startWordPosition="406" endWordPosition="409">ask. For instance, the Automatic Arabic Text Correction Shared Task is an interesting testbed to develop and evaluate spelling correction systems for Arabic trained either on naturally occurring errors in texts written by humans (e.g., non-native speakers), or machines (e.g., MT output). In such tasks, participants are asked to implement a system that takes as input Modern Standard Arabic texts with various spelling errors and automatically correct them. In this paper, we describe the CMUQ system we developed to participate in the The First Shared Task on Automatic Text Correction for Arabic (Mohit et al., 2014). Our system combines rule-based linguistic techniques with statistical language modeling techniques and machine translationbased methods. Our system outperforms the baseline, achieves a better correction quality and reaches an Fscore of 62.96% on the development set of QALB corpus (Zaghouani et al., 2014) and 65.42% on the test set. The remainder of this paper is organized as follows. First, we review the main previous efforts for automatic spelling correction, in Section 2. In Section 3, we describe our system, which consists of several modules. We continue with our experiments on the shared</context>
</contexts>
<marker>Mohit, Rozovskaya, Habash, Zaghouani, Obeid, 2014</marker>
<rawString>Behrang Mohit, Alla Rozovskaya, Nizar Habash, Wajdi Zaghouani, and Ossama Obeid. 2014. The First QALB Shared Task on Automatic Text Correction for Arabic. In Proceedings of EMNLP Workshop on Arabic Natural Language Processing, Doha, Qatar, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>page</pages>
<contexts>
<context position="11840" citStr="Och and Ney, 2003" startWordPosition="1908" endWordPosition="1911">where the source language is pre-processed incorrect Arabic text, and the reference is correct Arabic. Feature 14 extraction, rule-based correction, and character de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for testing and evaluation. We use fast align, the aligner included with the cdec decoder (Dyer et al., 2010) as the word aligner with grow-diag as the symmetrization heuristic (Och and Ney, 2003), and build a 5-gram language model from the correct Arabic training data with KenLM (Heafield et al., 2013). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 Experiments and Results We train and evalu</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. In Computational Linguistics, page 1951.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Error-Tolerant Finite-State Recognition with Applications to Morphological Analysis and Spelling Correction.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="3941" citStr="Oflazer, 1996" startWordPosition="597" endWordPosition="598"> task 2014 dev set (Section 4). Then, we give an analysis of our system output in Section 5. Finally, we conclude and hint towards future improvement of the system, in Section 6. 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and </context>
</contexts>
<marker>Oflazer, 1996</marker>
<rawString>Kemal Oflazer. 1996. Error-Tolerant Finite-State Recognition with Applications to Morphological Analysis and Spelling Correction. Computational Linguistics, 22(1):73–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="12007" citStr="Papineni et al., 2002" startWordPosition="1937" endWordPosition="1940">de-duplication are applied to both the train and dev sets. All but the last 1,000 sentences of the train data are used at the training set for the phrasebased model, the last 1,000 sentences of the train data are used as a tuning set, and the dev set is used for testing and evaluation. We use fast align, the aligner included with the cdec decoder (Dyer et al., 2010) as the word aligner with grow-diag as the symmetrization heuristic (Och and Ney, 2003), and build a 5-gram language model from the correct Arabic training data with KenLM (Heafield et al., 2013). The system is evaluated with BLEU (Papineni et al., 2002) and then scored for precision, recall, and F1 measure against the dev set reference. We tested several different reordering window sizes since this is not a standard translation task, so we may want shorter distance reordering. Although 7 is the default size, we tested 7, 5, 4, 3, and 0, and found that a window of size 4 produces the best result according to BLEU score and F1 measure. 4 Experiments and Results We train and evaluate our system with the training and development datasets provided for the shared task and the m2Scorer (Dahlmeier and Ng, 2012b). These datasets are extracted from th</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the Association for Computational Linguistics, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arfath Pasha</author>
<author>Mohamed Al-Badrashiny</author>
<author>Mona Diab</author>
<author>Ahmed El Kholy</author>
<author>Ramy Eskander</author>
<author>Nizar Habash</author>
<author>Manoj Pooleery</author>
<author>Owen Rambow</author>
<author>Ryan Roth</author>
</authors>
<title>MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<pages>1094--1101</pages>
<location>Reykjavik, Iceland.</location>
<marker>Pasha, Al-Badrashiny, Diab, El Kholy, Eskander, Habash, Pooleery, Rambow, Roth, 2014</marker>
<rawString>Arfath Pasha, Mohamed Al-Badrashiny, Mona Diab, Ahmed El Kholy, Ramy Eskander, Nizar Habash, Manoj Pooleery, Owen Rambow, and Ryan Roth. 2014. MADAMIRA: A Fast, Comprehensive Tool for Morphological Analysis and Disambiguation of Arabic. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 1094–1101, Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khaled Shaalan</author>
<author>Amin Allam</author>
<author>Abdallah Gomah</author>
</authors>
<title>Towards Automatic Spell Checking for Arabic.</title>
<date>2003</date>
<booktitle>In Proceedings of the 4th Conference on Language Engineering, Egyptian Society of Language Engineering</booktitle>
<location>(ELSE), Cairo, Egypt.</location>
<contexts>
<context position="3990" citStr="Shaalan et al., 2003" startWordPosition="604" endWordPosition="607">ive an analysis of our system output in Section 5. Finally, we conclude and hint towards future improvement of the system, in Section 6. 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using m</context>
</contexts>
<marker>Shaalan, Allam, Gomah, 2003</marker>
<rawString>Khaled Shaalan, Amin Allam, and Abdallah Gomah. 2003. Towards Automatic Spell Checking for Arabic. In Proceedings of the 4th Conference on Language Engineering, Egyptian Society of Language Engineering (ELSE), Cairo, Egypt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khaled Shaalan</author>
<author>Rana Aref</author>
<author>Aly Fahmy</author>
</authors>
<title>An Approach for Analyzing and Correcting Spelling Errors for Non-native Arabic Learners.</title>
<date>2010</date>
<booktitle>In Proceedings of The 7th International Conference on Informatics and Systems, INFOS2010, the special track on Natural Language Processing and Knowledge Mining,</booktitle>
<pages>28--30</pages>
<location>Cairo, Egypt.</location>
<contexts>
<context position="4073" citStr="Shaalan et al., 2010" startWordPosition="618" endWordPosition="621">owards future improvement of the system, in Section 6. 2 Related Work Automatic error detection and correction include automatic spelling checking, grammar checking and postediting. Numerous approaches (both supervised and unsupervised) have been explored to improve the fluency of the text and reduce the percentage of outof-vocabulary words using NLP tools, resources, and heuristics, e.g., morphological analyzers, language models, and edit-distance measure (Kukich, 1992; Oflazer, 1996; Zribi and Ben Ahmed, 2003; Shaalan et al., 2003; Haddad and Yaseen, 2007; Hassan et al., 2008; Habash, 2008; Shaalan et al., 2010). There has been a lot of work on error correction for English (e.g., (Golding and Roth, 1999)). Other approaches learn models of correction by training on paired examples of errors and their corrections, which is the main goal of this work. For Arabic, this issue was studied in various directions and in different research work. In 2003, Shaalan et al. (2003) presented work on the specification and classification of spelling errors in Arabic. Later on, Haddad and Yaseen (2007) presented a hybrid approach using morphological features and rules to fine 137 Proceedings of the EMNLP 2014 Workshop </context>
</contexts>
<marker>Shaalan, Aref, Fahmy, 2010</marker>
<rawString>Khaled Shaalan, Rana Aref, and Aly Fahmy. 2010. An Approach for Analyzing and Correcting Spelling Errors for Non-native Arabic Learners. In Proceedings of The 7th International Conference on Informatics and Systems, INFOS2010, the special track on Natural Language Processing and Knowledge Mining, pages 28–30, Cairo, Egypt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Khaled Shaalan</author>
<author>Mohammed Attia</author>
<author>Pavel Pecina</author>
<author>Younes Samih</author>
<author>Josef van Genabith</author>
</authors>
<title>Arabic Word Generation and Modelling for Spell Checking.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012),</booktitle>
<pages>719--725</pages>
<location>Istanbul, Turkey.</location>
<marker>Shaalan, Attia, Pecina, Samih, van Genabith, 2012</marker>
<rawString>Khaled Shaalan, Mohammed Attia, Pavel Pecina, Younes Samih, and Josef van Genabith. 2012. Arabic Word Generation and Modelling for Spell Checking. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC-2012), pages 719–725, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wajdi Zaghouani</author>
</authors>
<title>Behrang Mohit, Nizar Habash, Ossama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura Farra, Sarah Alkuhlani, and Kemal Oflazer.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<location>Reykjavik, Iceland.</location>
<marker>Zaghouani, 2014</marker>
<rawString>Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Ossama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura Farra, Sarah Alkuhlani, and Kemal Oflazer. 2014. Large Scale Arabic Error Annotation: Guidelines and Framework. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chiraz Zribi</author>
<author>Mohammed Ben Ahmed</author>
</authors>
<title>Efficient Automatic Correction of Misspelled Arabic Words Based on Contextual Information.</title>
<date>2003</date>
<booktitle>In Proceedings of the Knowledge-Based Intelligent Information and Engineering Systems Conference,</booktitle>
<pages>770--777</pages>
<location>Oxford, UK.</location>
<marker>Zribi, Ahmed, 2003</marker>
<rawString>Chiraz Zribi and Mohammed Ben Ahmed. 2003. Efficient Automatic Correction of Misspelled Arabic Words Based on Contextual Information. In Proceedings of the Knowledge-Based Intelligent Information and Engineering Systems Conference, pages 770–777, Oxford, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
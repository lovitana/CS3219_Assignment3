<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000461">
<title confidence="0.9942975">
The DCU Terminology Translation System for the Medical Query Subtask
at WMT14
</title>
<author confidence="0.99647">
Tsuyoshi Okita, Ali Hosseinzadeh Vahid, Andy Way, Qun Liu
</author>
<affiliation confidence="0.99186">
Dublin City University, School of Computing
</affiliation>
<address confidence="0.691575">
Glasnevin, Dublin 9
Ireland
</address>
<email confidence="0.99691">
{tokita,avahid,away,qliu}@computing.dcu.ie
</email>
<sectionHeader confidence="0.997349" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999747733333333">
This paper describes the Dublin City
University terminology translation system
used for our participation in the query
translation subtask in the medical trans-
lation task in the Workshop on Statisti-
cal Machine Translation (WMT14). We
deployed six different kinds of terminol-
ogy extraction methods, and participated
in three different tasks: FR–EN and EN–
FR query tasks, and the CLIR task. We
obtained 36.2 BLEU points absolute for
FR–EN and 28.8 BLEU points absolute
for EN–FR tasks where we obtained the
first place in both tasks. We obtained 51.8
BLEU points absolute for the CLIR task.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999941365079365">
This paper describes the terminology translation
system developed at Dublin City University for
our participation in the query translation subtask at
the Workshop on Statistical Machine Translation
(WMT14). We developed six kinds of terminol-
ogy extraction methods for the problem of medi-
cal terminology translation, especially where rare
and new words are considered. We have several
motivations which we address before providing a
description of the actual algorithms undeprinning
our work.
First, terminology translation cannot be seen
just as a simple extension of the translation process
if we use an analogy from human translation. Ter-
minology translation can be considered as more
important and a quite different task than transla-
tion per se, so we need a considerably different
way of solving this particular problem. Bilingual
terminology selection has been claimed to be the
touchstone in human translation, especially where
scientific and legal translation are concerned. Ter-
minology selection is often the hardest and most
time-consuming process in the translation work-
flow. Depending on the particular requirements of
the use-case (Way, 2013), users may not object to
disfluent translations, but will invariably be very
sensitive to the wrong selection of terminology,
even if the meaning of the chosen terms is correct.
This is especially true if this selected terminology
does not match with that preferred by the users
themselves, in which case users are likely to ex-
press some kind of complaint; it may even be that
the entire translation is rejected as sub-standard or
inappropriate on such grounds.
Second, we look at how to handle new and rare
words. If we inspect the process of human trans-
lation more closely, it is easy to identify several
differences compared to the methods used in sta-
tistical MT (SMT). Unless stipulated by the client,
the selection of bilingual terminology can be a
highly subjective process. Accordingly, it is not
necessarily the bilingual term-pair with the highest
probability that is chosen by the human translator.
It is often the case that statistical methods often
forget about or delete less frequent n-grams, but
rely on more frequent n-grams using maximum
likelihood or Maximum A Priori (MAP) meth-
ods. If some terminology is highly suitable, a
human translator can use it quite freely. Further-
more, there are a lot of new words in reality for
which new target equivalents have to be created by
the translators themselves, so the question arises
as to how human translators actually select ap-
propriate new terminology. Transliteration, which
is often supported by many Asian languages in-
cluding Hindi, Japanese, and Chinese, is perhaps
the easiest things to do under such circumstances.
Slight modifications of alphabets/accented charac-
ters can sometimes successfully create a valid new
term, even for European languages.
The remainder of this paper is organized as fol-
lows. Section 2 describes our algorithms. Our
decoding strategy in Section 3. Our experimen-
</bodyText>
<page confidence="0.974579">
239
</page>
<bodyText confidence="0.67884075">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 239–245,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
tal settings and results are presented in Section 4,
and we conclude in Section 5.
</bodyText>
<sectionHeader confidence="0.999369" genericHeader="method">
2 Our Methods
</sectionHeader>
<bodyText confidence="0.999969">
Apart from the conventional statistical approach to
extract bilingual terminology, this medical query
task reminds us of two frequently occurring prob-
lems which are often ignored: (i) “Can we forget
about terminology which occurs only once in a
corpus?”, and (ii) “What can we do if the termi-
nology does not occur in a corpus?” These two
problems require computationally quite different
approaches than what is usually done in the stan-
dard statistical approach. Furthermore, the medi-
cal query task in WMT14 provides a wide range of
corpora: parallel and monolingual corpora, as well
as dictionaries. These two interesting aspects mo-
tivate our extraction methods which we present in
this section, including one relatively new Machine
Learning algorithm of zero-shot learning arising
from recent developments in the neural network
community (Bengio et al., 2000; Mikolov et al.,
2013b).
</bodyText>
<subsectionHeader confidence="0.890248">
2.1 Translation Model
</subsectionHeader>
<bodyText confidence="0.999924">
Word alignment (Brown et al., 1993) and phrase
extraction (Koehn et al., 2003) can capture bilin-
gual word- and phrase-pairs with a good deal of
accuracy. We omit further details of these stan-
dard methods which are freely available elsewhere
in the SMT literature (e.g. (Koehn, 2010)).
</bodyText>
<subsectionHeader confidence="0.998865">
2.2 Extraction from Parallel Corpora
</subsectionHeader>
<bodyText confidence="0.999935833333334">
(Okita et al., 2010) addressed the problem of
capturing bilingual term-pairs from parallel data
which might otherwise not be detected by the
translation model. Hence, the requirement in
Okita et al. is not to use SMT/GIZA++ (Och and
Ney, 2003) to extract term-pairs, which are the
common focus in this medical query translation
task.
The classical algorithm of (Kupiec, 1993) used
in (Okita et al., 2010) counts the statistics of ter-
minology c(etermi, ftermj|st) on the source and
the target sides which jointly occur in a sentence
st after detecting candidate terms via POS tag-
ging, which are then summed up over the entire
corpus EN t—i c(etermi, ftermj|st). Then, the al-
gorithm adjusts the length of etermi and ftermj.
It can be said that this algorithm captures term-
pairs which occur rather frequently. However, this
apparent strength can also be seen in disadvanta-
geous terms since the search for terminology oc-
curs densely in each of the sentences which in-
creases the computational complexity of this algo-
rithm, and causes the method to take a consider-
able time to run. Furthermore, if we suppose that
most frequent term-pairs are to be extracted via a
standard translation model (as described briefly in
the previous section), our efforts to search among
frequent pairs is not likely to bring about further
gain.
It is possible to approach this in a reverse man-
ner: “less frequent pairs can be outstanding term
candidates”. Accordingly, if our aim changes to
capture only those less frequent pairs, the situation
changes dramatically. The number of terms we
need to capture is considerably decreased. Many
sentences do not include any terminology at all,
and only a relatively small subset of sentences in-
cludes a few terms, such that term-pairs become
sparse with regard to sentences. Term-pairs can
be found rather easily if a candidate term-pair co-
occurs on the source and the target sides and on
the condition that the items in the term-pair actu-
ally correspond with one another.
This condition can be easily checked in various
ways. One way is to translate the source side of
the targeted pairs with the alignment option in the
Moses decoder (Koehn et al., 2007), which we did
in this evaluation campaign. Another way is to use
asupervised aligner, such as the Berkeley aligner
(Haghighi et al., 2009), to align the targeted pairs
and check whether they are actually aligned or not.
We assume two predefined sets of terms at
the outset, Eterm = {eterm1, . . . , etermn} and
Fterm = {fterm1, . . . , ftermn}. We search for
possible alignment links between the term-pair
only when they co-occur in the same sentence.
One obvious advantage of this approach is the
computational complexity which is fairly low.
Note that the result of (Okita et al., 2010)
shows that the frequency-based approach of (Ku-
piec, 1993) worked well for NTCIR patent termi-
nology (Fujii et al., 2010), which otherwise would
have been difficult to capture via the traditional
SMT/GIZA++ method. In contrast, however, this
did not work well on the Europarl corpus (Koehn,
2005).
</bodyText>
<page confidence="0.961593">
240
</page>
<subsectionHeader confidence="0.988781">
2.3 Terminology Dictionaries
</subsectionHeader>
<bodyText confidence="0.999801833333333">
Terminology dictionaries themselves are obvi-
ously among the most important resources for
bilingual term-pairs. In this medical query transla-
tion subtask, two corpora are provided for this pur-
pose: (i) Unified Medical Language System cor-
pus (UMLS corpus),1 and (ii) Wiki entries.2
</bodyText>
<sectionHeader confidence="0.4886575" genericHeader="method">
2.4 Extraction from Terminology
Dictionaries: lower-order n-grams
</sectionHeader>
<bodyText confidence="0.989292647058824">
Terminology dictionaries provide reliable higher-
order n-gram pairs. However, they do not often
provide the correspondences between the lower-
order n-grams contained therein. For example, the
UMLS corpus provides a term-pair of “abdominal
timent abdominal” (EN FR). However, such ter-
minology dictionaries often do not explicitly pro-
(EN FR). Clearly, these terminology dictionaries
implicitly provide the correspondent pairs. Note
that UMLS and Wiki entries provide terminol-
ogy dictionaries. Hence, it is possible to obtain
some suggestion by higher order n-gram models if
we know their alignments between words on the
source and target sides. Algorithm 1 shows the
overall procedure.
Algorithm 1 Lower-order n-gram extraction algo-
rithm
</bodyText>
<listItem confidence="0.9990125">
1: Perform monolingual word alignment for
higher-order n-gram pairs.
2: Collect only the reliable alignment pairs (i.e.
discard unreliable alignment pairs).
3: Extract the lower-order word pairs of our in-
terest.
</listItem>
<subsectionHeader confidence="0.917052">
2.5 Extraction from Monolingual Corpora:
Transliteration and Abbreviation
</subsectionHeader>
<bodyText confidence="0.9558">
Monolingual corpora can be used in various ways,
including:
</bodyText>
<listItem confidence="0.958074">
1. Transliteration: Many languages support the
</listItem>
<bodyText confidence="0.8044954">
fundamental mechanism of between Euro-
pean and Asian languages. Japanese even
supports a special alphabet – katakana – for
this purpose. Chinese and Hindi also per-
mit transliteration using their own alphabets.
</bodyText>
<footnote confidence="0.9997085">
1http://www.nlm.nih.gov/research/umls/.
2http://www.wikipedia.org.
</footnote>
<bodyText confidence="0.999225">
However, even among European languages,
this mechanism makes it possible to find
possible translation counterparts for a given
term. In this query task, we did this only
for the French-to-English direction and only
for words containing accented characters (by
rule-based conversion).
</bodyText>
<listItem confidence="0.687627">
2. Abbreviation: It is often the case that abbre-
</listItem>
<bodyText confidence="0.95866725">
viations should be resolved in the same lan-
guage. If the translation includes some ab-
breviation, such as “C. difficile”, this needs
to be investigated exhaustively in the same
language. However, in the specific domain
of medical terminology, it is quite likely that
possible phrase matches will be successfully
identified.
</bodyText>
<subsectionHeader confidence="0.8566685">
2.6 Extraction from Monolingual Corpora:
Zero-Shot Learning
</subsectionHeader>
<bodyText confidence="0.9353555">
Algorithm 2 Algorithm to connect two word em-
bedding space
</bodyText>
<listItem confidence="0.8813263125">
1: Prepare the monolingual source and target
sentences.
2: Prepare the dictionary which consists of U
entries of source and target sentences among
non-stop-words.
3: Train the neural network language model on
the source side and obtain the continuous
space real vectors of X dimensions for each
word.
4: Train the neural network language model on
the target side and obtain the continuous space
real vectors of X dimensions for each word.
5: Using the real vectors obtained in the above
steps, obtain the linear mapping between the
dictionary in two continuous spaces using
canonical component analysis (CCA).
</listItem>
<bodyText confidence="0.999374666666667">
Another interesting terminology extraction
method requires neither parallel nor comparable
corpora, but rather just monolingual corpora on
both sides (possibly unrelated to each other) to-
gether with a small amount of dictionary entries
which provide already known correspondences
between words on the source and target sides
(henceforth, we refer to this as the ‘dictionary’).
This method uses the recently developed zero-shot
learning (Palatucci et al., 2009) using neural net-
work language modelling (Bengio et al., 2000;
Mikolov et al., 2013b). Then, we train both sides
</bodyText>
<page confidence="0.989238">
241
</page>
<bodyText confidence="0.996465071428571">
with the neural network language model, and use
a continuous space representation to project words
to each other on the basis of a small amount of
correspondences in the dictionary. If we assume
that each continuous space is linear (Mikolov et
al., 2013c), we can connect them via linear projec-
tion (Mikolov et al., 2013b). Algorithm 2 shows
this situation.
In our experiments we use U the same as the
entries of Wiki and X as 50. Algorithm 3 shows
the algorithm to extract the counterpart of OOV
words.
Algorithm 3 Algorithm to extract the counterpart
of OOV words.
</bodyText>
<listItem confidence="0.9401715">
1: Prepare the projection by Algorithm 2.
2: Detect unknown words in the translation out-
puts.
3: Do the projection of it (the source word) into
the target word using the trained linear map-
pings in the training step.
</listItem>
<sectionHeader confidence="0.963478" genericHeader="method">
3 Decoding Strategy
</sectionHeader>
<bodyText confidence="0.999882547619048">
We deploy six kinds of extraction methods: (1)
translation model, (2) extraction from parallel cor-
pora, (3) terminology dictionaries, (4) lower-order
n-grams, (5) transliteration and abbreviation, and
(6) zero-shot learning. Among these we deploy
four of them – (2), (4), (5) and (6) – in a limited
context, while the remaining two are used with-
out any context, mainly owing to time constraints;
only when we did not find the correspondent pairs
via (1) and (3), did we complement this by the
other methods.
The detected bilingual term-pairs using (1) and
(3) can be combined using various methods. One
way is to employ a method similar to (confu-
sion network-based) system combination (Okita
and van Genabith, 2011; Okita and van Genabith,
2012). First we make a lattice: if we regard one
candidate of (1) and two candidates in (3) as trans-
lation outputs where the words of two candidates
in (3) are connected using an underscore (i.e. one
word), we can make a lattice. Then, we can deploy
monotonic decoding over them. If we do this for
the devset and then apply it to the test set, we can
incorporate a possible preference learnt from the
development set, i.e. whether the query transla-
tor prefers method (1) or UMLS/Wiki translation.
MERT process and language model are applied in
a similar manner with (confusion network-based)
system combination (cf. (Okita and van Genabith,
2011)).
We note also that a lattice structure is useful for
handling grammatical coordination. Since queries
are formed by real users, reserved words for
database query such as “AND” (or “ET” (FR)) and
“OR” (or “OU” (FR)) are frequently observed in
the test set. Furthermore, there is repeated use of
“and” more than twice, for example “douleur ab-
nominal et Helicobacter pylori et cancer”, which
makes it very difficult to detect the correct coor-
dination boundaries. The lattice on the input side
can express such ambiguity at the cost of splitting
the source-side sentence in a different manner.
</bodyText>
<sectionHeader confidence="0.998903" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999983852941176">
The baseline is obtained in the following way. The
GIZA++ implementation (Och and Ney, 2003) of
IBM Model 4 is used as the baseline for word
alignment: Model 4 is incrementally trained by
performing 5 iterations of Model 1, 5 iterations of
HMM, 3 iterations of Model 3, and 3 iterations
of Model 4. For phrase extraction the grow-diag-
final heuristics described in (Koehn et al., 2003) is
used to derive the refined alignment from bidirec-
tional alignments. We then perform MERT (Och,
2003) which optimizes parameter settings using
the BLEU metric (Papineni et al., 2002), while a 5-
gram language model is derived with Kneser-Ney
smoothing (Kneser and Ney, 1995) trained using
SRILM (Stolcke, 2002). We use the whole train-
ing corpora including the WMT14 translation task
corpora as well as medical domain data. UMLS
and Wikipedia are used just as training corpora for
the baseline.
For the extraction from parallel corpora (cf.
Section 2.2), we used Genia tagger (Tsuruoka and
Tsujii, 2005) and the Berkeley parser (Petrov and
Klein, 2007). For the zero-shot learning (cf. Sec-
tion 2.6) we used scikit learn (Pedregosa et al.,
2011), word2vec (Mikolov et al., 2013a), and a
recurrent neural network (Mikolov, 2012). Other
tools used are in-house software.
Table 2 shows the results for the FR–EN query
task. We obtained 36.2 BLEU points absolute,
which is an improvement of 6.3 BLEU point ab-
solute (21.1% relative) over the baseline. Table
3 shows the results for the EN–FR query task.
We obtained 28.8 BLEU points absolute, which
is an improvement of 8.7 BLEU points abso-
</bodyText>
<page confidence="0.993843">
242
</page>
<bodyText confidence="0.9968248">
lute (43% relative) over the baseline. Our sys-
tem was the best system for both of these tasks.
These improvements over the baseline were sta-
tistically significant by a paired bootstrap test
(Koehn, 2004).
</bodyText>
<table confidence="0.9991746">
Query task FR–EN
Our method baseline
BLEU 36.2 29.9
BLEU cased 30.9 26.5
TER 0.340 0.443
</table>
<tableCaption confidence="0.985647">
Table 1: Results for FR–EN query task.
</tableCaption>
<figureCaption confidence="0.5971005">
extraction LM MERT BLEU (cased)
(1) - (6) all Y 30.9
(1), (2), (3) all Y 30.3
(1), (3), (6) all Y 30.1
(1), (3), (4) all Y 29.1
(1), (3), (5) all Y 29.0
</figureCaption>
<listItem confidence="0.937089857142857">
(1) and (3) all Y 29.0
(1) and (3) medical Y 27.5
(1) and (3) WMT Y 27.0
(1) and (3) medical N 25.1
(1) and (3) WMT N 24.3
(1) medical Y 25.9
(1) WMT Y 25.0
</listItem>
<bodyText confidence="0.977974454545455">
Table 2: Table shows the effects of extraction
methods, language model and MERT process. All
the measurements are by BLEU (cased). In this
table, “medical” indicates a language model built
on all the medical corpora while “WMT” indicates
a language model built on all the non-medical cor-
pora. Note that some sentence in testset can be
considered as non-medical domain. Extraction
methods (1) - (6) correspond to those described in
Section 2.1 - 2.6.
Table 4 shows the results for CLIR task. We
obtained 51.8 BLEU points absolute, which is an
improvement of 9.4 BLEU point absolute (22.2%
relative) over the baseline. Although CLIR task al-
lowed 10-best lists, our submission included only
1-best list. This resulted in the score of P@5 of
0.348 and P@10 of 0.346 which correspond to
the second place, despite a good result in terms
of BLEU. This is since unlike BLEU score P@5
and P@10 measure whether the whole elements
in reference and hypothesis are matched or not.
We noticed that our submission included a lot of
</bodyText>
<table confidence="0.9986278">
Query task EN–FR
Our method baseline
BLEU 28.8 20.1
BLEU cased 27.7 18.7
TER 0.483 0.582
</table>
<tableCaption confidence="0.998745">
Table 3: Results for EN–FR query task.
</tableCaption>
<bodyText confidence="0.999674888888889">
near miss sentences only in terms of capitaliza-
tion: “abnominal pain and Helicobacter pylori and
cancer” (reference) and “abnominal pain and heli-
cobacter pylori and cancer” (submission). These
are counted as incorrect in terms of P@5 and
P@10.3 Noted that after submission we obtained
the revised score of P@5 of 0.560 and P@10 of
0.560 with the same method but with 2-best lists
which handles the capitalization varieties.
</bodyText>
<table confidence="0.999360153846154">
CLIR task FR–EN
Our method baseline
BLEU 51.8 42.2
BLEU cased 46.0 38.3
TER 0.364 0.398
P@5 0.348 (0.560*) –
P@10 0.346 (0.560*) –
NDCG@5 0.306 –
NDCG@10 0.307 –
MAP 0.2252 –
Rprec 0.2358 –
bpref 0.3659 –
relRet 1524 –
</table>
<tableCaption confidence="0.994034">
Table 4: Results for CLIR task.
</tableCaption>
<sectionHeader confidence="0.993292" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999896363636363">
This paper provides a description of the Dublin
City University terminology translation system for
our participation in the query translation subtask
in the medical translation task in the Workshop on
Statistical Machine Translation (WMT14). We de-
ployed six different kinds of terminology extrac-
tion methods. We obtained 36.2 BLEU points ab-
solute for FR–EN, and 28.8 BLEU points abso-
lute for EN–FR tasks, obtaining first place on both
tasks. We obtained 51.8 BLEU points absolute for
the CLIR task.
</bodyText>
<footnote confidence="0.998318333333333">
3The method which incorporates variation in capitaliza-
tion in its n-best lists outperforms the best result in terms of
P@5 and P@10.
</footnote>
<page confidence="0.99412">
243
</page>
<table confidence="0.795929444444444">
Acknowledgments
This research is supported by the Science Founda-
tion Ireland (Grant 07/CE/I1142) as part of CNGL
at Dublin City University.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
Machine Translation Summit, pages 79–86.
Philipp Koehn. 2010. Statistical machine translation.
Cambridge University Press.
</table>
<sectionHeader confidence="0.985487" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999645191489362">
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2000. A neural probabilistic language model. In
Proceedings of Neural Information Systems, pages
1137–1155.
Peter F. Brown, Vincent J.D Pietra, Stephen A.D.Pietra,
and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, Vol.19, Issue 2,
pages 263–311.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2010. Overview of the
patent translation task at the NTCIR-8 workshop.
In Proceedings of the 8th NTCIR Workshop Meet-
ing on Evaluation of Information Access Technolo-
gies: Information Retrieval, Question Answering
and Cross-lingual Information Access, pages 293–
302.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised itg models. In In Proceedings of the Confer-
ence of Association for Computational Linguistics,
pages 923–931.
Reinhard Kneser and Hermann Ney. 1995. Im-
proved backing-off for n-gram language modeling.
In Proceedings of the IEEE International Confer-
ence on Acoustics, Speech and Signal Processing,
pages 181–184.
Philipp Koehn, Franz Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computationa Linguistics (HLT / NAACL
2003), pages 115–124.
Philipp Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177–180.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2004), pages 388–395.
Julian. Kupiec. 1993. An algorithm for finding Noun
phrase correspondences in bilingual corpora. In
Proceedings of the 31st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 17–22.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at International Conference on Learning Represen-
tations.
Tomas Mikolov, Quoc V. Le, and Ilya Sutskever.
2013b. Exploiting similarities among languages for
machine translation. ArXiv:1309.4168.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics / Human Lan-
guage Technology (NAACL/HLT 2005), pages 746–
751.
Tomas Mikolov. 2012. Statistical language models
based on neural networks. PhD thesis at Brno Uni-
versity of Technology.
Franz Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19–51.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160–167.
Tsuyoshi Okita and Josef van Genabith. 2011. DCU
Confusion Network-based System Combination for
ML4HMT. Shared Task on Applying Machine
Learning techniques to optimising the division of
labour in Hybrid MT (ML4HMT-2011, collocated
with LIHMT-2011), pages 93–98.
Tsuyoshi Okita and Josef van Genabith. 2012. Mini-
mum Bayes Risk Decoding with Enlarged Hypoth-
esis Space in System Combination. In Proceed-
ings of 13th International Conference on Intelligent
Text Processing and Computational Linguistics (CI-
CLING 2012), pages 40–51.
Tsuyoshi Okita, Alfredo Maldonado Guerra, Yvette
Graham, and Andy Way. 2010. Multi-word
expression-sensitive word alignment. In Proceed-
ings of the Fourth International Workshop On Cross
Ling ual Information Access (CLIA2010, collocated
with COLING2010), pages 26–34.
</reference>
<page confidence="0.978148">
244
</page>
<reference confidence="0.9981713125">
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton,
and Tom Mitchell. 2009. Zero-shot learning with
semantic output codes. In Neural Information Pro-
cessing Systems (NIPS), pages 1410–1418.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
BLEU: A Method For Automatic Evaluation of Ma-
chine Translation. In Proceedings of the 40th An-
nual Meeting of the Association for Computational
Linguistics (ACL-02).
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In Proceedings
of AAAI (Nectar Track), pages 1663–1666.
Andreas Stolcke. 2002. SRILM – An extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, pages 901–904.
Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidi-
rectional inference with the easiest-first strategy
for tagging sequence data. In Proceedings of the
Conference on Human Language Technology / Em-
pirical Methods on Natural Language Processing
(HLT/EMNLP 2005), pages 467–474.
Andy Way. 2013. Traditional and emerging use-cases
for machine translation. In Proceedings of Translat-
ing and the Computer 35.
</reference>
<page confidence="0.998673">
245
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.284266">
<title confidence="0.843869">The DCU Terminology Translation System for the Medical Query Subtask at WMT14</title>
<author confidence="0.804431">Tsuyoshi Okita</author>
<author confidence="0.804431">Ali Hosseinzadeh Vahid</author>
<author confidence="0.804431">Andy Way</author>
<author confidence="0.804431">Qun</author>
<affiliation confidence="0.64616">Dublin City University, School of</affiliation>
<address confidence="0.417293">Glasnevin, Dublin</address>
<abstract confidence="0.9920153125">This paper describes the Dublin City University terminology translation system used for our participation in the query translation subtask in the medical translation task in the Workshop on Statistical Machine Translation (WMT14). We deployed six different kinds of terminology extraction methods, and participated in three different tasks: FR–EN and EN– FR query tasks, and the CLIR task. We obtained 36.2 BLEU points absolute for FR–EN and 28.8 BLEU points absolute for EN–FR tasks where we obtained the first place in both tasks. We obtained 51.8 BLEU points absolute for the CLIR task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical machine translation.</title>
<date>2010</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="5345" citStr="Koehn, 2010" startWordPosition="831" endWordPosition="832"> well as dictionaries. These two interesting aspects motivate our extraction methods which we present in this section, including one relatively new Machine Learning algorithm of zero-shot learning arising from recent developments in the neural network community (Bengio et al., 2000; Mikolov et al., 2013b). 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi, ftermj|st) on the source and the target sides which jointly occur in a sentence st after detecting candidate</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical machine translation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2000</date>
<booktitle>In Proceedings of Neural Information Systems,</booktitle>
<pages>1137--1155</pages>
<contexts>
<context position="5015" citStr="Bengio et al., 2000" startWordPosition="775" endWordPosition="778">in a corpus?”, and (ii) “What can we do if the terminology does not occur in a corpus?” These two problems require computationally quite different approaches than what is usually done in the standard statistical approach. Furthermore, the medical query task in WMT14 provides a wide range of corpora: parallel and monolingual corpora, as well as dictionaries. These two interesting aspects motivate our extraction methods which we present in this section, including one relatively new Machine Learning algorithm of zero-shot learning arising from recent developments in the neural network community (Bengio et al., 2000; Mikolov et al., 2013b). 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och a</context>
<context position="12131" citStr="Bengio et al., 2000" startWordPosition="1889" endWordPosition="1892">mapping between the dictionary in two continuous spaces using canonical component analysis (CCA). Another interesting terminology extraction method requires neither parallel nor comparable corpora, but rather just monolingual corpora on both sides (possibly unrelated to each other) together with a small amount of dictionary entries which provide already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). This method uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013b). Then, we train both sides 241 with the neural network language model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences in the dictionary. If we assume that each continuous space is linear (Mikolov et al., 2013c), we can connect them via linear projection (Mikolov et al., 2013b). Algorithm 2 shows this situation. In our experiments we use U the same as the entries of Wiki and X as 50. Algorithm 3 shows the algorithm to extract the counterpart of OOV words. Algorithm 3 Algorithm to extract the cou</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2000</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. In Proceedings of Neural Information Systems, pages 1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J D Pietra</author>
<author>Stephen A D Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics, Vol.19, Issue</journal>
<volume>2</volume>
<pages>263--311</pages>
<contexts>
<context position="5098" citStr="Brown et al., 1993" startWordPosition="788" endWordPosition="791">us?” These two problems require computationally quite different approaches than what is usually done in the standard statistical approach. Furthermore, the medical query task in WMT14 provides a wide range of corpora: parallel and monolingual corpora, as well as dictionaries. These two interesting aspects motivate our extraction methods which we present in this section, including one relatively new Machine Learning algorithm of zero-shot learning arising from recent developments in the neural network community (Bengio et al., 2000; Mikolov et al., 2013b). 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical que</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J.D Pietra, Stephen A.D.Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, Vol.19, Issue 2, pages 263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
<author>Masao Utiyama</author>
<author>Mikio Yamamoto</author>
</authors>
<title>Takehito Utsuro, Terumasa Ehara, Hiroshi Echizenya, and Sayori Shimohata.</title>
<date>2010</date>
<booktitle>In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-lingual Information Access,</booktitle>
<pages>293--302</pages>
<contexts>
<context position="8268" citStr="Fujii et al., 2010" startWordPosition="1323" endWordPosition="1326">h as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. We assume two predefined sets of terms at the outset, Eterm = {eterm1, . . . , etermn} and Fterm = {fterm1, . . . , ftermn}. We search for possible alignment links between the term-pair only when they co-occur in the same sentence. One obvious advantage of this approach is the computational complexity which is fairly low. Note that the result of (Okita et al., 2010) shows that the frequency-based approach of (Kupiec, 1993) worked well for NTCIR patent terminology (Fujii et al., 2010), which otherwise would have been difficult to capture via the traditional SMT/GIZA++ method. In contrast, however, this did not work well on the Europarl corpus (Koehn, 2005). 240 2.3 Terminology Dictionaries Terminology dictionaries themselves are obviously among the most important resources for bilingual term-pairs. In this medical query translation subtask, two corpora are provided for this purpose: (i) Unified Medical Language System corpus (UMLS corpus),1 and (ii) Wiki entries.2 2.4 Extraction from Terminology Dictionaries: lower-order n-grams Terminology dictionaries provide reliable hi</context>
</contexts>
<marker>Fujii, Utiyama, Yamamoto, 2010</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, Terumasa Ehara, Hiroshi Echizenya, and Sayori Shimohata. 2010. Overview of the patent translation task at the NTCIR-8 workshop. In Proceedings of the 8th NTCIR Workshop Meeting on Evaluation of Information Access Technologies: Information Retrieval, Question Answering and Cross-lingual Information Access, pages 293– 302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>John Blitzer</author>
<author>John DeNero</author>
<author>Dan Klein</author>
</authors>
<title>Better word alignments with supervised itg models. In</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference of Association for Computational Linguistics,</booktitle>
<pages>923--931</pages>
<contexts>
<context position="7698" citStr="Haghighi et al., 2009" startWordPosition="1222" endWordPosition="1225">ubset of sentences includes a few terms, such that term-pairs become sparse with regard to sentences. Term-pairs can be found rather easily if a candidate term-pair cooccurs on the source and the target sides and on the condition that the items in the term-pair actually correspond with one another. This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. We assume two predefined sets of terms at the outset, Eterm = {eterm1, . . . , etermn} and Fterm = {fterm1, . . . , ftermn}. We search for possible alignment links between the term-pair only when they co-occur in the same sentence. One obvious advantage of this approach is the computational complexity which is fairly low. Note that the result of (Okita et al., 2010) shows that the frequency-based approach of (Kupiec, 1993) worked well for NTCIR patent terminology (Fujii et al., 2010), which otherwise would have b</context>
</contexts>
<marker>Haghighi, Blitzer, DeNero, Klein, 2009</marker>
<rawString>Aria Haghighi, John Blitzer, John DeNero, and Dan Klein. 2009. Better word alignments with supervised itg models. In In Proceedings of the Conference of Association for Computational Linguistics, pages 923–931.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for n-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<pages>181--184</pages>
<contexts>
<context position="15652" citStr="Kneser and Ney, 1995" startWordPosition="2483" endWordPosition="2486">g way. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diagfinal heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT (Och, 2003) which optimizes parameter settings using the BLEU metric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we used scikit learn (Pedregosa et al., 2011), word2vec (Mikolov et al., 2013a), and a recurrent neural network (Mikolov, 2012). Other tools used are in-house software. Table 2</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for n-gram language modeling. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computationa Linguistics (HLT / NAACL</booktitle>
<pages>115--124</pages>
<contexts>
<context position="5141" citStr="Koehn et al., 2003" startWordPosition="795" endWordPosition="798">ally quite different approaches than what is usually done in the standard statistical approach. Furthermore, the medical query task in WMT14 provides a wide range of corpora: parallel and monolingual corpora, as well as dictionaries. These two interesting aspects motivate our extraction methods which we present in this section, including one relatively new Machine Learning algorithm of zero-shot learning arising from recent developments in the neural network community (Bengio et al., 2000; Mikolov et al., 2013b). 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorith</context>
<context position="15375" citStr="Koehn et al., 2003" startWordPosition="2439" endWordPosition="2442">er”, which makes it very difficult to detect the correct coordination boundaries. The lattice on the input side can express such ambiguity at the cost of splitting the source-side sentence in a different manner. 4 Experimental Results The baseline is obtained in the following way. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diagfinal heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT (Och, 2003) which optimizes parameter settings using the BLEU metric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, </context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computationa Linguistics (HLT / NAACL 2003), pages 115–124.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: Open source toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="7559" citStr="Koehn et al., 2007" startWordPosition="1199" endWordPosition="1202"> terms we need to capture is considerably decreased. Many sentences do not include any terminology at all, and only a relatively small subset of sentences includes a few terms, such that term-pairs become sparse with regard to sentences. Term-pairs can be found rather easily if a candidate term-pair cooccurs on the source and the target sides and on the condition that the items in the term-pair actually correspond with one another. This condition can be easily checked in various ways. One way is to translate the source side of the targeted pairs with the alignment option in the Moses decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. We assume two predefined sets of terms at the outset, Eterm = {eterm1, . . . , etermn} and Fterm = {fterm1, . . . , ftermn}. We search for possible alignment links between the term-pair only when they co-occur in the same sentence. One obvious advantage of this approach is the computational complexity which is fairly low. Note that the result of (Okita et al., 2010) shows that</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2007. Moses: Open source toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP 2004),</booktitle>
<pages>388--395</pages>
<contexts>
<context position="16768" citStr="Koehn, 2004" startWordPosition="2671" endWordPosition="2672"> and a recurrent neural network (Mikolov, 2012). Other tools used are in-house software. Table 2 shows the results for the FR–EN query task. We obtained 36.2 BLEU points absolute, which is an improvement of 6.3 BLEU point absolute (21.1% relative) over the baseline. Table 3 shows the results for the EN–FR query task. We obtained 28.8 BLEU points absolute, which is an improvement of 8.7 BLEU points abso242 lute (43% relative) over the baseline. Our system was the best system for both of these tasks. These improvements over the baseline were statistically significant by a paired bootstrap test (Koehn, 2004). Query task FR–EN Our method baseline BLEU 36.2 29.9 BLEU cased 30.9 26.5 TER 0.340 0.443 Table 1: Results for FR–EN query task. extraction LM MERT BLEU (cased) (1) - (6) all Y 30.9 (1), (2), (3) all Y 30.3 (1), (3), (6) all Y 30.1 (1), (3), (4) all Y 29.1 (1), (3), (5) all Y 29.0 (1) and (3) all Y 29.0 (1) and (3) medical Y 27.5 (1) and (3) WMT Y 27.0 (1) and (3) medical N 25.1 (1) and (3) WMT N 24.3 (1) medical Y 25.9 (1) WMT Y 25.0 Table 2: Table shows the effects of extraction methods, language model and MERT process. All the measurements are by BLEU (cased). In this table, “medical” indi</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP 2004), pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kupiec</author>
</authors>
<title>An algorithm for finding Noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>17--22</pages>
<contexts>
<context position="5760" citStr="Kupiec, 1993" startWordPosition="896" endWordPosition="897">apture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi, ftermj|st) on the source and the target sides which jointly occur in a sentence st after detecting candidate terms via POS tagging, which are then summed up over the entire corpus EN t—i c(etermi, ftermj|st). Then, the algorithm adjusts the length of etermi and ftermj. It can be said that this algorithm captures termpairs which occur rather frequently. However, this apparent strength can also be seen in disadvantageous terms since the search for terminology occurs densely in each of the sentences which increases the c</context>
<context position="8206" citStr="Kupiec, 1993" startWordPosition="1313" endWordPosition="1315">campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. We assume two predefined sets of terms at the outset, Eterm = {eterm1, . . . , etermn} and Fterm = {fterm1, . . . , ftermn}. We search for possible alignment links between the term-pair only when they co-occur in the same sentence. One obvious advantage of this approach is the computational complexity which is fairly low. Note that the result of (Okita et al., 2010) shows that the frequency-based approach of (Kupiec, 1993) worked well for NTCIR patent terminology (Fujii et al., 2010), which otherwise would have been difficult to capture via the traditional SMT/GIZA++ method. In contrast, however, this did not work well on the Europarl corpus (Koehn, 2005). 240 2.3 Terminology Dictionaries Terminology dictionaries themselves are obviously among the most important resources for bilingual term-pairs. In this medical query translation subtask, two corpora are provided for this purpose: (i) Unified Medical Language System corpus (UMLS corpus),1 and (ii) Wiki entries.2 2.4 Extraction from Terminology Dictionaries: lo</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Julian. Kupiec. 1993. An algorithm for finding Noun phrase correspondences in bilingual corpora. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 17–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at International Conference on Learning Representations.</booktitle>
<contexts>
<context position="5037" citStr="Mikolov et al., 2013" startWordPosition="779" endWordPosition="782">i) “What can we do if the terminology does not occur in a corpus?” These two problems require computationally quite different approaches than what is usually done in the standard statistical approach. Furthermore, the medical query task in WMT14 provides a wide range of corpora: parallel and monolingual corpora, as well as dictionaries. These two interesting aspects motivate our extraction methods which we present in this section, including one relatively new Machine Learning algorithm of zero-shot learning arising from recent developments in the neural network community (Bengio et al., 2000; Mikolov et al., 2013b). 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extra</context>
<context position="12153" citStr="Mikolov et al., 2013" startWordPosition="1893" endWordPosition="1896">ictionary in two continuous spaces using canonical component analysis (CCA). Another interesting terminology extraction method requires neither parallel nor comparable corpora, but rather just monolingual corpora on both sides (possibly unrelated to each other) together with a small amount of dictionary entries which provide already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). This method uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013b). Then, we train both sides 241 with the neural network language model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences in the dictionary. If we assume that each continuous space is linear (Mikolov et al., 2013c), we can connect them via linear projection (Mikolov et al., 2013b). Algorithm 2 shows this situation. In our experiments we use U the same as the entries of Wiki and X as 50. Algorithm 3 shows the algorithm to extract the counterpart of OOV words. Algorithm 3 Algorithm to extract the counterpart of OOV words.</context>
<context position="16153" citStr="Mikolov et al., 2013" startWordPosition="2565" endWordPosition="2568">ric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we used scikit learn (Pedregosa et al., 2011), word2vec (Mikolov et al., 2013a), and a recurrent neural network (Mikolov, 2012). Other tools used are in-house software. Table 2 shows the results for the FR–EN query task. We obtained 36.2 BLEU points absolute, which is an improvement of 6.3 BLEU point absolute (21.1% relative) over the baseline. Table 3 shows the results for the EN–FR query task. We obtained 28.8 BLEU points absolute, which is an improvement of 8.7 BLEU points abso242 lute (43% relative) over the baseline. Our system was the best system for both of these tasks. These improvements over the baseline were statistically significant by a paired bootstrap tes</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Quoc V Le</author>
<author>Ilya Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation.</title>
<date>2013</date>
<journal>ArXiv:1309.4168.</journal>
<contexts>
<context position="5037" citStr="Mikolov et al., 2013" startWordPosition="779" endWordPosition="782">i) “What can we do if the terminology does not occur in a corpus?” These two problems require computationally quite different approaches than what is usually done in the standard statistical approach. Furthermore, the medical query task in WMT14 provides a wide range of corpora: parallel and monolingual corpora, as well as dictionaries. These two interesting aspects motivate our extraction methods which we present in this section, including one relatively new Machine Learning algorithm of zero-shot learning arising from recent developments in the neural network community (Bengio et al., 2000; Mikolov et al., 2013b). 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extra</context>
<context position="12153" citStr="Mikolov et al., 2013" startWordPosition="1893" endWordPosition="1896">ictionary in two continuous spaces using canonical component analysis (CCA). Another interesting terminology extraction method requires neither parallel nor comparable corpora, but rather just monolingual corpora on both sides (possibly unrelated to each other) together with a small amount of dictionary entries which provide already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). This method uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013b). Then, we train both sides 241 with the neural network language model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences in the dictionary. If we assume that each continuous space is linear (Mikolov et al., 2013c), we can connect them via linear projection (Mikolov et al., 2013b). Algorithm 2 shows this situation. In our experiments we use U the same as the entries of Wiki and X as 50. Algorithm 3 shows the algorithm to extract the counterpart of OOV words. Algorithm 3 Algorithm to extract the counterpart of OOV words.</context>
<context position="16153" citStr="Mikolov et al., 2013" startWordPosition="2565" endWordPosition="2568">ric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we used scikit learn (Pedregosa et al., 2011), word2vec (Mikolov et al., 2013a), and a recurrent neural network (Mikolov, 2012). Other tools used are in-house software. Table 2 shows the results for the FR–EN query task. We obtained 36.2 BLEU points absolute, which is an improvement of 6.3 BLEU point absolute (21.1% relative) over the baseline. Table 3 shows the results for the EN–FR query task. We obtained 28.8 BLEU points absolute, which is an improvement of 8.7 BLEU points abso242 lute (43% relative) over the baseline. Our system was the best system for both of these tasks. These improvements over the baseline were statistically significant by a paired bootstrap tes</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Tomas Mikolov, Quoc V. Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for machine translation. ArXiv:1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics / Human Language Technology (NAACL/HLT</booktitle>
<pages>746--751</pages>
<contexts>
<context position="5037" citStr="Mikolov et al., 2013" startWordPosition="779" endWordPosition="782">i) “What can we do if the terminology does not occur in a corpus?” These two problems require computationally quite different approaches than what is usually done in the standard statistical approach. Furthermore, the medical query task in WMT14 provides a wide range of corpora: parallel and monolingual corpora, as well as dictionaries. These two interesting aspects motivate our extraction methods which we present in this section, including one relatively new Machine Learning algorithm of zero-shot learning arising from recent developments in the neural network community (Bengio et al., 2000; Mikolov et al., 2013b). 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extra</context>
<context position="12153" citStr="Mikolov et al., 2013" startWordPosition="1893" endWordPosition="1896">ictionary in two continuous spaces using canonical component analysis (CCA). Another interesting terminology extraction method requires neither parallel nor comparable corpora, but rather just monolingual corpora on both sides (possibly unrelated to each other) together with a small amount of dictionary entries which provide already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). This method uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013b). Then, we train both sides 241 with the neural network language model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences in the dictionary. If we assume that each continuous space is linear (Mikolov et al., 2013c), we can connect them via linear projection (Mikolov et al., 2013b). Algorithm 2 shows this situation. In our experiments we use U the same as the entries of Wiki and X as 50. Algorithm 3 shows the algorithm to extract the counterpart of OOV words. Algorithm 3 Algorithm to extract the counterpart of OOV words.</context>
<context position="16153" citStr="Mikolov et al., 2013" startWordPosition="2565" endWordPosition="2568">ric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we used scikit learn (Pedregosa et al., 2011), word2vec (Mikolov et al., 2013a), and a recurrent neural network (Mikolov, 2012). Other tools used are in-house software. Table 2 shows the results for the FR–EN query task. We obtained 36.2 BLEU points absolute, which is an improvement of 6.3 BLEU point absolute (21.1% relative) over the baseline. Table 3 shows the results for the EN–FR query task. We obtained 28.8 BLEU points absolute, which is an improvement of 8.7 BLEU points abso242 lute (43% relative) over the baseline. Our system was the best system for both of these tasks. These improvements over the baseline were statistically significant by a paired bootstrap tes</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics / Human Language Technology (NAACL/HLT 2005), pages 746– 751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Statistical language models based on neural networks.</title>
<date>2012</date>
<tech>PhD thesis</tech>
<institution>at Brno University of Technology.</institution>
<contexts>
<context position="16203" citStr="Mikolov, 2012" startWordPosition="2574" endWordPosition="2575">del is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we used scikit learn (Pedregosa et al., 2011), word2vec (Mikolov et al., 2013a), and a recurrent neural network (Mikolov, 2012). Other tools used are in-house software. Table 2 shows the results for the FR–EN query task. We obtained 36.2 BLEU points absolute, which is an improvement of 6.3 BLEU point absolute (21.1% relative) over the baseline. Table 3 shows the results for the EN–FR query task. We obtained 28.8 BLEU points absolute, which is an improvement of 8.7 BLEU points abso242 lute (43% relative) over the baseline. Our system was the best system for both of these tasks. These improvements over the baseline were statistically significant by a paired bootstrap test (Koehn, 2004). Query task FR–EN Our method basel</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Tomas Mikolov. 2012. Statistical language models based on neural networks. PhD thesis at Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="5628" citStr="Och and Ney, 2003" startWordPosition="874" endWordPosition="877"> 2000; Mikolov et al., 2013b). 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi, ftermj|st) on the source and the target sides which jointly occur in a sentence st after detecting candidate terms via POS tagging, which are then summed up over the entire corpus EN t—i c(etermi, ftermj|st). Then, the algorithm adjusts the length of etermi and ftermj. It can be said that this algorithm captures termpairs which occur rather frequently. However, this apparent strength can </context>
<context position="15083" citStr="Och and Ney, 2003" startWordPosition="2387" endWordPosition="2390"> Since queries are formed by real users, reserved words for database query such as “AND” (or “ET” (FR)) and “OR” (or “OU” (FR)) are frequently observed in the test set. Furthermore, there is repeated use of “and” more than twice, for example “douleur abnominal et Helicobacter pylori et cancer”, which makes it very difficult to detect the correct coordination boundaries. The lattice on the input side can express such ambiguity at the cost of splitting the source-side sentence in a different manner. 4 Experimental Results The baseline is obtained in the following way. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diagfinal heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT (Och, 2003) which optimizes parameter settings using the BLEU metric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="15479" citStr="Och, 2003" startWordPosition="2458" endWordPosition="2459">an express such ambiguity at the cost of splitting the source-side sentence in a different manner. 4 Experimental Results The baseline is obtained in the following way. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diagfinal heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT (Och, 2003) which optimizes parameter settings using the BLEU metric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsuyoshi Okita</author>
<author>Josef van Genabith</author>
</authors>
<title>DCU Confusion Network-based System Combination for ML4HMT. Shared Task on Applying Machine Learning techniques to optimising the division of labour</title>
<date>2011</date>
<booktitle>in Hybrid MT (ML4HMT-2011, collocated with LIHMT-2011),</booktitle>
<pages>93--98</pages>
<marker>Okita, van Genabith, 2011</marker>
<rawString>Tsuyoshi Okita and Josef van Genabith. 2011. DCU Confusion Network-based System Combination for ML4HMT. Shared Task on Applying Machine Learning techniques to optimising the division of labour in Hybrid MT (ML4HMT-2011, collocated with LIHMT-2011), pages 93–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsuyoshi Okita</author>
<author>Josef van Genabith</author>
</authors>
<title>Minimum Bayes Risk Decoding with Enlarged Hypothesis Space in System Combination.</title>
<date>2012</date>
<booktitle>In Proceedings of 13th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING</booktitle>
<pages>40--51</pages>
<marker>Okita, van Genabith, 2012</marker>
<rawString>Tsuyoshi Okita and Josef van Genabith. 2012. Minimum Bayes Risk Decoding with Enlarged Hypothesis Space in System Combination. In Proceedings of 13th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING 2012), pages 40–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsuyoshi Okita</author>
<author>Alfredo Maldonado Guerra</author>
<author>Yvette Graham</author>
<author>Andy Way</author>
</authors>
<title>Multi-word expression-sensitive word alignment.</title>
<date>2010</date>
<booktitle>In ProceedLing ual Information Access (CLIA2010, collocated with COLING2010),</booktitle>
<pages>26--34</pages>
<contexts>
<context position="5405" citStr="Okita et al., 2010" startWordPosition="838" endWordPosition="841">motivate our extraction methods which we present in this section, including one relatively new Machine Learning algorithm of zero-shot learning arising from recent developments in the neural network community (Bengio et al., 2000; Mikolov et al., 2013b). 2.1 Translation Model Word alignment (Brown et al., 1993) and phrase extraction (Koehn et al., 2003) can capture bilingual word- and phrase-pairs with a good deal of accuracy. We omit further details of these standard methods which are freely available elsewhere in the SMT literature (e.g. (Koehn, 2010)). 2.2 Extraction from Parallel Corpora (Okita et al., 2010) addressed the problem of capturing bilingual term-pairs from parallel data which might otherwise not be detected by the translation model. Hence, the requirement in Okita et al. is not to use SMT/GIZA++ (Och and Ney, 2003) to extract term-pairs, which are the common focus in this medical query translation task. The classical algorithm of (Kupiec, 1993) used in (Okita et al., 2010) counts the statistics of terminology c(etermi, ftermj|st) on the source and the target sides which jointly occur in a sentence st after detecting candidate terms via POS tagging, which are then summed up over the en</context>
<context position="8148" citStr="Okita et al., 2010" startWordPosition="1303" endWordPosition="1306">s decoder (Koehn et al., 2007), which we did in this evaluation campaign. Another way is to use asupervised aligner, such as the Berkeley aligner (Haghighi et al., 2009), to align the targeted pairs and check whether they are actually aligned or not. We assume two predefined sets of terms at the outset, Eterm = {eterm1, . . . , etermn} and Fterm = {fterm1, . . . , ftermn}. We search for possible alignment links between the term-pair only when they co-occur in the same sentence. One obvious advantage of this approach is the computational complexity which is fairly low. Note that the result of (Okita et al., 2010) shows that the frequency-based approach of (Kupiec, 1993) worked well for NTCIR patent terminology (Fujii et al., 2010), which otherwise would have been difficult to capture via the traditional SMT/GIZA++ method. In contrast, however, this did not work well on the Europarl corpus (Koehn, 2005). 240 2.3 Terminology Dictionaries Terminology dictionaries themselves are obviously among the most important resources for bilingual term-pairs. In this medical query translation subtask, two corpora are provided for this purpose: (i) Unified Medical Language System corpus (UMLS corpus),1 and (ii) Wiki </context>
</contexts>
<marker>Okita, Guerra, Graham, Way, 2010</marker>
<rawString>Tsuyoshi Okita, Alfredo Maldonado Guerra, Yvette Graham, and Andy Way. 2010. Multi-word expression-sensitive word alignment. In ProceedLing ual Information Access (CLIA2010, collocated with COLING2010), pages 26–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Palatucci</author>
<author>Dean Pomerleau</author>
<author>Geoffrey Hinton</author>
<author>Tom Mitchell</author>
</authors>
<title>Zero-shot learning with semantic output codes.</title>
<date>2009</date>
<booktitle>In Neural Information Processing Systems (NIPS),</booktitle>
<pages>1410--1418</pages>
<contexts>
<context position="12070" citStr="Palatucci et al., 2009" startWordPosition="1879" endWordPosition="1882"> the real vectors obtained in the above steps, obtain the linear mapping between the dictionary in two continuous spaces using canonical component analysis (CCA). Another interesting terminology extraction method requires neither parallel nor comparable corpora, but rather just monolingual corpora on both sides (possibly unrelated to each other) together with a small amount of dictionary entries which provide already known correspondences between words on the source and target sides (henceforth, we refer to this as the ‘dictionary’). This method uses the recently developed zero-shot learning (Palatucci et al., 2009) using neural network language modelling (Bengio et al., 2000; Mikolov et al., 2013b). Then, we train both sides 241 with the neural network language model, and use a continuous space representation to project words to each other on the basis of a small amount of correspondences in the dictionary. If we assume that each continuous space is linear (Mikolov et al., 2013c), we can connect them via linear projection (Mikolov et al., 2013b). Algorithm 2 shows this situation. In our experiments we use U the same as the entries of Wiki and X as 50. Algorithm 3 shows the algorithm to extract the count</context>
</contexts>
<marker>Palatucci, Pomerleau, Hinton, Mitchell, 2009</marker>
<rawString>Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and Tom Mitchell. 2009. Zero-shot learning with semantic output codes. In Neural Information Processing Systems (NIPS), pages 1410–1418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: A Method For Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02).</booktitle>
<contexts>
<context position="15560" citStr="Papineni et al., 2002" startWordPosition="2468" endWordPosition="2471">ntence in a different manner. 4 Experimental Results The baseline is obtained in the following way. The GIZA++ implementation (Och and Ney, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diagfinal heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT (Och, 2003) which optimizes parameter settings using the BLEU metric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we used scikit learn (Pedregosa et al., 2011), word2vec (Mikolov et al., 2013a), and</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: A Method For Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL-02).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="16121" citStr="Pedregosa et al., 2011" startWordPosition="2560" endWordPosition="2563">rameter settings using the BLEU metric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we used scikit learn (Pedregosa et al., 2011), word2vec (Mikolov et al., 2013a), and a recurrent neural network (Mikolov, 2012). Other tools used are in-house software. Table 2 shows the results for the FR–EN query task. We obtained 36.2 BLEU points absolute, which is an improvement of 6.3 BLEU point absolute (21.1% relative) over the baseline. Table 3 shows the results for the EN–FR query task. We obtained 28.8 BLEU points absolute, which is an improvement of 8.7 BLEU points abso242 lute (43% relative) over the baseline. Our system was the best system for both of these tasks. These improvements over the baseline were statistically signi</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dan Klein</author>
</authors>
<title>Learning and inference for hierarchically split PCFGs.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI (Nectar Track),</booktitle>
<pages>1663--1666</pages>
<contexts>
<context position="16029" citStr="Petrov and Klein, 2007" startWordPosition="2544" endWordPosition="2547">alignment from bidirectional alignments. We then perform MERT (Och, 2003) which optimizes parameter settings using the BLEU metric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we used scikit learn (Pedregosa et al., 2011), word2vec (Mikolov et al., 2013a), and a recurrent neural network (Mikolov, 2012). Other tools used are in-house software. Table 2 shows the results for the FR–EN query task. We obtained 36.2 BLEU points absolute, which is an improvement of 6.3 BLEU point absolute (21.1% relative) over the baseline. Table 3 shows the results for the EN–FR query task. We obtained 28.8 BLEU points absolute, which is an improvement of 8.7 BLEU points abso242 lute (43% relative) over the baseline. Our system was the best s</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>Slav Petrov and Dan Klein. 2007. Learning and inference for hierarchically split PCFGs. In Proceedings of AAAI (Nectar Track), pages 1663–1666.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM – An extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="15688" citStr="Stolcke, 2002" startWordPosition="2490" endWordPosition="2491">ey, 2003) of IBM Model 4 is used as the baseline for word alignment: Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3 iterations of Model 4. For phrase extraction the grow-diagfinal heuristics described in (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT (Och, 2003) which optimizes parameter settings using the BLEU metric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we used scikit learn (Pedregosa et al., 2011), word2vec (Mikolov et al., 2013a), and a recurrent neural network (Mikolov, 2012). Other tools used are in-house software. Table 2 shows the results for the FR–EN que</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM – An extensible language modeling toolkit. In Proceedings of the International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Bidirectional inference with the easiest-first strategy for tagging sequence data.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology / Empirical Methods on Natural Language Processing (HLT/EMNLP</booktitle>
<pages>467--474</pages>
<contexts>
<context position="15980" citStr="Tsuruoka and Tsujii, 2005" startWordPosition="2536" endWordPosition="2539"> (Koehn et al., 2003) is used to derive the refined alignment from bidirectional alignments. We then perform MERT (Och, 2003) which optimizes parameter settings using the BLEU metric (Papineni et al., 2002), while a 5- gram language model is derived with Kneser-Ney smoothing (Kneser and Ney, 1995) trained using SRILM (Stolcke, 2002). We use the whole training corpora including the WMT14 translation task corpora as well as medical domain data. UMLS and Wikipedia are used just as training corpora for the baseline. For the extraction from parallel corpora (cf. Section 2.2), we used Genia tagger (Tsuruoka and Tsujii, 2005) and the Berkeley parser (Petrov and Klein, 2007). For the zero-shot learning (cf. Section 2.6) we used scikit learn (Pedregosa et al., 2011), word2vec (Mikolov et al., 2013a), and a recurrent neural network (Mikolov, 2012). Other tools used are in-house software. Table 2 shows the results for the FR–EN query task. We obtained 36.2 BLEU points absolute, which is an improvement of 6.3 BLEU point absolute (21.1% relative) over the baseline. Table 3 shows the results for the EN–FR query task. We obtained 28.8 BLEU points absolute, which is an improvement of 8.7 BLEU points abso242 lute (43% relat</context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Yoshimasa Tsuruoka and Jun’ichi Tsujii. 2005. Bidirectional inference with the easiest-first strategy for tagging sequence data. In Proceedings of the Conference on Human Language Technology / Empirical Methods on Natural Language Processing (HLT/EMNLP 2005), pages 467–474.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
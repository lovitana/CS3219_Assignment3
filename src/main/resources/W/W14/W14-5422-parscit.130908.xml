<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.083395">
<title confidence="0.986064">
Towards automatic annotation of communicative gesturing
</title>
<author confidence="0.997954">
Kristiina Jokinen Graham Wilcock
</author>
<affiliation confidence="0.9038475">
University of Tartu University of Helsinki
Estonia Finland
</affiliation>
<email confidence="0.985605">
kristiina.jokinen@ut.ee graham.wilcock@helsinki.fi
</email>
<sectionHeader confidence="0.993419" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.991843666666667">
We report on-going work on automatic annotation of head and hand gestures in videos of conversational inter-
action. The Anvil annotation tool was extended by two plugins for automatic face and hand tracking. The results
of automatic annotation are compared with the human annotations on the same data.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954692307692">
Hand and head movements are important in human communication as they not only accompany
speech to emphasize the message, but also coordinate and control the interaction. However, video
analysis of human behaviour is a slow and resource-consuming procedure even by trained annotators
using tools such as Anvil (Kipp 2001). There is an urgent need for more advanced tools to speed up
the process by performing higher-level annotation functions automatically.
We use two Anvil plugins, a face tracker (Jongejan 2012) and a hand tracker (Saatmann 2014), that
automatically create annotations for head and hand movements. Objects are recognized based on visu-
al features such as colour and texture, and Haar-liked digital image features, using OpenCV frame-
work. Motion trajectories are estimated by calculating the mean velocity and acceleration during the
time span of a set of frames (we experimented with 7 frames as more than 10 makes the algorithm in-
sensitive for quick, short movements). Movement annotations with respect to velocity and acceleration
are marked on the appropriate Anvil track, to indicate the movement and its start and stop. The inter-
face has controls for minimum saturation threshold and for how many frames to skip (Figure 1).
</bodyText>
<figureCaption confidence="0.896401">
Figure 1 Anvil interface of the new hand tracker plugin.
</figureCaption>
<bodyText confidence="0.914223454545455">
2 Comparison of human and automatic annotations
Compared with human annotation the trackers are good at detecting some movements but prone to
mis-detecting other movements. Problems occurred e.g. when the hue of the hands was similar to the
background colour, or if the direction of the movement is reversed quickly, so that the time span is not
long enough to detect velocity up to the thresholds (short head movements). Acceleration annotation
did not recognize movements if they start and stop slowly. Changing the detection threshold can im-
prove results, but is a trade-off as it prevents small movements being detected. However, the plugins
will be of great help in multimodal analysis. Using the plugins reduces the time spent on annotating
these movements, which in turn results annotations in increased productivity.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.931908">
124
</page>
<note confidence="0.9356495">
Proceedings of the 25th International Conference on Computational Linguistics, pages 124–125,
Dublin, Ireland, August 23-29 2014.
</note>
<bodyText confidence="0.99956">
Here we present a more detailed analysis of the human and automatic annotations with reference to
face tracking. The annotated hand and head movements are listed in Table 1. From the collected data
we used four sample videos, each about six minutes long, altogether 45 303 frames. Table 2 shows the
number of elements automatically recognized using velocity and acceleration, with precision scores,
i.e. manually annotated gestures correctly recognized by the automatic annotation.
</bodyText>
<table confidence="0.904002777777778">
Head movements
Nod down Backward Waggle
Nod up Forward Shake
Turn sideways Tilt Other
Hand movements
Both
Single
Complex
Other
</table>
<tableCaption confidence="0.999468">
Table 1. Annotation features for head and hand movements.
</tableCaption>
<table confidence="0.999898">
Gesture Manual annotation Velocity Acceleration
NodDown 149 110 (74%) 108 (72%)
NodUp 42 15 (36%) 27 (64%)
TurnSide 40 29 (73%) 27 (68%)
HeadBackward 27 18 (67%) 14 (52%)
HeadForward 21 17 (81%) 18 (86%)
Tilt 57 35 (61%) 29 (51%)
Waggle 12 11 (92%) 8 (67%)
HeadOther 3 2 (67%) 1 (33%)
Total 351 237 (73%) 232 (66%)
</table>
<tableCaption confidence="0.9327255">
Table 2. Manual and automatic head movement annotations for 4 videos.
Precision: Velocity 73%, Acceleration 66%
</tableCaption>
<bodyText confidence="0.678920666666667">
Figure 2 shows two examples of the annotation results on the Anvil annotation board, one where the
face tracker recognized head movements appropriately, and one where the face tracker “invented”
movements which the human annotator does not recognize as communicative gestures.
</bodyText>
<figureCaption confidence="0.994314">
Figure 2. Face tracker detecting manual annotation categories (left) and inventing face movements (right).
</figureCaption>
<sectionHeader confidence="0.999218" genericHeader="discussions">
3 Future work
</sectionHeader>
<bodyText confidence="0.999981833333333">
Following the work outlined in Jokinen and Scherer (2012), we will compare the top-down linguistic-
pragmatic analysis of movements with the bottom-up signal-level observations. We will also use a
machine-learning approach to analyse if there are any systematics with the problematic cases. We may
also explore if a recognized movement can be automatically interpreted with respect to communicative
intentions. In human-robot interaction, the automatic gesture recognition model can be used to study
the robot’s understanding of the situation and of human control gestures, cf. Han et al. (2012).
</bodyText>
<sectionHeader confidence="0.998617" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988421454545454">
Han, J., Campbell, N., Jokinen, K. and Wilcock, G. (2012). Investigating the use of non-verbal cues in human-
robot interaction with a Nao robot, in Proceedings of 3rd IEEE International Conference on Cognitive Info-
communications (CogInfoCom 2012), Kosice, 679-683.
Jokinen, K. and Scherer S. (2012). Embodied Communicative Activity in Cooperative Conversational Interac-
tions - studies in Visual Interaction Management. Acta Polytechnica Hungarica. 9(1), pp. 19-40.
Jongejan, B. (2012) Automatic annotation of face velocity and acceleration in Anvil. Proceedings of the Lan-
guage Resources and Evaluation Conference (LREC-2012). Istanbul, Turkey.
Kipp, M. (2001). Anvil – A generic annotation tool for multimodal dialogue. Proceedings of the Seventh Euro-
pean Conference on Speech Communication and Technology, pp. 1367-1370.
Saatmann, P. (2014). Experiments With Hand-tracking Algorithm in Video Conversations. Proceedings of the
5th Nordic Symposium on Multimodal Communication.
</reference>
<page confidence="0.99806">
125
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.780823">
<title confidence="0.999651">Towards automatic annotation of communicative gesturing</title>
<author confidence="0.999988">Kristiina Jokinen Graham Wilcock</author>
<affiliation confidence="0.93802">University of Tartu University of Helsinki Estonia Finland</affiliation>
<email confidence="0.910672">kristiina.jokinen@ut.eegraham.wilcock@helsinki.fi</email>
<abstract confidence="0.995776">We report on-going work on automatic annotation of head and hand gestures in videos of conversational interaction. The Anvil annotation tool was extended by two plugins for automatic face and hand tracking. The results of automatic annotation are compared with the human annotations on the same data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Han</author>
<author>N Campbell</author>
<author>K Jokinen</author>
<author>G Wilcock</author>
</authors>
<title>Investigating the use of non-verbal cues in humanrobot interaction with a Nao robot,</title>
<date>2012</date>
<booktitle>in Proceedings of 3rd IEEE International Conference on Cognitive Infocommunications (CogInfoCom 2012), Kosice,</booktitle>
<pages>679--683</pages>
<marker>Han, Campbell, Jokinen, Wilcock, 2012</marker>
<rawString>Han, J., Campbell, N., Jokinen, K. and Wilcock, G. (2012). Investigating the use of non-verbal cues in humanrobot interaction with a Nao robot, in Proceedings of 3rd IEEE International Conference on Cognitive Infocommunications (CogInfoCom 2012), Kosice, 679-683.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Jokinen</author>
<author>S Scherer</author>
</authors>
<title>Embodied Communicative Activity</title>
<date>2012</date>
<booktitle>in Cooperative Conversational Interactions - studies in Visual Interaction Management. Acta Polytechnica Hungarica.</booktitle>
<volume>9</volume>
<issue>1</issue>
<pages>pp.</pages>
<marker>Jokinen, Scherer, 2012</marker>
<rawString>Jokinen, K. and Scherer S. (2012). Embodied Communicative Activity in Cooperative Conversational Interactions - studies in Visual Interaction Management. Acta Polytechnica Hungarica. 9(1), pp. 19-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Jongejan</author>
</authors>
<title>Automatic annotation of face velocity and acceleration in Anvil.</title>
<date>2012</date>
<booktitle>Proceedings of the Language Resources and Evaluation Conference (LREC-2012).</booktitle>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="1037" citStr="Jongejan 2012" startWordPosition="148" endWordPosition="149">lts of automatic annotation are compared with the human annotations on the same data. 1 Introduction Hand and head movements are important in human communication as they not only accompany speech to emphasize the message, but also coordinate and control the interaction. However, video analysis of human behaviour is a slow and resource-consuming procedure even by trained annotators using tools such as Anvil (Kipp 2001). There is an urgent need for more advanced tools to speed up the process by performing higher-level annotation functions automatically. We use two Anvil plugins, a face tracker (Jongejan 2012) and a hand tracker (Saatmann 2014), that automatically create annotations for head and hand movements. Objects are recognized based on visual features such as colour and texture, and Haar-liked digital image features, using OpenCV framework. Motion trajectories are estimated by calculating the mean velocity and acceleration during the time span of a set of frames (we experimented with 7 frames as more than 10 makes the algorithm insensitive for quick, short movements). Movement annotations with respect to velocity and acceleration are marked on the appropriate Anvil track, to indicate the mov</context>
</contexts>
<marker>Jongejan, 2012</marker>
<rawString>Jongejan, B. (2012) Automatic annotation of face velocity and acceleration in Anvil. Proceedings of the Language Resources and Evaluation Conference (LREC-2012). Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kipp</author>
</authors>
<title>Anvil – A generic annotation tool for multimodal dialogue.</title>
<date>2001</date>
<booktitle>Proceedings of the Seventh European Conference on Speech Communication and Technology,</booktitle>
<pages>1367--1370</pages>
<contexts>
<context position="844" citStr="Kipp 2001" startWordPosition="118" endWordPosition="119"> automatic annotation of head and hand gestures in videos of conversational interaction. The Anvil annotation tool was extended by two plugins for automatic face and hand tracking. The results of automatic annotation are compared with the human annotations on the same data. 1 Introduction Hand and head movements are important in human communication as they not only accompany speech to emphasize the message, but also coordinate and control the interaction. However, video analysis of human behaviour is a slow and resource-consuming procedure even by trained annotators using tools such as Anvil (Kipp 2001). There is an urgent need for more advanced tools to speed up the process by performing higher-level annotation functions automatically. We use two Anvil plugins, a face tracker (Jongejan 2012) and a hand tracker (Saatmann 2014), that automatically create annotations for head and hand movements. Objects are recognized based on visual features such as colour and texture, and Haar-liked digital image features, using OpenCV framework. Motion trajectories are estimated by calculating the mean velocity and acceleration during the time span of a set of frames (we experimented with 7 frames as more t</context>
</contexts>
<marker>Kipp, 2001</marker>
<rawString>Kipp, M. (2001). Anvil – A generic annotation tool for multimodal dialogue. Proceedings of the Seventh European Conference on Speech Communication and Technology, pp. 1367-1370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Saatmann</author>
</authors>
<title>Experiments With Hand-tracking Algorithm in Video Conversations.</title>
<date>2014</date>
<booktitle>Proceedings of the 5th Nordic Symposium on Multimodal Communication.</booktitle>
<contexts>
<context position="1072" citStr="Saatmann 2014" startWordPosition="154" endWordPosition="155">pared with the human annotations on the same data. 1 Introduction Hand and head movements are important in human communication as they not only accompany speech to emphasize the message, but also coordinate and control the interaction. However, video analysis of human behaviour is a slow and resource-consuming procedure even by trained annotators using tools such as Anvil (Kipp 2001). There is an urgent need for more advanced tools to speed up the process by performing higher-level annotation functions automatically. We use two Anvil plugins, a face tracker (Jongejan 2012) and a hand tracker (Saatmann 2014), that automatically create annotations for head and hand movements. Objects are recognized based on visual features such as colour and texture, and Haar-liked digital image features, using OpenCV framework. Motion trajectories are estimated by calculating the mean velocity and acceleration during the time span of a set of frames (we experimented with 7 frames as more than 10 makes the algorithm insensitive for quick, short movements). Movement annotations with respect to velocity and acceleration are marked on the appropriate Anvil track, to indicate the movement and its start and stop. The i</context>
</contexts>
<marker>Saatmann, 2014</marker>
<rawString>Saatmann, P. (2014). Experiments With Hand-tracking Algorithm in Video Conversations. Proceedings of the 5th Nordic Symposium on Multimodal Communication.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
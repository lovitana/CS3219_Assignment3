<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000275">
<title confidence="0.985664">
Semi-supervised Graph-based Genre Classification for Web Pages
</title>
<author confidence="0.975">
Noushin Rezapour Asheghi
</author>
<affiliation confidence="0.9965925">
School of Computing
University of Leeds
</affiliation>
<email confidence="0.946545">
scs5nra@leeds.ac.uk
</email>
<author confidence="0.953603">
Katja Markert
</author>
<affiliation confidence="0.97104425">
L3S Research Center
Leibniz Universit¨at Hannover
and School of Computing
University of Leeds
</affiliation>
<email confidence="0.970049">
markert@l3s.de
</email>
<author confidence="0.991006">
Serge Sharoff
</author>
<affiliation confidence="0.986402666666667">
School of Modern
Languages and Cultures
University of Leeds
</affiliation>
<email confidence="0.983359">
s.sharoff@leeds.ac.uk
</email>
<sectionHeader confidence="0.997243" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958">
Until now, it is still unclear which set of
features produces the best result in au-
tomatic genre classification on the web.
Therefore, in the first set of experiments,
we compared a wide range of content-
based features which are extracted from
the data appearing within the web pages.
The results show that lexical features such
as word unigrams and character n-grams
have more discriminative power in genre
classification compared to features such
as part-of-speech n-grams and text statis-
tics. In a second set of experiments,
with the aim of learning from the neigh-
bouring web pages, we investigated the
performance of a semi-supervised graph-
based model, which is a novel technique
in genre classification. The results show
that our semi-supervised min-cut algo-
rithm improves the overall genre classifi-
cation accuracy. However, it seems that
some genre classes benefit more from this
graph-based model than others.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890634146341">
In Automatic Genre Identification (AGI), docu-
ments are classified based on their genres rather
than their topics or subjects. Genre classes such
as editorial, interview, news and blog which are
recognizable by their distinct purposes, can be on
any topic. The most important application of AGI
could be in Information Retrieval. If a user could
use the search engine to retrieve web pages from
a specific genre such as news articles, reviews
or blogs, search results could be more beneficial.
With the aim of enhancing search engines, AGI
has attracted a lot of attention (see Section 2).
In this paper, we investigate two important open
questions in AGI. The first question is: what set
of features produces the best result in genre clas-
sification on the web? The drawbacks of exist-
ing genre-annotated web corpora (low inter-coder
agreement; false correlations between topic and
genre classes) resulted in researchers’ doubt on the
outcomes of classification models based on these
corpora (Sharoff et al., 2010). Therefore, in order
to answer this question, we perform genre classi-
fication with a wide range of features on a reli-
able and source diverse genre-annotated web cor-
pus. The second question that we investigate in
this paper is: could we exploit the graph structure
of the web to increase genre classification accu-
racy? With the aim of learning from the neigh-
bouring web pages, we investigated the perfor-
mance of a semi-supervised graph-based model,
which is a novel technique in genre classification.
The remainder of this paper is structured as fol-
lows. After reviewing related work in Section 2,
we compare different supervised genre classifica-
tion models based on various lexical, POS-based
and text statistics features in Section 3. Section 4
describes our semi-supervised graph-based classi-
fication experiment, where we use the multi-class
min-cut algorithm as a novel technique in genre
classification. Section 5 concludes the findings
and discusses future work.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999065">
There has been a considerable body of research
in AGI. In previous studies on automatic genre
classification of web pages, various types of fea-
tures such as common words (Stamatatos et al.,
2000), function words (Argamon et al., 1998),
word unigrams (Freund et al., 2006), character
n-grams (Kanaris and Stamatatos, 2007), part-of-
speech tags (Karlgren and Cutting, 1994) , part-
of-speech trigrams (Argamon et al., 1998; San-
tini, 2007), document statistics (e.g. average sen-
tence length, average word length and type/token
ratio) (Finn and Kushmerick, 2006; Kessler et
</bodyText>
<page confidence="0.991684">
39
</page>
<note confidence="0.4955015">
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47,
October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998557138297873">
al., 1997), HTML tags (e.g. (Santini, 2007))
have been explored. However, researchers con-
ducted genre classification experiments with dif-
ferent features on different corpora with differ-
ent sets of genre labels. As a result, it is dif-
ficult to compare them. This motivated Sharoff
et al. (2010) to examine a wide range of word-
based, character-based and POS-based features on
the existing genre-annotated corpora. They re-
ported that word unigrams and character 4-grams
outperform other features in genre classification.
However, they concluded that the results cannot
be trusted because of two main reasons. First,
some of these collections exhibit low inter-coder
agreement and any results based on unreliable data
could be misleading. Second, the spurious cor-
relation between topic and genre classes in some
of these corpora was one of the reasons for some
of the very impressive results reported by Sharoff
et al. (2010). These good results were achieved
by detecting topics rather than genres of individ-
ual texts. A similar point was made by Petrenz
and Webber (2010) who examined the impact of
topic change on the performance of AGI systems.
They showed that a shift in topic can have a mas-
sive impact on genre classification models which
are based on lexical features such as word uni-
grams or character n-grams. Therefore, the ques-
tion which set of features produces the best result
in automatic genre classification on the web is still
an open question. In order to investigate this ques-
tion, we perform genre classification with a wide
range of features on a reliable and topically diverse
dataset. Section 3.1 describes the dataset and the
experimental setup.
Most of the current works in the field of AGI
concentrated on extracting features from the con-
tent of the documents and classify them by em-
ploying a standard supervised algorithm. How-
ever, on the web there are other sources of
information which can be utilized to improve
genre classification of web pages. For instance,
the web has a graph structure and web pages
are connected via hyper-links. These connec-
tions could be exploited to improve genre clas-
sification. Various graph-based classification al-
gorithms have been proposed to improve topic
classification for web pages, such as the re-
laxation labelling algorithm (Chakrabarti et al.,
1998), iterative classification algorithm (Lu and
Getoor, 2003), Markov logic networks (Crane
and McDowell, 2012), random graph walk (Lin
and Cohen, 2010) and weighted-vote relational
neighbour algorithm (Macskassy and Provost,
2007). These classification algorithms which uti-
lize hyper-link connections between web pages
to construct graphs, outperformed the classifiers
which are solely based on textual content of the
web pages for topic classification. Such connected
data presents opportunities for boosting the perfor-
mance of genre classification too.
Graph-based web page classification presented
in studies such as (Crane and McDowell, 2012;
Lu and Getoor, 2003; Macskassy and Provost,
2007) on the WebKB dataset (CRAVEN, 1998)
could be considered as genre classification as op-
posed to topic classification. The WebKB dataset
contains web pages from four computer science
departments categorised into seven classes: stu-
dent, faculty, staff, department, course, project
and other. However, this dataset is very specific
to the academic domain with low coverage for
the web overall, whereas we examine graph-based
learning for automatic genre classification of web
pages on a much more general dataset with pop-
ular genre classes such as news, blog and edito-
rial. Moreover, the graph-based algorithms used
on the WebKB dataset are all supervised and were
performed on a very clean and noise free dataset
which was achieved by removing the class other.
Class other contains all the web pages which do
not belong to any other predefined classes. How-
ever, our experiment is in a semi-supervised man-
ner which is a much more realistic scenario on the
web, because it is highly unlikely that for each
web page, we have genre labels for all its neigh-
bouring web pages as well. Therefore, we per-
form our experiment on a very noisy dataset where
neighbouring web pages could belong to none of
our predefined genre classes. Section 4 describes
our semi-supervised graph-based classification ex-
periment, where we use a multi-class min-cut al-
gorithm as a novel technique in genre classifica-
tion.
</bodyText>
<sectionHeader confidence="0.998341" genericHeader="method">
3 Content-based Classification
</sectionHeader>
<subsectionHeader confidence="0.991491">
3.1 Dataset and Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9995956">
Petrenz and Webber (2010) and Sharoff et
al. (2010) emphasize that the impact of topic on
genre classification should be eliminated or con-
trolled. In order to avoid the influence of topic on
genre classification, some researchers (e.g. (Sta-
</bodyText>
<page confidence="0.998045">
40
</page>
<table confidence="0.993993222222222">
Genre Number of # of pages from Fleiss’s κ
web pages websites the same website
max min med
Personal Homepage (php) 304 288 9 1 1 0.858
Company/ Business Homepage (com) 264 264 1 1 1 0.713
Educational Organization Homepage (edu) 299 299 1 1 1 0.953
Personal Blog /Diary (blog) 244 215 9 1 1 0.812
Online Shop (shop) 292 209 23 1 1 0.830
Instruction/ How to (instruction) 231 142 15 1 1 0.871
Recipe 332 116 8 1 1 0.971
News 330 127 12 1 1 0.801
Editorial 310 69 11 1 3 0.877
Conversational Forum (forum) 280 106 11 1 1 0.951
Biography (bio) 242 190 15 1 1 0.905
Frequently Asked Questions (faq) 201 140 8 1 1 0.915
Review 266 179 15 1 1 0.880
Story 184 24 38 1 7 0.953
Interview 185 154 11 1 1 0.905
</table>
<tableCaption confidence="0.735495">
Table 1: Statistics for each category illustrate source diversity and reliability of the corpus (Asheghi et
al., 2014). To save space, in this paper we use the abbreviation of genre labels which are specified after
the genre names.
</tableCaption>
<bodyText confidence="0.999152897058824">
matatos et al., 2000) and (Argamon et al., 1998))
use only topic independent features such as com-
mon words or function words in genre classifica-
tion. However, neither of these features are exclu-
sive to genre classification. Function words and
common words are used in authorship classifica-
tion (e.g. (Argamon et al., 2007)) because they can
capture the style of the authors without being in-
fluenced by the topics of the texts. On the other
hand, word unigrams are a popular document rep-
resentation in topic classification. If we want these
models to capture the genre of documents without
being influenced by their topics or the style of their
authors, we must eliminate the influence of these
factors on genre classification by keeping them
constant across the genre classes in the training
data. That means all the documents in the train-
ing set should be about the same topic and written
by the same person. However, constructing such a
dataset is practically impossible for genre classes
on the web. The other more practical solution to
this problem would be to collect data from various
topics and sources in order to minimize the im-
pact of these factors on genre classification. For
that reason, we (Asheghi et al., 2014) created a
web genre annotated corpus which is reliable (with
Fleiss’s kappa (Fleiss, 1971) equal to 0.874) and
source diverse. We tried to reduce the influence
of topic, the writing style of the authors as well as
the design of the websites on genre classification
by collecting data from various sources and top-
ics. The corpus consists of 3964 web pages from
2522 different websites, distributed across 15 gen-
res (Table 1).
Moreover, we prepared two versions of the
corpus: the original text and the main text cor-
pora. First, we converted web pages to plain
text by removing HTML markup using the Krd-
Wrd tool (Steger and Stemle, 2009). This re-
sulted in the original text corpus which contains
individual web pages with all the textual elements
present on them. Moreover, in order to investigate
the influence of boilerplate parts (e.g. advertise-
ments, headers, footers, template materials, navi-
gation menus and lists of links) of the web pages
on genre classification, we removed the boilerplate
parts and extracted the main text of each web page
using the justext tool 1. This resulted in the cre-
ation of the main text corpus. This is the first time
that the performance of genre classification mod-
els is compared on both the original and the main
text of the web pages.
Since the outputs of the justext tool for 518 of
the web pages were empty files, the main text cor-
pus has fewer pages. However, the main text cor-
pus still has a balanced distribution with a rela-
tively large number of web pages per category. Ta-
ble 2 compares the number of web pages in the two
versions of the corpus. For all the experiments we
use this corpus via 10-fold cross-validation on the
web pages. Also, in order to minimize the effect
of factors such as topic, the writing style of the au-
thors and the design of the websites even further,
we ensured that all the web pages from the same
website are in the same fold. Many, if not all of the
previous studies in automatic genre classification
on the web ignored this essential step when divid-
ing the data into folds. For machine learning, we
</bodyText>
<footnote confidence="0.990173">
1http://code.google.com/p/justext/
</footnote>
<page confidence="0.997123">
41
</page>
<table confidence="0.992656823529412">
Genre Number of web pages in corpora
Original text Main text
php 304 221
com 264 190
edu 299 191
blog 244 242
shop 292 221
instruction 231 229
recipe 332 243
news 330 320
editorial 310 307
forum 280 251
bio 242 242
faq 201 160
review 266 262
story 184 184
interview 185 183
</table>
<tableCaption confidence="0.9522665">
Table 2: Number of web pages in individual genre
classes in both original text and main text corpora.
</tableCaption>
<bodyText confidence="0.999276375">
chose Support Vector Machines (SVM) because
it has been shown by other researchers in AGI
(e.g. (Santini, 2007)) that SVM produces better or
at least similar results compared to other machine
learning algorithms. We used the one-versus-one
multi-class SVM implemented in Weka 2 with the
default setting. All the experiments are carried out
on both the original text and the main text corpora.
</bodyText>
<subsectionHeader confidence="0.992804">
3.2 Features
</subsectionHeader>
<bodyText confidence="0.999979181818182">
In order to compare the performance of differ-
ent lexical and structural features used in previous
work, we reimplemented the following published
approaches to AGI: function words (Argamon et
al., 1998), part-of-speech n-grams (Santini, 2007),
word unigrams (Freund et al., 2006) and charac-
ter 4-grams binary representation (Sharoff et al.,
2010). We also explored the discriminative power
of other features such as readability features (Pitler
and Nenkova, 2008), HTML tags 3 and named en-
tity tags in genre classification (Table 3). This is
the first time that some of these features such as
average depth of syntax trees and entity coherence
features (Barzilay and Lapata, 2008) are used for
genre classification. To set a base-line, we used
a list of genre names (e.g. news, editorial, in-
terview, review) as features. We used two differ-
ent feature representations: binary and normalized
frequency. In the binary representation of a doc-
ument, the value for each feature is either one or
zero which represents the presence or the absence
of each feature respectively. In the normalized fre-
</bodyText>
<footnote confidence="0.9999305">
2http://www.cs.waikato.ac.nz/ml/weka/
3http://www.w3schools.com/tags/ref byfunc.asp
</footnote>
<bodyText confidence="0.999965928571429">
quency representation of a document, the value for
each feature is the frequency of that feature which
is normalized by the length of the document.
For extracting lexical features, we tokenized
each document using the Stanford tokenizer (in-
cluded as part of the Stanford part of speech tag-
ger (Toutanova et al., 2003)) and converted all the
tokens to lower case. For extracting POS tags
and named entity tags, we used the Stanford max-
imum entropy tagger 4 and the Stanford Named
Entity Recognizer 5 respectively. For extracting
some of the readability features such as average
parse tree height and average number of noun and
verb phrases per sentences, we used the Stanford
Parser (Klein and Manning, 2003). However, web
pages must be cleaned before they can be fed to
a parser, because parsers cannot handle tables and
list of links. Therefore, we only used the main
text of each web page as an input to the parser.
For web pages for which the justext tool produced
empty files, we treated these features as missing
values. Moreover, we used the Brown Coherence
Toolkit 6 to construct the entity grid for each web
page and computed the probability of each entity
transition type. This tool needs the parsed version
of the text as an input. Therefore, for web pages
for which the justext tool produced empty files, we
also treated these features as missing values.
</bodyText>
<subsectionHeader confidence="0.939238">
3.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999993789473684">
Table 4 shows the result of the different feature
sets listed in the previous section on both the orig-
inal text and the main text corpora. At first glance,
we see that the results of genre classification on
the original text corpus are higher than the main
text corpus. This shows that boiler plates contain
valuable information which helps genre classifica-
tion.
Moreover, the results show that binary repre-
sentation of word unigrams is the best performing
feature set when we use the whole text of the web
pages. However, on the main text corpus, charac-
ter 4-grams outperform other features. This con-
firms the results reported in (Sharoff et al., 2010).
The results also highlight that the performance of
POS-based features are much less accurate than
that of textual features such as word unigrams and
character n-grams. The results also show that the
combination of word unigrams, text statistics and
</bodyText>
<footnote confidence="0.999984666666667">
4http://nlp.stanford.edu/software/tagger.shtml
5http://nlp.stanford.edu/software/CRF-NER.shtml
6http://www.cs.brown.edu/ melsner/manual.html
</footnote>
<page confidence="0.991112">
42
</page>
<table confidence="0.999330846153846">
Category Features
Token features number of tokens and number of types
normalized frequency of punctuation marks and currency characters
Named entity tags normalized frequency of tags: time, location, organization, person, money, date
Readability features average parse tree height
average sentence length and word length
standard deviation of sentence length and of word length
average number of syllables per word
type/token ratio
average number of noun phrases and verb phrases per sentence
entity coherence features (Barzilay and Lapata, 2008)
HTML tags normalized frequency of tags for: sections / style, formatting, programming,
visual features such as forms, images, lists and tables
</table>
<tableCaption confidence="0.999796">
Table 3: List of text statistics features explored in this paper
</tableCaption>
<bodyText confidence="0.999989857142857">
part of speech features resulted in improving genre
classification accuracy (compared to the accuracy
achieved by word unigrams alone), for both origi-
nal and main text corpora. However, while the im-
provement for the main text corpus is statistically
significant 7, there is no significant difference be-
tween these two models for the original corpus.
Surprisingly, adding part of speech 3-grams to the
word unigrams features decreased the genre clas-
sification accuracy in both original and main text
corpora. The reason could be that the model is
over-fitted on the training data and as a result, it
performs poorly on the test data. Therefore, com-
bining various features will not always improve
the performance of the classification task. More-
over, for extracting POS-based features and some
of the text statistics features we rely on tools such
as part-of-speech taggers and parsers whose per-
formance varies for different genres. Even the best
part-of-speech taggers and parsers are error prone
and cannot be trusted on new unseen genres.
</bodyText>
<sectionHeader confidence="0.999577" genericHeader="method">
4 Graph-based Classification
</sectionHeader>
<bodyText confidence="0.97331647826087">
Until now we extracted features only from the con-
tent of the web pages. However, other sources
of information such as the connections and the
link patterns between the web pages could be ex-
ploited to improve genre classification. The under-
lying assumption of this approach is that a page is
more likely to be connected to pages with the same
genre category. For example, if the neighbouring
web pages of a particular web page are labelled
as shop, it is more likely that this web page is a
shop too, whereas, it is highly unlikely that it is a
news or editorial. This property (i.e. entities with
similar labels are more likely to be connected) is
known as homophily (Sen et al., 2008). We hy-
7McNemar test at the significance level of 5%
pothesis that homophily exists for genre classes
and it can help us to improve genre classifica-
tion on the web. In this paper, we use a semi-
supervised graph-based algorithm namely, multi-
class min-cut, which is a novel approach in genre
classification. This algorithm, which is a collec-
tive classification method, considers the class la-
bels of all the web pages within a graph.
</bodyText>
<subsectionHeader confidence="0.991628">
4.1 Multi-class Min-cut: The Main Idea
</subsectionHeader>
<bodyText confidence="0.999976071428571">
The Min-cut classification algorithm originally
proposed by Blum and Chawla (2001) is based
on the idea that linked entities have a tendency
to belong to the same class. In other words, it
is based on the homophily assumption. There-
fore, it should be able to improve genre classifica-
tion on the web if our hypothesis holds. However,
this technique is a binary classification algorithm,
whereas, we have a multi-class problem. Unfor-
tunately, multi-class min-cut is NP-hard and there
is no exact solution for it. Nevertheless, Ganchev
and Pereira (2007) proposed a multi-class exten-
sion to Blum and Chawla (2001)’s min-cut algo-
rithm by encoding a multi-class min-cut problem
as an instance of metric labelling. Kleinberg and
Tardos (1999; 2002) introduced metric labelling
for the first time. The main idea of metric labelling
for web page classification can be described as fol-
lows:
Assume we have a weighted and undirected
graph G = (V, E) where each vertex v ∈ V is a
web page and the edges represent the hyper-links
between the web pages. The task is to classify
these web pages into a set of labels L. This task can
be denoted as a function f : V → L. In order to
do this labelling task in an optimal way, we need to
minimize two different types of costs. First, there
is a non-negative cost c(v, l) for assigning label l
</bodyText>
<page confidence="0.999628">
43
</page>
<table confidence="0.911944647058823">
Feature set Original text Main text
genre names bin 57.39 29.02
genre name nf 38.29 14.16
function words bin 65.71 55.57
function words nf 74.95 66.86
word unigrams bin 89.32 76.61
word unigrams nf 85.21 74.91
character 4-grams bin 87.96 78.88
POS-3grams bin 73.18 61.23
POS-3grams nf 70.28 57.83
POS-2grams bin 64.10 54.91
POS-2grams nf 68.94 60.76
POS nf 60.14 54.64
text statistics 55.47 59.17
word unigrams bin + text statistics 89.48 78.09
word uni-grams bin + text statistics + POS nf 89.63 78.24
word uni-grams bin + POS 3-grams bin 88.14 75.59
</table>
<tableCaption confidence="0.91797">
Table 4: Classification accuracy of different features in genre classification. bin and nf refer to the use of
binary and normalized frequency representation of the features respectively.
</tableCaption>
<bodyText confidence="0.998964714285714">
to web page v. Second, if two web pages v1 and v2
are connected together with an edge a with weight
we, we need to pay a cost of we · d(f(v1), f(v2))
where d(.,.) denotes distance between the two la-
bels. A big distance value between labels indicates
less similarity between them. Therefore, the total
cost of labelling task f is:
</bodyText>
<equation confidence="0.9971205">
E(f) = 1: c(v, f(v)) +
v∈V (1)
1: we · d(f(v1), f(v2))
e=(v1,v2)∈E
</equation>
<bodyText confidence="0.9999281">
Kleinberg and Tardos (1999; 2002) developed
an algorithm for minimizing E(f). However,
their algorithm uses linear programming which is
impractical for large data (Boykov et al., 2001).
In a separate study for metric labelling problems,
Boykov et al. (2001) have developed a multi-way
min-cut algorithm to minimize E(f). This algo-
rithm is very fast and can be applied to large-scale
problems with good performance (Boykov et al.,
2001).
</bodyText>
<subsectionHeader confidence="0.999754">
4.2 Selection of unlabelled data
</subsectionHeader>
<bodyText confidence="0.999877714285714">
A web page w has different kind of neighbours on
the web such as parents, children, siblings, grand
parents and grand children which are mainly dif-
ferentiated based on the distance to the target web
page as well as the direction of the links (Qi and
Davison, 2009). Since the identification of chil-
dren of a web page (i.e. web pages which have
</bodyText>
<table confidence="0.905218416666667">
Cosine # of unlabelled Average # of
similarity web pages neighbours
&gt; 0 103,372 40.65
&gt; 0.1 98,824 39.08
&gt; 0.2 87,834 34.23
&gt; 0.3 70,602 26.46
&gt; 0.4 50,232 17.52
&gt; 0.5 28,437 8.62
&gt; 0.6 13,919 3.77
&gt; 0.7 7,241 1.86
&gt; 0.8 3,772 0.98
&gt; 0.9 1,732 0.44
</table>
<tableCaption confidence="0.960254">
Table 5: Number of unlabelled web pages with
</tableCaption>
<bodyText confidence="0.995160142857143">
different cosine similarity thresholds. The last col-
umn shows the average number of neighbours per
labelled page.
direct links from the target web page) is a straight-
forward task as their URLs can be extracted from
the HTML version of the target web page, in this
study, we explore the effect of the target web
pages’ children on genre classification. Therefore,
in this experiment, by neighbouring web pages we
mean the web pages’ children. In order to collect
the neighbouring web pages, for every web page in
the data set, we extracted all its out-going URLs
and downloaded them as unlabelled data. How-
ever, using all these neighbouring pages could
hurt the genre classification accuracy because web
pages are noisy (e.g. links to advertisements) and
some neighbours could have different genres than
the target page. In order to control the negative im-
pact of such neighbours, we could preselect a sub-
set of neighbours whose content are close enough
to the target page. To implement this idea, we
</bodyText>
<page confidence="0.998034">
44
</page>
<bodyText confidence="0.999691625">
computed the cosine similarity between the web
page w and its neighbouring web pages and used
different threshold to select the neighbours. If u is
a neighbour of w and −→u and −→w are the represen-
tative feature vectors of these two web pages re-
spectively, we could compute the cosine similarity
between these two web pages using the following
formula:
</bodyText>
<equation confidence="0.9603694">
cos(−→w , −→u ) =
�ni=1 wi × ui
Y �n Y �n
i=1(wi)2 × i=1(ui)2
(2)
</equation>
<bodyText confidence="0.9999802">
where n is the number of the dimensions of the
vectors and wi is the value of the ith dimension
of the vector −→w . Since the word unigrams bi-
nary representation model yields the best result for
content-based genre classification, we used this
representation of web pages to construct their fea-
ture vectors. Table 5 shows the number of unla-
belled data and the average number of neighbours
per labelled web page for different cosine similar-
ity thresholds.
</bodyText>
<subsectionHeader confidence="0.9999285">
4.3 Formulation of Semi-supervised
Multi-class Min-cuts
</subsectionHeader>
<bodyText confidence="0.997348">
The formulation of semi-supervised multi-class
min-cut for genre classification involves the fol-
lowing steps:
</bodyText>
<listItem confidence="0.970034818181818">
1. We built the weighted and undirected graph
G = (V, E) where vertices are the web pages
(labelled and unlabelled) and the edges rep-
resent the hyper-links between the web pages
and set the weights to 1.
2. For training nodes, set the cost of the correct
label to zero and all other labels to a large
constant.
3. For test nodes and unlabelled nodes, we set
the cost of each label using a supervised clas-
sifier (SVM) using the following formula:
</listItem>
<equation confidence="0.971653">
c(w, l) = 1 − pl(w) (3)
</equation>
<bodyText confidence="0.9999208">
where c(w, l) is the cost of label l for web
page w and pl(w) is the probability of w be-
longing to the label l which is computed by a
supervised SVM using word unigrams binary
representation of the web pages.
</bodyText>
<listItem confidence="0.550695333333333">
4. Set d(i, j), which denotes the distance be-
tween two labels i and j, to 1 if i =6 j and
zero otherwise.
5. Employ Boykov et al. (2001) algorithm to
find the minimum total cost using multiway
min-cut algorithm.
</listItem>
<subsectionHeader confidence="0.985368">
4.4 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999992975">
We divided the labelled data into 10 folds again
ensuring that all the web pages from the same
websites are in the same fold. We used 8 folds
for training, one fold for validation and one fold
for testing. We learnt the best cosine similar-
ity threshold using validation data and then eval-
uated it on the test data. Tables 6 and 7 illus-
trate the results of the multi-class min-cut algo-
rithm and the content-based algorithm (both using
word unigrams as features) respectively. The re-
sults show that the multi-class min-cut algorithm
significantly outperforms 8 the content-based clas-
sifier for the cosine similarity equal or greater than
0.8 which was chosen on the validation data. It
must be noted that the result of the multi-class
min-cut algorithm when we used all the neigh-
bouring pages was much lower than the content-
based algorithm due to noise. The results also
shows that some genre classes such as news, edito-
rial, blog, interview and instruction benefited more
than other genre classes from the neighbouring
web pages. Genre categories with improved re-
sults are shown in bold in Table 6. The homophily
property of these genre categories was the reason
behind this improvement. For example, the fact
that a news article is more likely to be linked to
other news articles, whereas, an editorial is more
likely to be linked to other editorials, helped us to
differentiate these two categories further. On the
other hand, we observe no improvement or even
decrease in F-measure for some genre categories
such as frequently asked questions, forums and
company home pages. Two reasons could have
contributed to these results. First, the homophily
property might not exist for these categories. Sec-
ond, the homophily property holds for these cate-
gories, however, in order to benefit from this prop-
erty, we need to examine other neighbours of the
target web pages such as parents, siblings, grand
parents, grand children or even more distant neigh-
</bodyText>
<footnote confidence="0.871527">
8McNemar test at the significance level of 5%
</footnote>
<table confidence="0.7275725">
−→ w · −→ u
k −→w kk−→u k
</table>
<page confidence="0.78705">
45
</page>
<table confidence="0.995235294117647">
class Recall Precision F1-measure
php 0.928 0.850 0.887
forum 0.925 0.977 0.951
review 0.895 0.832 0.862
news 0.897 0.798 0.845
com 0.897 0.891 0.894
shop 0.860 0.965 0.910
instruction 0.870 0.914 0.892
recipe 0.994 0.991 0.993
blog 0.889 0.879 0.884
bio 0.905 0.948 0.926
editorial 0.800 0.932 0.861
faq 0.902 0.841 0.870
edu 0.957 0.963 0.960
story 0.902 0.943 0.922
interview 0.870 0.809 0.839
overall accuracy = 90.11%
</table>
<tableCaption confidence="0.9977165">
Table 6: Recall, Precision and F-measure for multi-
class min-cut genre classification.
</tableCaption>
<bodyText confidence="0.667249">
bours.
</bodyText>
<sectionHeader confidence="0.956195" genericHeader="conclusions">
5 Conclusions and Future work
</sectionHeader>
<table confidence="0.995184">
class Recall Precision F1-measure
php 0.938 0.798 0.862
forum 0.943 0.974 0.958
review 0.872 0.859 0.866
news 0.894 0.782 0.835
com 0.920 0.874 0.897
shop 0.849 0.950 0.897
instruction 0.866 0.889 0.877
recipe 0.988 0.988 0.988
blog 0.865 0.841 0.853
bio 0.884 0.926 0.905
editorial 0.765 0.926 0.837
faq 0.866 0.879 0.872
edu 0.950 0.969 0.959
story 0.864 0.941 0.901
interview 0.827 0.785 0.805
overall accuracy = 88.98% 9
</table>
<tableCaption confidence="0.998192">
Table 7: Recall, Precision and F-measure for content-based
genre classification using word unigrams feature set
</tableCaption>
<bodyText confidence="0.98692504">
First international workshop on innovative informa-
tion systems, pages 85–92. Citeseer.
In the first set of experiments, we compared
a diverse range of content-based features in
genre classification using a reliable and source
diverse genre-annotated corpus. The evaluation
shows that lexical features outperformed all
other features. Source diversity of the corpus
minimized the influence of topic, authorship and
web page design on genre classification. In the
second experiment, we significantly improved the
genre classification result using a semi-supervised
min-cut algorithm by employing the children of
the target web pages as unlabelled data. The
results of this method which takes advantage of
the graph structure of the web shows that some
genre classes benefit more than others from the
neighbouring web pages. The homophily property
of genre categories such as news, blogs and edi-
torial was the reason behind the improvement of
genre classification in this experiment. In future
work, we would like to examine the effect of other
types of neighbours on genre classification of
web pages and experiment with other graph-based
algorithms.
</bodyText>
<sectionHeader confidence="0.977659" genericHeader="references">
References
</sectionHeader>
<footnote confidence="0.823571833333333">
Shlomo Argamon, Moshe Koppel, and Galit Avneri.
1998. Routing documents according to style. In
9Please note that in this experiment we had less training
data because we used 8 folds for training, one fold for valida-
tion and one fold for testing. As a result, the accuracy of word
unigrams is slightly lower than the result reported in Table 4.
</footnote>
<reference confidence="0.995013470588236">
Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-
han Raj Hota, Navendu Garg, and Shlomo Levitan.
2007. Stylistic text classification using functional
lexical features. Journal of the American Society
for Information Science and Technology, 58(6):802–
822.
Noushin Rezapour Asheghi, Serge Sharoff, and Katja
Markert. 2014. Designing and evaluating a reliable
corpus of web genres via crowd-sourcing. In Pro-
ceedings of the Ninth International Conference on
Language Resources and Evaluation (LREC’14).
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1–34.
Avrim Blum and Shuchi Chawla. 2001. Learning from
labeled and unlabeled data using graph mincuts. In
Proceedings of the Eighteenth International Confer-
ence on Machine Learning, pages 19–26. Morgan
Kaufmann Publishers Inc.
Yuri Boykov, Olga Veksler, and Ramin Zabih. 2001.
Fast approximate energy minimization via graph
cuts. Pattern Analysis and Machine Intelligence,
IEEE Transactions on, 23(11):1222–1239.
Soumen Chakrabarti, Byron Dom, and Piotr Indyk.
1998. Enhanced hypertext categorization using hy-
perlinks. In ACM SIGMOD Record, volume 27,
pages 307–318. ACM.
Robert Crane and Luke McDowell. 2012. Investigat-
ing markov logic networks for collective classifica-
tion. In ICAART (1), pages 5–15.
M CRAVEN. 1998. Learning to extract symbolic
knowledge from the world wide web. In Proc. of the
15th National Conference on Artificial Intelligence
(AAAI-98).
</reference>
<page confidence="0.989976">
46
</page>
<reference confidence="0.999555072916667">
Aidan Finn and Nicholas Kushmerick. 2006. Learning
to classify documents according to genre. Journal
of the American Society for Information Science and
Technology, 57(11):1506–1518.
J.L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378.
L. Freund, C.L.A. Clarke, and E.G. Toms. 2006. To-
wards genre classification for ir in the workplace.
In Proceedings of the 1st international conference
on Information interaction in context, pages 30–36.
ACM.
Kuzman Ganchev and Fernando Pereira. 2007. Trans-
ductive structured classification through constrained
min-cuts. TextGraphs-2: Graph-Based Algorithms
for Natural Language Processing, page 37.
Ioannis Kanaris and Efstathios Stamatatos. 2007.
Webpage genre identification using variable-length
character n-grams. In Tools with Artificial Intelli-
gence, 2007. ICTAI 2007. 19th IEEE International
Conference on, volume 2, pages 3–10. IEEE.
J. Karlgren and D. Cutting. 1994. Recognizing text
genres with simple metrics using discriminant anal-
ysis. In Proceedings of the 15th conference on Com-
putational linguistics-Volume 2, pages 1071–1075.
B. Kessler, G. Numberg, and H. Schutze. 1997. Au-
tomatic detection of text genre. In Proceedings of
the 35th Annual Meeting of the Association for Com-
putational Linguistics and Eighth Conference of the
European Chapter of the Association for Computa-
tional Linguistics, pages 32–38.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430. Asso-
ciation for Computational Linguistics.
Jon Kleinberg and Eva Tardos. 1999. Approximation
algorithms for classification problems with pairwise
relationships: Metric labeling and markov random
fields. In focs, page 14. Published by the IEEE Com-
puter Society.
Jon Kleinberg and Eva Tardos. 2002. Approximation
algorithms for classification problems with pairwise
relationships: Metric labeling and markov random
fields. Journal of the ACM (JACM), 49(5):616–639.
Frank Lin and William W Cohen. 2010. Semi-
supervised classification of network data using very
few labels. In Advances in Social Networks Analysis
and Mining (ASONAM), 2010 International Confer-
ence on, pages 192–199. IEEE.
Q. Lu and L. Getoor. 2003. Link-based classification
using labeled and unlabeled data. The Continuum
from Labeled to Unlabeled Data in Machine Learn-
ing &amp; Data Mining, page 88.
Sofus A Macskassy and Foster Provost. 2007. Classifi-
cation in networked data: A toolkit and a univariate
case study. The Journal of Machine Learning Re-
search, 8:935–983.
P. Petrenz and B. Webber. 2010. Stable classification
of text genres. Computational Linguistics, (Early
Access):1–9.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
pages 186–195.
Xiaoguang Qi and Brian D Davison. 2009. Web page
classification: Features and algorithms. ACM Com-
puting Surveys (CSUR), 41(2):12.
Marina Santini. 2007. Automatic identification of
genre in web pages. Ph.D. thesis, University of
Brighton.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise
Getoor, Brian Galligher, and Tina Eliassi-Rad.
2008. Collective classification in network data. AI
magazine, 29(3):93.
S. Sharoff, Z. Wu, and K. Markert. 2010. The web li-
brary of babel: evaluating genre collections. In Pro-
ceedings of the Seventh Conference on International
Language Resources and Evaluation, pages 3063–
3070.
Efstathios Stamatatos, Nikos Fakotakis, and George
Kokkinakis. 2000. Text genre detection using com-
mon word frequencies. In Proceedings of the 18th
conference on Computational linguistics-Volume 2,
pages 808–814.
Johannes M. Steger and Egon W. Stemle. 2009. Krd-
Wrd – architecture for unified processing of web
content.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180.
</reference>
<page confidence="0.999488">
47
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.262934">
<title confidence="0.998882">Semi-supervised Graph-based Genre Classification for Web Pages</title>
<author confidence="0.918559">Noushin Rezapour</author>
<affiliation confidence="0.9973795">School of University of</affiliation>
<email confidence="0.949842">scs5nra@leeds.ac.uk</email>
<author confidence="0.434129">Katja</author>
<affiliation confidence="0.96245725">L3S Research Leibniz Universit¨at and School of University of</affiliation>
<email confidence="0.813849">markert@l3s.de</email>
<author confidence="0.927246">Serge</author>
<affiliation confidence="0.993714333333333">School of Languages and University of</affiliation>
<email confidence="0.989291">s.sharoff@leeds.ac.uk</email>
<abstract confidence="0.999490125">Until now, it is still unclear which set of features produces the best result in automatic genre classification on the web. Therefore, in the first set of experiments, we compared a wide range of contentbased features which are extracted from the data appearing within the web pages. The results show that lexical features such word unigrams and character have more discriminative power in genre classification compared to features such part-of-speech and text statistics. In a second set of experiments, with the aim of learning from the neighbouring web pages, we investigated the performance of a semi-supervised graphbased model, which is a novel technique in genre classification. The results show that our semi-supervised min-cut algorithm improves the overall genre classification accuracy. However, it seems that some genre classes benefit more from this graph-based model than others.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shlomo Argamon</author>
<author>Casey Whitelaw</author>
<author>Paul Chase</author>
<author>Sobhan Raj Hota</author>
<author>Navendu Garg</author>
<author>Shlomo Levitan</author>
</authors>
<title>Stylistic text classification using functional lexical features.</title>
<date>2007</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>58</volume>
<issue>6</issue>
<pages>822</pages>
<contexts>
<context position="10024" citStr="Argamon et al., 2007" startWordPosition="1605" endWordPosition="1608">1 1 0.880 Story 184 24 38 1 7 0.953 Interview 185 154 11 1 1 0.905 Table 1: Statistics for each category illustrate source diversity and reliability of the corpus (Asheghi et al., 2014). To save space, in this paper we use the abbreviation of genre labels which are specified after the genre names. matatos et al., 2000) and (Argamon et al., 1998)) use only topic independent features such as common words or function words in genre classification. However, neither of these features are exclusive to genre classification. Function words and common words are used in authorship classification (e.g. (Argamon et al., 2007)) because they can capture the style of the authors without being influenced by the topics of the texts. On the other hand, word unigrams are a popular document representation in topic classification. If we want these models to capture the genre of documents without being influenced by their topics or the style of their authors, we must eliminate the influence of these factors on genre classification by keeping them constant across the genre classes in the training data. That means all the documents in the training set should be about the same topic and written by the same person. However, con</context>
</contexts>
<marker>Argamon, Whitelaw, Chase, Hota, Garg, Levitan, 2007</marker>
<rawString>Shlomo Argamon, Casey Whitelaw, Paul Chase, Sobhan Raj Hota, Navendu Garg, and Shlomo Levitan. 2007. Stylistic text classification using functional lexical features. Journal of the American Society for Information Science and Technology, 58(6):802– 822.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noushin Rezapour Asheghi</author>
<author>Serge Sharoff</author>
<author>Katja Markert</author>
</authors>
<title>Designing and evaluating a reliable corpus of web genres via crowd-sourcing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14).</booktitle>
<contexts>
<context position="9588" citStr="Asheghi et al., 2014" startWordPosition="1532" endWordPosition="1535">nal Organization Homepage (edu) 299 299 1 1 1 0.953 Personal Blog /Diary (blog) 244 215 9 1 1 0.812 Online Shop (shop) 292 209 23 1 1 0.830 Instruction/ How to (instruction) 231 142 15 1 1 0.871 Recipe 332 116 8 1 1 0.971 News 330 127 12 1 1 0.801 Editorial 310 69 11 1 3 0.877 Conversational Forum (forum) 280 106 11 1 1 0.951 Biography (bio) 242 190 15 1 1 0.905 Frequently Asked Questions (faq) 201 140 8 1 1 0.915 Review 266 179 15 1 1 0.880 Story 184 24 38 1 7 0.953 Interview 185 154 11 1 1 0.905 Table 1: Statistics for each category illustrate source diversity and reliability of the corpus (Asheghi et al., 2014). To save space, in this paper we use the abbreviation of genre labels which are specified after the genre names. matatos et al., 2000) and (Argamon et al., 1998)) use only topic independent features such as common words or function words in genre classification. However, neither of these features are exclusive to genre classification. Function words and common words are used in authorship classification (e.g. (Argamon et al., 2007)) because they can capture the style of the authors without being influenced by the topics of the texts. On the other hand, word unigrams are a popular document rep</context>
<context position="10928" citStr="Asheghi et al., 2014" startWordPosition="1760" endWordPosition="1763"> by their topics or the style of their authors, we must eliminate the influence of these factors on genre classification by keeping them constant across the genre classes in the training data. That means all the documents in the training set should be about the same topic and written by the same person. However, constructing such a dataset is practically impossible for genre classes on the web. The other more practical solution to this problem would be to collect data from various topics and sources in order to minimize the impact of these factors on genre classification. For that reason, we (Asheghi et al., 2014) created a web genre annotated corpus which is reliable (with Fleiss’s kappa (Fleiss, 1971) equal to 0.874) and source diverse. We tried to reduce the influence of topic, the writing style of the authors as well as the design of the websites on genre classification by collecting data from various sources and topics. The corpus consists of 3964 web pages from 2522 different websites, distributed across 15 genres (Table 1). Moreover, we prepared two versions of the corpus: the original text and the main text corpora. First, we converted web pages to plain text by removing HTML markup using the K</context>
</contexts>
<marker>Asheghi, Sharoff, Markert, 2014</marker>
<rawString>Noushin Rezapour Asheghi, Serge Sharoff, and Katja Markert. 2014. Designing and evaluating a reliable corpus of web genres via crowd-sourcing. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: An entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="14502" citStr="Barzilay and Lapata, 2008" startWordPosition="2376" endWordPosition="2379">cal and structural features used in previous work, we reimplemented the following published approaches to AGI: function words (Argamon et al., 1998), part-of-speech n-grams (Santini, 2007), word unigrams (Freund et al., 2006) and character 4-grams binary representation (Sharoff et al., 2010). We also explored the discriminative power of other features such as readability features (Pitler and Nenkova, 2008), HTML tags 3 and named entity tags in genre classification (Table 3). This is the first time that some of these features such as average depth of syntax trees and entity coherence features (Barzilay and Lapata, 2008) are used for genre classification. To set a base-line, we used a list of genre names (e.g. news, editorial, interview, review) as features. We used two different feature representations: binary and normalized frequency. In the binary representation of a document, the value for each feature is either one or zero which represents the presence or the absence of each feature respectively. In the normalized fre2http://www.cs.waikato.ac.nz/ml/weka/ 3http://www.w3schools.com/tags/ref byfunc.asp quency representation of a document, the value for each feature is the frequency of that feature which is </context>
<context position="17983" citStr="Barzilay and Lapata, 2008" startWordPosition="2919" endWordPosition="2922">ord.edu/software/CRF-NER.shtml 6http://www.cs.brown.edu/ melsner/manual.html 42 Category Features Token features number of tokens and number of types normalized frequency of punctuation marks and currency characters Named entity tags normalized frequency of tags: time, location, organization, person, money, date Readability features average parse tree height average sentence length and word length standard deviation of sentence length and of word length average number of syllables per word type/token ratio average number of noun phrases and verb phrases per sentence entity coherence features (Barzilay and Lapata, 2008) HTML tags normalized frequency of tags for: sections / style, formatting, programming, visual features such as forms, images, lists and tables Table 3: List of text statistics features explored in this paper part of speech features resulted in improving genre classification accuracy (compared to the accuracy achieved by word unigrams alone), for both original and main text corpora. However, while the improvement for the main text corpus is statistically significant 7, there is no significant difference between these two models for the original corpus. Surprisingly, adding part of speech 3-gra</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: An entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Shuchi Chawla</author>
</authors>
<title>Learning from labeled and unlabeled data using graph mincuts.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning,</booktitle>
<pages>pages</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="20497" citStr="Blum and Chawla (2001)" startWordPosition="3335" endWordPosition="3338">e more likely to be connected) is known as homophily (Sen et al., 2008). We hy7McNemar test at the significance level of 5% pothesis that homophily exists for genre classes and it can help us to improve genre classification on the web. In this paper, we use a semisupervised graph-based algorithm namely, multiclass min-cut, which is a novel approach in genre classification. This algorithm, which is a collective classification method, considers the class labels of all the web pages within a graph. 4.1 Multi-class Min-cut: The Main Idea The Min-cut classification algorithm originally proposed by Blum and Chawla (2001) is based on the idea that linked entities have a tendency to belong to the same class. In other words, it is based on the homophily assumption. Therefore, it should be able to improve genre classification on the web if our hypothesis holds. However, this technique is a binary classification algorithm, whereas, we have a multi-class problem. Unfortunately, multi-class min-cut is NP-hard and there is no exact solution for it. Nevertheless, Ganchev and Pereira (2007) proposed a multi-class extension to Blum and Chawla (2001)’s min-cut algorithm by encoding a multi-class min-cut problem as an ins</context>
</contexts>
<marker>Blum, Chawla, 2001</marker>
<rawString>Avrim Blum and Shuchi Chawla. 2001. Learning from labeled and unlabeled data using graph mincuts. In Proceedings of the Eighteenth International Conference on Machine Learning, pages 19–26. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuri Boykov</author>
<author>Olga Veksler</author>
<author>Ramin Zabih</author>
</authors>
<title>Fast approximate energy minimization via graph cuts. Pattern Analysis and Machine Intelligence,</title>
<date>2001</date>
<journal>IEEE Transactions on,</journal>
<volume>23</volume>
<issue>11</issue>
<contexts>
<context position="23064" citStr="Boykov et al., 2001" startWordPosition="3781" endWordPosition="3784">entation of the features respectively. to web page v. Second, if two web pages v1 and v2 are connected together with an edge a with weight we, we need to pay a cost of we · d(f(v1), f(v2)) where d(.,.) denotes distance between the two labels. A big distance value between labels indicates less similarity between them. Therefore, the total cost of labelling task f is: E(f) = 1: c(v, f(v)) + v∈V (1) 1: we · d(f(v1), f(v2)) e=(v1,v2)∈E Kleinberg and Tardos (1999; 2002) developed an algorithm for minimizing E(f). However, their algorithm uses linear programming which is impractical for large data (Boykov et al., 2001). In a separate study for metric labelling problems, Boykov et al. (2001) have developed a multi-way min-cut algorithm to minimize E(f). This algorithm is very fast and can be applied to large-scale problems with good performance (Boykov et al., 2001). 4.2 Selection of unlabelled data A web page w has different kind of neighbours on the web such as parents, children, siblings, grand parents and grand children which are mainly differentiated based on the distance to the target web page as well as the direction of the links (Qi and Davison, 2009). Since the identification of children of a web pa</context>
<context position="26846" citStr="Boykov et al. (2001)" startWordPosition="4457" endWordPosition="4460">ts to 1. 2. For training nodes, set the cost of the correct label to zero and all other labels to a large constant. 3. For test nodes and unlabelled nodes, we set the cost of each label using a supervised classifier (SVM) using the following formula: c(w, l) = 1 − pl(w) (3) where c(w, l) is the cost of label l for web page w and pl(w) is the probability of w belonging to the label l which is computed by a supervised SVM using word unigrams binary representation of the web pages. 4. Set d(i, j), which denotes the distance between two labels i and j, to 1 if i =6 j and zero otherwise. 5. Employ Boykov et al. (2001) algorithm to find the minimum total cost using multiway min-cut algorithm. 4.4 Results and Discussion We divided the labelled data into 10 folds again ensuring that all the web pages from the same websites are in the same fold. We used 8 folds for training, one fold for validation and one fold for testing. We learnt the best cosine similarity threshold using validation data and then evaluated it on the test data. Tables 6 and 7 illustrate the results of the multi-class min-cut algorithm and the content-based algorithm (both using word unigrams as features) respectively. The results show that </context>
</contexts>
<marker>Boykov, Veksler, Zabih, 2001</marker>
<rawString>Yuri Boykov, Olga Veksler, and Ramin Zabih. 2001. Fast approximate energy minimization via graph cuts. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 23(11):1222–1239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soumen Chakrabarti</author>
<author>Byron Dom</author>
<author>Piotr Indyk</author>
</authors>
<title>Enhanced hypertext categorization using hyperlinks.</title>
<date>1998</date>
<journal>In ACM SIGMOD Record,</journal>
<volume>27</volume>
<pages>307--318</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="6378" citStr="Chakrabarti et al., 1998" startWordPosition="997" endWordPosition="1000">rks in the field of AGI concentrated on extracting features from the content of the documents and classify them by employing a standard supervised algorithm. However, on the web there are other sources of information which can be utilized to improve genre classification of web pages. For instance, the web has a graph structure and web pages are connected via hyper-links. These connections could be exploited to improve genre classification. Various graph-based classification algorithms have been proposed to improve topic classification for web pages, such as the relaxation labelling algorithm (Chakrabarti et al., 1998), iterative classification algorithm (Lu and Getoor, 2003), Markov logic networks (Crane and McDowell, 2012), random graph walk (Lin and Cohen, 2010) and weighted-vote relational neighbour algorithm (Macskassy and Provost, 2007). These classification algorithms which utilize hyper-link connections between web pages to construct graphs, outperformed the classifiers which are solely based on textual content of the web pages for topic classification. Such connected data presents opportunities for boosting the performance of genre classification too. Graph-based web page classification presented i</context>
</contexts>
<marker>Chakrabarti, Dom, Indyk, 1998</marker>
<rawString>Soumen Chakrabarti, Byron Dom, and Piotr Indyk. 1998. Enhanced hypertext categorization using hyperlinks. In ACM SIGMOD Record, volume 27, pages 307–318. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Crane</author>
<author>Luke McDowell</author>
</authors>
<title>Investigating markov logic networks for collective classification.</title>
<date>2012</date>
<booktitle>In ICAART (1),</booktitle>
<pages>5--15</pages>
<contexts>
<context position="6486" citStr="Crane and McDowell, 2012" startWordPosition="1011" endWordPosition="1014">hem by employing a standard supervised algorithm. However, on the web there are other sources of information which can be utilized to improve genre classification of web pages. For instance, the web has a graph structure and web pages are connected via hyper-links. These connections could be exploited to improve genre classification. Various graph-based classification algorithms have been proposed to improve topic classification for web pages, such as the relaxation labelling algorithm (Chakrabarti et al., 1998), iterative classification algorithm (Lu and Getoor, 2003), Markov logic networks (Crane and McDowell, 2012), random graph walk (Lin and Cohen, 2010) and weighted-vote relational neighbour algorithm (Macskassy and Provost, 2007). These classification algorithms which utilize hyper-link connections between web pages to construct graphs, outperformed the classifiers which are solely based on textual content of the web pages for topic classification. Such connected data presents opportunities for boosting the performance of genre classification too. Graph-based web page classification presented in studies such as (Crane and McDowell, 2012; Lu and Getoor, 2003; Macskassy and Provost, 2007) on the WebKB </context>
</contexts>
<marker>Crane, McDowell, 2012</marker>
<rawString>Robert Crane and Luke McDowell. 2012. Investigating markov logic networks for collective classification. In ICAART (1), pages 5–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M CRAVEN</author>
</authors>
<title>Learning to extract symbolic knowledge from the world wide web.</title>
<date>1998</date>
<booktitle>In Proc. of the 15th National Conference on Artificial Intelligence (AAAI-98).</booktitle>
<contexts>
<context position="7108" citStr="CRAVEN, 1998" startWordPosition="1101" endWordPosition="1102">graph walk (Lin and Cohen, 2010) and weighted-vote relational neighbour algorithm (Macskassy and Provost, 2007). These classification algorithms which utilize hyper-link connections between web pages to construct graphs, outperformed the classifiers which are solely based on textual content of the web pages for topic classification. Such connected data presents opportunities for boosting the performance of genre classification too. Graph-based web page classification presented in studies such as (Crane and McDowell, 2012; Lu and Getoor, 2003; Macskassy and Provost, 2007) on the WebKB dataset (CRAVEN, 1998) could be considered as genre classification as opposed to topic classification. The WebKB dataset contains web pages from four computer science departments categorised into seven classes: student, faculty, staff, department, course, project and other. However, this dataset is very specific to the academic domain with low coverage for the web overall, whereas we examine graph-based learning for automatic genre classification of web pages on a much more general dataset with popular genre classes such as news, blog and editorial. Moreover, the graph-based algorithms used on the WebKB dataset are</context>
</contexts>
<marker>CRAVEN, 1998</marker>
<rawString>M CRAVEN. 1998. Learning to extract symbolic knowledge from the world wide web. In Proc. of the 15th National Conference on Artificial Intelligence (AAAI-98).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aidan Finn</author>
<author>Nicholas Kushmerick</author>
</authors>
<title>Learning to classify documents according to genre.</title>
<date>2006</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>57</volume>
<issue>11</issue>
<contexts>
<context position="3857" citStr="Finn and Kushmerick, 2006" startWordPosition="594" endWordPosition="597">ion 5 concludes the findings and discusses future work. 2 Related Work There has been a considerable body of research in AGI. In previous studies on automatic genre classification of web pages, various types of features such as common words (Stamatatos et al., 2000), function words (Argamon et al., 1998), word unigrams (Freund et al., 2006), character n-grams (Kanaris and Stamatatos, 2007), part-ofspeech tags (Karlgren and Cutting, 1994) , partof-speech trigrams (Argamon et al., 1998; Santini, 2007), document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 1997), HTML tags (e.g. (Santini, 2007)) have been explored. However, researchers conducted genre classification experiments with different features on different corpora with different sets of genre labels. As a result, it is difficult to compare them. This motivated Sharoff et al. (2010) to examine a wide range of wordbased, character-based and POS-based features on the existing genre-</context>
</contexts>
<marker>Finn, Kushmerick, 2006</marker>
<rawString>Aidan Finn and Nicholas Kushmerick. 2006. Learning to classify documents according to genre. Journal of the American Society for Information Science and Technology, 57(11):1506–1518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="11019" citStr="Fleiss, 1971" startWordPosition="1776" endWordPosition="1777"> genre classification by keeping them constant across the genre classes in the training data. That means all the documents in the training set should be about the same topic and written by the same person. However, constructing such a dataset is practically impossible for genre classes on the web. The other more practical solution to this problem would be to collect data from various topics and sources in order to minimize the impact of these factors on genre classification. For that reason, we (Asheghi et al., 2014) created a web genre annotated corpus which is reliable (with Fleiss’s kappa (Fleiss, 1971) equal to 0.874) and source diverse. We tried to reduce the influence of topic, the writing style of the authors as well as the design of the websites on genre classification by collecting data from various sources and topics. The corpus consists of 3964 web pages from 2522 different websites, distributed across 15 genres (Table 1). Moreover, we prepared two versions of the corpus: the original text and the main text corpora. First, we converted web pages to plain text by removing HTML markup using the KrdWrd tool (Steger and Stemle, 2009). This resulted in the original text corpus which conta</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>J.L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Freund</author>
<author>C L A Clarke</author>
<author>E G Toms</author>
</authors>
<title>Towards genre classification for ir in the workplace.</title>
<date>2006</date>
<booktitle>In Proceedings of the 1st international conference on Information interaction in context,</booktitle>
<pages>30--36</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3574" citStr="Freund et al., 2006" startWordPosition="553" endWordPosition="556"> classification models based on various lexical, POS-based and text statistics features in Section 3. Section 4 describes our semi-supervised graph-based classification experiment, where we use the multi-class min-cut algorithm as a novel technique in genre classification. Section 5 concludes the findings and discusses future work. 2 Related Work There has been a considerable body of research in AGI. In previous studies on automatic genre classification of web pages, various types of features such as common words (Stamatatos et al., 2000), function words (Argamon et al., 1998), word unigrams (Freund et al., 2006), character n-grams (Kanaris and Stamatatos, 2007), part-ofspeech tags (Karlgren and Cutting, 1994) , partof-speech trigrams (Argamon et al., 1998; Santini, 2007), document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 1997), HTML tags (e.g. (Santini, 2007)) have been explored. However, researchers conducted genre classifi</context>
<context position="14101" citStr="Freund et al., 2006" startWordPosition="2311" endWordPosition="2314"> other researchers in AGI (e.g. (Santini, 2007)) that SVM produces better or at least similar results compared to other machine learning algorithms. We used the one-versus-one multi-class SVM implemented in Weka 2 with the default setting. All the experiments are carried out on both the original text and the main text corpora. 3.2 Features In order to compare the performance of different lexical and structural features used in previous work, we reimplemented the following published approaches to AGI: function words (Argamon et al., 1998), part-of-speech n-grams (Santini, 2007), word unigrams (Freund et al., 2006) and character 4-grams binary representation (Sharoff et al., 2010). We also explored the discriminative power of other features such as readability features (Pitler and Nenkova, 2008), HTML tags 3 and named entity tags in genre classification (Table 3). This is the first time that some of these features such as average depth of syntax trees and entity coherence features (Barzilay and Lapata, 2008) are used for genre classification. To set a base-line, we used a list of genre names (e.g. news, editorial, interview, review) as features. We used two different feature representations: binary and </context>
</contexts>
<marker>Freund, Clarke, Toms, 2006</marker>
<rawString>L. Freund, C.L.A. Clarke, and E.G. Toms. 2006. Towards genre classification for ir in the workplace. In Proceedings of the 1st international conference on Information interaction in context, pages 30–36. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Fernando Pereira</author>
</authors>
<title>Transductive structured classification through constrained min-cuts. TextGraphs-2: Graph-Based Algorithms for Natural Language Processing,</title>
<date>2007</date>
<pages>37</pages>
<contexts>
<context position="20966" citStr="Ganchev and Pereira (2007)" startWordPosition="3413" endWordPosition="3416">f all the web pages within a graph. 4.1 Multi-class Min-cut: The Main Idea The Min-cut classification algorithm originally proposed by Blum and Chawla (2001) is based on the idea that linked entities have a tendency to belong to the same class. In other words, it is based on the homophily assumption. Therefore, it should be able to improve genre classification on the web if our hypothesis holds. However, this technique is a binary classification algorithm, whereas, we have a multi-class problem. Unfortunately, multi-class min-cut is NP-hard and there is no exact solution for it. Nevertheless, Ganchev and Pereira (2007) proposed a multi-class extension to Blum and Chawla (2001)’s min-cut algorithm by encoding a multi-class min-cut problem as an instance of metric labelling. Kleinberg and Tardos (1999; 2002) introduced metric labelling for the first time. The main idea of metric labelling for web page classification can be described as follows: Assume we have a weighted and undirected graph G = (V, E) where each vertex v ∈ V is a web page and the edges represent the hyper-links between the web pages. The task is to classify these web pages into a set of labels L. This task can be denoted as a function f : V →</context>
</contexts>
<marker>Ganchev, Pereira, 2007</marker>
<rawString>Kuzman Ganchev and Fernando Pereira. 2007. Transductive structured classification through constrained min-cuts. TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, page 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Kanaris</author>
<author>Efstathios Stamatatos</author>
</authors>
<title>Webpage genre identification using variable-length character n-grams.</title>
<date>2007</date>
<booktitle>In Tools with Artificial Intelligence,</booktitle>
<volume>2</volume>
<pages>3--10</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="3624" citStr="Kanaris and Stamatatos, 2007" startWordPosition="559" endWordPosition="562">exical, POS-based and text statistics features in Section 3. Section 4 describes our semi-supervised graph-based classification experiment, where we use the multi-class min-cut algorithm as a novel technique in genre classification. Section 5 concludes the findings and discusses future work. 2 Related Work There has been a considerable body of research in AGI. In previous studies on automatic genre classification of web pages, various types of features such as common words (Stamatatos et al., 2000), function words (Argamon et al., 1998), word unigrams (Freund et al., 2006), character n-grams (Kanaris and Stamatatos, 2007), part-ofspeech tags (Karlgren and Cutting, 1994) , partof-speech trigrams (Argamon et al., 1998; Santini, 2007), document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 1997), HTML tags (e.g. (Santini, 2007)) have been explored. However, researchers conducted genre classification experiments with different features on diff</context>
</contexts>
<marker>Kanaris, Stamatatos, 2007</marker>
<rawString>Ioannis Kanaris and Efstathios Stamatatos. 2007. Webpage genre identification using variable-length character n-grams. In Tools with Artificial Intelligence, 2007. ICTAI 2007. 19th IEEE International Conference on, volume 2, pages 3–10. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Karlgren</author>
<author>D Cutting</author>
</authors>
<title>Recognizing text genres with simple metrics using discriminant analysis.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th conference on Computational linguistics-Volume 2,</booktitle>
<pages>1071--1075</pages>
<contexts>
<context position="3673" citStr="Karlgren and Cutting, 1994" startWordPosition="566" endWordPosition="569">ection 3. Section 4 describes our semi-supervised graph-based classification experiment, where we use the multi-class min-cut algorithm as a novel technique in genre classification. Section 5 concludes the findings and discusses future work. 2 Related Work There has been a considerable body of research in AGI. In previous studies on automatic genre classification of web pages, various types of features such as common words (Stamatatos et al., 2000), function words (Argamon et al., 1998), word unigrams (Freund et al., 2006), character n-grams (Kanaris and Stamatatos, 2007), part-ofspeech tags (Karlgren and Cutting, 1994) , partof-speech trigrams (Argamon et al., 1998; Santini, 2007), document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 1997), HTML tags (e.g. (Santini, 2007)) have been explored. However, researchers conducted genre classification experiments with different features on different corpora with different sets of genre labels</context>
</contexts>
<marker>Karlgren, Cutting, 1994</marker>
<rawString>J. Karlgren and D. Cutting. 1994. Recognizing text genres with simple metrics using discriminant analysis. In Proceedings of the 15th conference on Computational linguistics-Volume 2, pages 1071–1075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kessler</author>
<author>G Numberg</author>
<author>H Schutze</author>
</authors>
<title>Automatic detection of text genre.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>32--38</pages>
<marker>Kessler, Numberg, Schutze, 1997</marker>
<rawString>B. Kessler, G. Numberg, and H. Schutze. 1997. Automatic detection of text genre. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, pages 32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15703" citStr="Klein and Manning, 2003" startWordPosition="2565" endWordPosition="2568"> that feature which is normalized by the length of the document. For extracting lexical features, we tokenized each document using the Stanford tokenizer (included as part of the Stanford part of speech tagger (Toutanova et al., 2003)) and converted all the tokens to lower case. For extracting POS tags and named entity tags, we used the Stanford maximum entropy tagger 4 and the Stanford Named Entity Recognizer 5 respectively. For extracting some of the readability features such as average parse tree height and average number of noun and verb phrases per sentences, we used the Stanford Parser (Klein and Manning, 2003). However, web pages must be cleaned before they can be fed to a parser, because parsers cannot handle tables and list of links. Therefore, we only used the main text of each web page as an input to the parser. For web pages for which the justext tool produced empty files, we treated these features as missing values. Moreover, we used the Brown Coherence Toolkit 6 to construct the entity grid for each web page and computed the probability of each entity transition type. This tool needs the parsed version of the text as an input. Therefore, for web pages for which the justext tool produced empt</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
<author>Eva Tardos</author>
</authors>
<title>Approximation algorithms for classification problems with pairwise relationships: Metric labeling and markov random fields.</title>
<date>1999</date>
<booktitle>In focs,</booktitle>
<pages>14</pages>
<publisher>IEEE Computer Society.</publisher>
<note>Published by the</note>
<contexts>
<context position="21150" citStr="Kleinberg and Tardos (1999" startWordPosition="3442" endWordPosition="3445">linked entities have a tendency to belong to the same class. In other words, it is based on the homophily assumption. Therefore, it should be able to improve genre classification on the web if our hypothesis holds. However, this technique is a binary classification algorithm, whereas, we have a multi-class problem. Unfortunately, multi-class min-cut is NP-hard and there is no exact solution for it. Nevertheless, Ganchev and Pereira (2007) proposed a multi-class extension to Blum and Chawla (2001)’s min-cut algorithm by encoding a multi-class min-cut problem as an instance of metric labelling. Kleinberg and Tardos (1999; 2002) introduced metric labelling for the first time. The main idea of metric labelling for web page classification can be described as follows: Assume we have a weighted and undirected graph G = (V, E) where each vertex v ∈ V is a web page and the edges represent the hyper-links between the web pages. The task is to classify these web pages into a set of labels L. This task can be denoted as a function f : V → L. In order to do this labelling task in an optimal way, we need to minimize two different types of costs. First, there is a non-negative cost c(v, l) for assigning label l 43 Feature</context>
<context position="22906" citStr="Kleinberg and Tardos (1999" startWordPosition="3758" endWordPosition="3761">in 88.14 75.59 Table 4: Classification accuracy of different features in genre classification. bin and nf refer to the use of binary and normalized frequency representation of the features respectively. to web page v. Second, if two web pages v1 and v2 are connected together with an edge a with weight we, we need to pay a cost of we · d(f(v1), f(v2)) where d(.,.) denotes distance between the two labels. A big distance value between labels indicates less similarity between them. Therefore, the total cost of labelling task f is: E(f) = 1: c(v, f(v)) + v∈V (1) 1: we · d(f(v1), f(v2)) e=(v1,v2)∈E Kleinberg and Tardos (1999; 2002) developed an algorithm for minimizing E(f). However, their algorithm uses linear programming which is impractical for large data (Boykov et al., 2001). In a separate study for metric labelling problems, Boykov et al. (2001) have developed a multi-way min-cut algorithm to minimize E(f). This algorithm is very fast and can be applied to large-scale problems with good performance (Boykov et al., 2001). 4.2 Selection of unlabelled data A web page w has different kind of neighbours on the web such as parents, children, siblings, grand parents and grand children which are mainly differentiat</context>
</contexts>
<marker>Kleinberg, Tardos, 1999</marker>
<rawString>Jon Kleinberg and Eva Tardos. 1999. Approximation algorithms for classification problems with pairwise relationships: Metric labeling and markov random fields. In focs, page 14. Published by the IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Kleinberg</author>
<author>Eva Tardos</author>
</authors>
<title>Approximation algorithms for classification problems with pairwise relationships: Metric labeling and markov random fields.</title>
<date>2002</date>
<journal>Journal of the ACM (JACM),</journal>
<volume>49</volume>
<issue>5</issue>
<marker>Kleinberg, Tardos, 2002</marker>
<rawString>Jon Kleinberg and Eva Tardos. 2002. Approximation algorithms for classification problems with pairwise relationships: Metric labeling and markov random fields. Journal of the ACM (JACM), 49(5):616–639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Lin</author>
<author>William W Cohen</author>
</authors>
<title>Semisupervised classification of network data using very few labels.</title>
<date>2010</date>
<booktitle>In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on,</booktitle>
<pages>192--199</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="6527" citStr="Lin and Cohen, 2010" startWordPosition="1018" endWordPosition="1021">hm. However, on the web there are other sources of information which can be utilized to improve genre classification of web pages. For instance, the web has a graph structure and web pages are connected via hyper-links. These connections could be exploited to improve genre classification. Various graph-based classification algorithms have been proposed to improve topic classification for web pages, such as the relaxation labelling algorithm (Chakrabarti et al., 1998), iterative classification algorithm (Lu and Getoor, 2003), Markov logic networks (Crane and McDowell, 2012), random graph walk (Lin and Cohen, 2010) and weighted-vote relational neighbour algorithm (Macskassy and Provost, 2007). These classification algorithms which utilize hyper-link connections between web pages to construct graphs, outperformed the classifiers which are solely based on textual content of the web pages for topic classification. Such connected data presents opportunities for boosting the performance of genre classification too. Graph-based web page classification presented in studies such as (Crane and McDowell, 2012; Lu and Getoor, 2003; Macskassy and Provost, 2007) on the WebKB dataset (CRAVEN, 1998) could be considere</context>
</contexts>
<marker>Lin, Cohen, 2010</marker>
<rawString>Frank Lin and William W Cohen. 2010. Semisupervised classification of network data using very few labels. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on, pages 192–199. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Lu</author>
<author>L Getoor</author>
</authors>
<title>Link-based classification using labeled and unlabeled data. The Continuum from Labeled to Unlabeled Data</title>
<date>2003</date>
<booktitle>in Machine Learning &amp; Data Mining,</booktitle>
<pages>88</pages>
<contexts>
<context position="6436" citStr="Lu and Getoor, 2003" startWordPosition="1004" endWordPosition="1007">m the content of the documents and classify them by employing a standard supervised algorithm. However, on the web there are other sources of information which can be utilized to improve genre classification of web pages. For instance, the web has a graph structure and web pages are connected via hyper-links. These connections could be exploited to improve genre classification. Various graph-based classification algorithms have been proposed to improve topic classification for web pages, such as the relaxation labelling algorithm (Chakrabarti et al., 1998), iterative classification algorithm (Lu and Getoor, 2003), Markov logic networks (Crane and McDowell, 2012), random graph walk (Lin and Cohen, 2010) and weighted-vote relational neighbour algorithm (Macskassy and Provost, 2007). These classification algorithms which utilize hyper-link connections between web pages to construct graphs, outperformed the classifiers which are solely based on textual content of the web pages for topic classification. Such connected data presents opportunities for boosting the performance of genre classification too. Graph-based web page classification presented in studies such as (Crane and McDowell, 2012; Lu and Getoor</context>
</contexts>
<marker>Lu, Getoor, 2003</marker>
<rawString>Q. Lu and L. Getoor. 2003. Link-based classification using labeled and unlabeled data. The Continuum from Labeled to Unlabeled Data in Machine Learning &amp; Data Mining, page 88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sofus A Macskassy</author>
<author>Foster Provost</author>
</authors>
<title>Classification in networked data: A toolkit and a univariate case study.</title>
<date>2007</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>8--935</pages>
<contexts>
<context position="6606" citStr="Macskassy and Provost, 2007" startWordPosition="1027" endWordPosition="1030">n be utilized to improve genre classification of web pages. For instance, the web has a graph structure and web pages are connected via hyper-links. These connections could be exploited to improve genre classification. Various graph-based classification algorithms have been proposed to improve topic classification for web pages, such as the relaxation labelling algorithm (Chakrabarti et al., 1998), iterative classification algorithm (Lu and Getoor, 2003), Markov logic networks (Crane and McDowell, 2012), random graph walk (Lin and Cohen, 2010) and weighted-vote relational neighbour algorithm (Macskassy and Provost, 2007). These classification algorithms which utilize hyper-link connections between web pages to construct graphs, outperformed the classifiers which are solely based on textual content of the web pages for topic classification. Such connected data presents opportunities for boosting the performance of genre classification too. Graph-based web page classification presented in studies such as (Crane and McDowell, 2012; Lu and Getoor, 2003; Macskassy and Provost, 2007) on the WebKB dataset (CRAVEN, 1998) could be considered as genre classification as opposed to topic classification. The WebKB dataset</context>
</contexts>
<marker>Macskassy, Provost, 2007</marker>
<rawString>Sofus A Macskassy and Foster Provost. 2007. Classification in networked data: A toolkit and a univariate case study. The Journal of Machine Learning Research, 8:935–983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Petrenz</author>
<author>B Webber</author>
</authors>
<title>Stable classification of text genres. Computational Linguistics,</title>
<date>2010</date>
<location>(Early Access):1–9.</location>
<contexts>
<context position="5133" citStr="Petrenz and Webber (2010)" startWordPosition="791" endWordPosition="794"> and character 4-grams outperform other features in genre classification. However, they concluded that the results cannot be trusted because of two main reasons. First, some of these collections exhibit low inter-coder agreement and any results based on unreliable data could be misleading. Second, the spurious correlation between topic and genre classes in some of these corpora was one of the reasons for some of the very impressive results reported by Sharoff et al. (2010). These good results were achieved by detecting topics rather than genres of individual texts. A similar point was made by Petrenz and Webber (2010) who examined the impact of topic change on the performance of AGI systems. They showed that a shift in topic can have a massive impact on genre classification models which are based on lexical features such as word unigrams or character n-grams. Therefore, the question which set of features produces the best result in automatic genre classification on the web is still an open question. In order to investigate this question, we perform genre classification with a wide range of features on a reliable and topically diverse dataset. Section 3.1 describes the dataset and the experimental setup. Mo</context>
<context position="8553" citStr="Petrenz and Webber (2010)" startWordPosition="1333" endWordPosition="1336">r, our experiment is in a semi-supervised manner which is a much more realistic scenario on the web, because it is highly unlikely that for each web page, we have genre labels for all its neighbouring web pages as well. Therefore, we perform our experiment on a very noisy dataset where neighbouring web pages could belong to none of our predefined genre classes. Section 4 describes our semi-supervised graph-based classification experiment, where we use a multi-class min-cut algorithm as a novel technique in genre classification. 3 Content-based Classification 3.1 Dataset and Experimental Setup Petrenz and Webber (2010) and Sharoff et al. (2010) emphasize that the impact of topic on genre classification should be eliminated or controlled. In order to avoid the influence of topic on genre classification, some researchers (e.g. (Sta40 Genre Number of # of pages from Fleiss’s κ web pages websites the same website max min med Personal Homepage (php) 304 288 9 1 1 0.858 Company/ Business Homepage (com) 264 264 1 1 1 0.713 Educational Organization Homepage (edu) 299 299 1 1 1 0.953 Personal Blog /Diary (blog) 244 215 9 1 1 0.812 Online Shop (shop) 292 209 23 1 1 0.830 Instruction/ How to (instruction) 231 142 15 1</context>
</contexts>
<marker>Petrenz, Webber, 2010</marker>
<rawString>P. Petrenz and B. Webber. 2010. Stable classification of text genres. Computational Linguistics, (Early Access):1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Ani Nenkova</author>
</authors>
<title>Revisiting readability: A unified framework for predicting text quality.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>186--195</pages>
<contexts>
<context position="14285" citStr="Pitler and Nenkova, 2008" startWordPosition="2338" endWordPosition="2341">ti-class SVM implemented in Weka 2 with the default setting. All the experiments are carried out on both the original text and the main text corpora. 3.2 Features In order to compare the performance of different lexical and structural features used in previous work, we reimplemented the following published approaches to AGI: function words (Argamon et al., 1998), part-of-speech n-grams (Santini, 2007), word unigrams (Freund et al., 2006) and character 4-grams binary representation (Sharoff et al., 2010). We also explored the discriminative power of other features such as readability features (Pitler and Nenkova, 2008), HTML tags 3 and named entity tags in genre classification (Table 3). This is the first time that some of these features such as average depth of syntax trees and entity coherence features (Barzilay and Lapata, 2008) are used for genre classification. To set a base-line, we used a list of genre names (e.g. news, editorial, interview, review) as features. We used two different feature representations: binary and normalized frequency. In the binary representation of a document, the value for each feature is either one or zero which represents the presence or the absence of each feature respecti</context>
</contexts>
<marker>Pitler, Nenkova, 2008</marker>
<rawString>Emily Pitler and Ani Nenkova. 2008. Revisiting readability: A unified framework for predicting text quality. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 186–195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoguang Qi</author>
<author>Brian D Davison</author>
</authors>
<title>Web page classification: Features and algorithms.</title>
<date>2009</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="23614" citStr="Qi and Davison, 2009" startWordPosition="3875" endWordPosition="3878">r programming which is impractical for large data (Boykov et al., 2001). In a separate study for metric labelling problems, Boykov et al. (2001) have developed a multi-way min-cut algorithm to minimize E(f). This algorithm is very fast and can be applied to large-scale problems with good performance (Boykov et al., 2001). 4.2 Selection of unlabelled data A web page w has different kind of neighbours on the web such as parents, children, siblings, grand parents and grand children which are mainly differentiated based on the distance to the target web page as well as the direction of the links (Qi and Davison, 2009). Since the identification of children of a web page (i.e. web pages which have Cosine # of unlabelled Average # of similarity web pages neighbours &gt; 0 103,372 40.65 &gt; 0.1 98,824 39.08 &gt; 0.2 87,834 34.23 &gt; 0.3 70,602 26.46 &gt; 0.4 50,232 17.52 &gt; 0.5 28,437 8.62 &gt; 0.6 13,919 3.77 &gt; 0.7 7,241 1.86 &gt; 0.8 3,772 0.98 &gt; 0.9 1,732 0.44 Table 5: Number of unlabelled web pages with different cosine similarity thresholds. The last column shows the average number of neighbours per labelled page. direct links from the target web page) is a straightforward task as their URLs can be extracted from the HTML ve</context>
</contexts>
<marker>Qi, Davison, 2009</marker>
<rawString>Xiaoguang Qi and Brian D Davison. 2009. Web page classification: Features and algorithms. ACM Computing Surveys (CSUR), 41(2):12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Santini</author>
</authors>
<title>Automatic identification of genre in web pages.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Brighton.</institution>
<contexts>
<context position="3736" citStr="Santini, 2007" startWordPosition="578" endWordPosition="580"> experiment, where we use the multi-class min-cut algorithm as a novel technique in genre classification. Section 5 concludes the findings and discusses future work. 2 Related Work There has been a considerable body of research in AGI. In previous studies on automatic genre classification of web pages, various types of features such as common words (Stamatatos et al., 2000), function words (Argamon et al., 1998), word unigrams (Freund et al., 2006), character n-grams (Kanaris and Stamatatos, 2007), part-ofspeech tags (Karlgren and Cutting, 1994) , partof-speech trigrams (Argamon et al., 1998; Santini, 2007), document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 1997), HTML tags (e.g. (Santini, 2007)) have been explored. However, researchers conducted genre classification experiments with different features on different corpora with different sets of genre labels. As a result, it is difficult to compare them. This motivated </context>
<context position="13528" citStr="Santini, 2007" startWordPosition="2225" endWordPosition="2226"> ignored this essential step when dividing the data into folds. For machine learning, we 1http://code.google.com/p/justext/ 41 Genre Number of web pages in corpora Original text Main text php 304 221 com 264 190 edu 299 191 blog 244 242 shop 292 221 instruction 231 229 recipe 332 243 news 330 320 editorial 310 307 forum 280 251 bio 242 242 faq 201 160 review 266 262 story 184 184 interview 185 183 Table 2: Number of web pages in individual genre classes in both original text and main text corpora. chose Support Vector Machines (SVM) because it has been shown by other researchers in AGI (e.g. (Santini, 2007)) that SVM produces better or at least similar results compared to other machine learning algorithms. We used the one-versus-one multi-class SVM implemented in Weka 2 with the default setting. All the experiments are carried out on both the original text and the main text corpora. 3.2 Features In order to compare the performance of different lexical and structural features used in previous work, we reimplemented the following published approaches to AGI: function words (Argamon et al., 1998), part-of-speech n-grams (Santini, 2007), word unigrams (Freund et al., 2006) and character 4-grams bina</context>
</contexts>
<marker>Santini, 2007</marker>
<rawString>Marina Santini. 2007. Automatic identification of genre in web pages. Ph.D. thesis, University of Brighton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prithviraj Sen</author>
<author>Galileo Namata</author>
<author>Mustafa Bilgic</author>
<author>Lise Getoor</author>
<author>Brian Galligher</author>
<author>Tina Eliassi-Rad</author>
</authors>
<title>Collective classification in network data.</title>
<date>2008</date>
<journal>AI magazine,</journal>
<pages>29--3</pages>
<contexts>
<context position="19946" citStr="Sen et al., 2008" startWordPosition="3244" endWordPosition="3247">es. However, other sources of information such as the connections and the link patterns between the web pages could be exploited to improve genre classification. The underlying assumption of this approach is that a page is more likely to be connected to pages with the same genre category. For example, if the neighbouring web pages of a particular web page are labelled as shop, it is more likely that this web page is a shop too, whereas, it is highly unlikely that it is a news or editorial. This property (i.e. entities with similar labels are more likely to be connected) is known as homophily (Sen et al., 2008). We hy7McNemar test at the significance level of 5% pothesis that homophily exists for genre classes and it can help us to improve genre classification on the web. In this paper, we use a semisupervised graph-based algorithm namely, multiclass min-cut, which is a novel approach in genre classification. This algorithm, which is a collective classification method, considers the class labels of all the web pages within a graph. 4.1 Multi-class Min-cut: The Main Idea The Min-cut classification algorithm originally proposed by Blum and Chawla (2001) is based on the idea that linked entities have a</context>
</contexts>
<marker>Sen, Namata, Bilgic, Getoor, Galligher, Eliassi-Rad, 2008</marker>
<rawString>Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. 2008. Collective classification in network data. AI magazine, 29(3):93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sharoff</author>
<author>Z Wu</author>
<author>K Markert</author>
</authors>
<title>The web library of babel: evaluating genre collections.</title>
<date>2010</date>
<booktitle>In Proceedings of the Seventh Conference on International Language Resources and Evaluation,</booktitle>
<pages>3063--3070</pages>
<contexts>
<context position="2319" citStr="Sharoff et al., 2010" startWordPosition="353" endWordPosition="356">om a specific genre such as news articles, reviews or blogs, search results could be more beneficial. With the aim of enhancing search engines, AGI has attracted a lot of attention (see Section 2). In this paper, we investigate two important open questions in AGI. The first question is: what set of features produces the best result in genre classification on the web? The drawbacks of existing genre-annotated web corpora (low inter-coder agreement; false correlations between topic and genre classes) resulted in researchers’ doubt on the outcomes of classification models based on these corpora (Sharoff et al., 2010). Therefore, in order to answer this question, we perform genre classification with a wide range of features on a reliable and source diverse genre-annotated web corpus. The second question that we investigate in this paper is: could we exploit the graph structure of the web to increase genre classification accuracy? With the aim of learning from the neighbouring web pages, we investigated the performance of a semi-supervised graph-based model, which is a novel technique in genre classification. The remainder of this paper is structured as follows. After reviewing related work in Section 2, we</context>
<context position="4357" citStr="Sharoff et al. (2010)" startWordPosition="668" endWordPosition="671">, document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 1997), HTML tags (e.g. (Santini, 2007)) have been explored. However, researchers conducted genre classification experiments with different features on different corpora with different sets of genre labels. As a result, it is difficult to compare them. This motivated Sharoff et al. (2010) to examine a wide range of wordbased, character-based and POS-based features on the existing genre-annotated corpora. They reported that word unigrams and character 4-grams outperform other features in genre classification. However, they concluded that the results cannot be trusted because of two main reasons. First, some of these collections exhibit low inter-coder agreement and any results based on unreliable data could be misleading. Second, the spurious correlation between topic and genre classes in some of these corpora was one of the reasons for some of the very impressive results repor</context>
<context position="8579" citStr="Sharoff et al. (2010)" startWordPosition="1338" endWordPosition="1341">-supervised manner which is a much more realistic scenario on the web, because it is highly unlikely that for each web page, we have genre labels for all its neighbouring web pages as well. Therefore, we perform our experiment on a very noisy dataset where neighbouring web pages could belong to none of our predefined genre classes. Section 4 describes our semi-supervised graph-based classification experiment, where we use a multi-class min-cut algorithm as a novel technique in genre classification. 3 Content-based Classification 3.1 Dataset and Experimental Setup Petrenz and Webber (2010) and Sharoff et al. (2010) emphasize that the impact of topic on genre classification should be eliminated or controlled. In order to avoid the influence of topic on genre classification, some researchers (e.g. (Sta40 Genre Number of # of pages from Fleiss’s κ web pages websites the same website max min med Personal Homepage (php) 304 288 9 1 1 0.858 Company/ Business Homepage (com) 264 264 1 1 1 0.713 Educational Organization Homepage (edu) 299 299 1 1 1 0.953 Personal Blog /Diary (blog) 244 215 9 1 1 0.812 Online Shop (shop) 292 209 23 1 1 0.830 Instruction/ How to (instruction) 231 142 15 1 1 0.871 Recipe 332 116 8 </context>
<context position="14168" citStr="Sharoff et al., 2010" startWordPosition="2321" endWordPosition="2324"> better or at least similar results compared to other machine learning algorithms. We used the one-versus-one multi-class SVM implemented in Weka 2 with the default setting. All the experiments are carried out on both the original text and the main text corpora. 3.2 Features In order to compare the performance of different lexical and structural features used in previous work, we reimplemented the following published approaches to AGI: function words (Argamon et al., 1998), part-of-speech n-grams (Santini, 2007), word unigrams (Freund et al., 2006) and character 4-grams binary representation (Sharoff et al., 2010). We also explored the discriminative power of other features such as readability features (Pitler and Nenkova, 2008), HTML tags 3 and named entity tags in genre classification (Table 3). This is the first time that some of these features such as average depth of syntax trees and entity coherence features (Barzilay and Lapata, 2008) are used for genre classification. To set a base-line, we used a list of genre names (e.g. news, editorial, interview, review) as features. We used two different feature representations: binary and normalized frequency. In the binary representation of a document, t</context>
<context position="17042" citStr="Sharoff et al., 2010" startWordPosition="2797" endWordPosition="2800">different feature sets listed in the previous section on both the original text and the main text corpora. At first glance, we see that the results of genre classification on the original text corpus are higher than the main text corpus. This shows that boiler plates contain valuable information which helps genre classification. Moreover, the results show that binary representation of word unigrams is the best performing feature set when we use the whole text of the web pages. However, on the main text corpus, character 4-grams outperform other features. This confirms the results reported in (Sharoff et al., 2010). The results also highlight that the performance of POS-based features are much less accurate than that of textual features such as word unigrams and character n-grams. The results also show that the combination of word unigrams, text statistics and 4http://nlp.stanford.edu/software/tagger.shtml 5http://nlp.stanford.edu/software/CRF-NER.shtml 6http://www.cs.brown.edu/ melsner/manual.html 42 Category Features Token features number of tokens and number of types normalized frequency of punctuation marks and currency characters Named entity tags normalized frequency of tags: time, location, organ</context>
</contexts>
<marker>Sharoff, Wu, Markert, 2010</marker>
<rawString>S. Sharoff, Z. Wu, and K. Markert. 2010. The web library of babel: evaluating genre collections. In Proceedings of the Seventh Conference on International Language Resources and Evaluation, pages 3063– 3070.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
<author>Nikos Fakotakis</author>
<author>George Kokkinakis</author>
</authors>
<title>Text genre detection using common word frequencies.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics-Volume 2,</booktitle>
<pages>808--814</pages>
<contexts>
<context position="3498" citStr="Stamatatos et al., 2000" startWordPosition="541" endWordPosition="544">After reviewing related work in Section 2, we compare different supervised genre classification models based on various lexical, POS-based and text statistics features in Section 3. Section 4 describes our semi-supervised graph-based classification experiment, where we use the multi-class min-cut algorithm as a novel technique in genre classification. Section 5 concludes the findings and discusses future work. 2 Related Work There has been a considerable body of research in AGI. In previous studies on automatic genre classification of web pages, various types of features such as common words (Stamatatos et al., 2000), function words (Argamon et al., 1998), word unigrams (Freund et al., 2006), character n-grams (Kanaris and Stamatatos, 2007), part-ofspeech tags (Karlgren and Cutting, 1994) , partof-speech trigrams (Argamon et al., 1998; Santini, 2007), document statistics (e.g. average sentence length, average word length and type/token ratio) (Finn and Kushmerick, 2006; Kessler et 39 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39–47, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics al., 1997), HTML tags (e.g. (Santi</context>
</contexts>
<marker>Stamatatos, Fakotakis, Kokkinakis, 2000</marker>
<rawString>Efstathios Stamatatos, Nikos Fakotakis, and George Kokkinakis. 2000. Text genre detection using common word frequencies. In Proceedings of the 18th conference on Computational linguistics-Volume 2, pages 808–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes M Steger</author>
<author>Egon W Stemle</author>
</authors>
<title>KrdWrd – architecture for unified processing of web content.</title>
<date>2009</date>
<contexts>
<context position="11564" citStr="Steger and Stemle, 2009" startWordPosition="1870" endWordPosition="1873">eb genre annotated corpus which is reliable (with Fleiss’s kappa (Fleiss, 1971) equal to 0.874) and source diverse. We tried to reduce the influence of topic, the writing style of the authors as well as the design of the websites on genre classification by collecting data from various sources and topics. The corpus consists of 3964 web pages from 2522 different websites, distributed across 15 genres (Table 1). Moreover, we prepared two versions of the corpus: the original text and the main text corpora. First, we converted web pages to plain text by removing HTML markup using the KrdWrd tool (Steger and Stemle, 2009). This resulted in the original text corpus which contains individual web pages with all the textual elements present on them. Moreover, in order to investigate the influence of boilerplate parts (e.g. advertisements, headers, footers, template materials, navigation menus and lists of links) of the web pages on genre classification, we removed the boilerplate parts and extracted the main text of each web page using the justext tool 1. This resulted in the creation of the main text corpus. This is the first time that the performance of genre classification models is compared on both the origina</context>
</contexts>
<marker>Steger, Stemle, 2009</marker>
<rawString>Johannes M. Steger and Egon W. Stemle. 2009. KrdWrd – architecture for unified processing of web content.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<contexts>
<context position="15313" citStr="Toutanova et al., 2003" startWordPosition="2500" endWordPosition="2503">binary and normalized frequency. In the binary representation of a document, the value for each feature is either one or zero which represents the presence or the absence of each feature respectively. In the normalized fre2http://www.cs.waikato.ac.nz/ml/weka/ 3http://www.w3schools.com/tags/ref byfunc.asp quency representation of a document, the value for each feature is the frequency of that feature which is normalized by the length of the document. For extracting lexical features, we tokenized each document using the Stanford tokenizer (included as part of the Stanford part of speech tagger (Toutanova et al., 2003)) and converted all the tokens to lower case. For extracting POS tags and named entity tags, we used the Stanford maximum entropy tagger 4 and the Stanford Named Entity Recognizer 5 respectively. For extracting some of the readability features such as average parse tree height and average number of noun and verb phrases per sentences, we used the Stanford Parser (Klein and Manning, 2003). However, web pages must be cleaned before they can be fed to a parser, because parsers cannot handle tables and list of links. Therefore, we only used the main text of each web page as an input to the parser.</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
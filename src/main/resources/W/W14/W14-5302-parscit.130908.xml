<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021856">
<title confidence="0.9945895">
Diachronic proximity vs. data sparsity in cross-lingual parser projection.
A case study on Germanic
</title>
<author confidence="0.991743">
Maria Sukhareva
</author>
<affiliation confidence="0.986829">
Goethe University Frankfurt
</affiliation>
<email confidence="0.955319">
sukharev@em.uni-frankfurt.de
</email>
<author confidence="0.977392">
Christian Chiarcos
</author>
<affiliation confidence="0.976754">
Goethe University Frankfurt
</affiliation>
<email confidence="0.967182">
chiarcos@em.uni-frankfurt.de
</email>
<sectionHeader confidence="0.993135" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9988285">
For the study of historical language varieties, the sparsity of training data imposes immense prob-
lems on syntactic annotation and the development of NLP tools that automatize the process. In
this paper, we explore strategies to compensate the lack of training data by including data from
related varieties in a series of annotation projection experiments from English to four old Ger-
manic languages: On dependency syntax projected from English to one or multiple language(s),
we train a fragment-aware parser trained and apply it to the target language. For parser training,
we consider small datasets from the target language as a baseline, and compare it with models
trained on larger datasets from multiple varieties with different degrees of relatedness, thereby
balancing sparsity and diachronic proximity.
Our experiments show
</bodyText>
<listItem confidence="0.903158">
(a) that including related language data to training data in the target language can improve
parsing performance,
(b) that a parser trained on data from two related languages (and none from the target language)
can reach a performance that is statistically not significantly worse than that of a parser
trained on the projections to the target language, and
(c) that both conclusions holds only among the three most closely related languages under
consideration, but not necessarily the fourth.
</listItem>
<bodyText confidence="0.9993665">
The experiments motivate the compilation of a larger parallel corpus of historical Germanic va-
rieties as a basis for subsequent studies.
</bodyText>
<sectionHeader confidence="0.884219" genericHeader="keywords">
1 Background and motivation
</sectionHeader>
<bodyText confidence="0.999970333333334">
We describe an experiment on annotation projection (Yarowski and Ngai, 2001) between different Ger-
manic languages, resp., their historical varieties, with the goal to assess to what extent sparsity of parallel
data can be compensated by material from varieties related to the target variety, and studying the impact
of diachronic proximity onto such applications.
Statistical NLP of historical language data involves general issues typical for low-resource languages
(the lack of annotated corpora, data sparsity, etc.), but also very specific challenges such as lack of stan-
dardized orthography, unsystematized punctuation, and a considerable degree of morphological varia-
tion. At the same time, historical languages can be viewed as variants of their modern descendants rather
than entirely independent languages, a situation comparable to low-resource languages for which a di-
achronically related major language exists. Technologies for the cross-lingual adaptation of NLP tools or
training of NLP tools on multiple dialects or language stages are thus of practical relevance to not only
historical linguistics, but also to modern low-resource languages.
</bodyText>
<footnote confidence="0.6576075">
The final paper will be published under a Creative Commons Attribution 4.0 International Licence (CC-BY), http:
//creativecommons.org/licenses/by/4.0/.
</footnote>
<page confidence="0.987172">
11
</page>
<note confidence="0.985351">
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 11–20,
Dublin, Ireland, August 23 2014.
</note>
<bodyText confidence="0.999170454545455">
in this context, historical language allows to study the impact of the parameter of diachronic related-
ness, as it can be adjusted relatively freely, e.g., by choosing dialects which common ancestor existed
just a few generations before rather than languages separated for centuries. A focused study of the im-
pact of diachronic relatedness on projected annotations requires sufficient amounts of parallel texts for
major language stages, and comparable annotations as a gold standard for evaluation. In this regard, the
Germanic languages provide us with a especially promising sandbox to develop such algorithms due to
the abundance of annotated corpora and NLP tools of the modern Germanic languages, most noteably
Modern English.
We employ annotation projection from EN to Middle English (ME), Old English (OE) and the less
closely related Early Modern High German (DE) and Middle Icelandic (IS) for which we possess com-
parable annotations, and test the following hypotheses:
</bodyText>
<listItem confidence="0.909383333333333">
(H1) Adding data from related varieties compensates the sparsity of target language training data.
(H2) Data from related languages compensates the lack of target language training data.
(H3) The greater the diachronic proximity, the better the performance of (H1) and (H2).
</listItem>
<bodyText confidence="0.99998675">
We test these hypotheses in the following setup: (1) Hyperlemmatization: Different historical variants
are normalized to a consistent standard, e.g., represented by a modern language (Bollmann et al., 2011).
We emulate hyperlemmatization by English glosses automatically obtained through SMT. (2) Projection:
We create training data for a fragment-aware dependency parser (Spreyer et al., 2010) using annotation
projection from modern English. (3) Combination and evaluation: Parser modules are trained on differ-
ent training data sets, and evaluated against existing gold annotations.
In our setting, we enforce data sparsity by using deliberately small training data sets. This is because
we emulate the situation of less-documented languages that will be in the focus of subsequent experi-
ments, namely, Old High German and Old Saxon, which are relatively poorly documented. We do hope,
however, that scalable NLP solutions can be developed if we add background information from their de-
scendants (Middle/Early Modern High German, Middle/Modern Low German), or closely related, and
better documented varieties (Old English, Middle Dutch).
Hence, the goal of our experiment is not to develop state-of-the-art parsers, but to detect statistically
significant differences in parsing performance. If these can be confirmed, this motivates creating a larger
corpus of parallel texts in Germanic languages as a basis for subsequent studies and more advanced,
projection-based technologies for older and under-resourced Germanic languages.
</bodyText>
<sectionHeader confidence="0.722968" genericHeader="introduction">
2 Languages and corpus data
</sectionHeader>
<bodyText confidence="0.999024733333333">
We use parallel biblical texts in Old English (OE), Middle English (ME), Middle Icelandic (IS) and Early
Modern High German (DE). This selection is determined by the availability of syntactically annotated
corpora with closely related annotation schemes. As these schemes are derived from the Penn TreeBank
(PTB) bracketing guidelines (Taylor et al., 2003a), we decided to use Modern English (EN) as a source
for the projections.
The Germanic languages derive from Proto-Germanic as a common ancestor. OE and Old High
German separated in the 5th c. The antecessor of IS separated from this branch about 500 years earlier.
Among Germanic languages, great differences emerged, but most languages developed similarly towards
a loss of morphology and a more rigid syntax, a tendency particularly prevalent in EN.
As compared to this, OE had a relatively free OV word order, with grammatical roles conveyed through
morphological markers. The OE case marking system distinguished four cases, but eventually collapsed
during ME, resulting in a strict strict VO word order in EN (Trips, 2002; van Kemenade and Los, 2009;
Cummings, 2010).
Unlike EN, DE preserved four cases, and a relatively free word order (Ebert, 1976). A characteristic
of German are separable verb prefixes, leading to 1 : n mappings in the statistical alignment with EN.
</bodyText>
<page confidence="0.997977">
12
</page>
<figureCaption confidence="0.999277">
Figure 1: Workflow
</figureCaption>
<bodyText confidence="0.996673454545454">
Unlike EN and DE, IS is a North Germanic language. It is assumed to be conservative, with relatively
free word order with both OV and VO patterns and a rich morphology that leads to many 1 : n alignments
with EN, e.g., for suffixed definite articles; we thus expect special challenges for annotation projection
under conditions with limited training data.
Different from the old languages, EN developed a rigid word order and a largely reduced morphol-
ogy. A direct adaptation of an existing English parser to (hyperlemmatized) OE, IS or DE is thus not
promising. Therefore, we employ an approach based on annotation projection.
The corpus data we used consists of parsed bible fragments from manually annotated corpora, mostly
the gospels of Matthew (Mt), Mark (Mr), John (J) and Luke (L), from which we drew a test set of 147
sentences and a training set of 437 sentences for every language.
ME and OE The Penn-Helsinki Parsed Corpus of Middle English (PPCME2)1 and the York-Toronto-
Helsinki Parsed Corpus of Old English Prose (Taylor et al., 2003b, YCOE) use a variant of the PTB
annotation schema (Taylor et al., 2003a). YCOE contains the full West Saxon Gospel, but PPCME2
contains only a small fragment of a Wycliffite gospel of John, the ME data is thus complemented
with parts of Genesis (G) and Numbers (N).
IS The Icelandic Parsed Historical Corpus (R¨ognvaldsson et al., 2012, IcePaHC) is annotated follow-
ing YCOE with slight modifications for specifics of IS. We use the gospel of John from Oddur
Gottsk´alksson’s New Testament, a direct translation from Luther.
DE The Parsed Corpus of Early New High German2 contains three gospels from Luther’s Septembertes-
tament (1522). As an IcePaHC side-project, it adapts the IS annotation scheme.
EN For EN, we use the ESV Bible.3 Due to a moderate number of archaisms, it is particularly well-
suited for automated annotation.
</bodyText>
<sectionHeader confidence="0.995413" genericHeader="method">
3 Experimental setup
</sectionHeader>
<bodyText confidence="0.999961857142857">
We study the projection of dependency syntax, as it is considered particularly suitable for free word-order
languages like IS, OE and DE. The existing constituent annotations were thus converted with standard
tools for PTB conversion. Figure 1 summarizes the experimental setup.
For annotating EN, we created dependency versions of WSJ and Brown sections of the PTB with
the LTH Converter (Johansson and Nugues, 2007). We trained Malt 1.7.2 (Nivre, 2003), optimized its
features with MaltOptimizer (Ballesteros and Nivre, 2012), and parsed the EN bible using the resulting
feature model.
</bodyText>
<footnote confidence="0.999924">
1http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html
2http://enhgcorpus.wikispaces.com
3http://esv.org
</footnote>
<page confidence="0.999199">
13
</page>
<bodyText confidence="0.999765433333333">
The ME, OE, DE and IS datasets were word aligned with EN using GIZA++ (Och and Ney, 2003).
1 : n alignments were resolved to the most probable 1 : 1 mapping. During annotation projection,
we assume that the aligned words represent the respective heads for the remaining n − 1 words. These
dependent words are assigned the dependency relation FRAG to the word that got the highest score in
the translation table. This solution solves, among others, the problem of separable verb prefixes in DE,
for example, DE ruffen with prefix an would be aligned to English word call: As P(”call”|”an”) &lt;
P(”call”|”ruffen”), the syntactic information of ”call” will be projected to ”ruffen” and ”an” will be
its dependent labeled with ”FRAG”. The projected dependency trees were checked on well-formedness,
sentences with cycles were dismissed from the data set.
We formed training sets containing 437 sentences for ME, OE, DE, IS. Monolingual data sets were
combined into bi-, tri- or quadrilingual training data sets with a simple concatenation, thereby creating
less sparse, but more heterogeneous training data sets. For every language, test data was taken from J,
174 sentences per language.
We used the projected dependencies to train fMalt (Spreyer et al., 2010), a fragment-aware dependency
parser, in order to maximize the gain of information from incomplete projections.
In our setting, fMalt used two features, POS and hyperlemmas.
POS The tagsets of the historical corpora originate in PTB, but show incompatible adaptations to the
native morphosyntax. Tagset extensions on grammatical case in OE, IS and DE were removed and
language-specific extensions for auxiliaries and modal verbs were leveled, in favor of a common,
but underspecified tagset for all four languages. As these generalized tags preserve information not
found in EN, they were fed into the parser.
(hyper-)lemma Lexicalization is utterly important for the dependency parsing (Kawahara and Uchi-
moto, 2007), but to generalize over specifics of historical language varieties, hyperlemmatization
needs to be performed. Similar to Zeman and Resnik (2008), we use projected English words as
hyperlemmas and feed them into the parser. Hyperlemmatization against a closely related languages
is acceptable as we can expect that the syntactic properties of words are likely to be similar.
The projected annotations were then evaluated against dependency annotations created analoguously to
the EN annotations from manual PTB-style constituency syntax. As LTH works exclusively on PTB
data, the historical corpora were converted with its antecessor Penn2Malt4 using user-defined head-rules
(Yamada and Matsumoto, 2003).
</bodyText>
<sectionHeader confidence="0.981728" genericHeader="method">
4 Evaluation results
</sectionHeader>
<table confidence="0.982914857142857">
baseline AUAS worst model AUAS best model AUAS
UAS +1 +2 +1 +2 +3
ME .60 +DE +.00n.s. +DE+IS -.01n.s. +OE +.01n.s. +OE+IS +.01n.s. -.00n.s.
OE .31 +IS -.00n.s. +DE+IS -.02n.s. +DE +.02n.s. +ME+DE +.00n.s. +.02n.s.
DE .41 +OE +.02n.s. +OE+IS +.03∗ +ME +.04∗∗∗ +ME+IS +.03∗ +.04∗∗
IS .32 +IS -.02n.s. +DE+OE -.02n.s. +ME +.00n.s. +ME+DE -.01n.s. -.04∗∗
(a) trained on target and related language(s)
baseline AUAS worst model AUAS best model AUAS
UAS 1 2 1 2 3
ME .60 OE -.09∗∗∗ DE-IS -.01n.s IS -.05∗∗∗ IS+OE -.02n.s. -.02n.s.
OE .31 DE -.03∗ ME-DE -.01n.s. ME -.02n.s. ME+IS -.01n.s. -.00n.s.
DE .41 OE -.01n.s. OE-IS +.02n.s. IS +.02n.s. IS+ME +.05∗∗∗ +.04∗∗
IS .32 OE -.07∗∗∗ DE-OE -.02n.s. ME -.06∗∗∗ ME+DE -.02n.s. -.04∗∗
(b) trained on related language(s) alone
</table>
<tableCaption confidence="0.999904">
Table 1: Performance of best- and worst-performing parsing models (UAS diff. vs. baseline with X2: ∗ p &lt; .05, ∗∗ p &lt; .01, ∗∗∗ p &lt; .005)
</tableCaption>
<bodyText confidence="0.988415333333333">
We evaluate the unlabeled attachment score (Collins et al., 1999, UAS), i.e., the proportion of tokens in
a sentence (without punctuation) that are assigned the correct head, on test sets of 174 sentences in each
language.
</bodyText>
<footnote confidence="0.994442">
4http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html
</footnote>
<page confidence="0.998541">
14
</page>
<bodyText confidence="0.999941263157894">
As a baseline for the evaluation we take the performance of the parser trained solely on the target
language data. As shown in Tab. 1 (second col.), the UAS scores mirror both the diachronic relatedness
(ME&gt;DE&gt;IS), as well as the relative loss of morphology (ME&gt;DE&gt;IS/OE), indicating that diachronic
relatedness may not be the only factor licensing the applicability of the annotation projection scenario
(H3). It is also important, though, to keep in mind that the OE and IS translations of the Bible had
considerable influence of Latin syntax, whereas DE and ME translations aimed for a language easy to
understand.
Table 1a gives the best and worst results for the unlabeled attachment score for the parser trained on
target and related language(s) (H1). With the exception of DE, we observed no significant differences in
UAS scores relative to the baseline. DE may benefit from ME because of its more flexible syntax (thus
closer to ME [and OE] than to Modern English), and from IS because of Luther’s direct influence on the
IS bible. That ME did not mutually benefit from German may be due to the good quality of ME annota-
tion projections (resulting from its proximity to EN). Parsers trained on trilingual and quadrilingual sets
exhibited no improvement over the bilingual sets. Taken together, we found no positive effect of using
additional training data from language stages diachronically separated for more than 500 years (e.g.,
OE/ME), but also, we did not find a negative effect among the West Germanic languages. If additional
training material is carefully chosen among particularly closely related varieties, however, the DE effect
can be replicated, and then, including related language data to training data in the target language can
improve parsing performance.
While in our setting, training data from related languages may (but does not have to) improve a parser
training if training data for the target language is available, it may very well be employed fruitfully if
no training data for the target language is available (H2): Table 1b shows that, unsurprisingly, parsers
trained only on one related language had the lowest performance in the experiment, so using multiple
train languages seems to compensate language-specific idiosyncrasies. The best-performing parsing
models trained on two or more related languages achieved a performance not significantly worse (if not
better) than models being trained on target language data. This effect extends to all languages except
for IS and indicates that a careful choice of additional training data from related varieties may facilitate
annotation projection. Equally important (and valid across all languages) is that none of the models
trained on one language outperformed any of the model trained on two languages. Using training data
from two related languages doesn’t seem to hurt performance in our setting. Adding a third language
did not yield systematic improvements, the scores for trilingual models are in the range of the bilingual
models.
Again, DE is exceptionally good, benefitting from being a direct source of the IS translation as well as
structurally comparable to ME. In both settings, the worst-performing language is IS, with a significant
drop in annotation projection quality with Western Germanic material added, indicating that diachronic
distance between Northern and Western Germanic languages limits the applicability of (H2), thereby
supporting (H3).
Taken together, our results indicate
</bodyText>
<listItem confidence="0.996075">
1. a significant positive effect for the Western Germanic languages (ME, OE, DE) for (H2), and
2. a significant negative effect for Western and Northern Germanic languages (IS) for (H2)
</listItem>
<bodyText confidence="0.99971">
As a tentative hypothesis, one may speculate that languages separated for 1000 years (OE-IS) or more
are too remote from each other to provide helpful background information, but that languages separated
within the last 750 years (ME-DE) or less are still sufficiently close. This novel assumption may provide
a guideline for future efforts to project annotations among related languages, and is thus of immense
practical relevance for developing future NLP tools for historical and less-resourced language varieties.
Ultimately, one may formulate rules of best practice like the following:
</bodyText>
<listItem confidence="0.963661666666667">
• If no syntactic annotations for a target language are available, annotation projection among closely
related languages may be a solution. Even with limited amounts of parallel data, diachronic dis-
tances of more than 500 years can be successfully bridged (EN/ME, baseline).
</listItem>
<page confidence="0.929811">
15
</page>
<bodyText confidence="0.850291">
• If no syntactic annotations for a target language are available, a parser trained on hyperlemmatized
corpora in two languages may yield a performance comparable to a parser trained on small amounts
of target data. A parser trained on hyperlemmatized monolingual data may be significantly worse
(H2).
• The sparsity of parallel text to conduct annotation projection and train a (hyperlemmatized) parser
can only be compensated by adding parallel data from one related language if these are closely
diachronically related (with a separation being less than, say, 500 years ago) and at a similar de-
velopmental stage (DE/ME, H1). Adding data from multiple, equally remote languages does not
necessarily improve the results further.
At the current state, such recommendations would be premature, they require deeper investigation, but
with the confirmation of (H2) and (H3), we can now motivate larger-scale efforts to compile a massive
parallel corpus of historical Germanic language varieties as a basis for subsequent studies. Initial steps
towards this goal are described in the following section.
</bodyText>
<sectionHeader confidence="0.67953" genericHeader="method">
5 Towards a massive parallel corpus of historical Germanic languages
</sectionHeader>
<bodyText confidence="0.9999791875">
With the long-term goal to systematically assess the impact of the factor of diachronic proximity, we
focus on annotation projection among the Germanic languages as test field. The Germanic languages
represent a particularly well-resourced, well-documented and well-studied language family which devel-
opment during the last 1800 years is not only well-explored, but also documented with great amounts
of (parallel) data, ranging from the 4th century Gothic bible over a wealth of Bible translations since
the middle ages to the modern age of communication with its abundance of textual resources for even
marginal varieties. Motivated from our experiment, we thus began to compile a parallel corpus of his-
torical and dialectal Germanic language varieties. Primary source data for a massive parallel corpus of
historical varieties of any European language is mostly to be drawn from the Bible and related literature.
The Bible is the single most translated book in the world and available in a vast majority of world lan-
guages. It is also often the case that there are several biblical translation existing for a language. Bible
data also represents the majority of parallel data available for historical Germanic languages, and for the
case of OS and OHG, gospel harmonies represent even the majority of data currently known. Beyond
this, the corpus includes Bible excerpts and paraphrases from all Germanic languages and their major
historical stages.
Tab. 2 gives an overview over the current status of the Parallel Bible Corpus. At the moment, 271 texts
with about 38.4M tokens have been processed, converted from their original format and verse-aligned
according to their original markup or with a lexicon-supported geometric sentence aligner (T´oth et al.,
2008). In the table, ‘text’ means any document ranging from a small excerpt such as the Lord’s Prayer
(despite their marginal size valuable to develop algorithms for normalization/[hyper]lemmatization) over
gospel harmonies and paraphrases to the entire bible that has been successfully aligned with Bible verses.
The compiled corpus, excerpts and fragments for all Germanic languages marked up with IDs for verses,
chapters and books. For data representation, we employed an XML version of the CES-scheme de-
veloped by Resnik et al. (1997). Having outgrown the scale of Resnik’s earlier project by far, we are
currently in transition to TEI P5.
As it is compiled from different sources, the corpus cannot be released under a free or an academic
license. It contains material without explicit copyright statement, with proprietary content (e.g., from
existing corpora), or available for personal use only. Instead, we plan to share the extraction and con-
version scripts we used. For the experiments we aim to prepare, we focus on primary data, the texts in
this collection are not annotated. Where annotations are available from other corpora or can be produced
with existing tools, however, these annotated versions will be aligned with the Bibles and included in
subsequent experiments.
</bodyText>
<page confidence="0.994476">
16
</page>
<table confidence="0.999622333333333">
after 1800- 1600- 1400- 1100- before
1900 1900 1800 1600 1400 1100
West Germanic
English 2 2 2 6 3 (+2) 1
Pidgin/Creol 2
Scots (6) (1)
Frisian 2 (+8) (12)
Dutch 4 1 5 (1)
L. Franconian (47) (21)
Afrikaans 3
German 3 1 (19) 1 (+4) 1 (+1) 1
dialects 3 (+2)
Yiddish 1
Low German 3 (+18) (66) (2) 1
Plautdietsch 2
Danish 1 North &amp; East Germanic
Swedish 3 (3) (1)
Bokm˚al 2
Nynorsk 2
Icelandic 1 1
Faroese 1
Norn (2)
Gothic 1
tokens 21.8M 3.2M 2.7M 9.2M 1.2M 0.2M
</table>
<tableCaption confidence="0.9475775">
Table 2: Verse-aligned texts in the Germanic parallel Bible corpus (parentheses indicate marginal frag-
ments with less than 50,000 tokens)
</tableCaption>
<sectionHeader confidence="0.894736" genericHeader="conclusions">
6 Summary and outlook
</sectionHeader>
<bodyText confidence="0.992567032258065">
This paper describes a motivational experiment on annotation projection, or more precisely, strategies
to compensate data sparsity (the lack of parallel data) with material from related, but heterogeneous
varieties to facilitate cross-language parser adaptation for low-resource historical languages. We used a
fragment-aware dependency parser trained on annotation projections from ESV Bible to four historical
languages.
Our results indicate a lexicalized fragment-aware parser trained on a small amount of annotation pro-
jections can yield good results on closely related languages. In a situation of the absence of training
data for the target language (or, for example, in the situation where there is no parallel corpora for the
target language), a hyperlemmatized parser trained on (projected) annotations from two or more related
languages is likely to outperform a parser trained on a single related language.
We achieved statistically significant differences in parser performance trained on (a) target language
data, and (b) target language and data from related varieties, resp. (c) data from related varieties only.
These indicate that closely related languages (say, with a common ancestor about 750 years ago, such as
DE and ME) have some potential to compensate sparsity of parallel data in the target variety, wheres this
potential does not seem to exist for more remotely related languages (say, with a common ancestor more
than 1000 years ago such as OE and IS).
The experimental results revealed that the parser performance can, indeed, be improved by means of
including a related language to the training data, but we had a significant effect for only one language
under consideration, indicating that the diachronic proximity of the languages considered was possibly
too large, and thereby motivating subsequent experiments, and in particular, the creation of a larger
parallel corpus of historical Germanic language varieties. We described initial steps in the compilation
of this corpus.
Our experiment raises a number of open issues that are to be pursued in subsequent studies:
1. Our setup has a clear bias towards English (in the annotation schemes used and the source annota-
tions), and parser performance was strongly affected by the syntactic difference between the target
language and Modern English from which the syntactic dependencies were projected, indicating the
relevance of diachronic relatedness as well as the developmental state of a related language. Sub-
sequent experiments will hence address the inclusion of richer morphological features, projection
from other languages and evaluation against syntactic annotations according to other schemes not
derived from the Penn Treebank, as currently available, for example, for Old High German, Old
Norse, and Gothic.
</bodyText>
<page confidence="0.997807">
17
</page>
<bodyText confidence="0.996682326530613">
2. The hyperlemmatization in our approach was achieved through alignment/SMT, and a similar
lexically-oriented approach has been suggested by (Zeman and Resnik, 2008). Alternative strate-
gies more suitable for scenarios with limited amounts of training data may include the use of ortho-
graphical normalization techniques (Bollmann et al., 2011) or substring-based machine translation
(Neubig et al., 2012) and are also subject to on-going research. We assume that SMT-based hyper-
lemmatization introduces more noise than these strategies, so that it is harder to achieve statistically
significant results. Our findings are thus likely to remain valid regardless of the hyperlemmatization
strategy. This hypothesis is, however, yet to be confirmed in subsequent studies.
3. Our experiment mostly deals with data translated from (or at least informed by) the Latin Vulgate.
Our data may be biased by translation strategies which evolved over time, from very literal trans-
lations (actually, glossings) of Latin texts in the early middle ages to Reformation-time translations
aiming to grasp the intended meaning rather than to preserve the original formulation. A focus on
classical languages is, however, inherent to the parallel material in our domain. A representative
investigation of annotation projection techniques thus requires the consideration of quasi-parallel
data along with parallel data. This can be found in the great wealth of medieval religious literature,
with Bible paraphrases, gospel harmonies, sermons and homilies as well as poetic and prose adap-
tations of biblical motives. The parallel corpus of Germanic languages thus needs to be extended
accordingly.
4. One may wonder how the annotation projection approach performs in comparison to direct applica-
tions of modern language NLP tools to normalized historical data language (Scheible et al., 2011).
While it is unlikely that such an approach could scale beyond closely related varieties, success-
ful experiments on the annotation of normalized historical language have been reported, although
mostly focused on token-level annotations (POS, lemma, morphology) of language stages which
syntax does not greatly deviate from modern rules (Rayson et al., 2007; Pennacchiotti and Zan-
zotto, 2008; Kestemont et al., 2010; Bollmann, 2013). For the annotation of more remotely related
varieties with more drastic differences in word order rigidity or morphology as considered here,
however, projection techniques are more promising as they have been successfully applied to un-
related languages, as well, but still benefit from diachronic proximity, cf. Meyer (2011) for the
projection-based morphological analysis of Modern and Old Russian.
The goal of our experiment was not to achieve state-of-the-art performance, but to show whether back-
ground material from related languages with different degrees of diachronic distance can help to com-
pensate data sparsity, in this case with an experiment on annotation projection. This hypothesis could be
confirmed and we found effects that – even on the minimal amounts of data considered for this study –
indicated statistically significant improvements.
It is thus to be expected that even greater improvements can be achieved by considering more closely
related pairs of languages, with greater amounts of data. The further exploration of this hypothesis is
the driving force behind our efforts to compile a massive corpus of parallel and quasi-parallel texts for
all major varieties of synchronic and historical Germanic languages. Algorithms successfully tested in
this context can be expected to be applicable to other scenarios in which, e.g., well-researched modern
languages may be employed to facilitate the creation of NLP tools for less-ressourced, related languages.
Our efforts are thus not specific to historical languages.
As the diachronic development and the diversification of the Germanic languages is well-documented
in this body of data, and the linguistic processes involved are well-researched, this data set represents an
extraordinarily valuable resource for philological and comparitve studies as well as Natural Language
Processing. In particular, we are interested in developing algorithms that explore and exploit the variable
degree of diachronic relatedness found between the languages in our sample. At the same time, we
cooperate with researchers from philology, historical and comparative linguistics, which research on
intertextuality, diachronic lexicology, phonology, morphology and syntax we aim to support with NLP
tools developed on the basis of this body of parallel text.
</bodyText>
<page confidence="0.998639">
18
</page>
<sectionHeader confidence="0.990196" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999817285714286">
Miguel Ballesteros and Joakim Nivre. 2012. Maltoptimizer: A system for maltparser optimization. In Nicoletta
Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph
Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International
Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language
Resources Association (ELRA).
Marcel Bollmann, Florian Petran, and Stefanie Dipper. 2011. Rule-based normalization of historical texts. In Pro-
ceedings of the Workshop on Language Technologies for Digital Humanities and Cultural Heritage (LaTeCH-
2011), pages 34–42, Hissar, Bulgaria, September.
Marcel Bollmann. 2013. POS tagging for historical texts with sparse training data. In Proceedings of the 7th
Linguistic Annotation Workshop and Interoperability with Discourse, pages 11–18, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Michael Collins, Lance Ramshaw, Jan Hajiˇc, and Christoph Tillmann. 1999. A statistical parser for czech. In
Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational
Linguistics, pages 505–512. Association for Computational Linguistics.
Michael Cummings. 2010. An Introduction to the Grammar of Old English: A Systemic Functional Approach.
Functional Linguistics. Equinox Publishing Limited.
Robert P. Ebert. 1976. Infinitival complement constructions in Early New High German. Linguistische Arbeiten.
De Gruyter.
Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In
Proceedings of NODALIDA 2007, pages 105–112, Tartu, Estonia, May 25-26.
Daisuke Kawahara and Kiyotaka Uchimoto. 2007. Minimally lexicalized dependency parsing. In Proceedings
of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 205–208.
Association for Computational Linguistics.
Mike Kestemont, Walter Daelemans, and Guy De Pauw. 2010. Weigh your words: Memory-based lemma-retrieval
for Middle Dutch literary texts. In CLIN 2010. Computational linguistics in the Netherlands 20, Utrecht, The
Netherlands, May.
Roland Meyer. 2011. New wine in old wineskins? Tagging Old Russian via annotation projection from modern
translations. Russian linguistics, 35(2):267–281.
Graham Neubig, Taro Watanabe, Shinsuke Mori, and Tatsuya Kawahara. 2012. Machine translation without words
through substring alignment. In Proceedings of the 50th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 165–174, Jeju Island, Korea, July. Association for Computational
Linguistics.
Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT).
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational linguistics, 29(1):19–51.
Marco Pennacchiotti and Fabio Massimo Zanzotto. 2008. Natural language processing across time: An empirical
investigation on italian. In Advances in Natural Language Processing, pages 371–382. Springer.
Paul Rayson, Dawn Archer, Alistair Baron, Jonathan Culpeper, and Nicholas Smith. 2007. Tagging the Bard:
Evaluating the accuracy of a modern POS tagger on Early Modern English corpora. In Proceedings of the 4th
Corpus Linguistics Conference (CL-2007), Birmingham, UK.
Philip Resnik, Mari Broman Olsen, and Mona Diab. 1997. Creating a parallel corpus from the book of 2000
tongues. In Proc. of the Text Encoding Initiative 10th Anniversary User Conference (TEI-10).
Eir´ıkur R¨ognvaldsson, Anton Karl Ingason, Einar Freyr Sigurdhsson, and Joel Wallenberg. 2012. The Icelandic
Parsed Historical Corpus (IcePaHC). In LREC, pages 1977–1984.
Silke Scheible, Richard J. Whitt, Martin Durrell, and Paul Bennett. 2011. Evaluating an ’off-the-shelf’ POS-
tagger on Early Modern German text. In Proceedings of the 5th ACL-HLT Workshop on Language Technology
for Cultural Heritage, Social Sciences, and Humanities (LaTeCH-2011), pages 19–23, Portland, OR, USA,
June.
</reference>
<page confidence="0.98379">
19
</page>
<reference confidence="0.999182722222222">
Kathrin Spreyer, Lilja Ovrelid, and Jonas Kuhn. 2010. Training parsers on partial trees: A cross-language com-
parison. In Proc. of the 7th International Conference on Language Resources and Evaluation (LREC-2010),
Valletta, Malta, May.
Ann Taylor, Mitchell Marcus, and Beatrice Santorini. 2003a. The Penn treebank: an overview. In Treebanks,
pages 5–22. Springer.
Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003b. The york-toronto-helsinki parsed corpus
of old english prose. University of York.
Krisztina T´oth, Rich´ard Farkas, and Andr´as Kocsor. 2008. Sentence alignment of hungarian-english parallel
corpora using a hybrid algorithm. Acta Cybern., 18(3):463–478, January.
C. Trips. 2002. From OV to VO in Early Middle English. Linguistics today. John Benjamins Pub.
A. van Kemenade and B. Los. 2009. The Handbook of the History of English. Blackwell Handbooks in Linguis-
tics. John Wiley &amp; Sons.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In
Proceedings of IWPT. Vol. 3. 2003.
David Yarowski and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection
across aligned corpora. In Proceedings of NAACL 2001, pages 200–207.
Daniel Zeman and Philip Resnik. 2008. Cross-language parser adaptation between related languages. In IJCNLP-
08 Workshop on NLP for Less Privileged Languages, pages 35–41, Hyderabad, India, Jan.
</reference>
<page confidence="0.99482">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000618">
<title confidence="0.847378">Diachronic proximity vs. data sparsity in cross-lingual parser projection. A case study on Germanic</title>
<author confidence="0.939811">Maria</author>
<affiliation confidence="0.988337">Goethe University Frankfurt</affiliation>
<email confidence="0.94545">sukharev@em.uni-frankfurt.de</email>
<author confidence="0.981065">Christian</author>
<affiliation confidence="0.994268">Goethe University Frankfurt</affiliation>
<email confidence="0.992216">chiarcos@em.uni-frankfurt.de</email>
<abstract confidence="0.998169430379747">For the study of historical language varieties, the sparsity of training data imposes immense problems on syntactic annotation and the development of NLP tools that automatize the process. In this paper, we explore strategies to compensate the lack of training data by including data from related varieties in a series of annotation projection experiments from English to four old Germanic languages: On dependency syntax projected from English to one or multiple language(s), we train a fragment-aware parser trained and apply it to the target language. For parser training, we consider small datasets from the target language as a baseline, and compare it with models trained on larger datasets from multiple varieties with different degrees of relatedness, thereby balancing sparsity and diachronic proximity. Our experiments show (a) that including related language data to training data in the target language can improve parsing performance, (b) that a parser trained on data from two related languages (and none from the target language) can reach a performance that is statistically not significantly worse than that of a parser trained on the projections to the target language, and (c) that both conclusions holds only among the three most closely related languages under consideration, but not necessarily the fourth. The experiments motivate the compilation of a larger parallel corpus of historical Germanic varieties as a basis for subsequent studies. 1 Background and motivation We describe an experiment on annotation projection (Yarowski and Ngai, 2001) between different Germanic languages, resp., their historical varieties, with the goal to assess to what extent sparsity of parallel data can be compensated by material from varieties related to the target variety, and studying the impact of diachronic proximity onto such applications. Statistical NLP of historical language data involves general issues typical for low-resource languages (the lack of annotated corpora, data sparsity, etc.), but also very specific challenges such as lack of standardized orthography, unsystematized punctuation, and a considerable degree of morphological variation. At the same time, historical languages can be viewed as variants of their modern descendants rather than entirely independent languages, a situation comparable to low-resource languages for which a diachronically related major language exists. Technologies for the cross-lingual adaptation of NLP tools or training of NLP tools on multiple dialects or language stages are thus of practical relevance to not only historical linguistics, but also to modern low-resource languages. final paper will be published under a Creative Commons Attribution 4.0 International Licence (CC-BY), 11 of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and pages Dublin, Ireland, August 23 2014. this context, historical language allows to study the impact of the parameter of relatedas it can be adjusted relatively freely, e.g., by choosing dialects which common ancestor existed just a few generations before rather than languages separated for centuries. A focused study of the impact of diachronic relatedness on projected annotations requires sufficient amounts of parallel texts for major language stages, and comparable annotations as a gold standard for evaluation. In this regard, the Germanic languages provide us with a especially promising sandbox to develop such algorithms due to the abundance of annotated corpora and NLP tools of the modern Germanic languages, most noteably Modern English. We employ annotation projection from EN to Middle English (ME), Old English (OE) and the less closely related Early Modern High German (DE) and Middle Icelandic (IS) for which we possess comparable annotations, and test the following hypotheses: Adding data from related varieties the sparsity target language training data. Data from related languages the lack target language training data. The greater the the better the performance of (H1) and (H2). test these hypotheses in the following setup: (1) Different historical variants are normalized to a consistent standard, e.g., represented by a modern language (Bollmann et al., 2011). emulate hyperlemmatization by English glosses automatically obtained through SMT. (2) We create training data for a fragment-aware dependency parser (Spreyer et al., 2010) using annotation from modern English. (3) and Parser modules are trained on different training data sets, and evaluated against existing gold annotations. In our setting, we enforce data sparsity by using deliberately small training data sets. This is because we emulate the situation of less-documented languages that will be in the focus of subsequent experiments, namely, Old High German and Old Saxon, which are relatively poorly documented. We do hope, however, that scalable NLP solutions can be developed if we add background information from their descendants (Middle/Early Modern High German, Middle/Modern Low German), or closely related, and better documented varieties (Old English, Middle Dutch). Hence, the goal of our experiment is not to develop state-of-the-art parsers, but to detect statistically significant differences in parsing performance. If these can be confirmed, this motivates creating a larger corpus of parallel texts in Germanic languages as a basis for subsequent studies and more advanced, projection-based technologies for older and under-resourced Germanic languages. 2 Languages and corpus data We use parallel biblical texts in Old English (OE), Middle English (ME), Middle Icelandic (IS) and Early Modern High German (DE). This selection is determined by the availability of syntactically annotated corpora with closely related annotation schemes. As these schemes are derived from the Penn TreeBank (PTB) bracketing guidelines (Taylor et al., 2003a), we decided to use Modern English (EN) as a source for the projections. Germanic languages from Proto-Germanic as a common ancestor. OE and Old High German separated in the 5th c. The antecessor of IS separated from this branch about 500 years earlier. Among Germanic languages, great differences emerged, but most languages developed similarly towards a loss of morphology and a more rigid syntax, a tendency particularly prevalent in EN. As compared to this, OE had a relatively free OV word order, with grammatical roles conveyed through morphological markers. The OE case marking system distinguished four cases, but eventually collapsed</abstract>
<note confidence="0.9200405">during ME, resulting in a strict strict VO word order in EN (Trips, 2002; van Kemenade and Los, 2009; Cummings, 2010). Unlike EN, DE preserved four cases, and a relatively free word order (Ebert, 1976). A characteristic German are separable verb prefixes, leading to : in the statistical alignment with EN. 12 Figure 1: Workflow</note>
<abstract confidence="0.936608808362369">Unlike EN and DE, IS is a North Germanic language. It is assumed to be conservative, with relatively word order with both OV and VO patterns and a rich morphology that leads to many : with EN, e.g., for suffixed definite articles; we thus expect special challenges for annotation projection under conditions with limited training data. Different from the old languages, EN developed a rigid word order and a largely reduced morphology. A direct adaptation of an existing English parser to (hyperlemmatized) OE, IS or DE is thus not promising. Therefore, we employ an approach based on annotation projection. corpus data used consists of parsed bible fragments from manually annotated corpora, mostly the gospels of Matthew (Mt), Mark (Mr), John (J) and Luke (L), from which we drew a test set of 147 sentences and a training set of 437 sentences for every language. and OE Penn-Helsinki Parsed Corpus of Middle English and the York-Toronto- Helsinki Parsed Corpus of Old English Prose (Taylor et al., 2003b, YCOE) use a variant of the PTB annotation schema (Taylor et al., 2003a). YCOE contains the full West Saxon Gospel, but PPCME2 contains only a small fragment of a Wycliffite gospel of John, the ME data is thus complemented with parts of Genesis (G) and Numbers (N). Icelandic Parsed Historical Corpus (R¨ognvaldsson et al., 2012, IcePaHC) is annotated following YCOE with slight modifications for specifics of IS. We use the gospel of John from Oddur Gottsk´alksson’s New Testament, a direct translation from Luther. Parsed Corpus of Early New High contains three gospels from Luther’s Septembertestament (1522). As an IcePaHC side-project, it adapts the IS annotation scheme. EN, we use the ESV Due to a moderate number of archaisms, it is particularly wellsuited for automated annotation. 3 Experimental setup study the projection of as it is considered particularly suitable for free word-order languages like IS, OE and DE. The existing constituent annotations were thus converted with standard tools for PTB conversion. Figure 1 summarizes the experimental setup. we created dependency versions of WSJ and Brown sections of the PTB with the LTH Converter (Johansson and Nugues, 2007). We trained Malt 1.7.2 (Nivre, 2003), optimized its features with MaltOptimizer (Ballesteros and Nivre, 2012), and parsed the EN bible using the resulting feature model. 13 ME, OE, DE and IS datasets were aligned EN using GIZA++ (Och and Ney, 2003). : were resolved to the most probable : 1 During assume that the aligned words represent the respective heads for the remaining These words are assigned the dependency relation the word that got the highest score in the translation table. This solution solves, among others, the problem of separable verb prefixes in DE, example, DE prefix be aligned to English word As the syntactic information of be projected to be its dependent labeled with ”FRAG”. The projected dependency trees were checked on well-formedness, sentences with cycles were dismissed from the data set. formed sets 437 sentences for ME, OE, DE, IS. Monolingual data sets were combined into bi-, trior quadrilingual training data sets with a simple concatenation, thereby creating sparse, but more heterogeneous training data sets. For every language, data taken from J, 174 sentences per language. We used the projected dependencies to train fMalt (Spreyer et al., 2010), a fragment-aware dependency parser, in order to maximize the gain of information from incomplete projections. In our setting, fMalt used two features, POS and hyperlemmas. tagsets of the historical corpora originate in PTB, but show incompatible adaptations to the native morphosyntax. Tagset extensions on grammatical case in OE, IS and DE were removed and language-specific extensions for auxiliaries and modal verbs were leveled, in favor of a common, but underspecified tagset for all four languages. As these generalized tags preserve information not found in EN, they were fed into the parser. is utterly important for the dependency parsing (Kawahara and Uchimoto, 2007), but to generalize over specifics of historical language varieties, hyperlemmatization needs to be performed. Similar to Zeman and Resnik (2008), we use projected English words as hyperlemmas and feed them into the parser. Hyperlemmatization against a closely related languages is acceptable as we can expect that the syntactic properties of words are likely to be similar. projected annotations were then dependency annotations created analoguously to the EN annotations from manual PTB-style constituency syntax. As LTH works exclusively on PTB the historical corpora were converted with its antecessor using user-defined head-rules (Yamada and Matsumoto, 2003). 4 Evaluation results baseline worst model best model UAS +1 +2 +1 +2 +3 ME .60 +DE +DE+IS +OE +OE+IS OE .31 +IS +DE+IS +DE +ME+DE DE .41 +OE +OE+IS +ME +ME+IS IS .32 +IS +DE+OE +ME +ME+DE trained on and language(s) baseline worst model best model UAS 1 2 1 2 3 ME .60 OE DE-IS IS IS+OE OE .31 DE ME-DE ME ME+IS DE .41 OE OE-IS IS IS+ME IS .32 OE DE-OE ME ME+DE trained on related language(s) 1: of bestand worst-performing parsing models (UAS diff. vs. baseline with &lt; &lt; &lt; We evaluate the unlabeled attachment score (Collins et al., 1999, UAS), i.e., the proportion of tokens in a sentence (without punctuation) that are assigned the correct head, on test sets of 174 sentences in each language. nivre/research/Penn2Malt.html 14 a the evaluation we take the performance of the parser trained solely on the target language data. As shown in Tab. 1 (second col.), the UAS scores mirror both the diachronic relatedness as well as the relative loss of morphology indicating that diachronic relatedness may not be the only factor licensing the applicability of the annotation projection scenario (H3). It is also important, though, to keep in mind that the OE and IS translations of the Bible had considerable influence of Latin syntax, whereas DE and ME translations aimed for a language easy to understand. Table 1a gives the best and worst results for the unlabeled attachment score for the parser trained on and related language(s) With the exception of DE, we observed no significant differences in UAS scores relative to the baseline. DE may benefit from ME because of its more flexible syntax (thus closer to ME [and OE] than to Modern English), and from IS because of Luther’s direct influence on the IS bible. That ME did not mutually benefit from German may be due to the good quality of ME annotation projections (resulting from its proximity to EN). Parsers trained on trilingual and quadrilingual sets no improvement over the bilingual sets. Taken together, we found positive effect using additional training data from language stages diachronically separated for more than 500 years (e.g., but also, we did find negative effect among the West Germanic languages. If additional training material is carefully chosen among particularly closely related varieties, however, the DE effect can be replicated, and then, including related language data to training data in the target language can improve parsing performance. While in our setting, training data from related languages may (but does not have to) improve a parser if training data for the target language is available, it may very well be employed fruitfully training data for the target language is available Table 1b shows that, unsurprisingly, parsers trained only on one related language had the lowest performance in the experiment, so using multiple train languages seems to compensate language-specific idiosyncrasies. The best-performing parsing trained on more related languages achieved a performance not significantly worse (if not better) than models being trained on target language data. This effect extends to all languages except IS and indicates that a choice additional training data from related varieties may facilitate projection. Equally important (and valid across all languages) is that the models trained on one language outperformed any of the model trained on two languages. Using training data from two related languages doesn’t seem to hurt performance in our setting. Adding a third language did not yield systematic improvements, the scores for trilingual models are in the range of the bilingual models. Again, DE is exceptionally good, benefitting from being a direct source of the IS translation as well as structurally comparable to ME. In both settings, the worst-performing language is IS, with a significant drop in annotation projection quality with Western Germanic material added, indicating that diachronic between Northern and Western Germanic languages limits the applicability of thereby Taken together, our results indicate a significant positive effect for the Western Germanic languages (ME, OE, DE) for and a significant negative effect for Western and Northern Germanic languages (IS) for As a tentative hypothesis, one may speculate that languages separated for 1000 years (OE-IS) or more are too remote from each other to provide helpful background information, but that languages separated within the last 750 years (ME-DE) or less are still sufficiently close. This novel assumption may provide a guideline for future efforts to project annotations among related languages, and is thus of immense practical relevance for developing future NLP tools for historical and less-resourced language varieties. Ultimately, one may formulate rules of best practice like the following: • If no syntactic annotations for a target language are available, annotation projection among closely related languages may be a solution. Even with limited amounts of parallel data, diachronic distances of more than 500 years can be successfully bridged (EN/ME, baseline). 15 • If no syntactic annotations for a target language are available, a parser trained on hyperlemmatized corpora in two languages may yield a performance comparable to a parser trained on small amounts of target data. A parser trained on hyperlemmatized monolingual data may be significantly worse (H2). • The sparsity of parallel text to conduct annotation projection and train a (hyperlemmatized) parser only be compensated by adding parallel data from language if these are closely related (with a separation being less than, say, 500 years ago) a similar developmental stage (DE/ME, H1). Adding data from multiple, equally remote languages does not necessarily improve the results further. At the current state, such recommendations would be premature, they require deeper investigation, but the confirmation of we can now motivate larger-scale efforts to compile a massive parallel corpus of historical Germanic language varieties as a basis for subsequent studies. Initial steps towards this goal are described in the following section. 5 Towards a massive parallel corpus of historical Germanic languages With the long-term goal to systematically assess the impact of the factor of diachronic proximity, we focus on annotation projection among the Germanic languages as test field. The Germanic languages represent a particularly well-resourced, well-documented and well-studied language family which development during the last 1800 years is not only well-explored, but also documented with great amounts of (parallel) data, ranging from the 4th century Gothic bible over a wealth of Bible translations since the middle ages to the modern age of communication with its abundance of textual resources for even marginal varieties. Motivated from our experiment, we thus began to compile a parallel corpus of historical and dialectal Germanic language varieties. Primary source data for a massive parallel corpus of historical varieties of any European language is mostly to be drawn from the Bible and related literature. The Bible is the single most translated book in the world and available in a vast majority of world languages. It is also often the case that there are several biblical translation existing for a language. Bible data also represents the majority of parallel data available for historical Germanic languages, and for the case of OS and OHG, gospel harmonies represent even the majority of data currently known. Beyond this, the corpus includes Bible excerpts and paraphrases from all Germanic languages and their major historical stages. Tab. 2 gives an overview over the current status of the Parallel Bible Corpus. At the moment, 271 texts with about 38.4M tokens have been processed, converted from their original format and verse-aligned according to their original markup or with a lexicon-supported geometric sentence aligner (T´oth et al., 2008). In the table, ‘text’ means any document ranging from a small excerpt such as the Lord’s Prayer (despite their marginal size valuable to develop algorithms for normalization/[hyper]lemmatization) over gospel harmonies and paraphrases to the entire bible that has been successfully aligned with Bible verses. The compiled corpus, excerpts and fragments for all Germanic languages marked up with IDs for verses, chapters and books. For data representation, we employed an XML version of the CES-scheme developed by Resnik et al. (1997). Having outgrown the scale of Resnik’s earlier project by far, we are currently in transition to TEI P5. As it is compiled from different sources, the corpus cannot be released under a free or an academic license. It contains material without explicit copyright statement, with proprietary content (e.g., from existing corpora), or available for personal use only. Instead, we plan to share the extraction and conversion scripts we used. For the experiments we aim to prepare, we focus on primary data, the texts in this collection are not annotated. Where annotations are available from other corpora or can be produced with existing tools, however, these annotated versions will be aligned with the Bibles and included in subsequent experiments. 16 after 1800- 1600- 1400- 1100before 1900 1900 1800 1600 1400 1100 West Germanic English 2 2 2 6 3 (+2) 1 Pidgin/Creol 2 Scots (6) (1) Frisian 2 (+8) (12) Dutch 4 1 5 (1) L. Franconian (47) (21) Afrikaans 3 German 3 1 (19) 1 (+4) 1 (+1) 1 dialects 3 (+2) Yiddish 1 Low German 3 (+18) (66) (2) 1 Plautdietsch 2 Danish 1 North &amp; East Germanic Swedish 3 (3) (1) Bokm˚al 2 Nynorsk 2 Icelandic 1 1 Faroese 1 Norn (2) Gothic 1 tokens 21.8M 3.2M 2.7M 9.2M 1.2M 0.2M Table 2: Verse-aligned texts in the Germanic parallel Bible corpus (parentheses indicate marginal fragments with less than 50,000 tokens) 6 Summary and outlook This paper describes a motivational experiment on annotation projection, or more precisely, strategies to compensate data sparsity (the lack of parallel data) with material from related, but heterogeneous varieties to facilitate cross-language parser adaptation for low-resource historical languages. We used a fragment-aware dependency parser trained on annotation projections from ESV Bible to four historical languages. Our results indicate a lexicalized fragment-aware parser trained on a small amount of annotation projections can yield good results on closely related languages. In a situation of the absence of training data for the target language (or, for example, in the situation where there is no parallel corpora for the target language), a hyperlemmatized parser trained on (projected) annotations from two or more related languages is likely to outperform a parser trained on a single related language. We achieved statistically significant differences in parser performance trained on (a) target language data, and (b) target language and data from related varieties, resp. (c) data from related varieties only. These indicate that closely related languages (say, with a common ancestor about 750 years ago, such as DE and ME) have some potential to compensate sparsity of parallel data in the target variety, wheres this potential does not seem to exist for more remotely related languages (say, with a common ancestor more than 1000 years ago such as OE and IS). The experimental results revealed that the parser performance can, indeed, be improved by means of including a related language to the training data, but we had a significant effect for only one language under consideration, indicating that the diachronic proximity of the languages considered was possibly too large, and thereby motivating subsequent experiments, and in particular, the creation of a larger parallel corpus of historical Germanic language varieties. We described initial steps in the compilation of this corpus. Our experiment raises a number of open issues that are to be pursued in subsequent studies: 1. Our setup has a clear bias towards English (in the annotation schemes used and the source annotations), and parser performance was strongly affected by the syntactic difference between the target language and Modern English from which the syntactic dependencies were projected, indicating the relevance of diachronic relatedness as well as the developmental state of a related language. Subsequent experiments will hence address the inclusion of richer morphological features, projection from other languages and evaluation against syntactic annotations according to other schemes not derived from the Penn Treebank, as currently available, for example, for Old High German, Old Norse, and Gothic. 17 The our approach was achieved through alignment/SMT, and a similar lexically-oriented approach has been suggested by (Zeman and Resnik, 2008). Alternative strategies more suitable for scenarios with limited amounts of training data may include the use of orthographical normalization techniques (Bollmann et al., 2011) or substring-based machine translation (Neubig et al., 2012) and are also subject to on-going research. We assume that SMT-based hyperlemmatization introduces more noise than these strategies, so that it is harder to achieve statistically significant results. Our findings are thus likely to remain valid regardless of the hyperlemmatization strategy. This hypothesis is, however, yet to be confirmed in subsequent studies. 3. Our experiment mostly deals with data translated from (or at least informed by) the Latin Vulgate. Our data may be biased by translation strategies which evolved over time, from very literal translations (actually, glossings) of Latin texts in the early middle ages to Reformation-time translations aiming to grasp the intended meaning rather than to preserve the original formulation. A focus on classical languages is, however, inherent to the parallel material in our domain. A representative investigation of annotation projection techniques thus requires the consideration of quasi-parallel data along with parallel data. This can be found in the great wealth of medieval religious literature, with Bible paraphrases, gospel harmonies, sermons and homilies as well as poetic and prose adaptations of biblical motives. The parallel corpus of Germanic languages thus needs to be extended accordingly. 4. One may wonder how the annotation projection approach performs in comparison to direct applications of modern language NLP tools to normalized historical data language (Scheible et al., 2011). While it is unlikely that such an approach could scale beyond closely related varieties, successful experiments on the annotation of normalized historical language have been reported, although mostly focused on token-level annotations (POS, lemma, morphology) of language stages which syntax does not greatly deviate from modern rules (Rayson et al., 2007; Pennacchiotti and Zanzotto, 2008; Kestemont et al., 2010; Bollmann, 2013). For the annotation of more remotely related varieties with more drastic differences in word order rigidity or morphology as considered here, however, projection techniques are more promising as they have been successfully applied to unrelated languages, as well, but still benefit from diachronic proximity, cf. Meyer (2011) for the projection-based morphological analysis of Modern and Old Russian. The goal of our experiment was not to achieve state-of-the-art performance, but to show whether background material from related languages with different degrees of diachronic distance can help to compensate data sparsity, in this case with an experiment on annotation projection. This hypothesis could be confirmed and we found effects that – even on the minimal amounts of data considered for this study – indicated statistically significant improvements. It is thus to be expected that even greater improvements can be achieved by considering more closely related pairs of languages, with greater amounts of data. The further exploration of this hypothesis is the driving force behind our efforts to compile a massive corpus of parallel and quasi-parallel texts for all major varieties of synchronic and historical Germanic languages. Algorithms successfully tested in this context can be expected to be applicable to other scenarios in which, e.g., well-researched modern languages may be employed to facilitate the creation of NLP tools for less-ressourced, related languages. Our efforts are thus not specific to historical languages. As the diachronic development and the diversification of the Germanic languages is well-documented in this body of data, and the linguistic processes involved are well-researched, this data set represents an extraordinarily valuable resource for philological and comparitve studies as well as Natural Language Processing. In particular, we are interested in developing algorithms that explore and exploit the variable degree of diachronic relatedness found between the languages in our sample. At the same time, we cooperate with researchers from philology, historical and comparative linguistics, which research on intertextuality, diachronic lexicology, phonology, morphology and syntax we aim to support with NLP tools developed on the basis of this body of parallel text. 18</abstract>
<title confidence="0.96691">References</title>
<author confidence="0.994342">Maltoptimizer A system for maltparser optimization In Nicoletta</author>
<affiliation confidence="0.829802">Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, of the Eight International</affiliation>
<title confidence="0.578413">on Language Resources and Evaluation Istanbul, Turkey, may. European Language</title>
<note confidence="0.9433584">Resources Association (ELRA). Bollmann, Florian Petran, and Stefanie Dipper. 2011. Rule-based normalization of historical texts. In Proceedings of the Workshop on Language Technologies for Digital Humanities and Cultural Heritage (LaTeCHpages 34–42, Hissar, Bulgaria, September. Bollmann. 2013. POS tagging for historical texts with sparse training data. In of the 7th Annotation Workshop and Interoperability with pages 11–18, Sofia, Bulgaria, August. Association for Computational Linguistics. Michael Collins, Lance Ramshaw, Jan Hajiˇc, and Christoph Tillmann. 1999. A statistical parser for czech. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational pages 505–512. Association for Computational Linguistics.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Miguel Ballesteros</author>
<author>Joakim Nivre</author>
</authors>
<title>Maltoptimizer: A system for maltparser optimization.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="9809" citStr="Ballesteros and Nivre, 2012" startWordPosition="1483" endWordPosition="1486">te number of archaisms, it is particularly wellsuited for automated annotation. 3 Experimental setup We study the projection of dependency syntax, as it is considered particularly suitable for free word-order languages like IS, OE and DE. The existing constituent annotations were thus converted with standard tools for PTB conversion. Figure 1 summarizes the experimental setup. For annotating EN, we created dependency versions of WSJ and Brown sections of the PTB with the LTH Converter (Johansson and Nugues, 2007). We trained Malt 1.7.2 (Nivre, 2003), optimized its features with MaltOptimizer (Ballesteros and Nivre, 2012), and parsed the EN bible using the resulting feature model. 1http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html 2http://enhgcorpus.wikispaces.com 3http://esv.org 13 The ME, OE, DE and IS datasets were word aligned with EN using GIZA++ (Och and Ney, 2003). 1 : n alignments were resolved to the most probable 1 : 1 mapping. During annotation projection, we assume that the aligned words represent the respective heads for the remaining n − 1 words. These dependent words are assigned the dependency relation FRAG to the word that got the highest score in the translation table. This s</context>
</contexts>
<marker>Ballesteros, Nivre, 2012</marker>
<rawString>Miguel Ballesteros and Joakim Nivre. 2012. Maltoptimizer: A system for maltparser optimization. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Bollmann</author>
<author>Florian Petran</author>
<author>Stefanie Dipper</author>
</authors>
<title>Rule-based normalization of historical texts.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Language Technologies for Digital Humanities and Cultural Heritage (LaTeCH2011),</booktitle>
<pages>34--42</pages>
<location>Hissar, Bulgaria,</location>
<contexts>
<context position="4672" citStr="Bollmann et al., 2011" startWordPosition="680" endWordPosition="683">ated Early Modern High German (DE) and Middle Icelandic (IS) for which we possess comparable annotations, and test the following hypotheses: (H1) Adding data from related varieties compensates the sparsity of target language training data. (H2) Data from related languages compensates the lack of target language training data. (H3) The greater the diachronic proximity, the better the performance of (H1) and (H2). We test these hypotheses in the following setup: (1) Hyperlemmatization: Different historical variants are normalized to a consistent standard, e.g., represented by a modern language (Bollmann et al., 2011). We emulate hyperlemmatization by English glosses automatically obtained through SMT. (2) Projection: We create training data for a fragment-aware dependency parser (Spreyer et al., 2010) using annotation projection from modern English. (3) Combination and evaluation: Parser modules are trained on different training data sets, and evaluated against existing gold annotations. In our setting, we enforce data sparsity by using deliberately small training data sets. This is because we emulate the situation of less-documented languages that will be in the focus of subsequent experiments, namely, O</context>
<context position="26412" citStr="Bollmann et al., 2011" startWordPosition="4055" endWordPosition="4058">ddress the inclusion of richer morphological features, projection from other languages and evaluation against syntactic annotations according to other schemes not derived from the Penn Treebank, as currently available, for example, for Old High German, Old Norse, and Gothic. 17 2. The hyperlemmatization in our approach was achieved through alignment/SMT, and a similar lexically-oriented approach has been suggested by (Zeman and Resnik, 2008). Alternative strategies more suitable for scenarios with limited amounts of training data may include the use of orthographical normalization techniques (Bollmann et al., 2011) or substring-based machine translation (Neubig et al., 2012) and are also subject to on-going research. We assume that SMT-based hyperlemmatization introduces more noise than these strategies, so that it is harder to achieve statistically significant results. Our findings are thus likely to remain valid regardless of the hyperlemmatization strategy. This hypothesis is, however, yet to be confirmed in subsequent studies. 3. Our experiment mostly deals with data translated from (or at least informed by) the Latin Vulgate. Our data may be biased by translation strategies which evolved over time,</context>
</contexts>
<marker>Bollmann, Petran, Dipper, 2011</marker>
<rawString>Marcel Bollmann, Florian Petran, and Stefanie Dipper. 2011. Rule-based normalization of historical texts. In Proceedings of the Workshop on Language Technologies for Digital Humanities and Cultural Heritage (LaTeCH2011), pages 34–42, Hissar, Bulgaria, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcel Bollmann</author>
</authors>
<title>POS tagging for historical texts with sparse training data.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>11--18</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="28370" citStr="Bollmann, 2013" startWordPosition="4347" endWordPosition="4348">wonder how the annotation projection approach performs in comparison to direct applications of modern language NLP tools to normalized historical data language (Scheible et al., 2011). While it is unlikely that such an approach could scale beyond closely related varieties, successful experiments on the annotation of normalized historical language have been reported, although mostly focused on token-level annotations (POS, lemma, morphology) of language stages which syntax does not greatly deviate from modern rules (Rayson et al., 2007; Pennacchiotti and Zanzotto, 2008; Kestemont et al., 2010; Bollmann, 2013). For the annotation of more remotely related varieties with more drastic differences in word order rigidity or morphology as considered here, however, projection techniques are more promising as they have been successfully applied to unrelated languages, as well, but still benefit from diachronic proximity, cf. Meyer (2011) for the projection-based morphological analysis of Modern and Old Russian. The goal of our experiment was not to achieve state-of-the-art performance, but to show whether background material from related languages with different degrees of diachronic distance can help to c</context>
</contexts>
<marker>Bollmann, 2013</marker>
<rawString>Marcel Bollmann. 2013. POS tagging for historical texts with sparse training data. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 11–18, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Lance Ramshaw</author>
<author>Jan Hajiˇc</author>
<author>Christoph Tillmann</author>
</authors>
<title>A statistical parser for czech.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>505--512</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Collins, Ramshaw, Hajiˇc, Tillmann, 1999</marker>
<rawString>Michael Collins, Lance Ramshaw, Jan Hajiˇc, and Christoph Tillmann. 1999. A statistical parser for czech. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 505–512. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Cummings</author>
</authors>
<title>An Introduction to the Grammar of Old English: A Systemic Functional Approach. Functional Linguistics.</title>
<date>2010</date>
<publisher>Equinox Publishing Limited.</publisher>
<contexts>
<context position="7156" citStr="Cummings, 2010" startWordPosition="1053" endWordPosition="1054">High German separated in the 5th c. The antecessor of IS separated from this branch about 500 years earlier. Among Germanic languages, great differences emerged, but most languages developed similarly towards a loss of morphology and a more rigid syntax, a tendency particularly prevalent in EN. As compared to this, OE had a relatively free OV word order, with grammatical roles conveyed through morphological markers. The OE case marking system distinguished four cases, but eventually collapsed during ME, resulting in a strict strict VO word order in EN (Trips, 2002; van Kemenade and Los, 2009; Cummings, 2010). Unlike EN, DE preserved four cases, and a relatively free word order (Ebert, 1976). A characteristic of German are separable verb prefixes, leading to 1 : n mappings in the statistical alignment with EN. 12 Figure 1: Workflow Unlike EN and DE, IS is a North Germanic language. It is assumed to be conservative, with relatively free word order with both OV and VO patterns and a rich morphology that leads to many 1 : n alignments with EN, e.g., for suffixed definite articles; we thus expect special challenges for annotation projection under conditions with limited training data. Different from t</context>
</contexts>
<marker>Cummings, 2010</marker>
<rawString>Michael Cummings. 2010. An Introduction to the Grammar of Old English: A Systemic Functional Approach. Functional Linguistics. Equinox Publishing Limited.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert P Ebert</author>
</authors>
<title>Infinitival complement constructions in Early New High German. Linguistische Arbeiten. De Gruyter.</title>
<date>1976</date>
<contexts>
<context position="7240" citStr="Ebert, 1976" startWordPosition="1067" endWordPosition="1068">ut 500 years earlier. Among Germanic languages, great differences emerged, but most languages developed similarly towards a loss of morphology and a more rigid syntax, a tendency particularly prevalent in EN. As compared to this, OE had a relatively free OV word order, with grammatical roles conveyed through morphological markers. The OE case marking system distinguished four cases, but eventually collapsed during ME, resulting in a strict strict VO word order in EN (Trips, 2002; van Kemenade and Los, 2009; Cummings, 2010). Unlike EN, DE preserved four cases, and a relatively free word order (Ebert, 1976). A characteristic of German are separable verb prefixes, leading to 1 : n mappings in the statistical alignment with EN. 12 Figure 1: Workflow Unlike EN and DE, IS is a North Germanic language. It is assumed to be conservative, with relatively free word order with both OV and VO patterns and a rich morphology that leads to many 1 : n alignments with EN, e.g., for suffixed definite articles; we thus expect special challenges for annotation projection under conditions with limited training data. Different from the old languages, EN developed a rigid word order and a largely reduced morphology. </context>
</contexts>
<marker>Ebert, 1976</marker>
<rawString>Robert P. Ebert. 1976. Infinitival complement constructions in Early New High German. Linguistische Arbeiten. De Gruyter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA</booktitle>
<pages>105--112</pages>
<location>Tartu, Estonia,</location>
<contexts>
<context position="9699" citStr="Johansson and Nugues, 2007" startWordPosition="1468" endWordPosition="1471">n IcePaHC side-project, it adapts the IS annotation scheme. EN For EN, we use the ESV Bible.3 Due to a moderate number of archaisms, it is particularly wellsuited for automated annotation. 3 Experimental setup We study the projection of dependency syntax, as it is considered particularly suitable for free word-order languages like IS, OE and DE. The existing constituent annotations were thus converted with standard tools for PTB conversion. Figure 1 summarizes the experimental setup. For annotating EN, we created dependency versions of WSJ and Brown sections of the PTB with the LTH Converter (Johansson and Nugues, 2007). We trained Malt 1.7.2 (Nivre, 2003), optimized its features with MaltOptimizer (Ballesteros and Nivre, 2012), and parsed the EN bible using the resulting feature model. 1http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html 2http://enhgcorpus.wikispaces.com 3http://esv.org 13 The ME, OE, DE and IS datasets were word aligned with EN using GIZA++ (Och and Ney, 2003). 1 : n alignments were resolved to the most probable 1 : 1 mapping. During annotation projection, we assume that the aligned words represent the respective heads for the remaining n − 1 words. These dependent words are</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Proceedings of NODALIDA 2007, pages 105–112, Tartu, Estonia, May 25-26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Kiyotaka Uchimoto</author>
</authors>
<title>Minimally lexicalized dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>205--208</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11962" citStr="Kawahara and Uchimoto, 2007" startWordPosition="1807" endWordPosition="1811">incomplete projections. In our setting, fMalt used two features, POS and hyperlemmas. POS The tagsets of the historical corpora originate in PTB, but show incompatible adaptations to the native morphosyntax. Tagset extensions on grammatical case in OE, IS and DE were removed and language-specific extensions for auxiliaries and modal verbs were leveled, in favor of a common, but underspecified tagset for all four languages. As these generalized tags preserve information not found in EN, they were fed into the parser. (hyper-)lemma Lexicalization is utterly important for the dependency parsing (Kawahara and Uchimoto, 2007), but to generalize over specifics of historical language varieties, hyperlemmatization needs to be performed. Similar to Zeman and Resnik (2008), we use projected English words as hyperlemmas and feed them into the parser. Hyperlemmatization against a closely related languages is acceptable as we can expect that the syntactic properties of words are likely to be similar. The projected annotations were then evaluated against dependency annotations created analoguously to the EN annotations from manual PTB-style constituency syntax. As LTH works exclusively on PTB data, the historical corpora w</context>
</contexts>
<marker>Kawahara, Uchimoto, 2007</marker>
<rawString>Daisuke Kawahara and Kiyotaka Uchimoto. 2007. Minimally lexicalized dependency parsing. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 205–208. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Kestemont</author>
<author>Walter Daelemans</author>
<author>Guy De Pauw</author>
</authors>
<title>Weigh your words: Memory-based lemma-retrieval for Middle Dutch literary texts.</title>
<date>2010</date>
<booktitle>In CLIN 2010. Computational linguistics in the Netherlands 20,</booktitle>
<location>Utrecht, The Netherlands,</location>
<marker>Kestemont, Daelemans, De Pauw, 2010</marker>
<rawString>Mike Kestemont, Walter Daelemans, and Guy De Pauw. 2010. Weigh your words: Memory-based lemma-retrieval for Middle Dutch literary texts. In CLIN 2010. Computational linguistics in the Netherlands 20, Utrecht, The Netherlands, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roland Meyer</author>
</authors>
<title>New wine in old wineskins? Tagging Old Russian via annotation projection from modern translations. Russian linguistics,</title>
<date>2011</date>
<pages>35--2</pages>
<contexts>
<context position="28696" citStr="Meyer (2011)" startWordPosition="4395" endWordPosition="4396">historical language have been reported, although mostly focused on token-level annotations (POS, lemma, morphology) of language stages which syntax does not greatly deviate from modern rules (Rayson et al., 2007; Pennacchiotti and Zanzotto, 2008; Kestemont et al., 2010; Bollmann, 2013). For the annotation of more remotely related varieties with more drastic differences in word order rigidity or morphology as considered here, however, projection techniques are more promising as they have been successfully applied to unrelated languages, as well, but still benefit from diachronic proximity, cf. Meyer (2011) for the projection-based morphological analysis of Modern and Old Russian. The goal of our experiment was not to achieve state-of-the-art performance, but to show whether background material from related languages with different degrees of diachronic distance can help to compensate data sparsity, in this case with an experiment on annotation projection. This hypothesis could be confirmed and we found effects that – even on the minimal amounts of data considered for this study – indicated statistically significant improvements. It is thus to be expected that even greater improvements can be ac</context>
</contexts>
<marker>Meyer, 2011</marker>
<rawString>Roland Meyer. 2011. New wine in old wineskins? Tagging Old Russian via annotation projection from modern translations. Russian linguistics, 35(2):267–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>Machine translation without words through substring alignment.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>165--174</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="26473" citStr="Neubig et al., 2012" startWordPosition="4063" endWordPosition="4066">on from other languages and evaluation against syntactic annotations according to other schemes not derived from the Penn Treebank, as currently available, for example, for Old High German, Old Norse, and Gothic. 17 2. The hyperlemmatization in our approach was achieved through alignment/SMT, and a similar lexically-oriented approach has been suggested by (Zeman and Resnik, 2008). Alternative strategies more suitable for scenarios with limited amounts of training data may include the use of orthographical normalization techniques (Bollmann et al., 2011) or substring-based machine translation (Neubig et al., 2012) and are also subject to on-going research. We assume that SMT-based hyperlemmatization introduces more noise than these strategies, so that it is harder to achieve statistically significant results. Our findings are thus likely to remain valid regardless of the hyperlemmatization strategy. This hypothesis is, however, yet to be confirmed in subsequent studies. 3. Our experiment mostly deals with data translated from (or at least informed by) the Latin Vulgate. Our data may be biased by translation strategies which evolved over time, from very literal translations (actually, glossings) of Lati</context>
</contexts>
<marker>Neubig, Watanabe, Mori, Kawahara, 2012</marker>
<rawString>Graham Neubig, Taro Watanabe, Shinsuke Mori, and Tatsuya Kawahara. 2012. Machine translation without words through substring alignment. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 165–174, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT).</booktitle>
<contexts>
<context position="9736" citStr="Nivre, 2003" startWordPosition="1476" endWordPosition="1477">scheme. EN For EN, we use the ESV Bible.3 Due to a moderate number of archaisms, it is particularly wellsuited for automated annotation. 3 Experimental setup We study the projection of dependency syntax, as it is considered particularly suitable for free word-order languages like IS, OE and DE. The existing constituent annotations were thus converted with standard tools for PTB conversion. Figure 1 summarizes the experimental setup. For annotating EN, we created dependency versions of WSJ and Brown sections of the PTB with the LTH Converter (Johansson and Nugues, 2007). We trained Malt 1.7.2 (Nivre, 2003), optimized its features with MaltOptimizer (Ballesteros and Nivre, 2012), and parsed the EN bible using the resulting feature model. 1http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html 2http://enhgcorpus.wikispaces.com 3http://esv.org 13 The ME, OE, DE and IS datasets were word aligned with EN using GIZA++ (Och and Ney, 2003). 1 : n alignments were resolved to the most probable 1 : 1 mapping. During annotation projection, we assume that the aligned words represent the respective heads for the remaining n − 1 words. These dependent words are assigned the dependency relation FRA</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="10080" citStr="Och and Ney, 2003" startWordPosition="1515" endWordPosition="1518">thus converted with standard tools for PTB conversion. Figure 1 summarizes the experimental setup. For annotating EN, we created dependency versions of WSJ and Brown sections of the PTB with the LTH Converter (Johansson and Nugues, 2007). We trained Malt 1.7.2 (Nivre, 2003), optimized its features with MaltOptimizer (Ballesteros and Nivre, 2012), and parsed the EN bible using the resulting feature model. 1http://www.ling.upenn.edu/hist-corpora/PPCME2-RELEASE-3/index.html 2http://enhgcorpus.wikispaces.com 3http://esv.org 13 The ME, OE, DE and IS datasets were word aligned with EN using GIZA++ (Och and Ney, 2003). 1 : n alignments were resolved to the most probable 1 : 1 mapping. During annotation projection, we assume that the aligned words represent the respective heads for the remaining n − 1 words. These dependent words are assigned the dependency relation FRAG to the word that got the highest score in the translation table. This solution solves, among others, the problem of separable verb prefixes in DE, for example, DE ruffen with prefix an would be aligned to English word call: As P(”call”|”an”) &lt; P(”call”|”ruffen”), the syntactic information of ”call” will be projected to ”ruffen” and ”an” wil</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Pennacchiotti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Natural language processing across time: An empirical investigation on italian.</title>
<date>2008</date>
<booktitle>In Advances in Natural Language Processing,</booktitle>
<pages>371--382</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="28329" citStr="Pennacchiotti and Zanzotto, 2008" startWordPosition="4338" endWordPosition="4342">nguages thus needs to be extended accordingly. 4. One may wonder how the annotation projection approach performs in comparison to direct applications of modern language NLP tools to normalized historical data language (Scheible et al., 2011). While it is unlikely that such an approach could scale beyond closely related varieties, successful experiments on the annotation of normalized historical language have been reported, although mostly focused on token-level annotations (POS, lemma, morphology) of language stages which syntax does not greatly deviate from modern rules (Rayson et al., 2007; Pennacchiotti and Zanzotto, 2008; Kestemont et al., 2010; Bollmann, 2013). For the annotation of more remotely related varieties with more drastic differences in word order rigidity or morphology as considered here, however, projection techniques are more promising as they have been successfully applied to unrelated languages, as well, but still benefit from diachronic proximity, cf. Meyer (2011) for the projection-based morphological analysis of Modern and Old Russian. The goal of our experiment was not to achieve state-of-the-art performance, but to show whether background material from related languages with different deg</context>
</contexts>
<marker>Pennacchiotti, Zanzotto, 2008</marker>
<rawString>Marco Pennacchiotti and Fabio Massimo Zanzotto. 2008. Natural language processing across time: An empirical investigation on italian. In Advances in Natural Language Processing, pages 371–382. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Rayson</author>
<author>Dawn Archer</author>
<author>Alistair Baron</author>
<author>Jonathan Culpeper</author>
<author>Nicholas Smith</author>
</authors>
<title>Tagging the Bard: Evaluating the accuracy of a modern POS tagger on Early Modern English corpora.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th Corpus Linguistics Conference (CL-2007),</booktitle>
<location>Birmingham, UK.</location>
<contexts>
<context position="28295" citStr="Rayson et al., 2007" startWordPosition="4334" endWordPosition="4337">corpus of Germanic languages thus needs to be extended accordingly. 4. One may wonder how the annotation projection approach performs in comparison to direct applications of modern language NLP tools to normalized historical data language (Scheible et al., 2011). While it is unlikely that such an approach could scale beyond closely related varieties, successful experiments on the annotation of normalized historical language have been reported, although mostly focused on token-level annotations (POS, lemma, morphology) of language stages which syntax does not greatly deviate from modern rules (Rayson et al., 2007; Pennacchiotti and Zanzotto, 2008; Kestemont et al., 2010; Bollmann, 2013). For the annotation of more remotely related varieties with more drastic differences in word order rigidity or morphology as considered here, however, projection techniques are more promising as they have been successfully applied to unrelated languages, as well, but still benefit from diachronic proximity, cf. Meyer (2011) for the projection-based morphological analysis of Modern and Old Russian. The goal of our experiment was not to achieve state-of-the-art performance, but to show whether background material from re</context>
</contexts>
<marker>Rayson, Archer, Baron, Culpeper, Smith, 2007</marker>
<rawString>Paul Rayson, Dawn Archer, Alistair Baron, Jonathan Culpeper, and Nicholas Smith. 2007. Tagging the Bard: Evaluating the accuracy of a modern POS tagger on Early Modern English corpora. In Proceedings of the 4th Corpus Linguistics Conference (CL-2007), Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Mari Broman Olsen</author>
<author>Mona Diab</author>
</authors>
<title>Creating a parallel corpus from the book of 2000 tongues.</title>
<date>1997</date>
<booktitle>In Proc. of the Text Encoding Initiative 10th Anniversary User Conference (TEI-10).</booktitle>
<contexts>
<context position="21891" citStr="Resnik et al. (1997)" startWordPosition="3345" endWordPosition="3348">nal markup or with a lexicon-supported geometric sentence aligner (T´oth et al., 2008). In the table, ‘text’ means any document ranging from a small excerpt such as the Lord’s Prayer (despite their marginal size valuable to develop algorithms for normalization/[hyper]lemmatization) over gospel harmonies and paraphrases to the entire bible that has been successfully aligned with Bible verses. The compiled corpus, excerpts and fragments for all Germanic languages marked up with IDs for verses, chapters and books. For data representation, we employed an XML version of the CES-scheme developed by Resnik et al. (1997). Having outgrown the scale of Resnik’s earlier project by far, we are currently in transition to TEI P5. As it is compiled from different sources, the corpus cannot be released under a free or an academic license. It contains material without explicit copyright statement, with proprietary content (e.g., from existing corpora), or available for personal use only. Instead, we plan to share the extraction and conversion scripts we used. For the experiments we aim to prepare, we focus on primary data, the texts in this collection are not annotated. Where annotations are available from other corpo</context>
</contexts>
<marker>Resnik, Olsen, Diab, 1997</marker>
<rawString>Philip Resnik, Mari Broman Olsen, and Mona Diab. 1997. Creating a parallel corpus from the book of 2000 tongues. In Proc. of the Text Encoding Initiative 10th Anniversary User Conference (TEI-10).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eir´ıkur R¨ognvaldsson</author>
<author>Anton Karl Ingason</author>
<author>Einar Freyr Sigurdhsson</author>
<author>Joel Wallenberg</author>
</authors>
<title>The Icelandic Parsed Historical Corpus (IcePaHC). In</title>
<date>2012</date>
<booktitle>LREC,</booktitle>
<pages>1977--1984</pages>
<marker>R¨ognvaldsson, Ingason, Sigurdhsson, Wallenberg, 2012</marker>
<rawString>Eir´ıkur R¨ognvaldsson, Anton Karl Ingason, Einar Freyr Sigurdhsson, and Joel Wallenberg. 2012. The Icelandic Parsed Historical Corpus (IcePaHC). In LREC, pages 1977–1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silke Scheible</author>
<author>Richard J Whitt</author>
<author>Martin Durrell</author>
<author>Paul Bennett</author>
</authors>
<title>Evaluating an ’off-the-shelf’ POStagger on Early Modern German text.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH-2011),</booktitle>
<pages>pages</pages>
<location>Portland, OR, USA,</location>
<contexts>
<context position="27938" citStr="Scheible et al., 2011" startWordPosition="4282" endWordPosition="4285">n. A representative investigation of annotation projection techniques thus requires the consideration of quasi-parallel data along with parallel data. This can be found in the great wealth of medieval religious literature, with Bible paraphrases, gospel harmonies, sermons and homilies as well as poetic and prose adaptations of biblical motives. The parallel corpus of Germanic languages thus needs to be extended accordingly. 4. One may wonder how the annotation projection approach performs in comparison to direct applications of modern language NLP tools to normalized historical data language (Scheible et al., 2011). While it is unlikely that such an approach could scale beyond closely related varieties, successful experiments on the annotation of normalized historical language have been reported, although mostly focused on token-level annotations (POS, lemma, morphology) of language stages which syntax does not greatly deviate from modern rules (Rayson et al., 2007; Pennacchiotti and Zanzotto, 2008; Kestemont et al., 2010; Bollmann, 2013). For the annotation of more remotely related varieties with more drastic differences in word order rigidity or morphology as considered here, however, projection techn</context>
</contexts>
<marker>Scheible, Whitt, Durrell, Bennett, 2011</marker>
<rawString>Silke Scheible, Richard J. Whitt, Martin Durrell, and Paul Bennett. 2011. Evaluating an ’off-the-shelf’ POStagger on Early Modern German text. In Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH-2011), pages 19–23, Portland, OR, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathrin Spreyer</author>
<author>Lilja Ovrelid</author>
<author>Jonas Kuhn</author>
</authors>
<title>Training parsers on partial trees: A cross-language comparison.</title>
<date>2010</date>
<booktitle>In Proc. of the 7th International Conference on Language Resources and Evaluation (LREC-2010),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="4860" citStr="Spreyer et al., 2010" startWordPosition="705" endWordPosition="708">tes the sparsity of target language training data. (H2) Data from related languages compensates the lack of target language training data. (H3) The greater the diachronic proximity, the better the performance of (H1) and (H2). We test these hypotheses in the following setup: (1) Hyperlemmatization: Different historical variants are normalized to a consistent standard, e.g., represented by a modern language (Bollmann et al., 2011). We emulate hyperlemmatization by English glosses automatically obtained through SMT. (2) Projection: We create training data for a fragment-aware dependency parser (Spreyer et al., 2010) using annotation projection from modern English. (3) Combination and evaluation: Parser modules are trained on different training data sets, and evaluated against existing gold annotations. In our setting, we enforce data sparsity by using deliberately small training data sets. This is because we emulate the situation of less-documented languages that will be in the focus of subsequent experiments, namely, Old High German and Old Saxon, which are relatively poorly documented. We do hope, however, that scalable NLP solutions can be developed if we add background information from their descenda</context>
<context position="11246" citStr="Spreyer et al., 2010" startWordPosition="1701" endWordPosition="1704">on of ”call” will be projected to ”ruffen” and ”an” will be its dependent labeled with ”FRAG”. The projected dependency trees were checked on well-formedness, sentences with cycles were dismissed from the data set. We formed training sets containing 437 sentences for ME, OE, DE, IS. Monolingual data sets were combined into bi-, tri- or quadrilingual training data sets with a simple concatenation, thereby creating less sparse, but more heterogeneous training data sets. For every language, test data was taken from J, 174 sentences per language. We used the projected dependencies to train fMalt (Spreyer et al., 2010), a fragment-aware dependency parser, in order to maximize the gain of information from incomplete projections. In our setting, fMalt used two features, POS and hyperlemmas. POS The tagsets of the historical corpora originate in PTB, but show incompatible adaptations to the native morphosyntax. Tagset extensions on grammatical case in OE, IS and DE were removed and language-specific extensions for auxiliaries and modal verbs were leveled, in favor of a common, but underspecified tagset for all four languages. As these generalized tags preserve information not found in EN, they were fed into th</context>
</contexts>
<marker>Spreyer, Ovrelid, Kuhn, 2010</marker>
<rawString>Kathrin Spreyer, Lilja Ovrelid, and Jonas Kuhn. 2010. Training parsers on partial trees: A cross-language comparison. In Proc. of the 7th International Conference on Language Resources and Evaluation (LREC-2010), Valletta, Malta, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Taylor</author>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
</authors>
<title>The Penn treebank: an overview.</title>
<date>2003</date>
<booktitle>In Treebanks,</booktitle>
<pages>5--22</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="6383" citStr="Taylor et al., 2003" startWordPosition="928" endWordPosition="931">hese can be confirmed, this motivates creating a larger corpus of parallel texts in Germanic languages as a basis for subsequent studies and more advanced, projection-based technologies for older and under-resourced Germanic languages. 2 Languages and corpus data We use parallel biblical texts in Old English (OE), Middle English (ME), Middle Icelandic (IS) and Early Modern High German (DE). This selection is determined by the availability of syntactically annotated corpora with closely related annotation schemes. As these schemes are derived from the Penn TreeBank (PTB) bracketing guidelines (Taylor et al., 2003a), we decided to use Modern English (EN) as a source for the projections. The Germanic languages derive from Proto-Germanic as a common ancestor. OE and Old High German separated in the 5th c. The antecessor of IS separated from this branch about 500 years earlier. Among Germanic languages, great differences emerged, but most languages developed similarly towards a loss of morphology and a more rigid syntax, a tendency particularly prevalent in EN. As compared to this, OE had a relatively free OV word order, with grammatical roles conveyed through morphological markers. The OE case marking sy</context>
<context position="8431" citStr="Taylor et al., 2003" startWordPosition="1265" endWordPosition="1268">largely reduced morphology. A direct adaptation of an existing English parser to (hyperlemmatized) OE, IS or DE is thus not promising. Therefore, we employ an approach based on annotation projection. The corpus data we used consists of parsed bible fragments from manually annotated corpora, mostly the gospels of Matthew (Mt), Mark (Mr), John (J) and Luke (L), from which we drew a test set of 147 sentences and a training set of 437 sentences for every language. ME and OE The Penn-Helsinki Parsed Corpus of Middle English (PPCME2)1 and the York-TorontoHelsinki Parsed Corpus of Old English Prose (Taylor et al., 2003b, YCOE) use a variant of the PTB annotation schema (Taylor et al., 2003a). YCOE contains the full West Saxon Gospel, but PPCME2 contains only a small fragment of a Wycliffite gospel of John, the ME data is thus complemented with parts of Genesis (G) and Numbers (N). IS The Icelandic Parsed Historical Corpus (R¨ognvaldsson et al., 2012, IcePaHC) is annotated following YCOE with slight modifications for specifics of IS. We use the gospel of John from Oddur Gottsk´alksson’s New Testament, a direct translation from Luther. DE The Parsed Corpus of Early New High German2 contains three gospels from</context>
</contexts>
<marker>Taylor, Marcus, Santorini, 2003</marker>
<rawString>Ann Taylor, Mitchell Marcus, and Beatrice Santorini. 2003a. The Penn treebank: an overview. In Treebanks, pages 5–22. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Taylor</author>
<author>Anthony Warner</author>
<author>Susan Pintzuk</author>
<author>Frank Beths</author>
</authors>
<title>The york-toronto-helsinki parsed corpus of old english prose.</title>
<date>2003</date>
<institution>University of York.</institution>
<contexts>
<context position="6383" citStr="Taylor et al., 2003" startWordPosition="928" endWordPosition="931">hese can be confirmed, this motivates creating a larger corpus of parallel texts in Germanic languages as a basis for subsequent studies and more advanced, projection-based technologies for older and under-resourced Germanic languages. 2 Languages and corpus data We use parallel biblical texts in Old English (OE), Middle English (ME), Middle Icelandic (IS) and Early Modern High German (DE). This selection is determined by the availability of syntactically annotated corpora with closely related annotation schemes. As these schemes are derived from the Penn TreeBank (PTB) bracketing guidelines (Taylor et al., 2003a), we decided to use Modern English (EN) as a source for the projections. The Germanic languages derive from Proto-Germanic as a common ancestor. OE and Old High German separated in the 5th c. The antecessor of IS separated from this branch about 500 years earlier. Among Germanic languages, great differences emerged, but most languages developed similarly towards a loss of morphology and a more rigid syntax, a tendency particularly prevalent in EN. As compared to this, OE had a relatively free OV word order, with grammatical roles conveyed through morphological markers. The OE case marking sy</context>
<context position="8431" citStr="Taylor et al., 2003" startWordPosition="1265" endWordPosition="1268">largely reduced morphology. A direct adaptation of an existing English parser to (hyperlemmatized) OE, IS or DE is thus not promising. Therefore, we employ an approach based on annotation projection. The corpus data we used consists of parsed bible fragments from manually annotated corpora, mostly the gospels of Matthew (Mt), Mark (Mr), John (J) and Luke (L), from which we drew a test set of 147 sentences and a training set of 437 sentences for every language. ME and OE The Penn-Helsinki Parsed Corpus of Middle English (PPCME2)1 and the York-TorontoHelsinki Parsed Corpus of Old English Prose (Taylor et al., 2003b, YCOE) use a variant of the PTB annotation schema (Taylor et al., 2003a). YCOE contains the full West Saxon Gospel, but PPCME2 contains only a small fragment of a Wycliffite gospel of John, the ME data is thus complemented with parts of Genesis (G) and Numbers (N). IS The Icelandic Parsed Historical Corpus (R¨ognvaldsson et al., 2012, IcePaHC) is annotated following YCOE with slight modifications for specifics of IS. We use the gospel of John from Oddur Gottsk´alksson’s New Testament, a direct translation from Luther. DE The Parsed Corpus of Early New High German2 contains three gospels from</context>
</contexts>
<marker>Taylor, Warner, Pintzuk, Beths, 2003</marker>
<rawString>Ann Taylor, Anthony Warner, Susan Pintzuk, and Frank Beths. 2003b. The york-toronto-helsinki parsed corpus of old english prose. University of York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Krisztina T´oth</author>
<author>Rich´ard Farkas</author>
<author>Andr´as Kocsor</author>
</authors>
<title>Sentence alignment of hungarian-english parallel corpora using a hybrid algorithm.</title>
<date>2008</date>
<journal>Acta Cybern.,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>T´oth, Farkas, Kocsor, 2008</marker>
<rawString>Krisztina T´oth, Rich´ard Farkas, and Andr´as Kocsor. 2008. Sentence alignment of hungarian-english parallel corpora using a hybrid algorithm. Acta Cybern., 18(3):463–478, January. C. Trips. 2002. From OV to VO in Early Middle English. Linguistics today. John Benjamins Pub.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A van Kemenade</author>
<author>B Los</author>
</authors>
<title>The Handbook of the History of English. Blackwell Handbooks in Linguistics.</title>
<date>2009</date>
<publisher>John Wiley &amp; Sons.</publisher>
<marker>van Kemenade, Los, 2009</marker>
<rawString>A. van Kemenade and B. Los. 2009. The Handbook of the History of English. Blackwell Handbooks in Linguistics. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<volume>3</volume>
<contexts>
<context position="12665" citStr="Yamada and Matsumoto, 2003" startWordPosition="1907" endWordPosition="1910">mmatization needs to be performed. Similar to Zeman and Resnik (2008), we use projected English words as hyperlemmas and feed them into the parser. Hyperlemmatization against a closely related languages is acceptable as we can expect that the syntactic properties of words are likely to be similar. The projected annotations were then evaluated against dependency annotations created analoguously to the EN annotations from manual PTB-style constituency syntax. As LTH works exclusively on PTB data, the historical corpora were converted with its antecessor Penn2Malt4 using user-defined head-rules (Yamada and Matsumoto, 2003). 4 Evaluation results baseline AUAS worst model AUAS best model AUAS UAS +1 +2 +1 +2 +3 ME .60 +DE +.00n.s. +DE+IS -.01n.s. +OE +.01n.s. +OE+IS +.01n.s. -.00n.s. OE .31 +IS -.00n.s. +DE+IS -.02n.s. +DE +.02n.s. +ME+DE +.00n.s. +.02n.s. DE .41 +OE +.02n.s. +OE+IS +.03∗ +ME +.04∗∗∗ +ME+IS +.03∗ +.04∗∗ IS .32 +IS -.02n.s. +DE+OE -.02n.s. +ME +.00n.s. +ME+DE -.01n.s. -.04∗∗ (a) trained on target and related language(s) baseline AUAS worst model AUAS best model AUAS UAS 1 2 1 2 3 ME .60 OE -.09∗∗∗ DE-IS -.01n.s IS -.05∗∗∗ IS+OE -.02n.s. -.02n.s. OE .31 DE -.03∗ ME-DE -.01n.s. ME -.02n.s. ME+IS -.0</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Hiroyasu Yamada and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings of IWPT. Vol. 3. 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowski</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of NAACL</booktitle>
<pages>200--207</pages>
<contexts>
<context position="1828" citStr="Yarowski and Ngai, 2001" startWordPosition="264" endWordPosition="267">, (b) that a parser trained on data from two related languages (and none from the target language) can reach a performance that is statistically not significantly worse than that of a parser trained on the projections to the target language, and (c) that both conclusions holds only among the three most closely related languages under consideration, but not necessarily the fourth. The experiments motivate the compilation of a larger parallel corpus of historical Germanic varieties as a basis for subsequent studies. 1 Background and motivation We describe an experiment on annotation projection (Yarowski and Ngai, 2001) between different Germanic languages, resp., their historical varieties, with the goal to assess to what extent sparsity of parallel data can be compensated by material from varieties related to the target variety, and studying the impact of diachronic proximity onto such applications. Statistical NLP of historical language data involves general issues typical for low-resource languages (the lack of annotated corpora, data sparsity, etc.), but also very specific challenges such as lack of standardized orthography, unsystematized punctuation, and a considerable degree of morphological variatio</context>
</contexts>
<marker>Yarowski, Ngai, 2001</marker>
<rawString>David Yarowski and Grace Ngai. 2001. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In Proceedings of NAACL 2001, pages 200–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Philip Resnik</author>
</authors>
<title>Cross-language parser adaptation between related languages.</title>
<date>2008</date>
<booktitle>In IJCNLP08 Workshop on NLP for Less Privileged Languages,</booktitle>
<pages>35--41</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="12107" citStr="Zeman and Resnik (2008)" startWordPosition="1828" endWordPosition="1831">show incompatible adaptations to the native morphosyntax. Tagset extensions on grammatical case in OE, IS and DE were removed and language-specific extensions for auxiliaries and modal verbs were leveled, in favor of a common, but underspecified tagset for all four languages. As these generalized tags preserve information not found in EN, they were fed into the parser. (hyper-)lemma Lexicalization is utterly important for the dependency parsing (Kawahara and Uchimoto, 2007), but to generalize over specifics of historical language varieties, hyperlemmatization needs to be performed. Similar to Zeman and Resnik (2008), we use projected English words as hyperlemmas and feed them into the parser. Hyperlemmatization against a closely related languages is acceptable as we can expect that the syntactic properties of words are likely to be similar. The projected annotations were then evaluated against dependency annotations created analoguously to the EN annotations from manual PTB-style constituency syntax. As LTH works exclusively on PTB data, the historical corpora were converted with its antecessor Penn2Malt4 using user-defined head-rules (Yamada and Matsumoto, 2003). 4 Evaluation results baseline AUAS worst</context>
<context position="26235" citStr="Zeman and Resnik, 2008" startWordPosition="4029" endWordPosition="4032">ctic dependencies were projected, indicating the relevance of diachronic relatedness as well as the developmental state of a related language. Subsequent experiments will hence address the inclusion of richer morphological features, projection from other languages and evaluation against syntactic annotations according to other schemes not derived from the Penn Treebank, as currently available, for example, for Old High German, Old Norse, and Gothic. 17 2. The hyperlemmatization in our approach was achieved through alignment/SMT, and a similar lexically-oriented approach has been suggested by (Zeman and Resnik, 2008). Alternative strategies more suitable for scenarios with limited amounts of training data may include the use of orthographical normalization techniques (Bollmann et al., 2011) or substring-based machine translation (Neubig et al., 2012) and are also subject to on-going research. We assume that SMT-based hyperlemmatization introduces more noise than these strategies, so that it is harder to achieve statistically significant results. Our findings are thus likely to remain valid regardless of the hyperlemmatization strategy. This hypothesis is, however, yet to be confirmed in subsequent studies</context>
</contexts>
<marker>Zeman, Resnik, 2008</marker>
<rawString>Daniel Zeman and Philip Resnik. 2008. Cross-language parser adaptation between related languages. In IJCNLP08 Workshop on NLP for Less Privileged Languages, pages 35–41, Hyderabad, India, Jan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003469">
<title confidence="0.805178">
Different Texts, Same Metaphors: Unigrams and Beyond
</title>
<author confidence="0.722335">
Beata Beigman Klebanov, Chee Wee Leong, Michael Heilman, Michael Flor
</author>
<affiliation confidence="0.647853">
Educational Testing Service
</affiliation>
<address confidence="0.8322075">
660 Rosedale Road
Princeton, NJ 08541
</address>
<email confidence="0.998717">
{bbeigmanklebanov,cleong,mheilman,mflor}@ets.org
</email>
<sectionHeader confidence="0.994794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999928357142857">
Current approaches to supervised learning
of metaphor tend to use sophisticated fea-
tures and restrict their attention to con-
structions and contexts where these fea-
tures apply. In this paper, we describe the
development of a supervised learning sys-
tem to classify all content words in a run-
ning text as either being used metaphori-
cally or not. We start by examining the
performance of a simple unigram baseline
that achieves surprisingly good results for
some of the datasets. We then show how
the recall of the system can be improved
over this strong baseline.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959166666667">
Current approaches to supervised learning of
metaphor tend to (a) use sophisticated features
based on theories of metaphor, (b) apply to cer-
tain selected constructions, like adj-noun or verb-
object pairs, and (c) concentrate on metaphors
of certain kind, such as metaphors about gover-
nance or about the mind. In this paper, we de-
scribe the development of a supervised machine
learning system to classify all content words in a
running text as either being used metaphorically
or not – a task not yet addressed in the literature,
to our knowledge. This approach would enable,
for example, quantification of the extent to which
a given text uses metaphor, or the extent to which
two different texts use similar metaphors. Both of
these questions are important in our target appli-
cation – scoring texts (in our case, essays written
for a test) for various aspects of effective use of
language, one of them being the use of metaphor.
We start by examining the performance of a
simple unigram baseline that achieves surprisingly
good results for some of the datasets. We then
show how the recall of the system can be improved
over this strong baseline.
</bodyText>
<sectionHeader confidence="0.994791" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999971875">
We use two datasets that feature full text anno-
tations of metaphors: A set of essays written for
a large-scale assessment of college graduates and
the VUAmsterdam corpus (Steen et al., 2010),1
containing articles from four genres sampled from
the BNC. Table 1 shows the sizes of the six sets,
as well as the proportion of metaphors in them; the
following sections explain their composition.
</bodyText>
<table confidence="0.999463125">
Data #Texts #NVAR #metaphors
tokens (%)
News 49 18,519 3,405 (18%)
Fiction 11 17,836 2,497 (14%)
Academic 12 29,469 3,689 (13%)
Conversation 18 15,667 1,149 ( 7%)
Essay Set A 85 21,838 2,368 (11%)
Essay Set B 79 22,662 2,745 (12%)
</table>
<tableCaption confidence="0.997126">
Table 1: Datasets used in this study. NVAR =
</tableCaption>
<keyword confidence="0.542512">
Nouns, Verbs, Adjectives, Adverbs, as tagged by
the Stanford POS tagger (Toutanova et al., 2003).
</keyword>
<subsectionHeader confidence="0.893207">
2.1 VUAmsterdam Data
</subsectionHeader>
<bodyText confidence="0.999803714285714">
The dataset consists of 117 fragments sampled
across four genres: Academic, News, Conversa-
tion, and Fiction. Each genre is represented by ap-
proximately the same number of tokens, although
the number of texts differs greatly, where the news
archive has the largest number of texts.
We randomly sampled 23% of the texts from
each genre to set aside for a blind test to be carried
out at a later date with a more advanced system;
the current experiments are performed using cross-
validation on the remaining 90 fragments: 10-fold
on News, 9-fold on Conversation, 11 on Fiction,
and 12 on Academic. All instances from the same
text were always placed in the same fold.
</bodyText>
<footnote confidence="0.989278">
1http://www2.let.vu.nl/oz/metaphorlab/metcor/search/index.html
</footnote>
<page confidence="0.987673">
11
</page>
<bodyText confidence="0.929399">
Proceedings of the Second Workshop on Metaphor in NLP, pages 11–17,
Baltimore, MD, USA, 26 June 2014. c�2014 Association for Computational Linguistics
The data is annotated using MIP-VU proce-
dure. It is based on the MIP procedure (Prag-
glejaz, 2007), extending it to handle metaphori-
city through reference (such as marking did as a
metaphor in As the weather broke up, so did their
friendship) and allow for explicit coding of diffi-
cult cases where a group of annotators could not
arrive at a consensus. The tagset is rich and is
organized hierarchically, detecting various types
of metaphors, words that flag the presense of
metaphors, etc. In this paper, we consider only the
top-level partition, labeling all content words with
the tag “function=mrw” (metaphor-related word)
as metaphors, while all other content words are la-
beled as non-metaphors.2
</bodyText>
<subsectionHeader confidence="0.998181">
2.2 Essay Data
</subsectionHeader>
<bodyText confidence="0.9999535">
The dataset consists of 224 essays written for a
high-stakes large-scale assessment of analytical
writing taken by college graduates aspiring to en-
ter a graduate school in the United States. Out of
these, 80 were set aside for future experiments and
not used for this paper. Of the remaining essays,
85 essays discuss the statement “High-speed elec-
tronic communications media, such as electronic
mail and television, tend to prevent meaningful
and thoughtful communication” (Set A), and 79
discuss the statement “In the age of television,
reading books is not as important as it once was.
People can learn as much by watching television
as they can by reading books.” (Set B). Multiple
essays on the same topic is a unique feature of this
dataset, allowing the examination of the effect of
topic on performance, by comparing performance
in within-topic and across-topic settings.
The essays were annotated using a protocol
that prefers a reader’s intuition over a formal de-
finition, and emphasizes the connection between
metaphor and the arguments that are put forward
by the writer. The protocol is presented in detail
in Beigman Klebanov and Flor (2013). All essays
were doubly annotated. The reliability is n = 0.58
for Set A and n = 0.56 for Set B. We merge the two
annotations (union), following the observation in
a previous study Beigman Klebanov et al. (2008)
that attention slips play a large role in accounting
for observed disagreements.
We will report results for 10-fold cross-
validation on each of sets A and B, as well as
</bodyText>
<footnote confidence="0.7277775">
2We note that this top-level partition was used for many
of the analyses discussed in (Steen et al., 2010).
</footnote>
<bodyText confidence="0.988626666666667">
across prompts, where the machine learner would
be trained on Set A and tested on Set B and vice
versa.
</bodyText>
<sectionHeader confidence="0.968183" genericHeader="method">
3 Supervised Learning of Metaphor
</sectionHeader>
<bodyText confidence="0.9899715">
For this study, we consider each content-word to-
ken in a text as an instance to be classified as a
metaphor or non-metaphor. We use the logistic
regression classifier in the SKLL package (Blan-
chard et al., 2013), which is based on scikit-learn
(Pedregosa et al., 2011), optimizing for F1 score
(class “metaphor”). We consider the following
features for metaphor detection.
</bodyText>
<listItem confidence="0.852867454545455">
• Unigrams (U): All content words from the
relevant training data are used as features,
without stemming or lemmatization.
• Part-of-Speech (P): We use Stanford POS
tagger 3.3.0 and the full Penn Treebank tagset
for content words (tags starting with A, N, V,
and J), removing the auxiliaries have, be, do.
• Concreteness (C): We use Brysbaert et al.
(2013) database of concreteness ratings for
about 40,000 English words. The mean ra-
tings, ranging 1-5, are binned in 0.25 incre-
ments; each bin is used as a binary feature.
• Topic models (T): We use Latent Dirich-
let Allocation (Blei et al., 2003) to derive
a 100-topic model from the NYT corpus
years 2003–2007 (Sandhaus, 2008) to rep-
resent common topics of public discussion.
The NYT data was lemmatized using NLTK
(Bird, 2006). We used the gensim toolkit
(ˇReh˚uˇrek and Sojka, 2010) for building the
models, with default parameters. The score
assigned to an instance w on a topic t is
</listItem>
<equation confidence="0.7983875">
logP(w|t)
P (w) where P(w) were estimated from
</equation>
<bodyText confidence="0.97729175">
the Cigaword corpus (Parker et al., 2009).
These features are based on the hypothesis
that certain topics are likelier to be used as
source domains for metaphors than others.
</bodyText>
<sectionHeader confidence="0.999907" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.998358">
For each dataset, we present the results for the
unigram model (baseline) and the results for the
full model containing all the features. For cross-
validation results, all words from the same text
were always placed in the same fold, to ensure that
we are evaluating generalization across texts.
</bodyText>
<page confidence="0.959302">
12
</page>
<figure confidence="0.999465052631579">
M Unigram UPCT
F P R F P R F
.20 .72 .43 .53 .70 .47 .56
.22 .79 .54 .64 .76 .60 .67
.20 .58 .45 .50 .56 .50 .53
.22 .71 .28 .40 .72 .35 .47
.31 .62 .38 .47 .61 .43 .51
.25 .54 .23 .32 .54 .24 .33
.23 .51 .20 .27 .50 .22 .28
.14 .39 .14 .21 .36 .15 .21
Data
Set A
Set B
B-A
A-B
News
Fiction
Acad.
Conv.
</figure>
<figureCaption confidence="0.6475035">
Table 2: Summary of performance, in terms of
precision, recall, and Fl. Set A, B, and VUAm-
sterdam: cross-validation. B-A and A-B: Training
on B and testing on A, and vice versa, respectively.
Column M: Fi of a pseudo-system that classifies
all words as metaphors.
</figureCaption>
<subsectionHeader confidence="0.989941">
4.1 Performance of the Baseline Model
</subsectionHeader>
<bodyText confidence="0.9999798125">
First, we observe the strong performance of the
unigram baseline for the cross-validation within
sets A and B (rows 1 and 2 in Table 2). For a
new essay, about half its metaphors will have been
observed in a sample of a few dozen essays on the
same topic; these words are also consistently used
as metaphors, as precision is above 70%. Once the
same-topic assumption is relaxed down to related
topics, the sharing of metaphor is reduced (com-
pare rows 1 vs 3 and 2 vs 4), but still substantial.
Moving to VUAmsterdam data, we observe that
the performance of the unigram model on the
News partition is comparable to its performance in
the cross-prompt scenario in the essay data (com-
pare row 5 to rows 3-4 in Table 2), suggesting that
the News fragments tend to discuss a set of related
topics and exhibit substantial sharing of metaphors
across texts.
The performance of the unigram model is much
lower for the other VUAmsterdam partitions, al-
though it is still non-trivial, as evidenced by its
consistent improvement over a pseudo-baseline
that classifies all words as metaphor, attaining
100% recall (shown in column M in Table 2). The
weaker performance could be due to highly diver-
gent topics between texts in each of the partitions.
It is also possible that the number of different
texts in these partitions is insufficient for covering
the metaphors that are common in these kinds of
texts – recall that these partitions have small num-
bers of long texts, whereas the News partition has
a larger number of short texts (see Table 1).
</bodyText>
<subsectionHeader confidence="0.998653">
4.2 Beyond Baseline
</subsectionHeader>
<bodyText confidence="0.999881022222222">
The addition of topic model, POS, and concrete-
ness features produces a significant increase in
recall across all evaluations (p &lt; 0.01), using
McNemar’s test of the significance of differ-
ences between correlated proportions (McNemar,
1947). Even for Conversations, where recall
improvement is the smallest and Fi score does
not improve, the UPCT model recovers all 161
metaphors found by the unigrams plus 14 addi-
tional metaphors, yielding a significant result on
the correlated test.
We next investigate the relative contribution of
the different types of features in the UPCT model
by ablating each type and observing the effect on
performance. Table 3 shows ablation results for
essay and News data, where substantial improve-
ments over the unigram baseline were produced.
We observe, as expected, that the unigram fea-
tures contributed the most, as removing them re-
sults in the most dramatic drop in performance,
although the combination of concreteness, POS,
and topic models recovers about one-fourth of
metaphors with over 50% precision, showing non-
trivial performance on essay data.
The second most effective feature set for essay
data are the topic models – they are responsible for
most of the recall gain obtained by the UPCT mo-
del. For example, one of the topics with a positive
weight in essays in set B deals with visual ima-
gery, its top 5 most likely words in the NYT being
picture, image, photograph, camera, photo. This
topic is often used metaphorically, with words
like superficial, picture, framed, reflective, mirror,
capture, vivid, distorted, exposure, scenes, face,
background that were all observed as metaphors in
Set B. In the News data, a topic that deals with hur-
ricane Katrina received a positive weight, as words
of suffering and recovery from distaster are often
used metaphorically when discussing other things:
starved, severed, awash, damaged, relief, victim,
distress, hits, swept, bounce, response, recovering,
suffering.
The part-of-speech features help improve recall
across all datasets in Table 3, while concreteness
features are effective only for some of the sets.
</bodyText>
<sectionHeader confidence="0.983009" genericHeader="method">
5 Discussion: Metaphor &amp; Word Sense
</sectionHeader>
<bodyText confidence="0.993411">
The classical “one sense per discourse” finding of
Gale et al. (1992) that words keep their senses
within the same text 98% of the time suggests that
</bodyText>
<page confidence="0.994471">
13
</page>
<table confidence="0.919674875">
M
U
UPCT
– U
– P
– C
– T
Set A cross-val. Set B cross-val. Train B : Test A Train A : Test B News
P R F P R F P R F P R F P R F
.11 1.0 .20 .12 1.0 .22 .11 1.0 .20 .12 1.0 .22 .18 1.0 .31
.72 .43 .53 .79 .54 .64 .58 .45 .50 .71 .28 .40 .62 .38 .47
.70 .47 .56 .76 .60 .67 .56 .50 .53 .72 .35 .47 .61 .43 .51
.58 .21 .31 .63 .28 .38 .44 .21 .29 .59 .18 .27 .55 .23 .32
.71 .46 .56 .76 .58 .66 .57 .48 .52 .70 .33 .45 .61 .41 .49
.70 .46 .55 .77 .58 .66 .56 .50 .53 .71 .34 .46 .61 .43 .50
.71 .43 .53 .78 .55 .65 .57 .45 .51 .71 .29 .41 .62 .41 .49
</table>
<tableCaption confidence="0.999978">
Table 3: Ablation evaluations. Model M is a pseudo-system that classifies all instances as metaphors.
</tableCaption>
<bodyText confidence="0.999924487804878">
if a word is used as a metaphor once in a text, it is
very likely to be a metaphor if it is used again in
the same text. Indeed, this is the reason for putting
all words from the same text in the same fold in
cross-validations, as training and testing on diffe-
rent parts of the same text would produce inflated
estimates of metaphor classification performance.
Koeling et al. (2005) extend the notion of dis-
course beyond a single text to a domain, such as
articles on Finance, Sports, and a general BNC
domain. For a set of words that each have at
least one Finance and one Sports sense and not
more than 12 senses in total, guessing the pre-
dominant sense in Finance and Sports yielded 77%
and 76% precision, respectively. Our results with
the unigram model show that guessing “metaphor”
based on a sufficient proportion of previously ob-
served metaphorical uses in the given domain
yields about 76% precision for essays on the same
topic. Thus, metaphoricity distinctions in same-
topic essays behave similarly to sense distinctions
for polysemous words with a predominant sense
in the Finance and Sports articles, keeping to their
domain-specific predominant sense 4 of the time.
Note that a domain-specific predominant sense
may or may not be the same as the most frequent
sense overall; similarly, a word’s tendency to be
used metaphorically might be domain specific or
general. The results for the BNC at large are likely
to reflect general rather than domain-specific sense
distributions. According to Koeling et al. (2005),
guessing the predominant sense in the BNC yields
51% precision; our finding for BNC News is 62%
precision for the unigram model. The difference
could be due to the mixing of the BNC genres in
Koeling et al. (2005), given the lower precision of
metaphoricity prediction in non-news (Table 2).
In all, our results suggest that the pattern of
metaphorical and non-metaphorical use is in line
with that of dominant word-sense for more and
less topically restricted domains.
</bodyText>
<sectionHeader confidence="0.999937" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999969685714286">
The extent to which different texts use similar
metaphors was addressed by Pasanek and Scul-
ley (2008) for corpora written by the same author.
They studied metaphors of mind in the oeuvre
of 7 authors, including John Milton and William
Shakespeare. They created a set of metaphori-
cal and non-metaphorical references to the mind
using excerpts from various texts written by these
authors. Using cross-validation with unigram
features for each of the authors separately, they
present very high accuracies (85%-94%), suggest-
ing that authors are highly self-consistent in the
metaphors of mind they select. They also find
good generalizations between some pairs of au-
thors, due to borrowing or literary allusion.
Studies using political texts, such as speeches
by politicians or news articles discussing politi-
cally important events, documented repeated use
of words from certain source domains, such as
rejuvenation in Tony Blair’s speeches (Charteris-
Black, 2005) or railroad metaphors in articles dis-
cussing political integration of Europe (Musolff,
2000). Our results regarding settings with substan-
tial topical consistency second these observations.
According to the Conceptual Metaphor theory
(Lakoff and Johnson, 1980), we expect certain ba-
sic metaphors to be highly ubiquitous in any cor-
pus of texts, such as TIME IS SPACE or UP IS
GOOD. To the extent that these metaphors are
realized through frequent content words, we ex-
pect some cross-text generalization power for a
unigram model. Perhaps the share of these basic
metaphors in all metaphors in a text is reflected
most faithfully in the peformance of the unigram
model on the non-News partitions of the VUAms-
</bodyText>
<page confidence="0.997878">
14
</page>
<bodyText confidence="0.999767">
terdam data, where topical sharing is minimal.
Approaches to metaphor detection are often ei-
ther rule-based or unsupervised (Martin, 1990;
Fass, 1991; Shutova et al., 2010; Shutova and
Sun, 2013; Li et al., 2013), although supervised
approaches have recently been attempted with the
advent of relatively large collections of metaphor-
annotated materials (Mohler et al., 2013; Hovy et
al., 2013; Pasanek and Sculley, 2008; Gedigan
et al., 2006). These approaches are difficult to
compare to our results, as these typically are not
whole texts but excerpts, and only certain kinds of
metaphors are annotated, such as metaphors about
governance or about the mind, or only words be-
longing to certain syntactic or semantic class are
annotated, such as verbs3 or motion words only.
Concreteness as a predictor of metaphoricity
was discussed in Turney et al. (2011) in the context
of concrete adjectives modifying abstract nouns.
The POS features are inspired by the discussion
of the preference and aversion of various POS
towards metaphoricity in Goatly (1997). Heintz
et al. (2013) use LDA topics built on Wikipedia
along with manually constructed seed lists for po-
tential source and target topics in the broad tar-
get domain of governance, in order to identify
sentences using lexica from both source and tar-
get domains as potentially containing metaphors.
Bethard et al. (2009) use LDA topics built on BNC
as features for classifying metaphorical and non-
metaphorical uses of 9 words in 450 sentences that
use these words, modeling metaphorical vs non-
metaphorical contexts for these words. In both
cases, LDA is used to capture the topical compo-
sition of a sentence; in contrast, we use LDA to
capture the tendency of words belonging to a topic
to be used metaphorically in a given discourse.
Dunn (2013) compared algorithms based on
various theories of metaphor on VUAmsterdam
data. The evaluations were done at sentence level,
where a sentence is metaphorical if it contains at
least one metaphorically used word. In this ac-
counting, the distribution is almost a mirror-image
of our setting, as 84% of sentences in News were
labeled as metaphorical, whereas 18% of content
words are tagged as such. The News partition was
very difficult for the systems examined in Dunn
(2013) – three of the four systems failed to pre-
dict any non-metaphorical sentences, and the one
system that did so suffered from a low recall of
</bodyText>
<subsectionHeader confidence="0.907291">
3as in Shutova and Teufel (2010)
</subsectionHeader>
<bodyText confidence="0.999749083333333">
metaphors, 20%. Dunn (2013) shows that the
different systems he compared had relatively low
agreement (κ &lt; 0.3); he interprets this finding as
suggesting that the different theories underlying
the models capture different aspects of metapho-
ricity and therefore detect different metaphors. It
is therefore likely that features derived from the
various models would fruitfully complement each
other in a supervised learning setting; our findings
suggest that the simplest building block – that of
a unigram model – should not be ignored in such
experiments.
</bodyText>
<sectionHeader confidence="0.991341" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999994305555556">
We address supervised learning of metaphoricity
of words of any content part of speech in a running
text. To our knowledge, this task has not yet been
studied in the literature. We experimented with a
simple unigram model that was surprisingly suc-
cessful for some of the datasets, and showed how
its recall can be further improved using topic mo-
dels, POS, and concreteness features.
The generally solid performance of the unigram
features suggests that these features should not be
neglected when trying to predict metaphors in a
supervised learning paradigm. Inasmuch as me-
taphoricity classification is similar to a coarse-
grained word sense disambiguation, a unigram
model can be thought of as a crude predominant
sense model for WSD, and is the more effective
the more topically homogeneous the data.
By evaluating models with LDA-based topic
features in addition to unigrams, we showed that
topical homogeneity can be exploited beyond uni-
grams. In topically homogeneous data, certain
topics commonly discussed in the public sphere
might not be addressed, yet their general fa-
miliarity avails them as sources for metaphors.
For essays on communication, topics like sports
and architecture are unlikely to be discussed; yet
metaphors from these domains can be used, such
as leveling of the playing field through cheap and
fast communications or buildling bridges across
cultures through the internet.
In future work, we intend to add features that
capture the relationship between the current word
and its immediate context, as well as add essays
from additional prompts to build a more topically
diverse set for exploration of cross-topic generali-
zation of our models for essay data.
</bodyText>
<page confidence="0.996914">
15
</page>
<sectionHeader confidence="0.982705" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999158416666666">
Beata Beigman Klebanov and Michael Flor. 2013.
Argumentation-relevant metaphors in test-taker es-
says. In Proceedings of the First Workshop on
Metaphor in NLP, pages 11–20, Atlanta, Georgia,
June. Association for Computational Linguistics.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In
COLING 2008 workshop on Human Judgments in
Computational Linguistics, pages 2–7, Manchester,
UK.
Steven Bethard, Vicky Tzuyin Lai, and James Martin.
2009. Topic model analysis of metaphor frequency
for psycholinguistic stimuli. In Proceedings of the
Workshop on Computational Approaches to Linguis-
tic Creativity, CALC ’09, pages 9–16, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Steven Bird. 2006. NLTK: The natural language
toolkit. In Proceedings of the ACL, Interactive Pre-
sentations, pages 69–72.
Daniel Blanchard, Michael Heilman,
and Nitin Madnani. 2013. SciKit-
Learn Laboratory. GitHub repository,
https://github.com/EducationalTestingService/skll.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Research, 3:993–1022.
Marc Brysbaert, Amy Beth Warriner, and Victor Ku-
perman. 2013. Concreteness ratings for 40 thou-
sand generally known english word lemmas. Behav-
ior Research Methods, pages 1–8.
Jonathan Charteris-Black. 2005. Politicians and
rhetoric: The persuasive power of metaphors. Pal-
grave MacMillan, Houndmills, UK and New York.
Jonathan Dunn. 2013. What metaphor identification
systems can tell us about metaphor-in-language. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 1–10, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Dan Fass. 1991. Met*: A method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49–90.
William Gale, Kenneth Church, and David Yarowsky.
1992. One sense per discourse. In Proceedings of
the Speech and Natural Language Workshop, pages
233–237.
Matt Gedigan, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the 3rd Workshop on Scalable Natural
Language Understanding, pages 41–48, New York.
Andrew Goatly. 1997. The Language of Metaphors.
Routledge, London.
Ilana Heintz, Ryan Gabbard, Mahesh Srivastava, Dave
Barner, Donald Black, Majorie Friedman, and Ralph
Weischedel. 2013. Automatic Extraction of Lin-
guistic Metaphors with LDA Topic Modeling. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 58–66, Atlanta, Georgia, June. Associa-
tion for Computational Linguistics.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Pro-
ceedings of the First Workshop on Metaphor in NLP,
pages 52–57, Atlanta, GA. Association for Compu-
tational Linguistics.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of HLT-
EMNLP, pages 419–426, Vancouver, Canada. Asso-
ciation for Computational Linguistics.
George Lakoff and Mark Johnson. 1980. Metaphors
we live by. University of Chicago Press, Chicago.
Hongsong Li, Kenny Q. Zhu, and Haixun Wang. 2013.
Data-driven metaphor recognition and explanation.
Transactions of the ACL, 1:379–390.
James Martin. 1990. A computational model of
metaphor interpretation. Academic Press Profes-
sional, Inc., San Diego, CA, USA.
Quinn McNemar. 1947. Note on the sampling error
of the difference between correlated proportions or
percentages. Psychometrika, 12(2):153–157.
Michael Mohler, David Bracewell, Marc Tomlinson,
and David Hinote. 2013. Semantic signatures for
example-based linguistic metaphor detection. In
Proceedings of the First Workshop on Metaphor in
NLP, pages 27–35, Atlanta, GA. Association for
Computational Linguistics.
Andreas Musolff. 2000. Mirror images of Eu-
rope: Metaphors in the public debate about
Europe in Britain and Germany. M¨unchen:
Iudicium. Annotated data is available at
http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.
Robert Parker, David Graff, Junbo Kong, Ke Chen, and
Kazuaki Maeda. 2009. English Gigaword Fourth
Edition LDC2009T13. Linguistic Data Consortium,
Philadelphia.
Bradley Pasanek and D. Sculley. 2008. Mining mil-
lions of metaphors. Literary and Linguistic Com-
puting, 23(3):345–360.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake VanderPlas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
</reference>
<page confidence="0.970947">
16
</page>
<reference confidence="0.999886590909091">
Group Pragglejaz. 2007. MIP: A Method for Iden-
tifying Metaphorically Used Words in Discourse.
Metaphor and Symbol, 22(1):1–39.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software
Framework for Topic Modelling with Large Cor-
pora. In Proceedings of the LREC 2010 Workshop
on New Challenges for NLP Frameworks, pages 45–
50, Valletta, Malta, May. ELRA.
Evan Sandhaus. 2008. The New York Times Anno-
tated Corpus. LDC Catalog No: LDC2008T19.
Ekaterina Shutova and Lin Sun. 2013. Unsu-
pervised metaphor identification using hierarchical
graph factorization clustering. In Proceedings of
HLT-NAACL, pages 978–988.
Ekaterina Shutova and Simone Teufel. 2010.
Metaphor corpus annotated for source - target do-
main mappings. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC’10), pages 3255–
3261, Valletta, Malta, May. European Language Re-
sources Association (ELRA).
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of the 23rd International
Conference on Computational Linguistics (COL-
ING), pages 1002–1010.
Gerard Steen, Aletta Dorst, Berenike Herrmann, Anna
Kaal, Tina Krennmayr, and Trijntje Pasma. 2010. A
Methodfor Linguistic Metaphor Identification. Am-
sterdam: John Benjamins.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of NAACL, pages 252–259.
Peter Turney, Yair Neuman, Dan Assaf, and Yohai Co-
hen. 2011. Literal and metaphorical sense identi-
fication through concrete and abstract context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 680–
690, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.999409">
17
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894618">
<title confidence="0.996333">Different Texts, Same Metaphors: Unigrams and Beyond</title>
<author confidence="0.996185">Beata Beigman Klebanov</author>
<author confidence="0.996185">Chee Wee Leong</author>
<author confidence="0.996185">Michael Heilman</author>
<author confidence="0.996185">Michael</author>
<affiliation confidence="0.973812">Educational Testing</affiliation>
<address confidence="0.9644845">660 Rosedale Princeton, NJ</address>
<abstract confidence="0.9994048">Current approaches to supervised learning of metaphor tend to use sophisticated features and restrict their attention to constructions and contexts where these features apply. In this paper, we describe the development of a supervised learning system to classify all content words in a running text as either being used metaphorically or not. We start by examining the performance of a simple unigram baseline that achieves surprisingly good results for some of the datasets. We then show how the recall of the system can be improved over this strong baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Michael Flor</author>
</authors>
<title>Argumentation-relevant metaphors in test-taker essays.</title>
<date>2013</date>
<booktitle>In Proceedings of the First Workshop on Metaphor in NLP,</booktitle>
<pages>11--20</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="5515" citStr="Klebanov and Flor (2013)" startWordPosition="893" endWordPosition="896">vision, reading books is not as important as it once was. People can learn as much by watching television as they can by reading books.” (Set B). Multiple essays on the same topic is a unique feature of this dataset, allowing the examination of the effect of topic on performance, by comparing performance in within-topic and across-topic settings. The essays were annotated using a protocol that prefers a reader’s intuition over a formal definition, and emphasizes the connection between metaphor and the arguments that are put forward by the writer. The protocol is presented in detail in Beigman Klebanov and Flor (2013). All essays were doubly annotated. The reliability is n = 0.58 for Set A and n = 0.56 for Set B. We merge the two annotations (union), following the observation in a previous study Beigman Klebanov et al. (2008) that attention slips play a large role in accounting for observed disagreements. We will report results for 10-fold crossvalidation on each of sets A and B, as well as 2We note that this top-level partition was used for many of the analyses discussed in (Steen et al., 2010). across prompts, where the machine learner would be trained on Set A and tested on Set B and vice versa. 3 Super</context>
</contexts>
<marker>Klebanov, Flor, 2013</marker>
<rawString>Beata Beigman Klebanov and Michael Flor. 2013. Argumentation-relevant metaphors in test-taker essays. In Proceedings of the First Workshop on Metaphor in NLP, pages 11–20, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Beigman Klebanov</author>
<author>Eyal Beigman</author>
<author>Daniel Diermeier</author>
</authors>
<title>Analyzing disagreements.</title>
<date>2008</date>
<booktitle>In COLING 2008 workshop on Human Judgments in Computational Linguistics,</booktitle>
<pages>2--7</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="5727" citStr="Klebanov et al. (2008)" startWordPosition="932" endWordPosition="935">t, allowing the examination of the effect of topic on performance, by comparing performance in within-topic and across-topic settings. The essays were annotated using a protocol that prefers a reader’s intuition over a formal definition, and emphasizes the connection between metaphor and the arguments that are put forward by the writer. The protocol is presented in detail in Beigman Klebanov and Flor (2013). All essays were doubly annotated. The reliability is n = 0.58 for Set A and n = 0.56 for Set B. We merge the two annotations (union), following the observation in a previous study Beigman Klebanov et al. (2008) that attention slips play a large role in accounting for observed disagreements. We will report results for 10-fold crossvalidation on each of sets A and B, as well as 2We note that this top-level partition was used for many of the analyses discussed in (Steen et al., 2010). across prompts, where the machine learner would be trained on Set A and tested on Set B and vice versa. 3 Supervised Learning of Metaphor For this study, we consider each content-word token in a text as an instance to be classified as a metaphor or non-metaphor. We use the logistic regression classifier in the SKLL packag</context>
</contexts>
<marker>Klebanov, Beigman, Diermeier, 2008</marker>
<rawString>Beata Beigman Klebanov, Eyal Beigman, and Daniel Diermeier. 2008. Analyzing disagreements. In COLING 2008 workshop on Human Judgments in Computational Linguistics, pages 2–7, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
<author>Vicky Tzuyin Lai</author>
<author>James Martin</author>
</authors>
<title>Topic model analysis of metaphor frequency for psycholinguistic stimuli.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, CALC ’09,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18144" citStr="Bethard et al. (2009)" startWordPosition="3068" endWordPosition="3071"> or motion words only. Concreteness as a predictor of metaphoricity was discussed in Turney et al. (2011) in the context of concrete adjectives modifying abstract nouns. The POS features are inspired by the discussion of the preference and aversion of various POS towards metaphoricity in Goatly (1997). Heintz et al. (2013) use LDA topics built on Wikipedia along with manually constructed seed lists for potential source and target topics in the broad target domain of governance, in order to identify sentences using lexica from both source and target domains as potentially containing metaphors. Bethard et al. (2009) use LDA topics built on BNC as features for classifying metaphorical and nonmetaphorical uses of 9 words in 450 sentences that use these words, modeling metaphorical vs nonmetaphorical contexts for these words. In both cases, LDA is used to capture the topical composition of a sentence; in contrast, we use LDA to capture the tendency of words belonging to a topic to be used metaphorically in a given discourse. Dunn (2013) compared algorithms based on various theories of metaphor on VUAmsterdam data. The evaluations were done at sentence level, where a sentence is metaphorical if it contains a</context>
</contexts>
<marker>Bethard, Lai, Martin, 2009</marker>
<rawString>Steven Bethard, Vicky Tzuyin Lai, and James Martin. 2009. Topic model analysis of metaphor frequency for psycholinguistic stimuli. In Proceedings of the Workshop on Computational Approaches to Linguistic Creativity, CALC ’09, pages 9–16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL, Interactive Presentations,</booktitle>
<pages>69--72</pages>
<contexts>
<context position="7293" citStr="Bird, 2006" startWordPosition="1202" endWordPosition="1203">gger 3.3.0 and the full Penn Treebank tagset for content words (tags starting with A, N, V, and J), removing the auxiliaries have, be, do. • Concreteness (C): We use Brysbaert et al. (2013) database of concreteness ratings for about 40,000 English words. The mean ratings, ranging 1-5, are binned in 0.25 increments; each bin is used as a binary feature. • Topic models (T): We use Latent Dirichlet Allocation (Blei et al., 2003) to derive a 100-topic model from the NYT corpus years 2003–2007 (Sandhaus, 2008) to represent common topics of public discussion. The NYT data was lemmatized using NLTK (Bird, 2006). We used the gensim toolkit (ˇReh˚uˇrek and Sojka, 2010) for building the models, with default parameters. The score assigned to an instance w on a topic t is logP(w|t) P (w) where P(w) were estimated from the Cigaword corpus (Parker et al., 2009). These features are based on the hypothesis that certain topics are likelier to be used as source domains for metaphors than others. 4 Results For each dataset, we present the results for the unigram model (baseline) and the results for the full model containing all the features. For crossvalidation results, all words from the same text were always </context>
</contexts>
<marker>Bird, 2006</marker>
<rawString>Steven Bird. 2006. NLTK: The natural language toolkit. In Proceedings of the ACL, Interactive Presentations, pages 69–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Blanchard</author>
<author>Michael Heilman</author>
<author>Nitin Madnani</author>
</authors>
<date>2013</date>
<institution>SciKitLearn Laboratory.</institution>
<note>GitHub repository, https://github.com/EducationalTestingService/skll.</note>
<contexts>
<context position="6353" citStr="Blanchard et al., 2013" startWordPosition="1043" endWordPosition="1047">at attention slips play a large role in accounting for observed disagreements. We will report results for 10-fold crossvalidation on each of sets A and B, as well as 2We note that this top-level partition was used for many of the analyses discussed in (Steen et al., 2010). across prompts, where the machine learner would be trained on Set A and tested on Set B and vice versa. 3 Supervised Learning of Metaphor For this study, we consider each content-word token in a text as an instance to be classified as a metaphor or non-metaphor. We use the logistic regression classifier in the SKLL package (Blanchard et al., 2013), which is based on scikit-learn (Pedregosa et al., 2011), optimizing for F1 score (class “metaphor”). We consider the following features for metaphor detection. • Unigrams (U): All content words from the relevant training data are used as features, without stemming or lemmatization. • Part-of-Speech (P): We use Stanford POS tagger 3.3.0 and the full Penn Treebank tagset for content words (tags starting with A, N, V, and J), removing the auxiliaries have, be, do. • Concreteness (C): We use Brysbaert et al. (2013) database of concreteness ratings for about 40,000 English words. The mean ratings</context>
</contexts>
<marker>Blanchard, Heilman, Madnani, 2013</marker>
<rawString>Daniel Blanchard, Michael Heilman, and Nitin Madnani. 2013. SciKitLearn Laboratory. GitHub repository, https://github.com/EducationalTestingService/skll.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--993</pages>
<contexts>
<context position="7111" citStr="Blei et al., 2003" startWordPosition="1170" endWordPosition="1173">r metaphor detection. • Unigrams (U): All content words from the relevant training data are used as features, without stemming or lemmatization. • Part-of-Speech (P): We use Stanford POS tagger 3.3.0 and the full Penn Treebank tagset for content words (tags starting with A, N, V, and J), removing the auxiliaries have, be, do. • Concreteness (C): We use Brysbaert et al. (2013) database of concreteness ratings for about 40,000 English words. The mean ratings, ranging 1-5, are binned in 0.25 increments; each bin is used as a binary feature. • Topic models (T): We use Latent Dirichlet Allocation (Blei et al., 2003) to derive a 100-topic model from the NYT corpus years 2003–2007 (Sandhaus, 2008) to represent common topics of public discussion. The NYT data was lemmatized using NLTK (Bird, 2006). We used the gensim toolkit (ˇReh˚uˇrek and Sojka, 2010) for building the models, with default parameters. The score assigned to an instance w on a topic t is logP(w|t) P (w) where P(w) were estimated from the Cigaword corpus (Parker et al., 2009). These features are based on the hypothesis that certain topics are likelier to be used as source domains for metaphors than others. 4 Results For each dataset, we prese</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Brysbaert</author>
<author>Amy Beth Warriner</author>
<author>Victor Kuperman</author>
</authors>
<title>Concreteness ratings for 40 thousand generally known english word lemmas. Behavior Research Methods,</title>
<date>2013</date>
<pages>1--8</pages>
<contexts>
<context position="6871" citStr="Brysbaert et al. (2013)" startWordPosition="1127" endWordPosition="1130">r or non-metaphor. We use the logistic regression classifier in the SKLL package (Blanchard et al., 2013), which is based on scikit-learn (Pedregosa et al., 2011), optimizing for F1 score (class “metaphor”). We consider the following features for metaphor detection. • Unigrams (U): All content words from the relevant training data are used as features, without stemming or lemmatization. • Part-of-Speech (P): We use Stanford POS tagger 3.3.0 and the full Penn Treebank tagset for content words (tags starting with A, N, V, and J), removing the auxiliaries have, be, do. • Concreteness (C): We use Brysbaert et al. (2013) database of concreteness ratings for about 40,000 English words. The mean ratings, ranging 1-5, are binned in 0.25 increments; each bin is used as a binary feature. • Topic models (T): We use Latent Dirichlet Allocation (Blei et al., 2003) to derive a 100-topic model from the NYT corpus years 2003–2007 (Sandhaus, 2008) to represent common topics of public discussion. The NYT data was lemmatized using NLTK (Bird, 2006). We used the gensim toolkit (ˇReh˚uˇrek and Sojka, 2010) for building the models, with default parameters. The score assigned to an instance w on a topic t is logP(w|t) P (w) wh</context>
</contexts>
<marker>Brysbaert, Warriner, Kuperman, 2013</marker>
<rawString>Marc Brysbaert, Amy Beth Warriner, and Victor Kuperman. 2013. Concreteness ratings for 40 thousand generally known english word lemmas. Behavior Research Methods, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Charteris-Black</author>
</authors>
<title>Politicians and rhetoric: The persuasive power of metaphors.</title>
<date>2005</date>
<publisher>Palgrave MacMillan, Houndmills, UK and</publisher>
<location>New York.</location>
<marker>Charteris-Black, 2005</marker>
<rawString>Jonathan Charteris-Black. 2005. Politicians and rhetoric: The persuasive power of metaphors. Palgrave MacMillan, Houndmills, UK and New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Dunn</author>
</authors>
<title>What metaphor identification systems can tell us about metaphor-in-language.</title>
<date>2013</date>
<booktitle>In Proceedings of the First Workshop on Metaphor in NLP,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="18570" citStr="Dunn (2013)" startWordPosition="3144" endWordPosition="3145">pics in the broad target domain of governance, in order to identify sentences using lexica from both source and target domains as potentially containing metaphors. Bethard et al. (2009) use LDA topics built on BNC as features for classifying metaphorical and nonmetaphorical uses of 9 words in 450 sentences that use these words, modeling metaphorical vs nonmetaphorical contexts for these words. In both cases, LDA is used to capture the topical composition of a sentence; in contrast, we use LDA to capture the tendency of words belonging to a topic to be used metaphorically in a given discourse. Dunn (2013) compared algorithms based on various theories of metaphor on VUAmsterdam data. The evaluations were done at sentence level, where a sentence is metaphorical if it contains at least one metaphorically used word. In this accounting, the distribution is almost a mirror-image of our setting, as 84% of sentences in News were labeled as metaphorical, whereas 18% of content words are tagged as such. The News partition was very difficult for the systems examined in Dunn (2013) – three of the four systems failed to predict any non-metaphorical sentences, and the one system that did so suffered from a </context>
</contexts>
<marker>Dunn, 2013</marker>
<rawString>Jonathan Dunn. 2013. What metaphor identification systems can tell us about metaphor-in-language. In Proceedings of the First Workshop on Metaphor in NLP, pages 1–10, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Fass</author>
</authors>
<title>Met*: A method for discriminating metonymy and metaphor by computer.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="16920" citStr="Fass, 1991" startWordPosition="2873" endWordPosition="2874">hnson, 1980), we expect certain basic metaphors to be highly ubiquitous in any corpus of texts, such as TIME IS SPACE or UP IS GOOD. To the extent that these metaphors are realized through frequent content words, we expect some cross-text generalization power for a unigram model. Perhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions of the VUAms14 terdam data, where topical sharing is minimal. Approaches to metaphor detection are often either rule-based or unsupervised (Martin, 1990; Fass, 1991; Shutova et al., 2010; Shutova and Sun, 2013; Li et al., 2013), although supervised approaches have recently been attempted with the advent of relatively large collections of metaphorannotated materials (Mohler et al., 2013; Hovy et al., 2013; Pasanek and Sculley, 2008; Gedigan et al., 2006). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as ver</context>
</contexts>
<marker>Fass, 1991</marker>
<rawString>Dan Fass. 1991. Met*: A method for discriminating metonymy and metaphor by computer. Computational Linguistics, 17(1):49–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>One sense per discourse.</title>
<date>1992</date>
<booktitle>In Proceedings of the Speech and Natural Language Workshop,</booktitle>
<pages>233--237</pages>
<contexts>
<context position="12356" citStr="Gale et al. (1992)" startWordPosition="2066" endWordPosition="2069">round that were all observed as metaphors in Set B. In the News data, a topic that deals with hurricane Katrina received a positive weight, as words of suffering and recovery from distaster are often used metaphorically when discussing other things: starved, severed, awash, damaged, relief, victim, distress, hits, swept, bounce, response, recovering, suffering. The part-of-speech features help improve recall across all datasets in Table 3, while concreteness features are effective only for some of the sets. 5 Discussion: Metaphor &amp; Word Sense The classical “one sense per discourse” finding of Gale et al. (1992) that words keep their senses within the same text 98% of the time suggests that 13 M U UPCT – U – P – C – T Set A cross-val. Set B cross-val. Train B : Test A Train A : Test B News P R F P R F P R F P R F P R F .11 1.0 .20 .12 1.0 .22 .11 1.0 .20 .12 1.0 .22 .18 1.0 .31 .72 .43 .53 .79 .54 .64 .58 .45 .50 .71 .28 .40 .62 .38 .47 .70 .47 .56 .76 .60 .67 .56 .50 .53 .72 .35 .47 .61 .43 .51 .58 .21 .31 .63 .28 .38 .44 .21 .29 .59 .18 .27 .55 .23 .32 .71 .46 .56 .76 .58 .66 .57 .48 .52 .70 .33 .45 .61 .41 .49 .70 .46 .55 .77 .58 .66 .56 .50 .53 .71 .34 .46 .61 .43 .50 .71 .43 .53 .78 .55 .65 .57 </context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>William Gale, Kenneth Church, and David Yarowsky. 1992. One sense per discourse. In Proceedings of the Speech and Natural Language Workshop, pages 233–237.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Gedigan</author>
<author>John Bryant</author>
<author>Srini Narayanan</author>
<author>Branimir Ciric</author>
</authors>
<title>Catching metaphors.</title>
<date>2006</date>
<booktitle>In Proceedings of the 3rd Workshop on Scalable Natural Language Understanding,</booktitle>
<pages>41--48</pages>
<location>New York.</location>
<contexts>
<context position="17213" citStr="Gedigan et al., 2006" startWordPosition="2917" endWordPosition="2920">erhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions of the VUAms14 terdam data, where topical sharing is minimal. Approaches to metaphor detection are often either rule-based or unsupervised (Martin, 1990; Fass, 1991; Shutova et al., 2010; Shutova and Sun, 2013; Li et al., 2013), although supervised approaches have recently been attempted with the advent of relatively large collections of metaphorannotated materials (Mohler et al., 2013; Hovy et al., 2013; Pasanek and Sculley, 2008; Gedigan et al., 2006). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as verbs3 or motion words only. Concreteness as a predictor of metaphoricity was discussed in Turney et al. (2011) in the context of concrete adjectives modifying abstract nouns. The POS features are inspired by the discussion of the preference and aversion of various POS towards metaphoricity in G</context>
</contexts>
<marker>Gedigan, Bryant, Narayanan, Ciric, 2006</marker>
<rawString>Matt Gedigan, John Bryant, Srini Narayanan, and Branimir Ciric. 2006. Catching metaphors. In Proceedings of the 3rd Workshop on Scalable Natural Language Understanding, pages 41–48, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Goatly</author>
</authors>
<title>The Language of Metaphors.</title>
<date>1997</date>
<location>Routledge, London.</location>
<contexts>
<context position="17825" citStr="Goatly (1997)" startWordPosition="3017" endWordPosition="3018">). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as verbs3 or motion words only. Concreteness as a predictor of metaphoricity was discussed in Turney et al. (2011) in the context of concrete adjectives modifying abstract nouns. The POS features are inspired by the discussion of the preference and aversion of various POS towards metaphoricity in Goatly (1997). Heintz et al. (2013) use LDA topics built on Wikipedia along with manually constructed seed lists for potential source and target topics in the broad target domain of governance, in order to identify sentences using lexica from both source and target domains as potentially containing metaphors. Bethard et al. (2009) use LDA topics built on BNC as features for classifying metaphorical and nonmetaphorical uses of 9 words in 450 sentences that use these words, modeling metaphorical vs nonmetaphorical contexts for these words. In both cases, LDA is used to capture the topical composition of a se</context>
</contexts>
<marker>Goatly, 1997</marker>
<rawString>Andrew Goatly. 1997. The Language of Metaphors. Routledge, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilana Heintz</author>
<author>Ryan Gabbard</author>
<author>Mahesh Srivastava</author>
<author>Dave Barner</author>
<author>Donald Black</author>
<author>Majorie Friedman</author>
<author>Ralph Weischedel</author>
</authors>
<title>Automatic Extraction of Linguistic Metaphors with LDA Topic Modeling.</title>
<date>2013</date>
<booktitle>In Proceedings of the First Workshop on Metaphor in NLP,</booktitle>
<pages>58--66</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="17847" citStr="Heintz et al. (2013)" startWordPosition="3019" endWordPosition="3022">ches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as verbs3 or motion words only. Concreteness as a predictor of metaphoricity was discussed in Turney et al. (2011) in the context of concrete adjectives modifying abstract nouns. The POS features are inspired by the discussion of the preference and aversion of various POS towards metaphoricity in Goatly (1997). Heintz et al. (2013) use LDA topics built on Wikipedia along with manually constructed seed lists for potential source and target topics in the broad target domain of governance, in order to identify sentences using lexica from both source and target domains as potentially containing metaphors. Bethard et al. (2009) use LDA topics built on BNC as features for classifying metaphorical and nonmetaphorical uses of 9 words in 450 sentences that use these words, modeling metaphorical vs nonmetaphorical contexts for these words. In both cases, LDA is used to capture the topical composition of a sentence; in contrast, w</context>
</contexts>
<marker>Heintz, Gabbard, Srivastava, Barner, Black, Friedman, Weischedel, 2013</marker>
<rawString>Ilana Heintz, Ryan Gabbard, Mahesh Srivastava, Dave Barner, Donald Black, Majorie Friedman, and Ralph Weischedel. 2013. Automatic Extraction of Linguistic Metaphors with LDA Topic Modeling. In Proceedings of the First Workshop on Metaphor in NLP, pages 58–66, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dirk Hovy</author>
<author>Shashank Srivastava</author>
<author>Sujay Kumar Jauhar</author>
<author>Mrinmaya Sachan</author>
<author>Kartik Goyal</author>
<author>Huying Li</author>
<author>Whitney Sanders</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying metaphorical word use with tree kernels.</title>
<date>2013</date>
<booktitle>In Proceedings of the First Workshop on Metaphor in NLP,</booktitle>
<pages>52--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, GA.</location>
<contexts>
<context position="17163" citStr="Hovy et al., 2013" startWordPosition="2909" endWordPosition="2912">xt generalization power for a unigram model. Perhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions of the VUAms14 terdam data, where topical sharing is minimal. Approaches to metaphor detection are often either rule-based or unsupervised (Martin, 1990; Fass, 1991; Shutova et al., 2010; Shutova and Sun, 2013; Li et al., 2013), although supervised approaches have recently been attempted with the advent of relatively large collections of metaphorannotated materials (Mohler et al., 2013; Hovy et al., 2013; Pasanek and Sculley, 2008; Gedigan et al., 2006). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as verbs3 or motion words only. Concreteness as a predictor of metaphoricity was discussed in Turney et al. (2011) in the context of concrete adjectives modifying abstract nouns. The POS features are inspired by the discussion of the preference and </context>
</contexts>
<marker>Hovy, Srivastava, Jauhar, Sachan, Goyal, Li, Sanders, Hovy, 2013</marker>
<rawString>Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar, Mrinmaya Sachan, Kartik Goyal, Huying Li, Whitney Sanders, and Eduard Hovy. 2013. Identifying metaphorical word use with tree kernels. In Proceedings of the First Workshop on Metaphor in NLP, pages 52–57, Atlanta, GA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob Koeling</author>
<author>Diana McCarthy</author>
<author>John Carroll</author>
</authors>
<title>Domain-specific sense distributions and predominant sense acquisition.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTEMNLP,</booktitle>
<pages>419--426</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Vancouver, Canada.</location>
<contexts>
<context position="13472" citStr="Koeling et al. (2005)" startWordPosition="2317" endWordPosition="2320"> .45 .61 .41 .49 .70 .46 .55 .77 .58 .66 .56 .50 .53 .71 .34 .46 .61 .43 .50 .71 .43 .53 .78 .55 .65 .57 .45 .51 .71 .29 .41 .62 .41 .49 Table 3: Ablation evaluations. Model M is a pseudo-system that classifies all instances as metaphors. if a word is used as a metaphor once in a text, it is very likely to be a metaphor if it is used again in the same text. Indeed, this is the reason for putting all words from the same text in the same fold in cross-validations, as training and testing on different parts of the same text would produce inflated estimates of metaphor classification performance. Koeling et al. (2005) extend the notion of discourse beyond a single text to a domain, such as articles on Finance, Sports, and a general BNC domain. For a set of words that each have at least one Finance and one Sports sense and not more than 12 senses in total, guessing the predominant sense in Finance and Sports yielded 77% and 76% precision, respectively. Our results with the unigram model show that guessing “metaphor” based on a sufficient proportion of previously observed metaphorical uses in the given domain yields about 76% precision for essays on the same topic. Thus, metaphoricity distinctions in sametop</context>
<context position="14831" citStr="Koeling et al. (2005)" startWordPosition="2545" endWordPosition="2548"> to their domain-specific predominant sense 4 of the time. Note that a domain-specific predominant sense may or may not be the same as the most frequent sense overall; similarly, a word’s tendency to be used metaphorically might be domain specific or general. The results for the BNC at large are likely to reflect general rather than domain-specific sense distributions. According to Koeling et al. (2005), guessing the predominant sense in the BNC yields 51% precision; our finding for BNC News is 62% precision for the unigram model. The difference could be due to the mixing of the BNC genres in Koeling et al. (2005), given the lower precision of metaphoricity prediction in non-news (Table 2). In all, our results suggest that the pattern of metaphorical and non-metaphorical use is in line with that of dominant word-sense for more and less topically restricted domains. 6 Related Work The extent to which different texts use similar metaphors was addressed by Pasanek and Sculley (2008) for corpora written by the same author. They studied metaphors of mind in the oeuvre of 7 authors, including John Milton and William Shakespeare. They created a set of metaphorical and non-metaphorical references to the mind u</context>
</contexts>
<marker>Koeling, McCarthy, Carroll, 2005</marker>
<rawString>Rob Koeling, Diana McCarthy, and John Carroll. 2005. Domain-specific sense distributions and predominant sense acquisition. In Proceedings of HLTEMNLP, pages 419–426, Vancouver, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Metaphors we live by.</title>
<date>1980</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="16322" citStr="Lakoff and Johnson, 1980" startWordPosition="2769" endWordPosition="2772">select. They also find good generalizations between some pairs of authors, due to borrowing or literary allusion. Studies using political texts, such as speeches by politicians or news articles discussing politically important events, documented repeated use of words from certain source domains, such as rejuvenation in Tony Blair’s speeches (CharterisBlack, 2005) or railroad metaphors in articles discussing political integration of Europe (Musolff, 2000). Our results regarding settings with substantial topical consistency second these observations. According to the Conceptual Metaphor theory (Lakoff and Johnson, 1980), we expect certain basic metaphors to be highly ubiquitous in any corpus of texts, such as TIME IS SPACE or UP IS GOOD. To the extent that these metaphors are realized through frequent content words, we expect some cross-text generalization power for a unigram model. Perhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions of the VUAms14 terdam data, where topical sharing is minimal. Approaches to metaphor detection are often either rule-based or unsupervised (Martin, 1990; Fass, 1991; </context>
</contexts>
<marker>Lakoff, Johnson, 1980</marker>
<rawString>George Lakoff and Mark Johnson. 1980. Metaphors we live by. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongsong Li</author>
<author>Kenny Q Zhu</author>
<author>Haixun Wang</author>
</authors>
<title>Data-driven metaphor recognition and explanation.</title>
<date>2013</date>
<journal>Transactions of the ACL,</journal>
<pages>1--379</pages>
<contexts>
<context position="16983" citStr="Li et al., 2013" startWordPosition="2883" endWordPosition="2886">hly ubiquitous in any corpus of texts, such as TIME IS SPACE or UP IS GOOD. To the extent that these metaphors are realized through frequent content words, we expect some cross-text generalization power for a unigram model. Perhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions of the VUAms14 terdam data, where topical sharing is minimal. Approaches to metaphor detection are often either rule-based or unsupervised (Martin, 1990; Fass, 1991; Shutova et al., 2010; Shutova and Sun, 2013; Li et al., 2013), although supervised approaches have recently been attempted with the advent of relatively large collections of metaphorannotated materials (Mohler et al., 2013; Hovy et al., 2013; Pasanek and Sculley, 2008; Gedigan et al., 2006). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as verbs3 or motion words only. Concreteness as a predictor of metaph</context>
</contexts>
<marker>Li, Zhu, Wang, 2013</marker>
<rawString>Hongsong Li, Kenny Q. Zhu, and Haixun Wang. 2013. Data-driven metaphor recognition and explanation. Transactions of the ACL, 1:379–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Martin</author>
</authors>
<title>A computational model of metaphor interpretation.</title>
<date>1990</date>
<publisher>Academic Press Professional, Inc.,</publisher>
<location>San Diego, CA, USA.</location>
<contexts>
<context position="16908" citStr="Martin, 1990" startWordPosition="2871" endWordPosition="2872">(Lakoff and Johnson, 1980), we expect certain basic metaphors to be highly ubiquitous in any corpus of texts, such as TIME IS SPACE or UP IS GOOD. To the extent that these metaphors are realized through frequent content words, we expect some cross-text generalization power for a unigram model. Perhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions of the VUAms14 terdam data, where topical sharing is minimal. Approaches to metaphor detection are often either rule-based or unsupervised (Martin, 1990; Fass, 1991; Shutova et al., 2010; Shutova and Sun, 2013; Li et al., 2013), although supervised approaches have recently been attempted with the advent of relatively large collections of metaphorannotated materials (Mohler et al., 2013; Hovy et al., 2013; Pasanek and Sculley, 2008; Gedigan et al., 2006). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated,</context>
</contexts>
<marker>Martin, 1990</marker>
<rawString>James Martin. 1990. A computational model of metaphor interpretation. Academic Press Professional, Inc., San Diego, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quinn McNemar</author>
</authors>
<title>Note on the sampling error of the difference between correlated proportions or percentages.</title>
<date>1947</date>
<journal>Psychometrika,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="10384" citStr="McNemar, 1947" startWordPosition="1754" endWordPosition="1755">ergent topics between texts in each of the partitions. It is also possible that the number of different texts in these partitions is insufficient for covering the metaphors that are common in these kinds of texts – recall that these partitions have small numbers of long texts, whereas the News partition has a larger number of short texts (see Table 1). 4.2 Beyond Baseline The addition of topic model, POS, and concreteness features produces a significant increase in recall across all evaluations (p &lt; 0.01), using McNemar’s test of the significance of differences between correlated proportions (McNemar, 1947). Even for Conversations, where recall improvement is the smallest and Fi score does not improve, the UPCT model recovers all 161 metaphors found by the unigrams plus 14 additional metaphors, yielding a significant result on the correlated test. We next investigate the relative contribution of the different types of features in the UPCT model by ablating each type and observing the effect on performance. Table 3 shows ablation results for essay and News data, where substantial improvements over the unigram baseline were produced. We observe, as expected, that the unigram features contributed t</context>
</contexts>
<marker>McNemar, 1947</marker>
<rawString>Quinn McNemar. 1947. Note on the sampling error of the difference between correlated proportions or percentages. Psychometrika, 12(2):153–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>David Bracewell</author>
<author>Marc Tomlinson</author>
<author>David Hinote</author>
</authors>
<title>Semantic signatures for example-based linguistic metaphor detection.</title>
<date>2013</date>
<booktitle>In Proceedings of the First Workshop on Metaphor in NLP,</booktitle>
<pages>27--35</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, GA.</location>
<contexts>
<context position="17144" citStr="Mohler et al., 2013" startWordPosition="2905" endWordPosition="2908"> expect some cross-text generalization power for a unigram model. Perhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions of the VUAms14 terdam data, where topical sharing is minimal. Approaches to metaphor detection are often either rule-based or unsupervised (Martin, 1990; Fass, 1991; Shutova et al., 2010; Shutova and Sun, 2013; Li et al., 2013), although supervised approaches have recently been attempted with the advent of relatively large collections of metaphorannotated materials (Mohler et al., 2013; Hovy et al., 2013; Pasanek and Sculley, 2008; Gedigan et al., 2006). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as verbs3 or motion words only. Concreteness as a predictor of metaphoricity was discussed in Turney et al. (2011) in the context of concrete adjectives modifying abstract nouns. The POS features are inspired by the discussion of </context>
</contexts>
<marker>Mohler, Bracewell, Tomlinson, Hinote, 2013</marker>
<rawString>Michael Mohler, David Bracewell, Marc Tomlinson, and David Hinote. 2013. Semantic signatures for example-based linguistic metaphor detection. In Proceedings of the First Workshop on Metaphor in NLP, pages 27–35, Atlanta, GA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Musolff</author>
</authors>
<title>Mirror images of Europe: Metaphors in the public debate about Europe in Britain and Germany. M¨unchen: Iudicium. Annotated data is available at http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.</title>
<date>2000</date>
<contexts>
<context position="16155" citStr="Musolff, 2000" startWordPosition="2749" endWordPosition="2750">ch of the authors separately, they present very high accuracies (85%-94%), suggesting that authors are highly self-consistent in the metaphors of mind they select. They also find good generalizations between some pairs of authors, due to borrowing or literary allusion. Studies using political texts, such as speeches by politicians or news articles discussing politically important events, documented repeated use of words from certain source domains, such as rejuvenation in Tony Blair’s speeches (CharterisBlack, 2005) or railroad metaphors in articles discussing political integration of Europe (Musolff, 2000). Our results regarding settings with substantial topical consistency second these observations. According to the Conceptual Metaphor theory (Lakoff and Johnson, 1980), we expect certain basic metaphors to be highly ubiquitous in any corpus of texts, such as TIME IS SPACE or UP IS GOOD. To the extent that these metaphors are realized through frequent content words, we expect some cross-text generalization power for a unigram model. Perhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions</context>
</contexts>
<marker>Musolff, 2000</marker>
<rawString>Andreas Musolff. 2000. Mirror images of Europe: Metaphors in the public debate about Europe in Britain and Germany. M¨unchen: Iudicium. Annotated data is available at http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Parker</author>
<author>David Graff</author>
<author>Junbo Kong</author>
<author>Ke Chen</author>
<author>Kazuaki Maeda</author>
</authors>
<date>2009</date>
<booktitle>English Gigaword Fourth Edition LDC2009T13. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="7541" citStr="Parker et al., 2009" startWordPosition="1243" endWordPosition="1246"> English words. The mean ratings, ranging 1-5, are binned in 0.25 increments; each bin is used as a binary feature. • Topic models (T): We use Latent Dirichlet Allocation (Blei et al., 2003) to derive a 100-topic model from the NYT corpus years 2003–2007 (Sandhaus, 2008) to represent common topics of public discussion. The NYT data was lemmatized using NLTK (Bird, 2006). We used the gensim toolkit (ˇReh˚uˇrek and Sojka, 2010) for building the models, with default parameters. The score assigned to an instance w on a topic t is logP(w|t) P (w) where P(w) were estimated from the Cigaword corpus (Parker et al., 2009). These features are based on the hypothesis that certain topics are likelier to be used as source domains for metaphors than others. 4 Results For each dataset, we present the results for the unigram model (baseline) and the results for the full model containing all the features. For crossvalidation results, all words from the same text were always placed in the same fold, to ensure that we are evaluating generalization across texts. 12 M Unigram UPCT F P R F P R F .20 .72 .43 .53 .70 .47 .56 .22 .79 .54 .64 .76 .60 .67 .20 .58 .45 .50 .56 .50 .53 .22 .71 .28 .40 .72 .35 .47 .31 .62 .38 .47 .</context>
</contexts>
<marker>Parker, Graff, Kong, Chen, Maeda, 2009</marker>
<rawString>Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2009. English Gigaword Fourth Edition LDC2009T13. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Pasanek</author>
<author>D Sculley</author>
</authors>
<title>Mining millions of metaphors.</title>
<date>2008</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>23--3</pages>
<contexts>
<context position="15204" citStr="Pasanek and Sculley (2008)" startWordPosition="2603" endWordPosition="2607">ns. According to Koeling et al. (2005), guessing the predominant sense in the BNC yields 51% precision; our finding for BNC News is 62% precision for the unigram model. The difference could be due to the mixing of the BNC genres in Koeling et al. (2005), given the lower precision of metaphoricity prediction in non-news (Table 2). In all, our results suggest that the pattern of metaphorical and non-metaphorical use is in line with that of dominant word-sense for more and less topically restricted domains. 6 Related Work The extent to which different texts use similar metaphors was addressed by Pasanek and Sculley (2008) for corpora written by the same author. They studied metaphors of mind in the oeuvre of 7 authors, including John Milton and William Shakespeare. They created a set of metaphorical and non-metaphorical references to the mind using excerpts from various texts written by these authors. Using cross-validation with unigram features for each of the authors separately, they present very high accuracies (85%-94%), suggesting that authors are highly self-consistent in the metaphors of mind they select. They also find good generalizations between some pairs of authors, due to borrowing or literary all</context>
<context position="17190" citStr="Pasanek and Sculley, 2008" startWordPosition="2913" endWordPosition="2916">ower for a unigram model. Perhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions of the VUAms14 terdam data, where topical sharing is minimal. Approaches to metaphor detection are often either rule-based or unsupervised (Martin, 1990; Fass, 1991; Shutova et al., 2010; Shutova and Sun, 2013; Li et al., 2013), although supervised approaches have recently been attempted with the advent of relatively large collections of metaphorannotated materials (Mohler et al., 2013; Hovy et al., 2013; Pasanek and Sculley, 2008; Gedigan et al., 2006). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as verbs3 or motion words only. Concreteness as a predictor of metaphoricity was discussed in Turney et al. (2011) in the context of concrete adjectives modifying abstract nouns. The POS features are inspired by the discussion of the preference and aversion of various POS tow</context>
</contexts>
<marker>Pasanek, Sculley, 2008</marker>
<rawString>Bradley Pasanek and D. Sculley. 2008. Mining millions of metaphors. Literary and Linguistic Computing, 23(3):345–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Gael Varoquaux</author>
<author>Alexandre Gramfort</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David</location>
<contexts>
<context position="6410" citStr="Pedregosa et al., 2011" startWordPosition="1053" endWordPosition="1056">served disagreements. We will report results for 10-fold crossvalidation on each of sets A and B, as well as 2We note that this top-level partition was used for many of the analyses discussed in (Steen et al., 2010). across prompts, where the machine learner would be trained on Set A and tested on Set B and vice versa. 3 Supervised Learning of Metaphor For this study, we consider each content-word token in a text as an instance to be classified as a metaphor or non-metaphor. We use the logistic regression classifier in the SKLL package (Blanchard et al., 2013), which is based on scikit-learn (Pedregosa et al., 2011), optimizing for F1 score (class “metaphor”). We consider the following features for metaphor detection. • Unigrams (U): All content words from the relevant training data are used as features, without stemming or lemmatization. • Part-of-Speech (P): We use Stanford POS tagger 3.3.0 and the full Penn Treebank tagset for content words (tags starting with A, N, V, and J), removing the auxiliaries have, be, do. • Concreteness (C): We use Brysbaert et al. (2013) database of concreteness ratings for about 40,000 English words. The mean ratings, ranging 1-5, are binned in 0.25 increments; each bin is</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake VanderPlas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Group Pragglejaz</author>
</authors>
<title>MIP: A Method for Identifying Metaphorically Used Words</title>
<date>2007</date>
<booktitle>in Discourse. Metaphor and Symbol,</booktitle>
<pages>22--1</pages>
<contexts>
<context position="3741" citStr="Pragglejaz, 2007" startWordPosition="607" endWordPosition="609">st to be carried out at a later date with a more advanced system; the current experiments are performed using crossvalidation on the remaining 90 fragments: 10-fold on News, 9-fold on Conversation, 11 on Fiction, and 12 on Academic. All instances from the same text were always placed in the same fold. 1http://www2.let.vu.nl/oz/metaphorlab/metcor/search/index.html 11 Proceedings of the Second Workshop on Metaphor in NLP, pages 11–17, Baltimore, MD, USA, 26 June 2014. c�2014 Association for Computational Linguistics The data is annotated using MIP-VU procedure. It is based on the MIP procedure (Pragglejaz, 2007), extending it to handle metaphoricity through reference (such as marking did as a metaphor in As the weather broke up, so did their friendship) and allow for explicit coding of difficult cases where a group of annotators could not arrive at a consensus. The tagset is rich and is organized hierarchically, detecting various types of metaphors, words that flag the presense of metaphors, etc. In this paper, we consider only the top-level partition, labeling all content words with the tag “function=mrw” (metaphor-related word) as metaphors, while all other content words are labeled as non-metaphor</context>
</contexts>
<marker>Pragglejaz, 2007</marker>
<rawString>Group Pragglejaz. 2007. MIP: A Method for Identifying Metaphorically Used Words in Discourse. Metaphor and Symbol, 22(1):1–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software Framework for Topic Modelling with Large Corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<publisher>ELRA.</publisher>
<location>Valletta, Malta,</location>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45– 50, Valletta, Malta, May. ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times Annotated Corpus.</title>
<date>2008</date>
<publisher>LDC Catalog</publisher>
<location>No:</location>
<contexts>
<context position="7192" citStr="Sandhaus, 2008" startWordPosition="1185" endWordPosition="1186">ata are used as features, without stemming or lemmatization. • Part-of-Speech (P): We use Stanford POS tagger 3.3.0 and the full Penn Treebank tagset for content words (tags starting with A, N, V, and J), removing the auxiliaries have, be, do. • Concreteness (C): We use Brysbaert et al. (2013) database of concreteness ratings for about 40,000 English words. The mean ratings, ranging 1-5, are binned in 0.25 increments; each bin is used as a binary feature. • Topic models (T): We use Latent Dirichlet Allocation (Blei et al., 2003) to derive a 100-topic model from the NYT corpus years 2003–2007 (Sandhaus, 2008) to represent common topics of public discussion. The NYT data was lemmatized using NLTK (Bird, 2006). We used the gensim toolkit (ˇReh˚uˇrek and Sojka, 2010) for building the models, with default parameters. The score assigned to an instance w on a topic t is logP(w|t) P (w) where P(w) were estimated from the Cigaword corpus (Parker et al., 2009). These features are based on the hypothesis that certain topics are likelier to be used as source domains for metaphors than others. 4 Results For each dataset, we present the results for the unigram model (baseline) and the results for the full mode</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The New York Times Annotated Corpus. LDC Catalog No: LDC2008T19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
<author>Lin Sun</author>
</authors>
<title>Unsupervised metaphor identification using hierarchical graph factorization clustering.</title>
<date>2013</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>978--988</pages>
<contexts>
<context position="16965" citStr="Shutova and Sun, 2013" startWordPosition="2879" endWordPosition="2882">sic metaphors to be highly ubiquitous in any corpus of texts, such as TIME IS SPACE or UP IS GOOD. To the extent that these metaphors are realized through frequent content words, we expect some cross-text generalization power for a unigram model. Perhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions of the VUAms14 terdam data, where topical sharing is minimal. Approaches to metaphor detection are often either rule-based or unsupervised (Martin, 1990; Fass, 1991; Shutova et al., 2010; Shutova and Sun, 2013; Li et al., 2013), although supervised approaches have recently been attempted with the advent of relatively large collections of metaphorannotated materials (Mohler et al., 2013; Hovy et al., 2013; Pasanek and Sculley, 2008; Gedigan et al., 2006). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as verbs3 or motion words only. Concreteness as a p</context>
</contexts>
<marker>Shutova, Sun, 2013</marker>
<rawString>Ekaterina Shutova and Lin Sun. 2013. Unsupervised metaphor identification using hierarchical graph factorization clustering. In Proceedings of HLT-NAACL, pages 978–988.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ekaterina Shutova</author>
<author>Simone Teufel</author>
</authors>
<title>Metaphor corpus annotated for source - target domain mappings.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<pages>3255--3261</pages>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors,</editor>
<location>Valletta, Malta,</location>
<contexts>
<context position="19216" citStr="Shutova and Teufel (2010)" startWordPosition="3251" endWordPosition="3254">based on various theories of metaphor on VUAmsterdam data. The evaluations were done at sentence level, where a sentence is metaphorical if it contains at least one metaphorically used word. In this accounting, the distribution is almost a mirror-image of our setting, as 84% of sentences in News were labeled as metaphorical, whereas 18% of content words are tagged as such. The News partition was very difficult for the systems examined in Dunn (2013) – three of the four systems failed to predict any non-metaphorical sentences, and the one system that did so suffered from a low recall of 3as in Shutova and Teufel (2010) metaphors, 20%. Dunn (2013) shows that the different systems he compared had relatively low agreement (κ &lt; 0.3); he interprets this finding as suggesting that the different theories underlying the models capture different aspects of metaphoricity and therefore detect different metaphors. It is therefore likely that features derived from the various models would fruitfully complement each other in a supervised learning setting; our findings suggest that the simplest building block – that of a unigram model – should not be ignored in such experiments. 7 Conclusions We address supervised learnin</context>
</contexts>
<marker>Shutova, Teufel, 2010</marker>
<rawString>Ekaterina Shutova and Simone Teufel. 2010. Metaphor corpus annotated for source - target domain mappings. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel Tapias, editors, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), pages 3255– 3261, Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ekaterina Shutova</author>
<author>Lin Sun</author>
<author>Anna Korhonen</author>
</authors>
<title>Metaphor identification using verb and noun clustering.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1002--1010</pages>
<contexts>
<context position="16942" citStr="Shutova et al., 2010" startWordPosition="2875" endWordPosition="2878">, we expect certain basic metaphors to be highly ubiquitous in any corpus of texts, such as TIME IS SPACE or UP IS GOOD. To the extent that these metaphors are realized through frequent content words, we expect some cross-text generalization power for a unigram model. Perhaps the share of these basic metaphors in all metaphors in a text is reflected most faithfully in the peformance of the unigram model on the non-News partitions of the VUAms14 terdam data, where topical sharing is minimal. Approaches to metaphor detection are often either rule-based or unsupervised (Martin, 1990; Fass, 1991; Shutova et al., 2010; Shutova and Sun, 2013; Li et al., 2013), although supervised approaches have recently been attempted with the advent of relatively large collections of metaphorannotated materials (Mohler et al., 2013; Hovy et al., 2013; Pasanek and Sculley, 2008; Gedigan et al., 2006). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as verbs3 or motion words on</context>
</contexts>
<marker>Shutova, Sun, Korhonen, 2010</marker>
<rawString>Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010. Metaphor identification using verb and noun clustering. In Proceedings of the 23rd International Conference on Computational Linguistics (COLING), pages 1002–1010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Steen</author>
</authors>
<title>Aletta Dorst, Berenike Herrmann, Anna Kaal, Tina Krennmayr, and Trijntje Pasma.</title>
<date>2010</date>
<location>Amsterdam: John Benjamins.</location>
<marker>Steen, 2010</marker>
<rawString>Gerard Steen, Aletta Dorst, Berenike Herrmann, Anna Kaal, Tina Krennmayr, and Trijntje Pasma. 2010. A Methodfor Linguistic Metaphor Identification. Amsterdam: John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>252--259</pages>
<contexts>
<context position="2740" citStr="Toutanova et al., 2003" startWordPosition="447" endWordPosition="450">es and the VUAmsterdam corpus (Steen et al., 2010),1 containing articles from four genres sampled from the BNC. Table 1 shows the sizes of the six sets, as well as the proportion of metaphors in them; the following sections explain their composition. Data #Texts #NVAR #metaphors tokens (%) News 49 18,519 3,405 (18%) Fiction 11 17,836 2,497 (14%) Academic 12 29,469 3,689 (13%) Conversation 18 15,667 1,149 ( 7%) Essay Set A 85 21,838 2,368 (11%) Essay Set B 79 22,662 2,745 (12%) Table 1: Datasets used in this study. NVAR = Nouns, Verbs, Adjectives, Adverbs, as tagged by the Stanford POS tagger (Toutanova et al., 2003). 2.1 VUAmsterdam Data The dataset consists of 117 fragments sampled across four genres: Academic, News, Conversation, and Fiction. Each genre is represented by approximately the same number of tokens, although the number of texts differs greatly, where the news archive has the largest number of texts. We randomly sampled 23% of the texts from each genre to set aside for a blind test to be carried out at a later date with a more advanced system; the current experiments are performed using crossvalidation on the remaining 90 fragments: 10-fold on News, 9-fold on Conversation, 11 on Fiction, and</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network. In Proceedings of NAACL, pages 252–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Yair Neuman</author>
<author>Dan Assaf</author>
<author>Yohai Cohen</author>
</authors>
<title>Literal and metaphorical sense identification through concrete and abstract context.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>680--690</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17628" citStr="Turney et al. (2011)" startWordPosition="2985" endWordPosition="2988">roaches have recently been attempted with the advent of relatively large collections of metaphorannotated materials (Mohler et al., 2013; Hovy et al., 2013; Pasanek and Sculley, 2008; Gedigan et al., 2006). These approaches are difficult to compare to our results, as these typically are not whole texts but excerpts, and only certain kinds of metaphors are annotated, such as metaphors about governance or about the mind, or only words belonging to certain syntactic or semantic class are annotated, such as verbs3 or motion words only. Concreteness as a predictor of metaphoricity was discussed in Turney et al. (2011) in the context of concrete adjectives modifying abstract nouns. The POS features are inspired by the discussion of the preference and aversion of various POS towards metaphoricity in Goatly (1997). Heintz et al. (2013) use LDA topics built on Wikipedia along with manually constructed seed lists for potential source and target topics in the broad target domain of governance, in order to identify sentences using lexica from both source and target domains as potentially containing metaphors. Bethard et al. (2009) use LDA topics built on BNC as features for classifying metaphorical and nonmetapho</context>
</contexts>
<marker>Turney, Neuman, Assaf, Cohen, 2011</marker>
<rawString>Peter Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification through concrete and abstract context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 680– 690, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010267">
<title confidence="0.905142">
Information density, Heaps’ Law, and perception of factiness in news
</title>
<author confidence="0.763936">
Miriam Boon
</author>
<affiliation confidence="0.615851">
Technology and Social Behavior, Northwestern University
Evanston, IL 60208, USA
</affiliation>
<email confidence="0.963055">
MiriamBoon2012@u.northwestern.edu
</email>
<sectionHeader confidence="0.99302" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99992725">
Seeking information online can be an exer-
cise in time wasted wading through repeti-
tive, verbose text with little actual content.
Some documents are more densely popu-
lated with factoids (fact-like claims) than
others. The densest documents are poten-
tially the most efficient use of time, likely
to include the most information. Thus
some measure of “factiness” might be use-
ful to readers. Based on crowdsourced rat-
ings of the factual content of 772 online
news articles, we find that after controlling
for widely varying document length using
Heaps’ Law, a significant positive correla-
tion exists between perceived factual con-
tent and relative information entropy.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999899071428571">
In today’s information-based society, finding ac-
curate information is of concern to everyone.
There are many obstacles to this goal. Not all peo-
ple are equally skilled at judging the veracity of
a factoid (a term used here to indicate something
that is stated as a fact, but that may or may not ac-
tually be true.). Nor is it always easy to find the
single drop of content you need amidst the oceans
of the Internet. Even for those equipped with both
skill and access, time is always a limiting factor.
It is this last problem with which this paper is
concerned. How can we identify content that most
efficiently conveys the most information, given
that any information seeker’s time is limited?
</bodyText>
<subsectionHeader confidence="0.975463">
1.1 The difficulty with factoids
</subsectionHeader>
<bodyText confidence="0.999890470588235">
Imagine that we must select from a set of
documents those that efficiently convey the
most information in the fewest words possi-
ble; that is, those with the highest factoid rate,
count(factoids)/count(words). A human do-
ing this by hand would count the factoids and
words in each document. Automating this exact
approach would require ‘teaching’ the computer
to identify unique factoids in a document, which
requires being able to recognize and discard re-
dundant factoids, which requires at least a rudi-
mentary understanding of each factoid’s meaning.
These are all difficult tasks for a computer.
Luckily, to achieve our goal, we don’t need to
know which sentences are factoids. What we need
is a good heuristic estimate of information density
that computers can easily calculate.
</bodyText>
<subsectionHeader confidence="0.99807">
1.2 Linking vocabulary to factoids
</subsectionHeader>
<bodyText confidence="0.999977666666667">
To insert new information into a text, an author
must add words, making the document longer.
While the new information can sometimes be con-
veyed using the same vocabulary as the rest of
the text, if the information is sufficiently different
from what is already present, it will also likely in-
troduce new vocabulary words.
The result is that the introduction of a new fac-
toid into a text is likely to also introduce new vo-
cabulary, unless it is redundant. Thus, the more
non-redundant factoids a text contains, the more
varied the vocabulary of the text is likely to be.
</bodyText>
<subsectionHeader confidence="0.973182">
1.3 From vocabulary to relative entropy
</subsectionHeader>
<bodyText confidence="0.99856775">
Vocabulary is commonly used in connection with
Shannon’s information entropy to measure such
things as surprisal, redundancy, perplexity, and, of
course, information density (Shannon, 1949; Mc-
Farlane et al., 2009).
Entropy models text as being created via a
Markov process. In its most basic form, it can be
written as:
</bodyText>
<equation confidence="0.993938333333333">
L
H = −K E pilog2pi (1)
i=0
</equation>
<bodyText confidence="0.996770666666667">
where K is a constant chosen based on the units,
L is the length of the document, and pi is the
probability of the ith word. This equation works
</bodyText>
<page confidence="0.990933">
33
</page>
<note confidence="0.508203">
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 33–37,
</note>
<page confidence="0.345421">
Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics
</page>
<bodyText confidence="0.999801068965517">
equally well whether it is used for unigrams, bi-
grams, or trigrams.
Consider for a moment the relationship between
entropy and length, vocabulary, and the probabil-
ity of each vocabulary word. Entropy increases
as both document length and vocabulary increase.
Words with lower probability increase entropy
more than those with higher probabilities. For
this study, probabilities were calculated based on
corpus-wide frequencies. This means that, in the-
ory, a large number of the words in a document
could have very low probability.
Given two documents of equal length on the
same topic, only one of which is rich in infor-
mation, we might wonder why the information-
poor document is, relatively speaking, so long or
the information-rich document is so short. This
can be explained by noting that: 1. “translation”
into simpler versions of a language often leads to
a longer text, 2. simple versions of languages gen-
erally consist of the most common words in that
language, and 3. words that are less common of-
ten have more specific, information-dense, com-
plex meanings. Similarly, inefficient word choices
typically make excessive use of highly probable
function words, which do not increase the entropy
as much as less common words. Thus, we can ex-
pect the entropy to be higher for the denser docu-
ment.
</bodyText>
<subsectionHeader confidence="0.967099">
1.4 Controlling for document length with
Heaps‘ Law
</subsectionHeader>
<bodyText confidence="0.999936588235294">
While entropy may not rise as fast with the repe-
tition or addition of highly probable words, how-
ever, every word added does still increase the en-
tropy. This follows naturally from the fact that for
every word added, another term is added to the
summation. We can try to compensate by dividing
by document length. But dividing by document
length doesn’t remove this dependency. I propose
that this is because, as Heap’s Law tells us, the
vocabulary used in a document has a positive re-
lationship with document size (Heaps, 1978). To
control for this effect, I fit a curve for unigrams,
bigrams, and trigrams to create a model for these
relationships; an example can be seen in Figure 1.
I then used that model to calculate the expected
document length, expected entropy, and relative
entropy, as follows:
</bodyText>
<equation confidence="0.994622166666666">
Lexp = 10(log10v−b)/m (2)
Hexp = HLexp (3)
L
H
Hrel = (4)
Hexp
</equation>
<bodyText confidence="0.9558147">
Here the subscript ‘exp’ stands for ‘expected’
and the subscript ‘rel’ for ‘relative.’ This cal-
culation eliminates the dependency on document
length.
Figure 1: Top: As you can see there is a strong re-
lationship between document length and entropy.
R2=0.992, p &gt; F: &lt; 0.0001. Bottom: Relative
entropy, which controls for that relationship, no
longer has a significant nor strong relationship
with document length. R2=0.0017, p &gt; F: 0.2425
</bodyText>
<sectionHeader confidence="0.929337" genericHeader="introduction">
2 Data and Analysis
</sectionHeader>
<bodyText confidence="0.999699">
To further pursue the hypothesis that residual en-
tropy could be used to identify news articles with
lots of factoids, and thus, a sense of ‘factiness,’ a
labeled data set is necessary. Lots of websites al-
low users to rate articles, but those ratings don’t
have anything to do with the presence of factoids.
Labeling a data set of adequate size by hand would
be tedious, time-consuming, and costly.
</bodyText>
<page confidence="0.99621">
34
</page>
<figureCaption confidence="0.666092">
Figure 2: Mousing over the question makes the
text “Is it based on facts or opinions?” appear in
pale grey text. Clicking on the question mark icon
next to the question, “Is this story factual?” reveals
an explanation of what the user should be rating.
</figureCaption>
<subsectionHeader confidence="0.998349">
2.1 Crowdsourcing with NewsTrust
</subsectionHeader>
<bodyText confidence="0.999866720930233">
Fortunately, a project called NewsTrust provided a
feasible alternative. NewsTrust, founded in 2005
by Fabrice Florin, created four sets of specific re-
view questions designed to inspire reviewers to
think critically about the quality of articles they re-
view. NewsTrust partnered with independent aca-
demic researchers Cliff Lampe and Kelly Garrett
to validate the review questions. They jointly ad-
ministered a survey in which respondents were
asked to complete one of the review instruments
regarding either the original version of an article
or blog post, or a degraded version.
The independent study found that even the
less experienced, less knowledgeable readers were
able to distinguish between the two versions of
the story. The shortest review instrument, with
only one question, had the most discriminating
power, while the slightly longer normative re-
view instrument (which added five more ques-
tions) yielded responses from non-experts that
most closely matched those of NewsTrust’s expert
journalists (Lampe and Garrett, 2007; Florin et
al., 2006; Florin, 2009).
Using their validated survey instrument, New-
sTrust created a system that allowed users to read
articles elsewhere, rate them using one of the four
review instruments, and even rate other NewsTrust
users’ reviews of articles. Each user has a trust-
worthiness rating (which can be bolstered by be-
coming validated as a journalist expert), and each
article has a composite rating, a certainty level for
that rating, reviews, and ratings of reviews.
One of the dimensions of journalistic quality
for which NewsTrust users rate articles is called
‘facts’. This can be taken as an aspect of “facti-
ness”: the extent to which people perceived the
article as truthful and factual. It follows that, to
the extent that the users are making a good-faith
attempt to rate articles based on facts regardless of
the soundness of their judgment about what is or
is not true, articles with a high rating for ‘facts’
should have more factoids, and therefore a higher
density of information.
</bodyText>
<subsectionHeader confidence="0.998486">
2.2 Data acquisition
</subsectionHeader>
<bodyText confidence="0.999967">
When this research project was launched, New-
sTrust had recently been acquired by the Poynter
Institute. Although they were open to making their
data available for research purposes, they were not
yet able to access the data in order to do so. In-
stead, the review data for over 11000 stories from
NewsTrust’s political section were retrieved using
Python, Requests, and Beautiful Soup. A combi-
nation of Alchemy API, digital library archives,
and custom scrapers for 19 different publication
websites were used to harvest the corresponding
article texts.
It quickly became clear, however, that it would
not be possible to completely capture all 11,000
articles. Some of the independent blogs and web-
sites no longer existed. Others had changed their
link structure, making it difficult to find the cor-
rect article. A great deal of content was behind
paywalls, or simply did not have a webpage struc-
ture that lent itself to clean extraction. As the text
would be used for automated analysis, it was es-
sential that the extracted text be as clean of de-
tritus as possible. As a result, the dataset shrank
from a potential 11,000 rated articles to only 3300
for which I could be confident of having clean
text. Approximately 2600 of those articles have
been rated by at least one NewsTrust user based
on factiness, and after removing any with fewer
than four facts ratings, the data set shrank fur-
ther to only 8051 articles. Unigrams, bigrams, and
trigrams were extracted from these articles using
the Natural Language Toolkit, NLTK; all text was
lowercased, and only alphanumeric words were
included.
</bodyText>
<subsectionHeader confidence="0.999234">
2.3 Analysis
</subsectionHeader>
<bodyText confidence="0.999967333333333">
The relationship between length and vocabulary
was modeled using the optimize toolkit from
SciPy, and visualized with MatPlotLib. The result-
</bodyText>
<footnote confidence="0.926488">
1One outlier document was removed.
</footnote>
<page confidence="0.998258">
35
</page>
<figureCaption confidence="0.9272515">
Figure 3: Top: Bivariate fit for bigrams. Bottom:
Oneway ANOVA for bigrams.
</figureCaption>
<bodyText confidence="0.999848">
ing relationship was used to calculate the relative
entropy for each document.
For bivariate analysis, 772 of these documents
could be used2. But for the oneway analysis,
documents needed to be separated into two dis-
tinct clusters. We used Weka’s K-Means cluster-
ing algorithm to find the location of three clus-
ters. The 90% confidence interval for each ar-
ticle (calculated using the invididual user ratings
for ‘facts’) was used to determine cluster member-
ship. That is, articles for which that confidence
interval would overlap with both the upper and
lower cluster were discarded (738 documents in
total). This process was repeated for 80 and 85%
confidence levels; they yielded more data points
(198 and 458), a higher level of significance, and
a lower R2. A 95% confidence level did not yield
enough articles with a low facts rating to analyze.
</bodyText>
<sectionHeader confidence="0.999882" genericHeader="background">
3 Results and Discussion
</sectionHeader>
<bodyText confidence="0.98868865">
The bivariate analysis showed a small but signif-
icant positive relationship between factual rating
233 documents with particularly low confidence levels for
their rating were removed
and relative entropy as calculated for unigrams, bi-
grams, and to a lesser extent, trigrams. The results
can be seen in Table 1 and Figure 3. These re-
lationships strengthened according to the ANOVA
for the more distinct high and low factiness classi-
fications.
If we accept the assumption that the articles
rated by NewsTrust users as highly factual will
contain a higher density of factoids, then this re-
sult supports the hypothesis that relative entropy is
positively correlated with that characteristic. Con-
versely, if we accept the assumption that entropy
should be correlated with factoid density, then this
result supports the claim that NewsTrust users ef-
fectively identify articles that are more informa-
tion dense. Future work on the fact-rated sub-
</bodyText>
<table confidence="0.9985922">
Unigram Bigram Trigram
Bivar. R2 0.033 0.032 0.014
p &gt; F &lt; 0.0001 &lt; 0.0001 &lt; 0.0008
Oneway R2 0.086 0.084 0.082
p &gt; Iti 0.0154 0.0163 0.0178
</table>
<tableCaption confidence="0.79016">
Table 1: Bivariate analysis (n = 772) and Oneway
ANOVA (n = 68).
</tableCaption>
<bodyText confidence="0.999924416666667">
corpus has two obvious directions. First, and most
closely related to the work described in this pa-
per, is the goal of proving either assumption in
a more controlled experiment. If one of these
assumptions can be supported, then it strength-
ens the claim about the other, which will be in-
teresting from both a linguistic perspective, and
a human-computer interaction perspective. The
other avenue of inquiry that follows naturally from
this work is to look for other textual features that
might, in combination, enable the automatic pre-
diction of fact ratings based on article text.
</bodyText>
<sectionHeader confidence="0.998361" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999930888888889">
This work was partly supported by the Technol-
ogy and Social Behavior program at Northwest-
ern University, the National Science Foundation,
the Knight Foundation, and Google. Many thanks
to Dr. Darren Gergle for his insight on the larger
NewsTrust data set, to Dr. Janet Pierrehumbert
for her guidance on entropy and factiness, and to
Dr. Larry Birnbaum for his intellectual guidance
as well as his assistance on this paper.
</bodyText>
<page confidence="0.997478">
36
</page>
<sectionHeader confidence="0.995865" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999834157894737">
Fabrice Florin, Cliff Lampe, and Kelly Garrett. 2006.
Survey report summary - NewsTrust.
Fabrice Florin. 2009. NewsTrust communications
2009 report. Technical report.
Harold Stanley Heaps. 1978. Information retrieval:
Computational and theoretical aspects. Academic
Press, Inc.
Cliff Lampe and R. Kelly Garrett. 2007. It’s all news
to me: The effect of instruments on ratings provi-
sion. In System Sciences, 2007. HICSS 2007. 40th
Annual Hawaii International Conference on, page
180b180b.
Delano J. McFarlane, Noemie Elhadad, and Rita
Kukafka. 2009. Perplexity analysis of obesity news
coverage. AMIA Annual Symposium Proceedings,
2009:426–430. 00001.
Claude E. Shannon. 1949. The mathematical theory
of communication. Urbana, University of Illinois
Press.
</reference>
<page confidence="0.999605">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.197293">
<title confidence="0.903655">Information density, Heaps’ Law, and perception of factiness in news</title>
<author confidence="0.494406">Miriam</author>
<note confidence="0.565715">Technology and Social Behavior, Northwestern Evanston, IL 60208, MiriamBoon2012@u.northwestern.edu</note>
<abstract confidence="0.997637">Seeking information online can be an exercise in time wasted wading through repetitive, verbose text with little actual content. Some documents are more densely populated with factoids (fact-like claims) than others. The densest documents are potentially the most efficient use of time, likely to include the most information. Thus some measure of “factiness” might be useful to readers. Based on crowdsourced ratings of the factual content of 772 online news articles, we find that after controlling for widely varying document length using Heaps’ Law, a significant positive correlation exists between perceived factual content and relative information entropy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fabrice Florin</author>
<author>Cliff Lampe</author>
<author>Kelly Garrett</author>
</authors>
<date>2006</date>
<note>Survey report summary - NewsTrust.</note>
<contexts>
<context position="8153" citStr="Florin et al., 2006" startWordPosition="1334" endWordPosition="1337"> were asked to complete one of the review instruments regarding either the original version of an article or blog post, or a degraded version. The independent study found that even the less experienced, less knowledgeable readers were able to distinguish between the two versions of the story. The shortest review instrument, with only one question, had the most discriminating power, while the slightly longer normative review instrument (which added five more questions) yielded responses from non-experts that most closely matched those of NewsTrust’s expert journalists (Lampe and Garrett, 2007; Florin et al., 2006; Florin, 2009). Using their validated survey instrument, NewsTrust created a system that allowed users to read articles elsewhere, rate them using one of the four review instruments, and even rate other NewsTrust users’ reviews of articles. Each user has a trustworthiness rating (which can be bolstered by becoming validated as a journalist expert), and each article has a composite rating, a certainty level for that rating, reviews, and ratings of reviews. One of the dimensions of journalistic quality for which NewsTrust users rate articles is called ‘facts’. This can be taken as an aspect of </context>
</contexts>
<marker>Florin, Lampe, Garrett, 2006</marker>
<rawString>Fabrice Florin, Cliff Lampe, and Kelly Garrett. 2006. Survey report summary - NewsTrust.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrice Florin</author>
</authors>
<title>NewsTrust communications</title>
<date>2009</date>
<tech>Technical report.</tech>
<contexts>
<context position="8168" citStr="Florin, 2009" startWordPosition="1338" endWordPosition="1339">te one of the review instruments regarding either the original version of an article or blog post, or a degraded version. The independent study found that even the less experienced, less knowledgeable readers were able to distinguish between the two versions of the story. The shortest review instrument, with only one question, had the most discriminating power, while the slightly longer normative review instrument (which added five more questions) yielded responses from non-experts that most closely matched those of NewsTrust’s expert journalists (Lampe and Garrett, 2007; Florin et al., 2006; Florin, 2009). Using their validated survey instrument, NewsTrust created a system that allowed users to read articles elsewhere, rate them using one of the four review instruments, and even rate other NewsTrust users’ reviews of articles. Each user has a trustworthiness rating (which can be bolstered by becoming validated as a journalist expert), and each article has a composite rating, a certainty level for that rating, reviews, and ratings of reviews. One of the dimensions of journalistic quality for which NewsTrust users rate articles is called ‘facts’. This can be taken as an aspect of “factiness”: th</context>
</contexts>
<marker>Florin, 2009</marker>
<rawString>Fabrice Florin. 2009. NewsTrust communications 2009 report. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Stanley Heaps</author>
</authors>
<title>Information retrieval: Computational and theoretical aspects.</title>
<date>1978</date>
<publisher>Academic Press, Inc.</publisher>
<contexts>
<context position="5613" citStr="Heaps, 1978" startWordPosition="922" endWordPosition="923"> higher for the denser document. 1.4 Controlling for document length with Heaps‘ Law While entropy may not rise as fast with the repetition or addition of highly probable words, however, every word added does still increase the entropy. This follows naturally from the fact that for every word added, another term is added to the summation. We can try to compensate by dividing by document length. But dividing by document length doesn’t remove this dependency. I propose that this is because, as Heap’s Law tells us, the vocabulary used in a document has a positive relationship with document size (Heaps, 1978). To control for this effect, I fit a curve for unigrams, bigrams, and trigrams to create a model for these relationships; an example can be seen in Figure 1. I then used that model to calculate the expected document length, expected entropy, and relative entropy, as follows: Lexp = 10(log10v−b)/m (2) Hexp = HLexp (3) L H Hrel = (4) Hexp Here the subscript ‘exp’ stands for ‘expected’ and the subscript ‘rel’ for ‘relative.’ This calculation eliminates the dependency on document length. Figure 1: Top: As you can see there is a strong relationship between document length and entropy. R2=0.992, p </context>
</contexts>
<marker>Heaps, 1978</marker>
<rawString>Harold Stanley Heaps. 1978. Information retrieval: Computational and theoretical aspects. Academic Press, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cliff Lampe</author>
<author>R Kelly Garrett</author>
</authors>
<title>It’s all news to me: The effect of instruments on ratings provision.</title>
<date>2007</date>
<booktitle>In System Sciences,</booktitle>
<pages>180--180</pages>
<contexts>
<context position="8132" citStr="Lampe and Garrett, 2007" startWordPosition="1330" endWordPosition="1333">rvey in which respondents were asked to complete one of the review instruments regarding either the original version of an article or blog post, or a degraded version. The independent study found that even the less experienced, less knowledgeable readers were able to distinguish between the two versions of the story. The shortest review instrument, with only one question, had the most discriminating power, while the slightly longer normative review instrument (which added five more questions) yielded responses from non-experts that most closely matched those of NewsTrust’s expert journalists (Lampe and Garrett, 2007; Florin et al., 2006; Florin, 2009). Using their validated survey instrument, NewsTrust created a system that allowed users to read articles elsewhere, rate them using one of the four review instruments, and even rate other NewsTrust users’ reviews of articles. Each user has a trustworthiness rating (which can be bolstered by becoming validated as a journalist expert), and each article has a composite rating, a certainty level for that rating, reviews, and ratings of reviews. One of the dimensions of journalistic quality for which NewsTrust users rate articles is called ‘facts’. This can be t</context>
</contexts>
<marker>Lampe, Garrett, 2007</marker>
<rawString>Cliff Lampe and R. Kelly Garrett. 2007. It’s all news to me: The effect of instruments on ratings provision. In System Sciences, 2007. HICSS 2007. 40th Annual Hawaii International Conference on, page 180b180b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Delano J McFarlane</author>
<author>Noemie Elhadad</author>
<author>Rita Kukafka</author>
</authors>
<title>Perplexity analysis of obesity news coverage.</title>
<date>2009</date>
<booktitle>AMIA Annual Symposium Proceedings,</booktitle>
<volume>2009</volume>
<pages>00001</pages>
<contexts>
<context position="3251" citStr="McFarlane et al., 2009" startWordPosition="521" endWordPosition="525">e information is sufficiently different from what is already present, it will also likely introduce new vocabulary words. The result is that the introduction of a new factoid into a text is likely to also introduce new vocabulary, unless it is redundant. Thus, the more non-redundant factoids a text contains, the more varied the vocabulary of the text is likely to be. 1.3 From vocabulary to relative entropy Vocabulary is commonly used in connection with Shannon’s information entropy to measure such things as surprisal, redundancy, perplexity, and, of course, information density (Shannon, 1949; McFarlane et al., 2009). Entropy models text as being created via a Markov process. In its most basic form, it can be written as: L H = −K E pilog2pi (1) i=0 where K is a constant chosen based on the units, L is the length of the document, and pi is the probability of the ith word. This equation works 33 Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 33–37, Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics equally well whether it is used for unigrams, bigrams, or trigrams. Consider for a moment the relationship between ent</context>
</contexts>
<marker>McFarlane, Elhadad, Kukafka, 2009</marker>
<rawString>Delano J. McFarlane, Noemie Elhadad, and Rita Kukafka. 2009. Perplexity analysis of obesity news coverage. AMIA Annual Symposium Proceedings, 2009:426–430. 00001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>The mathematical theory of communication.</title>
<date>1949</date>
<publisher>Press.</publisher>
<institution>Urbana, University of Illinois</institution>
<contexts>
<context position="3226" citStr="Shannon, 1949" startWordPosition="519" endWordPosition="520">the text, if the information is sufficiently different from what is already present, it will also likely introduce new vocabulary words. The result is that the introduction of a new factoid into a text is likely to also introduce new vocabulary, unless it is redundant. Thus, the more non-redundant factoids a text contains, the more varied the vocabulary of the text is likely to be. 1.3 From vocabulary to relative entropy Vocabulary is commonly used in connection with Shannon’s information entropy to measure such things as surprisal, redundancy, perplexity, and, of course, information density (Shannon, 1949; McFarlane et al., 2009). Entropy models text as being created via a Markov process. In its most basic form, it can be written as: L H = −K E pilog2pi (1) i=0 where K is a constant chosen based on the units, L is the length of the document, and pi is the probability of the ith word. This equation works 33 Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 33–37, Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics equally well whether it is used for unigrams, bigrams, or trigrams. Consider for a moment the</context>
</contexts>
<marker>Shannon, 1949</marker>
<rawString>Claude E. Shannon. 1949. The mathematical theory of communication. Urbana, University of Illinois Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
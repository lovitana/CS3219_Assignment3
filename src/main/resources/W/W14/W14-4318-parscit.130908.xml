<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003249">
<title confidence="0.9976235">
Extractive Summarization and Dialogue Act Modeling on Email
Threads: An Integrated Probabilistic Approach
</title>
<author confidence="0.992458">
Tatsuro Oya and Giuseppe Carenini
</author>
<affiliation confidence="0.951058">
Department of Computer Science
University of British Columbia
Vancouver, B.C. Canada
</affiliation>
<email confidence="0.994795">
{toya, carenini}@cs.ubc.ca
</email>
<sectionHeader confidence="0.99559" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998028222222222">
In this paper, we present a novel supervised
approach to the problem of summarizing
email conversations and modeling dialogue
acts. We assume that there is a relationship
between dialogue acts and important sen-
tences. Based on this assumption, we intro-
duce a sequential graphical model approach
which simultaneously summarizes email
conversation and models dialogue acts. We
compare our model with sequential and
non-sequential models, which independent-
ly conduct the tasks of extractive summari-
zation and dialogue act modeling. An
empirical evaluation shows that our ap-
proach significantly outperforms all base-
lines in classifying correct summary
sentences without losing performance on
dialogue act modeling task.
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968705882353">
Nowadays, an overwhelming amount of text in-
formation can be found on the web. Most of this
information is redundant and thus the task of
document summarization has attracted much at-
tention. Since emails in particular are used for a
wide variety of purposes, the process of automat-
ically summarizing emails might be of great
benefit in dealing with this excessive amount of
information. Much work has already been con-
ducted on email summarization. The first re-
search on this topic was conducted by Rambow
et al. (2004), who took a supervised learning ap-
proach to extracting important sentences. A
study on the supervised summarization of email
threads was also performed by Ulrich et al.
(2009). This study used the regression-based
method for classification. There have been stud-
ies on unsupervised summarization of email
threads as well. Zhou et al. (2007, 2008) pro-
posed a graph-based unsupervised approach to
email conversation summarization using clue
words, i.e., recurring words contained in replies.
In addition, the task of labeling sentences
with dialogue acts has become important and has
been employed in many conversation analysis
systems. For example, applications such as meet-
ing summarization and collaborative task learn-
ing agents use dialogue acts as their underlying
structure (Allen et al., 2007; Murray et al.,
2010). In a previous work, Cohen et al. (2004)
defined a set of “email acts” and employed text
classification methods to detect these acts in
emails. Later, Carvalho et al. (2006) employed a
combination of n-gram sequences as features and
then used a supervised machine learning method
to improve the accuracy of this email act classifi-
cation. In addition, Shafiq et al. (2011) presented
unsupervised dialogue act labeling methods. In
their work, they introduced a graph-based meth-
od and two probabilistic sequence-labeling
methods for modeling dialogue acts.
However, little work has been done on dis-
covering the relationship between dialogue acts
and extractive summaries. If there is a relation-
ship between them, combining these approaches
so as to model both simultaneously will yield
better results. In this paper, we investigate this
hypothesis by introducing a new sequential
graphical model approach that performs dialogue
act modeling and extractive summarization joint-
ly on email threads.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9995258">
While email summarization and dialogue act
modeling have been effectively studied, in most
previous work, these tasks were studied inde-
pendently. This section provides related work for
each task separately.
</bodyText>
<page confidence="0.988095">
133
</page>
<note confidence="0.808754">
Proceedings of the SIGDIAL 2014 Conference, pages 133–140,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.856153">
2.1 Extractive Summarization
</subsectionHeader>
<bodyText confidence="0.999931636363636">
Rambow et al. (2004) introduced sentence ex-
traction techniques that work for email threads.
In their work, they introduced email-specific fea-
tures and used a machine learning method to
classify whether or not a sentence should be in-
corporated into a summary. Their experiments
demonstrated that their features were highly ef-
fective for email summarization.
Ulrich et al. (2009) proposed a regression-
based machine learning approaches to email
thread summarization. They compared regres-
sion-based classifiers to binary classifiers and
showed that their approach significantly im-
proves the summarization accuracy. They em-
ployed the feature set introduced by Rambow et
al. (2004) as their baseline and introduced new
features that are also effective for email summa-
rization. Some of their features refer to dialogue
acts but the assumption is that they are computed
before the summarization task is performed. Our
work is aimed at a much closer integration of the
two tasks by modeling them simultaneously.
Carenini et al. (2007) developed a fragment
quotation graph that can capture a fine-grain
conversation structure in email threads, which
we will describe in detail in Section 3. They then
introduced a ClueWordSummarizer (CWS), a
graph-based unsupervised summarization ap-
proach based on the concept of clue words,
which are recurring words found in email replies.
Their experiment showed that the CWS performs
better than the email summarization approach in
Rambow et al. (2004).
Extractive summarization using a sequential
labeling technique has also been studied. While
this is not an email summarization, Shen et al.
(2007) proposed a linear-chain Conditional Ran-
dom Field (CRF) based approach for extractive
document summarization. In their work, they
treated the summarization task as a sequence la-
beling problem to take advantage of interaction
relationships between sentences; their approach
showed significant improvement when compared
with non-sequential classifiers.
</bodyText>
<subsectionHeader confidence="0.997591">
2.2 Dialogue Act Modeling
</subsectionHeader>
<bodyText confidence="0.999961516129032">
The first studies on the dialogue act modeling in
emails were performed by Cohen et al. (2004).
They defined “email speech acts” (e.g., Request,
Deliver, Propose, and Commit) and used ma-
chine learning methods to classify emails accord-
ing to the intent of the sender.
Carvalho et al. (2006) further developed this
initial proposal by using contextual information
such as combinations of n-gram sequences in
emails as their features for a supervised learning
approach. The experiment showed that their ap-
proach reduced classification error rates by
26.4%. Shafiq et al. (2011) proposed unsuper-
vised dialogue act modeling in email threads and
on forums. They introduced a graph-based and
two probabilistic unsupervised approaches for
modeling dialogue acts. By comparing those ap-
proaches, they demonstrated that the probabilis-
tic approaches were quite effective and
performed better than the graph-based one.
While the following work is not done on the
email domain, Kim et al. (2010) introduced a
dialogue act classification on one-on-one online
chat forums. To be able to capture sequential
dialogue act dependency on chats, they applied a
CRF model. They demonstrated that, compared
with other classifiers, their CRF model per-
formed the best. In their later work (Kim et al.,
2012), they extended the domain to multi-party
live chats and proposed new features for that
domain.
</bodyText>
<sectionHeader confidence="0.8100265" genericHeader="method">
3 Capturing Conversation Structure in
Email Threads
</sectionHeader>
<bodyText confidence="0.999948076923077">
In this section, we describe how to build a frag-
ment quotation graph which captures the conver-
sation structure of any email thread at finer
granularity. This graph was developed and
shown to be effective by Carenini et al. (2011).
A key assumption of this approach is that in or-
der to effectively perform summarization and
dialogue act modeling, a fine graph representa-
tion of the underlying conversation structure is
needed.
Here, we start with the sample email conver-
sation shown in Figure 1 (a). For convenience,
the content of the emails is represented as a se-
quence of fragments.
First, we identify all new and quoted frag-
ments. For example, email E1 is composed of
one new fragment, ‘b’, and one quoted fragment,
‘a’. As for email E3, since we do not yet know
whether or not ‘d’ and ‘e’ are different frag-
ments, we consider E3 as being composed of one
new fragment, ‘de’ and one quoted fragment, ‘b’.
Second, we identify distinct fragments. To do
this, we first identify overlaps by comparing
fragments with each other. If necessary, we split
the fragments and remove any duplicates from
them. For example, a fragment, ‘de’, in E3 is
</bodyText>
<page confidence="0.993403">
134
</page>
<bodyText confidence="0.9992835">
split into ‘d’ and ‘e’ after being compared with
fragments in E4 and the duplicates are removed.
By applying this process to all of the emails,
seven distinct fragments, a, b ..., and, g remain in
this example.
In the third step, edges which represent the
replying relationships among the fragments are
created. These edges are determined based on the
assumption that any fragment is a reply to neigh-
boring quotations (the quoted fragments immedi-
ately preceding or following the current one). For
example, the neighboring nodes of ‘f’ in E4 are
‘d’ and ‘e’. Thus, we create two edges from node
‘f’ in E4 to node ‘d’ and ‘e’ in E3. In the same
way, we see that the neighboring node of ‘g’ in
E4 is ‘e’. Hence, there is one edge from node ‘g’
to ‘e’. If no quotation is contained in a reply
email, we connect the fragments in the email to
fragments in emails to which it reply.
In email threads, there are cases in which the
original email with its quotations is missing from
the user’s folder, as in the case of ‘a’ in Figure 1
(a). These types of emails are called hidden
emails. Carenini et al. (2005) studied in detail
how these email types might be treated and their
influence on email summarization.
</bodyText>
<figureCaption confidence="0.753996333333333">
Figure 1 (b) shows the completed fragment
quotation graph of the email thread shown in
Figure 1 (a). In the fragment quotation graph
</figureCaption>
<bodyText confidence="0.956306076923077">
structure, all paths (e.g., a-b-c, a-b-d-f, a-b-e-f,
and a-b-e-g in Figure 1 (b)) capture the adjacent
relationships between email fragments. Hence,
we use every path that can be derived from the
graph as our dataset. However, in this case, when
we run the labeling task on these paths, we ob-
tain multiple labels for some of the sentences
because the sentences in fragments such as ‘a’,
‘b’, and ‘f’ in Figure 1 (b) are shared among
multiple paths. Therefore, to assign a label to one
of these sentences, we take the label more fre-
quently assigned to that sentence when all its
paths are considered (i.e., the majority vote).
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.999267666666667">
For both dialogue act modeling and extractive
summarization, many effective sentence features
have been discovered so far. Interestingly, some
common features are shown to be effective in
both tasks. This section explains the features
used in our model. We begin with the features
for extractive summarization and then describe
how we derive the features for dialogue act mod-
eling. All the features explained in this section,
whether they belong to extractive summarization
or dialogue act modeling, are included in our
model.
</bodyText>
<figure confidence="0.987892333333333">
(a) A possible configuration of an email conversation
(E2 and E3 reply to E1, and E4 replies to E3)
(b) An example of a fragment quotation graph
</figure>
<figureCaption confidence="0.992593">
Figure 1: A fragment quotation graph derived from a
possible configuration of an email conversation
</figureCaption>
<subsectionHeader confidence="0.992973">
4.1 Extractive Summarization Features
</subsectionHeader>
<bodyText confidence="0.937938111111111">
The features we use for extractive summarization
are mostly from Carenini et al. (2008) and Ram-
bow et al. (2004) and have proven to be effective
on conversational data. Details of these features
are described below. Note that all sentences in an
email thread are ordered based on paths derived
from a fragment quotation graph.
Length Feature: The number of words in
each sentence.
Relative Position Feature: The number of
sentences preceding the current divided by
the total number of sentences in one path.
Thread Name Overlaps Feature: The num-
ber of overlaps of the content words between
the email thread title and a sentence.
Subject Name Overlaps Feature: The num-
ber of overlaps of the content words between
the subject of the email and a sentence.
</bodyText>
<listItem confidence="0.96768275">
Question Feature: A binary feature that in-
dicates whether or not a sentence has a ques-
tion mark.
CC Feature: A binary feature that indicates
</listItem>
<bodyText confidence="0.781317">
whether or not an email contains CC.
</bodyText>
<page confidence="0.994737">
135
</page>
<bodyText confidence="0.9522959">
Participation Dominance Feature: The
number of utterances each person makes in
one path.
Finally, we also include a simplified version of
the ClueWordScore (CWS) developed by
Carenini et al. (2007), which is listed below.
Simplified CWS Feature: The number of
overlaps of the content words that occur in
both the current and adjacent sentences in the
path, ignoring stopwords.
</bodyText>
<subsectionHeader confidence="0.98717">
4.2 Dialogue Act Features
</subsectionHeader>
<bodyText confidence="0.999967085106383">
The relative positions and length features have
proven to be beneficial to both tasks (Jeong et al.,
2009; Carenini et al., 2008). Hence, these are
categorized as both dialogue acts and extractive
summarization features. In addition, we use word
and POS n-grams as our features for dialogue act
modeling. These features are extracted by the
following process explained in Carvalho et al.
(2006). However, we extend the original ap-
proach in order to further abstract n-gram fea-
tures to avoid making them too sparse to be
effective. In this section, we describe the deriva-
tion process in detail.
A multi-step approach is used to generate
word n-gram features. First, all words are tagged
with the named entity using the Stanford Named
Entity Recognizer (Finkel et al., 2005), and are
then replaced with these tags. Second, a se-
quence of word-replacement tasks is applied to
all email messages. Initially, some types of punc-
tuation marks (e.g., &lt;&gt;()[];:. and ,) and extra
spaces are removed. Then, shortened phrases
such as “I’m” and “We’ll” are substituted for
more formal versions such as “I am” and “We
will”. Next, other replacement tasks are per-
formed. Some of them are described in Table1.
In the third step, unigrams and bigrams are ex-
tracted. In this paper, unigrams and bigrams refer
to all possible sequences of length one and two
terms. After extracting all unigrams and bigrams
for each dialogue act, we then compute Infor-
mation Gain Score (Forman, 2003) and select the
n-grams whose scores are in the top five greatest
on the training set. In this way, we can automati-
cally detect features that represent the character-
istics of each dialogue act. In addition to word n-
grams, we also include POS n-grams in our fea-
tures. In a similar way, we first tag each word in
sentences with POS using the Stanford POS tag-
ger (Toutanova et al., 2003). Then, for each dia-
logue act, we extract bigrams and trigrams, all of
which are scored by the Information Gain. Based
on their scores, we select the POS bigram and
trigram features whose scores are within the top
five greatest. One example of word n-gram fea-
tures for a Question dialogue act selected by this
derivation method is shown in Table 2.
</bodyText>
<table confidence="0.999695769230769">
Pattern Replacement
‘why’, ‘where’, ‘who’, ‘what’ ‘when’ [WWHH]
nominative pronouns [I]
objective pronouns [ME]
&apos;it&apos;, &apos;those&apos;, &apos;these&apos;, &apos;this&apos;, &apos;that&apos; [IT]
&apos;will&apos;, ‘would&apos;, &apos;shall&apos;, &apos;should&apos;, &apos;must&apos; [MODAL_STRONG]
‘can&apos;, &apos;could&apos;, &apos;may&apos;, &apos;might&apos; [MODAL_WEAK]
&apos;do&apos;, &apos;does&apos;, &apos;did&apos;, ‘done&apos; [DO]
&apos;is&apos;, &apos;was&apos;, &apos;were&apos;, &apos;are&apos;, &apos;been&apos; &apos;be&apos;, &apos;am&apos; [BE]
&apos;after&apos; , &apos;before&apos;, &apos;during&apos; [AAAFTER]
‘Jack”, “Wendy” [Personal_PRONOUN]
“New York” [LOCATION]
“Acme Corp.” [ORGANIZATION]
</table>
<tableCaption confidence="0.989373">
Table 1: Some Preprocessing Replacement Pattern
</tableCaption>
<table confidence="0.999950666666667">
Word Unigram Word Bigram
? [MODAL_STRONG] [I]
anyone [IT] ?
WWHH [DO] anyone
deny [WWHH] [BE]
[Personal _PRONOUN] [BE] [IT]
</table>
<tableCaption confidence="0.9987235">
Table 2: Sample word n-grams selected as the fea-
tures for Question dialogue act
</tableCaption>
<sectionHeader confidence="0.983275" genericHeader="method">
5 The Sequential Labeling Task
</sectionHeader>
<bodyText confidence="0.9999531">
We use a Dynamic Conditional Random Field
(DCRF) (Sutton et al., 2004) for labeling tasks.
A DCRF is a generalization of a linear-chain
CRF which allows us to represent complex inter-
action between labels. To be more precise, it is a
conditionally-trained undirected graphical model
whose structure and parameters are repeated over
a sequence. Hence, it is the most appropriate
method for performing multiple labeling tasks on
the same sequence.
</bodyText>
<page confidence="0.995123">
136
</page>
<bodyText confidence="0.960300235294118">
Our DCRF uses the graph structure shown in
Figure 2 with one chain (the top X nodes) model-
ing extractive summary and the other (the middle
Y nodes) modeling dialogue acts. Each node in
the observation sequence (the bottom Z nodes)
corresponds to each sentence in a path of the
fragment quotation graph of the email thread. As
shown in Figure 2, the graph structure captures
the relationship between extractive summaries
and dialogue acts by connecting their nodes.
We use Mallet1 (McCallum, 2002) to implement
our DCRF model. It uses l2-based regularization
to avoid overfitting, and a limited BFGS fitting
algorithm to learn the DCRF model parameters.
Also, it uses tree-based reparameterization
(Wainwright et al., 2002) to compute the poste-
rior marginal, or inference.
</bodyText>
<figureCaption confidence="0.9978175">
Figure 2: The DCRF model used to create extractive
summaries and model dialogue acts
</figureCaption>
<sectionHeader confidence="0.987614" genericHeader="method">
6 Empirical Evaluations
</sectionHeader>
<subsectionHeader confidence="0.748794">
6.1 Dataset Setup
</subsectionHeader>
<bodyText confidence="0.999615789473684">
In our experiment, the publically available BC3
corpus2 (Ulrich et al., 2008) is used for training
and evaluation purposes. The corpus contains
email threads from the World Wide Web Con-
sortium (W3C) mailing list. It consists of 40
threads with an average of five emails per thread.
The corpus provides extractive summaries of
each email thread, all of which were annotated
by three annotators. Hence, we use sentences that
are selected by more than one annotator as the
gold standard summary for each conversation.
In addition, all sentences in the 39 out of 40
threads are annotated for dialogue act tags. The
tagset consists of five general and 12 specific
tags. All of these tags are based on Jeong et al.
(2009). For our experiment, considering that our
data is relatively small, we decide to use the
coarser five tag set. The details are shown in Ta-
ble 3.
</bodyText>
<footnote confidence="0.998629">
1 http://mallet.cs.umass.edu
2 http://www.cs.ubc.ca/nest/lci/bc3.html
</footnote>
<table confidence="0.999444">
Tag Description Relative Frequency (%)
S Statement 73.8
Q Question 7.92
R Reply 5.23
Su Suggestion 5.62
M Miscellaneous 7.46
</table>
<tableCaption confidence="0.992277">
Table 3: Dialogue act tag categories and their relative
frequency in the BC3 corpus
</tableCaption>
<bodyText confidence="0.999402">
After removing quoted sentences and redundant
information such as senders and addresses, 1300
distinct sentences remain in the 39 email threads.
The detailed content of the corpus is summarized
in Table 4.
</bodyText>
<table confidence="0.9987789">
Total
Dataset
No. of Threads 39
No. of Sentences 1300
No. of Extractive Summary Sentences 521
No. of S Sentences 959
No. of Q Sentences 103
No. of R Sentences 68
No. of Su Sentences 73
No. of M Sentences 97
</table>
<tableCaption confidence="0.999614">
Table 4: Detailed content of the BC3 corpus
</tableCaption>
<subsectionHeader confidence="0.997993">
6.2 Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999918">
Here, we introduce evaluation metrics for our
joint model of extractive summarization and dia-
logue act recognition.
The CRF model has been shown to be the ef-
fective one in both dialogue act modeling and
extractive summarization (Shen et al., 2007; Kim
et al., 2010; Kim et al., 2012). Hence, for com-
parison, we implement two different CRFs, one
for extractive summarization and the other for
dialogue act modeling. When classifying extrac-
tive summaries using the CRF, we only use its
extractive summarization features. Similarly,
when modeling dialogue acts, we only use its
dialogue act features. In addition, we also com-
</bodyText>
<page confidence="0.995834">
137
</page>
<bodyText confidence="0.994328583333333">
pare our system with a non-sequential classifier,
a support vector machine (SVM), with the same
settings as those described above. For these im-
plementations, we use Mallet and SVM-light
package3 (Joachims, 1999).
In our experiment, we first measure separate-
ly the performance of extractive summarization
and dialogue act modeling. The performance of
extractive summarization is measured by its av-
eraged precision, recall, and F-measure. For dia-
logue acts, we report the averaged-micro and
macro accuracies as well as the averaged accura-
cies of each dialogue act.
Second, we evaluate the combined perfor-
mance of extractive summarization and dialogue
act modeling tasks. In general, we are interested
in the dialogue acts in summary sentences be-
cause they can be later used as input for other
natural language processing applications such as
automatic abstractive summarization (Murray et
al., 2010). Therefore, we measure the perfor-
mance of our model with the following modified
precision (Pre’), recall (Rec’), and F-measure
(F’):
</bodyText>
<figure confidence="0.96251">
{No. of correctly classified sentences } 1
{No.of sentences classified as summary setences} ( )
{No. of correctly classified sentences} (2)
{No.o f true summary sentences}
(3)
</figure>
<bodyText confidence="0.9997515">
where a correctly classified sentence refers to a
true summary sentence that is classified as such
and whose dialogue acts are also correctly classi-
fied.
</bodyText>
<subsectionHeader confidence="0.999137">
6.3 Experiment Procedure
</subsectionHeader>
<bodyText confidence="0.999935333333333">
For all cases, we run five sets of 10-fold cross
validation to train and test the classifiers on a
shuffled dataset and calculate the average of the
results. For each cross validation run, we extract
all features following the process described in
Section 4 on the training set. When comparing
these two baselines with our model, we report p-
values obtained from a student paired t-test on
the results to determine their significance.
</bodyText>
<footnote confidence="0.825516">
3 http://www.cs.cornell.edu/people/tj/svm_light
</footnote>
<subsectionHeader confidence="0.80904">
6.4 Results
</subsectionHeader>
<bodyText confidence="0.99819725">
The performances of extractive summarization
and dialogue act modeling using the three meth-
ods are summarized in Table 5 and 6, respective-
ly.
</bodyText>
<table confidence="0.997031">
DCRF CRF SVM
F-measure 0.485 0.428 0.397
t-test’s p-value 0.00046 2.5E-07
Precision 0.562 0.591 0.675
Recall 0.457 0.370 0.308
</table>
<tableCaption confidence="0.995866">
Table 5: A comparison of the extractive summariza-
tion performance of our DCRF model and the two
baselines based on precision, recall, and F-measure
</tableCaption>
<table confidence="0.9999102">
DCRF CRF SVM
Micro Accuracy 0.785 0.779 0.775
t-test’s p-value 0.116 0.036
Macro Accuracy 0.516 0.516 0.304
t-test’s p-value 0.950 5.2E-32
S Accuracy 0.901 0.892 0.999
Q Accuracy 0.832 0.809 0.465
R Accuracy 0.580 0.575 0.05
Su Accuracy 0.139 0.108 0.00
M Accuracy 0.126 0.198 0.00
</table>
<tableCaption confidence="0.983371333333333">
Table 6: A comparison of the dialogue act modeling
performance of our DCRF model and the two base-
lines based on averaged accuracies
</tableCaption>
<bodyText confidence="0.999225466666667">
From Table 5, we observe that, in terms of
extractive summarization results, our DCRF
model significantly outperforms the two base-
lines. Noticeable improvements can be seen for
the recall and F-measure. In terms of F-measure,
compared with the CRF and SVM, our model
improves by 5.7% and 8.8% respectively. The p-
values obtained from the t-test indicate that our
results are statistically significantly different (p &lt;
0.05) from those of the two baselines.
Regarding dialogue act modeling, the results
are summarized in Table 6. While no improve-
ment is shown for the micro-averaged accuracy,
our model and the CRF significantly outperform
the SVM in terms of the macro-averaged accura-
</bodyText>
<page confidence="0.996694">
138
</page>
<bodyText confidence="0.999030205128206">
cy. Both our model and the CRF consider the
sequential structure of the conversation, which is
not captured in the SVM model. Clearly, this
indicates that the sequential models are effective
in modeling dialogue acts due to their ability to
capture the inter-utterance relations of conversa-
tions.
Compared with the CRF, our DCRF model
outperforms it in most cases except in classifying
the ‘M’ dialogue act. However these improve-
ments are not significant as t-test of both macro
and micro-averaged accuracies indicate that the
differences are not statistically significant (p &gt;
0.05).
Another item to be mentioned here is that the
accuracies of classifying ‘R’, ‘Su’ and ‘M’ dia-
logue acts are relatively low. This issue applies
to all classifiers and is plausibly due to the small
dataset. There are only 68, 73 and 97 sentences,
respectively, out of 1300 that are labeled as ‘R’,
‘Su’ and ‘M’ in the BC3 corpus. Since our dia-
logue act classifiers rely heavily on n-gram fea-
tures, were the data small, these features would
be too sparse to effectively represent the charac-
teristics of the dialogue acts. However, compared
with the SVM results, our joint model and the
CRF perform significantly better in classifying
these dialogue acts. This also explains why the
sequential model is preferable in dialogue act
modeling.
Note that despite the small dataset, all the
classifiers are relatively accurate in classifying
‘Q’. This is because n-gram features selected for
‘Q’ such as ‘?’ and ‘WWHH’ are very specific to
this dialogue act, which makes the task of ‘Q’
classification easier compared to those of others.
Next, we discuss the result of the com-
bined performance. The performances of our
model and the two baselines are summarized in
</bodyText>
<tableCaption confidence="0.731535">
Table 7.
</tableCaption>
<table confidence="0.9905764">
DCRF CRF SVM
F-measure’ 0.352 0.324 0.292
t-test’s p-value 0.015 3.3E-05
Precision’ 0.407 0.450 0.501
Recall’ 0.335 0.280 0.227
</table>
<tableCaption confidence="0.854194">
Table 7: A comparison of the overall performance of
our DCRF model and the two baselines based on
modified precision, recall and F-measure
</tableCaption>
<bodyText confidence="0.999980071428571">
We see that our DCRF model significantly
outperforms the two baselines. While our model
yields the lowest Pre’ of all, its Rec’ is much
greater than the other two baselines and this
leads to its achieving the highest F’. Compared
with the CRF and SVM, the F’ obtained from
our system improves by 2.8% and 6% respec-
tively. In addition, the p-values show that the
results of our model are statistically significant
(p &lt; 0.05) compared with those of the two base-
lines.
Overall, these experiments clearly indicate
that our model is effective in classifying both
dialogue acts and summary sentences.
</bodyText>
<sectionHeader confidence="0.998052" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999980076923077">
In this work, we have explored a new automated
approach for extractive summarization and dia-
logue act modeling on email threads. In particu-
lar, we have presented a statistical approach for
jointly modeling dialogue acts and extractive
summarization in a single DCRF. The empirical
results demonstrate that our approach outper-
forms the two baselines on the summarization
task without loss of performance on the dialogue
act modeling one. In the future, we would like to
extend our approach by exploiting more effective
features. We also plan to apply our approach to
different domains possessing large dataset.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99982825">
We are grateful to Yashar Mehdad, Raimond Ng,
Maryam Tavafi and Shafiq Joty for their com-
ments and UBC LCI group and ICICS for finan-
cial support.
</bodyText>
<sectionHeader confidence="0.998541" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998623666666666">
J. Allen, N. Chambers, G. Ferguson, L. Galescu, H.
Jung, and W. Taysom. Plow: A collaborative task
learning agent. In AAAI-07, pages 22–26, 2007.
Giuseppe Carenini, Gabriel Murray, and Raymond
Ng. 2011. Methods for Mining and Summarizing
Text Conversations. Morgan Claypool.
Giuseppe Carenini, Raymond Ng, and Xiaodong
Zhou. 2005. Scalable discovery of hidden emails
from large folders. In ACM SIGKDD’05, pages
544–549.
Giuseppe Carenini, Raymond Ng, and Xiaodong
Zhou. 2008. Summarizing Emails with Conversa-
tional Cohesion and Subjectivity In proceeding
46th Annual Meetint Assoc.for Computational Lin-
guistics, page 353-361.
</reference>
<page confidence="0.994569">
139
</page>
<reference confidence="0.998728435185185">
Giuseppe Carenini, Raymond Ng, and Xiaodong
Zhou. 2007. Summarizing email conversations
with clue words. 16th International World Wide
Web Conference (ACM WWW’07).
Vitor R. Carvalho and William W. Cohen. 2006. Im-
proving ”email speech acts” analysis via n-gram
selection. In Proceedings of the HLT-IAACL 2006
Workshop on Analyzing Conversations in Text and
Speech, ACTS ’09, pages 35–41, Stroudsburg, PA,
USA. Association for Computational Linguistics.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
“speech acts”. In Proceedings of Empirical Meth-
ods in Iatural Language Processing, pages 309–
316, Barcelona, Spain, July.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Infor-
mation into Information Extraction Systems by
Gibbs Sampling. In Proceedings of the 43nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2005), pp. 363-370.
George Forman. 2003. An extensive empirical study
of feature selection metrics for text classification.
The Journal of Machine Learning Research,
3:1289–1305.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae
Lee. 2009. Semi-supervised speech act recognition
in emails and forums. In Proceedings of the 2009
Conference on Empirical Methods in Iatural Lan-
guage Processing.
Thorsten Joachims. 1999 Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning, B. Schölkopf and C.
Burges and A. Smola (ed.), MIT-Press, 1999.
Shafiq Joty, Giuseppe Carenini, and Lin, Chin-Yew
Lin. 2011. Unsupervised Modeling of Dialog Acts
in Asynchronous Conversations. In Proceedings of
the twenty second International Joint Conference
on Artificial Intelligence (IJCAI) 2011. Barcelona,
Spain.
Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and
Raymond Ng. 2009 Finding Topics in Emails: Is
LDA enough? IIPS-2009 workshop on applica-
tions for topic models: text and beyond. Whistler,
Canada.
McCallum, A. Kachites, 2002. MALLET: A Machine
Learning for Language Toolkit.
http://mallet.cs.umass.edu.
Su Nam Kim, Lawrence Cavedon, and Timothy
Baldwin. 2010a. Classifying dialogue acts in 1-to-1
live chats. In Proceedings of the 2010 Conference
on Empirical Methods in Iatural Language Pro-
cessing (EMNLP 2010), pages 862–871, Boston,
USA.
Su Nam Kim, Lawrence Cavedon and Timothy Bald-
win (2012) Classifying Dialogue Acts in Multi-
party Live Chats, In Proceedings of the 26th Pacif-
ic Asia Conference on Language, Information and
Computation (PACLIC 26), Bali, Indonesia, pp.
463—472.
Gabriel Murray and Giuseppe Carenini. 2008. Sum-
marizing Spoken and Written Conversations. Em-
pirical Methods in ILP (EMILP 2008), Waikiki,
Hawaii, 2008.
Gabriel Murray and Giuseppe Carenini. 2010. Sum-
marizing Spoken and Written Conversations. Gen-
erating and Validating Abstracts of Meeting
Conversations: a User study (IILG 2010), Dublin,
Ireland, 2010.
Gabriel Murray, Renals Steve, and Carletta Jean.
2005a. Extrative summarization of meeting record-
ings. In Proceeding of Interspeech 2005, Lisbon,
Portugal, pages 593-596.
Owen Rambow, Lokesh Shrestha, John Chen, and
Chirsty Lauridsen. 2004. Summarizing email
threads. In Proceedings of HLTIAACL 2004.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and
Zheng Chen. 2007. Document summarization us-
ing conditional random fields. In Proc. of IJCAI,
volume 7, 2862–2867.
Charles Sutton, Khashayar Rohanimanesh, and An-
drew McCallum. 2004. Dynamic conditional
random fields: Factorized probabilistic models for
labeling and segmenting sequence data. In Proc.
ICML.
Maryam Tavafi, Yashar Mehdad, Shafiq Joty,
Giuseppe Carenini and Raymond Ng. 2013. Dia-
logue Act Recognition in Synchronous and Asyn-
chronous Conversations. In Proceedings of the
SIGDIAL 2013 Conference, pages 117–121, Metz,
France. Association for Computational Linguistics.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of HLT-IAACL 2003, pp.
252-259.
Jan Ulrich, Giuseppe Carenini, Gabriel Murray,
and Raymond T. Ng: Regression-Based Summari-
zation of Email Conversations. ICWSM 2009
Jan Ulrich, Gabriel Murray, and Giuseppe Carenini.
2008. A publicly available annotated corpus for
supervised email summarization. AAAI-2008
EMAIL Workshop.
Martin J. Wainwright, Tommi Jaakkola, and Alan S.
Willsky. 2002. Treebased Reparameterization for
Approximate Inference on Loopy Graphs. In Ad-
vances in Ieural Information Processing Systems
14, pages 1001 1008. MIT Press.
</reference>
<page confidence="0.997533">
140
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.924904">
<title confidence="0.999619">Extractive Summarization and Dialogue Act Modeling on Threads: An Integrated Probabilistic Approach</title>
<author confidence="0.990682">Oya</author>
<affiliation confidence="0.99997">Department of Computer University of British</affiliation>
<address confidence="0.977718">Vancouver, B.C.</address>
<email confidence="0.957257">toya@cs.ubc.ca</email>
<email confidence="0.957257">carenini@cs.ubc.ca</email>
<abstract confidence="0.999759157894737">In this paper, we present a novel supervised approach to the problem of summarizing email conversations and modeling dialogue acts. We assume that there is a relationship between dialogue acts and important sentences. Based on this assumption, we introduce a sequential graphical model approach which simultaneously summarizes email conversation and models dialogue acts. We compare our model with sequential and non-sequential models, which independently conduct the tasks of extractive summarization and dialogue act modeling. An empirical evaluation shows that our approach significantly outperforms all baselines in classifying correct summary sentences without losing performance on dialogue act modeling task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Allen</author>
<author>N Chambers</author>
<author>G Ferguson</author>
<author>L Galescu</author>
<author>H Jung</author>
<author>W Taysom</author>
</authors>
<title>Plow: A collaborative task learning agent.</title>
<date>2007</date>
<booktitle>In AAAI-07,</booktitle>
<pages>22--26</pages>
<contexts>
<context position="2306" citStr="Allen et al., 2007" startWordPosition="342" endWordPosition="345">his study used the regression-based method for classification. There have been studies on unsupervised summarization of email threads as well. Zhou et al. (2007, 2008) proposed a graph-based unsupervised approach to email conversation summarization using clue words, i.e., recurring words contained in replies. In addition, the task of labeling sentences with dialogue acts has become important and has been employed in many conversation analysis systems. For example, applications such as meeting summarization and collaborative task learning agents use dialogue acts as their underlying structure (Allen et al., 2007; Murray et al., 2010). In a previous work, Cohen et al. (2004) defined a set of “email acts” and employed text classification methods to detect these acts in emails. Later, Carvalho et al. (2006) employed a combination of n-gram sequences as features and then used a supervised machine learning method to improve the accuracy of this email act classification. In addition, Shafiq et al. (2011) presented unsupervised dialogue act labeling methods. In their work, they introduced a graph-based method and two probabilistic sequence-labeling methods for modeling dialogue acts. However, little work ha</context>
</contexts>
<marker>Allen, Chambers, Ferguson, Galescu, Jung, Taysom, 2007</marker>
<rawString>J. Allen, N. Chambers, G. Ferguson, L. Galescu, H. Jung, and W. Taysom. Plow: A collaborative task learning agent. In AAAI-07, pages 22–26, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Gabriel Murray</author>
<author>Raymond Ng</author>
</authors>
<title>Methods for Mining and Summarizing Text Conversations.</title>
<date>2011</date>
<publisher>Morgan Claypool.</publisher>
<contexts>
<context position="7387" citStr="Carenini et al. (2011)" startWordPosition="1116" endWordPosition="1119"> online chat forums. To be able to capture sequential dialogue act dependency on chats, they applied a CRF model. They demonstrated that, compared with other classifiers, their CRF model performed the best. In their later work (Kim et al., 2012), they extended the domain to multi-party live chats and proposed new features for that domain. 3 Capturing Conversation Structure in Email Threads In this section, we describe how to build a fragment quotation graph which captures the conversation structure of any email thread at finer granularity. This graph was developed and shown to be effective by Carenini et al. (2011). A key assumption of this approach is that in order to effectively perform summarization and dialogue act modeling, a fine graph representation of the underlying conversation structure is needed. Here, we start with the sample email conversation shown in Figure 1 (a). For convenience, the content of the emails is represented as a sequence of fragments. First, we identify all new and quoted fragments. For example, email E1 is composed of one new fragment, ‘b’, and one quoted fragment, ‘a’. As for email E3, since we do not yet know whether or not ‘d’ and ‘e’ are different fragments, we consider</context>
</contexts>
<marker>Carenini, Murray, Ng, 2011</marker>
<rawString>Giuseppe Carenini, Gabriel Murray, and Raymond Ng. 2011. Methods for Mining and Summarizing Text Conversations. Morgan Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Xiaodong Zhou</author>
</authors>
<title>Scalable discovery of hidden emails from large folders.</title>
<date>2005</date>
<booktitle>In ACM SIGKDD’05,</booktitle>
<pages>544--549</pages>
<contexts>
<context position="9404" citStr="Carenini et al. (2005)" startWordPosition="1480" endWordPosition="1483">urrent one). For example, the neighboring nodes of ‘f’ in E4 are ‘d’ and ‘e’. Thus, we create two edges from node ‘f’ in E4 to node ‘d’ and ‘e’ in E3. In the same way, we see that the neighboring node of ‘g’ in E4 is ‘e’. Hence, there is one edge from node ‘g’ to ‘e’. If no quotation is contained in a reply email, we connect the fragments in the email to fragments in emails to which it reply. In email threads, there are cases in which the original email with its quotations is missing from the user’s folder, as in the case of ‘a’ in Figure 1 (a). These types of emails are called hidden emails. Carenini et al. (2005) studied in detail how these email types might be treated and their influence on email summarization. Figure 1 (b) shows the completed fragment quotation graph of the email thread shown in Figure 1 (a). In the fragment quotation graph structure, all paths (e.g., a-b-c, a-b-d-f, a-b-e-f, and a-b-e-g in Figure 1 (b)) capture the adjacent relationships between email fragments. Hence, we use every path that can be derived from the graph as our dataset. However, in this case, when we run the labeling task on these paths, we obtain multiple labels for some of the sentences because the sentences in f</context>
</contexts>
<marker>Carenini, Ng, Zhou, 2005</marker>
<rawString>Giuseppe Carenini, Raymond Ng, and Xiaodong Zhou. 2005. Scalable discovery of hidden emails from large folders. In ACM SIGKDD’05, pages 544–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Xiaodong Zhou</author>
</authors>
<date>2008</date>
<booktitle>Summarizing Emails with Conversational Cohesion and Subjectivity In proceeding 46th Annual Meetint Assoc.for Computational Linguistics,</booktitle>
<pages>353--361</pages>
<contexts>
<context position="11173" citStr="Carenini et al. (2008)" startWordPosition="1771" endWordPosition="1774">tures for extractive summarization and then describe how we derive the features for dialogue act modeling. All the features explained in this section, whether they belong to extractive summarization or dialogue act modeling, are included in our model. (a) A possible configuration of an email conversation (E2 and E3 reply to E1, and E4 replies to E3) (b) An example of a fragment quotation graph Figure 1: A fragment quotation graph derived from a possible configuration of an email conversation 4.1 Extractive Summarization Features The features we use for extractive summarization are mostly from Carenini et al. (2008) and Rambow et al. (2004) and have proven to be effective on conversational data. Details of these features are described below. Note that all sentences in an email thread are ordered based on paths derived from a fragment quotation graph. Length Feature: The number of words in each sentence. Relative Position Feature: The number of sentences preceding the current divided by the total number of sentences in one path. Thread Name Overlaps Feature: The number of overlaps of the content words between the email thread title and a sentence. Subject Name Overlaps Feature: The number of overlaps of t</context>
<context position="12553" citStr="Carenini et al., 2008" startWordPosition="2002" endWordPosition="2005">ark. CC Feature: A binary feature that indicates whether or not an email contains CC. 135 Participation Dominance Feature: The number of utterances each person makes in one path. Finally, we also include a simplified version of the ClueWordScore (CWS) developed by Carenini et al. (2007), which is listed below. Simplified CWS Feature: The number of overlaps of the content words that occur in both the current and adjacent sentences in the path, ignoring stopwords. 4.2 Dialogue Act Features The relative positions and length features have proven to be beneficial to both tasks (Jeong et al., 2009; Carenini et al., 2008). Hence, these are categorized as both dialogue acts and extractive summarization features. In addition, we use word and POS n-grams as our features for dialogue act modeling. These features are extracted by the following process explained in Carvalho et al. (2006). However, we extend the original approach in order to further abstract n-gram features to avoid making them too sparse to be effective. In this section, we describe the derivation process in detail. A multi-step approach is used to generate word n-gram features. First, all words are tagged with the named entity using the Stanford Na</context>
</contexts>
<marker>Carenini, Ng, Zhou, 2008</marker>
<rawString>Giuseppe Carenini, Raymond Ng, and Xiaodong Zhou. 2008. Summarizing Emails with Conversational Cohesion and Subjectivity In proceeding 46th Annual Meetint Assoc.for Computational Linguistics, page 353-361.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Xiaodong Zhou</author>
</authors>
<title>Summarizing email conversations with clue words.</title>
<date>2007</date>
<booktitle>16th International World Wide Web Conference (ACM WWW’07).</booktitle>
<contexts>
<context position="4749" citStr="Carenini et al. (2007)" startWordPosition="712" endWordPosition="715">d machine learning approaches to email thread summarization. They compared regression-based classifiers to binary classifiers and showed that their approach significantly improves the summarization accuracy. They employed the feature set introduced by Rambow et al. (2004) as their baseline and introduced new features that are also effective for email summarization. Some of their features refer to dialogue acts but the assumption is that they are computed before the summarization task is performed. Our work is aimed at a much closer integration of the two tasks by modeling them simultaneously. Carenini et al. (2007) developed a fragment quotation graph that can capture a fine-grain conversation structure in email threads, which we will describe in detail in Section 3. They then introduced a ClueWordSummarizer (CWS), a graph-based unsupervised summarization approach based on the concept of clue words, which are recurring words found in email replies. Their experiment showed that the CWS performs better than the email summarization approach in Rambow et al. (2004). Extractive summarization using a sequential labeling technique has also been studied. While this is not an email summarization, Shen et al. (20</context>
<context position="12218" citStr="Carenini et al. (2007)" startWordPosition="1947" endWordPosition="1950">read Name Overlaps Feature: The number of overlaps of the content words between the email thread title and a sentence. Subject Name Overlaps Feature: The number of overlaps of the content words between the subject of the email and a sentence. Question Feature: A binary feature that indicates whether or not a sentence has a question mark. CC Feature: A binary feature that indicates whether or not an email contains CC. 135 Participation Dominance Feature: The number of utterances each person makes in one path. Finally, we also include a simplified version of the ClueWordScore (CWS) developed by Carenini et al. (2007), which is listed below. Simplified CWS Feature: The number of overlaps of the content words that occur in both the current and adjacent sentences in the path, ignoring stopwords. 4.2 Dialogue Act Features The relative positions and length features have proven to be beneficial to both tasks (Jeong et al., 2009; Carenini et al., 2008). Hence, these are categorized as both dialogue acts and extractive summarization features. In addition, we use word and POS n-grams as our features for dialogue act modeling. These features are extracted by the following process explained in Carvalho et al. (2006)</context>
</contexts>
<marker>Carenini, Ng, Zhou, 2007</marker>
<rawString>Giuseppe Carenini, Raymond Ng, and Xiaodong Zhou. 2007. Summarizing email conversations with clue words. 16th International World Wide Web Conference (ACM WWW’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vitor R Carvalho</author>
<author>William W Cohen</author>
</authors>
<title>Improving ”email speech acts” analysis via n-gram selection.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-IAACL 2006 Workshop on Analyzing Conversations in Text and Speech, ACTS ’09,</booktitle>
<pages>35--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Carvalho, Cohen, 2006</marker>
<rawString>Vitor R. Carvalho and William W. Cohen. 2006. Improving ”email speech acts” analysis via n-gram selection. In Proceedings of the HLT-IAACL 2006 Workshop on Analyzing Conversations in Text and Speech, ACTS ’09, pages 35–41, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor R Carvalho</author>
<author>Tom M Mitchell</author>
</authors>
<title>Learning to classify email into “speech acts”.</title>
<date>2004</date>
<booktitle>In Proceedings of Empirical Methods in Iatural Language Processing,</booktitle>
<pages>309--316</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2369" citStr="Cohen et al. (2004)" startWordPosition="354" endWordPosition="357"> There have been studies on unsupervised summarization of email threads as well. Zhou et al. (2007, 2008) proposed a graph-based unsupervised approach to email conversation summarization using clue words, i.e., recurring words contained in replies. In addition, the task of labeling sentences with dialogue acts has become important and has been employed in many conversation analysis systems. For example, applications such as meeting summarization and collaborative task learning agents use dialogue acts as their underlying structure (Allen et al., 2007; Murray et al., 2010). In a previous work, Cohen et al. (2004) defined a set of “email acts” and employed text classification methods to detect these acts in emails. Later, Carvalho et al. (2006) employed a combination of n-gram sequences as features and then used a supervised machine learning method to improve the accuracy of this email act classification. In addition, Shafiq et al. (2011) presented unsupervised dialogue act labeling methods. In their work, they introduced a graph-based method and two probabilistic sequence-labeling methods for modeling dialogue acts. However, little work has been done on discovering the relationship between dialogue ac</context>
<context position="5823" citStr="Cohen et al. (2004)" startWordPosition="870" endWordPosition="873">. Extractive summarization using a sequential labeling technique has also been studied. While this is not an email summarization, Shen et al. (2007) proposed a linear-chain Conditional Random Field (CRF) based approach for extractive document summarization. In their work, they treated the summarization task as a sequence labeling problem to take advantage of interaction relationships between sentences; their approach showed significant improvement when compared with non-sequential classifiers. 2.2 Dialogue Act Modeling The first studies on the dialogue act modeling in emails were performed by Cohen et al. (2004). They defined “email speech acts” (e.g., Request, Deliver, Propose, and Commit) and used machine learning methods to classify emails according to the intent of the sender. Carvalho et al. (2006) further developed this initial proposal by using contextual information such as combinations of n-gram sequences in emails as their features for a supervised learning approach. The experiment showed that their approach reduced classification error rates by 26.4%. Shafiq et al. (2011) proposed unsupervised dialogue act modeling in email threads and on forums. They introduced a graph-based and two proba</context>
</contexts>
<marker>Cohen, Carvalho, Mitchell, 2004</marker>
<rawString>William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell. 2004. Learning to classify email into “speech acts”. In Proceedings of Empirical Methods in Iatural Language Processing, pages 309– 316, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>363--370</pages>
<contexts>
<context position="13196" citStr="Finkel et al., 2005" startWordPosition="2107" endWordPosition="2110">gorized as both dialogue acts and extractive summarization features. In addition, we use word and POS n-grams as our features for dialogue act modeling. These features are extracted by the following process explained in Carvalho et al. (2006). However, we extend the original approach in order to further abstract n-gram features to avoid making them too sparse to be effective. In this section, we describe the derivation process in detail. A multi-step approach is used to generate word n-gram features. First, all words are tagged with the named entity using the Stanford Named Entity Recognizer (Finkel et al., 2005), and are then replaced with these tags. Second, a sequence of word-replacement tasks is applied to all email messages. Initially, some types of punctuation marks (e.g., &lt;&gt;()[];:. and ,) and extra spaces are removed. Then, shortened phrases such as “I’m” and “We’ll” are substituted for more formal versions such as “I am” and “We will”. Next, other replacement tasks are performed. Some of them are described in Table1. In the third step, unigrams and bigrams are extracted. In this paper, unigrams and bigrams refer to all possible sequences of length one and two terms. After extracting all unigra</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Forman</author>
</authors>
<title>An extensive empirical study of feature selection metrics for text classification.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1289</pages>
<contexts>
<context position="13887" citStr="Forman, 2003" startWordPosition="2226" endWordPosition="2227">t tasks is applied to all email messages. Initially, some types of punctuation marks (e.g., &lt;&gt;()[];:. and ,) and extra spaces are removed. Then, shortened phrases such as “I’m” and “We’ll” are substituted for more formal versions such as “I am” and “We will”. Next, other replacement tasks are performed. Some of them are described in Table1. In the third step, unigrams and bigrams are extracted. In this paper, unigrams and bigrams refer to all possible sequences of length one and two terms. After extracting all unigrams and bigrams for each dialogue act, we then compute Information Gain Score (Forman, 2003) and select the n-grams whose scores are in the top five greatest on the training set. In this way, we can automatically detect features that represent the characteristics of each dialogue act. In addition to word ngrams, we also include POS n-grams in our features. In a similar way, we first tag each word in sentences with POS using the Stanford POS tagger (Toutanova et al., 2003). Then, for each dialogue act, we extract bigrams and trigrams, all of which are scored by the Information Gain. Based on their scores, we select the POS bigram and trigram features whose scores are within the top fi</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>George Forman. 2003. An extensive empirical study of feature selection metrics for text classification. The Journal of Machine Learning Research, 3:1289–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minwoo Jeong</author>
<author>Chin-Yew Lin</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>Semi-supervised speech act recognition in emails and forums.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Iatural Language Processing.</booktitle>
<contexts>
<context position="12529" citStr="Jeong et al., 2009" startWordPosition="1998" endWordPosition="2001">nce has a question mark. CC Feature: A binary feature that indicates whether or not an email contains CC. 135 Participation Dominance Feature: The number of utterances each person makes in one path. Finally, we also include a simplified version of the ClueWordScore (CWS) developed by Carenini et al. (2007), which is listed below. Simplified CWS Feature: The number of overlaps of the content words that occur in both the current and adjacent sentences in the path, ignoring stopwords. 4.2 Dialogue Act Features The relative positions and length features have proven to be beneficial to both tasks (Jeong et al., 2009; Carenini et al., 2008). Hence, these are categorized as both dialogue acts and extractive summarization features. In addition, we use word and POS n-grams as our features for dialogue act modeling. These features are extracted by the following process explained in Carvalho et al. (2006). However, we extend the original approach in order to further abstract n-gram features to avoid making them too sparse to be effective. In this section, we describe the derivation process in detail. A multi-step approach is used to generate word n-gram features. First, all words are tagged with the named enti</context>
<context position="17432" citStr="Jeong et al. (2009)" startWordPosition="2798" endWordPosition="2801">ng and evaluation purposes. The corpus contains email threads from the World Wide Web Consortium (W3C) mailing list. It consists of 40 threads with an average of five emails per thread. The corpus provides extractive summaries of each email thread, all of which were annotated by three annotators. Hence, we use sentences that are selected by more than one annotator as the gold standard summary for each conversation. In addition, all sentences in the 39 out of 40 threads are annotated for dialogue act tags. The tagset consists of five general and 12 specific tags. All of these tags are based on Jeong et al. (2009). For our experiment, considering that our data is relatively small, we decide to use the coarser five tag set. The details are shown in Table 3. 1 http://mallet.cs.umass.edu 2 http://www.cs.ubc.ca/nest/lci/bc3.html Tag Description Relative Frequency (%) S Statement 73.8 Q Question 7.92 R Reply 5.23 Su Suggestion 5.62 M Miscellaneous 7.46 Table 3: Dialogue act tag categories and their relative frequency in the BC3 corpus After removing quoted sentences and redundant information such as senders and addresses, 1300 distinct sentences remain in the 39 email threads. The detailed content of the co</context>
</contexts>
<marker>Jeong, Lin, Lee, 2009</marker>
<rawString>Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee. 2009. Semi-supervised speech act recognition in emails and forums. In Proceedings of the 2009 Conference on Empirical Methods in Iatural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<date>1999</date>
<booktitle>Making large-Scale SVM Learning Practical. Advances in Kernel Methods -Support Vector Learning,</booktitle>
<editor>B. Schölkopf and C. Burges and A. Smola (ed.), MIT-Press,</editor>
<contexts>
<context position="19174" citStr="Joachims, 1999" startWordPosition="3083" endWordPosition="3084">ation (Shen et al., 2007; Kim et al., 2010; Kim et al., 2012). Hence, for comparison, we implement two different CRFs, one for extractive summarization and the other for dialogue act modeling. When classifying extractive summaries using the CRF, we only use its extractive summarization features. Similarly, when modeling dialogue acts, we only use its dialogue act features. In addition, we also com137 pare our system with a non-sequential classifier, a support vector machine (SVM), with the same settings as those described above. For these implementations, we use Mallet and SVM-light package3 (Joachims, 1999). In our experiment, we first measure separately the performance of extractive summarization and dialogue act modeling. The performance of extractive summarization is measured by its averaged precision, recall, and F-measure. For dialogue acts, we report the averaged-micro and macro accuracies as well as the averaged accuracies of each dialogue act. Second, we evaluate the combined performance of extractive summarization and dialogue act modeling tasks. In general, we are interested in the dialogue acts in summary sentences because they can be later used as input for other natural language pro</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999 Making large-Scale SVM Learning Practical. Advances in Kernel Methods -Support Vector Learning, B. Schölkopf and C. Burges and A. Smola (ed.), MIT-Press, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Chin-Yew Lin Lin</author>
</authors>
<title>Unsupervised Modeling of Dialog Acts in Asynchronous Conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of the twenty second International Joint Conference on Artificial Intelligence (IJCAI)</booktitle>
<location>Barcelona,</location>
<marker>Joty, Carenini, Lin, 2011</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, and Lin, Chin-Yew Lin. 2011. Unsupervised Modeling of Dialog Acts in Asynchronous Conversations. In Proceedings of the twenty second International Joint Conference on Artificial Intelligence (IJCAI) 2011. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Gabriel Murray</author>
<author>Raymond Ng</author>
</authors>
<title>Finding Topics in Emails: Is LDA enough? IIPS-2009 workshop on applications for topic models: text and beyond.</title>
<date>2009</date>
<location>Whistler, Canada.</location>
<marker>Joty, Carenini, Murray, Ng, 2009</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, Gabriel Murray, and Raymond Ng. 2009 Finding Topics in Emails: Is LDA enough? IIPS-2009 workshop on applications for topic models: text and beyond. Whistler, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kachites McCallum</author>
</authors>
<title>MALLET: A Machine Learning for Language Toolkit.</title>
<date>2002</date>
<location>http://mallet.cs.umass.edu.</location>
<contexts>
<context position="16314" citStr="McCallum, 2002" startWordPosition="2617" endWordPosition="2618"> a sequence. Hence, it is the most appropriate method for performing multiple labeling tasks on the same sequence. 136 Our DCRF uses the graph structure shown in Figure 2 with one chain (the top X nodes) modeling extractive summary and the other (the middle Y nodes) modeling dialogue acts. Each node in the observation sequence (the bottom Z nodes) corresponds to each sentence in a path of the fragment quotation graph of the email thread. As shown in Figure 2, the graph structure captures the relationship between extractive summaries and dialogue acts by connecting their nodes. We use Mallet1 (McCallum, 2002) to implement our DCRF model. It uses l2-based regularization to avoid overfitting, and a limited BFGS fitting algorithm to learn the DCRF model parameters. Also, it uses tree-based reparameterization (Wainwright et al., 2002) to compute the posterior marginal, or inference. Figure 2: The DCRF model used to create extractive summaries and model dialogue acts 6 Empirical Evaluations 6.1 Dataset Setup In our experiment, the publically available BC3 corpus2 (Ulrich et al., 2008) is used for training and evaluation purposes. The corpus contains email threads from the World Wide Web Consortium (W3C</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>McCallum, A. Kachites, 2002. MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Lawrence Cavedon</author>
<author>Timothy Baldwin</author>
</authors>
<title>Classifying dialogue acts in 1-to-1 live chats.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Iatural Language Processing (EMNLP 2010),</booktitle>
<pages>862--871</pages>
<location>Boston, USA.</location>
<contexts>
<context position="6710" citStr="Kim et al. (2010)" startWordPosition="1006" endWordPosition="1009">ch as combinations of n-gram sequences in emails as their features for a supervised learning approach. The experiment showed that their approach reduced classification error rates by 26.4%. Shafiq et al. (2011) proposed unsupervised dialogue act modeling in email threads and on forums. They introduced a graph-based and two probabilistic unsupervised approaches for modeling dialogue acts. By comparing those approaches, they demonstrated that the probabilistic approaches were quite effective and performed better than the graph-based one. While the following work is not done on the email domain, Kim et al. (2010) introduced a dialogue act classification on one-on-one online chat forums. To be able to capture sequential dialogue act dependency on chats, they applied a CRF model. They demonstrated that, compared with other classifiers, their CRF model performed the best. In their later work (Kim et al., 2012), they extended the domain to multi-party live chats and proposed new features for that domain. 3 Capturing Conversation Structure in Email Threads In this section, we describe how to build a fragment quotation graph which captures the conversation structure of any email thread at finer granularity.</context>
<context position="18601" citStr="Kim et al., 2010" startWordPosition="2992" endWordPosition="2995">9 email threads. The detailed content of the corpus is summarized in Table 4. Total Dataset No. of Threads 39 No. of Sentences 1300 No. of Extractive Summary Sentences 521 No. of S Sentences 959 No. of Q Sentences 103 No. of R Sentences 68 No. of Su Sentences 73 No. of M Sentences 97 Table 4: Detailed content of the BC3 corpus 6.2 Evaluation Metrics Here, we introduce evaluation metrics for our joint model of extractive summarization and dialogue act recognition. The CRF model has been shown to be the effective one in both dialogue act modeling and extractive summarization (Shen et al., 2007; Kim et al., 2010; Kim et al., 2012). Hence, for comparison, we implement two different CRFs, one for extractive summarization and the other for dialogue act modeling. When classifying extractive summaries using the CRF, we only use its extractive summarization features. Similarly, when modeling dialogue acts, we only use its dialogue act features. In addition, we also com137 pare our system with a non-sequential classifier, a support vector machine (SVM), with the same settings as those described above. For these implementations, we use Mallet and SVM-light package3 (Joachims, 1999). In our experiment, we fir</context>
</contexts>
<marker>Kim, Cavedon, Baldwin, 2010</marker>
<rawString>Su Nam Kim, Lawrence Cavedon, and Timothy Baldwin. 2010a. Classifying dialogue acts in 1-to-1 live chats. In Proceedings of the 2010 Conference on Empirical Methods in Iatural Language Processing (EMNLP 2010), pages 862–871, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Su Nam Kim</author>
<author>Lawrence Cavedon</author>
<author>Timothy Baldwin</author>
</authors>
<title>Classifying Dialogue Acts in Multiparty Live Chats,</title>
<date>2012</date>
<booktitle>In Proceedings of the 26th Pacific Asia Conference on Language, Information and Computation (PACLIC 26),</booktitle>
<pages>463--472</pages>
<location>Bali, Indonesia,</location>
<contexts>
<context position="7010" citStr="Kim et al., 2012" startWordPosition="1054" endWordPosition="1057">ed a graph-based and two probabilistic unsupervised approaches for modeling dialogue acts. By comparing those approaches, they demonstrated that the probabilistic approaches were quite effective and performed better than the graph-based one. While the following work is not done on the email domain, Kim et al. (2010) introduced a dialogue act classification on one-on-one online chat forums. To be able to capture sequential dialogue act dependency on chats, they applied a CRF model. They demonstrated that, compared with other classifiers, their CRF model performed the best. In their later work (Kim et al., 2012), they extended the domain to multi-party live chats and proposed new features for that domain. 3 Capturing Conversation Structure in Email Threads In this section, we describe how to build a fragment quotation graph which captures the conversation structure of any email thread at finer granularity. This graph was developed and shown to be effective by Carenini et al. (2011). A key assumption of this approach is that in order to effectively perform summarization and dialogue act modeling, a fine graph representation of the underlying conversation structure is needed. Here, we start with the sa</context>
<context position="18620" citStr="Kim et al., 2012" startWordPosition="2996" endWordPosition="2999">he detailed content of the corpus is summarized in Table 4. Total Dataset No. of Threads 39 No. of Sentences 1300 No. of Extractive Summary Sentences 521 No. of S Sentences 959 No. of Q Sentences 103 No. of R Sentences 68 No. of Su Sentences 73 No. of M Sentences 97 Table 4: Detailed content of the BC3 corpus 6.2 Evaluation Metrics Here, we introduce evaluation metrics for our joint model of extractive summarization and dialogue act recognition. The CRF model has been shown to be the effective one in both dialogue act modeling and extractive summarization (Shen et al., 2007; Kim et al., 2010; Kim et al., 2012). Hence, for comparison, we implement two different CRFs, one for extractive summarization and the other for dialogue act modeling. When classifying extractive summaries using the CRF, we only use its extractive summarization features. Similarly, when modeling dialogue acts, we only use its dialogue act features. In addition, we also com137 pare our system with a non-sequential classifier, a support vector machine (SVM), with the same settings as those described above. For these implementations, we use Mallet and SVM-light package3 (Joachims, 1999). In our experiment, we first measure separate</context>
</contexts>
<marker>Kim, Cavedon, Baldwin, 2012</marker>
<rawString>Su Nam Kim, Lawrence Cavedon and Timothy Baldwin (2012) Classifying Dialogue Acts in Multiparty Live Chats, In Proceedings of the 26th Pacific Asia Conference on Language, Information and Computation (PACLIC 26), Bali, Indonesia, pp. 463—472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
</authors>
<title>Summarizing Spoken and Written Conversations.</title>
<date>2008</date>
<booktitle>Empirical Methods in ILP (EMILP</booktitle>
<location>Waikiki, Hawaii,</location>
<marker>Murray, Carenini, 2008</marker>
<rawString>Gabriel Murray and Giuseppe Carenini. 2008. Summarizing Spoken and Written Conversations. Empirical Methods in ILP (EMILP 2008), Waikiki, Hawaii, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
</authors>
<title>Summarizing Spoken and Written Conversations. Generating and Validating Abstracts of Meeting Conversations: a User study (IILG</title>
<date>2010</date>
<location>Dublin, Ireland,</location>
<marker>Murray, Carenini, 2010</marker>
<rawString>Gabriel Murray and Giuseppe Carenini. 2010. Summarizing Spoken and Written Conversations. Generating and Validating Abstracts of Meeting Conversations: a User study (IILG 2010), Dublin, Ireland, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Renals Steve</author>
<author>Carletta Jean</author>
</authors>
<title>Extrative summarization of meeting recordings.</title>
<date>2005</date>
<booktitle>In Proceeding of Interspeech</booktitle>
<pages>593--596</pages>
<location>Lisbon, Portugal,</location>
<marker>Murray, Steve, Jean, 2005</marker>
<rawString>Gabriel Murray, Renals Steve, and Carletta Jean. 2005a. Extrative summarization of meeting recordings. In Proceeding of Interspeech 2005, Lisbon, Portugal, pages 593-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Owen Rambow</author>
<author>Lokesh Shrestha</author>
<author>John Chen</author>
<author>Chirsty Lauridsen</author>
</authors>
<title>Summarizing email threads.</title>
<date>2004</date>
<booktitle>In Proceedings of HLTIAACL</booktitle>
<contexts>
<context position="1509" citStr="Rambow et al. (2004)" startWordPosition="222" endWordPosition="225">ing correct summary sentences without losing performance on dialogue act modeling task. 1 Introduction Nowadays, an overwhelming amount of text information can be found on the web. Most of this information is redundant and thus the task of document summarization has attracted much attention. Since emails in particular are used for a wide variety of purposes, the process of automatically summarizing emails might be of great benefit in dealing with this excessive amount of information. Much work has already been conducted on email summarization. The first research on this topic was conducted by Rambow et al. (2004), who took a supervised learning approach to extracting important sentences. A study on the supervised summarization of email threads was also performed by Ulrich et al. (2009). This study used the regression-based method for classification. There have been studies on unsupervised summarization of email threads as well. Zhou et al. (2007, 2008) proposed a graph-based unsupervised approach to email conversation summarization using clue words, i.e., recurring words contained in replies. In addition, the task of labeling sentences with dialogue acts has become important and has been employed in m</context>
<context position="3744" citStr="Rambow et al. (2004)" startWordPosition="557" endWordPosition="560">ts. In this paper, we investigate this hypothesis by introducing a new sequential graphical model approach that performs dialogue act modeling and extractive summarization jointly on email threads. 2 Related Work While email summarization and dialogue act modeling have been effectively studied, in most previous work, these tasks were studied independently. This section provides related work for each task separately. 133 Proceedings of the SIGDIAL 2014 Conference, pages 133–140, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics 2.1 Extractive Summarization Rambow et al. (2004) introduced sentence extraction techniques that work for email threads. In their work, they introduced email-specific features and used a machine learning method to classify whether or not a sentence should be incorporated into a summary. Their experiments demonstrated that their features were highly effective for email summarization. Ulrich et al. (2009) proposed a regressionbased machine learning approaches to email thread summarization. They compared regression-based classifiers to binary classifiers and showed that their approach significantly improves the summarization accuracy. They empl</context>
<context position="5204" citStr="Rambow et al. (2004)" startWordPosition="781" endWordPosition="784">d before the summarization task is performed. Our work is aimed at a much closer integration of the two tasks by modeling them simultaneously. Carenini et al. (2007) developed a fragment quotation graph that can capture a fine-grain conversation structure in email threads, which we will describe in detail in Section 3. They then introduced a ClueWordSummarizer (CWS), a graph-based unsupervised summarization approach based on the concept of clue words, which are recurring words found in email replies. Their experiment showed that the CWS performs better than the email summarization approach in Rambow et al. (2004). Extractive summarization using a sequential labeling technique has also been studied. While this is not an email summarization, Shen et al. (2007) proposed a linear-chain Conditional Random Field (CRF) based approach for extractive document summarization. In their work, they treated the summarization task as a sequence labeling problem to take advantage of interaction relationships between sentences; their approach showed significant improvement when compared with non-sequential classifiers. 2.2 Dialogue Act Modeling The first studies on the dialogue act modeling in emails were performed by </context>
<context position="11198" citStr="Rambow et al. (2004)" startWordPosition="1776" endWordPosition="1780">ization and then describe how we derive the features for dialogue act modeling. All the features explained in this section, whether they belong to extractive summarization or dialogue act modeling, are included in our model. (a) A possible configuration of an email conversation (E2 and E3 reply to E1, and E4 replies to E3) (b) An example of a fragment quotation graph Figure 1: A fragment quotation graph derived from a possible configuration of an email conversation 4.1 Extractive Summarization Features The features we use for extractive summarization are mostly from Carenini et al. (2008) and Rambow et al. (2004) and have proven to be effective on conversational data. Details of these features are described below. Note that all sentences in an email thread are ordered based on paths derived from a fragment quotation graph. Length Feature: The number of words in each sentence. Relative Position Feature: The number of sentences preceding the current divided by the total number of sentences in one path. Thread Name Overlaps Feature: The number of overlaps of the content words between the email thread title and a sentence. Subject Name Overlaps Feature: The number of overlaps of the content words between </context>
</contexts>
<marker>Rambow, Shrestha, Chen, Lauridsen, 2004</marker>
<rawString>Owen Rambow, Lokesh Shrestha, John Chen, and Chirsty Lauridsen. 2004. Summarizing email threads. In Proceedings of HLTIAACL 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dou Shen</author>
<author>Jian-Tao Sun</author>
<author>Hua Li</author>
<author>Qiang Yang</author>
<author>Zheng Chen</author>
</authors>
<title>Document summarization using conditional random fields.</title>
<date>2007</date>
<booktitle>In Proc. of IJCAI,</booktitle>
<volume>7</volume>
<pages>2862--2867</pages>
<contexts>
<context position="5352" citStr="Shen et al. (2007)" startWordPosition="803" endWordPosition="806">i et al. (2007) developed a fragment quotation graph that can capture a fine-grain conversation structure in email threads, which we will describe in detail in Section 3. They then introduced a ClueWordSummarizer (CWS), a graph-based unsupervised summarization approach based on the concept of clue words, which are recurring words found in email replies. Their experiment showed that the CWS performs better than the email summarization approach in Rambow et al. (2004). Extractive summarization using a sequential labeling technique has also been studied. While this is not an email summarization, Shen et al. (2007) proposed a linear-chain Conditional Random Field (CRF) based approach for extractive document summarization. In their work, they treated the summarization task as a sequence labeling problem to take advantage of interaction relationships between sentences; their approach showed significant improvement when compared with non-sequential classifiers. 2.2 Dialogue Act Modeling The first studies on the dialogue act modeling in emails were performed by Cohen et al. (2004). They defined “email speech acts” (e.g., Request, Deliver, Propose, and Commit) and used machine learning methods to classify em</context>
<context position="18583" citStr="Shen et al., 2007" startWordPosition="2988" endWordPosition="2991">ces remain in the 39 email threads. The detailed content of the corpus is summarized in Table 4. Total Dataset No. of Threads 39 No. of Sentences 1300 No. of Extractive Summary Sentences 521 No. of S Sentences 959 No. of Q Sentences 103 No. of R Sentences 68 No. of Su Sentences 73 No. of M Sentences 97 Table 4: Detailed content of the BC3 corpus 6.2 Evaluation Metrics Here, we introduce evaluation metrics for our joint model of extractive summarization and dialogue act recognition. The CRF model has been shown to be the effective one in both dialogue act modeling and extractive summarization (Shen et al., 2007; Kim et al., 2010; Kim et al., 2012). Hence, for comparison, we implement two different CRFs, one for extractive summarization and the other for dialogue act modeling. When classifying extractive summaries using the CRF, we only use its extractive summarization features. Similarly, when modeling dialogue acts, we only use its dialogue act features. In addition, we also com137 pare our system with a non-sequential classifier, a support vector machine (SVM), with the same settings as those described above. For these implementations, we use Mallet and SVM-light package3 (Joachims, 1999). In our </context>
</contexts>
<marker>Shen, Sun, Li, Yang, Chen, 2007</marker>
<rawString>Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document summarization using conditional random fields. In Proc. of IJCAI, volume 7, 2862–2867.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Khashayar Rohanimanesh</author>
<author>Andrew McCallum</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In</title>
<date>2004</date>
<booktitle>Proc. ICML.</booktitle>
<contexts>
<context position="15439" citStr="Sutton et al., 2004" startWordPosition="2473" endWordPosition="2476">[MODAL_STRONG] ‘can&apos;, &apos;could&apos;, &apos;may&apos;, &apos;might&apos; [MODAL_WEAK] &apos;do&apos;, &apos;does&apos;, &apos;did&apos;, ‘done&apos; [DO] &apos;is&apos;, &apos;was&apos;, &apos;were&apos;, &apos;are&apos;, &apos;been&apos; &apos;be&apos;, &apos;am&apos; [BE] &apos;after&apos; , &apos;before&apos;, &apos;during&apos; [AAAFTER] ‘Jack”, “Wendy” [Personal_PRONOUN] “New York” [LOCATION] “Acme Corp.” [ORGANIZATION] Table 1: Some Preprocessing Replacement Pattern Word Unigram Word Bigram ? [MODAL_STRONG] [I] anyone [IT] ? WWHH [DO] anyone deny [WWHH] [BE] [Personal _PRONOUN] [BE] [IT] Table 2: Sample word n-grams selected as the features for Question dialogue act 5 The Sequential Labeling Task We use a Dynamic Conditional Random Field (DCRF) (Sutton et al., 2004) for labeling tasks. A DCRF is a generalization of a linear-chain CRF which allows us to represent complex interaction between labels. To be more precise, it is a conditionally-trained undirected graphical model whose structure and parameters are repeated over a sequence. Hence, it is the most appropriate method for performing multiple labeling tasks on the same sequence. 136 Our DCRF uses the graph structure shown in Figure 2 with one chain (the top X nodes) modeling extractive summary and the other (the middle Y nodes) modeling dialogue acts. Each node in the observation sequence (the bottom</context>
</contexts>
<marker>Sutton, Rohanimanesh, McCallum, 2004</marker>
<rawString>Charles Sutton, Khashayar Rohanimanesh, and Andrew McCallum. 2004. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. In Proc. ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maryam Tavafi</author>
<author>Yashar Mehdad</author>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
</authors>
<title>Dialogue Act Recognition in Synchronous and Asynchronous Conversations.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>117--121</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Metz, France.</location>
<marker>Tavafi, Mehdad, Joty, Carenini, Ng, 2013</marker>
<rawString>Maryam Tavafi, Yashar Mehdad, Shafiq Joty, Giuseppe Carenini and Raymond Ng. 2013. Dialogue Act Recognition in Synchronous and Asynchronous Conversations. In Proceedings of the SIGDIAL 2013 Conference, pages 117–121, Metz, France. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-IAACL</booktitle>
<pages>252--259</pages>
<contexts>
<context position="14271" citStr="Toutanova et al., 2003" startWordPosition="2296" endWordPosition="2299"> bigrams are extracted. In this paper, unigrams and bigrams refer to all possible sequences of length one and two terms. After extracting all unigrams and bigrams for each dialogue act, we then compute Information Gain Score (Forman, 2003) and select the n-grams whose scores are in the top five greatest on the training set. In this way, we can automatically detect features that represent the characteristics of each dialogue act. In addition to word ngrams, we also include POS n-grams in our features. In a similar way, we first tag each word in sentences with POS using the Stanford POS tagger (Toutanova et al., 2003). Then, for each dialogue act, we extract bigrams and trigrams, all of which are scored by the Information Gain. Based on their scores, we select the POS bigram and trigram features whose scores are within the top five greatest. One example of word n-gram features for a Question dialogue act selected by this derivation method is shown in Table 2. Pattern Replacement ‘why’, ‘where’, ‘who’, ‘what’ ‘when’ [WWHH] nominative pronouns [I] objective pronouns [ME] &apos;it&apos;, &apos;those&apos;, &apos;these&apos;, &apos;this&apos;, &apos;that&apos; [IT] &apos;will&apos;, ‘would&apos;, &apos;shall&apos;, &apos;should&apos;, &apos;must&apos; [MODAL_STRONG] ‘can&apos;, &apos;could&apos;, &apos;may&apos;, &apos;might&apos; [MODAL</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher Manning, and Yoram Singer. 2003. Feature-Rich Part-ofSpeech Tagging with a Cyclic Dependency Network. In Proceedings of HLT-IAACL 2003, pp. 252-259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Ulrich</author>
<author>Giuseppe Carenini</author>
<author>Gabriel Murray</author>
<author>Raymond T Ng</author>
</authors>
<title>Regression-Based Summarization of Email Conversations.</title>
<date>2009</date>
<publisher>ICWSM</publisher>
<contexts>
<context position="1685" citStr="Ulrich et al. (2009)" startWordPosition="250" endWordPosition="253">web. Most of this information is redundant and thus the task of document summarization has attracted much attention. Since emails in particular are used for a wide variety of purposes, the process of automatically summarizing emails might be of great benefit in dealing with this excessive amount of information. Much work has already been conducted on email summarization. The first research on this topic was conducted by Rambow et al. (2004), who took a supervised learning approach to extracting important sentences. A study on the supervised summarization of email threads was also performed by Ulrich et al. (2009). This study used the regression-based method for classification. There have been studies on unsupervised summarization of email threads as well. Zhou et al. (2007, 2008) proposed a graph-based unsupervised approach to email conversation summarization using clue words, i.e., recurring words contained in replies. In addition, the task of labeling sentences with dialogue acts has become important and has been employed in many conversation analysis systems. For example, applications such as meeting summarization and collaborative task learning agents use dialogue acts as their underlying structur</context>
<context position="4101" citStr="Ulrich et al. (2009)" startWordPosition="612" endWordPosition="615">. This section provides related work for each task separately. 133 Proceedings of the SIGDIAL 2014 Conference, pages 133–140, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics 2.1 Extractive Summarization Rambow et al. (2004) introduced sentence extraction techniques that work for email threads. In their work, they introduced email-specific features and used a machine learning method to classify whether or not a sentence should be incorporated into a summary. Their experiments demonstrated that their features were highly effective for email summarization. Ulrich et al. (2009) proposed a regressionbased machine learning approaches to email thread summarization. They compared regression-based classifiers to binary classifiers and showed that their approach significantly improves the summarization accuracy. They employed the feature set introduced by Rambow et al. (2004) as their baseline and introduced new features that are also effective for email summarization. Some of their features refer to dialogue acts but the assumption is that they are computed before the summarization task is performed. Our work is aimed at a much closer integration of the two tasks by mode</context>
</contexts>
<marker>Ulrich, Carenini, Murray, Ng, 2009</marker>
<rawString>Jan Ulrich, Giuseppe Carenini, Gabriel Murray, and Raymond T. Ng: Regression-Based Summarization of Email Conversations. ICWSM 2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Ulrich</author>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
</authors>
<title>A publicly available annotated corpus for supervised email summarization.</title>
<date>2008</date>
<booktitle>AAAI-2008 EMAIL Workshop.</booktitle>
<contexts>
<context position="16794" citStr="Ulrich et al., 2008" startWordPosition="2688" endWordPosition="2691">tructure captures the relationship between extractive summaries and dialogue acts by connecting their nodes. We use Mallet1 (McCallum, 2002) to implement our DCRF model. It uses l2-based regularization to avoid overfitting, and a limited BFGS fitting algorithm to learn the DCRF model parameters. Also, it uses tree-based reparameterization (Wainwright et al., 2002) to compute the posterior marginal, or inference. Figure 2: The DCRF model used to create extractive summaries and model dialogue acts 6 Empirical Evaluations 6.1 Dataset Setup In our experiment, the publically available BC3 corpus2 (Ulrich et al., 2008) is used for training and evaluation purposes. The corpus contains email threads from the World Wide Web Consortium (W3C) mailing list. It consists of 40 threads with an average of five emails per thread. The corpus provides extractive summaries of each email thread, all of which were annotated by three annotators. Hence, we use sentences that are selected by more than one annotator as the gold standard summary for each conversation. In addition, all sentences in the 39 out of 40 threads are annotated for dialogue act tags. The tagset consists of five general and 12 specific tags. All of these</context>
</contexts>
<marker>Ulrich, Murray, Carenini, 2008</marker>
<rawString>Jan Ulrich, Gabriel Murray, and Giuseppe Carenini. 2008. A publicly available annotated corpus for supervised email summarization. AAAI-2008 EMAIL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin J Wainwright</author>
<author>Tommi Jaakkola</author>
<author>Alan S Willsky</author>
</authors>
<title>Treebased Reparameterization for Approximate Inference on Loopy Graphs.</title>
<date>2002</date>
<booktitle>In Advances in Ieural Information Processing Systems 14,</booktitle>
<pages>1001--1008</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16540" citStr="Wainwright et al., 2002" startWordPosition="2648" endWordPosition="2651">active summary and the other (the middle Y nodes) modeling dialogue acts. Each node in the observation sequence (the bottom Z nodes) corresponds to each sentence in a path of the fragment quotation graph of the email thread. As shown in Figure 2, the graph structure captures the relationship between extractive summaries and dialogue acts by connecting their nodes. We use Mallet1 (McCallum, 2002) to implement our DCRF model. It uses l2-based regularization to avoid overfitting, and a limited BFGS fitting algorithm to learn the DCRF model parameters. Also, it uses tree-based reparameterization (Wainwright et al., 2002) to compute the posterior marginal, or inference. Figure 2: The DCRF model used to create extractive summaries and model dialogue acts 6 Empirical Evaluations 6.1 Dataset Setup In our experiment, the publically available BC3 corpus2 (Ulrich et al., 2008) is used for training and evaluation purposes. The corpus contains email threads from the World Wide Web Consortium (W3C) mailing list. It consists of 40 threads with an average of five emails per thread. The corpus provides extractive summaries of each email thread, all of which were annotated by three annotators. Hence, we use sentences that </context>
</contexts>
<marker>Wainwright, Jaakkola, Willsky, 2002</marker>
<rawString>Martin J. Wainwright, Tommi Jaakkola, and Alan S. Willsky. 2002. Treebased Reparameterization for Approximate Inference on Loopy Graphs. In Advances in Ieural Information Processing Systems 14, pages 1001 1008. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
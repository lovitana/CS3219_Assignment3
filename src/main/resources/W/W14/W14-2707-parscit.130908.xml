<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000481">
<title confidence="0.820565">
Detecting and Evaluating Local Text Reuse in Social Networks
</title>
<author confidence="0.987624">
Shaobin Xu*, David A. Smith*, Abigail Mullen†, and Ryan Cordell$
</author>
<affiliation confidence="0.968495666666667">
NULab for Texts, Maps, and Networks
College of Computer and Information Science*, Department of History†, Department of English$
Northeastern University, Boston, MA
</affiliation>
<sectionHeader confidence="0.987789" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969166666667">
Texts propagate among participants in
many social networks and provide evi-
dence for network structure. We describe
intrinsic and extrinsic evaluations for algo-
rithms that detect clusters of reused pas-
sages embedded within longer documents
in large collections. We explore applica-
tions of these approaches to two case stud-
ies: the culture of free reprinting in the
nineteenth-century United States and the
use of similar language in the public state-
ments of U.S. members of Congress.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999958155555556">
While many studies of social networks use surveys
and direct observation to catalogue actors (nodes)
and their interactions (edges), we often cannot di-
rectly observe network links. Instead, we might
observe behavior by network participants that pro-
vides indirect evidence for social ties.
One revealing form of shared behavior is the
reuse of text by different social actors. Meth-
ods to uncover invisible links among sources of
text methods would have broad applicability be-
cause of the very general nature of the problem—
sources of text include websites, newspapers, in-
dividuals, corporations, political parties, and so
on. Further, discerning those hidden links be-
tween sources would provide more effective ways
of identifying the provenance and diverse sources
of information, and to build predictive models of
the diffusion of information.
There are substantial challenges, however, in
building a methodology to study text reuse, includ-
ing: scalable detection of reused passages; iden-
tification of appropriate statistical models of text
mutation; inference methods for characterizing
missing nodes that originate or mediate text trans-
mission; link inference conditioned on textual
topics; and the development of testbeds through
which predictions of the resulting models might
be validated against some broader understanding
of the processes of transmission.
In this paper, we sketch relevant features of our
two testbed collections (§2) and then describe ini-
tial progress on developing algorithms for detect-
ing reused passages embedded within the larger
text output of social network nodes (§3). We then
describe an intrinsic evaluation of the efficiency of
these techniques for scaling up text reuse detec-
tion (§4). Finally, we perform an extrinsic evalua-
tion of the network links inferred from text reuse
by correlating them with side information about
the underlying social networks (§5). A prelimi-
nary version of the text reuse detection system was
presented for a single, smaller corpus in (Anony-
mous, 2013), but without the extrinsic or much of
the intrinsic evaluation and without data on the un-
derlying networks.
</bodyText>
<sectionHeader confidence="0.581727" genericHeader="method">
2 Case Studies in Text Reuse
</sectionHeader>
<bodyText confidence="0.999493166666667">
The case studies in this paper, which form the
basis for our experimental evaluations below, in-
volve two fairly divergent domains: the infor-
mational and literary ecology of the nineteenth-
century United States and of twenty-first century
U.S. legislators.
</bodyText>
<subsectionHeader confidence="0.982347">
2.1 Tracking Viral Texts in 19c Newspapers
</subsectionHeader>
<bodyText confidence="0.9970506">
In American Literature and the Culture of Reprint-
ing, McGill (2003) argues that American literary
culture in the nineteenth century was shaped by the
widespread practice of reprinting stories and po-
ems, usually without authorial permission or even
</bodyText>
<page confidence="0.960679">
50
</page>
<note confidence="0.6195595">
Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 50–57,
Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.994758868852459">
knowledge, in newspapers, magazines, and books.
Without substantial copyright enforcement, texts
circulated promiscuously through the print market
and were often revised by editors during the pro-
cess. These “viral” texts—be they news stories,
short fiction, or poetry—are much more than his-
torical curiosities. The texts that editors chose to
pass on are useful barometers of what was excit-
ing or important to readers during the period, and
thus offer significant insight into the priorities and
concerns of the culture.
Nineteenth-century U.S. newspapers were usu-
ally associated with a particular political party, re-
ligious denomination, or social cause (e.g., tem-
perance or abolition). Mapping the specific lo-
cations and venues in which varied texts circu-
lated would therefore allow us to answer ques-
tions about how reprinting and the public sphere in
general were affected by geography, communica-
tion and transportation networks, and social, polit-
ical, and religious affinities. These effects should
be particularly observable in the period before the
Civil War and the rise of wire services that broad-
cast content at industrial scales (Figure 1).
To study the reprint culture of this period, we
crawled the online newspaper archives of the Li-
brary of Congress’s Chronicling America project
(chroniclingamerica.loc.gov). Since
the Chronicling America project aggregates state-
level digitization efforts, there are some significant
gaps: e.g., there are no newspapers from Mas-
sachusetts, which played a not insubstantial role
in the literary culture of the period. While we con-
tinue to collect data from other sources in order to
improve our network analysis, the current dataset
remains a useful, and open, testbed for text reuse
detection and analysis of overall trends. For the
pre-Civil War period, this corpus contains 1.6 bil-
lion words from 41,829 issues of 132 newspapers.
Another difficulty with this collection is that it
consists of the OCR’d text of newspaper issues
without any marking of article breaks, headlines,
or other structure. The local alignment methods
described in §3 are designed not only to mitigate
this problem, but also to deal with partial reprint-
ing. One newspaper issue, for instance, might
reprint chapters 4 and 5 of a Thackeray novel
while another issue prints only chapter 5.
Since our goal is to detect texts that spread from
one venue to another, we are not interested in texts
that were reprinted frequently in the same newspa-
Figure 1: Newspaper issues mentioning “associ-
ated press” by year, from the Chronicling America
corpus. The black regression line fits the raw num-
ber of issues; the red line fits counts corrected for
the number of times the Associated Press is men-
tioned in each issue.
per, or series, to use the cataloguing term. This in-
cludes material such as mastheads and manifestos
and also the large number of advertisements that
recur week after week in the same newspaper.
</bodyText>
<subsectionHeader confidence="0.998956">
2.2 Statements by Members of Congress
</subsectionHeader>
<bodyText confidence="0.999982545454546">
Members of the U.S. Congress are of course even
more responsive to political debates and incentives
than nineteenth-century newspapers. Representa-
tives and senators are also a very well-studied so-
cial network. Following Margolin et al. (2013),
we analyzed a dataset of more than 400,000 pub-
lic statements made by members of the 112th Sen-
ate and House between January 2011 and August
2012. The statements were downloaded from the
Vote Smart Project website (votesmart.com).
According to Vote Smart, the Members’ pub-
lic statements include any press releases, state-
ments, newspaper articles, interviews, blog en-
tries, newsletters, legislative committee websites,
campaign websites and cable news show websites
(Meet the Press, This Week, etc.) that contain
direct quotes from the Member. Since we are
primarily interested in the connections between
Members, we will, as we see below, want to fil-
ter out reuse among different statements by the
same member. That information could be interest-
ing for other reasons—for instance, tracking slight
</bodyText>
<page confidence="0.997441">
51
</page>
<bodyText confidence="0.999817428571429">
changes in the phrasing of talking points or sub-
stantive positions.
We supplemented these texts with categorical
data chambers and parties and with continuous
representations of ideology using the first dimen-
sion of the DW-NOMINATE scores (Carroll et al.,
2009).
</bodyText>
<sectionHeader confidence="0.984316" genericHeader="method">
3 Text Reuse Detection
</sectionHeader>
<bodyText confidence="0.999968966666667">
As noted above, we are interested in detecting pas-
sages of text reuse (poems or stories; political talk-
ing points) that comprise a small fraction of the
containing documents (newspaper issues; political
speeches). Using the terminology of biological se-
quence alignment, we are interested in local align-
ments between documents. In text reuse detection
research, two primary methods are n-gram shin-
gling and locality-sensitive hashing (LSH) (Hen-
zinger, 2006). The need for local alignments
makes LSH less practical without performing a
large number of sliding-window matches.
In contrast to work on near-duplicate document
detection and to work on “meme tracking” that
takes text between quotation marks as the unit of
reuse (Leskovec et al., 2009; Suen et al., 2013),
here the boundaries of the reused passages are not
known. Also in contrast to work on the contempo-
rary news cycle and blogosphere, we are interested
both in texts that are reprinted within a few days
and after many years. We thus cannot exclude
potentially matching documents for being far re-
moved in time. Text reuse that occurs only among
documents from the same “source” (run of news-
papers; Member of Congress) should be excluded.
Similarly, Henzinger (2006) notes that many of the
errors in near-duplicate webpage detection arose
from false matches among documents from the
same website that shared boilerplate navigational
elements.
</bodyText>
<subsectionHeader confidence="0.999564">
3.1 Efficient N-gram Indexing
</subsectionHeader>
<bodyText confidence="0.999987291666667">
The first step is to build for each n-gram feature an
inverted index of the documents where it appears.
As in other duplicate detection and text reuse ap-
plications, we are only interested in the n-grams
shared by two or more documents. The index,
therefore, does not need to contain entries for the
n-grams that occur only once. We use the two-
pass space-efficient algorithm described in Huston
et al. (2011), which, empirically, is very efficient
on large collections. In a first pass, n-grams are
hashed into a fixed number of bins. On the sec-
ond pass, n-grams that hash to bins with one oc-
cupant can be discarded; other postings are passed
through. Due to hash collisions, there may still
be a small number of singleton n-grams that reach
this stage. These singletons are filtered out as the
index is written.
In building an index of n-grams, an index of
(n-1)-grams can also provide a useful filter. No
5-gram, for example, can occur twice unless its
constituent 4-grams occur at least twice. We do
not use this optimization in our experiments; in
practice, n-gram indexing is less expensive than
the later steps.
</bodyText>
<subsectionHeader confidence="0.999697">
3.2 Extracting and Ranking Candidate Pairs
</subsectionHeader>
<bodyText confidence="0.990141666666666">
Once we have an inverted index of the documents
that contain each (skip) n-gram, we use it to gen-
erate and rank document pairs that are candidates
for containing reprinted texts. Each entry, or post-
ing list, in the index may be viewed as a set of pairs
(di, pi) that record the document identifier and po-
sition in that document of that n-gram.
Once we have a posting list of documents con-
taining each distinct n-gram, we output all pairs of
documents in each list. We suppress repeated n-
grams that appear in different issues of the same
newspaper. These repetitions often occur in edito-
rial boilerplate or advertisements, which, while in-
teresting, are outside the scope of this project. We
also suppress n-grams that generate more than (u )
2
pairs, where u is a parameter.1 These frequent n-
grams are likely to be common fixed phrases. Fil-
tering terms with high document frequency has led
to significant speed increases with small loss in ac-
curacy in other document similarity work (Elsayed
et al., 2008). We then sort the list of repeated n-
grams by document pair, which allows us to assign
a score to each pair based on the number of over-
lapping n-grams and the distinctiveness of those
n-grams. Table 1 shows the parameters for trading
off recall and precision at this stage.
</bodyText>
<subsectionHeader confidence="0.999891">
3.3 Computing Local Alignments
</subsectionHeader>
<bodyText confidence="0.999847333333333">
The initial pass returns a large ranked list of can-
didate document pairs, but it ignores the order
of the n-grams as they occur in each document.
We therefore employ local alignment techniques
to find compact passages with the highest proba-
bility of matching. The goal of this alignment is
</bodyText>
<footnote confidence="0.984142">
1The filter is parameterized this way because it is applied
after removing document pairs in the same series.
</footnote>
<page confidence="0.991798">
52
</page>
<table confidence="0.47102825">
n n-gram order
w maximum width of skip n-grams
g minimum gap of skip n-grams
u maximum distinct series in the posting list
</table>
<tableCaption confidence="0.986138">
Table 1: Parameters for text reuse detection
</tableCaption>
<bodyText confidence="0.999971876923077">
to increase the precision of the detected document
pairs while maintaining high recall. Due to the
high rate of OCR errors, many n-grams in match-
ing articles will contain slight differences.
Unlike some partial duplicate detection tech-
niques based on global alignment (Yalniz et al.,
2011), we cannot expect all or even most of the
articles in two newspaper issues, or the text in two
books with a shared quotation, to align. Rather,
as in some work on biological subsequence align-
ment (Gusfield, 1997), we are looking for re-
gions of high overlap embedded within sequences
that are otherwise unrelated. We therefore em-
ploy the Smith-Waterman dynamic programming
algorithm with an affine gap penalty. This use
of model-based alignment distinguishes this ap-
proach for other work, for detecting shorter quota-
tions, that greedily expands areas of n-gram over-
lap (Kolak and Schilit, 2008; Horton et al., 2010).
We do, however, prune the dynamic programming
search by forcing the alignment to go through po-
sition pairs that contain a matching n-gram from
the previous step, as long as the two n-grams are
unique in their respective texts. Even the exact
Smith-Waterman algorithm, however, is an ap-
proximation to the problem we aim to solve. If,
for instance, two separate articles from one news-
paper issue were reprinted in another newspaper
issue in the opposite order—or separated by a long
span of unrelated matter—the local alignment al-
gorithm would simply output the better-aligned ar-
ticle pair and ignore the other. Anecdotally, we
only observed this phenomenon once in the news-
paper collection, where two different parodies of
the same poem were reprinted in the same issue.
In any case, our approach can easily align differ-
ent passages in the same document to passages in
two other documents.
The dynamic program proceeds as follows. In
this paper, two documents would be treated as se-
quences of text X and Y whose individual charac-
ters are indexed as Xi and Yj. Let W (Xi, Yj) be
the score of aligning character Xi to character Yj.
Higher scores are better. We use a scoring function
where only exact character matches get a positive
score and any other pair gets a negative score. We
also account for additional text appearing on either
X or Y . Let Wg be the score, which is negative,
of starting a “gap”, where one sequence includes
text not in the other. Let Wc be the cost for con-
tinuing a gap for one more character. This “affine
gap” model assigns a lower cost to continuing a
gap than to starting one, which has the effect of
making the gaps more contiguous. We use an as-
signment of weights fairly standard in genetic se-
quences where matching characters score 2, mis-
matched characters score -1, beginning a gap costs
-5, and continuing a gap costs -0.5. We leave for
future work the optimization of these weights for
the task of capturing shared policy ideas.
As with other dynamic programming algo-
rithms such as Levenshtein distance, the Smith-
Waterman algorithm operates by filling in a
“chart” of partial results. The chart in this case
is a set of cells indexed by the characters in X and
</bodyText>
<equation confidence="0.936385266666667">
Y , and we initialize it as follows:
H(0, 0) = 0
H(i, 0) = E(i, 0) = Wg + i · Wc
H(0,j) = F(0,j) = Wg + j · Wc
The algorithm is then defined by the following re-
currence relations:
{ 0
E(i, j)
F(i, j)
H(i − 1, j − 1) + W(Xi,Yj)
�E(i, j) = max
E(i,j − 1) + Wc
H(i, j − 1) + Wg + Wc
r F(i − 1, j) + Wc
F(i,j) = max Sl H(i − 1,j) + Wg + Wc
</equation>
<bodyText confidence="0.999969363636364">
The main entry in each cell H(i, j) represents the
score of the best alignment that terminates at po-
sition i and j in each sequence. The intermediate
quantities E and F are used for evaluating gaps.
Due to taking a max with 0, H(i, j) cannot be neg-
ative. This is what allows Smith-Waterman to ig-
nore text before and after the locally aligned sub-
strings of each input.
After completing the chart, we then find the op-
timum alignment by tracing back from the cell
with the highest cumulative value H(i, j) until a
</bodyText>
<equation confidence="0.592803">
H(i, j) = max
</equation>
<page confidence="0.990611">
53
</page>
<bodyText confidence="0.999811416666667">
cell with a value of 0 is reached. These two cells
represent the bounds of the sequence, and the over-
all SW alignment score reflects the extent to which
the characters in the sequences align and the over-
all length of the sequence.
In our implementation, we include one further
speedup: since in a previous step we identified n-
grams that are shared between the two documents,
we assume that any alignment of those documents
must include those n-grams as matches. In some
cases, this anchoring of the alignment might lead
to suboptimal SW alignment scores.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="method">
4 Intrinsic Evaluation
</sectionHeader>
<bodyText confidence="0.999975297297297">
To evaluate the precision and recall of text reuse
detection, we create a pseudo-relevant set of doc-
ument pairs by pooling the results of several runs
with different parameter settings. For each doc-
ument pair found in the union of these runs, we
observe the length, in matching characters, of the
longest local alignment. (Using matching charac-
ter length allows us to abstract somewhat from the
precise cost matrix.) We can then observe how
many aligned passages each method retrieves that
are at least 50,000 character matches in length, at
least 20,000 character matches in length, and so
on. The candidate pairs are sorted by the number
of overlapping n-grams; we measure the pseudo-
recall at several length cutoffs. For each position
in a ranked list of document pairs, we then mea-
sure the precision: what proportion of documents
retrieved are in fact 50k, 20k, etc., in length? Since
we wish to rank documents by the length of the
aligned passages they contain, this is a reason-
able metric. One summary of these various values
is the average precision: the mean of the preci-
sion at every rank position that contains an actu-
ally relevant document pair. One of the few earlier
evaluations of local text reuse, by Seo and Croft
(2008), compared fingerprinting methods to a tri-
gram baseline. Since their corpus contained short
individual news articles, the extent of the reused
passages was evaluated qualitatively rather than by
alignment.
Figure 2 shows the average precision of differ-
ent parameter settings on the newspaper collec-
tion, ranked by the number of pairs each returns.
If the pairwise document step returns a large num-
ber of pairs, we will have to perform a large num-
ber of more costly Smith-Waterman alignments.
On this collection, a good tradeoff between space
</bodyText>
<figure confidence="0.462955">
Pairs examined
</figure>
<figureCaption confidence="0.90307">
Figure 2: Average precision for aligned passages
of different minimum length in characters. Verti-
cal red lines indicate the performance of different
parameter settings (see Table 1).
</figureCaption>
<bodyText confidence="0.999352842105263">
and speed is achieved by skip bigram features. In
the best case, we look at bigrams where there is a
gap of at least 95, and not more than 105, words
between the first and second terms (n=2 u=100
w=105 g=95).
While average precision is a good summary of
the quality of the ranked list at any one point,
many applications will simply be concerned with
the total recall after some fixed amount of pro-
cessing. Figure 3 also summarizes these recall re-
sults by the absolute number of document pairs
examined. From these results, it is clear the
several good settings perform well at retrieving
all reprinted passages of at least 5000 charac-
ters. Even using the pseudo-recall metric, how-
ever, even the best operating points fail in the end
to retrieve about 10% of the reprints detected by
some other setting for all documents of at least
1000 characters.
</bodyText>
<sectionHeader confidence="0.991885" genericHeader="method">
5 Extrinsic Evaluation
</sectionHeader>
<bodyText confidence="0.999741142857143">
While political scientists, historians, and literary
scholars will, we hope, find these techniques use-
ful and perform close reading and manual analysis
on texts of interest, we would like to validate our
results without a costly annotation campaign. In
this paper, we explore the correlation of patterns of
text reuse with what is already known from other
</bodyText>
<figure confidence="0.981878411764706">
5e+06 1e+07 2e+07 5e+07
Average precision
0.0 0.1 0.2 0.3 0.4 0.5
10k
1k
2k
1k
2k
1k
1k
10k
2k
10k
1k
54
Recall
Pairs examined
</figure>
<figureCaption confidence="0.9935935">
Figure 3: (Pseudo-)Recall for aligned passages of
different minimum lengths in characters.
</figureCaption>
<bodyText confidence="0.999889486486487">
sources about the connections among Members of
Congress, newspaper editors, and so on. This idea
was inspired by Margolin et al. (2013), who used
these techniques to test rhetorical theories of “se-
mantic organizing processes” on the congressional
statements corpus.
The approach is quite simple: measure the cor-
relation between some metric of text reuse be-
tween actors in a social network and other features
of the network links between those actors. The
metric of text reuse might be simply the number of
exact n-grams shared by the language of two au-
thors (Margolin et al., 2013); alternately, it might
be the absolute or relative length of all the aligned
passages shared by two authors or the tree distance
between them in a phylogenetic reconstruction. To
measure the correlation of a text reuse metric with
a single network, we can simply use Pearson’s cor-
relation; for more networks, we can use multivari-
ate regression. Due to, for instance, autocorrela-
tion among edges arising from a particular node,
we cannot proceed as if the weight of each edge in
the text reuse network can be compared indepen-
dently to the weight of the corresponding edges in
other networks. We therefore use nonparametric
permutation tests using the quadratic assignment
procedure (QAP) to resample several networks
with the same structure but different labels and
weights. The QAP achieves this by reordering the
rows and columns of one network’s adjacency ma-
trix according to the same permutation. The per-
muted network then has the same structure—e.g.,
degree distribution—but should no longer exhibit
the same correlations with the other network(s).
We can run QAP to generate confidence intervals
for both single (Krackhardt, 1987) and multiple
correlations (Dekker et al., 2007).
</bodyText>
<subsectionHeader confidence="0.9991">
5.1 Congressional Statements
</subsectionHeader>
<bodyText confidence="0.999982692307692">
We model the connection between the log magni-
tude of reused text and the strength of ties among
Members according to whether they are in the
same chamber and how similar they are on the
first dimension of the DW-nominate ideological
scale (Carroll et al., 2009). On the left side of Ta-
ble 2 are shown the results for correlating reused
passages of certain minimum lengths (10, 16, 32
words) with these underlying features. On the
right are shown the similar results of (Margolin
et al., 2013) that simply used the exact size of
the n-gram overlap between Members’ statements
for increasing values of n. The alignment anal-
ysis proposed in this paper achieves similar re-
sults when passages and n-grams are short. Our
analysis, however, achieves higher single and mul-
tiple correlations among networks are the pas-
sages grow longer. This is unsurprising since the
probability of an exact 32-gram match is much
smaller than that of a 32-word-long alignment that
might contain a few differences. In particular,
the much higher coefficients for DW-nominate at
longer aligned lengths suggests that ideological in-
fluence still dominates over similarities induced by
the procedural environment of each congressional
chamber.
</bodyText>
<subsectionHeader confidence="0.95848">
5.2 Network Connections of 19c Reprints
</subsectionHeader>
<bodyText confidence="0.999962666666667">
For the antebellum newspaper corpus, we are also
interested in how political affinity correlates with
reprinting similar texts. We have also added
variables for social causes such as temperance,
women’s rights, and abolition that—while cer-
tainly not orthogonal to political commitments—
might sometimes operate independently. In addi-
tion, we also added a “shared state” variable to ac-
count for shared political and social environments
of more limited scope. Figure 4 shows a partic-
ularly strong example of a geographic effect: the
statement of the radical abolitionist John Brown
after being condemned to death for attacking a
federal arsenal and attempting to raise a slave re-
bellion was very unlikely to be published in the
</bodyText>
<figure confidence="0.7420275">
1e+01 1e+03 1e+05 1e+07
0.0 0.2 0.4 0.6 0.8 1.0
</figure>
<page confidence="0.941353">
55
</page>
<table confidence="0.957710888888889">
aligned passages of &gt; n words n-grams of length
10 16 32 8 16 32
First-order Pearson correlations
DW-nominate 0.26*** 0.25*** 0.23*** 0.26*** 0.22*** 0.16***
same chamber 0.05* 0.08** 0.13*** -0.05*** 0.21*** 0.10***
Regression coefficients
DW-nominate 0.72*** 0.75*** 0.74*** 1.31*** 2.67*** 0.36
same chamber 0.15** 0.27*** 0.42*** 0.20 3.14*** 0.81***
R-squared .069 .070 .073 .068 .073 .010
</table>
<tableCaption confidence="0.997739">
Table 2: Correlations between log length of aligned text and other author networks in public statements
by Members of Congress. *p &lt; .05, * * p &lt; .01, * * *p &lt; .001
</tableCaption>
<bodyText confidence="0.998826794117647">
South.
Using information from the Chronicling Amer-
ica cataloguing and from other newspaper histo-
ries, we coded each of the 132 newspapers in the
corpus with these political and social affinities.
We then counted the number of reprinted passages
shared by each pair of newspapers. There is not
a deterministic relationship between the number
of pairs of newspapers sharing an affinity and the
number of reprints shared by those papers. While
our admittedly partial corpus only contains a sin-
gle pair of avowedly abolitionist papers—a radical
position at the time—those two papers shared ar-
ticles 306 times, compared for instance to the 71
stories shared among the 6 pairs of “nativist” pa-
pers.
Table 3 shows that geographic proximity had by
far the strongest correlation with (log) reprinting
counts. Interestingly, the only political affinity to
show as strong a correlation was the Republican
party, which in this period had just been organized
and, one might suppose, was trying to control
its “message”. The Republicans were more ge-
ographically concentrated in any case, compared
to the sectionally more diffuse Democrats. An-
other counterexample is the Whigs, the party from
which the new Republican party drew many of its
members, which also has a slight negative effect
on reprinting. The only other large coefficients
are in the complete model for smaller movements
such as nativism and abolition. It is interesting to
speculate about whether the speed or faithfulness
of reprinting—as opposed to the volume—might
be correlated with more of these variables.
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.987866727272727">
We have presented techniques for detecting reused
passages embedded within the larger discourses
Figure 4: Reprints of John Brown’s 1859 speech
at his sentencing. Counties are shaded with histor-
ical population data, where available. Even taking
population differences into account, few newspa-
pers in the South printed the abolitionist’s state-
ment.
produced by actors in social networks. Some of
this shared content is as brief as partisan talking
points or lines of poetry; other reprints can en-
compass extensive legislative boilerplate or chap-
ters of novels. The longer passages are easier to
detect, with prefect pseudo-recall without exhaus-
tive scanning of the corpus. Precision-recall trade-
offs will vary with the density of text reuse and
the noise introduced by optical character recog-
nition and other features of data collection. We
then showed the feasibility of using network re-
gression to measure the correlations between con-
nections inferred from text reuse and networks de-
rived from outside information.
</bodyText>
<sectionHeader confidence="0.999549" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987854">
Royce Carroll, Jeff Lewis, James Lo, Nolan McCarty,
Keith Poole, and Howard Rosenthal. 2009. Measur-
</reference>
<page confidence="0.997703">
56
</page>
<table confidence="0.999147727272727">
newspaper pairs of regression w/pairs
affinity papers reprints &gt; 1 &gt; 10 &gt; 100
Republican 1176 134,302 0.74*** 0.73* 0.72***
Whig 1176 91,139 -0.35 -0.34 -0.35
Democrat 1081 62,609 -0.08 -0.09 -0.07
same state 672 103,057 1.12*** 1.11*** 1.13***
anti-secession 435 22,009 -0.58* -0.58 -0.60
anti-slavery 231 12,742 -0.65 -0.64 -0.60
pro-slavery 120 11,040 -0.35 -0.35 -0.27
Free-State 15 1,194 0.80 0.80
Constitutional Union 15 1,070 -0.21 -0.21
pro-secession 15 529 0.11 0.11
Free Soil 10 1,936 -0.42 -0.42
Copperhead 10 797 1.53 1.54
temperance 6 560 0.65
independent 6 186 -0.22
nativist 6 71 -1.93*
women’s rights 3 721 1.91
abolitionist 1 306 3.49**
Know-Nothing 1 25 1.33
Mormon 1 3 -1.13
R-squared – – .065 .063 .062
</table>
<tableCaption confidence="0.994883">
Table 3: Correlations between shared reprints between 19c newspapers and political and other affinities.
</tableCaption>
<reference confidence="0.977568565217392">
While many Whig papers became Republican, they do not completely overlap in our dataset; the identical
number of pairs is coincidental.
ing bias and uncertainty in DW-NOMINATE ideal
point estimates via the parametric bootstrap. Politi-
cal Analysis, 17(3).
David Dekker, David Krackhardt, and Tom Snijders.
2007. Sensitivity of MRQAP tests to collinear-
ity and autocorrelation conditions. Psychometrika,
72(4):563–581.
Tamer Elsayed, Jimmy Lin, and Douglas W. Oard.
2008. Pairwise document similarity in large collec-
tions with MapReduce. In ACL Short Papers.
Dan Gusfield. 1997. Algorithms on Strings, Trees, and
Sequences. Cambridge University Press.
Monika Henzinger. 2006. Finding near-duplicate web
pages: A large-scale evaluation of algorithms. In
SIGIR.
Russell Horton, Mark Olsen, and Glenn Roe. 2010.
Something borrowed: Sequence alignment and the
identification of similar passages in large text collec-
tions. Digital Studies / Le champ num´erique, 2(1).
Samuel Huston, Alistair Moffat, and W. Bruce Croft.
2011. Efficient indexing of repeated n-grams. In
WSDM.
Okan Kolak and Bill N. Schilit. 2008. Generating links
by mining quotations. In Hypertext.
David Krackhardt. 1987. QAP partialling as a test of
spuriousness. Social Networks, 9(2):171–186.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In KDD.
Drew Margolin, Yu-Ru Lin, and David Lazer. 2013.
Why so similar?: Identifying semantic organizing
processes in large textual corpora. SSRN.
Meredith L. McGill. 2003. American Literature and
the Culture of Reprinting, 1834–1853. U. Penn.
Press.
Jangwon Seo and W. Bruce Croft. 2008. Local text
reuse detection. In SIGIR.
Caroline Suen, Sandy Huang, Chantat Eksombatchai,
Rok Sosiˇc, and Jure Leskovec. 2013. NIFTY: A
system for large scale information flow tracking and
clustering. In WWW.
Ismet Zeki Yalniz, Ethem F. Can, and R. Manmatha.
2011. Partial duplicate detection for large book col-
lections. In CIKM.
</reference>
<page confidence="0.999145">
57
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.239539">
<title confidence="0.999983">Detecting and Evaluating Local Text Reuse in Social Networks</title>
<author confidence="0.999184">David A Abigail</author>
<affiliation confidence="0.430471666666667">NULab for Texts, Maps, and of Computer and Information Department of Department of Northeastern University, Boston, MA</affiliation>
<abstract confidence="0.994411769230769">Texts propagate among participants in many social networks and provide evidence for network structure. We describe intrinsic and extrinsic evaluations for algorithms that detect clusters of reused passages embedded within longer documents in large collections. We explore applications of these approaches to two case studies: the culture of free reprinting in the nineteenth-century United States and the use of similar language in the public statements of U.S. members of Congress.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Royce Carroll</author>
<author>Jeff Lewis</author>
<author>James Lo</author>
<author>Nolan McCarty</author>
<author>Keith Poole</author>
<author>Howard Rosenthal</author>
</authors>
<date>2009</date>
<publisher>Measur-</publisher>
<contexts>
<context position="7968" citStr="Carroll et al., 2009" startWordPosition="1233" endWordPosition="1236">cable news show websites (Meet the Press, This Week, etc.) that contain direct quotes from the Member. Since we are primarily interested in the connections between Members, we will, as we see below, want to filter out reuse among different statements by the same member. That information could be interesting for other reasons—for instance, tracking slight 51 changes in the phrasing of talking points or substantive positions. We supplemented these texts with categorical data chambers and parties and with continuous representations of ideology using the first dimension of the DW-NOMINATE scores (Carroll et al., 2009). 3 Text Reuse Detection As noted above, we are interested in detecting passages of text reuse (poems or stories; political talking points) that comprise a small fraction of the containing documents (newspaper issues; political speeches). Using the terminology of biological sequence alignment, we are interested in local alignments between documents. In text reuse detection research, two primary methods are n-gram shingling and locality-sensitive hashing (LSH) (Henzinger, 2006). The need for local alignments makes LSH less practical without performing a large number of sliding-window matches. I</context>
<context position="22492" citStr="Carroll et al., 2009" startWordPosition="3730" endWordPosition="3733">ncy matrix according to the same permutation. The permuted network then has the same structure—e.g., degree distribution—but should no longer exhibit the same correlations with the other network(s). We can run QAP to generate confidence intervals for both single (Krackhardt, 1987) and multiple correlations (Dekker et al., 2007). 5.1 Congressional Statements We model the connection between the log magnitude of reused text and the strength of ties among Members according to whether they are in the same chamber and how similar they are on the first dimension of the DW-nominate ideological scale (Carroll et al., 2009). On the left side of Table 2 are shown the results for correlating reused passages of certain minimum lengths (10, 16, 32 words) with these underlying features. On the right are shown the similar results of (Margolin et al., 2013) that simply used the exact size of the n-gram overlap between Members’ statements for increasing values of n. The alignment analysis proposed in this paper achieves similar results when passages and n-grams are short. Our analysis, however, achieves higher single and multiple correlations among networks are the passages grow longer. This is unsurprising since the pr</context>
</contexts>
<marker>Carroll, Lewis, Lo, McCarty, Poole, Rosenthal, 2009</marker>
<rawString>Royce Carroll, Jeff Lewis, James Lo, Nolan McCarty, Keith Poole, and Howard Rosenthal. 2009. Measur-</rawString>
</citation>
<citation valid="false">
<authors>
<author>While many</author>
</authors>
<title>Whig papers became Republican, they do not completely overlap in our dataset; the identical number of pairs is coincidental.</title>
<marker>many, </marker>
<rawString>While many Whig papers became Republican, they do not completely overlap in our dataset; the identical number of pairs is coincidental.</rawString>
</citation>
<citation valid="false">
<title>ing bias and uncertainty in DW-NOMINATE ideal point estimates via the parametric bootstrap.</title>
<journal>Political Analysis,</journal>
<volume>17</volume>
<issue>3</issue>
<marker></marker>
<rawString>ing bias and uncertainty in DW-NOMINATE ideal point estimates via the parametric bootstrap. Political Analysis, 17(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dekker</author>
<author>David Krackhardt</author>
<author>Tom Snijders</author>
</authors>
<title>Sensitivity of MRQAP tests to collinearity and autocorrelation conditions.</title>
<date>2007</date>
<journal>Psychometrika,</journal>
<volume>72</volume>
<issue>4</issue>
<contexts>
<context position="22200" citStr="Dekker et al., 2007" startWordPosition="3681" endWordPosition="3684">ng edges in other networks. We therefore use nonparametric permutation tests using the quadratic assignment procedure (QAP) to resample several networks with the same structure but different labels and weights. The QAP achieves this by reordering the rows and columns of one network’s adjacency matrix according to the same permutation. The permuted network then has the same structure—e.g., degree distribution—but should no longer exhibit the same correlations with the other network(s). We can run QAP to generate confidence intervals for both single (Krackhardt, 1987) and multiple correlations (Dekker et al., 2007). 5.1 Congressional Statements We model the connection between the log magnitude of reused text and the strength of ties among Members according to whether they are in the same chamber and how similar they are on the first dimension of the DW-nominate ideological scale (Carroll et al., 2009). On the left side of Table 2 are shown the results for correlating reused passages of certain minimum lengths (10, 16, 32 words) with these underlying features. On the right are shown the similar results of (Margolin et al., 2013) that simply used the exact size of the n-gram overlap between Members’ state</context>
</contexts>
<marker>Dekker, Krackhardt, Snijders, 2007</marker>
<rawString>David Dekker, David Krackhardt, and Tom Snijders. 2007. Sensitivity of MRQAP tests to collinearity and autocorrelation conditions. Psychometrika, 72(4):563–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tamer Elsayed</author>
<author>Jimmy Lin</author>
<author>Douglas W Oard</author>
</authors>
<title>Pairwise document similarity in large collections with MapReduce. In</title>
<date>2008</date>
<journal>ACL Short Papers.</journal>
<contexts>
<context position="11592" citStr="Elsayed et al., 2008" startWordPosition="1839" endWordPosition="1842">s containing each distinct n-gram, we output all pairs of documents in each list. We suppress repeated ngrams that appear in different issues of the same newspaper. These repetitions often occur in editorial boilerplate or advertisements, which, while interesting, are outside the scope of this project. We also suppress n-grams that generate more than (u ) 2 pairs, where u is a parameter.1 These frequent ngrams are likely to be common fixed phrases. Filtering terms with high document frequency has led to significant speed increases with small loss in accuracy in other document similarity work (Elsayed et al., 2008). We then sort the list of repeated ngrams by document pair, which allows us to assign a score to each pair based on the number of overlapping n-grams and the distinctiveness of those n-grams. Table 1 shows the parameters for trading off recall and precision at this stage. 3.3 Computing Local Alignments The initial pass returns a large ranked list of candidate document pairs, but it ignores the order of the n-grams as they occur in each document. We therefore employ local alignment techniques to find compact passages with the highest probability of matching. The goal of this alignment is 1The </context>
</contexts>
<marker>Elsayed, Lin, Oard, 2008</marker>
<rawString>Tamer Elsayed, Jimmy Lin, and Douglas W. Oard. 2008. Pairwise document similarity in large collections with MapReduce. In ACL Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees, and Sequences.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="12970" citStr="Gusfield, 1997" startWordPosition="2078" endWordPosition="2079">m gap of skip n-grams u maximum distinct series in the posting list Table 1: Parameters for text reuse detection to increase the precision of the detected document pairs while maintaining high recall. Due to the high rate of OCR errors, many n-grams in matching articles will contain slight differences. Unlike some partial duplicate detection techniques based on global alignment (Yalniz et al., 2011), we cannot expect all or even most of the articles in two newspaper issues, or the text in two books with a shared quotation, to align. Rather, as in some work on biological subsequence alignment (Gusfield, 1997), we are looking for regions of high overlap embedded within sequences that are otherwise unrelated. We therefore employ the Smith-Waterman dynamic programming algorithm with an affine gap penalty. This use of model-based alignment distinguishes this approach for other work, for detecting shorter quotations, that greedily expands areas of n-gram overlap (Kolak and Schilit, 2008; Horton et al., 2010). We do, however, prune the dynamic programming search by forcing the alignment to go through position pairs that contain a matching n-gram from the previous step, as long as the two n-grams are uni</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on Strings, Trees, and Sequences. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Monika Henzinger</author>
</authors>
<title>Finding near-duplicate web pages: A large-scale evaluation of algorithms.</title>
<date>2006</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="8449" citStr="Henzinger, 2006" startWordPosition="1307" endWordPosition="1309"> and parties and with continuous representations of ideology using the first dimension of the DW-NOMINATE scores (Carroll et al., 2009). 3 Text Reuse Detection As noted above, we are interested in detecting passages of text reuse (poems or stories; political talking points) that comprise a small fraction of the containing documents (newspaper issues; political speeches). Using the terminology of biological sequence alignment, we are interested in local alignments between documents. In text reuse detection research, two primary methods are n-gram shingling and locality-sensitive hashing (LSH) (Henzinger, 2006). The need for local alignments makes LSH less practical without performing a large number of sliding-window matches. In contrast to work on near-duplicate document detection and to work on “meme tracking” that takes text between quotation marks as the unit of reuse (Leskovec et al., 2009; Suen et al., 2013), here the boundaries of the reused passages are not known. Also in contrast to work on the contemporary news cycle and blogosphere, we are interested both in texts that are reprinted within a few days and after many years. We thus cannot exclude potentially matching documents for being far</context>
</contexts>
<marker>Henzinger, 2006</marker>
<rawString>Monika Henzinger. 2006. Finding near-duplicate web pages: A large-scale evaluation of algorithms. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell Horton</author>
<author>Mark Olsen</author>
<author>Glenn Roe</author>
</authors>
<title>Something borrowed: Sequence alignment and the identification of similar passages in large text collections.</title>
<date>2010</date>
<booktitle>Digital Studies / Le champ num´erique,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="13372" citStr="Horton et al., 2010" startWordPosition="2139" endWordPosition="2142">2011), we cannot expect all or even most of the articles in two newspaper issues, or the text in two books with a shared quotation, to align. Rather, as in some work on biological subsequence alignment (Gusfield, 1997), we are looking for regions of high overlap embedded within sequences that are otherwise unrelated. We therefore employ the Smith-Waterman dynamic programming algorithm with an affine gap penalty. This use of model-based alignment distinguishes this approach for other work, for detecting shorter quotations, that greedily expands areas of n-gram overlap (Kolak and Schilit, 2008; Horton et al., 2010). We do, however, prune the dynamic programming search by forcing the alignment to go through position pairs that contain a matching n-gram from the previous step, as long as the two n-grams are unique in their respective texts. Even the exact Smith-Waterman algorithm, however, is an approximation to the problem we aim to solve. If, for instance, two separate articles from one newspaper issue were reprinted in another newspaper issue in the opposite order—or separated by a long span of unrelated matter—the local alignment algorithm would simply output the better-aligned article pair and ignore</context>
</contexts>
<marker>Horton, Olsen, Roe, 2010</marker>
<rawString>Russell Horton, Mark Olsen, and Glenn Roe. 2010. Something borrowed: Sequence alignment and the identification of similar passages in large text collections. Digital Studies / Le champ num´erique, 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Huston</author>
<author>Alistair Moffat</author>
<author>W Bruce Croft</author>
</authors>
<title>Efficient indexing of repeated n-grams.</title>
<date>2011</date>
<booktitle>In WSDM.</booktitle>
<contexts>
<context position="9835" citStr="Huston et al. (2011)" startWordPosition="1534" endWordPosition="1537">(2006) notes that many of the errors in near-duplicate webpage detection arose from false matches among documents from the same website that shared boilerplate navigational elements. 3.1 Efficient N-gram Indexing The first step is to build for each n-gram feature an inverted index of the documents where it appears. As in other duplicate detection and text reuse applications, we are only interested in the n-grams shared by two or more documents. The index, therefore, does not need to contain entries for the n-grams that occur only once. We use the twopass space-efficient algorithm described in Huston et al. (2011), which, empirically, is very efficient on large collections. In a first pass, n-grams are hashed into a fixed number of bins. On the second pass, n-grams that hash to bins with one occupant can be discarded; other postings are passed through. Due to hash collisions, there may still be a small number of singleton n-grams that reach this stage. These singletons are filtered out as the index is written. In building an index of n-grams, an index of (n-1)-grams can also provide a useful filter. No 5-gram, for example, can occur twice unless its constituent 4-grams occur at least twice. We do not u</context>
</contexts>
<marker>Huston, Moffat, Croft, 2011</marker>
<rawString>Samuel Huston, Alistair Moffat, and W. Bruce Croft. 2011. Efficient indexing of repeated n-grams. In WSDM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Okan Kolak</author>
<author>Bill N Schilit</author>
</authors>
<title>Generating links by mining quotations.</title>
<date>2008</date>
<booktitle>In Hypertext.</booktitle>
<contexts>
<context position="13350" citStr="Kolak and Schilit, 2008" startWordPosition="2135" endWordPosition="2138">lignment (Yalniz et al., 2011), we cannot expect all or even most of the articles in two newspaper issues, or the text in two books with a shared quotation, to align. Rather, as in some work on biological subsequence alignment (Gusfield, 1997), we are looking for regions of high overlap embedded within sequences that are otherwise unrelated. We therefore employ the Smith-Waterman dynamic programming algorithm with an affine gap penalty. This use of model-based alignment distinguishes this approach for other work, for detecting shorter quotations, that greedily expands areas of n-gram overlap (Kolak and Schilit, 2008; Horton et al., 2010). We do, however, prune the dynamic programming search by forcing the alignment to go through position pairs that contain a matching n-gram from the previous step, as long as the two n-grams are unique in their respective texts. Even the exact Smith-Waterman algorithm, however, is an approximation to the problem we aim to solve. If, for instance, two separate articles from one newspaper issue were reprinted in another newspaper issue in the opposite order—or separated by a long span of unrelated matter—the local alignment algorithm would simply output the better-aligned a</context>
</contexts>
<marker>Kolak, Schilit, 2008</marker>
<rawString>Okan Kolak and Bill N. Schilit. 2008. Generating links by mining quotations. In Hypertext.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Krackhardt</author>
</authors>
<title>QAP partialling as a test of spuriousness.</title>
<date>1987</date>
<journal>Social Networks,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="22152" citStr="Krackhardt, 1987" startWordPosition="3676" endWordPosition="3677">ndependently to the weight of the corresponding edges in other networks. We therefore use nonparametric permutation tests using the quadratic assignment procedure (QAP) to resample several networks with the same structure but different labels and weights. The QAP achieves this by reordering the rows and columns of one network’s adjacency matrix according to the same permutation. The permuted network then has the same structure—e.g., degree distribution—but should no longer exhibit the same correlations with the other network(s). We can run QAP to generate confidence intervals for both single (Krackhardt, 1987) and multiple correlations (Dekker et al., 2007). 5.1 Congressional Statements We model the connection between the log magnitude of reused text and the strength of ties among Members according to whether they are in the same chamber and how similar they are on the first dimension of the DW-nominate ideological scale (Carroll et al., 2009). On the left side of Table 2 are shown the results for correlating reused passages of certain minimum lengths (10, 16, 32 words) with these underlying features. On the right are shown the similar results of (Margolin et al., 2013) that simply used the exact s</context>
</contexts>
<marker>Krackhardt, 1987</marker>
<rawString>David Krackhardt. 1987. QAP partialling as a test of spuriousness. Social Networks, 9(2):171–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jure Leskovec</author>
<author>Lars Backstrom</author>
<author>Jon Kleinberg</author>
</authors>
<title>Meme-tracking and the dynamics of the news cycle.</title>
<date>2009</date>
<booktitle>In KDD.</booktitle>
<contexts>
<context position="8738" citStr="Leskovec et al., 2009" startWordPosition="1352" endWordPosition="1355">rise a small fraction of the containing documents (newspaper issues; political speeches). Using the terminology of biological sequence alignment, we are interested in local alignments between documents. In text reuse detection research, two primary methods are n-gram shingling and locality-sensitive hashing (LSH) (Henzinger, 2006). The need for local alignments makes LSH less practical without performing a large number of sliding-window matches. In contrast to work on near-duplicate document detection and to work on “meme tracking” that takes text between quotation marks as the unit of reuse (Leskovec et al., 2009; Suen et al., 2013), here the boundaries of the reused passages are not known. Also in contrast to work on the contemporary news cycle and blogosphere, we are interested both in texts that are reprinted within a few days and after many years. We thus cannot exclude potentially matching documents for being far removed in time. Text reuse that occurs only among documents from the same “source” (run of newspapers; Member of Congress) should be excluded. Similarly, Henzinger (2006) notes that many of the errors in near-duplicate webpage detection arose from false matches among documents from the </context>
</contexts>
<marker>Leskovec, Backstrom, Kleinberg, 2009</marker>
<rawString>Jure Leskovec, Lars Backstrom, and Jon Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In KDD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Drew Margolin</author>
<author>Yu-Ru Lin</author>
<author>David Lazer</author>
</authors>
<title>Why so similar?: Identifying semantic organizing processes in large textual corpora.</title>
<date>2013</date>
<publisher>SSRN.</publisher>
<contexts>
<context position="6907" citStr="Margolin et al. (2013)" startWordPosition="1068" endWordPosition="1071">on line fits the raw number of issues; the red line fits counts corrected for the number of times the Associated Press is mentioned in each issue. per, or series, to use the cataloguing term. This includes material such as mastheads and manifestos and also the large number of advertisements that recur week after week in the same newspaper. 2.2 Statements by Members of Congress Members of the U.S. Congress are of course even more responsive to political debates and incentives than nineteenth-century newspapers. Representatives and senators are also a very well-studied social network. Following Margolin et al. (2013), we analyzed a dataset of more than 400,000 public statements made by members of the 112th Senate and House between January 2011 and August 2012. The statements were downloaded from the Vote Smart Project website (votesmart.com). According to Vote Smart, the Members’ public statements include any press releases, statements, newspaper articles, interviews, blog entries, newsletters, legislative committee websites, campaign websites and cable news show websites (Meet the Press, This Week, etc.) that contain direct quotes from the Member. Since we are primarily interested in the connections betw</context>
<context position="20570" citStr="Margolin et al. (2013)" startWordPosition="3418" endWordPosition="3421"> techniques useful and perform close reading and manual analysis on texts of interest, we would like to validate our results without a costly annotation campaign. In this paper, we explore the correlation of patterns of text reuse with what is already known from other 5e+06 1e+07 2e+07 5e+07 Average precision 0.0 0.1 0.2 0.3 0.4 0.5 10k 1k 2k 1k 2k 1k 1k 10k 2k 10k 1k 54 Recall Pairs examined Figure 3: (Pseudo-)Recall for aligned passages of different minimum lengths in characters. sources about the connections among Members of Congress, newspaper editors, and so on. This idea was inspired by Margolin et al. (2013), who used these techniques to test rhetorical theories of “semantic organizing processes” on the congressional statements corpus. The approach is quite simple: measure the correlation between some metric of text reuse between actors in a social network and other features of the network links between those actors. The metric of text reuse might be simply the number of exact n-grams shared by the language of two authors (Margolin et al., 2013); alternately, it might be the absolute or relative length of all the aligned passages shared by two authors or the tree distance between them in a phylog</context>
<context position="22723" citStr="Margolin et al., 2013" startWordPosition="3771" endWordPosition="3774">idence intervals for both single (Krackhardt, 1987) and multiple correlations (Dekker et al., 2007). 5.1 Congressional Statements We model the connection between the log magnitude of reused text and the strength of ties among Members according to whether they are in the same chamber and how similar they are on the first dimension of the DW-nominate ideological scale (Carroll et al., 2009). On the left side of Table 2 are shown the results for correlating reused passages of certain minimum lengths (10, 16, 32 words) with these underlying features. On the right are shown the similar results of (Margolin et al., 2013) that simply used the exact size of the n-gram overlap between Members’ statements for increasing values of n. The alignment analysis proposed in this paper achieves similar results when passages and n-grams are short. Our analysis, however, achieves higher single and multiple correlations among networks are the passages grow longer. This is unsurprising since the probability of an exact 32-gram match is much smaller than that of a 32-word-long alignment that might contain a few differences. In particular, the much higher coefficients for DW-nominate at longer aligned lengths suggests that ide</context>
</contexts>
<marker>Margolin, Lin, Lazer, 2013</marker>
<rawString>Drew Margolin, Yu-Ru Lin, and David Lazer. 2013. Why so similar?: Identifying semantic organizing processes in large textual corpora. SSRN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meredith L McGill</author>
</authors>
<date>2003</date>
<booktitle>American Literature and the Culture of Reprinting, 1834–1853. U.</booktitle>
<publisher>Penn. Press.</publisher>
<contexts>
<context position="3319" citStr="McGill (2003)" startWordPosition="506" endWordPosition="507">ary version of the text reuse detection system was presented for a single, smaller corpus in (Anonymous, 2013), but without the extrinsic or much of the intrinsic evaluation and without data on the underlying networks. 2 Case Studies in Text Reuse The case studies in this paper, which form the basis for our experimental evaluations below, involve two fairly divergent domains: the informational and literary ecology of the nineteenthcentury United States and of twenty-first century U.S. legislators. 2.1 Tracking Viral Texts in 19c Newspapers In American Literature and the Culture of Reprinting, McGill (2003) argues that American literary culture in the nineteenth century was shaped by the widespread practice of reprinting stories and poems, usually without authorial permission or even 50 Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 50–57, Baltimore, Maryland USA, 27 June 2014. c�2014 Association for Computational Linguistics knowledge, in newspapers, magazines, and books. Without substantial copyright enforcement, texts circulated promiscuously through the print market and were often revised by editors during the process. These “viral” texts—</context>
</contexts>
<marker>McGill, 2003</marker>
<rawString>Meredith L. McGill. 2003. American Literature and the Culture of Reprinting, 1834–1853. U. Penn. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jangwon Seo</author>
<author>W Bruce Croft</author>
</authors>
<title>Local text reuse detection.</title>
<date>2008</date>
<booktitle>In SIGIR.</booktitle>
<contexts>
<context position="18248" citStr="Seo and Croft (2008)" startWordPosition="3030" endWordPosition="3033">are sorted by the number of overlapping n-grams; we measure the pseudorecall at several length cutoffs. For each position in a ranked list of document pairs, we then measure the precision: what proportion of documents retrieved are in fact 50k, 20k, etc., in length? Since we wish to rank documents by the length of the aligned passages they contain, this is a reasonable metric. One summary of these various values is the average precision: the mean of the precision at every rank position that contains an actually relevant document pair. One of the few earlier evaluations of local text reuse, by Seo and Croft (2008), compared fingerprinting methods to a trigram baseline. Since their corpus contained short individual news articles, the extent of the reused passages was evaluated qualitatively rather than by alignment. Figure 2 shows the average precision of different parameter settings on the newspaper collection, ranked by the number of pairs each returns. If the pairwise document step returns a large number of pairs, we will have to perform a large number of more costly Smith-Waterman alignments. On this collection, a good tradeoff between space Pairs examined Figure 2: Average precision for aligned pas</context>
</contexts>
<marker>Seo, Croft, 2008</marker>
<rawString>Jangwon Seo and W. Bruce Croft. 2008. Local text reuse detection. In SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Suen</author>
<author>Sandy Huang</author>
<author>Chantat Eksombatchai</author>
<author>Rok Sosiˇc</author>
<author>Jure Leskovec</author>
</authors>
<title>NIFTY: A system for large scale information flow tracking and clustering.</title>
<date>2013</date>
<booktitle>In WWW.</booktitle>
<marker>Suen, Huang, Eksombatchai, Sosiˇc, Leskovec, 2013</marker>
<rawString>Caroline Suen, Sandy Huang, Chantat Eksombatchai, Rok Sosiˇc, and Jure Leskovec. 2013. NIFTY: A system for large scale information flow tracking and clustering. In WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ismet Zeki Yalniz</author>
<author>Ethem F Can</author>
<author>R Manmatha</author>
</authors>
<title>Partial duplicate detection for large book collections.</title>
<date>2011</date>
<booktitle>In CIKM.</booktitle>
<contexts>
<context position="12757" citStr="Yalniz et al., 2011" startWordPosition="2038" endWordPosition="2041">bility of matching. The goal of this alignment is 1The filter is parameterized this way because it is applied after removing document pairs in the same series. 52 n n-gram order w maximum width of skip n-grams g minimum gap of skip n-grams u maximum distinct series in the posting list Table 1: Parameters for text reuse detection to increase the precision of the detected document pairs while maintaining high recall. Due to the high rate of OCR errors, many n-grams in matching articles will contain slight differences. Unlike some partial duplicate detection techniques based on global alignment (Yalniz et al., 2011), we cannot expect all or even most of the articles in two newspaper issues, or the text in two books with a shared quotation, to align. Rather, as in some work on biological subsequence alignment (Gusfield, 1997), we are looking for regions of high overlap embedded within sequences that are otherwise unrelated. We therefore employ the Smith-Waterman dynamic programming algorithm with an affine gap penalty. This use of model-based alignment distinguishes this approach for other work, for detecting shorter quotations, that greedily expands areas of n-gram overlap (Kolak and Schilit, 2008; Horto</context>
</contexts>
<marker>Yalniz, Can, Manmatha, 2011</marker>
<rawString>Ismet Zeki Yalniz, Ethem F. Can, and R. Manmatha. 2011. Partial duplicate detection for large book collections. In CIKM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000369">
<title confidence="0.997549">
Predicting Attrition Along the Way: The UIUC Model
</title>
<author confidence="0.994357">
Bussaba Amnueypornsakul, Suma Bhat and Phakpoom Chinprutthiwong
</author>
<affiliation confidence="0.8605875">
University of Illinois,
Urbana-Champaign, USA
</affiliation>
<email confidence="0.995295">
{amnueyp1,spbhat2,chinpru2}@illinois.edu
</email>
<sectionHeader confidence="0.994709" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9993572">
Discussion forum and clickstream are two primary
data streams that enable mining of student behav-
ior in a massively open online course. A student’s
participation in the discussion forum gives direct
access to the opinions and concerns of the student.
However, the low participation (5-10%) in discus-
sion forums, prompts the modeling of user behav-
ior based on clickstream information. Here we
study a predictive model for learner attrition on a
given week using information mined just from the
clickstream. Features that are related to the quiz
attempt/submission and those that capture inter-
action with various course components are found
to be reasonable predictors of attrition in a given
week.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982275">
As an emerging area that promises new hori-
zons in the landscape resulting from the merger
of technology and pedagogy massively open on-
line courses (MOOCs) offer unprecedented av-
enues for analyzing many aspects of learning at
scales not imagine before. The concept though in
its incipient stages offers a fertile ground for an-
alyzing learner characteristics that span demogra-
phies, learning styles, and motivating factors. At
the same time, their asynchronous and impersonal
approach to learning and teaching, gives rise to
several challenges, one of which is student reten-
tion.
In the absence of a personal communication be-
tween the teacher and the student in such a sce-
nario, it becomes imperative to be able to under-
stand class dynamics based on the course logs that
are available. This serves the efforts of the in-
structor to better attend to the needs of the class
at large. One such analysis is to be able to predict
if a student will drop out or continue his/her par-
ticipation in the course which is the shared task of
the EMNLP 2014 Workshop on Modeling Large
Scale Social Interaction in Massively Open Online
Courses (Rose and Siemens, 2014).
Our approach is to model student attrition as be-
ing a function of interaction with various course
components.
</bodyText>
<sectionHeader confidence="0.99958" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.9999400625">
The task of predicting student behavior has been
the topic of several recent studies. In this context
course logs have been analyzed with an effort to
predict students? behavior. The available studies
can be classified based on the type of course data
that has been used for the analysis as those us-
ing discussion forum data and those using click-
stream data.
Studies using only discussion forum to under-
stand user-behavior rely only on available discus-
sion forum posts as their source of information. In
this context, in (Ros´e et al., 2014) it was observed
that students’ forum activity in the first week can
reasonably predict the likelihood of users drop-
ping out. Taking a sentiment analysis approach,
Wen et al. (Wen et al., 2014b) observed a corre-
lation between user sentiments expressed via fo-
rum posts and their chance of dropping out. Mo-
tivation being a crucial aspect for a successful on-
line learning experience, (Wen et al., 2014a) em-
ploys computational linguistic models to measure
learner motivation and cognitive engagement from
the text of forum posts and observe that participa-
tion in discussion forums is a strong indicator of
student commitment.
Even though discussion forum serves as a rich
source of information that offers insights into
many aspects of student behavior, it has been ob-
served that a very small percentage of students
(5-10%) actually participate in the discussion fo-
rum. As an alternate data trace of student inter-
action with the course material, the clickstream
</bodyText>
<page confidence="0.979816">
55
</page>
<bodyText confidence="0.950696571428571">
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 55–59,
October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
data of users contains a wider range of informa-
tion affording other perspectives of student behav-
ior. This is the theme of studies such as (Guo
and Reinecke, 2014), which is focused on the nav-
igation behavior of various demographic groups,
(Kizilcec et al., 2013) which seeks to understand
how students engage with the course, (Ramesh
et al., 2014), that attempts to understand student
disengagement and their learning patterns towards
minimizing dropout rate and (Stephens-Martinez
et al., 2014) which seeks to model motivations of
users by mining clickstream data.
In this study, the task is to predict if a user
will stay in the course or drop out using infor-
mation from forum posts and clickstream infor-
mation. Our approach is to use only clickstream
information and is motivated by key insights such
as interaction with the various course components
and quiz attempt/submission.
</bodyText>
<sectionHeader confidence="0.998518" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999895857142857">
Data from one MOOC with approximately 30K
students was distributed as training data. This
included discussion post information and click-
stream information of the students with com-
pletely anonymized user ids. Of this a subset of
6583 users was considered the held-out dataset on
which we report the performance of the model.
</bodyText>
<subsectionHeader confidence="0.999853">
3.1 Preprocessing Stage
</subsectionHeader>
<bodyText confidence="0.97452919047619">
Since participants (posters) in the discussion fo-
rum constitute a very small minority of the users
in a course (between 5-10% as observed in prior
studies), we mine the clickstream information for
course-interaction. From the clickstream we ex-
tract the following information to indicate involve-
ment in the course.
• Total watch time: From the video view infor-
mation the amount of time watched is calcu-
lated by taking the summation of the differ-
ence between the time of the last event a user
interacts with a video and the initial time a
user starts the same video. If a user is idle
for longer than 50 minutes, we add the differ-
ence between the current time before the user
goes idle and the time the user initially inter-
acts with the video to the total time. The new
initial time will be after the user goes active
again. Then we repeat the process until there
is no more viewing action in the clickstream
for that user.
</bodyText>
<listItem confidence="0.9981553">
• Number of quiz attempts;
• Number of quiz submissions;
• Number of times a user visits the discussion
forum;
• Number of times a user posts: The number
of times a user posts in a forum is counted.
This count includes whether the user starts a
thread, posts, or comments.
• Action sequence: We define an action se-
quence of a given user as being the sequence
</listItem>
<bodyText confidence="0.899066045454545">
of course-related activity in a given week for
a given user. It captures the user’s interac-
tion with the various components of a course
in chronological order, such as seeking infor-
mation on the course-wiki, watching a lecture
video, posting in the discussion forum. The
activities are, p = forum post, a = quiz at-
tempt, s = quiz submit, l = lecture page view,
d = lecture download, f = forum view, w =
wiki page visited, t = learning tool page vis-
ited, o = play video. As an example, the ac-
tion sequence of a user wwaaws in a given
week indicates that the user began the course-
activity with a visit to the course wiki, fol-
lowed by another visit to the wiki, then at-
tempted the quiz two successive times and fi-
nally submitted the quiz.
Each of the items listed above, captures impor-
tant aspects of interaction with the course serv-
ing as an index of attrition; the more a user inter-
acts with the course in a given week, the less the
chances are of dropping out in that week.
</bodyText>
<figureCaption confidence="0.999886">
Figure 1: Percentage of each type of users
</figureCaption>
<figure confidence="0.9961565">
Drop
Inactive
Active
47%
32%
21%
</figure>
<page confidence="0.981622">
56
</page>
<bodyText confidence="0.999713">
An exploratory analysis of the data reveals that
there are three classes of users based on their in-
teraction with the course components as revealed
by the clickstream activity. More specifically, with
respect to the length of their action sequence, the
</bodyText>
<listItem confidence="0.852867">
3 classes are:
1. Active: This class is the majority class rep-
resented by 47% of the users in the course.
The users actively interact with more than
one component of the course and their enroll-
ment status shows that they did not drop.
2. Drop: This is the class represented by a rel-
ative minority of the users (21%). The users
hardly interact with the course and from their
enrollment status they have dropped.
3. Inactive: This class of students, represented
</listItem>
<bodyText confidence="0.8553025625">
by 32% of the course, shares commonalities
with the first two classes. Whereas their en-
rollment status indicates that they have not
dropped (similar to the Active group), their
clickstream information shows that their level
of course activity is similar to that of the
Drop class (as evidenced by the length of
their action sequence. We define a user to be
inactive if the action sequence is less than 2
and the user is still enrolled in the course.
The distribution of the three classes of users in the
training data is shown in Figure 1. This key ob-
servation of the presence of three classes of users
prompts us to consider three models to predict user
attrition on any given week since we only predict
whether a user dropped or not.
</bodyText>
<listItem confidence="0.836456">
1. Mode 1 (Mod1): Inactive users are modeled
to be users that dropped because of their sim-
ilar activity pattern;
2. Mode 2 (Mod2): Inactive users are modeled
as Active users because they did not formally
drop out;
3. Mode 3 (Mod3): Inactive users are modeled
as Drop with a probability of 0.5 and Active
with a probability of 0.5. This is because they
share status attributes with Active and inter-
action attributes with Drop.
</listItem>
<sectionHeader confidence="0.999166" genericHeader="method">
4 Features
</sectionHeader>
<bodyText confidence="0.79067675">
We use two classes of features to represent user-
behavior in a course and summarize them as fol-
lows.
• Quiz related: The features in this class are:
whether a user submitted the quiz (binary),
whether a user attempted the quiz (binary),
whether a user attempted but did not submit
the quiz (binary). The intuition behind this
set of features is that in general quiz-related
activity denotes a more committed student
with a higher level of involvement with the
course. This set is also intended to capture
three levels of commitment, ranging from
only an attempt at the lowest level, attempt-
ing but not submitting at a medium level, to
submitting the quiz being the highest level.
</bodyText>
<listItem confidence="0.990481444444444">
• Activity related: The features in this category
are derived from the action sequence of the
user during that week and they are:
1. Length of the action sequence (nu-
meric);
2. The number of times each activity (p, a,
s, l, d, f, w, o, or t) occurred (numeric);
3. The number of wiki page visits/length of
the action sequence (numeric).
</listItem>
<bodyText confidence="0.999870333333333">
The features essentially capture the degree of
involvement as a whole and the extent of in-
teraction with each component.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.954822">
5.1 Models
</subsectionHeader>
<bodyText confidence="0.997333666666667">
We consider two input data distributions of the
training data: a) a specific case, where the inac-
tive users are excluded. In this case, the model is
trained only on users that are either active or those
that have dropped. b) a general case, where the
inactive users are included as is. In both cases, the
testing data has the inactive users included, but are
either modeled as Mode 1, 2 or 3. This results in
6 models {specific, general} x {Mode1, Mode2,
Mode3}.
We train an SVM for each model and ob-
serve that an rbf kernel achieves the best accuracy
among the kernel choices. We use the scikit imple-
mentation of SVM (Pedregosa et al., 2011). The
parameter γ was tuned to maximize accuracy via 5
fold cross validation on the entire training set. We
observe that the performance of Mode 3 was much
lower than that of Modes 1 and 2 and thus exclude
it from the results.
The tuned models were finally evaluated for ac-
curacy, precision, recall, F-measure and Cohen’s
</bodyText>
<page confidence="0.997433">
57
</page>
<table confidence="0.936833166666667">
K on the held-out dataset. inactive users, but operates in Mode 1 (re-
5.2 Experimental Results gards inactive users as drop).
Mode 1 Mode 2
Specific General Specific General
Baseline 46.42% 46.42% 78.66% 78.66%
Accuracy 91.31% 85.34% 78.48% 78.56%
</table>
<tableCaption confidence="0.920837">
Table 1: Accuracy of the models after parameter
tuning.
</tableCaption>
<bodyText confidence="0.999948611111111">
We compare the accuracy of the tuned mod-
els with a simple baseline which classifies a user,
who, during a given week, submits the quiz and
has an action sequence length more than 1 as
one who will not drop. The baseline accuracy is
46.42% for Mode 1 and 78.66% for Mode 2. We
observe that modeling the inactive user as one who
drops performs significantly better than the base-
line, whereas modeling the inactive user as one
who stays, does not improve the baseline. This
is summarized in Table 1.
Of these models we chose two of the best per-
forming models and evaluate them on the held-
out data. The chosen models were: Model 1 =
(specific,Mode1) and Model 2 = (general,Mode2).
The resulting tuned Model 1 (inactive = drop) had
-y = 0.1 and Model 2 (inactive = stay) had a
-y = 0.3 and C as the default value.
</bodyText>
<table confidence="0.997118">
Model 1 Model 2
Accuracy 50.98% 80.40%
Cohen’s Kappa -0.06 0.065
P 0.167 0.482
R 0.371 0.058
F 0.228 0.104
</table>
<tableCaption confidence="0.878440333333333">
Table 2: Accuracy, Cohen’s kappa, Precision (P),
Recall (R) and F-measure (F) scores for the mod-
els on the held-out data.
</tableCaption>
<bodyText confidence="0.9995773">
The performance (accuracy, Cohen’s K, preci-
sion, recall and F-measure scores of the two mod-
els on the held-out data are shown in Table 2. The
final model submitted for evaluation on the test set
is Model 2. It was more general since its training
data included the inactive users as well. However,
the skew in the data distribution is even larger for
this model.
We highlight some important observations
based on the result.
</bodyText>
<listItem confidence="0.92851">
• Model 2, which is trained to be more gen-
eral and has the inactive users included, but
operates in Mode 2 (regards inactive users
as active) has a better accuracy compared to
Model 1, which is trained by excluding the
• In terms of the K score, Model 2 shows some
agreement, but Model 1 shows no agreement.
• The increased accuracy of Model 2 comes at
the expense of reduced recall. This suggests
that Model 2 has more false negatives com-
pared to Model 1 on the held-out set.
• Even with reduced recall, Model 2 is more
</listItem>
<bodyText confidence="0.978735333333333">
precise than Model 1. This implies that
Model 1 tends to infer a larger fraction of
false positives compared to Model 2.
</bodyText>
<sectionHeader confidence="0.999944" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.998">
6.1 Data Imbalance
</subsectionHeader>
<bodyText confidence="0.9999342">
The impact of class imbalance on the SVM clas-
sifier is well-known to result in the majority class
being well represented compared to the minority
class (Longadge and Dongre, 2013). In our mod-
eling with different input data distributions as in
the specific case (Model 1), where we exclude in-
active users, the data imbalance could have signif-
icantly affected the performance. This is because,
the class of active users is more than double the
size of the class of users who dropped.
Our attempt to counter the effect of the minor-
ity class by oversampling, resulted in no improve-
ment in performance. In future explorations, other
efforts to counter the data imbalance may be help-
ful.
</bodyText>
<subsectionHeader confidence="0.996763">
6.2 Parameter tuning
</subsectionHeader>
<bodyText confidence="0.999955666666667">
The models studied here were tuned to maximize
accuracy. In the future, models that are tuned to
maximize Cohen’s K may be worth exploring.
</bodyText>
<subsectionHeader confidence="0.994691">
6.3 Ablation Analysis
</subsectionHeader>
<table confidence="0.988576">
Quiz Related Activity Related
Model 1 80.48% 50.95%
Model 2 80.48% 80.41%
</table>
<tableCaption confidence="0.8844575">
Table 3: Accuracy and kappa scores for the mod-
els by removing the corresponding set of features.
</tableCaption>
<bodyText confidence="0.975896">
Table 3 summarizes the results of the ablation
study conducted for each model by removing each
class of features. For Model 1, the activity-related
features constitute the most important set of fea-
tures as seen by the drop in accuracy resulting
from its omission. For Model 2, however, both
sets of features have nearly the same effect.
</bodyText>
<page confidence="0.997522">
58
</page>
<sectionHeader confidence="0.970553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999121090909091">
Philip J. Guo and Katharina Reinecke. 2014. Demo-
graphic differences in how students navigate through
moocs. In Proceedings of the First ACM Conference
on Learning @ Scale Conference, L@S ’14, pages
21–30, New York, NY, USA. ACM.
Ren´e F. Kizilcec, Chris Piech, and Emily Schnei-
der. 2013. Deconstructing disengagement: Analyz-
ing learner subpopulations in massive open online
courses. In Proceedings of the Third International
Conference on Learning Analytics and Knowledge,
LAK ’13, pages 170–179, New York, NY, USA.
ACM.
R. Longadge and S. Dongre. 2013. Class Imbalance
Problem in Data Mining Review. ArXiv e-prints,
May.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
Arti Ramesh, Dan Goldwasser, Bert Huang, Hal
Daume III, and Lise Getoor. 2014. Uncovering hid-
den engagement patterns for predicting learner per-
formance in moocs. In ACM Conference on Learn-
ing at Scale, Annual Conference Series. ACM, ACM
Press.
Carolyn Rose and George Siemens. 2014. Shared task
on prediction of dropout over time in massively open
online courses. In Proceedings of the 2014 Empiri-
cal Methods in Natural Language Processing Work-
shop on Modeling Large Scale Social Interaction in
Massively Open Online Courses.
Carolyn Penstein Ros´e, Ryan Carlson, Diyi Yang,
Miaomiao Wen, Lauren Resnick, Pam Goldman,
and Jennifer Sherer. 2014. Social factors that con-
tribute to attrition in moocs. In Proceedings of the
First ACM Conference on Learning @ Scale Con-
ference, L@S ’14, pages 197–198, New York, NY,
USA. ACM.
Kristin Stephens-Martinez, Marti A. Hearst, and Ar-
mando Fox. 2014. Monitoring moocs: Which infor-
mation sources do instructors value? In Proceedings
of the First ACM Conference on Learning @ Scale
Conference, L@S ’14, pages 79–88, New York, NY,
USA. ACM.
Miaomiao Wen, Diyi Yang, and Carolyn Penstein
Ros´e. 2014a. Linguistic reflections of student
engagement in massive open online courses. In
ICWSM.
Miaomiao Wen, Diyi Yang, and Carolyn Penstein
Ros´e. 2014b. Sentiment analysis in mooc discus-
sion forums: What does it tell us? In the 7th Inter-
national Conference on Educational Data Mining.
</reference>
<page confidence="0.999257">
59
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.400158">
<title confidence="0.998098">Predicting Attrition Along the Way: The UIUC Model</title>
<author confidence="0.870069">Bussaba Amnueypornsakul</author>
<author confidence="0.870069">Suma Bhat</author>
<author confidence="0.870069">Phakpoom</author>
<affiliation confidence="0.75089">University of Urbana-Champaign,</affiliation>
<abstract confidence="0.9903734375">Discussion forum and clickstream are two primary data streams that enable mining of student behavior in a massively open online course. A student’s participation in the discussion forum gives direct access to the opinions and concerns of the student. However, the low participation (5-10%) in discussion forums, prompts the modeling of user behavior based on clickstream information. Here we study a predictive model for learner attrition on a given week using information mined just from the clickstream. Features that are related to the quiz attempt/submission and those that capture interaction with various course components are found to be reasonable predictors of attrition in a given week.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip J Guo</author>
<author>Katharina Reinecke</author>
</authors>
<title>Demographic differences in how students navigate through moocs.</title>
<date>2014</date>
<booktitle>In Proceedings of the First ACM Conference on Learning @ Scale Conference, L@S ’14,</booktitle>
<pages>21--30</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4057" citStr="Guo and Reinecke, 2014" startWordPosition="650" endWordPosition="653">t offers insights into many aspects of student behavior, it has been observed that a very small percentage of students (5-10%) actually participate in the discussion forum. As an alternate data trace of student interaction with the course material, the clickstream 55 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 55–59, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics data of users contains a wider range of information affording other perspectives of student behavior. This is the theme of studies such as (Guo and Reinecke, 2014), which is focused on the navigation behavior of various demographic groups, (Kizilcec et al., 2013) which seeks to understand how students engage with the course, (Ramesh et al., 2014), that attempts to understand student disengagement and their learning patterns towards minimizing dropout rate and (Stephens-Martinez et al., 2014) which seeks to model motivations of users by mining clickstream data. In this study, the task is to predict if a user will stay in the course or drop out using information from forum posts and clickstream information. Our approach is to use only clickstream informat</context>
</contexts>
<marker>Guo, Reinecke, 2014</marker>
<rawString>Philip J. Guo and Katharina Reinecke. 2014. Demographic differences in how students navigate through moocs. In Proceedings of the First ACM Conference on Learning @ Scale Conference, L@S ’14, pages 21–30, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ren´e F Kizilcec</author>
<author>Chris Piech</author>
<author>Emily Schneider</author>
</authors>
<title>Deconstructing disengagement: Analyzing learner subpopulations in massive open online courses.</title>
<date>2013</date>
<booktitle>In Proceedings of the Third International Conference on Learning Analytics and Knowledge, LAK ’13,</booktitle>
<pages>170--179</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4157" citStr="Kizilcec et al., 2013" startWordPosition="666" endWordPosition="669">ntage of students (5-10%) actually participate in the discussion forum. As an alternate data trace of student interaction with the course material, the clickstream 55 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 55–59, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics data of users contains a wider range of information affording other perspectives of student behavior. This is the theme of studies such as (Guo and Reinecke, 2014), which is focused on the navigation behavior of various demographic groups, (Kizilcec et al., 2013) which seeks to understand how students engage with the course, (Ramesh et al., 2014), that attempts to understand student disengagement and their learning patterns towards minimizing dropout rate and (Stephens-Martinez et al., 2014) which seeks to model motivations of users by mining clickstream data. In this study, the task is to predict if a user will stay in the course or drop out using information from forum posts and clickstream information. Our approach is to use only clickstream information and is motivated by key insights such as interaction with the various course components and quiz</context>
</contexts>
<marker>Kizilcec, Piech, Schneider, 2013</marker>
<rawString>Ren´e F. Kizilcec, Chris Piech, and Emily Schneider. 2013. Deconstructing disengagement: Analyzing learner subpopulations in massive open online courses. In Proceedings of the Third International Conference on Learning Analytics and Knowledge, LAK ’13, pages 170–179, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Longadge</author>
<author>S Dongre</author>
</authors>
<title>Class Imbalance Problem in Data Mining Review. ArXiv e-prints,</title>
<date>2013</date>
<contexts>
<context position="14100" citStr="Longadge and Dongre, 2013" startWordPosition="2442" endWordPosition="2445">ms of the K score, Model 2 shows some agreement, but Model 1 shows no agreement. • The increased accuracy of Model 2 comes at the expense of reduced recall. This suggests that Model 2 has more false negatives compared to Model 1 on the held-out set. • Even with reduced recall, Model 2 is more precise than Model 1. This implies that Model 1 tends to infer a larger fraction of false positives compared to Model 2. 6 Discussion 6.1 Data Imbalance The impact of class imbalance on the SVM classifier is well-known to result in the majority class being well represented compared to the minority class (Longadge and Dongre, 2013). In our modeling with different input data distributions as in the specific case (Model 1), where we exclude inactive users, the data imbalance could have significantly affected the performance. This is because, the class of active users is more than double the size of the class of users who dropped. Our attempt to counter the effect of the minority class by oversampling, resulted in no improvement in performance. In future explorations, other efforts to counter the data imbalance may be helpful. 6.2 Parameter tuning The models studied here were tuned to maximize accuracy. In the future, mode</context>
</contexts>
<marker>Longadge, Dongre, 2013</marker>
<rawString>R. Longadge and S. Dongre. 2013. Class Imbalance Problem in Data Mining Review. ArXiv e-prints, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="11157" citStr="Pedregosa et al., 2011" startWordPosition="1911" endWordPosition="1914">ut data distributions of the training data: a) a specific case, where the inactive users are excluded. In this case, the model is trained only on users that are either active or those that have dropped. b) a general case, where the inactive users are included as is. In both cases, the testing data has the inactive users included, but are either modeled as Mode 1, 2 or 3. This results in 6 models {specific, general} x {Mode1, Mode2, Mode3}. We train an SVM for each model and observe that an rbf kernel achieves the best accuracy among the kernel choices. We use the scikit implementation of SVM (Pedregosa et al., 2011). The parameter γ was tuned to maximize accuracy via 5 fold cross validation on the entire training set. We observe that the performance of Mode 3 was much lower than that of Modes 1 and 2 and thus exclude it from the results. The tuned models were finally evaluated for accuracy, precision, recall, F-measure and Cohen’s 57 K on the held-out dataset. inactive users, but operates in Mode 1 (re5.2 Experimental Results gards inactive users as drop). Mode 1 Mode 2 Specific General Specific General Baseline 46.42% 46.42% 78.66% 78.66% Accuracy 91.31% 85.34% 78.48% 78.56% Table 1: Accuracy of the mod</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arti Ramesh</author>
<author>Dan Goldwasser</author>
<author>Bert Huang</author>
<author>Hal Daume</author>
<author>Lise Getoor</author>
</authors>
<title>Uncovering hidden engagement patterns for predicting learner performance in moocs.</title>
<date>2014</date>
<booktitle>In ACM Conference on Learning at Scale, Annual Conference Series.</booktitle>
<publisher>ACM, ACM Press.</publisher>
<contexts>
<context position="4242" citStr="Ramesh et al., 2014" startWordPosition="680" endWordPosition="683"> data trace of student interaction with the course material, the clickstream 55 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 55–59, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics data of users contains a wider range of information affording other perspectives of student behavior. This is the theme of studies such as (Guo and Reinecke, 2014), which is focused on the navigation behavior of various demographic groups, (Kizilcec et al., 2013) which seeks to understand how students engage with the course, (Ramesh et al., 2014), that attempts to understand student disengagement and their learning patterns towards minimizing dropout rate and (Stephens-Martinez et al., 2014) which seeks to model motivations of users by mining clickstream data. In this study, the task is to predict if a user will stay in the course or drop out using information from forum posts and clickstream information. Our approach is to use only clickstream information and is motivated by key insights such as interaction with the various course components and quiz attempt/submission. 3 Data Data from one MOOC with approximately 30K students was di</context>
</contexts>
<marker>Ramesh, Goldwasser, Huang, Daume, Getoor, 2014</marker>
<rawString>Arti Ramesh, Dan Goldwasser, Bert Huang, Hal Daume III, and Lise Getoor. 2014. Uncovering hidden engagement patterns for predicting learner performance in moocs. In ACM Conference on Learning at Scale, Annual Conference Series. ACM, ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn Rose</author>
<author>George Siemens</author>
</authors>
<title>Shared task on prediction of dropout over time in massively open online courses.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses.</booktitle>
<contexts>
<context position="2072" citStr="Rose and Siemens, 2014" startWordPosition="326" endWordPosition="329">eral challenges, one of which is student retention. In the absence of a personal communication between the teacher and the student in such a scenario, it becomes imperative to be able to understand class dynamics based on the course logs that are available. This serves the efforts of the instructor to better attend to the needs of the class at large. One such analysis is to be able to predict if a student will drop out or continue his/her participation in the course which is the shared task of the EMNLP 2014 Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses (Rose and Siemens, 2014). Our approach is to model student attrition as being a function of interaction with various course components. 2 Related Works The task of predicting student behavior has been the topic of several recent studies. In this context course logs have been analyzed with an effort to predict students? behavior. The available studies can be classified based on the type of course data that has been used for the analysis as those using discussion forum data and those using clickstream data. Studies using only discussion forum to understand user-behavior rely only on available discussion forum posts as </context>
</contexts>
<marker>Rose, Siemens, 2014</marker>
<rawString>Carolyn Rose and George Siemens. 2014. Shared task on prediction of dropout over time in massively open online courses. In Proceedings of the 2014 Empirical Methods in Natural Language Processing Workshop on Modeling Large Scale Social Interaction in Massively Open Online Courses.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn Penstein Ros´e</author>
<author>Ryan Carlson</author>
<author>Diyi Yang</author>
<author>Miaomiao Wen</author>
<author>Lauren Resnick</author>
<author>Pam Goldman</author>
<author>Jennifer Sherer</author>
</authors>
<title>Social factors that contribute to attrition in moocs.</title>
<date>2014</date>
<booktitle>In Proceedings of the First ACM Conference on Learning @ Scale Conference, L@S ’14,</booktitle>
<pages>197--198</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<marker>Ros´e, Carlson, Yang, Wen, Resnick, Goldman, Sherer, 2014</marker>
<rawString>Carolyn Penstein Ros´e, Ryan Carlson, Diyi Yang, Miaomiao Wen, Lauren Resnick, Pam Goldman, and Jennifer Sherer. 2014. Social factors that contribute to attrition in moocs. In Proceedings of the First ACM Conference on Learning @ Scale Conference, L@S ’14, pages 197–198, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristin Stephens-Martinez</author>
<author>Marti A Hearst</author>
<author>Armando Fox</author>
</authors>
<title>Monitoring moocs: Which information sources do instructors value?</title>
<date>2014</date>
<booktitle>In Proceedings of the First ACM Conference on Learning @ Scale Conference, L@S ’14,</booktitle>
<pages>79--88</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="4390" citStr="Stephens-Martinez et al., 2014" startWordPosition="699" endWordPosition="702">n Natural Language Processing (EMNLP), pages 55–59, October 25-29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics data of users contains a wider range of information affording other perspectives of student behavior. This is the theme of studies such as (Guo and Reinecke, 2014), which is focused on the navigation behavior of various demographic groups, (Kizilcec et al., 2013) which seeks to understand how students engage with the course, (Ramesh et al., 2014), that attempts to understand student disengagement and their learning patterns towards minimizing dropout rate and (Stephens-Martinez et al., 2014) which seeks to model motivations of users by mining clickstream data. In this study, the task is to predict if a user will stay in the course or drop out using information from forum posts and clickstream information. Our approach is to use only clickstream information and is motivated by key insights such as interaction with the various course components and quiz attempt/submission. 3 Data Data from one MOOC with approximately 30K students was distributed as training data. This included discussion post information and clickstream information of the students with completely anonymized user id</context>
</contexts>
<marker>Stephens-Martinez, Hearst, Fox, 2014</marker>
<rawString>Kristin Stephens-Martinez, Marti A. Hearst, and Armando Fox. 2014. Monitoring moocs: Which information sources do instructors value? In Proceedings of the First ACM Conference on Learning @ Scale Conference, L@S ’14, pages 79–88, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miaomiao Wen</author>
<author>Diyi Yang</author>
<author>Carolyn Penstein Ros´e</author>
</authors>
<title>Linguistic reflections of student engagement in massive open online courses.</title>
<date>2014</date>
<booktitle>In ICWSM.</booktitle>
<marker>Wen, Yang, Ros´e, 2014</marker>
<rawString>Miaomiao Wen, Diyi Yang, and Carolyn Penstein Ros´e. 2014a. Linguistic reflections of student engagement in massive open online courses. In ICWSM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miaomiao Wen</author>
<author>Diyi Yang</author>
<author>Carolyn Penstein Ros´e</author>
</authors>
<title>Sentiment analysis in mooc discussion forums: What does it tell us?</title>
<date>2014</date>
<booktitle>In the 7th International Conference on Educational Data Mining.</booktitle>
<marker>Wen, Yang, Ros´e, 2014</marker>
<rawString>Miaomiao Wen, Diyi Yang, and Carolyn Penstein Ros´e. 2014b. Sentiment analysis in mooc discussion forums: What does it tell us? In the 7th International Conference on Educational Data Mining.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
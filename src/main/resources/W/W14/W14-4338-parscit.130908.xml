<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000103">
<title confidence="0.998208">
Optimizing Generative Dialog State Tracker
via Cascading Gradient Descent
</title>
<author confidence="0.997223">
Byung-Jun Lee1, Woosang Lim1, Daejoong Kim2, Kee-Eung Kim1
</author>
<affiliation confidence="0.9868">
1 Department of Computer Science, KAIST 2 LG Electronics
</affiliation>
<sectionHeader confidence="0.980073" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866888888889">
For robust spoken dialog management,
various dialog state tracking methods have
been proposed. Although discriminative
models are gaining popularity due to their
superior performance, generative models
based on the Partially Observable Markov
Decision Process model still remain at-
tractive since they provide an integrated
framework for dialog state tracking and
dialog policy optimization. Although a
straightforward way to fit a generative
model is to independently train the com-
ponent probability models, we present a
gradient descent algorithm that simultane-
ously train all the component models. We
show that the resulting tracker performs
competitively with other top-performing
trackers that participated in DSTC2.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999608">
Spoken dialog systems, a field rapidly growing
with the spread of smart mobile devices, has to
deal with challenges to become a primary user in-
terface for natural interaction using conversations.
One of the challenges is to maintain the state of
the dialog in the conversational process, which is
called dialog state tracking. The dialog state en-
capsulates the information needed to successfully
finish the dialog, such as users’ goal or requests,
and thus it is an essential entity in spoken dia-
log systems. However, the error incurred by Au-
tomatic Speech Recognition (ASR) and Spoken
Language Understanding (SLU) makes the true
user utterance not directly observable, and this
makes it difficult to figure out the true dialog state.
Various methods have been used to construct
dialog state trackers. The traditional methods
used in most commercial systems use hand-crafted
rules that typically rely on the most likely result
from SLU. However, these rule-based systems are
prone to frequent errors as the most likely result
is not always correct. Hence, these systems of-
ten drive the users to respond using simple key-
words and to explicitly confirm everything they
say, which is far from a natural conversational in-
teraction. An accurate tracking of the dialog state
is crucial for natural and efficient dialogs. On the
other hand, modern methods take a statistical ap-
proach to calculate the posterior distribution over
the dialog states using multiple results from SLU
in order to overcome the error in the most likely
SLU result.
Statistical dialog state trackers can be catego-
rized into two approaches depending on how the
posterior calculation is modeled. The generative
approach uses the generative model that describes
how the SLU results are generated from the hidden
dialog state and uses the Bayes’ rule to calculate
the posterior. It has been a popular approach for
statistical dialog state tracking, since it naturally
fits into the Partially Observable Markov Decision
Process (POMDP) (Williams and Young, 2007),
an integrated model for dialog state tracking and
dialog strategy optimization. In the POMDP point
of view, the dialog state tracking is essentially be-
lief monitoring, which is the task of calculating
posterior distribution over the hidden state given
the history of observations. Examples of the dia-
log state trackers that take the generative approach
include (Young et al., 2010; Thomson and Young,
2010; Raux and Ma, 2011)
On the other hand, the discriminative approach
directly models the posterior distribution. Since
it avoids modeling of unnecessary aspects of the
task, it typically achieves a better tracking accu-
racy compared to the generative approach. Ex-
amples of discriminative dialog state trackers in-
clude (Lee, 2013; Metallinou et al., 2013). How-
ever, their feature functions often refer to past ob-
servations, and it remains yet to be seen whether
</bodyText>
<page confidence="0.98097">
273
</page>
<note confidence="0.7302695">
Proceedings of the SIGDIAL 2014 Conference, pages 273–281,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999958307692308">
the discriminative approach can be successfully
incorporated into POMDP or reinforcement learn-
ing (RL) for dialog strategy optimization.
This paper is concerned with the generative ap-
proach to dialog state tracking. In our earlier
work (Kim et al., 2013), the optimization of the
tracker was carried out independently for each
component model (observation model, user action
model, and belief refinement model) that com-
prised our tracker. This was not exactly a proper
way to train the tracker for overall performance
optimization. In this paper, we present an opti-
mization method, which we call “cascading gra-
dient descent”, that trains component models si-
multaneously. We show that this approach yields
a dialog state tracker that performs on par with the
best ones that participated in the second Dialog
State Tracking Challenge (DSTC2).
The rest of the paper is organized as follows:
We briefly review the background of our work in
section 2, and present our method in section 3. We
then explain the DSTC2 dialog domain and the ex-
perimental settings in section 4, and discuss the re-
sults in section 5. Finally, we conclude the paper
with the summary and the suggestion for future
work in section 6.
</bodyText>
<sectionHeader confidence="0.861765" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999957">
The dialog state tracking is formalized as fol-
lows: In each turn of the dialog, the spoken dia-
log system executes system action a, and the user
with goal g responds to the system with utterance
u. The dialog state in each turn is defined s =
(u, g, h), where h is the dialog history encapsulat-
ing additional information needed for tracking the
dialog state (Williams et al., 2005). The SLU pro-
cesses the user utterance and generates the results
as an N-best list o = [(˜u1, f1), ... , (˜uN, fN)],
where ˜ui is the hypothesized user utterance and
fi is its confidence score1. Without loss of gener-
ality, we assume that the last item in the N-best
list is the null item (0,1− EN−1 i=1fi), representing
the set of unrecognized user utterances. The statis-
tical dialog state tracker maintains the probability
distribution over states, called the belief.
</bodyText>
<subsectionHeader confidence="0.956768">
2.1 Discriminative Dialog State Tracking
</subsectionHeader>
<bodyText confidence="0.9763925">
Dialog state trackers taking the discriminative ap-
proach calculates the belief via trained conditional
</bodyText>
<footnote confidence="0.937503">
1Here we assume that �N−&apos;
</footnote>
<bodyText confidence="0.461399">
i=&apos; fi ≤ 1, acting as a posterior
</bodyText>
<subsectionHeader confidence="0.532338">
of N-best list.
</subsectionHeader>
<bodyText confidence="0.999859">
models that represent the belief directly. Maxi-
mum Entropy is widely used for the discriminative
approach, which formulates the belief as follows:
</bodyText>
<equation confidence="0.999662">
b0(g) = P(g|x) = η · exp(wT φ(x)) (1)
</equation>
<bodyText confidence="0.999222333333333">
where η is the normalizing constant, x =
(u1, a1, g1, . . . , ut, at, gt) is the history of user ac-
tions, system actions, and user goals up to the cur-
rent dialog turn t, φ(·) is the vector of feature
functions on x, and finally, w is the set of model
parameters to be learned from dialog data.
According to the formulation, the posterior
computation has to be carried out for all possible
user goals in order to obtain the normalizing con-
stant η. This is not feasible for real dialog domains
that have a large number of user goals (the DSTC2
dialog domain used on this paper has 371070 user
goals).
Consequently, it is important for the discrimina-
tive approach to reduce the size of the state space.
(Metallinou et al., 2013) adopts the idea behind the
HIS model and confines the set of possible goals
to those appeared in SLU results. (Lee, 2013) as-
sumed conditional independence between dialog
state components to address scalability, and used
conditional random field.
</bodyText>
<subsectionHeader confidence="0.987905">
2.2 Generative Dialog State Tracking
</subsectionHeader>
<bodyText confidence="0.999721285714286">
In contrast, the generative approach to the dialog
state tracking calculates the belief using Bayes’
rule, with the belief from the last turn b as a
prior and the likelihood given the user utter-
ance hypotheses Pr(o|a, g, h). In the prior work
(Williams et al., 2005), the likelihood is factored
and some independence assumptions are made:
</bodyText>
<equation confidence="0.9934448">
b0(g0, h0) = η � Pr(o|u) Pr(u|g0, a) ·
u
� �Pr(h0|g0, u, h, a) Pr(g0|g, a)b(g, h)
h g
(2)
</equation>
<bodyText confidence="0.9995436">
where η is the normalizing constant and u is
marginalized out in the belief.
Scalability became the important issue, just as
in the generative approach. One way to reduce
the amount of computation is to group the states
into partitions, proposed as the Hidden Informa-
tion State (HIS) model (Young et al., 2010). Be-
ginning with one root partition with the probabil-
ity of 1, partitions are split when the distinction
is required by observations, i.e. a user utterance
</bodyText>
<page confidence="0.99592">
274
</page>
<bodyText confidence="0.99981996">
hypothesis from SLU. This confines the possible
goal state to the values that have been appeared as
SLU hypotheses, and provides scalability without
a loss of accuracy when the coverage of N-best list
is large enough to include the true utterance. Us-
ing the HIS model with an additional assumption
that the user goal does not change (goal transition
from one to another is 0), the belief update equa-
tion (2) is reformulated as follows:
where ψ is a set of user goals that share the same
belief. Each probability model in the above equa-
tion has a name: Pr(o|u) is called the observation
model, Pr(u|ψ0, a) is called the user action model,
Pr(ψ0|ψ) is called the belief refinement model, and
Pr(h0|ψ0, u, h, a) is called the history model.
In this paper, we used the last turn’s belief of di-
alog states as history state and preserved its depen-
dence in the observation model to improve perfor-
mance. With the changes, observation model can
distinguish user actions without their value. For
example, request alternative user action may have
the power of diminishing dominant partitions, and
it can only be learnt by the dependence with par-
tition confidence. The belief update formula used
in this paper becomes:
</bodyText>
<equation confidence="0.998575">
b0(ψ0) = η X Pr(o|u, a, h) ·
u (4)
Pr(u|ψ0, a) Pr(ψ0|ψ)b(ψ)
</equation>
<bodyText confidence="0.99985325">
Other approaches to cope with the scalabil-
ity problem in dialog state tracking is to adopt
factorized dynamic Bayesian network by making
conditional independence assumptions among di-
alog state components, and use approximate in-
ference algorithms such as loopy belief propa-
gation (Thomson and Young, 2010) or blocked
Gibbs sampling (Raux and Ma, 2011).
</bodyText>
<sectionHeader confidence="0.986816" genericHeader="method">
3 Cascading Gradient Descent
</sectionHeader>
<bodyText confidence="0.999898136363636">
Although equation (4) is an elegant formulation
of the dialog state tracking via Bayes rule, there
has not been an integrated learning algorithm that
simultaneously optimizes component probability
models, i.e. the observation, the user action, and
the belief refinement models. Our prior work (Kim
et al., 2013) relied on independently training each
component probability model, and then simply
plugging them into (4). Since the independent op-
timization of component probability models does
not lend itself to the optimization of overall dia-
log state tracking performance, we added an ex-
tra post-processing step called “belief transforma-
tion” in order to fine tune the results obtained
from equation (4). Unfortunately, this effort gen-
erally resulted in overfitting to the training data.
In this paper, we present an integrated learning al-
gorithm that simultaneously optimizes the compo-
nent probability models of the HIS model.
We start with defining an objective function
which measures the error of the dialog state track-
ing:
</bodyText>
<equation confidence="0.903729">
2(b(ψt
1 i) − ri
t)2 (5)
</equation>
<bodyText confidence="0.999979125">
where t is the dialog turn, i is the partition index,
rti is the binary label with value 1 if and only if
the partition ψt i contains the true user goal. Note
that our objective function coincides with the `2
performance metrics used in DSTC.
We then express component probability models
as functions of features, which are parameterized
by sets of weights, and rewrite equation (4):
</bodyText>
<equation confidence="0.9849354">
b(ψt i) =ηt X Pr,,,O(ut, ft, at, b(ψt−1
i )) ·
(ut,ft)∈ot
Pr,,,U(ut|ψti,at)Pr,,,R(ψti |t−1i)b( Z−1)
(6)
</equation>
<bodyText confidence="0.999938333333333">
where wO, wU, and wR represent the set of pa-
rameters for the observation, the user action, and
the belief refinement models, respectively.
Our learning method is basically a gradient de-
scent. The gradient of E with respect to wO is
derived as follows:
</bodyText>
<equation confidence="0.993653258064516">
i)
(b(ψt i) − rt i)∂b(ψt
∂wO
By convenience, we define:
Pr(o|u) Pr(u|ψ0, a) ·
X Pr(h0|ψ0, u, h, a) Pr(ψ0|ψ)b(ψ, h) (3)
h
X
b0(ψ0, h0) = η
u
XT
t=1
E=
X
i
∂E
XT
t=1
=
∂wO
X
i
X Pr,,,O(ut, ft, at, b(ψt−1
δt = i )) ·
i
(ut,ft)∈ot
Pr,,,U(ut|ψti, at) Pr,,,R (ψti  |ψZ −1) b (ψt−1i)
X= ptOptUptRb(ψt−1
(ut,ft)∈ot i )
Xηt = ( δti)−1, b(ψti) = ηtδti
i
</equation>
<page confidence="0.986326">
275
</page>
<bodyText confidence="0.943717">
and then obtain:
</bodyText>
<equation confidence="0.97709375">
∂b(ψt−1 X
i )
∂wO
(ut,ft)∈ot
</equation>
<bodyText confidence="0.9880945">
Gradients for the parameters of other com-
ponent probability models are derived similarly.
We call our algorithm cascading gradient descent
since the gradient ∂b(ψt
∂w requires computation of
i)
the gradient in the previous dialog turn ∂b(ψi−1)
∂w ,
hence reflecting the temporal impact of the param-
eter change in throughout the dialog turns.
Once we obtain the gradients, we update the pa-
rameters using the gradient descent
</bodyText>
<equation confidence="0.98153">
I∂E �
,
∂wO
</equation>
<bodyText confidence="0.999919">
where α is the stepsize parameter. α is initially set
to 0.5 and decreased by multiplying 110 whenever
the overall cost function increases.
</bodyText>
<sectionHeader confidence="0.9977525" genericHeader="method">
4 Dialog State Tracking in the
Restaurant Information Domain
</sectionHeader>
<bodyText confidence="0.999810714285714">
This section describes the dialog domain used for
the evaluation of our dialog tracker and the com-
ponent probability models used for the domain.
An instruction on how to obtain the dataset and
a more detailed description on the dialog domain
can be found in the DSTC2 summary paper (Hen-
derson et al., 2014).
</bodyText>
<subsectionHeader confidence="0.989902">
4.1 Task Description
</subsectionHeader>
<bodyText confidence="0.999886217391304">
We used the DSTC2 dialog domain in which
the user queries the database of local restaurants.
The dataset for the restaurant information domain
were originally collected using Amazon Mechani-
cal Turk. A usual dialog proceeds as follows: the
user specifies the constraints (e.g. type of food,
location, etc) or the name of restaurant he wants,
and the system offers the name of a restaurant that
qualifies the constraints. User then accepts the of-
fer, and requests for additional information about
accepted restaurant. The dialog ends when all the
information requested by the user is provided.
The dialog state tracker should thereby clar-
ify three types of information inside the state:
goal, method, and requested. The goal state is
composed of name, pricerange, area, and food
slots, which is the information of the constraints
that the user has. The method state represents
what method user is using to accomplish his goal,
whose value is one of the none, by constraints, by
alternatives, by name, or finished. Lastly, the re-
quested state represents the information currently
requested by the user, such as the address, phone
number, postal code, etc. In this paper, we restrict
ourselves to tracking the goal states only, but our
tracker can be easily extended to track others as
well.
The dialog state tracker updates the belief turn
by turn, receiving SLU N-best hypotheses each
with an SLU confidence score in every turn. De-
spite the large number of states a dialog can have,
in the most cases, the coverage of N-best hypothe-
ses is enough to limit the consideration of possible
goal state to values that has been observed in SLU
hypotheses. Consequently, the task of the dialog
state tracker is to generate a set of observed val-
ues and their confidence scores for each slot, with
the confidence score corresponding to the poste-
rior probability of the goal state being the true goal
state. The dialog state tracker also maintains a spe-
cial goal state, called None, which represents that
the true goal state has not been observed. Its poste-
rior probability is also computed together with the
observed goal states as a part of the belief update.
For the rest of this section, we describe the models
chosen for each component probabilities.
</bodyText>
<subsectionHeader confidence="0.883417">
4.2 Observation Model
</subsectionHeader>
<bodyText confidence="0.999912">
The observation model that describes the genera-
tion of SLU result for the user utterance is defined
as
</bodyText>
<equation confidence="0.985566064516129">
Pr(o = hut, ft)|u, a, h) =
ηo PrwO(ut, ft, at, b(ψt−1
i ))
1
1 + exp(−wTOφO(ut, ft, at, b(ψt−1
i )) − bO)
I∂∂ u
� ∂E�
w0 R = wR − α
∂wR
w0U = wU − α
∂δt · ∂η t
∂wO ∂wO ηt + ∂wO δti
∂b(ψt i)
where =(b(ψi−1) ∂ptO ptUptR
∂δti ∂wO
(ut,ft)∈ot
∂wO
∂δt X
i ·
ηt − b(ψt i)
∂wO
∂δti, ·t
∂wO η
if
+
�
pt Opt Upt .
R
w0O = wO − α
= ηo
</equation>
<page confidence="0.996621">
276
</page>
<table confidence="0.998677347826087">
user action feature : 34 × system action feature : 5 × type of feature : 3 = 510
Inform action: 12 × ×
[food, pricerange, name, area] offer or Bias tern
× inform (always 1)
[not match, slot match, value match]
% consistency check with system action
canthelp or
canthelp.exception
Action with values : 8
Value of user confidence
ft
[confirm, deny] expl-conf or
× impl-conf or
[food, pricerange, name, area] request
Action without values : 14
select Value of last turn’s confidence
b(ψt−1
i )
[ack, affirm, bye, hello, negate,
repeat, reqmore, reqalts, thankyou,
request, null, confirm, deny, inform]
confirm-domain or
welcomemsg
</table>
<tableCaption confidence="0.999757">
Table 1: 510 features used in observation model are specified.
</tableCaption>
<bodyText confidence="0.9954928">
where φO(ut, ft, at, b(Ot−1
i )) is the vector of fea-
tures taken from the hypothesized user action ut,
its confidence score ft generated from SLU, sys-
tem action at, and the belief of partition we are
dealing with b(Ot−1
i ) from history state. Normal-
ization constante ηo can be ignored since it is sub-
sumed by overall normalization constant η. Fea-
ture details are specified in table 1.
</bodyText>
<subsectionHeader confidence="0.9899">
4.3 User Action Model
</subsectionHeader>
<bodyText confidence="0.999925333333333">
Similar to the observation model, the user action
model that predicts the user action given the pre-
vious system action and user goal is defined as
</bodyText>
<equation confidence="0.980248333333333">
Pr(ut|Oti, at) = PrwU(ut|Ot i,at)
exp(wTUφU(ut, Ot i, at))
Eu exp(wTUφU(u, Oti, at))
</equation>
<bodyText confidence="0.999908166666667">
where φU(ut, Oti, at) ∈ {0,1}322 is the vector of
features taken from the (hypothesized) user action
ut, system action at, and the partition being up-
datedOti. Softmax function is used to normal-
ize over possible user actions. Feature details are
specified in table 2.
</bodyText>
<subsectionHeader confidence="0.987889">
4.4 Belief Refinement Model
</subsectionHeader>
<bodyText confidence="0.964165">
The belief refinement model predicts how the par-
tition of the user goal will evolve at the next dialog
turn. We defined it as a mixture of the empirical
distribution and the uniform distribution obtained
from the training data:
</bodyText>
<equation confidence="0.998429">
PrwR(Ot i|Ot−1
i )
1 occurrence(Of,Ot-1)
Z i
1 + exp(−wR) occurrence(Ot−1
i )
C1
+ + ex 1 − 1 p1(−wR)) |Ot−1
i |
</equation>
<bodyText confidence="0.955623956521739">
where occurrence(Oti, Ot−1
i ) is the number of
consecutive dialog turns in the training data with
user goals being consistent with Ot−1
i in the
previous turn and Ot i in the current turn, and
occurrence(Ot−1
i ) is defined similarly for a single
turn only. The ratio of the two, which corresponds
to the partition split probability used in (Young et
al., 2010), is the first term in the mixture. On the
other hand, if we use this empirical distribution
only, we cannot deal with novel user goals that do
not appear in the training data. Assuming that user
goals are generated from the uniform distribution,
the probability that the user goal is in a partition
O is |ψ|
N where |O |is the number of user goals in
the partition O, and N is the total number of user
goals. The probability that Ot i gets split from Ot−1
i
is then |ψt
i |.Hence, we mix the two probabilities
</bodyText>
<equation confidence="0.900244">
i|
|ψt−1
</equation>
<bodyText confidence="0.997514333333334">
for the resulting model.
The mixture weight is the only parameter of the
belief refinement model, which is learned as a part
of the cascading gradient descent. Note that we
use the sigmoid function in order to make the op-
timization unconstrained.
</bodyText>
<page confidence="0.988367">
277
</page>
<table confidence="0.943443102564102">
user action feature : 35 × system action feature : 8 + remaining actions: 42 = 322
Inform action : 24 Confirm/deny action: 16
[food, pricerange, name, area]
×
[not match, slot match, value match]
% consistency check
with system action
×
[not match, match]
% consistency check
with partition
Action without values : 11
[ack, affirm, bye, hello, negate,
repeat, reqalts, reqmore,
thankyou, request, null]
[offer or inform,
canthelp or
canthelp.exception,
expl-conf or
impl-conf or
request,
× select]
×
[not match, match]
% consistency check
with partition
[confirm, deny]
×
[food, pricerange, name, area]
×
[not match, match]
% consistency check
with partition
+
Remaining system action: 26
[confirm-domain or welcomemsg]
×
[24 inform actions, null, others]
% corresponding user actions
</table>
<tableCaption confidence="0.995657">
Table 2: 322 features used in user action model are specified.
</tableCaption>
<sectionHeader confidence="0.992218" genericHeader="method">
5 Experimental Details
</sectionHeader>
<subsectionHeader confidence="0.939785">
5.1 Datasets
</subsectionHeader>
<bodyText confidence="0.940349888888889">
The restaurant information domain used in
DSTC2 is arranged into three datasets: train, dev,
test. The first two datasets are labeled with the true
user goals and user actions to optimize the dialog
state tracker before submission. The half of the di-
alogs are created with artificially degraded speech
recognizers, intended to better distinguish the per-
formances of trackers. Details of each dataset are
as below:
</bodyText>
<listItem confidence="0.99342">
• dstc2 train: Composed of 1612 dialogs of
11405 turns, produced from two different
dialog managers with a hand-crafted dialog
policy.
• dstc2 dev: Composed of 506 dialogs of
3836 turns, produced from the dialog man-
agers used in dstc2 train set. Most of dialog
state trackers show lower performance on this
dataset than others.
• dstc2 test: Composed of 1117 dialogs of
9689 turns, produced from the dialog policy
trained by reinforcement learning, which is
not used for the train and dev datasets.
</listItem>
<bodyText confidence="0.9998228">
We used both train and dev sets as the training
data, as if they were one big dataset. Although
the true labels for the test dataset were made pub-
lic after the challenge, we did not use these labels
in any way for optimizing our tracker.
</bodyText>
<subsectionHeader confidence="0.999837">
5.2 Pre-training
</subsectionHeader>
<bodyText confidence="0.949376388888889">
One of the drawbacks in using gradient descent
is convergence to a local optimum. We also ob-
served this phenomena during the training of our
dialog state tracker via cascading gradient descent.
Randomized initialization of parameters is a com-
mon practice for gradient descent, but given the
high-dimensionality of the parameter space, the
randomized initialization had a limited effect in
converging to a sufficiently good local optimum.
We adopted a pre-training phase where the pa-
rameters of each component model are optimized
individually. Once the pre-training is done for
each component model, we gathered the param-
eter values and took them as the initial parameter
value for the cascading gradient descent. This pre-
training phase helped tremendously converging to
a good local optimum, and reduced the number of
iterations as well. We pre-trained the parameters
of each component model as follows:
• Observation Model: True user action labels
in the training set are used as targets for the
observation model. For every user action hy-
pothesis in the N-best list, set the target value
to 1 if the user action hypothesis is the true
user action, and 0 otherwise. A simple gradi-
ent descent was used for pre-training.
• User Action Model: Although the user ac-
tion and the system action labels are avail-
able, the partition of the user goals is not
readily available. However, the latter can be
easily obtained by running an unoptimized
tracker. Thus, using the labels in the train-
ing set and the generated partitions, we set
the target value to 1 if the user action hypoth-
esis is the true user action and the partition is
consistent with the true user action, and 0 oth-
</bodyText>
<page confidence="0.983905">
278
</page>
<figure confidence="0.99932">
(a) Evaluation on accuracy metric (higher is better)
(b) Evaluation on L2 metric (lower is better) (c) Evaluation on ROCV 2,ca05 metric (higher is better)
</figure>
<figureCaption confidence="0.9927455">
Figure 1: The overall results of proposed method. Each figure shows the evaluations over dstc2 test
dataset by featured metrics (joint accuracy, joint l2, joint roc.v2) in DSTC2.
</figureCaption>
<bodyText confidence="0.5424355">
erwise. A simple gradient descent was also
used for pre-training.
</bodyText>
<listItem confidence="0.984050333333333">
• Belief Refinement Model: Since there is
only a single parameter for this model, we did
not perform pre-training.
</listItem>
<subsectionHeader confidence="0.757947">
5.3 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.99998965">
Table 3 shows the test set score of tracker im-
plemented based on proposed algorithm, with the
score of other trackers submitted to DSTC2. We
tried 200 random initialised weights to train model
with proposed algorithm, and learned model with
the lowest training L2 error is picked to show
the result on the test set. Because we only used
live SLU and past data to track dialog state, other
tracker results with the same condition are selected
to compare with our tracker.
The implementation of our algorithm was not
ready until the DSTC2 deadline. We participated
as Team 8 using the old optimization method
in (Kim et al., 2013). As shown in the table 3,
the new algorithm shows a substantial improve-
ment, achieving almost 15% decrease in the L2
error. Since both trackers are fine-tuned, this im-
provement seems to be originated from the new
optimization algorithm.
For all three featured metrics used to evalu-
ate, tracker constructed with our proposed method
shows competitive performance. The key to excel
baseline tracker was to discover the relation be-
tween user action and system action. For exam-
ple, user actions that tell about the same slot sys-
tem was talking about but giving different value
are usually correcting wrong recognitions so far,
which should significantly reduce the belief over
state the system was tracking.
Due to the objective function that is designed to
optimize L2 error, our tracker shows better perfor-
mance at L2 error than the other metrics. For both
all goal metric and joint goal metric, our tracker
shows low L2 error when compared to other track-
ers while the rank of accuracy metric is not so
high. When the fact that our method as a genera-
tive state tracker benefits from the ability to be eas-
ily incorporated into POMDP framework is con-
sidered, only similar performance to other trackers
is satisfactory.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.957844666666667">
In this paper, we propose a simple method that
optimizes overall parameters of generative state
tracker using ”Cascading Gradient Descent” al-
</bodyText>
<page confidence="0.993476">
279
</page>
<table confidence="0.999839083333333">
All goal
Team 0 1 3 4 6 7 8 9 Ours
Entry 1 2 0 0 3 0 1 2 4 1 0
Accuracy 0.886 0.88 0.837 0.892 0.895 0.884 0.882 0.885 0.894 0.873 0.77 0.886
AvgP 0.865 0.852 0.778 0.856 0.853 0.789 0.833 0.843 0.862 0.827 0.782 0.846
L2 0.192 0.198 0.289 0.189 0.17 0.197 0.189 0.184 0.179 0.227 0.358 0.186
MRR 0.918 0.914 0.87 0.911 0.927 0.917 0.916 0.918 0.922 0.904 0.833 0.918
ROCV 1,ca05 0.777 0.767 0.0 0.778 0.842 0.773 0.786 0.809 0.806 0.635 0.0 0.805
ROCV 1,eer 0.139 0.133 0.0 0.119 0.103 0.135 0.123 0.116 0.116 0.163 0.219 0.120
ROCV 2,ca05 0.0 0.0 0.0 0.0 0.3 0.27 0.417 0.384 0.154 0.0 0.0 0.197
UpdateAcc 0.886 0.881 0.837 0.891 0.895 0.882 0.88 0.883 0.894 0.873 0.769 0.886
UpdatePrec 0.898 0.897 0.846 0.904 0.907 0.898 0.895 0.897 0.903 0.886 0.804 0.896
</table>
<tableCaption confidence="0.9890595">
Table 3: Test set scores averaged over all goal slots of our proposed algorithm and other trackers are
presented. The goal slots are composed of food, pricerange, name and area.
</tableCaption>
<table confidence="0.9995375">
Joint goal
Team 0 1 3 4 6 7 8 9 Ours
Entry 1 2 0 0 3 0 1 2 4 1 0
Accuracy 0.719 0.711 0.601 0.729 0.737 0.713 0.707 0.718 0.735 0.699 0.499 0.726
AvgP 0.678 0.66 0.503 0.659 0.636 0.54 0.619 0.638 0.673 0.583 0.522 0.658
L2 0.464 0.466 0.649 0.452 0.406 0.461 0.447 0.437 0.433 0.498 0.76 0.427
MRR 0.779 0.757 0.661 0.763 0.804 0.767 0.765 0.772 0.787 0.749 0.608 0.775
ROCV 1,ca05 0.332 0.316 0.096 0.32 0.461 0.324 0.395 0.432 0.349 0.22 0.0 0.438
ROCV 1,eer 0.256 0.254 0.382 0.249 0.208 0.281 0.241 0.226 0.243 0.299 0.313 0.218
ROCV 2,ca05 0.0 0.0 0.064 0.0 0.321 0.1 0.223 0.207 0.086 0.067 0.0 0.135
UpdateAcc 0.489 0.487 0.37 0.495 0.507 0.473 0.466 0.476 0.514 0.459 0.325 0.488
UpdatePrec 0.729 0.694 0.677 0.759 0.726 0.748 0.743 0.743 0.703 0.692 0.54 0.71
</table>
<tableCaption confidence="0.994344">
Table 4: Test set scores of joint goal slot of our proposed algorithm and other trackers are presented. The
</tableCaption>
<bodyText confidence="0.988146342857143">
joint goal slot is a slot that is treated as correct when every goal slot is correct.
gorithm. Using proposed method on Hidden In-
formation State model, we construct a tracker
that performs competitively with DSTC2 par-
ticipants, who mostly adopt discriminative ap-
proaches. Since generative approach has much
more potential to be extended to more com-
plex models or toward different domains such as
DSTC3, our tracker has the advantage over the
other trackers.
Hidden Information State (HIS) model with
cascading gradient descent has far more steps of
improvement remaining. Although history state
in current paper only includes previous partition
belief due to implementation convenience, utiliz-
ing additional history state is the key to improve
performance even more. History state can in-
clude any information depending on how we de-
fine the state. The reason why the discriminative
state tracking methods generally show good per-
formance in terms of accuracy is rich set of poten-
tially informative features, which can be employed
by the history state.
In addition to the future improvements with his-
tory state, we can consider improving each prob-
ability models. In this paper, probability mod-
els are modeled with sigmoid function or soft-
max function over weighted features, which is in
other words a neural network with no hidden layer.
The model used in this paper can naturally devel-
oped by adding hidden layers, and ultimately deep
learning techniques could be applicable. Apply-
ing deep learning techniques could help the his-
tory state to find out influential hidden features to
employ.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9985078">
This work was supported by the IT R&amp;D program
of MKE/KEIT. [10041678, The Original Technol-
ogy Development of Interactive Intelligent Per-
sonal Assistant Software for the Information Ser-
vice on multiple domains]
</bodyText>
<sectionHeader confidence="0.998512" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.979515">
Matthew Henderson, Blaise Thomson, and Jason
Williams. 2014. The second dialog state tracking
</reference>
<page confidence="0.952075">
280
</page>
<reference confidence="0.999742756756757">
challenge. In Proceedings of the SIGdial 2014 Con-
ference, Baltimore, U.S.A., June.
Daejoong Kim, Jaedeug Choi, Kee-Eung Kim, Jungsu
Lee, and Jinho Sohn. 2013. A specific analysis of
a dialog state tracker in a challenge. In Proceedings
of the SIGDIAL 2013 Conference, pages 462–466.
Sungjin Lee. 2013. Structured discriminative model
for dialog state tracking. Proceedings of the SIG-
DIAL 2013 Conference, pages 442–451.
Angeliki Metallinou, Dan Bohus, and Jason D
Williams. 2013. Discriminative state tracking for
spoken dialog systems. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguastics, pages 466–475.
Antoine Raux and Yi Ma. 2011. Efficient probabilistic
tracking of user goal and dialog history for spoken
dialog systems. In INTERSPEECH, pages 801–804.
Blaise Thomson and Steve Young. 2010. Bayesian
update of dialogue state: A pomdp framework for
spoken dialogue systems. Computer Speech &amp; Lan-
guage, 24(4):562–588.
Jason D Williams and Steve Young. 2007. Partially
observable markov decision processes for spoken
dialog systems. Computer Speech &amp; Language,
21(2):393–422.
Jason D Williams, Pascal Poupart, and Steve Young.
2005. Factored partially observable markov deci-
sion processes for dialogue management. In 4th
Workshop on Knowledge and Reasoning in Practi-
cal Dialog Systems, International Joint Conference
on Artificial Intelligence (IJCAI), pages 76–82.
Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois
Mairesse, Jost Schatzmann, Blaise Thomson, and
Kai Yu. 2010. The hidden information state model:
A practical framework for pomdp-based spoken dia-
logue management. Computer Speech &amp; Language,
24(2):150–174.
</reference>
<page confidence="0.997911">
281
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.234414">
<title confidence="0.6375045">Optimizing Generative Dialog State via Cascading Gradient Descent Woosang Daejoong Kee-Eung of Computer Science, KAIST 2LG Electronics</title>
<abstract confidence="0.998278105263158">For robust spoken dialog management, various dialog state tracking methods have been proposed. Although discriminative models are gaining popularity due to their superior performance, generative models based on the Partially Observable Markov Decision Process model still remain attractive since they provide an integrated framework for dialog state tracking and dialog policy optimization. Although a straightforward way to fit a generative model is to independently train the component probability models, we present a gradient descent algorithm that simultaneously train all the component models. We show that the resulting tracker performs competitively with other top-performing trackers that participated in DSTC2.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Matthew Henderson</author>
<author>Blaise Thomson</author>
<author>Jason Williams</author>
</authors>
<title>The second dialog state tracking challenge.</title>
<date>2014</date>
<booktitle>In Proceedings of the SIGdial 2014 Conference,</booktitle>
<location>Baltimore, U.S.A.,</location>
<contexts>
<context position="13149" citStr="Henderson et al., 2014" startWordPosition="2175" endWordPosition="2179">out the dialog turns. Once we obtain the gradients, we update the parameters using the gradient descent I∂E � , ∂wO where α is the stepsize parameter. α is initially set to 0.5 and decreased by multiplying 110 whenever the overall cost function increases. 4 Dialog State Tracking in the Restaurant Information Domain This section describes the dialog domain used for the evaluation of our dialog tracker and the component probability models used for the domain. An instruction on how to obtain the dataset and a more detailed description on the dialog domain can be found in the DSTC2 summary paper (Henderson et al., 2014). 4.1 Task Description We used the DSTC2 dialog domain in which the user queries the database of local restaurants. The dataset for the restaurant information domain were originally collected using Amazon Mechanical Turk. A usual dialog proceeds as follows: the user specifies the constraints (e.g. type of food, location, etc) or the name of restaurant he wants, and the system offers the name of a restaurant that qualifies the constraints. User then accepts the offer, and requests for additional information about accepted restaurant. The dialog ends when all the information requested by the use</context>
</contexts>
<marker>Henderson, Thomson, Williams, 2014</marker>
<rawString>Matthew Henderson, Blaise Thomson, and Jason Williams. 2014. The second dialog state tracking challenge. In Proceedings of the SIGdial 2014 Conference, Baltimore, U.S.A., June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daejoong Kim</author>
<author>Jaedeug Choi</author>
<author>Kee-Eung Kim</author>
<author>Jungsu Lee</author>
<author>Jinho Sohn</author>
</authors>
<title>A specific analysis of a dialog state tracker in a challenge.</title>
<date>2013</date>
<booktitle>In Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>462--466</pages>
<contexts>
<context position="4231" citStr="Kim et al., 2013" startWordPosition="645" endWordPosition="648">e approach. Examples of discriminative dialog state trackers include (Lee, 2013; Metallinou et al., 2013). However, their feature functions often refer to past observations, and it remains yet to be seen whether 273 Proceedings of the SIGDIAL 2014 Conference, pages 273–281, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics the discriminative approach can be successfully incorporated into POMDP or reinforcement learning (RL) for dialog strategy optimization. This paper is concerned with the generative approach to dialog state tracking. In our earlier work (Kim et al., 2013), the optimization of the tracker was carried out independently for each component model (observation model, user action model, and belief refinement model) that comprised our tracker. This was not exactly a proper way to train the tracker for overall performance optimization. In this paper, we present an optimization method, which we call “cascading gradient descent”, that trains component models simultaneously. We show that this approach yields a dialog state tracker that performs on par with the best ones that participated in the second Dialog State Tracking Challenge (DSTC2). The rest of t</context>
<context position="10331" citStr="Kim et al., 2013" startWordPosition="1688" endWordPosition="1691">ing is to adopt factorized dynamic Bayesian network by making conditional independence assumptions among dialog state components, and use approximate inference algorithms such as loopy belief propagation (Thomson and Young, 2010) or blocked Gibbs sampling (Raux and Ma, 2011). 3 Cascading Gradient Descent Although equation (4) is an elegant formulation of the dialog state tracking via Bayes rule, there has not been an integrated learning algorithm that simultaneously optimizes component probability models, i.e. the observation, the user action, and the belief refinement models. Our prior work (Kim et al., 2013) relied on independently training each component probability model, and then simply plugging them into (4). Since the independent optimization of component probability models does not lend itself to the optimization of overall dialog state tracking performance, we added an extra post-processing step called “belief transformation” in order to fine tune the results obtained from equation (4). Unfortunately, this effort generally resulted in overfitting to the training data. In this paper, we present an integrated learning algorithm that simultaneously optimizes the component probability models o</context>
<context position="23863" citStr="Kim et al., 2013" startWordPosition="4028" endWordPosition="4031">ws the test set score of tracker implemented based on proposed algorithm, with the score of other trackers submitted to DSTC2. We tried 200 random initialised weights to train model with proposed algorithm, and learned model with the lowest training L2 error is picked to show the result on the test set. Because we only used live SLU and past data to track dialog state, other tracker results with the same condition are selected to compare with our tracker. The implementation of our algorithm was not ready until the DSTC2 deadline. We participated as Team 8 using the old optimization method in (Kim et al., 2013). As shown in the table 3, the new algorithm shows a substantial improvement, achieving almost 15% decrease in the L2 error. Since both trackers are fine-tuned, this improvement seems to be originated from the new optimization algorithm. For all three featured metrics used to evaluate, tracker constructed with our proposed method shows competitive performance. The key to excel baseline tracker was to discover the relation between user action and system action. For example, user actions that tell about the same slot system was talking about but giving different value are usually correcting wron</context>
</contexts>
<marker>Kim, Choi, Kim, Lee, Sohn, 2013</marker>
<rawString>Daejoong Kim, Jaedeug Choi, Kee-Eung Kim, Jungsu Lee, and Jinho Sohn. 2013. A specific analysis of a dialog state tracker in a challenge. In Proceedings of the SIGDIAL 2013 Conference, pages 462–466.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungjin Lee</author>
</authors>
<title>Structured discriminative model for dialog state tracking.</title>
<date>2013</date>
<booktitle>Proceedings of the SIGDIAL 2013 Conference,</booktitle>
<pages>442--451</pages>
<contexts>
<context position="3693" citStr="Lee, 2013" startWordPosition="566" endWordPosition="567">te tracking is essentially belief monitoring, which is the task of calculating posterior distribution over the hidden state given the history of observations. Examples of the dialog state trackers that take the generative approach include (Young et al., 2010; Thomson and Young, 2010; Raux and Ma, 2011) On the other hand, the discriminative approach directly models the posterior distribution. Since it avoids modeling of unnecessary aspects of the task, it typically achieves a better tracking accuracy compared to the generative approach. Examples of discriminative dialog state trackers include (Lee, 2013; Metallinou et al., 2013). However, their feature functions often refer to past observations, and it remains yet to be seen whether 273 Proceedings of the SIGDIAL 2014 Conference, pages 273–281, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics the discriminative approach can be successfully incorporated into POMDP or reinforcement learning (RL) for dialog strategy optimization. This paper is concerned with the generative approach to dialog state tracking. In our earlier work (Kim et al., 2013), the optimization of the tracker was carried out independentl</context>
<context position="7311" citStr="Lee, 2013" startWordPosition="1186" endWordPosition="1187"> set of model parameters to be learned from dialog data. According to the formulation, the posterior computation has to be carried out for all possible user goals in order to obtain the normalizing constant η. This is not feasible for real dialog domains that have a large number of user goals (the DSTC2 dialog domain used on this paper has 371070 user goals). Consequently, it is important for the discriminative approach to reduce the size of the state space. (Metallinou et al., 2013) adopts the idea behind the HIS model and confines the set of possible goals to those appeared in SLU results. (Lee, 2013) assumed conditional independence between dialog state components to address scalability, and used conditional random field. 2.2 Generative Dialog State Tracking In contrast, the generative approach to the dialog state tracking calculates the belief using Bayes’ rule, with the belief from the last turn b as a prior and the likelihood given the user utterance hypotheses Pr(o|a, g, h). In the prior work (Williams et al., 2005), the likelihood is factored and some independence assumptions are made: b0(g0, h0) = η � Pr(o|u) Pr(u|g0, a) · u � �Pr(h0|g0, u, h, a) Pr(g0|g, a)b(g, h) h g (2) where η i</context>
</contexts>
<marker>Lee, 2013</marker>
<rawString>Sungjin Lee. 2013. Structured discriminative model for dialog state tracking. Proceedings of the SIGDIAL 2013 Conference, pages 442–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Metallinou</author>
<author>Dan Bohus</author>
<author>Jason D Williams</author>
</authors>
<title>Discriminative state tracking for spoken dialog systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguastics,</booktitle>
<pages>466--475</pages>
<contexts>
<context position="3719" citStr="Metallinou et al., 2013" startWordPosition="568" endWordPosition="571"> is essentially belief monitoring, which is the task of calculating posterior distribution over the hidden state given the history of observations. Examples of the dialog state trackers that take the generative approach include (Young et al., 2010; Thomson and Young, 2010; Raux and Ma, 2011) On the other hand, the discriminative approach directly models the posterior distribution. Since it avoids modeling of unnecessary aspects of the task, it typically achieves a better tracking accuracy compared to the generative approach. Examples of discriminative dialog state trackers include (Lee, 2013; Metallinou et al., 2013). However, their feature functions often refer to past observations, and it remains yet to be seen whether 273 Proceedings of the SIGDIAL 2014 Conference, pages 273–281, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics the discriminative approach can be successfully incorporated into POMDP or reinforcement learning (RL) for dialog strategy optimization. This paper is concerned with the generative approach to dialog state tracking. In our earlier work (Kim et al., 2013), the optimization of the tracker was carried out independently for each component model</context>
<context position="7189" citStr="Metallinou et al., 2013" startWordPosition="1162" endWordPosition="1165">ons, system actions, and user goals up to the current dialog turn t, φ(·) is the vector of feature functions on x, and finally, w is the set of model parameters to be learned from dialog data. According to the formulation, the posterior computation has to be carried out for all possible user goals in order to obtain the normalizing constant η. This is not feasible for real dialog domains that have a large number of user goals (the DSTC2 dialog domain used on this paper has 371070 user goals). Consequently, it is important for the discriminative approach to reduce the size of the state space. (Metallinou et al., 2013) adopts the idea behind the HIS model and confines the set of possible goals to those appeared in SLU results. (Lee, 2013) assumed conditional independence between dialog state components to address scalability, and used conditional random field. 2.2 Generative Dialog State Tracking In contrast, the generative approach to the dialog state tracking calculates the belief using Bayes’ rule, with the belief from the last turn b as a prior and the likelihood given the user utterance hypotheses Pr(o|a, g, h). In the prior work (Williams et al., 2005), the likelihood is factored and some independence</context>
</contexts>
<marker>Metallinou, Bohus, Williams, 2013</marker>
<rawString>Angeliki Metallinou, Dan Bohus, and Jason D Williams. 2013. Discriminative state tracking for spoken dialog systems. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguastics, pages 466–475.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Raux</author>
<author>Yi Ma</author>
</authors>
<title>Efficient probabilistic tracking of user goal and dialog history for spoken dialog systems.</title>
<date>2011</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>801--804</pages>
<contexts>
<context position="3387" citStr="Raux and Ma, 2011" startWordPosition="518" endWordPosition="521">sterior. It has been a popular approach for statistical dialog state tracking, since it naturally fits into the Partially Observable Markov Decision Process (POMDP) (Williams and Young, 2007), an integrated model for dialog state tracking and dialog strategy optimization. In the POMDP point of view, the dialog state tracking is essentially belief monitoring, which is the task of calculating posterior distribution over the hidden state given the history of observations. Examples of the dialog state trackers that take the generative approach include (Young et al., 2010; Thomson and Young, 2010; Raux and Ma, 2011) On the other hand, the discriminative approach directly models the posterior distribution. Since it avoids modeling of unnecessary aspects of the task, it typically achieves a better tracking accuracy compared to the generative approach. Examples of discriminative dialog state trackers include (Lee, 2013; Metallinou et al., 2013). However, their feature functions often refer to past observations, and it remains yet to be seen whether 273 Proceedings of the SIGDIAL 2014 Conference, pages 273–281, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics the discri</context>
<context position="9989" citStr="Raux and Ma, 2011" startWordPosition="1637" endWordPosition="1640">est alternative user action may have the power of diminishing dominant partitions, and it can only be learnt by the dependence with partition confidence. The belief update formula used in this paper becomes: b0(ψ0) = η X Pr(o|u, a, h) · u (4) Pr(u|ψ0, a) Pr(ψ0|ψ)b(ψ) Other approaches to cope with the scalability problem in dialog state tracking is to adopt factorized dynamic Bayesian network by making conditional independence assumptions among dialog state components, and use approximate inference algorithms such as loopy belief propagation (Thomson and Young, 2010) or blocked Gibbs sampling (Raux and Ma, 2011). 3 Cascading Gradient Descent Although equation (4) is an elegant formulation of the dialog state tracking via Bayes rule, there has not been an integrated learning algorithm that simultaneously optimizes component probability models, i.e. the observation, the user action, and the belief refinement models. Our prior work (Kim et al., 2013) relied on independently training each component probability model, and then simply plugging them into (4). Since the independent optimization of component probability models does not lend itself to the optimization of overall dialog state tracking performan</context>
</contexts>
<marker>Raux, Ma, 2011</marker>
<rawString>Antoine Raux and Yi Ma. 2011. Efficient probabilistic tracking of user goal and dialog history for spoken dialog systems. In INTERSPEECH, pages 801–804.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blaise Thomson</author>
<author>Steve Young</author>
</authors>
<title>Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems.</title>
<date>2010</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="3367" citStr="Thomson and Young, 2010" startWordPosition="514" endWordPosition="517"> rule to calculate the posterior. It has been a popular approach for statistical dialog state tracking, since it naturally fits into the Partially Observable Markov Decision Process (POMDP) (Williams and Young, 2007), an integrated model for dialog state tracking and dialog strategy optimization. In the POMDP point of view, the dialog state tracking is essentially belief monitoring, which is the task of calculating posterior distribution over the hidden state given the history of observations. Examples of the dialog state trackers that take the generative approach include (Young et al., 2010; Thomson and Young, 2010; Raux and Ma, 2011) On the other hand, the discriminative approach directly models the posterior distribution. Since it avoids modeling of unnecessary aspects of the task, it typically achieves a better tracking accuracy compared to the generative approach. Examples of discriminative dialog state trackers include (Lee, 2013; Metallinou et al., 2013). However, their feature functions often refer to past observations, and it remains yet to be seen whether 273 Proceedings of the SIGDIAL 2014 Conference, pages 273–281, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Li</context>
<context position="9943" citStr="Thomson and Young, 2010" startWordPosition="1629" endWordPosition="1632"> user actions without their value. For example, request alternative user action may have the power of diminishing dominant partitions, and it can only be learnt by the dependence with partition confidence. The belief update formula used in this paper becomes: b0(ψ0) = η X Pr(o|u, a, h) · u (4) Pr(u|ψ0, a) Pr(ψ0|ψ)b(ψ) Other approaches to cope with the scalability problem in dialog state tracking is to adopt factorized dynamic Bayesian network by making conditional independence assumptions among dialog state components, and use approximate inference algorithms such as loopy belief propagation (Thomson and Young, 2010) or blocked Gibbs sampling (Raux and Ma, 2011). 3 Cascading Gradient Descent Although equation (4) is an elegant formulation of the dialog state tracking via Bayes rule, there has not been an integrated learning algorithm that simultaneously optimizes component probability models, i.e. the observation, the user action, and the belief refinement models. Our prior work (Kim et al., 2013) relied on independently training each component probability model, and then simply plugging them into (4). Since the independent optimization of component probability models does not lend itself to the optimizat</context>
</contexts>
<marker>Thomson, Young, 2010</marker>
<rawString>Blaise Thomson and Steve Young. 2010. Bayesian update of dialogue state: A pomdp framework for spoken dialogue systems. Computer Speech &amp; Language, 24(4):562–588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Steve Young</author>
</authors>
<title>Partially observable markov decision processes for spoken dialog systems.</title>
<date>2007</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>21</volume>
<issue>2</issue>
<contexts>
<context position="2960" citStr="Williams and Young, 2007" startWordPosition="450" endWordPosition="453">ior distribution over the dialog states using multiple results from SLU in order to overcome the error in the most likely SLU result. Statistical dialog state trackers can be categorized into two approaches depending on how the posterior calculation is modeled. The generative approach uses the generative model that describes how the SLU results are generated from the hidden dialog state and uses the Bayes’ rule to calculate the posterior. It has been a popular approach for statistical dialog state tracking, since it naturally fits into the Partially Observable Markov Decision Process (POMDP) (Williams and Young, 2007), an integrated model for dialog state tracking and dialog strategy optimization. In the POMDP point of view, the dialog state tracking is essentially belief monitoring, which is the task of calculating posterior distribution over the hidden state given the history of observations. Examples of the dialog state trackers that take the generative approach include (Young et al., 2010; Thomson and Young, 2010; Raux and Ma, 2011) On the other hand, the discriminative approach directly models the posterior distribution. Since it avoids modeling of unnecessary aspects of the task, it typically achieve</context>
</contexts>
<marker>Williams, Young, 2007</marker>
<rawString>Jason D Williams and Steve Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech &amp; Language, 21(2):393–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason D Williams</author>
<author>Pascal Poupart</author>
<author>Steve Young</author>
</authors>
<title>Factored partially observable markov decision processes for dialogue management.</title>
<date>2005</date>
<booktitle>In 4th Workshop on Knowledge and Reasoning in Practical Dialog Systems, International Joint Conference on Artificial Intelligence (IJCAI),</booktitle>
<pages>76--82</pages>
<contexts>
<context position="5590" citStr="Williams et al., 2005" startWordPosition="879" endWordPosition="882"> explain the DSTC2 dialog domain and the experimental settings in section 4, and discuss the results in section 5. Finally, we conclude the paper with the summary and the suggestion for future work in section 6. 2 Background and Related Work The dialog state tracking is formalized as follows: In each turn of the dialog, the spoken dialog system executes system action a, and the user with goal g responds to the system with utterance u. The dialog state in each turn is defined s = (u, g, h), where h is the dialog history encapsulating additional information needed for tracking the dialog state (Williams et al., 2005). The SLU processes the user utterance and generates the results as an N-best list o = [(˜u1, f1), ... , (˜uN, fN)], where ˜ui is the hypothesized user utterance and fi is its confidence score1. Without loss of generality, we assume that the last item in the N-best list is the null item (0,1− EN−1 i=1fi), representing the set of unrecognized user utterances. The statistical dialog state tracker maintains the probability distribution over states, called the belief. 2.1 Discriminative Dialog State Tracking Dialog state trackers taking the discriminative approach calculates the belief via trained</context>
<context position="7739" citStr="Williams et al., 2005" startWordPosition="1252" endWordPosition="1255">pproach to reduce the size of the state space. (Metallinou et al., 2013) adopts the idea behind the HIS model and confines the set of possible goals to those appeared in SLU results. (Lee, 2013) assumed conditional independence between dialog state components to address scalability, and used conditional random field. 2.2 Generative Dialog State Tracking In contrast, the generative approach to the dialog state tracking calculates the belief using Bayes’ rule, with the belief from the last turn b as a prior and the likelihood given the user utterance hypotheses Pr(o|a, g, h). In the prior work (Williams et al., 2005), the likelihood is factored and some independence assumptions are made: b0(g0, h0) = η � Pr(o|u) Pr(u|g0, a) · u � �Pr(h0|g0, u, h, a) Pr(g0|g, a)b(g, h) h g (2) where η is the normalizing constant and u is marginalized out in the belief. Scalability became the important issue, just as in the generative approach. One way to reduce the amount of computation is to group the states into partitions, proposed as the Hidden Information State (HIS) model (Young et al., 2010). Beginning with one root partition with the probability of 1, partitions are split when the distinction is required by observa</context>
</contexts>
<marker>Williams, Poupart, Young, 2005</marker>
<rawString>Jason D Williams, Pascal Poupart, and Steve Young. 2005. Factored partially observable markov decision processes for dialogue management. In 4th Workshop on Knowledge and Reasoning in Practical Dialog Systems, International Joint Conference on Artificial Intelligence (IJCAI), pages 76–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve Young</author>
<author>Milica Gaˇsi´c</author>
<author>Simon Keizer</author>
<author>Franc¸ois Mairesse</author>
<author>Jost Schatzmann</author>
<author>Blaise Thomson</author>
<author>Kai Yu</author>
</authors>
<title>The hidden information state model: A practical framework for pomdp-based spoken dialogue management.</title>
<date>2010</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>24</volume>
<issue>2</issue>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>Steve Young, Milica Gaˇsi´c, Simon Keizer, Franc¸ois Mairesse, Jost Schatzmann, Blaise Thomson, and Kai Yu. 2010. The hidden information state model: A practical framework for pomdp-based spoken dialogue management. Computer Speech &amp; Language, 24(2):150–174.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002954">
<title confidence="0.995812">
Exploring Options for Fast Domain Adaptation of Dependency Parsers
</title>
<author confidence="0.998843">
Viktor Pekar, Juntao Yu, Mohab El-karef, Bernd Bohnet
</author>
<affiliation confidence="0.997927">
School of Computer Science
University of Birmingham
</affiliation>
<address confidence="0.824224">
Birmingham, UK
</address>
<email confidence="0.999287">
{v.pekar,jxy362,mxe346,b.bohnet}@cs.bham.ac.uk
</email>
<sectionHeader confidence="0.994808" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941307692308">
The paper explores different domain-independent techniques to adapt a dependency parser
trained on a general-language corpus to parse web texts (online reviews, newsgroup posts, we-
blogs): co-training, word clusters, and a crowd-sourced dictionary. We examine the relative
utility of these techniques as well as different ways to put them together to achieve maximum
parsing accuracy. While we find that co-training and word clusters produce the most promising
results, there is little additive improvement when combining the two techniques, which suggests
that in the absence of large grammatical discrepancies between the training and test domains,
they address largely the same problem, that of unknown vocabulary, with word clusters being
a somewhat more effective solution for it. Our highest results were achieved by a combination
of word clusters and co-training, significantly improving on the baseline, by up to 1.67%. Eval-
uation of the best configurations on the SANCL-2012 test data (Petrov and McDonald, 2012)
showed that they outperform all the shared task submissions that used a single parser to parse
test data, averaging the results across all the test sets.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999863888888889">
Domain adaptation of a statistical dependency parser is a problem that is of much importance for many
practical NLP applications. Previous research has shown that the accuracy of parsing significantly drops
when a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomed-
ical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al.,
2013). In a preliminary experiment, we looked at the effect of cross-domain parsing on three state-of-
the-art parsers – Malt (Nivre, 2009), MST (McDonald and Pereira, 2006), and Mate parser (Bohnet et
al., 2013) – trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotes
v5.0 corpus as well as the in-domain CoNLL09 test set. The results (see Table 1) indicate that depending
on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%.
</bodyText>
<table confidence="0.998504285714286">
Domain MST MALT Mate
Newswire 84.8 81.7 87.1
Pivot Texts 84.9 83.0 86.6
Broadcast News 79.4 78.1 81.2
Magazines 77.1 74.7 79.3
Broadcast Conversation 73.4 70.5 74.4
CoNLL09 test 86.9 84.7 90.1
</table>
<tableCaption confidence="0.8814545">
Table 1: Labelled accuracy scores achieved by the MST, Malt, and Mate parsers trained on CoNLL09
data and tested on different specialist domains.
</tableCaption>
<bodyText confidence="0.778089">
In a typical domain adaptation scenario, there are in-domain texts that are manually annotated and
that are used to train a general-language parser, and out-of-domain or target domain texts that are
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.978931">
54
</page>
<note confidence="0.7581795">
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014.
</note>
<bodyText confidence="0.999690666666667">
parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be
available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem
of domain adaption, previous work focused on weakly supervised methods to re-train parsers on auto-
matically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman
et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et
al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target
domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of
research aims specifically to overcome the lexical gap between the training data and the target domain
texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010),
the use of external lexica and morphological clues to predict PoS tags of unknown target domain words
(Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled
target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from
word co-occurrences obtained from unannotated texts (Zhou et al., 2011).
The goal of this paper is to investigate a combination of such techniques to adapt a general-language
parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual
annotation. In our study we include several techniques that have been shown to be reasonably effective
for domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon,
as well as automatically annotated texts produced with the help of co-training. All these techniques
are domain-independent and can be applied to new target domains given unlabelled texts form these
domains. We explore the relative utility of these methods and ways to combine them for maximum
parser accuracy.
</bodyText>
<sectionHeader confidence="0.999865" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.999597">
2.1 Text normalization
</subsectionHeader>
<bodyText confidence="0.999929125">
User-generated content on the web is notoriously low-quality, containing slang, abbreviations, inconsis-
tent grammar and spelling. Foster (2010) investigated lexical phenomena that appear on online discus-
sion forums that present common problems for parsing and compiled a list of such phenomena along
with their transformations. Applying the transformations to test sentences helped to bring the F-score up
by 2.7%. A similar approach was taken by Khan et al. (2013a) who found that it performed better than
spelling correction based on the Levenshtein distance. Gadde et al. (2011) use a word clustering method
and language modelling in order to align misspelled words with their regular spelling. Their method of
cleaning noisy text helped to increase the accuracy of PoS tagging of SMS data by 3.5%.
</bodyText>
<subsectionHeader confidence="0.999175">
2.2 External lexica
</subsectionHeader>
<bodyText confidence="0.9999266">
To adapt the Link parser to the medical domain, Szolovitz (2003) extended its lexicon with terms from
the UMLS Specialist Lexicon. Pyysalo et al. (2006) take the same approach and together with predicting
the PoS tags for out-of-vocabulary words based on their morphology this allowed them to achieve a 10%
reduction in the error rate of parsing. External lexica have also been used to improve out-of-domain PoS
tagging (Li et al., 2012).
</bodyText>
<subsectionHeader confidence="0.999822">
2.3 Word clusters
</subsectionHeader>
<bodyText confidence="0.9999078">
In order to reduce the amount of annotated data to train a dependency parser, Koo et al. (2008) used
word clusters computed from unlabelled data as features for training a parser. The same approach has
proved to be effective for out-of-domain parsing, where there are many words in the test data unseen
during training, and word clusters computed from in-domain data similarly help to deal with the vocab-
ulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et
al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts
(Candito et al., 2011) and web texts (Øvrelid and Skjærholt, 2012). Word clusters created with Brown
clustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013).
Bansal et al. (2014) introduced continuous word representations and showed them to increase parsing
accuracy both on the Penn Treebank and on web data.
</bodyText>
<page confidence="0.997108">
55
</page>
<subsectionHeader confidence="0.999492">
2.4 Co-training
</subsectionHeader>
<bodyText confidence="0.999930647058824">
Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification
problem from a limited amount of labelled data and a large amount of unlabelled data, whereby two or
more views on the data, i.e. feature subsets, or two or more different learning algorithms are employed
that complement each other to bootstrap additional training data from the unlabelled dataset. Co-training
algorithms have been successfully used in NLP tasks, and specifically for parsing. Sarkar (2001) showed
the both precision and recall of a phrase structure parser can be increased using a co-training procedure
that iteratively adds the most confidently parsed sentences from two different views to the training set.
Steedman et al.(2003) used two different parsers that supplied training data to each other in a bootstrap-
ping manner.
A number of studies specifically aimed to use co-training for domain adaptation of a dependency
parser. Sagae (2007) used two different learning algorithms of their graph-based parser to complete a
one iteration of co-training, getting an improvement of 2-3%, which was the best result on the out-of-
domain track of the CoNLL07 shared task (Nilsson et al., 2007). An interesting finding of their work was
that the agreement between the two classifiers during testing was a very good predictor of accuracy. More
recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm
uses three learners and each learner was designed to learn from those automatically classified unlabelled
data where the other two learners agreed on the classification label.
</bodyText>
<sectionHeader confidence="0.998675" genericHeader="method">
3 Experimental set-up
</sectionHeader>
<subsectionHeader confidence="0.997912">
3.1 Parsers
</subsectionHeader>
<bodyText confidence="0.99999025">
In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira,
2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins
et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all
the parsers were the same and came from the Mate parser.
</bodyText>
<subsectionHeader confidence="0.998145">
3.2 Baseline
</subsectionHeader>
<bodyText confidence="0.999903666666667">
As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation
techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain
test data.
</bodyText>
<subsectionHeader confidence="0.9981">
3.3 Data
</subsectionHeader>
<bodyText confidence="0.999974764705882">
The experiments were conducted on annotated data on web-related domains available in the Ontonotes
v.5 and SANCL datasets, since a large amount of unlabelled data required for most domain adaptation
techniques is widely available.
OntoNotes. In experiments with weblog texts, we used the CoNLL09 training dataset (Hajiˇc et al.,
2009) as the general-language training data. The CoNLL09 test dataset was used to evaluate in-domain
parsing. To create an out-of-domain test set, we selected the last 10% of the weblogs section of the
OntoNotes v5.0 corpus1, in order to make the size of the out-of-domain test data comparable to that of
the in-domain test data, i.e. of CoNLL09 test. The OntoNotes corpus was converted to the CoNLL09
format using the LTH constituent-to-dependency conversion tool (Johansson and Nugues, 2007).
SANCL. In order to compare our results with the results achieved by participants in the SANCL-2012
shared task, we also ran experiments on the Stanford dependences of three SANCL test sets (answers,
newsgroups and reviews). In these experiments we used the training set, test sets, unlabelled data,
as well as the evaluation script provided by SANCL-2012 organizers (Petrov and McDonald, 2012).
Tables 2 and 3 show the sizes of the OntoNotes and SANCL datasets as well as several measures
of lexical and grammatical characteristics of the data. The average sentence length (in tokens) and the
average number of subjects, roughly corresponding to the number of clauses in the sentence, aim to
characterize the syntactic complexity of the sentences: the higher these values, the more complex the
</bodyText>
<footnote confidence="0.908515">
1https://catalog.ldc.upenn.edu/LDC2013T19
</footnote>
<page confidence="0.996704">
56
</page>
<bodyText confidence="0.999775666666667">
structure of the sentences is likely to be. The ratio of word forms absent from training data describes
how different the train and test data are in terms of vocabulary.
We see that in the OntoNotes test set the average sentence length and the number of subjects per
sentence is very similar to those in the train data. In SANCL test sets, these measures are more different,
but the values indicate a smaller syntactic complexity than in the train data. The amount of unknown
vocabulary in all the four test sets is between 5% and 8%.
</bodyText>
<table confidence="0.998895833333333">
CoNLL09 train CoNLL09 test OntoNotes test
Sentences 39,279 2,399 2,150
Tokens 958,167 57,676 42,144
Sentence length 24.61 24.59 23.4
Subjects 1.8 1.83 1.89
Unk. wordforms ratio 0.0 0.011 0.05
</table>
<tableCaption confidence="0.99709">
Table 2: The size of OntoNotes train and test datasets.
</tableCaption>
<table confidence="0.995004666666667">
SANCL train Answers test Newsgroups test Reviews test
Sentences 30,060 1,744 1,195 1,906
Tokens 731,678 28,823 20,651 28,086
Sentence length 24.56 18.44 22.79 16.35
Subjects 1.69 1.78 1.62 1.5
Unk. wordforms ratio 0.0 0.064 0.084 0.051
</table>
<tableCaption confidence="0.999905">
Table 3: The size of SANCL train and test datasets.
</tableCaption>
<bodyText confidence="0.982558">
Unlabelled Data. As unlabelled target domain data we used the unlabelled dataset from the SANCL-
2012 shared task. In experiments with word clusters, the entire dataset was used without any pre-
processing. In the co-training experiments, we pre-processed the data by removing sentences that are
longer than 500 tokens, or contained non-English words (this reduced the test set by 2%). Table 4 de-
scribes the size of the subsets of the unlabelled data.
</bodyText>
<table confidence="0.967879333333333">
Emails Weblogs Answers Newsgroups Reviews
Sentences 1,194,173 524,834 27,274 1,000,000 1,965,350
Tokens 17,047,731 10,356,284 424,299 18,424,657 29,289,169
</table>
<tableCaption confidence="0.999475">
Table 4: The size of unlabelled datasets.
</tableCaption>
<subsectionHeader confidence="0.974426">
3.4 Evaluation method
</subsectionHeader>
<bodyText confidence="0.9999895">
As a measure of parser accuracy, we report labeled attachment scores (LAS), the percentage of depen-
dencies which are attached and labeled correctly. Significance testing was performed using paired t-test.
</bodyText>
<sectionHeader confidence="0.999376" genericHeader="method">
4 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.99327">
4.1 Text normalization
</subsectionHeader>
<bodyText confidence="0.999429285714286">
We used a manually compiled lexicon containing Internet-specific spellings of certain words aligned
with their traditional spellings, e.g. u ⇒ you, gr8 ⇒ great, don,t ⇒ don‘t, as well as a number of regular
expressions to deal with extra symbols usually added for emphasis (This is sooooo good., This *is*
great.). After the original word forms were read by the parser, the lexicon and the regular expressions
were applied to normalize the spelling of the words. This produced only a very insignificant gain on the
baseline. A manual examination of the test data in both OntoNotes and SANCL has shown that in fact
although it comes from the web it contains very few examples of “Internet speak”.
</bodyText>
<page confidence="0.990127">
57
</page>
<subsectionHeader confidence="0.992005">
4.2 Word clusters
</subsectionHeader>
<bodyText confidence="0.9937246">
We used Liang’s (2005) implementation of the Brown clustering algorithm to create clusters of words
found in unlabelled domain texts. The output of the algorithm are word types assigned to discrete hi-
erarchical clusters, with clusters assigned ids in the form of bit strings of varying length corresponding
to clusters of different granularity. We experimentally set the maximum length of the bit string to 6,
collapsing more fine-grained clusters. Instead of replacing the original word forms and/or PoS tags with
cluster ids as was done in some previous studies (Koo et al., 2008; Candito et al., 2011; T¨ackstr¨om et
al., 2013), the ids of clusters were used to generate additional features in the representations of the word
forms, as this also produced better results in the preliminary runs. Below we describe experiments with
several other parameters of the clustering algorithm.
Number of clusters. As an input parameter, the Brown clustering algorithm requires a desired number
of clusters. Initially discarding all word types with a count of less than 3, we experimented with different
numbers of clusters and found that an optimal settings lies around 600 and 800 clusters, which gives an
improvement on the baseline of 0.9% for out-of-domain texts; but there does not seem to be noticeable
differences between specific numbers of clusters (see Table 5, statistically significant differences to the
baseline are indicated by stars2).
</bodyText>
<table confidence="0.9996475">
Number of clusters CoNLL09 OntoNotes
50 90.46** 78.10*
100 90.28* 78.40**
200 90.27 78.39**
400 90.37** 78.20**
600 90.40** 78.43**
800 90.30* 78.14**
Baseline 90.07 77.54
</table>
<tableCaption confidence="0.995962">
Table 5: The effect of the number of word clusters on in- and out-of-domain parsing, using the reviews
</tableCaption>
<bodyText confidence="0.93795375">
and weblogs subsets of the SANCL-2012 unlabelled data.
Filtering rare words. Due to the inevitable data sparseness, the algorithm is likely to mis-cluster
infrequent words. At the same time, it is rare words that are not seen during parser training and are
potentially of greatest value if included into word clusters. We examined several thresholds on word
frequency and their impact on parsing accuracy (see Table 6; statistically significant differences to the
baseline are indicated by stars). We found very slight differences between these three thresholds, al-
though the cut-off point of 3 showed the best results. Hence in further experiments with word clusters
we used this cut-off point.
</bodyText>
<table confidence="0.99957425">
Min. freq. CoNLL09 OntoNotes
1 90.36** 78.12*
3 90.40** 78.43**
5 90.22 78.24**
</table>
<tableCaption confidence="0.7631365">
Table 6: The effect of filtering out rare words on word clusters, using the reviews and weblogs subsets
of the SANCL-2012 unlabelled data.
</tableCaption>
<bodyText confidence="0.998202">
Amount of unlabelled data. To examine the effect that the size of unlabelled data from which word
clusters are computed, has on parser accuracy, we compared parser accuracy achieved when using only
the reviews and weblogs subsets of the SANCL corpus (39.6 mln word tokens), and when using the
entire SANCL dataset (75.2 mln tokens). These results are shown in Table 7, significant improvements
on the smaller set are indicated by stars. As expected, a larger amount of data does improve the parsing
accuracy, and the improvement is greater for out-of-domain parsing (+0.55% vs. +0.32%).
</bodyText>
<footnote confidence="0.980854">
2In this and the following tables, one star indicates significance at the P &lt; 0.05 level, two stars at the P &lt; 0.01 level.
</footnote>
<page confidence="0.982578">
58
</page>
<table confidence="0.994202">
CoNLL09 OntoNotes
Reviews and Weblogs 90.30 78.14
Entire SANCL dataset 90.62* 78.69*
</table>
<tableCaption confidence="0.997775">
Table 7: The effect of the size of unlabelled data on word clusters, discarding word types with count less
than 3.
</tableCaption>
<bodyText confidence="0.980930909090909">
Relevant domain data. Furthermore, we were interested if simply adding more unlabelled data, not
necessarily from the relevant domain, produced the same increase in accuracy. We obtained the plain-
text claims and description parts of 13,600 patents freely available in the Global IP Database which
is based on the Espacenet3, creating a corpus with 42.5 mln tokens, i.e. which was similar in size to
the reviews and weblogs sections of the SANCL unlabelled dataset. Table 8 compares results achieved
when building clusters from the patents corpus and when using the reviews and weblogs texts from the
SANCL unlabelled dataset. Despite the fact that the size of the two datasets is comparable, we find that
while creating clusters from an irrelevant domain does gain on the baseline (+0.25%), the improvement
for clusters built from the relevant domain texts is noticeably higher (+0.6%). The difference between
the accuracy on the legal texts and the accuracy on the reviews and weblogs texts is significant at the
p &lt; 0.05 level.
</bodyText>
<table confidence="0.985037666666667">
CoNLL09 OntoNotes
Legal texts 90.19 77.77
Reviews and Weblogs 90.30 78.14*
</table>
<tableCaption confidence="0.9890455">
Table 8: The effect of the domain of unlabelled data on word clusters, discarding word types with count
less than 3.
</tableCaption>
<subsectionHeader confidence="0.997027">
4.3 External lexicon
</subsectionHeader>
<bodyText confidence="0.9999751">
It is possible to supply to the dependency parser an external lexicon, where word forms are provided
with PoS tags. Wiktionary, a companion project for Wikipedia that aims to produce a free, large-scale
multilingual dictionary, is a large and constantly growing crowd-sourced resource that appears attrac-
tive for NLP research. Wiktionary encodes word definitions, pronunciation, translations, etymology,
word forms and part-of-speech information. PoS tag dictionaries derived from Wiktionary have been
previously used for out-of-domain PoS tagging (Li et al., 2012) and for PoS tagging of resource-poor
languages (T¨ackstr¨om et al., 2013).
To create a lexicon for the parser, we extracted 753,970 English word forms and their PoS tags from
a dump of Wiktionary4. Wiktionary uses a rather informal set of PoS labels; to convert them to the
CoNLL09 tag set, we manually aligned all unique PoS tags found in Wiktionary with those of the
CoNLL09 tag set. We compared the accuracy achieved by the parser when the lexicon was supplied,
as well as when the lexicon was supplied together with the best configuration word clusters (800 clusters
built from the entire SANCL dataset after filtering words with the count less than 3). Table 9 shows
results achieved with these settings in comparison to the baseline (improvements on the baseline are in-
dicated with stars). When the lexicon is used on its own, we observe only slight gains on the baseline,
on both in-domain and out-domain data, and neither are statistically significant. When combining the
lexicon and word clusters, the accuracy actually decreases compared to using word clusters on their own.
Thus the best combination of domain adaptation techniques so far included the use of 800 word clusters
built from the entire SANCL unlabelled dataset, after filtering out word forms with the count less than 3,
with text normalization, but without the Wiktionary lexicon (+1.15% on the baseline).
</bodyText>
<footnote confidence="0.9998235">
3http://www.epo.org/searching/free/espacenet.html
4http://wiki.dbpedia.org/Wiktionary
</footnote>
<page confidence="0.995283">
59
</page>
<table confidence="0.9968124">
CoNLL09 OntoNotes
Wiktionary 90.22 77.73
Clusters 90.62 ** 78.69 **
Wiktionary+Clusters 90.44 78.49 **
Baseline 90.07 77.54
</table>
<tableCaption confidence="0.999857">
Table 9: The effect of the Wiktionary lexicon on parsing accuracy.
</tableCaption>
<subsectionHeader confidence="0.998946">
4.4 Co-Training
</subsectionHeader>
<bodyText confidence="0.9997578">
Following Sagae (2007), the overall approach to parser co-training we adopted was as follows. First,
several parsers were combined to generate additional training data from unlabelled data, i.e. were used
as source learners for co-training. Then, the Mate parser was re-trained on the augmented training set
and tested on a test set, i.e. used as the evaluation learner. The reason Mate was selected the evaluation
learner was that it achieved the best results on the test data in its default settings (see Table 10).
</bodyText>
<table confidence="0.9846372">
CoNLL09 OntoNotes
Mate 90.07 77.54
MST 86.9 75.35
Turbo 85.94 74.85
Malt 84.72 72.63
</table>
<tableCaption confidence="0.999321">
Table 10: The baselines of parsers used in co-training experiments.
</tableCaption>
<bodyText confidence="0.996995111111111">
Agreement-based co-training. We first experimented with three pairwise parser combinations: using
Mate as one source learner and each of the other three parsers as the other source learner in order to obtain
additional training data. If two learners agreed on the parse of an unlabelled sentence, i.e. assigned each
word form the same dependency label and attached it to the same head, this was taken as an indication of
a correct parse, and the sentence was added to the training set. We experimented with different amounts
of the additional training sentences added to the main training set in such a manner: 10k, 20k, and 30k
sentences. The results of these experiments are shown in Table 11 (significant differences to the baseline
results are indicated by stars). The best result is obtained by Mate Malt pair, which outperforms the
baseline by just above 1%.
</bodyText>
<table confidence="0.9986724">
+10k +20k +30k
Mate+Malt 78.22** 78.61** 78.61**
Mate+MST 78.10** 78.23** 78.31**
Mate+Turbo 77.94** 77.84* 77.99**
Baseline 77.54
</table>
<tableCaption confidence="0.999406">
Table 11: Agreement-based co-training using two parsers.
</tableCaption>
<bodyText confidence="0.999665363636364">
Removing short sentences from unlabelled data. We noticed that among those sentences where
two parsers agreed, many tended to be very short: the average number of tokens in generated additional
training data was 8 per sentence, while both the training and test set contain much longer sentences
on average: the OntoNotes test set had 19.6 tokens/sentence and the CoNLL09 training set had 24.4
tokens/sentence. Such short sentences in the additional training data may be less useful or even harmful
for learning an accurate model of the target domain, than those that approximate both training and test
data. We experimented with several thresholds (4, 5, and 6 tokens) on the sentence length below which
sentences were removed from the additional training data. Table 12 shows that discarding short sentences
did improve accuracy by up to 0.25%, though none of the improvements were significant.
Three learners co-training. In the previous experiments, the Mate parser was used both as a source
learner and as the evaluation learner. Therefore it was likely that the additional training data did not
</bodyText>
<page confidence="0.995893">
60
</page>
<table confidence="0.9995604">
Mate+Malt, +30k Avg. Length
&gt;6 tokens 78.88 13.1
&gt;5 tokens 78.61 12.67
&gt;4 tokens 78.67 11.94
All sentences 78.61 8.35
</table>
<tableCaption confidence="0.999855">
Table 12: The effect of removing short sentences from generated training data.
</tableCaption>
<bodyText confidence="0.998695142857143">
contain sufficiently novel examples based on which the evaluation parser could adapt better to the new
domain. Thus we next tried the tri-training algorithm (Zhou and Li, 2005), where two parsers are used
as source learners and a third as the evalaution learner. We used Malt and MST as source learners,
identifying sentences which they parsed in the same manner, and using these sentences to retrain the
Mate parser. We find that the tri-training algorithm performs better than the set-up with two parsers:
on 10k and 20k additional sentences, it achieves an accuracy increase on Mate+Malt, significant at the
p &lt; 0.05 level (see Table 13).
</bodyText>
<table confidence="0.997429">
+10k +20k +30k
Mate+Malt+MST 78.70* 79.12* 78.95
Mate+Malt 78.43 78.70 78.88
</table>
<tableCaption confidence="0.9614">
Table 13: Accuracy scores for tri-training (Mate+Malt+MST) and the best two-parser co-training algo-
rithm (Mate+Malt).
</tableCaption>
<sectionHeader confidence="0.475261" genericHeader="method">
5 Combining co-training with clusters and an external lexicon
</sectionHeader>
<subsectionHeader confidence="0.95483">
5.1 OntoNotes test set
</subsectionHeader>
<bodyText confidence="0.995841">
We explored several possibilities to combine co-training with word clusters and an external lexicon, each
time supplying word clusters and/or the lexicon to the Mate parser when it is being retrained on additional
training data and applied to the test data. The following configurations of each of the techniques were
used:
</bodyText>
<listItem confidence="0.9956785">
• Word clusters: 800 clusters generated from the entire SANCL unlabelled dataset, after discarding
word types with the count less than 3.
• Lexicon: Wiktionary
• Co-training: Retraining the Mate parser on the combination of initial training set and 20k automat-
</listItem>
<bodyText confidence="0.919053833333333">
ically parsed sentences (agreed by Malt and MST) which contained more than 6 tokens.
The results showed that all three combinations failed to obtain significant improvements over co-
training alone. The best result is achieved by combining co-training and clusters, which obtains an
increase of only 0.09% on co-training; this is however, the greatest overall improvement on the baseline
(+1.67%). The combination of co-training and a Wiktionary lexicon in fact harms accuracy (see Table
14).
</bodyText>
<subsectionHeader confidence="0.995492">
5.2 SANCL test set
</subsectionHeader>
<bodyText confidence="0.99634325">
In order to compare different technique combinations with the results achieved by participants of the
SANCL-2012 shared task, we evaluated them on the SANCL test set5.
As the results in Table 15 indicate, similarly to the results on OntoNotes, word clusters usually fare
much better than the Wiktionary-based lexicon, while the latter fails to produce statistically significant
</bodyText>
<footnote confidence="0.87647">
5Note that the data was annotated in the Stanford format.
</footnote>
<page confidence="0.995156">
61
</page>
<table confidence="0.99841025">
OntoNotes
Co-training 79.12**
Clusters 78.69**
Wiktionary 77.73
Co-training+Clusters 79.21**
Co-training+Wiktionary 78.89*
Co-training+Clusters+Wiktionary 79.19**
Baseline 77.54
</table>
<tableCaption confidence="0.999915">
Table 14: Combination of co-training with word clusters and an external lexicon, OntoNotes test set.
</tableCaption>
<bodyText confidence="0.999779">
improvements on the baseline. The best accuracy overall was achieved by combinations of techniques,
in all the three subdomains, improving on the baseline by up to 1.3%.
Comparing the results achieved by our best configurations with the results of the shared task, we see
that our labelled accuracy averaged across the subdomains was just above the Stanford-2 system (80.31
vs. 80.25), which ranked 5th of all the twelve submissions (Petrov and McDonald, 2012). Although our
results are still 3.15% lower than DCU-Paris13, the best system at SANCL-2012, the top four results
were all generated by combination systems (Le Roux et al., 2012; Zhang et al., 2012; McClosky et al.,
2012); our highest results only produced by the Mate parser, hence our best configuration achieved the
best performance of a single parser.
</bodyText>
<table confidence="0.9995707">
Answers Newsgroups Reviews Average
Co-training 77.18 82.72** 78.21 79.37
Clusters 78.04** 83.06* 79.03** 80.04
Wiktionary 77.61 82.8 78.32 79.57
Clusters+Wiktionary 78.19** 83.38* 79.36** 80.31
Co-training+Clusters 78.05* 83.29** 78.8** 80.04
Co-training+Clusters+Wiktionary 78.33** 83.35** 78.84* 80.17
Baseline 77.03 82.4 78.12 79.18
SANCL Stanford-2 77.5 83.56 79.7 80.25
SANCL Best (DCU-Paris13) 81.15 85.38 83.86 83.46
</table>
<tableCaption confidence="0.999882">
Table 15: Combination of co-training with word clusters and an external lexicon, SANCL test set.
</tableCaption>
<bodyText confidence="0.9999452">
The results on both the OntoNotes and SANCL datasets show that on their own, word clusters and co-
training often improve significantly on the baseline, but their combination results only in minor further
improvements (only up to 0.32%). Word clusters aim specifically to deal with the unknown vocabulary
problem, and, since there seem to be no major grammatical differences between the train and test do-
mains (see Section 3.3), it is likely that the main benefit derived from co-training is the compensation
for unknown domain vocabulary. Word clusters also seem a better way to approach this problem: they
perform better than co-training on three out of four subdomains. The explanation that unknown vocab-
ulary is the main issue for domain adaptation in this domain pair is further supported by the fact that
combinations of word clusters with a Wiktionary lexicon sometimes performed better than combinations
involving co-training (on newsgroups and reviews).
</bodyText>
<sectionHeader confidence="0.999335" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9974814">
In this paper we described experiments with several domain adaptation techniques, in order to quickly
adapt a general-language parser to parse web data. We find that the best combination of the techniques
improves significantly on the baseline (up to 1.67%), and achieves very promising results on the SANCL-
2012 shared task data, outperforming all submissions that used a single parser, in terms of labelled
accuracy score averaged across three test sets.
</bodyText>
<page confidence="0.997426">
62
</page>
<bodyText confidence="0.999951071428572">
Our experiments with word clusters showed that word clusters derived from unlabelled domain texts
consistently contribute to a greater parsing accuracy, and that both the domain relevance of the unlabelled
data and its quantity are major factors for successful exploitation of word clusters. Experiments with a
crowd-sourced PoS lexicon however were not as conclusive: whereas supplying the lexicon to the parser
often resulted in certain accuracy gains, they were not as large as those for word clusters. This suggests
word clusters created automatically from relevant domain texts are a better tool to deal with unknown
vocabulary than a generic hand-crafted and wide-coverage lexicon. Another interesting finding was
that co-training was most effective when the evaluation parser was not used for creating extra training
data (the so-called tri-training technique), and when removing very short sentences from automatically
labelled data before re-training the evaluation parser.
With respect to combining co-training with word clusters, we could not find clear evidence for additive
improvement. This suggests that co-training solves largely the same problem as word clusters, i.e.,
unknown target domain vocabulary, and that for the web texts under study unknown vocabulary is a much
more significant impediment for domain adaptation than grammatical differences between domains.
</bodyText>
<sectionHeader confidence="0.997658" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.674932">
The research was supported by FP7 ICT project “Workbench for Interactive Contrastive Analysis of
Patent Documentation” under grant no. FP7-SME-606163.
</bodyText>
<sectionHeader confidence="0.998135" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997699266666667">
Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency
parsing. In Proceedings of the 52nd annual meeting of the Association for Computational Linguistics.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of
the Eleventh Annual Conference on Computational Learning Theory, COLT’ 98, pages 92–100, New York, NY,
USA. ACM.
Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Rich´ard Farkas Filip Ginter, and Jan Hajic. 2013. Joint morpho-
logical and syntactic analysis for richly inflected languages. Transactions of the Associtation for Computational
Linguistics, 1.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18:467–479.
Alicia Burga, Joan Codina, Gabriella Ferraro, Horacio Saggion, and Leo Wanner. 2013. The challenge of syntactic
dependency parsing adaptation for the patent domain. In ESSLLI-13 Workshop on Extrinsic Parse Improvement.
Marie Candito, Enrique Henestroza Anguiano, and Djam Seddah. 2011. A word clustering approach to domain
adaptation: Effective parsing of biomedical texts. In IWPT, pages 37–42. The Association for Computational
Linguistics.
Jennifer Foster. 2010. ”cba to check the spelling”: Investigating parser performance on discussion forum posts. In
HLT-NAACL, pages 381–384. The Association for Computational Linguistics.
Phani Gadde, L. V. Subramaniam, and Tanveer A. Faruquie. 2011. Adapting a WSJ trained part-of-speech tag-
ger to noisy text: Preliminary results. In Proceedings of the 2011 Joint Workshop on Multilingual OCR and
Analytics for Noisy Unstructured Text Data, MOCRAND11, pages 51–58, New York, NY, USA. ACM.
Daniel Gildea. 2001. Corpus variation and parser performance. In Lillian Lee and Donna Harman, editors,
Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, EMNLP ’01,
pages 167–202, Stroudsburg. Association for Computational Linguistics.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez,
Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and
Yi Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.
In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5,
Boulder, Colorado, USA.
Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In 16th
Nordic Conference of Computational Linguistics, pages 105–112. University of Tartu.
</reference>
<page confidence="0.995055">
63
</page>
<reference confidence="0.997598574468085">
Mohammad Khan, Markus Dickinson, and Sandra K¨ubler. 2013a. Does size matter? text and grammar revision
for parsing social media data. In Proceedings of the Workshop on Language Analysis in Social Media, pages
1–10, Atlanta, Georgia, June. Association for Computational Linguistics.
Mohammad Khan, Markus Dickinson, and Sandra K¨ubler. 2013b. Towards domain adaptation for parsing web
data. In Galia Angelova, Kalina Bontcheva, and Ruslan Mitkov, editors, RANLP, pages 357–364. RANLP 2011
Organising Committee / ACL.
Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In In Proc.
ACL/HLT.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad Zadeh Kaljahi, and Anton Bryl. 2012. Dcu-
paris13 systems for the sancl 2012 shared task.
Matthew Lease and Eugene Charniak. 2005. Parsing biomedical literature. In Robert Dale, Kam-Fai Wong, Jian
Su, and Oi Yee Kwong, editors, IJCNLP, volume 3651 of Lecture Notes in Computer Science, pages 58–69.
Springer.
Shen Li, Joo Graa, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In EMNLP-CoNLL, pages
1389–1398. ACL.
Percy Liang. 2005. Semi-supervised learning for natural language. In MASTERS THESIS, MIT.
Andr´e FT Martins, Noah A Smith, Eric P Xing, Pedro MQ Aguiar, and M´ario AT Figueiredo. 2010. Turbo parsers:
Dependency parsing by approximate variational inference. In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages 34–44. Association for Computational Linguistics.
David McClosky and Eugene Charniak. 2008. Self-training for biomedical parsing. In ACL (Short Papers), pages
101–104. The Association for Computer Linguistics.
David McClosky, Wanxiang Che, Marta Recasens, Mengqiu Wang, Richard Socher, and Christopher Manning.
2012. Stanfords system for parsing the english web. In Workshop on the Syntactic Analysis of Non-Canonical
Language (SANCL 2012). Montreal, Canada.
Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algo-
rithms. In EACL.
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The conll 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915–932. sn.
Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Con-
ference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages 351–359. Association for Computational Linguistics.
Lilja Øvrelid and Arne Skjærholt. 2012. Lexical categories for improved parsing of web data. In Proceedings of
COLING 2012: Posters, pages 903–912, Mumbai, India, December. The COLING 2012 Organizing Committee.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013.
Improved part-of-speech tagging for online conversational text with word clusters. In HLT-NAACL, pages 380–
390. The Association for Computational Linguistics.
Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Notes of the
First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59.
Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate determin-
istic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 705–713, Stroudsburg, PA, USA. Association for Computational Linguistics.
Barbara Plank and Gertjan Van Noord. 2011. Effective measures of domain similarity for parsing. In Proceedings
of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-
Volume 1, pages 1566–1576. Association for Computational Linguistics.
Sampo Pyysalo, Tapio Salakoski, Sophie Aubin, and Adeline Nazarenko. 2006. Lexical adaptation of link
grammar to the biomedical sublanguage: a comparative evaluation of three approaches. BMC Bioinformat-
ics, 7(Suppl 3).
</reference>
<page confidence="0.986418">
64
</page>
<reference confidence="0.999773846153846">
Ines Rehbein. 2011. Data point selection for self-training. In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, SPMRL ’11, pages 62–67, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Kenji Sagae. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In In
Proceedings of the Eleventh Conference on Computational Natural Language Learning.
Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of the Second Meeting
of the North American Chapter of the Association for Computational Linguistics on Language Technologies,
NAACL ’01, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.
Anders Søgaard and Barbara Plank. 2012. Parsing the web as covariate shift. In Workshop on the Syntactic
Analysis of Non-Canonical Language (SANCL2012), Montreal, Canada.
Mark Steedman, Anoop Sarkar, Miles Osborne, Rebecca Hwa, Stephen Clark, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In EACL, pages
331–338. The Association for Computer Linguistics.
Peter Szolovits. 2003. Adding a medical lexicon to an English parser. In AMIA Annual Symposium Proceedings,
volume 2003, page 639. American Medical Informatics Association.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan T. McDonald, and Joakim Nivre. 2013. Token and type
constraints for cross-lingual part-of-speech tagging. TACL, 1:1–12.
Meishan Zhang, Wanxiang Che, Yijia Liu, Zhenghua Li, and Ting Liu. 2012. Hit dependency parsing: Bootstrap
aggregating heterogeneous parsers. In Notes of the First Workshop on Syntactic Analysis of Non-Canonical
Language (SANCL).
Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data using three classifiers. Knowledge and
Data Engineering, IEEE Transactions on, 17(11):1529–1541.
Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve
statistical dependency parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 1556–1565, Stroudsburg, PA, USA.
Association for Computational Linguistics.
</reference>
<page confidence="0.999615">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.549291">
<title confidence="0.999927">Exploring Options for Fast Domain Adaptation of Dependency Parsers</title>
<author confidence="0.9947">Viktor Pekar</author>
<author confidence="0.9947">Juntao Yu</author>
<author confidence="0.9947">Mohab El-karef</author>
<author confidence="0.9947">Bernd</author>
<affiliation confidence="0.9997835">School of Computer University of</affiliation>
<address confidence="0.558622">Birmingham,</address>
<abstract confidence="0.999069642857143">The paper explores different domain-independent techniques to adapt a dependency parser trained on a general-language corpus to parse web texts (online reviews, newsgroup posts, weblogs): co-training, word clusters, and a crowd-sourced dictionary. We examine the relative utility of these techniques as well as different ways to put them together to achieve maximum parsing accuracy. While we find that co-training and word clusters produce the most promising results, there is little additive improvement when combining the two techniques, which suggests that in the absence of large grammatical discrepancies between the training and test domains, they address largely the same problem, that of unknown vocabulary, with word clusters being a somewhat more effective solution for it. Our highest results were achieved by a combination of word clusters and co-training, significantly improving on the baseline, by up to 1.67%. Evaluation of the best configurations on the SANCL-2012 test data (Petrov and McDonald, 2012) showed that they outperform all the shared task submissions that used a single parser to parse test data, averaging the results across all the test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd annual meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4466" citStr="Bansal et al., 2014" startWordPosition="666" endWordPosition="669">d in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated texts (Zhou et al., 2011). The goal of this paper is to investigate a combination of such techniques to adapt a general-language parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual annotation. In our study we include several techniques that have been shown to be reasonably effective for domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon, as well as automatically annotated texts produced with the help of co-trai</context>
<context position="7453" citStr="Bansal et al. (2014)" startWordPosition="1135" endWordPosition="1138">e effective for out-of-domain parsing, where there are many words in the test data unseen during training, and word clusters computed from in-domain data similarly help to deal with the vocabulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts (Candito et al., 2011) and web texts (Øvrelid and Skjærholt, 2012). Word clusters created with Brown clustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013). Bansal et al. (2014) introduced continuous word representations and showed them to increase parsing accuracy both on the Penn Treebank and on web data. 55 2.4 Co-training Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification problem from a limited amount of labelled data and a large amount of unlabelled data, whereby two or more views on the data, i.e. feature subsets, or two or more different learning algorithms are employed that complement each other to bootstrap additional training data from the unlabelled dataset. Co-training algorithms have been successfully u</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the 52nd annual meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
<author>Tom Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT’ 98,</booktitle>
<pages>92--100</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="7641" citStr="Blum and Mitchell, 1998" startWordPosition="1163" endWordPosition="1166">e vocabulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts (Candito et al., 2011) and web texts (Øvrelid and Skjærholt, 2012). Word clusters created with Brown clustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013). Bansal et al. (2014) introduced continuous word representations and showed them to increase parsing accuracy both on the Penn Treebank and on web data. 55 2.4 Co-training Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification problem from a limited amount of labelled data and a large amount of unlabelled data, whereby two or more views on the data, i.e. feature subsets, or two or more different learning algorithms are employed that complement each other to bootstrap additional training data from the unlabelled dataset. Co-training algorithms have been successfully used in NLP tasks, and specifically for parsing. Sarkar (2001) showed the both precision and recall of a phrase structure parser can be increased using a co-training procedure that iterativ</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT’ 98, pages 92–100, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
<author>Igor Boguslavsky</author>
<author>Rich´ard Farkas Filip Ginter</author>
<author>Jan Hajic</author>
</authors>
<title>Joint morphological and syntactic analysis for richly inflected languages.</title>
<date>2013</date>
<journal>Transactions of the Associtation for Computational Linguistics,</journal>
<volume>1</volume>
<contexts>
<context position="2055" citStr="Bohnet et al., 2013" startWordPosition="301" endWordPosition="304">omain adaptation of a statistical dependency parser is a problem that is of much importance for many practical NLP applications. Previous research has shown that the accuracy of parsing significantly drops when a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomedical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al., 2013). In a preliminary experiment, we looked at the effect of cross-domain parsing on three state-ofthe-art parsers – Malt (Nivre, 2009), MST (McDonald and Pereira, 2006), and Mate parser (Bohnet et al., 2013) – trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotes v5.0 corpus as well as the in-domain CoNLL09 test set. The results (see Table 1) indicate that depending on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%. Domain MST MALT Mate Newswire 84.8 81.7 87.1 Pivot Texts 84.9 83.0 86.6 Broadcast News 79.4 78.1 81.2 Magazines 77.1 74.7 79.3 Broadcast Conversation 73.4 70.5 74.4 CoNLL09 test 86.9 84.7 90.1 Table 1: Labelled accuracy scores achieved by the MST, Malt, and Mate parsers trained on CoNLL09 data and tested </context>
<context position="9437" citStr="Bohnet et al., 2013" startWordPosition="1442" endWordPosition="1445"> interesting finding of their work was that the agreement between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabelled data where the other two learners agreed on the classification label. 3 Experimental set-up 3.1 Parsers In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira, 2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all the parsers were the same and came from the Mate parser. 3.2 Baseline As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain test data. 3.3 Data The experiments were conducted on annotated data on web-related domains available in the Ontonotes v.5 and SANCL datasets, since a large amount of unla</context>
</contexts>
<marker>Bohnet, Nivre, Boguslavsky, Ginter, Hajic, 2013</marker>
<rawString>Bernd Bohnet, Joakim Nivre, Igor Boguslavsky, Rich´ard Farkas Filip Ginter, and Jan Hajic. 2013. Joint morphological and syntactic analysis for richly inflected languages. Transactions of the Associtation for Computational Linguistics, 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--467</pages>
<contexts>
<context position="7138" citStr="Brown et al. (1992)" startWordPosition="1083" endWordPosition="1086">lexica have also been used to improve out-of-domain PoS tagging (Li et al., 2012). 2.3 Word clusters In order to reduce the amount of annotated data to train a dependency parser, Koo et al. (2008) used word clusters computed from unlabelled data as features for training a parser. The same approach has proved to be effective for out-of-domain parsing, where there are many words in the test data unseen during training, and word clusters computed from in-domain data similarly help to deal with the vocabulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts (Candito et al., 2011) and web texts (Øvrelid and Skjærholt, 2012). Word clusters created with Brown clustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013). Bansal et al. (2014) introduced continuous word representations and showed them to increase parsing accuracy both on the Penn Treebank and on web data. 55 2.4 Co-training Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification problem from a limited amount o</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18:467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alicia Burga</author>
<author>Joan Codina</author>
<author>Gabriella Ferraro</author>
<author>Horacio Saggion</author>
<author>Leo Wanner</author>
</authors>
<title>The challenge of syntactic dependency parsing adaptation for the patent domain.</title>
<date>2013</date>
<booktitle>In ESSLLI-13 Workshop on Extrinsic Parse Improvement.</booktitle>
<contexts>
<context position="1850" citStr="Burga et al., 2013" startWordPosition="268" endWordPosition="271"> data (Petrov and McDonald, 2012) showed that they outperform all the shared task submissions that used a single parser to parse test data, averaging the results across all the test sets. 1 Introduction Domain adaptation of a statistical dependency parser is a problem that is of much importance for many practical NLP applications. Previous research has shown that the accuracy of parsing significantly drops when a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomedical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al., 2013). In a preliminary experiment, we looked at the effect of cross-domain parsing on three state-ofthe-art parsers – Malt (Nivre, 2009), MST (McDonald and Pereira, 2006), and Mate parser (Bohnet et al., 2013) – trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotes v5.0 corpus as well as the in-domain CoNLL09 test set. The results (see Table 1) indicate that depending on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%. Domain MST MALT Mate Newswire 84.8 81.7 87.1 Pivot Texts 84.9 83.0 86.6 Broadcast News 79.4 78.1 81.2</context>
</contexts>
<marker>Burga, Codina, Ferraro, Saggion, Wanner, 2013</marker>
<rawString>Alicia Burga, Joan Codina, Gabriella Ferraro, Horacio Saggion, and Leo Wanner. 2013. The challenge of syntactic dependency parsing adaptation for the patent domain. In ESSLLI-13 Workshop on Extrinsic Parse Improvement.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Enrique Henestroza Anguiano</author>
<author>Djam Seddah</author>
</authors>
<title>A word clustering approach to domain adaptation: Effective parsing of biomedical texts.</title>
<date>2011</date>
<booktitle>In IWPT,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="4444" citStr="Candito et al., 2011" startWordPosition="662" endWordPosition="665">entences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated texts (Zhou et al., 2011). The goal of this paper is to investigate a combination of such techniques to adapt a general-language parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual annotation. In our study we include several techniques that have been shown to be reasonably effective for domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon, as well as automatically annotated texts produced wi</context>
<context position="7253" citStr="Candito et al., 2011" startWordPosition="1101" endWordPosition="1104">reduce the amount of annotated data to train a dependency parser, Koo et al. (2008) used word clusters computed from unlabelled data as features for training a parser. The same approach has proved to be effective for out-of-domain parsing, where there are many words in the test data unseen during training, and word clusters computed from in-domain data similarly help to deal with the vocabulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts (Candito et al., 2011) and web texts (Øvrelid and Skjærholt, 2012). Word clusters created with Brown clustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013). Bansal et al. (2014) introduced continuous word representations and showed them to increase parsing accuracy both on the Penn Treebank and on web data. 55 2.4 Co-training Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification problem from a limited amount of labelled data and a large amount of unlabelled data, whereby two or more views on the data, i.e. feature subsets,</context>
<context position="14861" citStr="Candito et al., 2011" startWordPosition="2310" endWordPosition="2313">.2 Word clusters We used Liang’s (2005) implementation of the Brown clustering algorithm to create clusters of words found in unlabelled domain texts. The output of the algorithm are word types assigned to discrete hierarchical clusters, with clusters assigned ids in the form of bit strings of varying length corresponding to clusters of different granularity. We experimentally set the maximum length of the bit string to 6, collapsing more fine-grained clusters. Instead of replacing the original word forms and/or PoS tags with cluster ids as was done in some previous studies (Koo et al., 2008; Candito et al., 2011; T¨ackstr¨om et al., 2013), the ids of clusters were used to generate additional features in the representations of the word forms, as this also produced better results in the preliminary runs. Below we describe experiments with several other parameters of the clustering algorithm. Number of clusters. As an input parameter, the Brown clustering algorithm requires a desired number of clusters. Initially discarding all word types with a count of less than 3, we experimented with different numbers of clusters and found that an optimal settings lies around 600 and 800 clusters, which gives an imp</context>
</contexts>
<marker>Candito, Anguiano, Seddah, 2011</marker>
<rawString>Marie Candito, Enrique Henestroza Anguiano, and Djam Seddah. 2011. A word clustering approach to domain adaptation: Effective parsing of biomedical texts. In IWPT, pages 37–42. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>cba to check the spelling”: Investigating parser performance on discussion forum posts.</title>
<date>2010</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>381--384</pages>
<contexts>
<context position="4196" citStr="Foster, 2010" startWordPosition="626" endWordPosition="627">utomatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated texts (Zhou et al., 2011). The goal of this paper is to investigate a combination of such techniques to adapt a general-language parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual annotation.</context>
<context position="5484" citStr="Foster (2010)" startWordPosition="816" endWordPosition="817">onably effective for domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon, as well as automatically annotated texts produced with the help of co-training. All these techniques are domain-independent and can be applied to new target domains given unlabelled texts form these domains. We explore the relative utility of these methods and ways to combine them for maximum parser accuracy. 2 Related work 2.1 Text normalization User-generated content on the web is notoriously low-quality, containing slang, abbreviations, inconsistent grammar and spelling. Foster (2010) investigated lexical phenomena that appear on online discussion forums that present common problems for parsing and compiled a list of such phenomena along with their transformations. Applying the transformations to test sentences helped to bring the F-score up by 2.7%. A similar approach was taken by Khan et al. (2013a) who found that it performed better than spelling correction based on the Levenshtein distance. Gadde et al. (2011) use a word clustering method and language modelling in order to align misspelled words with their regular spelling. Their method of cleaning noisy text helped to</context>
</contexts>
<marker>Foster, 2010</marker>
<rawString>Jennifer Foster. 2010. ”cba to check the spelling”: Investigating parser performance on discussion forum posts. In HLT-NAACL, pages 381–384. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phani Gadde</author>
<author>L V Subramaniam</author>
<author>Tanveer A Faruquie</author>
</authors>
<title>Adapting a WSJ trained part-of-speech tagger to noisy text: Preliminary results.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Joint Workshop on Multilingual OCR and Analytics for Noisy Unstructured Text Data, MOCRAND11,</booktitle>
<pages>51--58</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="5922" citStr="Gadde et al. (2011)" startWordPosition="883" endWordPosition="886">2 Related work 2.1 Text normalization User-generated content on the web is notoriously low-quality, containing slang, abbreviations, inconsistent grammar and spelling. Foster (2010) investigated lexical phenomena that appear on online discussion forums that present common problems for parsing and compiled a list of such phenomena along with their transformations. Applying the transformations to test sentences helped to bring the F-score up by 2.7%. A similar approach was taken by Khan et al. (2013a) who found that it performed better than spelling correction based on the Levenshtein distance. Gadde et al. (2011) use a word clustering method and language modelling in order to align misspelled words with their regular spelling. Their method of cleaning noisy text helped to increase the accuracy of PoS tagging of SMS data by 3.5%. 2.2 External lexica To adapt the Link parser to the medical domain, Szolovitz (2003) extended its lexicon with terms from the UMLS Specialist Lexicon. Pyysalo et al. (2006) take the same approach and together with predicting the PoS tags for out-of-vocabulary words based on their morphology this allowed them to achieve a 10% reduction in the error rate of parsing. External lex</context>
</contexts>
<marker>Gadde, Subramaniam, Faruquie, 2011</marker>
<rawString>Phani Gadde, L. V. Subramaniam, and Tanveer A. Faruquie. 2011. Adapting a WSJ trained part-of-speech tagger to noisy text: Preliminary results. In Proceedings of the 2011 Joint Workshop on Multilingual OCR and Analytics for Noisy Unstructured Text Data, MOCRAND11, pages 51–58, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, EMNLP ’01,</booktitle>
<pages>167--202</pages>
<editor>In Lillian Lee and Donna Harman, editors,</editor>
<publisher>Association for Computational Linguistics.</publisher>
<location>Stroudsburg.</location>
<contexts>
<context position="1734" citStr="Gildea, 2001" startWordPosition="251" endWordPosition="252">cantly improving on the baseline, by up to 1.67%. Evaluation of the best configurations on the SANCL-2012 test data (Petrov and McDonald, 2012) showed that they outperform all the shared task submissions that used a single parser to parse test data, averaging the results across all the test sets. 1 Introduction Domain adaptation of a statistical dependency parser is a problem that is of much importance for many practical NLP applications. Previous research has shown that the accuracy of parsing significantly drops when a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomedical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al., 2013). In a preliminary experiment, we looked at the effect of cross-domain parsing on three state-ofthe-art parsers – Malt (Nivre, 2009), MST (McDonald and Pereira, 2006), and Mate parser (Bohnet et al., 2013) – trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotes v5.0 corpus as well as the in-domain CoNLL09 test set. The results (see Table 1) indicate that depending on the application domain, the parsing accuracy can suffer an absolute drop of a</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Lillian Lee and Donna Harman, editors, Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, EMNLP ’01, pages 167–202, Stroudsburg. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In 16th Nordic Conference of Computational Linguistics,</booktitle>
<pages>105--112</pages>
<institution>University of Tartu.</institution>
<contexts>
<context position="10708" citStr="Johansson and Nugues, 2007" startWordPosition="1646" endWordPosition="1649">ion techniques is widely available. OntoNotes. In experiments with weblog texts, we used the CoNLL09 training dataset (Hajiˇc et al., 2009) as the general-language training data. The CoNLL09 test dataset was used to evaluate in-domain parsing. To create an out-of-domain test set, we selected the last 10% of the weblogs section of the OntoNotes v5.0 corpus1, in order to make the size of the out-of-domain test data comparable to that of the in-domain test data, i.e. of CoNLL09 test. The OntoNotes corpus was converted to the CoNLL09 format using the LTH constituent-to-dependency conversion tool (Johansson and Nugues, 2007). SANCL. In order to compare our results with the results achieved by participants in the SANCL-2012 shared task, we also ran experiments on the Stanford dependences of three SANCL test sets (answers, newsgroups and reviews). In these experiments we used the training set, test sets, unlabelled data, as well as the evaluation script provided by SANCL-2012 organizers (Petrov and McDonald, 2012). Tables 2 and 3 show the sizes of the OntoNotes and SANCL datasets as well as several measures of lexical and grammatical characteristics of the data. The average sentence length (in tokens) and the avera</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In 16th Nordic Conference of Computational Linguistics, pages 105–112. University of Tartu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Khan</author>
<author>Markus Dickinson</author>
<author>Sandra K¨ubler</author>
</authors>
<title>Does size matter? text and grammar revision for parsing social media data.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Language Analysis in Social Media,</booktitle>
<pages>1--10</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>Khan, Dickinson, K¨ubler, 2013</marker>
<rawString>Mohammad Khan, Markus Dickinson, and Sandra K¨ubler. 2013a. Does size matter? text and grammar revision for parsing social media data. In Proceedings of the Workshop on Language Analysis in Social Media, pages 1–10, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Khan</author>
<author>Markus Dickinson</author>
<author>Sandra K¨ubler</author>
</authors>
<title>Towards domain adaptation for parsing web data.</title>
<date>2013</date>
<booktitle>RANLP 2011 Organising Committee / ACL.</booktitle>
<pages>357--364</pages>
<editor>In Galia Angelova, Kalina Bontcheva, and Ruslan Mitkov, editors, RANLP,</editor>
<marker>Khan, Dickinson, K¨ubler, 2013</marker>
<rawString>Mohammad Khan, Markus Dickinson, and Sandra K¨ubler. 2013b. Towards domain adaptation for parsing web data. In Galia Angelova, Kalina Bontcheva, and Ruslan Mitkov, editors, RANLP, pages 357–364. RANLP 2011 Organising Committee / ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing. In</title>
<date>2008</date>
<booktitle>In Proc. ACL/HLT.</booktitle>
<contexts>
<context position="6715" citStr="Koo et al. (2008)" startWordPosition="1016" endWordPosition="1019">accuracy of PoS tagging of SMS data by 3.5%. 2.2 External lexica To adapt the Link parser to the medical domain, Szolovitz (2003) extended its lexicon with terms from the UMLS Specialist Lexicon. Pyysalo et al. (2006) take the same approach and together with predicting the PoS tags for out-of-vocabulary words based on their morphology this allowed them to achieve a 10% reduction in the error rate of parsing. External lexica have also been used to improve out-of-domain PoS tagging (Li et al., 2012). 2.3 Word clusters In order to reduce the amount of annotated data to train a dependency parser, Koo et al. (2008) used word clusters computed from unlabelled data as features for training a parser. The same approach has proved to be effective for out-of-domain parsing, where there are many words in the test data unseen during training, and word clusters computed from in-domain data similarly help to deal with the vocabulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts (Candito et al., 2011) and web texts (Øvrelid and Skjærholt, 2012). Word clusters cr</context>
<context position="14839" citStr="Koo et al., 2008" startWordPosition="2306" endWordPosition="2309">ernet speak”. 57 4.2 Word clusters We used Liang’s (2005) implementation of the Brown clustering algorithm to create clusters of words found in unlabelled domain texts. The output of the algorithm are word types assigned to discrete hierarchical clusters, with clusters assigned ids in the form of bit strings of varying length corresponding to clusters of different granularity. We experimentally set the maximum length of the bit string to 6, collapsing more fine-grained clusters. Instead of replacing the original word forms and/or PoS tags with cluster ids as was done in some previous studies (Koo et al., 2008; Candito et al., 2011; T¨ackstr¨om et al., 2013), the ids of clusters were used to generate additional features in the representations of the word forms, as this also produced better results in the preliminary runs. Below we describe experiments with several other parameters of the clustering algorithm. Number of clusters. As an input parameter, the Brown clustering algorithm requires a desired number of clusters. Initially discarding all word types with a count of less than 3, we experimented with different numbers of clusters and found that an optimal settings lies around 600 and 800 cluste</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In In Proc. ACL/HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Le Roux</author>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Rasul Samad Zadeh Kaljahi</author>
<author>Anton Bryl</author>
</authors>
<date>2012</date>
<booktitle>Dcuparis13 systems for the sancl</booktitle>
<note>shared task.</note>
<marker>Le Roux, Foster, Wagner, Kaljahi, Bryl, 2012</marker>
<rawString>Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad Zadeh Kaljahi, and Anton Bryl. 2012. Dcuparis13 systems for the sancl 2012 shared task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Lease</author>
<author>Eugene Charniak</author>
</authors>
<title>Parsing biomedical literature.</title>
<date>2005</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>3651</volume>
<pages>58--69</pages>
<editor>In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors, IJCNLP,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1779" citStr="Lease and Charniak, 2005" startWordPosition="256" endWordPosition="259"> by up to 1.67%. Evaluation of the best configurations on the SANCL-2012 test data (Petrov and McDonald, 2012) showed that they outperform all the shared task submissions that used a single parser to parse test data, averaging the results across all the test sets. 1 Introduction Domain adaptation of a statistical dependency parser is a problem that is of much importance for many practical NLP applications. Previous research has shown that the accuracy of parsing significantly drops when a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomedical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al., 2013). In a preliminary experiment, we looked at the effect of cross-domain parsing on three state-ofthe-art parsers – Malt (Nivre, 2009), MST (McDonald and Pereira, 2006), and Mate parser (Bohnet et al., 2013) – trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotes v5.0 corpus as well as the in-domain CoNLL09 test set. The results (see Table 1) indicate that depending on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%. Domain MST MALT Mate Newswire </context>
</contexts>
<marker>Lease, Charniak, 2005</marker>
<rawString>Matthew Lease and Eugene Charniak. 2005. Parsing biomedical literature. In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee Kwong, editors, IJCNLP, volume 3651 of Lecture Notes in Computer Science, pages 58–69. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shen Li</author>
<author>Joo Graa</author>
<author>Ben Taskar</author>
</authors>
<title>Wiki-ly supervised part-of-speech tagging.</title>
<date>2012</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>1389--1398</pages>
<publisher>ACL.</publisher>
<contexts>
<context position="6600" citStr="Li et al., 2012" startWordPosition="995" endWordPosition="998">to align misspelled words with their regular spelling. Their method of cleaning noisy text helped to increase the accuracy of PoS tagging of SMS data by 3.5%. 2.2 External lexica To adapt the Link parser to the medical domain, Szolovitz (2003) extended its lexicon with terms from the UMLS Specialist Lexicon. Pyysalo et al. (2006) take the same approach and together with predicting the PoS tags for out-of-vocabulary words based on their morphology this allowed them to achieve a 10% reduction in the error rate of parsing. External lexica have also been used to improve out-of-domain PoS tagging (Li et al., 2012). 2.3 Word clusters In order to reduce the amount of annotated data to train a dependency parser, Koo et al. (2008) used word clusters computed from unlabelled data as features for training a parser. The same approach has proved to be effective for out-of-domain parsing, where there are many words in the test data unseen during training, and word clusters computed from in-domain data similarly help to deal with the vocabulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et al. (1992) method have been shown to be beneficial for adapting dependen</context>
<context position="19614" citStr="Li et al., 2012" startWordPosition="3064" endWordPosition="3067">types with count less than 3. 4.3 External lexicon It is possible to supply to the dependency parser an external lexicon, where word forms are provided with PoS tags. Wiktionary, a companion project for Wikipedia that aims to produce a free, large-scale multilingual dictionary, is a large and constantly growing crowd-sourced resource that appears attractive for NLP research. Wiktionary encodes word definitions, pronunciation, translations, etymology, word forms and part-of-speech information. PoS tag dictionaries derived from Wiktionary have been previously used for out-of-domain PoS tagging (Li et al., 2012) and for PoS tagging of resource-poor languages (T¨ackstr¨om et al., 2013). To create a lexicon for the parser, we extracted 753,970 English word forms and their PoS tags from a dump of Wiktionary4. Wiktionary uses a rather informal set of PoS labels; to convert them to the CoNLL09 tag set, we manually aligned all unique PoS tags found in Wiktionary with those of the CoNLL09 tag set. We compared the accuracy achieved by the parser when the lexicon was supplied, as well as when the lexicon was supplied together with the best configuration word clusters (800 clusters built from the entire SANCL </context>
</contexts>
<marker>Li, Graa, Taskar, 2012</marker>
<rawString>Shen Li, Joo Graa, and Ben Taskar. 2012. Wiki-ly supervised part-of-speech tagging. In EMNLP-CoNLL, pages 1389–1398. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
</authors>
<title>Semi-supervised learning for natural language. In</title>
<date>2005</date>
<tech>MASTERS THESIS, MIT.</tech>
<marker>Liang, 2005</marker>
<rawString>Percy Liang. 2005. Semi-supervised learning for natural language. In MASTERS THESIS, MIT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e FT Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro MQ Aguiar</author>
<author>M´ario AT Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>34--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9494" citStr="Martins et al., 2010" startWordPosition="1451" endWordPosition="1454">t between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabelled data where the other two learners agreed on the classification label. 3 Experimental set-up 3.1 Parsers In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira, 2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all the parsers were the same and came from the Mate parser. 3.2 Baseline As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain test data. 3.3 Data The experiments were conducted on annotated data on web-related domains available in the Ontonotes v.5 and SANCL datasets, since a large amount of unlabelled data required for most domain adaptation technique</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andr´e FT Martins, Noah A Smith, Eric P Xing, Pedro MQ Aguiar, and M´ario AT Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 34–44. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>Self-training for biomedical parsing.</title>
<date>2008</date>
<booktitle>In ACL (Short Papers),</booktitle>
<pages>101--104</pages>
<institution>The Association for Computer Linguistics.</institution>
<contexts>
<context position="3744" citStr="McClosky and Charniak, 2008" startWordPosition="556" endWordPosition="559">kshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014. parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), dis</context>
</contexts>
<marker>McClosky, Charniak, 2008</marker>
<rawString>David McClosky and Eugene Charniak. 2008. Self-training for biomedical parsing. In ACL (Short Papers), pages 101–104. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Wanxiang Che</author>
<author>Marta Recasens</author>
<author>Mengqiu Wang</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Stanfords system for parsing the english web.</title>
<date>2012</date>
<booktitle>In Workshop on the Syntactic Analysis of Non-Canonical Language (SANCL 2012).</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="27732" citStr="McClosky et al., 2012" startWordPosition="4331" endWordPosition="4334">achieved by combinations of techniques, in all the three subdomains, improving on the baseline by up to 1.3%. Comparing the results achieved by our best configurations with the results of the shared task, we see that our labelled accuracy averaged across the subdomains was just above the Stanford-2 system (80.31 vs. 80.25), which ranked 5th of all the twelve submissions (Petrov and McDonald, 2012). Although our results are still 3.15% lower than DCU-Paris13, the best system at SANCL-2012, the top four results were all generated by combination systems (Le Roux et al., 2012; Zhang et al., 2012; McClosky et al., 2012); our highest results only produced by the Mate parser, hence our best configuration achieved the best performance of a single parser. Answers Newsgroups Reviews Average Co-training 77.18 82.72** 78.21 79.37 Clusters 78.04** 83.06* 79.03** 80.04 Wiktionary 77.61 82.8 78.32 79.57 Clusters+Wiktionary 78.19** 83.38* 79.36** 80.31 Co-training+Clusters 78.05* 83.29** 78.8** 80.04 Co-training+Clusters+Wiktionary 78.33** 83.35** 78.84* 80.17 Baseline 77.03 82.4 78.12 79.18 SANCL Stanford-2 77.5 83.56 79.7 80.25 SANCL Best (DCU-Paris13) 81.15 85.38 83.86 83.46 Table 15: Combination of co-training with</context>
</contexts>
<marker>McClosky, Che, Recasens, Wang, Socher, Manning, 2012</marker>
<rawString>David McClosky, Wanxiang Che, Marta Recasens, Mengqiu Wang, Richard Socher, and Christopher Manning. 2012. Stanfords system for parsing the english web. In Workshop on the Syntactic Analysis of Non-Canonical Language (SANCL 2012). Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="2016" citStr="McDonald and Pereira, 2006" startWordPosition="294" endWordPosition="297">lts across all the test sets. 1 Introduction Domain adaptation of a statistical dependency parser is a problem that is of much importance for many practical NLP applications. Previous research has shown that the accuracy of parsing significantly drops when a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomedical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al., 2013). In a preliminary experiment, we looked at the effect of cross-domain parsing on three state-ofthe-art parsers – Malt (Nivre, 2009), MST (McDonald and Pereira, 2006), and Mate parser (Bohnet et al., 2013) – trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotes v5.0 corpus as well as the in-domain CoNLL09 test set. The results (see Table 1) indicate that depending on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%. Domain MST MALT Mate Newswire 84.8 81.7 87.1 Pivot Texts 84.9 83.0 86.6 Broadcast News 79.4 78.1 81.2 Magazines 77.1 74.7 79.3 Broadcast Conversation 73.4 70.5 74.4 CoNLL09 test 86.9 84.7 90.1 Table 1: Labelled accuracy scores achieved by the MST, Malt, and Mate pars</context>
<context position="9381" citStr="McDonald and Pereira, 2006" startWordPosition="1434" endWordPosition="1437">ain track of the CoNLL07 shared task (Nilsson et al., 2007). An interesting finding of their work was that the agreement between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabelled data where the other two learners agreed on the classification label. 3 Experimental set-up 3.1 Parsers In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira, 2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all the parsers were the same and came from the Mate parser. 3.2 Baseline As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain test data. 3.3 Data The experiments were conducted on annotated data on web-related domains available in the Ontono</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The conll</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL,</booktitle>
<pages>915--932</pages>
<contexts>
<context position="8813" citStr="Nilsson et al., 2007" startWordPosition="1347" endWordPosition="1350">eased using a co-training procedure that iteratively adds the most confidently parsed sentences from two different views to the training set. Steedman et al.(2003) used two different parsers that supplied training data to each other in a bootstrapping manner. A number of studies specifically aimed to use co-training for domain adaptation of a dependency parser. Sagae (2007) used two different learning algorithms of their graph-based parser to complete a one iteration of co-training, getting an improvement of 2-3%, which was the best result on the out-ofdomain track of the CoNLL07 shared task (Nilsson et al., 2007). An interesting finding of their work was that the agreement between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabelled data where the other two learners agreed on the classification label. 3 Experimental set-up 3.1 Parsers In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira, 2006), the transition-based Mate pars</context>
</contexts>
<marker>Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The conll 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL, pages 915–932. sn.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Non-projective dependency parsing in expected linear time.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>351--359</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1982" citStr="Nivre, 2009" startWordPosition="291" endWordPosition="292"> averaging the results across all the test sets. 1 Introduction Domain adaptation of a statistical dependency parser is a problem that is of much importance for many practical NLP applications. Previous research has shown that the accuracy of parsing significantly drops when a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomedical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al., 2013). In a preliminary experiment, we looked at the effect of cross-domain parsing on three state-ofthe-art parsers – Malt (Nivre, 2009), MST (McDonald and Pereira, 2006), and Mate parser (Bohnet et al., 2013) – trained on the CoNLL09 dataset and tested on texts from different domains in the OntoNotes v5.0 corpus as well as the in-domain CoNLL09 test set. The results (see Table 1) indicate that depending on the application domain, the parsing accuracy can suffer an absolute drop of as much as 16%. Domain MST MALT Mate Newswire 84.8 81.7 87.1 Pivot Texts 84.9 83.0 86.6 Broadcast News 79.4 78.1 81.2 Magazines 77.1 74.7 79.3 Broadcast Conversation 73.4 70.5 74.4 CoNLL09 test 86.9 84.7 90.1 Table 1: Labelled accuracy scores achiev</context>
<context position="9336" citStr="Nivre, 2009" startWordPosition="1429" endWordPosition="1430">e best result on the out-ofdomain track of the CoNLL07 shared task (Nilsson et al., 2007). An interesting finding of their work was that the agreement between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabelled data where the other two learners agreed on the classification label. 3 Experimental set-up 3.1 Parsers In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira, 2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all the parsers were the same and came from the Mate parser. 3.2 Baseline As the baseline we used the Mate parser, as it showed the highest accuracy when no domain adaptation techniques were used, i.e. trained on an in-domain training dataset and applied directly to out-of-domain test data. 3.3 Data The experiments were conducted on annotated data o</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>Joakim Nivre. 2009. Non-projective dependency parsing in expected linear time. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 351–359. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lilja Øvrelid</author>
<author>Arne Skjærholt</author>
</authors>
<title>Lexical categories for improved parsing of web data.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012: Posters,</booktitle>
<pages>903--912</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="7297" citStr="Øvrelid and Skjærholt, 2012" startWordPosition="1108" endWordPosition="1111">o train a dependency parser, Koo et al. (2008) used word clusters computed from unlabelled data as features for training a parser. The same approach has proved to be effective for out-of-domain parsing, where there are many words in the test data unseen during training, and word clusters computed from in-domain data similarly help to deal with the vocabulary discrepancies between the training and test datasets. Discrete word clusters produced by Brown et al. (1992) method have been shown to be beneficial for adapting dependency parsers to biomedical texts (Candito et al., 2011) and web texts (Øvrelid and Skjærholt, 2012). Word clusters created with Brown clustering method have also been used to adapt a PoS tagger to Twitter posts (Owoputi et al., 2013). Bansal et al. (2014) introduced continuous word representations and showed them to increase parsing accuracy both on the Penn Treebank and on web data. 55 2.4 Co-training Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification problem from a limited amount of labelled data and a large amount of unlabelled data, whereby two or more views on the data, i.e. feature subsets, or two or more different learning algorithm</context>
</contexts>
<marker>Øvrelid, Skjærholt, 2012</marker>
<rawString>Lilja Øvrelid and Arne Skjærholt. 2012. Lexical categories for improved parsing of web data. In Proceedings of COLING 2012: Posters, pages 903–912, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In HLT-NAACL, pages 380– 390. The Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
</authors>
<title>Overview of the 2012 shared task on parsing the web.</title>
<date>2012</date>
<booktitle>In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),</booktitle>
<volume>59</volume>
<contexts>
<context position="1264" citStr="Petrov and McDonald, 2012" startWordPosition="176" endWordPosition="179">ind that co-training and word clusters produce the most promising results, there is little additive improvement when combining the two techniques, which suggests that in the absence of large grammatical discrepancies between the training and test domains, they address largely the same problem, that of unknown vocabulary, with word clusters being a somewhat more effective solution for it. Our highest results were achieved by a combination of word clusters and co-training, significantly improving on the baseline, by up to 1.67%. Evaluation of the best configurations on the SANCL-2012 test data (Petrov and McDonald, 2012) showed that they outperform all the shared task submissions that used a single parser to parse test data, averaging the results across all the test sets. 1 Introduction Domain adaptation of a statistical dependency parser is a problem that is of much importance for many practical NLP applications. Previous research has shown that the accuracy of parsing significantly drops when a general-language model is applied to narrow domains like financial news (Gildea, 2001), biomedical texts (Lease and Charniak, 2005), web data (Petrov and McDonald, 2012), or patents (Burga et al., 2013). In a prelimi</context>
<context position="11103" citStr="Petrov and McDonald, 2012" startWordPosition="1707" endWordPosition="1710">-of-domain test data comparable to that of the in-domain test data, i.e. of CoNLL09 test. The OntoNotes corpus was converted to the CoNLL09 format using the LTH constituent-to-dependency conversion tool (Johansson and Nugues, 2007). SANCL. In order to compare our results with the results achieved by participants in the SANCL-2012 shared task, we also ran experiments on the Stanford dependences of three SANCL test sets (answers, newsgroups and reviews). In these experiments we used the training set, test sets, unlabelled data, as well as the evaluation script provided by SANCL-2012 organizers (Petrov and McDonald, 2012). Tables 2 and 3 show the sizes of the OntoNotes and SANCL datasets as well as several measures of lexical and grammatical characteristics of the data. The average sentence length (in tokens) and the average number of subjects, roughly corresponding to the number of clauses in the sentence, aim to characterize the syntactic complexity of the sentences: the higher these values, the more complex the 1https://catalog.ldc.upenn.edu/LDC2013T19 56 structure of the sentences is likely to be. The ratio of word forms absent from training data describes how different the train and test data are in terms</context>
<context position="27510" citStr="Petrov and McDonald, 2012" startWordPosition="4294" endWordPosition="4297">ry 78.89* Co-training+Clusters+Wiktionary 79.19** Baseline 77.54 Table 14: Combination of co-training with word clusters and an external lexicon, OntoNotes test set. improvements on the baseline. The best accuracy overall was achieved by combinations of techniques, in all the three subdomains, improving on the baseline by up to 1.3%. Comparing the results achieved by our best configurations with the results of the shared task, we see that our labelled accuracy averaged across the subdomains was just above the Stanford-2 system (80.31 vs. 80.25), which ranked 5th of all the twelve submissions (Petrov and McDonald, 2012). Although our results are still 3.15% lower than DCU-Paris13, the best system at SANCL-2012, the top four results were all generated by combination systems (Le Roux et al., 2012; Zhang et al., 2012; McClosky et al., 2012); our highest results only produced by the Mate parser, hence our best configuration achieved the best performance of a single parser. Answers Newsgroups Reviews Average Co-training 77.18 82.72** 78.21 79.37 Clusters 78.04** 83.06* 79.03** 80.04 Wiktionary 77.61 82.8 78.32 79.57 Clusters+Wiktionary 78.19** 83.38* 79.36** 80.31 Co-training+Clusters 78.05* 83.29** 78.8** 80.04 </context>
</contexts>
<marker>Petrov, McDonald, 2012</marker>
<rawString>Slav Petrov and Ryan McDonald. 2012. Overview of the 2012 shared task on parsing the web. In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Pi-Chuan Chang</author>
<author>Michael Ringgaard</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Uptraining for accurate deterministic question parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>705--713</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3798" citStr="Petrov et al., 2010" startWordPosition="564" endWordPosition="567">s and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014. parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabe</context>
</contexts>
<marker>Petrov, Chang, Ringgaard, Alshawi, 2010</marker>
<rawString>Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, and Hiyan Alshawi. 2010. Uptraining for accurate deterministic question parsing. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 705–713, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Plank</author>
<author>Gertjan Van Noord</author>
</authors>
<title>Effective measures of domain similarity for parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1,</booktitle>
<pages>1566--1576</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Plank, Van Noord, 2011</marker>
<rawString>Barbara Plank and Gertjan Van Noord. 2011. Effective measures of domain similarity for parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1, pages 1566–1576. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sampo Pyysalo</author>
<author>Tapio Salakoski</author>
<author>Sophie Aubin</author>
<author>Adeline Nazarenko</author>
</authors>
<title>Lexical adaptation of link grammar to the biomedical sublanguage: a comparative evaluation of three approaches.</title>
<date>2006</date>
<journal>BMC Bioinformatics,</journal>
<volume>7</volume>
<contexts>
<context position="4339" citStr="Pyysalo et al., 2006" startWordPosition="647" endWordPosition="650">cClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated texts (Zhou et al., 2011). The goal of this paper is to investigate a combination of such techniques to adapt a general-language parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual annotation. In our study we include several techniques that have been shown to be reasonably effective for domain adaptation: text normalization, the use </context>
<context position="6315" citStr="Pyysalo et al. (2006)" startWordPosition="948" endWordPosition="951">ons to test sentences helped to bring the F-score up by 2.7%. A similar approach was taken by Khan et al. (2013a) who found that it performed better than spelling correction based on the Levenshtein distance. Gadde et al. (2011) use a word clustering method and language modelling in order to align misspelled words with their regular spelling. Their method of cleaning noisy text helped to increase the accuracy of PoS tagging of SMS data by 3.5%. 2.2 External lexica To adapt the Link parser to the medical domain, Szolovitz (2003) extended its lexicon with terms from the UMLS Specialist Lexicon. Pyysalo et al. (2006) take the same approach and together with predicting the PoS tags for out-of-vocabulary words based on their morphology this allowed them to achieve a 10% reduction in the error rate of parsing. External lexica have also been used to improve out-of-domain PoS tagging (Li et al., 2012). 2.3 Word clusters In order to reduce the amount of annotated data to train a dependency parser, Koo et al. (2008) used word clusters computed from unlabelled data as features for training a parser. The same approach has proved to be effective for out-of-domain parsing, where there are many words in the test data</context>
</contexts>
<marker>Pyysalo, Salakoski, Aubin, Nazarenko, 2006</marker>
<rawString>Sampo Pyysalo, Tapio Salakoski, Sophie Aubin, and Adeline Nazarenko. 2006. Lexical adaptation of link grammar to the biomedical sublanguage: a comparative evaluation of three approaches. BMC Bioinformatics, 7(Suppl 3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ines Rehbein</author>
</authors>
<title>Data point selection for self-training.</title>
<date>2011</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Parsing of Morphologically Rich Languages, SPMRL ’11,</booktitle>
<pages>62--67</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3760" citStr="Rehbein, 2011" startWordPosition="560" endWordPosition="561">of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014. parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continu</context>
</contexts>
<marker>Rehbein, 2011</marker>
<rawString>Ines Rehbein. 2011. Data point selection for self-training. In Proceedings of the Second Workshop on Statistical Parsing of Morphologically Rich Languages, SPMRL ’11, pages 62–67, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
</authors>
<title>Dependency parsing and domain adaptation with lr models and parser ensembles. In</title>
<date>2007</date>
<booktitle>In Proceedings of the Eleventh Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="8568" citStr="Sagae (2007)" startWordPosition="1309" endWordPosition="1310">tional training data from the unlabelled dataset. Co-training algorithms have been successfully used in NLP tasks, and specifically for parsing. Sarkar (2001) showed the both precision and recall of a phrase structure parser can be increased using a co-training procedure that iteratively adds the most confidently parsed sentences from two different views to the training set. Steedman et al.(2003) used two different parsers that supplied training data to each other in a bootstrapping manner. A number of studies specifically aimed to use co-training for domain adaptation of a dependency parser. Sagae (2007) used two different learning algorithms of their graph-based parser to complete a one iteration of co-training, getting an improvement of 2-3%, which was the best result on the out-ofdomain track of the CoNLL07 shared task (Nilsson et al., 2007). An interesting finding of their work was that the agreement between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabel</context>
<context position="21314" citStr="Sagae (2007)" startWordPosition="3329" endWordPosition="3330">us the best combination of domain adaptation techniques so far included the use of 800 word clusters built from the entire SANCL unlabelled dataset, after filtering out word forms with the count less than 3, with text normalization, but without the Wiktionary lexicon (+1.15% on the baseline). 3http://www.epo.org/searching/free/espacenet.html 4http://wiki.dbpedia.org/Wiktionary 59 CoNLL09 OntoNotes Wiktionary 90.22 77.73 Clusters 90.62 ** 78.69 ** Wiktionary+Clusters 90.44 78.49 ** Baseline 90.07 77.54 Table 9: The effect of the Wiktionary lexicon on parsing accuracy. 4.4 Co-Training Following Sagae (2007), the overall approach to parser co-training we adopted was as follows. First, several parsers were combined to generate additional training data from unlabelled data, i.e. were used as source learners for co-training. Then, the Mate parser was re-trained on the augmented training set and tested on a test set, i.e. used as the evaluation learner. The reason Mate was selected the evaluation learner was that it achieved the best results on the test data in its default settings (see Table 10). CoNLL09 OntoNotes Mate 90.07 77.54 MST 86.9 75.35 Turbo 85.94 74.85 Malt 84.72 72.63 Table 10: The basel</context>
</contexts>
<marker>Sagae, 2007</marker>
<rawString>Kenji Sagae. 2007. Dependency parsing and domain adaptation with lr models and parser ensembles. In In Proceedings of the Eleventh Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anoop Sarkar</author>
</authors>
<title>Applying co-training methods to statistical parsing.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies, NAACL ’01,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3676" citStr="Sarkar, 2001" startWordPosition="549" endWordPosition="550">eativecommons.org/licenses/by/4.0/ 54 First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014. parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unkn</context>
<context position="8114" citStr="Sarkar (2001)" startWordPosition="1238" endWordPosition="1239">showed them to increase parsing accuracy both on the Penn Treebank and on web data. 55 2.4 Co-training Co-training (Blum and Mitchell, 1998) is a paradigm for weakly supervised learning of a classification problem from a limited amount of labelled data and a large amount of unlabelled data, whereby two or more views on the data, i.e. feature subsets, or two or more different learning algorithms are employed that complement each other to bootstrap additional training data from the unlabelled dataset. Co-training algorithms have been successfully used in NLP tasks, and specifically for parsing. Sarkar (2001) showed the both precision and recall of a phrase structure parser can be increased using a co-training procedure that iteratively adds the most confidently parsed sentences from two different views to the training set. Steedman et al.(2003) used two different parsers that supplied training data to each other in a bootstrapping manner. A number of studies specifically aimed to use co-training for domain adaptation of a dependency parser. Sagae (2007) used two different learning algorithms of their graph-based parser to complete a one iteration of co-training, getting an improvement of 2-3%, wh</context>
</contexts>
<marker>Sarkar, 2001</marker>
<rawString>Anoop Sarkar. 2001. Applying co-training methods to statistical parsing. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics on Language Technologies, NAACL ’01, pages 1–8, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Søgaard</author>
<author>Barbara Plank</author>
</authors>
<title>Parsing the web as covariate shift.</title>
<date>2012</date>
<booktitle>In Workshop on the Syntactic Analysis of Non-Canonical Language (SANCL2012),</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="3951" citStr="Søgaard and Plank, 2012" startWordPosition="588" endWordPosition="591">ain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated</context>
</contexts>
<marker>Søgaard, Plank, 2012</marker>
<rawString>Anders Søgaard and Barbara Plank. 2012. Parsing the web as covariate shift. In Workshop on the Syntactic Analysis of Non-Canonical Language (SANCL2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Anoop Sarkar</author>
<author>Miles Osborne</author>
<author>Rebecca Hwa</author>
<author>Stephen Clark</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Bootstrapping statistical parsers from small datasets.</title>
<date>2003</date>
<booktitle>In EACL,</booktitle>
<pages>331--338</pages>
<publisher>The Association</publisher>
<institution>for Computer Linguistics.</institution>
<contexts>
<context position="3700" citStr="Steedman et al., 2003" startWordPosition="551" endWordPosition="554">org/licenses/by/4.0/ 54 First Joint Workshop on Statistical Parsing of Morphologically Rich Languages and Syntactic Analysis of Non-Canonical Languages, pages 54–65 Dublin, Ireland, August 23-29 2014. parsed during parser testing. In addition, a certain amount of unlabelled target domain texts may be available that can be leveraged in this or that way to facilitate domain adaptation. To address the problem of domain adaption, previous work focused on weakly supervised methods to re-train parsers on automatically parsed out-of-domain texts, through techniques such as co-training (Sarkar, 2001; Steedman et al., 2003), self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words </context>
</contexts>
<marker>Steedman, Sarkar, Osborne, Hwa, Clark, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Anoop Sarkar, Miles Osborne, Rebecca Hwa, Stephen Clark, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Bootstrapping statistical parsers from small datasets. In EACL, pages 331–338. The Association for Computer Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Szolovits</author>
</authors>
<title>Adding a medical lexicon to an English parser.</title>
<date>2003</date>
<booktitle>In AMIA Annual Symposium Proceedings,</booktitle>
<volume>volume</volume>
<pages>639</pages>
<publisher>American Medical Informatics Association.</publisher>
<contexts>
<context position="4316" citStr="Szolovits, 2003" startWordPosition="645" endWordPosition="646"> self-training (McClosky and Charniak, 2008; Rehbein, 2011), and uptraining (Petrov et al., 2010); selecting or weighting sentences from annotated in-domain data that fit best with the target domain (Plank and Van Noord, 2011; Søgaard and Plank, 2012; Khan et al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated texts (Zhou et al., 2011). The goal of this paper is to investigate a combination of such techniques to adapt a general-language parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual annotation. In our study we include several techniques that have been shown to be reasonably effective for domain adaptation: text </context>
</contexts>
<marker>Szolovits, 2003</marker>
<rawString>Peter Szolovits. 2003. Adding a medical lexicon to an English parser. In AMIA Annual Symposium Proceedings, volume 2003, page 639. American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan T McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and type constraints for cross-lingual part-of-speech tagging.</title>
<date>2013</date>
<tech>TACL,</tech>
<pages>1--1</pages>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan T. McDonald, and Joakim Nivre. 2013. Token and type constraints for cross-lingual part-of-speech tagging. TACL, 1:1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meishan Zhang</author>
<author>Wanxiang Che</author>
<author>Yijia Liu</author>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
</authors>
<title>Hit dependency parsing: Bootstrap aggregating heterogeneous parsers.</title>
<date>2012</date>
<booktitle>In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).</booktitle>
<contexts>
<context position="8991" citStr="Zhang et al. (2012)" startWordPosition="1376" endWordPosition="1379">nt parsers that supplied training data to each other in a bootstrapping manner. A number of studies specifically aimed to use co-training for domain adaptation of a dependency parser. Sagae (2007) used two different learning algorithms of their graph-based parser to complete a one iteration of co-training, getting an improvement of 2-3%, which was the best result on the out-ofdomain track of the CoNLL07 shared task (Nilsson et al., 2007). An interesting finding of their work was that the agreement between the two classifiers during testing was a very good predictor of accuracy. More recently, Zhang et al. (2012) used a tri-training algorithm for parser domain adaptation. The algorithm uses three learners and each learner was designed to learn from those automatically classified unlabelled data where the other two learners agreed on the classification label. 3 Experimental set-up 3.1 Parsers In the experiments we included the Malt parser (Nivre, 2009), the MST parser (McDonald and Pereira, 2006), the transition-based Mate parser (Bohnet et al., 2013), and the graph-based Turbo parser (Martins et al., 2010). All the parsers were used with their default settings, and PoS tags used in the input of all th</context>
<context position="27708" citStr="Zhang et al., 2012" startWordPosition="4327" endWordPosition="4330">ccuracy overall was achieved by combinations of techniques, in all the three subdomains, improving on the baseline by up to 1.3%. Comparing the results achieved by our best configurations with the results of the shared task, we see that our labelled accuracy averaged across the subdomains was just above the Stanford-2 system (80.31 vs. 80.25), which ranked 5th of all the twelve submissions (Petrov and McDonald, 2012). Although our results are still 3.15% lower than DCU-Paris13, the best system at SANCL-2012, the top four results were all generated by combination systems (Le Roux et al., 2012; Zhang et al., 2012; McClosky et al., 2012); our highest results only produced by the Mate parser, hence our best configuration achieved the best performance of a single parser. Answers Newsgroups Reviews Average Co-training 77.18 82.72** 78.21 79.37 Clusters 78.04** 83.06* 79.03** 80.04 Wiktionary 77.61 82.8 78.32 79.57 Clusters+Wiktionary 78.19** 83.38* 79.36** 80.31 Co-training+Clusters 78.05* 83.29** 78.8** 80.04 Co-training+Clusters+Wiktionary 78.33** 83.35** 78.84* 80.17 Baseline 77.03 82.4 78.12 79.18 SANCL Stanford-2 77.5 83.56 79.7 80.25 SANCL Best (DCU-Paris13) 81.15 85.38 83.86 83.46 Table 15: Combina</context>
</contexts>
<marker>Zhang, Che, Liu, Li, Liu, 2012</marker>
<rawString>Meishan Zhang, Wanxiang Che, Yijia Liu, Zhenghua Li, and Ting Liu. 2012. Hit dependency parsing: Bootstrap aggregating heterogeneous parsers. In Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi-Hua Zhou</author>
<author>Ming Li</author>
</authors>
<title>Tri-training: Exploiting unlabeled data using three classifiers.</title>
<date>2005</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>17</volume>
<issue>11</issue>
<contexts>
<context position="24492" citStr="Zhou and Li, 2005" startWordPosition="3835" endWordPosition="3838">of the improvements were significant. Three learners co-training. In the previous experiments, the Mate parser was used both as a source learner and as the evaluation learner. Therefore it was likely that the additional training data did not 60 Mate+Malt, +30k Avg. Length &gt;6 tokens 78.88 13.1 &gt;5 tokens 78.61 12.67 &gt;4 tokens 78.67 11.94 All sentences 78.61 8.35 Table 12: The effect of removing short sentences from generated training data. contain sufficiently novel examples based on which the evaluation parser could adapt better to the new domain. Thus we next tried the tri-training algorithm (Zhou and Li, 2005), where two parsers are used as source learners and a third as the evalaution learner. We used Malt and MST as source learners, identifying sentences which they parsed in the same manner, and using these sentences to retrain the Mate parser. We find that the tri-training algorithm performs better than the set-up with two parsers: on 10k and 20k additional sentences, it achieves an accuracy increase on Mate+Malt, significant at the p &lt; 0.05 level (see Table 13). +10k +20k +30k Mate+Malt+MST 78.70* 79.12* 78.95 Mate+Malt 78.43 78.70 78.88 Table 13: Accuracy scores for tri-training (Mate+Malt+MST</context>
</contexts>
<marker>Zhou, Li, 2005</marker>
<rawString>Zhi-Hua Zhou and Ming Li. 2005. Tri-training: Exploiting unlabeled data using three classifiers. Knowledge and Data Engineering, IEEE Transactions on, 17(11):1529–1541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guangyou Zhou</author>
<author>Jun Zhao</author>
<author>Kang Liu</author>
<author>Li Cai</author>
</authors>
<title>Exploiting web-derived selectional preference to improve statistical dependency parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11,</booktitle>
<pages>1556--1565</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4577" citStr="Zhou et al., 2011" startWordPosition="680" endWordPosition="683">t al., 2013b). Another line of research aims specifically to overcome the lexical gap between the training data and the target domain texts. These approaches include techniques such as text pre-processing and normalization (Foster, 2010), the use of external lexica and morphological clues to predict PoS tags of unknown target domain words (Szolovits, 2003; Pyysalo et al., 2006), discrete or continuous word clusters computed from unlabelled target domain texts (Candito et al., 2011; Bansal et al., 2014), selectional preferences modelled from word co-occurrences obtained from unannotated texts (Zhou et al., 2011). The goal of this paper is to investigate a combination of such techniques to adapt a general-language parser to parse web data (weblogs, online reviews, newsgroups, and answers) without resorting to manual annotation. In our study we include several techniques that have been shown to be reasonably effective for domain adaptation: text normalization, the use of word clusters, an external crowd-sourced lexicon, as well as automatically annotated texts produced with the help of co-training. All these techniques are domain-independent and can be applied to new target domains given unlabelled tex</context>
</contexts>
<marker>Zhou, Zhao, Liu, Cai, 2011</marker>
<rawString>Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai. 2011. Exploiting web-derived selectional preference to improve statistical dependency parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, HLT ’11, pages 1556–1565, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000362">
<title confidence="0.998231">
A Benchmark Dataset for Automatic Detection of Claims and
Evidence in the Context of Controversial Topics
</title>
<author confidence="0.647341">
Ehud Aharoni* Anatoly Polnarov* Tamar Lavee† Daniel Hershcovich
</author>
<affiliation confidence="0.51513">
IBM Haifa Research Hebrew University, IBM Haifa Research IBM Haifa Research
</affiliation>
<note confidence="0.771844">
Lab, Haifa, Israel Israel Lab, Haifa, Israel Lab, Haifa, Israel
</note>
<author confidence="0.714942">
Ran Levy Ruty Rinott Dan Gutfreund Noam Slonim‡
</author>
<affiliation confidence="0.515207">
IBM Haifa Research IBM Haifa Research IBM Haifa Research IBM Haifa Research
Lab, Haifa, Israel Lab, Haifa, Israel Lab, Haifa, Israel Lab, Haifa, Israel
</affiliation>
<sectionHeader confidence="0.976883" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.990537555555556">
We describe a novel and unique argumenta-
tive structure dataset. This corpus consists of
data extracted from hundreds of Wikipedia ar-
ticles using a meticulously monitored manual
annotation process. The result is 2,683 argu-
ment elements, collected in the context of 33
controversial topics, organized under a simple
claim-evidence structure. The obtained data
are publicly available for academic research.
</bodyText>
<sectionHeader confidence="0.998776" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.93801428125">
One major obstacle in developing automatic ar-
gumentation mining techniques is the scarcity of
relevant high quality annotated data. Here, we
describe a novel and unique benchmark data that
relies on a simple argument model and elabo-
rates on the associated annotation process. Most
importantly, the argumentative elements were
gathered in the context of pre-defined controver-
sial topics, which distinguishes our work from
other previous related corpora. 1 Two recent
* These authors contributed equally to this manuscript.
† Present affiliation: Yahoo!
‡ Corresponding author, at noams@il.ibm.com
works that are currently under review [Rinott et
al, Levy et al] have reported first results over
different subsets of this data, which is now pub-
lically available for academic research upon re-
quest. We believe that this novel corpus should
be of practical importance to many researches,
and in particular to the emerging community of
argumentation mining.
Unlike the classical Toulmin model (Freeley
and Steinberg 2008), we considered a simple and
robust argument structure comprising only two
components – claim and associated supporting
evidence. The argumentative structures were
carefully annotated under a pre-defined topic,
introduced as a debate motion. As the collected
data covers a diverse set of 33 motions, we ex-
pect it could be used to develop generic tools for
automatic detection and construction of argu-
mentative structures in the context of new topics.
</bodyText>
<sectionHeader confidence="0.996574" genericHeader="method">
2 Data Model
</sectionHeader>
<bodyText confidence="0.999626">
We defined and implemented the following con-
cepts:
Topic – a short, usually controversial statement
that defines the subject of interest. Context De-
</bodyText>
<footnote confidence="0.981471333333333">
1 E.g., AraucariaDB (Reed 2005, Moens et al 2007) and
Vaccine/Injury Project (V/IP) Corpus (Ashley and Walker
2013).
</footnote>
<page confidence="0.996875">
64
</page>
<bodyText confidence="0.959763444444445">
Proceedings of the First Workshop on Argumentation Mining, pages 64–68,
Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics
pendent Claim (CDC) – a general concise
statement that directly supports or contests the
Topic. Context Dependent Evidence (CDE) – a
text segment that directly supports a CDC in the
context of a given Topic. Examples are given in
Section 6.
Furthermore, since one can support a claim us-
ing different types of evidence (Rieke et al 2012,
Seech 2008), we defined and considered three
CDE types: Study: Results of a quantitative
analysis of data given as numbers or as conclu-
sions. Expert: Testimony by a person / group /
committee / organization with some known ex-
pertise in or authority on the topic. Anecdotal: a
description of specific event(s)/instance(s) or
concrete example(s).
</bodyText>
<sectionHeader confidence="0.794848" genericHeader="method">
3 Labeling Challenges and Approach
</sectionHeader>
<bodyText confidence="0.999986529411765">
The main challenge we faced in collecting the
annotated data was the inherently elusive nature
of concepts such as &amp;quot;claim&amp;quot; and &amp;quot;evidence.&amp;quot; To
address that we formulated two sets of criteria
for CDC and CDE, respectively, and relied on a
team of about 20 carefully trained in-house la-
belers whose work was closely monitored. To
further enhance the quality of the collected data
we adopted a two-stage labeling approach. First,
a team of five labelers worked independently on
the same text and prepared the initial set of can-
didate CDCs or candidate CDEs. Next, a team of
five labelers—not necessarily the same five—
independently crosschecked the joint list of the
detected candidates, each of which was either
confirmed or rejected. Candidates confirmed by
at least three labelers were included in the corpus.
</bodyText>
<sectionHeader confidence="0.916195" genericHeader="method">
4 Labeling Guidelines
</sectionHeader>
<bodyText confidence="0.999985875">
The labeling guidelines defined the concepts of
Topic, CDC, CDE, and CDE types, along with
relevant examples. According to these guidelines,
given a Topic, a text fragment should be labeled
as a CDC if and only if it complies with all of
the following five CDC criteria: Strength:
Strong content that directly supports or contests
the provided Topic. Generality: General content
that deals with a relatively broad idea. Phrasing:
Is well phrased, or requires at most a single and
minor &amp;quot;allowed&amp;quot; change.2 Keeping text spirit:
Keeps the spirit of the original text from which it
was extracted. Topic unity: Deals with one, or at
most two related topics. Four CDE criteria were
defined in a similar way, given a Topic and a
CDC, except for the generality criterion.
</bodyText>
<sectionHeader confidence="0.926295" genericHeader="method">
5 Labeling Details
</sectionHeader>
<bodyText confidence="0.951895916666667">
The labeling process was carried out in the
GATE environment (https://gate.ac.uk/). The 33
Topics were selected at random from the debate
motions at http://idebate.org/ database. The la-
beling process was divided into five stages:
Search: Given a Topic, five labelers were
asked to independently search English Wikipe-
dia3 for articles with promising content.
Claim Detection: At this stage, five labelers
independently detected candidate CDCs support-
ing or contesting the Topic within each article
suggested by the Search team.
Claim Confirmation: At this stage, five la-
belers independently cross-examined the candi-
date CDCs suggested at the Claim Detection
stage, aiming to confirm a candidate and its sen-
timent as to the given Topic, or reject it by refer-
ring to one of the five CDC Criteria it fails to
meet. The candidate CDCs confirmed by at least
three labelers were forwarded to the next stage.
Evidence Detection: At this stage, five la-
belers independently detected candidate CDEs
supporting a confirmed CDC in the context of
the given Topic. The search for CDEs was done
</bodyText>
<footnote confidence="0.54735425">
2 For example, anaphora resolution. The enclosed data set
contains the corrected version as well, as proposed by the
labelers.
3 We considered the Wikipedia dump as of April 3, 2012.
</footnote>
<page confidence="0.998869">
65
</page>
<bodyText confidence="0.9995791875">
only in the same article where the corresponding
CDC was found.
Evidence Confirmation: This stage was car-
ried out in a way similar to Claim Confirmation.
The only difference was that the labelers were
required to classify accepted CDE under one or
more CDE types.
Labelers training and feedback: Before join-
ing actual labeling tasks, novice labelers were
assigned with several completed tasks and were
expected to show a reasonable degree of agree-
ment with a consensus solution prepared in ad-
vance by the project administrators. In addition,
the results of each Claim Confirmation task were
examined by one or two of the authors (AP and
NS) to ensure the conformity to the guidelines.
In case crude mistakes were spotted, the corre-
sponding labeler was requested to revise and
resubmit. Due to the large numbers of CDE can-
didates, it was impractical to rely on such a rig-
orous monitoring process in Evidence Confirma-
tion. Instead, Evidence Consensus Solutions
were created for selected articles by several ex-
perienced labelers, who first solved the tasks
independently and then reached consensus in a
joint meeting. Afterwards, the tasks were as-
signed to the rest of the labelers. Their results on
these tasks were juxtaposed with the Consensus
Solutions, and on the basis of this comparison
individual feedback reports were drafted and
sent to the team members. Each labeler received
such a report on an approximately weekly basis.
</bodyText>
<sectionHeader confidence="0.992287" genericHeader="method">
6 Data Summary
</sectionHeader>
<bodyText confidence="0.999825176470588">
For 33 debate motions, a total of 586 Wikipedia
articles were labeled. The labeling process re-
sulted in 1,392 CDCs distributed across 321 ar-
ticles. In 12 debate motions, for which 350 dis-
tinct CDCs were confirmed across 104 articles,
we further completed the CDE labeling, ending
up with a total of 1,291 confirmed CDEs – 431
of type Study, 516 of type Expert, and 529 of
type Anecdotal. Note that some CDEs were as-
sociated with more than one type (for example,
118 CDEs were classified both under the type
Study and Expert).
Presented in Tables 1 and 2 are several exam-
ples of CDCs and CDEs gathered under the
Topics we worked with, as well as some inac-
ceptable candidates illustrating some of the sub-
tleties of the performed work.
</bodyText>
<table confidence="0.9928595">
Topic The sale of violent video games to mi-
nors should be banned
(Pro) Violent video games can increase chil-
CDC dren’s aggression
(Pro) Video game publishers unethically train
CDC children in the use of weapons
Note that a valid CDC is not necessarily fac-
tual.
(Con) Violent games affect children positively
CDC
Invalid Video game addiction is excessive or
CDC 1 compulsive use of computer and video
games that interferes with daily life.
This statement defines a concept relevant to
the Topic, not a relevant claim.
Invalid Violent TV shows just mirror the vio-
CDC 2 lence that goes on in the real world.
This claim is not relevant enough to Topic.
Invalid Violent video games should not be sold
CDC 3 to children.
This candidate simply repeats the Topic and
thus is not considered a valid CDC.
Invalid “Doom” has been blamed for nationally
CDC 4 covered school shooting.
This candidate fails the generality criterion,
as it focuses on a specific single video game.
Note that it could serve as CDE to a more
general CDC.
</table>
<tableCaption confidence="0.999171">
Table 1: Examples of CDCs and invalid CDCs.
</tableCaption>
<bodyText confidence="0.536272125">
Topic 1 The sale of violent video games to
minors should be banned
(Pro) CDC Violent video games increase youth
violence
CDE The most recent large scale meta-
(Study) analysis—examining 130 studies with
over 130,000 subjects worldwide—
concluded that exposure to violent
</bodyText>
<page confidence="0.752271">
66
</page>
<bodyText confidence="0.397963333333333">
video games causes both short term
and long term aggression in players
CDE In April 2000, a 16-year-old teenager
(Anecdotal) murdered his father, mother and sister
proclaiming that he was on an &amp;quot;aveng-
ing mission&amp;quot; for the main character of
the video game Final Fantasy VIII
Invalid While most experts reject any link
CDE between video games content and re-
al-life violence, some media scholars
argue that the connection exists.
Invalid, because it includes information
that contests the CDC.
Topic 2 The use of performance enhancing
drugs in sports should be permitted
(Con) CDC Drug abuse can be harmful to one’s
health and even deadly.
CDE According to some nurse practition-
(Expert) ers, stopping substance abuse can
reduce the risk of dying early and also
reduce some health risks like heart
disease, lung disease, and strokes
Invalid Suicide is very common in adolescent
CDE alcohol abusers, with 1 in 4 suicides
in adolescents being related to alcohol
abuse.
Although the candidate CDE does support
the CDC, the notion of adolescent alcohol
abusers is irrelevant to the Topic. There-
fore, the candidate is invalid.
</bodyText>
<tableCaption confidence="0.994559">
Table 2: Examples of CDE and invalid CDE.
</tableCaption>
<sectionHeader confidence="0.870964" genericHeader="evaluation">
7 Agreement and Recall Results
</sectionHeader>
<bodyText confidence="0.99833575">
To evaluate the labelers’ agreement we used Co-
hen’s kappa coefficient (Landis and Koch 1977).
The average measure was calculated over all
labelers&apos; pairs, for each pair taking those articles
on which the corresponding labelers worked to-
gether and omitting labeler pairs which labeled
together less than 100 CDCs/CDEs. This strate-
gy was chosen since no two labelers worked on
the exact same tasks, so standard multi-rater
agreement measures could not be applied. The
obtained average kappa was 0.39 and 0.4 in the
Claim confirmation and Evidence confirmation
stages, respectively, which we consider satisfac-
tory given the subtlety of the concepts involved
and the fact that the tasks naturally required a
certain extent of subjective decision making.
We further employed a simple method to ob-
tain a rough estimate of the recall at the detection
stages. For CDCs (and similarly for CDEs), let n
be the number of CDCs detected and confirmed
in a given article, and x be the unknown total
number of CDCs in this article. Assuming the i-
th labeler detects a ratio of x, and taking a
strong assumption of independence between the
labelers, we get:
.
We estimated from the observed data, and
computed x for each article. We were then able
to compute the estimated recall per motion, end-
ing up with the estimated average recall of
90.6% and 90.0% for CDCs and CDEs, respec-
tively.
</bodyText>
<sectionHeader confidence="0.992944" genericHeader="discussions">
8 Future Work and Conclusion
</sectionHeader>
<bodyText confidence="0.999784944444444">
There are several natural ways to proceed further.
First, a considerable increase in the quantity of
gathered CDE data can be achieved by expand-
ing the search scope beyond the article in which
the CDC is found. Second, the argument model
can be enhanced – for example, to include coun-
ter-CDE (i.e., evidence that contest the CDC).
Third, one may look into ways to add more la-
beling layers on the top of the existing model
(for example, distinguishing between factual
CDCs, value CDCs, and so forth). Fourth, new
topics and new sources besides Wikipedia can be
considered.
The data is released and available upon request
for academic research. We hope that it will prove
useful for different data mining communities,
and particularly for various purposes in the field
of Argumentation Mining.
</bodyText>
<page confidence="0.999286">
67
</page>
<sectionHeader confidence="0.995886" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999966973684211">
Austin J. Freeley and David L. Steinberg. 2008.
Argumentation and Debate. Wadsworth,
Belmont, California.
Chris Reed. 2005. &amp;quot;Preliminary Results from an
Argument Corpus&amp;quot; in Proceedings of the IY
Symposium on Social Communication, San-
tiago de Cuba, pp. 576-580.
J. Richard Landis and Gary G. Koch. 1977. &amp;quot;The
measurement of observer agreement for
categorical data.&amp;quot; Biometrics 33:159-174.
Kevid D. Ashley and Vern R. Walker. 2013.
&amp;quot;Toward Constructing Evidence-Based Le-
gal Arguments Using Legal Decision Doc-
uments and Machine Learning&amp;quot; in Proceed-
ings of the Fourteenth International Con-
ference on Artificial Intelligence and Law
(ICAIL ’13), Rome, Italy, pp. 176-180.
Marie-Francine Moens, Erik Boiy, Raquel Mo-
chales Palau, and Chris Reed. 2007. &amp;quot;Au-
tomatic Detection of Arguments in Legal
Texts&amp;quot; in Proceedings of the International
Conference on AI &amp; Law (ICAIL-2007),
Stanford, CA, pp. 225-230.
Richard D. Rieke, Malcolm O. Sillars and Tarla
Rai Peterson. 2012. Argumentation and
Critical Decision Making (8e). Prentice
Hall, USA.
Ran Levy, Yonatan Bilu, Daniel Hershcovich,
Ehud Aharoni and Noam Slonim. &amp;quot;Context
Dependent Claim Detection.&amp;quot; Submitted
Ruty Rinott, Lena Dankin, Carlos Alzate, Ehud
Aharoni and Noam Slonim. &amp;quot;Show Me
Your Evidence – an Automatic Method for
Context Dependent Evidence Detection.&amp;quot;
Submitted.
Zachary Seech. 2008. Writing Philosophy Pa-
pers (5th edition). Wadsworth, Cengage
Learning, Belmont, California.
</reference>
<page confidence="0.999447">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930548">
<title confidence="0.9960195">A Benchmark Dataset for Automatic Detection of Claims and Evidence in the Context of Controversial Topics</title>
<author confidence="0.985302">Aharoni Anatoly Polnarov Tamar Hershcovich</author>
<affiliation confidence="0.999595">IBM Haifa Research Hebrew University, IBM Haifa Research IBM Haifa Research</affiliation>
<address confidence="0.994053">Lab, Haifa, Israel Israel Lab, Haifa, Israel Lab, Haifa, Israel</address>
<author confidence="0.978451">Levy Ruty Rinott Dan Gutfreund Noam</author>
<affiliation confidence="0.99919">IBM Haifa Research IBM Haifa Research IBM Haifa Research IBM Haifa Research</affiliation>
<address confidence="0.993949">Lab, Haifa, Israel Lab, Haifa, Israel Lab, Haifa, Israel Lab, Haifa, Israel</address>
<abstract confidence="0.998425">We describe a novel and unique argumentative structure dataset. This corpus consists of data extracted from hundreds of Wikipedia articles using a meticulously monitored manual annotation process. The result is 2,683 argument elements, collected in the context of 33 controversial topics, organized under a simple claim-evidence structure. The obtained data are publicly available for academic research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Austin J Freeley</author>
<author>David L Steinberg</author>
</authors>
<title>Argumentation and Debate.</title>
<date>2008</date>
<location>Wadsworth, Belmont, California.</location>
<contexts>
<context position="1954" citStr="Freeley and Steinberg 2008" startWordPosition="289" endWordPosition="292">ich distinguishes our work from other previous related corpora. 1 Two recent * These authors contributed equally to this manuscript. † Present affiliation: Yahoo! ‡ Corresponding author, at noams@il.ibm.com works that are currently under review [Rinott et al, Levy et al] have reported first results over different subsets of this data, which is now publically available for academic research upon request. We believe that this novel corpus should be of practical importance to many researches, and in particular to the emerging community of argumentation mining. Unlike the classical Toulmin model (Freeley and Steinberg 2008), we considered a simple and robust argument structure comprising only two components – claim and associated supporting evidence. The argumentative structures were carefully annotated under a pre-defined topic, introduced as a debate motion. As the collected data covers a diverse set of 33 motions, we expect it could be used to develop generic tools for automatic detection and construction of argumentative structures in the context of new topics. 2 Data Model We defined and implemented the following concepts: Topic – a short, usually controversial statement that defines the subject of interest</context>
</contexts>
<marker>Freeley, Steinberg, 2008</marker>
<rawString>Austin J. Freeley and David L. Steinberg. 2008. Argumentation and Debate. Wadsworth, Belmont, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Reed</author>
</authors>
<title>Preliminary Results from an Argument Corpus&amp;quot;</title>
<date>2005</date>
<booktitle>in Proceedings of the IY Symposium on Social Communication, Santiago de Cuba,</booktitle>
<pages>576--580</pages>
<contexts>
<context position="2596" citStr="Reed 2005" startWordPosition="392" endWordPosition="393">st argument structure comprising only two components – claim and associated supporting evidence. The argumentative structures were carefully annotated under a pre-defined topic, introduced as a debate motion. As the collected data covers a diverse set of 33 motions, we expect it could be used to develop generic tools for automatic detection and construction of argumentative structures in the context of new topics. 2 Data Model We defined and implemented the following concepts: Topic – a short, usually controversial statement that defines the subject of interest. Context De1 E.g., AraucariaDB (Reed 2005, Moens et al 2007) and Vaccine/Injury Project (V/IP) Corpus (Ashley and Walker 2013). 64 Proceedings of the First Workshop on Argumentation Mining, pages 64–68, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics pendent Claim (CDC) – a general concise statement that directly supports or contests the Topic. Context Dependent Evidence (CDE) – a text segment that directly supports a CDC in the context of a given Topic. Examples are given in Section 6. Furthermore, since one can support a claim using different types of evidence (Rieke et al 2012, Seech 2008),</context>
</contexts>
<marker>Reed, 2005</marker>
<rawString>Chris Reed. 2005. &amp;quot;Preliminary Results from an Argument Corpus&amp;quot; in Proceedings of the IY Symposium on Social Communication, Santiago de Cuba, pp. 576-580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.&amp;quot;</title>
<date>1977</date>
<journal>Biometrics</journal>
<pages>33--159</pages>
<contexts>
<context position="11243" citStr="Landis and Koch 1977" startWordPosition="1824" endWordPosition="1827">se practition(Expert) ers, stopping substance abuse can reduce the risk of dying early and also reduce some health risks like heart disease, lung disease, and strokes Invalid Suicide is very common in adolescent CDE alcohol abusers, with 1 in 4 suicides in adolescents being related to alcohol abuse. Although the candidate CDE does support the CDC, the notion of adolescent alcohol abusers is irrelevant to the Topic. Therefore, the candidate is invalid. Table 2: Examples of CDE and invalid CDE. 7 Agreement and Recall Results To evaluate the labelers’ agreement we used Cohen’s kappa coefficient (Landis and Koch 1977). The average measure was calculated over all labelers&apos; pairs, for each pair taking those articles on which the corresponding labelers worked together and omitting labeler pairs which labeled together less than 100 CDCs/CDEs. This strategy was chosen since no two labelers worked on the exact same tasks, so standard multi-rater agreement measures could not be applied. The obtained average kappa was 0.39 and 0.4 in the Claim confirmation and Evidence confirmation stages, respectively, which we consider satisfactory given the subtlety of the concepts involved and the fact that the tasks naturally</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. &amp;quot;The measurement of observer agreement for categorical data.&amp;quot; Biometrics 33:159-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevid D Ashley</author>
<author>Vern R Walker</author>
</authors>
<title>Toward Constructing Evidence-Based Legal Arguments Using Legal Decision Documents and Machine Learning&amp;quot;</title>
<date>2013</date>
<booktitle>in Proceedings of the Fourteenth International Conference on Artificial Intelligence and Law (ICAIL ’13),</booktitle>
<pages>176--180</pages>
<location>Rome, Italy,</location>
<contexts>
<context position="2681" citStr="Ashley and Walker 2013" startWordPosition="403" endWordPosition="406">ated supporting evidence. The argumentative structures were carefully annotated under a pre-defined topic, introduced as a debate motion. As the collected data covers a diverse set of 33 motions, we expect it could be used to develop generic tools for automatic detection and construction of argumentative structures in the context of new topics. 2 Data Model We defined and implemented the following concepts: Topic – a short, usually controversial statement that defines the subject of interest. Context De1 E.g., AraucariaDB (Reed 2005, Moens et al 2007) and Vaccine/Injury Project (V/IP) Corpus (Ashley and Walker 2013). 64 Proceedings of the First Workshop on Argumentation Mining, pages 64–68, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics pendent Claim (CDC) – a general concise statement that directly supports or contests the Topic. Context Dependent Evidence (CDE) – a text segment that directly supports a CDC in the context of a given Topic. Examples are given in Section 6. Furthermore, since one can support a claim using different types of evidence (Rieke et al 2012, Seech 2008), we defined and considered three CDE types: Study: Results of a quantitative analysis</context>
</contexts>
<marker>Ashley, Walker, 2013</marker>
<rawString>Kevid D. Ashley and Vern R. Walker. 2013. &amp;quot;Toward Constructing Evidence-Based Legal Arguments Using Legal Decision Documents and Machine Learning&amp;quot; in Proceedings of the Fourteenth International Conference on Artificial Intelligence and Law (ICAIL ’13), Rome, Italy, pp. 176-180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Francine Moens</author>
<author>Erik Boiy</author>
<author>Raquel Mochales Palau</author>
<author>Chris Reed</author>
</authors>
<title>Automatic Detection of Arguments in Legal Texts&amp;quot;</title>
<date>2007</date>
<booktitle>in Proceedings of the International Conference on AI &amp; Law (ICAIL-2007),</booktitle>
<pages>225--230</pages>
<location>Stanford, CA,</location>
<contexts>
<context position="2615" citStr="Moens et al 2007" startWordPosition="394" endWordPosition="397"> structure comprising only two components – claim and associated supporting evidence. The argumentative structures were carefully annotated under a pre-defined topic, introduced as a debate motion. As the collected data covers a diverse set of 33 motions, we expect it could be used to develop generic tools for automatic detection and construction of argumentative structures in the context of new topics. 2 Data Model We defined and implemented the following concepts: Topic – a short, usually controversial statement that defines the subject of interest. Context De1 E.g., AraucariaDB (Reed 2005, Moens et al 2007) and Vaccine/Injury Project (V/IP) Corpus (Ashley and Walker 2013). 64 Proceedings of the First Workshop on Argumentation Mining, pages 64–68, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics pendent Claim (CDC) – a general concise statement that directly supports or contests the Topic. Context Dependent Evidence (CDE) – a text segment that directly supports a CDC in the context of a given Topic. Examples are given in Section 6. Furthermore, since one can support a claim using different types of evidence (Rieke et al 2012, Seech 2008), we defined and con</context>
</contexts>
<marker>Moens, Boiy, Palau, Reed, 2007</marker>
<rawString>Marie-Francine Moens, Erik Boiy, Raquel Mochales Palau, and Chris Reed. 2007. &amp;quot;Automatic Detection of Arguments in Legal Texts&amp;quot; in Proceedings of the International Conference on AI &amp; Law (ICAIL-2007), Stanford, CA, pp. 225-230.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard D Rieke</author>
<author>Malcolm O Sillars</author>
<author>Tarla Rai Peterson</author>
</authors>
<date>2012</date>
<booktitle>Argumentation and Critical Decision Making (8e).</booktitle>
<publisher>Prentice Hall, USA.</publisher>
<contexts>
<context position="3182" citStr="Rieke et al 2012" startWordPosition="483" endWordPosition="486">1 E.g., AraucariaDB (Reed 2005, Moens et al 2007) and Vaccine/Injury Project (V/IP) Corpus (Ashley and Walker 2013). 64 Proceedings of the First Workshop on Argumentation Mining, pages 64–68, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics pendent Claim (CDC) – a general concise statement that directly supports or contests the Topic. Context Dependent Evidence (CDE) – a text segment that directly supports a CDC in the context of a given Topic. Examples are given in Section 6. Furthermore, since one can support a claim using different types of evidence (Rieke et al 2012, Seech 2008), we defined and considered three CDE types: Study: Results of a quantitative analysis of data given as numbers or as conclusions. Expert: Testimony by a person / group / committee / organization with some known expertise in or authority on the topic. Anecdotal: a description of specific event(s)/instance(s) or concrete example(s). 3 Labeling Challenges and Approach The main challenge we faced in collecting the annotated data was the inherently elusive nature of concepts such as &amp;quot;claim&amp;quot; and &amp;quot;evidence.&amp;quot; To address that we formulated two sets of criteria for CDC and CDE, respectivel</context>
</contexts>
<marker>Rieke, Sillars, Peterson, 2012</marker>
<rawString>Richard D. Rieke, Malcolm O. Sillars and Tarla Rai Peterson. 2012. Argumentation and Critical Decision Making (8e). Prentice Hall, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ran Levy</author>
</authors>
<title>Yonatan Bilu, Daniel Hershcovich, Ehud Aharoni and Noam Slonim. &amp;quot;Context Dependent Claim Detection.&amp;quot;</title>
<note>Submitted</note>
<marker>Levy, </marker>
<rawString>Ran Levy, Yonatan Bilu, Daniel Hershcovich, Ehud Aharoni and Noam Slonim. &amp;quot;Context Dependent Claim Detection.&amp;quot; Submitted</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ruty Rinott</author>
<author>Lena Dankin</author>
<author>Carlos Alzate</author>
</authors>
<title>Ehud Aharoni and Noam Slonim. &amp;quot;Show Me Your Evidence – an Automatic Method for Context Dependent Evidence Detection.&amp;quot;</title>
<note>Submitted.</note>
<marker>Rinott, Dankin, Alzate, </marker>
<rawString>Ruty Rinott, Lena Dankin, Carlos Alzate, Ehud Aharoni and Noam Slonim. &amp;quot;Show Me Your Evidence – an Automatic Method for Context Dependent Evidence Detection.&amp;quot; Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zachary Seech</author>
</authors>
<title>Writing Philosophy Papers (5th edition).</title>
<date>2008</date>
<location>Wadsworth, Cengage Learning, Belmont, California.</location>
<contexts>
<context position="3195" citStr="Seech 2008" startWordPosition="487" endWordPosition="488">B (Reed 2005, Moens et al 2007) and Vaccine/Injury Project (V/IP) Corpus (Ashley and Walker 2013). 64 Proceedings of the First Workshop on Argumentation Mining, pages 64–68, Baltimore, Maryland USA, June 26, 2014. c�2014 Association for Computational Linguistics pendent Claim (CDC) – a general concise statement that directly supports or contests the Topic. Context Dependent Evidence (CDE) – a text segment that directly supports a CDC in the context of a given Topic. Examples are given in Section 6. Furthermore, since one can support a claim using different types of evidence (Rieke et al 2012, Seech 2008), we defined and considered three CDE types: Study: Results of a quantitative analysis of data given as numbers or as conclusions. Expert: Testimony by a person / group / committee / organization with some known expertise in or authority on the topic. Anecdotal: a description of specific event(s)/instance(s) or concrete example(s). 3 Labeling Challenges and Approach The main challenge we faced in collecting the annotated data was the inherently elusive nature of concepts such as &amp;quot;claim&amp;quot; and &amp;quot;evidence.&amp;quot; To address that we formulated two sets of criteria for CDC and CDE, respectively, and relied</context>
</contexts>
<marker>Seech, 2008</marker>
<rawString>Zachary Seech. 2008. Writing Philosophy Papers (5th edition). Wadsworth, Cengage Learning, Belmont, California.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
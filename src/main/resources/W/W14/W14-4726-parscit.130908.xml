<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030068">
<title confidence="0.994319">
Wordfinding Problems and How to Overcome them Ultimately
With the Help of a Computer
</title>
<author confidence="0.743182">
Michael Zock
</author>
<note confidence="0.505884333333333">
LIF-CNRS / TALEP
163, Avenue de Luminy
13288 Marseille / France
</note>
<email confidence="0.987801">
michael.zock@lif.univ-mrs.fr
</email>
<sectionHeader confidence="0.993593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979176470588">
Our ultimate goal is to help authors to find an elusive word. Whenever we need a word, we look it
up in the place where it is stored, the dictionary or the mental lexicon. The question is how do we
manage to find the word, and how do we succeed to do this so quickly? While these are difficult
questions, I believe to have some practical answers for them. Since it is unreasonable to perform
search in the entire lexicon, I suggest to start by reducing this space (step-1) and to present then
the remaining candidates in a clustered and labeled form, i.e. categorial tree (step-2). The goal
of this second step is to support navigation.
Search space is determined by considering words directly related to the input, i.e. direct neigh-
bors (associations/co-occurrences). To this end many resources could be used. For example, one
may consider an associative network like the Edinburgh Association Thesaurus (E.A.T.). As this
will still yield too many hits, I suggest to cluster and label the outputs. This labeling is cru-
cial for navigation, as we want users to find the target quickly, rather than drown them under a
huge, unstructured list of words. Note, that in order to determine properly the initial search space
(step-1), we must have already well understood the input [mouse1 / mouse2 (rodent/device)], as
otherwise our list will contain a lot of noise, presenting ’cat, cheese’ together with ’computer,
mouse pad’, which is not quite what we want, since some of these candidates are irrelevant, i.e.
beyond the scope of the user’s goal.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.96559255">
Whenever we read a book, write a letter, or launch a query on Google, we always use words, the short-
hand labels for more or less well specified thoughts. No doubt, words are important, a fact nicely ex-
pressed by Wilkins (1972) when he writes: without grammar very little can be conveyed, without vocab-
ulary, nothing can be conveyed. Still, ubiquitous as they may be, words have to be learned, that is, they
have to be stored, remembered, and retrieved. Given the role words play in our daily lives, it is surprising
to see how little we have to offer so far to help humans to memorize, find or retrieve them. Hoping to
contribute to a change for this, I have started to work on one of these tasks: word access, also called
retrieval or wordfinding.
Imagine the following situation: your goal is to express the following ideas: superior dark coffee
made of beans from Arabia” by a single word, but you cannot access the corresponding form mocha,
even though you know it, since you’ve used it not so long ago. This kind of problem, known as the
tip-of-the-tongue (TOT)-problem, has received a lot of attention from psychologists (Schwartz, 2002;
Brown, 1991). It has always been pointed out that people being in this state know quite a bit concerning
the elusive word (Brown and McNeill, 1996). Hence, using it should allow us to reduce the search space.
Put differently, it would be nice to have a system capable to use whatever you have, incomplete as it may
be, to help you find what you cannot recall. For example, for the case at hand, one might think of dark,
coffee, beans, and Arabia, to expect from system a set of reasonable candidates, like arabica, espresso,
or mocha. In the remainder of this paper I will try to show how this might be achieved, but before doing
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.961056">
221
</page>
<note confidence="0.920163">
Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 221–229,
Dublin, Ireland, August 23, 2014.
</note>
<bodyText confidence="0.9903365">
so, I would like to clarify what I mean by computer-aided lexical access, what characterizes the problem
of word production, i.e. the process.
</bodyText>
<sectionHeader confidence="0.575325" genericHeader="method">
2 Computer-aided lexical access
</sectionHeader>
<bodyText confidence="0.999855676470588">
Under normal circumstances, words are accessed on the fly, that is, the lexical access is immediate,
involontary and autonomous. Also, it takes place without any external help. As we all know, things do
not always work that smoothly, which is why we may ask for help. In this latter case, lexical access
is deliberate, incremental (i.e., distributed over time), and may be mediated via some external resource
(another person or a dictionary). This situation may well arise in writing, where we are much more
demanding and where we have much more time. Hence words are chosen with much more care than
during speaking, i.e., spontaneous discourse.
I view computer-aided lexical access as an interactive, cognitive process. It is interactive as it involves
two cooperative agents, the user and the computer, and it is cognitive as it is largely knowledge-driven.
The knowledge concerns words, i.e. meanings and forms, as well as their relations to other words. Since
the knowledge of both agents is incomplete, they cooperate: neither of them alone can point to the target
word (tw), but by working together they can. It is as if one had the (semantic) map and the other the
compass, i.e., the knowledge to decide where to go. Since both types of knowledge are necessary, they
complete each other, helping utlimately the user to find the elusive word, which is the goal.
To be more concrete, consider some user input (one or several words), the system reacts by providing
all directly associated words. Since all words are linked, they form a graph, which has two major conse-
quences : the system knows everyone, the immediate neighbors, the neighbors’ neighbors, etc. and the
user can initiate search from anywhere, to continue it until he has reached the target word, tw. Everything
being connected, everything is reachable, at least in principle. Search may require several steps, but in
most cases the number of steps is surprisingly small.
As mentioned already, the user definitely has some knowledge concerning words, their components
and their organisation in the mental lexicon, but this knowledge is by no means complete. The user also
has some knowledge (or, more precisely, meta-knowledge) concerning the topology of the graph,1 but he
certainly does not know as much as the system. The fact that an author does have this kind of knowledge
is revealed via word associations (Cramer, 1968; Deese, 1965; Nelson et al., 1998; Kiss et al., 1972) and
via the observed average path length (Vitevitch, 2008) needed in order to get from some starting point
(sw) to the goal (tw). This path is generally quite short. It hardly ever exceeds three steps, and in many
cases even less: search is launched via an item directly related to the tw (direct neighbor).
If the user does not know too much concerning the topology of the network, he does know quite a bit
concerning the tw,2 information the system has no clue of at this point. Eventhough it knows ’everyone’
in the network, it cannot do mind-reading, i.e. guess the precise word a user has in mind (tw) when
providing a specific input (sw). Yet the user can. Even if he cannot access the word at a given moment,
he can recognize it when seeing it (alone or in a list). This fact is well established in the literature on the
’tip-of-the-tongue problem’ (Aitchison, 2003).
</bodyText>
<sectionHeader confidence="0.948501" genericHeader="method">
3 From mind to mouth, or what characterizes the process of word production?
</sectionHeader>
<bodyText confidence="0.999772">
According to the father of modern linguistics (de Saussure, 1916), word forms (signifier) and their asso-
ciated meaning (signified) are but one, called the sign. They are said to be an inseparable unit. This is in
sharp contrast to what psychologists tell us about words synthesis. For example, one of the leading spe-
cialists of language production (Levelt, 1989; Levelt, 1999) has convincingly shown that, when speaking
</bodyText>
<footnote confidence="0.982437111111111">
1For example, he knows that for a given word form there are similar forms in terms of sound or meaning. There are also
words that are more general/specific, or others meaning exactly the opposite than a given input. This kind of knowledge is so
obvious and so frequent that it is encoded in many resources like WordNet, Roget’s thesaurus or more traditional dictionaries
(incuding synonym and rhyming dictionaries).
2For example, parts of the form (rhymes with x: health/wealth) or meaning, like the ’type’ (animal), the ’function’ (used for
eating) or the ’relationship’ (synonym, antonym, ...) with respect the source word (sw). He may even be able to provide parts
of the definition (say, ’very small’ for ’liliput’). His main problem problem resides in the fact that he cannot access at this very
moment the exact word form (he experiences the so called ’tip-of-the-tongue problem, TOT), which is why he tries to find it in
a lexical resource (dictionary).
</footnote>
<page confidence="0.997929">
222
</page>
<bodyText confidence="0.998715">
we go, step by step, from meanings (concepts), to the lexical concept (also called lemma) to the sound
(written or spoken form). Depending on the theory, there may be retroaction or not, a lower level, say,
phonology, influencing a higher level, the lexical concept.
Note that the notion of lemma has completely different meanings in psychology and in lexicography.
While for linguists it is roughly speaking the word’s base-form or dictionary-form, for psycholinguists it
is a schema, i.e. an abstract form representing a specific meaning (a lexicalized concept) and a syntactic
category (part of speech), but it lacks entirely specific values concerning the form (sounds/graphemes).
This is being take care of at the next step (sound form encoding). In short, in contrast to Saussure’s view,
the information contributing to what we commonly call words (lemma or word forms) is distributed.
This is a well established empirical fact observed by psychologists working on the time course of word
production (Stemberger, 1985; Levelt and Schriefers, 1987; Dell et al., 1999), as by those who analyze
and interpret speech errors (Fromkin, 1973; Fromkin, 1980; Fromkin, 1993).
Yet, what concerns us here in particular is the following: as noted, speakers go from meanings to
sounds via lexical concepts (abstract word forms). More importantly, the conceptual input may lack in-
formation to determine a precise lexical form. Put differently, rather than starting from a full fledged def-
inition or complete meaning representation, authors may well start from an underspecified input (’small
bird’ rather than ’sparrow’). Note that the specific requirements of a culture may help us to clarify our
thoughts, as well as induce biases or imprecisions because of lexical gaps. Hence we end up using an
existing words (eventhough it does not express excatly what we had in mind) rather than coining a new
one fitting better our purpose (expressibility problem). For a psycholinguistic explanation concerning
gradual refinement, see (Zock, 1996).
Let me briefly illustrate this here via an example, and comment then on the way how specific knowl-
edge states may ask for different kind of information from the lexicon. Suppose you wanted to talk about
a given reptile having certain features (dangerous, size, living space, ...). If you cannot come up immedi-
ately with the intended word, any of the following could be candidates: alligator, crocodile, cayman. At
some point you need to make up your mind though, as the form synthesizer needs to know what items to
activate so that it can produce the corresponding form (graphemes, sounds).
concepts (word definitions, semantic fields: translation encyclopedic relations lexical relations clang relations
conceptual primitives) (thesaurus- or domain relations) equivalent word (syntagmatic associations) synonyms, antonyms (sound related words)
large voracious aquatic reptile aqua3c rep3le in another language crocodile: hypernyms, ... crocodile4Nile
having a long snout cocodril4crocodile voracious, water, tropics meronym : snout
</bodyText>
<figureCaption confidence="0.998713">
Figure 1: Underspecified input and progressive refinement
</figureCaption>
<bodyText confidence="0.999993181818182">
As we can see in the figure above, there are two critical moments in word production: meaning speci-
fication (A-B) and sound-form encoding (B-C). It is generally this latter part that poses problems. How
to resolve it has been nicely illustrated in an experiment done by (James and Burke, 2000). They showed
that phonologically similar words of the target could resolve the TOT state. To show this they put partici-
pants into the TOT state by presenting them low-frequency words: abdicate, amnesty, anagram,.... Those
who failed were used for the experiment. Next the experimenters read a list of words containing parts
of the syllables of the TOT word. For example, if the definition ’to renounce a throne’ put a participant
into a TOT state, he was asked to read aloud a list of ten words, like abstract, indigent, truncate, each of
which contains a syllable of the target. For the other half, participants were given a list of 10 phonologi-
cally unrelated words. After that participants were primed again to produce the elusive word (abdicate).
As the results clearly showed those who were asked to read phonologically related words resolved more
</bodyText>
<figure confidence="0.993637285714286">
specified meaning
(lexicalized concept)
scene
(visual input)
A
reptile
A1
alligator
crocodile
cayman
B C
crocodile
form
1 1 2 3 4 5 6
</figure>
<page confidence="0.997015">
223
</page>
<bodyText confidence="0.9994568">
TOT states than those who were presented with unrelated words.
This is a nice example. Alas, we cannot make use of it, as, not knowing the target word we cannot
increase (directly) the activation level of the phonological form. Hence we have to resort to another
method, namely, association networks (see section 3). Let us see how search strategies may depend on
cognitive states.
</bodyText>
<sectionHeader confidence="0.727946" genericHeader="method">
4 Search strategies function of variable cognitive states
</sectionHeader>
<bodyText confidence="0.999867459459459">
Search is always based on knowledge. Depending on the knowledge available at the onset one will
perform a specific kind of search. Put differently, there are different information needs as there are
different search strategies.
There are at least three things that authors typically know when looking for a specific word: its mean-
ing (definition) or at least part of it (this is the most frequent situation), its lexical relations (hyponymy,
synonymy, antonymy, etc.), and the collocational or encyclopedic relations it entertains with other words
(Paris-city, Paris-French capital, etc.). Hence there are several ways to access a word (see Figure 1): via
its meaning (concepts, meaning fragments), via syntagmatic links (thesaurus- or encyclopedic relations),
via its form (rhymes), via lexical relations, via syntactic patterns (search in a corpus), and, of course, via
another language (translation). Note that access by meaning is the golden route, i.e. the most normal
way. We tend to use other means only if we fail to access straight away the desired word.
I will consider here only one of them, word associations (mostly, encyclopaedic relations). Note
that, people being in the TOT-state clearly know more than that. Psychologists who have studied this
phenomenon (Brown and McNeill, 1996; Vigliocco et al., 1997) have found that their subjects had
access not only to meanings (the words definition), but also to information concerning grammar (gender)
and lexical form: sound, morphology and part of speech. While all this information could be used
to constrain the search space, the ideal dictionary being multiply indexed, I will deal here only with
semantically related words (associations, collocations in the large sense of the word). Before discussing
how such a dictionary could be built and used, let us consider a possible search scenario.
I start from the assumption that in our mind, all words are connected, the mental lexicon (brain) being a
network. This being so, anything can be reached from anywhere. The user enters the graph by providing
whatever comes to his mind (source-word), following the links until he has reached the target. As has
been shown (Motter et al., 2002), our mental lexicon has small-world properties: very few steps are
needed to get from the source-word to the target word. Another assumption I make is the following:
when looking for a word, people tend to start from a close neighbour, which implies that users have
some meta-knowledge containing the topology of the network (or the structure of their mental lexicon):
what are the nodes, how are they linked to their neighbours, and what are more or less direct neighbours
? For example, we know that black is related to white, and that both words are fairly close, at least a lot
closer than, say, black and flower.
Search can be viewed as a dialogue. The user provides as input the words that a concept he wishes
to express evokes, and the system displays then all (directly) connected words. If this list contains the
target search stops, otherwise it will continue. The user chooses a word of the list, or keys in an entirely
different word. The first part described is the simplest case: the target is a direct neighbour. The second
addresses the problem of indirect associations, the distance being bigger than 1.
Before presenting our method in section 3, let us say a few words about existing resources. Since
the conversion of meaning to sounds is mediated via a lexicon, one may wonder to what extent existing
resources can be of help.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.9999178">
While there are many kinds of dictionaries or lexical resources, very few of them can be said to meet
truly the authors’ needs. To be fair though, one must admit that great efforts have been made to improve
the situation both with respect to lexical resources and electronic dictionaries. In fact, there are quite
a few onomasiological dictionaries (van Sterkenburg, 2003). For example, Roget’s Thesaurus (Roget,
1852), analogical dictionaries (Boissi`ere, 1862; Robert et al., 1993), Longman’s Language Activator
</bodyText>
<page confidence="0.991246">
224
</page>
<bodyText confidence="0.999764086956522">
(Summers, 1993), various network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990),
MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989),
’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997).
Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various
collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Ed-
monds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia).
Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s
Sketch Engine (Kilgarriff et al., 2004). There has also been quite a lot of work on the time-course of
word production, i.e. the way how one gets progressively from a more or less precise idea to its expres-
sion, a word expressed in written or spoken form. See for example (Levelt et al., 1999; Dell et al., 1999).
Clearly, a lot of progress has been made during the last two decades, yet more can be done especially
with respect to indexing (the organization of the data) and navigation.
Two key idea underlying modern lexical resources are the notions of ’graphs’ and ’association’. For a
useful introduction to graph-based natural language processing, see (Mihalcea and Radev, 2011). Associ-
ations have a long history. The idea according to which the mental lexicon (or encyclopedia) is basically
an associative network, composed of nodes (words or concepts) and links (associations) is not new at all.
Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in
work done by philosophers (Locke, Hume), physiologists (James &amp; Stuart Mills), psychologists (Galton,
1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvan-
eveldt, 1989). For good introductions see (H¨ormann, 1972; Cramer, 1968) and more recently (Spitzer,
1999). The notion of association is also implicit in work on semantic networks (Quillian, 1968), hyper-
text (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al., 1999) and, of course, in WordNet
(Miller, 1990; Fellbaum, 1998).
</bodyText>
<sectionHeader confidence="0.95571" genericHeader="method">
6 The framework for building and using our resource
</sectionHeader>
<bodyText confidence="0.99991552">
To understand the problems at stake, I describe the communicative setting (system, user), the existing
and necessary components, as well as the information flow (see figure 2).
Imagine an author wishing to convey the name of a special beverage (’mocha’) commonly found in
coffee shops. Failing to do so, he tries to find it in a lexicon. Since dictionaries are too huge to be
scanned from beginning to the end, I suggest another solution : reduce the search space based on some
input (step-1) and presentation of the results (all directly related words) in a clustered form (step-2).
More concretely speaking, I suggest to have a system that accepts whatever comes to an author’s mind,
say ’coffee’ in our ’mocha’ case, to present then all directly associated words. Put differently, given
some cue, we want the system to guess the user’s goal (the elusive word). If this list contains the target,
search stops, otherwise the user will pick one of the associated terms or provide an entirely new word
and the whole process is repeated again, that is, the system will come up with a new set of proposals.
What I’ve just described here corresponds to step-1 in figure 2 (see next page). While there are a
number of resources that one could use to allow for this transition, I rely here on the E.A.T., i.e. the
’Edinburgh Association Thesaurus’. Note that the output produced by this resource is still too big to be
really useful. Suppose that each input word yielded 50 outputs (the EAT often presents 100, and one
could think of a lot more). Having provided three words the system will return 150 outputs. Actually, it
will take an intersection of the associated words to avoid redundancies. Since this list is still too big to
be scanned linearly (one by one), I suggest to structure it, by clustering words into categories (step-2).
This yields a tree whose leaves are words (our potential targets) and whose nodes are categories, that
is, also words, but with a completely different status, namely to group words. Category names function
like signposts, signalling the user the direction to go. Note that it is not the system that decides on the
direction, but the user. Seeing the names of the categories he can make reasonable guesses concerning
their content. Categories act somehow like signposts signaling the user the kind of words he is likely
to find if he goes one way or another. Indeed, knowing the name of a category (fruit, animal), the user
can guess the kind of words contained in each bag, a prediction which is all the more likely as each
</bodyText>
<footnote confidence="0.932569">
3http://onelook.com/reverse-dictionary.shtml
</footnote>
<page confidence="0.995537">
225
</page>
<figure confidence="0.998841222222222">
p cate
e wo
COOKY
DRINK
setof
LO
TASTE OOD
p
2° via worda reso
D
Is
D
the
BREAK 4 0.04
ESPRESSO 4004
di:
POT 3 003word:
CREAM 2 0.02
ne HOUSE 2 0.02 or i
MILK 2 002
i04 01DRI1K 1 0.1
assoc
Itp1 ys
(E
de
NC
� �
</figure>
<figureCaption confidence="0.999483">
Figure 2: Architecture of the components and information flow
</figureCaption>
<page confidence="0.994794">
226
</page>
<bodyText confidence="0.999871555555556">
category contains only terms directly associated with the source word. Assuming that the user knows
the category of the searched word,4 he should be able to look in the right bag and take the best turn.
Navigating in a categorial tree, the user can search at a fairly high level (class) rather than at the level of
words (instances). This reduces not only the cognitive load, but it increases also chances of finding the
target, while speeding up search, i.e. the time needed to find a word.
Remains the question of how to build this resource and how to accomplish these two steps. I have
explained already the first transition going from A-B. The system enriches the input by taking all associ-
ated words, words he will find in the EAT. Obviously, other strategies are possible, and this is precisely
one of the points I would like to experiment with in the future : check which knowledge source (corpus,
association thesaurus, lexical resource) produces the best set of candidates, i.e. the best search space and
the best structure in order to navigate. The solution of the second step is quite a bit more complicated,
as putting words into clusters is one thing, naming them is another. Yet, arguably this is a crucial step,
as it allows the user to navigate on this basis. Of course, one could question the very need of labels, and
perhaps this is not too much of an issue if we have only say, 3-4 categories. I am nevertheless strongly
convinced that the problem is real, as soon as the number of categories (hence the words to be classified)
grows. To conclude, I think it is fair to say that the first stage is clearly within reach, while the automatic
construction of the categorical tree remains a true challenge despite the vast literature devoted to this
topic or to strongly related problems (Zhang et al., 2012; Biemann, 2012; Everitt et al., 2011).
</bodyText>
<sectionHeader confidence="0.954048" genericHeader="conclusions">
7 Outlook and conclusion
</sectionHeader>
<bodyText confidence="0.999954714285714">
I have started from the observation that words are important and that their accessibility can be a problem.
In order to help a dictionary user to overcome it I have presented a method showing promise. In particular,
I have shown how to reduce the search space, how to present a set of plausible candidates and what needs
to be done next (clustering and naming them) to reduce the search space and to support navigation. In
particular, I have proposed the creation of a categorial tree whose leaves contain the (potential target)
words and the nodes the names of their categories. The role of the latter is to avoid the user to search
in non relevant parts of the tree. Since words are grouped in named clusters, the user does not have to
go through the whole list of words anymore. Rather he navigates in a tree (top-to-botton, left to right),
choosing first the category and then its members, to check whether any of them corresponds to the desired
target word.
Even if the details of this work turn out to be wrong (this is just preliminary work), I believe and hope
that the overall framework is of the right sort, allowing for a rich set of experimentation in particular
with respect to determining the search space and the clustering. Concerning evaluation, the ultimate
judge will be, of course, the user, as only s/he can tell us whether our resource fits his/her needs or goals.
</bodyText>
<sectionHeader confidence="0.998557" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997733727272727">
Jean Aitchison. 2003. Words in the Mind: an Introduction to the Mental Lexicon (3d edition). Blackwell, Oxford.
Morton Benson, Evelyn Benson, and Robert A Ilson. 2010. The BBI Combinatory Dictionary of English. John
Benjamins, Philadelphia.
Theodore Bernstein. 1975. Bernstein’s Reverse Dictionary. Crown, New York.
Chris Biemann. 2012. Structure Discovery in Natural Language. Springer.
Jean Baptiste Prudence Boissi`ere. 1862. Dictionnaire analogique de la langue franc¸aise : r´epertoire complet des
mots par les id´ees et des id´ees par les mots. Larousse et A. Boyer, Paris.
Roger Brown and David McNeill. 1996. The tip of the tounge phenomenon. Journal of Verbal Learning and
Verbal Behaviour, 5:325–337.
Allan S. Brown. 1991. The tip of the tongue experience a review and evaluation. Psychological Bulletin, 10:204–
223.
</reference>
<footnote confidence="0.9607445">
4A fact which has been systematically observed for people being in the ’tip of the tongue state’ who may tell the listener
that they are looking for the name of ”a fruit typically found in PLACE”, in order to get ’kiwi’.
</footnote>
<page confidence="0.992413">
227
</page>
<reference confidence="0.995597697674419">
Vannevar Bush. 1945. As we may think. The Atlantic Monthly, 176:101–108.
Phebe Cramer. 1968. Word association. Academic Press, New York.
Ferdinand de Saussure. 1916. Cours de linguistique g´en´erale. Payot, Paris.
James Deese. 1965. The structure of associations in language and thought. Johns Hopkins Press.
Gary Dell, Franklin Chang, and Zenzi M. Griffin. 1999. Connectionist models of language production: Lexical
access and grammatical encoding. Cognitive Science, 23:517–542.
Zhendong Dong and Qiang Dong. 2006. HOWNET and the computation of meaning. World Scientific, London.
David Edmonds, editor. 1999. The Oxford Reverse Dictionary. Oxford University Press, Oxford, Oxford.
Brian S. Everitt, Sabine Landau, Morven Leese, and Daniel Stahl. 2011. Cluster Analysis. John Wiley and Sons.
Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database and some of its Applications. MIT
Press.
Thierry Fontenelle. 1997. Turning a Bilingual Dictionary into a Lexical-Semantic Database. Max Niemeyer,
T¨ubingen.
Siegmund Freud. 1901. Psychopathology of everyday life. Payot, Paris, 1997 edition.
Victoria Fromkin, editor. 1973. Speech errors as linguistic evidence. Mouton, The Hague.
Victoria Fromkin. 1980. Errors in linguistic performance: Slips of the tongue, ear, pen and hand.
Victoria Fromkin. 1993. Speech production. In J. Berko-Gleason and N. Bernstein Ratner, editors, Psycholinguis-
tics. Harcourt, Brace, Jovanovich, Fort Worth, TX.
Francis Galton. 1880. Psychometric experiments. Brain, 2:149–162.
Hans H¨ormann. 1972. Introduction a` la psycholinquistique. Larousse, Paris, France.
Lori James and Deborah Burke. 2000. Phonological priming effects on word retrieval and tip-of-the-tongue
experiences in young and older adults. Journal of Experimental Psychology: Learning, Memory, and Cognition,
6(26):1378—1391.
James Jenkins. 1970. The 1952 Minnesota word association norms. In L. Postman and G. Kepper, editors, Norms
of Word Association, pages 1–38. Academic Press, New York, NY.
Carl Jung and Franz Riklin. 1906. Experimentelle Untersuchungen ¨uber Assoziationen Gesunder. In C. G. Jung,
editor, Diagnostische Assoziationsstudien, pages 7–145. Barth, Leipzig, Germany.
John Kahn. 1989. Reader’s Digest Reverse Dictionary. Reader’s Digest, London.
Adam Kilgarriff, Pavel Rychl´y, Pavel Smrˇz, and David Tugwell. 2004. The Sketch Engine. In Proceedings of the
Eleventh EURALEX International Congress, pages 105–116, Lorient, France.
George Kiss, Christine Amstrong, and Robert Milroy. 1972. The associative thesaurus of English. Ediburgh
University Press, Edinburgh.
William Levelt and Herbert Schriefers. 1987. Stages of lexical access. In G. Kempen, editor, Natural Lan-
guage Generation: New Results in Artificial Intelligence, Psychology, and Linguistics, pages 395–404. Nijhoff,
Dordrecht.
William Levelt, A. Roelofs, and A. Meyer. 1999. A theory of lexical access in speech production. Behavioral and
Brain Sciences, 22(1):1–75.
William Levelt. 1989. Speaking : From Intention to Articulation. MIT Press, Cambridge, MA.
William Levelt. 1999. Language production: a blueprint of the speaker. In C. Brown and P. Hagoort, editors,
Neurocognition of Language, pages 83—122. Oxford University Press.
Igor Aleksandroviˇc Mel’ˇcuk and Alain Polgu`ere. 2007. Lexique actif du franc¸ais : l’apprentissage du vocabu-
laire fond´e sur 20 000 d´erivations s´emantiques et collocations du franc¸ais. Champs linguistiques. De Boeck,
Bruxelles.
</reference>
<page confidence="0.974682">
228
</page>
<reference confidence="0.999595046511628">
Rada Mihalcea and Dragomir Radev. 2011. Graph-Based Natural Language Processing and Information Re-
trieval. Cambridge University Press, Cambridge, UK.
George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Introduction
to WordNet: An on-line lexical database. International Journal of Lexicography, 3(4), pages 235–244.
George Armitage Miller. 1990. Wordnet: An on-line lexical database. International Journal of Lexicography,
3(4).
Adilson E. Motter, Alessandro P. S. de Moura, Ying-Cheng Lai, and Partha Dasgupta. 2002. Topology of the
conceptual network of language. Physical Review E, 65(6).
Douglas Nelson, Cathy McEvoy, and Thomas Schreiber. 1998. The university of South Florida word association,
rhyme, and word fragment norms.
Ted Nelson. 1967. Xanadu projet hypertextuel.
Ross Quillian. 1968. Semantic memory. In M. Minsky, editor, Semantic Information Processing, pages 216–270.
MIT Press, Cambridge, MA.
Stephen Richardson, William Dolan, and Lucy Vanderwende. 1998. Mindnet: Acquiring and structuring semantic
information from text. In ACL-COLING’98, pages 1098–1102.
Paul Robert, Alain Rey, and J. Rey-Debove. 1993. Dictionnaire alphabetique et analogique de la Langue
Franc¸aise. Le Robert, Paris.
Peter Roget. 1852. Thesaurus of English Words and Phrases. Longman, London.
Michael Rundell and G. (Eds.) Fox. 2002. Macmillan English Dictionary for Advanced Learners. Macmillan,
Oxford.
Roger Schvaneveldt, editor. 1989. Pathfinder Associative Networks: studies in knowledge organization. Ablex,
Norwood, New Jersey, US.
Bennett Schwartz. 2002. Tip-of-the-tongue states: Phenomenology, mechanism, and lexical retrieval. Lawrence
Erlbaum Associates, Mahwah, NJ.
Gerardo Sierra. 2000. The onomasiological dictionary: a gap in lexicography. In Proceedings of the Ninth Euralex
International Congress, pages 223–235, IMS, Universit¨at Stuttgart.
Manfred Spitzer. 1999. The mind within the net: models of learning, thinking and acting. MIT Press, Cambridge,
MA.
Joseph Paul Stemberger. 1985. An interactive activation model of language production. In A. W. Ellis, editor,
Progress in the Psychology of Language, volume 1, pages 143–186. Erlbaum.
Della Summers. 1993. Language Activator: the world’s first production dictionary. Longman, London.
Piet van Sterkenburg. 2003. Onomasiological specifications and a concise history of onomasiological dictionar-
ies. In A Practical Guide to Lexicography, volume A Practical Guide to Lexicography, pages 127—143. John
Benjamins, Amsterdam.
Gabriella Vigliocco, Tiziana Antonini, and Garrett Merrill. 1997. Grammatical gender is on the tip of italian
tongues. Psychological Science, 4(8):314–317.
Michael Vitevitch. 2008. What can graph theory tell us about word learning and lexical retrieval? Journal of
Speech, Language, and Hearing Research, 51:408—422.
David Wilkins. 1972. Linguistics and Language Teaching. Edward Arnold, London.
Ziqi Zhang, Anna Lisa Gentile, and Fabio Ciravegna. 2012. Recent advances in methods of lexical semantic
relatedness – a survey. Journal of Natural Language Engineering, Cambridge Universtiy Press, 19(4):411–479.
Michael Zock. 1996. The power of words in message planning. In Proceedings of the 16th conference on
Computational linguistics, pages 990–995, Morristown, NJ, USA. Association for Computational Linguistics.
</reference>
<page confidence="0.998913">
229
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.866762">
<title confidence="0.9989385">Wordfinding Problems and How to Overcome them With the Help of a Computer</title>
<author confidence="0.994825">Michael</author>
<affiliation confidence="0.9973">LIF-CNRS /</affiliation>
<address confidence="0.9798715">163, Avenue de 13288 Marseille /</address>
<email confidence="0.978685">michael.zock@lif.univ-mrs.fr</email>
<abstract confidence="0.995823">Our ultimate goal is to help authors to find an elusive word. Whenever we need a word, we look it up in the place where it is stored, the dictionary or the mental lexicon. The question is how do we manage to find the word, and how do we succeed to do this so quickly? While these are difficult questions, I believe to have some practical answers for them. Since it is unreasonable to perform search in the entire lexicon, I suggest to start by reducing this space (step-1) and to present then the remaining candidates in a clustered and labeled form, i.e. categorial tree (step-2). The goal of this second step is to support navigation. Search space is determined by considering words directly related to the input, i.e. direct neighbors (associations/co-occurrences). To this end many resources could be used. For example, one may consider an associative network like the Edinburgh Association Thesaurus (E.A.T.). As this will still yield too many hits, I suggest to cluster and label the outputs. This labeling is crucial for navigation, as we want users to find the target quickly, rather than drown them under a huge, unstructured list of words. Note, that in order to determine properly the initial search space we must have already well understood the input as our list will contain a lot of noise, presenting together with which is not quite what we want, since some of these candidates are irrelevant, i.e. beyond the scope of the user’s goal.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean Aitchison</author>
</authors>
<title>Words in the Mind: an Introduction to the Mental Lexicon (3d edition).</title>
<date>2003</date>
<publisher>Blackwell,</publisher>
<location>Oxford.</location>
<contexts>
<context position="7388" citStr="Aitchison, 2003" startWordPosition="1239" endWordPosition="1240">item directly related to the tw (direct neighbor). If the user does not know too much concerning the topology of the network, he does know quite a bit concerning the tw,2 information the system has no clue of at this point. Eventhough it knows ’everyone’ in the network, it cannot do mind-reading, i.e. guess the precise word a user has in mind (tw) when providing a specific input (sw). Yet the user can. Even if he cannot access the word at a given moment, he can recognize it when seeing it (alone or in a list). This fact is well established in the literature on the ’tip-of-the-tongue problem’ (Aitchison, 2003). 3 From mind to mouth, or what characterizes the process of word production? According to the father of modern linguistics (de Saussure, 1916), word forms (signifier) and their associated meaning (signified) are but one, called the sign. They are said to be an inseparable unit. This is in sharp contrast to what psychologists tell us about words synthesis. For example, one of the leading specialists of language production (Levelt, 1989; Levelt, 1999) has convincingly shown that, when speaking 1For example, he knows that for a given word form there are similar forms in terms of sound or meaning</context>
</contexts>
<marker>Aitchison, 2003</marker>
<rawString>Jean Aitchison. 2003. Words in the Mind: an Introduction to the Mental Lexicon (3d edition). Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morton Benson</author>
<author>Evelyn Benson</author>
<author>Robert A Ilson</author>
</authors>
<date>2010</date>
<booktitle>The BBI Combinatory Dictionary of English. John Benjamins,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="18228" citStr="Benson et al., 2010" startWordPosition="2962" endWordPosition="2965">aries (van Sterkenburg, 2003). For example, Roget’s Thesaurus (Roget, 1852), analogical dictionaries (Boissi`ere, 1862; Robert et al., 1993), Longman’s Language Activator 224 (Summers, 1993), various network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990), MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989), ’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997). Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). There has also been quite a lot of work on the time-course of word production, i.e. the way how one gets progressively from a more or less precise idea to its expression, a word expressed in written or spoken form. See for example (Levelt et al., 1999; Dell et al., 1999). Clearly, a lot of progress has bee</context>
</contexts>
<marker>Benson, Benson, Ilson, 2010</marker>
<rawString>Morton Benson, Evelyn Benson, and Robert A Ilson. 2010. The BBI Combinatory Dictionary of English. John Benjamins, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theodore Bernstein</author>
</authors>
<title>Bernstein’s Reverse Dictionary.</title>
<date>1975</date>
<booktitle>Structure Discovery in Natural Language.</booktitle>
<publisher>Crown,</publisher>
<location>New York. Chris Biemann.</location>
<contexts>
<context position="18267" citStr="Bernstein, 1975" startWordPosition="2968" endWordPosition="2969">Roget’s Thesaurus (Roget, 1852), analogical dictionaries (Boissi`ere, 1862; Robert et al., 1993), Longman’s Language Activator 224 (Summers, 1993), various network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990), MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989), ’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997). Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). There has also been quite a lot of work on the time-course of word production, i.e. the way how one gets progressively from a more or less precise idea to its expression, a word expressed in written or spoken form. See for example (Levelt et al., 1999; Dell et al., 1999). Clearly, a lot of progress has been made during the last two decades, yet</context>
</contexts>
<marker>Bernstein, 1975</marker>
<rawString>Theodore Bernstein. 1975. Bernstein’s Reverse Dictionary. Crown, New York. Chris Biemann. 2012. Structure Discovery in Natural Language. Springer.</rawString>
</citation>
<citation valid="false">
<title>Dictionnaire analogique de la langue franc¸aise : r´epertoire complet des mots par les id´ees et des id´ees par les mots. Larousse</title>
<location>Paris.</location>
<marker></marker>
<rawString>Jean Baptiste Prudence Boissi`ere. 1862. Dictionnaire analogique de la langue franc¸aise : r´epertoire complet des mots par les id´ees et des id´ees par les mots. Larousse et A. Boyer, Paris.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Brown</author>
<author>David McNeill</author>
</authors>
<title>The tip of the tounge phenomenon.</title>
<date>1996</date>
<journal>Journal of Verbal Learning and Verbal Behaviour,</journal>
<pages>5--325</pages>
<contexts>
<context position="3050" citStr="Brown and McNeill, 1996" startWordPosition="515" endWordPosition="518">to work on one of these tasks: word access, also called retrieval or wordfinding. Imagine the following situation: your goal is to express the following ideas: superior dark coffee made of beans from Arabia” by a single word, but you cannot access the corresponding form mocha, even though you know it, since you’ve used it not so long ago. This kind of problem, known as the tip-of-the-tongue (TOT)-problem, has received a lot of attention from psychologists (Schwartz, 2002; Brown, 1991). It has always been pointed out that people being in this state know quite a bit concerning the elusive word (Brown and McNeill, 1996). Hence, using it should allow us to reduce the search space. Put differently, it would be nice to have a system capable to use whatever you have, incomplete as it may be, to help you find what you cannot recall. For example, for the case at hand, one might think of dark, coffee, beans, and Arabia, to expect from system a set of reasonable candidates, like arabica, espresso, or mocha. In the remainder of this paper I will try to show how this might be achieved, but before doing This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings fo</context>
<context position="14998" citStr="Brown and McNeill, 1996" startWordPosition="2440" endWordPosition="2443">pts, meaning fragments), via syntagmatic links (thesaurus- or encyclopedic relations), via its form (rhymes), via lexical relations, via syntactic patterns (search in a corpus), and, of course, via another language (translation). Note that access by meaning is the golden route, i.e. the most normal way. We tend to use other means only if we fail to access straight away the desired word. I will consider here only one of them, word associations (mostly, encyclopaedic relations). Note that, people being in the TOT-state clearly know more than that. Psychologists who have studied this phenomenon (Brown and McNeill, 1996; Vigliocco et al., 1997) have found that their subjects had access not only to meanings (the words definition), but also to information concerning grammar (gender) and lexical form: sound, morphology and part of speech. While all this information could be used to constrain the search space, the ideal dictionary being multiply indexed, I will deal here only with semantically related words (associations, collocations in the large sense of the word). Before discussing how such a dictionary could be built and used, let us consider a possible search scenario. I start from the assumption that in ou</context>
</contexts>
<marker>Brown, McNeill, 1996</marker>
<rawString>Roger Brown and David McNeill. 1996. The tip of the tounge phenomenon. Journal of Verbal Learning and Verbal Behaviour, 5:325–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Allan S Brown</author>
</authors>
<title>The tip of the tongue experience a review and evaluation.</title>
<date>1991</date>
<journal>Psychological Bulletin,</journal>
<volume>10</volume>
<pages>223</pages>
<contexts>
<context position="2915" citStr="Brown, 1991" startWordPosition="493" endWordPosition="494"> offer so far to help humans to memorize, find or retrieve them. Hoping to contribute to a change for this, I have started to work on one of these tasks: word access, also called retrieval or wordfinding. Imagine the following situation: your goal is to express the following ideas: superior dark coffee made of beans from Arabia” by a single word, but you cannot access the corresponding form mocha, even though you know it, since you’ve used it not so long ago. This kind of problem, known as the tip-of-the-tongue (TOT)-problem, has received a lot of attention from psychologists (Schwartz, 2002; Brown, 1991). It has always been pointed out that people being in this state know quite a bit concerning the elusive word (Brown and McNeill, 1996). Hence, using it should allow us to reduce the search space. Put differently, it would be nice to have a system capable to use whatever you have, incomplete as it may be, to help you find what you cannot recall. For example, for the case at hand, one might think of dark, coffee, beans, and Arabia, to expect from system a set of reasonable candidates, like arabica, espresso, or mocha. In the remainder of this paper I will try to show how this might be achieved,</context>
</contexts>
<marker>Brown, 1991</marker>
<rawString>Allan S. Brown. 1991. The tip of the tongue experience a review and evaluation. Psychological Bulletin, 10:204– 223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vannevar Bush</author>
</authors>
<title>As we may think. The Atlantic Monthly,</title>
<date>1945</date>
<pages>176--101</pages>
<contexts>
<context position="19911" citStr="Bush, 1945" startWordPosition="3230" endWordPosition="3231">, composed of nodes (words or concepts) and links (associations) is not new at all. Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James &amp; Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989). For good introductions see (H¨ormann, 1972; Cramer, 1968) and more recently (Spitzer, 1999). The notion of association is also implicit in work on semantic networks (Quillian, 1968), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al., 1999) and, of course, in WordNet (Miller, 1990; Fellbaum, 1998). 6 The framework for building and using our resource To understand the problems at stake, I describe the communicative setting (system, user), the existing and necessary components, as well as the information flow (see figure 2). Imagine an author wishing to convey the name of a special beverage (’mocha’) commonly found in coffee shops. Failing to do so, he tries to find it in a lexicon. Since dictionaries are too huge to be scanned from beginning to the end, I suggest another </context>
</contexts>
<marker>Bush, 1945</marker>
<rawString>Vannevar Bush. 1945. As we may think. The Atlantic Monthly, 176:101–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phebe Cramer</author>
</authors>
<title>Word association.</title>
<date>1968</date>
<publisher>Academic Press,</publisher>
<location>New York. Ferdinand</location>
<contexts>
<context position="6460" citStr="Cramer, 1968" startWordPosition="1073" endWordPosition="1074">eing connected, everything is reachable, at least in principle. Search may require several steps, but in most cases the number of steps is surprisingly small. As mentioned already, the user definitely has some knowledge concerning words, their components and their organisation in the mental lexicon, but this knowledge is by no means complete. The user also has some knowledge (or, more precisely, meta-knowledge) concerning the topology of the graph,1 but he certainly does not know as much as the system. The fact that an author does have this kind of knowledge is revealed via word associations (Cramer, 1968; Deese, 1965; Nelson et al., 1998; Kiss et al., 1972) and via the observed average path length (Vitevitch, 2008) needed in order to get from some starting point (sw) to the goal (tw). This path is generally quite short. It hardly ever exceeds three steps, and in many cases even less: search is launched via an item directly related to the tw (direct neighbor). If the user does not know too much concerning the topology of the network, he does know quite a bit concerning the tw,2 information the system has no clue of at this point. Eventhough it knows ’everyone’ in the network, it cannot do mind</context>
<context position="19763" citStr="Cramer, 1968" startWordPosition="3207" endWordPosition="3208">d Radev, 2011). Associations have a long history. The idea according to which the mental lexicon (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not new at all. Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James &amp; Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989). For good introductions see (H¨ormann, 1972; Cramer, 1968) and more recently (Spitzer, 1999). The notion of association is also implicit in work on semantic networks (Quillian, 1968), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al., 1999) and, of course, in WordNet (Miller, 1990; Fellbaum, 1998). 6 The framework for building and using our resource To understand the problems at stake, I describe the communicative setting (system, user), the existing and necessary components, as well as the information flow (see figure 2). Imagine an author wishing to convey the name of a special beverage (’mocha’) commonly found in coffee sh</context>
</contexts>
<marker>Cramer, 1968</marker>
<rawString>Phebe Cramer. 1968. Word association. Academic Press, New York. Ferdinand de Saussure. 1916. Cours de linguistique g´en´erale. Payot, Paris. James Deese. 1965. The structure of associations in language and thought. Johns Hopkins Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gary Dell</author>
<author>Franklin Chang</author>
<author>Zenzi M Griffin</author>
</authors>
<title>Connectionist models of language production: Lexical access and grammatical encoding.</title>
<date>1999</date>
<journal>Cognitive Science,</journal>
<pages>23--517</pages>
<contexts>
<context position="9923" citStr="Dell et al., 1999" startWordPosition="1644" endWordPosition="1647">psycholinguists it is a schema, i.e. an abstract form representing a specific meaning (a lexicalized concept) and a syntactic category (part of speech), but it lacks entirely specific values concerning the form (sounds/graphemes). This is being take care of at the next step (sound form encoding). In short, in contrast to Saussure’s view, the information contributing to what we commonly call words (lemma or word forms) is distributed. This is a well established empirical fact observed by psychologists working on the time course of word production (Stemberger, 1985; Levelt and Schriefers, 1987; Dell et al., 1999), as by those who analyze and interpret speech errors (Fromkin, 1973; Fromkin, 1980; Fromkin, 1993). Yet, what concerns us here in particular is the following: as noted, speakers go from meanings to sounds via lexical concepts (abstract word forms). More importantly, the conceptual input may lack information to determine a precise lexical form. Put differently, rather than starting from a full fledged definition or complete meaning representation, authors may well start from an underspecified input (’small bird’ rather than ’sparrow’). Note that the specific requirements of a culture may help </context>
<context position="18792" citStr="Dell et al., 1999" startWordPosition="3056" endWordPosition="3059">various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). There has also been quite a lot of work on the time-course of word production, i.e. the way how one gets progressively from a more or less precise idea to its expression, a word expressed in written or spoken form. See for example (Levelt et al., 1999; Dell et al., 1999). Clearly, a lot of progress has been made during the last two decades, yet more can be done especially with respect to indexing (the organization of the data) and navigation. Two key idea underlying modern lexical resources are the notions of ’graphs’ and ’association’. For a useful introduction to graph-based natural language processing, see (Mihalcea and Radev, 2011). Associations have a long history. The idea according to which the mental lexicon (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not new at all. Actually</context>
</contexts>
<marker>Dell, Chang, Griffin, 1999</marker>
<rawString>Gary Dell, Franklin Chang, and Zenzi M. Griffin. 1999. Connectionist models of language production: Lexical access and grammatical encoding. Cognitive Science, 23:517–542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhendong Dong</author>
<author>Qiang Dong</author>
</authors>
<title>HOWNET and the computation of meaning. World Scientific,</title>
<date>2006</date>
<publisher>Oxford University Press,</publisher>
<location>London. David Edmonds, editor.</location>
<contexts>
<context position="17946" citStr="Dong and Dong, 2006" startWordPosition="2924" endWordPosition="2927">es, very few of them can be said to meet truly the authors’ needs. To be fair though, one must admit that great efforts have been made to improve the situation both with respect to lexical resources and electronic dictionaries. In fact, there are quite a few onomasiological dictionaries (van Sterkenburg, 2003). For example, Roget’s Thesaurus (Roget, 1852), analogical dictionaries (Boissi`ere, 1862; Robert et al., 1993), Longman’s Language Activator 224 (Summers, 1993), various network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990), MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989), ’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997). Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). There has also been quite</context>
</contexts>
<marker>Dong, Dong, 2006</marker>
<rawString>Zhendong Dong and Qiang Dong. 2006. HOWNET and the computation of meaning. World Scientific, London. David Edmonds, editor. 1999. The Oxford Reverse Dictionary. Oxford University Press, Oxford, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian S Everitt</author>
<author>Sabine Landau</author>
<author>Morven Leese</author>
<author>Daniel Stahl</author>
</authors>
<title>Cluster Analysis.</title>
<date>2011</date>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="24825" citStr="Everitt et al., 2011" startWordPosition="4086" endWordPosition="4089"> the user to navigate on this basis. Of course, one could question the very need of labels, and perhaps this is not too much of an issue if we have only say, 3-4 categories. I am nevertheless strongly convinced that the problem is real, as soon as the number of categories (hence the words to be classified) grows. To conclude, I think it is fair to say that the first stage is clearly within reach, while the automatic construction of the categorical tree remains a true challenge despite the vast literature devoted to this topic or to strongly related problems (Zhang et al., 2012; Biemann, 2012; Everitt et al., 2011). 7 Outlook and conclusion I have started from the observation that words are important and that their accessibility can be a problem. In order to help a dictionary user to overcome it I have presented a method showing promise. In particular, I have shown how to reduce the search space, how to present a set of plausible candidates and what needs to be done next (clustering and naming them) to reduce the search space and to support navigation. In particular, I have proposed the creation of a categorial tree whose leaves contain the (potential target) words and the nodes the names of their categ</context>
</contexts>
<marker>Everitt, Landau, Leese, Stahl, 2011</marker>
<rawString>Brian S. Everitt, Sabine Landau, Morven Leese, and Daniel Stahl. 2011. Cluster Analysis. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database and some of its Applications.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database and some of its Applications. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Fontenelle</author>
</authors>
<title>Turning a Bilingual Dictionary into a Lexical-Semantic Database.</title>
<date>1997</date>
<location>Max Niemeyer, T¨ubingen.</location>
<contexts>
<context position="18080" citStr="Fontenelle, 1997" startWordPosition="2942" endWordPosition="2943"> improve the situation both with respect to lexical resources and electronic dictionaries. In fact, there are quite a few onomasiological dictionaries (van Sterkenburg, 2003). For example, Roget’s Thesaurus (Roget, 1852), analogical dictionaries (Boissi`ere, 1862; Robert et al., 1993), Longman’s Language Activator 224 (Summers, 1993), various network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990), MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989), ’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997). Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). There has also been quite a lot of work on the time-course of word production, i.e. the way how one gets progressively from a more or less precise idea to its </context>
</contexts>
<marker>Fontenelle, 1997</marker>
<rawString>Thierry Fontenelle. 1997. Turning a Bilingual Dictionary into a Lexical-Semantic Database. Max Niemeyer, T¨ubingen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siegmund Freud</author>
</authors>
<title>Psychopathology of everyday life.</title>
<date>1901</date>
<editor>edition. Victoria Fromkin, editor.</editor>
<publisher>Payot,</publisher>
<location>Paris,</location>
<contexts>
<context position="19611" citStr="Freud, 1901" startWordPosition="3186" endWordPosition="3187">exical resources are the notions of ’graphs’ and ’association’. For a useful introduction to graph-based natural language processing, see (Mihalcea and Radev, 2011). Associations have a long history. The idea according to which the mental lexicon (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not new at all. Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James &amp; Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989). For good introductions see (H¨ormann, 1972; Cramer, 1968) and more recently (Spitzer, 1999). The notion of association is also implicit in work on semantic networks (Quillian, 1968), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al., 1999) and, of course, in WordNet (Miller, 1990; Fellbaum, 1998). 6 The framework for building and using our resource To understand the problems at stake, I describe the communicative setting (system, user), the existing and necessary components, </context>
</contexts>
<marker>Freud, 1901</marker>
<rawString>Siegmund Freud. 1901. Psychopathology of everyday life. Payot, Paris, 1997 edition. Victoria Fromkin, editor. 1973. Speech errors as linguistic evidence. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fromkin</author>
</authors>
<title>Errors in linguistic performance: Slips of the tongue, ear, pen and hand.</title>
<date>1980</date>
<contexts>
<context position="10006" citStr="Fromkin, 1980" startWordPosition="1659" endWordPosition="1660"> lexicalized concept) and a syntactic category (part of speech), but it lacks entirely specific values concerning the form (sounds/graphemes). This is being take care of at the next step (sound form encoding). In short, in contrast to Saussure’s view, the information contributing to what we commonly call words (lemma or word forms) is distributed. This is a well established empirical fact observed by psychologists working on the time course of word production (Stemberger, 1985; Levelt and Schriefers, 1987; Dell et al., 1999), as by those who analyze and interpret speech errors (Fromkin, 1973; Fromkin, 1980; Fromkin, 1993). Yet, what concerns us here in particular is the following: as noted, speakers go from meanings to sounds via lexical concepts (abstract word forms). More importantly, the conceptual input may lack information to determine a precise lexical form. Put differently, rather than starting from a full fledged definition or complete meaning representation, authors may well start from an underspecified input (’small bird’ rather than ’sparrow’). Note that the specific requirements of a culture may help us to clarify our thoughts, as well as induce biases or imprecisions because of lex</context>
</contexts>
<marker>Fromkin, 1980</marker>
<rawString>Victoria Fromkin. 1980. Errors in linguistic performance: Slips of the tongue, ear, pen and hand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victoria Fromkin</author>
</authors>
<title>Speech production.</title>
<date>1993</date>
<editor>In J. Berko-Gleason and N. Bernstein Ratner, editors, Psycholinguistics. Harcourt,</editor>
<location>Brace, Jovanovich, Fort Worth, TX.</location>
<contexts>
<context position="10022" citStr="Fromkin, 1993" startWordPosition="1661" endWordPosition="1662">ncept) and a syntactic category (part of speech), but it lacks entirely specific values concerning the form (sounds/graphemes). This is being take care of at the next step (sound form encoding). In short, in contrast to Saussure’s view, the information contributing to what we commonly call words (lemma or word forms) is distributed. This is a well established empirical fact observed by psychologists working on the time course of word production (Stemberger, 1985; Levelt and Schriefers, 1987; Dell et al., 1999), as by those who analyze and interpret speech errors (Fromkin, 1973; Fromkin, 1980; Fromkin, 1993). Yet, what concerns us here in particular is the following: as noted, speakers go from meanings to sounds via lexical concepts (abstract word forms). More importantly, the conceptual input may lack information to determine a precise lexical form. Put differently, rather than starting from a full fledged definition or complete meaning representation, authors may well start from an underspecified input (’small bird’ rather than ’sparrow’). Note that the specific requirements of a culture may help us to clarify our thoughts, as well as induce biases or imprecisions because of lexical gaps. Hence</context>
</contexts>
<marker>Fromkin, 1993</marker>
<rawString>Victoria Fromkin. 1993. Speech production. In J. Berko-Gleason and N. Bernstein Ratner, editors, Psycholinguistics. Harcourt, Brace, Jovanovich, Fort Worth, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Psychometric experiments Brain</author>
</authors>
<title>Introduction a` la psycholinquistique.</title>
<date>1972</date>
<location>Larousse, Paris, France.</location>
<marker>Brain, 1972</marker>
<rawString>Francis Galton. 1880. Psychometric experiments. Brain, 2:149–162. Hans H¨ormann. 1972. Introduction a` la psycholinquistique. Larousse, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lori James</author>
<author>Deborah Burke</author>
</authors>
<title>Phonological priming effects on word retrieval and tip-of-the-tongue experiences in young and older adults.</title>
<date>2000</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<volume>6</volume>
<issue>26</issue>
<contexts>
<context position="12269" citStr="James and Burke, 2000" startWordPosition="2001" endWordPosition="2004">quivalent word (syntagmatic associations) synonyms, antonyms (sound related words) large voracious aquatic reptile aqua3c rep3le in another language crocodile: hypernyms, ... crocodile4Nile having a long snout cocodril4crocodile voracious, water, tropics meronym : snout Figure 1: Underspecified input and progressive refinement As we can see in the figure above, there are two critical moments in word production: meaning specification (A-B) and sound-form encoding (B-C). It is generally this latter part that poses problems. How to resolve it has been nicely illustrated in an experiment done by (James and Burke, 2000). They showed that phonologically similar words of the target could resolve the TOT state. To show this they put participants into the TOT state by presenting them low-frequency words: abdicate, amnesty, anagram,.... Those who failed were used for the experiment. Next the experimenters read a list of words containing parts of the syllables of the TOT word. For example, if the definition ’to renounce a throne’ put a participant into a TOT state, he was asked to read aloud a list of ten words, like abstract, indigent, truncate, each of which contains a syllable of the target. For the other half,</context>
</contexts>
<marker>James, Burke, 2000</marker>
<rawString>Lori James and Deborah Burke. 2000. Phonological priming effects on word retrieval and tip-of-the-tongue experiences in young and older adults. Journal of Experimental Psychology: Learning, Memory, and Cognition, 6(26):1378—1391.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Jenkins</author>
</authors>
<title>The 1952 Minnesota word association norms.</title>
<date>1970</date>
<booktitle>Norms of Word Association,</booktitle>
<pages>1--38</pages>
<editor>In L. Postman and G. Kepper, editors,</editor>
<publisher>Academic Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="19683" citStr="Jenkins, 1970" startWordPosition="3196" endWordPosition="3197">useful introduction to graph-based natural language processing, see (Mihalcea and Radev, 2011). Associations have a long history. The idea according to which the mental lexicon (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not new at all. Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James &amp; Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989). For good introductions see (H¨ormann, 1972; Cramer, 1968) and more recently (Spitzer, 1999). The notion of association is also implicit in work on semantic networks (Quillian, 1968), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al., 1999) and, of course, in WordNet (Miller, 1990; Fellbaum, 1998). 6 The framework for building and using our resource To understand the problems at stake, I describe the communicative setting (system, user), the existing and necessary components, as well as the information flow (see figure 2). Imagine an author wishin</context>
</contexts>
<marker>Jenkins, 1970</marker>
<rawString>James Jenkins. 1970. The 1952 Minnesota word association norms. In L. Postman and G. Kepper, editors, Norms of Word Association, pages 1–38. Academic Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Jung</author>
<author>Franz Riklin</author>
</authors>
<title>Experimentelle Untersuchungen ¨uber Assoziationen Gesunder. In</title>
<date>1906</date>
<booktitle>Diagnostische Assoziationsstudien,</booktitle>
<pages>7--145</pages>
<editor>C. G. Jung, editor,</editor>
<location>Barth, Leipzig, Germany. John Kahn.</location>
<contexts>
<context position="19635" citStr="Jung and Riklin, 1906" startWordPosition="3188" endWordPosition="3191">ces are the notions of ’graphs’ and ’association’. For a useful introduction to graph-based natural language processing, see (Mihalcea and Radev, 2011). Associations have a long history. The idea according to which the mental lexicon (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not new at all. Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James &amp; Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989). For good introductions see (H¨ormann, 1972; Cramer, 1968) and more recently (Spitzer, 1999). The notion of association is also implicit in work on semantic networks (Quillian, 1968), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al., 1999) and, of course, in WordNet (Miller, 1990; Fellbaum, 1998). 6 The framework for building and using our resource To understand the problems at stake, I describe the communicative setting (system, user), the existing and necessary components, as well as the informati</context>
</contexts>
<marker>Jung, Riklin, 1906</marker>
<rawString>Carl Jung and Franz Riklin. 1906. Experimentelle Untersuchungen ¨uber Assoziationen Gesunder. In C. G. Jung, editor, Diagnostische Assoziationsstudien, pages 7–145. Barth, Leipzig, Germany. John Kahn. 1989. Reader’s Digest Reverse Dictionary. Reader’s Digest, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Pavel Rychl´y</author>
<author>Pavel Smrˇz</author>
<author>David Tugwell</author>
</authors>
<title>The Sketch Engine.</title>
<date>2004</date>
<booktitle>In Proceedings of the Eleventh EURALEX International Congress,</booktitle>
<pages>105--116</pages>
<location>Lorient, France.</location>
<marker>Kilgarriff, Rychl´y, Smrˇz, Tugwell, 2004</marker>
<rawString>Adam Kilgarriff, Pavel Rychl´y, Pavel Smrˇz, and David Tugwell. 2004. The Sketch Engine. In Proceedings of the Eleventh EURALEX International Congress, pages 105–116, Lorient, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Kiss</author>
<author>Christine Amstrong</author>
<author>Robert Milroy</author>
</authors>
<title>The associative thesaurus of English.</title>
<date>1972</date>
<publisher>Ediburgh University Press,</publisher>
<location>Edinburgh.</location>
<contexts>
<context position="6514" citStr="Kiss et al., 1972" startWordPosition="1081" endWordPosition="1084">t in principle. Search may require several steps, but in most cases the number of steps is surprisingly small. As mentioned already, the user definitely has some knowledge concerning words, their components and their organisation in the mental lexicon, but this knowledge is by no means complete. The user also has some knowledge (or, more precisely, meta-knowledge) concerning the topology of the graph,1 but he certainly does not know as much as the system. The fact that an author does have this kind of knowledge is revealed via word associations (Cramer, 1968; Deese, 1965; Nelson et al., 1998; Kiss et al., 1972) and via the observed average path length (Vitevitch, 2008) needed in order to get from some starting point (sw) to the goal (tw). This path is generally quite short. It hardly ever exceeds three steps, and in many cases even less: search is launched via an item directly related to the tw (direct neighbor). If the user does not know too much concerning the topology of the network, he does know quite a bit concerning the tw,2 information the system has no clue of at this point. Eventhough it knows ’everyone’ in the network, it cannot do mind-reading, i.e. guess the precise word a user has in mi</context>
</contexts>
<marker>Kiss, Amstrong, Milroy, 1972</marker>
<rawString>George Kiss, Christine Amstrong, and Robert Milroy. 1972. The associative thesaurus of English. Ediburgh University Press, Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Levelt</author>
<author>Herbert Schriefers</author>
</authors>
<title>Stages of lexical access.</title>
<date>1987</date>
<booktitle>Natural Language Generation: New Results in Artificial Intelligence, Psychology, and Linguistics,</booktitle>
<pages>395--404</pages>
<editor>In G. Kempen, editor,</editor>
<location>Nijhoff, Dordrecht.</location>
<contexts>
<context position="9903" citStr="Levelt and Schriefers, 1987" startWordPosition="1640" endWordPosition="1643">form or dictionary-form, for psycholinguists it is a schema, i.e. an abstract form representing a specific meaning (a lexicalized concept) and a syntactic category (part of speech), but it lacks entirely specific values concerning the form (sounds/graphemes). This is being take care of at the next step (sound form encoding). In short, in contrast to Saussure’s view, the information contributing to what we commonly call words (lemma or word forms) is distributed. This is a well established empirical fact observed by psychologists working on the time course of word production (Stemberger, 1985; Levelt and Schriefers, 1987; Dell et al., 1999), as by those who analyze and interpret speech errors (Fromkin, 1973; Fromkin, 1980; Fromkin, 1993). Yet, what concerns us here in particular is the following: as noted, speakers go from meanings to sounds via lexical concepts (abstract word forms). More importantly, the conceptual input may lack information to determine a precise lexical form. Put differently, rather than starting from a full fledged definition or complete meaning representation, authors may well start from an underspecified input (’small bird’ rather than ’sparrow’). Note that the specific requirements of</context>
</contexts>
<marker>Levelt, Schriefers, 1987</marker>
<rawString>William Levelt and Herbert Schriefers. 1987. Stages of lexical access. In G. Kempen, editor, Natural Language Generation: New Results in Artificial Intelligence, Psychology, and Linguistics, pages 395–404. Nijhoff, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Levelt</author>
<author>A Roelofs</author>
<author>A Meyer</author>
</authors>
<title>A theory of lexical access in speech production.</title>
<date>1999</date>
<journal>Behavioral and Brain Sciences,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="18772" citStr="Levelt et al., 1999" startWordPosition="3052" endWordPosition="3055">008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). There has also been quite a lot of work on the time-course of word production, i.e. the way how one gets progressively from a more or less precise idea to its expression, a word expressed in written or spoken form. See for example (Levelt et al., 1999; Dell et al., 1999). Clearly, a lot of progress has been made during the last two decades, yet more can be done especially with respect to indexing (the organization of the data) and navigation. Two key idea underlying modern lexical resources are the notions of ’graphs’ and ’association’. For a useful introduction to graph-based natural language processing, see (Mihalcea and Radev, 2011). Associations have a long history. The idea according to which the mental lexicon (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not </context>
</contexts>
<marker>Levelt, Roelofs, Meyer, 1999</marker>
<rawString>William Levelt, A. Roelofs, and A. Meyer. 1999. A theory of lexical access in speech production. Behavioral and Brain Sciences, 22(1):1–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Levelt</author>
</authors>
<title>Speaking : From Intention to Articulation.</title>
<date>1989</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="7827" citStr="Levelt, 1989" startWordPosition="1311" endWordPosition="1312">a given moment, he can recognize it when seeing it (alone or in a list). This fact is well established in the literature on the ’tip-of-the-tongue problem’ (Aitchison, 2003). 3 From mind to mouth, or what characterizes the process of word production? According to the father of modern linguistics (de Saussure, 1916), word forms (signifier) and their associated meaning (signified) are but one, called the sign. They are said to be an inseparable unit. This is in sharp contrast to what psychologists tell us about words synthesis. For example, one of the leading specialists of language production (Levelt, 1989; Levelt, 1999) has convincingly shown that, when speaking 1For example, he knows that for a given word form there are similar forms in terms of sound or meaning. There are also words that are more general/specific, or others meaning exactly the opposite than a given input. This kind of knowledge is so obvious and so frequent that it is encoded in many resources like WordNet, Roget’s thesaurus or more traditional dictionaries (incuding synonym and rhyming dictionaries). 2For example, parts of the form (rhymes with x: health/wealth) or meaning, like the ’type’ (animal), the ’function’ (used for</context>
</contexts>
<marker>Levelt, 1989</marker>
<rawString>William Levelt. 1989. Speaking : From Intention to Articulation. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Levelt</author>
</authors>
<title>Language production: a blueprint of the speaker.</title>
<date>1999</date>
<booktitle>Neurocognition of Language,</booktitle>
<pages>83--122</pages>
<editor>In C. Brown and P. Hagoort, editors,</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="7842" citStr="Levelt, 1999" startWordPosition="1313" endWordPosition="1314">, he can recognize it when seeing it (alone or in a list). This fact is well established in the literature on the ’tip-of-the-tongue problem’ (Aitchison, 2003). 3 From mind to mouth, or what characterizes the process of word production? According to the father of modern linguistics (de Saussure, 1916), word forms (signifier) and their associated meaning (signified) are but one, called the sign. They are said to be an inseparable unit. This is in sharp contrast to what psychologists tell us about words synthesis. For example, one of the leading specialists of language production (Levelt, 1989; Levelt, 1999) has convincingly shown that, when speaking 1For example, he knows that for a given word form there are similar forms in terms of sound or meaning. There are also words that are more general/specific, or others meaning exactly the opposite than a given input. This kind of knowledge is so obvious and so frequent that it is encoded in many resources like WordNet, Roget’s thesaurus or more traditional dictionaries (incuding synonym and rhyming dictionaries). 2For example, parts of the form (rhymes with x: health/wealth) or meaning, like the ’type’ (animal), the ’function’ (used for eating) or the</context>
</contexts>
<marker>Levelt, 1999</marker>
<rawString>William Levelt. 1999. Language production: a blueprint of the speaker. In C. Brown and P. Hagoort, editors, Neurocognition of Language, pages 83—122. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Aleksandroviˇc Mel’ˇcuk</author>
<author>Alain Polgu`ere</author>
</authors>
<title>Lexique actif du franc¸ais : l’apprentissage du vocabulaire fond´e sur 20 000 d´erivations s´emantiques et collocations du franc¸ais. Champs linguistiques. De Boeck,</title>
<date>2007</date>
<location>Bruxelles.</location>
<marker>Mel’ˇcuk, Polgu`ere, 2007</marker>
<rawString>Igor Aleksandroviˇc Mel’ˇcuk and Alain Polgu`ere. 2007. Lexique actif du franc¸ais : l’apprentissage du vocabulaire fond´e sur 20 000 d´erivations s´emantiques et collocations du franc¸ais. Champs linguistiques. De Boeck, Bruxelles.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dragomir Radev</author>
</authors>
<title>Graph-Based Natural Language Processing and Information Retrieval.</title>
<date>2011</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="19164" citStr="Mihalcea and Radev, 2011" startWordPosition="3114" endWordPosition="3117">n quite a lot of work on the time-course of word production, i.e. the way how one gets progressively from a more or less precise idea to its expression, a word expressed in written or spoken form. See for example (Levelt et al., 1999; Dell et al., 1999). Clearly, a lot of progress has been made during the last two decades, yet more can be done especially with respect to indexing (the organization of the data) and navigation. Two key idea underlying modern lexical resources are the notions of ’graphs’ and ’association’. For a useful introduction to graph-based natural language processing, see (Mihalcea and Radev, 2011). Associations have a long history. The idea according to which the mental lexicon (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not new at all. Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James &amp; Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989). For good introductions see (H¨ormann, 1972; Cramer, 1968) </context>
</contexts>
<marker>Mihalcea, Radev, 2011</marker>
<rawString>Rada Mihalcea and Dragomir Radev. 2011. Graph-Based Natural Language Processing and Information Retrieval. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>235--244</pages>
<contexts>
<context position="17881" citStr="Miller et al., 1990" startWordPosition="2914" endWordPosition="2917">ork While there are many kinds of dictionaries or lexical resources, very few of them can be said to meet truly the authors’ needs. To be fair though, one must admit that great efforts have been made to improve the situation both with respect to lexical resources and electronic dictionaries. In fact, there are quite a few onomasiological dictionaries (van Sterkenburg, 2003). For example, Roget’s Thesaurus (Roget, 1852), analogical dictionaries (Boissi`ere, 1862; Robert et al., 1993), Longman’s Language Activator 224 (Summers, 1993), various network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990), MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989), ’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997). Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s S</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. 1990. Introduction to WordNet: An on-line lexical database. International Journal of Lexicography, 3(4), pages 235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Armitage Miller</author>
</authors>
<title>Wordnet: An on-line lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="20011" citStr="Miller, 1990" startWordPosition="3246" endWordPosition="3247">ry notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James &amp; Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989). For good introductions see (H¨ormann, 1972; Cramer, 1968) and more recently (Spitzer, 1999). The notion of association is also implicit in work on semantic networks (Quillian, 1968), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al., 1999) and, of course, in WordNet (Miller, 1990; Fellbaum, 1998). 6 The framework for building and using our resource To understand the problems at stake, I describe the communicative setting (system, user), the existing and necessary components, as well as the information flow (see figure 2). Imagine an author wishing to convey the name of a special beverage (’mocha’) commonly found in coffee shops. Failing to do so, he tries to find it in a lexicon. Since dictionaries are too huge to be scanned from beginning to the end, I suggest another solution : reduce the search space based on some input (step-1) and presentation of the results (all</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>George Armitage Miller. 1990. Wordnet: An on-line lexical database. International Journal of Lexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adilson E Motter</author>
<author>Alessandro P S de Moura</author>
<author>Ying-Cheng Lai</author>
<author>Partha Dasgupta</author>
</authors>
<title>Topology of the conceptual network of language. Physical Review E,</title>
<date>2002</date>
<marker>Motter, de Moura, Lai, Dasgupta, 2002</marker>
<rawString>Adilson E. Motter, Alessandro P. S. de Moura, Ying-Cheng Lai, and Partha Dasgupta. 2002. Topology of the conceptual network of language. Physical Review E, 65(6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Nelson</author>
<author>Cathy McEvoy</author>
<author>Thomas Schreiber</author>
</authors>
<title>The university of South Florida word association, rhyme, and word fragment norms. Ted Nelson.</title>
<date>1998</date>
<note>Xanadu projet hypertextuel.</note>
<contexts>
<context position="6494" citStr="Nelson et al., 1998" startWordPosition="1077" endWordPosition="1080">is reachable, at least in principle. Search may require several steps, but in most cases the number of steps is surprisingly small. As mentioned already, the user definitely has some knowledge concerning words, their components and their organisation in the mental lexicon, but this knowledge is by no means complete. The user also has some knowledge (or, more precisely, meta-knowledge) concerning the topology of the graph,1 but he certainly does not know as much as the system. The fact that an author does have this kind of knowledge is revealed via word associations (Cramer, 1968; Deese, 1965; Nelson et al., 1998; Kiss et al., 1972) and via the observed average path length (Vitevitch, 2008) needed in order to get from some starting point (sw) to the goal (tw). This path is generally quite short. It hardly ever exceeds three steps, and in many cases even less: search is launched via an item directly related to the tw (direct neighbor). If the user does not know too much concerning the topology of the network, he does know quite a bit concerning the tw,2 information the system has no clue of at this point. Eventhough it knows ’everyone’ in the network, it cannot do mind-reading, i.e. guess the precise w</context>
</contexts>
<marker>Nelson, McEvoy, Schreiber, 1998</marker>
<rawString>Douglas Nelson, Cathy McEvoy, and Thomas Schreiber. 1998. The university of South Florida word association, rhyme, and word fragment norms. Ted Nelson. 1967. Xanadu projet hypertextuel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Quillian</author>
</authors>
<title>Semantic memory.</title>
<date>1968</date>
<booktitle>Semantic Information Processing,</booktitle>
<pages>216--270</pages>
<editor>In M. Minsky, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="19887" citStr="Quillian, 1968" startWordPosition="3226" endWordPosition="3227">cally an associative network, composed of nodes (words or concepts) and links (associations) is not new at all. Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James &amp; Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989). For good introductions see (H¨ormann, 1972; Cramer, 1968) and more recently (Spitzer, 1999). The notion of association is also implicit in work on semantic networks (Quillian, 1968), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al., 1999) and, of course, in WordNet (Miller, 1990; Fellbaum, 1998). 6 The framework for building and using our resource To understand the problems at stake, I describe the communicative setting (system, user), the existing and necessary components, as well as the information flow (see figure 2). Imagine an author wishing to convey the name of a special beverage (’mocha’) commonly found in coffee shops. Failing to do so, he tries to find it in a lexicon. Since dictionaries are too huge to be scanned from beginning to the</context>
</contexts>
<marker>Quillian, 1968</marker>
<rawString>Ross Quillian. 1968. Semantic memory. In M. Minsky, editor, Semantic Information Processing, pages 216–270. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Richardson</author>
<author>William Dolan</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Mindnet: Acquiring and structuring semantic information from text.</title>
<date>1998</date>
<booktitle>In ACL-COLING’98,</booktitle>
<pages>1098--1102</pages>
<contexts>
<context position="17916" citStr="Richardson et al., 1998" startWordPosition="2919" endWordPosition="2922">of dictionaries or lexical resources, very few of them can be said to meet truly the authors’ needs. To be fair though, one must admit that great efforts have been made to improve the situation both with respect to lexical resources and electronic dictionaries. In fact, there are quite a few onomasiological dictionaries (van Sterkenburg, 2003). For example, Roget’s Thesaurus (Roget, 1852), analogical dictionaries (Boissi`ere, 1862; Robert et al., 1993), Longman’s Language Activator 224 (Summers, 1993), various network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990), MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989), ’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997). Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 20</context>
</contexts>
<marker>Richardson, Dolan, Vanderwende, 1998</marker>
<rawString>Stephen Richardson, William Dolan, and Lucy Vanderwende. 1998. Mindnet: Acquiring and structuring semantic information from text. In ACL-COLING’98, pages 1098–1102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Robert</author>
<author>Alain Rey</author>
<author>J Rey-Debove</author>
</authors>
<title>Dictionnaire alphabetique et analogique de la Langue Franc¸aise. Le Robert,</title>
<date>1993</date>
<location>Paris.</location>
<contexts>
<context position="17748" citStr="Robert et al., 1993" startWordPosition="2898" endWordPosition="2901">nversion of meaning to sounds is mediated via a lexicon, one may wonder to what extent existing resources can be of help. 5 Related work While there are many kinds of dictionaries or lexical resources, very few of them can be said to meet truly the authors’ needs. To be fair though, one must admit that great efforts have been made to improve the situation both with respect to lexical resources and electronic dictionaries. In fact, there are quite a few onomasiological dictionaries (van Sterkenburg, 2003). For example, Roget’s Thesaurus (Roget, 1852), analogical dictionaries (Boissi`ere, 1862; Robert et al., 1993), Longman’s Language Activator 224 (Summers, 1993), various network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990), MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989), ’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997). Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) </context>
</contexts>
<marker>Robert, Rey, Rey-Debove, 1993</marker>
<rawString>Paul Robert, Alain Rey, and J. Rey-Debove. 1993. Dictionnaire alphabetique et analogique de la Langue Franc¸aise. Le Robert, Paris.</rawString>
</citation>
<citation valid="false">
<title>Thesaurus of English Words and Phrases.</title>
<location>Longman, London.</location>
<marker></marker>
<rawString>Peter Roget. 1852. Thesaurus of English Words and Phrases. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Rundell</author>
<author>G Fox</author>
</authors>
<title>Macmillan English Dictionary for Advanced Learners.</title>
<date>2002</date>
<publisher>Macmillan,</publisher>
<location>Oxford.</location>
<contexts>
<context position="18428" citStr="Rundell and Fox, 2002" startWordPosition="2990" endWordPosition="2993">s network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990), MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989), ’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997). Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). There has also been quite a lot of work on the time-course of word production, i.e. the way how one gets progressively from a more or less precise idea to its expression, a word expressed in written or spoken form. See for example (Levelt et al., 1999; Dell et al., 1999). Clearly, a lot of progress has been made during the last two decades, yet more can be done especially with respect to indexing (the organization of the data) and navigation. Two key idea underlying modern lexical resources are the not</context>
</contexts>
<marker>Rundell, Fox, 2002</marker>
<rawString>Michael Rundell and G. (Eds.) Fox. 2002. Macmillan English Dictionary for Advanced Learners. Macmillan, Oxford.</rawString>
</citation>
<citation valid="true">
<title>Pathfinder Associative Networks: studies in knowledge organization.</title>
<date>1989</date>
<editor>Roger Schvaneveldt, editor.</editor>
<publisher>Ablex,</publisher>
<location>Norwood, New Jersey, US.</location>
<marker>1989</marker>
<rawString>Roger Schvaneveldt, editor. 1989. Pathfinder Associative Networks: studies in knowledge organization. Ablex, Norwood, New Jersey, US.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bennett Schwartz</author>
</authors>
<title>Tip-of-the-tongue states: Phenomenology, mechanism, and lexical retrieval. Lawrence Erlbaum Associates,</title>
<date>2002</date>
<location>Mahwah, NJ.</location>
<contexts>
<context position="2901" citStr="Schwartz, 2002" startWordPosition="491" endWordPosition="492">ittle we have to offer so far to help humans to memorize, find or retrieve them. Hoping to contribute to a change for this, I have started to work on one of these tasks: word access, also called retrieval or wordfinding. Imagine the following situation: your goal is to express the following ideas: superior dark coffee made of beans from Arabia” by a single word, but you cannot access the corresponding form mocha, even though you know it, since you’ve used it not so long ago. This kind of problem, known as the tip-of-the-tongue (TOT)-problem, has received a lot of attention from psychologists (Schwartz, 2002; Brown, 1991). It has always been pointed out that people being in this state know quite a bit concerning the elusive word (Brown and McNeill, 1996). Hence, using it should allow us to reduce the search space. Put differently, it would be nice to have a system capable to use whatever you have, incomplete as it may be, to help you find what you cannot recall. For example, for the case at hand, one might think of dark, coffee, beans, and Arabia, to expect from system a set of reasonable candidates, like arabica, espresso, or mocha. In the remainder of this paper I will try to show how this migh</context>
</contexts>
<marker>Schwartz, 2002</marker>
<rawString>Bennett Schwartz. 2002. Tip-of-the-tongue states: Phenomenology, mechanism, and lexical retrieval. Lawrence Erlbaum Associates, Mahwah, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerardo Sierra</author>
</authors>
<title>The onomasiological dictionary: a gap in lexicography.</title>
<date>2000</date>
<booktitle>In Proceedings of the Ninth Euralex International Congress,</booktitle>
<pages>223--235</pages>
<institution>IMS, Universit¨at Stuttgart.</institution>
<contexts>
<context position="18137" citStr="Sierra, 2000" startWordPosition="2951" endWordPosition="2952"> and electronic dictionaries. In fact, there are quite a few onomasiological dictionaries (van Sterkenburg, 2003). For example, Roget’s Thesaurus (Roget, 1852), analogical dictionaries (Boissi`ere, 1862; Robert et al., 1993), Longman’s Language Activator 224 (Summers, 1993), various network-based dictionaries: WordNet (Fellbaum, 1998; Miller et al., 1990), MindNet (Richardson et al., 1998), HowNet (Dong and Dong, 2006), Pathfinder (Schvaneveldt, 1989), ’The active vocabulary for French’ (Mel’ˇcuk and Polgu`ere, 2007) and Fontenelle (Fontenelle, 1997). Other proposals have been made by Sierra (Sierra, 2000) and Moerdijk (2008). There are also various collocation dictionaries (Benson et al., 2010), reverse dictionaries (Bernstein, 1975; Kahn, 1989; Edmonds, 1999) and OneLook,3 which combines a dictionary (WordNet) and an encyclopedia (Wikipedia). Finally, there is MEDAL (Rundell and Fox, 2002), a thesaurus produced with the help of Kilgariff’s Sketch Engine (Kilgarriff et al., 2004). There has also been quite a lot of work on the time-course of word production, i.e. the way how one gets progressively from a more or less precise idea to its expression, a word expressed in written or spoken form. S</context>
</contexts>
<marker>Sierra, 2000</marker>
<rawString>Gerardo Sierra. 2000. The onomasiological dictionary: a gap in lexicography. In Proceedings of the Ninth Euralex International Congress, pages 223–235, IMS, Universit¨at Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Spitzer</author>
</authors>
<title>The mind within the net: models of learning, thinking and acting.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="19797" citStr="Spitzer, 1999" startWordPosition="3212" endWordPosition="3213"> a long history. The idea according to which the mental lexicon (or encyclopedia) is basically an associative network, composed of nodes (words or concepts) and links (associations) is not new at all. Actually the very notion of association goes back at least to Aristotle (350BC), but it is also inherent in work done by philosophers (Locke, Hume), physiologists (James &amp; Stuart Mills), psychologists (Galton, 1880; Freud, 1901; Jung and Riklin, 1906) and psycholinguists (Deese, 1965; Jenkins, 1970; Schvaneveldt, 1989). For good introductions see (H¨ormann, 1972; Cramer, 1968) and more recently (Spitzer, 1999). The notion of association is also implicit in work on semantic networks (Quillian, 1968), hypertext (Bush, 1945), the web (Nelson, 1967), connectionism (Dell et al., 1999) and, of course, in WordNet (Miller, 1990; Fellbaum, 1998). 6 The framework for building and using our resource To understand the problems at stake, I describe the communicative setting (system, user), the existing and necessary components, as well as the information flow (see figure 2). Imagine an author wishing to convey the name of a special beverage (’mocha’) commonly found in coffee shops. Failing to do so, he tries to</context>
</contexts>
<marker>Spitzer, 1999</marker>
<rawString>Manfred Spitzer. 1999. The mind within the net: models of learning, thinking and acting. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Paul Stemberger</author>
</authors>
<title>An interactive activation model of language production.</title>
<date>1985</date>
<booktitle>Progress in the Psychology of Language,</booktitle>
<volume>1</volume>
<pages>143--186</pages>
<editor>In A. W. Ellis, editor,</editor>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="9874" citStr="Stemberger, 1985" startWordPosition="1638" endWordPosition="1639">g the word’s base-form or dictionary-form, for psycholinguists it is a schema, i.e. an abstract form representing a specific meaning (a lexicalized concept) and a syntactic category (part of speech), but it lacks entirely specific values concerning the form (sounds/graphemes). This is being take care of at the next step (sound form encoding). In short, in contrast to Saussure’s view, the information contributing to what we commonly call words (lemma or word forms) is distributed. This is a well established empirical fact observed by psychologists working on the time course of word production (Stemberger, 1985; Levelt and Schriefers, 1987; Dell et al., 1999), as by those who analyze and interpret speech errors (Fromkin, 1973; Fromkin, 1980; Fromkin, 1993). Yet, what concerns us here in particular is the following: as noted, speakers go from meanings to sounds via lexical concepts (abstract word forms). More importantly, the conceptual input may lack information to determine a precise lexical form. Put differently, rather than starting from a full fledged definition or complete meaning representation, authors may well start from an underspecified input (’small bird’ rather than ’sparrow’). Note that</context>
</contexts>
<marker>Stemberger, 1985</marker>
<rawString>Joseph Paul Stemberger. 1985. An interactive activation model of language production. In A. W. Ellis, editor, Progress in the Psychology of Language, volume 1, pages 143–186. Erlbaum. Della Summers. 1993. Language Activator: the world’s first production dictionary. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piet van Sterkenburg</author>
</authors>
<title>Onomasiological specifications and a concise history of onomasiological dictionaries.</title>
<date>2003</date>
<booktitle>In A Practical Guide to Lexicography, volume A Practical Guide to Lexicography,</booktitle>
<pages>127--143</pages>
<publisher>John Benjamins,</publisher>
<location>Amsterdam.</location>
<marker>van Sterkenburg, 2003</marker>
<rawString>Piet van Sterkenburg. 2003. Onomasiological specifications and a concise history of onomasiological dictionaries. In A Practical Guide to Lexicography, volume A Practical Guide to Lexicography, pages 127—143. John Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriella Vigliocco</author>
<author>Tiziana Antonini</author>
<author>Garrett Merrill</author>
</authors>
<title>Grammatical gender is on the tip of italian tongues.</title>
<date>1997</date>
<journal>Psychological Science,</journal>
<volume>4</volume>
<issue>8</issue>
<contexts>
<context position="15023" citStr="Vigliocco et al., 1997" startWordPosition="2444" endWordPosition="2447">via syntagmatic links (thesaurus- or encyclopedic relations), via its form (rhymes), via lexical relations, via syntactic patterns (search in a corpus), and, of course, via another language (translation). Note that access by meaning is the golden route, i.e. the most normal way. We tend to use other means only if we fail to access straight away the desired word. I will consider here only one of them, word associations (mostly, encyclopaedic relations). Note that, people being in the TOT-state clearly know more than that. Psychologists who have studied this phenomenon (Brown and McNeill, 1996; Vigliocco et al., 1997) have found that their subjects had access not only to meanings (the words definition), but also to information concerning grammar (gender) and lexical form: sound, morphology and part of speech. While all this information could be used to constrain the search space, the ideal dictionary being multiply indexed, I will deal here only with semantically related words (associations, collocations in the large sense of the word). Before discussing how such a dictionary could be built and used, let us consider a possible search scenario. I start from the assumption that in our mind, all words are con</context>
</contexts>
<marker>Vigliocco, Antonini, Merrill, 1997</marker>
<rawString>Gabriella Vigliocco, Tiziana Antonini, and Garrett Merrill. 1997. Grammatical gender is on the tip of italian tongues. Psychological Science, 4(8):314–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Vitevitch</author>
</authors>
<title>What can graph theory tell us about word learning and lexical retrieval?</title>
<date>2008</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<pages>51--408</pages>
<contexts>
<context position="6573" citStr="Vitevitch, 2008" startWordPosition="1092" endWordPosition="1093"> cases the number of steps is surprisingly small. As mentioned already, the user definitely has some knowledge concerning words, their components and their organisation in the mental lexicon, but this knowledge is by no means complete. The user also has some knowledge (or, more precisely, meta-knowledge) concerning the topology of the graph,1 but he certainly does not know as much as the system. The fact that an author does have this kind of knowledge is revealed via word associations (Cramer, 1968; Deese, 1965; Nelson et al., 1998; Kiss et al., 1972) and via the observed average path length (Vitevitch, 2008) needed in order to get from some starting point (sw) to the goal (tw). This path is generally quite short. It hardly ever exceeds three steps, and in many cases even less: search is launched via an item directly related to the tw (direct neighbor). If the user does not know too much concerning the topology of the network, he does know quite a bit concerning the tw,2 information the system has no clue of at this point. Eventhough it knows ’everyone’ in the network, it cannot do mind-reading, i.e. guess the precise word a user has in mind (tw) when providing a specific input (sw). Yet the user </context>
</contexts>
<marker>Vitevitch, 2008</marker>
<rawString>Michael Vitevitch. 2008. What can graph theory tell us about word learning and lexical retrieval? Journal of Speech, Language, and Hearing Research, 51:408—422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Wilkins</author>
</authors>
<title>Linguistics and Language Teaching.</title>
<date>1972</date>
<location>Edward Arnold, London.</location>
<contexts>
<context position="1985" citStr="Wilkins (1972)" startWordPosition="333" endWordPosition="334">o determine properly the initial search space (step-1), we must have already well understood the input [mouse1 / mouse2 (rodent/device)], as otherwise our list will contain a lot of noise, presenting ’cat, cheese’ together with ’computer, mouse pad’, which is not quite what we want, since some of these candidates are irrelevant, i.e. beyond the scope of the user’s goal. 1 Introduction Whenever we read a book, write a letter, or launch a query on Google, we always use words, the shorthand labels for more or less well specified thoughts. No doubt, words are important, a fact nicely expressed by Wilkins (1972) when he writes: without grammar very little can be conveyed, without vocabulary, nothing can be conveyed. Still, ubiquitous as they may be, words have to be learned, that is, they have to be stored, remembered, and retrieved. Given the role words play in our daily lives, it is surprising to see how little we have to offer so far to help humans to memorize, find or retrieve them. Hoping to contribute to a change for this, I have started to work on one of these tasks: word access, also called retrieval or wordfinding. Imagine the following situation: your goal is to express the following ideas:</context>
</contexts>
<marker>Wilkins, 1972</marker>
<rawString>David Wilkins. 1972. Linguistics and Language Teaching. Edward Arnold, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziqi Zhang</author>
<author>Anna Lisa Gentile</author>
<author>Fabio Ciravegna</author>
</authors>
<title>Recent advances in methods of lexical semantic relatedness – a survey.</title>
<date>2012</date>
<journal>Journal of Natural Language Engineering, Cambridge</journal>
<publisher>Universtiy Press,</publisher>
<contexts>
<context position="24787" citStr="Zhang et al., 2012" startWordPosition="4080" endWordPosition="4083">his is a crucial step, as it allows the user to navigate on this basis. Of course, one could question the very need of labels, and perhaps this is not too much of an issue if we have only say, 3-4 categories. I am nevertheless strongly convinced that the problem is real, as soon as the number of categories (hence the words to be classified) grows. To conclude, I think it is fair to say that the first stage is clearly within reach, while the automatic construction of the categorical tree remains a true challenge despite the vast literature devoted to this topic or to strongly related problems (Zhang et al., 2012; Biemann, 2012; Everitt et al., 2011). 7 Outlook and conclusion I have started from the observation that words are important and that their accessibility can be a problem. In order to help a dictionary user to overcome it I have presented a method showing promise. In particular, I have shown how to reduce the search space, how to present a set of plausible candidates and what needs to be done next (clustering and naming them) to reduce the search space and to support navigation. In particular, I have proposed the creation of a categorial tree whose leaves contain the (potential target) words </context>
</contexts>
<marker>Zhang, Gentile, Ciravegna, 2012</marker>
<rawString>Ziqi Zhang, Anna Lisa Gentile, and Fabio Ciravegna. 2012. Recent advances in methods of lexical semantic relatedness – a survey. Journal of Natural Language Engineering, Cambridge Universtiy Press, 19(4):411–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Zock</author>
</authors>
<title>The power of words in message planning.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th conference on Computational linguistics,</booktitle>
<pages>990--995</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10883" citStr="Zock, 1996" startWordPosition="1795" endWordPosition="1796">t differently, rather than starting from a full fledged definition or complete meaning representation, authors may well start from an underspecified input (’small bird’ rather than ’sparrow’). Note that the specific requirements of a culture may help us to clarify our thoughts, as well as induce biases or imprecisions because of lexical gaps. Hence we end up using an existing words (eventhough it does not express excatly what we had in mind) rather than coining a new one fitting better our purpose (expressibility problem). For a psycholinguistic explanation concerning gradual refinement, see (Zock, 1996). Let me briefly illustrate this here via an example, and comment then on the way how specific knowledge states may ask for different kind of information from the lexicon. Suppose you wanted to talk about a given reptile having certain features (dangerous, size, living space, ...). If you cannot come up immediately with the intended word, any of the following could be candidates: alligator, crocodile, cayman. At some point you need to make up your mind though, as the form synthesizer needs to know what items to activate so that it can produce the corresponding form (graphemes, sounds). concept</context>
</contexts>
<marker>Zock, 1996</marker>
<rawString>Michael Zock. 1996. The power of words in message planning. In Proceedings of the 16th conference on Computational linguistics, pages 990–995, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
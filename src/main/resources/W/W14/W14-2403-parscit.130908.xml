<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000943">
<title confidence="0.5595">
Large-scale CCG Induction from the Groningen Meaning Bank
</title>
<author confidence="0.910017">
Sebastian Beschke*, Yang Liu†and Wolfgang Menzel*
</author>
<affiliation confidence="0.999512">
*Department of Informatics, University of Hamburg, Germany
</affiliation>
<email confidence="0.933874">
{beschke,menzel}@informatik.uni-hamburg.de
</email>
<affiliation confidence="0.971617">
†State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing, China
</affiliation>
<email confidence="0.99496">
liuyang2011@tsinghua.edu.cn
</email>
<sectionHeader confidence="0.997326" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996390625">
In present CCG-based semantic parsing
systems, the extraction of a semantic
grammar from sentence-meaning exam-
ples poses a computational challenge. An
important factor is the decomposition of
the sentence meaning into smaller parts,
each corresponding to the meaning of a
word or phrase. This has so far limited
supervised semantic parsing to small, spe-
cialised corpora. We propose a set of
heuristics that render the splitting of mean-
ing representations feasible on a large-
scale corpus, and present a method for
grammar induction capable of extracting a
semantic CCG from the Groningen Mean-
ing Bank.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9993715">
Combinatory Categorial Grammar (CCG) forms
the basis of many current approaches to semantic
parsing. It is attractive for semantic parsing due
to its unified treatment of syntax and semantics,
where the construction of the meaning representa-
tion directly follows the syntactic analysis (Steed-
man, 2001). However, the supervised induction of
semantic CCGs—the inference of a CCG from a
corpus of sentence-meaning pairs—has so far only
been partially solved. While approaches are avail-
able that work on small corpora focused on spe-
cific domains (such as Geoquery and Freebase QA
for question answering (Zelle and Mooney, 1996;
Cai and Yates, 2013)), we are not aware of any
approach that allows the extraction of a seman-
tic CCG from a wide-coverage corpus such as the
Groningen Meaning Bank (GMB) (Basile et al.,
2012). This work attempts to fill this gap.
Analogous to the work of Kwiatkowski et al.
(2010), we view grammar induction as a series of
splitting steps, each of which essentially reverses
a CCG derivation step. However, we diverge
from their approach by applying novel heuristics
for searching the space of possible splits. The
combination of alignment consistency and single-
branching recursion turns out to produce a man-
ageable number of lexical items for most sen-
tences in the GMB, while statistical measures
and manual inspection suggest that many of these
items are also plausible.
</bodyText>
<sectionHeader confidence="0.9647905" genericHeader="method">
2 Searching the space of CCG
derivations
</sectionHeader>
<bodyText confidence="0.999977724137931">
Our search heuristics are embedded into a very
general splitting algorithm, Algorithm 1. Given
a sentence-meaning pair, it iterates over all possi-
ble sentence-meaning splits in two steps. First, a
split index in the sentence is chosen along with a
binary CCG-combinator to be reversed (the syn-
tactic split). Then, the meaning representation is
split accordingly to reverse the application of the
selected combinator (the semantic split). E. g., for
the forward application combinator, the meaning
representation z is split into f, g so that z = fg
(modulo α, β,η conversions). By identifying f
with the left half l of the sentence and g with the
right half r, we obtain two new phrase-meaning
pairs, which are then split recursively.
This algorithm combines two challenging
search problems. Recursive syntactic splitting
searches the space of syntactic CCG derivations
that yield the sentence, which is exponential in the
length of the sentence. Semantic splitting, given
the flexibility of A-calculus, has infinitely many
solutions. The crucial question is how to prune
the parts of the search space that are unlikely to
lead to good results.
Our strategy to address this problem is to apply
heuristics that constrain the results returned by se-
mantic splitting. By yielding no results on certain
inputs, this at the same time constrains the syntac-
tic search space. The following descriptions there-
</bodyText>
<page confidence="0.980679">
12
</page>
<bodyText confidence="0.845724222222222">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 12–16,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
fore relate to the implementation of the SEMSPLIT 1x
function.
Algorithm 1 A general splitting algorithm. C is
the set of binary CCG combinators. The SEM-
SPLIT function returns possible splits of a meaning
representation according to the reverse application
of a combinator.
</bodyText>
<equation confidence="0.972871333333333">
function SPLIT(x, z)
if jxj = 1 then
return {(x, z)}
else
G +— 0
for0&lt;i&lt;_ jxj − 1 and c E C do
l +— x0 ... xi−1
r +— xz ... xjxj−1
S +— SEMSPLIT(c, z)
for (f, g) E S do
G +— G U SPLIT(l, f)
U SPLIT(r, g)
</equation>
<listItem confidence="0.5822226">
end for
end for
return G
end if
end function
</listItem>
<subsectionHeader confidence="0.998897">
2.1 Alignment consistency
</subsectionHeader>
<bodyText confidence="0.9967045">
The first heuristic we introduce is borrowed from
the field of statistical machine translation. There,
alignments between words of two languages are
used to identify corresponding phrase pairs, as
in the well-known GHKM algorithm (Galley et
al., 2004). In order to apply the same strategy
to meaning representations, we represent them as
their abstract syntax trees. Following Li et al.
(2013), we can then align words in the sentence
and nodes in the meaning representation to iden-
tify components that correspond to each other.
This allows us to impose an extra constraint on
the generation of splits: We require that nodes in f
not be aligned to any words in the right sentence-
half r, and conversely, that nodes in g not be
aligned to words in l.
Alignment consistency helps the search to fo-
cus on more plausible splits by grouping elements
of the meaning representation with the words that
evoked them. However, by itself it does not signif-
icantly limit the search space, as it is still possible
to extract infinitely many semantic splits from any
sentence at any splitting index.
Example: Given the word-to-meaning
</bodyText>
<figure confidence="0.624719">
1y
</figure>
<figureCaption confidence="0.9500495">
Figure 1: An example word-to-meaning align-
ment. Splits across any of the alignment edges are
</figureCaption>
<bodyText confidence="0.860452888888889">
prohibited. E. g., we cannot produce a split whose
meaning representation contains both vincent and
mia.
alignment from Figure 1, a split that is
excluded by the alignment criterion is:
(Vincent : λg.1x.1y.vincent(x) n love(x,y) n
g(y)), (loves Mia : λy.mia(y)). This is because
the node “love” (in f) is aligned to the word
“loves” (in r).
</bodyText>
<subsectionHeader confidence="0.999727">
2.2 Single-branching recursive splitting
</subsectionHeader>
<bodyText confidence="0.99945316">
The second heuristic is best described as a search
strategy over possible semantic splits. In the fol-
lowing presentation, we presume that alignment
consistency is being enforced. Again, it is helpful
to view the meaning representation as an abstract
syntax tree.
Recall that our goal is to find two expressions
f, g to be associated with the sentence halves l, r.
In a special case, this problem is easily solved: If
we can find some split node X which governs all
nodes aligned to words in r, but no nodes aligned
to words in l, we can simply extract the sub-tree
rooted at X and replace it with a variable. E. g.,
z = a(bc) can be split into f = λx.a(xc) and
g = b, which can be recombined by application.
However, requiring the existence of exactly two
such contiguous components can be overly restric-
tive, as Figure 2 illustrates. Instead, we say that we
decompose z into a hierarchy of components, with
a split node at the root of each component. These
components are labelled as f- and g-components
in an alternating fashion.
In this hierarchy, the members of an f-
component are not allowed to have alignments to
words in l. A corresponding requirement holds for
</bodyText>
<figure confidence="0.8970056">
n
vincent(x) love mia
Vincent loves Mia
13
@
</figure>
<figureCaption confidence="0.533673">
Figure 2: Illustration of single-branching recur-
sion: Assume that the leaves of the meaning rep-
resentation a(bc)(de) are aligned as given to the
words x0 ... x4, and that we wish to split the sen-
tence at index 2. The indicated split partitions the
meaning representation into three hierarchically
nested components and yields f = Ax.xc(de) and
g = Ay.a(by), which can be recombined using ap-
plication.
</figureCaption>
<bodyText confidence="0.9976068125">
g-components.
The single-branching criterion states that all
split nodes lie on a common path from the root,
or in other words, every component is the parent
of at most one sub-component.
In comparison to more flexible strategies,
single-branching recursive splitting has the advan-
tage of requiring a minimum of additionally gen-
erated structure. For every component, we only
need to introduce one new bound variable for the
body plus one for every variable that occurs free
under the split node.
Together with the alignment consistency cri-
terion, single-branching recursive splitting limits
the search space sufficiently to make a full search
tractable in many cases.
</bodyText>
<subsectionHeader confidence="0.998691">
2.3 Other heuristics
</subsectionHeader>
<bodyText confidence="0.999894566666667">
The following heuristics seem promising but are
left to be explored in future work.
Min-cut splitting In this strategy, we place no
restriction on which split nodes are chosen. In-
stead, we require that the overall count of split
nodes is minimal, which is equivalent to saying
that the edges cut by the split form a minimum cut
separating the nodes aligned to the left and right
halves of the sentence, respectively. This strat-
egy has the advantage of being able to handle any
alignment/split point combination, but requires a
more complex splitting pattern and thus more ad-
ditional structure than single-branching recursion.
Syntax-driven splitting Since CCG is based
on the assumption that semantic and syntactic
derivations are isomorphic, we might use syntac-
tic annotations to guide the search of the deriva-
tion space and only consider splits along con-
stituent boundaries. Syntactic annotations might
be present in the data or generated by standard
tools. However, initial tests have shown that this
requirement is too restrictive when combined with
our two main heuristics.
Obviously, an effective combination of heuris-
tics needs to be found. One particular configura-
tion which seems promising is alignment consis-
tency combined with min-cut splitting (which is
more permissive than single-branching recursion)
and syntax-driven splitting (which adds an extra
restriction).
</bodyText>
<sectionHeader confidence="0.999807" genericHeader="method">
3 Discussion
</sectionHeader>
<bodyText confidence="0.999983166666667">
We present some empirical observations about the
behaviour of the above-mentioned heuristics. Our
observations are based on a grammar extracted
from the GMB. A formal evaluation of our system
in the context of a full semantic parsing system is
left for future work.
</bodyText>
<subsectionHeader confidence="0.998982">
3.1 Implementation
</subsectionHeader>
<bodyText confidence="0.999839375">
Currently, our system implements single-
branching recursive splitting along with alignment
consistency. We extracted the word-to-meaning
alignments from the CCG derivations annotated
in the GMB, but kept only alignment edges
to predicate nodes. Sentence grammars were
extracted by generating an initial item for each
sentence and feeding it to the SPLIT procedure.
In addition to alignment consistency and single-
branching recursion, we enforce three simple cri-
teria to rule out highly implausible items: The
count of arrows in an extracted meaning represen-
tation’s type is limited to eight, the number of split
nodes is limited to three, and the number of free
variables in extracted components is also limited
to three.
A major limitation of our implementation is that
it currently only considers the application combi-
nator during splitting. We take this as a main rea-
son for the limited granularity we observe in our
output. Generalisation of the splitting implemen-
tation to other combinators such as composition is
therefore necessary before performing any serious
evaluation.
</bodyText>
<figure confidence="0.996828">
a
x0 x1 x2 x3 x4
@
@
b c
@
d e
</figure>
<page confidence="0.982725">
14
</page>
<subsectionHeader confidence="0.996997">
3.2 Manual inspection
</subsectionHeader>
<bodyText confidence="0.999987815789474">
Manual inspection of the generated grammars
leads to two general observations.
Firstly, many single-word items present in the
CCG annotations of the GMB are recovered.
While this behaviour is not required, it is en-
couraging, as these items exhibit a relatively sim-
ple structure and would be expected to generalise
well.
At the same time, many multi-word phrases
remain in the data that cannot be split further,
and are therefore unlikely to generalise well. We
have identified two likely causes for this phe-
nomenon: The missing implementation of a com-
position combinator, and coarse alignments.
Composition splits would enable the splitting of
items which do not decompose well (i. e., do not
pass the search heuristics in use) under the appli-
cation combinator. Since composition occurs fre-
quently in GMB derivations, it is to be expected
that its lack noticeably impoverishes the quality of
the extracted grammar.
The extraction of alignments currently in use in
our implementation works by retracing the CCG
derivations annotated in the GMB, and thus es-
tablishing a link between a word and the set of
meaning representation elements introduced by it.
However, our current implementation only han-
dles the most common derivation nodes and oth-
erwise cuts this retracing process short, making
alignments to the entire phrase governed by an in-
termediate node. This may cause the correspond-
ing part of the search to be pruned due to a search
space explosion. We plan to investigate using a
statistical alignment tool instead, possibly using
supplementary heuristics for determining aligned
words and nodes. As an additional advantage, this
would remove the need for annotated CCG deriva-
tions in the data.
</bodyText>
<subsectionHeader confidence="0.99953">
3.3 Statistical observations
</subsectionHeader>
<bodyText confidence="0.999965454545454">
From the total 47 230 sentences present in the
GMB, our software was able to extract a sentence
grammar for 43 046 sentences. Failures occurred
either because processing took longer than 20 min-
utes, because the count of items extracted for a
single sentence surpassed 10 000, or due to pro-
cessing errors.
On average, 825 items were extracted per sen-
tence with a median of 268. After removing dupli-
cate items, the combined grammar for the whole
GMB consisted of about 32 million items. While
the running time of splitting is still exponential
and gets out of hand on some examples, most sen-
tences are processed within seconds.
Single-word items were extracted for 46% of
word occurrences. Ideally, we would like to ob-
tain single-word items for as many words as pos-
sible, as those items have the highest potential to
generalise to unseen data. For those occurrences
where no single-word item was extracted, the me-
dian length of the smallest extracted item was 12,
with a maximum of 49.
</bodyText>
<sectionHeader confidence="0.999461" genericHeader="evaluation">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999982076923077">
We have presented a method for bringing the in-
duction of semantic CCGs to a larger scale than
has been feasible so far. Using the heuristics of
alignment consistency and single-branching recur-
sive splitting, we are able to extract a grammar
from the full GMB. Our observations suggest a
mixed outcome: We obtain desirable single-word
items for only about half of all word occurrences.
However, due to the incompleteness of the im-
plementation and the lack of a formal evaluation,
these observations do not yet permit any conclu-
sions. In future work, we will address both of
these shortcomings.
</bodyText>
<sectionHeader confidence="0.998981" genericHeader="conclusions">
5 Final remarks
</sectionHeader>
<bodyText confidence="0.9997398">
The software implementing the presented func-
tionality is available for download1.
This work has been supported by the Ger-
man Research Foundation (DFG) as part of the
CINACS international graduate research group.
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999791333333333">
Valerio Basile, Johan Bos, Kilian Evang, and Noortje
Venhuizen. 2012. Developing a large semantically
annotated corpus. In Proceedings of LREC’12, Is-
tanbul, Turkey.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of ACL 2013, Sofia, Bul-
garia.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts, USA.
</reference>
<footnote confidence="0.6514045">
1http://nats-www.informatik.
uni-hamburg.de/User/SebastianBeschke
</footnote>
<page confidence="0.984168">
15
</page>
<reference confidence="0.997944266666667">
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of EMNLP 2010,
Cambridge, Massachusetts, USA.
Peng Li, Yang Liu, and Maosong Sun. 2013. An ex-
tended GHKM algorithm for inducing λ-scfg. In
Proceedings of AAAI 2013, Bellevue, Washington,
USA.
Mark Steedman. 2001. The Syntactic Process. MIT
Press, January.
John M. Zelle and Raymond J. Mooney. 1996. Learn-
ing to parse database queries using inductive logic
programming. In Proceedings ofAAAI-96, Portland,
Oregon, USA.
</reference>
<page confidence="0.998691">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.578426">
<title confidence="0.981192">Large-scale CCG Induction from the Groningen Meaning Bank</title>
<author confidence="0.995455">Yang</author>
<affiliation confidence="0.975108">of Informatics, University of Hamburg, Key Laboratory of Intelligent Technology and Tsinghua National Laboratory for Information Science and Department of Computer Science and Technology, Tsinghua University, Beijing,</affiliation>
<email confidence="0.940256">liuyang2011@tsinghua.edu.cn</email>
<abstract confidence="0.980474352941177">In present CCG-based semantic parsing systems, the extraction of a semantic grammar from sentence-meaning examples poses a computational challenge. An important factor is the decomposition of the sentence meaning into smaller parts, each corresponding to the meaning of a word or phrase. This has so far limited supervised semantic parsing to small, specialised corpora. We propose a set of heuristics that render the splitting of meaning representations feasible on a largescale corpus, and present a method for grammar induction capable of extracting a semantic CCG from the Groningen Meaning Bank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Valerio Basile</author>
<author>Johan Bos</author>
<author>Kilian Evang</author>
<author>Noortje Venhuizen</author>
</authors>
<title>Developing a large semantically annotated corpus.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC’12,</booktitle>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="1890" citStr="Basile et al., 2012" startWordPosition="274" endWordPosition="277">re the construction of the meaning representation directly follows the syntactic analysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (2010), we view grammar induction as a series of splitting steps, each of which essentially reverses a CCG derivation step. However, we diverge from their approach by applying novel heuristics for searching the space of possible splits. The combination of alignment consistency and singlebranching recursion turns out to produce a manageable number of lexical items for most sentences in the GMB, while statistical measures and manual inspection suggest that many of these items are also plausible. 2 Searching the sp</context>
</contexts>
<marker>Basile, Bos, Evang, Venhuizen, 2012</marker>
<rawString>Valerio Basile, Johan Bos, Kilian Evang, and Noortje Venhuizen. 2012. Developing a large semantically annotated corpus. In Proceedings of LREC’12, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL 2013,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1719" citStr="Cai and Yates, 2013" startWordPosition="243" endWordPosition="246">ammar (CCG) forms the basis of many current approaches to semantic parsing. It is attractive for semantic parsing due to its unified treatment of syntax and semantics, where the construction of the meaning representation directly follows the syntactic analysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (2010), we view grammar induction as a series of splitting steps, each of which essentially reverses a CCG derivation step. However, we diverge from their approach by applying novel heuristics for searching the space of possible splits. The combination of alignment consistency and singlebranching recursion turns out to produce a manageable numb</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Proceedings of ACL 2013, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of HLT-NAACL 2004,</booktitle>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="4873" citStr="Galley et al., 2004" startWordPosition="769" endWordPosition="772">ible splits of a meaning representation according to the reverse application of a combinator. function SPLIT(x, z) if jxj = 1 then return {(x, z)} else G +— 0 for0&lt;i&lt;_ jxj − 1 and c E C do l +— x0 ... xi−1 r +— xz ... xjxj−1 S +— SEMSPLIT(c, z) for (f, g) E S do G +— G U SPLIT(l, f) U SPLIT(r, g) end for end for return G end if end function 2.1 Alignment consistency The first heuristic we introduce is borrowed from the field of statistical machine translation. There, alignments between words of two languages are used to identify corresponding phrase pairs, as in the well-known GHKM algorithm (Galley et al., 2004). In order to apply the same strategy to meaning representations, we represent them as their abstract syntax trees. Following Li et al. (2013), we can then align words in the sentence and nodes in the meaning representation to identify components that correspond to each other. This allows us to impose an extra constraint on the generation of splits: We require that nodes in f not be aligned to any words in the right sentencehalf r, and conversely, that nodes in g not be aligned to words in l. Alignment consistency helps the search to focus on more plausible splits by grouping elements of the m</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of HLT-NAACL 2004, Boston, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP 2010,</booktitle>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="1979" citStr="Kwiatkowski et al. (2010)" startWordPosition="290" endWordPosition="293">lysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (2010), we view grammar induction as a series of splitting steps, each of which essentially reverses a CCG derivation step. However, we diverge from their approach by applying novel heuristics for searching the space of possible splits. The combination of alignment consistency and singlebranching recursion turns out to produce a manageable number of lexical items for most sentences in the GMB, while statistical measures and manual inspection suggest that many of these items are also plausible. 2 Searching the space of CCG derivations Our search heuristics are embedded into a very general splitting a</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higherorder unification. In Proceedings of EMNLP 2010, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
</authors>
<title>An extended GHKM algorithm for inducing λ-scfg.</title>
<date>2013</date>
<booktitle>In Proceedings of AAAI 2013,</booktitle>
<location>Bellevue, Washington, USA.</location>
<contexts>
<context position="5015" citStr="Li et al. (2013)" startWordPosition="792" endWordPosition="795"> else G +— 0 for0&lt;i&lt;_ jxj − 1 and c E C do l +— x0 ... xi−1 r +— xz ... xjxj−1 S +— SEMSPLIT(c, z) for (f, g) E S do G +— G U SPLIT(l, f) U SPLIT(r, g) end for end for return G end if end function 2.1 Alignment consistency The first heuristic we introduce is borrowed from the field of statistical machine translation. There, alignments between words of two languages are used to identify corresponding phrase pairs, as in the well-known GHKM algorithm (Galley et al., 2004). In order to apply the same strategy to meaning representations, we represent them as their abstract syntax trees. Following Li et al. (2013), we can then align words in the sentence and nodes in the meaning representation to identify components that correspond to each other. This allows us to impose an extra constraint on the generation of splits: We require that nodes in f not be aligned to any words in the right sentencehalf r, and conversely, that nodes in g not be aligned to words in l. Alignment consistency helps the search to focus on more plausible splits by grouping elements of the meaning representation with the words that evoked them. However, by itself it does not significantly limit the search space, as it is still pos</context>
</contexts>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Peng Li, Yang Liu, and Maosong Sun. 2013. An extended GHKM algorithm for inducing λ-scfg. In Proceedings of AAAI 2013, Bellevue, Washington, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2001</date>
<publisher>MIT Press,</publisher>
<contexts>
<context position="1376" citStr="Steedman, 2001" startWordPosition="189" endWordPosition="191"> limited supervised semantic parsing to small, specialised corpora. We propose a set of heuristics that render the splitting of meaning representations feasible on a largescale corpus, and present a method for grammar induction capable of extracting a semantic CCG from the Groningen Meaning Bank. 1 Introduction Combinatory Categorial Grammar (CCG) forms the basis of many current approaches to semantic parsing. It is attractive for semantic parsing due to its unified treatment of syntax and semantics, where the construction of the meaning representation directly follows the syntactic analysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (20</context>
</contexts>
<marker>Steedman, 2001</marker>
<rawString>Mark Steedman. 2001. The Syntactic Process. MIT Press, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings ofAAAI-96,</booktitle>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="1697" citStr="Zelle and Mooney, 1996" startWordPosition="239" endWordPosition="242">ombinatory Categorial Grammar (CCG) forms the basis of many current approaches to semantic parsing. It is attractive for semantic parsing due to its unified treatment of syntax and semantics, where the construction of the meaning representation directly follows the syntactic analysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (2010), we view grammar induction as a series of splitting steps, each of which essentially reverses a CCG derivation step. However, we diverge from their approach by applying novel heuristics for searching the space of possible splits. The combination of alignment consistency and singlebranching recursion turns out to pro</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John M. Zelle and Raymond J. Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings ofAAAI-96, Portland, Oregon, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.749569">
LAYERED: Metric for Machine Translation Evaluation
</title>
<author confidence="0.965255">
Shubham Gautam
</author>
<affiliation confidence="0.927011">
Computer Science &amp; Engineering,
IIT Bombay
</affiliation>
<email confidence="0.948593">
shubhamg@cse.iitb.ac.in
</email>
<author confidence="0.972496">
Pushpak Bhattacharyya
</author>
<affiliation confidence="0.929115">
Computer Science &amp; Engineering,
IIT Bombay
</affiliation>
<email confidence="0.969734">
pb@cse.iitb.ac.in
</email>
<sectionHeader confidence="0.994597" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899222222222">
This paper describes the LAYERED met-
ric which is used for the shared WMT’14
metrics task. Various metrics exist for MT
evaluation: BLEU (Papineni, 2002), ME-
TEOR (Alon Lavie, 2007), TER (Snover,
2006) etc., but are found inadequate in
quite a few language settings like, for ex-
ample, in case of free word order lan-
guages. In this paper, we propose an MT
evaluation scheme that is based on the
NLP layers: lexical, syntactic and seman-
tic. We contend that higher layer met-
rics are after all needed. Results are pre-
sented on the corpora of ACL-WMT, 2013
and 2014. We end with a metric which is
composed of weighted metrics at individ-
ual layers, which correlates very well with
human judgment.
</bodyText>
<sectionHeader confidence="0.998129" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999816">
Evaluation is an integral component of machine
translation (MT). Human evaluation is difficult
and time consuming so there is a need for a metric
which can give the better evaluation in correlation
to human judgement. There are several existing
metrics such as: BLEU, METEOR etc. but these
only deal with the lexical layer combining bag of
words and n-gram based approach.
We present an analysis of BLEU and the higher
layer metrics on the ACL WMT 2013 corpora
with 3 language pairs: French-English, Spanish-
English and German-English. For syntactic layer,
we considered three metrics: Hamming score,
Kendall’s Tau distance score and the spearman
rank score. Syntactic layer metrics take care of
reordering within the words of the sentences so
these may play an important role when there is
a decision to be made between two MT output
sentences of two different systems when both the
sentences have same number of n-gram matches
wrt the reference sentence but there is a differ-
ence in the ordering of the sentence. We will dis-
cuss these metrics in detail in the following sec-
tions. The next NLP layer in consideration is the
semantic layer which deals with the meaning of
the sentences. For semantic layer, we considered
two metrics: Shallow semantic score and Deep se-
mantic score. On semantic layer, we considered
entailment based measures to get the score.
Ananthkrishnan et al. (2007) mentioned some
issues in automatic evaluation using BLEU. There
are some disadvantages of the existing metrics
also such as: BLEU does not take care of reorder-
ing of the words in the sentence. BLEU-like met-
rics can give same score by permuting word or-
der. These metrics can be unreliable at the level
of individual sentences because there can be small
number of n-grams involved. We would see in this
paper that the correlation of BLEU is lower com-
pared to the semantic layer metrics.
Section 2 presents the study of related work in
MT evaluation. Section 3 presents the importance
of each NLP layer in evaluation of MT output. It
discusses the metrics that each layer contributes to
the achievement of the final result. In section 4,
various experiments are presented with each met-
ric on the top 10 ranking systems of WMT 13
corpora which are ranked on the basis of the hu-
man ranking. Each metric is discussed with the
graphical representation so that it would become
clear to analyze the effect of each metric. In sec-
tion 5, spearman correlation of the metrics is cal-
culated with human judgement and comparisons
are shown. In section 6, we discuss the need of a
metric which should be a combination of the met-
rics presented in the above sections and present a
weighted metric which is the amalgamation of the
metrics at individual layers. Section 7 presents the
results of the proposed metric on WMT 14 data
and compares it with other existing metrics.
</bodyText>
<page confidence="0.529425">
387
</page>
<note confidence="0.6288095">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 387–393,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.999593" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999961378378379">
Machine translation evaluation has always re-
mained as the most popular measure to judge the
quality of a system output compared to the refer-
ence translation. Papineni (2002) proposed BLEU
as an automatic MT evaluation metric which is
based on the n-gram matching of the reference
and candidate sentences. This is still considered
as the most reliable metric and used widely in
the MT community for the determination of the
translation quality. BLEU averages the precision
for unigram, bigram and up to 4-gram and ap-
plies a length penalty if the generated sentence
is shorter than the best matching (in length) ref-
erence translation. Alternative approaches have
been designed to address problems with BLEU.
Doddington and George (2003) proposed NIST
metric which is derived from the BLEU evalua-
tion criterion but differs in one fundamental as-
pect: instead of n-gram precision, the informa-
tion gain from each n-gram is taken into account.
TER (Snover, 2006) tries to improve the hypothe-
sis/reference matching process based on the edit-
distance and METEOR (Alon Lavie, 2007) con-
sidered linguistic evidence, mostly lexical similar-
ity, for more intelligent matching. Liu and Gildea
(2005), Owczarzak et al. (2007), and Zhang et al.
(2004) use syntactic overlap to calculate the sim-
ilarity between the hypothesis and the reference.
Pad´o and Galley (2009) proposed a metric that
evaluates MT output based on a rich set of textual
entailment features. There are different works that
have been done at various NLP layers. Gim´enez
tl al. (2010) provided various linguistic measures
for MT evaluation at different NLP layers. Ding
Liu and Daniel Gildea (2005) focussed the study
on the syntactic features that can be helpful while
evaluation.
</bodyText>
<sectionHeader confidence="0.9438805" genericHeader="method">
3 Significance of NLP Layers in NIT
Evaluation
</sectionHeader>
<bodyText confidence="0.999491">
In this section, we discuss the different NLP layers
and how these are important for evalution of MT
output. We discuss here the significance of three
NLP layers: Lexical, Syntactic and Semantic lay-
ers.
</bodyText>
<subsectionHeader confidence="0.999827">
3.1 Lexical Layer
</subsectionHeader>
<bodyText confidence="0.9999865625">
Lexical layer emphasizes on the comparison of the
words in its original form irrespective of any lexi-
cal corpora or any other resource. There are some
metrics in MT evaluation which considers only
these features. Most popular of them is BLEU,
this is based on the n-gram approach and consid-
ers the matching upto 4-grams in the reference and
the candidate translation. BLEU is designed to
approximate human judgement at a corpus level,
and performs badly if used to evaluate the quality
of individual sentences. Another important metric
at this layer is TER (Translation Edit Rate) which
measures the number of edits required to change
a system output into one of the references. For
our experiments, we would consider BLEU as the
baseline metric on lexical layer.
</bodyText>
<subsectionHeader confidence="0.999828">
3.2 Syntactic Layer
</subsectionHeader>
<bodyText confidence="0.999975416666667">
Syntactic layer takes care of the syntax of the
sentence. It mainly focusses on the reordering
of the words within a sentence. Birch and Os-
borne (2011) has mentioned some metrics on this
layer: Hamming score and Kendall’s Tau Dis-
tance (KTD) score. We additionally calculated the
spearman rank score on this layer. Scores are cal-
culated first by giving ranking of words in the ref-
erence sentence and then putting the rank number
of the word in the candidate sentence. Now, we
have the relative ranking of the words of both the
sentences, so final score is calculated.
</bodyText>
<subsectionHeader confidence="0.999769">
3.3 Semantic Layer
</subsectionHeader>
<bodyText confidence="0.999987095238095">
Semantic layer goes into the meaning of the sen-
tence, so we need to compare the dependency tree
of the sentences. At this layer, we used entailment
based metrics for the comparison of dependencies.
Pad´o and Galley (2009) illustrated the use of text
entailment based features for MT evaluation. We
introduced two metrics at this layer: first is Shal-
low semantic score, which is based on the depen-
dencies generated by a shallow parser and then
the dependency comparison is carried out. Sec-
ond is Deep semantic score, which goes more deep
into the semantic of the sentence. For shallow se-
mantic score, we used stanford dependency parser
(Marie-Catherine et al., 2006) while for deep se-
mantic score, we used UNL (Universal Network-
ing Language)1 dependency generator.
Semantic layer may play an important role
when there are different words in two sentences
but they are synonym of each other or are related
to each other in some manner. In this case, lexical
and syntactic layers can’t identify the similarity of
</bodyText>
<footnote confidence="0.807917">
1http://www.undl.org/unlsys/unl/unl2005/UW.htm
</footnote>
<page confidence="0.799546">
388
</page>
<bodyText confidence="0.999952444444445">
the sentences because there exist a need of some
semantic background knowledge which occurs at
the semantic layer. Another important role of se-
mantic layer is that there can be cases when there
is reordering of the phrases in the sentences, e.g.,
active-passive voice sentences. In these cases, de-
pendencies between the words remain intact and
this can be captured through dependency tree gen-
erated by the parser.
</bodyText>
<sectionHeader confidence="0.999254" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999970166666667">
We conducted the experiments on WMT 13 cor-
pora for French-English, Spanish-English and
German-English language pairs. We calculated
the score of each metric for the top 10 ranking
system (wmt, 2013) (as per human judgement) for
each language pair.
</bodyText>
<note confidence="0.536191">
Note:
</note>
<listItem confidence="0.998732666666667">
1. In the graphs, metric score is multiplied by 100
so that a better view can be captured.
2. In each graph, the scores of French-English (fr-
en), Spanish-English (es-en) and German-English
(de-en) language pairs are represented by red,
black and blue lines respectively.
</listItem>
<subsectionHeader confidence="0.9885395">
4.1 BLEU Score
4.2 Syntactic Layer
</subsectionHeader>
<bodyText confidence="0.999946">
Because the BLEU score was not able to capture
the idealistic curve in the last section so we consid-
ered the syntactic layer metrics. This layer is con-
sidered because it takes care of the reordering of
the words within the sentence pair. The idea here
is that if one candidate translation has lower re-
ordering of words w.r.t. reference translation then
it has higher chances of matching to the reference
sentence.
</bodyText>
<subsectionHeader confidence="0.699158">
4.2.1 Hamming Score
</subsectionHeader>
<bodyText confidence="0.999983571428571">
The hamming distance measures the number of
disagreements between two permutations. First
we calculate the hamming distance and then cal-
culate the fraction of words placed in the same po-
sition in both sentences, finally we calculate the
hamming score by subtracting the fraction from 1.
It is formulated as follows:
</bodyText>
<equation confidence="0.970487333333333">
�
0; if 7r(i) = u(i)
1; otherwise
</equation>
<bodyText confidence="0.997583571428571">
where, n is the length of the permutation.
Hamming scores for all three language pairs
mentioned above are shown in fig. 2. As we can
see from the graph that initially its not good for the
top ranking systems but it follows the ideal curve
for the discrimination of lower ranking systems for
the language pairs fr-en and es-en.
</bodyText>
<equation confidence="0.953319">
dh(7r, u) = 1 �n i=1 xi
n
, xi =
</equation>
<figureCaption confidence="0.996382">
Figure 1: BLEU Score
</figureCaption>
<bodyText confidence="0.999943571428571">
We can see from the graph of fig. 1 that for de-
en and es-en language pair, BLEU is not able to
capture the phenomenon appropriately. In fact, it
is worse in de-en pair. Because the graph should
be of decreasing manner i.e., as the rank of the sys-
tem increases (system gets lower rank compared to
the previous one), the score should also decrease.
</bodyText>
<figureCaption confidence="0.973909">
Figure 2: Hamming Score
</figureCaption>
<subsubsectionHeader confidence="0.570194">
4.2.2 Kendall’s Tau Distance (KTD)
</subsubsectionHeader>
<bodyText confidence="0.999878">
Kendall’s tau distance is the minimum number of
transpositions of two adjacent symbols necessary
to transform one permutation into another. It rep-
resents the percentage of pairs of elements which
</bodyText>
<page confidence="0.777159">
389
</page>
<bodyText confidence="0.9853145">
share the same order between two permutations. It
is defined as follows:
</bodyText>
<equation confidence="0.8614994">
390 4.3 Semantic Layer
�En En j=1 zij
i=1
dk(π, σ) = 1 −
1; otherwise
</equation>
<bodyText confidence="0.9911156">
This can be used for measuring word order dif-
ferences as the relative ordering of words has been
taken into account. KTD scores are shown in fig.
3. It also follows the same phenomenon as the
hamming score for fr-en an
</bodyText>
<equation confidence="0.623699333333333">
�
0; if π(i) &lt; π(j) and σ(i) &lt; σ(j)
where, zij =
</equation>
<bodyText confidence="0.601401">
d es-en pair but for de-
en pair, it gives the worst results.
</bodyText>
<figureCaption confidence="0.963582">
Figure 3: KTD Score
</figureCaption>
<equation confidence="0.3255844">
2
1
Ed
6 n(n2
1)
</equation>
<bodyText confidence="0.517741833333333">
where, di = xi
yi is the difference between
the ranks of words of two sentences.
Spearman score lies between -1 to +1 so we
convert it to the range of 0 to +1 so that all the
metrics would lie in the same ran
</bodyText>
<equation confidence="0.952544285714286">
=
−
iρ
−
−
ge.
d, what is it?
</equation>
<bodyText confidence="0.895542032258065">
According to wikipedia2,
entailment
(TE) in natural language processing is a direc-
tional relation between text fragments. The rela-
tion holds whenever the truth of one text fragment
follows from another text. In the TE framework,
the entailing and entailed texts are termed text (t)
and hypothesis (h),
First, the dependencies for both reference (R)
as well as candidate (C) translation are generated
using the parser that is used (will vary in both
the following metrics). Then, the entailment phe-
nomenon is applied fr
“Textual
respectively.”
om R to C i.e., dependencies
of C are searched in the dependency graph of R.
Matching number of dependencies are calculated,
then a score is obtained as follows:
ScoreR_C = layers as can be seen in the graphical representa-
tions. So, there arises a need to go higher in the
No. of matched dependencies of C in R
(1)
Total no. of dependencies of C
tences. At this layer, we considered two metrics.
Both metrics are based on the concept of text en-
tailment. First we should understan
Similarly, an
other score is also obtained by ap-
plying the entailment phenomenon in the reversed
direction i.e. from C to R as follows:
</bodyText>
<figure confidence="0.78955025">
No. of matched dependencies of
C
(
R in
</figure>
<figureCaption confidence="0.245183">
Total no. of dependencies of R
</figureCaption>
<bodyText confidence="0.7692035">
Final score is obtained by taking the average of
the above two scores as follows:
</bodyText>
<equation confidence="0.77957725">
+
ScoreR_C
ScoreC_R
2
</equation>
<bodyText confidence="0.989055090909091">
in the metrics at seman
tic layer:
This metric uses the
dependency parser
(Marie-Catherine et al., 2006) to generate the de-
pendencies. After getting the dependencies for
both reference (R) as well as candidate (C) trans-
lation, entailment phenomenon is applied an
stanford
d the
final score is obtained using eq. (3).
</bodyText>
<subsectionHeader confidence="0.779337">
4.2.3 Spearman Score
</subsectionHeader>
<bodyText confidence="0.967564153846154">
Spearman rank correlation coefficient is basically
used for assessing how well the relationship be-
tween two variables can be described using a
monotonic function. Because we are using syntac-
tic layer metrics to keep track of the reordering be-
tween two sentences, so this can be used by rank-
ing the words of the first sentence (ranging from
1 to n, where n is the length of the sentence) and
then checking where the particular word (with in-
dex i) is present in the second sentence in terms of
ranking. Finally, we calculated the spearman
score
as follows:
</bodyText>
<subsectionHeader confidence="0.928359">
Text Entailment
</subsectionHeader>
<bodyText confidence="0.979162">
Now, we discuss how can we use this concept
</bodyText>
<note confidence="0.53735">
4.3.1 Shallow Semantic Score
</note>
<footnote confidence="0.472576">
2http://wikipedia.org/
</footnote>
<equation confidence="0.990538333333333">
Z
ScoreC_R =
Scoreftnml =
</equation>
<bodyText confidence="0.999849">
We can see from the last two sections that there
were some loopholes on the metrics of both the
hierarchy. The next one in the queue is semantic
layer which takes care of the meaning of the sen-
</bodyText>
<figureCaption confidence="0.98076">
Figure 4: Shallow Semantic Score
</figureCaption>
<bodyText confidence="0.999962">
We can see from fig. 4 that for French-English
and Spanish-English pairs, the graph is very good
compared to the other metrics at the lower layers.
In fact, there is only one score in es-en pair that
a lower ranking system gets better score than the
higher ranking system.
</bodyText>
<subsectionHeader confidence="0.937589">
4.3.2 Deep Semantic Score
</subsectionHeader>
<bodyText confidence="0.986701142857143">
This metric uses the UNL dependency graph gen-
erator for taking care of the semantic of the sen-
tence that shallow dependency generator is not
able to capture. Similar to the shallow seman-
tic score, after getting the dependencies from the
UNL, entailment score is calculated in both direc-
tions i.e. R → C and C → R.
</bodyText>
<figureCaption confidence="0.988703">
Figure 5: Deep Semantic Score
</figureCaption>
<bodyText confidence="0.9987954">
Fig. 5 shows that deep semantic score curve
also follows the same path as shallow semantic
score. In fact, for Spanish-English pair, the path
is ideal i.e., the score is decreasing as the system
rank is increasing.
</bodyText>
<sectionHeader confidence="0.56754" genericHeader="method">
5 Correlation with Human Judgement
</sectionHeader>
<bodyText confidence="0.99853">
We calculated spearman rank correlation coeffi-
cient for the different scores calculated in the last
section. This score ranges from -1 to +1. Form ta-
</bodyText>
<table confidence="0.99971625">
Language Pair PBLEU PShallow PDeep
French-English 0.95 0.96 0.92
Spanish-English 0.89 0.98 1.00
German-English 0.36 0.88 0.89
</table>
<tableCaption confidence="0.8785255">
Table 1: Correlation with BLEU Score, Shallow
Semantic Score and Deep Semantic Score
</tableCaption>
<bodyText confidence="0.9997187">
ble 1, we can see that the correlation score is bet-
ter with semantic layer metrics compared to the
BLEU score (lower layer metrics). In compar-
ison to the WMT 13 results (wmt-result, 2013),
PShallow score for French-English pair is interme-
diate between the highest and lowest correlation
system. PD,,p score for Spanish-English is high-
est among all the systems presented at WMT 13.
So, it arises a need to take into account the seman-
tic of the sentence while evaluating the MT output.
</bodyText>
<sectionHeader confidence="0.937687" genericHeader="method">
6 Hybrid Approach
</sectionHeader>
<bodyText confidence="0.9997024375">
We reached to a situation where we can’t ig-
nore the score of any layer’s metric because each
metric helps to capture some of the phenomenon
which other may not capture. So, we used a hy-
brid approach where the final score of our pro-
posed metric depends on the layered metrics. As
already said, we performed our experiments on
ACL-WMT 2013 corpora, but it provided only the
rank of the systems. Due to availability of ranking
of the systems, we used SVM-rank to learn the pa-
rameters.
Our final metric looks as follows:
Final-Score = a*BLEU + b*Hamming + c*KTD
+ d*Spearman + e*Shallow-Semantic-Score +
f*Deep-Semantic-Score
where, a,b,c,d,e,f are parameters
</bodyText>
<subsectionHeader confidence="0.998567">
6.1 SVM-rank
</subsectionHeader>
<bodyText confidence="0.99999175">
SVM-rank learns the parameters from the training
data and builds a model which contains the learned
parameters. These parameters (model) can be used
for ranking of a new set of data.
</bodyText>
<subsectionHeader confidence="0.695667">
Parameters
</subsectionHeader>
<bodyText confidence="0.9985095">
We made the training data of the French-English,
Spanish-English and German-English language
</bodyText>
<page confidence="0.677995">
391
</page>
<table confidence="0.99936">
Metric Pearson Correlation
fr-en de-en hi-en cs-en ru-en Average
LAYERED .973 .893 .976 .940 .843 .925
BLEU .952 .831 .956 .908 .774 .884
METEOR .975 .926 .457 .980 .792 .826
NIST .955 .810 .783 .983 .785 .863
TER .952 .774 .618 .977 .796 .823
</table>
<tableCaption confidence="0.9999">
Table 2: Correlation with different metrics in WMT 14 Results
</tableCaption>
<bodyText confidence="0.9961245">
pairs. Then we ran SVM-rank and obtained the
scores for the parameters.
So, our final proposed metric looks like:
Final-Score = 0.26*BLEU + 0.13*Hamming +
0.03*KTD + 0.04*Spearman + 0.28*Shallow-
Semantic-Score + 0.26* Deep-Semantic-Score
</bodyText>
<sectionHeader confidence="0.975316" genericHeader="evaluation">
7 Performance in WMT 2014
</sectionHeader>
<bodyText confidence="0.999971133333333">
Table 2 shows the performance of our metric on
WMT 2014 data (wmt-result, 2014). It performed
very well in almost all language pairs and it
gave the highest correlation with human in Hindi-
English language pair. On an average, our corre-
lation was 0.925 with human considering all the
language pairs. This way, we stood out on sec-
ond position considering the average score while
the first ranking system obtained the correlation
of 0.942. Its clear from table 2 that the proposed
metric gives the correlation better than the stan-
dard metrics in most of the cases. If we look at
the average score of the metrics in table 2 then we
can see that LAYERED obtains much higher score
than the other metrics.
</bodyText>
<sectionHeader confidence="0.997785" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999690294117647">
Machine Translation Evaluation is an exciting
field that is attracting the researchers from the past
few years and the work in this field is enormous.
We started with the need of using higher layer
metrics while evaluating the MT output. We un-
derstand that it might be a little time consuming
but its efficient and correlation with human judge-
ment is better with semantic layer metric com-
pared to the lexical layer metric. Because, each
layer captures some linguistic phenomenon so we
can’t completely ignore the metrics at individual
layers. It gives rise to a hybrid approach which
gives the weightage for each metric for the calcu-
lation of final score. We can see from the results
of WMT 2014 that the correlation with LAYERED
metric is better than the standard existing metrics
in most of the language pairs.
</bodyText>
<sectionHeader confidence="0.99891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99962647368421">
Alexandra Birch, School of Informatics, University of
Edinburgh Reordering Metrics for Statistical Ma-
chine Translation. Phd Thesis, 2011.
Alexandra Birch and Miles Osborne Reordering Met-
rics for MT. Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies - Volume 1, se-
ries = HLT 2011.
Alon Lavie and Abhaya Agarwal. METEOR: An Auto-
matic Metric for MT Evaluation with High Levels of
Correlation with Human Judgments, Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, StatMT 2007.
Ananthakrishnan R and Pushpak Bhattacharyya and M
Sasikumar and Ritesh M Shah Some Issues in Auto-
matic Evaluation of English-Hindi MT: More Blues
for BLEU. ICON, 2007.
Doddington and George Automatic evaluation of
machine translation quality using N-gram co-
occurrence statistics, NIST. Proceedings of the
2nd International Conference on Human Language
Technology Research HLT 2002.
Ding Liu and Daniel Gildea Syntactic Features for
Evaluation of Machine Translation. Workshop On
Intrinsic And Extrinsic Evaluation Measures For
Machine Translation And/or Summarization, 2005.
Findings of the 2013 Workshop on Statistical Machine
Translation. ACL-WMT 2013.
Gim´enez, Jes´us and M`arquez, Llu´ıs Linguistic Mea-
sures for Automatic Machine Translation Evalua-
tion. Machine Translation, December, 2010.
Liu D, Gildea D Syntactic features for evaluation of
machine translation. ACL 2005 workshop on intrin-
sic and extrinsic evaluation measures for machine
translation and/or summarization.
Owczarzak K, Genabith J, Way A Evaluating ma-
chine translation with LFG dependencies. Machine
Translation 21(2):95119.
</reference>
<page confidence="0.526588">
392
</page>
<reference confidence="0.999592851851852">
Marie-Catherine de Marneffe, Bill MacCartney and
Christopher D. Manning. Generating Typed Depen-
dency Parses from Phrase Structure Parses. LREC
2006.
Matthew Snover and Bonnie Dorr and Richard
Schwartz and Linnea Micciulla and John Makhoul.
A Study of Translation Edit Rate with Targeted Hu-
man Annotation, In Proceedings of Association for
Machine Translation in the Americas, 2006.
Papineni, Kishore and Roukos, Salim and Ward, Todd
and Zhu, Wei-Jing. BLEU: A Method for Automatic
Evaluation of Machine Translation. Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL 2002.
Results of the WMT13 Metrics Shared Task. ACL-
WMT 2013.
Results of the WMT14 Metrics Shared Task. ACL-
WMT 2014.
Sebastian Pad´o and Michel Galley and Dan Jurafsky
and Chris Manning Robust Machine Translation
Evaluation with Entailment Features. Proceedings
of ACL-IJCNLP 2009, ACL 2009.
Zhang Y, Vogel S, Waibel A Interpreting Bleu/NIST
scores: how much improvement do we need to have
a better system?. In: Proceedings of the 4th interna-
tional conference on language resources and evalua-
tion. Lisbon, Portugal.
</reference>
<page confidence="0.968788">
393
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.314448">
<title confidence="0.998331">LAYERED: Metric for Machine Translation Evaluation</title>
<author confidence="0.839053">Shubham</author>
<affiliation confidence="0.948944">Computer Science &amp; IIT</affiliation>
<email confidence="0.700673">shubhamg@cse.iitb.ac.in</email>
<author confidence="0.77541">Pushpak</author>
<affiliation confidence="0.937846">Computer Science &amp; IIT</affiliation>
<email confidence="0.774563">pb@cse.iitb.ac.in</email>
<abstract confidence="0.998617736842105">This paper describes the LAYERED metric which is used for the shared WMT’14 metrics task. Various metrics exist for MT evaluation: BLEU (Papineni, 2002), ME- TEOR (Alon Lavie, 2007), TER (Snover, 2006) etc., but are found inadequate in quite a few language settings like, for example, in case of free word order languages. In this paper, we propose an MT evaluation scheme that is based on the NLP layers: lexical, syntactic and semantic. We contend that higher layer metrics are after all needed. Results are presented on the corpora of ACL-WMT, 2013 and 2014. We end with a metric which is composed of weighted metrics at individual layers, which correlates very well with human judgment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
</authors>
<title>School of Informatics, University of Edinburgh Reordering Metrics for Statistical Machine Translation. Phd Thesis,</title>
<date>2011</date>
<marker>Birch, 2011</marker>
<rawString>Alexandra Birch, School of Informatics, University of Edinburgh Reordering Metrics for Statistical Machine Translation. Phd Thesis, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Birch</author>
<author>Miles Osborne</author>
</authors>
<title>Reordering Metrics for MT.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, series = HLT</booktitle>
<contexts>
<context position="6903" citStr="Birch and Osborne (2011)" startWordPosition="1136" endWordPosition="1140"> 4-grams in the reference and the candidate translation. BLEU is designed to approximate human judgement at a corpus level, and performs badly if used to evaluate the quality of individual sentences. Another important metric at this layer is TER (Translation Edit Rate) which measures the number of edits required to change a system output into one of the references. For our experiments, we would consider BLEU as the baseline metric on lexical layer. 3.2 Syntactic Layer Syntactic layer takes care of the syntax of the sentence. It mainly focusses on the reordering of the words within a sentence. Birch and Osborne (2011) has mentioned some metrics on this layer: Hamming score and Kendall’s Tau Distance (KTD) score. We additionally calculated the spearman rank score on this layer. Scores are calculated first by giving ranking of words in the reference sentence and then putting the rank number of the word in the candidate sentence. Now, we have the relative ranking of the words of both the sentences, so final score is calculated. 3.3 Semantic Layer Semantic layer goes into the meaning of the sentence, so we need to compare the dependency tree of the sentences. At this layer, we used entailment based metrics for</context>
</contexts>
<marker>Birch, Osborne, 2011</marker>
<rawString>Alexandra Birch and Miles Osborne Reordering Metrics for MT. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, series = HLT 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments,</title>
<date>2007</date>
<booktitle>Proceedings of the Second Workshop on Statistical Machine Translation, StatMT</booktitle>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments, Proceedings of the Second Workshop on Statistical Machine Translation, StatMT 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Ananthakrishnan</author>
<author>Pushpak Bhattacharyya</author>
<author>M Sasikumar</author>
<author>M Ritesh</author>
</authors>
<title>Shah Some Issues in Automatic Evaluation of English-Hindi MT: More Blues for BLEU. ICON,</title>
<date>2007</date>
<marker>Ananthakrishnan, Bhattacharyya, Sasikumar, Ritesh, 2007</marker>
<rawString>Ananthakrishnan R and Pushpak Bhattacharyya and M Sasikumar and Ritesh M Shah Some Issues in Automatic Evaluation of English-Hindi MT: More Blues for BLEU. ICON, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doddington</author>
<author>George</author>
</authors>
<title>Automatic evaluation of machine translation quality using N-gram cooccurrence statistics,</title>
<date>2002</date>
<booktitle>NIST. Proceedings of the 2nd International Conference on Human Language Technology Research HLT</booktitle>
<marker>Doddington, George, 2002</marker>
<rawString>Doddington and George Automatic evaluation of machine translation quality using N-gram cooccurrence statistics, NIST. Proceedings of the 2nd International Conference on Human Language Technology Research HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation. Workshop On Intrinsic And Extrinsic Evaluation Measures For Machine Translation And/or Summarization,</title>
<date>2005</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. ACL-WMT</booktitle>
<contexts>
<context position="5149" citStr="Liu and Gildea (2005)" startWordPosition="846" endWordPosition="849">ce is shorter than the best matching (in length) reference translation. Alternative approaches have been designed to address problems with BLEU. Doddington and George (2003) proposed NIST metric which is derived from the BLEU evaluation criterion but differs in one fundamental aspect: instead of n-gram precision, the information gain from each n-gram is taken into account. TER (Snover, 2006) tries to improve the hypothesis/reference matching process based on the editdistance and METEOR (Alon Lavie, 2007) considered linguistic evidence, mostly lexical similarity, for more intelligent matching. Liu and Gildea (2005), Owczarzak et al. (2007), and Zhang et al. (2004) use syntactic overlap to calculate the similarity between the hypothesis and the reference. Pad´o and Galley (2009) proposed a metric that evaluates MT output based on a rich set of textual entailment features. There are different works that have been done at various NLP layers. Gim´enez tl al. (2010) provided various linguistic measures for MT evaluation at different NLP layers. Ding Liu and Daniel Gildea (2005) focussed the study on the syntactic features that can be helpful while evaluation. 3 Significance of NLP Layers in NIT Evaluation In</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea Syntactic Features for Evaluation of Machine Translation. Workshop On Intrinsic And Extrinsic Evaluation Measures For Machine Translation And/or Summarization, 2005. Findings of the 2013 Workshop on Statistical Machine Translation. ACL-WMT 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>M`arquez</author>
</authors>
<title>Llu´ıs Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation,</title>
<date>2010</date>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Gim´enez, Jes´us and M`arquez, Llu´ıs Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation, December, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Liu</author>
<author>D Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<journal>ACL</journal>
<contexts>
<context position="5149" citStr="Liu and Gildea (2005)" startWordPosition="846" endWordPosition="849">ce is shorter than the best matching (in length) reference translation. Alternative approaches have been designed to address problems with BLEU. Doddington and George (2003) proposed NIST metric which is derived from the BLEU evaluation criterion but differs in one fundamental aspect: instead of n-gram precision, the information gain from each n-gram is taken into account. TER (Snover, 2006) tries to improve the hypothesis/reference matching process based on the editdistance and METEOR (Alon Lavie, 2007) considered linguistic evidence, mostly lexical similarity, for more intelligent matching. Liu and Gildea (2005), Owczarzak et al. (2007), and Zhang et al. (2004) use syntactic overlap to calculate the similarity between the hypothesis and the reference. Pad´o and Galley (2009) proposed a metric that evaluates MT output based on a rich set of textual entailment features. There are different works that have been done at various NLP layers. Gim´enez tl al. (2010) provided various linguistic measures for MT evaluation at different NLP layers. Ding Liu and Daniel Gildea (2005) focussed the study on the syntactic features that can be helpful while evaluation. 3 Significance of NLP Layers in NIT Evaluation In</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Liu D, Gildea D Syntactic features for evaluation of machine translation. ACL 2005 workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Owczarzak</author>
<author>J Genabith</author>
<author>Way</author>
</authors>
<title>A Evaluating machine translation with LFG dependencies.</title>
<journal>Machine Translation</journal>
<volume>21</volume>
<issue>2</issue>
<marker>Owczarzak, Genabith, Way, </marker>
<rawString>Owczarzak K, Genabith J, Way A Evaluating machine translation with LFG dependencies. Machine Translation 21(2):95119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses. LREC</title>
<date>2006</date>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney and Christopher D. Manning. Generating Typed Dependency Parses from Phrase Structure Parses. LREC 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation,</title>
<date>2006</date>
<booktitle>In Proceedings of Association for Machine Translation in the Americas,</booktitle>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover and Bonnie Dorr and Richard Schwartz and Linnea Micciulla and John Makhoul. A Study of Translation Edit Rate with Targeted Human Annotation, In Proceedings of Association for Machine Translation in the Americas, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL</booktitle>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing. BLEU: A Method for Automatic Evaluation of Machine Translation. Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL 2002.</rawString>
</citation>
<citation valid="true">
<date>2013</date>
<booktitle>Results of the WMT13 Metrics Shared Task. ACLWMT</booktitle>
<marker>2013</marker>
<rawString>Results of the WMT13 Metrics Shared Task. ACLWMT 2013.</rawString>
</citation>
<citation valid="true">
<title>Results of the WMT14 Metrics Shared Task.</title>
<date>2014</date>
<publisher>ACLWMT</publisher>
<marker>2014</marker>
<rawString>Results of the WMT14 Metrics Shared Task. ACLWMT 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Pad´o</author>
<author>Michel Galley</author>
<author>Dan Jurafsky</author>
</authors>
<title>Chris Manning Robust Machine Translation Evaluation with Entailment Features.</title>
<date>2009</date>
<booktitle>Proceedings of ACL-IJCNLP 2009, ACL</booktitle>
<marker>Pad´o, Galley, Jurafsky, 2009</marker>
<rawString>Sebastian Pad´o and Michel Galley and Dan Jurafsky and Chris Manning Robust Machine Translation Evaluation with Entailment Features. Proceedings of ACL-IJCNLP 2009, ACL 2009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
</authors>
<title>Waibel A Interpreting Bleu/NIST scores: how much improvement do we need to have a better system?. In:</title>
<booktitle>Proceedings of the 4th international conference on language resources and evaluation.</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Zhang, Vogel, </marker>
<rawString>Zhang Y, Vogel S, Waibel A Interpreting Bleu/NIST scores: how much improvement do we need to have a better system?. In: Proceedings of the 4th international conference on language resources and evaluation. Lisbon, Portugal.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001384">
<title confidence="0.998406">
DALES: Automated Tool for Detection, Annotation, Labelling and
Segmentation of Multiple Objects in Multi-Camera Video Streams
</title>
<author confidence="0.997123">
M. Bhat and J. I.Olszewska
</author>
<affiliation confidence="0.9776015">
University of Gloucestershire
School of Computing and Technology
</affiliation>
<address confidence="0.654892">
The Park, Cheltenham, GL50 2RH, UK
</address>
<email confidence="0.99844">
jolszewska@glos.ac.uk
</email>
<sectionHeader confidence="0.99389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999943555555556">
In this paper, we propose a new software tool called DALES to extract semantic information
from multi-view videos based on the analysis of their visual content. Our system is fully auto-
matic and is well suited for multi-camera environment. Once the multi-view video sequences are
loaded into DALES, our software performs the detection, counting, and segmentation of the vi-
sual objects evolving in the provided video streams. Then, these objects of interest are processed
in order to be labelled, and the related frames are thus annotated with the corresponding seman-
tic content. Moreover, a textual script is automatically generated with the video annotations.
DALES system shows excellent performance in terms of accuracy and computational speed and
is robustly designed to ensure view synchronization.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99980776">
With the increasing use of electronic equipments, storage devices and computational systems for ap-
plications such as video surveillance (Kumar et al., 2010) and sport event monitoring (Alsuqayhi and
Olszewska, 2013), the development of automated tools to process the resulting big amount of visual data
in order to extract meaningful information becomes a necessity.
In particular, the design of multi-view video annotation systems is a challenging, new task. It aims
to process multi-view video streams which consist of video sequences of a dynamic scene captured
simultaneously by multiple cameras. Such multi-input system is dedicated to automatically analyse the
visual content of the multi-camera records and to generate semantic and visual annotations, in the way
to assist users in the understanding and reasoning about large amount of acquired data.
For this purpose, data should be processed through different, major stages such as object-of-interest
detection and segmentation, frame labelling, and video annotation. In the literature, most of the works
dealing with the analysis of multi-camera video streams are focused on the sole task of tracking multiple,
moving objects and use different approaches such as background subtraction (Diaz et al., 2013), Bayesian
framework (Hsu et al., 2013), particle filter (Choi and Yoo, 2013), or Cardinalized Probability Hypothesis
Density (CPHD) based filter (Lamard et al., 2013). On the other hand, research on video annotation has
lead to the development of several efficient systems (Town, 2004; Natarajan and Nevatia, 2005; Bai et
al., 2007; Vrusias et al., 2007), but all designed for a single camera video stream input.
In this paper, we describe a full system which takes multi-camera video stream inputs and performs vi-
sual data processing to generate multi-view video annotations. Our system has been developed in context
of outdoor video-surveillance and is an automatic Detection, Annotation, LabElling and Segmentation
(DALES) software tool. It presents also the advantage to have an entire chain of data processing from
visual to textual one, reducing thus the semantic gap.
As camera calibration is in general an expensive process (Black et al., 2002) and in real life, surveil-
lance application measurements of camera parameters are not readily available (Guler et al., 2003),
DALES system does not involve any camera calibration parameters.
</bodyText>
<footnote confidence="0.505161">
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
</footnote>
<page confidence="0.990334">
87
</page>
<note confidence="0.796455">
Proceedings of the 25th International Conference on Computational Linguistics, pages 87–94,
Dublin, Ireland, August 23-29 2014.
</note>
<figure confidence="0.996159">
(a) (b)
</figure>
<figureCaption confidence="0.996695">
Figure 1: Overview of the flow of data within DALES software: (a) Context level Data Flow Diagram;
(b) Second level Data Flow Diagram.
</figureCaption>
<bodyText confidence="0.9958095">
DALES provides not only the automatic annotation of multi-view video streams, but also performs,
in multi-camera environment, tasks such as multiple object-of-interest detection and segmentation, tar-
get counting and labelling, and image annotations. Our approach could cope with any dynamic scene
recorded by pan-tilt-zoom (PTZ) static cameras with overlapping color views, unlike systems such as
(Kettnaker and Zabih, 1999) based on non-overlapping cameras. The acquired multi-view sequences
could contain complex backgrounds, moving objects or noisy foregrounds, and present illumination
variations or poor resolution.
Hence, the contribution of this paper is twofold:
</bodyText>
<listItem confidence="0.853561">
• the automated, textual annotation of multi-camera video streams based on visual features;
• the development of a full, automatic system covering all the phases from multiple, visual multi-
motion target detection to multi-view video annotation and text-script generation.
</listItem>
<bodyText confidence="0.99713425">
The paper is structured as follows. In Section 2, we describe our DALES system for fast, multiple
video-object detection, segmentation, labeling and effective multi-view video annotation. Our tool has
been successfully tested on standard, real-world video-surveillance dataset as reported and discussed in
Section 3. Conclusions are presented in Section 4.
</bodyText>
<sectionHeader confidence="0.991665" genericHeader="method">
2 DALES System
</sectionHeader>
<bodyText confidence="0.998690666666667">
DALES system architecture is presented in Section 2.1, while its two main computational stages, one at
the object-of-interest level, the second one at the frame/video levels are described in Sections 2.2-2.3 and
Section 2.4, respectively.
</bodyText>
<subsectionHeader confidence="0.998207">
2.1 System Architecture
</subsectionHeader>
<bodyText confidence="0.999946090909091">
DALES software tool has been prototyped according to the Rapid Application Development (RAD)
methodology and using object-oriented approach (C++, 2011). The data flow diagrams (DFDs) shown
in Figs. 1 (a)-(b) display the flow of data within different stages of the system. DFDs give an account
of the type of data input, the processing involved with these data as well as the final data we get, each
higher level of DFDs elaborating the system further.
In order to start the computation of multi-view, multiple object-of-interest detection and counting,
target segmentation and labelling, multi-stream video annotations and script generation, DALES system
requires multi-view camera synchronization. This is achieved in the multiple views by checking the
correspondence of the file names of the files in all the views during the loading phase (Figs. 2(a), 3(a)).
On the other hand, DALES could be used as a viewer of annotated, multi-view videos by loading the
generated text script (Fig. 2(b)).
</bodyText>
<page confidence="0.998383">
88
</page>
<figureCaption confidence="0.973397666666667">
Figure 2: The initialisation of DALES software could be done either by providing (a) the video sequence
folder name, in order to use the software to process the multi-view video; or by providing (b) a text script
with video annotation, in order to run the software to visualize the frames and corresponding annotations.
</figureCaption>
<subsectionHeader confidence="0.995139">
2.2 Multiple-Object Detection, Segmentation, and Counting
</subsectionHeader>
<bodyText confidence="0.998080470588235">
A number of solutions exists for detecting multiple objects of interest in video scenes (Olszewska and
McCluskey, 2011; Olszewska, 2011; Olszewska, 2012b). In particular, background subtraction has long
been used in the literature as it provides a useful approach to both detect and segment objects of interest.
This method could be computed by difference between two consecutive frames (Archetti et al., 2006),
by subtracting the current frame from the background (Toyama et al., 1995; Haritaoglu et al., 2000), or
combining both frame difference and background subtraction techniques (Huang et al., 2007; Yao et al.,
2009).
In our scenario, images I(x,y) we consider may be taken from very different cameras, different light-
ing, etc. For that, we compute blobs separately in each of the views. The blobs are defined by labeled
connected regions, which are obtained by background subtraction. The latter technique consists in com-
puting the difference between the current image intensity I(x,y) and a background model, and afterwards,
in extracting the foreground.
To model the background, we adopt the running Gaussian average (RGA) (Wren et al., 1997), charac-
terized by the mean µb and the variance σ2b, rather than, for example, the Gaussian mixture model (GMM)
(Stauffer and Grimson, 1999; Friedman and Russell, 1997; Zivkovic and van der Heijden, 2004), since
the RGA method is much more suitable for real-time tracking.
Next, the foreground is determined by
</bodyText>
<equation confidence="0.997439666666667">
1 if |I (x, y) − µb  |&gt; n · σb, with n ∈ N0,
F (x, y) = (1)
0 otherwise.
</equation>
<bodyText confidence="0.999626">
Finally, morphological operations (Haralick, 1988) are applied to the extracted foreground F, in order
to exploit the existing information on the neighboring pixels,
</bodyText>
<equation confidence="0.721527">
f (x,y) = Morph(F(x,y)). (2)
</equation>
<subsectionHeader confidence="0.999627">
2.3 Object Labeling
</subsectionHeader>
<bodyText confidence="0.989943">
In this work, the detected and segmented object from a frame is automatically labelled by comparing
it with a given example object based on the Scale Invariant Feature Transform (SIFT) descriptors (Ol-
szewska, 2012a).
</bodyText>
<figure confidence="0.966043">
(a) (b)
</figure>
<page confidence="0.626846">
89
</page>
<table confidence="0.632041307692308">
Algorithm 1 Matching Algorithm
Given A&apos; = A,B&apos; = B,M = 0,
for all ai E A&apos; do
for all bj E B&apos; do
repeat
if
dP(ai,bj) = min dP(ai,b)
bEB&apos;
∧ dP(bj,ai) = min dP(bj,a)
aEA&apos;
∧ dP(ai,bj) G dH(A,B)
∧ dP(bj,ai) G dH(A,B)
then
</table>
<equation confidence="0.82915">
(ai,bj) C M
∧ A&apos; = A\{ai}∧B&apos; = B\{bj}
end if
until A&apos; =� 0 V B&apos; =� 0
</equation>
<listItem confidence="0.704219333333333">
end for
end for
return M
</listItem>
<bodyText confidence="0.999634">
The main steps of our labelling process are (i) the detection of object’s SIFT features which are robust
to rotation, translation, scale changes as well as some viewpoint variations, (ii) their matching by means
of the Embedded Double Matching Algorithm (Algorithm 1) and (iii) the label inheritance. Moreover,
our approach does not require any training and thus is online compatible.
The comparison between the query object and the candidate one is performed in the feature space in
order to be more computationally effective and to be more robust towards noise and affine transformation,
and is followed by the computation of an associated similarity measure dS(A,B), which is computed as
follows
</bodyText>
<equation confidence="0.93910075">
#M , (3)
dS(A,B) =
#A+#B
2
</equation>
<bodyText confidence="0.999468875">
with A and B, the sets of SIFT features of the query and the candidate objects, respectively, and M, the
set of the double-matched ones (Alqaisi et al., 2012).
The decision that a candidate object contains similar content to the query one is taken based on the fact
that the similarity measure dS(A,B) is above a given threshold. In the case when the similarity measure
dS(A,B) is below the given threshold, the candidate is rejected.
Finally, once the decision that a candidate object contains similar content to the query one has been
taken, the label of the candidate object is automatically mapped with the predefined label of the example
object.
</bodyText>
<subsectionHeader confidence="0.958632">
2.4 Multi-View Annotations
</subsectionHeader>
<bodyText confidence="0.999949857142857">
Hence, by using different objects’ examples, all the frames from the video dataset are indexed with
the relevant semantic labels based on their visual content similarity, while the objects of interest are
automatically labeled and localized within these frames (Olszewska, 2012a).
Unlike (Evans et al., 2013), which uses an early fusion, where all the cameras are used to make a
decision about detection and tracking of the objects of interest, DALES system performs a late fusion.
Indeed, in our system, objects of interest are detected and labelled in individual cameras independently.
Next, the results are combined on the majority voting principle based on the semantic consistency of
</bodyText>
<page confidence="0.996966">
90
</page>
<figureCaption confidence="0.761948">
Figure 3: Snapshots of DALES software windows, in the phase of: (a) loaded multi views; (b) detected
moving objects.
the labels in sequential frames and across multiple camera views, rather than exploring geometrical
correspondences of objects as in (Dai and Payandeh, 2013).
</figureCaption>
<sectionHeader confidence="0.994953" genericHeader="evaluation">
3 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.999223363636364">
To validate the DALES tool, we have applied our system on the standard dataset (PETS, 2001) consisting
of video-surveillance dynamic scene recorded by two PTZ cameras. This produces two videos. Each
contains 2688 frames, whose average resolution is of 576x768 pixels and which were captured in outdoor
environment. This database owns challenges of multi-view video stream, as well as quantity, pose,
motion, size, appearance and scale variations of the objects of interest, i.e. people and cars.
All the experiments have been run on a computer with Intel Core 2 Duo Pentium T9300, 2.5 GHz,
2Gb RAM, and using our DALES software implemented with C++ (C++, 2011).
Some examples of the results of our DALES system are presented in Fig. 3(b). These frames present
difficult situations such as poor foreground/background contrast or light reflection.
To assess the detection accuracy of DALES system, we adopt the standard criteria (Izadi and Saeedi,
2008) as follows:
</bodyText>
<equation confidence="0.8815105">
TP
detection rate (DR) = (4)
TP+FN,
FP
false detection rate (FAR) = FP+TP&apos;
(5)
</equation>
<bodyText confidence="0.986629">
with TP, true positive, FP, false positive, and FN, false negative.
The labelling accuracy of DALES system could be assessed using the following standard criterion:
with TN, true negative. accuracy = TP+TN (6)
TP+TN +FP+FN,
In Table 1, we have reported the average detection and false alarm rates of our DALES method against
the rates achieved by (Izadi and Saeedi, 2008), while in Table 2, we have displayed the average accuracy
of object-of-interest labelling of our DALES method against the rate obtained by (Athanasiadis et al.,
2007).
</bodyText>
<figure confidence="0.945562">
(a) (b)
</figure>
<page confidence="0.995952">
91
</page>
<tableCaption confidence="0.999752">
Table 1: Average detection rates of object-of-interests in video frames.
</tableCaption>
<table confidence="0.997230333333333">
(Izadi and Saeedi, 2008) DALES
average detection rate (DR) 91.3% 91.6%
average false alarm rate (FAR) 9.5% 4.9%
</table>
<tableCaption confidence="0.949669">
Table 2: Average accuracy of object-of-interest labelling in video frames.
</tableCaption>
<table confidence="0.9516525">
(Athanasiadis et al., 2007) DALES
average accuracy 85% 95%
</table>
<bodyText confidence="0.996303714285714">
From Tables 1-2, we can conclude that our DALES system provides reliable detection and counting
of objects of interest in multi-camera environment, and that the multiple-object labelling is very accurate
as well, outperforming state-of-the art techniques. DALES total precision to annotate multi-view videos
is therefore very high.
For all the dataset, the average computational speed of our DALES software is in the range of few
seconds, whereas the viewer function of DALES software takes only few milliseconds to process. Hence,
our developed system could be used in context of online scene analysis.
</bodyText>
<sectionHeader confidence="0.998648" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9997139">
Reliable, multi-view annotation of large amount of real-time visual data, such as surveillance videos or
sport event broadcasts, is a challenging topic we have copped with. For this purpose, we have developed
a new software tool called DALES which processes, in multi-camera environment, (i) multiple object-
of-interest detection and (ii) counting, (iii) target segmentation and (iv) labelling, (v) image annotations,
(vi) multi-stream video annotations and script generation. Moreover, our DALES software suits well as
a viewer to display a loaded script with the text annotations of the multi-camera video sequence and the
corresponding labelled multi-view images.
Our system shows excellent performance compared to the ones found in the literature, on one hand,
for multiple-target detection and segmentation and, on the other hand, for object labeling. Multi-stream
annotations with DALES are thus computationally efficient and accurate.
</bodyText>
<sectionHeader confidence="0.987527" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9860315625">
T. Alqaisi, D. Gledhill, and J. I. Olszewska. 2012. Embedded double matching of local descriptors for a fast
automatic recognition of real-world objects. In Proceedings of the IEEE International Conference on Image
Processing (ICIP’12), pages 2385–2388.
A. Alsuqayhi and J. I. Olszewska. 2013. Efficient optical character recognition system for automatic soccer
player’s identification. In Proceedings of the IAPR International Conference on Computer Analysis of Images
and Patterns Workshop (CAIP’13), pages 139–150.
F. Archetti, C. Manfredotti, V. Messina, and D. Sorrenti. 2006. Foreground-to-ghost discrimination in single-
difference pre-processing. In Proceedings of the International Conference on Advanced Concepts for Intelligent
Vision Systems, pages 23–30.
T. Athanasiadis, P. Mylonas, Y. Avrithis, and S. Kollias. 2007. Semantic image segmentation and object labeling.
IEEE Transactions on Circuits and Systems for Video Technology, 17(3):298–312.
L. Bai, S. Lao, G. J. F. Jones, and A. F. Smeaton. 2007. Video semantic content analysis based on ontology. In
Proceedings of the IEEE International Machine Vision and Image Processing Conference, pages 117–124.
J. Black, T. Ellis, and P. Rosin. 2002. Multi View Image Surveillance and Tracking. In Proceedings of the IEEE
Workshop on Motion and Video Computing, pages 169–174.
C++. 2011. C++ builder. Available online at: https://downloads.embarcadero.com/free/c builder.
</reference>
<page confidence="0.987381">
92
</page>
<note confidence="0.77498325">
J.-W. Choi and J.-H. Yoo. 2013. Real-time multi-person tracking in fixed surveillance camera environment. In
Proceedings of the IEEE International Conference on Consumer Electronics.
X. Dai and S. Payandeh. 2013. Geometry-based object association and consistent labeling in multi-camera surveil-
lance. IEEE Journal on Emerging and Selected Topics in Circuits and Systems, 3(2):175–184.
</note>
<reference confidence="0.995384674418604">
R. Diaz, S. Hallman, and C. C. Fowlkes. 2013. Detecting dynamic objects with multi-view background subtrac-
tion. In Proceedings of the IEEE International Conference on Computer Vision, pages 273–280.
M. Evans, C. J. Osborne, and J. Ferryman. 2013. Multicamera object detection and tracking with object size
estimation. In Proceedings of the IEEE International Conference on Advanced Video and Signal Based Surveil-
lance, pages 177–182.
N. Friedman and S. Russell. 1997. Image segmentation in video sequences: A probabilistic approach. In Pro-
ceedings of the 13th Conference on Uncertainty in AI.
S. Guler, J. M. Griffith, and I. A. Pushee. 2003. Tracking and handoff between multiple perspective camera views.
In Proceedings of the 32nd IEEE Workshop on Applied Imaginary Pattern Recognition, pages 275–281.
R. M. Haralick. 1988. Mathematical morphology and computer vision. In Proceedings of the IEEE Asilomar
Conference on Signals, Systems and Computers, volume 1, pages 468–479.
I. Haritaoglu, D. Harwood, and L. Davis. 2000. Real-time surveillance of people and their activities. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 77(8):809–830.
H.-H. Hsu, W.-M. Yang, and T. K. Shih. 2013. Multicamera object detection and tracking with object size
estimation. In Proceedings of the IEEE Conference Anthology, pages 1–4.
W. Huang, Z. Liu, and W. Pan. 2007. The precise recognition of moving object in complex background. In
Proceedings of 3rd IEEE International Conference on Natural Computation, volume 2, pages 246–252.
M. Izadi and P. Saeedi. 2008. Robust region-based background subtraction and shadow removing using colour
and gradient information. In Proceedings of the 19th IEEE International Conference on Pattern Recognition,
pages 1–5.
V. Kettnaker and R. Zabih. 1999. Bayesian multi-camera surveillance. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, volume 2, pages 1–5.
K. S. Kumar, S. Prasad, P. K. Saroj, and R. C. Tripathi. 2010. Multiple cameras using real-time object tracking for
surveillance and security system. In Proceedings of the IEEE International Conference on Emerging Trends in
Engineering and Technology, pages 213–218.
L. Lamard, R. Chapuis, and J.-P. Boyer. 2013. CPHD Filter addressing occlusions with pedestrians and vehicles
tracking. In Proceedings of the IEEE International Intelligent Vehicles Symposium, pages 1125–1130.
P. Natarajan and R. Nevatia. 2005. EDF: A framework for semantic annotation of video. In Proceedings of the
IEEE International Conference on Computer Vision Workshops, page 1876.
J. I. Olszewska and T. L. McCluskey. 2011. Ontology-coupled active contours for dynamic video scene un-
derstanding. In Proceedings of the IEEE International Conference on Intelligent Engineering Systems, pages
369–374.
J. I. Olszewska. 2011. Spatio-temporal visual ontology. In Proceedings of the 1st EPSRC Workshop on Vision
and Language (VL’2011).
J. I. Olszewska. 2012a. A new approach for automatic object labeling. In Proceedings of the 2nd EPSRC
Workshop on Vision and Language (VL’2012).
J. I. Olszewska. 2012b. Multi-target parametric active contours to support ontological domain representation. In
Proceedings of the RFIA Conference, pages 779–784.
PETS. 2001. PETSDataset. Available online at: ftp://ftp.pets.rdg.ac.uk/pub/PETS2001.
C. Stauffer and W. Grimson. 1999. Adaptive background mixture model for real-time tracking. In Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition.
C. Town. 2004. Ontology-driven Bayesian networks for dynamic scene understanding. In Proceedings of the
IEEE International Conference on Computer Vision and Pattern Recognition Workshops, page 116.
</reference>
<page confidence="0.983544">
93
</page>
<reference confidence="0.997353916666667">
K. Toyama, J. Krumm, B. Brumitt, and B. Meyers. 1995. Wallflower: Principles and practice of background
maintenance. In Proceedings of the IEEE International Conference on Computer Vision, volume 1, pages 255–
261.
B. Vrusias, D. Makris, J.-P. Renno, N. Newbold, K. Ahmad, and G. Jones. 2007. A framework for ontology
enriched semantic annotation of CCTV video. In Proceedings of the IEEE International Workshop on Image
Analysis for Multimedia Interactive Services, page 5.
C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland. 1997. Real-time tracking of the human body. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 19(7):780–785.
C. Yao, W. Li, and L. Gao. 2009. An efficient moving object detection algorithm using multi-mask. In Proceedings
of 6th IEEE International Conference on Fuzzy Systems and Knowledge Discovery, volume 5, pages 354–358.
Z. Zivkovic and F. van der Heijden. 2004. Recursive unsupervised learning of finite mixture models. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 26(5):651–656.
</reference>
<page confidence="0.999533">
94
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.457671">
<title confidence="0.9984">DALES: Automated Tool for Detection, Annotation, Labelling Segmentation of Multiple Objects in Multi-Camera Video Streams</title>
<author confidence="0.994595">M Bhat</author>
<author confidence="0.994595">J</author>
<affiliation confidence="0.9989115">University of School of Computing and</affiliation>
<address confidence="0.473296">The Park, Cheltenham, GL50 2RH,</address>
<email confidence="0.997498">jolszewska@glos.ac.uk</email>
<abstract confidence="0.9968711">In this paper, we propose a new software tool called DALES to extract semantic information from multi-view videos based on the analysis of their visual content. Our system is fully automatic and is well suited for multi-camera environment. Once the multi-view video sequences are loaded into DALES, our software performs the detection, counting, and segmentation of the visual objects evolving in the provided video streams. Then, these objects of interest are processed in order to be labelled, and the related frames are thus annotated with the corresponding semantic content. Moreover, a textual script is automatically generated with the video annotations. DALES system shows excellent performance in terms of accuracy and computational speed and is robustly designed to ensure view synchronization.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Alqaisi</author>
<author>D Gledhill</author>
<author>J I Olszewska</author>
</authors>
<title>Embedded double matching of local descriptors for a fast automatic recognition of real-world objects.</title>
<date>2012</date>
<booktitle>In Proceedings of the IEEE International Conference on Image Processing (ICIP’12),</booktitle>
<pages>2385--2388</pages>
<contexts>
<context position="10126" citStr="Alqaisi et al., 2012" startWordPosition="1560" endWordPosition="1563">m 1) and (iii) the label inheritance. Moreover, our approach does not require any training and thus is online compatible. The comparison between the query object and the candidate one is performed in the feature space in order to be more computationally effective and to be more robust towards noise and affine transformation, and is followed by the computation of an associated similarity measure dS(A,B), which is computed as follows #M , (3) dS(A,B) = #A+#B 2 with A and B, the sets of SIFT features of the query and the candidate objects, respectively, and M, the set of the double-matched ones (Alqaisi et al., 2012). The decision that a candidate object contains similar content to the query one is taken based on the fact that the similarity measure dS(A,B) is above a given threshold. In the case when the similarity measure dS(A,B) is below the given threshold, the candidate is rejected. Finally, once the decision that a candidate object contains similar content to the query one has been taken, the label of the candidate object is automatically mapped with the predefined label of the example object. 2.4 Multi-View Annotations Hence, by using different objects’ examples, all the frames from the video datas</context>
</contexts>
<marker>Alqaisi, Gledhill, Olszewska, 2012</marker>
<rawString>T. Alqaisi, D. Gledhill, and J. I. Olszewska. 2012. Embedded double matching of local descriptors for a fast automatic recognition of real-world objects. In Proceedings of the IEEE International Conference on Image Processing (ICIP’12), pages 2385–2388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Alsuqayhi</author>
<author>J I Olszewska</author>
</authors>
<title>Efficient optical character recognition system for automatic soccer player’s identification.</title>
<date>2013</date>
<booktitle>In Proceedings of the IAPR International Conference on Computer Analysis of Images and Patterns Workshop (CAIP’13),</booktitle>
<pages>139--150</pages>
<contexts>
<context position="1318" citStr="Alsuqayhi and Olszewska, 2013" startWordPosition="189" endWordPosition="192">vided video streams. Then, these objects of interest are processed in order to be labelled, and the related frames are thus annotated with the corresponding semantic content. Moreover, a textual script is automatically generated with the video annotations. DALES system shows excellent performance in terms of accuracy and computational speed and is robustly designed to ensure view synchronization. 1 Introduction With the increasing use of electronic equipments, storage devices and computational systems for applications such as video surveillance (Kumar et al., 2010) and sport event monitoring (Alsuqayhi and Olszewska, 2013), the development of automated tools to process the resulting big amount of visual data in order to extract meaningful information becomes a necessity. In particular, the design of multi-view video annotation systems is a challenging, new task. It aims to process multi-view video streams which consist of video sequences of a dynamic scene captured simultaneously by multiple cameras. Such multi-input system is dedicated to automatically analyse the visual content of the multi-camera records and to generate semantic and visual annotations, in the way to assist users in the understanding and reas</context>
</contexts>
<marker>Alsuqayhi, Olszewska, 2013</marker>
<rawString>A. Alsuqayhi and J. I. Olszewska. 2013. Efficient optical character recognition system for automatic soccer player’s identification. In Proceedings of the IAPR International Conference on Computer Analysis of Images and Patterns Workshop (CAIP’13), pages 139–150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Archetti</author>
<author>C Manfredotti</author>
<author>V Messina</author>
<author>D Sorrenti</author>
</authors>
<title>Foreground-to-ghost discrimination in singledifference pre-processing.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Advanced Concepts for Intelligent Vision Systems,</booktitle>
<pages>23--30</pages>
<contexts>
<context position="7362" citStr="Archetti et al., 2006" startWordPosition="1087" endWordPosition="1090">-view video; or by providing (b) a text script with video annotation, in order to run the software to visualize the frames and corresponding annotations. 2.2 Multiple-Object Detection, Segmentation, and Counting A number of solutions exists for detecting multiple objects of interest in video scenes (Olszewska and McCluskey, 2011; Olszewska, 2011; Olszewska, 2012b). In particular, background subtraction has long been used in the literature as it provides a useful approach to both detect and segment objects of interest. This method could be computed by difference between two consecutive frames (Archetti et al., 2006), by subtracting the current frame from the background (Toyama et al., 1995; Haritaoglu et al., 2000), or combining both frame difference and background subtraction techniques (Huang et al., 2007; Yao et al., 2009). In our scenario, images I(x,y) we consider may be taken from very different cameras, different lighting, etc. For that, we compute blobs separately in each of the views. The blobs are defined by labeled connected regions, which are obtained by background subtraction. The latter technique consists in computing the difference between the current image intensity I(x,y) and a backgroun</context>
</contexts>
<marker>Archetti, Manfredotti, Messina, Sorrenti, 2006</marker>
<rawString>F. Archetti, C. Manfredotti, V. Messina, and D. Sorrenti. 2006. Foreground-to-ghost discrimination in singledifference pre-processing. In Proceedings of the International Conference on Advanced Concepts for Intelligent Vision Systems, pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Athanasiadis</author>
<author>P Mylonas</author>
<author>Y Avrithis</author>
<author>S Kollias</author>
</authors>
<title>Semantic image segmentation and object labeling.</title>
<date>2007</date>
<booktitle>IEEE Transactions on Circuits and Systems for Video Technology,</booktitle>
<volume>17</volume>
<issue>3</issue>
<contexts>
<context position="13224" citStr="Athanasiadis et al., 2007" startWordPosition="2052" endWordPosition="2055">follows: TP detection rate (DR) = (4) TP+FN, FP false detection rate (FAR) = FP+TP&apos; (5) with TP, true positive, FP, false positive, and FN, false negative. The labelling accuracy of DALES system could be assessed using the following standard criterion: with TN, true negative. accuracy = TP+TN (6) TP+TN +FP+FN, In Table 1, we have reported the average detection and false alarm rates of our DALES method against the rates achieved by (Izadi and Saeedi, 2008), while in Table 2, we have displayed the average accuracy of object-of-interest labelling of our DALES method against the rate obtained by (Athanasiadis et al., 2007). (a) (b) 91 Table 1: Average detection rates of object-of-interests in video frames. (Izadi and Saeedi, 2008) DALES average detection rate (DR) 91.3% 91.6% average false alarm rate (FAR) 9.5% 4.9% Table 2: Average accuracy of object-of-interest labelling in video frames. (Athanasiadis et al., 2007) DALES average accuracy 85% 95% From Tables 1-2, we can conclude that our DALES system provides reliable detection and counting of objects of interest in multi-camera environment, and that the multiple-object labelling is very accurate as well, outperforming state-of-the art techniques. DALES total </context>
</contexts>
<marker>Athanasiadis, Mylonas, Avrithis, Kollias, 2007</marker>
<rawString>T. Athanasiadis, P. Mylonas, Y. Avrithis, and S. Kollias. 2007. Semantic image segmentation and object labeling. IEEE Transactions on Circuits and Systems for Video Technology, 17(3):298–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bai</author>
<author>S Lao</author>
<author>G J F Jones</author>
<author>A F Smeaton</author>
</authors>
<title>Video semantic content analysis based on ontology.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Machine Vision and Image Processing Conference,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="2700" citStr="Bai et al., 2007" startWordPosition="397" endWordPosition="400">ation, frame labelling, and video annotation. In the literature, most of the works dealing with the analysis of multi-camera video streams are focused on the sole task of tracking multiple, moving objects and use different approaches such as background subtraction (Diaz et al., 2013), Bayesian framework (Hsu et al., 2013), particle filter (Choi and Yoo, 2013), or Cardinalized Probability Hypothesis Density (CPHD) based filter (Lamard et al., 2013). On the other hand, research on video annotation has lead to the development of several efficient systems (Town, 2004; Natarajan and Nevatia, 2005; Bai et al., 2007; Vrusias et al., 2007), but all designed for a single camera video stream input. In this paper, we describe a full system which takes multi-camera video stream inputs and performs visual data processing to generate multi-view video annotations. Our system has been developed in context of outdoor video-surveillance and is an automatic Detection, Annotation, LabElling and Segmentation (DALES) software tool. It presents also the advantage to have an entire chain of data processing from visual to textual one, reducing thus the semantic gap. As camera calibration is in general an expensive process</context>
</contexts>
<marker>Bai, Lao, Jones, Smeaton, 2007</marker>
<rawString>L. Bai, S. Lao, G. J. F. Jones, and A. F. Smeaton. 2007. Video semantic content analysis based on ontology. In Proceedings of the IEEE International Machine Vision and Image Processing Conference, pages 117–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Black</author>
<author>T Ellis</author>
<author>P Rosin</author>
</authors>
<title>Multi View Image Surveillance and Tracking.</title>
<date>2002</date>
<booktitle>In Proceedings of the IEEE Workshop on Motion and Video Computing,</booktitle>
<pages>169--174</pages>
<contexts>
<context position="3321" citStr="Black et al., 2002" startWordPosition="494" endWordPosition="497">Vrusias et al., 2007), but all designed for a single camera video stream input. In this paper, we describe a full system which takes multi-camera video stream inputs and performs visual data processing to generate multi-view video annotations. Our system has been developed in context of outdoor video-surveillance and is an automatic Detection, Annotation, LabElling and Segmentation (DALES) software tool. It presents also the advantage to have an entire chain of data processing from visual to textual one, reducing thus the semantic gap. As camera calibration is in general an expensive process (Black et al., 2002) and in real life, surveillance application measurements of camera parameters are not readily available (Guler et al., 2003), DALES system does not involve any camera calibration parameters. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 87 Proceedings of the 25th International Conference on Computational Linguistics, pages 87–94, Dublin, Ireland, August 23-29 2014. (a) (b) Figure 1: Overview of the flow of data within DALES softwa</context>
</contexts>
<marker>Black, Ellis, Rosin, 2002</marker>
<rawString>J. Black, T. Ellis, and P. Rosin. 2002. Multi View Image Surveillance and Tracking. In Proceedings of the IEEE Workshop on Motion and Video Computing, pages 169–174. C++. 2011. C++ builder. Available online at: https://downloads.embarcadero.com/free/c builder.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Diaz</author>
<author>S Hallman</author>
<author>C C Fowlkes</author>
</authors>
<title>Detecting dynamic objects with multi-view background subtraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the IEEE International Conference on Computer Vision,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="2368" citStr="Diaz et al., 2013" startWordPosition="346" endWordPosition="349">tically analyse the visual content of the multi-camera records and to generate semantic and visual annotations, in the way to assist users in the understanding and reasoning about large amount of acquired data. For this purpose, data should be processed through different, major stages such as object-of-interest detection and segmentation, frame labelling, and video annotation. In the literature, most of the works dealing with the analysis of multi-camera video streams are focused on the sole task of tracking multiple, moving objects and use different approaches such as background subtraction (Diaz et al., 2013), Bayesian framework (Hsu et al., 2013), particle filter (Choi and Yoo, 2013), or Cardinalized Probability Hypothesis Density (CPHD) based filter (Lamard et al., 2013). On the other hand, research on video annotation has lead to the development of several efficient systems (Town, 2004; Natarajan and Nevatia, 2005; Bai et al., 2007; Vrusias et al., 2007), but all designed for a single camera video stream input. In this paper, we describe a full system which takes multi-camera video stream inputs and performs visual data processing to generate multi-view video annotations. Our system has been de</context>
</contexts>
<marker>Diaz, Hallman, Fowlkes, 2013</marker>
<rawString>R. Diaz, S. Hallman, and C. C. Fowlkes. 2013. Detecting dynamic objects with multi-view background subtraction. In Proceedings of the IEEE International Conference on Computer Vision, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Evans</author>
<author>C J Osborne</author>
<author>J Ferryman</author>
</authors>
<title>Multicamera object detection and tracking with object size estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the IEEE International Conference on Advanced Video and Signal Based Surveillance,</booktitle>
<pages>177--182</pages>
<contexts>
<context position="10954" citStr="Evans et al., 2013" startWordPosition="1690" endWordPosition="1693">re dS(A,B) is below the given threshold, the candidate is rejected. Finally, once the decision that a candidate object contains similar content to the query one has been taken, the label of the candidate object is automatically mapped with the predefined label of the example object. 2.4 Multi-View Annotations Hence, by using different objects’ examples, all the frames from the video dataset are indexed with the relevant semantic labels based on their visual content similarity, while the objects of interest are automatically labeled and localized within these frames (Olszewska, 2012a). Unlike (Evans et al., 2013), which uses an early fusion, where all the cameras are used to make a decision about detection and tracking of the objects of interest, DALES system performs a late fusion. Indeed, in our system, objects of interest are detected and labelled in individual cameras independently. Next, the results are combined on the majority voting principle based on the semantic consistency of 90 Figure 3: Snapshots of DALES software windows, in the phase of: (a) loaded multi views; (b) detected moving objects. the labels in sequential frames and across multiple camera views, rather than exploring geometrical</context>
</contexts>
<marker>Evans, Osborne, Ferryman, 2013</marker>
<rawString>M. Evans, C. J. Osborne, and J. Ferryman. 2013. Multicamera object detection and tracking with object size estimation. In Proceedings of the IEEE International Conference on Advanced Video and Signal Based Surveillance, pages 177–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Friedman</author>
<author>S Russell</author>
</authors>
<title>Image segmentation in video sequences: A probabilistic approach.</title>
<date>1997</date>
<booktitle>In Proceedings of the 13th Conference on Uncertainty in AI.</booktitle>
<contexts>
<context position="8272" citStr="Friedman and Russell, 1997" startWordPosition="1230" endWordPosition="1233">fferent cameras, different lighting, etc. For that, we compute blobs separately in each of the views. The blobs are defined by labeled connected regions, which are obtained by background subtraction. The latter technique consists in computing the difference between the current image intensity I(x,y) and a background model, and afterwards, in extracting the foreground. To model the background, we adopt the running Gaussian average (RGA) (Wren et al., 1997), characterized by the mean µb and the variance σ2b, rather than, for example, the Gaussian mixture model (GMM) (Stauffer and Grimson, 1999; Friedman and Russell, 1997; Zivkovic and van der Heijden, 2004), since the RGA method is much more suitable for real-time tracking. Next, the foreground is determined by 1 if |I (x, y) − µb |&gt; n · σb, with n ∈ N0, F (x, y) = (1) 0 otherwise. Finally, morphological operations (Haralick, 1988) are applied to the extracted foreground F, in order to exploit the existing information on the neighboring pixels, f (x,y) = Morph(F(x,y)). (2) 2.3 Object Labeling In this work, the detected and segmented object from a frame is automatically labelled by comparing it with a given example object based on the Scale Invariant Feature T</context>
</contexts>
<marker>Friedman, Russell, 1997</marker>
<rawString>N. Friedman and S. Russell. 1997. Image segmentation in video sequences: A probabilistic approach. In Proceedings of the 13th Conference on Uncertainty in AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Guler</author>
<author>J M Griffith</author>
<author>I A Pushee</author>
</authors>
<title>Tracking and handoff between multiple perspective camera views.</title>
<date>2003</date>
<booktitle>In Proceedings of the 32nd IEEE Workshop on Applied Imaginary Pattern Recognition,</booktitle>
<pages>275--281</pages>
<contexts>
<context position="3445" citStr="Guler et al., 2003" startWordPosition="513" endWordPosition="516">ch takes multi-camera video stream inputs and performs visual data processing to generate multi-view video annotations. Our system has been developed in context of outdoor video-surveillance and is an automatic Detection, Annotation, LabElling and Segmentation (DALES) software tool. It presents also the advantage to have an entire chain of data processing from visual to textual one, reducing thus the semantic gap. As camera calibration is in general an expensive process (Black et al., 2002) and in real life, surveillance application measurements of camera parameters are not readily available (Guler et al., 2003), DALES system does not involve any camera calibration parameters. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 87 Proceedings of the 25th International Conference on Computational Linguistics, pages 87–94, Dublin, Ireland, August 23-29 2014. (a) (b) Figure 1: Overview of the flow of data within DALES software: (a) Context level Data Flow Diagram; (b) Second level Data Flow Diagram. DALES provides not only the automatic annotatio</context>
</contexts>
<marker>Guler, Griffith, Pushee, 2003</marker>
<rawString>S. Guler, J. M. Griffith, and I. A. Pushee. 2003. Tracking and handoff between multiple perspective camera views. In Proceedings of the 32nd IEEE Workshop on Applied Imaginary Pattern Recognition, pages 275–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Haralick</author>
</authors>
<title>Mathematical morphology and computer vision.</title>
<date>1988</date>
<booktitle>In Proceedings of the IEEE Asilomar Conference on Signals, Systems and Computers,</booktitle>
<volume>1</volume>
<pages>468--479</pages>
<contexts>
<context position="8538" citStr="Haralick, 1988" startWordPosition="1282" endWordPosition="1283"> image intensity I(x,y) and a background model, and afterwards, in extracting the foreground. To model the background, we adopt the running Gaussian average (RGA) (Wren et al., 1997), characterized by the mean µb and the variance σ2b, rather than, for example, the Gaussian mixture model (GMM) (Stauffer and Grimson, 1999; Friedman and Russell, 1997; Zivkovic and van der Heijden, 2004), since the RGA method is much more suitable for real-time tracking. Next, the foreground is determined by 1 if |I (x, y) − µb |&gt; n · σb, with n ∈ N0, F (x, y) = (1) 0 otherwise. Finally, morphological operations (Haralick, 1988) are applied to the extracted foreground F, in order to exploit the existing information on the neighboring pixels, f (x,y) = Morph(F(x,y)). (2) 2.3 Object Labeling In this work, the detected and segmented object from a frame is automatically labelled by comparing it with a given example object based on the Scale Invariant Feature Transform (SIFT) descriptors (Olszewska, 2012a). (a) (b) 89 Algorithm 1 Matching Algorithm Given A&apos; = A,B&apos; = B,M = 0, for all ai E A&apos; do for all bj E B&apos; do repeat if dP(ai,bj) = min dP(ai,b) bEB&apos; ∧ dP(bj,ai) = min dP(bj,a) aEA&apos; ∧ dP(ai,bj) G dH(A,B) ∧ dP(bj,ai) G dH(</context>
</contexts>
<marker>Haralick, 1988</marker>
<rawString>R. M. Haralick. 1988. Mathematical morphology and computer vision. In Proceedings of the IEEE Asilomar Conference on Signals, Systems and Computers, volume 1, pages 468–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Haritaoglu</author>
<author>D Harwood</author>
<author>L Davis</author>
</authors>
<title>Real-time surveillance of people and their activities.</title>
<date>2000</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>77</volume>
<issue>8</issue>
<contexts>
<context position="7463" citStr="Haritaoglu et al., 2000" startWordPosition="1103" endWordPosition="1106">to visualize the frames and corresponding annotations. 2.2 Multiple-Object Detection, Segmentation, and Counting A number of solutions exists for detecting multiple objects of interest in video scenes (Olszewska and McCluskey, 2011; Olszewska, 2011; Olszewska, 2012b). In particular, background subtraction has long been used in the literature as it provides a useful approach to both detect and segment objects of interest. This method could be computed by difference between two consecutive frames (Archetti et al., 2006), by subtracting the current frame from the background (Toyama et al., 1995; Haritaoglu et al., 2000), or combining both frame difference and background subtraction techniques (Huang et al., 2007; Yao et al., 2009). In our scenario, images I(x,y) we consider may be taken from very different cameras, different lighting, etc. For that, we compute blobs separately in each of the views. The blobs are defined by labeled connected regions, which are obtained by background subtraction. The latter technique consists in computing the difference between the current image intensity I(x,y) and a background model, and afterwards, in extracting the foreground. To model the background, we adopt the running </context>
</contexts>
<marker>Haritaoglu, Harwood, Davis, 2000</marker>
<rawString>I. Haritaoglu, D. Harwood, and L. Davis. 2000. Real-time surveillance of people and their activities. IEEE Transactions on Pattern Analysis and Machine Intelligence, 77(8):809–830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-H Hsu</author>
<author>W-M Yang</author>
<author>T K Shih</author>
</authors>
<title>Multicamera object detection and tracking with object size estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the IEEE Conference Anthology,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="2407" citStr="Hsu et al., 2013" startWordPosition="352" endWordPosition="355">e multi-camera records and to generate semantic and visual annotations, in the way to assist users in the understanding and reasoning about large amount of acquired data. For this purpose, data should be processed through different, major stages such as object-of-interest detection and segmentation, frame labelling, and video annotation. In the literature, most of the works dealing with the analysis of multi-camera video streams are focused on the sole task of tracking multiple, moving objects and use different approaches such as background subtraction (Diaz et al., 2013), Bayesian framework (Hsu et al., 2013), particle filter (Choi and Yoo, 2013), or Cardinalized Probability Hypothesis Density (CPHD) based filter (Lamard et al., 2013). On the other hand, research on video annotation has lead to the development of several efficient systems (Town, 2004; Natarajan and Nevatia, 2005; Bai et al., 2007; Vrusias et al., 2007), but all designed for a single camera video stream input. In this paper, we describe a full system which takes multi-camera video stream inputs and performs visual data processing to generate multi-view video annotations. Our system has been developed in context of outdoor video-sur</context>
</contexts>
<marker>Hsu, Yang, Shih, 2013</marker>
<rawString>H.-H. Hsu, W.-M. Yang, and T. K. Shih. 2013. Multicamera object detection and tracking with object size estimation. In Proceedings of the IEEE Conference Anthology, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Huang</author>
<author>Z Liu</author>
<author>W Pan</author>
</authors>
<title>The precise recognition of moving object in complex background.</title>
<date>2007</date>
<booktitle>In Proceedings of 3rd IEEE International Conference on Natural Computation,</booktitle>
<volume>2</volume>
<pages>246--252</pages>
<contexts>
<context position="7557" citStr="Huang et al., 2007" startWordPosition="1116" endWordPosition="1119">and Counting A number of solutions exists for detecting multiple objects of interest in video scenes (Olszewska and McCluskey, 2011; Olszewska, 2011; Olszewska, 2012b). In particular, background subtraction has long been used in the literature as it provides a useful approach to both detect and segment objects of interest. This method could be computed by difference between two consecutive frames (Archetti et al., 2006), by subtracting the current frame from the background (Toyama et al., 1995; Haritaoglu et al., 2000), or combining both frame difference and background subtraction techniques (Huang et al., 2007; Yao et al., 2009). In our scenario, images I(x,y) we consider may be taken from very different cameras, different lighting, etc. For that, we compute blobs separately in each of the views. The blobs are defined by labeled connected regions, which are obtained by background subtraction. The latter technique consists in computing the difference between the current image intensity I(x,y) and a background model, and afterwards, in extracting the foreground. To model the background, we adopt the running Gaussian average (RGA) (Wren et al., 1997), characterized by the mean µb and the variance σ2b,</context>
</contexts>
<marker>Huang, Liu, Pan, 2007</marker>
<rawString>W. Huang, Z. Liu, and W. Pan. 2007. The precise recognition of moving object in complex background. In Proceedings of 3rd IEEE International Conference on Natural Computation, volume 2, pages 246–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Izadi</author>
<author>P Saeedi</author>
</authors>
<title>Robust region-based background subtraction and shadow removing using colour and gradient information.</title>
<date>2008</date>
<booktitle>In Proceedings of the 19th IEEE International Conference on Pattern Recognition,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="12594" citStr="Izadi and Saeedi, 2008" startWordPosition="1948" endWordPosition="1951"> owns challenges of multi-view video stream, as well as quantity, pose, motion, size, appearance and scale variations of the objects of interest, i.e. people and cars. All the experiments have been run on a computer with Intel Core 2 Duo Pentium T9300, 2.5 GHz, 2Gb RAM, and using our DALES software implemented with C++ (C++, 2011). Some examples of the results of our DALES system are presented in Fig. 3(b). These frames present difficult situations such as poor foreground/background contrast or light reflection. To assess the detection accuracy of DALES system, we adopt the standard criteria (Izadi and Saeedi, 2008) as follows: TP detection rate (DR) = (4) TP+FN, FP false detection rate (FAR) = FP+TP&apos; (5) with TP, true positive, FP, false positive, and FN, false negative. The labelling accuracy of DALES system could be assessed using the following standard criterion: with TN, true negative. accuracy = TP+TN (6) TP+TN +FP+FN, In Table 1, we have reported the average detection and false alarm rates of our DALES method against the rates achieved by (Izadi and Saeedi, 2008), while in Table 2, we have displayed the average accuracy of object-of-interest labelling of our DALES method against the rate obtained </context>
</contexts>
<marker>Izadi, Saeedi, 2008</marker>
<rawString>M. Izadi and P. Saeedi. 2008. Robust region-based background subtraction and shadow removing using colour and gradient information. In Proceedings of the 19th IEEE International Conference on Pattern Recognition, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Kettnaker</author>
<author>R Zabih</author>
</authors>
<title>Bayesian multi-camera surveillance.</title>
<date>1999</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<volume>2</volume>
<pages>1--5</pages>
<contexts>
<context position="4422" citStr="Kettnaker and Zabih, 1999" startWordPosition="648" endWordPosition="651">guistics, pages 87–94, Dublin, Ireland, August 23-29 2014. (a) (b) Figure 1: Overview of the flow of data within DALES software: (a) Context level Data Flow Diagram; (b) Second level Data Flow Diagram. DALES provides not only the automatic annotation of multi-view video streams, but also performs, in multi-camera environment, tasks such as multiple object-of-interest detection and segmentation, target counting and labelling, and image annotations. Our approach could cope with any dynamic scene recorded by pan-tilt-zoom (PTZ) static cameras with overlapping color views, unlike systems such as (Kettnaker and Zabih, 1999) based on non-overlapping cameras. The acquired multi-view sequences could contain complex backgrounds, moving objects or noisy foregrounds, and present illumination variations or poor resolution. Hence, the contribution of this paper is twofold: • the automated, textual annotation of multi-camera video streams based on visual features; • the development of a full, automatic system covering all the phases from multiple, visual multimotion target detection to multi-view video annotation and text-script generation. The paper is structured as follows. In Section 2, we describe our DALES system fo</context>
</contexts>
<marker>Kettnaker, Zabih, 1999</marker>
<rawString>V. Kettnaker and R. Zabih. 1999. Bayesian multi-camera surveillance. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, volume 2, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Kumar</author>
<author>S Prasad</author>
<author>P K Saroj</author>
<author>R C Tripathi</author>
</authors>
<title>Multiple cameras using real-time object tracking for surveillance and security system.</title>
<date>2010</date>
<booktitle>In Proceedings of the IEEE International Conference on Emerging Trends in Engineering and Technology,</booktitle>
<pages>213--218</pages>
<contexts>
<context position="1259" citStr="Kumar et al., 2010" startWordPosition="181" endWordPosition="184">tation of the visual objects evolving in the provided video streams. Then, these objects of interest are processed in order to be labelled, and the related frames are thus annotated with the corresponding semantic content. Moreover, a textual script is automatically generated with the video annotations. DALES system shows excellent performance in terms of accuracy and computational speed and is robustly designed to ensure view synchronization. 1 Introduction With the increasing use of electronic equipments, storage devices and computational systems for applications such as video surveillance (Kumar et al., 2010) and sport event monitoring (Alsuqayhi and Olszewska, 2013), the development of automated tools to process the resulting big amount of visual data in order to extract meaningful information becomes a necessity. In particular, the design of multi-view video annotation systems is a challenging, new task. It aims to process multi-view video streams which consist of video sequences of a dynamic scene captured simultaneously by multiple cameras. Such multi-input system is dedicated to automatically analyse the visual content of the multi-camera records and to generate semantic and visual annotation</context>
</contexts>
<marker>Kumar, Prasad, Saroj, Tripathi, 2010</marker>
<rawString>K. S. Kumar, S. Prasad, P. K. Saroj, and R. C. Tripathi. 2010. Multiple cameras using real-time object tracking for surveillance and security system. In Proceedings of the IEEE International Conference on Emerging Trends in Engineering and Technology, pages 213–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamard</author>
<author>R Chapuis</author>
<author>J-P Boyer</author>
</authors>
<title>CPHD Filter addressing occlusions with pedestrians and vehicles tracking.</title>
<date>2013</date>
<booktitle>In Proceedings of the IEEE International Intelligent Vehicles Symposium,</booktitle>
<pages>1125--1130</pages>
<contexts>
<context position="2535" citStr="Lamard et al., 2013" startWordPosition="370" endWordPosition="373">easoning about large amount of acquired data. For this purpose, data should be processed through different, major stages such as object-of-interest detection and segmentation, frame labelling, and video annotation. In the literature, most of the works dealing with the analysis of multi-camera video streams are focused on the sole task of tracking multiple, moving objects and use different approaches such as background subtraction (Diaz et al., 2013), Bayesian framework (Hsu et al., 2013), particle filter (Choi and Yoo, 2013), or Cardinalized Probability Hypothesis Density (CPHD) based filter (Lamard et al., 2013). On the other hand, research on video annotation has lead to the development of several efficient systems (Town, 2004; Natarajan and Nevatia, 2005; Bai et al., 2007; Vrusias et al., 2007), but all designed for a single camera video stream input. In this paper, we describe a full system which takes multi-camera video stream inputs and performs visual data processing to generate multi-view video annotations. Our system has been developed in context of outdoor video-surveillance and is an automatic Detection, Annotation, LabElling and Segmentation (DALES) software tool. It presents also the adva</context>
</contexts>
<marker>Lamard, Chapuis, Boyer, 2013</marker>
<rawString>L. Lamard, R. Chapuis, and J.-P. Boyer. 2013. CPHD Filter addressing occlusions with pedestrians and vehicles tracking. In Proceedings of the IEEE International Intelligent Vehicles Symposium, pages 1125–1130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Natarajan</author>
<author>R Nevatia</author>
</authors>
<title>EDF: A framework for semantic annotation of video.</title>
<date>2005</date>
<booktitle>In Proceedings of the IEEE International Conference on Computer Vision Workshops,</booktitle>
<pages>1876</pages>
<contexts>
<context position="2682" citStr="Natarajan and Nevatia, 2005" startWordPosition="393" endWordPosition="396">nterest detection and segmentation, frame labelling, and video annotation. In the literature, most of the works dealing with the analysis of multi-camera video streams are focused on the sole task of tracking multiple, moving objects and use different approaches such as background subtraction (Diaz et al., 2013), Bayesian framework (Hsu et al., 2013), particle filter (Choi and Yoo, 2013), or Cardinalized Probability Hypothesis Density (CPHD) based filter (Lamard et al., 2013). On the other hand, research on video annotation has lead to the development of several efficient systems (Town, 2004; Natarajan and Nevatia, 2005; Bai et al., 2007; Vrusias et al., 2007), but all designed for a single camera video stream input. In this paper, we describe a full system which takes multi-camera video stream inputs and performs visual data processing to generate multi-view video annotations. Our system has been developed in context of outdoor video-surveillance and is an automatic Detection, Annotation, LabElling and Segmentation (DALES) software tool. It presents also the advantage to have an entire chain of data processing from visual to textual one, reducing thus the semantic gap. As camera calibration is in general an</context>
</contexts>
<marker>Natarajan, Nevatia, 2005</marker>
<rawString>P. Natarajan and R. Nevatia. 2005. EDF: A framework for semantic annotation of video. In Proceedings of the IEEE International Conference on Computer Vision Workshops, page 1876.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J I Olszewska</author>
<author>T L McCluskey</author>
</authors>
<title>Ontology-coupled active contours for dynamic video scene understanding.</title>
<date>2011</date>
<booktitle>In Proceedings of the IEEE International Conference on Intelligent Engineering Systems,</booktitle>
<pages>369--374</pages>
<contexts>
<context position="7070" citStr="Olszewska and McCluskey, 2011" startWordPosition="1043" endWordPosition="1046">the other hand, DALES could be used as a viewer of annotated, multi-view videos by loading the generated text script (Fig. 2(b)). 88 Figure 2: The initialisation of DALES software could be done either by providing (a) the video sequence folder name, in order to use the software to process the multi-view video; or by providing (b) a text script with video annotation, in order to run the software to visualize the frames and corresponding annotations. 2.2 Multiple-Object Detection, Segmentation, and Counting A number of solutions exists for detecting multiple objects of interest in video scenes (Olszewska and McCluskey, 2011; Olszewska, 2011; Olszewska, 2012b). In particular, background subtraction has long been used in the literature as it provides a useful approach to both detect and segment objects of interest. This method could be computed by difference between two consecutive frames (Archetti et al., 2006), by subtracting the current frame from the background (Toyama et al., 1995; Haritaoglu et al., 2000), or combining both frame difference and background subtraction techniques (Huang et al., 2007; Yao et al., 2009). In our scenario, images I(x,y) we consider may be taken from very different cameras, differe</context>
</contexts>
<marker>Olszewska, McCluskey, 2011</marker>
<rawString>J. I. Olszewska and T. L. McCluskey. 2011. Ontology-coupled active contours for dynamic video scene understanding. In Proceedings of the IEEE International Conference on Intelligent Engineering Systems, pages 369–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J I Olszewska</author>
</authors>
<title>Spatio-temporal visual ontology.</title>
<date>2011</date>
<booktitle>In Proceedings of the 1st EPSRC Workshop on Vision and Language (VL’2011).</booktitle>
<contexts>
<context position="7087" citStr="Olszewska, 2011" startWordPosition="1047" endWordPosition="1048">used as a viewer of annotated, multi-view videos by loading the generated text script (Fig. 2(b)). 88 Figure 2: The initialisation of DALES software could be done either by providing (a) the video sequence folder name, in order to use the software to process the multi-view video; or by providing (b) a text script with video annotation, in order to run the software to visualize the frames and corresponding annotations. 2.2 Multiple-Object Detection, Segmentation, and Counting A number of solutions exists for detecting multiple objects of interest in video scenes (Olszewska and McCluskey, 2011; Olszewska, 2011; Olszewska, 2012b). In particular, background subtraction has long been used in the literature as it provides a useful approach to both detect and segment objects of interest. This method could be computed by difference between two consecutive frames (Archetti et al., 2006), by subtracting the current frame from the background (Toyama et al., 1995; Haritaoglu et al., 2000), or combining both frame difference and background subtraction techniques (Huang et al., 2007; Yao et al., 2009). In our scenario, images I(x,y) we consider may be taken from very different cameras, different lighting, etc.</context>
</contexts>
<marker>Olszewska, 2011</marker>
<rawString>J. I. Olszewska. 2011. Spatio-temporal visual ontology. In Proceedings of the 1st EPSRC Workshop on Vision and Language (VL’2011).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J I Olszewska</author>
</authors>
<title>A new approach for automatic object labeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2nd EPSRC Workshop on Vision and Language (VL’2012).</booktitle>
<contexts>
<context position="7104" citStr="Olszewska, 2012" startWordPosition="1049" endWordPosition="1050">of annotated, multi-view videos by loading the generated text script (Fig. 2(b)). 88 Figure 2: The initialisation of DALES software could be done either by providing (a) the video sequence folder name, in order to use the software to process the multi-view video; or by providing (b) a text script with video annotation, in order to run the software to visualize the frames and corresponding annotations. 2.2 Multiple-Object Detection, Segmentation, and Counting A number of solutions exists for detecting multiple objects of interest in video scenes (Olszewska and McCluskey, 2011; Olszewska, 2011; Olszewska, 2012b). In particular, background subtraction has long been used in the literature as it provides a useful approach to both detect and segment objects of interest. This method could be computed by difference between two consecutive frames (Archetti et al., 2006), by subtracting the current frame from the background (Toyama et al., 1995; Haritaoglu et al., 2000), or combining both frame difference and background subtraction techniques (Huang et al., 2007; Yao et al., 2009). In our scenario, images I(x,y) we consider may be taken from very different cameras, different lighting, etc. For that, we com</context>
<context position="8916" citStr="Olszewska, 2012" startWordPosition="1341" endWordPosition="1343">n, 2004), since the RGA method is much more suitable for real-time tracking. Next, the foreground is determined by 1 if |I (x, y) − µb |&gt; n · σb, with n ∈ N0, F (x, y) = (1) 0 otherwise. Finally, morphological operations (Haralick, 1988) are applied to the extracted foreground F, in order to exploit the existing information on the neighboring pixels, f (x,y) = Morph(F(x,y)). (2) 2.3 Object Labeling In this work, the detected and segmented object from a frame is automatically labelled by comparing it with a given example object based on the Scale Invariant Feature Transform (SIFT) descriptors (Olszewska, 2012a). (a) (b) 89 Algorithm 1 Matching Algorithm Given A&apos; = A,B&apos; = B,M = 0, for all ai E A&apos; do for all bj E B&apos; do repeat if dP(ai,bj) = min dP(ai,b) bEB&apos; ∧ dP(bj,ai) = min dP(bj,a) aEA&apos; ∧ dP(ai,bj) G dH(A,B) ∧ dP(bj,ai) G dH(A,B) then (ai,bj) C M ∧ A&apos; = A\{ai}∧B&apos; = B\{bj} end if until A&apos; =� 0 V B&apos; =� 0 end for end for return M The main steps of our labelling process are (i) the detection of object’s SIFT features which are robust to rotation, translation, scale changes as well as some viewpoint variations, (ii) their matching by means of the Embedded Double Matching Algorithm (Algorithm 1) and (i</context>
<context position="10923" citStr="Olszewska, 2012" startWordPosition="1687" endWordPosition="1688">e when the similarity measure dS(A,B) is below the given threshold, the candidate is rejected. Finally, once the decision that a candidate object contains similar content to the query one has been taken, the label of the candidate object is automatically mapped with the predefined label of the example object. 2.4 Multi-View Annotations Hence, by using different objects’ examples, all the frames from the video dataset are indexed with the relevant semantic labels based on their visual content similarity, while the objects of interest are automatically labeled and localized within these frames (Olszewska, 2012a). Unlike (Evans et al., 2013), which uses an early fusion, where all the cameras are used to make a decision about detection and tracking of the objects of interest, DALES system performs a late fusion. Indeed, in our system, objects of interest are detected and labelled in individual cameras independently. Next, the results are combined on the majority voting principle based on the semantic consistency of 90 Figure 3: Snapshots of DALES software windows, in the phase of: (a) loaded multi views; (b) detected moving objects. the labels in sequential frames and across multiple camera views, ra</context>
</contexts>
<marker>Olszewska, 2012</marker>
<rawString>J. I. Olszewska. 2012a. A new approach for automatic object labeling. In Proceedings of the 2nd EPSRC Workshop on Vision and Language (VL’2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J I Olszewska</author>
</authors>
<title>Multi-target parametric active contours to support ontological domain representation.</title>
<date>2012</date>
<booktitle>In Proceedings of the RFIA Conference,</booktitle>
<pages>779--784</pages>
<note>PETSDataset. Available online at: ftp://ftp.pets.rdg.ac.uk/pub/PETS2001.</note>
<contexts>
<context position="7104" citStr="Olszewska, 2012" startWordPosition="1049" endWordPosition="1050">of annotated, multi-view videos by loading the generated text script (Fig. 2(b)). 88 Figure 2: The initialisation of DALES software could be done either by providing (a) the video sequence folder name, in order to use the software to process the multi-view video; or by providing (b) a text script with video annotation, in order to run the software to visualize the frames and corresponding annotations. 2.2 Multiple-Object Detection, Segmentation, and Counting A number of solutions exists for detecting multiple objects of interest in video scenes (Olszewska and McCluskey, 2011; Olszewska, 2011; Olszewska, 2012b). In particular, background subtraction has long been used in the literature as it provides a useful approach to both detect and segment objects of interest. This method could be computed by difference between two consecutive frames (Archetti et al., 2006), by subtracting the current frame from the background (Toyama et al., 1995; Haritaoglu et al., 2000), or combining both frame difference and background subtraction techniques (Huang et al., 2007; Yao et al., 2009). In our scenario, images I(x,y) we consider may be taken from very different cameras, different lighting, etc. For that, we com</context>
<context position="8916" citStr="Olszewska, 2012" startWordPosition="1341" endWordPosition="1343">n, 2004), since the RGA method is much more suitable for real-time tracking. Next, the foreground is determined by 1 if |I (x, y) − µb |&gt; n · σb, with n ∈ N0, F (x, y) = (1) 0 otherwise. Finally, morphological operations (Haralick, 1988) are applied to the extracted foreground F, in order to exploit the existing information on the neighboring pixels, f (x,y) = Morph(F(x,y)). (2) 2.3 Object Labeling In this work, the detected and segmented object from a frame is automatically labelled by comparing it with a given example object based on the Scale Invariant Feature Transform (SIFT) descriptors (Olszewska, 2012a). (a) (b) 89 Algorithm 1 Matching Algorithm Given A&apos; = A,B&apos; = B,M = 0, for all ai E A&apos; do for all bj E B&apos; do repeat if dP(ai,bj) = min dP(ai,b) bEB&apos; ∧ dP(bj,ai) = min dP(bj,a) aEA&apos; ∧ dP(ai,bj) G dH(A,B) ∧ dP(bj,ai) G dH(A,B) then (ai,bj) C M ∧ A&apos; = A\{ai}∧B&apos; = B\{bj} end if until A&apos; =� 0 V B&apos; =� 0 end for end for return M The main steps of our labelling process are (i) the detection of object’s SIFT features which are robust to rotation, translation, scale changes as well as some viewpoint variations, (ii) their matching by means of the Embedded Double Matching Algorithm (Algorithm 1) and (i</context>
<context position="10923" citStr="Olszewska, 2012" startWordPosition="1687" endWordPosition="1688">e when the similarity measure dS(A,B) is below the given threshold, the candidate is rejected. Finally, once the decision that a candidate object contains similar content to the query one has been taken, the label of the candidate object is automatically mapped with the predefined label of the example object. 2.4 Multi-View Annotations Hence, by using different objects’ examples, all the frames from the video dataset are indexed with the relevant semantic labels based on their visual content similarity, while the objects of interest are automatically labeled and localized within these frames (Olszewska, 2012a). Unlike (Evans et al., 2013), which uses an early fusion, where all the cameras are used to make a decision about detection and tracking of the objects of interest, DALES system performs a late fusion. Indeed, in our system, objects of interest are detected and labelled in individual cameras independently. Next, the results are combined on the majority voting principle based on the semantic consistency of 90 Figure 3: Snapshots of DALES software windows, in the phase of: (a) loaded multi views; (b) detected moving objects. the labels in sequential frames and across multiple camera views, ra</context>
</contexts>
<marker>Olszewska, 2012</marker>
<rawString>J. I. Olszewska. 2012b. Multi-target parametric active contours to support ontological domain representation. In Proceedings of the RFIA Conference, pages 779–784. PETS. 2001. PETSDataset. Available online at: ftp://ftp.pets.rdg.ac.uk/pub/PETS2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stauffer</author>
<author>W Grimson</author>
</authors>
<title>Adaptive background mixture model for real-time tracking.</title>
<date>1999</date>
<booktitle>In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</booktitle>
<contexts>
<context position="8244" citStr="Stauffer and Grimson, 1999" startWordPosition="1226" endWordPosition="1229">er may be taken from very different cameras, different lighting, etc. For that, we compute blobs separately in each of the views. The blobs are defined by labeled connected regions, which are obtained by background subtraction. The latter technique consists in computing the difference between the current image intensity I(x,y) and a background model, and afterwards, in extracting the foreground. To model the background, we adopt the running Gaussian average (RGA) (Wren et al., 1997), characterized by the mean µb and the variance σ2b, rather than, for example, the Gaussian mixture model (GMM) (Stauffer and Grimson, 1999; Friedman and Russell, 1997; Zivkovic and van der Heijden, 2004), since the RGA method is much more suitable for real-time tracking. Next, the foreground is determined by 1 if |I (x, y) − µb |&gt; n · σb, with n ∈ N0, F (x, y) = (1) 0 otherwise. Finally, morphological operations (Haralick, 1988) are applied to the extracted foreground F, in order to exploit the existing information on the neighboring pixels, f (x,y) = Morph(F(x,y)). (2) 2.3 Object Labeling In this work, the detected and segmented object from a frame is automatically labelled by comparing it with a given example object based on t</context>
</contexts>
<marker>Stauffer, Grimson, 1999</marker>
<rawString>C. Stauffer and W. Grimson. 1999. Adaptive background mixture model for real-time tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Town</author>
</authors>
<title>Ontology-driven Bayesian networks for dynamic scene understanding.</title>
<date>2004</date>
<booktitle>In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition Workshops,</booktitle>
<pages>116</pages>
<contexts>
<context position="2653" citStr="Town, 2004" startWordPosition="391" endWordPosition="392"> object-of-interest detection and segmentation, frame labelling, and video annotation. In the literature, most of the works dealing with the analysis of multi-camera video streams are focused on the sole task of tracking multiple, moving objects and use different approaches such as background subtraction (Diaz et al., 2013), Bayesian framework (Hsu et al., 2013), particle filter (Choi and Yoo, 2013), or Cardinalized Probability Hypothesis Density (CPHD) based filter (Lamard et al., 2013). On the other hand, research on video annotation has lead to the development of several efficient systems (Town, 2004; Natarajan and Nevatia, 2005; Bai et al., 2007; Vrusias et al., 2007), but all designed for a single camera video stream input. In this paper, we describe a full system which takes multi-camera video stream inputs and performs visual data processing to generate multi-view video annotations. Our system has been developed in context of outdoor video-surveillance and is an automatic Detection, Annotation, LabElling and Segmentation (DALES) software tool. It presents also the advantage to have an entire chain of data processing from visual to textual one, reducing thus the semantic gap. As camera</context>
</contexts>
<marker>Town, 2004</marker>
<rawString>C. Town. 2004. Ontology-driven Bayesian networks for dynamic scene understanding. In Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition Workshops, page 116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Toyama</author>
<author>J Krumm</author>
<author>B Brumitt</author>
<author>B Meyers</author>
</authors>
<title>Wallflower: Principles and practice of background maintenance.</title>
<date>1995</date>
<booktitle>In Proceedings of the IEEE International Conference on Computer Vision,</booktitle>
<volume>1</volume>
<pages>255--261</pages>
<contexts>
<context position="7437" citStr="Toyama et al., 1995" startWordPosition="1099" endWordPosition="1102"> to run the software to visualize the frames and corresponding annotations. 2.2 Multiple-Object Detection, Segmentation, and Counting A number of solutions exists for detecting multiple objects of interest in video scenes (Olszewska and McCluskey, 2011; Olszewska, 2011; Olszewska, 2012b). In particular, background subtraction has long been used in the literature as it provides a useful approach to both detect and segment objects of interest. This method could be computed by difference between two consecutive frames (Archetti et al., 2006), by subtracting the current frame from the background (Toyama et al., 1995; Haritaoglu et al., 2000), or combining both frame difference and background subtraction techniques (Huang et al., 2007; Yao et al., 2009). In our scenario, images I(x,y) we consider may be taken from very different cameras, different lighting, etc. For that, we compute blobs separately in each of the views. The blobs are defined by labeled connected regions, which are obtained by background subtraction. The latter technique consists in computing the difference between the current image intensity I(x,y) and a background model, and afterwards, in extracting the foreground. To model the backgro</context>
</contexts>
<marker>Toyama, Krumm, Brumitt, Meyers, 1995</marker>
<rawString>K. Toyama, J. Krumm, B. Brumitt, and B. Meyers. 1995. Wallflower: Principles and practice of background maintenance. In Proceedings of the IEEE International Conference on Computer Vision, volume 1, pages 255– 261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Vrusias</author>
<author>D Makris</author>
<author>J-P Renno</author>
<author>N Newbold</author>
<author>K Ahmad</author>
<author>G Jones</author>
</authors>
<title>A framework for ontology enriched semantic annotation of CCTV video.</title>
<date>2007</date>
<booktitle>In Proceedings of the IEEE International Workshop on Image Analysis for Multimedia Interactive Services,</booktitle>
<pages>5</pages>
<contexts>
<context position="2723" citStr="Vrusias et al., 2007" startWordPosition="401" endWordPosition="404">ling, and video annotation. In the literature, most of the works dealing with the analysis of multi-camera video streams are focused on the sole task of tracking multiple, moving objects and use different approaches such as background subtraction (Diaz et al., 2013), Bayesian framework (Hsu et al., 2013), particle filter (Choi and Yoo, 2013), or Cardinalized Probability Hypothesis Density (CPHD) based filter (Lamard et al., 2013). On the other hand, research on video annotation has lead to the development of several efficient systems (Town, 2004; Natarajan and Nevatia, 2005; Bai et al., 2007; Vrusias et al., 2007), but all designed for a single camera video stream input. In this paper, we describe a full system which takes multi-camera video stream inputs and performs visual data processing to generate multi-view video annotations. Our system has been developed in context of outdoor video-surveillance and is an automatic Detection, Annotation, LabElling and Segmentation (DALES) software tool. It presents also the advantage to have an entire chain of data processing from visual to textual one, reducing thus the semantic gap. As camera calibration is in general an expensive process (Black et al., 2002) a</context>
</contexts>
<marker>Vrusias, Makris, Renno, Newbold, Ahmad, Jones, 2007</marker>
<rawString>B. Vrusias, D. Makris, J.-P. Renno, N. Newbold, K. Ahmad, and G. Jones. 2007. A framework for ontology enriched semantic annotation of CCTV video. In Proceedings of the IEEE International Workshop on Image Analysis for Multimedia Interactive Services, page 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Wren</author>
<author>A Azarbayejani</author>
<author>T Darrell</author>
<author>A P Pentland</author>
</authors>
<title>Real-time tracking of the human body.</title>
<date>1997</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>19</volume>
<issue>7</issue>
<contexts>
<context position="8105" citStr="Wren et al., 1997" startWordPosition="1203" endWordPosition="1206">e difference and background subtraction techniques (Huang et al., 2007; Yao et al., 2009). In our scenario, images I(x,y) we consider may be taken from very different cameras, different lighting, etc. For that, we compute blobs separately in each of the views. The blobs are defined by labeled connected regions, which are obtained by background subtraction. The latter technique consists in computing the difference between the current image intensity I(x,y) and a background model, and afterwards, in extracting the foreground. To model the background, we adopt the running Gaussian average (RGA) (Wren et al., 1997), characterized by the mean µb and the variance σ2b, rather than, for example, the Gaussian mixture model (GMM) (Stauffer and Grimson, 1999; Friedman and Russell, 1997; Zivkovic and van der Heijden, 2004), since the RGA method is much more suitable for real-time tracking. Next, the foreground is determined by 1 if |I (x, y) − µb |&gt; n · σb, with n ∈ N0, F (x, y) = (1) 0 otherwise. Finally, morphological operations (Haralick, 1988) are applied to the extracted foreground F, in order to exploit the existing information on the neighboring pixels, f (x,y) = Morph(F(x,y)). (2) 2.3 Object Labeling In</context>
</contexts>
<marker>Wren, Azarbayejani, Darrell, Pentland, 1997</marker>
<rawString>C. R. Wren, A. Azarbayejani, T. Darrell, and A. P. Pentland. 1997. Real-time tracking of the human body. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(7):780–785.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Yao</author>
<author>W Li</author>
<author>L Gao</author>
</authors>
<title>An efficient moving object detection algorithm using multi-mask.</title>
<date>2009</date>
<booktitle>In Proceedings of 6th IEEE International Conference on Fuzzy Systems and Knowledge Discovery,</booktitle>
<volume>5</volume>
<pages>354--358</pages>
<contexts>
<context position="7576" citStr="Yao et al., 2009" startWordPosition="1120" endWordPosition="1123">r of solutions exists for detecting multiple objects of interest in video scenes (Olszewska and McCluskey, 2011; Olszewska, 2011; Olszewska, 2012b). In particular, background subtraction has long been used in the literature as it provides a useful approach to both detect and segment objects of interest. This method could be computed by difference between two consecutive frames (Archetti et al., 2006), by subtracting the current frame from the background (Toyama et al., 1995; Haritaoglu et al., 2000), or combining both frame difference and background subtraction techniques (Huang et al., 2007; Yao et al., 2009). In our scenario, images I(x,y) we consider may be taken from very different cameras, different lighting, etc. For that, we compute blobs separately in each of the views. The blobs are defined by labeled connected regions, which are obtained by background subtraction. The latter technique consists in computing the difference between the current image intensity I(x,y) and a background model, and afterwards, in extracting the foreground. To model the background, we adopt the running Gaussian average (RGA) (Wren et al., 1997), characterized by the mean µb and the variance σ2b, rather than, for e</context>
</contexts>
<marker>Yao, Li, Gao, 2009</marker>
<rawString>C. Yao, W. Li, and L. Gao. 2009. An efficient moving object detection algorithm using multi-mask. In Proceedings of 6th IEEE International Conference on Fuzzy Systems and Knowledge Discovery, volume 5, pages 354–358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zivkovic</author>
<author>F van der Heijden</author>
</authors>
<title>Recursive unsupervised learning of finite mixture models.</title>
<date>2004</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>26</volume>
<issue>5</issue>
<marker>Zivkovic, van der Heijden, 2004</marker>
<rawString>Z. Zivkovic and F. van der Heijden. 2004. Recursive unsupervised learning of finite mixture models. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(5):651–656.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
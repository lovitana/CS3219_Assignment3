<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000026">
<title confidence="0.994458">
VERTa participation in the WMT14 Metrics Task
</title>
<author confidence="0.949844">
Elisabet Comelles
</author>
<affiliation confidence="0.918968">
Universitat de Barcelona
</affiliation>
<address confidence="0.938681">
Barcelona, Spain
</address>
<email confidence="0.999621">
elicomelles@ub.edu
</email>
<author confidence="0.697375">
Jordi Atserias
</author>
<affiliation confidence="0.612592">
Yahoo! Labs
</affiliation>
<address confidence="0.84">
Barcelona, Spain
</address>
<email confidence="0.939606">
jordi@yahoo-inc.com
</email>
<sectionHeader confidence="0.992393" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998561666666667">
In this paper we present VERTa, a lin-
guistically-motivated metric that com-
bines linguistic features at different lev-
els. We provide the linguistic motivation
on which the metric is based, as well as
describe the different modules in VERTa
and how they are combined. Finally, we
describe the two versions of VERTa,
VERTa-EQ and VERTa-W, sent to
WMT14 and report results obtained in
the experiments conducted with the
WMT12 and WMT13 data into English.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977596153847">
In the Machine Translation (MT) process, the
evaluation of MT systems plays a key role both
in their development and improvement. From the
MT metrics that have been developed during the
last decades, BLEU (Papineni et al., 2002) is one
of the most well-known and widely used, since it
is fast and easy to use. Nonetheless, researchers
such as (Callison-Burch et al., 2006) and (Lavie
and Dekowski, 2009) have claimed its weak-
nesses regarding translation quality and its ten-
dency to favour statistically-based MT systems.
As a consequence, other more complex metrics
that use linguistic information have been devel-
oped. Some use linguistic information at lexical
level, such as METEOR (Denkowski and Lavie,
2011); others rely on syntactic information, ei-
ther using constituent (Liu and Hildea, 2005) or
dependency analysis (Owczarzack et al., 2007a
and 2007b; He et al., 2010); others use more
complex information such as semantic roles
(Giménez and Márquez, 2007 and 2008a; Lo et
al., 2012). All these metrics focus on partial as-
pects of language; however, other researchers
have tried to combine information at different
linguistic levels in order to follow a more holistic
approach. Some of these metrics follow a ma-
chine-learning approach (Leusch and Ney, 2009;
Albrecht and Hwa, 2007a and 2007b), others
combine a wide variety of metrics in a simple
and straightforward way (Giménez, 2008b;
Giménez and Márquez, 2010; Specia and Gimé-
nez, 2010). However, very little research has
been performed on the impact of the linguistic
features used and how to combine this informa-
tion from a linguistic point of view. Hence, our
proposal is a linguistically-based metric, VERTa
(Comelles et al., 2012), which uses a wide vari-
ety of linguistic features at different levels, and
aims at combining them in order to provide a
wider and more accurate coverage than those
metrics working at a specific linguistic level. In
this paper we provide a description of the lin-
guistic information used in VERTa, the different
modules that form VERTa and how they are
combined according to the language evaluated
and the type of evaluation performed. Moreover,
the two versions of VERTa participating in
WMT14, VERTa-EQ and VERTa-W are de-
scribed. Finally, for the sake of comparison, we
use the data available in WMT12 and WMT13 to
compare both versions to the metrics participat-
ing in those shared tasks.
</bodyText>
<sectionHeader confidence="0.992458" genericHeader="method">
2 Linguistic Motivation
</sectionHeader>
<bodyText confidence="0.999756307692308">
Before developing VERTa, we analysed those
linguistic phenomena that an MT metric should
cover. From this analysis, we decided to organise
the information into the following groups:
Lexical information. The use of lexical
semantics plays a key role when compar-
ing a hypothesis and reference segment,
since it allows for identifying relations of
synonymy, hypernymy and hyponymy.
Morphological information. This type of
information is crucial when dealing with
languages with a rich inflectional mor-
phology, such as Spanish, French or Cata-
</bodyText>
<page confidence="0.978243">
368
</page>
<note confidence="0.438823">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 368–375,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99962245">
lan because it helps in covering phenom-
ena related to tense, mood, gender, num-
ber, aspect or case. In addition, morphol-
ogy in combination with syntax (morpho-
syntax) is also important to identify
agreement (i.e. subject-verb agreement).
This type of information should be taken
into account when evaluating the fluency
of a segment.
Syntactic information. This type of in-
formation covers syntactic structure, syn-
tactic relations and word order.
Semantic information. Named Entities
(NEs), sentence polarity and time expres-
sions are included here.
All this information described above should be
taken into account when developing a metric that
aims at covering linguistic phenomena at differ-
ent levels and evaluate both adequacy and flu-
ency.
</bodyText>
<sectionHeader confidence="0.996822" genericHeader="method">
3 Metric Description
</sectionHeader>
<bodyText confidence="0.99990736">
In order to cover the above linguistic features,
VERTa is organised into different modules:
Lexical similarity module, Morphological simi-
larity module, Dependency similarity module and
Semantic similarity module. Likewise, an Igram
similarity module has also been added in order to
account for similarity between chunks in the hy-
pothesis and reference segments. Each metric
works first individually and the final score is the
Fmean of the weighted combination of the Preci-
sion and Recall of each metric in order to get the
results which best correlate with human assess-
ment. This way, the different modules can be
weighted depending on their importance regard-
ing the type of evaluation (fluency or adequacy)
and language evaluated. In addition, the modular
design of this metric makes it suitable for all lan-
guages. Even those languages that do not have a
wide range of NLP tools available could be
evaluated, since each module can be used iso-
lated or in combination.
All metrics use a weighted precision and recall
over the number of matches of the particular
element of each level (words, dependency triples,
ngrams, etc) as shown below.
</bodyText>
<equation confidence="0.975388">
D
( ( h))
</equation>
<bodyText confidence="0.996952631578948">
Where r is the reference, h is the hypothesis
and ∇ is a function that given a segment will
return the elements of each level (e.g. words at
lexical level and triples at dependency level). D
is the set of different functions to project the
level element into the features associated to each
level, such as word-form, lemma or partial-
lemma at lexical level. is a function
that returns the number of matches according to
the feature a (i.e. the number of lexical matches
at the lexical level or the number of dependency
triples that match at the dependency level). Fi-
nally, W is the set of weights ]0 1] associated to
each of the different features in a particular level
in order to combine the different kinds of
matches considered in that level.
All modules forming VERTa and the linguis-
tic features used are described in detail in the
following subsections.
</bodyText>
<subsectionHeader confidence="0.999058">
3.1 Lexical module
</subsectionHeader>
<bodyText confidence="0.998169363636364">
Inspired by METEOR, the lexical module
matches lexical items in the hypothesis segment
to those in the reference segment taking into ac-
count several linguistic features. However, while
METEOR uses word-form, synonymy, stemming
and paraphrasing, VERTa relies on word-form,
synonymy1, lemma, partial lemma2, hypernyms
and hyponyms. In addition, a set of weights is
assigned to each type of match depending on
their importance as regards semantics (see Table
1).
</bodyText>
<table confidence="0.992161875">
W Match Examples
HYP REF
1 1 Word-form east east
2 1 Synonym believed considered
3 1 Hypernym barrel keg
4 1 Hyponym keg barrel
5 .8 Lemma is_BE are_BE
6 .6 Part-lemma danger dangerous
</table>
<tableCaption confidence="0.999498">
Table 1. Lexical matches and examples.
</tableCaption>
<subsectionHeader confidence="0.999543">
3.2 Morphological similarity module
</subsectionHeader>
<bodyText confidence="0.9999294">
The morphological similarity module is based on
the matches established in the lexical module
(except for the partial-lemma match) in combina-
tion with Part-of-Speech (PoS) tags from the an-
notated corpus3. The aim of this module is to
</bodyText>
<footnote confidence="0.7836506">
1 Information on synonyms, lemmas, hypernyms and
hyponyms is obtained from WordNet 3.0.
2 Lemmas that share the first four letters.
3 The corpus has been PoS tagged using the Stanford
Parser (de Marneffe et al. 2006).
</footnote>
<figure confidence="0.639263777777778">
P
W
(h)
nmatch
W nmatch
D
( (r))
R
(r)
</figure>
<page confidence="0.992543">
369
</page>
<bodyText confidence="0.9995545">
compensate the broader coverage of the lexical
module, preventing matches such as invites and
invite, which although similar in terms of mean-
ing, do not coincide as for their morphological
information. Therefore, this module turns more
appropriate to assess the fluency of a segment
rather than its adequacy. In addition, this module
will be particularly useful when evaluating lan-
guages with a richer inflectional morphology (i.e.
Romance languages).
In line with the lexical similarity metric, the
morphological similarity metric establishes
matches between items in the hypothesis and the
reference sentence and a set of weights (W) is
applied. However, instead of comparing single
lexical items as in the previous module, in this
module we compare pairs of features in the order
established in Table 2.
</bodyText>
<table confidence="0.999212461538462">
W Match Examples
HYP REF
1 (Word- (he, PRP) (he, PRP)
form, PoS)
1 (Synonym, (VIEW, (OPINON,
PoS) NNS) NNS)
1 (Hypern., (PUBLICA- (MAGA-
PoS) TION, NN) ZINE, NN)
1 (Hypon., (MAGA- (PUBLI-
PoS) ZINE, NN) CATION,
NN)
.8 (LEMMA, can_(CAN, Could_(C
PoS) MD) AN, MD)
</table>
<tableCaption confidence="0.999267">
Table 2. Morphological module matches.
</tableCaption>
<subsectionHeader confidence="0.986968">
3.3 Dependency similarity module
</subsectionHeader>
<bodyText confidence="0.9996476">
The dependency similarity metric helps in cap-
turing similarities between semantically compa-
rable expressions that show a different syntactic
structure (see Example 1), as well as changes in
word order (see Example 2).
</bodyText>
<equation confidence="0.449575">
Example 1:
</equation>
<bodyText confidence="0.9473544">
HYP: ...the interior minister...
REF: ...the minister of interior...
In example 1 both hypothesis and reference
chunks convey the same meaning but their syn-
tactic constructions are different.
</bodyText>
<equation confidence="0.205134">
Example 2:
</equation>
<bodyText confidence="0.917112272727273">
HYP: After a meeting Monday night with the
head of Egyptian intelligence chief Omar
Suleiman Haniya said....
REF: Haniya said, after a meeting on Monday
evening with the head of Egyptian Intelligence
General Omar Suleiman...
In example 2, the adjunct realised by the PP
After a meeting Monday night with the head of
Egyptian intelligence chief Omar Suleiman oc-
cupies different positions in the hypothesis and
reference strings. In the hypothesis it is located at
the beginning of the sentence, preceding the sub-
ject Haniya, whereas in the reference, it is placed
after the verb. By means of dependencies, we can
state that although located differently inside the
sentence, both subject and adjunct depend on the
verb.
This module works at sentence level and fol-
lows the approach used by (Owczarzack et al.,
2007a and 2007b) and (He et al., 2010) with
some linguistic additions in order to adapt it to
our metric combination. Similar to the morpho-
logical module, the dependency similarity metric
also relies first on those matches established at
lexical level − word-form, synonymy, hy-
pernymy, hyponymy and lemma − in order to
capture lexical variation across dependencies and
avoid relying only on surface word-form. Then,
by means of flat triples with the form La-
bel(Head, Mod) obtained from the parser4, four
different types of dependency matches have been
designed (see Table 3) and weights have been
assigned to each type of match.
</bodyText>
<table confidence="0.999847384615385">
W Match Type Match Descr.
1 Complete Label1=Label2
Head1=Head2
Mod1=Mod2
1 Partial_no_label Label1≠Label2
Head1=Head2
Mod1=Mod2
.9 Partial_no_mod Label1=Label2
Head1=Head2
Mod1≠Mod2
.7 Partial_no_head Label1=Label2
Head1≠Head2
Mod1=Mod2
</table>
<tableCaption confidence="0.999555">
Table 3. Dependency matches.
</tableCaption>
<bodyText confidence="0.99988525">
In addition, dependency categories also re-
ceive a different weight depending on how in-
formative they are: dep, det and _5 which receive
0.5, whereas the rest of categories are assigned
the maximum weight (1).
Finally, a set of language-dependent rules has
been added with two goals: 1) capturing similari-
ties between different syntactic structures con-
</bodyText>
<footnote confidence="0.966278">
4 Both hypothesis and reference strings are annotated
with dependency relations by means of the Stanford
parser (de Marneffe et al. 2006).
5 stands for no_dep_label
</footnote>
<page confidence="0.995206">
370
</page>
<bodyText confidence="0.999754333333333">
veying the same meaning; and 2) restricting cer-
tain dependency relations (i.e. subject word order
when translating from Arabic to English).
</bodyText>
<subsectionHeader confidence="0.961316">
3.4 Ngram similarity module
</subsectionHeader>
<bodyText confidence="0.997309153846154">
The ngram similarity metric matches chunks in
the hypothesis and reference segments and relies
on the matches set by the lexical similarity met-
ric, which allows us to work not only with word-
forms but also with synonyms, lemmas, partial-
lemmas, hypernyms and hyponyms as shown in
Example 3, where the chunks [the situation in
the area] and [the situation in the region] do
match, even though area and region do not share
the same word-form but a relation of synonymy.
Example 3:
HYP: ... the situation in the area...
REF: ... the situation in the region...
</bodyText>
<subsectionHeader confidence="0.951789">
3.5 Semantics similarity module
</subsectionHeader>
<bodyText confidence="0.999952344827586">
As confirmed by the lexical module, semantics
plays an important role in the evaluation of ade-
quacy. This has also been claimed by (Lo and
Wu, 2010) who report that their metric based on
semantic roles outperforms other well-known
metrics when adequacy is assessed. With this
aim in mind the semantic similarity module uses
other semantic features at sentence level: NEs,
time expressions and polarity.
Regarding NEs, we use Named-Entity recog-
nition (NER) and Named-Entity linking (NEL).
Following previous NE-based metrics (Reeder et
al., 2011 and Giménez, 2008) the NER metric6
aims at capturing similarities between NEs in the
hypothesis and reference segments. On the other
hand NEL7 focuses only on those NEs that ap-
pear on Wikipedia, which allows for linking NEs
regardless of their external form. Thus, EU and
European Union will be captured as the same
NE, since both of them are considered as the
same organisation in Wikipedia.
As regards time expressions, the TIMEX met-
ric matches temporal expressions in the hypothe-
sis and reference segments regardless of their
form. The tool used is the Stanford Temporal
Tagger (Chang and Manning, 2012) which rec-
ognizes not only points in time but also duration.
By means of this metric, different syntactic struc-
tures conveying the same time expression can be
</bodyText>
<footnote confidence="0.917804">
6 In order to identify NEs we use the Supersense Tag-
ger (Ciaramita and Altun, 2006).
</footnote>
<page confidence="0.286169">
7 The NEL module uses a graph-based NEL tool
</page>
<bodyText confidence="0.942229033333333">
(Hachey, Radford and Curran, 2010) which links NEs
in a text with those in Wikipedia pages.
matched, such as on February 3rd and on the
third of February.
Finally, it has been reported that negation
might pose a problem to SMT systems (Wetzel
and Bond, 2012). In order to answer such need, a
module that checks the polarity of the sentence
has been added using the dictionary strategy de-
scribed (Atserias et al., 2012):
Adding 0.5 for each weak positive word.
Adding 1.0 for each strong positive word.
Subtracting 0.5 for each weak negative
word.
Subtracting 1.0 for each strong negative
word.
For each query term score, the value is propa-
gated to the query term positions by reducing its
strength in a factor of 1/n, where n is the distance
between the query term and the polar term.
According to the experiments performed, this
module shows a low correlation with human
judgements on adequacy, since only partial as-
pects of translation are considered, whereas hu-
man judges assess whole segments. However,
regardless of how well/bad the module correlates
with human judgements, it proves useful to
check partial aspects of the segments translated,
such as the correct translation of NEs or the cor-
rect translation of negation.
</bodyText>
<subsectionHeader confidence="0.994116">
3.6 Metrics combination
</subsectionHeader>
<bodyText confidence="0.999996111111111">
The modular design of VERTa allows for pro-
viding different weights to each module depend-
ing on the type of evaluation and the language
evaluated. Thus following linguistic criteria
when evaluating adequacy, those modules which
must play a key role are the lexical and depend-
ency module, since they are more related to se-
mantics; whereas, when evaluating fluency those
related to morphology, morphosyntax and con-
stituent word order will be the most important.
Moreover, metrics can also be combined depend-
ing on the type of language evaluated. If a lan-
guage with a rich inflectional morphology is as-
sessed, the morphology module should be given
a higher weight; whereas if the language evalu-
ated does not show such a rich inflectional mor-
phology, the weight of the morphology module
should be lower.
</bodyText>
<sectionHeader confidence="0.995071" genericHeader="evaluation">
4 Experiments and results
</sectionHeader>
<bodyText confidence="0.98492575">
Experiments were carried out on WMT data,
specifically on WMT12 and WMT13 data, all
languages into English. Languages “all” include
French, German, Spanish and Czech for WMT12
</bodyText>
<page confidence="0.997348">
371
</page>
<bodyText confidence="0.994291500000001">
and French, German, Spanish, Czech and Rus-
sian for WMT13. Both segment and system level
evaluations were performed. Evaluation sets pro-
vided by WMT organizers were used to calculate
both segment and system level correlations.
Since VERTa has been mainly designed to as-
sess either adequacy or fluency separately, our
goal for WMT14 was to find the best combina-
tion in order to evaluate whole translation qual-
ity. Firstly we decided to explore the influence of
each module separately. To this aim, all modules
described above, except for the semantics one
were used and tested separately. Secondly, all
modules were assigned the same weight and
tested in combination (VERTa-EQ). The reason
why the semantics module was disregarded is
that it does not usually correlate well with human
judgements, as stated above. Each module was
set as follows:
Lexical module. As described above, ex-
cept for the use of hypernyms/hyponyms
matches that were disregarded.
Morphological module. As described
above, except for the lemma-PoS match
and the hypernyms/hyponyms-PoS match.
Dependency module. As described above.
Ngram module. As described above, using
a 2-gram length.
Finally, we used the module combination
aimed at evaluating adequacy, which is mainly
based on the dependency and lexical modules,
but with a stronger influence of the ngram mod-
ule in order to control word order (VERTa-W).
Weights were manually assigned, based on re-
sults obtained in previous experiments conducted
for adequacy and fluency (Comelles et al., 2012),
as follows:
Lexical module: 0.41
Morphological module: 0
Dependency module: 0.40
Ngram module: 0.19
Experiments aimed at evaluating the influence
of each module (see Table 4 and Table 5) show
that the dependency module, in the case of
WMT12 data, and the lexical module in the case
of WMT13 data, are the most effective ones.
However, the influence of the ngram module and
the morphological module varies depending on
the source language. The fact that the depend-
ency module correlates better with human
judgements than others might be due to its flexi-
bility to capture different syntactic constructions
that convey the same meaning. In addition, the
good performance of the lexical module is due to
the use of lexical semantic relations. On the other
hand, in general the morphological module
shows a better performance than the ngram one,
which might be due to the type of source lan-
guages and the possible translation mistakes. All
source languages are highly-inflected languages
and this might cause problems when translating
into English, since its inflectional morphology is
not as rich as theirs. As for the low performance
of the ngram module in the cs-en (especially, in
WMT12 data), it might be due to the fact that
Czech word order is unrestricted, whereas Eng-
lish shows a stricter word order and this might
cause translation issues. A longer ngram distance
might have been more appropriate to control
word order in this case.
</bodyText>
<table confidence="0.999616">
Module fr-en de-en es-en cs-en
Lexical .16 .20 .18 .14
Morph. .17 .19 .18 .12
Depend. .18 .24 .20 .17
Ngram .16 .17 .15 .08
</table>
<tableCaption confidence="0.8893435">
Table 4. Segment-level Kendall’s tau correla-
tion per module with WMT12 data.
</tableCaption>
<table confidence="0.998427166666667">
Module fr- de- es- cs- ru-
en en en en en
Lexical .239 .254 .294 .227 .220
Morph. .236 .243 .295 .214 .191
Depend. .232 .247 .275 .220 .199
Ngram .237 .245 .283 .213 .189
</table>
<tableCaption confidence="0.996796">
Table 5. Segment-level Kendall’s tau correla-
</tableCaption>
<bodyText confidence="0.970591882352941">
tion per module with WMT13 data.
Finally, two versions of VERTa were com-
pared: the unweighted combination (VERTa-EQ)
and the weighted one (VERTa-W). These two
versions were also compared to some of the best
performing metrics in WMT12 (see Table 6 and
Table 7) and WMT13 (see Table 8 and Table 9):
Spede07-pP, METEOR, SEMPOR and AMBER
(Callison-Burch et al., 2012); SIMPBLEU-
RECALL, METEOR and DEPREF-ALIGN 8 ).
As regards WMT12 data at segment level, the
unweighted version achieves similar results to
those obtained by the best performing metrics.
On the other hand, VERTa-W’s results are
slightly worse, especially for fr-en and es-en
pairs, which is due to the fact that the morpho-
logical module has been disregarded in this ver-
</bodyText>
<footnote confidence="0.928263">
8 http://www.statmt.org/wmt13/papers.html
</footnote>
<page confidence="0.995623">
372
</page>
<bodyText confidence="0.929605">
sion. Regarding system level correlation, neither
VERTa-EQ nor VERTa-W achieves a high cor-
relation with human judgements.
</bodyText>
<table confidence="0.985836583333333">
Metric fr-en de-en es-en cs-en
Spede07-pP .26 .28 .26 .21
METEOR .25 .27 .24 .21
VERTa-EQ .26 .28 .26 .20
VERTa-W .24 .28 .25 .20
Table 6. Segment-level Kendall’s tau correla-
tion WMT12.
Metric fr-en de-en es-en cs-en
SEMPOR .80 .92 .94 .94
AMBER .85 .79 .97 .83
VERTa-EQ .83 .71 .89 .66
VERTa-W .79 .73 .91 .66
</table>
<tableCaption confidence="0.996595">
Table 7. System-level Spearman’s rho correla-
</tableCaption>
<bodyText confidence="0.9790131">
tion WMT12.
As for segment level WMT13 results (see Ta-
ble 8), although both VERTa-EQ and VERTa-
W’s performance is worse than that of the two
best-performing metrics, both versions achieve a
third and fourth position for all language pairs,
except for fr-en. As regards system level correla-
tions (see Table 9), both versions of VERTa
show the best performance for de-en and ru-en
pairs, as well as for the average score.
</bodyText>
<sectionHeader confidence="0.990722" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999616875">
In this paper we have presented VERTa, a lin-
guistically-based MT metric. VERTa allows for
modular combination depending on the language
and type of evaluation conducted. Although
VERTa has been designed to evaluate adequacy
and fluency separately, in order to evaluate
whole MT quality, a couple of versions have
been used: VERTa-EQ, an unweighted version
that uses all modules, and VERTa-W a weighted
version that uses the lexical, dependency and
ngram modules.
Experiments have shown that the modules that
best correlate with human judgements are the
dependency and lexical modules. In addition,
both VERTa-EQ and VERTa-W have been com-
pared to the best performing metrics in WMT12
and WMT13 shared tasks. VERTa-EQ has
proved to be in line with results obtained by
Spede07-pP and METEOR in WMT12 at seg-
ment level, while in WMT13, both VERTa and
VERTa-W occupy the third and fourth position
after METEOR and DEPREF-ALIGN as regards
segment level and the first position at system
level.
In the future, we plan to continue working on
the improvement of VERTa and use automatic
tuning of module’s weight in order to achieve the
final version that best correlates with human
judgements on ranking. Likewise, we would like
to explore the use of VERTa to evaluate other
languages but English and how NLP tool errors
may influence the performance of the metric.
</bodyText>
<sectionHeader confidence="0.999067" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999538571428571">
We would like to acknowledge Victoria Arranz
and Irene Castellón for their valuable comments
and sharing their knowledge.
This work has been partially funded by the Span-
ish Government (projects SKATeR, TIN2012-
38584-C06-06 and Holopedia, TIN2010-21128-
C02-02).
</bodyText>
<table confidence="0.9997195">
Metric fr-en de-en es-en cs-en ru-en Average
SIMPBLEU-RECALL .303 .318 .388 .260 .234 .301
METEOR .264 .293 .324 .265 .239 .277
VERTa-EQ .252 .280 .318 .239 .215 .261
VERTa-W .253 .278 .314 .238 .222 .261
DEPREF-ALIGN .257 .267 .312 .228 .200 .253
</table>
<tableCaption confidence="0.948157">
Table 8. Segment-level Kendall’s tau correlation WMT13.
</tableCaption>
<table confidence="0.9998418">
Metric fr-en de-en es-en cs-en ru-en Average
METEOR .984 .961 .979 .964 .789 .935
DEPREF-ALIGN .995 .966 .965 .964 .768 .931
VERTa-EQ .989 .970 .972 .936 .814 .936
VERTa-W .989 .980 .972 .945 .868 .951
</table>
<tableCaption confidence="0.995754">
Table 9. System-level Spearman’s rho correlation WMT13.
</tableCaption>
<page confidence="0.998463">
373
</page>
<sectionHeader confidence="0.862148" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.998457933333333">
J. S. Albrecth and R. Hwa. 2007. A Re-examination
of Machine Learning Approaches for Sentence-
Level MT Evaluation. In The Proceedings of the
45th Annual Meeting of the ACL, Prague, Czech
Republic.
J. S. Albrecth and R. Hwa. 2007. Regression for Sen-
tence-Level MT Evaluation with Pseudo Refer-
ences. In The Proceedings of the 45th Annual
Meeting of the ACL, Prague, Czech Republic.
J. Atserias, R. Blanco, J. M. Chenlo and C.
Rodriquez. 2012. FBM-Yahoo at RepLab 2012,
CLEF (Online Working Notes/Labs/Workshop)
2012, September 20, 2012.
C. Callison-Burch, M. Osborne and P. Koehn. 2006.
Re-evaluating the role of BLEU in machine trans-
lation research. In Proceedings of the EACL 2006.
C. Callison-Burch, P. Kohen, Ch. Monz, M. Post, R.
Soricut and L. Specia. 2012. Findings of the 2012
Workshop on Statistical Machine Translation. In
Proceedings of the 7th Workshop on Statistical
Machine Translation. Montréal. Canada.
A. X. Chang and Ch. D. Manning. 2012. SUTIME: A
Library for Recognizing and Normalizing Time
Expressions. 8th International Conference on Lan-
guage Resources and Evaluation (LREC 2012).
M. Ciaramita and Y. Altun. 2006. Broad-coverage
sense disambiguation and information extraction
with a supersense sequence tagger. Empirical
Methods in Natural Language Processing
(EMNLP).
E. Comelles, J. Atserias, V. Arranz and I. Castellón.
2012. VERTa: Linguistic features in MT evalua-
tion. Proceedings of the Eighth International Con-
ference on Language Resources and Evaluation
(LREC&apos;12), Istanbul, Turkey.
M.C. de Marneffe, B. MacCartney and Ch. D. Man-
ning. 2006. Generating Typed Dependency Parses
from Phrase Structure Parses in Proceedings of the
5th Edition of the International Conference on
Language Resources and Evaluation (LREC-
2006). Genoa, Italy.
M. J. Denkowski and A. Lavie. 2011. METEOR 1.3:
Automatic Metric for Reliable Optimization and
Evaluation of Machine Translation Systems in
Proceedings of the 6th Workshop on Statistical
Machine Translation (ACL-2011). Edinburgh,
Scotland, UK.
J. Giménez and Ll. Màrquez. 2007. Linguistic fea-
tures for automatic evaluation of heterogeneous
MT systems in Proceedings of the 2nd Workshop
on Statistical Machine Translation (ACL), Prague,
Czech Repubilc.
J. Giménez and Ll. Màrquez. 2008. A smorgasbord of
features for automatic MT evaluation in Proceed-
ings of the 3rd Workshop on Statistical Machine
Translation (ACL). Columbus. OH.
J. Gimenez. 2008. Empirical Machine Translation
and its Evaluation. Doctoral Dissertation. UPC.
J. Giménez and Ll. Márquez. 2010. Linguistic Meas-
ures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3–4),77–86.
Springer.
B. Hachey, W. Radford and J. R. Curran. 2011.
Graph-based named entity linking with Wikipedia
in Proceedings of the 12th International confer-
ence on Web information system engineering,
pages 213-226, Springer-Verlag, Berlin, Heidel-
berg.
Y. He, J. Du, A. Way and J. van Genabith. 2010. The
DCU Dependency-based Metric in WMT-Metrics
MATR 2010. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
Metrics MATR (WMT 2010), Uppsala, Sweden.
A. Lavie and M. J. Denkowski. 2009. The METEOR
Metric for Automatic Evaluation of Machine
Translation. Machine Translation, 23.
G. Leusch and H. Ney. 2008. BLEUSP, INVWER,
CDER: Three improved MT evaluation measures.
In NIST Metrics for Machine Translation 2008
Evaluation (MericsMATR08), Waikiki, Honolulu,
Hawaii, October 2008.
D. Liu and D. Hildea. 2005. Syntactic Features for
Evaluation of Machine Translation in Proceedings
of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation
and/or Summarization, Ann Arbor
Ch.Lo and D. Wu. 2010. Semantic vs. Syntactic vs.
Ngram Structure for Machine Translation Evalua-
tion. In Proceedings of the 4th Workshop on Syntax
Semantics and Structure in Statistical Translation.
Beijing. China.
Ch. Lo, A. K. Tumurulu and D. Wu. 2012. Fully
Automatic Semantic MT Evaluation. Proceedings
of the 7th Wrokshop on Statistical Machine Trans-
lation, Montréal, Canada, June 7-8.
K. Owczarzak, J. van Genabith and A. Way. 2007.
Dependency-Based Automatic Evaluation for Ma-
chine Translation in Proceedings of SSST, NAACL-
HLT/AMTA Workshop on Syntax and Structure I
Statistical Translation, Rochester, New York.
K. Owczarzak, J. van Genabith and A. Way. 2007.
Labelled Dependencies in Machine Translation
Evaluation in Proceedings of the ACL Workshop
on Statistical Machine Translation, Prague, Czech
Republic.
</reference>
<page confidence="0.988385">
374
</page>
<reference confidence="0.999180666666667">
K. Papineni, S. Roukos, T. Ward and W. Zhu. 2002.
BLEU: A Method for Automatic Evaluation of
Machine Translation. In Proceedings of the 40th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL02). Philadelphia. PA.
F. Reeder, K. Miller, J. Doyon and J. White. 2001.
The Naming of Things and the Confusion of
Tongues: an MT Metric. Proceedings of the Work-
shop on MT Evaluation ”Who did what to whom?”
at Machine Translation Summit VIII.
L. Specia and J. Giménez. 2010. Combining Confi-
dence Estimation and Reference-based Metrics for
Segment-level MT Evaluation. The Ninth Confer-
ence of the Association for Machine Translation in
the Americas (AMTA 2010), Denver, Colorado.
D. Wetzel and F. Bond. 2012. Enriching Parallel Cor-
pora for Statistical Machine Translation with Se-
mantic Negation Rephrasing. Proceedings of SSST-
6, Sixth Workshop on Syntax, Semantics and Struc-
ture in Statistical Translation, Jeju, Republic of
Korea.
</reference>
<page confidence="0.999111">
375
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.265838">
<title confidence="0.959921">VERTa participation in the WMT14 Metrics Task</title>
<author confidence="0.928058">Elisabet</author>
<affiliation confidence="0.997225">Universitat de</affiliation>
<address confidence="0.649982">Barcelona, Spain</address>
<email confidence="0.999753">elicomelles@ub.edu</email>
<author confidence="0.640495">Jordi</author>
<affiliation confidence="0.784153">Yahoo!</affiliation>
<address confidence="0.913642">Barcelona, Spain</address>
<email confidence="0.999866">jordi@yahoo-inc.com</email>
<abstract confidence="0.982055846153846">In this paper we present VERTa, a linguistically-motivated metric that combines linguistic features at different levels. We provide the linguistic motivation on which the metric is based, as well as describe the different modules in VERTa and how they are combined. Finally, we describe the two versions of VERTa, VERTa-EQ and VERTa-W, sent to WMT14 and report results obtained in the experiments conducted with the WMT12 and WMT13 data into English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J S Albrecth</author>
<author>R Hwa</author>
</authors>
<title>A Re-examination of Machine Learning Approaches for SentenceLevel MT Evaluation.</title>
<date>2007</date>
<booktitle>In The Proceedings of the 45th Annual Meeting of the ACL,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Albrecth, Hwa, 2007</marker>
<rawString>J. S. Albrecth and R. Hwa. 2007. A Re-examination of Machine Learning Approaches for SentenceLevel MT Evaluation. In The Proceedings of the 45th Annual Meeting of the ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Albrecth</author>
<author>R Hwa</author>
</authors>
<title>Regression for Sentence-Level MT Evaluation with Pseudo References.</title>
<date>2007</date>
<booktitle>In The Proceedings of the 45th Annual Meeting of the ACL,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Albrecth, Hwa, 2007</marker>
<rawString>J. S. Albrecth and R. Hwa. 2007. Regression for Sentence-Level MT Evaluation with Pseudo References. In The Proceedings of the 45th Annual Meeting of the ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Atserias</author>
<author>R Blanco</author>
<author>J M Chenlo</author>
<author>C Rodriquez</author>
</authors>
<title>FBM-Yahoo at RepLab</title>
<date>2012</date>
<journal>CLEF (Online Working Notes/Labs/Workshop)</journal>
<contexts>
<context position="14203" citStr="Atserias et al., 2012" startWordPosition="2283" endWordPosition="2286"> different syntactic structures conveying the same time expression can be 6 In order to identify NEs we use the Supersense Tagger (Ciaramita and Altun, 2006). 7 The NEL module uses a graph-based NEL tool (Hachey, Radford and Curran, 2010) which links NEs in a text with those in Wikipedia pages. matched, such as on February 3rd and on the third of February. Finally, it has been reported that negation might pose a problem to SMT systems (Wetzel and Bond, 2012). In order to answer such need, a module that checks the polarity of the sentence has been added using the dictionary strategy described (Atserias et al., 2012): Adding 0.5 for each weak positive word. Adding 1.0 for each strong positive word. Subtracting 0.5 for each weak negative word. Subtracting 1.0 for each strong negative word. For each query term score, the value is propagated to the query term positions by reducing its strength in a factor of 1/n, where n is the distance between the query term and the polar term. According to the experiments performed, this module shows a low correlation with human judgements on adequacy, since only partial aspects of translation are considered, whereas human judges assess whole segments. However, regardless </context>
</contexts>
<marker>Atserias, Blanco, Chenlo, Rodriquez, 2012</marker>
<rawString>J. Atserias, R. Blanco, J. M. Chenlo and C. Rodriquez. 2012. FBM-Yahoo at RepLab 2012, CLEF (Online Working Notes/Labs/Workshop) 2012, September 20, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>M Osborne</author>
<author>P Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL</booktitle>
<contexts>
<context position="1036" citStr="Callison-Burch et al., 2006" startWordPosition="158" endWordPosition="161">e different modules in VERTa and how they are combined. Finally, we describe the two versions of VERTa, VERTa-EQ and VERTa-W, sent to WMT14 and report results obtained in the experiments conducted with the WMT12 and WMT13 data into English. 1 Introduction In the Machine Translation (MT) process, the evaluation of MT systems plays a key role both in their development and improvement. From the MT metrics that have been developed during the last decades, BLEU (Papineni et al., 2002) is one of the most well-known and widely used, since it is fast and easy to use. Nonetheless, researchers such as (Callison-Burch et al., 2006) and (Lavie and Dekowski, 2009) have claimed its weaknesses regarding translation quality and its tendency to favour statistically-based MT systems. As a consequence, other more complex metrics that use linguistic information have been developed. Some use linguistic information at lexical level, such as METEOR (Denkowski and Lavie, 2011); others rely on syntactic information, either using constituent (Liu and Hildea, 2005) or dependency analysis (Owczarzack et al., 2007a and 2007b; He et al., 2010); others use more complex information such as semantic roles (Giménez and Márquez, 2007 and 2008a</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>C. Callison-Burch, M. Osborne and P. Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In Proceedings of the EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Post Monz</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical Machine Translation.</booktitle>
<location>Montréal. Canada.</location>
<marker>Monz, Soricut, Specia, 2012</marker>
<rawString>C. Callison-Burch, P. Kohen, Ch. Monz, M. Post, R. Soricut and L. Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical Machine Translation. Montréal. Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Manning</author>
</authors>
<title>SUTIME: A Library for Recognizing and Normalizing Time Expressions.</title>
<date>2012</date>
<booktitle>8th International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="13496" citStr="Manning, 2012" startWordPosition="2159" endWordPosition="2160">., 2011 and Giménez, 2008) the NER metric6 aims at capturing similarities between NEs in the hypothesis and reference segments. On the other hand NEL7 focuses only on those NEs that appear on Wikipedia, which allows for linking NEs regardless of their external form. Thus, EU and European Union will be captured as the same NE, since both of them are considered as the same organisation in Wikipedia. As regards time expressions, the TIMEX metric matches temporal expressions in the hypothesis and reference segments regardless of their form. The tool used is the Stanford Temporal Tagger (Chang and Manning, 2012) which recognizes not only points in time but also duration. By means of this metric, different syntactic structures conveying the same time expression can be 6 In order to identify NEs we use the Supersense Tagger (Ciaramita and Altun, 2006). 7 The NEL module uses a graph-based NEL tool (Hachey, Radford and Curran, 2010) which links NEs in a text with those in Wikipedia pages. matched, such as on February 3rd and on the third of February. Finally, it has been reported that negation might pose a problem to SMT systems (Wetzel and Bond, 2012). In order to answer such need, a module that checks </context>
</contexts>
<marker>Manning, 2012</marker>
<rawString>A. X. Chang and Ch. D. Manning. 2012. SUTIME: A Library for Recognizing and Normalizing Time Expressions. 8th International Conference on Language Resources and Evaluation (LREC 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ciaramita</author>
<author>Y Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="13738" citStr="Ciaramita and Altun, 2006" startWordPosition="2200" endWordPosition="2203">s regardless of their external form. Thus, EU and European Union will be captured as the same NE, since both of them are considered as the same organisation in Wikipedia. As regards time expressions, the TIMEX metric matches temporal expressions in the hypothesis and reference segments regardless of their form. The tool used is the Stanford Temporal Tagger (Chang and Manning, 2012) which recognizes not only points in time but also duration. By means of this metric, different syntactic structures conveying the same time expression can be 6 In order to identify NEs we use the Supersense Tagger (Ciaramita and Altun, 2006). 7 The NEL module uses a graph-based NEL tool (Hachey, Radford and Curran, 2010) which links NEs in a text with those in Wikipedia pages. matched, such as on February 3rd and on the third of February. Finally, it has been reported that negation might pose a problem to SMT systems (Wetzel and Bond, 2012). In order to answer such need, a module that checks the polarity of the sentence has been added using the dictionary strategy described (Atserias et al., 2012): Adding 0.5 for each weak positive word. Adding 1.0 for each strong positive word. Subtracting 0.5 for each weak negative word. Subtra</context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>M. Ciaramita and Y. Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Comelles</author>
<author>J Atserias</author>
<author>V Arranz</author>
<author>I Castellón</author>
</authors>
<title>VERTa: Linguistic features in MT evaluation.</title>
<date>2012</date>
<booktitle>Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12),</booktitle>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="2362" citStr="Comelles et al., 2012" startWordPosition="367" endWordPosition="370">ave tried to combine information at different linguistic levels in order to follow a more holistic approach. Some of these metrics follow a machine-learning approach (Leusch and Ney, 2009; Albrecht and Hwa, 2007a and 2007b), others combine a wide variety of metrics in a simple and straightforward way (Giménez, 2008b; Giménez and Márquez, 2010; Specia and Gimé- nez, 2010). However, very little research has been performed on the impact of the linguistic features used and how to combine this information from a linguistic point of view. Hence, our proposal is a linguistically-based metric, VERTa (Comelles et al., 2012), which uses a wide variety of linguistic features at different levels, and aims at combining them in order to provide a wider and more accurate coverage than those metrics working at a specific linguistic level. In this paper we provide a description of the linguistic information used in VERTa, the different modules that form VERTa and how they are combined according to the language evaluated and the type of evaluation performed. Moreover, the two versions of VERTa participating in WMT14, VERTa-EQ and VERTa-W are described. Finally, for the sake of comparison, we use the data available in WMT</context>
<context position="17564" citStr="Comelles et al., 2012" startWordPosition="2823" endWordPosition="2826">f hypernyms/hyponyms matches that were disregarded. Morphological module. As described above, except for the lemma-PoS match and the hypernyms/hyponyms-PoS match. Dependency module. As described above. Ngram module. As described above, using a 2-gram length. Finally, we used the module combination aimed at evaluating adequacy, which is mainly based on the dependency and lexical modules, but with a stronger influence of the ngram module in order to control word order (VERTa-W). Weights were manually assigned, based on results obtained in previous experiments conducted for adequacy and fluency (Comelles et al., 2012), as follows: Lexical module: 0.41 Morphological module: 0 Dependency module: 0.40 Ngram module: 0.19 Experiments aimed at evaluating the influence of each module (see Table 4 and Table 5) show that the dependency module, in the case of WMT12 data, and the lexical module in the case of WMT13 data, are the most effective ones. However, the influence of the ngram module and the morphological module varies depending on the source language. The fact that the dependency module correlates better with human judgements than others might be due to its flexibility to capture different syntactic construc</context>
</contexts>
<marker>Comelles, Atserias, Arranz, Castellón, 2012</marker>
<rawString>E. Comelles, J. Atserias, V. Arranz and I. Castellón. 2012. VERTa: Linguistic features in MT evaluation. Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12), Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses</title>
<date>2006</date>
<booktitle>in Proceedings of the 5th Edition of the International Conference on Language Resources and Evaluation (LREC2006).</booktitle>
<location>Genoa, Italy.</location>
<marker>Manning, 2006</marker>
<rawString>M.C. de Marneffe, B. MacCartney and Ch. D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses in Proceedings of the 5th Edition of the International Conference on Language Resources and Evaluation (LREC2006). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Denkowski</author>
<author>A Lavie</author>
</authors>
<title>METEOR 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems</title>
<date>2011</date>
<booktitle>in Proceedings of the 6th Workshop on Statistical Machine Translation (ACL-2011).</booktitle>
<location>Edinburgh, Scotland, UK.</location>
<contexts>
<context position="1375" citStr="Denkowski and Lavie, 2011" startWordPosition="209" endWordPosition="212">e both in their development and improvement. From the MT metrics that have been developed during the last decades, BLEU (Papineni et al., 2002) is one of the most well-known and widely used, since it is fast and easy to use. Nonetheless, researchers such as (Callison-Burch et al., 2006) and (Lavie and Dekowski, 2009) have claimed its weaknesses regarding translation quality and its tendency to favour statistically-based MT systems. As a consequence, other more complex metrics that use linguistic information have been developed. Some use linguistic information at lexical level, such as METEOR (Denkowski and Lavie, 2011); others rely on syntactic information, either using constituent (Liu and Hildea, 2005) or dependency analysis (Owczarzack et al., 2007a and 2007b; He et al., 2010); others use more complex information such as semantic roles (Giménez and Márquez, 2007 and 2008a; Lo et al., 2012). All these metrics focus on partial aspects of language; however, other researchers have tried to combine information at different linguistic levels in order to follow a more holistic approach. Some of these metrics follow a machine-learning approach (Leusch and Ney, 2009; Albrecht and Hwa, 2007a and 2007b), others com</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>M. J. Denkowski and A. Lavie. 2011. METEOR 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems in Proceedings of the 6th Workshop on Statistical Machine Translation (ACL-2011). Edinburgh, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Màrquez</author>
</authors>
<title>Linguistic features for automatic evaluation of heterogeneous MT systems</title>
<date>2007</date>
<booktitle>in Proceedings of the 2nd Workshop on Statistical Machine Translation (ACL),</booktitle>
<location>Prague, Czech Repubilc.</location>
<marker>Màrquez, 2007</marker>
<rawString>J. Giménez and Ll. Màrquez. 2007. Linguistic features for automatic evaluation of heterogeneous MT systems in Proceedings of the 2nd Workshop on Statistical Machine Translation (ACL), Prague, Czech Repubilc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Màrquez</author>
</authors>
<title>A smorgasbord of features for automatic MT evaluation</title>
<date>2008</date>
<booktitle>in Proceedings of the 3rd Workshop on Statistical Machine Translation (ACL).</booktitle>
<location>Columbus. OH.</location>
<marker>Màrquez, 2008</marker>
<rawString>J. Giménez and Ll. Màrquez. 2008. A smorgasbord of features for automatic MT evaluation in Proceedings of the 3rd Workshop on Statistical Machine Translation (ACL). Columbus. OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gimenez</author>
</authors>
<title>Empirical Machine Translation and its Evaluation. Doctoral Dissertation.</title>
<date>2008</date>
<publisher>UPC.</publisher>
<marker>Gimenez, 2008</marker>
<rawString>J. Gimenez. 2008. Empirical Machine Translation and its Evaluation. Doctoral Dissertation. UPC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Márquez</author>
</authors>
<title>Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation,</title>
<date>2010</date>
<pages>24--3</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2084" citStr="Márquez, 2010" startWordPosition="324" endWordPosition="325">dency analysis (Owczarzack et al., 2007a and 2007b; He et al., 2010); others use more complex information such as semantic roles (Giménez and Márquez, 2007 and 2008a; Lo et al., 2012). All these metrics focus on partial aspects of language; however, other researchers have tried to combine information at different linguistic levels in order to follow a more holistic approach. Some of these metrics follow a machine-learning approach (Leusch and Ney, 2009; Albrecht and Hwa, 2007a and 2007b), others combine a wide variety of metrics in a simple and straightforward way (Giménez, 2008b; Giménez and Márquez, 2010; Specia and Gimé- nez, 2010). However, very little research has been performed on the impact of the linguistic features used and how to combine this information from a linguistic point of view. Hence, our proposal is a linguistically-based metric, VERTa (Comelles et al., 2012), which uses a wide variety of linguistic features at different levels, and aims at combining them in order to provide a wider and more accurate coverage than those metrics working at a specific linguistic level. In this paper we provide a description of the linguistic information used in VERTa, the different modules tha</context>
</contexts>
<marker>Márquez, 2010</marker>
<rawString>J. Giménez and Ll. Márquez. 2010. Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation, 24(3–4),77–86. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hachey</author>
<author>W Radford</author>
<author>J R Curran</author>
</authors>
<title>Graph-based named entity linking with Wikipedia</title>
<date>2011</date>
<booktitle>in Proceedings of the 12th International conference on Web information system engineering,</booktitle>
<pages>213--226</pages>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Hachey, Radford, Curran, 2011</marker>
<rawString>B. Hachey, W. Radford and J. R. Curran. 2011. Graph-based named entity linking with Wikipedia in Proceedings of the 12th International conference on Web information system engineering, pages 213-226, Springer-Verlag, Berlin, Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y He</author>
<author>J Du</author>
<author>A Way</author>
<author>J van Genabith</author>
</authors>
<title>The DCU Dependency-based Metric in WMT-Metrics MATR</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR (WMT 2010),</booktitle>
<location>Uppsala,</location>
<marker>He, Du, Way, van Genabith, 2010</marker>
<rawString>Y. He, J. Du, A. Way and J. van Genabith. 2010. The DCU Dependency-based Metric in WMT-Metrics MATR 2010. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and Metrics MATR (WMT 2010), Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>M J Denkowski</author>
</authors>
<title>The METEOR Metric for Automatic Evaluation of Machine Translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<volume>23</volume>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>A. Lavie and M. J. Denkowski. 2009. The METEOR Metric for Automatic Evaluation of Machine Translation. Machine Translation, 23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leusch</author>
<author>H Ney</author>
</authors>
<title>BLEUSP, INVWER, CDER: Three improved MT evaluation measures.</title>
<date>2008</date>
<booktitle>In NIST Metrics for Machine Translation 2008 Evaluation (MericsMATR08),</booktitle>
<location>Waikiki, Honolulu, Hawaii,</location>
<marker>Leusch, Ney, 2008</marker>
<rawString>G. Leusch and H. Ney. 2008. BLEUSP, INVWER, CDER: Three improved MT evaluation measures. In NIST Metrics for Machine Translation 2008 Evaluation (MericsMATR08), Waikiki, Honolulu, Hawaii, October 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Liu</author>
<author>D Hildea</author>
</authors>
<title>Syntactic Features for Evaluation of Machine Translation</title>
<date>2005</date>
<booktitle>in Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<location>Ann Arbor</location>
<contexts>
<context position="1462" citStr="Liu and Hildea, 2005" startWordPosition="222" endWordPosition="225">ring the last decades, BLEU (Papineni et al., 2002) is one of the most well-known and widely used, since it is fast and easy to use. Nonetheless, researchers such as (Callison-Burch et al., 2006) and (Lavie and Dekowski, 2009) have claimed its weaknesses regarding translation quality and its tendency to favour statistically-based MT systems. As a consequence, other more complex metrics that use linguistic information have been developed. Some use linguistic information at lexical level, such as METEOR (Denkowski and Lavie, 2011); others rely on syntactic information, either using constituent (Liu and Hildea, 2005) or dependency analysis (Owczarzack et al., 2007a and 2007b; He et al., 2010); others use more complex information such as semantic roles (Giménez and Márquez, 2007 and 2008a; Lo et al., 2012). All these metrics focus on partial aspects of language; however, other researchers have tried to combine information at different linguistic levels in order to follow a more holistic approach. Some of these metrics follow a machine-learning approach (Leusch and Ney, 2009; Albrecht and Hwa, 2007a and 2007b), others combine a wide variety of metrics in a simple and straightforward way (Giménez, 2008b; Gim</context>
</contexts>
<marker>Liu, Hildea, 2005</marker>
<rawString>D. Liu and D. Hildea. 2005. Syntactic Features for Evaluation of Machine Translation in Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Ann Arbor</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ch Lo</author>
<author>D Wu</author>
</authors>
<title>Semantic vs. Syntactic vs. Ngram Structure for Machine Translation Evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 4th Workshop on Syntax Semantics and Structure in Statistical Translation.</booktitle>
<location>Beijing.</location>
<contexts>
<context position="12493" citStr="Lo and Wu, 2010" startWordPosition="1998" endWordPosition="2001">e lexical similarity metric, which allows us to work not only with wordforms but also with synonyms, lemmas, partiallemmas, hypernyms and hyponyms as shown in Example 3, where the chunks [the situation in the area] and [the situation in the region] do match, even though area and region do not share the same word-form but a relation of synonymy. Example 3: HYP: ... the situation in the area... REF: ... the situation in the region... 3.5 Semantics similarity module As confirmed by the lexical module, semantics plays an important role in the evaluation of adequacy. This has also been claimed by (Lo and Wu, 2010) who report that their metric based on semantic roles outperforms other well-known metrics when adequacy is assessed. With this aim in mind the semantic similarity module uses other semantic features at sentence level: NEs, time expressions and polarity. Regarding NEs, we use Named-Entity recognition (NER) and Named-Entity linking (NEL). Following previous NE-based metrics (Reeder et al., 2011 and Giménez, 2008) the NER metric6 aims at capturing similarities between NEs in the hypothesis and reference segments. On the other hand NEL7 focuses only on those NEs that appear on Wikipedia, which al</context>
</contexts>
<marker>Lo, Wu, 2010</marker>
<rawString>Ch.Lo and D. Wu. 2010. Semantic vs. Syntactic vs. Ngram Structure for Machine Translation Evaluation. In Proceedings of the 4th Workshop on Syntax Semantics and Structure in Statistical Translation. Beijing. China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Tumurulu Lo</author>
<author>D Wu</author>
</authors>
<title>Fully Automatic Semantic MT Evaluation.</title>
<date>2012</date>
<booktitle>Proceedings of the 7th Wrokshop on Statistical Machine Translation,</booktitle>
<location>Montréal, Canada,</location>
<marker>Lo, Wu, 2012</marker>
<rawString>Ch. Lo, A. K. Tumurulu and D. Wu. 2012. Fully Automatic Semantic MT Evaluation. Proceedings of the 7th Wrokshop on Statistical Machine Translation, Montréal, Canada, June 7-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Owczarzak</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<title>Dependency-Based Automatic Evaluation for Machine Translation in</title>
<date>2007</date>
<booktitle>Proceedings of SSST, NAACLHLT/AMTA Workshop on Syntax and Structure I Statistical Translation,</booktitle>
<location>Rochester, New York.</location>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>K. Owczarzak, J. van Genabith and A. Way. 2007. Dependency-Based Automatic Evaluation for Machine Translation in Proceedings of SSST, NAACLHLT/AMTA Workshop on Syntax and Structure I Statistical Translation, Rochester, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Owczarzak</author>
<author>J van Genabith</author>
<author>A Way</author>
</authors>
<date>2007</date>
<booktitle>Labelled Dependencies in Machine Translation Evaluation in Proceedings of the ACL Workshop on Statistical Machine Translation,</booktitle>
<location>Prague, Czech Republic.</location>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>K. Owczarzak, J. van Genabith and A. Way. 2007. Labelled Dependencies in Machine Translation Evaluation in Proceedings of the ACL Workshop on Statistical Machine Translation, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W Zhu</author>
</authors>
<title>BLEU: A Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL02).</booktitle>
<location>Philadelphia. PA.</location>
<contexts>
<context position="892" citStr="Papineni et al., 2002" startWordPosition="133" endWordPosition="136">ombines linguistic features at different levels. We provide the linguistic motivation on which the metric is based, as well as describe the different modules in VERTa and how they are combined. Finally, we describe the two versions of VERTa, VERTa-EQ and VERTa-W, sent to WMT14 and report results obtained in the experiments conducted with the WMT12 and WMT13 data into English. 1 Introduction In the Machine Translation (MT) process, the evaluation of MT systems plays a key role both in their development and improvement. From the MT metrics that have been developed during the last decades, BLEU (Papineni et al., 2002) is one of the most well-known and widely used, since it is fast and easy to use. Nonetheless, researchers such as (Callison-Burch et al., 2006) and (Lavie and Dekowski, 2009) have claimed its weaknesses regarding translation quality and its tendency to favour statistically-based MT systems. As a consequence, other more complex metrics that use linguistic information have been developed. Some use linguistic information at lexical level, such as METEOR (Denkowski and Lavie, 2011); others rely on syntactic information, either using constituent (Liu and Hildea, 2005) or dependency analysis (Owcza</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward and W. Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL02). Philadelphia. PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Reeder</author>
<author>K Miller</author>
<author>J Doyon</author>
<author>J White</author>
</authors>
<title>did what to whom?” at Machine Translation Summit VIII.</title>
<date>2001</date>
<booktitle>The Naming of Things and the Confusion of Tongues: an MT Metric. Proceedings of the Workshop on MT Evaluation ”Who</booktitle>
<marker>Reeder, Miller, Doyon, White, 2001</marker>
<rawString>F. Reeder, K. Miller, J. Doyon and J. White. 2001. The Naming of Things and the Confusion of Tongues: an MT Metric. Proceedings of the Workshop on MT Evaluation ”Who did what to whom?” at Machine Translation Summit VIII.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>J Giménez</author>
</authors>
<title>Combining Confidence Estimation and Reference-based Metrics for Segment-level MT Evaluation.</title>
<date>2010</date>
<booktitle>The Ninth Conference of the Association for Machine Translation in the Americas (AMTA 2010),</booktitle>
<location>Denver, Colorado.</location>
<marker>Specia, Giménez, 2010</marker>
<rawString>L. Specia and J. Giménez. 2010. Combining Confidence Estimation and Reference-based Metrics for Segment-level MT Evaluation. The Ninth Conference of the Association for Machine Translation in the Americas (AMTA 2010), Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wetzel</author>
<author>F Bond</author>
</authors>
<title>Enriching Parallel Corpora for Statistical Machine Translation with Semantic Negation Rephrasing.</title>
<date>2012</date>
<booktitle>Proceedings of SSST6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation,</booktitle>
<location>Jeju, Republic of</location>
<contexts>
<context position="14043" citStr="Wetzel and Bond, 2012" startWordPosition="2255" endWordPosition="2258">rm. The tool used is the Stanford Temporal Tagger (Chang and Manning, 2012) which recognizes not only points in time but also duration. By means of this metric, different syntactic structures conveying the same time expression can be 6 In order to identify NEs we use the Supersense Tagger (Ciaramita and Altun, 2006). 7 The NEL module uses a graph-based NEL tool (Hachey, Radford and Curran, 2010) which links NEs in a text with those in Wikipedia pages. matched, such as on February 3rd and on the third of February. Finally, it has been reported that negation might pose a problem to SMT systems (Wetzel and Bond, 2012). In order to answer such need, a module that checks the polarity of the sentence has been added using the dictionary strategy described (Atserias et al., 2012): Adding 0.5 for each weak positive word. Adding 1.0 for each strong positive word. Subtracting 0.5 for each weak negative word. Subtracting 1.0 for each strong negative word. For each query term score, the value is propagated to the query term positions by reducing its strength in a factor of 1/n, where n is the distance between the query term and the polar term. According to the experiments performed, this module shows a low correlati</context>
</contexts>
<marker>Wetzel, Bond, 2012</marker>
<rawString>D. Wetzel and F. Bond. 2012. Enriching Parallel Corpora for Statistical Machine Translation with Semantic Negation Rephrasing. Proceedings of SSST6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, Jeju, Republic of Korea.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025251">
<title confidence="0.999143">
Extracting Socioeconomic Patterns from the News: Modelling Text and
Outlet Importance Jointly
</title>
<author confidence="0.994627">
Vasileios Lampos1, Daniel Preot¸iuc-Pietro2, Sina Samangooei3,
Douwe Gelling2, and Trevor Cohn4
</author>
<affiliation confidence="0.981631">
1 Department of Computer Science, University College London — v.lampos@ucl.ac.uk
2 Department of Computer Science, The University of Sheffield — {d.preotiuc,d.gelling}@shef.ac.uk
3 Electronics and Computer Science, University of Southampton — ss@ecs.soton.ac.uk
</affiliation>
<email confidence="0.404964">
4 Computing and Information Systems, The University of Melbourne — t.cohn@unimelb.edu.au
</email>
<sectionHeader confidence="0.978093" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934619047619">
Information from news articles can be used
to study correlations between textual dis-
course and socioeconomic patterns. This
work focuses on the task of understanding
how words contained in the news as well as
the news outlets themselves may relate to
a set of indicators, such as economic senti-
ment or unemployment rates. The bilinear
nature of the applied regression model fa-
cilitates learning jointly word and outlet
importance, supervised by these indicators.
By evaluating the predictive ability of the
extracted features, we can also assess their
relevance to the target socioeconomic phe-
nomena. Therefore, our approach can be
formulated as a potential NLP tool, partic-
ularly suitable to the computational social
science community, as it can be used to in-
terpret connections between vast amounts
of textual content and measurable society-
driven factors.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999405689655173">
Vast amounts of user-generated content on the Inter-
net as well as digitised textual resources allow us to
study text in connection to real world events across
large intervals of time. Over the last decade, there
has been a shift in user news consumption starting
with a move from offline to online sources (Lin
et al., 2005); in more recent years user-generated
news have also become prominent. However, tra-
ditional news outlets continue to be a central refer-
ence point (Nah and Chung, 2012) as they still have
the advantage of being professionally authored, al-
leviating the noisy nature of citizen journalism for-
mats.
Here, we present a framework for analysing so-
cioeconomic patterns in news articles. In contrast
to prior approaches, which primarily focus on the
textual contents, our analysis shows how Machine
Learning methods can be used to gain insights into
the interplay between text in news articles, the news
outlets and socioeconomic indicators. Our experi-
ments are performed on a set of EU-related news
summaries spanning over 8 years, with the inten-
tion to study two basic economic factors: EU’s
unemployment rate and Economic Sentiment Index
(ESI) (European Commision, 1997). To determine
connections between the news, the outlets and the
indicators of interest, we formulate our learning
task as bilinear text-based regression (Lampos et
al., 2013).
Approaches to learning the correlation of news,
or text in general, with real world indicators have
been performed in both unsupervised and super-
vised settings. For example, Flaounas et al. (2010)
uncover interesting patterns in EU’s Mediasphere,
whereas Schumaker and Chen (2009) demonstrate
that news articles can predict financial indicators.
Conversely, Bentley et al. (2014) show that emo-
tions in the textual content of books reflect back
on inflation and unemployment rates during the
20th century. Recently, Social Media text has been
intensively studied as a quicker, unobtrusive and
cheaper alternative to traditional surveys. Applica-
tion areas include politics (O’Connor et al., 2010),
finance (Bollen and Mao, 2011), health (Lampos
and Cristianini, 2012; Paul and Dredze, 2011) or
psychology (De Choudhury et al., 2013; Schwartz
et al., 2013).
In this paper, we apply a modified version of a
bilinear regularised regression model (BEN) pro-
posed for the task of voting intention inference
from Twitter content (Lampos et al., 2013). The
main characteristic of BEN is the ability of mod-
elling word frequencies as well as individual user
importance in a joint optimisation task. By apply-
ing it in the context of supervised news analysis,
we are able to visualise relevant discourse to a par-
ticular socioeconomic factor, identifying relevant
words together with important outlets.
</bodyText>
<page confidence="0.991469">
13
</page>
<note confidence="0.43682">
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 13–17,
Baltimore, Maryland, USA, June 26, 2014. c�2014 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.982122" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999995175">
We compiled a data set by crawling summaries
on news articles written in English language, pub-
lished by the Open Europe Think Tank.1 The press
summaries are daily aggregations of news items
about the EU or member countries with a focus
on politics; the news outlets used to compile each
summary are listed below the summary’s text. The
site is updated every weekday, with the major news
being covered in a couple of paragraphs, and other
less prevalent issues being mentioned in one para-
graph to as little as one sentence. The news sum-
maries were first published on February 2006; we
collected all of them up to mid-November 2013,
creating a data set with the temporal resolution of
1913 days (or 94 months).
The text was tokenised using the NLTK li-
brary (Bird et al., 2009). News outlets with fewer
than 5 mentions were removed, resulting in a total
of 435 sources. Each summary contains on average
14 news items, with an average of 3 news sources
per item; where multiple sources were present, the
summary was assigned to all the referenced news
outlets. After removing stop words, we ended up
with 8,413 unigrams and 19,045 bigrams; their
daily occurrences were normalised using the total
number of news items for that day.
For the purposes of our supervised analysis, we
use the response variables of ESI and unemploy-
ment rate across the EU. The monthly time series
of these socioeconomic indicators were retrieved
from Eurostat, EU’s statistical office (see the red
lines in Fig. 1a and 1b respectively). ESI is a com-
posite indicator often seen as an early predictor for
future economic developments (Gelper and Croux,
2010). It consists of five confidence indicators with
different weights: industrial (40%), services (30%),
consumer (20%), construction (5%) and retail trade
(5%). The unemployment rate is a seasonally ad-
justed ratio of the non employed persons over the
entire EU labour force.2
</bodyText>
<sectionHeader confidence="0.994747" genericHeader="method">
3 Models
</sectionHeader>
<bodyText confidence="0.999912">
A common approach to regression arises through
the application of generalised linear models. These
models use a feature vector input x and aim to build
a linear function of x for predicting a response
</bodyText>
<footnote confidence="0.93478">
1http://www.openeurope.org.uk/Page/
PressSummary/en/
2http://epp.eurostat.ec.europa.
eu/statistics_explained/index.php/
Unemployment_statistics
</footnote>
<equation confidence="0.982787">
variable y:
f(x) = xTw + β where x,w E Rm. (1)
</equation>
<bodyText confidence="0.997084652173913">
The objective is to find an f, which minimises a
model-dependent loss function (e.g. sum squared
error), optionally subject to a regularisation penalty
ψ; `2-norm regularisation (ridge regression) pe-
nalises high weights (Hoerl and Kennard, 1970),
while `1-norm regularisation (lasso) encourages
sparse solutions (Tibshirani, 1994). Sparsity is de-
sirable for avoiding overfitting, especially when
the dimensionality m is larger than the number of
training examples n (Hastie et al., 2009). Elastic
Net formulates a combination of `1 and `2-norm
regularisation defined by the objective:
+ ψEN(w, ρ) ,
where ρ denotes the regularisation parameters (Zou
and Hastie, 2005); we refer to this model as LEN
(Linear Elastic Net) in the remainder of the script.
In the context of voting intention inference from
Twitter content, Lampos et al. (2013) extended
LEN to a bilinear formulation, where a set of two
vector weights are learnt: one for words (w) and
one for users (u). This was motivated by the ob-
servation that only a sparse set of users may have
predictive value. The model now becomes:
</bodyText>
<equation confidence="0.995319">
f(X) = uTXw + β , (3)
</equation>
<bodyText confidence="0.98764">
where X is a matrix of word x users frequencies.
The bilinear optimisation objective is formulated
as:
</bodyText>
<equation confidence="0.9589872">
n
{w*,u*, β*} = argmin (uTXiw + β − yi)2
w,u,β i=1
+ ψEN(w, ρ1) + ψEN(u, ρ2) ,
(4)
</equation>
<bodyText confidence="0.9998426">
where Xi is the word x user frequency matrix, and
ρ1, ρ2 are the word and user regularisation param-
eters. This can be treated as a biconvex learning
task and be solved by iterating over two convex
processes: fixing w and learning u, and vice versa
(Lampos et al., 2013). Regularised regression on
both user and word spaces allows for an automatic
selection of the most important words and users,
performing at the same time an improved noise
filtering.
</bodyText>
<equation confidence="0.992934666666667">
n
{w*, β*} = argmin
w,β i=1
(XT.
aT •w+ 2
z Q—yi) (2)
</equation>
<page confidence="0.971391">
14
</page>
<bodyText confidence="0.999630888888889">
In our experiments, news outlets and socioeco-
nomic indicators replace users and voting intention
in the previous model formulation. To ease the in-
terpretation of the outputs, we further impose a
positivity constraint on the outlet weights u, i.e.
min(u) ≥ 0; this makes the model more restric-
tive, but, in our case, did not affect the prediction
performance. We refer to this model as BEN (Bi-
linear Elastic Net).
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999411146341463">
Both models are applied to the news summaries
data set with the aim to predict EU’s ESI and rate
of unemployment. The predictive capability of the
derived models, assessed by their respective infer-
ence performance, is used as a metric for judging
the degree of relevance between the learnt model
parameters – word and outlet weights – and the
response variable. A strong predictive performance
increases confidence on the soundness of those pa-
rameters.
To match input with the monthly temporal reso-
lution of the response variables, we compute the
mean monthly term frequencies for each outlet.
Evaluation is performed via a 10-fold validation,
where each fold’s training set is based on a mov-
ing window of p = 64 contiguous months, and the
test set consists of the following q = 3 months;
formally, the training and test sets for fold i are
based on months {q(i − 1) + 1, ..., q(i − 1) + p}
and {q(i − 1) + p + 1, ..., q(i − 1) + p + q} re-
spectively. In this way, we emulate a scenario
where we always train on past and predict future
points.
Performance results for LEN and BEN are pre-
sented in Table 1; we show the average Root Mean
Squared Error (RMSE) as well as an error rate
(RMSE over µ(y)) across folds to allow for a bet-
ter interpretation. BEN outperforms LEN in both
tasks, with a clearer improvement when predict-
ing ESI. Predictions for all folds are depicted in
Fig. 1a and 1b together with the actual values. Note
that reformulating the problem into a multi-task
learning scenario, where ESI and unemployment
are modelled jointly did not improve inference per-
formance.
The relatively small average error rates (&lt; 8.8%)
make meaningful a further analysis of the model’s
outputs. Due to space limitations, we choose to fo-
cus on the most recent results, depicting the models
derived in the 10th fold. Following the example of
Schwartz et al. (2013), we use a word cloud visu-
</bodyText>
<table confidence="0.944670333333333">
ESI Unemployment
LEN 9.253 (9.89%) 0.9275 (8.75%)
BEN 8.209 (8.77%) 0.9047 (8.52%)
</table>
<tableCaption confidence="0.931148666666667">
Table 1: 10-fold validation average RMSEs (and
error rates) for LEN and BEN on ESI and unem-
ployment rates prediction.
</tableCaption>
<figure confidence="0.988179428571429">
100
actual
predictions
2007 2008 2009 2010 2011 2012 2013
(a) ESI
2007 2008 2009 2010 2011 2012 2013
(b) Unemployment
</figure>
<figureCaption confidence="0.950984">
Figure 1: Time series of ESI and unemployment
together with BEN predictions (smoothed using a
3-point moving average).
</figureCaption>
<bodyText confidence="0.997168142857143">
alisation, where the font size is proportional to the
derived weights by applying BEN, flipped terms de-
note negative weights and colours are determined
by the frequency of use in the corpus (Fig. 2). Word
clouds depict the top-60 positively and negatively
weighted n-grams (120 in total) together with the
top-30 outlets; bigrams are separated by ‘ ’.
</bodyText>
<sectionHeader confidence="0.990941" genericHeader="conclusions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.9998995">
Our visualisations (Fig. 2) present various inter-
esting insights into the news and socioeconomic
features being explored, serving as a demonstra-
tion of the potential power of the proposed mod-
elling. Firstly, we notice that in the word cloud,
the size of a feature (BEN’s weight) is not tightly
connected with its colour (frequency in the corpus).
Also, the word clouds suggest that mostly different
terms and outlets are selected for the two indicators.
For example, ‘sky.it’ is predominant for ESI but
not for unemployment, while the opposite is true
for ‘hedgefundsreview.com’. Some of the words
selected for ESI reflect economical issues, such as
‘stimulus’ and ‘spending’, whereas key politicians
</bodyText>
<figure confidence="0.985839647058824">
actual
predictions
50
0
10
5
0
15
Frequency
Word
Outlet
Weight
a a
Polarity
Yes+ -
(a) ESI
(b) Unemployment
</figure>
<figureCaption confidence="0.999902">
Figure 2: Word clouds for words and outlets visualising the outputs of BEN.
</figureCaption>
<bodyText confidence="0.999963904761905">
like ‘david cameron’ and ‘berlusconi’, are major
participants in the word cloud for unemployment.
In addition, the visualisations show a strong neg-
ative relationship between unemployment and the
terms ‘food’, ‘russia’ and ‘agriculture’, but no such
relationship with respect to ESI. The disparity of
these selections is evidence for our framework’s
capability to highlight features of lesser or greater
importance to a given socioeconomic time series.
The exact interpretation of the selected words and
outlets is, perhaps, context-dependent and beyond
the scope of this work.
In this paper, we presented a framework for per-
forming a supervised analysis on news. An impor-
tant factor for this process is that the bilinear nature
of the learning function allows for a joint selection
of important words and news outlets. Prediction
performance is used as a reference point for de-
termining whether the extracted outputs (i.e. the
model’s parameters) encapsulate relevant informa-
tion regarding to the given indicator. Experiments
were conducted on a set of EU-related news sum-
maries and the supervising socioeconomic factors
were the EU-wide ESI and unemployment. BEN
outperformed the linear alternative (LEN), produc-
ing error rates below 8.8%.
The performance of our framework motivates
several extensions to be explored in future work.
Firstly, the incorporation of additional textual fea-
tures may improve predictive capability and allow
for richer interpretations of the term weights. For
example, we could extend our term vocabulary us-
ing n-grams with n &gt; 2, POS tags of words and
entities (people, companies, places, etc.). Further-
more, multi-task learning approaches as well as
models which incorporate the regularised learning
of weights for different countries might give us fur-
ther insights into the relationship between news,
geographic location and socioeconomic indicators.
Most importantly, we plan to gain a better under-
standing of the outputs by conducting a thorough
analysis in collaboration with domain experts.
</bodyText>
<page confidence="0.996494">
16
</page>
<sectionHeader confidence="0.99652" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99977225">
VL acknowledges the support from the EPSRC
IRC project EP/K031953/1. DPP, SS, DG and TC
were supported by EU-FP7-ICT project n.287863
(“TrendMiner”).
</bodyText>
<sectionHeader confidence="0.998504" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999469253521127">
R. Alexander Bentley, Alberto Acerbi, Paul Ormerod,
and Vasileios Lampos. 2014. Books average previ-
ous decade of economic misery. PLoS ONE, 9(1).
Steven Bird, Ewan Klein, and Edward Loper. 2009.
Natural Language Processing with Python. O’Reilly
Media.
Johan Bollen and Huina Mao. 2011. Twitter mood as a
stock market predictor. IEEE Computer, 44(10):91–
94.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013. Social media as a measurement tool
of depression in populations. In Proceedings ofACM
WebSci’13, pages 47–56.
European Commision. 1997. The joint harmonised EU
programme of business and consumer surveys. Euro-
pean economy: Reports and studies.
Ilias Flaounas, Marco Turchi, Omar Ali, Nick Fyson,
Tijl De Bie, Nick Mosdell, Justin Lewis, and Nello
Cristianini. 2010. The Structure of the EU Medias-
phere. PLoS ONE, 5(12), 12.
Sarah Gelper and Christophe Croux. 2010. On the con-
struction of the European Economic Sentiment Indi-
cator. Oxford Bulletin of Economics and Statistics,
72(1):47–62.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
2009. The Elements of Statistical Learning: Data
Mining, Inference, and Prediction. Springer.
Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge
regression: biased estimation for nonorthogonal prob-
lems. Technometrics, 12:55–67.
Vasileios Lampos and Nello Cristianini. 2012. Now-
casting events from the Social Web with statistical
learning. ACM TIST, 3(4):72:1–72:22.
Vasileios Lampos, Daniel Preot¸iuc-Pietro, and Trevor
Cohn. 2013. A user-centric model of voting inten-
tion from Social Media. In Proceedings of ACL’13,
pages 993–1003.
Carolyn Lin, Michael B. Salwen, Bruce Garrison, and
Paul D. Driscoll. 2005. Online news as a functional
substitute for offline news. Online news and the pub-
lic, pages 237–255.
Seungahn Nah and Deborah S. Chung. 2012. When cit-
izens meet both professional and citizen journalists:
Social trust, media credibility, and perceived journal-
istic roles among online community news readers.
Journalism, 13(6):714–730.
Brendan O’Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: linking text sentiment to
public opinion time series. In Proceedings of AAAI
ICWSM’10, pages 122–129.
Michael J. Paul and Mark Dredze. 2011. You
Are What You Tweet: Analyzing Twitter for Public
Health. In Proceedings of AAAI ICWSM’11, pages
265–272.
Robert P. Schumaker and Hsinchun Chen. 2009. Tex-
tual analysis of stock market prediction using break-
ing financial news: the AZFin text system. ACM
TOIS, 27(2):12:1–12:19.
H. Andrew Schwartz, Johannes C. Eichstaedt, Mar-
garet L. Kern, Lukasz Dziurzynski, Stephanie M. Ra-
mones, Megha Agrawal, Achal Shah, Michal Kosin-
ski, David Stillwell, Martin E. P. Seligman, and
Lyle H. Ungar. 2013. Personality, Gender, and
Age in the Language of Social Media: The Open-
Vocabulary Approach. PLoS ONE, 8(9).
Robert Tibshirani. 1994. Regression shrinkage and
selection via the lasso. JRSS: Series B, 58:267–288.
Hui Zou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. JRSS: Series B,
67(2):301–320.
</reference>
<page confidence="0.999395">
17
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.583783">
<title confidence="0.992052">Extracting Socioeconomic Patterns from the News: Modelling Text Outlet Importance Jointly</title>
<author confidence="0.999953">Daniel Sina</author>
<affiliation confidence="0.848425">of Computer Science, University College London — of Computer Science, The University of Sheffield — and Computer Science, University of Southampton — and Information Systems, The University of Melbourne —</affiliation>
<abstract confidence="0.996919227272727">Information from news articles can be used to study correlations between textual discourse and socioeconomic patterns. This work focuses on the task of understanding how words contained in the news as well as the news outlets themselves may relate to a set of indicators, such as economic sentiment or unemployment rates. The bilinear nature of the applied regression model facilitates learning jointly word and outlet importance, supervised by these indicators. By evaluating the predictive ability of the extracted features, we can also assess their relevance to the target socioeconomic phenomena. Therefore, our approach can be formulated as a potential NLP tool, particularly suitable to the computational social science community, as it can be used to interpret connections between vast amounts of textual content and measurable societydriven factors.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Alexander Bentley</author>
<author>Alberto Acerbi</author>
<author>Paul Ormerod</author>
<author>Vasileios Lampos</author>
</authors>
<title>Books average previous decade of economic misery.</title>
<date>2014</date>
<journal>PLoS ONE,</journal>
<volume>9</volume>
<issue>1</issue>
<contexts>
<context position="3167" citStr="Bentley et al. (2014)" startWordPosition="472" endWordPosition="475">ate and Economic Sentiment Index (ESI) (European Commision, 1997). To determine connections between the news, the outlets and the indicators of interest, we formulate our learning task as bilinear text-based regression (Lampos et al., 2013). Approaches to learning the correlation of news, or text in general, with real world indicators have been performed in both unsupervised and supervised settings. For example, Flaounas et al. (2010) uncover interesting patterns in EU’s Mediasphere, whereas Schumaker and Chen (2009) demonstrate that news articles can predict financial indicators. Conversely, Bentley et al. (2014) show that emotions in the textual content of books reflect back on inflation and unemployment rates during the 20th century. Recently, Social Media text has been intensively studied as a quicker, unobtrusive and cheaper alternative to traditional surveys. Application areas include politics (O’Connor et al., 2010), finance (Bollen and Mao, 2011), health (Lampos and Cristianini, 2012; Paul and Dredze, 2011) or psychology (De Choudhury et al., 2013; Schwartz et al., 2013). In this paper, we apply a modified version of a bilinear regularised regression model (BEN) proposed for the task of voting </context>
</contexts>
<marker>Bentley, Acerbi, Ormerod, Lampos, 2014</marker>
<rawString>R. Alexander Bentley, Alberto Acerbi, Paul Ormerod, and Vasileios Lampos. 2014. Books average previous decade of economic misery. PLoS ONE, 9(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media.</booktitle>
<contexts>
<context position="5159" citStr="Bird et al., 2009" startWordPosition="797" endWordPosition="800"> daily aggregations of news items about the EU or member countries with a focus on politics; the news outlets used to compile each summary are listed below the summary’s text. The site is updated every weekday, with the major news being covered in a couple of paragraphs, and other less prevalent issues being mentioned in one paragraph to as little as one sentence. The news summaries were first published on February 2006; we collected all of them up to mid-November 2013, creating a data set with the temporal resolution of 1913 days (or 94 months). The text was tokenised using the NLTK library (Bird et al., 2009). News outlets with fewer than 5 mentions were removed, resulting in a total of 435 sources. Each summary contains on average 14 news items, with an average of 3 news sources per item; where multiple sources were present, the summary was assigned to all the referenced news outlets. After removing stop words, we ended up with 8,413 unigrams and 19,045 bigrams; their daily occurrences were normalised using the total number of news items for that day. For the purposes of our supervised analysis, we use the response variables of ESI and unemployment rate across the EU. The monthly time series of t</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. O’Reilly Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bollen</author>
<author>Huina Mao</author>
</authors>
<title>Twitter mood as a stock market predictor.</title>
<date>2011</date>
<journal>IEEE Computer,</journal>
<volume>44</volume>
<issue>10</issue>
<pages>94</pages>
<contexts>
<context position="3514" citStr="Bollen and Mao, 2011" startWordPosition="525" endWordPosition="528">performed in both unsupervised and supervised settings. For example, Flaounas et al. (2010) uncover interesting patterns in EU’s Mediasphere, whereas Schumaker and Chen (2009) demonstrate that news articles can predict financial indicators. Conversely, Bentley et al. (2014) show that emotions in the textual content of books reflect back on inflation and unemployment rates during the 20th century. Recently, Social Media text has been intensively studied as a quicker, unobtrusive and cheaper alternative to traditional surveys. Application areas include politics (O’Connor et al., 2010), finance (Bollen and Mao, 2011), health (Lampos and Cristianini, 2012; Paul and Dredze, 2011) or psychology (De Choudhury et al., 2013; Schwartz et al., 2013). In this paper, we apply a modified version of a bilinear regularised regression model (BEN) proposed for the task of voting intention inference from Twitter content (Lampos et al., 2013). The main characteristic of BEN is the ability of modelling word frequencies as well as individual user importance in a joint optimisation task. By applying it in the context of supervised news analysis, we are able to visualise relevant discourse to a particular socioeconomic factor</context>
</contexts>
<marker>Bollen, Mao, 2011</marker>
<rawString>Johan Bollen and Huina Mao. 2011. Twitter mood as a stock market predictor. IEEE Computer, 44(10):91– 94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Munmun De Choudhury</author>
<author>Scott Counts</author>
<author>Eric Horvitz</author>
</authors>
<title>Social media as a measurement tool of depression in populations.</title>
<date>2013</date>
<booktitle>In Proceedings ofACM WebSci’13,</booktitle>
<pages>47--56</pages>
<marker>De Choudhury, Counts, Horvitz, 2013</marker>
<rawString>Munmun De Choudhury, Scott Counts, and Eric Horvitz. 2013. Social media as a measurement tool of depression in populations. In Proceedings ofACM WebSci’13, pages 47–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>European Commision</author>
</authors>
<title>The joint harmonised EU programme of business and consumer surveys. European economy: Reports and studies.</title>
<date>1997</date>
<contexts>
<context position="2611" citStr="Commision, 1997" startWordPosition="393" endWordPosition="394">he noisy nature of citizen journalism formats. Here, we present a framework for analysing socioeconomic patterns in news articles. In contrast to prior approaches, which primarily focus on the textual contents, our analysis shows how Machine Learning methods can be used to gain insights into the interplay between text in news articles, the news outlets and socioeconomic indicators. Our experiments are performed on a set of EU-related news summaries spanning over 8 years, with the intention to study two basic economic factors: EU’s unemployment rate and Economic Sentiment Index (ESI) (European Commision, 1997). To determine connections between the news, the outlets and the indicators of interest, we formulate our learning task as bilinear text-based regression (Lampos et al., 2013). Approaches to learning the correlation of news, or text in general, with real world indicators have been performed in both unsupervised and supervised settings. For example, Flaounas et al. (2010) uncover interesting patterns in EU’s Mediasphere, whereas Schumaker and Chen (2009) demonstrate that news articles can predict financial indicators. Conversely, Bentley et al. (2014) show that emotions in the textual content o</context>
</contexts>
<marker>Commision, 1997</marker>
<rawString>European Commision. 1997. The joint harmonised EU programme of business and consumer surveys. European economy: Reports and studies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilias Flaounas</author>
<author>Marco Turchi</author>
<author>Omar Ali</author>
</authors>
<title>Nick Fyson, Tijl De Bie, Nick Mosdell, Justin Lewis, and Nello Cristianini.</title>
<date>2010</date>
<booktitle>The Structure of the EU Mediasphere. PLoS ONE,</booktitle>
<volume>5</volume>
<issue>12</issue>
<pages>12</pages>
<contexts>
<context position="2984" citStr="Flaounas et al. (2010)" startWordPosition="448" endWordPosition="451">nomic indicators. Our experiments are performed on a set of EU-related news summaries spanning over 8 years, with the intention to study two basic economic factors: EU’s unemployment rate and Economic Sentiment Index (ESI) (European Commision, 1997). To determine connections between the news, the outlets and the indicators of interest, we formulate our learning task as bilinear text-based regression (Lampos et al., 2013). Approaches to learning the correlation of news, or text in general, with real world indicators have been performed in both unsupervised and supervised settings. For example, Flaounas et al. (2010) uncover interesting patterns in EU’s Mediasphere, whereas Schumaker and Chen (2009) demonstrate that news articles can predict financial indicators. Conversely, Bentley et al. (2014) show that emotions in the textual content of books reflect back on inflation and unemployment rates during the 20th century. Recently, Social Media text has been intensively studied as a quicker, unobtrusive and cheaper alternative to traditional surveys. Application areas include politics (O’Connor et al., 2010), finance (Bollen and Mao, 2011), health (Lampos and Cristianini, 2012; Paul and Dredze, 2011) or psyc</context>
</contexts>
<marker>Flaounas, Turchi, Ali, 2010</marker>
<rawString>Ilias Flaounas, Marco Turchi, Omar Ali, Nick Fyson, Tijl De Bie, Nick Mosdell, Justin Lewis, and Nello Cristianini. 2010. The Structure of the EU Mediasphere. PLoS ONE, 5(12), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Gelper</author>
<author>Christophe Croux</author>
</authors>
<title>On the construction of the European Economic Sentiment Indicator.</title>
<date>2010</date>
<journal>Oxford Bulletin of Economics and Statistics,</journal>
<volume>72</volume>
<issue>1</issue>
<contexts>
<context position="6014" citStr="Gelper and Croux, 2010" startWordPosition="939" endWordPosition="942">y was assigned to all the referenced news outlets. After removing stop words, we ended up with 8,413 unigrams and 19,045 bigrams; their daily occurrences were normalised using the total number of news items for that day. For the purposes of our supervised analysis, we use the response variables of ESI and unemployment rate across the EU. The monthly time series of these socioeconomic indicators were retrieved from Eurostat, EU’s statistical office (see the red lines in Fig. 1a and 1b respectively). ESI is a composite indicator often seen as an early predictor for future economic developments (Gelper and Croux, 2010). It consists of five confidence indicators with different weights: industrial (40%), services (30%), consumer (20%), construction (5%) and retail trade (5%). The unemployment rate is a seasonally adjusted ratio of the non employed persons over the entire EU labour force.2 3 Models A common approach to regression arises through the application of generalised linear models. These models use a feature vector input x and aim to build a linear function of x for predicting a response 1http://www.openeurope.org.uk/Page/ PressSummary/en/ 2http://epp.eurostat.ec.europa. eu/statistics_explained/index.p</context>
</contexts>
<marker>Gelper, Croux, 2010</marker>
<rawString>Sarah Gelper and Christophe Croux. 2010. On the construction of the European Economic Sentiment Indicator. Oxford Bulletin of Economics and Statistics, 72(1):47–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Hastie</author>
<author>Robert Tibshirani</author>
<author>Jerome Friedman</author>
</authors>
<date>2009</date>
<booktitle>The Elements of Statistical Learning: Data Mining, Inference, and Prediction.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="7176" citStr="Hastie et al., 2009" startWordPosition="1103" endWordPosition="1106">://epp.eurostat.ec.europa. eu/statistics_explained/index.php/ Unemployment_statistics variable y: f(x) = xTw + β where x,w E Rm. (1) The objective is to find an f, which minimises a model-dependent loss function (e.g. sum squared error), optionally subject to a regularisation penalty ψ; `2-norm regularisation (ridge regression) penalises high weights (Hoerl and Kennard, 1970), while `1-norm regularisation (lasso) encourages sparse solutions (Tibshirani, 1994). Sparsity is desirable for avoiding overfitting, especially when the dimensionality m is larger than the number of training examples n (Hastie et al., 2009). Elastic Net formulates a combination of `1 and `2-norm regularisation defined by the objective: + ψEN(w, ρ) , where ρ denotes the regularisation parameters (Zou and Hastie, 2005); we refer to this model as LEN (Linear Elastic Net) in the remainder of the script. In the context of voting intention inference from Twitter content, Lampos et al. (2013) extended LEN to a bilinear formulation, where a set of two vector weights are learnt: one for words (w) and one for users (u). This was motivated by the observation that only a sparse set of users may have predictive value. The model now becomes: </context>
</contexts>
<marker>Hastie, Tibshirani, Friedman, 2009</marker>
<rawString>Trevor Hastie, Robert Tibshirani, and Jerome Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arthur E Hoerl</author>
<author>Robert W Kennard</author>
</authors>
<title>Ridge regression: biased estimation for nonorthogonal problems.</title>
<date>1970</date>
<tech>Technometrics,</tech>
<pages>12--55</pages>
<contexts>
<context position="6934" citStr="Hoerl and Kennard, 1970" startWordPosition="1069" endWordPosition="1072">oach to regression arises through the application of generalised linear models. These models use a feature vector input x and aim to build a linear function of x for predicting a response 1http://www.openeurope.org.uk/Page/ PressSummary/en/ 2http://epp.eurostat.ec.europa. eu/statistics_explained/index.php/ Unemployment_statistics variable y: f(x) = xTw + β where x,w E Rm. (1) The objective is to find an f, which minimises a model-dependent loss function (e.g. sum squared error), optionally subject to a regularisation penalty ψ; `2-norm regularisation (ridge regression) penalises high weights (Hoerl and Kennard, 1970), while `1-norm regularisation (lasso) encourages sparse solutions (Tibshirani, 1994). Sparsity is desirable for avoiding overfitting, especially when the dimensionality m is larger than the number of training examples n (Hastie et al., 2009). Elastic Net formulates a combination of `1 and `2-norm regularisation defined by the objective: + ψEN(w, ρ) , where ρ denotes the regularisation parameters (Zou and Hastie, 2005); we refer to this model as LEN (Linear Elastic Net) in the remainder of the script. In the context of voting intention inference from Twitter content, Lampos et al. (2013) exten</context>
</contexts>
<marker>Hoerl, Kennard, 1970</marker>
<rawString>Arthur E. Hoerl and Robert W. Kennard. 1970. Ridge regression: biased estimation for nonorthogonal problems. Technometrics, 12:55–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Nello Cristianini</author>
</authors>
<title>Nowcasting events from the Social Web with statistical learning.</title>
<date>2012</date>
<journal>ACM TIST,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="3552" citStr="Lampos and Cristianini, 2012" startWordPosition="530" endWordPosition="533">and supervised settings. For example, Flaounas et al. (2010) uncover interesting patterns in EU’s Mediasphere, whereas Schumaker and Chen (2009) demonstrate that news articles can predict financial indicators. Conversely, Bentley et al. (2014) show that emotions in the textual content of books reflect back on inflation and unemployment rates during the 20th century. Recently, Social Media text has been intensively studied as a quicker, unobtrusive and cheaper alternative to traditional surveys. Application areas include politics (O’Connor et al., 2010), finance (Bollen and Mao, 2011), health (Lampos and Cristianini, 2012; Paul and Dredze, 2011) or psychology (De Choudhury et al., 2013; Schwartz et al., 2013). In this paper, we apply a modified version of a bilinear regularised regression model (BEN) proposed for the task of voting intention inference from Twitter content (Lampos et al., 2013). The main characteristic of BEN is the ability of modelling word frequencies as well as individual user importance in a joint optimisation task. By applying it in the context of supervised news analysis, we are able to visualise relevant discourse to a particular socioeconomic factor, identifying relevant words together </context>
</contexts>
<marker>Lampos, Cristianini, 2012</marker>
<rawString>Vasileios Lampos and Nello Cristianini. 2012. Nowcasting events from the Social Web with statistical learning. ACM TIST, 3(4):72:1–72:22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Lampos</author>
<author>Daniel Preot¸iuc-Pietro</author>
<author>Trevor Cohn</author>
</authors>
<title>A user-centric model of voting intention from Social Media.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL’13,</booktitle>
<pages>993--1003</pages>
<marker>Lampos, Preot¸iuc-Pietro, Cohn, 2013</marker>
<rawString>Vasileios Lampos, Daniel Preot¸iuc-Pietro, and Trevor Cohn. 2013. A user-centric model of voting intention from Social Media. In Proceedings of ACL’13, pages 993–1003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carolyn Lin</author>
<author>Michael B Salwen</author>
<author>Bruce Garrison</author>
<author>Paul D Driscoll</author>
</authors>
<title>Online news as a functional substitute for offline news. Online news and the public,</title>
<date>2005</date>
<pages>237--255</pages>
<contexts>
<context position="1747" citStr="Lin et al., 2005" startWordPosition="255" endWordPosition="258">oeconomic phenomena. Therefore, our approach can be formulated as a potential NLP tool, particularly suitable to the computational social science community, as it can be used to interpret connections between vast amounts of textual content and measurable societydriven factors. 1 Introduction Vast amounts of user-generated content on the Internet as well as digitised textual resources allow us to study text in connection to real world events across large intervals of time. Over the last decade, there has been a shift in user news consumption starting with a move from offline to online sources (Lin et al., 2005); in more recent years user-generated news have also become prominent. However, traditional news outlets continue to be a central reference point (Nah and Chung, 2012) as they still have the advantage of being professionally authored, alleviating the noisy nature of citizen journalism formats. Here, we present a framework for analysing socioeconomic patterns in news articles. In contrast to prior approaches, which primarily focus on the textual contents, our analysis shows how Machine Learning methods can be used to gain insights into the interplay between text in news articles, the news outle</context>
</contexts>
<marker>Lin, Salwen, Garrison, Driscoll, 2005</marker>
<rawString>Carolyn Lin, Michael B. Salwen, Bruce Garrison, and Paul D. Driscoll. 2005. Online news as a functional substitute for offline news. Online news and the public, pages 237–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seungahn Nah</author>
<author>Deborah S Chung</author>
</authors>
<title>When citizens meet both professional and citizen journalists: Social trust, media credibility, and perceived journalistic roles among online community news readers.</title>
<date>2012</date>
<journal>Journalism,</journal>
<volume>13</volume>
<issue>6</issue>
<contexts>
<context position="1914" citStr="Nah and Chung, 2012" startWordPosition="282" endWordPosition="285">an be used to interpret connections between vast amounts of textual content and measurable societydriven factors. 1 Introduction Vast amounts of user-generated content on the Internet as well as digitised textual resources allow us to study text in connection to real world events across large intervals of time. Over the last decade, there has been a shift in user news consumption starting with a move from offline to online sources (Lin et al., 2005); in more recent years user-generated news have also become prominent. However, traditional news outlets continue to be a central reference point (Nah and Chung, 2012) as they still have the advantage of being professionally authored, alleviating the noisy nature of citizen journalism formats. Here, we present a framework for analysing socioeconomic patterns in news articles. In contrast to prior approaches, which primarily focus on the textual contents, our analysis shows how Machine Learning methods can be used to gain insights into the interplay between text in news articles, the news outlets and socioeconomic indicators. Our experiments are performed on a set of EU-related news summaries spanning over 8 years, with the intention to study two basic econo</context>
</contexts>
<marker>Nah, Chung, 2012</marker>
<rawString>Seungahn Nah and Deborah S. Chung. 2012. When citizens meet both professional and citizen journalists: Social trust, media credibility, and perceived journalistic roles among online community news readers. Journalism, 13(6):714–730.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Ramnath Balasubramanyan</author>
<author>Bryan R Routledge</author>
<author>Noah A Smith</author>
</authors>
<title>From tweets to polls: linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of AAAI ICWSM’10,</booktitle>
<pages>122--129</pages>
<marker>O’Connor, Balasubramanyan, Routledge, Smith, 2010</marker>
<rawString>Brendan O’Connor, Ramnath Balasubramanyan, Bryan R. Routledge, and Noah A. Smith. 2010. From tweets to polls: linking text sentiment to public opinion time series. In Proceedings of AAAI ICWSM’10, pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Paul</author>
<author>Mark Dredze</author>
</authors>
<title>You Are What You Tweet: Analyzing Twitter for Public Health.</title>
<date>2011</date>
<booktitle>In Proceedings of AAAI ICWSM’11,</booktitle>
<pages>265--272</pages>
<contexts>
<context position="3576" citStr="Paul and Dredze, 2011" startWordPosition="534" endWordPosition="537">xample, Flaounas et al. (2010) uncover interesting patterns in EU’s Mediasphere, whereas Schumaker and Chen (2009) demonstrate that news articles can predict financial indicators. Conversely, Bentley et al. (2014) show that emotions in the textual content of books reflect back on inflation and unemployment rates during the 20th century. Recently, Social Media text has been intensively studied as a quicker, unobtrusive and cheaper alternative to traditional surveys. Application areas include politics (O’Connor et al., 2010), finance (Bollen and Mao, 2011), health (Lampos and Cristianini, 2012; Paul and Dredze, 2011) or psychology (De Choudhury et al., 2013; Schwartz et al., 2013). In this paper, we apply a modified version of a bilinear regularised regression model (BEN) proposed for the task of voting intention inference from Twitter content (Lampos et al., 2013). The main characteristic of BEN is the ability of modelling word frequencies as well as individual user importance in a joint optimisation task. By applying it in the context of supervised news analysis, we are able to visualise relevant discourse to a particular socioeconomic factor, identifying relevant words together with important outlets. </context>
</contexts>
<marker>Paul, Dredze, 2011</marker>
<rawString>Michael J. Paul and Mark Dredze. 2011. You Are What You Tweet: Analyzing Twitter for Public Health. In Proceedings of AAAI ICWSM’11, pages 265–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert P Schumaker</author>
<author>Hsinchun Chen</author>
</authors>
<title>Textual analysis of stock market prediction using breaking financial news: the AZFin text system.</title>
<date>2009</date>
<journal>ACM TOIS,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="3068" citStr="Schumaker and Chen (2009)" startWordPosition="459" endWordPosition="462">ries spanning over 8 years, with the intention to study two basic economic factors: EU’s unemployment rate and Economic Sentiment Index (ESI) (European Commision, 1997). To determine connections between the news, the outlets and the indicators of interest, we formulate our learning task as bilinear text-based regression (Lampos et al., 2013). Approaches to learning the correlation of news, or text in general, with real world indicators have been performed in both unsupervised and supervised settings. For example, Flaounas et al. (2010) uncover interesting patterns in EU’s Mediasphere, whereas Schumaker and Chen (2009) demonstrate that news articles can predict financial indicators. Conversely, Bentley et al. (2014) show that emotions in the textual content of books reflect back on inflation and unemployment rates during the 20th century. Recently, Social Media text has been intensively studied as a quicker, unobtrusive and cheaper alternative to traditional surveys. Application areas include politics (O’Connor et al., 2010), finance (Bollen and Mao, 2011), health (Lampos and Cristianini, 2012; Paul and Dredze, 2011) or psychology (De Choudhury et al., 2013; Schwartz et al., 2013). In this paper, we apply a</context>
</contexts>
<marker>Schumaker, Chen, 2009</marker>
<rawString>Robert P. Schumaker and Hsinchun Chen. 2009. Textual analysis of stock market prediction using breaking financial news: the AZFin text system. ACM TOIS, 27(2):12:1–12:19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Andrew Schwartz</author>
<author>Johannes C Eichstaedt</author>
<author>Margaret L Kern</author>
<author>Lukasz Dziurzynski</author>
<author>Stephanie M Ramones</author>
<author>Megha Agrawal</author>
<author>Achal Shah</author>
<author>Michal Kosinski</author>
<author>David Stillwell</author>
<author>Martin E P Seligman</author>
<author>Lyle H Ungar</author>
</authors>
<title>Personality, Gender, and Age in the Language of Social Media: The OpenVocabulary Approach.</title>
<date>2013</date>
<tech>PLoS ONE, 8(9).</tech>
<contexts>
<context position="3641" citStr="Schwartz et al., 2013" startWordPosition="545" endWordPosition="548">’s Mediasphere, whereas Schumaker and Chen (2009) demonstrate that news articles can predict financial indicators. Conversely, Bentley et al. (2014) show that emotions in the textual content of books reflect back on inflation and unemployment rates during the 20th century. Recently, Social Media text has been intensively studied as a quicker, unobtrusive and cheaper alternative to traditional surveys. Application areas include politics (O’Connor et al., 2010), finance (Bollen and Mao, 2011), health (Lampos and Cristianini, 2012; Paul and Dredze, 2011) or psychology (De Choudhury et al., 2013; Schwartz et al., 2013). In this paper, we apply a modified version of a bilinear regularised regression model (BEN) proposed for the task of voting intention inference from Twitter content (Lampos et al., 2013). The main characteristic of BEN is the ability of modelling word frequencies as well as individual user importance in a joint optimisation task. By applying it in the context of supervised news analysis, we are able to visualise relevant discourse to a particular socioeconomic factor, identifying relevant words together with important outlets. 13 Proceedings of the ACL 2014 Workshop on Language Technologies </context>
<context position="10779" citStr="Schwartz et al. (2013)" startWordPosition="1749" endWordPosition="1752">erpretation. BEN outperforms LEN in both tasks, with a clearer improvement when predicting ESI. Predictions for all folds are depicted in Fig. 1a and 1b together with the actual values. Note that reformulating the problem into a multi-task learning scenario, where ESI and unemployment are modelled jointly did not improve inference performance. The relatively small average error rates (&lt; 8.8%) make meaningful a further analysis of the model’s outputs. Due to space limitations, we choose to focus on the most recent results, depicting the models derived in the 10th fold. Following the example of Schwartz et al. (2013), we use a word cloud visuESI Unemployment LEN 9.253 (9.89%) 0.9275 (8.75%) BEN 8.209 (8.77%) 0.9047 (8.52%) Table 1: 10-fold validation average RMSEs (and error rates) for LEN and BEN on ESI and unemployment rates prediction. 100 actual predictions 2007 2008 2009 2010 2011 2012 2013 (a) ESI 2007 2008 2009 2010 2011 2012 2013 (b) Unemployment Figure 1: Time series of ESI and unemployment together with BEN predictions (smoothed using a 3-point moving average). alisation, where the font size is proportional to the derived weights by applying BEN, flipped terms denote negative weights and colours</context>
</contexts>
<marker>Schwartz, Eichstaedt, Kern, Dziurzynski, Ramones, Agrawal, Shah, Kosinski, Stillwell, Seligman, Ungar, 2013</marker>
<rawString>H. Andrew Schwartz, Johannes C. Eichstaedt, Margaret L. Kern, Lukasz Dziurzynski, Stephanie M. Ramones, Megha Agrawal, Achal Shah, Michal Kosinski, David Stillwell, Martin E. P. Seligman, and Lyle H. Ungar. 2013. Personality, Gender, and Age in the Language of Social Media: The OpenVocabulary Approach. PLoS ONE, 8(9).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso.</title>
<date>1994</date>
<booktitle>JRSS: Series B,</booktitle>
<pages>58--267</pages>
<contexts>
<context position="7019" citStr="Tibshirani, 1994" startWordPosition="1080" endWordPosition="1081"> use a feature vector input x and aim to build a linear function of x for predicting a response 1http://www.openeurope.org.uk/Page/ PressSummary/en/ 2http://epp.eurostat.ec.europa. eu/statistics_explained/index.php/ Unemployment_statistics variable y: f(x) = xTw + β where x,w E Rm. (1) The objective is to find an f, which minimises a model-dependent loss function (e.g. sum squared error), optionally subject to a regularisation penalty ψ; `2-norm regularisation (ridge regression) penalises high weights (Hoerl and Kennard, 1970), while `1-norm regularisation (lasso) encourages sparse solutions (Tibshirani, 1994). Sparsity is desirable for avoiding overfitting, especially when the dimensionality m is larger than the number of training examples n (Hastie et al., 2009). Elastic Net formulates a combination of `1 and `2-norm regularisation defined by the objective: + ψEN(w, ρ) , where ρ denotes the regularisation parameters (Zou and Hastie, 2005); we refer to this model as LEN (Linear Elastic Net) in the remainder of the script. In the context of voting intention inference from Twitter content, Lampos et al. (2013) extended LEN to a bilinear formulation, where a set of two vector weights are learnt: one </context>
</contexts>
<marker>Tibshirani, 1994</marker>
<rawString>Robert Tibshirani. 1994. Regression shrinkage and selection via the lasso. JRSS: Series B, 58:267–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Zou</author>
<author>Trevor Hastie</author>
</authors>
<title>Regularization and variable selection via the elastic net.</title>
<date>2005</date>
<journal>JRSS: Series B,</journal>
<volume>67</volume>
<issue>2</issue>
<contexts>
<context position="7356" citStr="Zou and Hastie, 2005" startWordPosition="1131" endWordPosition="1134"> a model-dependent loss function (e.g. sum squared error), optionally subject to a regularisation penalty ψ; `2-norm regularisation (ridge regression) penalises high weights (Hoerl and Kennard, 1970), while `1-norm regularisation (lasso) encourages sparse solutions (Tibshirani, 1994). Sparsity is desirable for avoiding overfitting, especially when the dimensionality m is larger than the number of training examples n (Hastie et al., 2009). Elastic Net formulates a combination of `1 and `2-norm regularisation defined by the objective: + ψEN(w, ρ) , where ρ denotes the regularisation parameters (Zou and Hastie, 2005); we refer to this model as LEN (Linear Elastic Net) in the remainder of the script. In the context of voting intention inference from Twitter content, Lampos et al. (2013) extended LEN to a bilinear formulation, where a set of two vector weights are learnt: one for words (w) and one for users (u). This was motivated by the observation that only a sparse set of users may have predictive value. The model now becomes: f(X) = uTXw + β , (3) where X is a matrix of word x users frequencies. The bilinear optimisation objective is formulated as: n {w*,u*, β*} = argmin (uTXiw + β − yi)2 w,u,β i=1 + ψE</context>
</contexts>
<marker>Zou, Hastie, 2005</marker>
<rawString>Hui Zou and Trevor Hastie. 2005. Regularization and variable selection via the elastic net. JRSS: Series B, 67(2):301–320.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
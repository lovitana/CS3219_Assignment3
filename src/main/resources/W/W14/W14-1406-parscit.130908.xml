<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.960484">
A Type-Driven Tensor-Based Semantics for CCG
</title>
<author confidence="0.998236">
Jean Maillard Stephen Clark Edward Grefenstette
</author>
<affiliation confidence="0.9983855">
University of Cambridge University of Cambridge University of Oxford
Computer Laboratory Computer Laboratory Department of Computer Science
</affiliation>
<email confidence="0.967479">
jm864@cam.ac.uk sc609@cam.ac.uk edward.grefenstette@cs.ox.ac.uk
</email>
<sectionHeader confidence="0.994119" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999824230769231">
This paper shows how the tensor-based se-
mantic framework of Coecke et al. can
be seamlessly integrated with Combina-
tory Categorial Grammar (CCG). The inte-
gration follows from the observation that
tensors are linear maps, and hence can
be manipulated using the combinators of
CCG, including type-raising and compo-
sition. Given the existence of robust,
wide-coverage CCG parsers, this opens up
the possibility of a practical, type-driven
compositional semantics based on distri-
butional representations.
</bodyText>
<sectionHeader confidence="0.996521" genericHeader="keywords">
1 Intoduction
</sectionHeader>
<bodyText confidence="0.999894163934426">
In this paper we show how tensor-based distribu-
tional semantics can be seamlessly integrated with
Combinatory Categorial Grammar (CCG, Steed-
man (2000)), building on the theoretical discus-
sion in Grefenstette (2013). Tensor-based distribu-
tional semantics represents the meanings of words
with particular syntactic types as tensors whose se-
mantic type matches that of the syntactic type (Co-
ecke et al., 2010). For example, the meaning of a
transitive verb with syntactic type (S\NP)/NP is
a 3rd-order tensor from the tensor product space
N ® S ® N. The seamless integration with CCG
arises from the (somewhat trivial) observation that
tensors are linear maps — a particular kind of
function — and hence can be manipulated using
CCG’s combinatory rules.
Tensor-based semantics arises from the desire to
enhance distributional semantics with some com-
positional structure, in order to make distribu-
tional semantics more of a complete semantic the-
ory, and to increase its utility in NLP applica-
tions. There are a number of suggestions for how
to add compositionality to a distributional seman-
tics (Clarke, 2012; Pulman, 2013; Erk, 2012).
One approach is to assume that the meanings of
all words are represented by context vectors, and
then combine those vectors using some operation,
such as vector addition, element-wise multiplica-
tion, or tensor product (Clark and Pulman, 2007;
Mitchell and Lapata, 2008). A more sophisticated
approach, which is the subject of this paper, is to
adapt the compositional process from formal se-
mantics (Dowty et al., 1981) and attempt to build
a distributional representation in step with the syn-
tactic derivation (Coecke et al., 2010; Baroni et al.,
2013). Finally, there is a third approach using neu-
ral networks, which perhaps lies in between the
two described above (Socher et al., 2010; Socher
et al., 2012). Here compositional distributed rep-
resentations are built using matrices operating on
vectors, with all parameters learnt through a su-
pervised learning procedure intended to optimise
performance on some NLP task, such as syntac-
tic parsing or sentiment analysis. The approach
of Hermann and Blunsom (2013) conditions the
vector combination operation on the syntactic type
of the combinands, moving it a little closer to the
more formal semantics-inspired approaches.
The remainder of the Introduction gives a short
summary of distributional semantics. The rest of
the paper introduces some mathematical notation
from multi-linear algebra, including Einstein nota-
tion, and then shows how the combinatory rules of
CCG, including type-raising and composition, can
be applied directly to tensor-based semantic rep-
resentations. As well as describing a tensor-based
semantics for CCG, a further goal of this paper is to
present the compositional framework of Coecke et
al. (2010), which is based on category theory, to a
computational linguistics audience using only the
mathematics of multi-linear algebra.
</bodyText>
<subsectionHeader confidence="0.988803">
1.1 Distributional Semantics
</subsectionHeader>
<bodyText confidence="0.9914285">
We assume a basic knowledge of distributional se-
mantics (Grefenstette, 1994; Sch¨utze, 1998). Re-
</bodyText>
<page confidence="0.994245">
46
</page>
<note confidence="0.991763">
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46–54,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999941338983051">
cent inroductions to the topic include Turney and
Pantel (2010) and Clark (2014).
A potentially useful distinction for this paper,
and one not commonly made, is between distri-
butional and distributed representations. Distri-
butional representations are inherently contextual,
and rely on the frequently quoted dictum from
Firth that “you shall know a word from the com-
pany it keeps” (Firth, 1957; Pulman, 2013). This
leads to the so-called distributional hypothesis that
words that occur in similar contexts tend to have
similar meanings, and to various proposals for
how to implement this hypothesis (Curran, 2004),
including alternative definitions of context; alter-
native weighting schemes which emphasize the
importance of some contexts over others; alterna-
tive similarity measures; and various dimension-
ality reduction schemes such as the well-known
LSA technique (Landauer and Dumais, 1997). An
interesting conceptual question is whether a sim-
ilar distributional hypothesis can be applied to
phrases and larger units: is it the case that sen-
tences, for example, have similar meanings if they
occur in similar contexts? Work which does ex-
tend the distributional hypothesis to larger units
includes Baroni and Zamparelli (2010), Clarke
(2012), and Baroni et al. (2013).
Distributed representations, on the other hand,
can be thought of simply as vectors (or possibly
higher-order tensors) of real numbers, where there
is no a priori interpretation of the basis vectors.
Neural networks can perhaps be categorised in this
way, since the resulting vector representations are
simply sequences of real numbers resulting from
the optimisation of some training criterion on a
training set (Collobert and Weston, 2008; Socher
et al., 2010). Whether these distributed represen-
tations can be given a contextual interpretation de-
pends on how they are trained.
One important point for this paper is that the
tensor-based compositional process makes no as-
sumptions about the interpretation of the tensors.
Hence in the remainder of the paper we make no
reference to how noun vectors or verb tensors,
for example, can be acquired (which, for the case
of the higher-order tensors, is a wide open re-
search question). However, in order to help the
reader who would prefer a more grounded dis-
cussion, one possibility is to obtain the noun vec-
tors using standard distributional techniques (Cur-
ran, 2004), and learn the higher-order tensors us-
ing recent techniques from “recursive” neural net-
works (Socher et al., 2010). Another possibility
is suggested by Grefenstette et al. (2013), extend-
ing the learning technique based on linear regres-
sion from Baroni and Zamparelli (2010) in which
“gold-standard” distributional representations are
assumed to be available for some phrases and
larger units.
</bodyText>
<sectionHeader confidence="0.965177" genericHeader="introduction">
2 Mathematical Preliminaries
</sectionHeader>
<bodyText confidence="0.999982125">
The tensor-based compositional process relies on
taking dot (or inner) products between vectors and
higher-order tensors. Dot products, and a number
of other operations on vectors and tensors, can be
conveniently written using Einstein notation (also
referred to as the Einstein summation convention).
In the rest of the paper we assume that the vector
spaces are over the field of real numbers.
</bodyText>
<subsectionHeader confidence="0.980433">
2.1 Einstein Notation
</subsectionHeader>
<bodyText confidence="0.996543">
The squared amplitude of a vector v E Rn is given
by:
</bodyText>
<equation confidence="0.993211">
n
M2 = vivi
i=1
</equation>
<bodyText confidence="0.739917">
Similarly, the dot product of two vectors v, w E
Rn is given by:
</bodyText>
<equation confidence="0.993981666666667">
n
v · w = viwi
i=1
</equation>
<bodyText confidence="0.9950055">
Denote the components of an m x n real matrix
A by Aij for 1 &lt; i &lt; m and 1 &lt; j &lt; n. Then
the matrix-vector product of A and v E Rn gives
a vector Av E Rm with components:
</bodyText>
<equation confidence="0.988227857142857">
n
(Av)i = Aijvj
j=1
We can also multiply an nxm matrix A and an
m x o matrix B to produce an n x o matrix AB
with components:
(AB)ij =
</equation>
<bodyText confidence="0.996973666666667">
The previous examples are some of the most
common operations in linear algebra, and they all
involve sums over repeated indices. They can be
simplified by introducing the Einstein summation
convention: summation over the relevant range
is implied on every component index that occurs
</bodyText>
<equation confidence="0.665401333333333">
�m
k=1
AikBkj
</equation>
<page confidence="0.989134">
47
</page>
<bodyText confidence="0.99924475">
twice. Pairs of indices that are summed over are
known as contracted, while the remaining indices
are known as free. Using this convention, the
above operations can be written as:
</bodyText>
<equation confidence="0.9929035">
|v|2 = vivi
v · w = viwi
</equation>
<bodyText confidence="0.981343266666667">
(Av)i = Aijvj, i.e. the contraction of v with
the second index of A
(AB)ij = AikBkj, i.e. the contraction of the
second index of A with the first of B
Note how the number of free indices is always
conserved between the left- and right-hand sides in
these examples. For instance, while the last equa-
tion has two indices on the left and four on the
right, the two extra indices on the right are con-
tracted. Hence counting the number offree indices
can be a quick way of determining what type of
object is given by a certain mathematical expres-
sion in Einstein notation: no free indices means
that an operation yields a scalar number, one free
index means a vector, two a matrix, and so on.
</bodyText>
<subsectionHeader confidence="0.99654">
2.2 Tensors
</subsectionHeader>
<bodyText confidence="0.999813333333333">
Linear Functionals Given a finite-dimensional
vector space Rn over R, a linear functional is a
linear map a : Rn —* R.
Let a vector v have components vi in a fixed ba-
sis. Then the result of applying a linear functional
a to v can be written as:
</bodyText>
<equation confidence="0.972136666666667">
⎛
⎜
a(v) = a1v1+···+anvn = (a1 ··· an) ⎝
</equation>
<bodyText confidence="0.999397">
The numbers ai are the components of the lin-
ear functional, which can also be pictured as a row
vector. Since there is a one-to-one correspondence
between row and column vectors, the above equa-
tion is equivalent to:
</bodyText>
<equation confidence="0.957722666666667">
⎛
⎜
v(a) = a1v1+···+anvn = (v1 ··· vn) ⎝
</equation>
<bodyText confidence="0.994986">
Using Einstein convention, the equations above
can be written as:
</bodyText>
<equation confidence="0.980981">
a(v) = viai = v(a)
</equation>
<bodyText confidence="0.9985065">
Thus every finite-dimensional vector is a linear
functional, and vice versa. Row and column vec-
tors are examples of first-order tensors.
Definition 1 (First-order tensor). Given a vector
space V over the field R, a first-order tensor T
can be defined as:
</bodyText>
<listItem confidence="0.99726625">
• an element of the vector space V,
• a linear map T : V —* R,
• a |V |-dimensional array of numbers Ti, for
1 &lt; i &lt; |V |.
</listItem>
<bodyText confidence="0.986381">
These three definitions are all equivalent. Given
a first-order tensor described using one of these
definitions, it is trivial to find the two other de-
scriptions.
Matrices An nxm matrix A over R can be rep-
resented by a two-dimensional array of real num-
bers Aij, for 1 &lt; i &lt; n and 1 &lt; j &lt; m.
Via matrix-vector multiplication, the matrix A
can be seen as a linear map A : Rm —* Rn. It
maps a vector v E Rm to a vector
with components
</bodyText>
<equation confidence="0.52403">
A(v)i = Aijvj.
</equation>
<bodyText confidence="0.999209">
We can also contract a vector with the first index
of the matrix, which gives us a map A : Rn —*
Rm. This corresponds to the operation
</bodyText>
<equation confidence="0.8593798">
⎛
⎜
(w1 ··· wn) ⎝
resulting in a vector with components
(wTA)i = Ajiwj.
</equation>
<bodyText confidence="0.9983505">
We can combine the two operations and see a
matrix as a map A : Rn x Rm —* R, defined by:
</bodyText>
<equation confidence="0.902498333333333">
⎛
⎜
wTAv = (w1 ··· wn) ⎝
</equation>
<bodyText confidence="0.791691333333333">
In Einstein notation, this operation can be writ-
ten as
wiAijvj,
</bodyText>
<figure confidence="0.999370357142858">
v1
...
vm
⎞
⎠ ,
⎟
⎞ ⎛
⎟ ⎜
⎠ ⎝
A11 ··· A1m
..
.. ..
An1 ··· Anm
⎛
⎜ ⎝
...
v1
...
⎞
⎠ ⎟
vn
a1
...
⎞
⎠⎟
an
A11 ··· A1m
..
.. ..
An1 ··· Anm
⎞
⎠ ,
⎟
...
A11 ··· A1m
..
.. ..
An1 ··· Anm
⎞ ⎛ v1 ⎞
⎟ ⎜ ... ⎠ ⎟
⎠ ⎝ vm
...
</figure>
<page confidence="0.951648">
48
</page>
<equation confidence="0.422193">
– T : V × W → R or T : V ⊗ W → R.
</equation>
<bodyText confidence="0.9932988">
Again, these definitions are all equivalent. Most
importantly, the four types of maps given in the
definition are isomorphic. Therefore specifying
one map is enough to specify all the others.
which yields a scalar (constant) value, consistent
with the fact that all the indices are contracted.
Finally, matrices can also be characterised in
terms of Kronecker products. Given two vectors
v ∈ Rn and w ∈ Rm, their Kronecker product
v ⊗ w is a matrix
</bodyText>
<figure confidence="0.962902777777778">
v1w1 ··· v1wm
...
..
.. ..
⎞
⎠ ,
⎟
vnw1 ··· vnwm
Tensors We can generalise these definitions to
the more general concept of tensor.
Definition 3 (Tensor). Given vector spaces
V1, ... , Vk over the field R, a kth-order tensor T
is defined as:
⎛
⎜
v ⊗ w = ⎝
with components
(v ⊗ w)ij = viwj.
</figure>
<bodyText confidence="0.989659333333333">
It is a general result in linear algebra that any
n × m matrix can be written as a finite sum of
Kronecker products Ek x(k) ⊗ y(k) of a set of
vectors x(k) and y(k). Note that the sum over k
is written explicitly as it would not be implied by
Einstein notation: this is because the index k does
not range over vector/matrix/tensor components,
but over a set of vectors, and hence that index ap-
pears in brackets.
</bodyText>
<listItem confidence="0.6691198">
An n × m matrix is an element of the tensor
space Rn ⊗ Rm, and it can also be seen as a linear
map A : Rn ⊗ Rm → R. This is because, given
a matrix B with decomposition Ek x(k) ⊗ y(k),
the matrix A can act as follows:
</listItem>
<equation confidence="0.981225222222222">
E
A(B) = Aij
k
A11 ··· A1m y(k)
(x(k) x(k)
n 1
(An1 · · · Anm y(k)
m
= AijBij.
</equation>
<bodyText confidence="0.998944666666667">
Again, counting the number of free indices in the
last line tells us that this operation yields a scalar.
Matrices are examples of second-order tensors.
</bodyText>
<construct confidence="0.843032333333333">
Definition 2 (Second-order tensor). Given vector
spaces V, W over the field R, a second-order ten-
sor T can be defined as:
</construct>
<listItem confidence="0.990813">
• an element of the vector space V ⊗ W,
• a |V  |× |W |-dimensional array of numbers
Tij, for 1 ≤ i ≤ |V  |and 1 ≤ j ≤ |W|,
• a (multi-) linear map:
–
</listItem>
<equation confidence="0.841089">
T : V → W,
–
T : W → V,
</equation>
<listItem confidence="0.999401">
• an element of the vector space V1 ⊗ · · · ⊗ Vk,
• a |V1 |× · · · × |Vk|, kth-dimensional array of
numbers Ti1···ik, for 1 ≤ ij ≤ |Vj|,
• a multi-linear map T : V1 × · · · × Vk → R.
</listItem>
<sectionHeader confidence="0.695976" genericHeader="method">
3 Tensor-Based CCG Semantics
</sectionHeader>
<bodyText confidence="0.999423416666667">
In this section we show how CCG’s syntactic types
can be given tensor-based meaning spaces, and
how the combinator’s employed by CCG to com-
bine syntactic categories carry over to those mean-
ing spaces, maintaining what is often described
as CCG’s “transparent interface” between syntax
and semantics. Here are some example syntactic
types, and the corresponding tensor spaces con-
taining the meanings of the words with those types
(using the notation syntactic type : semantic type).
We first assume that all atomic types have
meanings living in distinct vector spaces:
</bodyText>
<listItem confidence="0.99988">
• noun phrases, NP : N
• sentences, S : S
</listItem>
<bodyText confidence="0.99506175">
The recipe for determining the meaning space
of a complex syntactic type is to replace each
atomic type with its corresponding vector space
and the slashes with tensor product operators:
</bodyText>
<listItem confidence="0.999367571428571">
• Intransitive verb, S\NP : S ⊗ N
• Transitive verb, (S\NP)/NP : S ⊗ N ⊗ N
• Ditransitive verb, ((S\NP)/NP)/NP :
S ⊗ N ⊗ N ⊗ N
• Adverbial modifier, (S\NP)\(S\NP) :
S ⊗ N ⊗ S ⊗ N
• Preposition modifying NP, (NP\NP)/NP :
</listItem>
<equation confidence="0.558070833333333">
N ⊗ N ⊗ N
xi y(k)
(k)
j
=E
k
</equation>
<page confidence="0.984769">
49
</page>
<bodyText confidence="0.9999556875">
Hence the meaning of an intransitive verb, for
example, is a matrix in the tensor product space
S ® N. The meaning of a transitive verb is a
“cuboid”, or 3rd-order tensor, in the tensor product
space S ® N ® N. In the same way that the syntac-
tic type of an intransitive verb can be thought of as
a function — taking an NP and returning an S —
the meaning of an intransitive verb is also a func-
tion (linear map) — taking a vector in N and re-
turning a vector in S. Another way to think of this
function is that each element of the matrix spec-
ifies, for a pair of basis vectors (one from N and
one from S), what the result is on the S basis vec-
tor given a value on the N basis vector.
Now we describe how the combinatory rules
carry over to the meaning spaces.
</bodyText>
<subsectionHeader confidence="0.994125">
3.1 Application
</subsectionHeader>
<bodyText confidence="0.991728">
The function application rules of CCG are forward
(&gt;) and backward (&lt;) application:
</bodyText>
<equation confidence="0.973945">
X/Y Y ==�- X (&gt;)
Y X\Y ==�, X (&lt;)
</equation>
<bodyText confidence="0.999969538461538">
In a traditional semantics for CCG, if function
application is applied in the syntax, then function
application applies also in the semantics (Steed-
man, 2000). This is also true of the tensor-based
semantics. For example, the meaning of a subject
NP combines with the meaning of an intransitive
verb via matrix multiplication, which is equivalent
to applying the linear map corresponding to the
matrix to the vector representing the meaning of
the NP. Applying (multi-)linear maps in (multi-
)linear algebra is equivalent to applying tensor
contraction to the combining tensors. Here is the
case for an intransitive verb:
</bodyText>
<equation confidence="0.942873666666667">
Pat walks
NP S\NP
N S ® N
</equation>
<bodyText confidence="0.988544318181818">
Let Pat be assigned a vector P E N and walks
be assigned a second-order tensor W E S ® N.
Using the backward application combinator cor-
responds to feeding P, an element of N, into W,
seen as a function N —* S. In terms of tensor con-
traction, this is the following operation:
WijPj.
Here we use the convention that the indices
maintain the same order as the syntactic type.
Therefore, in the tensor of an object of type X/Y ,
the first index corresponds to the type X and the
second to the type Y . That is why, when perform-
ing the contraction corresponding to Pat walks,
P E N is contracted with the second index of
W E S ® N, and not the first.&apos; The first index
of W is then the only free index, telling us that the
above operation yields a first-order tensor (vector).
Since this index corresponds to S, we know that
applying backward application to Pat walks yields
a meaning vector in S.
Forward application is performed in the same
manner. Consider the following example:
</bodyText>
<equation confidence="0.945806333333333">
Pat kisses Sandy
NP (S\NP)/NP NP
N S ® N ® N N
</equation>
<bodyText confidence="0.9507505625">
with corresponding tensors P E N for Pat, K E
S ® N ® N for kisses and Y E N for Sandy.
The forward application deriving the type of
kisses Sandy corresponds to
KijkYk,
where Y is contracted with the third index of K
because we have maintained the order defined by
the type (S\NP)/NP: the third index then corre-
sponds to an argument NP coming from the right.
Counting the number of free indices in the
above expression tells us that it yields a second-
order tensor. Looking at the types corresponding
to the free indices tells us that this second-order
tensor is of type S ® N, which is the semantic type
of a verb phrase (or intransitive verb), as we have
already seen in the walks example.
</bodyText>
<subsectionHeader confidence="0.998608">
3.2 Composition
</subsectionHeader>
<bodyText confidence="0.999279">
The forward (&gt;B) and backward (&lt;B) composi-
tion rules are:
</bodyText>
<equation confidence="0.993052">
X/Y Y/Z =� X/Z (&gt;B)
Y \Z X\Y =� X\Z (&lt;B)
</equation>
<bodyText confidence="0.9959855">
Composition in the semantics also reduces to a
form of tensor contraction. Consider the following
example, in which might can combine with kiss
using forward composition:
</bodyText>
<table confidence="0.911385666666667">
Pat might kiss Sandy
NP (S\NP)/(S\NP) (S\NP)/NP NP
N S®N®S®N S ® N ® N N
</table>
<footnote confidence="0.9329398">
&apos;The particular order of the indices is not important, as
long as a convention such as this one is decided upon and
consistently applied to all types (so that tensor contraction
contracts the relevant tensors from each side when a combi-
nator is used).
</footnote>
<page confidence="0.997523">
50
</page>
<bodyText confidence="0.945382222222222">
with tensors M E S ® N ® S ® N for might and
K E S ® N ® N for kiss. Combining the meanings
of might and kiss corresponds to the following op-
eration:
MijklKklm,
yielding a tensor in S ® N ® N, which is the
correct semantic type for a phrase with syntactic
type (S\NP)/NP. Backward composition is per-
formed analogously.
</bodyText>
<subsectionHeader confidence="0.992972">
3.3 Backward-Crossed Composition
</subsectionHeader>
<bodyText confidence="0.928773">
English also requires the use of backward-crossed
composition (Steedman, 2000):
X/Y Z\X ===&gt;. Z/Y (&lt;B×)
In tensor terms, this is the same as forward com-
position; we just need to make sure that the con-
traction matches up the correct parts of each ten-
sor correctly. Consider the following backward-
crossed composition:
</bodyText>
<equation confidence="0.356597166666667">
(S\NP)/NP (S\NP)\(S\NP) ==&gt;.&lt;B× (S\NP)/NP
Let the two items on the left-hand side be rep-
resented by tensors A E S ® N ® N and B E
S ® N ® S ® N. Then, combining them with
backward-crossed composition in tensor terms is
BijklAklm,
</equation>
<bodyText confidence="0.993743142857143">
resulting in a tensor in S ® N ® N (correspond-
ing to the indices i, j and m). Note that we have
reversed the order of tensors in the contraction to
make the matching of the indices more transpar-
ent; however, tensor contraction is commutative
(since it corresponds to a sum over products) so
the order of the tensors does not affect the result.
</bodyText>
<subsectionHeader confidence="0.997514">
3.4 Type-raising
</subsectionHeader>
<bodyText confidence="0.999626">
The forward (&gt;T) and backward (&lt;T) type-
raising rules are:
</bodyText>
<equation confidence="0.999768">
X ===&gt;. T/(T\X) (&gt;T)
X ===&gt;. T\(T/X) (&lt;T)
</equation>
<bodyText confidence="0.998005111111111">
where T is a variable ranging over categories.
Suppose we are given an item of atomic type Y ,
with corresponding vector A E Y. If we apply
forward type-raising to it, we get a new tensor of
type A0 E T ® T ® Y. Now suppose the item of
type Y is followed by another item of type X\Y ,
with tensor B E X ® Y. A phrase consisting of
two words with types Y and X\Y can be parsed
in two different ways:
</bodyText>
<listItem confidence="0.901662">
• Y X\Y ==&gt;. X, by backward application;
• Y X\Y ==&gt;.T X/(X\Y ) X\Y , by forward
type-raising, and X/(X\Y ) X\Y ==&gt;. X, by
forward application.
</listItem>
<bodyText confidence="0.999825714285714">
Both ways of parsing this sentence yield an item
of type X, and crucially the meaning of the result-
ing item should be the same in both cases.2 This
property of type-raising provides an avenue into
determining what the tensor representation for the
type-raised category should be, since the tensor
representations must also be the same:
</bodyText>
<equation confidence="0.612415">
AjBij = A0ijkBjk.
</equation>
<bodyText confidence="0.999090333333333">
Moreover, this equation must hold for all items,
B. As a concrete example, the requirement says
that a subject NP combining with a verb phrase
S\NP must produce the same meaning for the
two alternative derivations, irrespective of the verb
phrase. This is equivalent to the requirement that
</bodyText>
<equation confidence="0.482437">
AjBij = A0ijkBjk, VB E X ® Y.
</equation>
<bodyText confidence="0.99846425">
So to arrive at the tensor representation, we sim-
ply have to solve the tensor equation above. We
start by renaming the dummy index j on the left-
hand side:
</bodyText>
<equation confidence="0.714574">
AkBik = A0ijkBjk.
</equation>
<bodyText confidence="0.999949">
We then insert a Kronecker delta (Sij = 1 if i = j
and 0 otherwise):
</bodyText>
<equation confidence="0.548153333333333">
AkBjkSij = A0ijkBjk.
Since the equation holds for all B, we are left with
A0ijk = SijAk,
</equation>
<bodyText confidence="0.9993505">
which gives us a recipe for performing type-
raising in a tensor-based model. The recipe is par-
ticularly simple and elegant: it corresponds to in-
serting the vector being type-raised into the 3rd-
order tensor at all places where the first two in-
dices are equal (with the rest of the elements in
the 3rd-order tensor being zero). For example, to
type-raise a subject NP, its meaning vector in N is
placed in the 3rd-order tensor S®S®N at all places
where the indices of the two S dimensions are the
same. Visually, the 3rd-order tensor correspond-
ing to the meaning of the type-raised category is
</bodyText>
<footnote confidence="0.929361">
2This property of CCG resulting from the use of type-
raising and composition is sometimes referred to as “spurious
ambiguity”.
</footnote>
<page confidence="0.998742">
51
</page>
<bodyText confidence="0.998505333333333">
a cubiod in which the noun vector is repeated a
number of times (once for each sentence index),
resulting in a series of “steps” progressing diag-
onally from the bottom of the cuboid to the top
(assuming a particular orientation).
The discussion so far has been somewhat ab-
stract, so to finish this section we include some
more examples with CCG categories, and show
that the tensor contraction operation has an intu-
itive similarity with the “cancellation law” of cat-
egorial grammar which applies in the syntax.
First consider the example of a subject NP
with meaning A, combining with a verb phrase
S\NP with meaning B, resulting in a sentence
with meaning C. In the syntax, the two NPs can-
cel. In the semantics, for each basis of the sentence
space S we perform an inner product between two
vectors in N:
</bodyText>
<equation confidence="0.539639">
Ci = AjBij
</equation>
<bodyText confidence="0.9923805">
Hence, inner products in the tensor space corre-
spond to cancellation in the syntax.
This correspondence extends to complex argu-
ments, and also to composition. Consider the sub-
ject type-raising case, in which a subject NP with
meaning A in S ® S ® N combines with a verb
phrase S\NP with meaning B, resulting in a sen-
tence with meaning C. Again we perform inner
product operations, but this time the inner product
is between two matrices:3
</bodyText>
<equation confidence="0.380901">
Ci = AijkBjk
</equation>
<bodyText confidence="0.999175090909091">
Note that two matrices are “cancelled” for each
basis vector of the sentence space (i.e. for each
index i in Ci).
As a final example, consider the forward com-
position from earlier, in which a modal verb with
meaning A in S®N®S®N combines with a tran-
sitive verb with meaning B in S ® N ® N to give
a transitive verb with meaning C in S ® N ® N.
Again the cancellation in the syntax corresponds
to inner products between matrices, but this time
we need an inner product for each combination of
</bodyText>
<sectionHeader confidence="0.729722" genericHeader="method">
3 indices:
</sectionHeader>
<subsectionHeader confidence="0.813454">
Cijk = AijlmBlmk
</subsectionHeader>
<bodyText confidence="0.963067166666667">
3To be more precise, the two matrices can be thought of
as vectors in the tensor space S ⊗ N and the inner product is
between these vectors. Another way to think of this opera-
tion is to “linearize” the two matrices into vectors and then
perform the inner product on these vectors.
For each i, j, k, two matrices — corresponding to
the l, m indices above — are “cancelled”.
This intuitive explanation extends to arguments
with any number of slashes. For example, a
composition where the cancelling categories are
(N/N)/(N/N) would require inner products be-
tween 4th-order tensors in N ® N ® N ® N.
</bodyText>
<sectionHeader confidence="0.999851" genericHeader="evaluation">
4 Related Work
</sectionHeader>
<bodyText confidence="0.99997035">
The tensor-based semantics presented in this pa-
per is effectively an extension of the Coecke et al.
(2010) framework to CCG, re-expressing in Ein-
stein notation the existing categorical CCG exten-
sion in Grefenstette (2013), which itself builds
on an earlier Lambek Grammar extension to the
framework by Coecke et al. (2013).
This work also bears some similarity to the
treatment of categorial grammars presented by Ba-
roni et al. (2013), which it effectively encompasses
by expressing the tensor contractions described by
Baroni et al. as Einstein summations. However,
this paper also covers CCG-specific operations not
discussed by Baroni et al., such as type-raising and
composition.
One difference between this paper and the orig-
inal work by Coecke et al. (2010) is that they use
pregroups as the syntactic formalism (Lambek,
2008), a context-free variant of categorial gram-
mar. In pregroups, cancellation in the syntax is
always between two atomic categories (or more
precisely, between an atomic category and its “ad-
joint”), whereas in CCG the arguments in complex
categories can be complex categories themselves.
To what extent this difference is significant re-
mains to be seen. For example, one area where this
may have an impact is when non-linearities are
added after contractions. Since the CCG contrac-
tions with complex arguments happen “in one go”,
whereas the corresponding pregroup cancellation
in the semantics would be a series of contractions,
many more non-linearities would be added in the
pregroup case.
Krishnamurthy and Mitchell (2013) is based on
a similar insight to this paper – that CCG provides
combinators which can manipulate functions op-
erating over vectors. Krishnamurthy and Mitchell
consider the function application case, whereas we
have shown how the type-raising and composition
operators apply naturally in this setting also.
</bodyText>
<page confidence="0.997859">
52
</page>
<sectionHeader confidence="0.998443" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999641810344827">
This paper provides a theoretical framework for
the development of a compositional distributional
semantics for CCG. Given the existence of ro-
bust, wide-coverage CCG parsers (Clark and Cur-
ran, 2007; Hockenmaier and Steedman, 2002),
together with various techniques for learning the
tensors, the opportunity exists for a practical im-
plementation. However, there are significant engi-
neering difficulties which need to be overcome.
Consider adapting the neural-network learning
techniques of Socher et al. (2012) to this prob-
lem.4 In terms of the number of tensors, the lexi-
con would need to contain a tensor for every word-
category pair; this is at least an order of magnitude
more tensors then the number of matrices learnt in
existing work (Socher et al., 2012; Hermann and
Blunsom, 2013). Furthermore, the order of the
tensors is now higher. Syntactic categories such as
((N/N)/(N/N))/((N/N)/(N/N)) are not un-
common in the wide-coverage grammar of Hock-
enmaier and Steedman (2007), which in this case
would require an 8th-order tensor. This combina-
tion of many word-category pairs and higher-order
tensors results in a huge number of parameters.
As a solution to this problem, we are investigat-
ing ways to reduce the number of parameters, for
example using tensor decomposition techniques
(Kolda and Bader, 2009). It may also be possi-
ble to reduce the size of some of the complex cat-
egories in the grammar. Many challenges remain
before a type-driven compositional distributional
semantics can be realised, similar to the work of
Bos for the model-theoretic case (Bos et al., 2004;
Bos, 2005), but in this paper we have set out the
theoretical framework for such an implementation.
Finally, we repeat a comment made earlier that
the compositional framework makes no assump-
tions about the underlying vector spaces, or how
they are to be interpreted. On the one hand, this
flexibility is welcome, since it means the frame-
work can encompass many techniques for building
word vectors (and tensors). On the other hand, it
means that a description of the framework is nec-
essarily abstract, and it leaves open the question
4Non-linear transformations are inherent to neural net-
works, whereas the framework in this paper is entirely linear.
However, as hinted at earlier in the paper, non-linear transfor-
mations can be applied to the output of each tensor, turning
the linear networks in this paper into extensions of those in
Socher et al. (2012) (extensions in the sense that the tensors
in Socher et al. (2012) do not extend beyond matrices).
of what the meaning spaces represent. The lat-
ter question is particularly pressing in the case of
the sentence space, and providing an interpretation
of such spaces remains a challenge for the distri-
butional semantics community, as well as relating
distributional semantics to more traditional topics
in semantics such as quantification and inference.
</bodyText>
<sectionHeader confidence="0.996962" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98518225">
Jean Maillard is supported by an EPSRC MPhil
studentship. Stephen Clark is supported by ERC
Starting Grant DisCoTex (306920) and EPSRC
grant EP/I037512/1. Edward Grefenstette is sup-
ported by EPSRC grant EP/I037512/1. We would
like to thank Tamara Polajnar, Laura Rimell, Nal
Kalchbrenner and Karl Moritz Hermann for useful
discussion.
</bodyText>
<sectionHeader confidence="0.998028" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99924303030303">
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), Cambridge, MA.
M. Baroni, R. Bernardi, and R. Zamparelli. 2013.
Frege in space: A program for compositional dis-
tributional semantics (to appear). Linguistic Issues
in Language Technologies.
Johan Bos, Stephen Clark, Mark Steedman, James R.
Curran, and Julia Hockenmaier. 2004. Wide-
coverage semantic representations from a CCG
parser. In Proceedings of COLING-04, pages 1240–
1246, Geneva, Switzerland.
Johan Bos. 2005. Towards wide-coverage seman-
tic interpretation. In Proceedings of the Sixth In-
ternational Workshop on Computational Semantics
(IWCS-6), pages 42–53, Tilburg, The Netherlands.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
Stephen Clark and Stephen Pulman. 2007. Combining
symbolic and distributional models of meaning. In
Proceedings of AAAI Spring Symposium on Quan-
tum Interaction, Stanford, CA. AAAI Press.
Stephen Clark. 2014. Vector space models of lexical
meaning (to appear). In Shalom Lappin and Chris
Fox, editors, Handbook of Contemporary Semantics
second edition. Wiley-Blackwell.
Daoud Clarke. 2012. A context-theoretic frame-
work for compositionality in distributional seman-
tics. Computational Linguistics, 38(1):41–71.
</reference>
<page confidence="0.986752">
53
</page>
<reference confidence="0.999931895833333">
B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Math-
ematical foundations for a compositional distribu-
tional model of meaning. In J. van Bentham,
M. Moortgat, and W. Buszkowski, editors, Linguis-
tic Analysis (Lambek Festschrift), volume 36, pages
345–384.
Bob Coecke, Edward Grefenstette, and Mehrnoosh
Sadrzadeh. 2013. Lambek vs. Lambek: Functorial
vector space semantics and string diagrams for Lam-
bek calculus. Annals of Pure and Applied Logic.
R. Collobert and J. Weston. 2008. A unified architec-
ture for natural language processing: Deep neural
networks with multitask learning. In International
Conference on Machine Learning, ICML, Helsinki,
Finland.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh.
D.R. Dowty, R.E. Wall, and S. Peters. 1981. Introduc-
tion to Montague Semantics. Dordrecht.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: a survey. Language and
Linguistics Compass, 6(10):635–653.
J. R. Firth. 1957. A synopsis of linguistic theory 1930-
1955. In Studies in Linguistic Analysis, pages 1–32.
Oxford: Philological Society.
Edward Grefenstette, Georgiana Dinu, YaoZhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multistep regression learning for composi-
tional distributional semantics. In Proceedings of
the 10th International Conference on Computational
Semantics (IWCS-13), Potsdam, Germany.
Gregory Grefenstette. 1994. Explorations in Auto-
matic Thesaurus Discovery. Kluwer.
Edward Grefenstette. 2013. Category-Theoretic
Quantitative Compositional Distributional Models
of Natural Language Semantics. Ph.D. thesis, Uni-
versity of Oxford.
Karl Moritz Hermann and Phil Blunsom. 2013. The
role of syntax in vector space models of composi-
tional semantics. Proceedings of ACL, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combi-
natory Categorial Grammar. In Proceedings of the
40th Meeting of the ACL, pages 335–342, Philadel-
phia, PA.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355–396.
T. G. Kolda and B. W. Bader. 2009. Tensor decompo-
sitions and applications. SIAM Review, 51(3):455–
500.
Jayant Krishnamurthy and Tom M. Mitchell. 2013.
Vector space semantic parsing: A framework for
compositional vector space models. In Proceed-
ings of the 2013 ACL Workshop on Continuous Vec-
tor Space Models and their Compositionality, Sofia,
Bulgaria.
Joachim Lambek. 2008. From Word to Sentence.
A Computational Algebraic Approach to Grammar.
Polimetrica.
T. K. Landauer and S. T. Dumais. 1997. A solu-
tion to Plato’s problem: the latent semantic analysis
theory of acquisition, induction and representation
of knowledge. Psychological Review, 104(2):211–
240.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08, pages 236–244, Columbus, OH.
Stephen Pulman. 2013. Distributional semantic mod-
els. In Sadrzadeh Heunen and Grefenstette, editors,
Compositional Methods in Physics and Linguistics.
Oxford University Press.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
124.
Richard Socher, Christopher D. Manning, and An-
drew Y. Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS Deep
Learning and Unsupervised Feature Learning Work-
shop, Vancouver, Canada.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic composi-
tionality through recursive matrix-vector spaces. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1201–
1211, Jeju, Korea.
Mark Steedman. 2000. The Syntactic Process. The
MIT Press, Cambridge, MA.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
</reference>
<page confidence="0.999018">
54
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.641464">
<title confidence="0.999948">A Type-Driven Tensor-Based Semantics for CCG</title>
<author confidence="0.999939">Jean Maillard Stephen Clark Edward Grefenstette</author>
<affiliation confidence="0.998661">University of Cambridge University of Cambridge University of Oxford Computer Laboratory Computer Laboratory Department of Computer Science</affiliation>
<email confidence="0.775867">jm864@cam.ac.uksc609@cam.ac.ukedward.grefenstette@cs.ox.ac.uk</email>
<abstract confidence="0.997110603174603">This paper shows how the tensor-based semantic framework of Coecke et al. can be seamlessly integrated with Combina- Categorial Grammar The integration follows from the observation that tensors are linear maps, and hence can be manipulated using the combinators of including type-raising and composition. Given the existence of robust, this opens up the possibility of a practical, type-driven compositional semantics based on distributional representations. 1 Intoduction In this paper we show how tensor-based distributional semantics can be seamlessly integrated with Categorial Grammar Steedman (2000)), building on the theoretical discussion in Grefenstette (2013). Tensor-based distributional semantics represents the meanings of words with particular syntactic types as tensors whose semantic type matches that of the syntactic type (Coecke et al., 2010). For example, the meaning of a verb with syntactic type a 3rd-order tensor from the tensor product space The seamless integration with arises from the (somewhat trivial) observation that tensors are linear maps — a particular kind of function — and hence can be manipulated using combinatory rules. Tensor-based semantics arises from the desire to enhance distributional semantics with some compositional structure, in order to make distributional semantics more of a complete semantic theand to increase its utility in applications. There are a number of suggestions for how to add compositionality to a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). One approach is to assume that the meanings of all words are represented by context vectors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a supervised learning procedure intended to optimise on some such as syntactic parsing or sentiment analysis. The approach of Hermann and Blunsom (2013) conditions the vector combination operation on the syntactic type of the combinands, moving it a little closer to the more formal semantics-inspired approaches.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>R Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing (EMNLP-10),</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="5314" citStr="Baroni and Zamparelli (2010)" startWordPosition="791" endWordPosition="794">urran, 2004), including alternative definitions of context; alternative weighting schemes which emphasize the importance of some contexts over others; alternative similarity measures; and various dimensionality reduction schemes such as the well-known LSA technique (Landauer and Dumais, 1997). An interesting conceptual question is whether a similar distributional hypothesis can be applied to phrases and larger units: is it the case that sentences, for example, have similar meanings if they occur in similar contexts? Work which does extend the distributional hypothesis to larger units includes Baroni and Zamparelli (2010), Clarke (2012), and Baroni et al. (2013). Distributed representations, on the other hand, can be thought of simply as vectors (or possibly higher-order tensors) of real numbers, where there is no a priori interpretation of the basis vectors. Neural networks can perhaps be categorised in this way, since the resulting vector representations are simply sequences of real numbers resulting from the optimisation of some training criterion on a training set (Collobert and Weston, 2008; Socher et al., 2010). Whether these distributed representations can be given a contextual interpretation depends on</context>
<context position="6746" citStr="Baroni and Zamparelli (2010)" startWordPosition="1020" endWordPosition="1023">e make no reference to how noun vectors or verb tensors, for example, can be acquired (which, for the case of the higher-order tensors, is a wide open research question). However, in order to help the reader who would prefer a more grounded discussion, one possibility is to obtain the noun vectors using standard distributional techniques (Curran, 2004), and learn the higher-order tensors using recent techniques from “recursive” neural networks (Socher et al., 2010). Another possibility is suggested by Grefenstette et al. (2013), extending the learning technique based on linear regression from Baroni and Zamparelli (2010) in which “gold-standard” distributional representations are assumed to be available for some phrases and larger units. 2 Mathematical Preliminaries The tensor-based compositional process relies on taking dot (or inner) products between vectors and higher-order tensors. Dot products, and a number of other operations on vectors and tensors, can be conveniently written using Einstein notation (also referred to as the Einstein summation convention). In the rest of the paper we assume that the vector spaces are over the field of real numbers. 2.1 Einstein Notation The squared amplitude of a vector</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>M. Baroni and R. Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Conference on Empirical Methods in Natural Language Processing (EMNLP-10), Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>R Bernardi</author>
<author>R Zamparelli</author>
</authors>
<title>Frege in space: A program for compositional distributional semantics (to appear). Linguistic Issues in Language Technologies.</title>
<date>2013</date>
<contexts>
<context position="2505" citStr="Baroni et al., 2013" startWordPosition="373" endWordPosition="376">a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). One approach is to assume that the meanings of all words are represented by context vectors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a supervised learning procedure intended to optimise performance on some NLP task, such as syntactic parsing or sentiment analysis. The approach of Hermann and Blunsom (2013) conditions the vector combination operation on the syntactic type of the combinands, moving it a little closer to the more formal semantic</context>
<context position="5355" citStr="Baroni et al. (2013)" startWordPosition="798" endWordPosition="801">f context; alternative weighting schemes which emphasize the importance of some contexts over others; alternative similarity measures; and various dimensionality reduction schemes such as the well-known LSA technique (Landauer and Dumais, 1997). An interesting conceptual question is whether a similar distributional hypothesis can be applied to phrases and larger units: is it the case that sentences, for example, have similar meanings if they occur in similar contexts? Work which does extend the distributional hypothesis to larger units includes Baroni and Zamparelli (2010), Clarke (2012), and Baroni et al. (2013). Distributed representations, on the other hand, can be thought of simply as vectors (or possibly higher-order tensors) of real numbers, where there is no a priori interpretation of the basis vectors. Neural networks can perhaps be categorised in this way, since the resulting vector representations are simply sequences of real numbers resulting from the optimisation of some training criterion on a training set (Collobert and Weston, 2008; Socher et al., 2010). Whether these distributed representations can be given a contextual interpretation depends on how they are trained. One important poin</context>
<context position="24556" citStr="Baroni et al. (2013)" startWordPosition="4434" endWordPosition="4438">any number of slashes. For example, a composition where the cancelling categories are (N/N)/(N/N) would require inner products between 4th-order tensors in N ® N ® N ® N. 4 Related Work The tensor-based semantics presented in this paper is effectively an extension of the Coecke et al. (2010) framework to CCG, re-expressing in Einstein notation the existing categorical CCG extension in Grefenstette (2013), which itself builds on an earlier Lambek Grammar extension to the framework by Coecke et al. (2013). This work also bears some similarity to the treatment of categorial grammars presented by Baroni et al. (2013), which it effectively encompasses by expressing the tensor contractions described by Baroni et al. as Einstein summations. However, this paper also covers CCG-specific operations not discussed by Baroni et al., such as type-raising and composition. One difference between this paper and the original work by Coecke et al. (2010) is that they use pregroups as the syntactic formalism (Lambek, 2008), a context-free variant of categorial grammar. In pregroups, cancellation in the syntax is always between two atomic categories (or more precisely, between an atomic category and its “adjoint”), wherea</context>
</contexts>
<marker>Baroni, Bernardi, Zamparelli, 2013</marker>
<rawString>M. Baroni, R. Bernardi, and R. Zamparelli. 2013. Frege in space: A program for compositional distributional semantics (to appear). Linguistic Issues in Language Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Stephen Clark</author>
<author>Mark Steedman</author>
<author>James R Curran</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Widecoverage semantic representations from a CCG parser.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04,</booktitle>
<pages>1240--1246</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="27578" citStr="Bos et al., 2004" startWordPosition="4909" endWordPosition="4912"> Steedman (2007), which in this case would require an 8th-order tensor. This combination of many word-category pairs and higher-order tensors results in a huge number of parameters. As a solution to this problem, we are investigating ways to reduce the number of parameters, for example using tensor decomposition techniques (Kolda and Bader, 2009). It may also be possible to reduce the size of some of the complex categories in the grammar. Many challenges remain before a type-driven compositional distributional semantics can be realised, similar to the work of Bos for the model-theoretic case (Bos et al., 2004; Bos, 2005), but in this paper we have set out the theoretical framework for such an implementation. Finally, we repeat a comment made earlier that the compositional framework makes no assumptions about the underlying vector spaces, or how they are to be interpreted. On the one hand, this flexibility is welcome, since it means the framework can encompass many techniques for building word vectors (and tensors). On the other hand, it means that a description of the framework is necessarily abstract, and it leaves open the question 4Non-linear transformations are inherent to neural networks, whe</context>
</contexts>
<marker>Bos, Clark, Steedman, Curran, Hockenmaier, 2004</marker>
<rawString>Johan Bos, Stephen Clark, Mark Steedman, James R. Curran, and Julia Hockenmaier. 2004. Widecoverage semantic representations from a CCG parser. In Proceedings of COLING-04, pages 1240– 1246, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Towards wide-coverage semantic interpretation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Workshop on Computational Semantics (IWCS-6),</booktitle>
<pages>42--53</pages>
<location>Tilburg, The Netherlands.</location>
<contexts>
<context position="27590" citStr="Bos, 2005" startWordPosition="4913" endWordPosition="4914">which in this case would require an 8th-order tensor. This combination of many word-category pairs and higher-order tensors results in a huge number of parameters. As a solution to this problem, we are investigating ways to reduce the number of parameters, for example using tensor decomposition techniques (Kolda and Bader, 2009). It may also be possible to reduce the size of some of the complex categories in the grammar. Many challenges remain before a type-driven compositional distributional semantics can be realised, similar to the work of Bos for the model-theoretic case (Bos et al., 2004; Bos, 2005), but in this paper we have set out the theoretical framework for such an implementation. Finally, we repeat a comment made earlier that the compositional framework makes no assumptions about the underlying vector spaces, or how they are to be interpreted. On the one hand, this flexibility is welcome, since it means the framework can encompass many techniques for building word vectors (and tensors). On the other hand, it means that a description of the framework is necessarily abstract, and it leaves open the question 4Non-linear transformations are inherent to neural networks, whereas the fra</context>
</contexts>
<marker>Bos, 2005</marker>
<rawString>Johan Bos. 2005. Towards wide-coverage semantic interpretation. In Proceedings of the Sixth International Workshop on Computational Semantics (IWCS-6), pages 42–53, Tilburg, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="26196" citStr="Clark and Curran, 2007" startWordPosition="4686" endWordPosition="4690">tions, many more non-linearities would be added in the pregroup case. Krishnamurthy and Mitchell (2013) is based on a similar insight to this paper – that CCG provides combinators which can manipulate functions operating over vectors. Krishnamurthy and Mitchell consider the function application case, whereas we have shown how the type-raising and composition operators apply naturally in this setting also. 52 5 Conclusion This paper provides a theoretical framework for the development of a compositional distributional semantics for CCG. Given the existence of robust, wide-coverage CCG parsers (Clark and Curran, 2007; Hockenmaier and Steedman, 2002), together with various techniques for learning the tensors, the opportunity exists for a practical implementation. However, there are significant engineering difficulties which need to be overcome. Consider adapting the neural-network learning techniques of Socher et al. (2012) to this problem.4 In terms of the number of tensors, the lexicon would need to contain a tensor for every wordcategory pair; this is at least an order of magnitude more tensors then the number of matrices learnt in existing work (Socher et al., 2012; Hermann and Blunsom, 2013). Furtherm</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>Stephen Pulman</author>
</authors>
<title>Combining symbolic and distributional models of meaning.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI Spring Symposium on Quantum Interaction,</booktitle>
<publisher>AAAI Press.</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="2194" citStr="Clark and Pulman, 2007" startWordPosition="322" endWordPosition="325">Tensor-based semantics arises from the desire to enhance distributional semantics with some compositional structure, in order to make distributional semantics more of a complete semantic theory, and to increase its utility in NLP applications. There are a number of suggestions for how to add compositionality to a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). One approach is to assume that the meanings of all words are represented by context vectors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a </context>
</contexts>
<marker>Clark, Pulman, 2007</marker>
<rawString>Stephen Clark and Stephen Pulman. 2007. Combining symbolic and distributional models of meaning. In Proceedings of AAAI Spring Symposium on Quantum Interaction, Stanford, CA. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector space models of lexical meaning (to appear).</title>
<date>2014</date>
<booktitle>Handbook of Contemporary Semantics</booktitle>
<editor>In Shalom Lappin and Chris Fox, editors,</editor>
<publisher>Wiley-Blackwell.</publisher>
<contexts>
<context position="4164" citStr="Clark (2014)" startWordPosition="621" endWordPosition="622">al of this paper is to present the compositional framework of Coecke et al. (2010), which is based on category theory, to a computational linguistics audience using only the mathematics of multi-linear algebra. 1.1 Distributional Semantics We assume a basic knowledge of distributional semantics (Grefenstette, 1994; Sch¨utze, 1998). Re46 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46–54, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics cent inroductions to the topic include Turney and Pantel (2010) and Clark (2014). A potentially useful distinction for this paper, and one not commonly made, is between distributional and distributed representations. Distributional representations are inherently contextual, and rely on the frequently quoted dictum from Firth that “you shall know a word from the company it keeps” (Firth, 1957; Pulman, 2013). This leads to the so-called distributional hypothesis that words that occur in similar contexts tend to have similar meanings, and to various proposals for how to implement this hypothesis (Curran, 2004), including alternative definitions of context; alternative weight</context>
</contexts>
<marker>Clark, 2014</marker>
<rawString>Stephen Clark. 2014. Vector space models of lexical meaning (to appear). In Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics second edition. Wiley-Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daoud Clarke</author>
</authors>
<title>A context-theoretic framework for compositionality in distributional semantics.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<contexts>
<context position="1925" citStr="Clarke, 2012" startWordPosition="282" endWordPosition="283">-order tensor from the tensor product space N ® S ® N. The seamless integration with CCG arises from the (somewhat trivial) observation that tensors are linear maps — a particular kind of function — and hence can be manipulated using CCG’s combinatory rules. Tensor-based semantics arises from the desire to enhance distributional semantics with some compositional structure, in order to make distributional semantics more of a complete semantic theory, and to increase its utility in NLP applications. There are a number of suggestions for how to add compositionality to a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). One approach is to assume that the meanings of all words are represented by context vectors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is </context>
<context position="5329" citStr="Clarke (2012)" startWordPosition="795" endWordPosition="796">ative definitions of context; alternative weighting schemes which emphasize the importance of some contexts over others; alternative similarity measures; and various dimensionality reduction schemes such as the well-known LSA technique (Landauer and Dumais, 1997). An interesting conceptual question is whether a similar distributional hypothesis can be applied to phrases and larger units: is it the case that sentences, for example, have similar meanings if they occur in similar contexts? Work which does extend the distributional hypothesis to larger units includes Baroni and Zamparelli (2010), Clarke (2012), and Baroni et al. (2013). Distributed representations, on the other hand, can be thought of simply as vectors (or possibly higher-order tensors) of real numbers, where there is no a priori interpretation of the basis vectors. Neural networks can perhaps be categorised in this way, since the resulting vector representations are simply sequences of real numbers resulting from the optimisation of some training criterion on a training set (Collobert and Weston, 2008; Socher et al., 2010). Whether these distributed representations can be given a contextual interpretation depends on how they are t</context>
</contexts>
<marker>Clarke, 2012</marker>
<rawString>Daoud Clarke. 2012. A context-theoretic framework for compositionality in distributional semantics. Computational Linguistics, 38(1):41–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Coecke</author>
<author>M Sadrzadeh</author>
<author>S Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning.</title>
<date>2010</date>
<booktitle>Linguistic Analysis (Lambek Festschrift),</booktitle>
<volume>36</volume>
<pages>345--384</pages>
<editor>In J. van Bentham, M. Moortgat, and W. Buszkowski, editors,</editor>
<contexts>
<context position="1227" citStr="Coecke et al., 2010" startWordPosition="165" endWordPosition="169">g and composition. Given the existence of robust, wide-coverage CCG parsers, this opens up the possibility of a practical, type-driven compositional semantics based on distributional representations. 1 Intoduction In this paper we show how tensor-based distributional semantics can be seamlessly integrated with Combinatory Categorial Grammar (CCG, Steedman (2000)), building on the theoretical discussion in Grefenstette (2013). Tensor-based distributional semantics represents the meanings of words with particular syntactic types as tensors whose semantic type matches that of the syntactic type (Coecke et al., 2010). For example, the meaning of a transitive verb with syntactic type (S\NP)/NP is a 3rd-order tensor from the tensor product space N ® S ® N. The seamless integration with CCG arises from the (somewhat trivial) observation that tensors are linear maps — a particular kind of function — and hence can be manipulated using CCG’s combinatory rules. Tensor-based semantics arises from the desire to enhance distributional semantics with some compositional structure, in order to make distributional semantics more of a complete semantic theory, and to increase its utility in NLP applications. There are a</context>
<context position="2483" citStr="Coecke et al., 2010" startWordPosition="369" endWordPosition="372"> compositionality to a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). One approach is to assume that the meanings of all words are represented by context vectors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a supervised learning procedure intended to optimise performance on some NLP task, such as syntactic parsing or sentiment analysis. The approach of Hermann and Blunsom (2013) conditions the vector combination operation on the syntactic type of the combinands, moving it a little closer to th</context>
<context position="24228" citStr="Coecke et al. (2010)" startWordPosition="4382" endWordPosition="4385">he inner product is between these vectors. Another way to think of this operation is to “linearize” the two matrices into vectors and then perform the inner product on these vectors. For each i, j, k, two matrices — corresponding to the l, m indices above — are “cancelled”. This intuitive explanation extends to arguments with any number of slashes. For example, a composition where the cancelling categories are (N/N)/(N/N) would require inner products between 4th-order tensors in N ® N ® N ® N. 4 Related Work The tensor-based semantics presented in this paper is effectively an extension of the Coecke et al. (2010) framework to CCG, re-expressing in Einstein notation the existing categorical CCG extension in Grefenstette (2013), which itself builds on an earlier Lambek Grammar extension to the framework by Coecke et al. (2013). This work also bears some similarity to the treatment of categorial grammars presented by Baroni et al. (2013), which it effectively encompasses by expressing the tensor contractions described by Baroni et al. as Einstein summations. However, this paper also covers CCG-specific operations not discussed by Baroni et al., such as type-raising and composition. One difference between</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>B. Coecke, M. Sadrzadeh, and S. Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. In J. van Bentham, M. Moortgat, and W. Buszkowski, editors, Linguistic Analysis (Lambek Festschrift), volume 36, pages 345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Lambek vs. Lambek: Functorial vector space semantics and string diagrams for Lambek calculus. Annals of Pure and Applied Logic.</title>
<date>2013</date>
<contexts>
<context position="24444" citStr="Coecke et al. (2013)" startWordPosition="4416" endWordPosition="4419">corresponding to the l, m indices above — are “cancelled”. This intuitive explanation extends to arguments with any number of slashes. For example, a composition where the cancelling categories are (N/N)/(N/N) would require inner products between 4th-order tensors in N ® N ® N ® N. 4 Related Work The tensor-based semantics presented in this paper is effectively an extension of the Coecke et al. (2010) framework to CCG, re-expressing in Einstein notation the existing categorical CCG extension in Grefenstette (2013), which itself builds on an earlier Lambek Grammar extension to the framework by Coecke et al. (2013). This work also bears some similarity to the treatment of categorial grammars presented by Baroni et al. (2013), which it effectively encompasses by expressing the tensor contractions described by Baroni et al. as Einstein summations. However, this paper also covers CCG-specific operations not discussed by Baroni et al., such as type-raising and composition. One difference between this paper and the original work by Coecke et al. (2010) is that they use pregroups as the syntactic formalism (Lambek, 2008), a context-free variant of categorial grammar. In pregroups, cancellation in the syntax i</context>
</contexts>
<marker>Coecke, Grefenstette, Sadrzadeh, 2013</marker>
<rawString>Bob Coecke, Edward Grefenstette, and Mehrnoosh Sadrzadeh. 2013. Lambek vs. Lambek: Functorial vector space semantics and string diagrams for Lambek calculus. Annals of Pure and Applied Logic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In International Conference on Machine Learning, ICML,</booktitle>
<location>Helsinki, Finland.</location>
<contexts>
<context position="5797" citStr="Collobert and Weston, 2008" startWordPosition="865" endWordPosition="868">s if they occur in similar contexts? Work which does extend the distributional hypothesis to larger units includes Baroni and Zamparelli (2010), Clarke (2012), and Baroni et al. (2013). Distributed representations, on the other hand, can be thought of simply as vectors (or possibly higher-order tensors) of real numbers, where there is no a priori interpretation of the basis vectors. Neural networks can perhaps be categorised in this way, since the resulting vector representations are simply sequences of real numbers resulting from the optimisation of some training criterion on a training set (Collobert and Weston, 2008; Socher et al., 2010). Whether these distributed representations can be given a contextual interpretation depends on how they are trained. One important point for this paper is that the tensor-based compositional process makes no assumptions about the interpretation of the tensors. Hence in the remainder of the paper we make no reference to how noun vectors or verb tensors, for example, can be acquired (which, for the case of the higher-order tensors, is a wide open research question). However, in order to help the reader who would prefer a more grounded discussion, one possibility is to obta</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>R. Collobert and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning, ICML, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Curran</author>
</authors>
<title>From Distributional to Semantic Similarity.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="4698" citStr="Curran, 2004" startWordPosition="702" endWordPosition="703">ent inroductions to the topic include Turney and Pantel (2010) and Clark (2014). A potentially useful distinction for this paper, and one not commonly made, is between distributional and distributed representations. Distributional representations are inherently contextual, and rely on the frequently quoted dictum from Firth that “you shall know a word from the company it keeps” (Firth, 1957; Pulman, 2013). This leads to the so-called distributional hypothesis that words that occur in similar contexts tend to have similar meanings, and to various proposals for how to implement this hypothesis (Curran, 2004), including alternative definitions of context; alternative weighting schemes which emphasize the importance of some contexts over others; alternative similarity measures; and various dimensionality reduction schemes such as the well-known LSA technique (Landauer and Dumais, 1997). An interesting conceptual question is whether a similar distributional hypothesis can be applied to phrases and larger units: is it the case that sentences, for example, have similar meanings if they occur in similar contexts? Work which does extend the distributional hypothesis to larger units includes Baroni and Z</context>
<context position="6472" citStr="Curran, 2004" startWordPosition="979" endWordPosition="981">ons can be given a contextual interpretation depends on how they are trained. One important point for this paper is that the tensor-based compositional process makes no assumptions about the interpretation of the tensors. Hence in the remainder of the paper we make no reference to how noun vectors or verb tensors, for example, can be acquired (which, for the case of the higher-order tensors, is a wide open research question). However, in order to help the reader who would prefer a more grounded discussion, one possibility is to obtain the noun vectors using standard distributional techniques (Curran, 2004), and learn the higher-order tensors using recent techniques from “recursive” neural networks (Socher et al., 2010). Another possibility is suggested by Grefenstette et al. (2013), extending the learning technique based on linear regression from Baroni and Zamparelli (2010) in which “gold-standard” distributional representations are assumed to be available for some phrases and larger units. 2 Mathematical Preliminaries The tensor-based compositional process relies on taking dot (or inner) products between vectors and higher-order tensors. Dot products, and a number of other operations on vecto</context>
</contexts>
<marker>Curran, 2004</marker>
<rawString>James R. Curran. 2004. From Distributional to Semantic Similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Dowty</author>
<author>R E Wall</author>
<author>S Peters</author>
</authors>
<title>Introduction to Montague Semantics.</title>
<date>1981</date>
<location>Dordrecht.</location>
<contexts>
<context position="2371" citStr="Dowty et al., 1981" startWordPosition="351" endWordPosition="354">mantic theory, and to increase its utility in NLP applications. There are a number of suggestions for how to add compositionality to a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). One approach is to assume that the meanings of all words are represented by context vectors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a supervised learning procedure intended to optimise performance on some NLP task, such as syntactic parsing or sentiment analysis. The approach of Hermann and Blunsom (2013) cond</context>
</contexts>
<marker>Dowty, Wall, Peters, 1981</marker>
<rawString>D.R. Dowty, R.E. Wall, and S. Peters. 1981. Introduction to Montague Semantics. Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: a survey. Language and Linguistics Compass,</title>
<date>2012</date>
<pages>6--10</pages>
<contexts>
<context position="1951" citStr="Erk, 2012" startWordPosition="286" endWordPosition="287">r product space N ® S ® N. The seamless integration with CCG arises from the (somewhat trivial) observation that tensors are linear maps — a particular kind of function — and hence can be manipulated using CCG’s combinatory rules. Tensor-based semantics arises from the desire to enhance distributional semantics with some compositional structure, in order to make distributional semantics more of a complete semantic theory, and to increase its utility in NLP applications. There are a number of suggestions for how to add compositionality to a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). One approach is to assume that the meanings of all words are represented by context vectors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neu</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: a survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Firth</author>
</authors>
<title>A synopsis of linguistic theory 1930-1955.</title>
<date>1957</date>
<booktitle>In Studies in Linguistic Analysis,</booktitle>
<pages>1--32</pages>
<publisher>Philological Society.</publisher>
<location>Oxford:</location>
<contexts>
<context position="4478" citStr="Firth, 1957" startWordPosition="669" endWordPosition="670">; Sch¨utze, 1998). Re46 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46–54, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics cent inroductions to the topic include Turney and Pantel (2010) and Clark (2014). A potentially useful distinction for this paper, and one not commonly made, is between distributional and distributed representations. Distributional representations are inherently contextual, and rely on the frequently quoted dictum from Firth that “you shall know a word from the company it keeps” (Firth, 1957; Pulman, 2013). This leads to the so-called distributional hypothesis that words that occur in similar contexts tend to have similar meanings, and to various proposals for how to implement this hypothesis (Curran, 2004), including alternative definitions of context; alternative weighting schemes which emphasize the importance of some contexts over others; alternative similarity measures; and various dimensionality reduction schemes such as the well-known LSA technique (Landauer and Dumais, 1997). An interesting conceptual question is whether a similar distributional hypothesis can be applied </context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>J. R. Firth. 1957. A synopsis of linguistic theory 1930-1955. In Studies in Linguistic Analysis, pages 1–32. Oxford: Philological Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Georgiana Dinu</author>
<author>YaoZhong Zhang</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Marco Baroni</author>
</authors>
<title>Multistep regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the 10th International Conference on Computational Semantics (IWCS-13),</booktitle>
<location>Potsdam, Germany.</location>
<contexts>
<context position="6651" citStr="Grefenstette et al. (2013)" startWordPosition="1005" endWordPosition="1008">no assumptions about the interpretation of the tensors. Hence in the remainder of the paper we make no reference to how noun vectors or verb tensors, for example, can be acquired (which, for the case of the higher-order tensors, is a wide open research question). However, in order to help the reader who would prefer a more grounded discussion, one possibility is to obtain the noun vectors using standard distributional techniques (Curran, 2004), and learn the higher-order tensors using recent techniques from “recursive” neural networks (Socher et al., 2010). Another possibility is suggested by Grefenstette et al. (2013), extending the learning technique based on linear regression from Baroni and Zamparelli (2010) in which “gold-standard” distributional representations are assumed to be available for some phrases and larger units. 2 Mathematical Preliminaries The tensor-based compositional process relies on taking dot (or inner) products between vectors and higher-order tensors. Dot products, and a number of other operations on vectors and tensors, can be conveniently written using Einstein notation (also referred to as the Einstein summation convention). In the rest of the paper we assume that the vector spa</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Edward Grefenstette, Georgiana Dinu, YaoZhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multistep regression learning for compositional distributional semantics. In Proceedings of the 10th International Conference on Computational Semantics (IWCS-13), Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
</authors>
<title>Explorations in Automatic Thesaurus Discovery.</title>
<date>1994</date>
<publisher>Kluwer.</publisher>
<contexts>
<context position="3867" citStr="Grefenstette, 1994" startWordPosition="578" endWordPosition="579">e mathematical notation from multi-linear algebra, including Einstein notation, and then shows how the combinatory rules of CCG, including type-raising and composition, can be applied directly to tensor-based semantic representations. As well as describing a tensor-based semantics for CCG, a further goal of this paper is to present the compositional framework of Coecke et al. (2010), which is based on category theory, to a computational linguistics audience using only the mathematics of multi-linear algebra. 1.1 Distributional Semantics We assume a basic knowledge of distributional semantics (Grefenstette, 1994; Sch¨utze, 1998). Re46 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46–54, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics cent inroductions to the topic include Turney and Pantel (2010) and Clark (2014). A potentially useful distinction for this paper, and one not commonly made, is between distributional and distributed representations. Distributional representations are inherently contextual, and rely on the frequently quoted dictum from Firth that “you shall know a word from the company it keeps” (</context>
</contexts>
<marker>Grefenstette, 1994</marker>
<rawString>Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery. Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
</authors>
<title>Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Oxford.</institution>
<contexts>
<context position="1035" citStr="Grefenstette (2013)" startWordPosition="138" endWordPosition="139">mbinatory Categorial Grammar (CCG). The integration follows from the observation that tensors are linear maps, and hence can be manipulated using the combinators of CCG, including type-raising and composition. Given the existence of robust, wide-coverage CCG parsers, this opens up the possibility of a practical, type-driven compositional semantics based on distributional representations. 1 Intoduction In this paper we show how tensor-based distributional semantics can be seamlessly integrated with Combinatory Categorial Grammar (CCG, Steedman (2000)), building on the theoretical discussion in Grefenstette (2013). Tensor-based distributional semantics represents the meanings of words with particular syntactic types as tensors whose semantic type matches that of the syntactic type (Coecke et al., 2010). For example, the meaning of a transitive verb with syntactic type (S\NP)/NP is a 3rd-order tensor from the tensor product space N ® S ® N. The seamless integration with CCG arises from the (somewhat trivial) observation that tensors are linear maps — a particular kind of function — and hence can be manipulated using CCG’s combinatory rules. Tensor-based semantics arises from the desire to enhance distri</context>
<context position="24343" citStr="Grefenstette (2013)" startWordPosition="4401" endWordPosition="4402"> into vectors and then perform the inner product on these vectors. For each i, j, k, two matrices — corresponding to the l, m indices above — are “cancelled”. This intuitive explanation extends to arguments with any number of slashes. For example, a composition where the cancelling categories are (N/N)/(N/N) would require inner products between 4th-order tensors in N ® N ® N ® N. 4 Related Work The tensor-based semantics presented in this paper is effectively an extension of the Coecke et al. (2010) framework to CCG, re-expressing in Einstein notation the existing categorical CCG extension in Grefenstette (2013), which itself builds on an earlier Lambek Grammar extension to the framework by Coecke et al. (2013). This work also bears some similarity to the treatment of categorial grammars presented by Baroni et al. (2013), which it effectively encompasses by expressing the tensor contractions described by Baroni et al. as Einstein summations. However, this paper also covers CCG-specific operations not discussed by Baroni et al., such as type-raising and composition. One difference between this paper and the original work by Coecke et al. (2010) is that they use pregroups as the syntactic formalism (La</context>
</contexts>
<marker>Grefenstette, 2013</marker>
<rawString>Edward Grefenstette. 2013. Category-Theoretic Quantitative Compositional Distributional Models of Natural Language Semantics. Ph.D. thesis, University of Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>The role of syntax in vector space models of compositional semantics.</title>
<date>2013</date>
<booktitle>Proceedings of ACL,</booktitle>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="2966" citStr="Hermann and Blunsom (2013)" startWordPosition="445" endWordPosition="448"> semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a supervised learning procedure intended to optimise performance on some NLP task, such as syntactic parsing or sentiment analysis. The approach of Hermann and Blunsom (2013) conditions the vector combination operation on the syntactic type of the combinands, moving it a little closer to the more formal semantics-inspired approaches. The remainder of the Introduction gives a short summary of distributional semantics. The rest of the paper introduces some mathematical notation from multi-linear algebra, including Einstein notation, and then shows how the combinatory rules of CCG, including type-raising and composition, can be applied directly to tensor-based semantic representations. As well as describing a tensor-based semantics for CCG, a further goal of this pap</context>
<context position="26786" citStr="Hermann and Blunsom, 2013" startWordPosition="4782" endWordPosition="4785"> CCG parsers (Clark and Curran, 2007; Hockenmaier and Steedman, 2002), together with various techniques for learning the tensors, the opportunity exists for a practical implementation. However, there are significant engineering difficulties which need to be overcome. Consider adapting the neural-network learning techniques of Socher et al. (2012) to this problem.4 In terms of the number of tensors, the lexicon would need to contain a tensor for every wordcategory pair; this is at least an order of magnitude more tensors then the number of matrices learnt in existing work (Socher et al., 2012; Hermann and Blunsom, 2013). Furthermore, the order of the tensors is now higher. Syntactic categories such as ((N/N)/(N/N))/((N/N)/(N/N)) are not uncommon in the wide-coverage grammar of Hockenmaier and Steedman (2007), which in this case would require an 8th-order tensor. This combination of many word-category pairs and higher-order tensors results in a huge number of parameters. As a solution to this problem, we are investigating ways to reduce the number of parameters, for example using tensor decomposition techniques (Kolda and Bader, 2009). It may also be possible to reduce the size of some of the complex categori</context>
</contexts>
<marker>Hermann, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2013. The role of syntax in vector space models of compositional semantics. Proceedings of ACL, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>Generative models for statistical parsing with Combinatory Categorial Grammar.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL,</booktitle>
<pages>335--342</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="26229" citStr="Hockenmaier and Steedman, 2002" startWordPosition="4691" endWordPosition="4694">earities would be added in the pregroup case. Krishnamurthy and Mitchell (2013) is based on a similar insight to this paper – that CCG provides combinators which can manipulate functions operating over vectors. Krishnamurthy and Mitchell consider the function application case, whereas we have shown how the type-raising and composition operators apply naturally in this setting also. 52 5 Conclusion This paper provides a theoretical framework for the development of a compositional distributional semantics for CCG. Given the existence of robust, wide-coverage CCG parsers (Clark and Curran, 2007; Hockenmaier and Steedman, 2002), together with various techniques for learning the tensors, the opportunity exists for a practical implementation. However, there are significant engineering difficulties which need to be overcome. Consider adapting the neural-network learning techniques of Socher et al. (2012) to this problem.4 In terms of the number of tensors, the lexicon would need to contain a tensor for every wordcategory pair; this is at least an order of magnitude more tensors then the number of matrices learnt in existing work (Socher et al., 2012; Hermann and Blunsom, 2013). Furthermore, the order of the tensors is </context>
</contexts>
<marker>Hockenmaier, Steedman, 2002</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2002. Generative models for statistical parsing with Combinatory Categorial Grammar. In Proceedings of the 40th Meeting of the ACL, pages 335–342, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="26978" citStr="Hockenmaier and Steedman (2007)" startWordPosition="4809" endWordPosition="4813">wever, there are significant engineering difficulties which need to be overcome. Consider adapting the neural-network learning techniques of Socher et al. (2012) to this problem.4 In terms of the number of tensors, the lexicon would need to contain a tensor for every wordcategory pair; this is at least an order of magnitude more tensors then the number of matrices learnt in existing work (Socher et al., 2012; Hermann and Blunsom, 2013). Furthermore, the order of the tensors is now higher. Syntactic categories such as ((N/N)/(N/N))/((N/N)/(N/N)) are not uncommon in the wide-coverage grammar of Hockenmaier and Steedman (2007), which in this case would require an 8th-order tensor. This combination of many word-category pairs and higher-order tensors results in a huge number of parameters. As a solution to this problem, we are investigating ways to reduce the number of parameters, for example using tensor decomposition techniques (Kolda and Bader, 2009). It may also be possible to reduce the size of some of the complex categories in the grammar. Many challenges remain before a type-driven compositional distributional semantics can be realised, similar to the work of Bos for the model-theoretic case (Bos et al., 2004</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Kolda</author>
<author>B W Bader</author>
</authors>
<title>Tensor decompositions and applications.</title>
<date>2009</date>
<journal>SIAM Review,</journal>
<volume>51</volume>
<issue>3</issue>
<pages>500</pages>
<contexts>
<context position="27310" citStr="Kolda and Bader, 2009" startWordPosition="4863" endWordPosition="4866">en the number of matrices learnt in existing work (Socher et al., 2012; Hermann and Blunsom, 2013). Furthermore, the order of the tensors is now higher. Syntactic categories such as ((N/N)/(N/N))/((N/N)/(N/N)) are not uncommon in the wide-coverage grammar of Hockenmaier and Steedman (2007), which in this case would require an 8th-order tensor. This combination of many word-category pairs and higher-order tensors results in a huge number of parameters. As a solution to this problem, we are investigating ways to reduce the number of parameters, for example using tensor decomposition techniques (Kolda and Bader, 2009). It may also be possible to reduce the size of some of the complex categories in the grammar. Many challenges remain before a type-driven compositional distributional semantics can be realised, similar to the work of Bos for the model-theoretic case (Bos et al., 2004; Bos, 2005), but in this paper we have set out the theoretical framework for such an implementation. Finally, we repeat a comment made earlier that the compositional framework makes no assumptions about the underlying vector spaces, or how they are to be interpreted. On the one hand, this flexibility is welcome, since it means th</context>
</contexts>
<marker>Kolda, Bader, 2009</marker>
<rawString>T. G. Kolda and B. W. Bader. 2009. Tensor decompositions and applications. SIAM Review, 51(3):455– 500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Vector space semantic parsing: A framework for compositional vector space models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="25677" citStr="Krishnamurthy and Mitchell (2013)" startWordPosition="4608" endWordPosition="4611">s always between two atomic categories (or more precisely, between an atomic category and its “adjoint”), whereas in CCG the arguments in complex categories can be complex categories themselves. To what extent this difference is significant remains to be seen. For example, one area where this may have an impact is when non-linearities are added after contractions. Since the CCG contractions with complex arguments happen “in one go”, whereas the corresponding pregroup cancellation in the semantics would be a series of contractions, many more non-linearities would be added in the pregroup case. Krishnamurthy and Mitchell (2013) is based on a similar insight to this paper – that CCG provides combinators which can manipulate functions operating over vectors. Krishnamurthy and Mitchell consider the function application case, whereas we have shown how the type-raising and composition operators apply naturally in this setting also. 52 5 Conclusion This paper provides a theoretical framework for the development of a compositional distributional semantics for CCG. Given the existence of robust, wide-coverage CCG parsers (Clark and Curran, 2007; Hockenmaier and Steedman, 2002), together with various techniques for learning </context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2013</marker>
<rawString>Jayant Krishnamurthy and Tom M. Mitchell. 2013. Vector space semantic parsing: A framework for compositional vector space models. In Proceedings of the 2013 ACL Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joachim Lambek</author>
</authors>
<title>From Word to Sentence. A Computational Algebraic Approach to Grammar.</title>
<date>2008</date>
<publisher>Polimetrica.</publisher>
<contexts>
<context position="24954" citStr="Lambek, 2008" startWordPosition="4498" endWordPosition="4499">3), which itself builds on an earlier Lambek Grammar extension to the framework by Coecke et al. (2013). This work also bears some similarity to the treatment of categorial grammars presented by Baroni et al. (2013), which it effectively encompasses by expressing the tensor contractions described by Baroni et al. as Einstein summations. However, this paper also covers CCG-specific operations not discussed by Baroni et al., such as type-raising and composition. One difference between this paper and the original work by Coecke et al. (2010) is that they use pregroups as the syntactic formalism (Lambek, 2008), a context-free variant of categorial grammar. In pregroups, cancellation in the syntax is always between two atomic categories (or more precisely, between an atomic category and its “adjoint”), whereas in CCG the arguments in complex categories can be complex categories themselves. To what extent this difference is significant remains to be seen. For example, one area where this may have an impact is when non-linearities are added after contractions. Since the CCG contractions with complex arguments happen “in one go”, whereas the corresponding pregroup cancellation in the semantics would be</context>
</contexts>
<marker>Lambek, 2008</marker>
<rawString>Joachim Lambek. 2008. From Word to Sentence. A Computational Algebraic Approach to Grammar. Polimetrica.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>S T Dumais</author>
</authors>
<title>A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge.</title>
<date>1997</date>
<journal>Psychological Review,</journal>
<volume>104</volume>
<issue>2</issue>
<pages>240</pages>
<contexts>
<context position="4979" citStr="Landauer and Dumais, 1997" startWordPosition="738" endWordPosition="741">tual, and rely on the frequently quoted dictum from Firth that “you shall know a word from the company it keeps” (Firth, 1957; Pulman, 2013). This leads to the so-called distributional hypothesis that words that occur in similar contexts tend to have similar meanings, and to various proposals for how to implement this hypothesis (Curran, 2004), including alternative definitions of context; alternative weighting schemes which emphasize the importance of some contexts over others; alternative similarity measures; and various dimensionality reduction schemes such as the well-known LSA technique (Landauer and Dumais, 1997). An interesting conceptual question is whether a similar distributional hypothesis can be applied to phrases and larger units: is it the case that sentences, for example, have similar meanings if they occur in similar contexts? Work which does extend the distributional hypothesis to larger units includes Baroni and Zamparelli (2010), Clarke (2012), and Baroni et al. (2013). Distributed representations, on the other hand, can be thought of simply as vectors (or possibly higher-order tensors) of real numbers, where there is no a priori interpretation of the basis vectors. Neural networks can pe</context>
</contexts>
<marker>Landauer, Dumais, 1997</marker>
<rawString>T. K. Landauer and S. T. Dumais. 1997. A solution to Plato’s problem: the latent semantic analysis theory of acquisition, induction and representation of knowledge. Psychological Review, 104(2):211– 240.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="2222" citStr="Mitchell and Lapata, 2008" startWordPosition="326" endWordPosition="329">rises from the desire to enhance distributional semantics with some compositional structure, in order to make distributional semantics more of a complete semantic theory, and to increase its utility in NLP applications. There are a number of suggestions for how to add compositionality to a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). One approach is to assume that the meanings of all words are represented by context vectors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a supervised learning procedur</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Pulman</author>
</authors>
<title>Distributional semantic models.</title>
<date>2013</date>
<booktitle>In Sadrzadeh Heunen and Grefenstette, editors, Compositional Methods in Physics and Linguistics.</booktitle>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1939" citStr="Pulman, 2013" startWordPosition="284" endWordPosition="285">from the tensor product space N ® S ® N. The seamless integration with CCG arises from the (somewhat trivial) observation that tensors are linear maps — a particular kind of function — and hence can be manipulated using CCG’s combinatory rules. Tensor-based semantics arises from the desire to enhance distributional semantics with some compositional structure, in order to make distributional semantics more of a complete semantic theory, and to increase its utility in NLP applications. There are a number of suggestions for how to add compositionality to a distributional semantics (Clarke, 2012; Pulman, 2013; Erk, 2012). One approach is to assume that the meanings of all words are represented by context vectors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approa</context>
<context position="4493" citStr="Pulman, 2013" startWordPosition="671" endWordPosition="672">998). Re46 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46–54, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics cent inroductions to the topic include Turney and Pantel (2010) and Clark (2014). A potentially useful distinction for this paper, and one not commonly made, is between distributional and distributed representations. Distributional representations are inherently contextual, and rely on the frequently quoted dictum from Firth that “you shall know a word from the company it keeps” (Firth, 1957; Pulman, 2013). This leads to the so-called distributional hypothesis that words that occur in similar contexts tend to have similar meanings, and to various proposals for how to implement this hypothesis (Curran, 2004), including alternative definitions of context; alternative weighting schemes which emphasize the importance of some contexts over others; alternative similarity measures; and various dimensionality reduction schemes such as the well-known LSA technique (Landauer and Dumais, 1997). An interesting conceptual question is whether a similar distributional hypothesis can be applied to phrases and </context>
</contexts>
<marker>Pulman, 2013</marker>
<rawString>Stephen Pulman. 2013. Distributional semantic models. In Sadrzadeh Heunen and Grefenstette, editors, Compositional Methods in Physics and Linguistics. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>124</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS Deep Learning and Unsupervised Feature Learning Workshop,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="2639" citStr="Socher et al., 2010" startWordPosition="396" endWordPosition="399">ented by context vectors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a supervised learning procedure intended to optimise performance on some NLP task, such as syntactic parsing or sentiment analysis. The approach of Hermann and Blunsom (2013) conditions the vector combination operation on the syntactic type of the combinands, moving it a little closer to the more formal semantics-inspired approaches. The remainder of the Introduction gives a short summary of distributional semantics. The rest of the paper intr</context>
<context position="5819" citStr="Socher et al., 2010" startWordPosition="869" endWordPosition="872">ontexts? Work which does extend the distributional hypothesis to larger units includes Baroni and Zamparelli (2010), Clarke (2012), and Baroni et al. (2013). Distributed representations, on the other hand, can be thought of simply as vectors (or possibly higher-order tensors) of real numbers, where there is no a priori interpretation of the basis vectors. Neural networks can perhaps be categorised in this way, since the resulting vector representations are simply sequences of real numbers resulting from the optimisation of some training criterion on a training set (Collobert and Weston, 2008; Socher et al., 2010). Whether these distributed representations can be given a contextual interpretation depends on how they are trained. One important point for this paper is that the tensor-based compositional process makes no assumptions about the interpretation of the tensors. Hence in the remainder of the paper we make no reference to how noun vectors or verb tensors, for example, can be acquired (which, for the case of the higher-order tensors, is a wide open research question). However, in order to help the reader who would prefer a more grounded discussion, one possibility is to obtain the noun vectors us</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS Deep Learning and Unsupervised Feature Learning Workshop, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1201--1211</pages>
<location>Jeju,</location>
<contexts>
<context position="2661" citStr="Socher et al., 2012" startWordPosition="400" endWordPosition="403">ors, and then combine those vectors using some operation, such as vector addition, element-wise multiplication, or tensor product (Clark and Pulman, 2007; Mitchell and Lapata, 2008). A more sophisticated approach, which is the subject of this paper, is to adapt the compositional process from formal semantics (Dowty et al., 1981) and attempt to build a distributional representation in step with the syntactic derivation (Coecke et al., 2010; Baroni et al., 2013). Finally, there is a third approach using neural networks, which perhaps lies in between the two described above (Socher et al., 2010; Socher et al., 2012). Here compositional distributed representations are built using matrices operating on vectors, with all parameters learnt through a supervised learning procedure intended to optimise performance on some NLP task, such as syntactic parsing or sentiment analysis. The approach of Hermann and Blunsom (2013) conditions the vector combination operation on the syntactic type of the combinands, moving it a little closer to the more formal semantics-inspired approaches. The remainder of the Introduction gives a short summary of distributional semantics. The rest of the paper introduces some mathematic</context>
<context position="26508" citStr="Socher et al. (2012)" startWordPosition="4730" endWordPosition="4733">ave shown how the type-raising and composition operators apply naturally in this setting also. 52 5 Conclusion This paper provides a theoretical framework for the development of a compositional distributional semantics for CCG. Given the existence of robust, wide-coverage CCG parsers (Clark and Curran, 2007; Hockenmaier and Steedman, 2002), together with various techniques for learning the tensors, the opportunity exists for a practical implementation. However, there are significant engineering difficulties which need to be overcome. Consider adapting the neural-network learning techniques of Socher et al. (2012) to this problem.4 In terms of the number of tensors, the lexicon would need to contain a tensor for every wordcategory pair; this is at least an order of magnitude more tensors then the number of matrices learnt in existing work (Socher et al., 2012; Hermann and Blunsom, 2013). Furthermore, the order of the tensors is now higher. Syntactic categories such as ((N/N)/(N/N))/((N/N)/(N/N)) are not uncommon in the wide-coverage grammar of Hockenmaier and Steedman (2007), which in this case would require an 8th-order tensor. This combination of many word-category pairs and higher-order tensors resu</context>
<context position="28437" citStr="Socher et al. (2012)" startWordPosition="5052" endWordPosition="5055">w they are to be interpreted. On the one hand, this flexibility is welcome, since it means the framework can encompass many techniques for building word vectors (and tensors). On the other hand, it means that a description of the framework is necessarily abstract, and it leaves open the question 4Non-linear transformations are inherent to neural networks, whereas the framework in this paper is entirely linear. However, as hinted at earlier in the paper, non-linear transformations can be applied to the output of each tensor, turning the linear networks in this paper into extensions of those in Socher et al. (2012) (extensions in the sense that the tensors in Socher et al. (2012) do not extend beyond matrices). of what the meaning spaces represent. The latter question is particularly pressing in the case of the sentence space, and providing an interpretation of such spaces remains a challenge for the distributional semantics community, as well as relating distributional semantics to more traditional topics in semantics such as quantification and inference. Acknowledgments Jean Maillard is supported by an EPSRC MPhil studentship. Stephen Clark is supported by ERC Starting Grant DisCoTex (306920) and EPSR</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1201– 1211, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="971" citStr="Steedman (2000)" startWordPosition="128" endWordPosition="130">mework of Coecke et al. can be seamlessly integrated with Combinatory Categorial Grammar (CCG). The integration follows from the observation that tensors are linear maps, and hence can be manipulated using the combinators of CCG, including type-raising and composition. Given the existence of robust, wide-coverage CCG parsers, this opens up the possibility of a practical, type-driven compositional semantics based on distributional representations. 1 Intoduction In this paper we show how tensor-based distributional semantics can be seamlessly integrated with Combinatory Categorial Grammar (CCG, Steedman (2000)), building on the theoretical discussion in Grefenstette (2013). Tensor-based distributional semantics represents the meanings of words with particular syntactic types as tensors whose semantic type matches that of the syntactic type (Coecke et al., 2010). For example, the meaning of a transitive verb with syntactic type (S\NP)/NP is a 3rd-order tensor from the tensor product space N ® S ® N. The seamless integration with CCG arises from the (somewhat trivial) observation that tensors are linear maps — a particular kind of function — and hence can be manipulated using CCG’s combinatory rules.</context>
<context position="15276" citStr="Steedman, 2000" startWordPosition="2735" endWordPosition="2737">nd returning a vector in S. Another way to think of this function is that each element of the matrix specifies, for a pair of basis vectors (one from N and one from S), what the result is on the S basis vector given a value on the N basis vector. Now we describe how the combinatory rules carry over to the meaning spaces. 3.1 Application The function application rules of CCG are forward (&gt;) and backward (&lt;) application: X/Y Y ==�- X (&gt;) Y X\Y ==�, X (&lt;) In a traditional semantics for CCG, if function application is applied in the syntax, then function application applies also in the semantics (Steedman, 2000). This is also true of the tensor-based semantics. For example, the meaning of a subject NP combines with the meaning of an intransitive verb via matrix multiplication, which is equivalent to applying the linear map corresponding to the matrix to the vector representing the meaning of the NP. Applying (multi-)linear maps in (multi)linear algebra is equivalent to applying tensor contraction to the combining tensors. Here is the case for an intransitive verb: Pat walks NP S\NP N S ® N Let Pat be assigned a vector P E N and walks be assigned a second-order tensor W E S ® N. Using the backward app</context>
<context position="18523" citStr="Steedman, 2000" startWordPosition="3334" endWordPosition="3335">nvention such as this one is decided upon and consistently applied to all types (so that tensor contraction contracts the relevant tensors from each side when a combinator is used). 50 with tensors M E S ® N ® S ® N for might and K E S ® N ® N for kiss. Combining the meanings of might and kiss corresponds to the following operation: MijklKklm, yielding a tensor in S ® N ® N, which is the correct semantic type for a phrase with syntactic type (S\NP)/NP. Backward composition is performed analogously. 3.3 Backward-Crossed Composition English also requires the use of backward-crossed composition (Steedman, 2000): X/Y Z\X ===&gt;. Z/Y (&lt;B×) In tensor terms, this is the same as forward composition; we just need to make sure that the contraction matches up the correct parts of each tensor correctly. Consider the following backwardcrossed composition: (S\NP)/NP (S\NP)\(S\NP) ==&gt;.&lt;B× (S\NP)/NP Let the two items on the left-hand side be represented by tensors A E S ® N ® N and B E S ® N ® S ® N. Then, combining them with backward-crossed composition in tensor terms is BijklAklm, resulting in a tensor in S ® N ® N (corresponding to the indices i, j and m). Note that we have reversed the order of tensors in the</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="4147" citStr="Turney and Pantel (2010)" startWordPosition="616" endWordPosition="619">mantics for CCG, a further goal of this paper is to present the compositional framework of Coecke et al. (2010), which is based on category theory, to a computational linguistics audience using only the mathematics of multi-linear algebra. 1.1 Distributional Semantics We assume a basic knowledge of distributional semantics (Grefenstette, 1994; Sch¨utze, 1998). Re46 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46–54, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics cent inroductions to the topic include Turney and Pantel (2010) and Clark (2014). A potentially useful distinction for this paper, and one not commonly made, is between distributional and distributed representations. Distributional representations are inherently contextual, and rely on the frequently quoted dictum from Firth that “you shall know a word from the company it keeps” (Firth, 1957; Pulman, 2013). This leads to the so-called distributional hypothesis that words that occur in similar contexts tend to have similar meanings, and to various proposals for how to implement this hypothesis (Curran, 2004), including alternative definitions of context; a</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000219">
<title confidence="0.991527">
Generating effective referring expressions using charts
</title>
<author confidence="0.99072">
Nikos Engonopoulos and Alexander Koller
</author>
<affiliation confidence="0.7457885">
University of Potsdam, Germany
{engonopo|akoller}@uni-potsdam.de
</affiliation>
<sectionHeader confidence="0.991403" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977266666667">
We present a novel approach for generat-
ing effective referring expressions (REs).
We define a synchronous grammar formal-
ism that relates surface strings with the
sets of objects they describe through an ab-
stract syntactic structure. The grammars
may choose to require or not that REs are
distinguishing. We then show how to com-
pute a chart that represents, in finite space,
the complete (possibly infinite) set of valid
REs for a target object. Finally, we pro-
pose a probability model that predicts how
the listener will understand the RE, and
show how to compute the most effective
RE according to this model from the chart.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99998046969697">
The fundamental challenge in the generation of re-
ferring expressions (REG) is to compute an RE
which is effective, i.e. understood as intended by
the listener. Throughout the history of REG, we
have approximated this as the problem of generat-
ing distinguishing REs, i.e. REs that are only satis-
fied by a unique individual in the domain. This has
been an eminently successful approach, as doc-
umented e.g. in the overview article of Krahmer
and van Deemter (2012) and a variety of recent
shared tasks involving RE generation (Gatt and
Belz, 2010; Belz et al., 2008; Koller et al., 2010).
Nonetheless, reducing effectiveness to unique-
ness is limiting in several ways. First, in complex,
real-world scenes it may not be feasible to gener-
ate fully distinguishing REs, or these may have to
be exceedingly complicated. It is also not neces-
sary to generate distinguishing REs in such situa-
tions, because listeners are very capable of taking
the discourse and task context into account to re-
solve even ambiguous REs. Conversely, listeners
can misunderstand even a distinguishing RE, so
uniqueness is no guarantee for success. We pro-
pose instead to define and train a probabilistic RE
resolution model P(a|t), which directly captures
the probability that the listener will resolve a given
RE t to some object a in the domain. An RE t will
then be “good enough” if P(a*|t) is very high for
the intended target referent a*.
Second, in an interactive setting like the GIVE
Challenge (Koller et al., 2010), the listener may
behave in a way that offers further information on
how they resolved the generated RE. Engonopou-
los et al. (2013) showed how an initial estimate
of the distribution P(a|t) can be continuously up-
dated based on the listener’s behavior, and that this
can improve a system’s ability to detect misunder-
standings. It seems hard to achieve this in a prin-
cipled way without an explicit model of P(a|t).
In this paper, we present an algorithm that gen-
erates the RE t that maximizes P(a*|t), i.e. the
RE that has the highest chance to be understood
correctly by the listener according to the proba-
bilistic RE resolution model. This is a challeng-
ing problem, since the algorithm must identify that
RE from a potentially infinite set of valid alterna-
tives. We achieve this by using a chart-based al-
gorithm, a standard approach in parsing and real-
ization, which has (to our knowledge) never been
used in REG.
We start by defining a synchronous grammar
formalism that relates surface strings to their in-
terpretations as sets of objects in a given domain
(Section 3). This formalism integrates REG with
surface realization, and allows us to specify in the
grammar whether REs are required to be distin-
guishing. We then show how to compute a chart
for a given grammar and target referent in Sec-
tion 4. Section 5 defines a log-linear model for
P(a|t), and presents a Viterbi-style algorithm for
computing the RE t from the chart that maximizes
P(a*|t). Section 6 concludes by discussing how
to apply our algorithm to the state-of-the-art ap-
proaches of Krahmer et al. (2003) and Golland et
al. (2010), and how to address a particular chal-
lenge involving cycles that arises when dealing
</bodyText>
<page confidence="0.990189">
6
</page>
<note confidence="0.8982245">
Proceedings of the INLG and SIGDIAL 2014 Joint Session, pages 6–15,
Philadelphia, Pennsylvania, 19 June 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.891327">
with probabilistic listener models.
</bodyText>
<sectionHeader confidence="0.999338" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.986373973684211">
RE generation is the task of generating a natural-
language expression that identifies an object to the
listener. Since the beginnings of modern REG
(Appelt, 1985; Dale and Reiter, 1995), this prob-
lem has been approximated as generating a dis-
tinguishing description, i.e. one which fits only
one object in the domain and not any of the oth-
ers. This perspective has made it possible to apply
search-based (Kelleher and Kruijff, 2006), logic-
based (Areces et al., 2008) and graph-based (Krah-
mer et al., 2003) methods to the problem, and
overall has been one of the success stories of NLG.
However, in practice, human speakers fre-
quently overspecify, i.e. they include information
in an RE beyond what is necessary to make it
distinguishing (Wardlow Lane and Ferreira, 2008;
Koolen et al., 2011). An NLG system, too, might
include redundant information in an RE to make it
easier to understand for the user. Conversely, an
RE that is produced by a human can often be eas-
ily resolved by the listener even if it is ambiguous.
Here we present an NLG system that directly uses
a probabilistic model of RE resolution, and is ca-
pable of generating ambiguous REs if it predicts
that the listener will understand them.
Most existing REG algorithms focus on gener-
ating distinguishing REs, and then select the one
that is best according to some criterion, e.g. most
human-like (Krahmer et al., 2003; FitzGerald et
al., 2013) or most likely to be understood (Garoufi
and Koller, 2013). By contrast, Mitchell et al.
(2013) describe a stochastic algorithm that com-
putes human-like, non-relational REs that may not
be distinguishing. Golland et al. (2010) are close
to our proposal in spirit, in that they use a log-
linear probability model of RE resolution to com-
pute a possibly non-distinguishing RE. However,
they use a trivial REG algorithm which is limited
to grammars that only permit a (small) finite set of
REs for each referent. This is in contrast to gen-
eral REG, where there is typically an infinite set
of valid REs, especially when relational REs (“the
button to the left of the plant”) are permitted.
Engonopoulos et al. (2013) describe how to up-
date an estimate for P(a|t) based on a log-linear
model based on observations of the listener’s be-
havior. They use a shallow model based on a string
t and not an RE derived from a grammar, and they
do not discuss how to generate the best t. The al-
gorithm we develop here fills this gap.
Our formalism for REG can be seen as a syn-
chronous grammar formalism; it simultaneously
derives strings and their interpretations, connect-
ing the two by an abstract syntactic representa-
tion. This allows performing REG and surface re-
alization with a single algorithm, along the lines
of SPUD (Stone et al., 2003) and its planning-
based implementation, CRISP (Koller and Stone,
2007). Probabilistic synchronous grammars are
widely used in statistical machine translation (Chi-
ang, 2007; Graehl et al., 2008; Jones et al., 2012)
and semantic parsing (Zettlemoyer and Collins,
2005; Wong and Mooney, 2007). Lu and Ng
(2011) have applied such grammars to surface re-
alization. Konstas and Lapata (2012) use related
techniques for content selection and surface real-
ization (with simple, non-recursive grammars).
Charts are standard tools for representing a
large space of possible linguistic analyses com-
pactly. Next to their use in parsing, they have also
been applied to surface realization (Kay, 1996;
Carroll et al., 1999; Kaplan and Wedekind, 2000).
To our knowledge, ours is the first work using
charts for REG. This is challenging because the
input to REG is much less structured than in pars-
ing or realization.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="method">
3 Grammars for RE generation
</sectionHeader>
<bodyText confidence="0.9999045">
We define a new grammar formalism that we use
for REG, which we call semantically intepreted
grammar (SIG). SIG is a synchronous grammar
formalism that relates natural language strings
with the sets of objects in a given domain which
they describe. It uses regular tree grammars
(RTGs) to describe languages of derivation trees,
which then project to strings and sets.
</bodyText>
<subsectionHeader confidence="0.999613">
3.1 Derivation trees
</subsectionHeader>
<bodyText confidence="0.995406461538462">
We describe the abstract syntax of an RE by its
derivation tree, which is a tree over some ranked
signature E of symbols representing lexicon en-
tries and grammatical constructions. A (ranked)
signature is a finite set of symbols r E E, each
of which is assigned an arity ar(r) E N0. A tree
over the signature E is a term r(t1, ... , tn), where
r E E, n = ar(r), and t1, ... , tn are trees over E.
We write T⌃ for the set of all trees over E.
Fig. 1b shows an example derivation tree for
the RE “the square button” over the signature
E = {def |1, square|1, button|0}, where r|n indi-
cates that the symbol r has arity n. In term nota-
</bodyText>
<page confidence="0.9976">
7
</page>
<figure confidence="0.997627">
(a) (b) (c)
</figure>
<figureCaption confidence="0.991283">
Figure 1: A SIG derivation tree (b) with its inter-
pretations (a, c).
</figureCaption>
<bodyText confidence="0.9263408">
tion, it is def (square(button)).
String interpretation. We interpret derivation
trees simultaneously as strings and sets. First, let
A be a finite alphabet, and let A* be the string al-
gebra over A. We define a string interpretation
over A as a function IS that maps each r|n 2 E
to a function IS(r) : (A*)n -+ A*. For instance,
we can assign string interpretations to our exam-
ple signature E as follows; we write w1 • w2 for
the concatenation of the strings w1 and w2.
</bodyText>
<equation confidence="0.999924666666667">
IS(def)(w1) = the • w1
IS(square)(w1) = square • w1
IS(button) = button
</equation>
<bodyText confidence="0.999848791666667">
Since the arity of IS(r) is the same as the ar-
ity of r for any r 2 E, we can use IS to recur-
sively map derivation trees to strings. Starting at
the leaves, we map the tree r(t1, ... , tn) to the
string IS(r)(IS(t1), ... , IS(tn)), where IS(ti) is
the string that results from recursively applying IS
to the subtree ti. In the example, the subtree button
is mapped to the string “button”. We then get
the string for the subtree square(button) by con-
catenating this with “square”, obtaining the string
“square button” and so on, as shown in Fig. 1c.
Relational interpretation. We further define a
relational interpretation IR, which maps each
r|n 2 E to a function IR(r) : R(U)n -+ R(U),
where R(U) is a class of relations. We define IR
over some first-order model structure M = (U, L),
where U is a finite universe U of individuals and L
interprets a finite set of predicate symbols as rela-
tions over U. We let R(U) be the set of all k-place
relations over U for all k &gt; 0. The subsets of U
are the special case of k = 1. We write k(R) for
the arity of a relation R 2 R(U).
For the purposes of this paper, we construct IR
by combining the following operations:
</bodyText>
<listItem confidence="0.9691645">
• The denotations of the atomic predicate sym-
bols of M; see Fig. 2 for an example.
</listItem>
<equation confidence="0.75359225">
U = {b1, b2, b3} button = {b1, b2, b3}
round = {b1, b3} square = {b2}
left of = {(b1, b2), (b2, b3)}
right of = {(b2, b1), (b3, b2)}
</equation>
<figureCaption confidence="0.903184">
Figure 2: A simple model, illustrated as a graph.
</figureCaption>
<listItem confidence="0.9860219">
• proji(R) = {ai  |(a1,... , ak(R)) 2 R} is
the projection to the i-th component; if i &gt;
k(R), it evaluates to 0.
• R1 ni R2 = {(a1, ... , ak(Rl)) 2 R1  |ai 2
R2} is the intersection on the i-th component
of R1; if i &gt; k(R1), it evaluates to 0.
• For any a 2 U, uniqa(R) evaluates to {a} if
R = {a}, and to 0 otherwise.
• For any a 2 U, membera(R) evaluates to
{a} if a 2 R, and to 0 otherwise.
</listItem>
<bodyText confidence="0.999483571428572">
For the example, we assume that we want to
generate REs over the scene shown in Fig. 2; it
consists of the universe U = {b1, b2, b3} and inter-
prets the atomic predicate symbols button, square,
round, left of, and right of. Given this, we can
assign a relational interpretation to the derivation
tree in Fig. 1b using the following mappings:
</bodyText>
<equation confidence="0.999898333333333">
IR(def)(R1) = R1
IR(square)(R1) = square n1 R1
IR(button) = button
</equation>
<bodyText confidence="0.999973333333333">
We evaluate a derivation tree to a relation as we
did for strings (cf. Fig. 1a). The subtree button
maps to the denotation of the symbol button, i.e.
{b1, b2, b3}. The subtree square(button) evaluates
to the intersection of this set with the set of square
individuals, i.e. {b2}; this is also the relational in-
terpretation of the entire derivation tree. We thus
see that “the square button” is an RE that describes
the individual b2 uniquely.
</bodyText>
<subsectionHeader confidence="0.999389">
3.2 Semantically interpreted grammars
</subsectionHeader>
<bodyText confidence="0.9939558">
Now we define grammars that describe relations
between strings and relations over U. We achieve
this by combining a regular tree grammar (RTG,
(G´ecseg and Steinby, 1997; Comon et al., 2007)),
describing a language of derivation trees, with a
string interpretation and a relational interpretation.
An RTG G = (N, E, 5, P) consists of a finite
set N of nonterminal symbols, a ranked signa-
ture E, a start symbol 5 2 N, and a finite set
P of production rules A -+ r(B1, ..., Bn), where
</bodyText>
<figure confidence="0.998998235294118">
{b2}
IR
������def
IS
���� “the square button”
1
0
/ square
J B B B @
button
•
1
the • A C C C
square button
(square
\1
button
</figure>
<page confidence="0.988007">
8
</page>
<bodyText confidence="0.99979303125">
A, B1, ... , B,,, E N and r|,,, E E. We say that
a tree t2 E TE can be derived in one step from
t1 E TE, t1 ==�&apos; t2, if it can be obtained by replac-
ing an occurrence of B in t1 with t and P con-
tains the rule B -+ t. A tree t,,, E TE can be
derived from t1, t1 ==�,⇤ t,,,, if there is a sequence
t1 ==�, ... ==�, t,,, of length n &gt; 0. For any nontermi-
nal A, we write LA(G) for the set of trees t E TE
with A ==�,⇤ t. We simply write L(G) for LS(G)
and call it the language of G.
We define a semantically interpreted grammar
(SIG) as a triple G = (G, IS, IR) of an RTG G
over some signature E, together with a string inter-
pretation IS over some alphabet A and a relational
interpretation IR over some universe U, both of
which interpret the symbols in E. We assume that
every terminal symbol r E E occurs in at most
one rule, and that the nonterminals of G are pairs
Ab of a syntactic category A and a semantic index
b = ix(Ab). A semantic index indicates the indi-
vidual in U to which a given constituent is meant
to refer, see e.g. (Kay, 1996; Stone et al., 2003).
Note that SIGs can be seen as specific Interpreted
Regular Tree Grammars (Koller and Kuhlmann,
2011) with a set and a string interpretation.
We ignore the start symbol of G. Instead, we
say that given some individual b E U and syntactic
category A, the set of referring expressions for b is
REg(A,b) = {t E LAb(G)  |IR(t) = {b}}, i.e.
we define an RE as a derivation tree that G can
derive from Ab and whose relational interpretation
is {b}. From t, we can read off the string IS(t).1
</bodyText>
<subsectionHeader confidence="0.999214">
3.3 An example grammar
</subsectionHeader>
<bodyText confidence="0.979429">
Consider the SIG G in Fig. 3 for example. The
grammar is written in template form. Each rule
is instantiated for all semantic indices specified
in the line above; e.g. the symbol round denotes
the set {b1, b3}, therefore there are rules Nb1 -+
roundb1(Nb1) and Nb3 -+ roundb3(Nb3). The val-
ues of IR and IS for each symbol are specified
below the RTG rule for that symbol.
We can use G to generate NPs that refer to the
target referent b2 given the model shown in Fig. 2
by finding trees in LNPb2 (G) that refer to {b2}.
One such tree is t1 = def b2(squareb2(buttonb2)),
a more detailed version of the tree in Fig. 1b.
It can be derived by NPb2 ==�, def b2(Nb2) ==�&apos;
defb2(squareb2(Nb2)) ==�, t1. Because IR(t1) =
{b2}, we see that t1 E REg(NP, b2); it represents
1Below, we will often write the RE as a string when the
derivation tree is clear.
for all a E U:
</bodyText>
<equation confidence="0.984587347826087">
NPa -+ defa(Na)
IS(defa)(w1) = the • w1
IR(defa)(R1) = membera(R1)
for all a E button:
Na -+ buttona
IS(buttona) = button
IR(buttona) = button
for all a E round:
Na -+ rounda(Na)
IS(rounda)(w1) = round • w1
IR(rounda)(R1) = round n1 R1
for all a E square:
Na -+ squarea(Na)
IS(squarea)(w1) = square • w1
IR(squarea)(R1) = square n1 R1
for all a, b E left of:
Na -+ leftofa,b(Na, NPb)
IS(leftof a,b)(w1, w2) = w1 • to • the • left • of • w2
IR(leftofa,b)(R1, R2) = proj1((left of n1 R1) n2 R2)
for all a, b E right of:
Na -+ rightofa,b(Na,NPb)
IS(rightof a,b)(w1, w2) = w1 • to • the • right • of • w2
IR(rightofa,b)(R1, R2) = proj1((right of n1 R1) n2 R2)
</equation>
<figureCaption confidence="0.994945">
Figure 3: An example SIG grammar.
</figureCaption>
<bodyText confidence="0.99967512">
the string IS(t1) = “the square button”.
A second derivation tree for b2 is t2 =
def b2(squareb2(squareb2(buttonb2))), correspond-
ing to IS(t2) = “the square square button”. It de-
rives from NPb2 in four steps, and has IR(t2) =
{b2}. Even the small grammar G licences an infi-
nite set of REs for b2, all of which are semantically
correct. Avoiding the generation of nonsensical
REs like “the square square button” is a techni-
cal challenge to which we will return in Section 6.
G can also derive relational REs; for instance, the
derivation tree in Fig. 6 for the string “the button
to the left of the square button” is in REg(NP, b1).
Finally, G considers the non-distinguishing t3 =
def b2(buttonb2) (for “the button”) a valid RE for
b2. This is because memberb2 will quietly project
the set {b1, b2, b3} (to which buttonb2 refers) to
{b2}. As discussed in previous sections, we want
to allow such non-unique REs and delegate the
judgment about their quality to the probability
model. It would still be straightforward, however,
to impose a hard uniqueness constraint, by simply
changing IR(defa)(R1) to uniqa(R1) in Fig. 3.
This would yield IR(t3) = 0, i.e. t3 would no
longer be in REg(NP, b2).
</bodyText>
<sectionHeader confidence="0.995487" genericHeader="method">
4 Chart-based RE generation
</sectionHeader>
<bodyText confidence="0.999968833333333">
We now present a chart-based algorithm for gener-
ating REs with SIG grammars. Charts allow us to
represent all REs for a target referent compactly,
and can be computed efficiently. We show in Sec-
tion 5 that charts also lend themselves well to com-
puting the most effective RE.
</bodyText>
<page confidence="0.598995">
9
</page>
<equation confidence="0.999624724137931">
Nb1 /{b1, b2, b3} ! buttonb1
Nb2/{b1, b2, b3} ! buttonb2
Nb3/{b1, b2, b3} ! buttonb3
Nb1 /{b1, b3} ! roundb1(Nb1 /{b1, b2, b3})
Nb3/{b1, b3} ! roundb3 (Nb3/{b1, b2, b3})
Nb1 /{b1, b3} ! roundb1 (Nb1 /{b1, b3})
Nb3 /{b1,b3} ! roundb3 (Nb3 /{b1, b3})
Nb2/{b2} ! squareb2(Nb2 /{b1, b2, b3})
Nb2/{b2} ! squareb2(Nb2/{b2})
NPb2 /{b2} ! defb2 (Nb2/{b1, b2, b3})
NPb2 /{b2} ! defb2(Nb2/{b2})
Nb1 /{b1} ! leftofb1,b2(Nb1 /{b1, b2, b3}, NPb2/{b2})
Nb1 /{b1} ! leftofb1,b2(Nb1 /{b1, b3}, NPb2/{b2})
Nb1 /{b1} ! leftofb1,b2(Nb1 /{b1}, NPb2/{b2})
Nb1 /{b1} ! roundb1(Nb1 /{b1})
NPb1 /{b1} ! defb1 (Nb1 /{b1, b2, b3})
NPb1/{b1} ! defb1 (Nb1/{b1, b3})
NPb1/{b1} ! defb1 (Nb1/{b1})
Nb3/{b3} ! rightofb3,b2(Nb3/{b1, b2, b3}, NPb2/{b2})
Nb3/{b3} ! rightofb3,b2(Nb3/{b1, b3}, NPb2/{b2})
Nb3/{b3} ! rightofb3,b2(Nb3/{b3}, NPb2/{b2})
Nb3/{b3} ! roundb3(Nb3/{b3})
NPb3/{b3} ! defb3(Nb3 /{b1, b2, b3})
NPb3/{b3} ! defb3(Nb3/{b1, b3})
NPb3/{b3} ! defb3(Nb3/{b3})
Nb2/{b2} ! leftofb2,b3(Nb2/{b1, b2, b3}, NPb3/{b3})
Nb2 /{b2} ! rightofb2,b1 (Nb2 /{b1, b2, b3}, NPb1 /{b1})
Nb2/{b2} ! leftofb2,b3(Nb2/{b2}, NPb3/{b3})
Nb2 /{b2} ! rightofb2,b1 (Nb2 /{b2}, NPb1 /{b1})
</equation>
<figureCaption confidence="0.999031">
Figure 4: The chart for the grammar in Fig. 3.
</figureCaption>
<subsectionHeader confidence="0.698262">
4.1 RE generation charts
</subsectionHeader>
<bodyText confidence="0.990993">
Generally speaking, a chart is a packed data struc-
ture which describes how larger syntactic repre-
sentations can be recursively built from smaller
ones. In applications such as parsing and sur-
face realization, the creation of a chart is driven
by the idea that we consume some input (words
or semantic atoms) as we build up larger struc-
tures. The parallel to this intuition in REG is that
“larger” chart entries are more precise descriptions
of the target, which is a weaker constraint than
input consumption. Nonetheless, we can define
REG charts whose entries are packed representa-
tions for large sets of possible REs, and compute
them in terms of these entries instead of RE sets.
Technically, we represent charts as RTGs over
an extended set of nonterminals. A chart for gener-
ating an RE of syntactic category A for an individ-
ual b E U is an RTG C = (N0, ⌃, S0, P0), where
N0 C_ N x R(U) and S0 = Ab/{b}. Intuitively,
the nonterminal Ab/{a1, ... , an} expresses that
we intend to generate an RE for b from A, but each
RE that we can derive from the nonterminal actu-
ally denotes the referent set {a1,... , an}.
A chart for the grammar in Fig. 3 is shown
in Fig. 4. To generate an NP for b2, we let
its start symbol be S0 = NPb2/{b2}. The rule
Nb2/{b1, b2, b3} -+ buttonb2 says that we can gen-
erate an RE t with IR(t) = {b1, b2, b3} from the
nonterminal symbol Nb2 by expanding this symbol
with the grammar rule Nb2 -+ buttonb2. Similarly,
</bodyText>
<equation confidence="0.58701125">
A -+ r(B1, ..., Bn) in G
B01 = B1/R1,..., B0 n = Bn/Rn in N0
Add A0 = A/IR(r)(R1, ..., Rn) to N0
Add rule A0 -+ r(B01, ..., B0n) to P0
</equation>
<figureCaption confidence="0.998285">
Figure 5: The chart computation algorithm.
</figureCaption>
<bodyText confidence="0.9974598">
the rule Nb2/{b2} -+ squareb2(Nb2/{b1, b2, b3})
expresses that we can generate an RE with
IR(t) = {b2} by expanding the nonterminal sym-
bol Nb2 into squareb2(t0), where t0 is any tree that
the chart can generate from Nb2/{b1, b2, b3}.
</bodyText>
<subsectionHeader confidence="0.999502">
4.2 Computing a chart
</subsectionHeader>
<bodyText confidence="0.9977976">
Given a SIG G, a syntactic category A, and a
target referent b, we can compute a chart C for
REg(A, b) using the parsing schema in Fig. 5.
The schema assumes that we have a rule A -+
r(B1, ... , Bn) in G; in addition, for each 1 &lt;
i &lt; n it assumes that we have already added
the nonterminal B0 i = Bi/Ri to the chart, in-
dicating that there is a tree ti with Bi =�,⇤ ti
and IR(ti) = Ri. Then we know that t =
r(t1, ... , tn) can be derived from A and that R0 =
IR(t) = IR(r)(R1, ... , Rn). We can therefore
add the nonterminal A0 = A/R0 and the produc-
tion rule A0 -+ r(B01, ... , B0n) to the chart; this
rule can be used as the first step in a derivation of t
from A0. We can optimize the algorithm by adding
A0 and the rule only if R0 =� 0.
The algorithm terminates when it can add no
more rules to the chart. Because U is finite, this
always happens after a finite number of steps, even
if there is an infinite set of REs. For instance, the
chart in Fig. 4 describes an infinite language of
REs, including “the square button”, “the button to
the left of the round button”, “the button to the left
of the button to the right of the square button”, etc.
Thus it represents relational REs that are nested
arbitrarily deeply through a finite number of rules.
After termination, the chart contains all rules by
which a nonterminal can be decomposed into other
(productive) nonterminals. As a result, L(C) con-
tains exactly the REs for b of category A:
</bodyText>
<construct confidence="0.956532333333333">
Theorem 1 If C is a chart for the SIG G, the syn-
tactic category A, and the target referent b, then
L(C) = REg(A, b).
</construct>
<sectionHeader confidence="0.972256" genericHeader="method">
5 Computing best referring expressions
</sectionHeader>
<bodyText confidence="0.9994692">
The chart algorithm allows us to compactly rep-
resent all REs for the target referent. We now
show how to compute the best RE from the chart.
We present a novel probability model P(b|t) for
RE resolution, and take the “best” RE to be the
</bodyText>
<page confidence="0.937451">
10
</page>
<figure confidence="0.9425002">
t b1 b2 b3
“the button” 0.33 0.33 0.33
“the round button” 0.45 0.10 0.45
“the button to the left 0.74 0.14 0.12
of the square button”
</figure>
<figureCaption confidence="0.909859333333333">
Figure 7: Probability distributions for some REs t.
Figure 6: The derivation tree for “the button to the
left of the square button”.
</figureCaption>
<bodyText confidence="0.996024375">
one with the highest chance to be understood as
intended. Next to the best RE itself, the algo-
rithm also computes the entire distribution P(b|t),
to support later updates in an interactive setting.
Nothing in our algorithm hinges on this par-
ticular model; it can also be used with any other
scoring model that satisfies a certain monotonicity
condition which we spell out in Section 5.2.
</bodyText>
<subsectionHeader confidence="0.974547">
5.1 A log-linear model for effective REs
</subsectionHeader>
<bodyText confidence="0.999965866666667">
We model the probability P(b|t) that the listener
will resolve the RE t to the object b using a
log-linear model with a set of feature functions
f(a, t, M), where a is an object, t is a derivation
tree, and M is the relational interpretation model.
We focus on features that only look at informa-
tion that is local to a specific subtree of the RE,
such as the label at the root. For instance, a feature
fround(a, t&apos;, M) might return 1 if the root label of
t&apos; is rounda and a is round in M, and 0 otherwise.
Another feature fdef(a, t&apos;, M) might return 1/k if
t&apos; is of the form def b(t&apos;&apos;), R = ZR(t&apos;&apos;) has k el-
ements, and a E R; and 0 otherwise. This fea-
ture counterbalances the ability of the grammar in
Fig. 3 to say “the w” even when w is a non-unique
description by penalizing descriptions with many
possible referents through lower feature values.
When generating a relational RE, the derivation
tree naturally splits into separate regions, each of
which is meant to identify a specific object. These
regions are distinguished by the semantic indices
in the nonterminals that derive them; e.g., in Fig. 6,
the subtree for “the square button” is an attempt to
refer to b2, whereas the RE as a whole is meant to
refer to b1. To find out how effective the RE is as
a description of b1, we evaluate the features at all
nodes in the region top(t) containing the root of t.
Each feature function fi is associated with a
weight wi. We obtain a score tuple sc(t&apos;) for some
subtree t&apos; of an RE as follows:
</bodyText>
<equation confidence="0.897376">
sc(t&apos;) = (s(a1, t&apos;, M),... , s(am, t&apos;, M)),
</equation>
<bodyText confidence="0.876788142857143">
where U = {a1, ... , am} and s(a, t&apos;, M) =
Eni=1 wi · fi(a, t&apos;, M). We then combine these
into a score tuple score(t) = EuEtop(t) sc(t.u)
for the whole RE t, where t.u is the subtree of
t below the node u. Finally, given a score tuple
s = (s1, ... , sm) for t, we define the usual log-
linear probability distribution as
</bodyText>
<equation confidence="0.988116">
esi
P(ai|t) = prob(ai, s) = Emj=1 esj .
The best RE for the target referent b is then
bestG(A, b) = arg max prob(b, sc(t)).
tEREG(A,b)
</equation>
<bodyText confidence="0.999868357142857">
For illustration, we consider a number of REs
for b1 in our running example. We use fround and
fdef and let wround = wdef = 1. In this case, the
RE “the button” has a score tuple (1/3,1/3,1/3),
which is the sum of the tuple (0, 0, 0) for fround
(since the RE does not use the “round” rule) and
the tuple (1/3,1/3,1/3) for fdef (since “button”
is three-way ambiguous in M). This yields a uni-
form probability distribution over U (see Fig. 7).
By contrast, “the round button” gets (3/2, 0, 3/2),
resulting in the distribution in the second line of
Fig. 7. This RE is judged better than “the button”
because it assigns a higher probability to b1.
Relational REs involve derivation trees with
multiple regions, only the top one of which is di-
rectly counted for P(b|t) (see Fig. 6). We incorpo-
rate the quality of the other regions through appro-
priate features. In the example, we use a feature
fleftof(a, t&apos;, M) = Eb:(a,b)Eleft of P(b|t&apos;&apos;), where
t&apos;&apos; is the second subtree of t&apos;. This feature com-
putes the probability that the referent to which the
listener resolves t&apos;&apos; is actually to the right of a,
and will thus take a high value if t&apos;&apos; is a good
RE for b2. Assuming a probability distribution of
P(b2|t&apos;) = 0.78 and P(b1|t&apos;) = P(b3|t&apos;) = 0.11
for t&apos; =“the square button”, we get the tuple
(0.78, 0.11, 0) for fleftof, yielding the third line
of Fig. 7 for wleftof = 1.
</bodyText>
<page confidence="0.99766">
11
</page>
<subsectionHeader confidence="0.999114">
5.2 Computing the best RE
</subsectionHeader>
<bodyText confidence="0.999364139534884">
We compute bestG(A, b) from the chart by adapt-
ing the Viterbi algorithm. Our key data structure
assigns a score tuple is(A0) to each nonterminal
A0 in the chart. Intuitively, if the semantic index
of A0 is b, then is(A0) is the score tuple sc(t) for
the tree t E LA0(C) which maximizes P(b|t). We
also record this best tree as bt(A0). Thus the al-
gorithm is correct if, after running it, we obtain
bestg(A, b) = bt(Ab/{b}).
As is standard in chart algorithms, we limit our
attention to features whose values can be com-
puted bottom-up by local operations. Specifically,
we assume that if A0 -+ r(B01,..., B0n) is a rule in
the chart and tz is the best RE for B0 z for all i, then
the best RE for A0 that can be built using this rule
is r(t1, ... , tn). This means that features must be
monotonic, i.e. that the RE that seemed locally
best for B0 z leads to the best RE overall.
Under this assumption, we can compute is(A0)
and bt(A0) bottom-up as shown in Fig. 8. We it-
erate over all nonterminals A0 in the chart in a
fixed linear order, which we call the evaluation
order. Then we compute is(A0) and bt(A0) by
maximizing over the rules for A0. Assume that
the best RE for A0 can be constructed using the
rule A0 -+ r(B01, ... , B0n). Then if, at the time we
evaluate A0, we have fully evaluated all the B0 z in
the sense that bt(B0z) is actually the best RE for
B0z, the algorithm will assign the best RE for A0
to bt(A0), and its score tuple to is(A0). Thus, if
we call an evaluation order exact if the nontermi-
nals on the right-hand side of each rule in the chart
come before the nonterminal on the left-hand side,
we can inductively prove the following theorem:
Theorem 2 If the evaluation order is exact, then
for every nonterminal A0 in the chart, we ob-
tain bt(A0) = arg maxt2LA0(C) P(ix(A0)|t) and
is(A0) = sc(bt(A0)).
In other words, the algorithm is correct if the
evaluation order is exact. If it is not, we might
compute a sub-optimal RE as bt(A0), which un-
derestimates is(A0). The choice of evaluation or-
der is thus crucial.
</bodyText>
<sectionHeader confidence="0.986748" genericHeader="method">
6 Evaluating charts with cycles
</sectionHeader>
<bodyText confidence="0.999813">
It remains to show how we can determine an ex-
act evaluation order for a given chart. One way to
think about the problem is to consider the order-
ing graph O(C) of the chart C (see Fig. 9 for an
example). This is a directed graph whose nodes
</bodyText>
<listItem confidence="0.859221333333333">
1: for nonterminals A0 in evaluation order do
2: for rules r of the form A0 -+ r(B01, ... ,B0n) do
3: a = ix(A0)
</listItem>
<equation confidence="0.990286">
4: t0 = r(bt(B01), ... , bt(B0n))
n
5: s = sc(t0) + is(B0i)
i=1
ix(B0i)=a
6: if prob(a, s) &gt; prob(a, is(A0)) then
7: is(A0) = s
8: bt(A0) = t0
</equation>
<figureCaption confidence="0.996139">
Figure 8: Computing the best RE.
</figureCaption>
<bodyText confidence="0.999640904761905">
are the nonterminals of the chart; for each rule
A0 -+ r(B01,..., B0n) in C, it has an edge from
B0 z to A0 for each i. If this graph is acyclic, we
can simply compute a topological sort of O(C)
to bring the nodes into a linear order in which
each B0 z precedes A0. This is enough to evalu-
ate charts using certain simpler models. For in-
stance, we can apply our REG algorithm to the
log-linear model of Golland et al. (2010). Because
they only generate REs with a bounded number of
relations, their grammars effectively only describe
finite languages. In such a case, our charts are al-
ways acyclic, and therefore a topological sort of
O(C) yields an exact evaluation order.
This simple approach will not work with gram-
mars that allow arbitrary recursion, as they can
lead to charts with cycles (indicating an infinite
set of valid REs). E.g. the chart in Fig. 4 contains
a rule Nb2/{b2} -+ squareb2(Nb2/{b2}) (shown
in Fig. 9), which can be used to construct the RE
t0 = “the square square button” in addition to the
RE t = “the square button”. Such cycles can be
increasing with respect to a log-linear probability
model, i.e. the model considers t0 a better RE than
t. Indeed, t has a score tuple of (0, 2, 0), giving
P(b2|t) = 0.78. By contrast, t0 has a score tuple
of (0, 3, 0), thus P(b2|t0) = 0.91. This can be con-
tinued indefinitely, with each addition of “square”
increasing the probability of being resolved to b2.
Thus, there is no best RE for b2; every RE can be
improved by adding another copy of “square”.
In such a situation, it is a challenge to even
compute any score for every nonterminal without
running into infinite loops. We can achieve this
by decomposing O(C) into its strongly connected
components (SCCs), i.e. the maximal subgraphs in
which each node is reachable from any other node.
We then consider the component graph O0(C); its
nodes are the SCCs of O(C), and it has an edge
from c1 to c2 if O(C) has an edge from some
node in c1 to some node in c2. O0(C) is acyclic
by construction, so we can compute a topological
</bodyText>
<page confidence="0.968977">
12
</page>
<figure confidence="0.683674">
rithm can make use of (see Fig. 10).
</figure>
<figureCaption confidence="0.8956405">
Figure 9: A fragment of the ordering graph for the
chart in Fig. 4. Dotted boxes mark SCCs.
Figure 10: A fragment of a chart ordering graph
for a grammar with enriched nonterminals.
</figureCaption>
<bodyText confidence="0.978392414634147">
sort and order all nonterminals from earlier SCCs
before all nonterminals from later SCCs. Within
each SCC, we order the nonterminals in the order
in which they were discovered by the algorithm in
Fig. 5. This yields a linear order on nonterminals,
which at least ensures that by the time we evaluate
a nonterminal A&apos;, there is at least one rule for A&apos;
whose right-hand nonterminals have all been eval-
uated; so is(A&apos;) gets at least some value.
In our example, we obtain the order
Nb2/{b1, b2, b3}, Nb2/{b2}, NPb2/{b2}. The
rule Nb2/{b2} -+ squareb2(Nb2/{b2}) will thus
not be considered in the evaluation of Nb2/{b2},
and the algorithm returns “the square button”.
The algorithm computes optimal REs for acyclic
charts, and also for charts where all cycles are
decreasing, i.e. using the rules in the cycle make
the RE worse. This enables us, for instance, to
encode the REG problem of Krahmer et al. (2003)
into ours by using a feature that evaluates the rule
for each attribute to its (negative) cost according
to the Krahmer model. Krahmer et al. assume that
every attribute has positive cost, and is only used
if it is necessary to make the RE distinguishing.
Thus all cycles in the chart are decreasing.
One limitation of the algorithm is that it does
not overspecify. Suppose that we extend the ex-
ample model in Fig. 2 with a color predicate
green = {b2}. We might then want to prefer
“the green square button” over “the square but-
ton” because it is easier to understand. But since
all square objects (i.e. {b2}) are also green, using
“green” does not change the denotation of the RE,
i.e. it is represented by a loop from Nb2/{b2} to
Nb2/{b2}, which is skipped by the algorithm. One
idea could be to break such cycles by the careful
use of a richer set of nonterminals in the gram-
mar; e.g., they might record the set of all attributes
that were used in the RE. Our example rule would
then become Nb2/{b2}/{square,green} -+
greenb2(Nb2/{b2}/{square}), which the algo-
</bodyText>
<sectionHeader confidence="0.998438" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999927934782609">
We have shown how to generate REs using charts.
Based on an algorithm for computing a chart of all
valid REs, we showed how to compute the RE that
maximizes the probability of being understood as
the target referent. Our algorithm integrates REG
with surface realization. It generates distinguish-
ing REs if this is specified in the grammar; oth-
erwise, it computes the best RE without regard to
uniqueness, using features that prefer unambigu-
ous REs as part of the probability model.
Our algorithm can be applied to earlier models
of REG, and in these cases is guaranteed to com-
pute optimal REs. The probability model we intro-
duced here is more powerful, and may not admit
“best” REs. We have shown how the algorithm
can still do something reasonable in such cases,
but this point deserves attention in future research,
especially with respect to overspecification.
We evaluated the performance of our chart al-
gorithm on a number of randomly sampled in-
put scenes from the GIVE Challenge, which con-
tained 24 objects on average. Our implementa-
tion is based on the IRTG tool available at irtg.
googlecode.com. While in the worst case the
chart computation is exponential in the input size,
in practice runtimes did not exceed 60 ms for the
grammar shown in Fig. 3.
We have focused here on computing best REs
given a probability model. We have left train-
ing the model and evaluating it on real-world data
for future work. Because our probability model
focuses on effectiveness for the listener, rather
than human-likeness, our immediate next step is to
train it on an interaction corpus which records the
reactions of human listeners to system-generated
REs. A further avenue of research is to deliber-
ately generate succinct but ambiguous REs when
the model predicts them to be easily understood.
We will explore ways of achieving this by combin-
ing the effectiveness model presented here with a
language model that prefers succinct REs.
Acknowledgments. We thank Emiel Krahmer,
Stephan Oepen, Konstantina Garoufi, Martin Vil-
lalba and the anonymous reviewers for their useful
comments and discussions. The authors were sup-
ported by the SFB 632 “Information Structure”.
</bodyText>
<page confidence="0.998819">
13
</page>
<sectionHeader confidence="0.9964" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997777133333333">
Douglas E. Appelt. 1985. Planning English sentences.
Cambridge University Press.
Carlos Areces, Alexander Koller, and Kristina Strieg-
nitz. 2008. Referring expressions as formulas of
description logic. In Proceedings of the 5th Inter-
national Natural Language Generation Conference
(INLG).
Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.
2008. The GREC challenge 2008: Overview and
evaluation results. In Proceedings of the 5th Inter-
national Conference on Natural Language Genera-
tion (INLG).
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznanski. 1999. An efficient chart generator
for (semi-)lexicalist grammars. In Proceedings of
the 7th European Workshop on Natural Language
Generation.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Hubert Comon, Max Dauchet, R´emi Gilleron, Christof
L¨oding, Florent Jacquemard, Denis Lugiez, Sophie
Tison, and Marc Tommasi. 2007. Tree automata
techniques and applications. Available on http:
//tata.gforge.inria.fr/.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean Maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233–263.
Nikos Engonopoulos, Martin Villalba, Ivan Titov, and
Alexander Koller. 2013. Predicting the resolution
of referring expressions from user behavior. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Seattle.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Konstantina Garoufi and Alexander Koller. 2013.
Generation of effective referring expressions in situ-
ated context. Language and Cognitive Processes.
Albert Gatt and Anja Belz. 2010. Introducing shared
task evaluation to NLG: The TUNA shared task
evaluation challenges. In E. Krahmer and M. The-
une, editors, Empirical Methods in Natural Lan-
guage Generation, number 5790 in LNCS, pages
264–293. Springer.
Ferenc G´ecseg and Magnus Steinby. 1997. Tree lan-
guages. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, chap-
ter 1, pages 1–68. Springer-Verlag.
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Computational
Linguistics, 34(3).
B. Jones, J. Andreas, D. Bauer, K.-M. Hermann, and
K. Knight. 2012. Semantics-based machine transla-
tion with hyperedge replacement grammars. In Pro-
ceedings of COLING.
Ron Kaplan and J¨urgen Wedekind. 2000. LFG gener-
ation produces context-free languages. In Proceed-
ings of the 18th COLING.
Martin Kay. 1996. Chart generation. In Proceedings
of the 34th ACL.
John Kelleher and Geert-Jan Kruijff. 2006. Incremen-
tal generation of spatial referring expressions in situ-
ated dialogue. In In Proceedings of Coling-ACL ’06,
Sydney Australia.
Alexander Koller and Marco Kuhlmann. 2011. A gen-
eralized view on parsing and translation. In Pro-
ceedings of the 12th International Conference on
Parsing Technologies, pages 2–13. Association for
Computational Linguistics.
Alexander Koller and Matthew Stone. 2007. Sentence
generation as a planning problem. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics (ACL).
Alexander Koller, Kristina Striegnitz, Donna Byron,
Justine Cassell, Robert Dale, Johanna Moore, and
Jon Oberlander. 2010. The First Challenge on
Generating Instructions in Virtual Environments.
In E. Krahmer and M. Theune, editors, Empirical
Methods in Natural Language Generation, number
5790 in LNAI, pages 337–361. Springer.
Yannis Konstas and Mirella Lapata. 2012. Concept-
to-text generation via discriminative reranking. In
Proceedings of the 50th ACL.
Ruud Koolen, Albert Gatt, Martijn Goudbeek, and
Emiel Krahmer. 2011. Factors causing overspec-
ification in definite descriptions. Journal of Prag-
matics, 43:3231–3250.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173–218.
Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53–72.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceedings
of EMNLP.
</reference>
<page confidence="0.985156">
14
</page>
<reference confidence="0.999792333333333">
Margaret Mitchell, Kees van Deemter, and Ehud Re-
iter. 2013. Generating expressions that refer to vis-
ible objects. In Proceedings of NAACL-HLT, pages
1174–1184.
Matthew Stone, Christine Doran, Bonnie Webber, To-
nia Bleam, and Martha Palmer. 2003. Microplan-
ning with communicative intentions: The SPUD
system. Computational Intelligence, 19(4):311–
381.
Liane Wardlow Lane and Victor Ferreira. 2008.
Speaker-external versus speaker-internal forces on
utterance form: Do cognitive demands override
threats to referential success? Journal of Experi-
mental Psychology: Learning, Memory, and Cogni-
tion, 34:1466–1481.
Yuk Wah Wong and Raymond J. Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of the 45th
ACL.
Luke S. Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. In Proceedings of the 21st Conference
on Uncertainty in Artificial Intelligence (UAI).
</reference>
<page confidence="0.997929">
15
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.711860">
<title confidence="0.998618">Generating effective referring expressions using charts</title>
<author confidence="0.758321">Engonopoulos</author>
<affiliation confidence="0.998432">University of Potsdam,</affiliation>
<email confidence="0.996659">{engonopo|akoller}@uni-potsdam.de</email>
<abstract confidence="0.9963496875">We present a novel approach for generating effective referring expressions (REs). We define a synchronous grammar formalism that relates surface strings with the sets of objects they describe through an abstract syntactic structure. The grammars may choose to require or not that REs are distinguishing. We then show how to compute a chart that represents, in finite space, the complete (possibly infinite) set of valid REs for a target object. Finally, we propose a probability model that predicts how the listener will understand the RE, and show how to compute the most effective RE according to this model from the chart.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
</authors>
<title>Planning English sentences.</title>
<date>1985</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="4361" citStr="Appelt, 1985" startWordPosition="719" endWordPosition="720">(a*|t). Section 6 concludes by discussing how to apply our algorithm to the state-of-the-art approaches of Krahmer et al. (2003) and Golland et al. (2010), and how to address a particular challenge involving cycles that arises when dealing 6 Proceedings of the INLG and SIGDIAL 2014 Joint Session, pages 6–15, Philadelphia, Pennsylvania, 19 June 2014. c�2014 Association for Computational Linguistics with probabilistic listener models. 2 Related Work RE generation is the task of generating a naturallanguage expression that identifies an object to the listener. Since the beginnings of modern REG (Appelt, 1985; Dale and Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object in the domain and not any of the others. This perspective has made it possible to apply search-based (Kelleher and Kruijff, 2006), logicbased (Areces et al., 2008) and graph-based (Krahmer et al., 2003) methods to the problem, and overall has been one of the success stories of NLG. However, in practice, human speakers frequently overspecify, i.e. they include information in an RE beyond what is necessary to make it distinguishing (Wardlow Lane and Ferreir</context>
</contexts>
<marker>Appelt, 1985</marker>
<rawString>Douglas E. Appelt. 1985. Planning English sentences. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Areces</author>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
</authors>
<title>Referring expressions as formulas of description logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Natural Language Generation Conference (INLG).</booktitle>
<contexts>
<context position="4665" citStr="Areces et al., 2008" startWordPosition="769" endWordPosition="772">, pages 6–15, Philadelphia, Pennsylvania, 19 June 2014. c�2014 Association for Computational Linguistics with probabilistic listener models. 2 Related Work RE generation is the task of generating a naturallanguage expression that identifies an object to the listener. Since the beginnings of modern REG (Appelt, 1985; Dale and Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object in the domain and not any of the others. This perspective has made it possible to apply search-based (Kelleher and Kruijff, 2006), logicbased (Areces et al., 2008) and graph-based (Krahmer et al., 2003) methods to the problem, and overall has been one of the success stories of NLG. However, in practice, human speakers frequently overspecify, i.e. they include information in an RE beyond what is necessary to make it distinguishing (Wardlow Lane and Ferreira, 2008; Koolen et al., 2011). An NLG system, too, might include redundant information in an RE to make it easier to understand for the user. Conversely, an RE that is produced by a human can often be easily resolved by the listener even if it is ambiguous. Here we present an NLG system that directly us</context>
</contexts>
<marker>Areces, Koller, Striegnitz, 2008</marker>
<rawString>Carlos Areces, Alexander Koller, and Kristina Striegnitz. 2008. Referring expressions as formulas of description logic. In Proceedings of the 5th International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anja Belz</author>
<author>Eric Kow</author>
<author>Jette Viethen</author>
<author>Albert Gatt</author>
</authors>
<title>The GREC challenge 2008: Overview and evaluation results.</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th International Conference on Natural Language Generation (INLG).</booktitle>
<contexts>
<context position="1373" citStr="Belz et al., 2008" startWordPosition="217" endWordPosition="220">according to this model from the chart. 1 Introduction The fundamental challenge in the generation of referring expressions (REG) is to compute an RE which is effective, i.e. understood as intended by the listener. Throughout the history of REG, we have approximated this as the problem of generating distinguishing REs, i.e. REs that are only satisfied by a unique individual in the domain. This has been an eminently successful approach, as documented e.g. in the overview article of Krahmer and van Deemter (2012) and a variety of recent shared tasks involving RE generation (Gatt and Belz, 2010; Belz et al., 2008; Koller et al., 2010). Nonetheless, reducing effectiveness to uniqueness is limiting in several ways. First, in complex, real-world scenes it may not be feasible to generate fully distinguishing REs, or these may have to be exceedingly complicated. It is also not necessary to generate distinguishing REs in such situations, because listeners are very capable of taking the discourse and task context into account to resolve even ambiguous REs. Conversely, listeners can misunderstand even a distinguishing RE, so uniqueness is no guarantee for success. We propose instead to define and train a prob</context>
</contexts>
<marker>Belz, Kow, Viethen, Gatt, 2008</marker>
<rawString>Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt. 2008. The GREC challenge 2008: Overview and evaluation results. In Proceedings of the 5th International Conference on Natural Language Generation (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Victor Poznanski</author>
</authors>
<title>An efficient chart generator for (semi-)lexicalist grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th European Workshop on Natural Language Generation.</booktitle>
<contexts>
<context position="7643" citStr="Carroll et al., 1999" startWordPosition="1270" endWordPosition="1273">synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. 3 Grammars for RE generation We define a new grammar formalism that we use for REG, which we call semantically intepreted grammar (SIG). SIG is a synchronous grammar formalism that relates natural language strings with the sets of objects in a given domain which they describe. It uses regular tree grammars (RTGs) to describe languages of derivation trees, which then project to strings and sets. 3.1 Deri</context>
</contexts>
<marker>Carroll, Copestake, Flickinger, Poznanski, 1999</marker>
<rawString>John Carroll, Ann Copestake, Dan Flickinger, and Victor Poznanski. 1999. An efficient chart generator for (semi-)lexicalist grammars. In Proceedings of the 7th European Workshop on Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="7108" citStr="Chiang, 2007" startWordPosition="1187" endWordPosition="1189">ot an RE derived from a grammar, and they do not discuss how to generate the best t. The algorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hubert Comon</author>
<author>Max Dauchet</author>
</authors>
<title>R´emi Gilleron, Christof L¨oding, Florent Jacquemard, Denis Lugiez, Sophie Tison, and Marc Tommasi.</title>
<date>2007</date>
<marker>Comon, Dauchet, 2007</marker>
<rawString>Hubert Comon, Max Dauchet, R´emi Gilleron, Christof L¨oding, Florent Jacquemard, Denis Lugiez, Sophie Tison, and Marc Tommasi. 2007. Tree automata techniques and applications. Available on http: //tata.gforge.inria.fr/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean Maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4385" citStr="Dale and Reiter, 1995" startWordPosition="721" endWordPosition="724">n 6 concludes by discussing how to apply our algorithm to the state-of-the-art approaches of Krahmer et al. (2003) and Golland et al. (2010), and how to address a particular challenge involving cycles that arises when dealing 6 Proceedings of the INLG and SIGDIAL 2014 Joint Session, pages 6–15, Philadelphia, Pennsylvania, 19 June 2014. c�2014 Association for Computational Linguistics with probabilistic listener models. 2 Related Work RE generation is the task of generating a naturallanguage expression that identifies an object to the listener. Since the beginnings of modern REG (Appelt, 1985; Dale and Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object in the domain and not any of the others. This perspective has made it possible to apply search-based (Kelleher and Kruijff, 2006), logicbased (Areces et al., 2008) and graph-based (Krahmer et al., 2003) methods to the problem, and overall has been one of the success stories of NLG. However, in practice, human speakers frequently overspecify, i.e. they include information in an RE beyond what is necessary to make it distinguishing (Wardlow Lane and Ferreira, 2008; Koolen et al., </context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the Gricean Maxims in the generation of referring expressions. Cognitive Science, 19(2):233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Engonopoulos</author>
<author>Martin Villalba</author>
<author>Ivan Titov</author>
<author>Alexander Koller</author>
</authors>
<title>Predicting the resolution of referring expressions from user behavior.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Seattle.</location>
<contexts>
<context position="2434" citStr="Engonopoulos et al. (2013)" startWordPosition="394" endWordPosition="398">guous REs. Conversely, listeners can misunderstand even a distinguishing RE, so uniqueness is no guarantee for success. We propose instead to define and train a probabilistic RE resolution model P(a|t), which directly captures the probability that the listener will resolve a given RE t to some object a in the domain. An RE t will then be “good enough” if P(a*|t) is very high for the intended target referent a*. Second, in an interactive setting like the GIVE Challenge (Koller et al., 2010), the listener may behave in a way that offers further information on how they resolved the generated RE. Engonopoulos et al. (2013) showed how an initial estimate of the distribution P(a|t) can be continuously updated based on the listener’s behavior, and that this can improve a system’s ability to detect misunderstandings. It seems hard to achieve this in a principled way without an explicit model of P(a|t). In this paper, we present an algorithm that generates the RE t that maximizes P(a*|t), i.e. the RE that has the highest chance to be understood correctly by the listener according to the probabilistic RE resolution model. This is a challenging problem, since the algorithm must identify that RE from a potentially infi</context>
<context position="6321" citStr="Engonopoulos et al. (2013)" startWordPosition="1052" endWordPosition="1055">al. (2013) describe a stochastic algorithm that computes human-like, non-relational REs that may not be distinguishing. Golland et al. (2010) are close to our proposal in spirit, in that they use a loglinear probability model of RE resolution to compute a possibly non-distinguishing RE. However, they use a trivial REG algorithm which is limited to grammars that only permit a (small) finite set of REs for each referent. This is in contrast to general REG, where there is typically an infinite set of valid REs, especially when relational REs (“the button to the left of the plant”) are permitted. Engonopoulos et al. (2013) describe how to update an estimate for P(a|t) based on a log-linear model based on observations of the listener’s behavior. They use a shallow model based on a string t and not an RE derived from a grammar, and they do not discuss how to generate the best t. The algorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (St</context>
</contexts>
<marker>Engonopoulos, Villalba, Titov, Koller, 2013</marker>
<rawString>Nikos Engonopoulos, Martin Villalba, Ivan Titov, and Alexander Koller. 2013. Predicting the resolution of referring expressions from user behavior. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas FitzGerald</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Learning distributions over logical forms for referring expression generation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5609" citStr="FitzGerald et al., 2013" startWordPosition="931" endWordPosition="934">2011). An NLG system, too, might include redundant information in an RE to make it easier to understand for the user. Conversely, an RE that is produced by a human can often be easily resolved by the listener even if it is ambiguous. Here we present an NLG system that directly uses a probabilistic model of RE resolution, and is capable of generating ambiguous REs if it predicts that the listener will understand them. Most existing REG algorithms focus on generating distinguishing REs, and then select the one that is best according to some criterion, e.g. most human-like (Krahmer et al., 2003; FitzGerald et al., 2013) or most likely to be understood (Garoufi and Koller, 2013). By contrast, Mitchell et al. (2013) describe a stochastic algorithm that computes human-like, non-relational REs that may not be distinguishing. Golland et al. (2010) are close to our proposal in spirit, in that they use a loglinear probability model of RE resolution to compute a possibly non-distinguishing RE. However, they use a trivial REG algorithm which is limited to grammars that only permit a (small) finite set of REs for each referent. This is in contrast to general REG, where there is typically an infinite set of valid REs, </context>
</contexts>
<marker>FitzGerald, Artzi, Zettlemoyer, 2013</marker>
<rawString>Nicholas FitzGerald, Yoav Artzi, and Luke Zettlemoyer. 2013. Learning distributions over logical forms for referring expression generation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantina Garoufi</author>
<author>Alexander Koller</author>
</authors>
<title>Generation of effective referring expressions in situated context. Language and Cognitive Processes.</title>
<date>2013</date>
<contexts>
<context position="5668" citStr="Garoufi and Koller, 2013" startWordPosition="941" endWordPosition="944">tion in an RE to make it easier to understand for the user. Conversely, an RE that is produced by a human can often be easily resolved by the listener even if it is ambiguous. Here we present an NLG system that directly uses a probabilistic model of RE resolution, and is capable of generating ambiguous REs if it predicts that the listener will understand them. Most existing REG algorithms focus on generating distinguishing REs, and then select the one that is best according to some criterion, e.g. most human-like (Krahmer et al., 2003; FitzGerald et al., 2013) or most likely to be understood (Garoufi and Koller, 2013). By contrast, Mitchell et al. (2013) describe a stochastic algorithm that computes human-like, non-relational REs that may not be distinguishing. Golland et al. (2010) are close to our proposal in spirit, in that they use a loglinear probability model of RE resolution to compute a possibly non-distinguishing RE. However, they use a trivial REG algorithm which is limited to grammars that only permit a (small) finite set of REs for each referent. This is in contrast to general REG, where there is typically an infinite set of valid REs, especially when relational REs (“the button to the left of </context>
</contexts>
<marker>Garoufi, Koller, 2013</marker>
<rawString>Konstantina Garoufi and Alexander Koller. 2013. Generation of effective referring expressions in situated context. Language and Cognitive Processes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Anja Belz</author>
</authors>
<title>Introducing shared task evaluation to NLG: The TUNA shared task evaluation challenges.</title>
<date>2010</date>
<booktitle>Empirical Methods in Natural Language Generation, number 5790 in LNCS,</booktitle>
<pages>264--293</pages>
<editor>In E. Krahmer and M. Theune, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1354" citStr="Gatt and Belz, 2010" startWordPosition="213" endWordPosition="216">he most effective RE according to this model from the chart. 1 Introduction The fundamental challenge in the generation of referring expressions (REG) is to compute an RE which is effective, i.e. understood as intended by the listener. Throughout the history of REG, we have approximated this as the problem of generating distinguishing REs, i.e. REs that are only satisfied by a unique individual in the domain. This has been an eminently successful approach, as documented e.g. in the overview article of Krahmer and van Deemter (2012) and a variety of recent shared tasks involving RE generation (Gatt and Belz, 2010; Belz et al., 2008; Koller et al., 2010). Nonetheless, reducing effectiveness to uniqueness is limiting in several ways. First, in complex, real-world scenes it may not be feasible to generate fully distinguishing REs, or these may have to be exceedingly complicated. It is also not necessary to generate distinguishing REs in such situations, because listeners are very capable of taking the discourse and task context into account to resolve even ambiguous REs. Conversely, listeners can misunderstand even a distinguishing RE, so uniqueness is no guarantee for success. We propose instead to defi</context>
</contexts>
<marker>Gatt, Belz, 2010</marker>
<rawString>Albert Gatt and Anja Belz. 2010. Introducing shared task evaluation to NLG: The TUNA shared task evaluation challenges. In E. Krahmer and M. Theune, editors, Empirical Methods in Natural Language Generation, number 5790 in LNCS, pages 264–293. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferenc G´ecseg</author>
<author>Magnus Steinby</author>
</authors>
<title>Tree languages.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>1--68</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<marker>G´ecseg, Steinby, 1997</marker>
<rawString>Ferenc G´ecseg and Magnus Steinby. 1997. Tree languages. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, volume 3, chapter 1, pages 1–68. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Golland</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A game-theoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="3903" citStr="Golland et al. (2010)" startWordPosition="648" endWordPosition="651"> surface strings to their interpretations as sets of objects in a given domain (Section 3). This formalism integrates REG with surface realization, and allows us to specify in the grammar whether REs are required to be distinguishing. We then show how to compute a chart for a given grammar and target referent in Section 4. Section 5 defines a log-linear model for P(a|t), and presents a Viterbi-style algorithm for computing the RE t from the chart that maximizes P(a*|t). Section 6 concludes by discussing how to apply our algorithm to the state-of-the-art approaches of Krahmer et al. (2003) and Golland et al. (2010), and how to address a particular challenge involving cycles that arises when dealing 6 Proceedings of the INLG and SIGDIAL 2014 Joint Session, pages 6–15, Philadelphia, Pennsylvania, 19 June 2014. c�2014 Association for Computational Linguistics with probabilistic listener models. 2 Related Work RE generation is the task of generating a naturallanguage expression that identifies an object to the listener. Since the beginnings of modern REG (Appelt, 1985; Dale and Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object i</context>
<context position="5836" citStr="Golland et al. (2010)" startWordPosition="966" endWordPosition="969">us. Here we present an NLG system that directly uses a probabilistic model of RE resolution, and is capable of generating ambiguous REs if it predicts that the listener will understand them. Most existing REG algorithms focus on generating distinguishing REs, and then select the one that is best according to some criterion, e.g. most human-like (Krahmer et al., 2003; FitzGerald et al., 2013) or most likely to be understood (Garoufi and Koller, 2013). By contrast, Mitchell et al. (2013) describe a stochastic algorithm that computes human-like, non-relational REs that may not be distinguishing. Golland et al. (2010) are close to our proposal in spirit, in that they use a loglinear probability model of RE resolution to compute a possibly non-distinguishing RE. However, they use a trivial REG algorithm which is limited to grammars that only permit a (small) finite set of REs for each referent. This is in contrast to general REG, where there is typically an infinite set of valid REs, especially when relational REs (“the button to the left of the plant”) are permitted. Engonopoulos et al. (2013) describe how to update an estimate for P(a|t) based on a log-linear model based on observations of the listener’s </context>
<context position="29492" citStr="Golland et al. (2010)" startWordPosition="5413" endWordPosition="5416">B0n) do 3: a = ix(A0) 4: t0 = r(bt(B01), ... , bt(B0n)) n 5: s = sc(t0) + is(B0i) i=1 ix(B0i)=a 6: if prob(a, s) &gt; prob(a, is(A0)) then 7: is(A0) = s 8: bt(A0) = t0 Figure 8: Computing the best RE. are the nonterminals of the chart; for each rule A0 -+ r(B01,..., B0n) in C, it has an edge from B0 z to A0 for each i. If this graph is acyclic, we can simply compute a topological sort of O(C) to bring the nodes into a linear order in which each B0 z precedes A0. This is enough to evaluate charts using certain simpler models. For instance, we can apply our REG algorithm to the log-linear model of Golland et al. (2010). Because they only generate REs with a bounded number of relations, their grammars effectively only describe finite languages. In such a case, our charts are always acyclic, and therefore a topological sort of O(C) yields an exact evaluation order. This simple approach will not work with grammars that allow arbitrary recursion, as they can lead to charts with cycles (indicating an infinite set of valid REs). E.g. the chart in Fig. 4 contains a rule Nb2/{b2} -+ squareb2(Nb2/{b2}) (shown in Fig. 9), which can be used to construct the RE t0 = “the square square button” in addition to the RE t = </context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic approach to generating spatial descriptions. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Jonathan May</author>
</authors>
<title>Training tree transducers.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="7129" citStr="Graehl et al., 2008" startWordPosition="1190" endWordPosition="1193">ed from a grammar, and they do not discuss how to generate the best t. The algorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts fo</context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>Jonathan Graehl, Kevin Knight, and Jonathan May. 2008. Training tree transducers. Computational Linguistics, 34(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Jones</author>
<author>J Andreas</author>
<author>D Bauer</author>
<author>K-M Hermann</author>
<author>K Knight</author>
</authors>
<title>Semantics-based machine translation with hyperedge replacement grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING.</booktitle>
<contexts>
<context position="7150" citStr="Jones et al., 2012" startWordPosition="1194" endWordPosition="1197">d they do not discuss how to generate the best t. The algorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challe</context>
</contexts>
<marker>Jones, Andreas, Bauer, Hermann, Knight, 2012</marker>
<rawString>B. Jones, J. Andreas, D. Bauer, K.-M. Hermann, and K. Knight. 2012. Semantics-based machine translation with hyperedge replacement grammars. In Proceedings of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kaplan</author>
<author>J¨urgen Wedekind</author>
</authors>
<title>LFG generation produces context-free languages.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th COLING.</booktitle>
<contexts>
<context position="7671" citStr="Kaplan and Wedekind, 2000" startWordPosition="1274" endWordPosition="1277">re widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. 3 Grammars for RE generation We define a new grammar formalism that we use for REG, which we call semantically intepreted grammar (SIG). SIG is a synchronous grammar formalism that relates natural language strings with the sets of objects in a given domain which they describe. It uses regular tree grammars (RTGs) to describe languages of derivation trees, which then project to strings and sets. 3.1 Derivation trees We describe the</context>
</contexts>
<marker>Kaplan, Wedekind, 2000</marker>
<rawString>Ron Kaplan and J¨urgen Wedekind. 2000. LFG generation produces context-free languages. In Proceedings of the 18th COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Chart generation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th ACL.</booktitle>
<contexts>
<context position="7621" citStr="Kay, 1996" startWordPosition="1268" endWordPosition="1269">babilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. 3 Grammars for RE generation We define a new grammar formalism that we use for REG, which we call semantically intepreted grammar (SIG). SIG is a synchronous grammar formalism that relates natural language strings with the sets of objects in a given domain which they describe. It uses regular tree grammars (RTGs) to describe languages of derivation trees, which then project to stri</context>
<context position="13871" citStr="Kay, 1996" startWordPosition="2492" endWordPosition="2493">L(G) for LS(G) and call it the language of G. We define a semantically interpreted grammar (SIG) as a triple G = (G, IS, IR) of an RTG G over some signature E, together with a string interpretation IS over some alphabet A and a relational interpretation IR over some universe U, both of which interpret the symbols in E. We assume that every terminal symbol r E E occurs in at most one rule, and that the nonterminals of G are pairs Ab of a syntactic category A and a semantic index b = ix(Ab). A semantic index indicates the individual in U to which a given constituent is meant to refer, see e.g. (Kay, 1996; Stone et al., 2003). Note that SIGs can be seen as specific Interpreted Regular Tree Grammars (Koller and Kuhlmann, 2011) with a set and a string interpretation. We ignore the start symbol of G. Instead, we say that given some individual b E U and syntactic category A, the set of referring expressions for b is REg(A,b) = {t E LAb(G) |IR(t) = {b}}, i.e. we define an RE as a derivation tree that G can derive from Ab and whose relational interpretation is {b}. From t, we can read off the string IS(t).1 3.3 An example grammar Consider the SIG G in Fig. 3 for example. The grammar is written in te</context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>Martin Kay. 1996. Chart generation. In Proceedings of the 34th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Kelleher</author>
<author>Geert-Jan Kruijff</author>
</authors>
<title>Incremental generation of spatial referring expressions in situated dialogue. In</title>
<date>2006</date>
<booktitle>In Proceedings of Coling-ACL ’06,</booktitle>
<location>Sydney</location>
<contexts>
<context position="4631" citStr="Kelleher and Kruijff, 2006" startWordPosition="763" endWordPosition="766">f the INLG and SIGDIAL 2014 Joint Session, pages 6–15, Philadelphia, Pennsylvania, 19 June 2014. c�2014 Association for Computational Linguistics with probabilistic listener models. 2 Related Work RE generation is the task of generating a naturallanguage expression that identifies an object to the listener. Since the beginnings of modern REG (Appelt, 1985; Dale and Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object in the domain and not any of the others. This perspective has made it possible to apply search-based (Kelleher and Kruijff, 2006), logicbased (Areces et al., 2008) and graph-based (Krahmer et al., 2003) methods to the problem, and overall has been one of the success stories of NLG. However, in practice, human speakers frequently overspecify, i.e. they include information in an RE beyond what is necessary to make it distinguishing (Wardlow Lane and Ferreira, 2008; Koolen et al., 2011). An NLG system, too, might include redundant information in an RE to make it easier to understand for the user. Conversely, an RE that is produced by a human can often be easily resolved by the listener even if it is ambiguous. Here we pres</context>
</contexts>
<marker>Kelleher, Kruijff, 2006</marker>
<rawString>John Kelleher and Geert-Jan Kruijff. 2006. Incremental generation of spatial referring expressions in situated dialogue. In In Proceedings of Coling-ACL ’06, Sydney Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Marco Kuhlmann</author>
</authors>
<title>A generalized view on parsing and translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies,</booktitle>
<pages>2--13</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="13994" citStr="Koller and Kuhlmann, 2011" startWordPosition="2510" endWordPosition="2513">le G = (G, IS, IR) of an RTG G over some signature E, together with a string interpretation IS over some alphabet A and a relational interpretation IR over some universe U, both of which interpret the symbols in E. We assume that every terminal symbol r E E occurs in at most one rule, and that the nonterminals of G are pairs Ab of a syntactic category A and a semantic index b = ix(Ab). A semantic index indicates the individual in U to which a given constituent is meant to refer, see e.g. (Kay, 1996; Stone et al., 2003). Note that SIGs can be seen as specific Interpreted Regular Tree Grammars (Koller and Kuhlmann, 2011) with a set and a string interpretation. We ignore the start symbol of G. Instead, we say that given some individual b E U and syntactic category A, the set of referring expressions for b is REg(A,b) = {t E LAb(G) |IR(t) = {b}}, i.e. we define an RE as a derivation tree that G can derive from Ab and whose relational interpretation is {b}. From t, we can read off the string IS(t).1 3.3 An example grammar Consider the SIG G in Fig. 3 for example. The grammar is written in template form. Each rule is instantiated for all semantic indices specified in the line above; e.g. the symbol round denotes </context>
</contexts>
<marker>Koller, Kuhlmann, 2011</marker>
<rawString>Alexander Koller and Marco Kuhlmann. 2011. A generalized view on parsing and translation. In Proceedings of the 12th International Conference on Parsing Technologies, pages 2–13. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Matthew Stone</author>
</authors>
<title>Sentence generation as a planning problem.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="7007" citStr="Koller and Stone, 2007" startWordPosition="1173" endWordPosition="1176">inear model based on observations of the listener’s behavior. They use a shallow model based on a string t and not an RE derived from a grammar, and they do not discuss how to generate the best t. The algorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realizat</context>
</contexts>
<marker>Koller, Stone, 2007</marker>
<rawString>Alexander Koller and Matthew Stone. 2007. Sentence generation as a planning problem. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
<author>Donna Byron</author>
<author>Justine Cassell</author>
<author>Robert Dale</author>
<author>Johanna Moore</author>
<author>Jon Oberlander</author>
</authors>
<title>The First Challenge on Generating Instructions in Virtual Environments.</title>
<date>2010</date>
<booktitle>Empirical Methods in Natural Language Generation, number 5790 in LNAI,</booktitle>
<pages>337--361</pages>
<editor>In E. Krahmer and M. Theune, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1395" citStr="Koller et al., 2010" startWordPosition="221" endWordPosition="224">odel from the chart. 1 Introduction The fundamental challenge in the generation of referring expressions (REG) is to compute an RE which is effective, i.e. understood as intended by the listener. Throughout the history of REG, we have approximated this as the problem of generating distinguishing REs, i.e. REs that are only satisfied by a unique individual in the domain. This has been an eminently successful approach, as documented e.g. in the overview article of Krahmer and van Deemter (2012) and a variety of recent shared tasks involving RE generation (Gatt and Belz, 2010; Belz et al., 2008; Koller et al., 2010). Nonetheless, reducing effectiveness to uniqueness is limiting in several ways. First, in complex, real-world scenes it may not be feasible to generate fully distinguishing REs, or these may have to be exceedingly complicated. It is also not necessary to generate distinguishing REs in such situations, because listeners are very capable of taking the discourse and task context into account to resolve even ambiguous REs. Conversely, listeners can misunderstand even a distinguishing RE, so uniqueness is no guarantee for success. We propose instead to define and train a probabilistic RE resolutio</context>
</contexts>
<marker>Koller, Striegnitz, Byron, Cassell, Dale, Moore, Oberlander, 2010</marker>
<rawString>Alexander Koller, Kristina Striegnitz, Donna Byron, Justine Cassell, Robert Dale, Johanna Moore, and Jon Oberlander. 2010. The First Challenge on Generating Instructions in Virtual Environments. In E. Krahmer and M. Theune, editors, Empirical Methods in Natural Language Generation, number 5790 in LNAI, pages 337–361. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannis Konstas</author>
<author>Mirella Lapata</author>
</authors>
<title>Conceptto-text generation via discriminative reranking.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th ACL.</booktitle>
<contexts>
<context position="7321" citStr="Konstas and Lapata (2012)" startWordPosition="1221" endWordPosition="1224"> it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. 3 Grammars for RE generation We define a new grammar formalism that we use for REG, </context>
</contexts>
<marker>Konstas, Lapata, 2012</marker>
<rawString>Yannis Konstas and Mirella Lapata. 2012. Conceptto-text generation via discriminative reranking. In Proceedings of the 50th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruud Koolen</author>
<author>Albert Gatt</author>
<author>Martijn Goudbeek</author>
<author>Emiel Krahmer</author>
</authors>
<title>Factors causing overspecification in definite descriptions.</title>
<date>2011</date>
<journal>Journal of Pragmatics,</journal>
<pages>43--3231</pages>
<contexts>
<context position="4990" citStr="Koolen et al., 2011" startWordPosition="823" endWordPosition="826">d Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object in the domain and not any of the others. This perspective has made it possible to apply search-based (Kelleher and Kruijff, 2006), logicbased (Areces et al., 2008) and graph-based (Krahmer et al., 2003) methods to the problem, and overall has been one of the success stories of NLG. However, in practice, human speakers frequently overspecify, i.e. they include information in an RE beyond what is necessary to make it distinguishing (Wardlow Lane and Ferreira, 2008; Koolen et al., 2011). An NLG system, too, might include redundant information in an RE to make it easier to understand for the user. Conversely, an RE that is produced by a human can often be easily resolved by the listener even if it is ambiguous. Here we present an NLG system that directly uses a probabilistic model of RE resolution, and is capable of generating ambiguous REs if it predicts that the listener will understand them. Most existing REG algorithms focus on generating distinguishing REs, and then select the one that is best according to some criterion, e.g. most human-like (Krahmer et al., 2003; FitzG</context>
</contexts>
<marker>Koolen, Gatt, Goudbeek, Krahmer, 2011</marker>
<rawString>Ruud Koolen, Albert Gatt, Martijn Goudbeek, and Emiel Krahmer. 2011. Factors causing overspecification in definite descriptions. Journal of Pragmatics, 43:3231–3250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>Computational generation of referring expressions: A survey.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>Krahmer, van Deemter, 2012</marker>
<rawString>Emiel Krahmer and Kees van Deemter. 2012. Computational generation of referring expressions: A survey. Computational Linguistics, 38(1):173–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Sebastiaan van Erk</author>
<author>Andr´e Verleg</author>
</authors>
<title>Graph-based generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg. 2003. Graph-based generation of referring expressions. Computational Linguistics, 29(1):53–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A probabilistic forest-to-string model for language generation from typed lambda calculus expressions.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7244" citStr="Lu and Ng (2011)" startWordPosition="1209" endWordPosition="1212">ur formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or realization. 3 Gramm</context>
</contexts>
<marker>Lu, Ng, 2011</marker>
<rawString>Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-to-string model for language generation from typed lambda calculus expressions. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Kees van Deemter</author>
<author>Ehud Reiter</author>
</authors>
<title>Generating expressions that refer to visible objects.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>1174--1184</pages>
<marker>Mitchell, van Deemter, Reiter, 2013</marker>
<rawString>Margaret Mitchell, Kees van Deemter, and Ehud Reiter. 2013. Generating expressions that refer to visible objects. In Proceedings of NAACL-HLT, pages 1174–1184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
<author>Christine Doran</author>
<author>Bonnie Webber</author>
<author>Tonia Bleam</author>
<author>Martha Palmer</author>
</authors>
<title>Microplanning with communicative intentions: The SPUD system.</title>
<date>2003</date>
<journal>Computational Intelligence,</journal>
<volume>19</volume>
<issue>4</issue>
<pages>381</pages>
<contexts>
<context position="6938" citStr="Stone et al., 2003" startWordPosition="1163" endWordPosition="1166">3) describe how to update an estimate for P(a|t) based on a log-linear model based on observations of the listener’s behavior. They use a shallow model based on a string t and not an RE derived from a grammar, and they do not discuss how to generate the best t. The algorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to </context>
<context position="13892" citStr="Stone et al., 2003" startWordPosition="2494" endWordPosition="2497">(G) and call it the language of G. We define a semantically interpreted grammar (SIG) as a triple G = (G, IS, IR) of an RTG G over some signature E, together with a string interpretation IS over some alphabet A and a relational interpretation IR over some universe U, both of which interpret the symbols in E. We assume that every terminal symbol r E E occurs in at most one rule, and that the nonterminals of G are pairs Ab of a syntactic category A and a semantic index b = ix(Ab). A semantic index indicates the individual in U to which a given constituent is meant to refer, see e.g. (Kay, 1996; Stone et al., 2003). Note that SIGs can be seen as specific Interpreted Regular Tree Grammars (Koller and Kuhlmann, 2011) with a set and a string interpretation. We ignore the start symbol of G. Instead, we say that given some individual b E U and syntactic category A, the set of referring expressions for b is REg(A,b) = {t E LAb(G) |IR(t) = {b}}, i.e. we define an RE as a derivation tree that G can derive from Ab and whose relational interpretation is {b}. From t, we can read off the string IS(t).1 3.3 An example grammar Consider the SIG G in Fig. 3 for example. The grammar is written in template form. Each rul</context>
</contexts>
<marker>Stone, Doran, Webber, Bleam, Palmer, 2003</marker>
<rawString>Matthew Stone, Christine Doran, Bonnie Webber, Tonia Bleam, and Martha Palmer. 2003. Microplanning with communicative intentions: The SPUD system. Computational Intelligence, 19(4):311– 381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liane Wardlow Lane</author>
<author>Victor Ferreira</author>
</authors>
<title>Speaker-external versus speaker-internal forces on utterance form: Do cognitive demands override threats to referential success?</title>
<date>2008</date>
<journal>Journal of Experimental Psychology: Learning, Memory, and Cognition,</journal>
<pages>34--1466</pages>
<contexts>
<context position="4968" citStr="Lane and Ferreira, 2008" startWordPosition="819" endWordPosition="822">EG (Appelt, 1985; Dale and Reiter, 1995), this problem has been approximated as generating a distinguishing description, i.e. one which fits only one object in the domain and not any of the others. This perspective has made it possible to apply search-based (Kelleher and Kruijff, 2006), logicbased (Areces et al., 2008) and graph-based (Krahmer et al., 2003) methods to the problem, and overall has been one of the success stories of NLG. However, in practice, human speakers frequently overspecify, i.e. they include information in an RE beyond what is necessary to make it distinguishing (Wardlow Lane and Ferreira, 2008; Koolen et al., 2011). An NLG system, too, might include redundant information in an RE to make it easier to understand for the user. Conversely, an RE that is produced by a human can often be easily resolved by the listener even if it is ambiguous. Here we present an NLG system that directly uses a probabilistic model of RE resolution, and is capable of generating ambiguous REs if it predicts that the listener will understand them. Most existing REG algorithms focus on generating distinguishing REs, and then select the one that is best according to some criterion, e.g. most human-like (Krahm</context>
</contexts>
<marker>Lane, Ferreira, 2008</marker>
<rawString>Liane Wardlow Lane and Victor Ferreira. 2008. Speaker-external versus speaker-internal forces on utterance form: Do cognitive demands override threats to referential success? Journal of Experimental Psychology: Learning, Memory, and Cognition, 34:1466–1481.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th ACL.</booktitle>
<contexts>
<context position="7226" citStr="Wong and Mooney, 2007" startWordPosition="1205" endWordPosition="1208">p here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structured than in parsing or re</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J. Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of the 45th ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="7202" citStr="Zettlemoyer and Collins, 2005" startWordPosition="1201" endWordPosition="1204">best t. The algorithm we develop here fills this gap. Our formalism for REG can be seen as a synchronous grammar formalism; it simultaneously derives strings and their interpretations, connecting the two by an abstract syntactic representation. This allows performing REG and surface realization with a single algorithm, along the lines of SPUD (Stone et al., 2003) and its planningbased implementation, CRISP (Koller and Stone, 2007). Probabilistic synchronous grammars are widely used in statistical machine translation (Chiang, 2007; Graehl et al., 2008; Jones et al., 2012) and semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Lu and Ng (2011) have applied such grammars to surface realization. Konstas and Lapata (2012) use related techniques for content selection and surface realization (with simple, non-recursive grammars). Charts are standard tools for representing a large space of possible linguistic analyses compactly. Next to their use in parsing, they have also been applied to surface realization (Kay, 1996; Carroll et al., 1999; Kaplan and Wedekind, 2000). To our knowledge, ours is the first work using charts for REG. This is challenging because the input to REG is much less structur</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S. Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
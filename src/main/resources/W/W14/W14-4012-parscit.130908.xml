<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000385">
<title confidence="0.978556">
On the Properties of Neural Machine Translation: Encoder–Decoder
Approaches
</title>
<author confidence="0.657023">
Kyunghyun Cho Bart van Merri¨enboer Dzmitry Bahdanau∗
</author>
<affiliation confidence="0.542532">
Universit´e de Montr´eal Jacobs University Bremen, Germany
</affiliation>
<author confidence="0.939548">
Yoshua Bengio
</author>
<affiliation confidence="0.937433">
Universit´e de Montr´eal, CIFAR Senior Fellow
</affiliation>
<sectionHeader confidence="0.978522" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892826086956">
Neural machine translation is a relatively
new approach to statistical machine trans-
lation based purely on neural networks.
The neural machine translation models of-
ten consist of an encoder and a decoder.
The encoder extracts a fixed-length repre-
sentation from a variable-length input sen-
tence, and the decoder generates a correct
translation from this representation. In this
paper, we focus on analyzing the proper-
ties of the neural machine translation us-
ing two models; RNN Encoder–Decoder
and a newly proposed gated recursive con-
volutional neural network. We show that
the neural machine translation performs
relatively well on short sentences without
unknown words, but its performance de-
grades rapidly as the length of the sentence
and the number of unknown words in-
crease. Furthermore, we find that the pro-
posed gated recursive convolutional net-
work learns a grammatical structure of a
sentence automatically.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996988963636364">
A new approach for statistical machine transla-
tion based purely on neural networks has recently
been proposed (Kalchbrenner and Blunsom, 2013;
Sutskever et al., 2014). This new approach, which
we refer to as neural machine translation, is in-
spired by the recent trend of deep representational
learning. All the neural network models used in
(Sutskever et al., 2014; Cho et al., 2014) consist of
an encoder and a decoder. The encoder extracts a
fixed-length vector representation from a variable-
length input sentence, and from this representation
the decoder generates a correct, variable-length
target translation.
∗ Research done while visiting Universit´e de Montr´eal
The emergence of the neural machine transla-
tion is highly significant, both practically and the-
oretically. Neural machine translation models re-
quire only a fraction of the memory needed by
traditional statistical machine translation (SMT)
models. The models we trained for this paper
require only 500MB of memory in total. This
stands in stark contrast with existing SMT sys-
tems, which often require tens of gigabytes of
memory. This makes the neural machine trans-
lation appealing in practice. Furthermore, un-
like conventional translation systems, each and ev-
ery component of the neural translation model is
trained jointly to maximize the translation perfor-
mance.
As this approach is relatively new, there has not
been much work on analyzing the properties and
behavior of these models. For instance: What
are the properties of sentences on which this ap-
proach performs better? How does the choice of
source/target vocabulary affect the performance?
In which cases does the neural machine translation
fail?
It is crucial to understand the properties and be-
havior of this new neural machine translation ap-
proach in order to determine future research di-
rections. Also, understanding the weaknesses and
strengths of neural machine translation might lead
to better ways of integrating SMT and neural ma-
chine translation systems.
In this paper, we analyze two neural machine
translation models. One of them is the RNN
Encoder–Decoder that was proposed recently in
(Cho et al., 2014). The other model replaces the
encoder in the RNN Encoder–Decoder model with
a novel neural network, which we call a gated
recursive convolutional neural network (grConv).
We evaluate these two models on the task of trans-
lation from French to English.
Our analysis shows that the performance of
the neural machine translation model degrades
</bodyText>
<page confidence="0.991871">
103
</page>
<note confidence="0.7814585">
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103–111,
October 25, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999893222222222">
quickly as the length of a source sentence in-
creases. Furthermore, we find that the vocabulary
size has a high impact on the translation perfor-
mance. Nonetheless, qualitatively we find that the
both models are able to generate correct transla-
tions most of the time. Furthermore, the newly
proposed grConv model is able to learn, without
supervision, a kind of syntactic structure over the
source language.
</bodyText>
<sectionHeader confidence="0.9589315" genericHeader="method">
2 Neural Networks for Variable-Length
Sequences
</sectionHeader>
<bodyText confidence="0.9998956">
In this section, we describe two types of neural
networks that are able to process variable-length
sequences. These are the recurrent neural net-
work and the proposed gated recursive convolu-
tional neural network.
</bodyText>
<subsectionHeader confidence="0.88639">
2.1 Recurrent Neural Network with Gated
Hidden Neurons
</subsectionHeader>
<figureCaption confidence="0.975833666666667">
Figure 1: The graphical illustration of (a) the re-
current neural network and (b) the hidden unit that
adaptively forgets and remembers.
</figureCaption>
<bodyText confidence="0.9943342">
A recurrent neural network (RNN, Fig. 1 (a))
works on a variable-length sequence x =
(x1, x2, · · · , xT) by maintaining a hidden state h
over time. At each timestep t, the hidden state h(t)
is updated by
</bodyText>
<equation confidence="0.998435">
� �
h(t) = f h(t−1), xt ,
</equation>
<bodyText confidence="0.999660230769231">
where f is an activation function. Often f is as
simple as performing a linear transformation on
the input vectors, summing them, and applying an
element-wise logistic sigmoid function.
An RNN can be used effectively to learn a dis-
tribution over a variable-length sequence by learn-
ing the distribution over the next input p(xt+1 |
xt, · · · , x1). For instance, in the case of a se-
quence of 1-of-K vectors, the distribution can be
learned by an RNN which has as an output
for all possible symbols j = 1, ... , K, where wj
are the rows of a weight matrix W. This results in
the joint distribution
</bodyText>
<equation confidence="0.997795">
T
p(x) = H p(xt  |xt−1, ... , x1).
t=1
</equation>
<bodyText confidence="0.999685846153846">
Recently, in (Cho et al., 2014) a new activation
function for RNNs was proposed. The new activa-
tion function augments the usual logistic sigmoid
activation function with two gating units called re-
set, r, and update, z, gates. Each gate depends on
the previous hidden state h(t−1), and the current
input xt controls the flow of information. This is
reminiscent of long short-term memory (LSTM)
units (Hochreiter and Schmidhuber, 1997). For
details about this unit, we refer the reader to (Cho
et al., 2014) and Fig. 1 (b). For the remainder of
this paper, we always use this new activation func-
tion.
</bodyText>
<subsectionHeader confidence="0.91569">
2.2 Gated Recursive Convolutional Neural
Network
</subsectionHeader>
<bodyText confidence="0.999866473684211">
Besides RNNs, another natural approach to deal-
ing with variable-length sequences is to use a re-
cursive convolutional neural network where the
parameters at each level are shared through the
whole network (see Fig. 2 (a)). In this section, we
introduce a binary convolutional neural network
whose weights are recursively applied to the input
sequence until it outputs a single fixed-length vec-
tor. In addition to a usual convolutional architec-
ture, we propose to use the previously mentioned
gating mechanism, which allows the recursive net-
work to learn the structure of the source sentences
on the fly.
Let x = (x1, x2, · · · , xT) be an input sequence,
where xt E Rd. The proposed gated recursive
convolutional neural network (grConv) consists of
four weight matrices Wl, Wr, Gl and Gr. At
each recursion level t E [1, T − 1], the activation
of the j-th hidden unit h(t)
</bodyText>
<equation confidence="0.949243166666667">
j is computed by
h(t)
j = ωc˜h(t)
j + ωlh(t−1)
j−1 + ωrh(t−1)
j , (1)
</equation>
<bodyText confidence="0.9999395">
where ωc, ωl and ωr are the values of a gater that
sum to 1. The hidden unit is initialized as
</bodyText>
<equation confidence="0.976942333333333">
(a) (b)
x
z
~
h h
r
p(xt,j = 1  |xt−1, ... , x1) = exp (wjh(t) ) h(0)
j = Uxj,
EKj�=1 exp (wj�h(t) �, where U projects the input into a hidden space.
</equation>
<page confidence="0.992338">
104
</page>
<figure confidence="0.993917">
ω
~ h
(a) (b) (c) (d)
</figure>
<figureCaption confidence="0.928770666666667">
Figure 2: The graphical illustration of (a) the recursive convolutional neural network and (b) the proposed
gated unit for the recursive convolutional neural network. (c–d) The example structures that may be
learned with the proposed gated unit.
</figureCaption>
<bodyText confidence="0.637467">
The new activation ˜h(t)
</bodyText>
<equation confidence="0.9779575">
j is computed as usual:
˜h(t) = φ(Wlhit)1 + Wrh(t)I
l ,
j
</equation>
<bodyText confidence="0.907673">
where φ is an element-wise nonlinearity.
The gating coefficients ω’s are computed by
</bodyText>
<equation confidence="0.9840206">
Zexp lGlh�t)1 + Grh�t)) ,
where Gl, Gr ∈ R3xd and
3
Z= X hexp (Glhj(t)1 + Grh�t))ik .
k=1
</equation>
<bodyText confidence="0.999902055555556">
According to this activation, one can think of
the activation of a single node at recursion level t
as a choice between either a new activation com-
puted from both left and right children, the acti-
vation from the left child, or the activation from
the right child. This choice allows the overall
structure of the recursive convolution to change
adaptively with respect to an input sample. See
Fig. 2 (b) for an illustration.
In this respect, we may even consider the pro-
posed grConv as doing a kind of unsupervised
parsing. If we consider the case where the gat-
ing unit makes a hard decision, i.e., ω follows an
1-of-K coding, it is easy to see that the network
adapts to the input and forms a tree-like structure
(See Fig. 2 (c–d)). However, we leave the further
investigation of the structure learned by this model
for future research.
</bodyText>
<sectionHeader confidence="0.925665" genericHeader="method">
3 Purely Neural Machine Translation
</sectionHeader>
<subsectionHeader confidence="0.978215">
3.1 Encoder–Decoder Approach
</subsectionHeader>
<bodyText confidence="0.921981333333333">
The task of translation can be understood from the
perspective of machine learning as learning the
La croissance économique a ralenti ces dernières années .
</bodyText>
<figureCaption confidence="0.999486">
Figure 3: The encoder–decoder architecture
</figureCaption>
<bodyText confidence="0.999967366666667">
conditional distribution p(f  |e) of a target sen-
tence (translation) f given a source sentence e.
Once the conditional distribution is learned by a
model, one can use the model to directly sample
a target sentence given a source sentence, either
by actual sampling or by using a (approximate)
search algorithm to find the maximum of the dis-
tribution.
A number of recent papers have proposed to
use neural networks to directly learn the condi-
tional distribution from a bilingual, parallel cor-
pus (Kalchbrenner and Blunsom, 2013; Cho et al.,
2014; Sutskever et al., 2014). For instance, the au-
thors of (Kalchbrenner and Blunsom, 2013) pro-
posed an approach involving a convolutional n-
gram model to extract a vector of a source sen-
tence which is decoded with an inverse convolu-
tional n-gram model augmented with an RNN. In
(Sutskever et al., 2014), an RNN with LSTM units
was used to encode a source sentence and starting
from the last hidden state, to decode a target sen-
tence. Similarly, the authors of (Cho et al., 2014)
proposed to use an RNN to encode and decode a
pair of source and target phrases.
At the core of all these recent works lies an
encoder–decoder architecture (see Fig. 3). The
encoder processes a variable-length input (source
sentence) and builds a fixed-length vector repre-
sentation (denoted as z in Fig. 3). Conditioned on
the encoded representation, the decoder generates
</bodyText>
<figure confidence="0.997389">
⎡ ωc ⎤
⎣ ωl ⎦ =
ωr
Decode
[z 1 ,z 2 , ... ,z d ]
Encode
Economic growth has slowed down in recent years .
</figure>
<page confidence="0.770219">
105
</page>
<bodyText confidence="0.979462565217391">
a variable-length sequence (target sentence).
Before (Sutskever et al., 2014) this encoder–
decoder approach was used mainly as a part of the
existing statistical machine translation (SMT) sys-
tem. This approach was used to re-rank the n-best
list generated by the SMT system in (Kalchbren-
ner and Blunsom, 2013), and the authors of (Cho
et al., 2014) used this approach to provide an ad-
ditional score for the existing phrase table.
In this paper, we concentrate on analyzing the
direct translation performance, as in (Sutskever et
al., 2014), with two model configurations. In both
models, we use an RNN with the gated hidden
unit (Cho et al., 2014), as this is one of the only
options that does not require a non-trivial way to
determine the target length. The first model will
use the same RNN with the gated hidden unit as
an encoder, as in (Cho et al., 2014), and the second
one will use the proposed gated recursive convo-
lutional neural network (grConv). We aim to un-
derstand the inductive bias of the encoder–decoder
approach on the translation performance measured
by BLEU.
</bodyText>
<sectionHeader confidence="0.977078" genericHeader="method">
4 Experiment Settings
</sectionHeader>
<subsectionHeader confidence="0.971452">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999950818181818">
We evaluate the encoder–decoder models on the
task of English-to-French translation. We use the
bilingual, parallel corpus which is a set of 348M
selected by the method in (Axelrod et al., 2011)
from a combination of Europarl (61M words),
news commentary (5.5M), UN (421M) and two
crawled corpora of 90M and 780M words respec-
tively.1 We did not use separate monolingual data.
The performance of the neural machien transla-
tion models was measured on the news-test2012,
news-test2013 and news-test2014 sets ( 3000 lines
each). When comparing to the SMT system, we
use news-test2012 and news-test2013 as our de-
velopment set for tuning the SMT system, and
news-test2014 as our test set.
Among all the sentence pairs in the prepared
parallel corpus, for reasons of computational ef-
ficiency we only use the pairs where both English
and French sentences are at most 30 words long to
train neural networks. Furthermore, we use only
the 30,000 most frequent words for both English
and French. All the other rare words are consid-
</bodyText>
<footnote confidence="0.946149">
1All the data can be downloaded from http:
//www-lium.univ-lemans.fr/˜schwenk/cslm_
joint_paper/.
</footnote>
<bodyText confidence="0.9017235">
ered unknown and are mapped to a special token
([UNK]).
</bodyText>
<subsectionHeader confidence="0.988947">
4.2 Models
</subsectionHeader>
<bodyText confidence="0.999980590909091">
We train two models: The RNN Encoder–
Decoder (RNNenc)(Cho et al., 2014) and the
newly proposed gated recursive convolutional
neural network (grConv). Note that both models
use an RNN with gated hidden units as a decoder
(see Sec. 2.1).
We use minibatch stochastic gradient descent
with AdaDelta (Zeiler, 2012) to train our two mod-
els. We initialize the square weight matrix (transi-
tion matrix) as an orthogonal matrix with its spec-
tral radius set to 1 in the case of the RNNenc and
0.4 in the case of the grConv. tanh and a rectifier
(max(0, x)) are used as the element-wise nonlin-
ear functions for the RNNenc and grConv respec-
tively.
The grConv has 2000 hidden neurons, whereas
the RNNenc has 1000 hidden neurons. The word
embeddings are 620-dimensional in both cases.2
Both models were trained for approximately 110
hours, which is equivalent to 296,144 updates and
846,322 updates for the grConv and RNNenc, re-
spectively.
</bodyText>
<subsubsectionHeader confidence="0.645745">
4.2.1 Translation using Beam-Search
</subsubsectionHeader>
<bodyText confidence="0.999794473684211">
We use a basic form of beam-search to find a trans-
lation that maximizes the conditional probability
given by a specific model (in this case, either the
RNNenc or the grConv). At each time step of
the decoder, we keep the s translation candidates
with the highest log-probability, where s = 10
is the beam-width. During the beam-search, we
exclude any hypothesis that includes an unknown
word. For each end-of-sequence symbol that is se-
lected among the highest scoring candidates the
beam-width is reduced by one, until the beam-
width reaches zero.
The beam-search to (approximately) find a se-
quence of maximum log-probability under RNN
was proposed and used successfully in (Graves,
2012) and (Boulanger-Lewandowski et al., 2013).
Recently, the authors of (Sutskever et al., 2014)
found this approach to be effective in purely neu-
ral machine translation based on LSTM units.
</bodyText>
<footnote confidence="0.9655484">
2In all cases, we train the whole network including the
word embedding matrix. The embedding dimensionality was
chosen to be quite large, as the preliminary experiments
with 155-dimensional embeddings showed rather poor per-
formance.
</footnote>
<page confidence="0.988952">
106
</page>
<figure confidence="0.966698107142857">
Development Test
Model
All
No UNK
16.60 17.50
19.12 20.99
28.92 32.00
24.73 27.03
21.74 22.94
32.20 35.40
Model Development Test
RNNenc 13.15 13.92
grConv 9.97 9.97
Moses 30.64 33.30
Moses+RNNenc* 31.48 34.64
Moses+LSTM◦ 32 35.65
RNNenc 21.01 23.45
grConv 17.19 18.22
Moses 32.77 35.63
All
No UNK
RNNenc
grConv
Moses
RNNenc
grConv
Moses
(a) All Lengths (b) 10–20 Words
</figure>
<tableCaption confidence="0.984302">
Table 1: BLEU scores computed on the development and test sets. The top three rows show the scores on
</tableCaption>
<bodyText confidence="0.9392335">
all the sentences, and the bottom three rows on the sentences having no unknown words. (*) The result
reported in (Cho et al., 2014) where the RNNenc was used to score phrase pairs in the phrase table. (◦)
The result reported in (Sutskever et al., 2014) where an encoder–decoder with LSTM units was used to
re-rank the n-best list generated by Moses.
When we use the beam-search to find the k best
translations, we do not use a usual log-probability
but one normalized with respect to the length of
the translation. This prevents the RNN decoder
from favoring shorter translations, behavior which
was observed earlier in, e.g., (Graves, 2013).
</bodyText>
<sectionHeader confidence="0.99898" genericHeader="evaluation">
5 Results and Analysis
</sectionHeader>
<subsectionHeader confidence="0.991194">
5.1 Quantitative Analysis
</subsectionHeader>
<bodyText confidence="0.999798036363637">
In this paper, we are interested in the properties
of the neural machine translation models. Specif-
ically, the translation quality with respect to the
length of source and/or target sentences and with
respect to the number of words unknown to the
model in each source/target sentence.
First, we look at how the BLEU score, reflect-
ing the translation performance, changes with re-
spect to the length of the sentences (see Fig. 4 (a)–
(b)). Clearly, both models perform relatively well
on short sentences, but suffer significantly as the
length of the sentences increases.
We observe a similar trend with the number of
unknown words, in Fig. 4 (c). As expected, the
performance degrades rapidly as the number of
unknown words increases. This suggests that it
will be an important challenge to increase the size
of vocabularies used by the neural machine trans-
lation system in the future. Although we only
present the result with the RNNenc, we observed
similar behavior for the grConv as well.
In Table 1 (a), we present the translation perfor-
mances obtained using the two models along with
the baseline phrase-based SMT system.3 Clearly
the phrase-based SMT system still shows the su-
perior performance over the proposed purely neu-
ral machine translation system, but we can see that
under certain conditions (no unknown words in
both source and reference sentences), the differ-
ence diminishes quite significantly. Furthermore,
if we consider only short sentences (10–20 words
per sentence), the difference further decreases (see
Table 1 (b).
Furthermore, it is possible to use the neural ma-
chine translation models together with the existing
phrase-based system, which was found recently in
(Cho et al., 2014; Sutskever et al., 2014) to im-
prove the overall translation performance (see Ta-
ble 1 (a)).
This analysis suggests that that the current neu-
ral translation approach has its weakness in han-
dling long sentences. The most obvious explana-
tory hypothesis is that the fixed-length vector rep-
resentation does not have enough capacity to en-
code a long sentence with complicated structure
and meaning. In order to encode a variable-length
sequence, a neural network may “sacrifice” some
of the important topics in the input sentence in or-
der to remember others.
This is in stark contrast to the conventional
phrase-based machine translation system (Koehn
et al., 2003). As we can see from Fig. 5, the
conventional system trained on the same dataset
(with additional monolingual data for the language
model) tends to get a higher BLEU score on longer
</bodyText>
<footnote confidence="0.989001">
3We used Moses as a baseline, trained with additional
monolingual data for a 4-gram language model.
</footnote>
<page confidence="0.994031">
107
</page>
<table confidence="0.993289457627119">
Source She explained her new position of foreign affairs and security policy representative as a reply to a
question: ”Who is the European Union? Which phone number should I call?”; i.e. as an important step
to unification and better clarity of Union’s policy towards countries such as China or India.
Reference Elle a explique le nouveau poste de la Haute representante pour les affaires etrangeres et la politique de
defense dans le cadre d’une reponse a� la question: ”Qui est qui a� l’Union europeenne?” ”A quel numero
de telephone dois-je appeler?”, donc comme un pas important vers l’unicite et une plus grande lisibilite
de la politique de l’Union face aux etats, comme est la Chine ou bien l’Inde.
RNNEnc Elle a decrit sa position en mati6re de politique etrang6re et de securite ainsi que la politique de l’Union
europeenne en mati�ere de gouvernance et de democratie .
grConv Elle a explique sa nouvelle politique etrang6re et de securite en reponse a� un certain nombre de questions
: ”Qu’est-ce que l’Union europeenne ? ” .
Moses Elle a explique son nouveau poste des affaires etrang6res et la politique de securite representant en
reponse a� une question: ”Qui est l’Union europeenne? Quel numero de telephone dois-je appeler?”;
c’est comme une etape importante de l’unification et une meilleure lisibilite de la politique de l’Union
a� des pays comme la Chine ou l’Inde .
Source The investigation should be complete by the end of the year when the findings will be presented to
Deutsche Bank’s board of managing directors - with recommendations for action.
Reference L’examen doit etre terming d’ici la fin de l’annee, ensuite les resultats du conseil d’administration de la
Deutsche Bank doivent etre presentes - avec recommandation, d’ habitude.
RNNEnc L’etude devrait etre terminee a� la fin de l’ annee, lorsque les conclusions seront presentees au conseil
d’administration de la Deutsche Bank, conseil d’association avec des mesures.
grConv L’enquete devrait etre terminee a� la fin de l’annee ou les conclusions seront presentees par le conseil
d’administration de la BCE a� la direction des recommandations.
Moses L’enquete devrait etre terming d’ici la fin de l’annee lorsque les resultats seront presentes a� la Deutsche
Bank conseil des directeurs generaux - avec des recommandations .
Source And there are thorny mechanical questions that must be resolved during that time, like how to bal-
ance the state’s mandate of ”adequate access” to licensed marijuana with its prohibitions on cannabis
businesses within 1,000 feet of a school, park, playground or child care center.
Reference Pendant ce temps, des questions pratiques restent en suspens: comment equilibrer le mandat de l’etat
qui garantit un acces approprie a� la marijuana agreee et interdit l’installation de commerces de vente de
cannabis dans un rayon de 30 km autour d’une ecole, d’un parc, d’un terrain de jeu ou d’une creche.
RNNEnc Il y a des questions preventives qui se posent quant a� l’equilibre des droits de l’enfant dans les limites
d’une entreprise de collecte de sang.
grConv De fac¸on generale, il y a des raisons de securite pour que les entreprises aient acces a� des milliers de
centres de prche, d’eau ou de recherche.
Moses Et il y a des problemes mecaniques complexes qui doivent etre resolues au cours de cette periode,
comme la maniere d’equilibrer le mandat de ”l’acc6s adequat” permis de marijuana avec l’interdiction
du cannabis aux entreprises de 1000 pieds d’une ecole de jeu ou de parc, le service de garde.
(a) Long Sentences
Source There is still no agreement as to which election rules to follow.
Reference Toutefois il n’existe toujours pas d’accord selon quel reglement de vote il faut proceder.
RNNEnc Il n’y a pas encore d’accord sur les r6gles electorales.
grConv Il n’y a pas encore d’accord sur la question des elections a� suivre.
Moses Il y a toujours pas d’accord sur l’election des r6gles a� suivre.
Source Many of these ideas may have been creative, but they didn’t necessarily work.
Reference Beaucoup de ces idees etaient creatives mais elles n’ont pas forcement fonctionne.
RNNEnc Bon nombre de ces idees ont peut-etre ete creatrices, mais elles ne s’appliquaient pas necessairement.
grConv Beaucoup de ces idees peuvent etre creatives, mais elles n’ont pas fonctionne.
Moses Beaucoup de ces idees ont pu etre creatif, mais ils n’ont pas necessairement.
Source There is a lot of consensus between the Left and the Right on this subject.
Reference C’est qu’il y a sur ce sujet un assez large consensus entre gauche et droite.
RNNEnc Il existe beaucoup de consensus entre la gauche et le droit a� la question.
grConv Il y a un consensus entre la gauche et le droit sur cette question.
Moses Il y a beaucoup de consensus entre la gauche et la droite sur ce sujet.
Source According to them, one can find any weapon at a low price right now.
Reference Selon eux, on peut trouver aujourd’hui a� Moscou n’importe quelle arme pour un prix raisonnable.
RNNEnc Selon eux, on peut se trouver de l’arme a� un prix trop bas.
grConv En tout cas, ils peuvent trouver une arme a� un prix tres bas a� la fois.
Moses Selon eux, on trouve une arme a� bas prix pour l’instant.
</table>
<tableCaption confidence="0.8396755">
(b) Short Sentences
Table 2: The sample translations along with the source sentences and the reference translations.
</tableCaption>
<page confidence="0.984542">
108
</page>
<figure confidence="0.999567972972973">
(a) RNNenc
(b) grConv
(c) RNNenc
0 10 20 30 40 50 60 70 80
Sentence length
BLEU score 20
15
10
5
0
Source text
Reference text
Both
0 10 20 30 40 50 60 70 80
Sentence length
BLEU score 20
15
10
5
0
Source text
Reference text
Both
0 2 4 6 8 10
Max. number of unknown words
Source text
Reference text
Both
24
22
BLEU score
20
18
16
14
12
10
</figure>
<figureCaption confidence="0.999286">
Figure 4: The BLEU scores achieved by (a) the RNNenc and (b) the grConv for sentences of a given
</figureCaption>
<bodyText confidence="0.992650916666667">
length. The plot is smoothed by taking a window of size 10. (c) The BLEU scores achieved by the RNN
model for sentences with less than a given number of unknown words.
sentences.
In fact, if we limit the lengths of both the source
sentence and the reference translation to be be-
tween 10 and 20 words and use only the sentences
with no unknown words, the BLEU scores on the
test set are 27.81 and 33.08 for the RNNenc and
Moses, respectively.
Note that we observed a similar trend even
when we used sentences of up to 50 words to train
these models.
</bodyText>
<subsectionHeader confidence="0.999731">
5.2 Qualitative Analysis
</subsectionHeader>
<bodyText confidence="0.999948615384616">
Although BLEU score is used as a de-facto stan-
dard metric for evaluating the performance of a
machine translation system, it is not the perfect
metric (see, e.g., (Song et al., 2013; Liu et al.,
2011)). Hence, here we present some of the ac-
tual translations generated from the two models,
RNNenc and grConv.
In Table. 2 (a)–(b), we show the translations of
some randomly selected sentences from the de-
velopment and test sets. We chose the ones that
have no unknown words. (a) lists long sentences
(longer than 30 words), and (b) short sentences
(shorter than 10 words). We can see that, despite
the difference in the BLEU scores, all three mod-
els (RNNenc, grConv and Moses) do a decent job
at translating, especially, short sentences. When
the source sentences are long, however, we no-
tice the performance degradation of the neural ma-
chine translation models.
Additionally, we present here what type of
structure the proposed gated recursive convolu-
tional network learns to represent. With a sample
sentence “Obama is the President of the United
States”, we present the parsing structure learned
by the grConv encoder and the generated transla-
tions, in Fig. 6. The figure suggests that the gr-
</bodyText>
<figure confidence="0.9728515">
0 10 20 30 40 50 60 70 80
Sentence length
</figure>
<figureCaption confidence="0.991863">
Figure 5: The BLEU scores achieved by an SMT
</figureCaption>
<bodyText confidence="0.891381458333334">
system for sentences of a given length. The plot
is smoothed by taking a window of size 10. We
use the solid, dotted and dashed lines to show the
effect of different lengths of source, reference or
both of them, respectively.
Conv extracts the vector representation of the sen-
tence by first merging “of the United States” to-
gether with “is the President of” and finally com-
bining this with “Obama is” and “.”, which is
well correlated with our intuition. Note, however,
that the structure learned by the grConv is differ-
ent from existing parsing approaches in the sense
that it returns soft parsing.
Despite the lower performance the grConv
showed compared to the RNN Encoder–Decoder,4
we find this property of the grConv learning a
grammar structure automatically interesting and
believe further investigation is needed.
4However, it should be noted that the number of gradient
updates used to train the grConv was a third of that used to
train the RNNenc. Longer training may change the result,
but for a fair comparison we chose to compare models which
were trained for an equal amount of time. Neither model was
trained to convergence.
</bodyText>
<figure confidence="0.998360769230769">
BLEU score
40
35
30
25
20
15
10
5
0
Source text
Reference text
Both
</figure>
<page confidence="0.747998">
109
</page>
<equation confidence="0.6180078">
+ +
+ + +
+ +
Translations
+
+ +
+ + +
+ +
+ + +
+ +
</equation>
<bodyText confidence="0.431481">
Obama is the President of the United States .
</bodyText>
<figure confidence="0.987698">
(a) (b)
</figure>
<figureCaption confidence="0.958918333333333">
Figure 6: (a) The visualization of the grConv structure when the input is “Obama is the President of
the United States.”. Only edges with gating coefficient ω higher than 0.1 are shown. (b) The top-10
translations generated by the grConv. The numbers in parentheses are the negative log-probability.
</figureCaption>
<bodyText confidence="0.8810674">
Obama est le Pr´esident des ´Etats-Unis . (2.06)
Obama est le pr´esident des ´Etats-Unis . (2.09)
Obama est le pr´esident des Etats-Unis . (2.61)
Obama est le Pr´esident des Etats-Unis . (3.33)
Barack Obama est le pr´esident des ´Etats-Unis . (4.41)
Barack Obama est le Pr´esident des ´Etats-Unis . (4.48)
Barack Obama est le pr´esident des Etats-Unis . (4.54)
L’Obama est le Pr´esident des ´Etats-Unis . (4.59)
L’Obama est le pr´esident des ´Etats-Unis . (4.67)
Obama est pr´esident du Congr`es des ´Etats-Unis .(5.09)
</bodyText>
<equation confidence="0.819190625">
+ +
+ +
+ +
+ +
+ +
+ +
+ +
+ +
</equation>
<sectionHeader confidence="0.994877" genericHeader="conclusions">
6 Conclusion and Discussion
</sectionHeader>
<bodyText confidence="0.999978833333333">
In this paper, we have investigated the property
of a recently introduced family of machine trans-
lation system based purely on neural networks.
We focused on evaluating an encoder–decoder ap-
proach, proposed recently in (Kalchbrenner and
Blunsom, 2013; Cho et al., 2014; Sutskever et al.,
2014), on the task of sentence-to-sentence trans-
lation. Among many possible encoder–decoder
models we specifically chose two models that dif-
fer in the choice of the encoder; (1) RNN with
gated hidden units and (2) the newly proposed
gated recursive convolutional neural network.
After training those two models on pairs of
English and French sentences, we analyzed their
performance using BLEU scores with respect to
the lengths of sentences and the existence of un-
known/rare words in sentences. Our analysis re-
vealed that the performance of the neural machine
translation suffers significantly from the length of
sentences. However, qualitatively, we found that
the both models are able to generate correct trans-
lations very well.
These analyses suggest a number of future re-
search directions in machine translation purely
based on neural networks.
Firstly, it is important to find a way to scale up
training a neural network both in terms of com-
putation and memory so that much larger vocabu-
laries for both source and target languages can be
used. Especially, when it comes to languages with
rich morphology, we may be required to come up
with a radically different approach in dealing with
words.
Secondly, more research is needed to prevent
the neural machine translation system from under-
performing with long sentences. Lastly, we need
to explore different neural architectures, especially
for the decoder. Despite the radical difference in
the architecture between RNN and grConv which
were used as an encoder, both models suffer from
the curse of sentence length. This suggests that it
may be due to the lack of representational power
in the decoder. Further investigation and research
are required.
In addition to the property of a general neural
machine translation system, we observed one in-
teresting property of the proposed gated recursive
convolutional neural network (grConv). The gr-
Conv was found to mimic the grammatical struc-
ture of an input sentence without any supervision
on syntactic structure of language. We believe this
property makes it appropriate for natural language
processing applications other than machine trans-
lation.
</bodyText>
<sectionHeader confidence="0.998821" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9990136">
The authors would like to acknowledge the sup-
port of the following agencies for research funding
and computing support: NSERC, Calcul Qu´ebec,
Compute Canada, the Canada Research Chairs
and CIFAR.
</bodyText>
<page confidence="0.990512">
110
</page>
<bodyText confidence="0.995179">
Matthew D. Zeiler. 2012. ADADELTA: an adap-
tive learning rate method. Technical report, arXiv
1212.5701.
</bodyText>
<sectionHeader confidence="0.941662" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996898038461538">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the ACL Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 355–362. Association for Compu-
tational Linguistics.
Nicolas Boulanger-Lewandowski, Yoshua Bengio, and
Pascal Vincent. 2013. Audio chord recognition with
recurrent neural networks. In ISMIR.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. In Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), October. to appear.
Alex Graves. 2012. Sequence transduction with re-
current neural networks. In Proceedings of the
29th International Conference on Machine Learning
(ICML 2012).
A. Graves. 2013. Generating sequences with recurrent
neural networks. arXiv:1308.0850 [cs.NE],
August.
S. Hochreiter and J. Schmidhuber. 1997. Long short-
term memory. Neural Computation, 9(8):1735–
1780.
Nal Kalchbrenner and Phil Blunsom. 2013. Two re-
current continuous translation models. In Proceed-
ings of the ACL Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
1700–1709. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ’03, pages 48–54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2011. Better evaluation metrics lead to better ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 375–384. Association for Computa-
tional Linguistics.
Xingyi Song, Trevor Cohn, and Lucia Specia. 2013.
BLEU deconstructed: Designing a better MT eval-
uation metric. In Proceedings of the 14th Interna-
tional Conference on Intelligent Text Processing and
Computational Linguistics (CICLING), March.
Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014.
Anonymized. In Anonymized.
</reference>
<page confidence="0.9988">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.732389">
<title confidence="0.997474">On the Properties of Neural Machine Translation: Encoder–Decoder Approaches</title>
<author confidence="0.993498">Cho Bart van_Merri¨enboer Dzmitry</author>
<affiliation confidence="0.896438">Universit´e de Montr´eal Jacobs University Bremen, Germany Yoshua Bengio Universit´e de Montr´eal, CIFAR Senior Fellow</affiliation>
<abstract confidence="0.999251541666666">Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>355--362</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11879" citStr="Axelrod et al., 2011" startWordPosition="1995" endWordPosition="1998">at does not require a non-trivial way to determine the target length. The first model will use the same RNN with the gated hidden unit as an encoder, as in (Cho et al., 2014), and the second one will use the proposed gated recursive convolutional neural network (grConv). We aim to understand the inductive bias of the encoder–decoder approach on the translation performance measured by BLEU. 4 Experiment Settings 4.1 Dataset We evaluate the encoder–decoder models on the task of English-to-French translation. We use the bilingual, parallel corpus which is a set of 348M selected by the method in (Axelrod et al., 2011) from a combination of Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 780M words respectively.1 We did not use separate monolingual data. The performance of the neural machien translation models was measured on the news-test2012, news-test2013 and news-test2014 sets ( 3000 lines each). When comparing to the SMT system, we use news-test2012 and news-test2013 as our development set for tuning the SMT system, and news-test2014 as our test set. Among all the sentence pairs in the prepared parallel corpus, for reasons of computational efficiency we only u</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 355–362. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicolas Boulanger-Lewandowski</author>
<author>Yoshua Bengio</author>
<author>Pascal Vincent</author>
</authors>
<title>Audio chord recognition with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In ISMIR.</booktitle>
<contexts>
<context position="14559" citStr="Boulanger-Lewandowski et al., 2013" startWordPosition="2432" endWordPosition="2435">bility given by a specific model (in this case, either the RNNenc or the grConv). At each time step of the decoder, we keep the s translation candidates with the highest log-probability, where s = 10 is the beam-width. During the beam-search, we exclude any hypothesis that includes an unknown word. For each end-of-sequence symbol that is selected among the highest scoring candidates the beam-width is reduced by one, until the beamwidth reaches zero. The beam-search to (approximately) find a sequence of maximum log-probability under RNN was proposed and used successfully in (Graves, 2012) and (Boulanger-Lewandowski et al., 2013). Recently, the authors of (Sutskever et al., 2014) found this approach to be effective in purely neural machine translation based on LSTM units. 2In all cases, we train the whole network including the word embedding matrix. The embedding dimensionality was chosen to be quite large, as the preliminary experiments with 155-dimensional embeddings showed rather poor performance. 106 Development Test Model All No UNK 16.60 17.50 19.12 20.99 28.92 32.00 24.73 27.03 21.74 22.94 32.20 35.40 Model Development Test RNNenc 13.15 13.92 grConv 9.97 9.97 Moses 30.64 33.30 Moses+RNNenc* 31.48 34.64 Moses+LS</context>
</contexts>
<marker>Boulanger-Lewandowski, Bengio, Vincent, 2013</marker>
<rawString>Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. 2013. Audio chord recognition with recurrent neural networks. In ISMIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
<author>Caglar Gulcehre</author>
<author>Fethi Bougares</author>
<author>Holger Schwenk</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning phrase representations using rnn encoder-decoder for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP</booktitle>
<note>to appear.</note>
<marker>Cho, van Merrienboer, Gulcehre, Bougares, Schwenk, Bengio, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), October. to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Sequence transduction with recurrent neural networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning (ICML</booktitle>
<contexts>
<context position="14518" citStr="Graves, 2012" startWordPosition="2429" endWordPosition="2430">e conditional probability given by a specific model (in this case, either the RNNenc or the grConv). At each time step of the decoder, we keep the s translation candidates with the highest log-probability, where s = 10 is the beam-width. During the beam-search, we exclude any hypothesis that includes an unknown word. For each end-of-sequence symbol that is selected among the highest scoring candidates the beam-width is reduced by one, until the beamwidth reaches zero. The beam-search to (approximately) find a sequence of maximum log-probability under RNN was proposed and used successfully in (Graves, 2012) and (Boulanger-Lewandowski et al., 2013). Recently, the authors of (Sutskever et al., 2014) found this approach to be effective in purely neural machine translation based on LSTM units. 2In all cases, we train the whole network including the word embedding matrix. The embedding dimensionality was chosen to be quite large, as the preliminary experiments with 155-dimensional embeddings showed rather poor performance. 106 Development Test Model All No UNK 16.60 17.50 19.12 20.99 28.92 32.00 24.73 27.03 21.74 22.94 32.20 35.40 Model Development Test RNNenc 13.15 13.92 grConv 9.97 9.97 Moses 30.64</context>
</contexts>
<marker>Graves, 2012</marker>
<rawString>Alex Graves. 2012. Sequence transduction with recurrent neural networks. In Proceedings of the 29th International Conference on Machine Learning (ICML 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Graves</author>
</authors>
<title>Generating sequences with recurrent neural networks.</title>
<date>2013</date>
<booktitle>arXiv:1308.0850 [cs.NE],</booktitle>
<contexts>
<context position="16055" citStr="Graves, 2013" startWordPosition="2681" endWordPosition="2682">ttom three rows on the sentences having no unknown words. (*) The result reported in (Cho et al., 2014) where the RNNenc was used to score phrase pairs in the phrase table. (◦) The result reported in (Sutskever et al., 2014) where an encoder–decoder with LSTM units was used to re-rank the n-best list generated by Moses. When we use the beam-search to find the k best translations, we do not use a usual log-probability but one normalized with respect to the length of the translation. This prevents the RNN decoder from favoring shorter translations, behavior which was observed earlier in, e.g., (Graves, 2013). 5 Results and Analysis 5.1 Quantitative Analysis In this paper, we are interested in the properties of the neural machine translation models. Specifically, the translation quality with respect to the length of source and/or target sentences and with respect to the number of words unknown to the model in each source/target sentence. First, we look at how the BLEU score, reflecting the translation performance, changes with respect to the length of the sentences (see Fig. 4 (a)– (b)). Clearly, both models perform relatively well on short sentences, but suffer significantly as the length of the </context>
</contexts>
<marker>Graves, 2013</marker>
<rawString>A. Graves. 2013. Generating sequences with recurrent neural networks. arXiv:1308.0850 [cs.NE], August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hochreiter</author>
<author>J Schmidhuber</author>
</authors>
<title>Long shortterm memory.</title>
<date>1997</date>
<journal>Neural Computation,</journal>
<volume>9</volume>
<issue>8</issue>
<pages>1780</pages>
<contexts>
<context position="6036" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="964" endWordPosition="967">rned by an RNN which has as an output for all possible symbols j = 1, ... , K, where wj are the rows of a weight matrix W. This results in the joint distribution T p(x) = H p(xt |xt−1, ... , x1). t=1 Recently, in (Cho et al., 2014) a new activation function for RNNs was proposed. The new activation function augments the usual logistic sigmoid activation function with two gating units called reset, r, and update, z, gates. Each gate depends on the previous hidden state h(t−1), and the current input xt controls the flow of information. This is reminiscent of long short-term memory (LSTM) units (Hochreiter and Schmidhuber, 1997). For details about this unit, we refer the reader to (Cho et al., 2014) and Fig. 1 (b). For the remainder of this paper, we always use this new activation function. 2.2 Gated Recursive Convolutional Neural Network Besides RNNs, another natural approach to dealing with variable-length sequences is to use a recursive convolutional neural network where the parameters at each level are shared through the whole network (see Fig. 2 (a)). In this section, we introduce a binary convolutional neural network whose weights are recursively applied to the input sequence until it outputs a single fixed-len</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>S. Hochreiter and J. Schmidhuber. 1997. Long shortterm memory. Neural Computation, 9(8):1735– 1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Two recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1700--1709</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1331" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="193" endWordPosition="196">translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically. 1 Introduction A new approach for statistical machine translation based purely on neural networks has recently been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). This new approach, which we refer to as neural machine translation, is inspired by the recent trend of deep representational learning. All the neural network models used in (Sutskever et al., 2014; Cho et al., 2014) consist of an encoder and a decoder. The encoder extracts a fixed-length vector representation from a variablelength input sentence, and from this representation the decoder generates a correct, variable-length target translation. ∗ Research done while visiting Universit´e de Montr´eal The emergence of the neural machine translation is highly significant,</context>
<context position="9594" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="1596" endWordPosition="1599">the La croissance économique a ralenti ces dernières années . Figure 3: The encoder–decoder architecture conditional distribution p(f |e) of a target sentence (translation) f given a source sentence e. Once the conditional distribution is learned by a model, one can use the model to directly sample a target sentence given a source sentence, either by actual sampling or by using a (approximate) search algorithm to find the maximum of the distribution. A number of recent papers have proposed to use neural networks to directly learn the conditional distribution from a bilingual, parallel corpus (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). For instance, the authors of (Kalchbrenner and Blunsom, 2013) proposed an approach involving a convolutional ngram model to extract a vector of a source sentence which is decoded with an inverse convolutional n-gram model augmented with an RNN. In (Sutskever et al., 2014), an RNN with LSTM units was used to encode a source sentence and starting from the last hidden state, to decode a target sentence. Similarly, the authors of (Cho et al., 2014) proposed to use an RNN to encode and decode a pair of source and target phrases. At the core of all these </context>
<context position="10881" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="1822" endWordPosition="1826">ig. 3). The encoder processes a variable-length input (source sentence) and builds a fixed-length vector representation (denoted as z in Fig. 3). Conditioned on the encoded representation, the decoder generates ⎡ ωc ⎤ ⎣ ωl ⎦ = ωr Decode [z 1 ,z 2 , ... ,z d ] Encode Economic growth has slowed down in recent years . 105 a variable-length sequence (target sentence). Before (Sutskever et al., 2014) this encoder– decoder approach was used mainly as a part of the existing statistical machine translation (SMT) system. This approach was used to re-rank the n-best list generated by the SMT system in (Kalchbrenner and Blunsom, 2013), and the authors of (Cho et al., 2014) used this approach to provide an additional score for the existing phrase table. In this paper, we concentrate on analyzing the direct translation performance, as in (Sutskever et al., 2014), with two model configurations. In both models, we use an RNN with the gated hidden unit (Cho et al., 2014), as this is one of the only options that does not require a non-trivial way to determine the target length. The first model will use the same RNN with the gated hidden unit as an encoder, as in (Cho et al., 2014), and the second one will use the proposed gated </context>
<context position="28803" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="4856" endWordPosition="4859"> Obama est le pr´esident des ´Etats-Unis . (4.41) Barack Obama est le Pr´esident des ´Etats-Unis . (4.48) Barack Obama est le pr´esident des Etats-Unis . (4.54) L’Obama est le Pr´esident des ´Etats-Unis . (4.59) L’Obama est le pr´esident des ´Etats-Unis . (4.67) Obama est pr´esident du Congr`es des ´Etats-Unis .(5.09) + + + + + + + + + + + + + + + + 6 Conclusion and Discussion In this paper, we have investigated the property of a recently introduced family of machine translation system based purely on neural networks. We focused on evaluating an encoder–decoder approach, proposed recently in (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), on the task of sentence-to-sentence translation. Among many possible encoder–decoder models we specifically chose two models that differ in the choice of the encoder; (1) RNN with gated hidden units and (2) the newly proposed gated recursive convolutional neural network. After training those two models on pairs of English and French sentences, we analyzed their performance using BLEU scores with respect to the lengths of sentences and the existence of unknown/rare words in sentences. Our analysis revealed that the performance of the neural machine t</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Two recurrent continuous translation models. In Proceedings of the ACL Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1700–1709. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18470" citStr="Koehn et al., 2003" startWordPosition="3069" endWordPosition="3072"> overall translation performance (see Table 1 (a)). This analysis suggests that that the current neural translation approach has its weakness in handling long sentences. The most obvious explanatory hypothesis is that the fixed-length vector representation does not have enough capacity to encode a long sentence with complicated structure and meaning. In order to encode a variable-length sequence, a neural network may “sacrifice” some of the important topics in the input sentence in order to remember others. This is in stark contrast to the conventional phrase-based machine translation system (Koehn et al., 2003). As we can see from Fig. 5, the conventional system trained on the same dataset (with additional monolingual data for the language model) tends to get a higher BLEU score on longer 3We used Moses as a baseline, trained with additional monolingual data for a 4-gram language model. 107 Source She explained her new position of foreign affairs and security policy representative as a reply to a question: ”Who is the European Union? Which phone number should I call?”; i.e. as an important step to unification and better clarity of Union’s policy towards countries such as China or India. Reference El</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 48–54, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Liu</author>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation metrics lead to better machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>375--384</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25275" citStr="Liu et al., 2011" startWordPosition="4240" endWordPosition="4243">own words. sentences. In fact, if we limit the lengths of both the source sentence and the reference translation to be between 10 and 20 words and use only the sentences with no unknown words, the BLEU scores on the test set are 27.81 and 33.08 for the RNNenc and Moses, respectively. Note that we observed a similar trend even when we used sentences of up to 50 words to train these models. 5.2 Qualitative Analysis Although BLEU score is used as a de-facto standard metric for evaluating the performance of a machine translation system, it is not the perfect metric (see, e.g., (Song et al., 2013; Liu et al., 2011)). Hence, here we present some of the actual translations generated from the two models, RNNenc and grConv. In Table. 2 (a)–(b), we show the translations of some randomly selected sentences from the development and test sets. We chose the ones that have no unknown words. (a) lists long sentences (longer than 30 words), and (b) short sentences (shorter than 10 words). We can see that, despite the difference in the BLEU scores, all three models (RNNenc, grConv and Moses) do a decent job at translating, especially, short sentences. When the source sentences are long, however, we notice the perfor</context>
</contexts>
<marker>Liu, Dahlmeier, Ng, 2011</marker>
<rawString>Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011. Better evaluation metrics lead to better machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 375–384. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xingyi Song</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>BLEU deconstructed: Designing a better MT evaluation metric.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING),</booktitle>
<contexts>
<context position="25256" citStr="Song et al., 2013" startWordPosition="4236" endWordPosition="4239">iven number of unknown words. sentences. In fact, if we limit the lengths of both the source sentence and the reference translation to be between 10 and 20 words and use only the sentences with no unknown words, the BLEU scores on the test set are 27.81 and 33.08 for the RNNenc and Moses, respectively. Note that we observed a similar trend even when we used sentences of up to 50 words to train these models. 5.2 Qualitative Analysis Although BLEU score is used as a de-facto standard metric for evaluating the performance of a machine translation system, it is not the perfect metric (see, e.g., (Song et al., 2013; Liu et al., 2011)). Hence, here we present some of the actual translations generated from the two models, RNNenc and grConv. In Table. 2 (a)–(b), we show the translations of some randomly selected sentences from the development and test sets. We chose the ones that have no unknown words. (a) lists long sentences (longer than 30 words), and (b) short sentences (shorter than 10 words). We can see that, despite the difference in the BLEU scores, all three models (RNNenc, grConv and Moses) do a decent job at translating, especially, short sentences. When the source sentences are long, however, w</context>
</contexts>
<marker>Song, Cohn, Specia, 2013</marker>
<rawString>Xingyi Song, Trevor Cohn, and Lucia Specia. 2013. BLEU deconstructed: Designing a better MT evaluation metric. In Proceedings of the 14th International Conference on Intelligent Text Processing and Computational Linguistics (CICLING), March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<date>2014</date>
<note>Anonymized. In Anonymized.</note>
<contexts>
<context position="1356" citStr="Sutskever et al., 2014" startWordPosition="197" endWordPosition="200">N Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically. 1 Introduction A new approach for statistical machine translation based purely on neural networks has recently been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). This new approach, which we refer to as neural machine translation, is inspired by the recent trend of deep representational learning. All the neural network models used in (Sutskever et al., 2014; Cho et al., 2014) consist of an encoder and a decoder. The encoder extracts a fixed-length vector representation from a variablelength input sentence, and from this representation the decoder generates a correct, variable-length target translation. ∗ Research done while visiting Universit´e de Montr´eal The emergence of the neural machine translation is highly significant, both practically and the</context>
<context position="9637" citStr="Sutskever et al., 2014" startWordPosition="1604" endWordPosition="1607">es années . Figure 3: The encoder–decoder architecture conditional distribution p(f |e) of a target sentence (translation) f given a source sentence e. Once the conditional distribution is learned by a model, one can use the model to directly sample a target sentence given a source sentence, either by actual sampling or by using a (approximate) search algorithm to find the maximum of the distribution. A number of recent papers have proposed to use neural networks to directly learn the conditional distribution from a bilingual, parallel corpus (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). For instance, the authors of (Kalchbrenner and Blunsom, 2013) proposed an approach involving a convolutional ngram model to extract a vector of a source sentence which is decoded with an inverse convolutional n-gram model augmented with an RNN. In (Sutskever et al., 2014), an RNN with LSTM units was used to encode a source sentence and starting from the last hidden state, to decode a target sentence. Similarly, the authors of (Cho et al., 2014) proposed to use an RNN to encode and decode a pair of source and target phrases. At the core of all these recent works lies an encoder–decoder archit</context>
<context position="11111" citStr="Sutskever et al., 2014" startWordPosition="1862" endWordPosition="1865">z 1 ,z 2 , ... ,z d ] Encode Economic growth has slowed down in recent years . 105 a variable-length sequence (target sentence). Before (Sutskever et al., 2014) this encoder– decoder approach was used mainly as a part of the existing statistical machine translation (SMT) system. This approach was used to re-rank the n-best list generated by the SMT system in (Kalchbrenner and Blunsom, 2013), and the authors of (Cho et al., 2014) used this approach to provide an additional score for the existing phrase table. In this paper, we concentrate on analyzing the direct translation performance, as in (Sutskever et al., 2014), with two model configurations. In both models, we use an RNN with the gated hidden unit (Cho et al., 2014), as this is one of the only options that does not require a non-trivial way to determine the target length. The first model will use the same RNN with the gated hidden unit as an encoder, as in (Cho et al., 2014), and the second one will use the proposed gated recursive convolutional neural network (grConv). We aim to understand the inductive bias of the encoder–decoder approach on the translation performance measured by BLEU. 4 Experiment Settings 4.1 Dataset We evaluate the encoder–de</context>
<context position="14610" citStr="Sutskever et al., 2014" startWordPosition="2440" endWordPosition="2443">nc or the grConv). At each time step of the decoder, we keep the s translation candidates with the highest log-probability, where s = 10 is the beam-width. During the beam-search, we exclude any hypothesis that includes an unknown word. For each end-of-sequence symbol that is selected among the highest scoring candidates the beam-width is reduced by one, until the beamwidth reaches zero. The beam-search to (approximately) find a sequence of maximum log-probability under RNN was proposed and used successfully in (Graves, 2012) and (Boulanger-Lewandowski et al., 2013). Recently, the authors of (Sutskever et al., 2014) found this approach to be effective in purely neural machine translation based on LSTM units. 2In all cases, we train the whole network including the word embedding matrix. The embedding dimensionality was chosen to be quite large, as the preliminary experiments with 155-dimensional embeddings showed rather poor performance. 106 Development Test Model All No UNK 16.60 17.50 19.12 20.99 28.92 32.00 24.73 27.03 21.74 22.94 32.20 35.40 Model Development Test RNNenc 13.15 13.92 grConv 9.97 9.97 Moses 30.64 33.30 Moses+RNNenc* 31.48 34.64 Moses+LSTM◦ 32 35.65 RNNenc 21.01 23.45 grConv 17.19 18.22 </context>
<context position="17836" citStr="Sutskever et al., 2014" startWordPosition="2966" endWordPosition="2969">-based SMT system.3 Clearly the phrase-based SMT system still shows the superior performance over the proposed purely neural machine translation system, but we can see that under certain conditions (no unknown words in both source and reference sentences), the difference diminishes quite significantly. Furthermore, if we consider only short sentences (10–20 words per sentence), the difference further decreases (see Table 1 (b). Furthermore, it is possible to use the neural machine translation models together with the existing phrase-based system, which was found recently in (Cho et al., 2014; Sutskever et al., 2014) to improve the overall translation performance (see Table 1 (a)). This analysis suggests that that the current neural translation approach has its weakness in handling long sentences. The most obvious explanatory hypothesis is that the fixed-length vector representation does not have enough capacity to encode a long sentence with complicated structure and meaning. In order to encode a variable-length sequence, a neural network may “sacrifice” some of the important topics in the input sentence in order to remember others. This is in stark contrast to the conventional phrase-based machine trans</context>
<context position="28846" citStr="Sutskever et al., 2014" startWordPosition="4864" endWordPosition="4867">Barack Obama est le Pr´esident des ´Etats-Unis . (4.48) Barack Obama est le pr´esident des Etats-Unis . (4.54) L’Obama est le Pr´esident des ´Etats-Unis . (4.59) L’Obama est le pr´esident des ´Etats-Unis . (4.67) Obama est pr´esident du Congr`es des ´Etats-Unis .(5.09) + + + + + + + + + + + + + + + + 6 Conclusion and Discussion In this paper, we have investigated the property of a recently introduced family of machine translation system based purely on neural networks. We focused on evaluating an encoder–decoder approach, proposed recently in (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), on the task of sentence-to-sentence translation. Among many possible encoder–decoder models we specifically chose two models that differ in the choice of the encoder; (1) RNN with gated hidden units and (2) the newly proposed gated recursive convolutional neural network. After training those two models on pairs of English and French sentences, we analyzed their performance using BLEU scores with respect to the lengths of sentences and the existence of unknown/rare words in sentences. Our analysis revealed that the performance of the neural machine translation suffers significantly from the l</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc Le. 2014. Anonymized. In Anonymized.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
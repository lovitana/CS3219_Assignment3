<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000057">
<title confidence="0.994296">
Domain Adaptation with Active Learning for Coreference Resolution
</title>
<author confidence="0.860774">
Shanheng Zhao
</author>
<note confidence="0.569323333333333">
Elance
441 Logue Ave
Mountain View, CA 94043, USA
</note>
<email confidence="0.995918">
szhao@elance.com
</email>
<author confidence="0.985865">
Hwee Tou Ng
</author>
<affiliation confidence="0.999324">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.703048">
13 Computing Drive, Singapore 117417
</address>
<email confidence="0.997466">
nght@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993858" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963695652174">
In the literature, most prior work on
coreference resolution centered on the
newswire domain. Although a coreference
resolution system trained on the newswire
domain performs well on newswire texts,
there is a huge performance drop when it is
applied to the biomedical domain. In this
paper, we present an approach integrat-
ing domain adaptation with active learning
to adapt coreference resolution from the
newswire domain to the biomedical do-
main. We explore the effect of domain
adaptation, active learning, and target do-
main instance weighting for coreference
resolution. Experimental results show
that domain adaptation with active learn-
ing and target domain instance weighting
achieves performance on MEDLINE ab-
stracts similar to a system trained on coref-
erence annotation of only target domain
training instances, but with a greatly re-
duced number of target domain training
instances that we need to annotate.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999916464285714">
Coreference resolution is the task of determin-
ing whether two or more noun phrases (NPs) in
a text refer to the same entity. Successful coref-
erence resolution benefits many natural language
processing (NLP) tasks, such as information ex-
traction and question answering. In the literature,
most prior work on coreference resolution recasts
the problem as a two-class classification problem.
Machine learning-based classifiers are applied to
determine whether a candidate anaphor and a po-
tential antecedent are coreferential (Soon et al.,
2001; Ng and Cardie, 2002; Stoyanov et al., 2009;
Zhao and Ng, 2010).
In recent years, with the advances in biologi-
cal and life science research, there is a rapidly in-
creasing number of biomedical texts, including re-
search papers, patent documents, etc. This results
in an increasing demand for applying natural lan-
guage processing and information retrieval tech-
niques to efficiently exploit information contained
in these large amounts of texts. However, corefer-
ence resolution, one of the core tasks in NLP, has
only a relatively small body of prior research in
the biomedical domain (Kim et al., 2011a; Kim et
al., 2011b).
A large body of prior research on coreference
resolution focuses on texts in the newswire do-
main. Standardized data sets, such as MUC
(DARPA Message Understanding Conference,
(MUC-6, 1995; MUC-7, 1998)) and ACE (NIST
Automatic Content Extraction Entity Detection
and Tracking task, (NIST, 2002)) data sets are
widely used in the study of coreference resolution.
Traditionally, in order to apply supervised ma-
chine learning approaches to an NLP task in a spe-
cific domain, one needs to collect a text corpus
in the domain and annotate it to serve as training
data. Compared to other NLP tasks, e.g., part-of-
speech (POS) tagging or named entity (NE) tag-
ging, the annotation for coreference resolution is
much more challenging and time-consuming. The
reason is that in tasks like POS tagging, an annota-
tor only needs to focus on each markable (a word,
in the case of POS tagging) and a small window
of its neighboring words. In contrast, to annotate
a coreferential relation, an annotator needs to first
recognize whether a certain text span is a mark-
able, and then scan through the text preceding the
markable (a potential anaphor) to look for the an-
tecedent. It also requires the annotator to under-
stand the text in order to annotate coreferential re-
lations, which are semantic in nature. If a mark-
able is non-anaphoric, the annotator has to scan to
the beginning of the text to realize that. Cohen
et al. (2010) reported that it took an average of 20
hours to annotate coreferential relations in a single
</bodyText>
<page confidence="0.992476">
21
</page>
<note confidence="0.9928945">
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 21–29,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998474326923077">
document with an average length of 6,155 words,
while an annotator could annotate 3,000 words per
hour in POS tag annotation (Marcus et al., 1993).
The simplest approach to avoid the time-
consuming data annotation in a new domain is
to train a coreference resolution system on a
resource-rich domain and apply it to a different
target domain without any additional data anno-
tation. Although coreference resolution systems
work well on test texts in the same domain as
the training texts, there is a huge performance
drop when they are tested on a different domain.
This motivates the usage of domain adaptation
techniques for coreference resolution: adapting a
coreference resolution system from one source do-
main in which we have a large collection of an-
notated data, to a second target domain in which
we need good performance. It is almost inevitable
that we annotate some data in the target domain to
achieve good coreference resolution performance.
The question is how to minimize the amount of an-
notation needed. In the literature, active learning
has been exploited to reduce the amount of anno-
tation needed (Lewis and Gale, 1994). In contrast
to annotating the entire data set, active learning se-
lects only a subset of the data to annotate in an iter-
ative process. How to apply active learning and in-
tegrate it with domain adaptation remains an open
problem for coreference resolution.
In this paper, we explore domain adaptation
for coreference resolution from the resource-rich
newswire domain to the biomedical domain. Our
approach comprises domain adaptation, active
learning, and target domain instance weighting
to leverage the existing annotated corpora from
the newswire domain, so as to reduce the cost
of developing a coreference resolution system in
the biomedical domain. Our approach achieves
comparable coreference resolution performance
on MEDLINE abstracts, but with a large reduction
in the number of training instances that we need to
annotate. To the best of our knowledge, our work
is the first to combine domain adaptation and ac-
tive learning for coreference resolution.
The rest of this paper is organized as follows.
We first review the related work in Section 2. Then
we describe the coreference resolution system in
Section 3, and the domain adaptation and active
learning techniques in Section 4. Experimental re-
sults are presented in Section 5. Finally, we ana-
lyze the results in Section 6 and conclude in Sec-
tion 7.
</bodyText>
<sectionHeader confidence="0.999211" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999786875">
Not only is there a relatively small body of prior
research on coreference resolution in the biomed-
ical domain, there are also fewer annotated cor-
pora in this domain. Casta˜no et al. (2002) were
among the first to annotate coreferential relations
in the biomedical domain. Their annotation only
concerned the pronominal and nominal anaphoric
expressions in 46 biomedical abstracts. Gasperin
and Briscoe (2007) annotated coreferential rela-
tions on 5 full articles in the biomedical domain,
but only on noun phrases referring to bio-entities.
Yang et al. (2004) annotated full NP coreferential
relations on biomedical abstracts of the GENIA
corpus. The ongoing project of the CRAFT cor-
pus is expected to annotate all coreferential rela-
tions on full text of biomedical articles (Cohen et
al., 2010).
Unlike the work of (Casta˜no et al., 2002),
(Gasperin and Briscoe, 2008), and (Gasperin,
2009) that resolved coreferential relations on cer-
tain restricted entities in the biomedical domain,
we resolve all NP coreferential relations. Al-
though the GENIA corpus contains 1,999 biomed-
ical abstracts, Yang et al. (2004) tested only on 200
abstracts under 5-fold cross validation. In contrast,
we randomly selected 399 abstracts in the 1,999
MEDLINE abstracts of the GENIA-MEDCo cor-
pus as the test set, and as such our evaluation was
carried out on a larger scale.
Domain adaptation has been studied and suc-
cessfully applied to many natural language pro-
cessing tasks (Jiang and Zhai, 2007; Daume III,
2007; Dahlmeier and Ng, 2010; Yang et al., 2012).
On the other hand, active learning has also been
applied to NLP tasks to reduce the need of data an-
notation in the literature (Tang et al., 2002; Laws
et al., 2012; Miller et al., 2012). Unlike the afore-
mentioned work that applied only one of domain
adaptation or active learning to NLP tasks, we
combine both. There is relatively less research
on combining domain adaptation and active learn-
ing together for NLP tasks (Chan and Ng, 2007;
Zhong et al., 2008; Rai et al., 2010). Chan and
Ng (2007) and Zhong et al. (2008) used count
merging and augment, respectively, as their do-
main adaptation techniques whereas we apply and
compare multiple state-of-the-art domain adapta-
tion techniques. Rai et al. (2010) exploited a
</bodyText>
<page confidence="0.993686">
22
</page>
<bodyText confidence="0.999913">
streaming active learning setting whereas ours is
pool-based.
Dahlmeier and Ng (2010) evaluated the perfor-
mance of three previously proposed domain adap-
tation algorithms for semantic role labeling. They
evaluated the performance of domain adaptation
with different sizes of target domain training data.
In each of their experiments with a certain target
domain training data size, the target domain train-
ing data were added all at once. In contrast, we add
the target domain training instances selectively in
an iterative process. Different from (Dahlmeier
and Ng, 2010), we weight the target domain in-
stances to further boost the performance of do-
main adaptation. Our work is the first system-
atic study of domain adaptation with active learn-
ing for coreference resolution. Although Gasperin
(2009) tried to apply active learning for anaphora
resolution, her results were negative: using ac-
tive learning was not better than randomly select-
ing instances in her work. Miwa et al. (2012)
incorporated a rule-based coreference resolution
system for automatic biomedical event extraction,
and showed that by adding training data from other
domains as supplementary training data and us-
ing domain adaptation, one can achieve a higher
F-measure in event extraction.
</bodyText>
<sectionHeader confidence="0.979352" genericHeader="method">
3 Coreference Resolution
</sectionHeader>
<bodyText confidence="0.999745458333333">
The gold standard annotation and the output by a
coreference resolution system are called the key
and the response, respectively. In both the key and
the response, a coreference chain is formed by a
set of coreferential markables. A markable is a
noun phrase which satisfies the markable defini-
tion in an individual corpus. Here is an example:
When the same MTHC lines are ex-
posed to TNF-alpha in combination with
IFN-gamma, the cells instead become
DC.
In the above sentence, the same MTHC lines
and the cells are referring to the same entity and
hence are coreferential. It is possible that more
than two markables are coreferential in a text. The
task of coreference resolution is to determine these
relations in a given text.
To evaluate the performance of coreference res-
olution, we follow the MUC evaluation metric in-
troduced by (Vilain et al., 1995). Let Si be an
equivalence class generated by the key (i.e., Si
is a coreference chain), and p(Si) be a partition
of Si relative to the response. Recall is the num-
ber of correctly identified links over the number of
</bodyText>
<equation confidence="0.46307">
Y-(|Si|−|p(Si)|)
links in the key: Recall =. Pre-
Y-(|Si|−1)
</equation>
<bodyText confidence="0.8718468">
cision, on the other hand, is defined in the oppo-
site way by switching the role of key and response.
F-measure is a trade-off between recall and preci-
sion: F = 2·Recall·Precision
Recall+Precision .
</bodyText>
<sectionHeader confidence="0.720296" genericHeader="method">
4 Domain Adaptation with Active
Learning
</sectionHeader>
<subsectionHeader confidence="0.998082">
4.1 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.999964888888889">
Domain adaptation is applicable when one has
a large amount of annotated training data in the
source domain and a small amount or none of
the annotated training data in the target domain.
We evaluate the AUGMENT technique introduced
by (Daume III, 2007), as well as the INSTANCE
WEIGHTING (IW) and the INSTANCE PRUNING
(IP) techniques introduced by (Jiang and Zhai,
2007).
</bodyText>
<sectionHeader confidence="0.520111" genericHeader="method">
4.1.1 AUGMENT
</sectionHeader>
<bodyText confidence="0.999754222222222">
Daume III (2007) introduced a simple domain
adaptation technique by feature space augmenta-
tion. It maps the feature space of each instance
into a feature space of higher dimension. Suppose
x is the feature vector of an instance. Define Φs
and Φt to be the mappings of an instance from
the original feature space to an augmented feature
space in the source and the target domain, respec-
tively:
</bodyText>
<equation confidence="0.999989">
Φs(x) = (x, x, 0) (1)
Φt(x) = (x, 0, x) (2)
</equation>
<bodyText confidence="0.999983">
where 0 = (0, 0, ... , 0) is a zero vector of length
|x|. The mapping can be treated as taking each
feature in the original feature space and making
three versions of it: a general version, a source-
specific version, and a target-specific version. The
augmented source domain data will contain only
the general and the source-specific versions, while
the augmented target domain data will contain
only the general and the target-specific versions.
</bodyText>
<sectionHeader confidence="0.9439455" genericHeader="method">
4.1.2 INSTANCE WEIGHTING and INSTANCE
PRUNING
</sectionHeader>
<bodyText confidence="0.998959">
Let x and y be the feature vector and the corre-
sponding true label of an instance, respectively.
</bodyText>
<page confidence="0.98818">
23
</page>
<bodyText confidence="0.999317310344828">
Jiang and Zhai (2007) pointed out that when ap-
plying a classifier trained on a source domain to
a target domain, the joint probability Pt(x, y) in
the target domain may be different from the joint
probability Ps(x, y) in the source domain. They
proposed a general framework to use Ps(x, y) to
estimate Pt(x, y). The joint probability P(x, y)
can be factored into P(x, y) = P(y|x)P(x). The
adaptation of the first component is labeling adap-
tation, while the adaptation of the second compo-
nent is instance adaptation. We explore only label-
ing adaptation.
To calibrate the conditional probability P(y|x)
from the source domain to the target domain, ide-
ally each source domain training instance (xi, yi)
should be given a weight p  |Although
3�yIx�.
Ps(ysi |xsi) can be estimated from the source do-
main training data, the estimation of Pt(ysi |xsi )
is much harder. Jiang and Zhai(2007) proposed
two methods to estimate Pt(ysi |xsi): INSTANCE
WEIGHTING and INSTANCE PRUNING. Both
methods first train a classifier with a small amount
of target domain training data. Then, INSTANCE
WEIGHTING directly estimates Pt(ysi |xsi) using
the trained classifier. INSTANCE PRUNING, on the
other hand, removes the top N source domain in-
stances that are predicted wrongly, ranked by the
prediction confidence.
</bodyText>
<subsubsectionHeader confidence="0.566599">
4.1.3 Target Domain Instance Weighting
</subsubsectionHeader>
<bodyText confidence="0.999891142857143">
Both INSTANCE WEIGHTING and INSTANCE
PRUNING set the weights of the source domain
instances. In domain adaptation, there are typi-
cally many more source domain training instances
than target domain training instances. Target do-
main instance weighting can effectively reduce the
imbalance. Unlike INSTANCE WEIGHTING and
INSTANCE PRUNING in which each source do-
main instance is weighted individually, we give
all target domain instances the same weight. This
target domain instance weighting scheme is not
only complementary to INSTANCE WEIGHTING
and INSTANCE PRUNING, but is also applicable
to AUGMENT.
</bodyText>
<subsectionHeader confidence="0.983839">
4.2 Active Learning
</subsectionHeader>
<bodyText confidence="0.963774333333333">
Active learning iteratively selects the most infor-
mative instances to label, adds them to the train-
ing data pool, and trains a new classifier with the
enlarged data pool. We follow (Lewis and Gale,
1994) and use the uncertainty sampling strategy in
our active learning setting.
</bodyText>
<table confidence="0.937723588235294">
Ds the set of source domain training instances
Dt the set of target domain training instances
Da ;
P coreference resolution system trained on Ds
T number of iterations
for i from 1 to T do
for each di E Dt do
di prediction of di using P _
pi prediction confidence of di
end for
D′a top N instances with the lowest pi
Da Da + D′a
Dt Dt − D′a
provide correct labels to the unlabeled instances in D′a
P coreference resolution system trained on Ds and
Da using the chosen domain adaptation technique
end for
</table>
<figureCaption confidence="0.988161">
Figure 1: An algorithm for domain adaptation
with active learning
</figureCaption>
<subsectionHeader confidence="0.590518">
4.3 Domain Adaptation with Active Learning
</subsectionHeader>
<bodyText confidence="0.947263047619048">
Combining domain adaptation and active learning
together, the algorithm we use is shown in Figure
1.
In our domain adaptation setting, there is a pa-
rameter At for target domain instance weighting.
Because the number of target domain instances is
different in each iteration, the weight should be ad-
justed in each iteration. We give all target domain
training instances an equal weight of At = Ns/Nt,
where Ns and Nt are the numbers of instances in
the source domain and the target domain in the
current iteration, respectively. We set N = 10 to
add 10 instances in each iteration to speed up the
active learning process.
To provide the correct labels, the labeling pro-
cess shows the text on the screen, highlights the
two NPs, and asks the annotator to decide if they
are coreferential. In our experiments, this is simu-
lated by providing the gold standard coreferential
information on this NP pair to the active learning
process.
</bodyText>
<sectionHeader confidence="0.999673" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.952154">
5.1 The Corpora
</subsectionHeader>
<bodyText confidence="0.999887571428571">
We explore domain adaptation from the newswire
domain to the biomedical domain. The newswire
and biomedical domain data that we use are the
ACE Phase-2 corpora and the GENIA-MEDCo
corpus, respectively. The ACE corpora con-
tain 422 and 92 training and test texts, re-
spectively (NIST, 2002). The texts come from
</bodyText>
<page confidence="0.994025">
24
</page>
<bodyText confidence="0.9987714">
three newswire sources: BNEWS, NPAPER, and
NWIRE. The GENIA-MEDCo corpus contains
1,999 MEDLINE abstracts1. We randomly split
the GENIA corpus into a training set and a test
set, containing 1,600 and 399 texts, respectively.
</bodyText>
<subsectionHeader confidence="0.99661">
5.2 The Coreference Resolution System
</subsectionHeader>
<bodyText confidence="0.999989384615385">
In this study, we use Reconcile, a state-of-the-
art coreference resolution system implemented by
(Stoyanov et al., 2009). The input to the corefer-
ence resolution system is raw text, and we apply a
sequence of preprocessing components to process
it. Following Reconcile, the individual prepro-
cessing steps include: 1) sentence segmentation
(using the OpenNLP toolkit2); 2) tokenization (us-
ing the OpenNLP toolkit); 3) POS tagging (using
the OpenNLP toolkit); 4) syntactic parsing (using
the Berkeley Parser3); and 5) named entity recog-
nition (using the Stanford NER4). Markables are
extracted as defined in each individual corpus. All
possible markable pairs in the training and test set
are extracted to form training and test instances,
respectively. The learning algorithm we use is
maximum entropy modeling, implemented in the
DALR package5 (Jiang and Zhai, 2007). The
coreference resolution system employs a compre-
hensive set of 62 features to represent each train-
ing and test instance, including lexical, proximity,
grammatical, and semantic features (Stoyanov et
al., 2009). We do not introduce additional features
motivated from the biomedical domain, but use the
same feature set for both the source and target do-
mains.
</bodyText>
<subsectionHeader confidence="0.994722">
5.3 Preprocessing
</subsectionHeader>
<bodyText confidence="0.9999632">
For the ACE corpora, all preprocessing compo-
nents use the original models (provided by the
OpenNLP toolkit, the Berkeley Parser, and the
Stanford NER). For the GENIA corpus, since it is
from a very different domain, the original models
do not perform well. However, the GENIA cor-
pus contains multiple layers of annotations. We
use these annotations to re-train each of the pre-
processing components (except tokenization) us-
ing the 1,600 training texts of the GENIA cor-
</bodyText>
<footnote confidence="0.920271666666667">
1http://nlp.i2r.a-star.edu.sg/medco.html
2http://opennlp.sourceforge.net/
3http://code.google.com/p/berkeleyparser/
4http://nlp.stanford.edu/ner/
5http://www.mysmu.edu/faculty/jingjiang/software/
DALR.html
</footnote>
<table confidence="0.993524538461538">
NPAPER NPAPER GENIA GENIA
TRAIN TEST TRAIN TEST
76 Number of Docs 399
17 1,600
Number of Words
Total 68,463 17,350 391,380 95,405
Avg. 900.8 1,020.6 244.6 239.1
Number of Markables
Total 21,492 5,153 99,408 24,397
Avg. 282.8 303.1 62.1 61.1
Number of Instances
Total 3,365,680 871,314 3,335,640 798,844
Avg. 44,285.3 51,253.8 2,084.8 2,002.1
</table>
<tableCaption confidence="0.946585">
Table 1: Statistics of the NPAPER and GENIA
data sets
</tableCaption>
<bodyText confidence="0.99924185">
pus6. We do not use any texts from the test set
when training these models. Also, we do not use
any NLP toolkits from the biomedical domain, but
only use general toolkits trained with biomedical
training data. These re-trained preprocessing com-
ponents are then applied to process the entire GE-
NIA corpus, including both the training and test
sets.
Instead of using the entire ACE corpora, we
choose the NPAPER portion of the ACE corpora
as the source domain in the experiments, because
it is the best performing one among the three por-
tions. Under these preprocessing settings, the
recall percentages of markable extraction on the
training and test set of the NPAPER corpus are
94.5% and 95.5% respectively, while the recall
percentages of markable extraction on the training
and test set of the GENIA corpus are 87.6% and
86.6% respectively. The statistics of the NPAPER
and the GENIA corpora are listed in Table 1.
</bodyText>
<subsectionHeader confidence="0.990277">
5.4 Baseline Results
</subsectionHeader>
<bodyText confidence="0.999934083333333">
Under our experimental settings, a coreference
resolution system that is trained on the NPA-
PER training set and tested on the NPAPER test
set achieves recall, precision, and F-measure of
59.0%, 70.6%, and 64.3%, respectively. This
is comparable to the state-of-the-art performance
(Stoyanov et al., 2009). Table 2 compares the per-
formance of testing on the GENIA test set, but
training with the GENIA training set or the NPA-
PER training set. Training with in-domain data
achieves an F-measure that is 9.1% higher than
training with out-of-domain data. Training with
</bodyText>
<footnote confidence="0.99640325">
6It turned out that the re-trained tokenization model gave
poorer performance and produced many errors on punctua-
tion symbols. Thus, we stuck to using the original tokeniza-
tion model.
</footnote>
<page confidence="0.996322">
25
</page>
<table confidence="0.999869">
Training Set Recall Precision F-measure
GENIA Training Set 37.7 71.9 49.5
NPAPER Training Set 30.3 60.7 40.4
</table>
<tableCaption confidence="0.996622">
Table 2: MUC F-measures on the GENIA test set
</tableCaption>
<bodyText confidence="0.99805825">
in-domain data is better than training with out-of-
domain data for both recall and precision. This
confirms the impact of domain difference between
the newswire and the biomedical domain.
</bodyText>
<subsectionHeader confidence="0.905821">
5.5 Domain Adaptation with Active Learning
</subsectionHeader>
<bodyText confidence="0.999302244444444">
In the experiments on domain adaptation with ac-
tive learning for coreference resolution, we as-
sume that the source domain training data are an-
notated. The target domain training data are not
annotated but are used as a data pool for instance
selection. The algorithm selects the instances in
the data pool to annotate and add them to the train-
ing data to update the classifier. The target domain
test set is strictly separated from this data pool, i.e.,
none of the target domain test data are used in the
instance selection process of active learning.
From Table 1, one can see that both training sets
in the NPAPER and the GENIA corpora contain
large numbers of training instances. Instead of us-
ing the entire training sets in the experiments, we
use a smaller subset due to several reasons. First,
to train a coreference resolution classifier, we do
not need so much training data (Soon et al., 2001).
Second, a large number of training instances will
slow the active learning process. Third, a smaller
source domain training corpus suggests a more
modest annotation effort even on the source do-
main. Lastly, a smaller target domain training cor-
pus means that fewer words need to be read by
human annotators to label the data.
We randomly choose 10 NPAPER texts as the
source domain training set. A coreference resolu-
tion system that is trained on these 10 texts and
tested on the entire NPAPER test set achieves re-
call, precision, and F-measure of 60.3%, 70.6%,
and 65.0%, respectively. This is comparable to
(actually slightly better than) a system trained on
the entire NPAPER training set. As for the GE-
NIA training set, we randomly choose 40 texts as
the target domain training data. To avoid selec-
tion bias, we perform 5 random trials, i.e., choos-
ing 5 sets, each containing 40 randomly selected
GENIA training texts. In the rest of this paper, all
performances of using 40 GENIA training texts are
the average scores over 5 runs, each of which uses
a different set of 40 texts.
In the previous section, we have presented the
domain adaptation techniques, the active learning
algorithm, as well as the target domain instance
weighting scheme. In the rest of this section, we
present the experimental results to show how do-
main adaptation, active learning, and target do-
main instance weighting help coreference resolu-
tion in a new domain. We use Augment, IW, and
IP to denote the three domain adaptation tech-
niques: AUGMENT, INSTANCE WEIGHTING, and
INSTANCE PRUNING, respectively. For a further
comparison, we explore another baseline method,
which is simply a concatenation of the source and
target domain data together, called Combine in the
rest of this paper. In all the experiments with ac-
tive learning, we run 100 iterations, which result
in the selection of 1,000 target domain instances.
The first experiment is to measure the effective-
ness of target domain instance weighting. We fix
on the use of uncertainty-based active learning,
and compare weighting and without weighting of
target domain instances (denoted as Weighted and
Unweighted). The learning curves are shown in
Figure 2. For Combine, Augment, and IP, it can be
seen that Weighted is a clear winner. As for IW, at
the beginning of active learning, Unweighted out-
performs Weighted, though it is unstable. At the
end of 100 iterations, Weighted outperforms Un-
weighted.
Since Weighted outperforms Unweighted, we
fix on the use of Weighted and explore the effec-
tiveness of active learning. For comparison, we try
another iterative process that randomly selects 10
instances in each iteration. We found that selection
of instances using active learning achieved better
performance than random selection in all cases.
This is because random selection may select in-
stances that the classifier has very high confidence
in, which will not help in improving the classifier.
In the third experiment, we fix on the use of
Weighted and Uncertainty since they perform the
best, and evaluate the effect of different domain
adaptation techniques. The learning curves are
shown in Figure 3. It can be seen that Augment
is the best performing system. For a closer look,
we tabulate the results in Table 3, with the statisti-
cal significance levels indicated. Statistical signif-
icance tests were conducted following (Chinchor,
2011).
</bodyText>
<page confidence="0.980447">
26
</page>
<figure confidence="0.999619">
MUC F−measure
MUC F−measure
Iteration
(a) Combine
Iteration
(b) Augment
0 20 40 60 80 100
20 25 30 35 40 45 50
Weighted + Uncertainty
Unweighted + Uncertainty
0 20 40 60 80 100
20 25 30 35 40 45 50
Weighted + Uncertainty
Unweighted + Uncertainty
MUC F−measure
20 25 30 35 40 45 50
Iteration
(c) IW
20 40 60 80 100
Iteration
(d) IP
0 20 40 60 80 100
MUC F−measure
20 25 30 35 40 45 50
Weighted + Uncertainty
Unweighted + Uncertainty
Weighted + Uncertainty
Unweighted + Uncertainty
</figure>
<figureCaption confidence="0.993974">
Figure 2: Learning curves of comparing target domain instances weighted vs. unweighted. All systems
use uncertainty-based active learning.
</figureCaption>
<table confidence="0.999624833333333">
Iteration 0 10 20 30 40 60 80 100
Combine+Unweighted 39.8 40.7 40.9 41.1 41.4 40.4 41.6 42.1
Combine+Weighted 39.8 40.9 44.0** 44.8 ** 45.2** 48.0** 47.7** 47.6 **
Augment+Weighted 39.8 44.1 **†† 46.0**†† 47.0 **†† 47.8**†† 49.1**†† 49.1**†† 49.0 **††
IW+Weighted 39.8 24.3 33.1 36.8 38.1 45.0** 48.2**†† 48.3 **††
IP+Weighted 39.8 34.4 40.7 43.4 ** 46.2**†† 48.0** 48.5**†† 48.5 **††
</table>
<tableCaption confidence="0.991421">
Table 3: MUC F-measures of different active learning settings on the GENIA test set. All systems use
</tableCaption>
<bodyText confidence="0.77295">
Uncertainty. Statistical significance is compared against Combine+Unweighted, where * and ** stand
for p &lt; 0.05 and p &lt; 0.01, respectively, and compared against Combine+Weighted, where †and ††stand
for p &lt; 0.05 and p &lt; 0.01, respectively.
</bodyText>
<sectionHeader confidence="0.973027" genericHeader="evaluation">
6 Analysis
</sectionHeader>
<bodyText confidence="0.9982320625">
Using only the source domain training data,
a coreference resolution system achieves an F-
measure of 39.8% on the GENIA test set (the col-
umn of “Iteration 0” in Table 3). From Figure 3
and Table 3, we can see that in the first few iter-
ations of active learning, domain adaptation does
not perform as well as using only the source do-
main training data. This is because when there
are very limited target domain data, the estima-
tion of the target domain is unreliable. Dahlmeier
and Ng (2010) reported similar findings though
they did not use active learning. With more iter-
ations, i.e., more target domain training data, do-
main adaptation is clearly superior. Among the
three domain adaptation techniques, Augment is
better than IW and IP. It not only achieves a higher
F-measure, but also a faster speed to adapt to a
new domain in active learning. Also, similar to
(Dahlmeier and Ng, 2010), we find that IP is gen-
erally better than IW. All systems (except IW)
with Weighted performs much better than Com-
bine+Unweighted. This shows the effectiveness
of target domain instance weighting. The aver-
age recall, precision, and F-measure of our best
model, Augment+Weighted, after 100 iterations
are 37.3%, 71.5%, and 49.0%, respectively. Com-
pared to training with only the NPAPER training
data, not only the F-measure, but also both the re-
call and precision are greatly improved (cf Table
2).
Among all the target domain instances that were
selected in Augment+Weighted, the average dis-
</bodyText>
<page confidence="0.983611">
27
</page>
<figure confidence="0.975342">
MUC F−measure
Iteration
</figure>
<figureCaption confidence="0.981553666666667">
Figure 3: Learning curves of different domain
adaptation methods. All systems use Weighted and
Uncertainty.
</figureCaption>
<bodyText confidence="0.999972459016394">
tance of the two markables in an instance (mea-
sured in sentence) is 3.4 (averaged over the 5
runs), which means an annotator needs to read 4
sentences on average to annotate an instance.
We also investigate the difference of corefer-
ence resolution between the newswire domain and
the biomedical domain, and the instances that
were selected in active learning which represent
this difference. One of the reasons that corefer-
ence resolution differs in the two domains is that
scientific writing in biomedical texts frequently
compares entities. For example,
In Cushing’s syndrome, the CR of GR
was normal in spite of the fact that the
CR of plasma cortisol was disturbed.
The two CRs refer to different entities and hence
are not coreferential. However, a system trained
on NPAPER predicts them as coreferential. In
the newswire domain, comparisons are less likely,
especially for named entities. For example, in
the newswire domain, London in most cases is
coreferential to other Londons. However, in the
biomedical domain, DNAs as in DNA of human
beings and DNA of monkeys are different enti-
ties. A coreference resolution system trained on
the newswire domain is unable to capture the dif-
ference between these two named entities, hence
predicting them as coreferential. This also jus-
tifies the need for domain adaptation for corefer-
ence resolution. For the above sentence, after ap-
plying our method, the adapted coreference res-
olution system is able to predict the two CRs as
non-coreferential.
Next, we show the effectiveness of our sys-
tem using domain adaptation with active learning
compared to a system trained with full corefer-
ence annotations. Averaged over 5 runs, a system
trained on a single GENIA training text achieves
an F-measure of 25.9%, which is significantly
lower than that achieved by our method. With
more GENIA training texts added, the F-measure
increases. After 80 texts are used, the system
trained on full annotations finally achieves an F-
measure of 49.2%, which is 0.2% higher than Aug-
ment+Weighted after 100 iterations. However, af-
ter 100 iterations, only 1,000 target domain in-
stances are annotated under our framework. Con-
sidering that one single text in the GENIA corpus
contains an average of over 2,000 instances (cf Ta-
ble 1), effectively we annotate only half of a text.
Compared to the 80 training texts needed, this is a
huge reduction. In order to achieve similar perfor-
mance, we only need to annotate 1/160 or 0.63%
of the complete set of training instances under our
framework of domain adaptation with active learn-
ing.
Lastly, although in this paper we reported exper-
imental results with the MUC evaluation metric,
we also evaluated our approach with other evalu-
ation metrics for coreference resolution, e.g., the
B-CUBED metric, and obtained similar findings.
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999952384615385">
In this paper, we presented an approach using
domain adaptation with active learning to adapt
coreference resolution from the newswire domain
to the biomedical domain. We explored the ef-
fect of domain adaptation, active learning, and
target domain instance weighting for coreference
resolution. Experimental results showed that do-
main adaptation with active learning and the tar-
get instance weighting scheme achieved a simi-
lar performance on MEDLINE abstracts but with
a greatly reduced number of annotated training
instances, compared to a system trained on full
coreference annotations.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9964925">
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
</bodyText>
<sectionHeader confidence="0.995334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.7746135">
Jos´e Casta˜no, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature.
In Proceedings of the International Symposium on
Reference Resolution.
</reference>
<figure confidence="0.999368666666667">
0 20 40 60 80 100
30 35 40 45 50
Combine
Augment
IW
IP
</figure>
<page confidence="0.992921">
28
</page>
<reference confidence="0.999651365384616">
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of the ACL2007.
Nancy Chinchor. 2011. Statistical significance of
MUC-6 results. In Proceedings of the Sixth Mes-
sage Understanding Conference.
K. Bretonnel Cohen, Arrick Lanfranchi, William
Corvey, William A. Baumgartner Jr., Christophe
Roeder, Philip V. Ogren, Martha Palmer, and
Lawrence Hunter. 2010. Annotation of all coref-
erence in biomedical text: Guideline selection and
adaptation. In BioTxtM 2010.
Daniel Dahlmeier and Hwee Tou Ng. 2010. Domain
adaptation for semantic role labeling in the biomed-
ical domain. Bioinformatics, 26(8):1098–1104.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the ACL2007.
Caroline Gasperin and Ted Briscoe. 2008. Statistical
anaphora resolution in biomedical texts. In Proceed-
ings of the COLIIG2008.
Caroline Gasperin, Nikiforos Karamanis, and Ruth
Seal. 2007. Annotation of anaphoric relations in
biomedical full-text articles using a domain-relevant
scheme. In Proceedings of the DAARC2007.
Caroline Gasperin. 2009. Active learning for anaphora
resolution. In Proceedings of the IAACL-HLT2009
Workshop on Active Learning for Iatural Language
Processing.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In Pro-
ceedings of the ACL2007.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun’ichi Tsujii. 2011a.
Overview of BioNLP shared task 2011. In Proceed-
ings of BioILP Shared Task 2011 Workshop.
Youngjun Kim, Ellen Riloff, and Nathan Gilbert.
2011b. The taming of Reconcile as a biomedical
coreference resolver. In Proceedings of BioILP
Shared Task 2011 Workshop.
Florian Laws, Florian Heimerl, and Hinrich Sch¨utze.
2012. Active learning for coreference resolution. In
Proceedings of the IAACL2012.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In
Proceedings of the SIGIR1994.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Timothy A. Miller, Dmitriy Dligach, and Guergana K.
Savova. 2012. Active learning for coreference reso-
lution. In Proceedings of the BioILP2012.
Makoto Miwa, Paul Thompson, and Sophia Ana-
niadou. 2012. Boosting automatic event ex-
traction from the literature using domain adapta-
tion and coreference resolution. Bioinformatics,
28(13):1759–1765.
MUC-6. 1995. Coreference task definition (v2.3, 8
Sep 95). In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
MUC-7. 1998. Coreference task definition (v3.0, 13
Jul 97). In Proceedings of the Seventh Message Un-
derstanding Conference (MUC-7).
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the ACL2002.
NIST. 2002. The ACE 2002 evaluation
plan. ftp://jaguar.ncsl.nist.gov/ace/doc/
ACE-EvalPlan-2002-v06.pdf.
Piyush Rai, Avishek Saha, Hal Daume, and Suresh
Venkatasubramanian. 2010. Domain adapta-
tion meets active learning. In Proceedings of the
IAACL-HLT2010 Workshop on Active Learning for
Iatural Language Processing.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the ACL-IJCILP2009.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proceedings of the ACL2002.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the MUC-6.
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2004. Improving noun phrase
coreference resolution by matching strings. In Pro-
ceedings of the IJCILP2004.
Jian Bo Yang, Qi Mao, Qiao Liang Xiang, Ivor W.
Tsang, Kian Ming A. Chai, and Hai Leong Chieu.
2012. Domain adaptation for coreference resolu-
tion: An adaptive ensemble approach. In Proceed-
ings of the EMILP2012.
Shanheng Zhao and Hwee Tou Ng. 2010. Maximum
metric score training for coreference resolution. In
Proceedings of the COLIIG2010.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An
empirical study. In Proceedings of the EMILP2008.
</reference>
<page confidence="0.999115">
29
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.442503">
<title confidence="0.997802">Domain Adaptation with Active Learning for Coreference Resolution</title>
<author confidence="0.49553">Shanheng</author>
<address confidence="0.962037">441 Logue Mountain View, CA 94043,</address>
<email confidence="0.999498">szhao@elance.com</email>
<author confidence="0.992749">Hwee Tou</author>
<affiliation confidence="0.9999215">Department of Computer National University of</affiliation>
<address confidence="0.994134">13 Computing Drive, Singapore</address>
<email confidence="0.993776">nght@comp.nus.edu.sg</email>
<abstract confidence="0.999623583333333">In the literature, most prior work on coreference resolution centered on the newswire domain. Although a coreference resolution system trained on the newswire domain performs well on newswire texts, there is a huge performance drop when it is applied to the biomedical domain. In this paper, we present an approach integrating domain adaptation with active learning to adapt coreference resolution from the newswire domain to the biomedical domain. We explore the effect of domain adaptation, active learning, and target domain instance weighting for coreference resolution. Experimental results show that domain adaptation with active learning and target domain instance weighting achieves performance on MEDLINE abstracts similar to a system trained on coreference annotation of only target domain training instances, but with a greatly reduced number of target domain training instances that we need to annotate.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jos´e Casta˜no</author>
<author>Jason Zhang</author>
<author>James Pustejovsky</author>
</authors>
<title>Anaphora resolution in biomedical literature.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Symposium on Reference Resolution.</booktitle>
<marker>Casta˜no, Zhang, Pustejovsky, 2002</marker>
<rawString>Jos´e Casta˜no, Jason Zhang, and James Pustejovsky. 2002. Anaphora resolution in biomedical literature. In Proceedings of the International Symposium on Reference Resolution.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation with active learning for word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL2007.</booktitle>
<contexts>
<context position="8549" citStr="Chan and Ng, 2007" startWordPosition="1373" endWordPosition="1376"> adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation techniques whereas we apply and compare multiple state-of-the-art domain adaptation techniques. Rai et al. (2010) exploited a 22 streaming active learning setting whereas ours is pool-based. Dahlmeier and Ng (2010) evaluated the performance of three previously proposed domain adaptation algorithms for semantic role labeling. They evaluated the performance of domain adaptation with different sizes of target domain training data. In each of </context>
</contexts>
<marker>Chan, Ng, 2007</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2007. Domain adaptation with active learning for word sense disambiguation. In Proceedings of the ACL2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>Statistical significance of MUC-6 results.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference.</booktitle>
<contexts>
<context position="25987" citStr="Chinchor, 2011" startWordPosition="4216" endWordPosition="4217"> in all cases. This is because random selection may select instances that the classifier has very high confidence in, which will not help in improving the classifier. In the third experiment, we fix on the use of Weighted and Uncertainty since they perform the best, and evaluate the effect of different domain adaptation techniques. The learning curves are shown in Figure 3. It can be seen that Augment is the best performing system. For a closer look, we tabulate the results in Table 3, with the statistical significance levels indicated. Statistical significance tests were conducted following (Chinchor, 2011). 26 MUC F−measure MUC F−measure Iteration (a) Combine Iteration (b) Augment 0 20 40 60 80 100 20 25 30 35 40 45 50 Weighted + Uncertainty Unweighted + Uncertainty 0 20 40 60 80 100 20 25 30 35 40 45 50 Weighted + Uncertainty Unweighted + Uncertainty MUC F−measure 20 25 30 35 40 45 50 Iteration (c) IW 20 40 60 80 100 Iteration (d) IP 0 20 40 60 80 100 MUC F−measure 20 25 30 35 40 45 50 Weighted + Uncertainty Unweighted + Uncertainty Weighted + Uncertainty Unweighted + Uncertainty Figure 2: Learning curves of comparing target domain instances weighted vs. unweighted. All systems use uncertainty</context>
</contexts>
<marker>Chinchor, 2011</marker>
<rawString>Nancy Chinchor. 2011. Statistical significance of MUC-6 results. In Proceedings of the Sixth Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bretonnel Cohen</author>
<author>Arrick Lanfranchi</author>
<author>William Corvey</author>
<author>William A Baumgartner Jr</author>
<author>Christophe Roeder</author>
<author>Philip V Ogren</author>
<author>Martha Palmer</author>
<author>Lawrence Hunter</author>
</authors>
<title>Annotation of all coreference in biomedical text: Guideline selection and adaptation.</title>
<date>2010</date>
<journal>In BioTxtM</journal>
<contexts>
<context position="3793" citStr="Cohen et al. (2010)" startWordPosition="600" endWordPosition="603"> annotator only needs to focus on each markable (a word, in the case of POS tagging) and a small window of its neighboring words. In contrast, to annotate a coreferential relation, an annotator needs to first recognize whether a certain text span is a markable, and then scan through the text preceding the markable (a potential anaphor) to look for the antecedent. It also requires the annotator to understand the text in order to annotate coreferential relations, which are semantic in nature. If a markable is non-anaphoric, the annotator has to scan to the beginning of the text to realize that. Cohen et al. (2010) reported that it took an average of 20 hours to annotate coreferential relations in a single 21 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 21–29, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics document with an average length of 6,155 words, while an annotator could annotate 3,000 words per hour in POS tag annotation (Marcus et al., 1993). The simplest approach to avoid the timeconsuming data annotation in a new domain is to train a coreference resolution system on a resource-ri</context>
<context position="7365" citStr="Cohen et al., 2010" startWordPosition="1174" endWordPosition="1177">et al. (2002) were among the first to annotate coreferential relations in the biomedical domain. Their annotation only concerned the pronominal and nominal anaphoric expressions in 46 biomedical abstracts. Gasperin and Briscoe (2007) annotated coreferential relations on 5 full articles in the biomedical domain, but only on noun phrases referring to bio-entities. Yang et al. (2004) annotated full NP coreferential relations on biomedical abstracts of the GENIA corpus. The ongoing project of the CRAFT corpus is expected to annotate all coreferential relations on full text of biomedical articles (Cohen et al., 2010). Unlike the work of (Casta˜no et al., 2002), (Gasperin and Briscoe, 2008), and (Gasperin, 2009) that resolved coreferential relations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and </context>
</contexts>
<marker>Cohen, Lanfranchi, Corvey, Jr, Roeder, Ogren, Palmer, Hunter, 2010</marker>
<rawString>K. Bretonnel Cohen, Arrick Lanfranchi, William Corvey, William A. Baumgartner Jr., Christophe Roeder, Philip V. Ogren, Martha Palmer, and Lawrence Hunter. 2010. Annotation of all coreference in biomedical text: Guideline selection and adaptation. In BioTxtM 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Domain adaptation for semantic role labeling in the biomedical domain.</title>
<date>2010</date>
<journal>Bioinformatics,</journal>
<volume>26</volume>
<issue>8</issue>
<contexts>
<context position="8090" citStr="Dahlmeier and Ng, 2010" startWordPosition="1291" endWordPosition="1294">esolved coreferential relations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their do</context>
<context position="9405" citStr="Dahlmeier and Ng, 2010" startWordPosition="1507" endWordPosition="1510">ptation techniques. Rai et al. (2010) exploited a 22 streaming active learning setting whereas ours is pool-based. Dahlmeier and Ng (2010) evaluated the performance of three previously proposed domain adaptation algorithms for semantic role labeling. They evaluated the performance of domain adaptation with different sizes of target domain training data. In each of their experiments with a certain target domain training data size, the target domain training data were added all at once. In contrast, we add the target domain training instances selectively in an iterative process. Different from (Dahlmeier and Ng, 2010), we weight the target domain instances to further boost the performance of domain adaptation. Our work is the first systematic study of domain adaptation with active learning for coreference resolution. Although Gasperin (2009) tried to apply active learning for anaphora resolution, her results were negative: using active learning was not better than randomly selecting instances in her work. Miwa et al. (2012) incorporated a rule-based coreference resolution system for automatic biomedical event extraction, and showed that by adding training data from other domains as supplementary training d</context>
<context position="27836" citStr="Dahlmeier and Ng (2010)" startWordPosition="4536" endWordPosition="4539">p &lt; 0.01, respectively, and compared against Combine+Weighted, where †and ††stand for p &lt; 0.05 and p &lt; 0.01, respectively. 6 Analysis Using only the source domain training data, a coreference resolution system achieves an Fmeasure of 39.8% on the GENIA test set (the column of “Iteration 0” in Table 3). From Figure 3 and Table 3, we can see that in the first few iterations of active learning, domain adaptation does not perform as well as using only the source domain training data. This is because when there are very limited target domain data, the estimation of the target domain is unreliable. Dahlmeier and Ng (2010) reported similar findings though they did not use active learning. With more iterations, i.e., more target domain training data, domain adaptation is clearly superior. Among the three domain adaptation techniques, Augment is better than IW and IP. It not only achieves a higher F-measure, but also a faster speed to adapt to a new domain in active learning. Also, similar to (Dahlmeier and Ng, 2010), we find that IP is generally better than IW. All systems (except IW) with Weighted performs much better than Combine+Unweighted. This shows the effectiveness of target domain instance weighting. The</context>
</contexts>
<marker>Dahlmeier, Ng, 2010</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2010. Domain adaptation for semantic role labeling in the biomedical domain. Bioinformatics, 26(8):1098–1104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daume</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL2007.</booktitle>
<marker>Daume, 2007</marker>
<rawString>Hal Daume III. 2007. Frustratingly easy domain adaptation. In Proceedings of the ACL2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Gasperin</author>
<author>Ted Briscoe</author>
</authors>
<title>Statistical anaphora resolution in biomedical texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the COLIIG2008.</booktitle>
<contexts>
<context position="7439" citStr="Gasperin and Briscoe, 2008" startWordPosition="1186" endWordPosition="1189">ions in the biomedical domain. Their annotation only concerned the pronominal and nominal anaphoric expressions in 46 biomedical abstracts. Gasperin and Briscoe (2007) annotated coreferential relations on 5 full articles in the biomedical domain, but only on noun phrases referring to bio-entities. Yang et al. (2004) annotated full NP coreferential relations on biomedical abstracts of the GENIA corpus. The ongoing project of the CRAFT corpus is expected to annotate all coreferential relations on full text of biomedical articles (Cohen et al., 2010). Unlike the work of (Casta˜no et al., 2002), (Gasperin and Briscoe, 2008), and (Gasperin, 2009) that resolved coreferential relations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and </context>
</contexts>
<marker>Gasperin, Briscoe, 2008</marker>
<rawString>Caroline Gasperin and Ted Briscoe. 2008. Statistical anaphora resolution in biomedical texts. In Proceedings of the COLIIG2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Gasperin</author>
<author>Nikiforos Karamanis</author>
<author>Ruth Seal</author>
</authors>
<title>Annotation of anaphoric relations in biomedical full-text articles using a domain-relevant scheme.</title>
<date>2007</date>
<booktitle>In Proceedings of the DAARC2007.</booktitle>
<marker>Gasperin, Karamanis, Seal, 2007</marker>
<rawString>Caroline Gasperin, Nikiforos Karamanis, and Ruth Seal. 2007. Annotation of anaphoric relations in biomedical full-text articles using a domain-relevant scheme. In Proceedings of the DAARC2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Gasperin</author>
</authors>
<title>Active learning for anaphora resolution.</title>
<date>2009</date>
<booktitle>In Proceedings of the IAACL-HLT2009 Workshop on Active Learning for Iatural Language Processing.</booktitle>
<contexts>
<context position="7461" citStr="Gasperin, 2009" startWordPosition="1191" endWordPosition="1192">ir annotation only concerned the pronominal and nominal anaphoric expressions in 46 biomedical abstracts. Gasperin and Briscoe (2007) annotated coreferential relations on 5 full articles in the biomedical domain, but only on noun phrases referring to bio-entities. Yang et al. (2004) annotated full NP coreferential relations on biomedical abstracts of the GENIA corpus. The ongoing project of the CRAFT corpus is expected to annotate all coreferential relations on full text of biomedical articles (Cohen et al., 2010). Unlike the work of (Casta˜no et al., 2002), (Gasperin and Briscoe, 2008), and (Gasperin, 2009) that resolved coreferential relations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III,</context>
<context position="9633" citStr="Gasperin (2009)" startWordPosition="1546" endWordPosition="1547">role labeling. They evaluated the performance of domain adaptation with different sizes of target domain training data. In each of their experiments with a certain target domain training data size, the target domain training data were added all at once. In contrast, we add the target domain training instances selectively in an iterative process. Different from (Dahlmeier and Ng, 2010), we weight the target domain instances to further boost the performance of domain adaptation. Our work is the first systematic study of domain adaptation with active learning for coreference resolution. Although Gasperin (2009) tried to apply active learning for anaphora resolution, her results were negative: using active learning was not better than randomly selecting instances in her work. Miwa et al. (2012) incorporated a rule-based coreference resolution system for automatic biomedical event extraction, and showed that by adding training data from other domains as supplementary training data and using domain adaptation, one can achieve a higher F-measure in event extraction. 3 Coreference Resolution The gold standard annotation and the output by a coreference resolution system are called the key and the response</context>
</contexts>
<marker>Gasperin, 2009</marker>
<rawString>Caroline Gasperin. 2009. Active learning for anaphora resolution. In Proceedings of the IAACL-HLT2009 Workshop on Active Learning for Iatural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Jiang</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Instance weighting for domain adaptation in NLP.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL2007.</booktitle>
<contexts>
<context position="8049" citStr="Jiang and Zhai, 2007" startWordPosition="1284" endWordPosition="1287">coe, 2008), and (Gasperin, 2009) that resolved coreferential relations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count mergi</context>
<context position="11882" citStr="Jiang and Zhai, 2007" startWordPosition="1917" endWordPosition="1920">the other hand, is defined in the opposite way by switching the role of key and response. F-measure is a trade-off between recall and precision: F = 2·Recall·Precision Recall+Precision . 4 Domain Adaptation with Active Learning 4.1 Domain Adaptation Domain adaptation is applicable when one has a large amount of annotated training data in the source domain and a small amount or none of the annotated training data in the target domain. We evaluate the AUGMENT technique introduced by (Daume III, 2007), as well as the INSTANCE WEIGHTING (IW) and the INSTANCE PRUNING (IP) techniques introduced by (Jiang and Zhai, 2007). 4.1.1 AUGMENT Daume III (2007) introduced a simple domain adaptation technique by feature space augmentation. It maps the feature space of each instance into a feature space of higher dimension. Suppose x is the feature vector of an instance. Define Φs and Φt to be the mappings of an instance from the original feature space to an augmented feature space in the source and the target domain, respectively: Φs(x) = (x, x, 0) (1) Φt(x) = (x, 0, x) (2) where 0 = (0, 0, ... , 0) is a zero vector of length |x|. The mapping can be treated as taking each feature in the original feature space and makin</context>
<context position="18170" citStr="Jiang and Zhai, 2007" startWordPosition="2956" endWordPosition="2959">it. Following Reconcile, the individual preprocessing steps include: 1) sentence segmentation (using the OpenNLP toolkit2); 2) tokenization (using the OpenNLP toolkit); 3) POS tagging (using the OpenNLP toolkit); 4) syntactic parsing (using the Berkeley Parser3); and 5) named entity recognition (using the Stanford NER4). Markables are extracted as defined in each individual corpus. All possible markable pairs in the training and test set are extracted to form training and test instances, respectively. The learning algorithm we use is maximum entropy modeling, implemented in the DALR package5 (Jiang and Zhai, 2007). The coreference resolution system employs a comprehensive set of 62 features to represent each training and test instance, including lexical, proximity, grammatical, and semantic features (Stoyanov et al., 2009). We do not introduce additional features motivated from the biomedical domain, but use the same feature set for both the source and target domains. 5.3 Preprocessing For the ACE corpora, all preprocessing components use the original models (provided by the OpenNLP toolkit, the Berkeley Parser, and the Stanford NER). For the GENIA corpus, since it is from a very different domain, the </context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jing Jiang and ChengXiang Zhai. 2007. Instance weighting for domain adaptation in NLP. In Proceedings of the ACL2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
</authors>
<title>Sampo Pyysalo, Tomoko Ohta, Robert Bossy, Ngan Nguyen, and Jun’ichi Tsujii. 2011a. Overview of BioNLP shared task</title>
<date>2011</date>
<booktitle>In Proceedings of BioILP Shared Task 2011 Workshop.</booktitle>
<marker>Kim, 2011</marker>
<rawString>Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, Ngan Nguyen, and Jun’ichi Tsujii. 2011a. Overview of BioNLP shared task 2011. In Proceedings of BioILP Shared Task 2011 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Youngjun Kim</author>
<author>Ellen Riloff</author>
<author>Nathan Gilbert</author>
</authors>
<title>The taming of Reconcile as a biomedical coreference resolver.</title>
<date>2011</date>
<booktitle>In Proceedings of BioILP Shared Task 2011 Workshop.</booktitle>
<contexts>
<context position="2360" citStr="Kim et al., 2011" startWordPosition="356" endWordPosition="359">Soon et al., 2001; Ng and Cardie, 2002; Stoyanov et al., 2009; Zhao and Ng, 2010). In recent years, with the advances in biological and life science research, there is a rapidly increasing number of biomedical texts, including research papers, patent documents, etc. This results in an increasing demand for applying natural language processing and information retrieval techniques to efficiently exploit information contained in these large amounts of texts. However, coreference resolution, one of the core tasks in NLP, has only a relatively small body of prior research in the biomedical domain (Kim et al., 2011a; Kim et al., 2011b). A large body of prior research on coreference resolution focuses on texts in the newswire domain. Standardized data sets, such as MUC (DARPA Message Understanding Conference, (MUC-6, 1995; MUC-7, 1998)) and ACE (NIST Automatic Content Extraction Entity Detection and Tracking task, (NIST, 2002)) data sets are widely used in the study of coreference resolution. Traditionally, in order to apply supervised machine learning approaches to an NLP task in a specific domain, one needs to collect a text corpus in the domain and annotate it to serve as training data. Compared to ot</context>
</contexts>
<marker>Kim, Riloff, Gilbert, 2011</marker>
<rawString>Youngjun Kim, Ellen Riloff, and Nathan Gilbert. 2011b. The taming of Reconcile as a biomedical coreference resolver. In Proceedings of BioILP Shared Task 2011 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Florian Laws</author>
<author>Florian Heimerl</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Active learning for coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the IAACL2012.</booktitle>
<marker>Laws, Heimerl, Sch¨utze, 2012</marker>
<rawString>Florian Laws, Florian Heimerl, and Hinrich Sch¨utze. 2012. Active learning for coreference resolution. In Proceedings of the IAACL2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Lewis</author>
<author>William A Gale</author>
</authors>
<title>A sequential algorithm for training text classifiers.</title>
<date>1994</date>
<booktitle>In Proceedings of the SIGIR1994.</booktitle>
<contexts>
<context position="5240" citStr="Lewis and Gale, 1994" startWordPosition="835" endWordPosition="838">drop when they are tested on a different domain. This motivates the usage of domain adaptation techniques for coreference resolution: adapting a coreference resolution system from one source domain in which we have a large collection of annotated data, to a second target domain in which we need good performance. It is almost inevitable that we annotate some data in the target domain to achieve good coreference resolution performance. The question is how to minimize the amount of annotation needed. In the literature, active learning has been exploited to reduce the amount of annotation needed (Lewis and Gale, 1994). In contrast to annotating the entire data set, active learning selects only a subset of the data to annotate in an iterative process. How to apply active learning and integrate it with domain adaptation remains an open problem for coreference resolution. In this paper, we explore domain adaptation for coreference resolution from the resource-rich newswire domain to the biomedical domain. Our approach comprises domain adaptation, active learning, and target domain instance weighting to leverage the existing annotated corpora from the newswire domain, so as to reduce the cost of developing a c</context>
<context position="15082" citStr="Lewis and Gale, 1994" startWordPosition="2443" endWordPosition="2446">ng instances. Target domain instance weighting can effectively reduce the imbalance. Unlike INSTANCE WEIGHTING and INSTANCE PRUNING in which each source domain instance is weighted individually, we give all target domain instances the same weight. This target domain instance weighting scheme is not only complementary to INSTANCE WEIGHTING and INSTANCE PRUNING, but is also applicable to AUGMENT. 4.2 Active Learning Active learning iteratively selects the most informative instances to label, adds them to the training data pool, and trains a new classifier with the enlarged data pool. We follow (Lewis and Gale, 1994) and use the uncertainty sampling strategy in our active learning setting. Ds the set of source domain training instances Dt the set of target domain training instances Da ; P coreference resolution system trained on Ds T number of iterations for i from 1 to T do for each di E Dt do di prediction of di using P _ pi prediction confidence of di end for D′a top N instances with the lowest pi Da Da + D′a Dt Dt − D′a provide correct labels to the unlabeled instances in D′a P coreference resolution system trained on Ds and Da using the chosen domain adaptation technique end for Figure 1: An algorith</context>
</contexts>
<marker>Lewis, Gale, 1994</marker>
<rawString>David D. Lewis and William A. Gale. 1994. A sequential algorithm for training text classifiers. In Proceedings of the SIGIR1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="4250" citStr="Marcus et al., 1993" startWordPosition="671" endWordPosition="674">elations, which are semantic in nature. If a markable is non-anaphoric, the annotator has to scan to the beginning of the text to realize that. Cohen et al. (2010) reported that it took an average of 20 hours to annotate coreferential relations in a single 21 Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 21–29, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics document with an average length of 6,155 words, while an annotator could annotate 3,000 words per hour in POS tag annotation (Marcus et al., 1993). The simplest approach to avoid the timeconsuming data annotation in a new domain is to train a coreference resolution system on a resource-rich domain and apply it to a different target domain without any additional data annotation. Although coreference resolution systems work well on test texts in the same domain as the training texts, there is a huge performance drop when they are tested on a different domain. This motivates the usage of domain adaptation techniques for coreference resolution: adapting a coreference resolution system from one source domain in which we have a large collecti</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy A Miller</author>
<author>Dmitriy Dligach</author>
<author>Guergana K Savova</author>
</authors>
<title>Active learning for coreference resolution.</title>
<date>2012</date>
<booktitle>In Proceedings of the BioILP2012.</booktitle>
<contexts>
<context position="8297" citStr="Miller et al., 2012" startWordPosition="1331" endWordPosition="1334">004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation techniques whereas we apply and compare multiple state-of-the-art domain adaptation techniques. Rai et al. (2010) exploited a 22 streaming active learning setting whereas ours is pool-based. </context>
</contexts>
<marker>Miller, Dligach, Savova, 2012</marker>
<rawString>Timothy A. Miller, Dmitriy Dligach, and Guergana K. Savova. 2012. Active learning for coreference resolution. In Proceedings of the BioILP2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Miwa</author>
<author>Paul Thompson</author>
<author>Sophia Ananiadou</author>
</authors>
<title>Boosting automatic event extraction from the literature using domain adaptation and coreference resolution.</title>
<date>2012</date>
<journal>Bioinformatics,</journal>
<volume>28</volume>
<issue>13</issue>
<contexts>
<context position="9819" citStr="Miwa et al. (2012)" startWordPosition="1575" endWordPosition="1578">ing data size, the target domain training data were added all at once. In contrast, we add the target domain training instances selectively in an iterative process. Different from (Dahlmeier and Ng, 2010), we weight the target domain instances to further boost the performance of domain adaptation. Our work is the first systematic study of domain adaptation with active learning for coreference resolution. Although Gasperin (2009) tried to apply active learning for anaphora resolution, her results were negative: using active learning was not better than randomly selecting instances in her work. Miwa et al. (2012) incorporated a rule-based coreference resolution system for automatic biomedical event extraction, and showed that by adding training data from other domains as supplementary training data and using domain adaptation, one can achieve a higher F-measure in event extraction. 3 Coreference Resolution The gold standard annotation and the output by a coreference resolution system are called the key and the response, respectively. In both the key and the response, a coreference chain is formed by a set of coreferential markables. A markable is a noun phrase which satisfies the markable definition i</context>
</contexts>
<marker>Miwa, Thompson, Ananiadou, 2012</marker>
<rawString>Makoto Miwa, Paul Thompson, and Sophia Ananiadou. 2012. Boosting automatic event extraction from the literature using domain adaptation and coreference resolution. Bioinformatics, 28(13):1759–1765.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-6</author>
</authors>
<title>Coreference task definition (v2.3, 8 Sep 95).</title>
<date>1995</date>
<booktitle>In Proceedings of the Sixth Message Understanding Conference (MUC-6).</booktitle>
<marker>MUC-6, 1995</marker>
<rawString>MUC-6. 1995. Coreference task definition (v2.3, 8 Sep 95). In Proceedings of the Sixth Message Understanding Conference (MUC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-7</author>
</authors>
<title>Coreference task definition (v3.0, 13 Jul 97).</title>
<date>1998</date>
<booktitle>In Proceedings of the Seventh Message Understanding Conference (MUC-7).</booktitle>
<marker>MUC-7, 1998</marker>
<rawString>MUC-7. 1998. Coreference task definition (v3.0, 13 Jul 97). In Proceedings of the Seventh Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL2002.</booktitle>
<contexts>
<context position="1782" citStr="Ng and Cardie, 2002" startWordPosition="262" endWordPosition="265">aining instances that we need to annotate. 1 Introduction Coreference resolution is the task of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing (NLP) tasks, such as information extraction and question answering. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002; Stoyanov et al., 2009; Zhao and Ng, 2010). In recent years, with the advances in biological and life science research, there is a rapidly increasing number of biomedical texts, including research papers, patent documents, etc. This results in an increasing demand for applying natural language processing and information retrieval techniques to efficiently exploit information contained in these large amounts of texts. However, coreference resolution, one of the core tasks in NLP, has only a relatively small body of prior research in the biomedical domain (Kim et al., 2011a; Kim et al., 2011b).</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the ACL2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NIST</author>
</authors>
<title>The ACE</title>
<date>2002</date>
<note>evaluation plan. ftp://jaguar.ncsl.nist.gov/ace/doc/ ACE-EvalPlan-2002-v06.pdf.</note>
<contexts>
<context position="2677" citStr="NIST, 2002" startWordPosition="406" endWordPosition="407">al language processing and information retrieval techniques to efficiently exploit information contained in these large amounts of texts. However, coreference resolution, one of the core tasks in NLP, has only a relatively small body of prior research in the biomedical domain (Kim et al., 2011a; Kim et al., 2011b). A large body of prior research on coreference resolution focuses on texts in the newswire domain. Standardized data sets, such as MUC (DARPA Message Understanding Conference, (MUC-6, 1995; MUC-7, 1998)) and ACE (NIST Automatic Content Extraction Entity Detection and Tracking task, (NIST, 2002)) data sets are widely used in the study of coreference resolution. Traditionally, in order to apply supervised machine learning approaches to an NLP task in a specific domain, one needs to collect a text corpus in the domain and annotate it to serve as training data. Compared to other NLP tasks, e.g., part-ofspeech (POS) tagging or named entity (NE) tagging, the annotation for coreference resolution is much more challenging and time-consuming. The reason is that in tasks like POS tagging, an annotator only needs to focus on each markable (a word, in the case of POS tagging) and a small window</context>
<context position="17017" citStr="NIST, 2002" startWordPosition="2782" endWordPosition="2783">correct labels, the labeling process shows the text on the screen, highlights the two NPs, and asks the annotator to decide if they are coreferential. In our experiments, this is simulated by providing the gold standard coreferential information on this NP pair to the active learning process. 5 Experiments 5.1 The Corpora We explore domain adaptation from the newswire domain to the biomedical domain. The newswire and biomedical domain data that we use are the ACE Phase-2 corpora and the GENIA-MEDCo corpus, respectively. The ACE corpora contain 422 and 92 training and test texts, respectively (NIST, 2002). The texts come from 24 three newswire sources: BNEWS, NPAPER, and NWIRE. The GENIA-MEDCo corpus contains 1,999 MEDLINE abstracts1. We randomly split the GENIA corpus into a training set and a test set, containing 1,600 and 399 texts, respectively. 5.2 The Coreference Resolution System In this study, we use Reconcile, a state-of-theart coreference resolution system implemented by (Stoyanov et al., 2009). The input to the coreference resolution system is raw text, and we apply a sequence of preprocessing components to process it. Following Reconcile, the individual preprocessing steps include:</context>
</contexts>
<marker>NIST, 2002</marker>
<rawString>NIST. 2002. The ACE 2002 evaluation plan. ftp://jaguar.ncsl.nist.gov/ace/doc/ ACE-EvalPlan-2002-v06.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piyush Rai</author>
<author>Avishek Saha</author>
<author>Hal Daume</author>
<author>Suresh Venkatasubramanian</author>
</authors>
<title>Domain adaptation meets active learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the IAACL-HLT2010 Workshop on Active Learning for Iatural Language Processing.</booktitle>
<contexts>
<context position="8588" citStr="Rai et al., 2010" startWordPosition="1381" endWordPosition="1384">sfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation techniques whereas we apply and compare multiple state-of-the-art domain adaptation techniques. Rai et al. (2010) exploited a 22 streaming active learning setting whereas ours is pool-based. Dahlmeier and Ng (2010) evaluated the performance of three previously proposed domain adaptation algorithms for semantic role labeling. They evaluated the performance of domain adaptation with different sizes of target domain training data. In each of their experiments with a certain target</context>
</contexts>
<marker>Rai, Saha, Daume, Venkatasubramanian, 2010</marker>
<rawString>Piyush Rai, Avishek Saha, Hal Daume, and Suresh Venkatasubramanian. 2010. Domain adaptation meets active learning. In Proceedings of the IAACL-HLT2010 Workshop on Active Learning for Iatural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="1761" citStr="Soon et al., 2001" startWordPosition="258" endWordPosition="261">of target domain training instances that we need to annotate. 1 Introduction Coreference resolution is the task of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing (NLP) tasks, such as information extraction and question answering. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002; Stoyanov et al., 2009; Zhao and Ng, 2010). In recent years, with the advances in biological and life science research, there is a rapidly increasing number of biomedical texts, including research papers, patent documents, etc. This results in an increasing demand for applying natural language processing and information retrieval techniques to efficiently exploit information contained in these large amounts of texts. However, coreference resolution, one of the core tasks in NLP, has only a relatively small body of prior research in the biomedical domain (Kim et al., 2011a</context>
<context position="22598" citStr="Soon et al., 2001" startWordPosition="3658" endWordPosition="3661">ces in the data pool to annotate and add them to the training data to update the classifier. The target domain test set is strictly separated from this data pool, i.e., none of the target domain test data are used in the instance selection process of active learning. From Table 1, one can see that both training sets in the NPAPER and the GENIA corpora contain large numbers of training instances. Instead of using the entire training sets in the experiments, we use a smaller subset due to several reasons. First, to train a coreference resolution classifier, we do not need so much training data (Soon et al., 2001). Second, a large number of training instances will slow the active learning process. Third, a smaller source domain training corpus suggests a more modest annotation effort even on the source domain. Lastly, a smaller target domain training corpus means that fewer words need to be read by human annotators to label the data. We randomly choose 10 NPAPER texts as the source domain training set. A coreference resolution system that is trained on these 10 texts and tested on the entire NPAPER test set achieves recall, precision, and F-measure of 60.3%, 70.6%, and 65.0%, respectively. This is comp</context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Nathan Gilbert</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
</authors>
<title>Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCILP2009.</booktitle>
<contexts>
<context position="1805" citStr="Stoyanov et al., 2009" startWordPosition="266" endWordPosition="269"> we need to annotate. 1 Introduction Coreference resolution is the task of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing (NLP) tasks, such as information extraction and question answering. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002; Stoyanov et al., 2009; Zhao and Ng, 2010). In recent years, with the advances in biological and life science research, there is a rapidly increasing number of biomedical texts, including research papers, patent documents, etc. This results in an increasing demand for applying natural language processing and information retrieval techniques to efficiently exploit information contained in these large amounts of texts. However, coreference resolution, one of the core tasks in NLP, has only a relatively small body of prior research in the biomedical domain (Kim et al., 2011a; Kim et al., 2011b). A large body of prior </context>
<context position="17424" citStr="Stoyanov et al., 2009" startWordPosition="2842" endWordPosition="2845">domain. The newswire and biomedical domain data that we use are the ACE Phase-2 corpora and the GENIA-MEDCo corpus, respectively. The ACE corpora contain 422 and 92 training and test texts, respectively (NIST, 2002). The texts come from 24 three newswire sources: BNEWS, NPAPER, and NWIRE. The GENIA-MEDCo corpus contains 1,999 MEDLINE abstracts1. We randomly split the GENIA corpus into a training set and a test set, containing 1,600 and 399 texts, respectively. 5.2 The Coreference Resolution System In this study, we use Reconcile, a state-of-theart coreference resolution system implemented by (Stoyanov et al., 2009). The input to the coreference resolution system is raw text, and we apply a sequence of preprocessing components to process it. Following Reconcile, the individual preprocessing steps include: 1) sentence segmentation (using the OpenNLP toolkit2); 2) tokenization (using the OpenNLP toolkit); 3) POS tagging (using the OpenNLP toolkit); 4) syntactic parsing (using the Berkeley Parser3); and 5) named entity recognition (using the Stanford NER4). Markables are extracted as defined in each individual corpus. All possible markable pairs in the training and test set are extracted to form training an</context>
<context position="20861" citStr="Stoyanov et al., 2009" startWordPosition="3365" endWordPosition="3368">ion on the training and test set of the NPAPER corpus are 94.5% and 95.5% respectively, while the recall percentages of markable extraction on the training and test set of the GENIA corpus are 87.6% and 86.6% respectively. The statistics of the NPAPER and the GENIA corpora are listed in Table 1. 5.4 Baseline Results Under our experimental settings, a coreference resolution system that is trained on the NPAPER training set and tested on the NPAPER test set achieves recall, precision, and F-measure of 59.0%, 70.6%, and 64.3%, respectively. This is comparable to the state-of-the-art performance (Stoyanov et al., 2009). Table 2 compares the performance of testing on the GENIA test set, but training with the GENIA training set or the NPAPER training set. Training with in-domain data achieves an F-measure that is 9.1% higher than training with out-of-domain data. Training with 6It turned out that the re-trained tokenization model gave poorer performance and produced many errors on punctuation symbols. Thus, we stuck to using the original tokenization model. 25 Training Set Recall Precision F-measure GENIA Training Set 37.7 71.9 49.5 NPAPER Training Set 30.3 60.7 40.4 Table 2: MUC F-measures on the GENIA test </context>
</contexts>
<marker>Stoyanov, Gilbert, Cardie, Riloff, 2009</marker>
<rawString>Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and Ellen Riloff. 2009. Conundrums in noun phrase coreference resolution: Making sense of the stateof-the-art. In Proceedings of the ACL-IJCILP2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Tang</author>
<author>Xiaoqiang Luo</author>
<author>Salim Roukos</author>
</authors>
<title>Active learning for statistical natural language parsing.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL2002.</booktitle>
<contexts>
<context position="8256" citStr="Tang et al., 2002" startWordPosition="1323" endWordPosition="1326">9 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation techniques whereas we apply and compare multiple state-of-the-art domain adaptation techniques. Rai et al. (2010) exploited a 22 streaming active lear</context>
</contexts>
<marker>Tang, Luo, Roukos, 2002</marker>
<rawString>Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002. Active learning for statistical natural language parsing. In Proceedings of the ACL2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the MUC-6.</booktitle>
<contexts>
<context position="10975" citStr="Vilain et al., 1995" startWordPosition="1763" endWordPosition="1766">kable is a noun phrase which satisfies the markable definition in an individual corpus. Here is an example: When the same MTHC lines are exposed to TNF-alpha in combination with IFN-gamma, the cells instead become DC. In the above sentence, the same MTHC lines and the cells are referring to the same entity and hence are coreferential. It is possible that more than two markables are coreferential in a text. The task of coreference resolution is to determine these relations in a given text. To evaluate the performance of coreference resolution, we follow the MUC evaluation metric introduced by (Vilain et al., 1995). Let Si be an equivalence class generated by the key (i.e., Si is a coreference chain), and p(Si) be a partition of Si relative to the response. Recall is the number of correctly identified links over the number of Y-(|Si|−|p(Si)|) links in the key: Recall =. PreY-(|Si|−1) cision, on the other hand, is defined in the opposite way by switching the role of key and response. F-measure is a trade-off between recall and precision: F = 2·Recall·Precision Recall+Precision . 4 Domain Adaptation with Active Learning 4.1 Domain Adaptation Domain adaptation is applicable when one has a large amount of a</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of the MUC-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Guodong Zhou</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Improving noun phrase coreference resolution by matching strings.</title>
<date>2004</date>
<booktitle>In Proceedings of the IJCILP2004.</booktitle>
<contexts>
<context position="7129" citStr="Yang et al. (2004)" startWordPosition="1136" endWordPosition="1139">lts in Section 6 and conclude in Section 7. 2 Related Work Not only is there a relatively small body of prior research on coreference resolution in the biomedical domain, there are also fewer annotated corpora in this domain. Casta˜no et al. (2002) were among the first to annotate coreferential relations in the biomedical domain. Their annotation only concerned the pronominal and nominal anaphoric expressions in 46 biomedical abstracts. Gasperin and Briscoe (2007) annotated coreferential relations on 5 full articles in the biomedical domain, but only on noun phrases referring to bio-entities. Yang et al. (2004) annotated full NP coreferential relations on biomedical abstracts of the GENIA corpus. The ongoing project of the CRAFT corpus is expected to annotate all coreferential relations on full text of biomedical articles (Cohen et al., 2010). Unlike the work of (Casta˜no et al., 2002), (Gasperin and Briscoe, 2008), and (Gasperin, 2009) that resolved coreferential relations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross</context>
</contexts>
<marker>Yang, Zhou, Su, Tan, 2004</marker>
<rawString>Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim Tan. 2004. Improving noun phrase coreference resolution by matching strings. In Proceedings of the IJCILP2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Bo Yang</author>
<author>Qi Mao</author>
<author>Qiao Liang Xiang</author>
<author>Ivor W Tsang</author>
<author>Kian Ming A Chai</author>
<author>Hai Leong Chieu</author>
</authors>
<title>Domain adaptation for coreference resolution: An adaptive ensemble approach.</title>
<date>2012</date>
<booktitle>In Proceedings of the EMILP2012.</booktitle>
<contexts>
<context position="8110" citStr="Yang et al., 2012" startWordPosition="1295" endWordPosition="1298">lations on certain restricted entities in the biomedical domain, we resolve all NP coreferential relations. Although the GENIA corpus contains 1,999 biomedical abstracts, Yang et al. (2004) tested only on 200 abstracts under 5-fold cross validation. In contrast, we randomly selected 399 abstracts in the 1,999 MEDLINE abstracts of the GENIA-MEDCo corpus as the test set, and as such our evaluation was carried out on a larger scale. Domain adaptation has been studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation tech</context>
</contexts>
<marker>Yang, Mao, Xiang, Tsang, Chai, Chieu, 2012</marker>
<rawString>Jian Bo Yang, Qi Mao, Qiao Liang Xiang, Ivor W. Tsang, Kian Ming A. Chai, and Hai Leong Chieu. 2012. Domain adaptation for coreference resolution: An adaptive ensemble approach. In Proceedings of the EMILP2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shanheng Zhao</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Maximum metric score training for coreference resolution.</title>
<date>2010</date>
<booktitle>In Proceedings of the COLIIG2010.</booktitle>
<contexts>
<context position="1825" citStr="Zhao and Ng, 2010" startWordPosition="270" endWordPosition="273"> Introduction Coreference resolution is the task of determining whether two or more noun phrases (NPs) in a text refer to the same entity. Successful coreference resolution benefits many natural language processing (NLP) tasks, such as information extraction and question answering. In the literature, most prior work on coreference resolution recasts the problem as a two-class classification problem. Machine learning-based classifiers are applied to determine whether a candidate anaphor and a potential antecedent are coreferential (Soon et al., 2001; Ng and Cardie, 2002; Stoyanov et al., 2009; Zhao and Ng, 2010). In recent years, with the advances in biological and life science research, there is a rapidly increasing number of biomedical texts, including research papers, patent documents, etc. This results in an increasing demand for applying natural language processing and information retrieval techniques to efficiently exploit information contained in these large amounts of texts. However, coreference resolution, one of the core tasks in NLP, has only a relatively small body of prior research in the biomedical domain (Kim et al., 2011a; Kim et al., 2011b). A large body of prior research on corefere</context>
</contexts>
<marker>Zhao, Ng, 2010</marker>
<rawString>Shanheng Zhao and Hwee Tou Ng. 2010. Maximum metric score training for coreference resolution. In Proceedings of the COLIIG2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
<author>Yee Seng Chan</author>
</authors>
<title>Word sense disambiguation using OntoNotes: An empirical study.</title>
<date>2008</date>
<booktitle>In Proceedings of the EMILP2008.</booktitle>
<contexts>
<context position="8569" citStr="Zhong et al., 2008" startWordPosition="1377" endWordPosition="1380">n studied and successfully applied to many natural language processing tasks (Jiang and Zhai, 2007; Daume III, 2007; Dahlmeier and Ng, 2010; Yang et al., 2012). On the other hand, active learning has also been applied to NLP tasks to reduce the need of data annotation in the literature (Tang et al., 2002; Laws et al., 2012; Miller et al., 2012). Unlike the aforementioned work that applied only one of domain adaptation or active learning to NLP tasks, we combine both. There is relatively less research on combining domain adaptation and active learning together for NLP tasks (Chan and Ng, 2007; Zhong et al., 2008; Rai et al., 2010). Chan and Ng (2007) and Zhong et al. (2008) used count merging and augment, respectively, as their domain adaptation techniques whereas we apply and compare multiple state-of-the-art domain adaptation techniques. Rai et al. (2010) exploited a 22 streaming active learning setting whereas ours is pool-based. Dahlmeier and Ng (2010) evaluated the performance of three previously proposed domain adaptation algorithms for semantic role labeling. They evaluated the performance of domain adaptation with different sizes of target domain training data. In each of their experiments wi</context>
</contexts>
<marker>Zhong, Ng, Chan, 2008</marker>
<rawString>Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008. Word sense disambiguation using OntoNotes: An empirical study. In Proceedings of the EMILP2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
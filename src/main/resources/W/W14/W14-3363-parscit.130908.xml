<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.983531">
Linear Mixture Models for Robust Machine Translation
</title>
<author confidence="0.800944">
Marine Carpuat, Cyril Goutte and George Foster
</author>
<affiliation confidence="0.8045735">
Multilingual Text Processing
National Research Council
</affiliation>
<address confidence="0.768576">
Ottawa, ON K1A0R6, Canada
</address>
<email confidence="0.997277">
firstname.lastname@nrc.ca
</email>
<sectionHeader confidence="0.993856" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869791666667">
As larger and more diverse parallel texts
become available, how can we lever-
age heterogeneous data to train robust
machine translation systems that achieve
good translation quality on various test
domains? This challenge has been ad-
dressed so far by repurposing techniques
developed for domain adaptation, such
as linear mixture models which combine
estimates learned on homogeneous sub-
domains. However, learning from large
heterogeneous corpora is quite different
from standard adaptation tasks with clear
domain distinctions. In this paper, we
show that linear mixture models can re-
liably improve translation quality in very
heterogeneous training conditions, even
if the mixtures do not use any domain
knowledge and attempt to learn generic
models rather than adapt them to the tar-
get domain. This surprising finding opens
new perspectives for using mixture mod-
els in machine translation beyond clear cut
domain adaptation tasks.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9986806">
While machine translation tasks used to be de-
fined by drawing training and test data from a sin-
gle well-defined domain, current systems have to
deal with increasingly heterogeneous data, both at
training and at test time. As larger and more di-
verse parallel texts become available, how can we
leverage heterogeneous data to train statistical ma-
chine translation (SMT) systems that achieve good
translation quality on various test domains?
So far, this challenge has been addressed by re-
purposing techniques developed for more clear-cut
domain adaptation scenarios, such as linear mix-
ture models (Koehn and Schroeder, 2007; Foster
and Kuhn, 2007; Sennrich, 2012b). Instead of es-
timating models on the whole training corpus at
once, linear mixture models are built as follows:
(1) partition the training corpus into homogeneous
domain-based component, (2) train one model per
component, (3) linearly mix models using weights
learned to adapt to the test domain, (4) replace re-
sulting model in translation system.
In this paper, we aim to gain a better under-
standing of the benefits of linear mixture models in
heterogeneous data conditions, by examining key
untested assumptions:
</bodyText>
<listItem confidence="0.950173611111111">
• Should mixture component capture domain
information? Previous work assumes that
training data should be organized into do-
mains. When manual domain distinctions are
not available, previous work uses clustering
approaches to approximate manual domain
distinctions (Sennrich, 2012a). However, it
is unclear whether it is necessary to use or
mimic domain distinctions in order to define
mixture components.
• Mixture models are usually assumed to im-
prove translation quality by giving more
weight to parts of the training corpus that are
more relevant to the test domain. Is this intu-
ition still valid in our more complex hetero-
geneous training conditions? If not, how do
mixture models affect translation probability
estimates?
</listItem>
<bodyText confidence="0.99541875">
In order to answer these questions, we propose
to study several variants of linear mixture mod-
els that reflect different modeling assumptions and
different levels of domain knowledge. We first
</bodyText>
<page confidence="0.988903">
499
</page>
<note confidence="0.7148255">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 499–509,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999654129032258">
consider two methods for setting mixture weights:
adaptation to the test domain via maximum like-
lihood, and uniform mixtures that make no as-
sumption about the domain of interest (Section 2).
Then, we will describe a wide range of tech-
niques that can be used to define mixture com-
ponents (Section 3). Again, these techniques re-
flect opposite modeling assumptions: manually
defined domains and automatic clusters attempt to
organize heterogeneous training sets into homo-
geneous groups that represent distinct domains,
while random samples capture no domain infor-
mation and simply provide different views of the
training set.
We present an empirical investigation of all the
variations outlined above using a strong system
trained on large and diverse training corpora, for
two language pairs and two distinct test domains.
Our results show that linear mixtures reliably and
robustly improve the quality of machine transla-
tion (Section 5). While they were originally de-
veloped for domain adaptation tasks, linear mix-
tures that have no domain knowledge can perform
as well as traditional mixtures meant to perform
domain adaptation. This suggests that improve-
ments do not stem from domain modeling per se,
but from better generic estimates from the hetero-
geneous training data. Further analysis shows that
the linear mixture estimates are very different from
estimates obtained using more explicit smoothing
schemes (Section 6).
</bodyText>
<sectionHeader confidence="0.983712" genericHeader="introduction">
2 Linear Mixtures for Translation
Models
</sectionHeader>
<bodyText confidence="0.9984778">
Does domain knowledge yield better translation
quality when learning linear mixture weights for
the translation model of a phrase-based MT sys-
tem? We leave the study of linear mixtures for
language and reordering models for future work.
</bodyText>
<subsectionHeader confidence="0.991092">
2.1 Maximum Likelihood Mixtures
</subsectionHeader>
<bodyText confidence="0.999983">
In the standard domain adaptation scenario, the
linear mixture combines translation probabilities
learned on distinct sub-domains in the training
corpus. The conditional translation probability of
phrase t given s is defined as:
</bodyText>
<equation confidence="0.971958333333333">
K
p(t|s) = Akpk(t|s) (1)
k=1
</equation>
<bodyText confidence="0.999930714285714">
where pk(t|s) is a conditional translation proba-
bility learned on subset k of the training corpus.
Note that for all phrase pairs (s, t) that are not ob-
served in component k of the training corpus, we
will have pk(t|s) = 0. As a result, the resulting
distributions are not normalized.
The weights Ak are learned to adapt the transla-
tion model to a development set, which represents
the domain of interest. First, we extract all phrase
pairs from the development set, using the same
technique used to extract phrases from the training
set as part of standard phrase-based MT training.
This yields a joint distribution ˜p(s, t), which can
be used to define a maximum likelihood objective:
</bodyText>
<equation confidence="0.997796">
K
�Aˆ = argmaxλ ˜p(s, t) log Akpk(s|t). (2)
s,t k=1
</equation>
<bodyText confidence="0.999815">
We use the Expectation Maximization algo-
rithm to solve this maximization problem.
</bodyText>
<subsectionHeader confidence="0.997815">
2.2 Uniform Mixtures
</subsectionHeader>
<bodyText confidence="0.9995135">
We will consider uniform mixtures where all com-
ponents are weighted equally:
</bodyText>
<equation confidence="0.9993145">
�K pk(t|s). (3)
1
p(t|s) = K
k=1
</equation>
<bodyText confidence="0.999946875">
In contrast with maximum likelihood mixtures,
uniform mixtures are not meant to adapt the trans-
lation model to a specific test domain. Instead,
they combine estimates learned on various subsets
of the data in the hope of obtaining a better es-
timate of the translation probability distributions
from the (possibly heterogeneous) training domain
as a whole.
</bodyText>
<subsectionHeader confidence="0.999372">
2.3 Why Not Use Loglinear Mixtures?
</subsectionHeader>
<bodyText confidence="0.999044266666667">
In current machine translation systems, there are
two straightforward ways to combine estimates
from heterogneous training data: linear and loglin-
ear mixtures. We argue that linear mixtures are a
better model for combining domain-specific prob-
abilities, since they sum translation probabilities,
while loglinear mixtures multiply probabilities. In
a loglinear mixture, a translation candidate t for a
phrase s will only be scored highly if all compo-
nents agree that it is highly probable. In contrast,
in a linear mixture, t can be a top translation can-
didate overall even if it is not a preferred transla-
tion in some of the components. When the train-
ing data is very heterogeneous, linear mixtures are
therefore preferable.
</bodyText>
<page confidence="0.986344">
500
</page>
<bodyText confidence="0.99966">
Previous work provides empirical evidence sup-
porting this. For instance, Foster et al. (2010)
found that linear mixtures outperform log linear
mixtures when adapting a French-English system
to the medical domain, as well as on a Chinese-
English NIST translation task.
</bodyText>
<subsectionHeader confidence="0.997372">
2.4 Estimating Conditional Translation
Probabilities
</subsectionHeader>
<bodyText confidence="0.999965444444444">
Within each mixture component, we extract all
phrase-pairs, compute relative frequencies, and
use Kneser-Ney smoothing (Chen et al., 2011) to
produce the final estimate of conditional transla-
tion probabilities pk(t s). Per-component proba-
bilities are then combined in Eq. 1 and 3. Simi-
larly, baseline translation probabilities are learned
using Kneser-Ney smoothed frequencies collected
on the entire training set.
</bodyText>
<sectionHeader confidence="0.987019" genericHeader="method">
3 Defining Mixture Components
</sectionHeader>
<bodyText confidence="0.99997225">
We assume that the heterogeneous training corpus
can be split into basic elements that will be or-
ganized in various ways to define the K mixture
components. Basic components could be docu-
ments or sets of sentences defined along various
criteria. Sennrich (2012a) show that using iso-
lated sentences as basic elements might not pro-
vide sufficient information, as smoothing com-
ponent assignments using neighboring sentences
benefits translation quality. In our experiments,
basic elements are sets of parallel sentences which
share the same provenance, genre and dialect, as
we will see in Section 4.
We consider four very different ways of defin-
ing mixture components by grouping the basic
corpus elements: (1) manual partition of the train-
ing corpus into domains, (2) automatically learn-
ing homogeneous domains using text clustering
algorithms, (3) random partitioning, (4) sampling
with replacement.
</bodyText>
<subsectionHeader confidence="0.998587">
3.1 Manually Defined Domains
</subsectionHeader>
<bodyText confidence="0.999949333333333">
Heterogeneous training data is usually grouped
into domains manually using provenance informa-
tion. In most previous work, such domain dis-
tinctions are very clear and easy to define. For
instance, Haddow (2013) uses European parlia-
ment proceedings to improve translation of text
in the movie subtitles and News Commentary do-
mains; Sennrich (2012a) aims to translate Alpine
Club reports using components trained on Euro-
pean parliament proceedings and movie subtitles.
Foster et al. (2010) work with a slightly differ-
ent setting when defining mixture components for
the NIST Chinese-English translation task: while
there is no single obvious “in-domain” component
in the NIST training set, homogeneous domains
can still be defined in a straightforward fashion
based on the provenance of the data (e.g., Hong
Kong Hansards vs. Hong Kong Law vs. News ar-
ticles from FBIS, etc.). We take a similar approach
in our experiments. However, we will see that
since our training data is very heterogeneous, we
take into account other dimensions beyond prove-
nance, such as genre and dialect information (Sec-
tion 4).
</bodyText>
<subsectionHeader confidence="0.9951215">
3.2 Induced Domains Using Automatic
Clustering Algorithms
</subsectionHeader>
<bodyText confidence="0.999992111111111">
We propose to use automatic text clustering tech-
niques to organize basic elements into homoge-
neous clusters that are seen as sub-domains. In our
experiments, we apply clustering algorithms to the
target (English) side of the corpus only.
Each corpus element is transformed into a
vector-space format by constructing a tf.idf vector
representation. After indexing, we filter out stop-
words as well as words occuring in a single doc-
ument. We then weight each word token by the
log of its frequency in the document, combined
with an inverse document frequency (Salton and
McGill, 1983) followed by a normalization to unit
length. The cosine similarity between each pair
of elements is obtained by simply computing the
scalar product, resulting in a N xN similarity ma-
trix, where N is the number of corpus elements.
For clustering, we used Ward’s hierarchical
clustering algorithm (Ward, 1963). We start with
one cluster per corpus element, i.e. N clusters.
From the similarity matrix, we identify the two
most similar clusters and merge them into a sin-
gle one, resulting in N −1 clusters. The similarity
matrix is updated using Ward’s method to form a
(N−1)x(N−1) similarity matrix. The process is
repeated on the new set of clusters, until we reach
the target number of clusters K.
</bodyText>
<subsectionHeader confidence="0.998671">
3.3 Random Partitioning
</subsectionHeader>
<bodyText confidence="0.9982302">
We consider random partitions of the training cor-
pus. They are generated by using a random num-
ber generator to assign each basic element to one
of K clusters. Resulting components therefore do
not capture any domain information. Each com-
</bodyText>
<page confidence="0.991091">
501
</page>
<table confidence="0.999506">
Arabic-English Training Conditions
train segs src en
8.5M 262M 207M
Test Domain 1: Webforum
dev (tune) segs src en
web1 (eval) 4.1k 66k 72k
web2 (eval) 2.2k 35k 38k
2.4k 37k 40k
Test Domain 2: News
dev (tune) segs src en
news (eval) 1664 54k 51k
813 32k 29k
</table>
<tableCaption confidence="0.98994">
Table 1: Statistics for Arabic-English data: Num-
</tableCaption>
<bodyText confidence="0.9687825">
ber of segments (segs), source tokens (src) and En-
glish tokens (en) for each corpus. For English dev
and test sets, word counts averaged across 2 refer-
ences.
</bodyText>
<table confidence="0.997913">
Chinese-English Training Conditions
train segs src en
11M 234M 253M
Test Domain 1: Webforum
dev (tune) segs src en
web1 (eval) 2.7k 61k 77k
web2 (eval) 1.4k 31k 38k
1.2k 29k 36k
Test Domain 2: News
dev (tune) segs src en
news (eval) 1.7k 39k 24k
0.7k 19k 19k
</table>
<tableCaption confidence="0.920025">
Table 2: Statistics for Chinese-English data: Num-
</tableCaption>
<bodyText confidence="0.994346333333333">
ber of segments (segs), source tokens (src) and En-
glish tokens (en) for each corpus. For English dev
and test sets, word counts averaged across 4 refer-
ences.
ponent can potentially be as heterogeneous as the
full training set.
</bodyText>
<subsectionHeader confidence="0.977578">
3.4 Random Sampling with Replacement
</subsectionHeader>
<bodyText confidence="0.999970818181818">
All previous techniques assume that the training
corpus should be partitioned into distinct clus-
ters. We now consider mixture components that
break this assumption, and simply represent sev-
eral, possibly overlapping, views of the training
corpus. They are defined by sampling basic corpus
elements uniformly with replacement. This ap-
proach simply requires defining a number of sam-
ples K and the size n of each sample. We set the
sample size n to the average size of the manual
clusters. We do not fix K in advance: in order to
provide a fair comparison with corpus partitioning
techniques where components achieve coverage of
the entire training set by definition, we keep gen-
erating samples until all basic elements have been
used, and use all resulting K components.
When using uniform linear mixtures, this ap-
proach is similar to bootstrap aggregating (bag-
ging) for regression (Breiman, 1996), where a
more stable model is learned by averaging K es-
timates obtained by sampling the training set uni-
formly and with replacement.
</bodyText>
<sectionHeader confidence="0.992486" genericHeader="method">
4 Experiment Settings
</sectionHeader>
<bodyText confidence="0.999916666666667">
We evaluate our linear mixture models on two
different language pairs, Arabic-English and
Chinese-English, and two different test domains.
</bodyText>
<subsectionHeader confidence="0.993448">
4.1 Training Conditions
</subsectionHeader>
<bodyText confidence="0.999990548387097">
We use the large-scale heterogeneous training con-
ditions defined in the DARPA BOLT project. Data
statistics for both language pairs are given in Ta-
bles 1 and 2. Training corpora cover a wide variety
of sources, genres, dialects, domains, topics.
For instance, for the Arabic task, the training
corpus is originally bundled into 48 files repre-
senting different provenance and epochs. The
data spans 15 genres (defined based on data
provenance, they range from lexicon to newswire,
United Nations, and many variants of web data
such as webforum, weblog, newsgroup, etc.) and
4 automatically tagged dialects (Egyptian, Levan-
tine, Modern Standard Arabic, and untagged). The
distribution along each of these dimensions is very
unbalanced, and each corpus file often contains
text in more than one genre, epoch or dialect.
As a result, we divide the large training corpus
into basic elements, based on the available meta-
data. We define basic corpus elements as a subset
of sentences from the same provenance (i.e. cor-
pus file), dialect and genre. For Arabic, splitting
the original 48 files along these dimensions yields
82 basic elements. Similarly, the Chinese data was
split into a set of 101 basic elements, using genre,
dialects, as well as time span information to split
the original files. Figure 1 shows the wide range of
component sizes in the Arabic and Chinese collec-
tion. For Arabice, notice that several components
are very small, from 6 lines and 90 words to 5.3
million lines and 137M words.
</bodyText>
<page confidence="0.981816">
502
</page>
<subsectionHeader confidence="0.357868">
Arabic corpus components
</subsectionHeader>
<bodyText confidence="0.321198">
corpus component
</bodyText>
<subsectionHeader confidence="0.433136">
Chinese corpus components
</subsectionHeader>
<bodyText confidence="0.855972411764706">
corpus component
and 101 Chinese-English (bott
om) corpus compo-
nents.
503 contains very informal text drawn from online dis-
= 17 for Chinese.
Automatic partitions are created as described
in Section 3. Preliminary experiments with the
hierarchical agglomerative clustering algorithm
showed that the number of clusters used did not
have a big impact on translation
so we
will only present results that use the same num-
ber of clusters as in the manual partitions (10 for
Arabic and 17 for Chinese).
Results for random partitions are averaged
across experiments run with four ran
</bodyText>
<equation confidence="0.596764">
Km
quality,1
</equation>
<bodyText confidence="0.746910125">
dom seeds.
bles 1 and 2: webforum an
d news.
The webforum test domain is defined by devel-
opment test sets made available through BOLT. It
tried K = {2, 4, ... , 18,
for Arabic and K =
{12, 14, ... ,
</bodyText>
<footnote confidence="0.553778">
1We
</footnote>
<page confidence="0.5326585">
20}
20} for Chinese, plus all basic components.
</page>
<bodyText confidence="0.987581266666667">
ce
on the newswire section of the NIST 2008 test set.
tences are almost exclusively written in the Egyp-
tian dialect. Therefore, Egyptian webforum data
is presumably the closest to the test domain, but
Egyptian weblogs or mixed-dialect broadcast con-
versations could potentially be useful as well.
We also test the Arabic and Chinese systems on
the news domain. The goal of these experiments is
to evaluate the robustness of linear mixtures across
different test domains. We use publicly available
test sets from the NIST evaluation. The dev set
used to learn maximum likelihood mixtures and
tune the translation system is the NIST section of
the 2006 test set. We evaluate system performan
</bodyText>
<listItem confidence="0.7619666">
• 6 hierarchical lexicalized reordering scores
(Galley and Manning, 2008)
• a word penalty, and aword-displacement dis-
tortion penalty
• aGood-Turing smoothed 4-gram language
model trained on the Gigaword corpus,
Kneser-Ney smoothed 5-gram models trained
on the English side of the training corpus, and
an additional 5-gram model trained on mono-
lingual webforum data.
</listItem>
<bodyText confidence="0.997249">
Weights for these features are learned using a
batch version of the MIRA algorithm (Chiang,
2012). Phrase pairs are extracted from several
word alignments of the training set: HMM, IBM2,
and IBM4. Word alignments are kept constant
across all experiments.
We apply our linear mixture models to both
translation probability scores, in each direction.
The reordering and lan
</bodyText>
<figure confidence="0.5222965">
2011)2
guage models are not
</figure>
<figureCaption confidence="0.999707">
Figure 1: Sizes of the 82 Arabic-English (top)
</figureCaption>
<subsectionHeader confidence="0.997132">
4.2 Definition of Mixture Components
</subsectionHeader>
<bodyText confidence="0.999956875">
Manual partitions were created first by the system
developers, based on intuitions on the nature of the
test domain and manual inspection of the training
data. The main goal was to group data into com-
ponents that are large enough to reliably estimate
translation probabilities, but small enough to be
homogeneous. This resulted in Km = 10 clusters
for Arabic, and
</bodyText>
<subsectionHeader confidence="0.999607">
4.3 Test Domains
</subsectionHeader>
<bodyText confidence="0.99965">
We consider two test domains, as described in Ta-
</bodyText>
<subsectionHeader confidence="0.991105">
4.4 Machine Translation System
</subsectionHeader>
<bodyText confidence="0.9996678">
We use an in-house implementation of a Phrase-
based Statistical Machine Translation system
(Koehn et al., 2007) to build strong baseline sys-
tems for both language pairs. Translation hypothe-
ses are scored according to the following features:
</bodyText>
<listItem confidence="0.8626">
• 4 phrase-table scores: Kneser-Ney smoothed
phrasal translation probabilites and lexical
weights, in both translation directions (Chen
et al.,
</listItem>
<footnote confidence="0.8587328">
Arabic-English system uses 6 additional binary fea-
tures which fire if a phrase-pair was generated by one of the
3 word alignment methods in each tran
2The
slation direction.
</footnote>
<figure confidence="0.999174571428571">
0 20 40 60 80
#lines / #words
1e+01 1e+05
0
000
000000000 000 0
00000000000000000000000 0
000000000000 0
0
0
00000
0
0
0
0
#lines
#words
0
0
0
0 20 40 60 80 100
#lines / #words
1e+03 1e+05 1e+07
●
●
●●●●●●●●●●●●● ●
●●
●●●●●
●● ●●●●●●●●●●●●●●●●●●●●●●●●●●
●
●
●
#lines
#words
●
</figure>
<bodyText confidence="0.9921115">
cussion of various topics. Taking these data sets
as the definition of the target domain, there is no
single obvious in-domain section of the training
corpus. For instance, for Arabic, the dev set sen-
</bodyText>
<table confidence="0.999774571428571">
Test domain Webforum
Arabic eval Forum1 Forum2
Linear mix 39.67 40.60
Loglinear mix 37.53 38.80
Chinese eval Forum1 Forum2
Linear mix 30.17 26.86
Loglinear mix 27.65 23.78
</table>
<tableCaption confidence="0.921519">
Table 3: Impact of mixture type on translation
quality as measured by BLEU.
</tableCaption>
<bodyText confidence="0.698042285714286">
adapted. Note that systems used to translate the
web1 and web2 test sets are always tuned on the
webforum tuning set, while systems used to trans-
late data in the news domain are tuned on a news
development set. The relevant tuning set is also
used for learning maximum likelkihood mixtures
when appropriate.
</bodyText>
<sectionHeader confidence="0.993254" genericHeader="method">
5 Findings: Impact on Translation
Quality
</sectionHeader>
<subsectionHeader confidence="0.999663">
5.1 Linear vs. Loglinear Mixtures
</subsectionHeader>
<bodyText confidence="0.999956272727273">
Before focusing exclusively on linear mixtures,
we confirm that they outperform loglinear mix-
tures. This comparison was conducted on the web-
forum domain, using manually defined domains
as components. For linear mixtures, we trained
the weights using maximum likelihood. Loglin-
ear mixture weights are trained by MIRA. Table 3
shows that linear mixtures yield consistently and
significantly higher BLEU scores than loglinear
mixtures, which is consistent with existing results
(Foster et al., 2010, inter alia).
</bodyText>
<subsectionHeader confidence="0.999831">
5.2 Impact of Mixture Components
</subsectionHeader>
<bodyText confidence="0.9982892">
We now focus on linear mixtures and measure the
impact on translation quality of the various com-
ponent types described in Section 3. In all cases,
mixtures weights are estimated by maximum like-
lihood. Results are summarized in Table 4 for both
Arabic and Chinese.
The main result is that all mixture models con-
sidered significantly improve on the “no mix”
baseline for both languages. Directly using the
101 basic elements for Chinese and the 82 basic
elements for Arabic significantly improves on the
baseline. Grouping the basic elements into coarser
clusters can further improve BLEU. For Arabic,
automatic partitioning (randomly or by clustering)
yields better BLEU scores than manual partition-
</bodyText>
<table confidence="0.9999312">
Test domain Webforum News
Arabic eval web] web2 news
Cluster domains 40.11 40.60 57.95
Random partition 40.43 40.63 57.78
Random sample 39.94 40.36 57.85
Manual domains 39.67 40.60 57.63
Basic elements 39.83 40.63 57.57
No mix 38.64 39.21 56.59
Chinese eval web] web2 news
Cluster domains 29.82 26.34 37.22
Random partition 29.50 26.21 36.83
Random sample 29.47 26.17 36.70
Manual domains 30.17 26.86 36.90
Basic elements 29.29 26.25 36.17
No mix 28.61 25.63 35.96
</table>
<tableCaption confidence="0.988859">
Table 4: Impact of mixture component definition
</tableCaption>
<bodyText confidence="0.92585575">
on BLEU score: there is no clear benefit to explic-
itly modeling domains.
ing, while the manual and cluster-based domains
yield the highest BLEU scores for Chinese.
</bodyText>
<subsectionHeader confidence="0.999578">
5.3 Impact of Mixture Weights
</subsectionHeader>
<bodyText confidence="0.999931666666667">
Does domain knowledge yield better translation
quality when learning linear mixture weights? We
answer this question by comparing the transla-
tion quality obtained with maximum likelihood
vs. uniform mixtures. The maximum likelihood
weights are set once per domain, using the rele-
vant domain development set, while the uniform
mixture is the same across all test domains.
Table 5 shows that maximum likelihood
weights generally have a slight advantage over
uniform weights, especially in the Webforum do-
main. On ”basic elements” in Arabic, the gain is
a massive 5 BLEU points, which we attribute to
the fact that, as shown in Figure 1, there are many
more very small components in Arabic. Those get
a disproportionate influence in the uniform mix-
ture, hurting the overall performance. On the other
hand, the uniform mixture performs better in the
News domain. This might be explained by the fact
that the tune and test sets are more distant in News
than in Webforum, as suggested by the fact that the
tuning BLEU scores are not as good at predicing
test BLEU rankings in the news domain as in the
webforum domain.
Overall, the difference in performance between
the best linear mixture and the ”no mix” baseline
is 1.4 to 1.6 BLEU on Arabic, and 0.7 to 1.3 BLEU
</bodyText>
<page confidence="0.995733">
504
</page>
<bodyText confidence="0.999115111111111">
on Chinese. By comparison, the delta between the
two weight setting approaches (maximum likeli-
hood vs. uniform), depending on the partition-
ing technique, is below 0.4 BLEU for Arabic (ex-
cept for Basic elements, +3.6 BLEU) and below
0.57 BLEU for Chinese. It is therefore clear that
the gain from using linear mixtures is much larger
than the influence of the mixture weight setting,
except in the one specific case discussed above.
Taken together, these results show that lin-
ear mixtures can reliably and robustly improve
the quality of machine translation. But surpris-
ingly, linear mixtures that have no domain knowl-
edge (random partition + uniform weights) can
sometimes perform as well as traditional mixtures
meant to perform domain adaptation. This sug-
gests that improvements cannot be only explained
by improved domain modeling.
</bodyText>
<table confidence="0.99994552">
Test domain Webforum News
Arabic eval web] web2 news
Cluster domains 40.11 40.60 57.95
w/ uniform mix 39.63 40.15 58.21
Random partition 40.43 40.63 57.78
w/ uniform mix 40.31 40.15 58.18
Random sample 39.94 40.36 58.06
w/ uniform mix 39.88 40.56 58.65
Manual domains 39.67 40.60 57.63
w/ uniform mix 39.93 40.18 58.00
Basic elements 39.83 40.63 57.57
w/ uniform mix 34.84 35.82 58.46
No mix 38.64 39.21 56.59
Chinese eval web] web2 news
Cluster domains 29.82 26.34 37.22
w/ uniform mix 29.44 25.94 37.47
Random partition 29.50 26.21 36.83
w/ uniform mix 29.43 25.89 36.95
Random sample 29.47 26.17 36.70
w/ uniform mix 28.47 25.54 36.61
Manual domains 30.17 26.86 36.90
w/ uniform mix 29.25 26.36 36.95
Basic elements 29.29 26.25 36.17
w/ uniform mix 29.23 25.81 36.63
No mix 28.61 25.63 35.96
</table>
<tableCaption confidence="0.9895435">
Table 5: Impact of linear mixture weights on trans-
lation quality as measured by BLEU: using do-
main knowledge when setting weights has an un-
reliable impact.
</tableCaption>
<sectionHeader confidence="0.535999" genericHeader="method">
6 Findings: Impact on Translation
Probability Estimates
</sectionHeader>
<bodyText confidence="0.999903615384615">
Thus far, all our experiments have measured the
impact of different types of linear mixtures on
overall translation quality. But what is the im-
pact of these various estimations methods on the
learned phrasal translation probability distribu-
tions themselves? More specifically, how do trans-
lation probabilities estimated using linear mixtures
differ from global “no mix” estimates? If linear
mixtures do not only capture domain knowledge
as suggested by Section 5, do they simply perform
a form of smoothing? If so, how does this im-
plicit smoothing compare to more explicit smooth-
ing schemes for translation probabilities?
</bodyText>
<subsectionHeader confidence="0.9461585">
6.1 How do linear mixtures affect translation
probabilities?
</subsectionHeader>
<bodyText confidence="0.995272545454545">
Let us compare translation probabilities estimated
directly on the entire corpus Pnomix(t|s), with lin-
ear mixtures pmix(t|s) = EKk=1 λkpk(t|s). The
difference between pmix(t|s) and pnomix(t|s) is
hard to represent analytically in the general case,
but studying a few particular cases can help us gain
a better understanding.
First, we observe that linear mixtures scale
down the contribution of component-specific
source phrases. Assume that the phrase s oc-
curs only once in the training corpus, with trans-
lation t. By definition, there is a single mixture
component k such that pmix(t|s) = λk, which is
likely to be smaller than pnomix(t|s) = 1. In the
slightly more general case where s occurs more
than once, but always in the same component k,
then pmix(t|s) = λkpnomix(t|s), which has no im-
pact on the ranking of translation candidates for s,
but yields a smaller feature value for the decoder.
Second, let us consider the case of very fre-
quent “general language” phrases. They should
have roughly the same translation distributions in
all mixture components: If the pk(t|s) distribu-
tions are the same in each component, the λk val-
ues learned do not matter, they have no impact on
pmix(t|s) = pnomix(t|s).
In between these extremes, the impact of linear
mixtures depends on the frequency and ambiguity
of translation candidates t across mixture compo-
nents. For instance, let us assume that the mixture
components are somehow defined such that they
partition the translate candidates t of a phrase s
into separate clusters. In that case, for each t, there
</bodyText>
<page confidence="0.994317">
505
</page>
<figure confidence="0.963009">
nomix unsmoothed vs. nomix
frequency bin
</figure>
<figureCaption confidence="0.9535745">
Figure 2: Comparing translation probability dis-
tributions with and without Kneser Ney smooth-
</figureCaption>
<bodyText confidence="0.977846333333333">
ing for Chinese phrase-tables: boxplots of Jensen-
Shannon divergences binned by source phrase fre-
quency. For instance, the box and whisker at x = 8
represent the distribution of the values of Jensen-
Shannon divergence between the unsmoothed and
smoothed translation probability distribution for
all Chinese phrases seen between 5 and 8 times
during phrase extraction.
is a k such that pk(t|s) = pnomix(t|s). The rank-
ing of translation candidates t for s according to
pmix(t|s) can be very different from pnomix(t|s),
as controlled by the A values used.
</bodyText>
<subsectionHeader confidence="0.999893">
6.2 Smoothing Effects
</subsectionHeader>
<bodyText confidence="0.999615833333333">
As a basis for comparison, let us analyze the
difference between unsmoothed relative frequen-
cies and smoothed translation probabilities using
a conventional smoothing scheme. We focus on
the Kneser-Ney smoothing scheme (Chen et al.,
2011), since it is used to smooth translation prob-
abilities in the ‘nomix” baseline as well as in all
mixture components.
For seen phrase pairs (with f(s, t) &gt; 0), the
difference between Kneser-Ney estimates pkn(t|s)
and relative frequency estimates prf(t|s) can be
written as:
</bodyText>
<equation confidence="0.96167">
f(s) f(s)
(4)
</equation>
<bodyText confidence="0.999963913043478">
where D is a discount coefficient, f(s) is the raw
frequency for source phrase s, n(s) is the num-
ber of translation candidates for s in the phrase-
table, pb(t) is a back-off distribution proportional
to n(t). The first term is a discount that increases
when s is rare, while the second term adds some
probability mass back, based on the frequency and
degree of ambiguity of the target phrase t. There-
fore, Kneser-Ney smoothing has primarily a dis-
count effect, applied on rare source phrases. In ad-
dition, for more frequent and ambiguous phrases,
the relative frequency can be adjusted up or down
depending on how ambiguous s and t are.
Overall, there are some similarities between the
impact of Kneser-Ney smoothing and linear mix-
tures, since one can expect that the translation
distributions will diverge more from global rela-
tive frequencies for rare phrases than for frequent
phrases. However, the discounting / down-scaling
effects are controlled by very different parameters
in linear mixtures than in Kneser-New smoothing.
In order to better understand these differences in
practice, an empirical analysis is required.
</bodyText>
<subsectionHeader confidence="0.997294">
6.3 Empirical Comparison
</subsectionHeader>
<bodyText confidence="0.999460482758621">
How do linear mixtures and smoothing affect
translation probabilities p(t|s) in practice? We
use the Jensen-Shannon divergence (Lin, 1991) to
quantify the distance between (a) various mixture
model estimates and (b) the global smoothed rela-
tive frequency estimates used in our baseline “no
mix” experiments. In addition, we also compare
the Kneser-Ney smoothed translation probabilities
with unsmoothed relative frequencies, in order to
highlight the difference between standard smooth-
ing techniques and linear mixture models.
Figures 2 and 3 show the distributions of di-
vergence values by source phrase frequencies for
Chinese-English phrase-tables. The divergence
from the global estimate is the largest for rare
phrases in all cases, as expected based on previous
Sections. However, the Figures also highlight the
different behavior of linear mixtures compared to
Kneser-Ney smoothing. The divergence values are
much higher overall for the linear mixtures than
for smoothing (note that the difference in range
on the y axis in Figure 2 vs. Figure 3). In addi-
tion, linear mixtures have a large impact on trans-
lation probabilities not only on the rarest source
phrases but also on relatively frequent phrases: in
Figure 3, the median Jensen-Shannon divergence
remains high for source phrases extracted up to
128 times from the training set3, while the median
value drops significantly as the frequency range in-
</bodyText>
<footnote confidence="0.6682765">
3Recall that we use multiple word alignment methods, so
extraction counts are summed across all alignment methods.
</footnote>
<figure confidence="0.9812482">
4 8 16 32 64 128 512 2048 8192 32768
JSdiv
0.00 0.02 0.04 0.06 0.08 0.10
D D ∗ n(s) ∗ pb(t)
prf(t|s) − pkn(t|s) = −
</figure>
<page confidence="0.848489">
506
</page>
<figureCaption confidence="0.9937205">
Figure 3: Comparing translation probability distributions of mixtures vs. “nomix” on Chinese webforum
data, including EM weights (top row) and uniform weights (bottom row).
</figureCaption>
<figure confidence="0.999417366666667">
JSdiv
0.0 0.1 0.2 0.3 0.4 0.5
JSdiv
0.0 0.1 0.2 0.3 0.4 0.5
JSdiv
0.0 0.1 0.2 0.3 0.4 0.5
JSdiv
0.0 0.1 0.2 0.3 0.4 0.5
mix manual domains vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
frequency bin
avg manual domains vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
frequency bin
mix cluster domains vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
frequency bin
avg cluster domains vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
frequency bin
mix random partitions vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
frequency bin
avg random partitions vs. nomix
4 8 16 32 64 128 512 2048 8192 32768
frequency bin
JSdiv
0.0 0.1 0.2 0.3 0.4 0.5
JSdiv
0.0 0.1 0.2 0.3 0.4 0.5
</figure>
<bodyText confidence="0.9978984">
creases in Figure 2. In addition, uniform mixtures
have an even higher impact on frequent phrases
than mixtures based on EM weights.
Furthermore, the nature of mixture components
used has a visible impact on the divergence distri-
butions in Figure 3: random partitions yield lower
divergences for very frequent source phrases.
Overall, the linear mixtures result in very differ-
ent translation probability distributions than global
estimates, including smoothed estimates. This
suggests that standard smoothing techniques can
be improved when learning from heterogeneous
training data, and that mixture components are
beneficial even when they do not explicitly cap-
ture domain distinctions.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.999872838709677">
Most previous work on domain adaptation in ma-
chine translation presupposes a clear-cut distinc-
tion between in-domain and out-of-domain data
(Koehn and Schroeder, 2007; Foster and Kuhn,
2007; Duh et al., 2010; Bisazza et al., 2011; Had-
dow and Koehn, 2012; Sennrich, 2012b; Haddow
and Koehn, 2012; Clark et al., 2012, among many
others). We focused instead on a different less-
studied question: how can we leverage training
data drawn from a wide variety of sources, genres,
time periods, to translate a domain represented by
a small development set?
Many approaches focus on mapping the test do-
main to a single subset of the training data. In con-
trast, we show that the test domain can be flex-
ibly represented by a mixture of many compo-
nents. Yamamoto and Sumita (2007) cluster the
parallel data using bilingual representations, and
assign data to a single cluster at test time. Wang
et al. (2012) show how to detect a known domain
at test time in order to configure a generic transla-
tion system with domain-specific feature weights.
Others select a subset of training data that is rele-
vant to the test domain, using e.g., IR techniques
(Hildebrand et al., 2005) or language model cross-
entropy (Axelrod et al., 2011).
Closer to this work, Sennrich (2012a) proposes
a sentence-level clustering approach to automati-
cally recover domain distinctions in a heteoroge-
neous corpus obtained by concatenating data from
a small number of very distant domains. The tar-
</bodyText>
<page confidence="0.987632">
507
</page>
<bodyText confidence="0.999991684210526">
get domain was Alpine Club reports, while out
of domain data sets comprised European parlia-
ment proceedings and movie subtitles. We address
training conditions where the dimensions for or-
ganizing the training data are not as clear-cut, and
show that partitions that do not attempt to mimick
domain distinctions can improve translation qual-
ity. It would be interesting to see whether our con-
clusion holds in these more artificial training set-
tings, and whether sentence-level corpus organiza-
tion could help translation quality in our settings.
Finally, recent work shows that linear mixture
weights can be optimized for BLEU, either di-
rectly (Haddow, 2013), or by simulating discrim-
inative training (Foster et al., 2013). In this pa-
per, we limited our studies to maximum likelihood
and uniform mixtures, however, the various mix-
ture component definitions proposed here can also
be applied when maximizing BLEU.
</bodyText>
<sectionHeader confidence="0.998352" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999995727272728">
We have presented an extensive study of lin-
ear mixtures for training translation models on
very heterogeneous data on Arabic-English and
Chinese-English translation tasks. In addition, we
evaluated the robustness of our models across two
distinct domains on the Arabic-English task.
Our results show that linear mixtures reliably
and robustly improve the quality of machine trans-
lation. Improvements on the mixture-free base-
line system range from 0.7 to 1.6 BLEU points
depending on the components and weights used.
While linear mixture translation models were orig-
inally proposed for domain adaptation tasks, we
showed that linear mixtures that have no domain
knowledge can perform as well or better than tra-
ditional mixtures meant to perform domain adap-
tation. This suggests that improvements with lin-
ear mixture models do not only stem from giving
more weight to sections of the training data that
are relevant to the test domain, as is assumed in
a standard domain adaptation task. Improvements
also come from averaging better generic estimates
from the heterogeneous training data. In other
words, in heterogeneous training settings, linear
mixture models improve translation quality even
though they do not perform domain adaptation.
Finally, we show that while linear mixtures can
be viewed as a smoothing technique, linear mix-
ture estimates do not diverge from global estimates
in the same way as Kneser-Ney smoothed transla-
tion probabilities. In particular, while smoothing
primarily has a large discounting effect for rare
source phrases, linear mixtures yield differences
in translation probabilities for phrases with a wider
range of frequencies.
These surprising results encourage us to rethink
the use of mixture models, and opens up new ways
of conceptualizing learning from heterogeneous
data beyond domain adaptation. In future work,
we will extend this study by varying the gran-
ularity of basic elements used to define mixture
components, including sentences and phrases, and
will explore how they compare with more general
smoothing techniques.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996494">
This research was supported in part by DARPA
contract HR0011-12-C-0014 under subcontract to
Raytheon BBN Technologies. The authors would
like to thank the reviewers and the PORTAGE
group at the National Research Council.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999821071428571">
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 355–362.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus interpolation methods for
phrase-based SMT adaptation. International Work-
shop on Spoken Language Translation (IWSLT).
Leo Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123–140.
Boxing Chen, Roland Kuhn, George Foster, and
Howard Johnson. 2011. Unpacking and transform-
ing feature functions: New ways to smooth phrase
tables. In Proceedings of Machine Translation Sum-
mit.
David Chiang. 2012. Hope and fear for discriminative
training of statistical translation models. Journal
of Machine Learning Research, 13(1):1159–1187,
April.
Jonathan H. Clark, Alon Lavie, and Chris Dyer. 2012.
One system, many domains: Open-domain statisti-
cal machine translation via feature augmentation. In
Proceedings of the Conference of the Association for
Machine Translation in the Americas.
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation in
statistical machine translation.
</reference>
<page confidence="0.974593">
508
</page>
<reference confidence="0.9997607">
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128–135.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing.
George Foster, Boxing Chen, and Roland Kuhn. 2013.
Simulating discriminative training for linear mixture
adaptation in statistical machine translation. In Pro-
ceedings of the XIV Machine Translation Summit,
pages 183–190.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’08, pages 848–856.
Barry Haddow and Philipp Koehn. 2012. Analysing
the effect of out-of-domain data on SMT systems. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, pages 422–432.
Barry Haddow. 2013. Applying pairwise ranked opti-
misation to improve the interpolation of translation
models. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 342–347.
Almut Silja Hildebrand, Matthias Eck, Stephan Vogel,
and Alex Waibel. 2005. Adaptation of the transla-
tion model for statistical machine translation based
on information retrieval. In European Association
for Machine Translation.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Trans-
lation. In Workshop on Statistical Machine Transla-
tion.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Annual Meeting of the Association for Computa-
tional Linguistics (ACL), demonstration session.
Jianhua Lin. 1991. Divergence measures based
on the shannon entropy. IEEE Trans. Inf. Theor.,
37(1):145–151, September.
Rico Sennrich. 2012a. Mixture-modeling with un-
supervised clusters for domain adaptation in statis-
tical machine translation. In 16th Conference of
the European Association for Machine Translation
(EAMT).
Rico Sennrich. 2012b. Perplexity minimization for
translation model adaptation in statistical machine
tra. In Thirteenth Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL).
Wei Wang, Klaus Macherey, Wolfgang Macherey,
Franz Och, and Peng Xu. 2012. Improved do-
main adaptation for statistical machine translation.
In 10th biennial conference of the Association for
Machine Translation in the Americas (AMTA).
Hirofumi Yamamoto and Eiichiro Sumita. 2007. Bilin-
gual cluster based models for statistical machine
translation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 514–523.
</reference>
<page confidence="0.998483">
509
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.332338">
<title confidence="0.756413">Linear Mixture Models for Robust Machine Translation Carpuat, Cyril Goutte</title>
<author confidence="0.595071">Multilingual Text</author>
<affiliation confidence="0.996824">National Research</affiliation>
<address confidence="0.955179">Ottawa, ON K1A0R6,</address>
<email confidence="0.998609">firstname.lastname@nrc.ca</email>
<abstract confidence="0.9997018">As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train robust machine translation systems that achieve good translation quality on various test domains? This challenge has been addressed so far by repurposing techniques developed for domain adaptation, such as linear mixture models which combine estimates learned on homogeneous subdomains. However, learning from large heterogeneous corpora is quite different from standard adaptation tasks with clear domain distinctions. In this paper, we show that linear mixture models can reliably improve translation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>355--362</pages>
<contexts>
<context position="34321" citStr="Axelrod et al., 2011" startWordPosition="5533" endWordPosition="5536">ain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 get domain was Alpine Club reports, while out of domain data sets comprised European parliament proceedings and movie subtitles. We address training conditions where the dimensions for organizing the training data are not as clear-cut, and show that partitions that do not attempt to mimick domain distinctions can improve translation quality. It would be</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 355–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arianna Bisazza</author>
<author>Nick Ruiz</author>
<author>Marcello Federico</author>
</authors>
<title>Fill-up versus interpolation methods for phrase-based SMT adaptation.</title>
<date>2011</date>
<booktitle>International Workshop on Spoken Language Translation (IWSLT).</booktitle>
<contexts>
<context position="33336" citStr="Bisazza et al., 2011" startWordPosition="5362" endWordPosition="5365">rce phrases. Overall, the linear mixtures result in very different translation probability distributions than global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many others). We focused instead on a different lessstudied question: how can we leverage training data drawn from a wide variety of sources, genres, time periods, to translate a domain represented by a small development set? Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, an</context>
</contexts>
<marker>Bisazza, Ruiz, Federico, 2011</marker>
<rawString>Arianna Bisazza, Nick Ruiz, and Marcello Federico. 2011. Fill-up versus interpolation methods for phrase-based SMT adaptation. International Workshop on Spoken Language Translation (IWSLT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Bagging predictors.</title>
<date>1996</date>
<booktitle>Machine Learning,</booktitle>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="13837" citStr="Breiman, 1996" startWordPosition="2183" endWordPosition="2184"> corpus elements uniformly with replacement. This approach simply requires defining a number of samples K and the size n of each sample. We set the sample size n to the average size of the manual clusters. We do not fix K in advance: in order to provide a fair comparison with corpus partitioning techniques where components achieve coverage of the entire training set by definition, we keep generating samples until all basic elements have been used, and use all resulting K components. When using uniform linear mixtures, this approach is similar to bootstrap aggregating (bagging) for regression (Breiman, 1996), where a more stable model is learned by averaging K estimates obtained by sampling the training set uniformly and with replacement. 4 Experiment Settings We evaluate our linear mixture models on two different language pairs, Arabic-English and Chinese-English, and two different test domains. 4.1 Training Conditions We use the large-scale heterogeneous training conditions defined in the DARPA BOLT project. Data statistics for both language pairs are given in Tables 1 and 2. Training corpora cover a wide variety of sources, genres, dialects, domains, topics. For instance, for the Arabic task, </context>
</contexts>
<marker>Breiman, 1996</marker>
<rawString>Leo Breiman. 1996. Bagging predictors. Machine Learning, 24(2):123–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
<author>George Foster</author>
<author>Howard Johnson</author>
</authors>
<title>Unpacking and transforming feature functions: New ways to smooth phrase tables.</title>
<date>2011</date>
<booktitle>In Proceedings of Machine Translation Summit.</booktitle>
<contexts>
<context position="7967" citStr="Chen et al., 2011" startWordPosition="1228" endWordPosition="1231">ven if it is not a preferred translation in some of the components. When the training data is very heterogeneous, linear mixtures are therefore preferable. 500 Previous work provides empirical evidence supporting this. For instance, Foster et al. (2010) found that linear mixtures outperform log linear mixtures when adapting a French-English system to the medical domain, as well as on a ChineseEnglish NIST translation task. 2.4 Estimating Conditional Translation Probabilities Within each mixture component, we extract all phrase-pairs, compute relative frequencies, and use Kneser-Ney smoothing (Chen et al., 2011) to produce the final estimate of conditional translation probabilities pk(t s). Per-component probabilities are then combined in Eq. 1 and 3. Similarly, baseline translation probabilities are learned using Kneser-Ney smoothed frequencies collected on the entire training set. 3 Defining Mixture Components We assume that the heterogeneous training corpus can be split into basic elements that will be organized in various ways to define the K mixture components. Basic components could be documents or sets of sentences defined along various criteria. Sennrich (2012a) show that using isolated sente</context>
<context position="28480" citStr="Chen et al., 2011" startWordPosition="4560" endWordPosition="4563">nsenShannon divergence between the unsmoothed and smoothed translation probability distribution for all Chinese phrases seen between 5 and 8 times during phrase extraction. is a k such that pk(t|s) = pnomix(t|s). The ranking of translation candidates t for s according to pmix(t|s) can be very different from pnomix(t|s), as controlled by the A values used. 6.2 Smoothing Effects As a basis for comparison, let us analyze the difference between unsmoothed relative frequencies and smoothed translation probabilities using a conventional smoothing scheme. We focus on the Kneser-Ney smoothing scheme (Chen et al., 2011), since it is used to smooth translation probabilities in the ‘nomix” baseline as well as in all mixture components. For seen phrase pairs (with f(s, t) &gt; 0), the difference between Kneser-Ney estimates pkn(t|s) and relative frequency estimates prf(t|s) can be written as: f(s) f(s) (4) where D is a discount coefficient, f(s) is the raw frequency for source phrase s, n(s) is the number of translation candidates for s in the phrasetable, pb(t) is a back-off distribution proportional to n(t). The first term is a discount that increases when s is rare, while the second term adds some probability m</context>
</contexts>
<marker>Chen, Kuhn, Foster, Johnson, 2011</marker>
<rawString>Boxing Chen, Roland Kuhn, George Foster, and Howard Johnson. 2011. Unpacking and transforming feature functions: New ways to smooth phrase tables. In Proceedings of Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hope and fear for discriminative training of statistical translation models.</title>
<date>2012</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="17722" citStr="Chiang, 2012" startWordPosition="2817" endWordPosition="2818">set used to learn maximum likelihood mixtures and tune the translation system is the NIST section of the 2006 test set. We evaluate system performan • 6 hierarchical lexicalized reordering scores (Galley and Manning, 2008) • a word penalty, and aword-displacement distortion penalty • aGood-Turing smoothed 4-gram language model trained on the Gigaword corpus, Kneser-Ney smoothed 5-gram models trained on the English side of the training corpus, and an additional 5-gram model trained on monolingual webforum data. Weights for these features are learned using a batch version of the MIRA algorithm (Chiang, 2012). Phrase pairs are extracted from several word alignments of the training set: HMM, IBM2, and IBM4. Word alignments are kept constant across all experiments. We apply our linear mixture models to both translation probability scores, in each direction. The reordering and lan 2011)2 guage models are not Figure 1: Sizes of the 82 Arabic-English (top) 4.2 Definition of Mixture Components Manual partitions were created first by the system developers, based on intuitions on the nature of the test domain and manual inspection of the training data. The main goal was to group data into components that </context>
</contexts>
<marker>Chiang, 2012</marker>
<rawString>David Chiang. 2012. Hope and fear for discriminative training of statistical translation models. Journal of Machine Learning Research, 13(1):1159–1187, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan H Clark</author>
<author>Alon Lavie</author>
<author>Chris Dyer</author>
</authors>
<title>One system, many domains: Open-domain statistical machine translation via feature augmentation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the Association for Machine Translation in the Americas.</booktitle>
<contexts>
<context position="33421" citStr="Clark et al., 2012" startWordPosition="5377" endWordPosition="5380">ity distributions than global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many others). We focused instead on a different lessstudied question: how can we leverage training data drawn from a wide variety of sources, genres, time periods, to translate a domain represented by a small development set? Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect</context>
</contexts>
<marker>Clark, Lavie, Dyer, 2012</marker>
<rawString>Jonathan H. Clark, Alon Lavie, and Chris Dyer. 2012. One system, many domains: Open-domain statistical machine translation via feature augmentation. In Proceedings of the Conference of the Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
</authors>
<title>Katsuhito Sudoh, and Hajime Tsukada.</title>
<date>2010</date>
<marker>Duh, 2010</marker>
<rawString>Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada. 2010. Analysis of translation model adaptation in statistical machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for SMT.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>128--135</pages>
<contexts>
<context position="1808" citStr="Foster and Kuhn, 2007" startWordPosition="267" endWordPosition="270">on tasks used to be defined by drawing training and test data from a single well-defined domain, current systems have to deal with increasingly heterogeneous data, both at training and at test time. As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train statistical machine translation (SMT) systems that achieve good translation quality on various test domains? So far, this challenge has been addressed by repurposing techniques developed for more clear-cut domain adaptation scenarios, such as linear mixture models (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012b). Instead of estimating models on the whole training corpus at once, linear mixture models are built as follows: (1) partition the training corpus into homogeneous domain-based component, (2) train one model per component, (3) linearly mix models using weights learned to adapt to the test domain, (4) replace resulting model in translation system. In this paper, we aim to gain a better understanding of the benefits of linear mixture models in heterogeneous data conditions, by examining key untested assumptions: • Should mixture component capture domain information? Previous wo</context>
<context position="33296" citStr="Foster and Kuhn, 2007" startWordPosition="5354" endWordPosition="5357">d lower divergences for very frequent source phrases. Overall, the linear mixtures result in very different translation probability distributions than global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many others). We focused instead on a different lessstudied question: how can we leverage training data drawn from a wide variety of sources, genres, time periods, to translate a domain represented by a small development set? Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel </context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for SMT. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 128–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7602" citStr="Foster et al. (2010)" startWordPosition="1177" endWordPosition="1180"> model for combining domain-specific probabilities, since they sum translation probabilities, while loglinear mixtures multiply probabilities. In a loglinear mixture, a translation candidate t for a phrase s will only be scored highly if all components agree that it is highly probable. In contrast, in a linear mixture, t can be a top translation candidate overall even if it is not a preferred translation in some of the components. When the training data is very heterogeneous, linear mixtures are therefore preferable. 500 Previous work provides empirical evidence supporting this. For instance, Foster et al. (2010) found that linear mixtures outperform log linear mixtures when adapting a French-English system to the medical domain, as well as on a ChineseEnglish NIST translation task. 2.4 Estimating Conditional Translation Probabilities Within each mixture component, we extract all phrase-pairs, compute relative frequencies, and use Kneser-Ney smoothing (Chen et al., 2011) to produce the final estimate of conditional translation probabilities pk(t s). Per-component probabilities are then combined in Eq. 1 and 3. Similarly, baseline translation probabilities are learned using Kneser-Ney smoothed frequenc</context>
<context position="9689" citStr="Foster et al. (2010)" startWordPosition="1488" endWordPosition="1491">eneous domains using text clustering algorithms, (3) random partitioning, (4) sampling with replacement. 3.1 Manually Defined Domains Heterogeneous training data is usually grouped into domains manually using provenance information. In most previous work, such domain distinctions are very clear and easy to define. For instance, Haddow (2013) uses European parliament proceedings to improve translation of text in the movie subtitles and News Commentary domains; Sennrich (2012a) aims to translate Alpine Club reports using components trained on European parliament proceedings and movie subtitles. Foster et al. (2010) work with a slightly different setting when defining mixture components for the NIST Chinese-English translation task: while there is no single obvious “in-domain” component in the NIST training set, homogeneous domains can still be defined in a straightforward fashion based on the provenance of the data (e.g., Hong Kong Hansards vs. Hong Kong Law vs. News articles from FBIS, etc.). We take a similar approach in our experiments. However, we will see that since our training data is very heterogeneous, we take into account other dimensions beyond provenance, such as genre and dialect informatio</context>
<context position="20730" citStr="Foster et al., 2010" startWordPosition="3305" endWordPosition="3308"> maximum likelkihood mixtures when appropriate. 5 Findings: Impact on Translation Quality 5.1 Linear vs. Loglinear Mixtures Before focusing exclusively on linear mixtures, we confirm that they outperform loglinear mixtures. This comparison was conducted on the webforum domain, using manually defined domains as components. For linear mixtures, we trained the weights using maximum likelihood. Loglinear mixture weights are trained by MIRA. Table 3 shows that linear mixtures yield consistently and significantly higher BLEU scores than loglinear mixtures, which is consistent with existing results (Foster et al., 2010, inter alia). 5.2 Impact of Mixture Components We now focus on linear mixtures and measure the impact on translation quality of the various component types described in Section 3. In all cases, mixtures weights are estimated by maximum likelihood. Results are summarized in Table 4 for both Arabic and Chinese. The main result is that all mixture models considered significantly improve on the “no mix” baseline for both languages. Directly using the 101 basic elements for Chinese and the 82 basic elements for Arabic significantly improves on the baseline. Grouping the basic elements into coarser</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Boxing Chen</author>
<author>Roland Kuhn</author>
</authors>
<title>Simulating discriminative training for linear mixture adaptation in statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the XIV Machine Translation Summit,</booktitle>
<pages>183--190</pages>
<contexts>
<context position="35285" citStr="Foster et al., 2013" startWordPosition="5683" endWordPosition="5686">ceedings and movie subtitles. We address training conditions where the dimensions for organizing the training data are not as clear-cut, and show that partitions that do not attempt to mimick domain distinctions can improve translation quality. It would be interesting to see whether our conclusion holds in these more artificial training settings, and whether sentence-level corpus organization could help translation quality in our settings. Finally, recent work shows that linear mixture weights can be optimized for BLEU, either directly (Haddow, 2013), or by simulating discriminative training (Foster et al., 2013). In this paper, we limited our studies to maximum likelihood and uniform mixtures, however, the various mixture component definitions proposed here can also be applied when maximizing BLEU. 8 Conclusion We have presented an extensive study of linear mixtures for training translation models on very heterogeneous data on Arabic-English and Chinese-English translation tasks. In addition, we evaluated the robustness of our models across two distinct domains on the Arabic-English task. Our results show that linear mixtures reliably and robustly improve the quality of machine translation. Improveme</context>
</contexts>
<marker>Foster, Chen, Kuhn, 2013</marker>
<rawString>George Foster, Boxing Chen, and Roland Kuhn. 2013. Simulating discriminative training for linear mixture adaptation in statistical machine translation. In Proceedings of the XIV Machine Translation Summit, pages 183–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Christopher D Manning</author>
</authors>
<title>A simple and effective hierarchical phrase reordering model.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>848--856</pages>
<contexts>
<context position="17331" citStr="Galley and Manning, 2008" startWordPosition="2755" endWordPosition="2758">webforum data is presumably the closest to the test domain, but Egyptian weblogs or mixed-dialect broadcast conversations could potentially be useful as well. We also test the Arabic and Chinese systems on the news domain. The goal of these experiments is to evaluate the robustness of linear mixtures across different test domains. We use publicly available test sets from the NIST evaluation. The dev set used to learn maximum likelihood mixtures and tune the translation system is the NIST section of the 2006 test set. We evaluate system performan • 6 hierarchical lexicalized reordering scores (Galley and Manning, 2008) • a word penalty, and aword-displacement distortion penalty • aGood-Turing smoothed 4-gram language model trained on the Gigaword corpus, Kneser-Ney smoothed 5-gram models trained on the English side of the training corpus, and an additional 5-gram model trained on monolingual webforum data. Weights for these features are learned using a batch version of the MIRA algorithm (Chiang, 2012). Phrase pairs are extracted from several word alignments of the training set: HMM, IBM2, and IBM4. Word alignments are kept constant across all experiments. We apply our linear mixture models to both translat</context>
</contexts>
<marker>Galley, Manning, 2008</marker>
<rawString>Michel Galley and Christopher D. Manning. 2008. A simple and effective hierarchical phrase reordering model. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 848–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
</authors>
<title>Analysing the effect of out-of-domain data on SMT systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>422--432</pages>
<contexts>
<context position="33360" citStr="Haddow and Koehn, 2012" startWordPosition="5366" endWordPosition="5370">the linear mixtures result in very different translation probability distributions than global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many others). We focused instead on a different lessstudied question: how can we leverage training data drawn from a wide variety of sources, genres, time periods, to translate a domain represented by a small development set? Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a singl</context>
</contexts>
<marker>Haddow, Koehn, 2012</marker>
<rawString>Barry Haddow and Philipp Koehn. 2012. Analysing the effect of out-of-domain data on SMT systems. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 422–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barry Haddow</author>
</authors>
<title>Applying pairwise ranked optimisation to improve the interpolation of translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>342--347</pages>
<contexts>
<context position="9412" citStr="Haddow (2013)" startWordPosition="1448" endWordPosition="1449">are the same provenance, genre and dialect, as we will see in Section 4. We consider four very different ways of defining mixture components by grouping the basic corpus elements: (1) manual partition of the training corpus into domains, (2) automatically learning homogeneous domains using text clustering algorithms, (3) random partitioning, (4) sampling with replacement. 3.1 Manually Defined Domains Heterogeneous training data is usually grouped into domains manually using provenance information. In most previous work, such domain distinctions are very clear and easy to define. For instance, Haddow (2013) uses European parliament proceedings to improve translation of text in the movie subtitles and News Commentary domains; Sennrich (2012a) aims to translate Alpine Club reports using components trained on European parliament proceedings and movie subtitles. Foster et al. (2010) work with a slightly different setting when defining mixture components for the NIST Chinese-English translation task: while there is no single obvious “in-domain” component in the NIST training set, homogeneous domains can still be defined in a straightforward fashion based on the provenance of the data (e.g., Hong Kong</context>
<context position="35221" citStr="Haddow, 2013" startWordPosition="5675" endWordPosition="5676">out of domain data sets comprised European parliament proceedings and movie subtitles. We address training conditions where the dimensions for organizing the training data are not as clear-cut, and show that partitions that do not attempt to mimick domain distinctions can improve translation quality. It would be interesting to see whether our conclusion holds in these more artificial training settings, and whether sentence-level corpus organization could help translation quality in our settings. Finally, recent work shows that linear mixture weights can be optimized for BLEU, either directly (Haddow, 2013), or by simulating discriminative training (Foster et al., 2013). In this paper, we limited our studies to maximum likelihood and uniform mixtures, however, the various mixture component definitions proposed here can also be applied when maximizing BLEU. 8 Conclusion We have presented an extensive study of linear mixtures for training translation models on very heterogeneous data on Arabic-English and Chinese-English translation tasks. In addition, we evaluated the robustness of our models across two distinct domains on the Arabic-English task. Our results show that linear mixtures reliably an</context>
</contexts>
<marker>Haddow, 2013</marker>
<rawString>Barry Haddow. 2013. Applying pairwise ranked optimisation to improve the interpolation of translation models. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 342–347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Matthias Eck</author>
<author>Stephan Vogel</author>
<author>Alex Waibel</author>
</authors>
<title>Adaptation of the translation model for statistical machine translation based on information retrieval.</title>
<date>2005</date>
<booktitle>In European Association for Machine Translation.</booktitle>
<contexts>
<context position="34267" citStr="Hildebrand et al., 2005" startWordPosition="5524" endWordPosition="5527">opment set? Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 get domain was Alpine Club reports, while out of domain data sets comprised European parliament proceedings and movie subtitles. We address training conditions where the dimensions for organizing the training data are not as clear-cut, and show that partitions that do not attempt to mimick domain dis</context>
</contexts>
<marker>Hildebrand, Eck, Vogel, Waibel, 2005</marker>
<rawString>Almut Silja Hildebrand, Matthias Eck, Stephan Vogel, and Alex Waibel. 2005. Adaptation of the translation model for statistical machine translation based on information retrieval. In European Association for Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Josh Schroeder</author>
</authors>
<title>Experiments in Domain Adaptation for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="1785" citStr="Koehn and Schroeder, 2007" startWordPosition="263" endWordPosition="266">ion While machine translation tasks used to be defined by drawing training and test data from a single well-defined domain, current systems have to deal with increasingly heterogeneous data, both at training and at test time. As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train statistical machine translation (SMT) systems that achieve good translation quality on various test domains? So far, this challenge has been addressed by repurposing techniques developed for more clear-cut domain adaptation scenarios, such as linear mixture models (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012b). Instead of estimating models on the whole training corpus at once, linear mixture models are built as follows: (1) partition the training corpus into homogeneous domain-based component, (2) train one model per component, (3) linearly mix models using weights learned to adapt to the test domain, (4) replace resulting model in translation system. In this paper, we aim to gain a better understanding of the benefits of linear mixture models in heterogeneous data conditions, by examining key untested assumptions: • Should mixture component capture domain i</context>
<context position="33273" citStr="Koehn and Schroeder, 2007" startWordPosition="5350" endWordPosition="5353">e 3: random partitions yield lower divergences for very frequent source phrases. Overall, the linear mixtures result in very different translation probability distributions than global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many others). We focused instead on a different lessstudied question: how can we leverage training data drawn from a wide variety of sources, genres, time periods, to translate a domain represented by a small development set? Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007</context>
</contexts>
<marker>Koehn, Schroeder, 2007</marker>
<rawString>Philipp Koehn and Josh Schroeder. 2007. Experiments in Domain Adaptation for Statistical Machine Translation. In Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session.</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="18679" citStr="Koehn et al., 2007" startWordPosition="2969" endWordPosition="2972">glish (top) 4.2 Definition of Mixture Components Manual partitions were created first by the system developers, based on intuitions on the nature of the test domain and manual inspection of the training data. The main goal was to group data into components that are large enough to reliably estimate translation probabilities, but small enough to be homogeneous. This resulted in Km = 10 clusters for Arabic, and 4.3 Test Domains We consider two test domains, as described in Ta4.4 Machine Translation System We use an in-house implementation of a Phrasebased Statistical Machine Translation system (Koehn et al., 2007) to build strong baseline systems for both language pairs. Translation hypotheses are scored according to the following features: • 4 phrase-table scores: Kneser-Ney smoothed phrasal translation probabilites and lexical weights, in both translation directions (Chen et al., Arabic-English system uses 6 additional binary features which fire if a phrase-pair was generated by one of the 3 word alignment methods in each tran 2The slation direction. 0 20 40 60 80 #lines / #words 1e+01 1e+05 0 000 000000000 000 0 00000000000000000000000 0 000000000000 0 0 0 00000 0 0 0 0 #lines #words 0 0 0 0 20 40 6</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Annual Meeting of the Association for Computational Linguistics (ACL), demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the shannon entropy.</title>
<date>1991</date>
<journal>IEEE Trans. Inf. Theor.,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="30054" citStr="Lin, 1991" startWordPosition="4814" endWordPosition="4815">en the impact of Kneser-Ney smoothing and linear mixtures, since one can expect that the translation distributions will diverge more from global relative frequencies for rare phrases than for frequent phrases. However, the discounting / down-scaling effects are controlled by very different parameters in linear mixtures than in Kneser-New smoothing. In order to better understand these differences in practice, an empirical analysis is required. 6.3 Empirical Comparison How do linear mixtures and smoothing affect translation probabilities p(t|s) in practice? We use the Jensen-Shannon divergence (Lin, 1991) to quantify the distance between (a) various mixture model estimates and (b) the global smoothed relative frequency estimates used in our baseline “no mix” experiments. In addition, we also compare the Kneser-Ney smoothed translation probabilities with unsmoothed relative frequencies, in order to highlight the difference between standard smoothing techniques and linear mixture models. Figures 2 and 3 show the distributions of divergence values by source phrase frequencies for Chinese-English phrase-tables. The divergence from the global estimate is the largest for rare phrases in all cases, a</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on the shannon entropy. IEEE Trans. Inf. Theor., 37(1):145–151, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Mixture-modeling with unsupervised clusters for domain adaptation in statistical machine translation.</title>
<date>2012</date>
<booktitle>In 16th Conference of the European Association for Machine Translation (EAMT).</booktitle>
<contexts>
<context position="1824" citStr="Sennrich, 2012" startWordPosition="271" endWordPosition="272">ined by drawing training and test data from a single well-defined domain, current systems have to deal with increasingly heterogeneous data, both at training and at test time. As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train statistical machine translation (SMT) systems that achieve good translation quality on various test domains? So far, this challenge has been addressed by repurposing techniques developed for more clear-cut domain adaptation scenarios, such as linear mixture models (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012b). Instead of estimating models on the whole training corpus at once, linear mixture models are built as follows: (1) partition the training corpus into homogeneous domain-based component, (2) train one model per component, (3) linearly mix models using weights learned to adapt to the test domain, (4) replace resulting model in translation system. In this paper, we aim to gain a better understanding of the benefits of linear mixture models in heterogeneous data conditions, by examining key untested assumptions: • Should mixture component capture domain information? Previous work assumes that </context>
<context position="8534" citStr="Sennrich (2012" startWordPosition="1317" endWordPosition="1318">nd use Kneser-Ney smoothing (Chen et al., 2011) to produce the final estimate of conditional translation probabilities pk(t s). Per-component probabilities are then combined in Eq. 1 and 3. Similarly, baseline translation probabilities are learned using Kneser-Ney smoothed frequencies collected on the entire training set. 3 Defining Mixture Components We assume that the heterogeneous training corpus can be split into basic elements that will be organized in various ways to define the K mixture components. Basic components could be documents or sets of sentences defined along various criteria. Sennrich (2012a) show that using isolated sentences as basic elements might not provide sufficient information, as smoothing component assignments using neighboring sentences benefits translation quality. In our experiments, basic elements are sets of parallel sentences which share the same provenance, genre and dialect, as we will see in Section 4. We consider four very different ways of defining mixture components by grouping the basic corpus elements: (1) manual partition of the training corpus into domains, (2) automatically learning homogeneous domains using text clustering algorithms, (3) random parti</context>
<context position="33376" citStr="Sennrich, 2012" startWordPosition="5371" endWordPosition="5372">lt in very different translation probability distributions than global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many others). We focused instead on a different lessstudied question: how can we leverage training data drawn from a wide variety of sources, genres, time periods, to translate a domain represented by a small development set? Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at tes</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012a. Mixture-modeling with unsupervised clusters for domain adaptation in statistical machine translation. In 16th Conference of the European Association for Machine Translation (EAMT).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rico Sennrich</author>
</authors>
<title>Perplexity minimization for translation model adaptation in statistical machine tra.</title>
<date>2012</date>
<booktitle>In Thirteenth Conference of the European Chapter of the Association for Computational Linguistics (EACL).</booktitle>
<contexts>
<context position="1824" citStr="Sennrich, 2012" startWordPosition="271" endWordPosition="272">ined by drawing training and test data from a single well-defined domain, current systems have to deal with increasingly heterogeneous data, both at training and at test time. As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train statistical machine translation (SMT) systems that achieve good translation quality on various test domains? So far, this challenge has been addressed by repurposing techniques developed for more clear-cut domain adaptation scenarios, such as linear mixture models (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012b). Instead of estimating models on the whole training corpus at once, linear mixture models are built as follows: (1) partition the training corpus into homogeneous domain-based component, (2) train one model per component, (3) linearly mix models using weights learned to adapt to the test domain, (4) replace resulting model in translation system. In this paper, we aim to gain a better understanding of the benefits of linear mixture models in heterogeneous data conditions, by examining key untested assumptions: • Should mixture component capture domain information? Previous work assumes that </context>
<context position="8534" citStr="Sennrich (2012" startWordPosition="1317" endWordPosition="1318">nd use Kneser-Ney smoothing (Chen et al., 2011) to produce the final estimate of conditional translation probabilities pk(t s). Per-component probabilities are then combined in Eq. 1 and 3. Similarly, baseline translation probabilities are learned using Kneser-Ney smoothed frequencies collected on the entire training set. 3 Defining Mixture Components We assume that the heterogeneous training corpus can be split into basic elements that will be organized in various ways to define the K mixture components. Basic components could be documents or sets of sentences defined along various criteria. Sennrich (2012a) show that using isolated sentences as basic elements might not provide sufficient information, as smoothing component assignments using neighboring sentences benefits translation quality. In our experiments, basic elements are sets of parallel sentences which share the same provenance, genre and dialect, as we will see in Section 4. We consider four very different ways of defining mixture components by grouping the basic corpus elements: (1) manual partition of the training corpus into domains, (2) automatically learning homogeneous domains using text clustering algorithms, (3) random parti</context>
<context position="33376" citStr="Sennrich, 2012" startWordPosition="5371" endWordPosition="5372">lt in very different translation probability distributions than global estimates, including smoothed estimates. This suggests that standard smoothing techniques can be improved when learning from heterogeneous training data, and that mixture components are beneficial even when they do not explicitly capture domain distinctions. 7 Related work Most previous work on domain adaptation in machine translation presupposes a clear-cut distinction between in-domain and out-of-domain data (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many others). We focused instead on a different lessstudied question: how can we leverage training data drawn from a wide variety of sources, genres, time periods, to translate a domain represented by a small development set? Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at tes</context>
</contexts>
<marker>Sennrich, 2012</marker>
<rawString>Rico Sennrich. 2012b. Perplexity minimization for translation model adaptation in statistical machine tra. In Thirteenth Conference of the European Chapter of the Association for Computational Linguistics (EACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Wang</author>
<author>Klaus Macherey</author>
<author>Wolfgang Macherey</author>
<author>Franz Och</author>
<author>Peng Xu</author>
</authors>
<title>Improved domain adaptation for statistical machine translation.</title>
<date>2012</date>
<booktitle>In 10th biennial conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<contexts>
<context position="34002" citStr="Wang et al. (2012)" startWordPosition="5478" endWordPosition="5481">w and Koehn, 2012; Clark et al., 2012, among many others). We focused instead on a different lessstudied question: how can we leverage training data drawn from a wide variety of sources, genres, time periods, to translate a domain represented by a small development set? Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous corpus obtained by concatenating data from a small number of very distant domains. The tar507 get domain was Alpine Club reports, </context>
</contexts>
<marker>Wang, Macherey, Macherey, Och, Xu, 2012</marker>
<rawString>Wei Wang, Klaus Macherey, Wolfgang Macherey, Franz Och, and Peng Xu. 2012. Improved domain adaptation for statistical machine translation. In 10th biennial conference of the Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hirofumi Yamamoto</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Bilingual cluster based models for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>514--523</pages>
<contexts>
<context position="33874" citStr="Yamamoto and Sumita (2007)" startWordPosition="5457" endWordPosition="5460">Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Duh et al., 2010; Bisazza et al., 2011; Haddow and Koehn, 2012; Sennrich, 2012b; Haddow and Koehn, 2012; Clark et al., 2012, among many others). We focused instead on a different lessstudied question: how can we leverage training data drawn from a wide variety of sources, genres, time periods, to translate a domain represented by a small development set? Many approaches focus on mapping the test domain to a single subset of the training data. In contrast, we show that the test domain can be flexibly represented by a mixture of many components. Yamamoto and Sumita (2007) cluster the parallel data using bilingual representations, and assign data to a single cluster at test time. Wang et al. (2012) show how to detect a known domain at test time in order to configure a generic translation system with domain-specific feature weights. Others select a subset of training data that is relevant to the test domain, using e.g., IR techniques (Hildebrand et al., 2005) or language model crossentropy (Axelrod et al., 2011). Closer to this work, Sennrich (2012a) proposes a sentence-level clustering approach to automatically recover domain distinctions in a heteorogeneous co</context>
</contexts>
<marker>Yamamoto, Sumita, 2007</marker>
<rawString>Hirofumi Yamamoto and Eiichiro Sumita. 2007. Bilingual cluster based models for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 514–523.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.122553">
<title confidence="0.981928">
NTHU at the CoNLL-2014 Shared Task
</title>
<author confidence="0.997308">
Jian-Cheng Wu*, Tzu-Hsi Yen*, Jim Chang*, Guan-Cheng Huang*,
Jimmy Chang*, Hsiang-Ling Hsu+, Yu-Wei Chang+, Jason S. Chang*
</author>
<affiliation confidence="0.991204333333333">
* Department of Computer Science
+ Institute of Information Systems and Applications
National Tsing Hua University
</affiliation>
<address confidence="0.9465725">
HsinChu, Taiwan, R.O.C. 30013
{wujc86, joseph.yen, cwebb.tw, a2961353,
</address>
<email confidence="0.97552">
rocmewtwo, ilovecat6717, teer1990, jason.jschang}@gmail.com
</email>
<sectionHeader confidence="0.995234" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999934818181818">
In this paper, we describe a system for cor-
recting grammatical errors in texts written
by non-native learners. In our approach, a
given sentence with syntactic features are
sent to a number of modules, each focuses
on a specific error type. A main program
integrates corrections from these modules
and outputs the corrected sentence. We
evaluated our system on the official test
data of the CoNLL-2014 shared task and
obtained 0.30 in F-measure.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.994604294117647">
Millions of non-native learners are using English
as their second language (ESL) or foreign lan-
guage (EFL). These learners often make different
kinds of grammatical errors and are not aware of
it. With a grammatical error corrector applies rules
or statistical learning methods, learners can use the
system to improve the quality of writing, and be-
come more aware of the common errors. It may
also help learners improve their writing skills.
The CoNLL-2014 shared task is aimed at pro-
moting research on correcting grammatical errors.
Types of errors handled in the shared task are ex-
tended from the five types in the previous shared
task to include all common errors present in an es-
say.
In this paper, we focus on the following errors
made by ESL writers:
</bodyText>
<listItem confidence="0.999975285714286">
• Spelling and comma
• Article and determiner
• Preposition
• Preposition + verb (interactive)
• Noun number
• Word form
• Subject-verb-agreement
</listItem>
<bodyText confidence="0.999882">
For each error type, we developed and tuned a
module based on the official development data. A
main program combines the correction hypotheses
from these modules and produces the final correc-
tion. If multiple modules propose different cor-
rections to the same word/phrase, the correction
proposed by the module with the highest precision
will be chosen.
</bodyText>
<sectionHeader confidence="0.991504" genericHeader="introduction">
2 Method
</sectionHeader>
<subsectionHeader confidence="0.999828">
2.1 Spelling and Comma module
</subsectionHeader>
<bodyText confidence="0.9996907">
In this section, we correct comma errors and
spelling errors, including missing/extraneous hy-
phens. For simplicity, we adopt Aspell1 and
GingerIt2 to detect spelling errors and generate
possible replacements, considered as confusable
words, which might contain the word with cor-
rect spelling. Then, we replace the word being
checked with confusable words to generate sen-
tences. Language models trained on well-formed
texts are used to measure the probability of these
</bodyText>
<footnote confidence="0.9981295">
1http://aspell.net/
2https://pypi.python.org/pypi/gingerit
We use GingerIt only for correcting missing/extraneous
hyphens
</footnote>
<page confidence="0.987741">
91
</page>
<note confidence="0.4763945">
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 91–95,
Baltimore, Maryland, 26-27 July 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998731833333333">
candidates. Candidate with the highest probability
is chosen as correction.
Omitted commas form a large proportion of
punctuation errors. We apply the CRF model pro-
posed by Israel, et. al. (2012) with some mod-
ification. We replace distance features with syn-
tactic features. More specifically, we do not use
features such as distances to the start of sentence
or last comma. And we add two features, one in-
dicates whether a word is at the end of a clause,
and the other indicates whether the current clause
starts with a prepositional phrase.
</bodyText>
<subsectionHeader confidence="0.999255">
2.2 Subject-verb-agreement module
</subsectionHeader>
<bodyText confidence="0.999591612903226">
This module corrects subject-verb-agreement er-
rors in a given sentence. Consider the sentence
”The boy in blue pants are my brother”. The cor-
rect sentence should be ”The boy in blue pants is
my brother”. Since a verb could be far from it’s
subject, using ngram counts may fail to detect and
correct such an error.
We use a rule-based method in this module.
In the first stage, we identify the subject of each
clause by using information from the parser. Both
the dependency relation and syntactic structure are
used in this stage. Dependency relations such as
nsubj and rcmod indicate subjects of subject-
verb relation. If there is a verb that has not been
assigned a subject via dependency relations, head
of noun phrase in the same clause will be used in-
stead. And in the second stage, we check whether
subject and verbs agree, for each clause in the sen-
tence.
Here we explain our correction process in more
detail. For each clause, the singular and plural
forms of verbs in the clause must be consistent
with the subject of the clause unless the subject
is a quantifier. Consider the following sentences:
The number of cats is ten.
A number of cats are playing.
Since our judgment only depends on the subject
number, it’s hard to tell whether should we use
a plural verb or not in this case. The quantifiers
we do not handle are listed as follow: number, lot,
quantity, variety, type, amount, neither.
</bodyText>
<subsectionHeader confidence="0.998936">
2.3 Number module and Forms module
</subsectionHeader>
<bodyText confidence="0.999667717948718">
We correct noun number error in two stages. In
the first stage, we generate a confusion set for each
word. While constructing confusion set for noun
number, both of the singular form and plural form
are included in the set. While constructing con-
fusion set for word forms, we use the word fam-
ilies in Academic Word List (AWL)3 and British
National Corpus (BNC4000) 4. Given a content
word, all the words in the same family except
antonyms are entered into the confusion set. How-
ever, comparative form and superlative form of an
adjective are eliminated from the confusion set,
since replacing an adjective with these forms is a
semantic rather than syntactic decision. The fol-
lowing examples illustrate what kinds of alterna-
tives are eliminated:
antonyms: misleading for the word lead
semantics: higher for the word high
Additionally, in the forms module, a correc-
tion is ignored, if it is actually correcting a verb
tense, subject-verb-agreement, or noun number er-
ror. We use part-of-speech (POS) tag to check this.
More specifically, any corrections that changes a
word with a VBZ tag to a word with a VBP or
VBD tag is ignored, and vice versa. And any cor-
rections that switches a noun between it’s singular
form and plural form is also ignored.
With the confusion sets, we proceed to the
second stage. In this stage, we use words in
the confusion set to attempt to replace potential
errors. Language models trained on well-formed
text are used to validate the replacement decisions.
Given a word w, If there is an alternative w’ that
fits in the context better, w is flagged as an error
and w’ is returned as a correction.
Here is our formula for correcting errors
w in the original sentence O with alternatives and
generate candidates C. We then generate the can-
didate R with the highest probability among all
</bodyText>
<footnote confidence="0.96647275">
3http://www.victoria.ac.nz/lals/
resources/academicwordlist/sublists
4http://simple.wiktionary.org/wiki/
Wiktionary:BNC_spoken_freq
</footnote>
<equation confidence="0.97189525">
2
P (R) = Pngram(R) + Prnn(R)
2
Promotion = P(R) − P(O)
|O|
While checking a content word w, we replace
Pngram(O) + Prnn(O)
P (O) =
</equation>
<page confidence="0.837802">
92
</page>
<bodyText confidence="0.998814888888889">
candidates. We use an interpolated probability5 of
ngram language model P,,,gra,,t and recurrent neu-
ral network language model Pr,,,,,,. Promotion in-
dicates the increase in probability per word after
we replace sentence O with the candidate R. We
use word number to normalize Promotion follow-
ing Dahlmeier, et al. (2012). Corrections are made
only if Promotion is higher than a empirically de-
termined threshold.6
</bodyText>
<subsectionHeader confidence="0.994134">
2.4 Article and Determiner module
</subsectionHeader>
<bodyText confidence="0.99993628125">
In this subsection, we describe how we correct er-
rors of omitting a determiner or adding a spuri-
ous determiner. The language models mentioned
in the last subsection are also used in this module.
We tune our thresholds for making corrections on
development data7, and found that deleting a de-
terminer should have a lower threshold while in-
serting one should have a higher one, so we set
different thresholds accordingly. 8
To cope with the situation where a deter-
miner/article is far ahead of the head of the noun
phrase, we apply another constraint while making
correction decision.
First, we calculate statistics on the head of noun
phrases. We extract the most frequent 100,000
terms in Web-1T 5-gram corpus. These terms are
used to search their definitions in Wikipedia (usu-
ally at the first paragraph). The characteristic of a
definition is that it has no prior context and most
of the noun phrases with a determiner are unique
or known to the general public. Heads of these
nouns phrases are likely to always appear with a
determiner.
Heads that tend to appear with a determiner
the help us to decide whether a determiner should
be added to a noun phrase. We add a determiner
using two constraints. We only insert a determiner
or an article, if the statistics indicate that head of
a noun phrase tends to have a determiner, or the
promotion of log probability is much higher than
the threshold. A similar constraint is also applied,
for deleting a determiner or an article.
</bodyText>
<footnote confidence="0.997981142857143">
5the probabilities present in the formula are log probabil-
ities
6the threshold for noun number module is 0.035 and 0.050
for word form module. These threshold were set empirically
after testing on development data
7test data of the CoNLL-2013 shared task
8the threshold for insertion is 0.035 and 0.040 for deletion
</footnote>
<subsectionHeader confidence="0.98286">
2.5 Preposition module
</subsectionHeader>
<bodyText confidence="0.999990630434783">
For preposition errors, we focus on handling two
types of errors: REPLACE and DELETE. A
preposition, which should be deleted from the
given sentence, is regarded as a DELETE error,
whereas for a preposition, which should be re-
placed with a more appropriate alternative, is re-
garded as a REPLACE error. In this work, we
correct the two types of errors based on the as-
sumption that the usage of preposition often de-
pends on the collocation relations with a verb or
noun. Therefore, we use the dependency relations
such as dobj and pobj, and prep to identify
the related words, and then we validate the usage
of prepositions, and correct the preposition errors.
A dependency-based model is proposed in this
paper to handle the preposition errors. The model
consists of two stages: detecting the possible
preposition errors and correcting the errors.
In the first stage, we use the Stanford depen-
dency parser (Klein and Manning, 2003) to extract
the dependency relations for each preposition. The
relation tuples, which contain the preposition, verb
or noun, and prepositional object. For example,
the tuple of verb-prep-object (listen, to, music),
or the tuple of noun-prep-object (point, of, view)
are extracted for validation. We identify a preposi-
tion containing as an error, if the tuple containing
the preposition does not occur in a reference list
built using a reference corpus. In order to resolve
the data sparseness and false alarm problems, we
need a sufficiently large list of validated tuples.
In this study, the reference tuple lists are gener-
ated from the Google Books Syntactic N-grams
(Goldberg and Orwant, 2013)9. For example, we
can find (come, to, end, 236864) and (lead, to, re-
sult, 57632) in the verb-preposition-object refer-
ence list. We have generated 21,773,752 different
dependency tuples for our purpose.
In the second stage, we attempt to correct all
potential preposition errors. At first, a list of can-
didate tuples is generated by substituting the orig-
inal preposition in the error tuple with alterna-
tive prepositions. For example, the generated can-
didate tuples for the error tuple (join, at, party)
will include (join, in, party), (join, on, party), etc.
On the other hand, the tuple, (join, party), which
</bodyText>
<footnote confidence="0.929964">
9Data sets available from http://
commondatastorage.googleapis.com/books/
syntactic-ngrams/index.html
</footnote>
<page confidence="0.998531">
93
</page>
<bodyText confidence="0.999349636363636">
deletes the preposition, is also taken into consid-
eration. All candidates are ranked according to
the frequency provided by the reference lists. The
preposition in the tuple with the highest frequency
is returned as the correction suggestion.
rial preposition-verb errors via translation. Eval-
uation on a set of sentences in a learner corpus
shows that the method corrects serial errors rea-
sonably well.
Consider an error sentence ”I have difficulty to
understand English.” The correct sentence for this
should be ”I have difficulty in understanding En-
glish.” It is hard to correct these two errors one by
one, since the errors are dependent on each other.
Intuitively, by identifying difficulty to understand
as containing serial errors and correct it to diffi-
culty in understanding, we can handle this kind of
problem more effectively.
First, we generate translation phrase table as
follows. We begin by selecting trigrams related to
serial errors and correction from Web 1T 5-gram.
Figure 1 shows some sample annotation trigrams.
Then, we group the selected trigrams by the first
and last word in the trigrams. See Figure 2 for a
sample VPV group of trigrams. Finally, we gener-
ate translation phrase table for each group. Figure
3 shows a sample translation phrase table.
At run time, we tag the input sentence with part
of speech information in order to find trigrams
that fit the type of serial errors. Then, we search
phrase table and generate translations for the
input phrases in a machine translation decoder to
produce a corrected sentence.
</bodyText>
<figureCaption confidence="0.9872265">
Figure 1: Sample annotated trigrams
Figure 2: Sample trigram group
Figure 3: Sample phrase translation for a trigram
group
</figureCaption>
<subsectionHeader confidence="0.93618">
2.6 Interactive errors module
</subsectionHeader>
<bodyText confidence="0.999916625">
This module uses a new method for correcting
serial grammatical errors in a given sentence in
learners writing. A statistical translation model is
generated to attempt to translate the input with se-
rial and interactive errors into a grammatical sen-
tence. The method involves automatically learn-
ing translation models based on Web-scale n-
gram. The model corrects trigrams containing se-
</bodyText>
<sectionHeader confidence="0.997365" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.998737222222222">
Two types of trigram language models, ngram
model and recurrent neural network (RNN) model,
are used in correcting spelling, noun number, word
form, and determiner errors. We trained the ngram
language model on English Gigaword and BNC
corpus, using the SRILM tool (Stolcke, 2002).
We train the RNN model with RNNLM toolkit
(Mikolov et al., 2011). Complexity of training the
RNN language model is much higher, so we train
it on a smaller corpus, the British National Corpus
(BNC).
We used the Stanford Parser (Klein and Christo-
pher D. Manning, 2003) to obtain dependency re-
lations in the preposition module, and to obtain
POS tags for the word form module. The subject-
verb-agreement module also uses dependency re-
lations contained in test data. Dependency rela-
tions in Google Books Syntactic N-grams (Gold-
</bodyText>
<page confidence="0.99784">
94
</page>
<bodyText confidence="0.9946132">
berg and Orwant, 2013) were also used to develop
our dendepency-based model in the preposition
module.
To assess the effectiveness of the proposed
method, we used the official training, develop-
ment, and test data of the CoNLL-2014 shared
task. On the test data, our system obtained the pre-
cision, recall and F0.5 score of 0.351, 0.189, and
0.299. The following table shows the performance
breakdown by module.
</bodyText>
<figureCaption confidence="0.973992">
Figure 4: The performance breakdown by module.
(Displayed in %)
</figureCaption>
<bodyText confidence="0.999650083333333">
In the spelling and hyphen module, candidates
from Aspell include words that only differ from
the original word in one character, s. Language
models are then used to choose the candidate with
highest probability as our correction. The module
therefore gives some corrections about noun num-
bers or subject-verb-agreement. As a result, some
corrections made by this module overlap with cor-
rections made by the noun numbers module and
the subject-verb-agreement module, which makes
the recall of correcting spelling and hyphen errors,
4.11%, overestimated.
</bodyText>
<sectionHeader confidence="0.99933" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999982769230769">
In this work, we have built several modules for er-
ror detection and correction. For different types
of errors, we developed modules independently
using different features and thresholds. If mul-
tiple modules propose different corrections to a
word/phrase, the one proposed by the module with
higher precision will be chosen. Many avenues
for future work present themselves. We plan to
integrate modules in a more flexible way. When
faced with different corrections made by different
modules, the decision would better be based on
the confidence of each correction with a uniform
standard, but not on the confidence of modules.
</bodyText>
<sectionHeader confidence="0.996557" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999932833333333">
We would like to acknowledge the funding sup-
ports from Delta Electronic Corp and National
Science Council (contract no: NSC 100-2511-S-
007-005-MY3), Taiwan. We are also thankful to
anonymous reviewers and the organizers of the
shared task.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977643902439">
Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng Ng
2012. NUS at the HOO 2012 Shared Task. In Pro-
ceedings of the Seventh Workshop on Building Ed-
ucational Applications Using NLP, Association for
Computational Linguistics, June 7.
Daniel Dahlmeier and Hwee Tou Ng, 2012. Better
Evaluation for Grammatical Error Correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL 2012).,568-672
Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu. 2013.
Building a Large Annotated Corpus of Learner En-
glish: The NUS Corpus of Learner English. In
Proceedings of the 8th Workshop on Innovative Use
of NLP for Building Educational Applications(BEA
2013)
Yoav Goldberg and Jon Orwant 2013. A dataset of
syntactic-ngrams over time from a very large cor-
pus of English books. In Proceedings of the Second
Joint Conference on Lexical and Computational Se-
mantics, Atlanta, GA, 2013.
Ross Israel, Joel Tetreault, and Martin Chodorow
2012. Correcting Comma Errors in Learner Essays,
and Restoring Commas in Newswire Text. In Pro-
ceeding of the 2012 Conference of the North Amer-
ica Chapter of the Association for Computational
Linguistics: Human Language Technologies,284-
294, Montreal Canada, June. Association for Com-
putational Linguistics
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, 423-430.
Tomas Mikolov, Anoop Deoras, Dan Povey, Lukar
Burget, and Jan Honza Cernocky 2011. Strategies
for Training Large Scale Neural Network Language
Models Proceedings of ASRU 2011
Andreas Stolcke 2002. SRILM-An Extensible Lan-
guage Modeling Toolkit In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, vol 2, 901-904
</reference>
<page confidence="0.999076">
95
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.128158">
<title confidence="0.850037">NTHU at the CoNLL-2014 Shared Task</title>
<author confidence="0.7805865">Jian-Cheng Wu</author>
<author confidence="0.7805865">Tzu-Hsi Yen</author>
<author confidence="0.7805865">Jim Chang</author>
<author confidence="0.7805865">Guan-Cheng Jimmy Chang</author>
<author confidence="0.7805865">Hsiang-Ling Hsu</author>
<author confidence="0.7805865">Yu-Wei Chang</author>
<author confidence="0.7805865">Jason S Chang</author>
<affiliation confidence="0.942889">Department of Computer Science + Institute of Information Systems and Applications National Tsing Hua</affiliation>
<address confidence="0.857844">HsinChu, Taiwan, R.O.C.</address>
<email confidence="0.535075">joseph.yen,cwebb.tw,ilovecat6717,teer1990,</email>
<abstract confidence="0.9854365">In this paper, we describe a system for correcting grammatical errors in texts written by non-native learners. In our approach, a given sentence with syntactic features are sent to a number of modules, each focuses on a specific error type. A main program integrates corrections from these modules and outputs the corrected sentence. We evaluated our system on the official test data of the CoNLL-2014 shared task and obtained 0.30 in F-measure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
</authors>
<title>Hwee Tou Ng, and Eric Jun Feng Ng 2012. NUS at the HOO 2012 Shared Task.</title>
<date></date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, Association for Computational Linguistics,</booktitle>
<marker>Dahlmeier, </marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Eric Jun Feng Ng 2012. NUS at the HOO 2012 Shared Task. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, Association for Computational Linguistics, June 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better Evaluation for Grammatical Error Correction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL</booktitle>
<pages>2012--568</pages>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng, 2012. Better Evaluation for Grammatical Error Correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2012).,568-672</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications(BEA</booktitle>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, Siew Mei Wu. 2013. Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English. In Proceedings of the 8th Workshop on Innovative Use of NLP for Building Educational Applications(BEA 2013)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Jon Orwant</author>
</authors>
<title>A dataset of syntactic-ngrams over time from a very large corpus of English books.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics,</booktitle>
<location>Atlanta, GA,</location>
<contexts>
<context position="10948" citStr="Goldberg and Orwant, 2013" startWordPosition="1784" endWordPosition="1787">n tuples, which contain the preposition, verb or noun, and prepositional object. For example, the tuple of verb-prep-object (listen, to, music), or the tuple of noun-prep-object (point, of, view) are extracted for validation. We identify a preposition containing as an error, if the tuple containing the preposition does not occur in a reference list built using a reference corpus. In order to resolve the data sparseness and false alarm problems, we need a sufficiently large list of validated tuples. In this study, the reference tuple lists are generated from the Google Books Syntactic N-grams (Goldberg and Orwant, 2013)9. For example, we can find (come, to, end, 236864) and (lead, to, result, 57632) in the verb-preposition-object reference list. We have generated 21,773,752 different dependency tuples for our purpose. In the second stage, we attempt to correct all potential preposition errors. At first, a list of candidate tuples is generated by substituting the original preposition in the error tuple with alternative prepositions. For example, the generated candidate tuples for the error tuple (join, at, party) will include (join, in, party), (join, on, party), etc. On the other hand, the tuple, (join, part</context>
</contexts>
<marker>Goldberg, Orwant, 2013</marker>
<rawString>Yoav Goldberg and Jon Orwant 2013. A dataset of syntactic-ngrams over time from a very large corpus of English books. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics, Atlanta, GA, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Israel</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Correcting Comma Errors in Learner Essays, and Restoring Commas in Newswire Text.</title>
<date>2012</date>
<booktitle>In Proceeding of the 2012 Conference of the North America Chapter of the Association for Computational Linguistics: Human Language Technologies,284-294,</booktitle>
<publisher>Association for Computational Linguistics</publisher>
<location>Montreal Canada,</location>
<marker>Israel, Tetreault, Chodorow, 2012</marker>
<rawString>Ross Israel, Joel Tetreault, and Martin Chodorow 2012. Correcting Comma Errors in Learner Essays, and Restoring Commas in Newswire Text. In Proceeding of the 2012 Conference of the North America Chapter of the Association for Computational Linguistics: Human Language Technologies,284-294, Montreal Canada, June. Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="10252" citStr="Klein and Manning, 2003" startWordPosition="1675" endWordPosition="1678">his work, we correct the two types of errors based on the assumption that the usage of preposition often depends on the collocation relations with a verb or noun. Therefore, we use the dependency relations such as dobj and pobj, and prep to identify the related words, and then we validate the usage of prepositions, and correct the preposition errors. A dependency-based model is proposed in this paper to handle the preposition errors. The model consists of two stages: detecting the possible preposition errors and correcting the errors. In the first stage, we use the Stanford dependency parser (Klein and Manning, 2003) to extract the dependency relations for each preposition. The relation tuples, which contain the preposition, verb or noun, and prepositional object. For example, the tuple of verb-prep-object (listen, to, music), or the tuple of noun-prep-object (point, of, view) are extracted for validation. We identify a preposition containing as an error, if the tuple containing the preposition does not occur in a reference list built using a reference corpus. In order to resolve the data sparseness and false alarm problems, we need a sufficiently large list of validated tuples. In this study, the referen</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Anoop Deoras</author>
<author>Dan Povey</author>
<author>Lukar Burget</author>
<author>Jan Honza</author>
</authors>
<title>Cernocky 2011. Strategies for Training Large Scale Neural Network Language Models</title>
<date>2011</date>
<booktitle>Proceedings of ASRU</booktitle>
<contexts>
<context position="14110" citStr="Mikolov et al., 2011" startWordPosition="2285" endWordPosition="2288">cal translation model is generated to attempt to translate the input with serial and interactive errors into a grammatical sentence. The method involves automatically learning translation models based on Web-scale ngram. The model corrects trigrams containing se3 Experiment Two types of trigram language models, ngram model and recurrent neural network (RNN) model, are used in correcting spelling, noun number, word form, and determiner errors. We trained the ngram language model on English Gigaword and BNC corpus, using the SRILM tool (Stolcke, 2002). We train the RNN model with RNNLM toolkit (Mikolov et al., 2011). Complexity of training the RNN language model is much higher, so we train it on a smaller corpus, the British National Corpus (BNC). We used the Stanford Parser (Klein and Christopher D. Manning, 2003) to obtain dependency relations in the preposition module, and to obtain POS tags for the word form module. The subjectverb-agreement module also uses dependency relations contained in test data. Dependency relations in Google Books Syntactic N-grams (Gold94 berg and Orwant, 2013) were also used to develop our dendepency-based model in the preposition module. To assess the effectiveness of the </context>
</contexts>
<marker>Mikolov, Deoras, Povey, Burget, Honza, 2011</marker>
<rawString>Tomas Mikolov, Anoop Deoras, Dan Povey, Lukar Burget, and Jan Honza Cernocky 2011. Strategies for Training Large Scale Neural Network Language Models Proceedings of ASRU 2011</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM-An Extensible Language Modeling Toolkit</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<contexts>
<context position="14044" citStr="Stolcke, 2002" startWordPosition="2275" endWordPosition="2276"> errors in a given sentence in learners writing. A statistical translation model is generated to attempt to translate the input with serial and interactive errors into a grammatical sentence. The method involves automatically learning translation models based on Web-scale ngram. The model corrects trigrams containing se3 Experiment Two types of trigram language models, ngram model and recurrent neural network (RNN) model, are used in correcting spelling, noun number, word form, and determiner errors. We trained the ngram language model on English Gigaword and BNC corpus, using the SRILM tool (Stolcke, 2002). We train the RNN model with RNNLM toolkit (Mikolov et al., 2011). Complexity of training the RNN language model is much higher, so we train it on a smaller corpus, the British National Corpus (BNC). We used the Stanford Parser (Klein and Christopher D. Manning, 2003) to obtain dependency relations in the preposition module, and to obtain POS tags for the word form module. The subjectverb-agreement module also uses dependency relations contained in test data. Dependency relations in Google Books Syntactic N-grams (Gold94 berg and Orwant, 2013) were also used to develop our dendepency-based mo</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke 2002. SRILM-An Extensible Language Modeling Toolkit In Proceedings of the International Conference on Spoken Language Processing, vol 2, 901-904</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.046593">
<title confidence="0.9244835">
Chinese Spelling Error Detection and Correction Based on Language
Model, Pronunciation, and Shape
</title>
<note confidence="0.537986333333333">
Junjie Yu and Zhenghua Li
Provincial Key Laboratory for Computer Information Processing Technology
Soochow University, China
</note>
<email confidence="0.971329">
20144227010@stu.suda.edu.cn; zhli13@suda.edu.cn
</email>
<sectionHeader confidence="0.993118" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994464">
Spelling check is an important preprocessing
task when dealing with user generated texts such
as tweets and product comments. Compared
with some western languages such as English,
Chinese spelling check is more complex because
there is no word delimiter in Chinese written
texts and misspelled characters can only be
determined in word level. Our system works as
follows. First, we use character-level n-gram
language models to detect potential misspelled
characters with low probabilities below some
predefined threshold. Second, for each potential
incorrect character, we generate a candidate set
based on pronunciation and shape similarities.
Third, we filter some candidate corrections if the
candidate cannot form a legal word with its
neighbors according to a word dictionary.
Finally, we find the best candidate with highest
language model probability. If the probability is
higher than a predefined threshold, then we
replace the original character; or we consider the
original character as correct and take no action.
Our preliminary experiments shows that our
simple method can achieve relatively high
precision but low recall.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925777777778">
Spelling check is a traditional and important
preprocessing task for natural language
processing, since spelling errors happen in
written texts, such as short messages, emails,
and so on. Lots of research has been devoted to
English spelling error detection and correction.
In English spelling error detection and
correction, the errors can be classified into “non-
word” error and “real-word” error (Kukich,
1992). Unlike English, Chinese words are not
separated by space and all characters in Chinese
are “real-word”. Therefore, automatic word
segmentation need to be applied in order to
produce words (Zhang et al., 2000). There are
many Chinese input methods (Zhang et al.,
2005). Different input methods lead to different
types of spelling errors. For example, input
methods based on pinyin which usually lead to
spelling errors of characters sharing similar
pronunciations; while input methods based on
radical methods usually lead to errors related to
character shapes. Huang et al. (2007) proposed a
learning model based on Chinese phonemic
alphabet to detect Chinese spelling errors. Yeh
et al. (2013) presented a method based on N-
gram ranked inverted index list to deal with this
problem.
</bodyText>
<sectionHeader confidence="0.930724" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999912">
Our system includes two cascaded components:
spelling error detection and spelling error
correction, as shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.914534">
2.1 Resources
</subsectionHeader>
<bodyText confidence="0.999997">
To train our language mode, we use a portion of
Chinese Gigaword version 2.0 (LDC2009T14),
which contains about 12 million traditional
Chinese sentences. We do not split sentence into
words, but treat each character as an individual
unit. In other words, our language model is
based on character. In order to take advantage of
the context information, we train a new language
model by reversing all sentences in the corpus.
So, we will calculate twice for one character
based on this two language models. And the
total score is the combination of both.
As misspelled characters in a sentence can
only be detected in word level, we construct a
word dictionary which contains about 300
thousand words collected from Internet. And the
SIGHAN organizer provides a dictionary
including about 5000 Chinese characters with
other characters in similar pronunciation or
shape which can be used in candidate generation.
</bodyText>
<subsectionHeader confidence="0.999529">
2.2 Spelling Error Detection
</subsectionHeader>
<bodyText confidence="0.999961">
In spelling error detection phase, we propose
two methods to deal with this problem. One is to
gather the characters which get a low score
</bodyText>
<page confidence="0.99429">
220
</page>
<figureCaption confidence="0.999843">
Figure 1: Framework of our proposed system
</figureCaption>
<bodyText confidence="0.99997948">
under language model. Another is to record any
independent characters after automatic word
segmentation. However, we find both will bring
in lots of irrelevant characters though most
errors have been discovered. Because Chen et al.
(2011) find the average amount of errors in a
learners’ corpus for a student essay is only 2, we
do not want to mark too many error characters to
cause false-alarm problem heavily.
In order to make the best of the two methods,
we prepare two steps to combine both. Step 1,
we calculate the score of each character in a
sentence by a forward-backward 5-gram
language model. While the score is less than the
threshold, the character and its location are sent
to Step 2. To find as more errors as possible, we
set the threshold in a quite tight value. However,
this will result in more irrelevant characters
which confuse the system. In Step 2, we need to
filter the characters generated in Step 1. We will
judge the character whether it can construct a
word. Otherwise, we make the assumption that it
may be a spelling error which means we are still
not sure about it. Anyhow, we will send the
results to next phase.
</bodyText>
<subsectionHeader confidence="0.999661">
2.3 Spelling Error Correction
</subsectionHeader>
<bodyText confidence="0.9999844">
In spelling error correction phase, we firstly
generate a candidate set for the error character.
Characters of similar pronunciations are the
most common source of spelling errors (Wu et
al., 2013). But there still exist some errors from
similar shape (Liu et al., 2011). So, the
candidate generation is based on a similar
pronunciation or shape dictionary. For more
details about the dictionary, please refer to Yeh
et al., (2013). Secondly, each character in the
candidate set will be tested whether it can form a
legal word with its neighbors. Here, the
character which can construct a legal word with
its neighbors will be left for calculating its score
by the language model. After filtering, the
number of candidates has been reduced which
will bring two benefits: most candidates that
have been cut are irrelevant characters and less
candidates makes the system be more efficient.
At last, the best candidate means one character
gets the highest score under a forward-backward
5-gram language model and the score is higher
than the threshold. If existing, the original
character finally will be recognized as an error
character and it will be replaced by the best
candidate.
We only use the language model to choose the
best candidate because we find that the language
model can get a quite high accuracy if we can
provide a suitable candidate set successfully.
</bodyText>
<sectionHeader confidence="0.998075" genericHeader="method">
3 Experimental Analysis
</sectionHeader>
<bodyText confidence="0.999645125">
In this paper, we use 300 sentences from the
final test of SIGHAN Bake-off 2013 as our
training data and 1000 sentences provided by the
SIGHAN organizer are our test data.
In our training data, there are 402 error
characters in total. We first test the recall of the
spelling error detection based on language
model.
</bodyText>
<table confidence="0.966676285714286">
Function Language model
threshold
Recall(%) #Characters
-4 26.67 2
-3 57.00 6
-2 86.67 18
-1 96.32 38
</table>
<tableCaption confidence="0.999944">
Table 1: Results on error detection
</tableCaption>
<bodyText confidence="0.671476">
Table 1 shows that when threshold become
tighter, the recall is higher. However, the
average number of characters increases quickly.
Average number of characters means how many
</bodyText>
<page confidence="0.998033">
221
</page>
<figureCaption confidence="0.994362">
Figure 2: Example to show how to construct a word
</figureCaption>
<table confidence="0.9995754">
Run False Detection Level Correction Level
Positive Rate
Accuracy Precision Recall F1 Accuracy Precision Recall F1
1 0.2524 0.4539 0.3881 0.1601 0.2267 0.4426 0.3527 0.1375 0.1978
2 0.032 0.5292 0.7385 0.0904 0.1611 0.5235 0.7119 0.0791 0.1424
</table>
<tableCaption confidence="0.999791">
Table 2: Results of our error detection and correction subtask
</tableCaption>
<bodyText confidence="0.999936019230769">
characters are marked as error characters by our
system. The average length of sentences in our
training data is about 70 characters. When the
threshold has been set to be -1, more than half of
the characters in a sentence have been marked as
errors on average. Though the recall is very high
in this case, too many correct characters have
been recognized as errors. So we prefer to give
up the high recall rather than reserve too many
irrelevant characters. As we mentioned in
Section 2.2, the average number of spelling
errors in a sentence is quite low. Threshold = -2
only leads to a slight reduce in recall but the
average number of characters have been cut
down by half.
As shown in Figure 1, we firstly prepare two
resources: a forward-backward 5-gram language
model and a word dictionary. As described in
previous sections, such two resources will be
applied into both spelling check detection and
correction. Then, we start to detect the error
characters in a sentence. For each character in a
sentence, if its score which calculated by the
forward-backward 5-gram language model is
less than the threshold value, it will be sent to
next phase. And the threshold is set at -2 as we
discussed before. Next, we will test the character
for constructing a word. We set the size of the
window at 4 which means the target character
can be combined with its neighbors at a distance
of 4 characters. For example, Figure 2 describes
the details.
After the target character is combined with its
neighbors, we will look up the word dictionary.
While none of combinations can be found in the
word dictionary, we make the assumption that
the target character may be an error. In this
example, none of these 7 words can be found in
word dictionary. So, the character “竟” in this
sentence would be marked as an error and sent
to next phase.
In spelling check correction phase, we first
generate candidates by similar pronunciation or
shape. Then the candidates are filtered by
constructing a word. This time, we reserve the
candidates which can construct a word with its
neighbors. At last, the rest candidates will be
ranked by language model. The best candidate
with its score higher than threshold will replace
the original character in the sentence. Here, the
threshold is the same with the value in detection
level.
</bodyText>
<sectionHeader confidence="0.915286" genericHeader="method">
4 Final Results
</sectionHeader>
<bodyText confidence="0.999493555555555">
In this bake-off, there are 1000 sentences and all
sentences contain at least more than one error.
Table 2 shows that the F1 score is very low
because we can only find a small portion of all
errors. However, the false positive rate and
precision is satisfactory especially for the false
positive rate. Such results are consistent with our
main idea that we choose to under-correct rather
than over-correct.
</bodyText>
<page confidence="0.990213">
222
</page>
<bodyText confidence="0.999762666666667">
We can see that the performance in detection
level and correction level are similar. As
described in previous sections, only when the
best candidate has been found, we will make the
conclusion that the target character is a spelling
error. The performance in correction level only
has a slight decrease compared with the
detection level. But the unavoidable reality is
that the recall is not good.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999722">
Based on n-gram language model and judging a
character whether it can form a legal word with
its neighbors, a simple approach is proposed to
detect and correct the spelling errors in
traditional Chinese text. To find the spelling
errors in sentence, the language model and a
word dictionary are both used. And in order to
reduce the false positive rate, the system only
treats the character as a spelling error when the
best candidate has been found.
</bodyText>
<sectionHeader confidence="0.99773" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.556906333333333">
This work was supported by National Natural
Science Foundation of China (Grant No.
61373095, 61333018).
</bodyText>
<sectionHeader confidence="0.936798" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999432860465117">
Chen, Y. Z., Wu, S. H., Yang, P. C., &amp; Ku, T.
(2011). Improve the detection of improperly
used Chinese characters in students&apos; essays
with error model. International Journal of
Continuing Engineering Education and Life
Long Learning, 21(1), 103-116.
Huang, C. M., Wu, M. C., &amp; Chang, C. C.
(2007). Error detection and correction based
on Chinese phonemic alphabet in Chinese
text. In Modeling Decisions for Artificial
Intelligence (pp. 463-476). Springer Berlin
Heidelberg.
Kukich, K. (1992). Techniques for automatically
correcting words in text. ACM Computing
Surveys (CSUR), 24(4), 377-439.
Liu, C. L., Lai, M. H., Tien, K. W., Chuang, Y.
H., Wu, S. H., &amp; Lee, C. Y. (2011). Visually
and phonologically similar characters in
incorrect Chinese words: Analyses,
identification, and applications. ACM
Transactions on Asian Language Information
Processing (TALIP), 10(2), 10.
Wu, S. H., Liu, C. L., &amp; Lee, L. H. Chinese
Spelling Check Evaluation at SIGHAN Bake-
off 2013. In Sixth International Joint
Conference on Natural Language
Processing (p. 35).
Yeh, J. F., Li, S. F., Wu, M. R., Chen, W. Y., &amp;
Su, M. C. (2013). Chinese Word Spelling
Correction Based on N-gram Ranked
Inverted Index List. In Sixth International
Joint Conference on Natural Language
Processing (p. 43).
Zhang, L., Huang, C., Zhou, M., &amp; Pan, H.
(2000). Automatic detecting/correcting errors
in Chinese text by an approximate word-
matching algorithm. In Proceedings of the
38th Annual Meeting on Association for
Computational Linguistics (pp. 248-254).
Association for Computational Linguistics.
ZHANG, Y. S., YU Shi-wen. (2006). Summary
of Text Automatic Proofreading Technology.
Application Research of Computers, 6.
</reference>
<page confidence="0.999141">
223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.358652">
<title confidence="0.78579525">Chinese Spelling Error Detection and Correction Based on Model, Pronunciation, and Shape Junjie Yu and Zhenghua Provincial Key Laboratory for Computer Information Processing</title>
<affiliation confidence="0.999585">Soochow University,</affiliation>
<address confidence="0.723798">20144227010@stu.suda.edu.cn; zhli13@suda.edu.cn</address>
<abstract confidence="0.999292192307692">Spelling check is an important preprocessing task when dealing with user generated texts such as tweets and product comments. Compared with some western languages such as English, Chinese spelling check is more complex because there is no word delimiter in Chinese written texts and misspelled characters can only be determined in word level. Our system works as follows. First, we use character-level n-gram language models to detect potential misspelled characters with low probabilities below some predefined threshold. Second, for each potential incorrect character, we generate a candidate set based on pronunciation and shape similarities. Third, we filter some candidate corrections if the candidate cannot form a legal word with its neighbors according to a word dictionary. Finally, we find the best candidate with highest language model probability. If the probability is higher than a predefined threshold, then we replace the original character; or we consider the original character as correct and take no action. Our preliminary experiments shows that our simple method can achieve relatively high precision but low recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Z Chen</author>
<author>S H Wu</author>
<author>P C Yang</author>
<author>T Ku</author>
</authors>
<title>Improve the detection of improperly used Chinese characters in students&apos; essays with error model.</title>
<date>2011</date>
<journal>International Journal of Continuing Engineering Education and Life Long Learning,</journal>
<volume>21</volume>
<issue>1</issue>
<pages>103--116</pages>
<contexts>
<context position="4152" citStr="Chen et al. (2011)" startWordPosition="629" endWordPosition="632">zer provides a dictionary including about 5000 Chinese characters with other characters in similar pronunciation or shape which can be used in candidate generation. 2.2 Spelling Error Detection In spelling error detection phase, we propose two methods to deal with this problem. One is to gather the characters which get a low score 220 Figure 1: Framework of our proposed system under language model. Another is to record any independent characters after automatic word segmentation. However, we find both will bring in lots of irrelevant characters though most errors have been discovered. Because Chen et al. (2011) find the average amount of errors in a learners’ corpus for a student essay is only 2, we do not want to mark too many error characters to cause false-alarm problem heavily. In order to make the best of the two methods, we prepare two steps to combine both. Step 1, we calculate the score of each character in a sentence by a forward-backward 5-gram language model. While the score is less than the threshold, the character and its location are sent to Step 2. To find as more errors as possible, we set the threshold in a quite tight value. However, this will result in more irrelevant characters w</context>
</contexts>
<marker>Chen, Wu, Yang, Ku, 2011</marker>
<rawString>Chen, Y. Z., Wu, S. H., Yang, P. C., &amp; Ku, T. (2011). Improve the detection of improperly used Chinese characters in students&apos; essays with error model. International Journal of Continuing Engineering Education and Life Long Learning, 21(1), 103-116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M Huang</author>
<author>M C Wu</author>
<author>C C Chang</author>
</authors>
<title>Error detection and correction based on Chinese phonemic alphabet in Chinese text.</title>
<date>2007</date>
<booktitle>In Modeling Decisions for Artificial Intelligence</booktitle>
<pages>463--476</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="2427" citStr="Huang et al. (2007)" startWordPosition="352" endWordPosition="355"> and “real-word” error (Kukich, 1992). Unlike English, Chinese words are not separated by space and all characters in Chinese are “real-word”. Therefore, automatic word segmentation need to be applied in order to produce words (Zhang et al., 2000). There are many Chinese input methods (Zhang et al., 2005). Different input methods lead to different types of spelling errors. For example, input methods based on pinyin which usually lead to spelling errors of characters sharing similar pronunciations; while input methods based on radical methods usually lead to errors related to character shapes. Huang et al. (2007) proposed a learning model based on Chinese phonemic alphabet to detect Chinese spelling errors. Yeh et al. (2013) presented a method based on Ngram ranked inverted index list to deal with this problem. 2 System Architecture Our system includes two cascaded components: spelling error detection and spelling error correction, as shown in Figure 1. 2.1 Resources To train our language mode, we use a portion of Chinese Gigaword version 2.0 (LDC2009T14), which contains about 12 million traditional Chinese sentences. We do not split sentence into words, but treat each character as an individual unit.</context>
</contexts>
<marker>Huang, Wu, Chang, 2007</marker>
<rawString>Huang, C. M., Wu, M. C., &amp; Chang, C. C. (2007). Error detection and correction based on Chinese phonemic alphabet in Chinese text. In Modeling Decisions for Artificial Intelligence (pp. 463-476). Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys (CSUR),</journal>
<volume>24</volume>
<issue>4</issue>
<pages>377--439</pages>
<contexts>
<context position="1845" citStr="Kukich, 1992" startWordPosition="264" endWordPosition="265">cter; or we consider the original character as correct and take no action. Our preliminary experiments shows that our simple method can achieve relatively high precision but low recall. 1 Introduction Spelling check is a traditional and important preprocessing task for natural language processing, since spelling errors happen in written texts, such as short messages, emails, and so on. Lots of research has been devoted to English spelling error detection and correction. In English spelling error detection and correction, the errors can be classified into “nonword” error and “real-word” error (Kukich, 1992). Unlike English, Chinese words are not separated by space and all characters in Chinese are “real-word”. Therefore, automatic word segmentation need to be applied in order to produce words (Zhang et al., 2000). There are many Chinese input methods (Zhang et al., 2005). Different input methods lead to different types of spelling errors. For example, input methods based on pinyin which usually lead to spelling errors of characters sharing similar pronunciations; while input methods based on radical methods usually lead to errors related to character shapes. Huang et al. (2007) proposed a learni</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Kukich, K. (1992). Techniques for automatically correcting words in text. ACM Computing Surveys (CSUR), 24(4), 377-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C L Liu</author>
<author>M H Lai</author>
<author>K W Tien</author>
<author>Y H Chuang</author>
<author>S H Wu</author>
<author>C Y Lee</author>
</authors>
<title>Visually and phonologically similar characters in incorrect Chinese words: Analyses, identification, and applications.</title>
<date>2011</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>10</volume>
<issue>2</issue>
<pages>10</pages>
<contexts>
<context position="5361" citStr="Liu et al., 2011" startWordPosition="844" endWordPosition="847">acters which confuse the system. In Step 2, we need to filter the characters generated in Step 1. We will judge the character whether it can construct a word. Otherwise, we make the assumption that it may be a spelling error which means we are still not sure about it. Anyhow, we will send the results to next phase. 2.3 Spelling Error Correction In spelling error correction phase, we firstly generate a candidate set for the error character. Characters of similar pronunciations are the most common source of spelling errors (Wu et al., 2013). But there still exist some errors from similar shape (Liu et al., 2011). So, the candidate generation is based on a similar pronunciation or shape dictionary. For more details about the dictionary, please refer to Yeh et al., (2013). Secondly, each character in the candidate set will be tested whether it can form a legal word with its neighbors. Here, the character which can construct a legal word with its neighbors will be left for calculating its score by the language model. After filtering, the number of candidates has been reduced which will bring two benefits: most candidates that have been cut are irrelevant characters and less candidates makes the system b</context>
</contexts>
<marker>Liu, Lai, Tien, Chuang, Wu, Lee, 2011</marker>
<rawString>Liu, C. L., Lai, M. H., Tien, K. W., Chuang, Y. H., Wu, S. H., &amp; Lee, C. Y. (2011). Visually and phonologically similar characters in incorrect Chinese words: Analyses, identification, and applications. ACM Transactions on Asian Language Information Processing (TALIP), 10(2), 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S H Wu</author>
<author>C L Liu</author>
<author>L H Lee</author>
</authors>
<title>Chinese Spelling Check Evaluation at SIGHAN Bakeoff</title>
<date>2013</date>
<booktitle>In Sixth International Joint Conference on Natural Language Processing</booktitle>
<contexts>
<context position="5288" citStr="Wu et al., 2013" startWordPosition="831" endWordPosition="834">n a quite tight value. However, this will result in more irrelevant characters which confuse the system. In Step 2, we need to filter the characters generated in Step 1. We will judge the character whether it can construct a word. Otherwise, we make the assumption that it may be a spelling error which means we are still not sure about it. Anyhow, we will send the results to next phase. 2.3 Spelling Error Correction In spelling error correction phase, we firstly generate a candidate set for the error character. Characters of similar pronunciations are the most common source of spelling errors (Wu et al., 2013). But there still exist some errors from similar shape (Liu et al., 2011). So, the candidate generation is based on a similar pronunciation or shape dictionary. For more details about the dictionary, please refer to Yeh et al., (2013). Secondly, each character in the candidate set will be tested whether it can form a legal word with its neighbors. Here, the character which can construct a legal word with its neighbors will be left for calculating its score by the language model. After filtering, the number of candidates has been reduced which will bring two benefits: most candidates that have </context>
</contexts>
<marker>Wu, Liu, Lee, 2013</marker>
<rawString>Wu, S. H., Liu, C. L., &amp; Lee, L. H. Chinese Spelling Check Evaluation at SIGHAN Bakeoff 2013. In Sixth International Joint Conference on Natural Language Processing (p. 35).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Yeh</author>
<author>S F Li</author>
<author>M R Wu</author>
<author>W Y Chen</author>
<author>M C Su</author>
</authors>
<title>Chinese Word Spelling Correction Based on N-gram Ranked Inverted Index List.</title>
<date>2013</date>
<booktitle>In Sixth International Joint Conference on Natural Language Processing</booktitle>
<contexts>
<context position="2541" citStr="Yeh et al. (2013)" startWordPosition="370" endWordPosition="373">n Chinese are “real-word”. Therefore, automatic word segmentation need to be applied in order to produce words (Zhang et al., 2000). There are many Chinese input methods (Zhang et al., 2005). Different input methods lead to different types of spelling errors. For example, input methods based on pinyin which usually lead to spelling errors of characters sharing similar pronunciations; while input methods based on radical methods usually lead to errors related to character shapes. Huang et al. (2007) proposed a learning model based on Chinese phonemic alphabet to detect Chinese spelling errors. Yeh et al. (2013) presented a method based on Ngram ranked inverted index list to deal with this problem. 2 System Architecture Our system includes two cascaded components: spelling error detection and spelling error correction, as shown in Figure 1. 2.1 Resources To train our language mode, we use a portion of Chinese Gigaword version 2.0 (LDC2009T14), which contains about 12 million traditional Chinese sentences. We do not split sentence into words, but treat each character as an individual unit. In other words, our language model is based on character. In order to take advantage of the context information, </context>
<context position="5522" citStr="Yeh et al., (2013)" startWordPosition="870" endWordPosition="873">therwise, we make the assumption that it may be a spelling error which means we are still not sure about it. Anyhow, we will send the results to next phase. 2.3 Spelling Error Correction In spelling error correction phase, we firstly generate a candidate set for the error character. Characters of similar pronunciations are the most common source of spelling errors (Wu et al., 2013). But there still exist some errors from similar shape (Liu et al., 2011). So, the candidate generation is based on a similar pronunciation or shape dictionary. For more details about the dictionary, please refer to Yeh et al., (2013). Secondly, each character in the candidate set will be tested whether it can form a legal word with its neighbors. Here, the character which can construct a legal word with its neighbors will be left for calculating its score by the language model. After filtering, the number of candidates has been reduced which will bring two benefits: most candidates that have been cut are irrelevant characters and less candidates makes the system be more efficient. At last, the best candidate means one character gets the highest score under a forward-backward 5-gram language model and the score is higher t</context>
</contexts>
<marker>Yeh, Li, Wu, Chen, Su, 2013</marker>
<rawString>Yeh, J. F., Li, S. F., Wu, M. R., Chen, W. Y., &amp; Su, M. C. (2013). Chinese Word Spelling Correction Based on N-gram Ranked Inverted Index List. In Sixth International Joint Conference on Natural Language Processing (p. 43).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Zhang</author>
<author>C Huang</author>
<author>M Zhou</author>
<author>H Pan</author>
</authors>
<title>Automatic detecting/correcting errors in Chinese text by an approximate wordmatching algorithm.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics</booktitle>
<pages>248--254</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="2055" citStr="Zhang et al., 2000" startWordPosition="295" endWordPosition="298">lling check is a traditional and important preprocessing task for natural language processing, since spelling errors happen in written texts, such as short messages, emails, and so on. Lots of research has been devoted to English spelling error detection and correction. In English spelling error detection and correction, the errors can be classified into “nonword” error and “real-word” error (Kukich, 1992). Unlike English, Chinese words are not separated by space and all characters in Chinese are “real-word”. Therefore, automatic word segmentation need to be applied in order to produce words (Zhang et al., 2000). There are many Chinese input methods (Zhang et al., 2005). Different input methods lead to different types of spelling errors. For example, input methods based on pinyin which usually lead to spelling errors of characters sharing similar pronunciations; while input methods based on radical methods usually lead to errors related to character shapes. Huang et al. (2007) proposed a learning model based on Chinese phonemic alphabet to detect Chinese spelling errors. Yeh et al. (2013) presented a method based on Ngram ranked inverted index list to deal with this problem. 2 System Architecture Our</context>
</contexts>
<marker>Zhang, Huang, Zhou, Pan, 2000</marker>
<rawString>Zhang, L., Huang, C., Zhou, M., &amp; Pan, H. (2000). Automatic detecting/correcting errors in Chinese text by an approximate wordmatching algorithm. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics (pp. 248-254). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S ZHANG</author>
<author>YU Shi-wen</author>
</authors>
<title>Summary of Text Automatic Proofreading Technology.</title>
<date>2006</date>
<journal>Application Research of Computers,</journal>
<volume>6</volume>
<marker>ZHANG, Shi-wen, 2006</marker>
<rawString>ZHANG, Y. S., YU Shi-wen. (2006). Summary of Text Automatic Proofreading Technology. Application Research of Computers, 6.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000458">
<title confidence="0.9987625">
Evaluating a Spoken Dialogue System that Detects and Adapts to User
Affective States
</title>
<author confidence="0.994149">
Diane Litman
</author>
<affiliation confidence="0.848594">
Computer Science Dept. &amp; LRDC
University of Pittsburgh
Pittsburgh, PA 15260 USA
</affiliation>
<email confidence="0.997503">
dlitman@pitt.edu
</email>
<author confidence="0.481675">
Kate Forbes-Riley
</author>
<affiliation confidence="0.609417666666667">
Learning Research &amp; Development Center
University of Pittsburgh
Pittsburgh, PA 15260 USA
</affiliation>
<email confidence="0.9985">
forbesk@pitt.edu
</email>
<sectionHeader confidence="0.993886" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999380090909091">
We present an evaluation of a spoken
dialogue system that detects and adapts
to user disengagement and uncertainty in
real-time. We compare this version of our
system to a version that adapts to only user
disengagement, and to a version that ig-
nores user disengagement and uncertainty
entirely. We find a significant increase in
task success when comparing both affect-
adaptive versions of our system to our non-
adaptive baseline, but only for male users.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971956521739">
There is increasing interest in building dialogue
systems that can detect and adapt to user affec-
tive states.1 However, while this line of research is
promising, there is still much work to be done. For
example, most research has focused on detecting
user affective states, rather than on developing di-
alogue strategies that adapt to such states once de-
tected. In addition, when affect-adaptive dialogue
systems have been developed, most systems detect
and adapt to only a single user state, and typically
assume that the same affect-adaptive strategy will
be equally effective for all users.
In this paper we take a step towards examin-
ing these issues, by presenting an evaluation of
three versions of an affect-adaptive spoken tuto-
rial dialogue system: one that detects and adapts
to both user disengagement and uncertainty, one
that adapts to only disengagement, and one that
doesn’t adapt to affect at all. Our evaluation exam-
ines the impact of adapting to differing numbers of
affective states on task success, and also examines
interactions with user gender. We target disen-
gagement and uncertainty because these were the
</bodyText>
<footnote confidence="0.930114">
1We use the term affect to describe emotions and attitudes
that impact how people communicate. Other researchers also
combine concepts of emotion, arousal, and attitudes where
emotion is not full-blown (Cowie and Cornelius, 2003).
</footnote>
<bodyText confidence="0.992615384615385">
most frequent affective states in prior studies with
our system and their presence was negatively cor-
related with task success2 (Forbes-Riley and Lit-
man, 2011; Forbes-Riley and Litman, 2012). The
detection of these and similar states is also of in-
terest to the larger speech and language processing
communities, e.g. (Wang and Hirschberg, 2011;
Bohus and Horvitz, 2009; Pon-Barry and Shieber,
2011). Our results suggest that while adapting
to affect increases task success compared to not
adapting at all, the utility of our current methods
varies with user gender. Also, we find no differ-
ence between adapting to one or two states.
</bodyText>
<sectionHeader confidence="0.999908" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999899">
2.1 Adapting to Multiple Affective States
</subsectionHeader>
<bodyText confidence="0.999977391304348">
While prior research has shown that users display
a range of affective states during spoken dialogue
(e.g. (Schuller et al., 2009)), only a few dialogue
systems have been developed that can adapt to
more than one user affective state (e.g., (D’Mello
et al., 2010; Acosta and Ward, 2011)). Further-
more, prior evaluations have compared adapting
to at least one affective state to not adapting to af-
fect at all, but have not examined the benefits of
adapting to one versus multiple affective states.
In a first evaluation comparing singly and mul-
tiply affect-adaptive dialogue systems, we com-
pared an existing system that adapted to uncer-
tainty to a new version that also adapted to disen-
gagement (Forbes-Riley and Litman, 2012). The
multiply-adaptive system increased motivation for
users with high disengagement, and reduced both
uncertainty and the likelihood of continued dis-
engagement. However, this evaluation was only
conducted in a “Wizard-of-Oz” scenario, where a
hidden human replaced the speech recognition, se-
mantic analysis, and affect detection components
of our dialogue system. We also conducted a post-
</bodyText>
<footnote confidence="0.971582">
2Our success measure is learning gain (Section 4).
</footnote>
<page confidence="0.971693">
181
</page>
<bodyText confidence="0.959988">
Proceedings of the SIGDIAL 2014 Conference, pages 181–185,
Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics
hoc correlational (rather than causal) study, using
data from an earlier fully-automated version of the
uncertainty-adaptive system. Regressions demon-
strated that using both automatically labeled dis-
engagement and uncertainty to predict task suc-
cess significantly outperformed using only disen-
gagement (Forbes-Riley et al., 2012). However, if
manual labels were instead used, only disengage-
ment was predictive of learning, and adding un-
certainty didn’t help. This suggests that detecting
multiple affective states might compensate for the
noise that is introduced in a fully-automated sys-
tem. In this paper we further investigate this hy-
pothesis, by evaluating the utility of adapting to
zero, one, or two affective states in a controlled
experiment involving fully-automated systems.
</bodyText>
<subsectionHeader confidence="0.997478">
2.2 Gender Effects in Dialogue
</subsectionHeader>
<bodyText confidence="0.999982230769231">
Differences in dialogue structure have been found
between male and female students talking to a hu-
man tutor (Boyer et al., 2007). Studies have also
shown gender differences in conversational en-
trainment patterns, for acoustic-prosodic features
in human-human dialogues (Levitan et al., 2012)
and articles in movie conversations (Danescu-
Niculescu-Mizil and Lee, 2011). For dialogue sys-
tems involving embodied conversational agents,
gender effects have been found for facial dis-
plays, with females preferring more expressive
agents (Foster and Oberlander, 2006). When used
for tutoring, females report more positive affect
when a learning companion is used, while males
are more negative (Woolf et al., 2010).
In our own prior work, we compared two
uncertainty-adaptive and one non-adaptive ver-
sions of a wizarded dialogue system. Our results
demonstrated that only one method of adapting to
user uncertainty increased task success, and only
for female users (Forbes-Riley and Litman, 2009).
In this paper we extend this line of research, by
adding an affective dialogue system that adapts
to two rather than just one user state to our eval-
uation, and by moving from wizarded to fully-
automated systems.
</bodyText>
<sectionHeader confidence="0.876006" genericHeader="method">
3 System, Experiment and Corpus
</sectionHeader>
<bodyText confidence="0.9999145">
Our corpus consists of dialogues between
users and three different versions of ITSPOKE
(Intelligent Tutoring SPOKEn dialog sys-
tem) (Forbes-Riley and Litman, 2011; Forbes-
Riley and Litman, 2012). ITSPOKE is a
speech-enhanced and otherwise modified version
of the Why2-Atlas text-based qualitative physics
tutor (VanLehn et al., 2002) that interacts with
users using a system initiative dialogue strategy.
User speech is first digitized from head-mounted
microphone input and sent to the PocketSphinx
recognizer.3 The recognition output is then clas-
sified as (in)correct with respect to the anticipated
physics content via semantic analysis (Jordan
et al., 2007). Simultaneously, user uncertainty
(UNC) and disengagement (DISE) are classified
from prosodic, lexical and contextual features
using two binary classification models (Forbes-
Riley et al., 2012). All statistical components of
the speech recognizer, the semantic analyzer, and
the uncertainty and disengagement detectors were
trained using prior ITSPOKE corpora.4 Finally,
ITSPOKE’s response is determined based on the
answer’s automatically labeled (in)correctness,
(un)certainty, and (dis)engagement and then sent
to the Cepstral text-to-speech system,5 as well as
displayed on a web-based interface.
Our corpus was collected in an experiment con-
sisting of three conditions (CONTROL, DISE,
DISE+UNC), where ITSPOKE used a different
method of affect-adaptation in each condition.
The experiment was designed to compare the ef-
fectiveness of not adapting to user affect in IT-
SPOKE (CONTROL), adapting to user disengage-
ment (DISE), and adapting to user disengagement
as well as user uncertainty (DISE+UNC).6
In CONTROL, ITSPOKE’s responses to user
utterances were based on only the correctness of
user answers. This version of the system thus ig-
nored any automatically detected user disengage-
ment or uncertainty. In particular, after each cor-
rect answer, ITSPOKE provided positive feedback
then moved on to the next topic. After incor-
rect answers, ITSPOKE instead provided negative
</bodyText>
<footnote confidence="0.9288536875">
3http://www.speech.cs.cmu.edu/pocketsphinx
4We have not yet performed the manual annotations
needed to evaluate our current versions of these components
in isolation. However, earlier versions of our affect detec-
tors yielded FMeasures of 69% and 68% for disengagement
and uncertainty, respectively, on par with the best perform-
ing affect detectors in the wider literature (Forbes-Riley and
Litman, 2011; Forbes-Riley et al., 2012).
5http://www.cepstral.com
6We did not include an uncertainty-only condition (UNC)
because in previous work we compared UNC versus CON-
TROL (Forbes-Riley and Litman, 2011) and DISE+UNC
versus UNC (Forbes-Riley and Litman, 2012). Further de-
tails and motivation for all experimental conditions can be
found in the description of our earlier Wizard-of-Oz experi-
ment (Forbes-Riley and Litman, 2012).
</footnote>
<page confidence="0.993741">
182
</page>
<bodyText confidence="0.999957266666667">
feedback, then provided remediation tutoring be-
fore moving on to the next topic.
In DISE, two adaptive responses were devel-
oped to allow ITSPOKE’s responses to consider
user disengagement as well as the correctness of
the user’s answer;7 however, this system version
still ignored user uncertainty. In particular, af-
ter each disengaged+correct answer, ITSPOKE
provided correctness feedback, a progress chart
showing user correctness on prior problems and
the current problem, and a brief re-engagement
tip. After each disengaged+incorrect answer, IT-
SPOKE provided incorrectness feedback, a brief
re-engagement tip, and an easier supplemental ex-
ercise, which consisted of an easy fill-in-the-blank
type question to reengage the user, followed by re-
mediation targeting the material on which the user
disengaged and answered incorrectly. Examples
of both types of adaptive responses are shown in
A.1 and A.2 of Appendix A, respectively.
In DISE+UNC, ITSPOKE responded to dis-
engagement as just described, but also adapted
to uncertainty. In particular, after each uncer-
tain+correct answer, ITSPOKE provided positive
correctness feedback, but then added the remedia-
tion designed for incorrect answers with the goal
of reducing the user’s uncertainty. A dialogue ex-
cerpt illustrating this strategy is shown in A.3 of
Appendix A. Note that when a single utterance is
predicted to be both disengaged and uncertain, the
DISE and UNC adaptations are combined.
Finally, our experimental procedure was as fol-
lows. College students who were native English
speakers and who had no college-level physics
read a short physics text, took a pretest, worked
5 physics problems (one problem per dialogue)
with the version of ITSPOKE from their experi-
mental condition, and took a posttest isomorphic
to the pretest. The pretest and posttest were taken
from our Wizard-of-Oz experiment and each con-
tained 26 multiple choice physics questions. Our
experiment yielded a corpus of 335 dialogues (5
per user) from 67 users (39 female and 28 male).
Average pretest8 and posttest scores were 50.4%
and 74.7% (out of 100%), respectively.
</bodyText>
<sectionHeader confidence="0.994767" genericHeader="method">
4 Performance Analysis
</sectionHeader>
<bodyText confidence="0.9987665">
Based on the prior research discussed in Section 2,
we had two experimental hypotheses:
</bodyText>
<footnote confidence="0.9953735">
7Engaged answers were treated as in CONTROL.
8Pretest did not differ across conditions (p = .92).
</footnote>
<table confidence="0.999659375">
Condition Learning Gain N
Mean (%) Std Err
DISE+UNC 53.2 5.0 23
DISE 51.4 4.8 22
CONTROL 46.6 4.7 22
Gender Learning Gain N
Male 53.2 4.3 28
Female 47.6 3.6 39
</table>
<tableCaption confidence="0.9823275">
Table 1: No effect of experimental condition
(p=.62) or gender (p=.32) on learning gain.
</tableCaption>
<table confidence="0.998461125">
Gender Condition Learning Gain N
Mn (%) Std Err
Male DISE+UNC 58.8 8.4 7
DISE 62.2 7.0 10
CONTROL 38.7 6.7 11
Female DISE+UNC 47.5 5.6 16
DISE 40.6 6.4 12
CONTROL 54.6 6.7 11
</table>
<tableCaption confidence="0.875581">
Table 2: Significant interaction between the ef-
fects of gender and condition on learning (p=.02).
</tableCaption>
<bodyText confidence="0.898604964285714">
H1: Responding to multiple affective states will
yield greater task success than responding to only
a single state (DISE+UNC &gt; DISE), which in
turn will outperform not responding to affect at all
(DISE &gt; CONTROL).
H2: The effects of ITSPOKE’s affect-
adaptation method and of gender will interact.
A two-way analysis of variance (ANOVA) was
thus conducted to examine the effect of ex-
perimental condition (DISE+UNC, DISE, CON-
TROL) and user gender (Male, Female) on task
success. As is typical in the tutoring domain, task
success was computed as (normalized) learning
gain: posttest−pretest
100−pretest .
Table 1 shows that although our results pat-
terned as hypothesized when considering all users,
the differences in learning gains were not statisti-
cally different across experimental conditions, F
(2, 61) = .487, p = .617. There were also no main
effects of gender, F (1, 61) = 1.014, p = .318.
In contrast, as shown in Table 2, there was a
statistically significant interaction between the ef-
fects of user gender and experimental condition on
learning gains, F (2, 61) = 4.141, p = .021. We
thus tested the simple effects of condition within
each level of gender to yield further insights.
For males, simple main effects analysis showed
</bodyText>
<page confidence="0.998305">
183
</page>
<bodyText confidence="0.999743458333333">
that there were statistically significant differences
in learning gains between experimental conditions
(p = .042). In particular, males in the DISE con-
dition had significantly higher learning gains than
males in the CONTROL condition (p = .019).
Males in the DISE+UNC condition also showed
a trend for higher learning gains than males in the
CONTROL condition (p = .066). However, males
in the DISE and DISE+UNC conditions showed
no difference in learning gains (p= .760).
For females, in contrast, simple main effects
analysis showed no statistically significant differ-
ences in learning gains between any experimental
conditions (p = .327).
In sum, hypothesis H1 regarding the utility of
affect adaptations was only partially supported by
our results, where (DISE+UNC = DISE) &gt; CON-
TROL, and only for males. That is, adapting to
affect was indeed better than not adapting at all,
but only for males (supporting hypothesis H2).
Contrary to H1, adapting to uncertainty over and
above disengagement did not provide any bene-
fit compared to adapting to disengagement alone
(DISE+UNC = DISE), for both genders.
</bodyText>
<sectionHeader confidence="0.987929" genericHeader="method">
5 Discussion and Future Directions
</sectionHeader>
<bodyText confidence="0.9999915625">
Our results contribute to the increasing body
of literature demonstrating the utility of adding
fully-automated affect-adaptation to existing spo-
ken dialogue systems. In particular, males in
our two affect-adaptive conditions (DISE+UNC
and DISE) learned more than males in the
non-adaptive CONTROL. While our prior work
demonstrated the benefits of adapting to uncer-
tainty, the current results demonstrate the impor-
tance of adapting to disengagement either alone
or in conjunction with uncertainty. However, we
also predicted that DISE+UNC should outperform
DISE, which was not the case. In future work we
will examine other performance measures besides
learning, and will manually annotate true disen-
gagement and uncertainty in order to group stu-
dents by amount of disengagement. Furthermore,
since the motivating prior studies discussed in Sec-
tion 2 were based on older versions of our system,
annotation could identify problematic differences
in training and testing data. A final potential is-
sue is that the re-engagement tips do not convey
exactly the same information.
Second, our results contribute to the literature
suggesting that gender effects should be consid-
ered when designing dialogue systems. We see
similar results as in our prior work; namely our
current results continue to suggest that males don’t
benefit from adapting to their uncertainty as com-
pared to ignoring it, but our current results also
suggest that males do benefit from adapting to
their disengagement. On the other hand, our cur-
rent results suggest that females do not benefit
from our disengagement adaptation and moreover,
combining it with our uncertainty adaptation re-
duces the benefit of the uncertainty adaptation for
them. This suggests the possibility of a differ-
ing affective hierarchy, in terms of how affective
states may impact the learning process of the two
genders differently. Our results yield an empirical
basis for future investigations into whether adap-
tive system performance can improve by adapting
to affect differently based on gender. However,
further research is needed to determine more ef-
fective combinations of disengagement and uncer-
tainty adaptations for both males and females, and
to investigate whether gender differences might be
related to other types of measurable user factors.
</bodyText>
<sectionHeader confidence="0.997475" genericHeader="method">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999280666666667">
This work is funded by NSF 0914615. We thank S.
Silliman for experimental support, and H. Nguyen,
W. Xiong, and the reviewers for feedback.
</bodyText>
<sectionHeader confidence="0.998548" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.999415666666667">
J. C. Acosta and N. G. Ward. 2011. Achieving rapport
with turn-by-turn, user-responsive emotional color-
ing. Speech Communication, 53(9-10):1137–1148.
D. Bohus and E. Horvitz. 2009. Models for multiparty
engagement in open-world dialog. In Proceedings
of SIGdial, pages 225–234, London, UK.
K. Boyer, M. Vouk, and J. Lester. 2007. The influ-
ence of learner characteristics on task-oriented tuto-
rial dialogue. Frontiers in Artificial Intelligence and
Applications, 158:365.
R. Cowie and R. R. Cornelius. 2003. Describing
the emotional states that are expressed in speech.
Speech Communication, 40(1-2):5–32.
C. Danescu-Niculescu-Mizil and L. Lee. 2011.
Chameleons in imagined conversations: A new ap-
proach to understanding coordination of linguistic
style in dialogs. In Proceedings of the 2nd Workshop
on Cognitive Modeling and Computational Linguis-
tics, pages 76–87.
S. D’Mello, B. Lehman, J. Sullins, R. Daigle,
R. Combs, K. Vogt, L. Perkins, and A. Graesser.
</reference>
<page confidence="0.994422">
184
</page>
<reference confidence="0.982150746031746">
2010. A time for emoting: When affect-sensitivity
is and isn’t effective at promoting deep learning. In
Intelligent Tutoring Systems Conference, pages 245–
254, Pittsburgh, PA, USA.
K. Forbes-Riley and D. Litman. 2009. A user
modeling-based performance analysis of a wizarded
uncertainty-adaptive dialogue system corpus. In
Proceedings Interspeech, pages 2467–2470.
K. Forbes-Riley and D. Litman. 2011. Benefits
and challenges of real-time uncertainty detection
and adaptation in a spoken dialogue computer tutor.
Speech Communication, 53(9–10):1115–1136.
K. Forbes-Riley and D. Litman. 2012. Adapting to
multiple affective states in spoken dialogue. In Pro-
ceedings of the 13th Annual Meeting of the Special
Interest Group on Discourse and Dialogue, SIG-
DIAL ’12, pages 217–226.
K. Forbes-Riley, D. Litman, H. Friedberg, and J. Drum-
mond. 2012. Intrinsic and extrinsic evaluation of
an automatic user disengagement detector for an
uncertainty-adaptive spoken dialogue system. In
Proceedings of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 91–102.
M. Foster and J. Oberlander. 2006. Data-driven gen-
eration of emphatic facial displays. Proceedings of
EACL, 2006:353–363.
P. Jordan, B. Hall, M. Ringenberg, Y. Cui, and C.P.
Rose. 2007. Tools for authoring a dialogue agent
that participates in learning studies. In Proc. Artifi-
cial Intelligence in Education (AIED), pages 43–50.
R. Levitan, A. Gravano, L. Willson, S. Benus,
J. Hirschberg, and A. Nenkova. 2012. Acoustic-
prosodic entrainment and social behavior. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 11–19.
H. Pon-Barry and S. Shieber. 2011. Recognizing un-
certainty in speech. EURASIP Journal on Advances
in Signal Processing.
B. Schuller, S. Steidl, and A. Batliner. 2009. The In-
terspeech 2009 emotion challenge. In Proc. Inter-
speech, pages 312–315, ISCA, Brighton, UK.
K. VanLehn, P. W. Jordan, C. Ros´e, D. Bhembe,
M. B¨ottner, A. Gaydos, M. Makatchev, U. Pap-
puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-
vastava, and R. Wilson. 2002. The architecture of
Why2-Atlas: A coach for qualitative physics essay
writing. In Proc. Intl. Conf. on Intelligent Tutoring
Systems.
W. Wang and J. Hirschberg. 2011. Detecting levels of
interest from spoken dialog with multistream predic-
tion feedback and similarity based hierarchical fu-
sion learning. In Proc. SIGDIAL, pages 152–161,
Portland, Oregon.
B. Woolf, I. Arroyo, D. Cooper, W. Burleson, and
K. Muldner. 2010. Affective tutors: Automatic de-
tection of and response to student emotion. In Ad-
vances in Intelligent Tutoring Systems, pages 207–
227. Springer.
Appendix A. Affect-Adaptive Examples.
A.1 After a Predicted Disengaged+Correct
Turn
</reference>
<bodyText confidence="0.819473888888889">
ITSPOKE: How does his velocity compare to
that of his keys?
USER: same same same
ITSPOKE: Right. [Feedback] Here’s a quick
progress update. [See Screenshot] It might help
to remember we will build on the topics we’re
discussing now. [Re-engagement Tip] Now let’s
see what happens after the man releases his keys.
[Next Topic]
</bodyText>
<sectionHeader confidence="0.677032" genericHeader="method">
A.2 After Predicting Disengaged+Incorrect
</sectionHeader>
<bodyText confidence="0.988428777777778">
ITSPOKE: How does his velocity compare to
that of his keys?
USER: vertical
ITSPOKE: That doesn’t sound right. [Feed-
back] Let’s think about this a little more. [Re-
engagement Tip] Since the man is holding his
keys, they aren’t moving relative to each other.
So their velocities must be WHAT? [Supplemen-
tal Exercise].
</bodyText>
<subsectionHeader confidence="0.500395">
A.3 After a Predicted Uncertain+Correct Turn
</subsectionHeader>
<bodyText confidence="0.998127444444445">
ITSPOKE: What are the forces exerted on the
man after he releases his keys?
USER: gravity???
ITSPOKE: Right. [Feedback] Gravity is the
only force acting on the man. This force is also
called the weight of the man. It’s always present
when an object is in earth’s gravitational field.
[Remediation]. So what’s the direction of the
force of gravity on the man ? [Next Topic]
</bodyText>
<page confidence="0.998303">
185
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.441621">
<title confidence="0.9947645">Evaluating a Spoken Dialogue System that Detects and Adapts to Affective States</title>
<author confidence="0.876719">Diane</author>
<affiliation confidence="0.9982735">Computer Science Dept. &amp; University of</affiliation>
<address confidence="0.994962">Pittsburgh, PA 15260</address>
<email confidence="0.635256">dlitman@pitt.edu</email>
<author confidence="0.889845">Kate</author>
<affiliation confidence="0.9970295">Learning Research &amp; Development University of</affiliation>
<address confidence="0.99003">Pittsburgh, PA 15260</address>
<email confidence="0.931979">forbesk@pitt.edu</email>
<abstract confidence="0.995792583333333">We present an evaluation of a spoken dialogue system that detects and adapts to user disengagement and uncertainty in real-time. We compare this version of our system to a version that adapts to only user disengagement, and to a version that ignores user disengagement and uncertainty entirely. We find a significant increase in task success when comparing both affectadaptive versions of our system to our nonadaptive baseline, but only for male users.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J C Acosta</author>
<author>N G Ward</author>
</authors>
<title>Achieving rapport with turn-by-turn, user-responsive emotional coloring.</title>
<date>2011</date>
<journal>Speech Communication,</journal>
<pages>53--9</pages>
<contexts>
<context position="3132" citStr="Acosta and Ward, 2011" startWordPosition="492" endWordPosition="495"> Horvitz, 2009; Pon-Barry and Shieber, 2011). Our results suggest that while adapting to affect increases task success compared to not adapting at all, the utility of our current methods varies with user gender. Also, we find no difference between adapting to one or two states. 2 Related Work 2.1 Adapting to Multiple Affective States While prior research has shown that users display a range of affective states during spoken dialogue (e.g. (Schuller et al., 2009)), only a few dialogue systems have been developed that can adapt to more than one user affective state (e.g., (D’Mello et al., 2010; Acosta and Ward, 2011)). Furthermore, prior evaluations have compared adapting to at least one affective state to not adapting to affect at all, but have not examined the benefits of adapting to one versus multiple affective states. In a first evaluation comparing singly and multiply affect-adaptive dialogue systems, we compared an existing system that adapted to uncertainty to a new version that also adapted to disengagement (Forbes-Riley and Litman, 2012). The multiply-adaptive system increased motivation for users with high disengagement, and reduced both uncertainty and the likelihood of continued disengagement</context>
</contexts>
<marker>Acosta, Ward, 2011</marker>
<rawString>J. C. Acosta and N. G. Ward. 2011. Achieving rapport with turn-by-turn, user-responsive emotional coloring. Speech Communication, 53(9-10):1137–1148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>E Horvitz</author>
</authors>
<title>Models for multiparty engagement in open-world dialog.</title>
<date>2009</date>
<booktitle>In Proceedings of SIGdial,</booktitle>
<pages>225--234</pages>
<location>London, UK.</location>
<contexts>
<context position="2524" citStr="Bohus and Horvitz, 2009" startWordPosition="391" endWordPosition="394"> because these were the 1We use the term affect to describe emotions and attitudes that impact how people communicate. Other researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). most frequent affective states in prior studies with our system and their presence was negatively correlated with task success2 (Forbes-Riley and Litman, 2011; Forbes-Riley and Litman, 2012). The detection of these and similar states is also of interest to the larger speech and language processing communities, e.g. (Wang and Hirschberg, 2011; Bohus and Horvitz, 2009; Pon-Barry and Shieber, 2011). Our results suggest that while adapting to affect increases task success compared to not adapting at all, the utility of our current methods varies with user gender. Also, we find no difference between adapting to one or two states. 2 Related Work 2.1 Adapting to Multiple Affective States While prior research has shown that users display a range of affective states during spoken dialogue (e.g. (Schuller et al., 2009)), only a few dialogue systems have been developed that can adapt to more than one user affective state (e.g., (D’Mello et al., 2010; Acosta and War</context>
</contexts>
<marker>Bohus, Horvitz, 2009</marker>
<rawString>D. Bohus and E. Horvitz. 2009. Models for multiparty engagement in open-world dialog. In Proceedings of SIGdial, pages 225–234, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Boyer</author>
<author>M Vouk</author>
<author>J Lester</author>
</authors>
<title>The influence of learner characteristics on task-oriented tutorial dialogue.</title>
<date>2007</date>
<booktitle>Frontiers in Artificial Intelligence and Applications,</booktitle>
<pages>158--365</pages>
<contexts>
<context position="5107" citStr="Boyer et al., 2007" startWordPosition="786" endWordPosition="789">012). However, if manual labels were instead used, only disengagement was predictive of learning, and adding uncertainty didn’t help. This suggests that detecting multiple affective states might compensate for the noise that is introduced in a fully-automated system. In this paper we further investigate this hypothesis, by evaluating the utility of adapting to zero, one, or two affective states in a controlled experiment involving fully-automated systems. 2.2 Gender Effects in Dialogue Differences in dialogue structure have been found between male and female students talking to a human tutor (Boyer et al., 2007). Studies have also shown gender differences in conversational entrainment patterns, for acoustic-prosodic features in human-human dialogues (Levitan et al., 2012) and articles in movie conversations (DanescuNiculescu-Mizil and Lee, 2011). For dialogue systems involving embodied conversational agents, gender effects have been found for facial displays, with females preferring more expressive agents (Foster and Oberlander, 2006). When used for tutoring, females report more positive affect when a learning companion is used, while males are more negative (Woolf et al., 2010). In our own prior wor</context>
</contexts>
<marker>Boyer, Vouk, Lester, 2007</marker>
<rawString>K. Boyer, M. Vouk, and J. Lester. 2007. The influence of learner characteristics on task-oriented tutorial dialogue. Frontiers in Artificial Intelligence and Applications, 158:365.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cowie</author>
<author>R R Cornelius</author>
</authors>
<title>Describing the emotional states that are expressed in speech.</title>
<date>2003</date>
<journal>Speech Communication,</journal>
<pages>40--1</pages>
<contexts>
<context position="2154" citStr="Cowie and Cornelius, 2003" startWordPosition="333" endWordPosition="336">torial dialogue system: one that detects and adapts to both user disengagement and uncertainty, one that adapts to only disengagement, and one that doesn’t adapt to affect at all. Our evaluation examines the impact of adapting to differing numbers of affective states on task success, and also examines interactions with user gender. We target disengagement and uncertainty because these were the 1We use the term affect to describe emotions and attitudes that impact how people communicate. Other researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). most frequent affective states in prior studies with our system and their presence was negatively correlated with task success2 (Forbes-Riley and Litman, 2011; Forbes-Riley and Litman, 2012). The detection of these and similar states is also of interest to the larger speech and language processing communities, e.g. (Wang and Hirschberg, 2011; Bohus and Horvitz, 2009; Pon-Barry and Shieber, 2011). Our results suggest that while adapting to affect increases task success compared to not adapting at all, the utility of our current methods varies with user gender. Also, we find no difference betw</context>
</contexts>
<marker>Cowie, Cornelius, 2003</marker>
<rawString>R. Cowie and R. R. Cornelius. 2003. Describing the emotional states that are expressed in speech. Speech Communication, 40(1-2):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Danescu-Niculescu-Mizil</author>
<author>L Lee</author>
</authors>
<title>Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics,</booktitle>
<pages>76--87</pages>
<marker>Danescu-Niculescu-Mizil, Lee, 2011</marker>
<rawString>C. Danescu-Niculescu-Mizil and L. Lee. 2011. Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs. In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 76–87.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S D’Mello</author>
<author>B Lehman</author>
<author>J Sullins</author>
<author>R Daigle</author>
<author>R Combs</author>
<author>K Vogt</author>
<author>L Perkins</author>
<author>A Graesser</author>
</authors>
<marker>D’Mello, Lehman, Sullins, Daigle, Combs, Vogt, Perkins, Graesser, </marker>
<rawString>S. D’Mello, B. Lehman, J. Sullins, R. Daigle, R. Combs, K. Vogt, L. Perkins, and A. Graesser.</rawString>
</citation>
<citation valid="true">
<title>A time for emoting: When affect-sensitivity is and isn’t effective at promoting deep learning.</title>
<date>2010</date>
<booktitle>In Intelligent Tutoring Systems Conference,</booktitle>
<pages>245--254</pages>
<location>Pittsburgh, PA, USA.</location>
<marker>2010</marker>
<rawString>2010. A time for emoting: When affect-sensitivity is and isn’t effective at promoting deep learning. In Intelligent Tutoring Systems Conference, pages 245– 254, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>A user modeling-based performance analysis of a wizarded uncertainty-adaptive dialogue system corpus.</title>
<date>2009</date>
<booktitle>In Proceedings Interspeech,</booktitle>
<pages>2467--2470</pages>
<contexts>
<context position="5967" citStr="Forbes-Riley and Litman, 2009" startWordPosition="911" endWordPosition="914">e, 2011). For dialogue systems involving embodied conversational agents, gender effects have been found for facial displays, with females preferring more expressive agents (Foster and Oberlander, 2006). When used for tutoring, females report more positive affect when a learning companion is used, while males are more negative (Woolf et al., 2010). In our own prior work, we compared two uncertainty-adaptive and one non-adaptive versions of a wizarded dialogue system. Our results demonstrated that only one method of adapting to user uncertainty increased task success, and only for female users (Forbes-Riley and Litman, 2009). In this paper we extend this line of research, by adding an affective dialogue system that adapts to two rather than just one user state to our evaluation, and by moving from wizarded to fullyautomated systems. 3 System, Experiment and Corpus Our corpus consists of dialogues between users and three different versions of ITSPOKE (Intelligent Tutoring SPOKEn dialog system) (Forbes-Riley and Litman, 2011; ForbesRiley and Litman, 2012). ITSPOKE is a speech-enhanced and otherwise modified version of the Why2-Atlas text-based qualitative physics tutor (VanLehn et al., 2002) that interacts with use</context>
</contexts>
<marker>Forbes-Riley, Litman, 2009</marker>
<rawString>K. Forbes-Riley and D. Litman. 2009. A user modeling-based performance analysis of a wizarded uncertainty-adaptive dialogue system corpus. In Proceedings Interspeech, pages 2467–2470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>Benefits and challenges of real-time uncertainty detection and adaptation in a spoken dialogue computer tutor.</title>
<date>2011</date>
<journal>Speech Communication,</journal>
<pages>53--9</pages>
<contexts>
<context position="2314" citStr="Forbes-Riley and Litman, 2011" startWordPosition="357" endWordPosition="361">dapt to affect at all. Our evaluation examines the impact of adapting to differing numbers of affective states on task success, and also examines interactions with user gender. We target disengagement and uncertainty because these were the 1We use the term affect to describe emotions and attitudes that impact how people communicate. Other researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). most frequent affective states in prior studies with our system and their presence was negatively correlated with task success2 (Forbes-Riley and Litman, 2011; Forbes-Riley and Litman, 2012). The detection of these and similar states is also of interest to the larger speech and language processing communities, e.g. (Wang and Hirschberg, 2011; Bohus and Horvitz, 2009; Pon-Barry and Shieber, 2011). Our results suggest that while adapting to affect increases task success compared to not adapting at all, the utility of our current methods varies with user gender. Also, we find no difference between adapting to one or two states. 2 Related Work 2.1 Adapting to Multiple Affective States While prior research has shown that users display a range of affecti</context>
<context position="6373" citStr="Forbes-Riley and Litman, 2011" startWordPosition="977" endWordPosition="980">ve and one non-adaptive versions of a wizarded dialogue system. Our results demonstrated that only one method of adapting to user uncertainty increased task success, and only for female users (Forbes-Riley and Litman, 2009). In this paper we extend this line of research, by adding an affective dialogue system that adapts to two rather than just one user state to our evaluation, and by moving from wizarded to fullyautomated systems. 3 System, Experiment and Corpus Our corpus consists of dialogues between users and three different versions of ITSPOKE (Intelligent Tutoring SPOKEn dialog system) (Forbes-Riley and Litman, 2011; ForbesRiley and Litman, 2012). ITSPOKE is a speech-enhanced and otherwise modified version of the Why2-Atlas text-based qualitative physics tutor (VanLehn et al., 2002) that interacts with users using a system initiative dialogue strategy. User speech is first digitized from head-mounted microphone input and sent to the PocketSphinx recognizer.3 The recognition output is then classified as (in)correct with respect to the anticipated physics content via semantic analysis (Jordan et al., 2007). Simultaneously, user uncertainty (UNC) and disengagement (DISE) are classified from prosodic, lexica</context>
<context position="8651" citStr="Forbes-Riley and Litman, 2011" startWordPosition="1303" endWordPosition="1306">automatically detected user disengagement or uncertainty. In particular, after each correct answer, ITSPOKE provided positive feedback then moved on to the next topic. After incorrect answers, ITSPOKE instead provided negative 3http://www.speech.cs.cmu.edu/pocketsphinx 4We have not yet performed the manual annotations needed to evaluate our current versions of these components in isolation. However, earlier versions of our affect detectors yielded FMeasures of 69% and 68% for disengagement and uncertainty, respectively, on par with the best performing affect detectors in the wider literature (Forbes-Riley and Litman, 2011; Forbes-Riley et al., 2012). 5http://www.cepstral.com 6We did not include an uncertainty-only condition (UNC) because in previous work we compared UNC versus CONTROL (Forbes-Riley and Litman, 2011) and DISE+UNC versus UNC (Forbes-Riley and Litman, 2012). Further details and motivation for all experimental conditions can be found in the description of our earlier Wizard-of-Oz experiment (Forbes-Riley and Litman, 2012). 182 feedback, then provided remediation tutoring before moving on to the next topic. In DISE, two adaptive responses were developed to allow ITSPOKE’s responses to consider user</context>
</contexts>
<marker>Forbes-Riley, Litman, 2011</marker>
<rawString>K. Forbes-Riley and D. Litman. 2011. Benefits and challenges of real-time uncertainty detection and adaptation in a spoken dialogue computer tutor. Speech Communication, 53(9–10):1115–1136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
</authors>
<title>Adapting to multiple affective states in spoken dialogue.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12,</booktitle>
<pages>217--226</pages>
<contexts>
<context position="2346" citStr="Forbes-Riley and Litman, 2012" startWordPosition="362" endWordPosition="365">uation examines the impact of adapting to differing numbers of affective states on task success, and also examines interactions with user gender. We target disengagement and uncertainty because these were the 1We use the term affect to describe emotions and attitudes that impact how people communicate. Other researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). most frequent affective states in prior studies with our system and their presence was negatively correlated with task success2 (Forbes-Riley and Litman, 2011; Forbes-Riley and Litman, 2012). The detection of these and similar states is also of interest to the larger speech and language processing communities, e.g. (Wang and Hirschberg, 2011; Bohus and Horvitz, 2009; Pon-Barry and Shieber, 2011). Our results suggest that while adapting to affect increases task success compared to not adapting at all, the utility of our current methods varies with user gender. Also, we find no difference between adapting to one or two states. 2 Related Work 2.1 Adapting to Multiple Affective States While prior research has shown that users display a range of affective states during spoken dialogue</context>
<context position="8905" citStr="Forbes-Riley and Litman, 2012" startWordPosition="1338" endWordPosition="1341">edu/pocketsphinx 4We have not yet performed the manual annotations needed to evaluate our current versions of these components in isolation. However, earlier versions of our affect detectors yielded FMeasures of 69% and 68% for disengagement and uncertainty, respectively, on par with the best performing affect detectors in the wider literature (Forbes-Riley and Litman, 2011; Forbes-Riley et al., 2012). 5http://www.cepstral.com 6We did not include an uncertainty-only condition (UNC) because in previous work we compared UNC versus CONTROL (Forbes-Riley and Litman, 2011) and DISE+UNC versus UNC (Forbes-Riley and Litman, 2012). Further details and motivation for all experimental conditions can be found in the description of our earlier Wizard-of-Oz experiment (Forbes-Riley and Litman, 2012). 182 feedback, then provided remediation tutoring before moving on to the next topic. In DISE, two adaptive responses were developed to allow ITSPOKE’s responses to consider user disengagement as well as the correctness of the user’s answer;7 however, this system version still ignored user uncertainty. In particular, after each disengaged+correct answer, ITSPOKE provided correctness feedback, a progress chart showing user correc</context>
</contexts>
<marker>Forbes-Riley, Litman, 2012</marker>
<rawString>K. Forbes-Riley and D. Litman. 2012. Adapting to multiple affective states in spoken dialogue. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12, pages 217–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Forbes-Riley</author>
<author>D Litman</author>
<author>H Friedberg</author>
<author>J Drummond</author>
</authors>
<title>Intrinsic and extrinsic evaluation of an automatic user disengagement detector for an uncertainty-adaptive spoken dialogue system.</title>
<date>2012</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>91--102</pages>
<contexts>
<context position="4492" citStr="Forbes-Riley et al., 2012" startWordPosition="690" endWordPosition="693">ntic analysis, and affect detection components of our dialogue system. We also conducted a post2Our success measure is learning gain (Section 4). 181 Proceedings of the SIGDIAL 2014 Conference, pages 181–185, Philadelphia, U.S.A., 18-20 June 2014. c�2014 Association for Computational Linguistics hoc correlational (rather than causal) study, using data from an earlier fully-automated version of the uncertainty-adaptive system. Regressions demonstrated that using both automatically labeled disengagement and uncertainty to predict task success significantly outperformed using only disengagement (Forbes-Riley et al., 2012). However, if manual labels were instead used, only disengagement was predictive of learning, and adding uncertainty didn’t help. This suggests that detecting multiple affective states might compensate for the noise that is introduced in a fully-automated system. In this paper we further investigate this hypothesis, by evaluating the utility of adapting to zero, one, or two affective states in a controlled experiment involving fully-automated systems. 2.2 Gender Effects in Dialogue Differences in dialogue structure have been found between male and female students talking to a human tutor (Boye</context>
<context position="8679" citStr="Forbes-Riley et al., 2012" startWordPosition="1307" endWordPosition="1310">engagement or uncertainty. In particular, after each correct answer, ITSPOKE provided positive feedback then moved on to the next topic. After incorrect answers, ITSPOKE instead provided negative 3http://www.speech.cs.cmu.edu/pocketsphinx 4We have not yet performed the manual annotations needed to evaluate our current versions of these components in isolation. However, earlier versions of our affect detectors yielded FMeasures of 69% and 68% for disengagement and uncertainty, respectively, on par with the best performing affect detectors in the wider literature (Forbes-Riley and Litman, 2011; Forbes-Riley et al., 2012). 5http://www.cepstral.com 6We did not include an uncertainty-only condition (UNC) because in previous work we compared UNC versus CONTROL (Forbes-Riley and Litman, 2011) and DISE+UNC versus UNC (Forbes-Riley and Litman, 2012). Further details and motivation for all experimental conditions can be found in the description of our earlier Wizard-of-Oz experiment (Forbes-Riley and Litman, 2012). 182 feedback, then provided remediation tutoring before moving on to the next topic. In DISE, two adaptive responses were developed to allow ITSPOKE’s responses to consider user disengagement as well as th</context>
</contexts>
<marker>Forbes-Riley, Litman, Friedberg, Drummond, 2012</marker>
<rawString>K. Forbes-Riley, D. Litman, H. Friedberg, and J. Drummond. 2012. Intrinsic and extrinsic evaluation of an automatic user disengagement detector for an uncertainty-adaptive spoken dialogue system. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 91–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Foster</author>
<author>J Oberlander</author>
</authors>
<title>Data-driven generation of emphatic facial displays.</title>
<date>2006</date>
<booktitle>Proceedings of EACL,</booktitle>
<pages>2006--353</pages>
<contexts>
<context position="5538" citStr="Foster and Oberlander, 2006" startWordPosition="844" endWordPosition="847">nt involving fully-automated systems. 2.2 Gender Effects in Dialogue Differences in dialogue structure have been found between male and female students talking to a human tutor (Boyer et al., 2007). Studies have also shown gender differences in conversational entrainment patterns, for acoustic-prosodic features in human-human dialogues (Levitan et al., 2012) and articles in movie conversations (DanescuNiculescu-Mizil and Lee, 2011). For dialogue systems involving embodied conversational agents, gender effects have been found for facial displays, with females preferring more expressive agents (Foster and Oberlander, 2006). When used for tutoring, females report more positive affect when a learning companion is used, while males are more negative (Woolf et al., 2010). In our own prior work, we compared two uncertainty-adaptive and one non-adaptive versions of a wizarded dialogue system. Our results demonstrated that only one method of adapting to user uncertainty increased task success, and only for female users (Forbes-Riley and Litman, 2009). In this paper we extend this line of research, by adding an affective dialogue system that adapts to two rather than just one user state to our evaluation, and by moving</context>
</contexts>
<marker>Foster, Oberlander, 2006</marker>
<rawString>M. Foster and J. Oberlander. 2006. Data-driven generation of emphatic facial displays. Proceedings of EACL, 2006:353–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jordan</author>
<author>B Hall</author>
<author>M Ringenberg</author>
<author>Y Cui</author>
<author>C P Rose</author>
</authors>
<title>Tools for authoring a dialogue agent that participates in learning studies.</title>
<date>2007</date>
<booktitle>In Proc. Artificial Intelligence in Education (AIED),</booktitle>
<pages>43--50</pages>
<contexts>
<context position="6871" citStr="Jordan et al., 2007" startWordPosition="1049" endWordPosition="1052">n users and three different versions of ITSPOKE (Intelligent Tutoring SPOKEn dialog system) (Forbes-Riley and Litman, 2011; ForbesRiley and Litman, 2012). ITSPOKE is a speech-enhanced and otherwise modified version of the Why2-Atlas text-based qualitative physics tutor (VanLehn et al., 2002) that interacts with users using a system initiative dialogue strategy. User speech is first digitized from head-mounted microphone input and sent to the PocketSphinx recognizer.3 The recognition output is then classified as (in)correct with respect to the anticipated physics content via semantic analysis (Jordan et al., 2007). Simultaneously, user uncertainty (UNC) and disengagement (DISE) are classified from prosodic, lexical and contextual features using two binary classification models (ForbesRiley et al., 2012). All statistical components of the speech recognizer, the semantic analyzer, and the uncertainty and disengagement detectors were trained using prior ITSPOKE corpora.4 Finally, ITSPOKE’s response is determined based on the answer’s automatically labeled (in)correctness, (un)certainty, and (dis)engagement and then sent to the Cepstral text-to-speech system,5 as well as displayed on a web-based interface.</context>
</contexts>
<marker>Jordan, Hall, Ringenberg, Cui, Rose, 2007</marker>
<rawString>P. Jordan, B. Hall, M. Ringenberg, Y. Cui, and C.P. Rose. 2007. Tools for authoring a dialogue agent that participates in learning studies. In Proc. Artificial Intelligence in Education (AIED), pages 43–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levitan</author>
<author>A Gravano</author>
<author>L Willson</author>
<author>S Benus</author>
<author>J Hirschberg</author>
<author>A Nenkova</author>
</authors>
<title>Acousticprosodic entrainment and social behavior.</title>
<date>2012</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>11--19</pages>
<contexts>
<context position="5270" citStr="Levitan et al., 2012" startWordPosition="807" endWordPosition="810">g multiple affective states might compensate for the noise that is introduced in a fully-automated system. In this paper we further investigate this hypothesis, by evaluating the utility of adapting to zero, one, or two affective states in a controlled experiment involving fully-automated systems. 2.2 Gender Effects in Dialogue Differences in dialogue structure have been found between male and female students talking to a human tutor (Boyer et al., 2007). Studies have also shown gender differences in conversational entrainment patterns, for acoustic-prosodic features in human-human dialogues (Levitan et al., 2012) and articles in movie conversations (DanescuNiculescu-Mizil and Lee, 2011). For dialogue systems involving embodied conversational agents, gender effects have been found for facial displays, with females preferring more expressive agents (Foster and Oberlander, 2006). When used for tutoring, females report more positive affect when a learning companion is used, while males are more negative (Woolf et al., 2010). In our own prior work, we compared two uncertainty-adaptive and one non-adaptive versions of a wizarded dialogue system. Our results demonstrated that only one method of adapting to u</context>
</contexts>
<marker>Levitan, Gravano, Willson, Benus, Hirschberg, Nenkova, 2012</marker>
<rawString>R. Levitan, A. Gravano, L. Willson, S. Benus, J. Hirschberg, and A. Nenkova. 2012. Acousticprosodic entrainment and social behavior. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 11–19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Pon-Barry</author>
<author>S Shieber</author>
</authors>
<title>Recognizing uncertainty in speech.</title>
<date>2011</date>
<booktitle>EURASIP Journal on Advances in Signal Processing.</booktitle>
<contexts>
<context position="2554" citStr="Pon-Barry and Shieber, 2011" startWordPosition="395" endWordPosition="398">We use the term affect to describe emotions and attitudes that impact how people communicate. Other researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). most frequent affective states in prior studies with our system and their presence was negatively correlated with task success2 (Forbes-Riley and Litman, 2011; Forbes-Riley and Litman, 2012). The detection of these and similar states is also of interest to the larger speech and language processing communities, e.g. (Wang and Hirschberg, 2011; Bohus and Horvitz, 2009; Pon-Barry and Shieber, 2011). Our results suggest that while adapting to affect increases task success compared to not adapting at all, the utility of our current methods varies with user gender. Also, we find no difference between adapting to one or two states. 2 Related Work 2.1 Adapting to Multiple Affective States While prior research has shown that users display a range of affective states during spoken dialogue (e.g. (Schuller et al., 2009)), only a few dialogue systems have been developed that can adapt to more than one user affective state (e.g., (D’Mello et al., 2010; Acosta and Ward, 2011)). Furthermore, prior </context>
</contexts>
<marker>Pon-Barry, Shieber, 2011</marker>
<rawString>H. Pon-Barry and S. Shieber. 2011. Recognizing uncertainty in speech. EURASIP Journal on Advances in Signal Processing.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B Schuller</author>
<author>S Steidl</author>
<author>A Batliner</author>
</authors>
<title>The Interspeech</title>
<date>2009</date>
<booktitle>In Proc. Interspeech,</booktitle>
<pages>pages</pages>
<contexts>
<context position="2976" citStr="Schuller et al., 2009" startWordPosition="465" endWordPosition="468">tection of these and similar states is also of interest to the larger speech and language processing communities, e.g. (Wang and Hirschberg, 2011; Bohus and Horvitz, 2009; Pon-Barry and Shieber, 2011). Our results suggest that while adapting to affect increases task success compared to not adapting at all, the utility of our current methods varies with user gender. Also, we find no difference between adapting to one or two states. 2 Related Work 2.1 Adapting to Multiple Affective States While prior research has shown that users display a range of affective states during spoken dialogue (e.g. (Schuller et al., 2009)), only a few dialogue systems have been developed that can adapt to more than one user affective state (e.g., (D’Mello et al., 2010; Acosta and Ward, 2011)). Furthermore, prior evaluations have compared adapting to at least one affective state to not adapting to affect at all, but have not examined the benefits of adapting to one versus multiple affective states. In a first evaluation comparing singly and multiply affect-adaptive dialogue systems, we compared an existing system that adapted to uncertainty to a new version that also adapted to disengagement (Forbes-Riley and Litman, 2012). The</context>
</contexts>
<marker>Schuller, Steidl, Batliner, 2009</marker>
<rawString>B. Schuller, S. Steidl, and A. Batliner. 2009. The Interspeech 2009 emotion challenge. In Proc. Interspeech, pages 312–315, ISCA, Brighton, UK. K. VanLehn, P. W. Jordan, C. Ros´e, D. Bhembe, M. B¨ottner, A. Gaydos, M. Makatchev, U. Pappuswamy, M. Ringenberg, A. Roque, S. Siler, R. Srivastava, and R. Wilson. 2002. The architecture of Why2-Atlas: A coach for qualitative physics essay writing. In Proc. Intl. Conf. on Intelligent Tutoring Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Wang</author>
<author>J Hirschberg</author>
</authors>
<title>Detecting levels of interest from spoken dialog with multistream prediction feedback and similarity based hierarchical fusion learning.</title>
<date>2011</date>
<booktitle>In Proc. SIGDIAL,</booktitle>
<pages>152--161</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="2499" citStr="Wang and Hirschberg, 2011" startWordPosition="387" endWordPosition="390">sengagement and uncertainty because these were the 1We use the term affect to describe emotions and attitudes that impact how people communicate. Other researchers also combine concepts of emotion, arousal, and attitudes where emotion is not full-blown (Cowie and Cornelius, 2003). most frequent affective states in prior studies with our system and their presence was negatively correlated with task success2 (Forbes-Riley and Litman, 2011; Forbes-Riley and Litman, 2012). The detection of these and similar states is also of interest to the larger speech and language processing communities, e.g. (Wang and Hirschberg, 2011; Bohus and Horvitz, 2009; Pon-Barry and Shieber, 2011). Our results suggest that while adapting to affect increases task success compared to not adapting at all, the utility of our current methods varies with user gender. Also, we find no difference between adapting to one or two states. 2 Related Work 2.1 Adapting to Multiple Affective States While prior research has shown that users display a range of affective states during spoken dialogue (e.g. (Schuller et al., 2009)), only a few dialogue systems have been developed that can adapt to more than one user affective state (e.g., (D’Mello et </context>
</contexts>
<marker>Wang, Hirschberg, 2011</marker>
<rawString>W. Wang and J. Hirschberg. 2011. Detecting levels of interest from spoken dialog with multistream prediction feedback and similarity based hierarchical fusion learning. In Proc. SIGDIAL, pages 152–161, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Woolf</author>
<author>I Arroyo</author>
<author>D Cooper</author>
<author>W Burleson</author>
<author>K Muldner</author>
</authors>
<title>Affective tutors: Automatic detection of and response to student emotion.</title>
<date>2010</date>
<booktitle>In Advances in Intelligent Tutoring Systems,</booktitle>
<pages>207--227</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5685" citStr="Woolf et al., 2010" startWordPosition="868" endWordPosition="871">ing to a human tutor (Boyer et al., 2007). Studies have also shown gender differences in conversational entrainment patterns, for acoustic-prosodic features in human-human dialogues (Levitan et al., 2012) and articles in movie conversations (DanescuNiculescu-Mizil and Lee, 2011). For dialogue systems involving embodied conversational agents, gender effects have been found for facial displays, with females preferring more expressive agents (Foster and Oberlander, 2006). When used for tutoring, females report more positive affect when a learning companion is used, while males are more negative (Woolf et al., 2010). In our own prior work, we compared two uncertainty-adaptive and one non-adaptive versions of a wizarded dialogue system. Our results demonstrated that only one method of adapting to user uncertainty increased task success, and only for female users (Forbes-Riley and Litman, 2009). In this paper we extend this line of research, by adding an affective dialogue system that adapts to two rather than just one user state to our evaluation, and by moving from wizarded to fullyautomated systems. 3 System, Experiment and Corpus Our corpus consists of dialogues between users and three different versio</context>
</contexts>
<marker>Woolf, Arroyo, Cooper, Burleson, Muldner, 2010</marker>
<rawString>B. Woolf, I. Arroyo, D. Cooper, W. Burleson, and K. Muldner. 2010. Affective tutors: Automatic detection of and response to student emotion. In Advances in Intelligent Tutoring Systems, pages 207– 227. Springer.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Appendix</author>
</authors>
<title>Affect-Adaptive Examples. A.1 After a Predicted Disengaged+Correct Turn</title>
<marker>Appendix, </marker>
<rawString>Appendix A. Affect-Adaptive Examples. A.1 After a Predicted Disengaged+Correct Turn</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
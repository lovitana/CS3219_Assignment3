<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.998425">
Preliminary Test of a Real-Time, Interactive Silent Speech Interface
Based on Electromagnetic Articulograph
</title>
<author confidence="0.998172">
Jun Wang
</author>
<affiliation confidence="0.97011675">
Dept. of Bioengineering
Callier Center for Communi-
cation Disorders
University of Texas at Dallas
</affiliation>
<email confidence="0.999044">
wangjun@utdallas.edu
</email>
<author confidence="0.973265">
Ashok Samal
</author>
<affiliation confidence="0.90078125">
Dept. of Computer Science &amp;
Engineering
University of Nebraska-
Lincoln
</affiliation>
<email confidence="0.999054">
samal@cse.unl.edu
</email>
<author confidence="0.987383">
Jordan R. Green
</author>
<affiliation confidence="0.81974675">
Dept. of Communication Sci-
ences &amp; Disorders
MGH Institute of Health Pro-
fessions
</affiliation>
<email confidence="0.998541">
jgreen2@mghihp.edu
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999851827586207">
A silent speech interface (SSI) maps articula-
tory movement data to speech output. Alt-
hough still in experimental stages, silent
speech interfaces hold significant potential
for facilitating oral communication in persons
after laryngectomy or with other severe voice
impairments. Despite the recent efforts on si-
lent speech recognition algorithm develop-
ment using offline data analysis, online test
of SSIs have rarely been conducted. In this
paper, we present a preliminary, online test of
a real-time, interactive SSI based on electro-
magnetic motion tracking. The SSI played
back synthesized speech sounds in response
to the user’s tongue and lip movements.
Three English talkers participated in this test,
where they mouthed (silently articulated)
phrases using the device to complete a
phrase-reading task. Among the three partici-
pants, 96.67% to 100% of the mouthed
phrases were correctly recognized and corre-
sponding synthesized sounds were played af-
ter a short delay. Furthermore, one participant
demonstrated the feasibility of using the SSI
for a short conversation. The experimental re-
sults demonstrated the feasibility and poten-
tial of silent speech interfaces based on elec-
tromagnetic articulograph for future clinical
applications.
</bodyText>
<sectionHeader confidence="0.999336" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9969495">
Daily communication is often a struggle for per-
sons who have undergone a laryngectomy, a sur-
gical removal of the larynx due to the treatment
of cancer (Bailey et al., 2006). In 2013, about
12,260 new cases of laryngeal cancer were esti-
mated in the United States (American Cancer
</bodyText>
<page confidence="0.987926">
38
</page>
<bodyText confidence="0.990821575">
Society, 2013). Currently, there are only limited
treatment options for these individuals including
(1) esophageal speech, which involves oscillation
of the esophagus and is difficult to learn; (2) tra-
cheo-esophageal speech, in which a voice pros-
thesis is placed in a tracheo-esophageal puncture;
and (3) electrolarynx, an external device held on
the neck during articulation, which produces a
robotic voice quality (Liu and Ng, 2007). Per-
haps the greatest disadvantage of these ap-
proaches is that they produce abnormal sounding
speech with a fundamental frequency that is low
and limited in range. The abnormal voice quality
output severely affects the social life of people
after laryngectomy (Liu and Ng, 2007). In addi-
tion, the tracheo-esophageal option requires an
additional surgery, which is not suitable for eve-
ry patient (Bailey et al., 2006). Although re-
search is being conducted on improving the
voice quality of esophageal or electrolarynx
speech (Doi et al., 2010; Toda et al., 2012), new
assistive technologies based on non-audio infor-
mation (e.g., visual or articulatory information)
may be a good alternative approach for providing
natural sounding speech output for persons after
laryngectomy.
Visual speech recognition (or automatic lip
reading) typically uses an optical camera to ob-
tain lip and/or facial features during speech (in-
cluding lip contour, color, opening, movement,
etc.) and then classify these features to speech
units (Meier et al., 2000; Oviatt, 2003). Howev-
er, due to the lack of information from tongue,
the primary articulator, visual speech recognition
(i.e., using visual information only, without
tongue and audio information) may obtain a low
accuracy (e.g., 30% - 40% for phoneme classifi-
cation, Livescu et al., 2007). Furthermore, Wang
and colleagues (2013b) have showed any single
tongue sensor (from tongue tip to tongue body
</bodyText>
<note confidence="0.6354705">
Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 38–45,
Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998387">
Figure 1. Design of the real-time silent speech interface.
</figureCaption>
<bodyText confidence="0.99990314084507">
back on the midsagittal line) encodes significant-
ly more information in distinguishing phonemes
than do lips. However, visual speech recognition
is well suited for applications with small-
vocabulary (e.g., a lip-reading based command-
and-control system for home appliance) or using
visual information as an additional source for
acoustic speech recognition, referred to as audio-
visual speech recognition (Potamianos et al.,
2003), because such a system based on portable
camera is convenient in practical use. In contrast,
SSIs, with tongue information, have potential to
obtain a high level of silent speech recognition
accuracy (without audio information). Currently,
two major obstacles for SSI development are
lack of (a) fast and accurate recognition algo-
rithms and (b) portable tongue motion tracking
devices for daily use.
SSIs convert articulatory information into text
that drives a text-to-speech synthesizer. Although
still in developmental stages (e.g., speaker-
dependent recognition, small-vocabulary), SSIs
even have potential to provide speech output
based on prerecorded samples of the patient’s
own voice (Denby et al., 2010; Green et al.,
2011; Wang et al., 2009). Potential articulatory
data acquisition methods for SSIs include ultra-
sound (Denby et al., 2011; Hueber et al., 2010),
surface electromyography electrodes (Heaton et
al., 2011; Jorgensen and Dusan, 2010), and elec-
tromagnetic articulograph (EMA) (Fagan et al.,
2008; Wang et al., 2009, 2012a).
Despite the recent effort on silent speech in-
terface research, online test of SSIs has rarely
been studied. So far, most of the published work
on SSIs has focused on development of silent
speech recognition algorithm through offline
analysis (i.e., using prerecorded data) (Fagan et
al., 2008; Heaton et al., 2011; Hofe et al., 2013;
Hueber et al., 2010; Jorgenson et al., 2010; Wang
et al., 2009a, 2012a, 2012b, 2013c). Ultrasound-39
based SSIs have been tested online with multiple
subjects and encouraging results were obtained
in a phrase reading task where the subjects were
asked to silently articulate sixty phrases (Denby
et al., 2011). SSI based on electromagnetic sens-
ing has been only tested using offline analysis
(using pre-recorded data) collected from single
subjects (Fagan et al., 2008; Hofe et al., 2013),
although some work simulated online testing
using prerecorded data (Wang et al., 2012a,
2012b, 2013c). Online tests of SSIs using elec-
tromagnetic articulograph with multiple subjects
are needed to show the feasibility and potential
of the SSIs for future clinical applications.
In this paper, we report a preliminary, online
test of a newly-developed, real-time, and interac-
tive SSI based on a commercial EMA. EMA
tracks articulatory motion by placing small sen-
sors on the surface of tongue and other articula-
tors (e.g., lips and jaw). EMA is well suited for
the early state of SSI development because it (1)
is non-invasive, (2) has a high spatial resolution
in motion tracking, (3) has a high sampling rate,
and (4) is affordable. In this experiment, partici-
pants used the real-time SSI to complete an
online phrase-reading task and one of them had a
short conversation with another person. The re-
sults demonstrated the feasibility and potential of
SSIs based on electromagnetic sensing for future
clinical applications.
</bodyText>
<sectionHeader confidence="0.999585" genericHeader="introduction">
2 Design
</sectionHeader>
<subsectionHeader confidence="0.998726">
2.1 Major design
</subsectionHeader>
<bodyText confidence="0.998606428571429">
Figure 1 illustrates the three-component design
of the SSI: (a) real-time articulatory motion
tracking using a commercial EMA, (b) online
silent speech recognition (converting articulation
information to text), and (c) text-to-speech syn-
thesis for speech output.
The EMA system (Wave Speech Research
</bodyText>
<figureCaption confidence="0.982607333333333">
Figure 2. Demo of a participant using the silent speech interface. The left picture illustrates the
coordinate system and sensor locations (sensor labels are described in text); in the right picture, a
participant is using the silent speech interface to finish the online test.
</figureCaption>
<bodyText confidence="0.999877833333333">
system, Northern Digital Inc., Waterloo, Canada)
was used to track the tongue and lip movement
in real-time. The sampling rate of the Wave sys-
tem was 100 Hz, which is adequate for this ap-
plication (Wang et al., 2012a, 2012b, 2013c).
The spatial accuracy of motion tracking using
Wave is 0.5 mm (Berry, 2011).
The online recognition component recognized
functional phrases from articulatory movements
in real-time. The recognition component is mod-
ular such that alternative classifiers can easily
replace and be integrated into the SSI. In this
preliminary test, recognition was speaker-
dependent, where training and testing data were
from the same speakers.
The third component played back either pre-
recorded or synthesized sounds using a text-to-
speech synthesizer (Huang et al., 1997).
</bodyText>
<subsectionHeader confidence="0.997227">
2.2 Other designs
</subsectionHeader>
<bodyText confidence="0.999114888888889">
A graphical user interface (GUI) is integrated
into the silent speech interface for ease of opera-
tion. Using the GUI, users can instantly re-train
the recognition engine (classifier) when new
training samples are available. Users can also
switch output voice (e.g., male or female).
Data transfer through TCP/IP. Data transfer
from the Wave system to the recognition unit
(software) is accomplished through TCP/IP, the
standard data transfer protocols on Internet. Be-
cause data bandwidth requirement is low (multi-
ple sensors, multiple spatial coordinates for each
sensor, at 100 Hz sampling rate), any 3G or fast-
er network connection will be sufficient for fu-
ture use with wireless data transfer.
Extensible (closed) vocabulary. In the early
stage of this development, closed-vocabulary
silent speech recognition was used; however, the
</bodyText>
<page confidence="0.980481">
40
</page>
<bodyText confidence="0.998187166666667">
vocabulary is extensible. Users can add new
phrases into the system through the GUI. Adding
a new phrase in the vocabulary is done in two
steps. The user (the patient) first enters the
phrase using a keyboard (keyboard input can also
be done by an assistant or speech pathologist),
and then produces a few training samples for the
phrase (a training sample is articulatory data la-
beled with a phrase). The system automatically
re-trains the recognition model integrating the
newly-added training samples. Users can delete
invalid training samples using the GUI as well.
</bodyText>
<subsectionHeader confidence="0.99773">
2.3 Real-time data processing
</subsectionHeader>
<bodyText confidence="0.99293364">
The tongue and lip movement positional data
obtained from the Wave system were processed
in real-time prior to being used for recognition.
This included the calculation of head-
independent positions of the tongue and lip sen-
sors and low pass filtering for removing noise.
The movements of the 6 DOF head sensor
were used to calculate the head-independent
movements of other sensors. The Wave system
represents object orientation or rotation (denoted
by yaw, pitch, and roll in Euler angles) in qua-
ternions, a four-dimensional vector. Quaternion
has its advantages over Euler angles. For exam-
ple, quaternion avoids the issue of gimbal lock
(one degree of freedom may be lost in a series of
rotation), and it is simpler to achieve smooth in-
terpolation using quaternion than using Euler
angles (Dam et al., 1998). Thus, quaternion has
been widely used in computer graphics, comput-
er vision, robotics, virtual reality, and flight dy-
namics (Kuipers, 1999). Given the unit quaterni-
on
q = (a, b, c, d) (1)
where a2 + b2 + c2 + d2 = 1, a 3 × 3 rotation ma-
trix R can be derived using Equation (2):
</bodyText>
<equation confidence="0.994337">
a2 + b2 − c2 − d 2 2bc − 2ad 2bd + 2ac

R=2bc+2ad a2−b2+c2−d2 2cd−2ab
 2bd − 2ac 2cd + 2ab a2 − b2 −c2 + d 2
</equation>
<bodyText confidence="0.99928075">
For details of how the quaternion is used in
Wave system, please refer to the Wave Real-
Time API manual and sample application
(Northern Digital Inc., Waterloo, Canada).
</bodyText>
<sectionHeader confidence="0.968875" genericHeader="method">
3 A Preliminary Online Test
</sectionHeader>
<subsectionHeader confidence="0.998559">
3.1 Participants &amp; Stimuli
</subsectionHeader>
<bodyText confidence="0.999952363636364">
Three American English talkers participated in
this experiment (two males and one female with
average age 25 and SD 3.5 years). No history of
speech, language, hearing, or any cognitive prob-
lems were reported.
Sixty phrases that are frequently used in daily
life by healthy people and AAC (augmentative
and alternative communication) users were used
in this experiment. Those phrases were selected
from the lists in Wang et al., 2012a and Beukel-
man and Gutmann, 1999.
</bodyText>
<subsectionHeader confidence="0.7293685">
3.2 Procedure
Setup
</subsectionHeader>
<bodyText confidence="0.999811178571428">
The Wave system tracks the motion of sensors
attached on the articulators by establishing an
electromagnetic field by a textbook-sized genera-
tor. Participants were seated with their head
within the calibrated magnetic field (Figure 2,
the right picture), facing a computer monitor that
displays the GUI of the SSI. The sensors were
attached to the surface of each articulator using
dental glue (PeriAcryl Oral Tissue Adhesive).
Prior to the experiment, each subject participated
in a three-minute training session (on how to use
the SSI), which also helped them adapt to the
oral sensors. Previous studies have shown those
sensors do not significantly affect their speech
output after a short practice (Katz et al., 2006;
Weismer and Bunton, 1999).
Figure 2 (left) shows the positions of the five
sensors attached to a participant’s forehead,
tongue, and lips (Green et al., 2003; 2013; Wang
et al., 2013a). One 6 DOF (spatial and rotational)
head sensor was attached to a nose bridge on a
pair of glasses (rather than on forehead skin di-
rectly), to avoid the skin artifact (Green et al.,
2007). Two 5 DOF sensors - TT (Tongue Tip)
and TB (Tongue Body Back) - were attached on
the midsagittal of the tongue. TT was located
approximately 10 mm from the tongue apex
(Wang et al., 2011, 2013a). TB was placed as far
</bodyText>
<page confidence="0.994342">
41
</page>
<bodyText confidence="0.986784542857143">
back as possible, depending on the participant’s
tongue length (Wang et al., 2013b). Lip move-
ments were captured by attaching two 5 DOF
sensors to the vermilion borders of the upper
(UL) and lower (LL) lips at midline. The four
sensors (i.e., TT, TB, UL, and LL) placements
were selected based on literature showing that
they are able to achieve as high recognition accu-
racy as that obtained using more tongue sensors
for this application (Wang et al., 2013b).
As mentioned previously, real-time prepro-
cessing of the positional time series was con-
ducted, including subtraction of head movements
from tongue and lip data and noise reduction us-
ing a 20 Hz low pass filter (Green et al., 2003;
Wang et al., 2013a). Although the tongue and lip
sensors are 5D, only the 3D spatial data (i.e., x, y,
and z) were used in this experiment.
Training
The training step was conducted to obtain a few
samples for each phrase. The participants were
asked to silently articulate all sixty phrases twice
at their comfortable speaking rate, while the
tongue and lip motion was recorded. Thus, each
phrase has at least two samples for training. Dy-
namic Time Warping (DTW) was used as the
classifier in this preliminary test, because of its
rapid execution (Denby et al., 2011), although
Gaussian mixture models may perform well too
when the number of training samples is small
(Broekx et al., 2013). DTW is typically used to
compare two single-dimensional time-series,
Training_Algorithm
Let T1... Tn be the sets of training samples for n
phrases, where
</bodyText>
<equation confidence="0.948271375">
Ti = {Ti,1, ... Ti,j, ... Ti,mi} are mi samples for
phrase i.
1 for i = 1 to n // n is the number of phrases
2 Li = sum(length(Ti,j)) / mi, j = 1 to mi;
3 T = Ti,1; // first sample of phrase i
3 for j = 2 to mi
4 (T&apos;, T&apos;i,j) = MDTW(T, Ti,j);
5 T = (T&apos; + T&apos;i,j) / 2;//amplitude mean
</equation>
<figure confidence="0.6684214">
6 T = time_normalize(T, Li);
7 end
8 Ri = T; // representative sample for phrase i
9 end
10 Output(R);
</figure>
<figureCaption confidence="0.94498025">
Figure 3. Training algorithm using DTW. The
function call MDTW() returns the average
DTW distances between the corresponding
sensors and dimensions of two data samples.
</figureCaption>
<figure confidence="0.9875384">



 
(2)
</figure>
<bodyText confidence="0.99683912">
thus we calculated the average DTW distance
across the corresponding sensors and dimensions
of two data samples. DTW was adapted as fol-
lows for training.
The training algorithm generated a repre-
sentative sample based on all available training
samples for each phrase. Pseudo-code of the
training algorithm is provided in Figure 3, for the
convenience of description. For each phrase i,
first, MDTW was applied to the first two training
samples, Ti,1 and Ti,2. Sample T is the amplitude-
mean of the warped samples T&apos;i,1 and T&apos;i,2 (time-
series) (Line 5). Next, T was time-normalized
(stretched) to the average length of all training
samples for this phrase (Li), which was to reduce
the effects of duration change caused by DTW
warping (Line 6). The procedure continued until
the last training sample Ti, mi (mi is the number of
training samples for phrase i). The final T was
the representative sample for phrase i.
The training procedure can be initiated by
pressing a button on the GUI anytime during the
use of the SSI.
Testing
During testing, each participant silently articulat-
ed the same list of phrases while the SSI recog-
nized each phrase and played corresponding syn-
thesized sounds. DTW was used to compare the
test sample with the representative training sam-
ple for each phrase (Ri, Figure 3). The phrase that
had the shortest DTW distance to the test sample
was recognized. The testing was triggered by
pressing a button on the GUI. If the phrase was
incorrectly predicted, the participant was allowed
to add at most two additional training samples
for that phrase.
Figure 2 (right) demonstrates a participant is
using the SSI during the test. After the partici-
pant silently articulated “Good afternoon”, the
SSI displayed the phrase on the screen, and
played the corresponding synthesized sound
simultaneously.
Finally, one participant used the SSI for a bidi-
rectional conversation with an investigator. Since
this prototype SSI has a closed-vocabulary
recognition component, the participant had to
choose the phrases that have been trained. This
task was intended to provide a demo of how the
SSI is used for daily communication. The script
of the conversation is as below:
</bodyText>
<footnote confidence="0.387550333333333">
Investigator: Hi DJ, How are you?
Subject: I’m fine. How are you doing?
Investigator: I’m good. Thanks.
</footnote>
<table confidence="0.9906216">
Subject Accuracy Latency # of Train-
(%) (s) ing Samples
S01 100 3.086 2.0
S02 96.67 1.403 2.4
S03 96.67 1.524 3.1
</table>
<tableCaption confidence="0.998082">
Table 1. Phrase classification accuracy and
latency for all three participants.
</tableCaption>
<bodyText confidence="0.2577586">
Subject: I use a silent speech interface to talk.
Investigator: That’s cool.
Subject: Do you understand me?
Investigator: Oh, yes.
Subject: That’s good.
</bodyText>
<sectionHeader confidence="0.998018" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999509837837838">
Table 1 lists the performance using the SSI for
all three participants in the online, phrase-
reading task. The three subjects obtained a
phrase recognition accuracy from 96.67% to
100.00%, with a latency from 1.403 second to
3.086 seconds, respectively. The high accuracy
and relatively short delay demonstrated the fea-
sibility and potential of SSIs based on electro-
magnetic articulograph.
The order of the participants in the experiment
was S01, S02, and then S03. After the experi-
ment of S01, where all three dimensional data (x,
y, and z) were used, we decided to use only y and
z for S02 and S03 to reduce the latency. As listed
in Table 1, the latencies of S02 and S03 did sig-
nificantly reduce, because less data was used for
online recognition.
Surprisingly, phrase recognition without using
x dimension (left-right) data led to a decrease of
accuracy and more training samples were re-
quired; prior research suggests that tongue
movement in this dimension is not significant
during speech in healthy talkers (Westbury,
1994). This observation is supported by partici-
pant S01, who had the highest accuracy and
needed fewer training samples for each phrase
(column 3 in Table 1). S02 and S03 used data of
only y and z dimensions. Of course, data from
more subjects are needed to confirm the potential
significance of the x dimension movement of the
tongue to silent speech recognition accuracy.
Data transfer between the Wave machine and
the SSI recognition component was done through
TCP/IP protocols and in real-time. In the future,
this design feature will allow the recognition
component to run on a smart phone or any wear-
able devices with an Internet connection (Cellu-
</bodyText>
<page confidence="0.998015">
42
</page>
<bodyText confidence="0.999829240740741">
lar or Wi-Fi). In this preliminary test, the indi-
vidual delays caused by TCP/IP data transfer,
online data preprocessing, and classification
were not measured and thus unknown. The delay
information may be useful for our future devel-
opment that the recognition component is de-
ployed on a smart-phone. A further study is
needed to obtain and analyze the delay infor-
mation.
The bidirectional dialogue by one of the par-
ticipants demonstrated how the SSI can be used
in daily conversation. To our best knowledge,
this is the first conversational demo using a SSI.
An informal survey to a few colleagues provided
positive feedback. The conversation was smooth,
although it is noticeably slower than a conversa-
tion between two healthy talkers. Importantly,
the voice output quality (determined by the text-
to-speech synthesizer) was natural, which strong-
ly supports the major motivation of SSI research:
to produce speech with natural voice quality that
current treatments cannot provide. A video demo
is available online (Wang, 2014).
The participants in this experiment were
young and healthy. It is, however, unknown if
the recognition accuracy may decrease or not for
users after laryngectomy, although a single pa-
tient study showed the accuracy may decrease
15-20% compared to healthy talkers using an
ultrasound-based SSI (Denby et al., 2011). Theo-
retically, the tongue motion patterns in (silent)
speech after the surgery should be no difference
with that of healthy talkers. In practice, however,
some patients after the surgery may be under
treatment for swallowing using radioactive de-
vices, which may affect their tongue motion pat-
terns in articulation. Thus, the performance of
SSIs may vary and depend on the condition of
the patients after laryngectomy. A test of the SSI
using multiple participants after laryngectomy is
needed to understand the performance of SSIs
for those patients under different conditions.
Although a demonstration of daily conversa-
tion using the SSI is provided, SSI based on the
non-portable Wave system is currently not ready
for practical use. Fortunately, more affordable
and portable electromagnetic devices are being
developed as are small handheld or wearable de-
vices (Fagan et al., 2008). Researchers are also
testing the efficacy of permanently implantable
and wireless sensors (Chen et al., 2012; Park et
al., 2012). In the future, those more portable, and
wireless articulatory motion tracking devices,
when they are ready, will be used to develop a
</bodyText>
<page confidence="0.998901">
43
</page>
<bodyText confidence="0.984603894736842">
portable SSI for practice use.
In this experiment, a simple DTW algorithm
was used to compare the training and testing
phrases, which is known to be slower than most
machine learning classifiers. Thus, in the future,
the latency can be significantly reduced by using
faster classifiers such as support vector machines
(Wang et al., 2013c) or hidden Markov models
(Heracleous and Hagita, 2011; King et al., 2007;
Rudzicz et al., 2012; Uraga and Hain, 2006).
Furthermore, in this proof-of-concept design,
the vocabulary was limited to a small set of
phrases, because our design required the whole
experiment (including training and testing) to be
done in about one hour. Additional work is need-
ed to test the feasibility of open-vocabulary
recognition, which will be much more usable for
people after laryngectomy or with other severe
voice impairments.
</bodyText>
<sectionHeader confidence="0.977613" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999981842105263">
A preliminary, online test of a SSI based on elec-
tromagnetic articulograph was conducted. The
results were encouraging revealing high phrase
recognition accuracy and short playback laten-
cies among three participants in a phrase-reading
task. In addition, a proof-of-concept demo of
bidirectional conversation using the SSI was
provided, which shows how the SSI can be used
for daily communication.
Future work includes: (1) testing the SSI with
patients after laryngectomy or with severe voice
impairment, (2) integrating a phoneme- or word-
level recognition (open-vocabulary) using faster
machine learning classifiers (e.g., support vector
machines or hidden Markov models), and (3)
exploring speaker-independent silent speech
recognition algorithms by normalizing the articu-
latory movement across speakers (e.g., due to the
anatomical difference of their tongues).
</bodyText>
<sectionHeader confidence="0.970229" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998709230769231">
This work was in part supported by the Callier
Excellence in Education Fund, University of
Texas at Dallas, and grants awarded by the Na-
tional Institutes of Health (R01 DC009890 and
R01 DC013547). We would like to thank Dr.
Thomas F. Campbell, Dr. William F. Katz, Dr.
Gregory S. Lee, Dr. Jennell C. Vick, Lindsey
Macy, Marcus Jones, Kristin J. Teplansky, Ve-
dad “Kelly” Fazel, Loren Montgomery, and
Kameron Johnson for their support or assistance.
We also thank the anonymous reviewers for their
comments and suggestions for improving the
quality of this paper.
</bodyText>
<note confidence="0.70454425">
References
American Cancer Society. 2013. Cancer Facts and
Figures 2013. American Cancer Society, Atlanta,
GA. Retrieved on February 18, 2014.
</note>
<reference confidence="0.998222775510204">
Bailey, B. J., Johnson, J. T., and Newlands, S. D.
2006. Head and Neck Surgery – Otolaryngology,
Lippincot, Williams &amp; Wilkins, Philadelphia, PA,
USA, 4th Ed., 1779-1780.
Berry, J. 2011. Accuracy of the NDI wave speech
research system, Journal of Speech, Language, and
Hearing Research, 54:1295-1301.
Beukelman, D. R., and Gutmann, M. 1999. Generic
Message List for AAC users with ALS.
http://aac.unl.edu/ALS_Message_List1.htm
Broekx, L., Dreesen, K., Gemmeke, J. F., and Van
Hamme, H. 2013. Comparing and combining clas-
sifiers for self-taught vocal interfaces, ACL/ISCA
Workshop on Speech and Language Processing for
Assistive Technologies, 21-28, 2013.
Chen, W.-H., Loke, W.-F., Thompson, G., and Jung,
B. 2012. A 0.5V, 440uW frequency synthesizer for
implantable medical devices, IEEE Journal of Sol-
id-State Circuits, 47:1896-1907.
Dam, E. B., Koch, M., and Lillholm, M. 1998. Qua-
ternions, interpolation and animation. Technical
Report DIKU-TR-98/5, University of Copenhagen.
Denby, B., Cai, J., Roussel, P., Dreyfus, G., Crevier-
Buchman, L., Pillot-Loiseau, C., Hueber, and T.,
Chollet, G. 2011. Tests of an interactive, phrase-
book-style post-laryngectomy voice-replacement
system, the 17th International Congress on Pho-
netic Sciences, Hong Kong, China, 572-575.
Denby, B., Schultz, T., Honda, K., Hueber, T., Gil-
bert, J. M., and Brumberg, J. S. 2010. Silent speech
interface, Speech Communication, 52:270-287.
Doi, H., Nakamura, K., Toda, T., Saruwatari, H., Shi-
kano, K. 2010. Esophageal speech enhancement
based on statistical voice conversion with Gaussian
mixture models, IEICE Transactions on Infor-
mation and Systems, E93-D, 9:2472-2482.
Fagan, M. J., Ell, S. R., Gilbert, J. M., Sarrazin, E.,
and Chapman, P. M. 2008. Development of a (si-
lent) speech recognition system for patients follow-
ing laryngectomy, Medical Engineering &amp; Physics,
30(4):419-425.
Green, P. D., Khan, Z., Creer, S. M. and Cunningham,
S. P. 2011. Reconstructing the voice of an individ-
ual following Laryngectomy, Augmentative and Al-
ternative Communication, 27(1):61-66.
Green, J. R., Wang, J., and Wilson, D. L. 2013.
SMASH: A tool for articulatory data processing
and analysis, Proc. Interspeech, 1331-35.
Green, J. R. and Wang, Y. 2003. Tongue-surface
</reference>
<page confidence="0.863079">
44
</page>
<reference confidence="0.998604007092199">
movement patterns during speech and swallowing,
Journal of the Acoustical Society of America,
113:2820-2833.
Hofe, R., Ell, S. R., Fagan, M. J., Gilbert, J. M.,
Green, P. D., Moore, R. K., and Rybchenko, S. I.
2013. Small-vocabulary speech recognition using a
silent speech interface based on magnetic sensing,
Speech Communication, 55(1):22-32.
Hofe, R., Ell, S. R., Fagan, M. J., Gilbert, J. M.,
Green, P. D., Moore, R. K., and Rybchenko, S. I.
2011. Speech Synthesis Parameter Generation for
the Assistive Silent Speech Interface MVOCA,
Proc. Interspeech, 3009-3012.
Huang, X. D., Acero, A., Hon, H.-W., Ju, Y.-C., Liu,
J., Meredith, S., and Plumpe, M. 1997. Recent Im-
provements on Microsoft’s Trainable Text-to-
Speech System: Whistler, Proc. IEEE Intl. Conf.
on Acoustics, Speech, and Signal Processing, 959-
962.
Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B.,
Dreyfus, G., Stone, M. 2010. Development of a si-
lent speech interface driven by ultrasound and opti-
cal images of the tongue and lips, Speech Commu-
nication, 52:288–300.
Heaton, J. T., Robertson, M., and Griffin, C. 2011.
Development of a wireless electromyographically
controlled electrolarynx voice prosthesis, Proc. of
the 33rd Annual Intl. Conf. of the IEEE Engineer-
ing in Medicine &amp; Biology Society, Boston, MA,
5352-5355.
Heracleous, P., and Hagita, N. 2011. Automatic
recognition of speech without any audio infor-
mation, Proc. IEEE Intl. Conf. on Acoustics,
Speech, and Signal Processing, 2392-2395.
Jorgensen, C. and Dusan, S. 2010. Speech interfaces
based upon surface electromyography, Speech
Communication, 52:354–366, 2010.
Katz, W., Bharadwaj, S., Rush, M., and Stettler, M.
2006. Influences of EMA receiver coils on speech
production by normal and aphasic/apraxic talkers,
Journal of Speech, Language, and Hearing Re-
search, 49:645-659.
Kent, R. D., Adams, S. G., and Tuner, G. S. 1996.
Models of speech production, in Principles of Ex-
perimental Phonetics, Ed., Lass, N. J., Mosby: St
Louis, MO.
King, S., Frankel, J. Livescu, K., McDermott, E.,
Richmond, K., Wester, M. 2007. Speech produc-
tion knowledge in automatic speech recognition,
Journal of the Acoustical Society of America,
121(2):723-742.
Kuipers, J. B. 1999. Quaternions and rotation Se-
quences: a Primer with Applications to Orbits,
Aerospace, and Virtual Reality, Princeton Univer-
sity Press, Princeton, NJ.
Liu, H., and Ng, M. L. 2007. Electrolarynx in voice
rehabilitation, Auris Nasus Larynx, 34(3): 327-332.
Livescu, K., Çetin, O., Hasegawa-Johnson, Mark,
King, S., Bartels, C., Borges, N., Kantor, A., et al.
(2007). Articulatory feature-based methods for
acoustic and audio-visual speech recognition:
Summary from the 2006 JHU Summer Workshop.
Proc. IEEE Intl. Conf. on Acoustics, Speech, and
Signal Processing, 621-624.
Meier, U., Stiefelhagen, R., Yang, J., and Waibel, A.
(2000). Towards Unrestricted Lip Reading. Inter-
national Journal of Pattern Recognition and Artifi-
cial Intelligence, 14(5): 571-585.
Oviatt, S. L. 2003. Multimodal interfaces, in Human–
Computer Interaction Handbook: Fundamentals,
Evolving Technologies and Emerging Applications,
Eds. Julie A. Jacko and Andrew Sears (Mahwah,
NJ:Erlbaum): 286–304.
Park, H., Kiani, M., Lee, H. M., Kim, J., Block, J.,
Gosselin, B., and Ghovanloo, M. 2012. A wireless
magnetoresistive sensing system for an intraoral
tongue-computer interface, IEEE Transactions on
Biomedical Circuits and Systems, 6(6):571-585.
Potamianos, G., Neti, C., Cravier, G., Garg, A. and
Senior, A. W. 2003. Recent advances in the auto-
matic recognition of audio-visual speech, Proc. of
IEEE, 91(9):1306-1326.
Rudzicz, F., Hirst, G., Van Lieshout, P. 2012. Vocal
tract representation in the recognition of cerebral
palsied speech, Journal of Speech, Language, and
Hearing Research, 55(4): 1190-1207.
Toda, T., Nakagiri, M., Shikano, K. 2012. Statistical
voice conversion techniques for body-conducted
unvoiced speech enhancement, IEEE Transactions
on Audio, Speech and Language Processing, 20(9):
2505-2517.
Uraga, E. and Hain, T. 2006. Automatic speech
recognition experiments with articulatory data,
Proc. Inerspeech, 353-356.
Wang, J., Samal, A., Green, J. R., and Carrell, T. D.
2009. Vowel recognition from articulatory position
time-series data, Proc. IEEE Intl. Conf. on Signal
Processing and Communication Systems, Omaha,
NE, 1-6.
Wang, J., Green, J. R., Samal, A., and Marx, D. B.
2011. Quantifying articulatory distinctiveness of
vowels, Proc. Interspeech, Florence, Italy, 277-
280.
Wang, J., Samal, A., Green, J. R., and Rudzicz, F.
2012a. Sentence recognition from articulatory
movements for silent speech interfaces, Proc. IEEE
Intl. Conf. on Acoustics, Speech, and Signal Pro-
cessing, 4985-4988.
Wang, J., Samal, A., Green, J. R., and Rudzicz, F.
2012b. Whole-word recognition from articulatory
movements for silent speech interfaces, Proc. In-
terspeech, 1327-30.
Wang, J., Green, J. R., Samal, A. and Yunusova, Y.
2013a. Articulatory distinctiveness of vowels and
consonants: A data-driven approach, Journal of
Speech, Language, and Hearing Research, 56,
1539-1551.
Wang, J., Green, J. R., and Samal, A. 2013b. Individ-
ual articulator&apos;s contribution to phoneme produc-
tion, Proc. IEEE Intl. Conf. on Acoustics, Speech,
and Signal Processing, Vancouver, Canada, 7795-
89.
Wang, J., Balasubramanian, A., Mojica de La Vega,
L., Green, J. R., Samal, A., and Prabhakaran, B.
2013c. Word recognition from continuous articula-
tory movement time-series data using symbolic
representations, ACL/ISCA Workshop on Speech
and Language Processing for Assistive Technolo-
gies, Grenoble, France, 119-127.
Wang J. 2014. DJ and his friend: A demo of conver-
sation using a real-time silent speech interface
based on electromagnetic articulograph. [Video].
Available: http://www.utdallas.edu/~wangjun/ssi-
demo.html
Weismer, G. and Bunton, K. (1999). Influences of
pellet markers on speech production behavior:
Acoustical and perceptual measures, Journal of the
Acoustical Society of America, 105: 2882-2891.
Westbury, J. 1994. X-ray microbeam speech produc-
tion database user’s handbook. University of Wis-
consin-Madison, Madison, Wisconsin.
</reference>
<page confidence="0.999392">
45
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.104551">
<title confidence="0.989536">Preliminary Test of a Real-Time, Interactive Silent Speech Based on Electromagnetic Articulograph</title>
<author confidence="0.971542">Jun</author>
<affiliation confidence="0.970981">Dept. of Center for cation University of Texas at Dallas</affiliation>
<email confidence="0.999558">wangjun@utdallas.edu</email>
<author confidence="0.640985">Ashok</author>
<affiliation confidence="0.957256">Dept. of Computer Science</affiliation>
<address confidence="0.528892">of</address>
<author confidence="0.459095">Lincoln</author>
<email confidence="0.999588">samal@cse.unl.edu</email>
<author confidence="0.999983">Jordan R Green</author>
<affiliation confidence="0.987988333333333">of Communication ences &amp; Institute of Health</affiliation>
<email confidence="0.855006">fessionsjgreen2@mghihp.edu</email>
<abstract confidence="0.9994606">A silent speech interface (SSI) maps articulatory movement data to speech output. Although still in experimental stages, silent speech interfaces hold significant potential for facilitating oral communication in persons after laryngectomy or with other severe voice impairments. Despite the recent efforts on silent speech recognition algorithm development using offline data analysis, online test of SSIs have rarely been conducted. In this paper, we present a preliminary, online test of a real-time, interactive SSI based on electromagnetic motion tracking. The SSI played back synthesized speech sounds in response to the user’s tongue and lip movements. Three English talkers participated in this test, where they mouthed (silently articulated) phrases using the device to complete a phrase-reading task. Among the three participants, 96.67% to 100% of the mouthed phrases were correctly recognized and corresponding synthesized sounds were played after a short delay. Furthermore, one participant demonstrated the feasibility of using the SSI for a short conversation. The experimental results demonstrated the feasibility and potential of silent speech interfaces based on electromagnetic articulograph for future clinical applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B J Bailey</author>
<author>J T Johnson</author>
<author>S D Newlands</author>
</authors>
<date>2006</date>
<booktitle>Head and Neck Surgery – Otolaryngology,</booktitle>
<pages>1779--1780</pages>
<publisher>Wilkins,</publisher>
<location>Lippincot, Williams</location>
<contexts>
<context position="1890" citStr="Bailey et al., 2006" startWordPosition="274" endWordPosition="277">mong the three participants, 96.67% to 100% of the mouthed phrases were correctly recognized and corresponding synthesized sounds were played after a short delay. Furthermore, one participant demonstrated the feasibility of using the SSI for a short conversation. The experimental results demonstrated the feasibility and potential of silent speech interfaces based on electromagnetic articulograph for future clinical applications. 1 Introduction Daily communication is often a struggle for persons who have undergone a laryngectomy, a surgical removal of the larynx due to the treatment of cancer (Bailey et al., 2006). In 2013, about 12,260 new cases of laryngeal cancer were estimated in the United States (American Cancer 38 Society, 2013). Currently, there are only limited treatment options for these individuals including (1) esophageal speech, which involves oscillation of the esophagus and is difficult to learn; (2) tracheo-esophageal speech, in which a voice prosthesis is placed in a tracheo-esophageal puncture; and (3) electrolarynx, an external device held on the neck during articulation, which produces a robotic voice quality (Liu and Ng, 2007). Perhaps the greatest disadvantage of these approaches </context>
</contexts>
<marker>Bailey, Johnson, Newlands, 2006</marker>
<rawString>Bailey, B. J., Johnson, J. T., and Newlands, S. D. 2006. Head and Neck Surgery – Otolaryngology, Lippincot, Williams &amp; Wilkins, Philadelphia, PA, USA, 4th Ed., 1779-1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berry</author>
</authors>
<title>Accuracy of the NDI wave speech research system,</title>
<date>2011</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<pages>54--1295</pages>
<contexts>
<context position="8368" citStr="Berry, 2011" startWordPosition="1275" endWordPosition="1276">stem (Wave Speech Research Figure 2. Demo of a participant using the silent speech interface. The left picture illustrates the coordinate system and sensor locations (sensor labels are described in text); in the right picture, a participant is using the silent speech interface to finish the online test. system, Northern Digital Inc., Waterloo, Canada) was used to track the tongue and lip movement in real-time. The sampling rate of the Wave system was 100 Hz, which is adequate for this application (Wang et al., 2012a, 2012b, 2013c). The spatial accuracy of motion tracking using Wave is 0.5 mm (Berry, 2011). The online recognition component recognized functional phrases from articulatory movements in real-time. The recognition component is modular such that alternative classifiers can easily replace and be integrated into the SSI. In this preliminary test, recognition was speakerdependent, where training and testing data were from the same speakers. The third component played back either prerecorded or synthesized sounds using a text-tospeech synthesizer (Huang et al., 1997). 2.2 Other designs A graphical user interface (GUI) is integrated into the silent speech interface for ease of operation. </context>
</contexts>
<marker>Berry, 2011</marker>
<rawString>Berry, J. 2011. Accuracy of the NDI wave speech research system, Journal of Speech, Language, and Hearing Research, 54:1295-1301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Beukelman</author>
<author>M Gutmann</author>
</authors>
<title>Generic Message List for AAC users with ALS.</title>
<date>1999</date>
<note>http://aac.unl.edu/ALS_Message_List1.htm</note>
<contexts>
<context position="12196" citStr="Beukelman and Gutmann, 1999" startWordPosition="1904" endWordPosition="1908">e refer to the Wave RealTime API manual and sample application (Northern Digital Inc., Waterloo, Canada). 3 A Preliminary Online Test 3.1 Participants &amp; Stimuli Three American English talkers participated in this experiment (two males and one female with average age 25 and SD 3.5 years). No history of speech, language, hearing, or any cognitive problems were reported. Sixty phrases that are frequently used in daily life by healthy people and AAC (augmentative and alternative communication) users were used in this experiment. Those phrases were selected from the lists in Wang et al., 2012a and Beukelman and Gutmann, 1999. 3.2 Procedure Setup The Wave system tracks the motion of sensors attached on the articulators by establishing an electromagnetic field by a textbook-sized generator. Participants were seated with their head within the calibrated magnetic field (Figure 2, the right picture), facing a computer monitor that displays the GUI of the SSI. The sensors were attached to the surface of each articulator using dental glue (PeriAcryl Oral Tissue Adhesive). Prior to the experiment, each subject participated in a three-minute training session (on how to use the SSI), which also helped them adapt to the ora</context>
</contexts>
<marker>Beukelman, Gutmann, 1999</marker>
<rawString>Beukelman, D. R., and Gutmann, M. 1999. Generic Message List for AAC users with ALS. http://aac.unl.edu/ALS_Message_List1.htm</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Broekx</author>
<author>K Dreesen</author>
<author>J F Gemmeke</author>
<author>H Van Hamme</author>
</authors>
<title>Comparing and combining classifiers for self-taught vocal interfaces,</title>
<date>2013</date>
<booktitle>ACL/ISCA Workshop on Speech and Language Processing for Assistive Technologies,</booktitle>
<pages>21--28</pages>
<marker>Broekx, Dreesen, Gemmeke, Van Hamme, 2013</marker>
<rawString>Broekx, L., Dreesen, K., Gemmeke, J. F., and Van Hamme, H. 2013. Comparing and combining classifiers for self-taught vocal interfaces, ACL/ISCA Workshop on Speech and Language Processing for Assistive Technologies, 21-28, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W-H Chen</author>
<author>W-F Loke</author>
<author>G Thompson</author>
<author>B Jung</author>
</authors>
<title>A 0.5V, 440uW frequency synthesizer for implantable medical devices,</title>
<date>2012</date>
<journal>IEEE Journal of Solid-State Circuits,</journal>
<pages>47--1896</pages>
<contexts>
<context position="22314" citStr="Chen et al., 2012" startWordPosition="3596" endWordPosition="3599">tion of the patients after laryngectomy. A test of the SSI using multiple participants after laryngectomy is needed to understand the performance of SSIs for those patients under different conditions. Although a demonstration of daily conversation using the SSI is provided, SSI based on the non-portable Wave system is currently not ready for practical use. Fortunately, more affordable and portable electromagnetic devices are being developed as are small handheld or wearable devices (Fagan et al., 2008). Researchers are also testing the efficacy of permanently implantable and wireless sensors (Chen et al., 2012; Park et al., 2012). In the future, those more portable, and wireless articulatory motion tracking devices, when they are ready, will be used to develop a 43 portable SSI for practice use. In this experiment, a simple DTW algorithm was used to compare the training and testing phrases, which is known to be slower than most machine learning classifiers. Thus, in the future, the latency can be significantly reduced by using faster classifiers such as support vector machines (Wang et al., 2013c) or hidden Markov models (Heracleous and Hagita, 2011; King et al., 2007; Rudzicz et al., 2012; Uraga a</context>
</contexts>
<marker>Chen, Loke, Thompson, Jung, 2012</marker>
<rawString>Chen, W.-H., Loke, W.-F., Thompson, G., and Jung, B. 2012. A 0.5V, 440uW frequency synthesizer for implantable medical devices, IEEE Journal of Solid-State Circuits, 47:1896-1907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E B Dam</author>
<author>M Koch</author>
<author>M Lillholm</author>
</authors>
<title>Quaternions, interpolation and animation.</title>
<date>1998</date>
<tech>Technical Report DIKU-TR-98/5,</tech>
<institution>University of Copenhagen.</institution>
<contexts>
<context position="11112" citStr="Dam et al., 1998" startWordPosition="1703" endWordPosition="1706">itions of the tongue and lip sensors and low pass filtering for removing noise. The movements of the 6 DOF head sensor were used to calculate the head-independent movements of other sensors. The Wave system represents object orientation or rotation (denoted by yaw, pitch, and roll in Euler angles) in quaternions, a four-dimensional vector. Quaternion has its advantages over Euler angles. For example, quaternion avoids the issue of gimbal lock (one degree of freedom may be lost in a series of rotation), and it is simpler to achieve smooth interpolation using quaternion than using Euler angles (Dam et al., 1998). Thus, quaternion has been widely used in computer graphics, computer vision, robotics, virtual reality, and flight dynamics (Kuipers, 1999). Given the unit quaternion q = (a, b, c, d) (1) where a2 + b2 + c2 + d2 = 1, a 3 × 3 rotation matrix R can be derived using Equation (2): a2 + b2 − c2 − d 2 2bc − 2ad 2bd + 2ac  R=2bc+2ad a2−b2+c2−d2 2cd−2ab  2bd − 2ac 2cd + 2ab a2 − b2 −c2 + d 2 For details of how the quaternion is used in Wave system, please refer to the Wave RealTime API manual and sample application (Northern Digital Inc., Waterloo, Canada). 3 A Preliminary Online Test 3.1 Parti</context>
</contexts>
<marker>Dam, Koch, Lillholm, 1998</marker>
<rawString>Dam, E. B., Koch, M., and Lillholm, M. 1998. Quaternions, interpolation and animation. Technical Report DIKU-TR-98/5, University of Copenhagen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Denby</author>
<author>J Cai</author>
<author>P Roussel</author>
<author>G Dreyfus</author>
<author>L CrevierBuchman</author>
<author>C Pillot-Loiseau</author>
<author>Hueber</author>
<author>T Chollet</author>
<author>G</author>
</authors>
<title>Tests of an interactive, phrasebook-style post-laryngectomy voice-replacement system,</title>
<date>2011</date>
<booktitle>the 17th International Congress on Phonetic Sciences,</booktitle>
<pages>572--575</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="5412" citStr="Denby et al., 2011" startWordPosition="803" endWordPosition="806">Currently, two major obstacles for SSI development are lack of (a) fast and accurate recognition algorithms and (b) portable tongue motion tracking devices for daily use. SSIs convert articulatory information into text that drives a text-to-speech synthesizer. Although still in developmental stages (e.g., speakerdependent recognition, small-vocabulary), SSIs even have potential to provide speech output based on prerecorded samples of the patient’s own voice (Denby et al., 2010; Green et al., 2011; Wang et al., 2009). Potential articulatory data acquisition methods for SSIs include ultrasound (Denby et al., 2011; Hueber et al., 2010), surface electromyography electrodes (Heaton et al., 2011; Jorgensen and Dusan, 2010), and electromagnetic articulograph (EMA) (Fagan et al., 2008; Wang et al., 2009, 2012a). Despite the recent effort on silent speech interface research, online test of SSIs has rarely been studied. So far, most of the published work on SSIs has focused on development of silent speech recognition algorithm through offline analysis (i.e., using prerecorded data) (Fagan et al., 2008; Heaton et al., 2011; Hofe et al., 2013; Hueber et al., 2010; Jorgenson et al., 2010; Wang et al., 2009a, 201</context>
<context position="14792" citStr="Denby et al., 2011" startWordPosition="2347" endWordPosition="2350"> using a 20 Hz low pass filter (Green et al., 2003; Wang et al., 2013a). Although the tongue and lip sensors are 5D, only the 3D spatial data (i.e., x, y, and z) were used in this experiment. Training The training step was conducted to obtain a few samples for each phrase. The participants were asked to silently articulate all sixty phrases twice at their comfortable speaking rate, while the tongue and lip motion was recorded. Thus, each phrase has at least two samples for training. Dynamic Time Warping (DTW) was used as the classifier in this preliminary test, because of its rapid execution (Denby et al., 2011), although Gaussian mixture models may perform well too when the number of training samples is small (Broekx et al., 2013). DTW is typically used to compare two single-dimensional time-series, Training_Algorithm Let T1... Tn be the sets of training samples for n phrases, where Ti = {Ti,1, ... Ti,j, ... Ti,mi} are mi samples for phrase i. 1 for i = 1 to n // n is the number of phrases 2 Li = sum(length(Ti,j)) / mi, j = 1 to mi; 3 T = Ti,1; // first sample of phrase i 3 for j = 2 to mi 4 (T&apos;, T&apos;i,j) = MDTW(T, Ti,j); 5 T = (T&apos; + T&apos;i,j) / 2;//amplitude mean 6 T = time_normalize(T, Li); 7 end 8 Ri </context>
<context position="21318" citStr="Denby et al., 2011" startWordPosition="3442" endWordPosition="3445">ealthy talkers. Importantly, the voice output quality (determined by the textto-speech synthesizer) was natural, which strongly supports the major motivation of SSI research: to produce speech with natural voice quality that current treatments cannot provide. A video demo is available online (Wang, 2014). The participants in this experiment were young and healthy. It is, however, unknown if the recognition accuracy may decrease or not for users after laryngectomy, although a single patient study showed the accuracy may decrease 15-20% compared to healthy talkers using an ultrasound-based SSI (Denby et al., 2011). Theoretically, the tongue motion patterns in (silent) speech after the surgery should be no difference with that of healthy talkers. In practice, however, some patients after the surgery may be under treatment for swallowing using radioactive devices, which may affect their tongue motion patterns in articulation. Thus, the performance of SSIs may vary and depend on the condition of the patients after laryngectomy. A test of the SSI using multiple participants after laryngectomy is needed to understand the performance of SSIs for those patients under different conditions. Although a demonstra</context>
</contexts>
<marker>Denby, Cai, Roussel, Dreyfus, CrevierBuchman, Pillot-Loiseau, Hueber, Chollet, G, 2011</marker>
<rawString>Denby, B., Cai, J., Roussel, P., Dreyfus, G., CrevierBuchman, L., Pillot-Loiseau, C., Hueber, and T., Chollet, G. 2011. Tests of an interactive, phrasebook-style post-laryngectomy voice-replacement system, the 17th International Congress on Phonetic Sciences, Hong Kong, China, 572-575.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Denby</author>
<author>T Schultz</author>
<author>K Honda</author>
<author>T Hueber</author>
<author>J M Gilbert</author>
<author>J S Brumberg</author>
</authors>
<title>Silent speech interface,</title>
<date>2010</date>
<journal>Speech Communication,</journal>
<pages>52--270</pages>
<contexts>
<context position="5275" citStr="Denby et al., 2010" startWordPosition="781" endWordPosition="784"> SSIs, with tongue information, have potential to obtain a high level of silent speech recognition accuracy (without audio information). Currently, two major obstacles for SSI development are lack of (a) fast and accurate recognition algorithms and (b) portable tongue motion tracking devices for daily use. SSIs convert articulatory information into text that drives a text-to-speech synthesizer. Although still in developmental stages (e.g., speakerdependent recognition, small-vocabulary), SSIs even have potential to provide speech output based on prerecorded samples of the patient’s own voice (Denby et al., 2010; Green et al., 2011; Wang et al., 2009). Potential articulatory data acquisition methods for SSIs include ultrasound (Denby et al., 2011; Hueber et al., 2010), surface electromyography electrodes (Heaton et al., 2011; Jorgensen and Dusan, 2010), and electromagnetic articulograph (EMA) (Fagan et al., 2008; Wang et al., 2009, 2012a). Despite the recent effort on silent speech interface research, online test of SSIs has rarely been studied. So far, most of the published work on SSIs has focused on development of silent speech recognition algorithm through offline analysis (i.e., using prerecorde</context>
</contexts>
<marker>Denby, Schultz, Honda, Hueber, Gilbert, Brumberg, 2010</marker>
<rawString>Denby, B., Schultz, T., Honda, K., Hueber, T., Gilbert, J. M., and Brumberg, J. S. 2010. Silent speech interface, Speech Communication, 52:270-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Doi</author>
<author>K Nakamura</author>
<author>T Toda</author>
<author>H Saruwatari</author>
<author>K Shikano</author>
</authors>
<title>Esophageal speech enhancement based on statistical voice conversion with Gaussian mixture models,</title>
<date>2010</date>
<journal>IEICE Transactions on Information and Systems,</journal>
<volume>93</volume>
<pages>9--2472</pages>
<contexts>
<context position="2976" citStr="Doi et al., 2010" startWordPosition="445" endWordPosition="448">ng articulation, which produces a robotic voice quality (Liu and Ng, 2007). Perhaps the greatest disadvantage of these approaches is that they produce abnormal sounding speech with a fundamental frequency that is low and limited in range. The abnormal voice quality output severely affects the social life of people after laryngectomy (Liu and Ng, 2007). In addition, the tracheo-esophageal option requires an additional surgery, which is not suitable for every patient (Bailey et al., 2006). Although research is being conducted on improving the voice quality of esophageal or electrolarynx speech (Doi et al., 2010; Toda et al., 2012), new assistive technologies based on non-audio information (e.g., visual or articulatory information) may be a good alternative approach for providing natural sounding speech output for persons after laryngectomy. Visual speech recognition (or automatic lip reading) typically uses an optical camera to obtain lip and/or facial features during speech (including lip contour, color, opening, movement, etc.) and then classify these features to speech units (Meier et al., 2000; Oviatt, 2003). However, due to the lack of information from tongue, the primary articulator, visual sp</context>
</contexts>
<marker>Doi, Nakamura, Toda, Saruwatari, Shikano, 2010</marker>
<rawString>Doi, H., Nakamura, K., Toda, T., Saruwatari, H., Shikano, K. 2010. Esophageal speech enhancement based on statistical voice conversion with Gaussian mixture models, IEICE Transactions on Information and Systems, E93-D, 9:2472-2482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Fagan</author>
<author>S R Ell</author>
<author>J M Gilbert</author>
<author>E Sarrazin</author>
<author>P M Chapman</author>
</authors>
<title>Development of a (silent) speech recognition system for patients following laryngectomy,</title>
<date>2008</date>
<journal>Medical Engineering &amp; Physics,</journal>
<pages>30--4</pages>
<contexts>
<context position="5581" citStr="Fagan et al., 2008" startWordPosition="827" endWordPosition="830">. SSIs convert articulatory information into text that drives a text-to-speech synthesizer. Although still in developmental stages (e.g., speakerdependent recognition, small-vocabulary), SSIs even have potential to provide speech output based on prerecorded samples of the patient’s own voice (Denby et al., 2010; Green et al., 2011; Wang et al., 2009). Potential articulatory data acquisition methods for SSIs include ultrasound (Denby et al., 2011; Hueber et al., 2010), surface electromyography electrodes (Heaton et al., 2011; Jorgensen and Dusan, 2010), and electromagnetic articulograph (EMA) (Fagan et al., 2008; Wang et al., 2009, 2012a). Despite the recent effort on silent speech interface research, online test of SSIs has rarely been studied. So far, most of the published work on SSIs has focused on development of silent speech recognition algorithm through offline analysis (i.e., using prerecorded data) (Fagan et al., 2008; Heaton et al., 2011; Hofe et al., 2013; Hueber et al., 2010; Jorgenson et al., 2010; Wang et al., 2009a, 2012a, 2012b, 2013c). Ultrasound-39 based SSIs have been tested online with multiple subjects and encouraging results were obtained in a phrase reading task where the subje</context>
<context position="22204" citStr="Fagan et al., 2008" startWordPosition="3580" endWordPosition="3583">ect their tongue motion patterns in articulation. Thus, the performance of SSIs may vary and depend on the condition of the patients after laryngectomy. A test of the SSI using multiple participants after laryngectomy is needed to understand the performance of SSIs for those patients under different conditions. Although a demonstration of daily conversation using the SSI is provided, SSI based on the non-portable Wave system is currently not ready for practical use. Fortunately, more affordable and portable electromagnetic devices are being developed as are small handheld or wearable devices (Fagan et al., 2008). Researchers are also testing the efficacy of permanently implantable and wireless sensors (Chen et al., 2012; Park et al., 2012). In the future, those more portable, and wireless articulatory motion tracking devices, when they are ready, will be used to develop a 43 portable SSI for practice use. In this experiment, a simple DTW algorithm was used to compare the training and testing phrases, which is known to be slower than most machine learning classifiers. Thus, in the future, the latency can be significantly reduced by using faster classifiers such as support vector machines (Wang et al.,</context>
</contexts>
<marker>Fagan, Ell, Gilbert, Sarrazin, Chapman, 2008</marker>
<rawString>Fagan, M. J., Ell, S. R., Gilbert, J. M., Sarrazin, E., and Chapman, P. M. 2008. Development of a (silent) speech recognition system for patients following laryngectomy, Medical Engineering &amp; Physics, 30(4):419-425.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Green</author>
<author>Z Khan</author>
<author>S M Creer</author>
<author>S P Cunningham</author>
</authors>
<title>Reconstructing the voice of an individual following Laryngectomy, Augmentative and Alternative Communication,</title>
<date>2011</date>
<pages>27--1</pages>
<contexts>
<context position="5295" citStr="Green et al., 2011" startWordPosition="785" endWordPosition="788">nformation, have potential to obtain a high level of silent speech recognition accuracy (without audio information). Currently, two major obstacles for SSI development are lack of (a) fast and accurate recognition algorithms and (b) portable tongue motion tracking devices for daily use. SSIs convert articulatory information into text that drives a text-to-speech synthesizer. Although still in developmental stages (e.g., speakerdependent recognition, small-vocabulary), SSIs even have potential to provide speech output based on prerecorded samples of the patient’s own voice (Denby et al., 2010; Green et al., 2011; Wang et al., 2009). Potential articulatory data acquisition methods for SSIs include ultrasound (Denby et al., 2011; Hueber et al., 2010), surface electromyography electrodes (Heaton et al., 2011; Jorgensen and Dusan, 2010), and electromagnetic articulograph (EMA) (Fagan et al., 2008; Wang et al., 2009, 2012a). Despite the recent effort on silent speech interface research, online test of SSIs has rarely been studied. So far, most of the published work on SSIs has focused on development of silent speech recognition algorithm through offline analysis (i.e., using prerecorded data) (Fagan et al</context>
</contexts>
<marker>Green, Khan, Creer, Cunningham, 2011</marker>
<rawString>Green, P. D., Khan, Z., Creer, S. M. and Cunningham, S. P. 2011. Reconstructing the voice of an individual following Laryngectomy, Augmentative and Alternative Communication, 27(1):61-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Green</author>
<author>J Wang</author>
<author>D L Wilson</author>
</authors>
<title>SMASH: A tool for articulatory data processing and analysis,</title>
<date>2013</date>
<booktitle>Proc. Interspeech,</booktitle>
<pages>1331--35</pages>
<marker>Green, Wang, Wilson, 2013</marker>
<rawString>Green, J. R., Wang, J., and Wilson, D. L. 2013. SMASH: A tool for articulatory data processing and analysis, Proc. Interspeech, 1331-35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Green</author>
<author>Y Wang</author>
</authors>
<title>Tongue-surface movement patterns during speech and swallowing,</title>
<date>2003</date>
<journal>Journal of the Acoustical Society of America,</journal>
<pages>113--2820</pages>
<marker>Green, Wang, 2003</marker>
<rawString>Green, J. R. and Wang, Y. 2003. Tongue-surface movement patterns during speech and swallowing, Journal of the Acoustical Society of America, 113:2820-2833.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hofe</author>
<author>S R Ell</author>
<author>M J Fagan</author>
<author>J M Gilbert</author>
<author>P D Green</author>
<author>R K Moore</author>
<author>S I Rybchenko</author>
</authors>
<title>Small-vocabulary speech recognition using a silent speech interface based on magnetic sensing,</title>
<date>2013</date>
<journal>Speech Communication,</journal>
<pages>55--1</pages>
<contexts>
<context position="5942" citStr="Hofe et al., 2013" startWordPosition="887" endWordPosition="890">l articulatory data acquisition methods for SSIs include ultrasound (Denby et al., 2011; Hueber et al., 2010), surface electromyography electrodes (Heaton et al., 2011; Jorgensen and Dusan, 2010), and electromagnetic articulograph (EMA) (Fagan et al., 2008; Wang et al., 2009, 2012a). Despite the recent effort on silent speech interface research, online test of SSIs has rarely been studied. So far, most of the published work on SSIs has focused on development of silent speech recognition algorithm through offline analysis (i.e., using prerecorded data) (Fagan et al., 2008; Heaton et al., 2011; Hofe et al., 2013; Hueber et al., 2010; Jorgenson et al., 2010; Wang et al., 2009a, 2012a, 2012b, 2013c). Ultrasound-39 based SSIs have been tested online with multiple subjects and encouraging results were obtained in a phrase reading task where the subjects were asked to silently articulate sixty phrases (Denby et al., 2011). SSI based on electromagnetic sensing has been only tested using offline analysis (using pre-recorded data) collected from single subjects (Fagan et al., 2008; Hofe et al., 2013), although some work simulated online testing using prerecorded data (Wang et al., 2012a, 2012b, 2013c). Onlin</context>
</contexts>
<marker>Hofe, Ell, Fagan, Gilbert, Green, Moore, Rybchenko, 2013</marker>
<rawString>Hofe, R., Ell, S. R., Fagan, M. J., Gilbert, J. M., Green, P. D., Moore, R. K., and Rybchenko, S. I. 2013. Small-vocabulary speech recognition using a silent speech interface based on magnetic sensing, Speech Communication, 55(1):22-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hofe</author>
<author>S R Ell</author>
<author>M J Fagan</author>
<author>J M Gilbert</author>
<author>P D Green</author>
<author>R K Moore</author>
<author>S I Rybchenko</author>
</authors>
<title>Speech Synthesis Parameter Generation for the Assistive Silent Speech Interface MVOCA,</title>
<date>2011</date>
<booktitle>Proc. Interspeech,</booktitle>
<pages>3009--3012</pages>
<marker>Hofe, Ell, Fagan, Gilbert, Green, Moore, Rybchenko, 2011</marker>
<rawString>Hofe, R., Ell, S. R., Fagan, M. J., Gilbert, J. M., Green, P. D., Moore, R. K., and Rybchenko, S. I. 2011. Speech Synthesis Parameter Generation for the Assistive Silent Speech Interface MVOCA, Proc. Interspeech, 3009-3012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X D Huang</author>
<author>A Acero</author>
<author>H-W Hon</author>
<author>Y-C Ju</author>
<author>J Liu</author>
<author>S Meredith</author>
<author>M Plumpe</author>
</authors>
<title>Recent Improvements on Microsoft’s Trainable Text-toSpeech System: Whistler,</title>
<date>1997</date>
<booktitle>Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>959--962</pages>
<contexts>
<context position="8845" citStr="Huang et al., 1997" startWordPosition="1342" endWordPosition="1345">s adequate for this application (Wang et al., 2012a, 2012b, 2013c). The spatial accuracy of motion tracking using Wave is 0.5 mm (Berry, 2011). The online recognition component recognized functional phrases from articulatory movements in real-time. The recognition component is modular such that alternative classifiers can easily replace and be integrated into the SSI. In this preliminary test, recognition was speakerdependent, where training and testing data were from the same speakers. The third component played back either prerecorded or synthesized sounds using a text-tospeech synthesizer (Huang et al., 1997). 2.2 Other designs A graphical user interface (GUI) is integrated into the silent speech interface for ease of operation. Using the GUI, users can instantly re-train the recognition engine (classifier) when new training samples are available. Users can also switch output voice (e.g., male or female). Data transfer through TCP/IP. Data transfer from the Wave system to the recognition unit (software) is accomplished through TCP/IP, the standard data transfer protocols on Internet. Because data bandwidth requirement is low (multiple sensors, multiple spatial coordinates for each sensor, at 100 H</context>
</contexts>
<marker>Huang, Acero, Hon, Ju, Liu, Meredith, Plumpe, 1997</marker>
<rawString>Huang, X. D., Acero, A., Hon, H.-W., Ju, Y.-C., Liu, J., Meredith, S., and Plumpe, M. 1997. Recent Improvements on Microsoft’s Trainable Text-toSpeech System: Whistler, Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, 959-962.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hueber</author>
<author>E-L Benaroya</author>
<author>G Chollet</author>
<author>B Denby</author>
<author>G Dreyfus</author>
<author>M Stone</author>
</authors>
<title>Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips,</title>
<date>2010</date>
<journal>Speech Communication,</journal>
<pages>52--288</pages>
<contexts>
<context position="5434" citStr="Hueber et al., 2010" startWordPosition="807" endWordPosition="810"> obstacles for SSI development are lack of (a) fast and accurate recognition algorithms and (b) portable tongue motion tracking devices for daily use. SSIs convert articulatory information into text that drives a text-to-speech synthesizer. Although still in developmental stages (e.g., speakerdependent recognition, small-vocabulary), SSIs even have potential to provide speech output based on prerecorded samples of the patient’s own voice (Denby et al., 2010; Green et al., 2011; Wang et al., 2009). Potential articulatory data acquisition methods for SSIs include ultrasound (Denby et al., 2011; Hueber et al., 2010), surface electromyography electrodes (Heaton et al., 2011; Jorgensen and Dusan, 2010), and electromagnetic articulograph (EMA) (Fagan et al., 2008; Wang et al., 2009, 2012a). Despite the recent effort on silent speech interface research, online test of SSIs has rarely been studied. So far, most of the published work on SSIs has focused on development of silent speech recognition algorithm through offline analysis (i.e., using prerecorded data) (Fagan et al., 2008; Heaton et al., 2011; Hofe et al., 2013; Hueber et al., 2010; Jorgenson et al., 2010; Wang et al., 2009a, 2012a, 2012b, 2013c). Ult</context>
</contexts>
<marker>Hueber, Benaroya, Chollet, Denby, Dreyfus, Stone, 2010</marker>
<rawString>Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B., Dreyfus, G., Stone, M. 2010. Development of a silent speech interface driven by ultrasound and optical images of the tongue and lips, Speech Communication, 52:288–300.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J T Heaton</author>
<author>M Robertson</author>
<author>C Griffin</author>
</authors>
<title>Development of a wireless electromyographically controlled electrolarynx voice prosthesis,</title>
<date>2011</date>
<booktitle>Proc. of the 33rd Annual Intl. Conf. of the IEEE Engineering in Medicine &amp; Biology Society,</booktitle>
<pages>5352--5355</pages>
<location>Boston, MA,</location>
<contexts>
<context position="5492" citStr="Heaton et al., 2011" startWordPosition="814" endWordPosition="817">urate recognition algorithms and (b) portable tongue motion tracking devices for daily use. SSIs convert articulatory information into text that drives a text-to-speech synthesizer. Although still in developmental stages (e.g., speakerdependent recognition, small-vocabulary), SSIs even have potential to provide speech output based on prerecorded samples of the patient’s own voice (Denby et al., 2010; Green et al., 2011; Wang et al., 2009). Potential articulatory data acquisition methods for SSIs include ultrasound (Denby et al., 2011; Hueber et al., 2010), surface electromyography electrodes (Heaton et al., 2011; Jorgensen and Dusan, 2010), and electromagnetic articulograph (EMA) (Fagan et al., 2008; Wang et al., 2009, 2012a). Despite the recent effort on silent speech interface research, online test of SSIs has rarely been studied. So far, most of the published work on SSIs has focused on development of silent speech recognition algorithm through offline analysis (i.e., using prerecorded data) (Fagan et al., 2008; Heaton et al., 2011; Hofe et al., 2013; Hueber et al., 2010; Jorgenson et al., 2010; Wang et al., 2009a, 2012a, 2012b, 2013c). Ultrasound-39 based SSIs have been tested online with multipl</context>
</contexts>
<marker>Heaton, Robertson, Griffin, 2011</marker>
<rawString>Heaton, J. T., Robertson, M., and Griffin, C. 2011. Development of a wireless electromyographically controlled electrolarynx voice prosthesis, Proc. of the 33rd Annual Intl. Conf. of the IEEE Engineering in Medicine &amp; Biology Society, Boston, MA, 5352-5355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Heracleous</author>
<author>N Hagita</author>
</authors>
<title>Automatic recognition of speech without any audio information,</title>
<date>2011</date>
<booktitle>Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>2392--2395</pages>
<contexts>
<context position="22864" citStr="Heracleous and Hagita, 2011" startWordPosition="3686" endWordPosition="3689">e efficacy of permanently implantable and wireless sensors (Chen et al., 2012; Park et al., 2012). In the future, those more portable, and wireless articulatory motion tracking devices, when they are ready, will be used to develop a 43 portable SSI for practice use. In this experiment, a simple DTW algorithm was used to compare the training and testing phrases, which is known to be slower than most machine learning classifiers. Thus, in the future, the latency can be significantly reduced by using faster classifiers such as support vector machines (Wang et al., 2013c) or hidden Markov models (Heracleous and Hagita, 2011; King et al., 2007; Rudzicz et al., 2012; Uraga and Hain, 2006). Furthermore, in this proof-of-concept design, the vocabulary was limited to a small set of phrases, because our design required the whole experiment (including training and testing) to be done in about one hour. Additional work is needed to test the feasibility of open-vocabulary recognition, which will be much more usable for people after laryngectomy or with other severe voice impairments. 5 Conclusion and Future Work A preliminary, online test of a SSI based on electromagnetic articulograph was conducted. The results were enc</context>
</contexts>
<marker>Heracleous, Hagita, 2011</marker>
<rawString>Heracleous, P., and Hagita, N. 2011. Automatic recognition of speech without any audio information, Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, 2392-2395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jorgensen</author>
<author>S Dusan</author>
</authors>
<title>Speech interfaces based upon surface electromyography,</title>
<date>2010</date>
<journal>Speech Communication,</journal>
<volume>52</volume>
<contexts>
<context position="5520" citStr="Jorgensen and Dusan, 2010" startWordPosition="818" endWordPosition="821">orithms and (b) portable tongue motion tracking devices for daily use. SSIs convert articulatory information into text that drives a text-to-speech synthesizer. Although still in developmental stages (e.g., speakerdependent recognition, small-vocabulary), SSIs even have potential to provide speech output based on prerecorded samples of the patient’s own voice (Denby et al., 2010; Green et al., 2011; Wang et al., 2009). Potential articulatory data acquisition methods for SSIs include ultrasound (Denby et al., 2011; Hueber et al., 2010), surface electromyography electrodes (Heaton et al., 2011; Jorgensen and Dusan, 2010), and electromagnetic articulograph (EMA) (Fagan et al., 2008; Wang et al., 2009, 2012a). Despite the recent effort on silent speech interface research, online test of SSIs has rarely been studied. So far, most of the published work on SSIs has focused on development of silent speech recognition algorithm through offline analysis (i.e., using prerecorded data) (Fagan et al., 2008; Heaton et al., 2011; Hofe et al., 2013; Hueber et al., 2010; Jorgenson et al., 2010; Wang et al., 2009a, 2012a, 2012b, 2013c). Ultrasound-39 based SSIs have been tested online with multiple subjects and encouraging r</context>
</contexts>
<marker>Jorgensen, Dusan, 2010</marker>
<rawString>Jorgensen, C. and Dusan, S. 2010. Speech interfaces based upon surface electromyography, Speech Communication, 52:354–366, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Katz</author>
<author>S Bharadwaj</author>
<author>M Rush</author>
<author>M Stettler</author>
</authors>
<title>Influences of EMA receiver coils on speech production by normal and aphasic/apraxic talkers,</title>
<date>2006</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<pages>49--645</pages>
<contexts>
<context position="12938" citStr="Katz et al., 2006" startWordPosition="2022" endWordPosition="2025">netic field by a textbook-sized generator. Participants were seated with their head within the calibrated magnetic field (Figure 2, the right picture), facing a computer monitor that displays the GUI of the SSI. The sensors were attached to the surface of each articulator using dental glue (PeriAcryl Oral Tissue Adhesive). Prior to the experiment, each subject participated in a three-minute training session (on how to use the SSI), which also helped them adapt to the oral sensors. Previous studies have shown those sensors do not significantly affect their speech output after a short practice (Katz et al., 2006; Weismer and Bunton, 1999). Figure 2 (left) shows the positions of the five sensors attached to a participant’s forehead, tongue, and lips (Green et al., 2003; 2013; Wang et al., 2013a). One 6 DOF (spatial and rotational) head sensor was attached to a nose bridge on a pair of glasses (rather than on forehead skin directly), to avoid the skin artifact (Green et al., 2007). Two 5 DOF sensors - TT (Tongue Tip) and TB (Tongue Body Back) - were attached on the midsagittal of the tongue. TT was located approximately 10 mm from the tongue apex (Wang et al., 2011, 2013a). TB was placed as far 41 back</context>
</contexts>
<marker>Katz, Bharadwaj, Rush, Stettler, 2006</marker>
<rawString>Katz, W., Bharadwaj, S., Rush, M., and Stettler, M. 2006. Influences of EMA receiver coils on speech production by normal and aphasic/apraxic talkers, Journal of Speech, Language, and Hearing Research, 49:645-659.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R D Kent</author>
<author>S G Adams</author>
<author>G S Tuner</author>
</authors>
<title>Models of speech production,</title>
<date>1996</date>
<booktitle>in Principles of Experimental Phonetics,</booktitle>
<location>Ed., Lass, N. J., Mosby: St Louis, MO.</location>
<marker>Kent, Adams, Tuner, 1996</marker>
<rawString>Kent, R. D., Adams, S. G., and Tuner, G. S. 1996. Models of speech production, in Principles of Experimental Phonetics, Ed., Lass, N. J., Mosby: St Louis, MO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S King</author>
<author>J Livescu Frankel</author>
<author>K McDermott</author>
<author>E Richmond</author>
<author>K Wester</author>
<author>M</author>
</authors>
<title>Speech production knowledge in automatic speech recognition,</title>
<date>2007</date>
<journal>Journal of the Acoustical Society of America,</journal>
<pages>121--2</pages>
<contexts>
<context position="22883" citStr="King et al., 2007" startWordPosition="3690" endWordPosition="3693">lantable and wireless sensors (Chen et al., 2012; Park et al., 2012). In the future, those more portable, and wireless articulatory motion tracking devices, when they are ready, will be used to develop a 43 portable SSI for practice use. In this experiment, a simple DTW algorithm was used to compare the training and testing phrases, which is known to be slower than most machine learning classifiers. Thus, in the future, the latency can be significantly reduced by using faster classifiers such as support vector machines (Wang et al., 2013c) or hidden Markov models (Heracleous and Hagita, 2011; King et al., 2007; Rudzicz et al., 2012; Uraga and Hain, 2006). Furthermore, in this proof-of-concept design, the vocabulary was limited to a small set of phrases, because our design required the whole experiment (including training and testing) to be done in about one hour. Additional work is needed to test the feasibility of open-vocabulary recognition, which will be much more usable for people after laryngectomy or with other severe voice impairments. 5 Conclusion and Future Work A preliminary, online test of a SSI based on electromagnetic articulograph was conducted. The results were encouraging revealing </context>
</contexts>
<marker>King, Frankel, McDermott, Richmond, Wester, M, 2007</marker>
<rawString>King, S., Frankel, J. Livescu, K., McDermott, E., Richmond, K., Wester, M. 2007. Speech production knowledge in automatic speech recognition, Journal of the Acoustical Society of America, 121(2):723-742.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Kuipers</author>
</authors>
<title>Quaternions and rotation Sequences: a Primer with Applications to Orbits, Aerospace, and Virtual Reality,</title>
<date>1999</date>
<publisher>Princeton University Press,</publisher>
<location>Princeton, NJ.</location>
<contexts>
<context position="11253" citStr="Kuipers, 1999" startWordPosition="1726" endWordPosition="1727">e head-independent movements of other sensors. The Wave system represents object orientation or rotation (denoted by yaw, pitch, and roll in Euler angles) in quaternions, a four-dimensional vector. Quaternion has its advantages over Euler angles. For example, quaternion avoids the issue of gimbal lock (one degree of freedom may be lost in a series of rotation), and it is simpler to achieve smooth interpolation using quaternion than using Euler angles (Dam et al., 1998). Thus, quaternion has been widely used in computer graphics, computer vision, robotics, virtual reality, and flight dynamics (Kuipers, 1999). Given the unit quaternion q = (a, b, c, d) (1) where a2 + b2 + c2 + d2 = 1, a 3 × 3 rotation matrix R can be derived using Equation (2): a2 + b2 − c2 − d 2 2bc − 2ad 2bd + 2ac  R=2bc+2ad a2−b2+c2−d2 2cd−2ab  2bd − 2ac 2cd + 2ab a2 − b2 −c2 + d 2 For details of how the quaternion is used in Wave system, please refer to the Wave RealTime API manual and sample application (Northern Digital Inc., Waterloo, Canada). 3 A Preliminary Online Test 3.1 Participants &amp; Stimuli Three American English talkers participated in this experiment (two males and one female with average age 25 and SD 3.5 yea</context>
</contexts>
<marker>Kuipers, 1999</marker>
<rawString>Kuipers, J. B. 1999. Quaternions and rotation Sequences: a Primer with Applications to Orbits, Aerospace, and Virtual Reality, Princeton University Press, Princeton, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Liu</author>
<author>M L Ng</author>
</authors>
<title>Electrolarynx in voice rehabilitation,</title>
<date>2007</date>
<journal>Auris Nasus Larynx,</journal>
<volume>34</volume>
<issue>3</issue>
<pages>327--332</pages>
<contexts>
<context position="2434" citStr="Liu and Ng, 2007" startWordPosition="358" endWordPosition="361">emoval of the larynx due to the treatment of cancer (Bailey et al., 2006). In 2013, about 12,260 new cases of laryngeal cancer were estimated in the United States (American Cancer 38 Society, 2013). Currently, there are only limited treatment options for these individuals including (1) esophageal speech, which involves oscillation of the esophagus and is difficult to learn; (2) tracheo-esophageal speech, in which a voice prosthesis is placed in a tracheo-esophageal puncture; and (3) electrolarynx, an external device held on the neck during articulation, which produces a robotic voice quality (Liu and Ng, 2007). Perhaps the greatest disadvantage of these approaches is that they produce abnormal sounding speech with a fundamental frequency that is low and limited in range. The abnormal voice quality output severely affects the social life of people after laryngectomy (Liu and Ng, 2007). In addition, the tracheo-esophageal option requires an additional surgery, which is not suitable for every patient (Bailey et al., 2006). Although research is being conducted on improving the voice quality of esophageal or electrolarynx speech (Doi et al., 2010; Toda et al., 2012), new assistive technologies based on </context>
</contexts>
<marker>Liu, Ng, 2007</marker>
<rawString>Liu, H., and Ng, M. L. 2007. Electrolarynx in voice rehabilitation, Auris Nasus Larynx, 34(3): 327-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Livescu</author>
<author>O Çetin</author>
<author>Mark Hasegawa-Johnson</author>
<author>S King</author>
<author>C Bartels</author>
<author>N Borges</author>
<author>A Kantor</author>
</authors>
<title>Articulatory feature-based methods for acoustic and audio-visual speech recognition:</title>
<date>2007</date>
<booktitle>Summary from the 2006 JHU Summer Workshop. Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>621--624</pages>
<contexts>
<context position="3761" citStr="Livescu et al., 2007" startWordPosition="564" endWordPosition="567">oviding natural sounding speech output for persons after laryngectomy. Visual speech recognition (or automatic lip reading) typically uses an optical camera to obtain lip and/or facial features during speech (including lip contour, color, opening, movement, etc.) and then classify these features to speech units (Meier et al., 2000; Oviatt, 2003). However, due to the lack of information from tongue, the primary articulator, visual speech recognition (i.e., using visual information only, without tongue and audio information) may obtain a low accuracy (e.g., 30% - 40% for phoneme classification, Livescu et al., 2007). Furthermore, Wang and colleagues (2013b) have showed any single tongue sensor (from tongue tip to tongue body Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 38–45, Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics Figure 1. Design of the real-time silent speech interface. back on the midsagittal line) encodes significantly more information in distinguishing phonemes than do lips. However, visual speech recognition is well suited for applications with smallvocabulary (e.g., a lip-reading based</context>
</contexts>
<marker>Livescu, Çetin, Hasegawa-Johnson, King, Bartels, Borges, Kantor, 2007</marker>
<rawString>Livescu, K., Çetin, O., Hasegawa-Johnson, Mark, King, S., Bartels, C., Borges, N., Kantor, A., et al. (2007). Articulatory feature-based methods for acoustic and audio-visual speech recognition: Summary from the 2006 JHU Summer Workshop. Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, 621-624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Meier</author>
<author>R Stiefelhagen</author>
<author>J Yang</author>
<author>A Waibel</author>
</authors>
<title>Towards Unrestricted Lip Reading.</title>
<date>2000</date>
<journal>International Journal of Pattern Recognition and Artificial Intelligence,</journal>
<volume>14</volume>
<issue>5</issue>
<pages>571--585</pages>
<contexts>
<context position="3472" citStr="Meier et al., 2000" startWordPosition="519" endWordPosition="522">lthough research is being conducted on improving the voice quality of esophageal or electrolarynx speech (Doi et al., 2010; Toda et al., 2012), new assistive technologies based on non-audio information (e.g., visual or articulatory information) may be a good alternative approach for providing natural sounding speech output for persons after laryngectomy. Visual speech recognition (or automatic lip reading) typically uses an optical camera to obtain lip and/or facial features during speech (including lip contour, color, opening, movement, etc.) and then classify these features to speech units (Meier et al., 2000; Oviatt, 2003). However, due to the lack of information from tongue, the primary articulator, visual speech recognition (i.e., using visual information only, without tongue and audio information) may obtain a low accuracy (e.g., 30% - 40% for phoneme classification, Livescu et al., 2007). Furthermore, Wang and colleagues (2013b) have showed any single tongue sensor (from tongue tip to tongue body Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 38–45, Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Lingui</context>
</contexts>
<marker>Meier, Stiefelhagen, Yang, Waibel, 2000</marker>
<rawString>Meier, U., Stiefelhagen, R., Yang, J., and Waibel, A. (2000). Towards Unrestricted Lip Reading. International Journal of Pattern Recognition and Artificial Intelligence, 14(5): 571-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Oviatt</author>
</authors>
<title>Multimodal interfaces, in Human– Computer Interaction Handbook: Fundamentals, Evolving Technologies and Emerging Applications, Eds. Julie A. Jacko and Andrew Sears (Mahwah,</title>
<date>2003</date>
<pages>286--304</pages>
<location>NJ:Erlbaum):</location>
<contexts>
<context position="3487" citStr="Oviatt, 2003" startWordPosition="523" endWordPosition="524">being conducted on improving the voice quality of esophageal or electrolarynx speech (Doi et al., 2010; Toda et al., 2012), new assistive technologies based on non-audio information (e.g., visual or articulatory information) may be a good alternative approach for providing natural sounding speech output for persons after laryngectomy. Visual speech recognition (or automatic lip reading) typically uses an optical camera to obtain lip and/or facial features during speech (including lip contour, color, opening, movement, etc.) and then classify these features to speech units (Meier et al., 2000; Oviatt, 2003). However, due to the lack of information from tongue, the primary articulator, visual speech recognition (i.e., using visual information only, without tongue and audio information) may obtain a low accuracy (e.g., 30% - 40% for phoneme classification, Livescu et al., 2007). Furthermore, Wang and colleagues (2013b) have showed any single tongue sensor (from tongue tip to tongue body Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 38–45, Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics Figure 1.</context>
</contexts>
<marker>Oviatt, 2003</marker>
<rawString>Oviatt, S. L. 2003. Multimodal interfaces, in Human– Computer Interaction Handbook: Fundamentals, Evolving Technologies and Emerging Applications, Eds. Julie A. Jacko and Andrew Sears (Mahwah, NJ:Erlbaum): 286–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Park</author>
<author>M Kiani</author>
<author>H M Lee</author>
<author>J Kim</author>
<author>J Block</author>
<author>B Gosselin</author>
<author>M Ghovanloo</author>
</authors>
<title>A wireless magnetoresistive sensing system for an intraoral tongue-computer interface,</title>
<date>2012</date>
<journal>IEEE Transactions on Biomedical Circuits and Systems,</journal>
<pages>6--6</pages>
<contexts>
<context position="22334" citStr="Park et al., 2012" startWordPosition="3600" endWordPosition="3603">s after laryngectomy. A test of the SSI using multiple participants after laryngectomy is needed to understand the performance of SSIs for those patients under different conditions. Although a demonstration of daily conversation using the SSI is provided, SSI based on the non-portable Wave system is currently not ready for practical use. Fortunately, more affordable and portable electromagnetic devices are being developed as are small handheld or wearable devices (Fagan et al., 2008). Researchers are also testing the efficacy of permanently implantable and wireless sensors (Chen et al., 2012; Park et al., 2012). In the future, those more portable, and wireless articulatory motion tracking devices, when they are ready, will be used to develop a 43 portable SSI for practice use. In this experiment, a simple DTW algorithm was used to compare the training and testing phrases, which is known to be slower than most machine learning classifiers. Thus, in the future, the latency can be significantly reduced by using faster classifiers such as support vector machines (Wang et al., 2013c) or hidden Markov models (Heracleous and Hagita, 2011; King et al., 2007; Rudzicz et al., 2012; Uraga and Hain, 2006). Furt</context>
</contexts>
<marker>Park, Kiani, Lee, Kim, Block, Gosselin, Ghovanloo, 2012</marker>
<rawString>Park, H., Kiani, M., Lee, H. M., Kim, J., Block, J., Gosselin, B., and Ghovanloo, M. 2012. A wireless magnetoresistive sensing system for an intraoral tongue-computer interface, IEEE Transactions on Biomedical Circuits and Systems, 6(6):571-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Potamianos</author>
<author>C Neti</author>
<author>G Cravier</author>
<author>A Garg</author>
<author>A W Senior</author>
</authors>
<title>Recent advances in the automatic recognition of audio-visual speech,</title>
<date>2003</date>
<booktitle>Proc. of IEEE,</booktitle>
<pages>91--9</pages>
<contexts>
<context position="4564" citStr="Potamianos et al., 2003" startWordPosition="678" endWordPosition="681">or Assistive Technologies (SLPAT), pages 38–45, Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics Figure 1. Design of the real-time silent speech interface. back on the midsagittal line) encodes significantly more information in distinguishing phonemes than do lips. However, visual speech recognition is well suited for applications with smallvocabulary (e.g., a lip-reading based commandand-control system for home appliance) or using visual information as an additional source for acoustic speech recognition, referred to as audiovisual speech recognition (Potamianos et al., 2003), because such a system based on portable camera is convenient in practical use. In contrast, SSIs, with tongue information, have potential to obtain a high level of silent speech recognition accuracy (without audio information). Currently, two major obstacles for SSI development are lack of (a) fast and accurate recognition algorithms and (b) portable tongue motion tracking devices for daily use. SSIs convert articulatory information into text that drives a text-to-speech synthesizer. Although still in developmental stages (e.g., speakerdependent recognition, small-vocabulary), SSIs even have</context>
</contexts>
<marker>Potamianos, Neti, Cravier, Garg, Senior, 2003</marker>
<rawString>Potamianos, G., Neti, C., Cravier, G., Garg, A. and Senior, A. W. 2003. Recent advances in the automatic recognition of audio-visual speech, Proc. of IEEE, 91(9):1306-1326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rudzicz</author>
<author>G Hirst</author>
<author>P Van Lieshout</author>
</authors>
<title>Vocal tract representation in the recognition of cerebral palsied speech,</title>
<date>2012</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<volume>55</volume>
<issue>4</issue>
<pages>1190--1207</pages>
<marker>Rudzicz, Hirst, Van Lieshout, 2012</marker>
<rawString>Rudzicz, F., Hirst, G., Van Lieshout, P. 2012. Vocal tract representation in the recognition of cerebral palsied speech, Journal of Speech, Language, and Hearing Research, 55(4): 1190-1207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Toda</author>
<author>M Nakagiri</author>
<author>K Shikano</author>
</authors>
<title>Statistical voice conversion techniques for body-conducted unvoiced speech enhancement,</title>
<date>2012</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>20</volume>
<issue>9</issue>
<pages>2505--2517</pages>
<contexts>
<context position="2996" citStr="Toda et al., 2012" startWordPosition="449" endWordPosition="452">hich produces a robotic voice quality (Liu and Ng, 2007). Perhaps the greatest disadvantage of these approaches is that they produce abnormal sounding speech with a fundamental frequency that is low and limited in range. The abnormal voice quality output severely affects the social life of people after laryngectomy (Liu and Ng, 2007). In addition, the tracheo-esophageal option requires an additional surgery, which is not suitable for every patient (Bailey et al., 2006). Although research is being conducted on improving the voice quality of esophageal or electrolarynx speech (Doi et al., 2010; Toda et al., 2012), new assistive technologies based on non-audio information (e.g., visual or articulatory information) may be a good alternative approach for providing natural sounding speech output for persons after laryngectomy. Visual speech recognition (or automatic lip reading) typically uses an optical camera to obtain lip and/or facial features during speech (including lip contour, color, opening, movement, etc.) and then classify these features to speech units (Meier et al., 2000; Oviatt, 2003). However, due to the lack of information from tongue, the primary articulator, visual speech recognition (i.</context>
</contexts>
<marker>Toda, Nakagiri, Shikano, 2012</marker>
<rawString>Toda, T., Nakagiri, M., Shikano, K. 2012. Statistical voice conversion techniques for body-conducted unvoiced speech enhancement, IEEE Transactions on Audio, Speech and Language Processing, 20(9): 2505-2517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Uraga</author>
<author>T Hain</author>
</authors>
<title>Automatic speech recognition experiments with articulatory data,</title>
<date>2006</date>
<booktitle>Proc. Inerspeech,</booktitle>
<pages>353--356</pages>
<contexts>
<context position="22928" citStr="Uraga and Hain, 2006" startWordPosition="3698" endWordPosition="3701">., 2012; Park et al., 2012). In the future, those more portable, and wireless articulatory motion tracking devices, when they are ready, will be used to develop a 43 portable SSI for practice use. In this experiment, a simple DTW algorithm was used to compare the training and testing phrases, which is known to be slower than most machine learning classifiers. Thus, in the future, the latency can be significantly reduced by using faster classifiers such as support vector machines (Wang et al., 2013c) or hidden Markov models (Heracleous and Hagita, 2011; King et al., 2007; Rudzicz et al., 2012; Uraga and Hain, 2006). Furthermore, in this proof-of-concept design, the vocabulary was limited to a small set of phrases, because our design required the whole experiment (including training and testing) to be done in about one hour. Additional work is needed to test the feasibility of open-vocabulary recognition, which will be much more usable for people after laryngectomy or with other severe voice impairments. 5 Conclusion and Future Work A preliminary, online test of a SSI based on electromagnetic articulograph was conducted. The results were encouraging revealing high phrase recognition accuracy and short pl</context>
</contexts>
<marker>Uraga, Hain, 2006</marker>
<rawString>Uraga, E. and Hain, T. 2006. Automatic speech recognition experiments with articulatory data, Proc. Inerspeech, 353-356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wang</author>
<author>A Samal</author>
<author>J R Green</author>
<author>T D Carrell</author>
</authors>
<title>Vowel recognition from articulatory position time-series data,</title>
<date>2009</date>
<booktitle>Proc. IEEE Intl. Conf. on Signal Processing and Communication Systems,</booktitle>
<pages>1--6</pages>
<location>Omaha, NE,</location>
<contexts>
<context position="5315" citStr="Wang et al., 2009" startWordPosition="789" endWordPosition="792">ential to obtain a high level of silent speech recognition accuracy (without audio information). Currently, two major obstacles for SSI development are lack of (a) fast and accurate recognition algorithms and (b) portable tongue motion tracking devices for daily use. SSIs convert articulatory information into text that drives a text-to-speech synthesizer. Although still in developmental stages (e.g., speakerdependent recognition, small-vocabulary), SSIs even have potential to provide speech output based on prerecorded samples of the patient’s own voice (Denby et al., 2010; Green et al., 2011; Wang et al., 2009). Potential articulatory data acquisition methods for SSIs include ultrasound (Denby et al., 2011; Hueber et al., 2010), surface electromyography electrodes (Heaton et al., 2011; Jorgensen and Dusan, 2010), and electromagnetic articulograph (EMA) (Fagan et al., 2008; Wang et al., 2009, 2012a). Despite the recent effort on silent speech interface research, online test of SSIs has rarely been studied. So far, most of the published work on SSIs has focused on development of silent speech recognition algorithm through offline analysis (i.e., using prerecorded data) (Fagan et al., 2008; Heaton et a</context>
</contexts>
<marker>Wang, Samal, Green, Carrell, 2009</marker>
<rawString>Wang, J., Samal, A., Green, J. R., and Carrell, T. D. 2009. Vowel recognition from articulatory position time-series data, Proc. IEEE Intl. Conf. on Signal Processing and Communication Systems, Omaha, NE, 1-6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wang</author>
<author>J R Green</author>
<author>A Samal</author>
<author>D B Marx</author>
</authors>
<title>Quantifying articulatory distinctiveness of vowels,</title>
<date>2011</date>
<booktitle>Proc. Interspeech,</booktitle>
<pages>277--280</pages>
<location>Florence, Italy,</location>
<contexts>
<context position="13500" citStr="Wang et al., 2011" startWordPosition="2124" endWordPosition="2127">speech output after a short practice (Katz et al., 2006; Weismer and Bunton, 1999). Figure 2 (left) shows the positions of the five sensors attached to a participant’s forehead, tongue, and lips (Green et al., 2003; 2013; Wang et al., 2013a). One 6 DOF (spatial and rotational) head sensor was attached to a nose bridge on a pair of glasses (rather than on forehead skin directly), to avoid the skin artifact (Green et al., 2007). Two 5 DOF sensors - TT (Tongue Tip) and TB (Tongue Body Back) - were attached on the midsagittal of the tongue. TT was located approximately 10 mm from the tongue apex (Wang et al., 2011, 2013a). TB was placed as far 41 back as possible, depending on the participant’s tongue length (Wang et al., 2013b). Lip movements were captured by attaching two 5 DOF sensors to the vermilion borders of the upper (UL) and lower (LL) lips at midline. The four sensors (i.e., TT, TB, UL, and LL) placements were selected based on literature showing that they are able to achieve as high recognition accuracy as that obtained using more tongue sensors for this application (Wang et al., 2013b). As mentioned previously, real-time preprocessing of the positional time series was conducted, including s</context>
</contexts>
<marker>Wang, Green, Samal, Marx, 2011</marker>
<rawString>Wang, J., Green, J. R., Samal, A., and Marx, D. B. 2011. Quantifying articulatory distinctiveness of vowels, Proc. Interspeech, Florence, Italy, 277-280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wang</author>
<author>A Samal</author>
<author>J R Green</author>
<author>F Rudzicz</author>
</authors>
<title>Sentence recognition from articulatory movements for silent speech interfaces,</title>
<date>2012</date>
<booktitle>Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>4985--4988</pages>
<contexts>
<context position="6519" citStr="Wang et al., 2012" startWordPosition="977" endWordPosition="980">; Heaton et al., 2011; Hofe et al., 2013; Hueber et al., 2010; Jorgenson et al., 2010; Wang et al., 2009a, 2012a, 2012b, 2013c). Ultrasound-39 based SSIs have been tested online with multiple subjects and encouraging results were obtained in a phrase reading task where the subjects were asked to silently articulate sixty phrases (Denby et al., 2011). SSI based on electromagnetic sensing has been only tested using offline analysis (using pre-recorded data) collected from single subjects (Fagan et al., 2008; Hofe et al., 2013), although some work simulated online testing using prerecorded data (Wang et al., 2012a, 2012b, 2013c). Online tests of SSIs using electromagnetic articulograph with multiple subjects are needed to show the feasibility and potential of the SSIs for future clinical applications. In this paper, we report a preliminary, online test of a newly-developed, real-time, and interactive SSI based on a commercial EMA. EMA tracks articulatory motion by placing small sensors on the surface of tongue and other articulators (e.g., lips and jaw). EMA is well suited for the early state of SSI development because it (1) is non-invasive, (2) has a high spatial resolution in motion tracking, (3) h</context>
<context position="8276" citStr="Wang et al., 2012" startWordPosition="1258" endWordPosition="1261">articulation information to text), and (c) text-to-speech synthesis for speech output. The EMA system (Wave Speech Research Figure 2. Demo of a participant using the silent speech interface. The left picture illustrates the coordinate system and sensor locations (sensor labels are described in text); in the right picture, a participant is using the silent speech interface to finish the online test. system, Northern Digital Inc., Waterloo, Canada) was used to track the tongue and lip movement in real-time. The sampling rate of the Wave system was 100 Hz, which is adequate for this application (Wang et al., 2012a, 2012b, 2013c). The spatial accuracy of motion tracking using Wave is 0.5 mm (Berry, 2011). The online recognition component recognized functional phrases from articulatory movements in real-time. The recognition component is modular such that alternative classifiers can easily replace and be integrated into the SSI. In this preliminary test, recognition was speakerdependent, where training and testing data were from the same speakers. The third component played back either prerecorded or synthesized sounds using a text-tospeech synthesizer (Huang et al., 1997). 2.2 Other designs A graphical</context>
<context position="12163" citStr="Wang et al., 2012" startWordPosition="1899" endWordPosition="1902">d in Wave system, please refer to the Wave RealTime API manual and sample application (Northern Digital Inc., Waterloo, Canada). 3 A Preliminary Online Test 3.1 Participants &amp; Stimuli Three American English talkers participated in this experiment (two males and one female with average age 25 and SD 3.5 years). No history of speech, language, hearing, or any cognitive problems were reported. Sixty phrases that are frequently used in daily life by healthy people and AAC (augmentative and alternative communication) users were used in this experiment. Those phrases were selected from the lists in Wang et al., 2012a and Beukelman and Gutmann, 1999. 3.2 Procedure Setup The Wave system tracks the motion of sensors attached on the articulators by establishing an electromagnetic field by a textbook-sized generator. Participants were seated with their head within the calibrated magnetic field (Figure 2, the right picture), facing a computer monitor that displays the GUI of the SSI. The sensors were attached to the surface of each articulator using dental glue (PeriAcryl Oral Tissue Adhesive). Prior to the experiment, each subject participated in a three-minute training session (on how to use the SSI), which </context>
</contexts>
<marker>Wang, Samal, Green, Rudzicz, 2012</marker>
<rawString>Wang, J., Samal, A., Green, J. R., and Rudzicz, F. 2012a. Sentence recognition from articulatory movements for silent speech interfaces, Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, 4985-4988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wang</author>
<author>A Samal</author>
<author>J R Green</author>
<author>F Rudzicz</author>
</authors>
<title>Whole-word recognition from articulatory movements for silent speech interfaces,</title>
<date>2012</date>
<booktitle>Proc. Interspeech,</booktitle>
<pages>1327--30</pages>
<contexts>
<context position="6519" citStr="Wang et al., 2012" startWordPosition="977" endWordPosition="980">; Heaton et al., 2011; Hofe et al., 2013; Hueber et al., 2010; Jorgenson et al., 2010; Wang et al., 2009a, 2012a, 2012b, 2013c). Ultrasound-39 based SSIs have been tested online with multiple subjects and encouraging results were obtained in a phrase reading task where the subjects were asked to silently articulate sixty phrases (Denby et al., 2011). SSI based on electromagnetic sensing has been only tested using offline analysis (using pre-recorded data) collected from single subjects (Fagan et al., 2008; Hofe et al., 2013), although some work simulated online testing using prerecorded data (Wang et al., 2012a, 2012b, 2013c). Online tests of SSIs using electromagnetic articulograph with multiple subjects are needed to show the feasibility and potential of the SSIs for future clinical applications. In this paper, we report a preliminary, online test of a newly-developed, real-time, and interactive SSI based on a commercial EMA. EMA tracks articulatory motion by placing small sensors on the surface of tongue and other articulators (e.g., lips and jaw). EMA is well suited for the early state of SSI development because it (1) is non-invasive, (2) has a high spatial resolution in motion tracking, (3) h</context>
<context position="8276" citStr="Wang et al., 2012" startWordPosition="1258" endWordPosition="1261">articulation information to text), and (c) text-to-speech synthesis for speech output. The EMA system (Wave Speech Research Figure 2. Demo of a participant using the silent speech interface. The left picture illustrates the coordinate system and sensor locations (sensor labels are described in text); in the right picture, a participant is using the silent speech interface to finish the online test. system, Northern Digital Inc., Waterloo, Canada) was used to track the tongue and lip movement in real-time. The sampling rate of the Wave system was 100 Hz, which is adequate for this application (Wang et al., 2012a, 2012b, 2013c). The spatial accuracy of motion tracking using Wave is 0.5 mm (Berry, 2011). The online recognition component recognized functional phrases from articulatory movements in real-time. The recognition component is modular such that alternative classifiers can easily replace and be integrated into the SSI. In this preliminary test, recognition was speakerdependent, where training and testing data were from the same speakers. The third component played back either prerecorded or synthesized sounds using a text-tospeech synthesizer (Huang et al., 1997). 2.2 Other designs A graphical</context>
<context position="12163" citStr="Wang et al., 2012" startWordPosition="1899" endWordPosition="1902">d in Wave system, please refer to the Wave RealTime API manual and sample application (Northern Digital Inc., Waterloo, Canada). 3 A Preliminary Online Test 3.1 Participants &amp; Stimuli Three American English talkers participated in this experiment (two males and one female with average age 25 and SD 3.5 years). No history of speech, language, hearing, or any cognitive problems were reported. Sixty phrases that are frequently used in daily life by healthy people and AAC (augmentative and alternative communication) users were used in this experiment. Those phrases were selected from the lists in Wang et al., 2012a and Beukelman and Gutmann, 1999. 3.2 Procedure Setup The Wave system tracks the motion of sensors attached on the articulators by establishing an electromagnetic field by a textbook-sized generator. Participants were seated with their head within the calibrated magnetic field (Figure 2, the right picture), facing a computer monitor that displays the GUI of the SSI. The sensors were attached to the surface of each articulator using dental glue (PeriAcryl Oral Tissue Adhesive). Prior to the experiment, each subject participated in a three-minute training session (on how to use the SSI), which </context>
</contexts>
<marker>Wang, Samal, Green, Rudzicz, 2012</marker>
<rawString>Wang, J., Samal, A., Green, J. R., and Rudzicz, F. 2012b. Whole-word recognition from articulatory movements for silent speech interfaces, Proc. Interspeech, 1327-30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wang</author>
<author>J R Green</author>
<author>A Samal</author>
<author>Y Yunusova</author>
</authors>
<title>Articulatory distinctiveness of vowels and consonants: A data-driven approach,</title>
<date>2013</date>
<journal>Journal of Speech, Language, and Hearing Research,</journal>
<volume>56</volume>
<pages>1539--1551</pages>
<contexts>
<context position="13122" citStr="Wang et al., 2013" startWordPosition="2053" endWordPosition="2056">isplays the GUI of the SSI. The sensors were attached to the surface of each articulator using dental glue (PeriAcryl Oral Tissue Adhesive). Prior to the experiment, each subject participated in a three-minute training session (on how to use the SSI), which also helped them adapt to the oral sensors. Previous studies have shown those sensors do not significantly affect their speech output after a short practice (Katz et al., 2006; Weismer and Bunton, 1999). Figure 2 (left) shows the positions of the five sensors attached to a participant’s forehead, tongue, and lips (Green et al., 2003; 2013; Wang et al., 2013a). One 6 DOF (spatial and rotational) head sensor was attached to a nose bridge on a pair of glasses (rather than on forehead skin directly), to avoid the skin artifact (Green et al., 2007). Two 5 DOF sensors - TT (Tongue Tip) and TB (Tongue Body Back) - were attached on the midsagittal of the tongue. TT was located approximately 10 mm from the tongue apex (Wang et al., 2011, 2013a). TB was placed as far 41 back as possible, depending on the participant’s tongue length (Wang et al., 2013b). Lip movements were captured by attaching two 5 DOF sensors to the vermilion borders of the upper (UL) a</context>
<context position="22809" citStr="Wang et al., 2013" startWordPosition="3678" endWordPosition="3681">t al., 2008). Researchers are also testing the efficacy of permanently implantable and wireless sensors (Chen et al., 2012; Park et al., 2012). In the future, those more portable, and wireless articulatory motion tracking devices, when they are ready, will be used to develop a 43 portable SSI for practice use. In this experiment, a simple DTW algorithm was used to compare the training and testing phrases, which is known to be slower than most machine learning classifiers. Thus, in the future, the latency can be significantly reduced by using faster classifiers such as support vector machines (Wang et al., 2013c) or hidden Markov models (Heracleous and Hagita, 2011; King et al., 2007; Rudzicz et al., 2012; Uraga and Hain, 2006). Furthermore, in this proof-of-concept design, the vocabulary was limited to a small set of phrases, because our design required the whole experiment (including training and testing) to be done in about one hour. Additional work is needed to test the feasibility of open-vocabulary recognition, which will be much more usable for people after laryngectomy or with other severe voice impairments. 5 Conclusion and Future Work A preliminary, online test of a SSI based on electromag</context>
</contexts>
<marker>Wang, Green, Samal, Yunusova, 2013</marker>
<rawString>Wang, J., Green, J. R., Samal, A. and Yunusova, Y. 2013a. Articulatory distinctiveness of vowels and consonants: A data-driven approach, Journal of Speech, Language, and Hearing Research, 56, 1539-1551.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wang</author>
<author>J R Green</author>
<author>A Samal</author>
</authors>
<title>Individual articulator&apos;s contribution to phoneme production,</title>
<date>2013</date>
<booktitle>Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>7795--89</pages>
<location>Vancouver, Canada,</location>
<contexts>
<context position="13122" citStr="Wang et al., 2013" startWordPosition="2053" endWordPosition="2056">isplays the GUI of the SSI. The sensors were attached to the surface of each articulator using dental glue (PeriAcryl Oral Tissue Adhesive). Prior to the experiment, each subject participated in a three-minute training session (on how to use the SSI), which also helped them adapt to the oral sensors. Previous studies have shown those sensors do not significantly affect their speech output after a short practice (Katz et al., 2006; Weismer and Bunton, 1999). Figure 2 (left) shows the positions of the five sensors attached to a participant’s forehead, tongue, and lips (Green et al., 2003; 2013; Wang et al., 2013a). One 6 DOF (spatial and rotational) head sensor was attached to a nose bridge on a pair of glasses (rather than on forehead skin directly), to avoid the skin artifact (Green et al., 2007). Two 5 DOF sensors - TT (Tongue Tip) and TB (Tongue Body Back) - were attached on the midsagittal of the tongue. TT was located approximately 10 mm from the tongue apex (Wang et al., 2011, 2013a). TB was placed as far 41 back as possible, depending on the participant’s tongue length (Wang et al., 2013b). Lip movements were captured by attaching two 5 DOF sensors to the vermilion borders of the upper (UL) a</context>
<context position="22809" citStr="Wang et al., 2013" startWordPosition="3678" endWordPosition="3681">t al., 2008). Researchers are also testing the efficacy of permanently implantable and wireless sensors (Chen et al., 2012; Park et al., 2012). In the future, those more portable, and wireless articulatory motion tracking devices, when they are ready, will be used to develop a 43 portable SSI for practice use. In this experiment, a simple DTW algorithm was used to compare the training and testing phrases, which is known to be slower than most machine learning classifiers. Thus, in the future, the latency can be significantly reduced by using faster classifiers such as support vector machines (Wang et al., 2013c) or hidden Markov models (Heracleous and Hagita, 2011; King et al., 2007; Rudzicz et al., 2012; Uraga and Hain, 2006). Furthermore, in this proof-of-concept design, the vocabulary was limited to a small set of phrases, because our design required the whole experiment (including training and testing) to be done in about one hour. Additional work is needed to test the feasibility of open-vocabulary recognition, which will be much more usable for people after laryngectomy or with other severe voice impairments. 5 Conclusion and Future Work A preliminary, online test of a SSI based on electromag</context>
</contexts>
<marker>Wang, Green, Samal, 2013</marker>
<rawString>Wang, J., Green, J. R., and Samal, A. 2013b. Individual articulator&apos;s contribution to phoneme production, Proc. IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, Vancouver, Canada, 7795-89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wang</author>
<author>A Balasubramanian</author>
<author>Mojica de La Vega</author>
<author>L Green</author>
<author>J R</author>
<author>A Samal</author>
<author>B Prabhakaran</author>
</authors>
<title>Word recognition from continuous articulatory movement time-series data using symbolic representations,</title>
<date>2013</date>
<booktitle>ACL/ISCA Workshop on Speech and Language Processing for Assistive Technologies,</booktitle>
<pages>119--127</pages>
<location>Grenoble, France,</location>
<contexts>
<context position="13122" citStr="Wang et al., 2013" startWordPosition="2053" endWordPosition="2056">isplays the GUI of the SSI. The sensors were attached to the surface of each articulator using dental glue (PeriAcryl Oral Tissue Adhesive). Prior to the experiment, each subject participated in a three-minute training session (on how to use the SSI), which also helped them adapt to the oral sensors. Previous studies have shown those sensors do not significantly affect their speech output after a short practice (Katz et al., 2006; Weismer and Bunton, 1999). Figure 2 (left) shows the positions of the five sensors attached to a participant’s forehead, tongue, and lips (Green et al., 2003; 2013; Wang et al., 2013a). One 6 DOF (spatial and rotational) head sensor was attached to a nose bridge on a pair of glasses (rather than on forehead skin directly), to avoid the skin artifact (Green et al., 2007). Two 5 DOF sensors - TT (Tongue Tip) and TB (Tongue Body Back) - were attached on the midsagittal of the tongue. TT was located approximately 10 mm from the tongue apex (Wang et al., 2011, 2013a). TB was placed as far 41 back as possible, depending on the participant’s tongue length (Wang et al., 2013b). Lip movements were captured by attaching two 5 DOF sensors to the vermilion borders of the upper (UL) a</context>
<context position="22809" citStr="Wang et al., 2013" startWordPosition="3678" endWordPosition="3681">t al., 2008). Researchers are also testing the efficacy of permanently implantable and wireless sensors (Chen et al., 2012; Park et al., 2012). In the future, those more portable, and wireless articulatory motion tracking devices, when they are ready, will be used to develop a 43 portable SSI for practice use. In this experiment, a simple DTW algorithm was used to compare the training and testing phrases, which is known to be slower than most machine learning classifiers. Thus, in the future, the latency can be significantly reduced by using faster classifiers such as support vector machines (Wang et al., 2013c) or hidden Markov models (Heracleous and Hagita, 2011; King et al., 2007; Rudzicz et al., 2012; Uraga and Hain, 2006). Furthermore, in this proof-of-concept design, the vocabulary was limited to a small set of phrases, because our design required the whole experiment (including training and testing) to be done in about one hour. Additional work is needed to test the feasibility of open-vocabulary recognition, which will be much more usable for people after laryngectomy or with other severe voice impairments. 5 Conclusion and Future Work A preliminary, online test of a SSI based on electromag</context>
</contexts>
<marker>Wang, Balasubramanian, Vega, Green, R, Samal, Prabhakaran, 2013</marker>
<rawString>Wang, J., Balasubramanian, A., Mojica de La Vega, L., Green, J. R., Samal, A., and Prabhakaran, B. 2013c. Word recognition from continuous articulatory movement time-series data using symbolic representations, ACL/ISCA Workshop on Speech and Language Processing for Assistive Technologies, Grenoble, France, 119-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wang</author>
</authors>
<title>DJ and his friend: A demo of conversation using a real-time silent speech interface based on electromagnetic articulograph.</title>
<date>2014</date>
<note>[Video]. Available: http://www.utdallas.edu/~wangjun/ssidemo.html</note>
<contexts>
<context position="21004" citStr="Wang, 2014" startWordPosition="3395" endWordPosition="3396">ticipants demonstrated how the SSI can be used in daily conversation. To our best knowledge, this is the first conversational demo using a SSI. An informal survey to a few colleagues provided positive feedback. The conversation was smooth, although it is noticeably slower than a conversation between two healthy talkers. Importantly, the voice output quality (determined by the textto-speech synthesizer) was natural, which strongly supports the major motivation of SSI research: to produce speech with natural voice quality that current treatments cannot provide. A video demo is available online (Wang, 2014). The participants in this experiment were young and healthy. It is, however, unknown if the recognition accuracy may decrease or not for users after laryngectomy, although a single patient study showed the accuracy may decrease 15-20% compared to healthy talkers using an ultrasound-based SSI (Denby et al., 2011). Theoretically, the tongue motion patterns in (silent) speech after the surgery should be no difference with that of healthy talkers. In practice, however, some patients after the surgery may be under treatment for swallowing using radioactive devices, which may affect their tongue mo</context>
</contexts>
<marker>Wang, 2014</marker>
<rawString>Wang J. 2014. DJ and his friend: A demo of conversation using a real-time silent speech interface based on electromagnetic articulograph. [Video]. Available: http://www.utdallas.edu/~wangjun/ssidemo.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Weismer</author>
<author>K Bunton</author>
</authors>
<title>Influences of pellet markers on speech production behavior: Acoustical and perceptual measures,</title>
<date>1999</date>
<journal>Journal of the Acoustical Society of America,</journal>
<volume>105</volume>
<pages>2882--2891</pages>
<contexts>
<context position="12965" citStr="Weismer and Bunton, 1999" startWordPosition="2026" endWordPosition="2029">xtbook-sized generator. Participants were seated with their head within the calibrated magnetic field (Figure 2, the right picture), facing a computer monitor that displays the GUI of the SSI. The sensors were attached to the surface of each articulator using dental glue (PeriAcryl Oral Tissue Adhesive). Prior to the experiment, each subject participated in a three-minute training session (on how to use the SSI), which also helped them adapt to the oral sensors. Previous studies have shown those sensors do not significantly affect their speech output after a short practice (Katz et al., 2006; Weismer and Bunton, 1999). Figure 2 (left) shows the positions of the five sensors attached to a participant’s forehead, tongue, and lips (Green et al., 2003; 2013; Wang et al., 2013a). One 6 DOF (spatial and rotational) head sensor was attached to a nose bridge on a pair of glasses (rather than on forehead skin directly), to avoid the skin artifact (Green et al., 2007). Two 5 DOF sensors - TT (Tongue Tip) and TB (Tongue Body Back) - were attached on the midsagittal of the tongue. TT was located approximately 10 mm from the tongue apex (Wang et al., 2011, 2013a). TB was placed as far 41 back as possible, depending on </context>
</contexts>
<marker>Weismer, Bunton, 1999</marker>
<rawString>Weismer, G. and Bunton, K. (1999). Influences of pellet markers on speech production behavior: Acoustical and perceptual measures, Journal of the Acoustical Society of America, 105: 2882-2891.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Westbury</author>
</authors>
<title>X-ray microbeam speech production database user’s handbook.</title>
<date>1994</date>
<institution>University of Wisconsin-Madison,</institution>
<location>Madison, Wisconsin.</location>
<contexts>
<context position="19326" citStr="Westbury, 1994" startWordPosition="3122" endWordPosition="3123">ants in the experiment was S01, S02, and then S03. After the experiment of S01, where all three dimensional data (x, y, and z) were used, we decided to use only y and z for S02 and S03 to reduce the latency. As listed in Table 1, the latencies of S02 and S03 did significantly reduce, because less data was used for online recognition. Surprisingly, phrase recognition without using x dimension (left-right) data led to a decrease of accuracy and more training samples were required; prior research suggests that tongue movement in this dimension is not significant during speech in healthy talkers (Westbury, 1994). This observation is supported by participant S01, who had the highest accuracy and needed fewer training samples for each phrase (column 3 in Table 1). S02 and S03 used data of only y and z dimensions. Of course, data from more subjects are needed to confirm the potential significance of the x dimension movement of the tongue to silent speech recognition accuracy. Data transfer between the Wave machine and the SSI recognition component was done through TCP/IP protocols and in real-time. In the future, this design feature will allow the recognition component to run on a smart phone or any wea</context>
</contexts>
<marker>Westbury, 1994</marker>
<rawString>Westbury, J. 1994. X-ray microbeam speech production database user’s handbook. University of Wisconsin-Madison, Madison, Wisconsin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001248">
<title confidence="0.976254">
Vowel and Diacritic Restoration for Social Media Texts
</title>
<author confidence="0.988253">
K¨ubra Adalı
</author>
<affiliation confidence="0.968168">
Dep. of Computer Eng.
Istanbul Technical University
Istanbul, Turkey
</affiliation>
<email confidence="0.992914">
kubraadali@itu.edu.tr
</email>
<author confidence="0.992657">
G¨uls¸en Eryiˇgit
</author>
<affiliation confidence="0.968305666666667">
Dep. of Computer Eng.
Istanbul Technical University
Istanbul, Turkey
</affiliation>
<email confidence="0.99622">
gulsen.cebiroglu@itu.edu.tr
</email>
<sectionHeader confidence="0.993849" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969136363636">
In this paper, we focus on two important
problems of social media text normaliza-
tion, namely: vowel and diacritic restora-
tion. For these two problems, we pro-
pose a hybrid model consisting both a dis-
criminative sequence classifier and a lan-
guage validator in order to select one of
the morphologically valid outputs of the
first stage. The proposed model is lan-
guage independent and has no need for
manual annotation of the training data. We
measured the performance both on syn-
thetic data specifically produced for these
two problems and on real social media
data. Our model (with 97.06% on syn-
thetic data) improves the state of the art
results for diacritization of Turkish by 3.65
percentage points on ambiguous cases and
for the vowel restoration by 45.77 percent-
age points over a rule based baseline with
62.66% accuracy. The results on real data
are 95.43% and 69.56% accordingly.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999662076923077">
In recent years, with the high usage of computers
and social networks like Facebook and Twitter, the
analysis of the social media language has become
a very popular and crucial form of business intelli-
gence. But unfortunately, this language is very dif-
ferent from the well edited written texts and much
more similar to the spoken language, so that, the
available NLP tools do not perform well on this
new platform.
As we all know, Twitter announced (at April
1st, 2013)1 that it is shifting to a two-tiered ser-
vice where the basic free service ‘Twttr” will only
allow to use consonants in the tweets. Although,
</bodyText>
<footnote confidence="0.956072">
1https://blog.twitter.com/2013/
annncng-twttr
</footnote>
<bodyText confidence="0.998842585365854">
this is a very funny joke, people nowadays are al-
ready very used to use this style of writing without
vowels in order to fit their messages into 140 char-
acters Twitter or 160 characters SMS messages.
As a result, the vowelization problem (Twttr ⇒
Twitter) is no more limited with some specific lan-
guage families (e.g.semitic languages) (Gal, 2002;
Zitouni et al., 2006) but it became a problem of so-
cial media text normalization in general.
Diacritics are some marks (e.g. accents, dots,
curves) added to the characters and have a wide
usage in many languages. The absence of these
marks in Web2.0 language is very common and
posses a big problem in the automatic process-
ing of this data by NLP tools. Although, in the
literature, the term “diacritization” is used both
for vowel and diacritic restoration for semitic lan-
guages, in this paper, we use this term only for
the task of converting an ASCII text to its proper
form (with accents and special characters). A
Turkish example is the word “dondu” (it is frozen)
which may be the ascii form of both “dondu”(it is
frozen) or “d¨ond¨u” (it returned) where the ambigu-
ity should be resolved according to the context. In
some studies, this task is also referred as “unicodi-
fication”(Scannell, 2011) or “deasciification”(T¨ur,
2000).
In this paper, we focus on these two important
problems of social text normalization, namely: di-
acritization and vowelization. These two problems
compose almost the quarter (26.5%) of the nor-
malization errors within a 25K Turkish Tweeter
Data Set. We propose a two stage hybrid model:
firstly a discriminative model as a sequence classi-
fication task by using CRFs (Conditional Random
Fields) and secondly a language validator over the
first stage’s results. Although in this paper, we pre-
sented our results on Turkish (which is a highly ag-
glutinative language with very long words full of
un-ascii characters), the proposed model is totally
language independent and has no need for manual
</bodyText>
<page confidence="0.98867">
53
</page>
<note confidence="0.9913665">
Proceedings of the 5th Workshop on Language Anal sis for Social Media (LASM) @ EACL 2014, pages 53–61,
Gothenburg, Sweden, April 26-30 2014. (c 2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999919">
annotation of the training data. For morpholog-
ically simpler languages, it would be enough to
use a lexicon lookup for the language validation
stage (whereas we used a morphological analyzer
for the case of Turkish). With our proposed model,
we obtained the highest results in the literature for
Turkish diacritization and vowelization.
The remaining of the paper is structured as fol-
lows: Section 2 discusses the related work, Sec-
tion 3 tries to show the complexity of diacritiza-
tion and the vowelization tasks by giving exam-
ples from an agglutinative language; Turkish. Sec-
tion 4 introduces our proposed model and Section
5 presents our experiments and results. The con-
clusion is given in Section 6.
</bodyText>
<sectionHeader confidence="0.999782" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999912075757576">
The vowelization problem is mostly studied for
semitic languages and many different methods are
applied to this problem. The problem is generally
referred as diacritization for these languages, since
diacritics are placed on consonants for the purpose
of vowel restoration. For example, the short vow-
els in Arabic are only pronounced by the use of
diacritics put on other consonants. Some of these
studies are as follows: Gal (2002) reports the re-
sults on Hebrew by using HMMs and Zitouni et
al. (2006) on Arabic by using maximum entropy
based models. Al-Shareef and Hain (Al-Shareef
and Hain, 2012) deals with the vowelization of
colloquial Arabic for automatic speech recogni-
tion task by using CRFs on speaker and contextual
information. Haertel et al. (2010) uses conditional
markov models for the vowel restoration problem
of Syriac. Nelken and Shieber (2005) uses a finite
state transducer approach for Arabic as well. To
the best of our knowledge, the vowelization work
on Turkish is the first study on a language which
do not possess the vowelization problem by its na-
ture. We believe that on that sense, our hybrid
model will be a good reference for future studies
in social media text normalization where the prob-
lem is disregarded in recent studies.
The diacritization task on the other hand is
not addressed as frequently as the vowelization
problem2. Some studies are as follows: Scan-
nell (2011) uses a Naive Bayes classifier for both
word-level and character-level modeling. Each
ambiguous character in the input is regarded as
2Here, we exclude all the works done for semitic lan-
guages. The reason is explained on the former paragraph.
an independent classification problem. They are
using lexicon lookup which is not feasible for ev-
ery possible word surface form in agglutinative or
highly inflected languages. They refer to a lan-
guage model in ambiguous cases. They tested
their system for 115 languages as well as for Turk-
ish (92.8% on a much easier data set than ours (re-
fer to Section 5.1) . Simard and Deslauriers (2001)
tries to recover the missing accents in French.
They are using a generative statistical model for
this purpose. De Pauw et al. (2007) also test their
MBL (memory based learning) model on differ-
ent languages. Although they do not test for Turk-
ish, the most attractive part of theirs results is that
the performances for highly inflectional languages
differ sharply from the others towards the negative
side. Nguyen and Ock (2010) deals with the dia-
critization of Vietnamese by using Adaboost and
C4.5.
The work done so far for the diacritization
of Turkish are from T¨ur (2000) (character-based
HMM model), Zemberek (2007), Y¨uret and de la
Maza (Y¨uret and de la Maza, 2006) (GPA: a kind
of decision list algorithms). We give the compar-
ison of the two later systems on our data set and
propose a discriminative equivalent of the HMM
approach used in T¨ur (2000) (see Section 5 for fur-
ther discussions). For the vowelization, the only
study that we could find is from T¨ur (2000) which
uses again the same character-level HMM model
into this problem (with an equivalent discrimina-
tive model given at Table 8 ±3ch model).
</bodyText>
<sectionHeader confidence="0.972575" genericHeader="method">
3 The complexity
</sectionHeader>
<bodyText confidence="0.99937">
This section tries to draw light upon the complex-
ity of diacritization and the vowelization tasks by
giving examples from an agglutinative language;
Turkish.
</bodyText>
<subsectionHeader confidence="0.99655">
3.1 Turkish
</subsectionHeader>
<bodyText confidence="0.999851">
Turkish is an agglutinative language which means
that words have theoretically an infinite number
of possible surface forms due to the iterative con-
catenation of inflectional and derivational suffixes.
As for other similar languages, this property of the
language makes impractical for Turkish words to
be validated by using a lexicon. And also, the in-
creasing length3 of the words creates a big search
space especially for the vowelization task.
</bodyText>
<footnote confidence="0.985528333333333">
3The average word length is calculated as 6.1 for Turkish
nouns and 7.6 for verbs in a 5 million word corpus(Akın and
Akın, 2007).
</footnote>
<page confidence="0.999226">
54
</page>
<bodyText confidence="0.999839125">
Turkish alphabet has 7 non-ascii characters that
don’t exist in the Latin alphabet (c¸, ˘g, ı, ˙I, ¨o, s¸,
¨u) and the ascii counterparts of these letters (c, g,
i, I, o, s, u) are also valid letters in the alphabet
which causes an important disambiguation prob-
lem at both word and sentence level. The alphabet
contains 8 vowels (a(A), e(E), ı(I), i(˙I), o(O), ¨o( ¨O),
u(U), ¨u( ¨U)) in total.
</bodyText>
<subsectionHeader confidence="0.998366">
3.2 Diacritization
</subsectionHeader>
<bodyText confidence="0.992557">
The following real example sentence taken from
social media “Ruyamda evde oldugunu gordum.”,
written by using only the ascii alphabet, has two
possible valid diacritized versions:
</bodyText>
<listItem confidence="0.995561333333333">
1. “R¨uyamda evde oldu˘gunu g¨ord¨um.”
(I had a dream that you were at home.)
2. “R¨uyamda evde ¨old¨u˘g¨un¨u g¨ord¨um.”
</listItem>
<bodyText confidence="0.982480304347826">
(I had a dream that you died at home.)
As can be observed from this sentence some of
the asciified words (e.g. “oldugunu”) has more
than one possible valid counterparts which causes
the meaning change of the whole sentence.
The problem is the decision of the appropri-
ate forms for the critical letters (C, G, I, O, S,
U)4. Although the problem seems like a multi-
class classification problem, it is in essence a
binary-classification task for each critical letter
and can be viewed as a binary sequence classifi-
cation task for the whole word so that the orig-
inal word will be chosen from (2n) possibilities
where n is the number of occurrence of critical
letters (C, G, I, O, S, U) in the ascii version.
For example the word “OldUGUnU” has (25=32)
possible transformations whereas only 2 of them
(“oldu˘gunu” and “¨old¨u˘g¨un¨u”) are valid Turkish
words. Figure 1 gives a second example and shows
all the possible (22=4) diacritized versions of the
word “aCI” where again only two of them are
valid words (emphasized with a bold background
colour): “ac¸ı”(angle) and “acı”(pain).
</bodyText>
<subsectionHeader confidence="0.998111">
3.3 Vowelization
</subsectionHeader>
<bodyText confidence="0.999760333333333">
Vowelization on the other hand causes much more
complexity when compared to diacritization. Each
position5 between consequent consonants, at the
</bodyText>
<footnote confidence="0.749320375">
4From this point on, we will show the ascii versions of
these letters as capital letters meaning that they may appear
in the diacritized version of the word either in their ascii form
or in their diacritized form. Ex: the capital C will become
either c or c¸ after the processing.
5For the sake of simplicity, we just assumed that only zero
or one vowel may appear between two consonants whereas
there exist some words with consecutive vowels (such as
</footnote>
<figureCaption confidence="0.999784">
Figure 1: Possible Diacritized Versions of “aCI”
</figureCaption>
<bodyText confidence="0.999723444444445">
beginning or ending of the word may take one
vowel or not resulting a selection from 9 class
labels (the 8 vowel letters + the null charac-
ter). For example, the vowelization of the word
“slm”(“hi” written without vowels, with n=4 po-
sitions s l m ) will produce 94 = 6561 possibili-
ties where 39 of them are valid Turkish words (e.g.
“salam”(salami), “sulama”(watering), “salım”(my
raft), “selam”(hi), “sılam”(my furlough) etc...).
</bodyText>
<figureCaption confidence="0.983715">
Figure 2: Proposed Model
</figureCaption>
<sectionHeader confidence="0.959709" genericHeader="method">
4 Proposed Model
</sectionHeader>
<bodyText confidence="0.970533166666667">
Most of the previous work in the literature (Sec-
tion 2) uses either some (generative of discrimina-
tive) machine learning models or some nlp tools
(e.g. morphological analyzers, pos taggers, lin-
guistic rules) in order to solve the vowelization
“saat”(clock)) although very rarely
</bodyText>
<page confidence="0.990353">
55
</page>
<bodyText confidence="0.999971">
problem. As it is shown in the previous section,
for languages with higher number of vowels and
word length due to their rich inflectional morphol-
ogy, the search space gets very high very rapidly.
Since the problem is mostly similar to generation,
in order to increase the likelihood of the generated
output word, most of the approaches include char-
acter level probabilities or relationships. In this
case, it is unfair to expect from a machine learn-
ing system to generate morphologically valid out-
puts (especially for highly inflectional languages)
while trying to maximize the overall character se-
quence probability.
We propose a two stage model (Figure 2) which
has basically two components.
</bodyText>
<listItem confidence="0.9949015">
1. a discriminative sequence classifier
2. a language validator
</listItem>
<subsectionHeader confidence="0.981392">
4.1 Discriminative Sequence Classifier
</subsectionHeader>
<bodyText confidence="0.999689222222222">
In the first stage, we use CRFs6 (Lafferty et al.,
2001) in order to produce the most probable out-
put words. This stage treats the diacritization and
vowelization as character level sequence labeling
tasks, but since it is a discriminative model, it is
also possible to provide neighboring words as fea-
tures into the system. During training, each in-
stance within a sequence has basically the follow-
ing main parts:
</bodyText>
<listItem confidence="0.9977328">
1. features related to the current and neighbor-
ing tokens (namely surface form or lemma)
2. features related to the current and neighbor-
ing characters$
3. class label
</listItem>
<bodyText confidence="0.999953692307692">
The test data is also prepared similarly except
the gold standard class labels.
Table 1 and Table 2 show instance samples for
the sample words (“OldUGUnU” and “ s l m ”)
given in Section 3. As can be observed from the
tables, we have 7 different class labels in diacritic
restoration and 9 different class labels in vowel
restoration (one can refer to Section 3 for the de-
tails). The sequences represent words in focus and
each instance line within a sequence stands for the
character position in focus. The sample for dia-
critization has 5 character features and 2 word fea-
tures where the current character feature limits the
</bodyText>
<footnote confidence="0.54030225">
6In this work, we used CRF++7 which is an open source
implementation of CRFs.
$The feature related to the current character is only avail-
able in diacritization model
</footnote>
<bodyText confidence="0.999659">
number of the class labels to be assigned to that
position by 2. The sample for vowelization has 1
word feature and 6 character features.
</bodyText>
<table confidence="0.997303">
Curr. Neig. Curr. Neig. Neig. Neig. Neig. Class
Letter Word(+1) Word Ch(-2) Ch(-1) Ch(+1) Ch(+2) Label
O GOrdUm OldUGUnU l d l d o¨
U GOrdUm OldUGUnU d U G U u¨
G GOrdUm OldUGUnU U G U n g˘
U GOrdUm OldUGUnU U n n U u¨
U GOrdUm OldUGUnU u¨
</table>
<tableCaption confidence="0.69588">
Table 1: Diacritization: Instance Representation
for the word ”oldugunu”
</tableCaption>
<table confidence="0.994049142857143">
“OldUGUnU” 5 critical positions
Curr. Neig. Neig. Neig. Neig. Neig. Neig. Class
Word Ch(-3) Ch(-2) Ch(-1) Ch(+1) Ch(+2) Ch(+3) Label
slm s s s s l m e
slm l l l m a
slm m m
slm
</table>
<tableCaption confidence="0.874169">
Table 2: Vowelization: Instance Representation
for the word “slm”
</tableCaption>
<bodyText confidence="0.97842525">
“ s l m ” 4 possible vowel positions
CRFs are log-linear models and in order to
get advantage of the useful feature combinations,
one needs to provide these as new features to the
CRFs. In order to adopt a systematic way, we took
the features’ combinations for character features
and word features separately. For character fea-
tures we took the combinations up to 6-grams for
±3ch and for the neighboring word features up to
4 grams. The number of features affects directly
the maximum amount of training data that could
be used during the experiments. The total number
of feature templates after the addition of feature
combinations ranges between 7 for the simplest
case and 30 for our model with maximum num-
ber of features. Three sample feature templates are
given below for the sample sequence of Table 1.
The templates are given in [pos,col] format, where
pos stands for the relative position of the token in
focus and col stands for the feature column num-
ber in the input file. U06 is the template for us-
ing the sixth9 feature in Table 1 (Neigh. Ch(+1)).
U13 is a bigram feature combination of 2nd and
3th features (the current token and the next token).
U11 is a fourgram feature combination of 4th, 5th,
6th and 7th features of our feature set that refers
to the group of the previous two characters and the
next two characters.
</bodyText>
<footnote confidence="0.9977475">
9in CRF++ feature templates the features indexes start
from 0.
</footnote>
<page confidence="0.955158">
56
</page>
<equation confidence="0.999509">
U06 : %x[0, 5]
U13 : %x[0, 1]/%x[0, 2]
U11 : %x[0, 3]/%x[0, 4]/%x[0, 5]/%x[0, 6]
</equation>
<subsectionHeader confidence="0.987923">
4.2 Language Validator
</subsectionHeader>
<bodyText confidence="0.99999355">
The n best sequences of the discriminative classi-
fier is then transferred to the language validator.
We use a two-level morphological analyzer (S¸ahin
et al., 2013) for the Turkish case since in this ag-
glutinative language it is impractical to validate a
word by making a lexicon lookup. But this sec-
ond part may be replaced by any language valida-
tor (for other languages) which will filter only the
valid outputs from the n best results of the discrim-
inative classifier. Figure 2 shows an example case
of the process for vowelization. The system takes
the consonant sequence “kd” and the 5 best output
of the first stage is produced as “kidi, kedi, kadı,
kado, kada”. The language validator then chooses
the most probable valid word “kedi” (cat) as its
output. One should notice that if none of the n
most probable results is a valid word, then the sys-
tem won’t produce any suggestion at all. We show
experimental results on the effect of the selection
of n in the next section.
</bodyText>
<sectionHeader confidence="0.996807" genericHeader="method">
5 Experimental Setup And Results
</sectionHeader>
<bodyText confidence="0.9999725">
In this section, we first present our datasets and
evaluation strategy. We then continue with the di-
acritization experiments and finally we share the
results of our vowelization experiments.
</bodyText>
<subsectionHeader confidence="0.947432">
5.1 Datasets and Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.999979954545455">
For both of the diacritization and vowelization
tasks, creating the labeled data is a straightforward
task since the reverse operations for these (con-
verting from formally written text to their Ascii
form or to a form without vowels) can be accom-
plished automatically for most of the languages
(except semitic languages where the vowels do not
appear in the formal form). To give an example
from Turkish, the word “oldu˘gunu” may be auto-
matically converted to the form “OldUGUnU” for
diacritization and “ l d g n ” for vowelization ex-
periments. We used data from three different cor-
pora: METU Corpus (Say et al., 2002) and two
web corpora from Yıldız and Tantu˘g (2012) and
Sak et al. (2011).
In order to judge different approaches fairly,
we aimed to create a decently though test set.
Since the vowelization task already comprises a
very high ambiguity, we focused to the ambigu-
ous diacritization samples. With this purpose, we
first took the Turkish dictionary and converted all
the lemmas within the dictionary into their Ascii
forms. We then created the possible diacritized
forms (Figure 1) and created a list of all ambigu-
ous lemmas (1221 lemmas in total) by finding all
the lemmas which could be produced as the out-
put of diacritization. For example “ac¸ı” and “acı”
are put into this list after this operation. Although
this ambiguous lemmas list may be extended by
also considering interfusing surface forms, for the
sake of simplicity we just considered to take the
ambiguous lemmas from the dictionary. We then
searched our three corpora (and the WEB where
not available in these) for the words with an am-
biguous lemma and created our test set so that for
each ambiguous lemma there is exactly one sen-
tence consisting of it. As a result, we collected a
test set of 1157 sentences (17923 tokens) consist-
ing of 1871 ambiguous words10 in total. The re-
maining sentences from the corpora are used dur-
ing training. Since the feature set size directly af-
fects the amount of usable training data, for differ-
ent experiment sets we used different size of train-
ing data each time trying to use the data from the
three corpora in equal amounts.
After evaluating with synthetically produced
training and test sets, we also tested our best per-
formed models on real data collected from social
media (25K tweets with at least one erroneous
token) and normalized manually (Eryi˘git et al.,
2013). This data consists 58836 tokens that have
text normalization problems where 3.75% is due
to missing vowels and 22.8% is due to misuse of
Turkish characters. In order to separate these spe-
cific error sets, we first automatically aligned the
original and normalized tweets and then applied
some filters over the aligned data: e.g. Deasci-
ification errors are selected so that the character
length of the original word and its normalized
form should be the same and the differing letters
should only be the versions of the same Turkish
characters.
For the evaluation of diacritization, we provide
two accuracy scores: Accuracy over the entire
words (AccOr ,.ll Equation 1) and accuracy over
the ambiguous words alone (AccAmb Equation 2
</bodyText>
<footnote confidence="0.840928">
10One should note that each sentence in the test set con-
tains at least one or more ambiguous surface forms. The test
data will be available to the researches via http://...
</footnote>
<page confidence="0.998837">
57
</page>
<bodyText confidence="0.998812333333333">
over 1871 ambiguous words in the test set). Since
the vowelization problem is almost entirely am-
biguous, the two scores are almost the same for the
entire tests (# of words Pt� # of amb. words).
That is why, for the vowelization task we provide
only AccOverall.
</bodyText>
<equation confidence="0.705161">
# of corr. diacritized amb. words
AccAmb = # of amb. words
(2)
</equation>
<bodyText confidence="0.999868545454546">
In the work of T¨ur (2000), the accuracy score
is provided as the correctly determined characters
which we do not find useful for the given tasks:
AccAmb = # of corr. diacritized amb. chars . This # of amb. chars
score gives credit to the systems although the pro-
duced output word is not a valid Turkish word. For
example, if a vowelization system produces an in-
valid output as “oldgn” for the input “ l d g n ”, it
will have a 1/5 (one correct character over 5 possi-
ble positions) score whereas in our evaluation this
output will be totally penalized.
</bodyText>
<subsectionHeader confidence="0.998629">
5.2 Diacritization Experiments
</subsectionHeader>
<bodyText confidence="0.999688448275862">
For diacritization, we designed four sets of ex-
periments. The first set of experiments (Table 3)
presents the results of our baseline systems. We
provide four baseline systems. The first one is a
rule based diacritics restorer which creates all the
possible diacritics for a given input and outputs the
first morphologically valid one. As the proposed
model does, the rule based system also validates
its outputs by using the morphological analyzer
introduced in Section 4.2. One can see from the
table that the accuracy on the ambiguous words of
this system is nearly 70%. Our second baseline
uses a unigram language model in order to select
the most probable valid output of the morpholog-
ical analyzer. Our third baseline is a baseline for
our discriminative classifier (with only f2 neigh-
boring characters) without the language validator
component. In this model, the top most output of
the CRF is accepted as the correct output word.
One can observe that this baseline although it per-
forms better than the rule based system, it is worse
than the second baseline with a language model
component. Our last baseline is the baseline for
the proposed system in this paper with a discrimi-
native classifier (using only f2 neighboring char-
acters) and a language validator which chooses the
first valid output within the top 5 results of the
classifier. It outperforms all the previous base-
lines.
</bodyText>
<table confidence="0.998796666666667">
Acc Acc
System Overall Amb
Rule based 90.38 69.17
Rule based + Unigram LM 91.94 83.54
CRF f2ch 87.93 77.24
CRF f2ch + Lang.Valid. 94.88 88.51
</table>
<tableCaption confidence="0.999289">
Table 3: Diacritization Baseline Results
</tableCaption>
<bodyText confidence="0.999967486486487">
The second set of experiments given in Table 4
is for the feature selection of the proposed model.
We test with the neighboring characters up to f3
and together with the surface form of the cur-
rent token sformcurr and/or the first n charac-
ters of the current token firstnchcurr as lemma
feature. For both of the first two sets of exper-
iments (Table 3 and Table 4) we used a train-
ing data of size 4591K (the max. possible size
for the most complex feature set in these experi-
ments; (last line of Table 4). It can be observed
from Table 4 that although f3ch (2nd line) per-
forms better than f2ch (1st line), when we use
these together with sformcurr we obtain better
results with f2ch (3rd line). Since f3ch (7 char-
acters in total) will be very close to the whole
number of characters within the surface form, the
new feature’s help is more modest in f3ch model.
In these experiments we try to optimize on the
overall accuracies. Our highest score in this ta-
ble is with the f2ch + sformcurr + first5chcurr
(last line) but since the difference between this and
f2ch + sformcurr is not statistically significant
(with McNemar’s test p&lt;0.01) and the size of the
maximum possible training data could still be im-
proved for the latter model, we decided to continue
with f2ch + sformcurr.
In the third set of diacritization experiments
(Table 5) we investigated the effect of using the
neighboring tokens as features. In this experi-
ment set, the training data size is decreased to a
much lower size, only 971K in order to be able
to train with f2 neighboring tokens. Each line
of the table is the addition of the surface forms
for the precised positions to the model of the first
line f2ch + sformcurr. We tested with all the
combinations in the f2 window size. For exam-
</bodyText>
<figure confidence="0.865319">
AccOverall =
# of words
# of corr. diacritized words
(1)
</figure>
<page confidence="0.97333">
58
</page>
<table confidence="0.999406833333333">
Feature Combinations Acc Acc
Overall Amb
±2ch 94.88 88.51
±3ch 95.76 91.05
±2ch + sformc„rr 96.26 91.60
±3ch + sformcurr 96.20 91.71
±2ch + first3chcurr 95.29 90.17
±2ch + first4chcurr 95.60 89.06
±2ch + first5chcurr 95.95 90.72
±2ch + sformcurr + first3chcurr 96.23 91.82
±2ch + sformcurr + first4chcurr 96.26 91.82
±2ch + sformcurr + first5chcurr 96.28 91.60
</table>
<tableCaption confidence="0.977231">
Table 4: Diacritization Feature Selection I
</tableCaption>
<table confidence="0.999765363636364">
Features Acc Acc
Overall Amb
±2ch + sformcurr 95.29 90.61
+sform0010 95.49 90.72
+sform0011 95.39 90.39
+sform0100 93.77 83.32
+sform0110 95.39 90.28
+sform0111 95.26 89.95
+sform1100 95.24 89.83
+sform1110 95.21 89.50
+sform1111 95.11 89.17
</table>
<tableCaption confidence="0.998804">
Table 5: Diacritization Feature Selection II
</tableCaption>
<bodyText confidence="0.947131666666667">
ple sform0010 means that the surface form of the
token at position +1 is added to the features. This
feature set outperformed all the other ones.
</bodyText>
<table confidence="0.998742166666667">
Acc Acc
System Overall Amb
Y¨uret (2006) 95.93 91.05
Zemberek (2007) 87.71 82.55
±2ch + sformcurr 96.15 92.04
±2ch + sformc„rr + sform0010 97.06 94.70
</table>
<tableCaption confidence="0.930817">
Table 6: Diacritization Results Comparison with
Previous Work
</tableCaption>
<bodyText confidence="0.983297285714286">
Finally, in Table 6, we give the comparison re-
sults of our proposed model with the available
Turkish deasciifiers (the tools’ original name given
by the authors) (Y¨uret and de la Maza, 2006; Akın
and Akın, 2007). We both tested by ±2ch +
sformcurr and ±2ch + sformcurr + sform0010.
Both of the models are tested with maximum pos-
sible size of training data: 10379K and 5764K suc-
cessively. Our proposed model for diacritization
outperformed all of the other methods with a suc-
cess rate of 97.06%. It outperformed the state of
the art by 1.13 percentage points in overall accu-
racy and by 3.65 percentage points in ambiguous
cases (both results statistically significant).
</bodyText>
<subsectionHeader confidence="0.996851">
5.3 Vowelization Experiments
</subsectionHeader>
<bodyText confidence="0.999700789473684">
For the vowelization, we designed similar set of
experiments. In Table 7, we provide the results
for a rulebased baseline and our proposed model
with ±2ch. It is certainly a very time consuming
process to produce all the possible forms for the
vowelization task (see Section 3.3). Thus, for the
rule based baseline we stopped the generation pro-
cess once we find a valid output. The baseline of
the proposed model provides a 28.44 percentage
points improvements over the rule based system.
We did not try to compare our results with the
work of T¨ur (2000) (an HMM model on charac-
ter level) firstly because the developed model was
not available for testing, secondly because the pro-
vided evaluation (see Section 5.1) was useless for
our purposes and finally because our ±3 charac-
ter model provided in the second line of Table 8 is
a discriminative counterpart of his 6-gram genera-
tive model.
</bodyText>
<table confidence="0.9926485">
System Acc
Overall
Rule based 16.89
CRF ±2ch+Lang.Valid. 45.33
</table>
<tableCaption confidence="0.999056">
Table 7: Vowelization Baseline Results
</tableCaption>
<bodyText confidence="0.99057325">
Table 8 gives the feature selection tests’ results
similarly to the previous section. This time we ob-
tained the highest score with ±3ch + sformcurr
59.17%. In this set of experiments, we used
4445K of training data.
In order to investigate the impact of neighbor-
ing tokens, in the experiments given in Table 9,
we had to continue with ±2ch + sformcurr with
</bodyText>
<table confidence="0.991109416666667">
Feature Combinations Acc
Overall
±2ch 45.33
±3ch 57.20
±2ch + sformcurr 57.22
±3ch + sformc„rr 59.17
±2ch + first3chcurr 40.44
±2ch + first4chcurr 40.48
±2ch + first5chcurr 44.22
±2ch + sformcurr + first3chcurr 45.89
±2ch + sformcurr + first4chcurr 45.89
±2ch + sformcurr + first5chcurr 49.58
</table>
<tableCaption confidence="0.999487">
Table 8: Vowelization Feature Selection I
</tableCaption>
<page confidence="0.981336">
59
</page>
<figure confidence="0.994238375">
�AcctopN
1 if result exists within top N
=
# of words
(3)
System Acc Acc
Overall top N
+3ch + sformcurr
With Top 5 Poss. from CRF 62.05 80.21
+3ch + sformcurr
With Top 7 Poss. from CRF 62.36 82.53
+3ch + sformcurr
With Top 10 Poss. from CRF 62.66 85.09
Features Acc
Overall
+2ch + sformcurr 54.07
+sform0010 50.89
+sform0011 49.60
+sform0100 31.84
+sform0110 49.41
+sform0111 47.78
+sform1100 48.98
+sform1110 47.88
+sform1111 47.21
</figure>
<tableCaption confidence="0.9983485">
Table 9: Vowelization Feature Selection II
Table 10: Vowelization Top N Results
</tableCaption>
<bodyText confidence="0.999645725">
971K of training data.11 We could not obtain any
improvement with the neighboring tokens. We re-
late these results to the fact that the neighboring
tokens are also in vowel-less form in the training
data so that this information do not help the dis-
ambiguation of the current token. Since we could
not add the word based features to this task by this
model, for future work we are planning to apply
a word based language model over the proposed
model’s possible output sequences.
In the final experiment set given in Table 10,
we trained our best performing model +3ch +
sformcurr with the maximum possible training
data (6653K). We also tested with different N val-
ues of CRF output. Although there is a slight in-
crease on the overall accuracy by passing from
N=5 to N=10, the increase is much higher when
we evaluate with AcctopN. Equation 3 gives the
calculation of this score which basically calculates
the highest score that could be obtained after per-
fect reranking of the top N results. In this score the
system is credited if the correct vowelized answer
is within the top N results of the system. We see
from the table that there is still a margin for the
improvement in top 10 results (up to 85.09% for
the best model). This strengthens our believe for
the need of a word based language model over the
proposed model outputs. Our vowelization model
in its current state achieves an accuracy score of
62.66% with a 45.77 percentage points improve-
ments over the rule based baseline.
11If we select the larger model, it is going to be impossi-
ble to feed enough training data to the system. Since in this
set of experiments (Table 9) we only investigate the impact
of neighboring tokens, we had/preferred to select the smaller
character model.
Finally we test our best models on voweliza-
tion and diacritization errors from our Tweeter
data set and obtained 95.43% for diacritization and
69.56% for vowelization.
</bodyText>
<sectionHeader confidence="0.998626" genericHeader="method">
6 Conclusion And Future Work
</sectionHeader>
<bodyText confidence="0.999964933333333">
In this paper, we proposed a hybrid model for the
diacritization and vowelization tasks which is an
emerging problem of social media text normaliza-
tion. Although the tasks are previously investi-
gated for different purposes especially for semitic
languages, to the best of our knowledge, this is
the first time that they are evaluated together for
the social media data on a language which do not
possess these problems in its formal form but only
in social media platform. We obtained the high-
est scores for the diacritization (97.06%) and vow-
elization (62.66%) of Turkish.
We have two future plans for the vowelization
part of the proposed model. The first one, as de-
tailed in previous section, is the application of a
word based language model over the valid CRF
outputs. The second one is the extension for par-
tial vowelization. Although in this work, we de-
signed the vowelization task as the overall genera-
tion of the entire vowels within a vowel-less word,
we observe from the social web data that people
also tend to write with partially missing vowels.
As an example, they are writing “sevyrm” instead
of the word “seviyorum” (I love). In this case, the
position between the consonants ‘s’ and ‘v’ is con-
strained to the letter ‘e’ and it is meaningless to
generate the other possibilities for the remaining 7
vowels. For this task, we are planning to focus on
constrained Viterbi algorithms during the decod-
ing stage.
</bodyText>
<page confidence="0.993726">
60
</page>
<bodyText confidence="0.998455">
The tool’s web api and the produced data sets
will be available to the researchers from the fol-
lowing address http://tools.nlp.itu.edu.tr/(Eryi˘git,
2014)
</bodyText>
<sectionHeader confidence="0.953981" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.999904333333333">
This work is part of a research project supported
by TUBITAK 1001(Grant number: 112E276) as
an ICT cost action (IC1207) project.
</bodyText>
<sectionHeader confidence="0.999136" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999660895833333">
Ahmet Afsin Akın and Mehmet D¨undar Akın. 2007.
Zemberek, an open source nlp framework for turkic
languages. Structure.
Sarah Al-Shareef and Thomas Hain. 2012. Crf-based
diacritisation of colloquial Arabic for automatic
speech recognition. In INTERSPEECH. ISCA.
Guy De Pauw, Peter W. Wagacha, and Gilles-Maurice
de Schryver. 2007. Automatic diacritic restoration
for resource-scarce languages. In Proceedings of the
10th international conference on Text, speech and
dialogue, TSD’07, pages 170–179, Berlin, Heidel-
berg. Springer-Verlag.
G¨uls¸en Eryi˘git, Fatih Samet C¸etin, Meltem Yanık,
Tanel Temel, and ˙Iyas C¸ic¸ekli. 2013. Turksent: A
sentiment annotation tool for social media. In Pro-
ceedings of the 7th Linguistic Annotation Workshop
and Interoperability with Discourse, pages 131–134,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
G¨uls¸en Eryi˘git. 2014. ITU Turkish NLP web service.
In Proceedings of the Demonstrations at the 14th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics (EACL), Gothen-
burg, Sweden, April. Association for Computational
Linguistics.
Ya’akov Gal. 2002. An hmm approach to vowel
restoration in Arabic and Hebrew. In Proceed-
ings of the ACL-02 workshop on Computational ap-
proaches to semitic languages, SEMITIC ’02, pages
1–7, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Robbie A. Haertel, Peter McClanahan, and Eric K.
Ringger. 2010. Automatic diacritization for low-
resource languages using a hybrid word and con-
sonant cmm. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, HLT ’10, pages 519–527, Stroudsburg, PA,
USA. Association for Computational Linguistics.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML, pages 282–289.
Rani Nelken and Stuart M. Shieber. 2005. Arabic di-
acritization using weighted finite-state transducers.
In Proceedings of the ACL Workshop on Compu-
tational Approaches to Semitic Languages, Semitic
’05, pages 79–86, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Kiem-Hieu Nguyen and Cheol-Young Ock. 2010. Di-
acritics restoration in Vietnamese: letter based vs.
syllable based model. In Proceedings of the 11th
Pacific Rim international conference on Trends in
artificial intelligence, PRICAI’10, pages 631–636,
Berlin, Heidelberg. Springer-Verlag.
Muhammet S¸ahin, Umut Sulubacak, and G¨uls¸en
Eryi˘git. 2013. Redefinition of Turkish morphol-
ogy using flag diacritics. In Proceedings of The
Tenth Symposium on Natural Language Processing
(SNLP-2013), Phuket, Thailand, October.
Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2011.
Resources for Turkish morphological processing.
Lang. Resour. Eval., 45(2):249–261, May.
Bilge Say, Deniz Zeyrek, Kemal Oflazer, and Umut
¨Ozge. 2002. Development of a corpus and a tree-
bank for present-day written Turkish. In Proceed-
ings of the Eleventh International Conference of
Turkish Linguistics, Famaguste, Cyprus, August.
Kevin P. Scannell. 2011. Statistical unicodification of
African languages. Lang. Resour. Eval., 45(3):375–
386, September.
Michel Simard and Alexandre Deslauriers. 2001.
Real-time automatic insertion of accents in French
text. Nat. Lang. Eng., 7(2):143–165, June.
G¨okhan T¨ur. 2000. A statistical information extrac-
tion system for Turkish. Ph.D. thesis, Department of
Computer Engineering and the Institute of Engineer-
ing and Science of Bilkent University, Ankara.
Eray Yıldız and C¨uneyd Tantu˘g. 2012. Evaluation
of sentence alignment methods for English-Turkish
parallel texts. In Proceedings of the First Workshop
on Language Resources and Technologies for Turkic
Languages (LREC), Istanbul, Turkey, 23-25 May.
Deniz Y¨uret and Michael de la Maza. 2006. The
greedy prepend algorithm for decision list induction.
In Proceedings of the 21st international conference
on Computer and Information Sciences, ISCIS’06,
pages 37–46, Berlin, Heidelberg. Springer-Verlag.
Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.
2006. Maximum entropy based restoration of Ara-
bic diacritics. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics
and the 44th annual meeting of the Association
for Computational Linguistics, ACL-44, pages 577–
584, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.999273">
61
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.392883">
<title confidence="0.989533">Vowel and Diacritic Restoration for Social Media Texts</title>
<author confidence="0.678373">K¨ubra</author>
<affiliation confidence="0.9853915">Dep. of Computer Istanbul Technical</affiliation>
<address confidence="0.800932">Istanbul,</address>
<email confidence="0.996794">kubraadali@itu.edu.tr</email>
<affiliation confidence="0.984543">Dep. of Computer Istanbul Technical</affiliation>
<address confidence="0.805195">Istanbul,</address>
<email confidence="0.997862">gulsen.cebiroglu@itu.edu.tr</email>
<abstract confidence="0.998440826086956">In this paper, we focus on two important problems of social media text normalization, namely: vowel and diacritic restoration. For these two problems, we propose a hybrid model consisting both a discriminative sequence classifier and a language validator in order to select one of the morphologically valid outputs of the first stage. The proposed model is language independent and has no need for manual annotation of the training data. We measured the performance both on synthetic data specifically produced for these two problems and on real social media data. Our model (with 97.06% on synthetic data) improves the state of the art results for diacritization of Turkish by 3.65 percentage points on ambiguous cases and for the vowel restoration by 45.77 percentage points over a rule based baseline with 62.66% accuracy. The results on real data are 95.43% and 69.56% accordingly.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Ahmet Afsin Akın and Mehmet D¨undar Akın.</title>
<date>2007</date>
<journal>Structure.</journal>
<contexts>
<context position="6898" citStr="(2007)" startWordPosition="1132" endWordPosition="1132">orks done for semitic languages. The reason is explained on the former paragraph. an independent classification problem. They are using lexicon lookup which is not feasible for every possible word surface form in agglutinative or highly inflected languages. They refer to a language model in ambiguous cases. They tested their system for 115 languages as well as for Turkish (92.8% on a much easier data set than ours (refer to Section 5.1) . Simard and Deslauriers (2001) tries to recover the missing accents in French. They are using a generative statistical model for this purpose. De Pauw et al. (2007) also test their MBL (memory based learning) model on different languages. Although they do not test for Turkish, the most attractive part of theirs results is that the performances for highly inflectional languages differ sharply from the others towards the negative side. Nguyen and Ock (2010) deals with the diacritization of Vietnamese by using Adaboost and C4.5. The work done so far for the diacritization of Turkish are from T¨ur (2000) (character-based HMM model), Zemberek (2007), Y¨uret and de la Maza (Y¨uret and de la Maza, 2006) (GPA: a kind of decision list algorithms). We give the com</context>
<context position="25990" citStr="(2007)" startWordPosition="4396" endWordPosition="4396">curr 96.26 91.82 ±2ch + sformcurr + first5chcurr 96.28 91.60 Table 4: Diacritization Feature Selection I Features Acc Acc Overall Amb ±2ch + sformcurr 95.29 90.61 +sform0010 95.49 90.72 +sform0011 95.39 90.39 +sform0100 93.77 83.32 +sform0110 95.39 90.28 +sform0111 95.26 89.95 +sform1100 95.24 89.83 +sform1110 95.21 89.50 +sform1111 95.11 89.17 Table 5: Diacritization Feature Selection II ple sform0010 means that the surface form of the token at position +1 is added to the features. This feature set outperformed all the other ones. Acc Acc System Overall Amb Y¨uret (2006) 95.93 91.05 Zemberek (2007) 87.71 82.55 ±2ch + sformcurr 96.15 92.04 ±2ch + sformc„rr + sform0010 97.06 94.70 Table 6: Diacritization Results Comparison with Previous Work Finally, in Table 6, we give the comparison results of our proposed model with the available Turkish deasciifiers (the tools’ original name given by the authors) (Y¨uret and de la Maza, 2006; Akın and Akın, 2007). We both tested by ±2ch + sformcurr and ±2ch + sformcurr + sform0010. Both of the models are tested with maximum possible size of training data: 10379K and 5764K successively. Our proposed model for diacritization outperformed all of the othe</context>
</contexts>
<marker>2007</marker>
<rawString>Ahmet Afsin Akın and Mehmet D¨undar Akın. 2007. Zemberek, an open source nlp framework for turkic languages. Structure.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarah Al-Shareef</author>
<author>Thomas Hain</author>
</authors>
<title>Crf-based diacritisation of colloquial Arabic for automatic speech recognition.</title>
<date>2012</date>
<booktitle>In INTERSPEECH. ISCA.</booktitle>
<contexts>
<context position="5323" citStr="Al-Shareef and Hain, 2012" startWordPosition="865" endWordPosition="868"> Section 6. 2 Related Work The vowelization problem is mostly studied for semitic languages and many different methods are applied to this problem. The problem is generally referred as diacritization for these languages, since diacritics are placed on consonants for the purpose of vowel restoration. For example, the short vowels in Arabic are only pronounced by the use of diacritics put on other consonants. Some of these studies are as follows: Gal (2002) reports the results on Hebrew by using HMMs and Zitouni et al. (2006) on Arabic by using maximum entropy based models. Al-Shareef and Hain (Al-Shareef and Hain, 2012) deals with the vowelization of colloquial Arabic for automatic speech recognition task by using CRFs on speaker and contextual information. Haertel et al. (2010) uses conditional markov models for the vowel restoration problem of Syriac. Nelken and Shieber (2005) uses a finite state transducer approach for Arabic as well. To the best of our knowledge, the vowelization work on Turkish is the first study on a language which do not possess the vowelization problem by its nature. We believe that on that sense, our hybrid model will be a good reference for future studies in social media text norma</context>
</contexts>
<marker>Al-Shareef, Hain, 2012</marker>
<rawString>Sarah Al-Shareef and Thomas Hain. 2012. Crf-based diacritisation of colloquial Arabic for automatic speech recognition. In INTERSPEECH. ISCA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guy De Pauw</author>
<author>Peter W Wagacha</author>
<author>Gilles-Maurice de Schryver</author>
</authors>
<title>Automatic diacritic restoration for resource-scarce languages.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th international conference on Text, speech and dialogue, TSD’07,</booktitle>
<pages>170--179</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>De Pauw, Wagacha, de Schryver, 2007</marker>
<rawString>Guy De Pauw, Peter W. Wagacha, and Gilles-Maurice de Schryver. 2007. Automatic diacritic restoration for resource-scarce languages. In Proceedings of the 10th international conference on Text, speech and dialogue, TSD’07, pages 170–179, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨uls¸en Eryi˘git</author>
</authors>
<title>Fatih Samet C¸etin, Meltem Yanık, Tanel Temel, and ˙Iyas C¸ic¸ekli.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,</booktitle>
<pages>131--134</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Eryi˘git, 2013</marker>
<rawString>G¨uls¸en Eryi˘git, Fatih Samet C¸etin, Meltem Yanık, Tanel Temel, and ˙Iyas C¸ic¸ekli. 2013. Turksent: A sentiment annotation tool for social media. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 131–134, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨uls¸en Eryi˘git</author>
</authors>
<title>ITU Turkish NLP web service.</title>
<date>2014</date>
<booktitle>In Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Gothenburg, Sweden,</location>
<marker>Eryi˘git, 2014</marker>
<rawString>G¨uls¸en Eryi˘git. 2014. ITU Turkish NLP web service. In Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics (EACL), Gothenburg, Sweden, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ya’akov Gal</author>
</authors>
<title>An hmm approach to vowel restoration in Arabic and Hebrew.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages, SEMITIC ’02,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2187" citStr="Gal, 2002" startWordPosition="351" endWordPosition="352">n this new platform. As we all know, Twitter announced (at April 1st, 2013)1 that it is shifting to a two-tiered service where the basic free service ‘Twttr” will only allow to use consonants in the tweets. Although, 1https://blog.twitter.com/2013/ annncng-twttr this is a very funny joke, people nowadays are already very used to use this style of writing without vowels in order to fit their messages into 140 characters Twitter or 160 characters SMS messages. As a result, the vowelization problem (Twttr ⇒ Twitter) is no more limited with some specific language families (e.g.semitic languages) (Gal, 2002; Zitouni et al., 2006) but it became a problem of social media text normalization in general. Diacritics are some marks (e.g. accents, dots, curves) added to the characters and have a wide usage in many languages. The absence of these marks in Web2.0 language is very common and posses a big problem in the automatic processing of this data by NLP tools. Although, in the literature, the term “diacritization” is used both for vowel and diacritic restoration for semitic languages, in this paper, we use this term only for the task of converting an ASCII text to its proper form (with accents and sp</context>
<context position="5156" citStr="Gal (2002)" startWordPosition="838" endWordPosition="839">agglutinative language; Turkish. Section 4 introduces our proposed model and Section 5 presents our experiments and results. The conclusion is given in Section 6. 2 Related Work The vowelization problem is mostly studied for semitic languages and many different methods are applied to this problem. The problem is generally referred as diacritization for these languages, since diacritics are placed on consonants for the purpose of vowel restoration. For example, the short vowels in Arabic are only pronounced by the use of diacritics put on other consonants. Some of these studies are as follows: Gal (2002) reports the results on Hebrew by using HMMs and Zitouni et al. (2006) on Arabic by using maximum entropy based models. Al-Shareef and Hain (Al-Shareef and Hain, 2012) deals with the vowelization of colloquial Arabic for automatic speech recognition task by using CRFs on speaker and contextual information. Haertel et al. (2010) uses conditional markov models for the vowel restoration problem of Syriac. Nelken and Shieber (2005) uses a finite state transducer approach for Arabic as well. To the best of our knowledge, the vowelization work on Turkish is the first study on a language which do not</context>
</contexts>
<marker>Gal, 2002</marker>
<rawString>Ya’akov Gal. 2002. An hmm approach to vowel restoration in Arabic and Hebrew. In Proceedings of the ACL-02 workshop on Computational approaches to semitic languages, SEMITIC ’02, pages 1–7, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbie A Haertel</author>
<author>Peter McClanahan</author>
<author>Eric K Ringger</author>
</authors>
<title>Automatic diacritization for lowresource languages using a hybrid word and consonant cmm.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10,</booktitle>
<pages>519--527</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5485" citStr="Haertel et al. (2010)" startWordPosition="890" endWordPosition="893">rally referred as diacritization for these languages, since diacritics are placed on consonants for the purpose of vowel restoration. For example, the short vowels in Arabic are only pronounced by the use of diacritics put on other consonants. Some of these studies are as follows: Gal (2002) reports the results on Hebrew by using HMMs and Zitouni et al. (2006) on Arabic by using maximum entropy based models. Al-Shareef and Hain (Al-Shareef and Hain, 2012) deals with the vowelization of colloquial Arabic for automatic speech recognition task by using CRFs on speaker and contextual information. Haertel et al. (2010) uses conditional markov models for the vowel restoration problem of Syriac. Nelken and Shieber (2005) uses a finite state transducer approach for Arabic as well. To the best of our knowledge, the vowelization work on Turkish is the first study on a language which do not possess the vowelization problem by its nature. We believe that on that sense, our hybrid model will be a good reference for future studies in social media text normalization where the problem is disregarded in recent studies. The diacritization task on the other hand is not addressed as frequently as the vowelization problem2</context>
</contexts>
<marker>Haertel, McClanahan, Ringger, 2010</marker>
<rawString>Robbie A. Haertel, Peter McClanahan, and Eric K. Ringger. 2010. Automatic diacritization for lowresource languages using a hybrid word and consonant cmm. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT ’10, pages 519–527, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="12718" citStr="Lafferty et al., 2001" startWordPosition="2090" endWordPosition="2093"> to generation, in order to increase the likelihood of the generated output word, most of the approaches include character level probabilities or relationships. In this case, it is unfair to expect from a machine learning system to generate morphologically valid outputs (especially for highly inflectional languages) while trying to maximize the overall character sequence probability. We propose a two stage model (Figure 2) which has basically two components. 1. a discriminative sequence classifier 2. a language validator 4.1 Discriminative Sequence Classifier In the first stage, we use CRFs6 (Lafferty et al., 2001) in order to produce the most probable output words. This stage treats the diacritization and vowelization as character level sequence labeling tasks, but since it is a discriminative model, it is also possible to provide neighboring words as features into the system. During training, each instance within a sequence has basically the following main parts: 1. features related to the current and neighboring tokens (namely surface form or lemma) 2. features related to the current and neighboring characters$ 3. class label The test data is also prepared similarly except the gold standard class lab</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rani Nelken</author>
<author>Stuart M Shieber</author>
</authors>
<title>Arabic diacritization using weighted finite-state transducers.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, Semitic ’05,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5587" citStr="Nelken and Shieber (2005)" startWordPosition="905" endWordPosition="908">r the purpose of vowel restoration. For example, the short vowels in Arabic are only pronounced by the use of diacritics put on other consonants. Some of these studies are as follows: Gal (2002) reports the results on Hebrew by using HMMs and Zitouni et al. (2006) on Arabic by using maximum entropy based models. Al-Shareef and Hain (Al-Shareef and Hain, 2012) deals with the vowelization of colloquial Arabic for automatic speech recognition task by using CRFs on speaker and contextual information. Haertel et al. (2010) uses conditional markov models for the vowel restoration problem of Syriac. Nelken and Shieber (2005) uses a finite state transducer approach for Arabic as well. To the best of our knowledge, the vowelization work on Turkish is the first study on a language which do not possess the vowelization problem by its nature. We believe that on that sense, our hybrid model will be a good reference for future studies in social media text normalization where the problem is disregarded in recent studies. The diacritization task on the other hand is not addressed as frequently as the vowelization problem2. Some studies are as follows: Scannell (2011) uses a Naive Bayes classifier for both word-level and c</context>
</contexts>
<marker>Nelken, Shieber, 2005</marker>
<rawString>Rani Nelken and Stuart M. Shieber. 2005. Arabic diacritization using weighted finite-state transducers. In Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, Semitic ’05, pages 79–86, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kiem-Hieu Nguyen</author>
<author>Cheol-Young Ock</author>
</authors>
<title>Diacritics restoration in Vietnamese: letter based vs. syllable based model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Pacific Rim international conference on Trends in artificial intelligence, PRICAI’10,</booktitle>
<pages>631--636</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<contexts>
<context position="7193" citStr="Nguyen and Ock (2010)" startWordPosition="1177" endWordPosition="1180">ge model in ambiguous cases. They tested their system for 115 languages as well as for Turkish (92.8% on a much easier data set than ours (refer to Section 5.1) . Simard and Deslauriers (2001) tries to recover the missing accents in French. They are using a generative statistical model for this purpose. De Pauw et al. (2007) also test their MBL (memory based learning) model on different languages. Although they do not test for Turkish, the most attractive part of theirs results is that the performances for highly inflectional languages differ sharply from the others towards the negative side. Nguyen and Ock (2010) deals with the diacritization of Vietnamese by using Adaboost and C4.5. The work done so far for the diacritization of Turkish are from T¨ur (2000) (character-based HMM model), Zemberek (2007), Y¨uret and de la Maza (Y¨uret and de la Maza, 2006) (GPA: a kind of decision list algorithms). We give the comparison of the two later systems on our data set and propose a discriminative equivalent of the HMM approach used in T¨ur (2000) (see Section 5 for further discussions). For the vowelization, the only study that we could find is from T¨ur (2000) which uses again the same character-level HMM mod</context>
</contexts>
<marker>Nguyen, Ock, 2010</marker>
<rawString>Kiem-Hieu Nguyen and Cheol-Young Ock. 2010. Diacritics restoration in Vietnamese: letter based vs. syllable based model. In Proceedings of the 11th Pacific Rim international conference on Trends in artificial intelligence, PRICAI’10, pages 631–636, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhammet S¸ahin</author>
<author>Umut Sulubacak</author>
<author>G¨uls¸en Eryi˘git</author>
</authors>
<title>Redefinition of Turkish morphology using flag diacritics.</title>
<date>2013</date>
<booktitle>In Proceedings of The Tenth Symposium on Natural Language Processing (SNLP-2013),</booktitle>
<location>Phuket, Thailand,</location>
<marker>S¸ahin, Sulubacak, Eryi˘git, 2013</marker>
<rawString>Muhammet S¸ahin, Umut Sulubacak, and G¨uls¸en Eryi˘git. 2013. Redefinition of Turkish morphology using flag diacritics. In Proceedings of The Tenth Symposium on Natural Language Processing (SNLP-2013), Phuket, Thailand, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Has¸im Sak</author>
<author>Tunga G¨ung¨or</author>
<author>Murat Sarac¸lar</author>
</authors>
<title>Resources for Turkish morphological processing.</title>
<date>2011</date>
<journal>Lang. Resour. Eval.,</journal>
<volume>45</volume>
<issue>2</issue>
<marker>Sak, G¨ung¨or, Sarac¸lar, 2011</marker>
<rawString>Has¸im Sak, Tunga G¨ung¨or, and Murat Sarac¸lar. 2011. Resources for Turkish morphological processing. Lang. Resour. Eval., 45(2):249–261, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bilge Say</author>
<author>Deniz Zeyrek</author>
<author>Kemal Oflazer</author>
<author>Umut ¨Ozge</author>
</authors>
<title>Development of a corpus and a treebank for present-day written Turkish.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eleventh International Conference of Turkish Linguistics,</booktitle>
<location>Famaguste, Cyprus,</location>
<marker>Say, Zeyrek, Oflazer, ¨Ozge, 2002</marker>
<rawString>Bilge Say, Deniz Zeyrek, Kemal Oflazer, and Umut ¨Ozge. 2002. Development of a corpus and a treebank for present-day written Turkish. In Proceedings of the Eleventh International Conference of Turkish Linguistics, Famaguste, Cyprus, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin P Scannell</author>
</authors>
<title>Statistical unicodification of African languages.</title>
<date>2011</date>
<journal>Lang. Resour. Eval.,</journal>
<volume>45</volume>
<issue>3</issue>
<pages>386</pages>
<contexts>
<context position="3089" citStr="Scannell, 2011" startWordPosition="506" endWordPosition="508">a big problem in the automatic processing of this data by NLP tools. Although, in the literature, the term “diacritization” is used both for vowel and diacritic restoration for semitic languages, in this paper, we use this term only for the task of converting an ASCII text to its proper form (with accents and special characters). A Turkish example is the word “dondu” (it is frozen) which may be the ascii form of both “dondu”(it is frozen) or “d¨ond¨u” (it returned) where the ambiguity should be resolved according to the context. In some studies, this task is also referred as “unicodification”(Scannell, 2011) or “deasciification”(T¨ur, 2000). In this paper, we focus on these two important problems of social text normalization, namely: diacritization and vowelization. These two problems compose almost the quarter (26.5%) of the normalization errors within a 25K Turkish Tweeter Data Set. We propose a two stage hybrid model: firstly a discriminative model as a sequence classification task by using CRFs (Conditional Random Fields) and secondly a language validator over the first stage’s results. Although in this paper, we presented our results on Turkish (which is a highly agglutinative language with </context>
<context position="6131" citStr="Scannell (2011)" startWordPosition="1000" endWordPosition="1002"> for the vowel restoration problem of Syriac. Nelken and Shieber (2005) uses a finite state transducer approach for Arabic as well. To the best of our knowledge, the vowelization work on Turkish is the first study on a language which do not possess the vowelization problem by its nature. We believe that on that sense, our hybrid model will be a good reference for future studies in social media text normalization where the problem is disregarded in recent studies. The diacritization task on the other hand is not addressed as frequently as the vowelization problem2. Some studies are as follows: Scannell (2011) uses a Naive Bayes classifier for both word-level and character-level modeling. Each ambiguous character in the input is regarded as 2Here, we exclude all the works done for semitic languages. The reason is explained on the former paragraph. an independent classification problem. They are using lexicon lookup which is not feasible for every possible word surface form in agglutinative or highly inflected languages. They refer to a language model in ambiguous cases. They tested their system for 115 languages as well as for Turkish (92.8% on a much easier data set than ours (refer to Section 5.1</context>
</contexts>
<marker>Scannell, 2011</marker>
<rawString>Kevin P. Scannell. 2011. Statistical unicodification of African languages. Lang. Resour. Eval., 45(3):375– 386, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Simard</author>
<author>Alexandre Deslauriers</author>
</authors>
<title>Real-time automatic insertion of accents in French text.</title>
<date>2001</date>
<journal>Nat. Lang. Eng.,</journal>
<volume>7</volume>
<issue>2</issue>
<contexts>
<context position="6764" citStr="Simard and Deslauriers (2001)" startWordPosition="1106" endWordPosition="1109">s a Naive Bayes classifier for both word-level and character-level modeling. Each ambiguous character in the input is regarded as 2Here, we exclude all the works done for semitic languages. The reason is explained on the former paragraph. an independent classification problem. They are using lexicon lookup which is not feasible for every possible word surface form in agglutinative or highly inflected languages. They refer to a language model in ambiguous cases. They tested their system for 115 languages as well as for Turkish (92.8% on a much easier data set than ours (refer to Section 5.1) . Simard and Deslauriers (2001) tries to recover the missing accents in French. They are using a generative statistical model for this purpose. De Pauw et al. (2007) also test their MBL (memory based learning) model on different languages. Although they do not test for Turkish, the most attractive part of theirs results is that the performances for highly inflectional languages differ sharply from the others towards the negative side. Nguyen and Ock (2010) deals with the diacritization of Vietnamese by using Adaboost and C4.5. The work done so far for the diacritization of Turkish are from T¨ur (2000) (character-based HMM m</context>
</contexts>
<marker>Simard, Deslauriers, 2001</marker>
<rawString>Michel Simard and Alexandre Deslauriers. 2001. Real-time automatic insertion of accents in French text. Nat. Lang. Eng., 7(2):143–165, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨okhan T¨ur</author>
</authors>
<title>A statistical information extraction system for Turkish.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Engineering and the Institute of Engineering and Science of Bilkent University,</institution>
<location>Ankara.</location>
<marker>T¨ur, 2000</marker>
<rawString>G¨okhan T¨ur. 2000. A statistical information extraction system for Turkish. Ph.D. thesis, Department of Computer Engineering and the Institute of Engineering and Science of Bilkent University, Ankara.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eray Yıldız</author>
<author>C¨uneyd Tantu˘g</author>
</authors>
<title>Evaluation of sentence alignment methods for English-Turkish parallel texts.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Workshop on Language Resources and Technologies for Turkic Languages (LREC),</booktitle>
<location>Istanbul,</location>
<marker>Yıldız, Tantu˘g, 2012</marker>
<rawString>Eray Yıldız and C¨uneyd Tantu˘g. 2012. Evaluation of sentence alignment methods for English-Turkish parallel texts. In Proceedings of the First Workshop on Language Resources and Technologies for Turkic Languages (LREC), Istanbul, Turkey, 23-25 May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Y¨uret</author>
<author>Michael de la Maza</author>
</authors>
<title>The greedy prepend algorithm for decision list induction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st international conference on Computer and Information Sciences, ISCIS’06,</booktitle>
<pages>37--46</pages>
<publisher>Springer-Verlag.</publisher>
<location>Berlin, Heidelberg.</location>
<marker>Y¨uret, Maza, 2006</marker>
<rawString>Deniz Y¨uret and Michael de la Maza. 2006. The greedy prepend algorithm for decision list induction. In Proceedings of the 21st international conference on Computer and Information Sciences, ISCIS’06, pages 37–46, Berlin, Heidelberg. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Imed Zitouni</author>
<author>Jeffrey S Sorensen</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Maximum entropy based restoration of Arabic diacritics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>577--584</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2210" citStr="Zitouni et al., 2006" startWordPosition="353" endWordPosition="356">platform. As we all know, Twitter announced (at April 1st, 2013)1 that it is shifting to a two-tiered service where the basic free service ‘Twttr” will only allow to use consonants in the tweets. Although, 1https://blog.twitter.com/2013/ annncng-twttr this is a very funny joke, people nowadays are already very used to use this style of writing without vowels in order to fit their messages into 140 characters Twitter or 160 characters SMS messages. As a result, the vowelization problem (Twttr ⇒ Twitter) is no more limited with some specific language families (e.g.semitic languages) (Gal, 2002; Zitouni et al., 2006) but it became a problem of social media text normalization in general. Diacritics are some marks (e.g. accents, dots, curves) added to the characters and have a wide usage in many languages. The absence of these marks in Web2.0 language is very common and posses a big problem in the automatic processing of this data by NLP tools. Although, in the literature, the term “diacritization” is used both for vowel and diacritic restoration for semitic languages, in this paper, we use this term only for the task of converting an ASCII text to its proper form (with accents and special characters). A Tu</context>
<context position="5226" citStr="Zitouni et al. (2006)" startWordPosition="850" endWordPosition="853">roposed model and Section 5 presents our experiments and results. The conclusion is given in Section 6. 2 Related Work The vowelization problem is mostly studied for semitic languages and many different methods are applied to this problem. The problem is generally referred as diacritization for these languages, since diacritics are placed on consonants for the purpose of vowel restoration. For example, the short vowels in Arabic are only pronounced by the use of diacritics put on other consonants. Some of these studies are as follows: Gal (2002) reports the results on Hebrew by using HMMs and Zitouni et al. (2006) on Arabic by using maximum entropy based models. Al-Shareef and Hain (Al-Shareef and Hain, 2012) deals with the vowelization of colloquial Arabic for automatic speech recognition task by using CRFs on speaker and contextual information. Haertel et al. (2010) uses conditional markov models for the vowel restoration problem of Syriac. Nelken and Shieber (2005) uses a finite state transducer approach for Arabic as well. To the best of our knowledge, the vowelization work on Turkish is the first study on a language which do not possess the vowelization problem by its nature. We believe that on th</context>
</contexts>
<marker>Zitouni, Sorensen, Sarikaya, 2006</marker>
<rawString>Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya. 2006. Maximum entropy based restoration of Arabic diacritics. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 577– 584, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
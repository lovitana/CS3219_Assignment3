<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000171">
<title confidence="0.978413">
Tolerant BLEU: a Submission to the WMT14 Metrics Task
</title>
<author confidence="0.981066">
Jindˇrich Libovick´y and Pavel Pecina
</author>
<affiliation confidence="0.9637565">
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal ad Applied Linguistics
</affiliation>
<email confidence="0.982423">
{libovicky, pecina}@ufal.mff.cuni.cz
</email>
<sectionHeader confidence="0.99713" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995981">
This paper describes a machine translation
metric submitted to the WMT14 Metrics
Task. It is a simple modification of the
standard BLEU metric using a monolin-
gual alignment of reference and test sen-
tences. The alignment is computed as
a minimum weighted maximum bipartite
matching of the translated and the refer-
ence sentence words with respect to the
relative edit distance of the word prefixes
and suffixes. The aligned words are in-
cluded in the n-gram precision compu-
tation with a penalty proportional to the
matching distance. The proposed tBLEU
metric is designed to be more tolerant to
errors in inflection, which usually does not
effect the understandability of a sentence,
and therefore be more suitable for measur-
ing quality of translation into morphologi-
cally richer languages.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999934696969697">
Automatic evaluation of machine translation (MT)
quality is an important part of the machine trans-
lation pipeline. The possibility to run an evalua-
tion algorithm many times while training a system
enables the system to be optimized with respect to
such a metric (e.g., by Minimum Error Rate Train-
ing (Och, 2003)). By achieving a high correlation
of the metric with human judgment, we expect the
system performance to be optimized also with re-
spect to the human perception of translation qual-
ity.
In this paper, we propose an MT metric called
tBLEU (tolerant BLEU) that is based on the stan-
dard BLEU (Papineni et al., 2002) and designed to
suit better when translation into morphologically
richer languages. We aim to have a simple lan-
guage independent metric that correlates with hu-
man judgment better than the standard BLEU.
Several metrics try to address this problem
as well and usually succeed to gain a higher
correlation with human judgment (e.g. ME-
TEOR (Denkowski and Lavie, 2011), TerrorCat
(Fishel et al., 2012)). However, they usually
use some language-dependent tools and resources
(METEOR uses stemmer and parahprasing tables,
TerrorCat uses lemmatization and needs training
data for each language pair) which prevent them
from being widely adopted.
In the next section, the previous work is briefly
summarized. Section 3 describes the metric in de-
tail. The experiments with the metric are described
in Section 4 and their results are summarized in
Section 5.
</bodyText>
<sectionHeader confidence="0.996129" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.898186875">
BLEU (Papineni et al., 2002) is an established and
the most widely used automatic metric for evalua-
tion of MT quality. It is computed as a harmonic
mean of the n-gram precisions multiplied by the
brevity penalty coefficient which ensures also high
recall. Formally:
)log pn ,
where BP is the brevity penaly defined as follows:
</bodyText>
<equation confidence="0.9907805">
� 1
BP = e1−
</equation>
<bodyText confidence="0.9960268">
c is the length of the test sentence (number of to-
kens), r is the length of the reference sentence, and
pn is the proportion of n-grams from the test sen-
tence found in the reference translations.
The original experiments with the English to
Chinese translation (Papineni et al., 2002) re-
ported very high correlation of BLEU with human
judgments. However, these scores were computed
using multiple reference translations (to capture
translation variability) but in practice, only one
</bodyText>
<equation confidence="0.956376875">
BLEU = BP · exp
4
1
4
�
n=1
ifc&gt;r
otherwise ,
</equation>
<page confidence="0.971496">
409
</page>
<affiliation confidence="0.3334725">
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 409–413,
Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics
</affiliation>
<figure confidence="0.993192704545455">
I am driving a new red car
Jedu novým cerveným autem
1
Jedu s novém cerveném auto.
0 2
3 6 3
1
Source:
Reference:
Translation:
Corrected and (Jedu, 1) (s, 1) (novým, 2/3) (cerveným, 5/6) (autem, 1/3)
wighted translation:
Unigram precision Bigram precision
Jedu
s
novém
cerveném
auto
Jedu 1
s 1
novým 2/3
cerveným 5/6
autem 1/3
Jedu s
s novém
novém cerveném
cerveném auto
Jedu s
s novým
novým cerveným
cerveným autem
avg(1,1) =
avg(1, 2/3) =
avg(2/3, 5/6) =
avg(5/6,1/3) =
1
5/6
3/4
7/12
tBLEU unigram precision = 11 5 - 0.367 16 4 - 0.333
BLEU unigram precision = 1 0.2 0
6 tBLEU bigram precision =
/ 5 = 12
BLEU bigram precision = 0 / 4 =
</figure>
<figureCaption confidence="0.675458666666667">
Figure 1: An example of the unigram and bigram precision computation for translation from English to
Czech with the test sentence having minor inflection errors and an additional preposition. The first two
lines contain the source sentence in English and a correct reference translation in Czech. On the third
line, there is an incorrectly translated sentence with errors in inflection. Between the second and the
third line, the matching with respect to the affix distance is shown. The fourth line contains the corrected
test sentence with the words weights. The bottom part of the figure shows computation of the unigram
and bigram precisions. The first column contains the original translation n-grams, the second one the
corrected n-grams, the third one the n-gram weights and the last one indicates whether a matching n-
gram is contained in the reference sentence.
</figureCaption>
<bodyText confidence="0.997771217391304">
reference translation is usually available and there-
fore the BLEU scores are often underestimated.
The main disadvantage of BLEU is the fact that
it treats words as atomic units and does not allow
any partial matches. Therefore, words which are
inflectional variants of each other are treated as
completely different words although their mean-
ing is similar (e.g. work, works, worked, working).
Further, the n-gram precision for n &gt; 1 penalizes
difference in word order between the reference and
the test sentences even though in languages with
free word order both sentences can be correct (Bo-
jar et al., 2010; Condon et al., 2009).
There are also other widely recognized MT
evaluation metrics: The NIST score (Dodding-
ton, 2002) is also an n-gram based metric, but
in addition it reflects how informative particular
n-grams are. A metric that achieves a very high
correlation with human judgment is METEOR
(Denkowski and Lavie, 2011). It creates a mono-
lingual alignment using language dependent tools
as stemmers and synonyms dictionaries and com-
putes weighted harmonic mean of precision and
recall based on the matching.
Some metrics are based on measuring the
edit distance between the reference and test sen-
tences. The Position-Independent Error Rate
(PER) (Leusch et al., 2003) is computed as
a length-normalized edit distance of sentences
treated as bags of words. The Translation Edit
Rate (TER) (Snover et al., 2006) is a number of
edit operation needed to change the test sentence
to the most similar reference sentence. In this
case, the allowed editing operations are insertions,
deletions and substitutions and also shifting words
within a sentence.
A different approach is used in TerrorCat
(Fishel et al., 2012). It uses frequencies of auto-
matically obtained translation error categories as
base for machine-learned pairwise comparison of
translation hypotheses.
In the Workshop of Machine Translation
(WMT) Metrics Task, several new MT metrics
compete annually (Mach´aˇcek and Bojar, 2013). In
the comptetition, METEOR and TerrorCat scored
better that the other mentioned metrics.
</bodyText>
<page confidence="0.99588">
410
</page>
<sectionHeader confidence="0.998151" genericHeader="method">
3 Metric Description
</sectionHeader>
<bodyText confidence="0.999918588235294">
tBLEU is computed in in two steps. Similarly to
the METEOR score, we first make a monolingual
alignment between the reference and the test sen-
tences and then apply an algorithm similar to the
standard BLEU but with modified n-gram preci-
sions.
The monolingual alignment is computed as a
minimum weighted maximum bipartite matching
between words in a reference sentence and a trans-
lation sentence1 using the Munkres assignment al-
gorithm (Munkres, 1957).
We define a weight of an alignment link as the
affix distance of the test sentence word wti and the
reference sentence word wrj: Let S be the longest
common substring of wti and wri . We can rewrite
the strings as a concatenation of a prefix, the com-
mon substring and a suffix:
</bodyText>
<equation confidence="0.9289764">
wt = wti,pSwti,s
wr = wrj,pSwrj,s
Further, we define the affix distance as:
� AD(wr wt) = max 1, L(wrj,p,wti,p)+L(wrs,j ,wts,i)1
|S |J
</equation>
<bodyText confidence="0.986837380952381">
if JSJ &gt; 0 and AD(wr, wt) = 1 otherwise. L is the
Levensthein distance between two strings.
For example the affix distance of two Czech
words vzpomenou and zapomenout (different
forms of verbs remember and forget) is computed
in the following way: The longest common sub-
string is pomenou which has a length of 7. The
prefixes are vz and za and their edit distance is 2.
The suffixes are an empty string and t which with
the edit distance 1. The total edit distance of pre-
fixes and suffixes is 3. By dividing the total edit
distance by the length of the longest common sub-
string, we get the affix distance 37 Pz� 0.43.
We denote the resulting set of matching pairs
of words as M = {(wri, wti)Im i=1 and for each test
sentence St = (wt1, ..., wtm) we create a corrected
sentence ˆSt = ( ˆwt1, ..., ˆwtm) such that
t _ r wr if ]wt: (w; wt) E M &amp; AD wt) &lt;_ E
wi Sl wti otherwise.
This means that the words from the test sen-
tence which were matched with the affix distance
</bodyText>
<footnote confidence="0.83991575">
1The matching is always one-to-one which means that
some words remain unmatched if the sentences have differ-
ent number of words.
Affix distance threshold
</footnote>
<figureCaption confidence="0.81688475">
Figure 2: Dependence of the Pearson’s correlation
of tBLEU with the WMT13 human judgments on
the affix distance threshold for translations from
English and to English.
</figureCaption>
<bodyText confidence="0.993701375">
smaller than c are “corrected” by substituting them
by the matching words from the reference sen-
tence. The threshold c is a free parameter of the
metric. When the threshold is set to zero, no
corrections are made and therefore the metric is
equivalent to the standard BLEU.
The words in the corrected sentence are as-
signed the weights as follows:
</bodyText>
<equation confidence="0.8179174">
v _ r 1 − AD( ˆwti, wti) if ˆwti � wt
i S 1
matching n-grams
...
and
...
( ˆwt1,
ˆwtn)
(wr1,
wrn)
</equation>
<bodyText confidence="0.835987">
bute to the n-gram precision with a score of
</bodyText>
<equation confidence="0.5988855">
s(wt t) =
1,...,wn
</equation>
<bodyText confidence="0.974126642857143">
In other words, the weights penalize the corrected
words proportionally to the affix distance from the
original words.
While computing the n-gram precision, two
contri
instead of one as it is in the standard BLEU. The
rest of the BLEU score computation remains un-
changed. While using multiple reference transla-
tion, the matching is done for each of the refer-
ence sentence, and while computing the n-gram
precision, the reference sentences with the highest
weight is chosen. The computation of the n-gram
precision is ill
ustrated in Figure 1.
</bodyText>
<figure confidence="0.991618433333333">
0 0.2 0.4 0.6 0.8 1
en-cs
en-de
en-es
en-fr
0 0.2 0.4 0.6 0.8 1
cs-en
de-en
es-en
fr-en
Pearson&apos;s correlation coeffitient
0.95
0.9
0.85
0.8
0.75
0.7
Affix distance threshold
Pearson&apos;s correlation coefficient
0.97
0.96
0.95
0.94
0.93
0.92
0.91
otherwise.
n
v( ˆwti) / n
i=1
</figure>
<page confidence="0.994552">
411
</page>
<table confidence="0.996052666666667">
direction BLEU METEOR tBLEU
.781 .860 .787
.835 .868 .850
.875 .878 .884
.887 .906 .906
.844 .878 .857
</table>
<tableCaption confidence="0.979764">
Table 1: System level Pearson’s correlation with
</tableCaption>
<table confidence="0.889301625">
the human judgment for systems translating from
English computed on the WMT13 dataset.
direction BLEU METEOR tBLEU
.925 .985 .927
.916 .962 .917
.957 .968 .953
.940 .983 .933
.923 .974 .935
</table>
<tableCaption confidence="0.991262">
Table 2: System level Pearson’s correlation with
</tableCaption>
<bodyText confidence="0.959968333333333">
the human judgment for systems translating to En-
glish computed on the WMT13 dataset.
en-cs
en-de
en-es
en-fr
from English
cs-en
de-en
es-en
fr-en
to English
</bodyText>
<sectionHeader confidence="0.995583" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999903642857143">
We evaluated the proposed metric on the dataset
used for the WMT13 Metrics Task (Mach´aˇcek and
Bojar, 2013). The dataset consists of 135 systems’
outputs in 10 directions (5 into English 5 out of
English). Each system’s output and the reference
translation contain 3000 sentences. According to
the WMT14 guidelines, we report the the Pear-
son’s correlation coefficient instead of the Spear-
man’s coefficient that was used in the last years.
Twenty values of the affix distance threshold
were tested in order to estimate what is the most
suitable threshold setting. We report only the sys-
tem level correlation because the metric is de-
signed to compare only the whole system outputs.
</bodyText>
<sectionHeader confidence="0.999894" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999961852941176">
The tBLEU metric generally improves the cor-
relation with human judgment over the standard
BLEU metric for directions from English to lan-
guages with richer inflection.
Examining the various threshold values showed
that dependence between the affix distance thresh-
old and the correlation with the human judgment
varies for different language pairs (Figure 2). For
translation from English to morphologically richer
languages than English – Czech, German, Spanish
and French – using the tBLEU metric increased
the correlation over the standard BLEU. For Czech
the correlation quickly decreases for threshold val-
ues bigger than 0.1, whereas for the other lan-
guages it still grows. We hypothesize this because
the big morphological changes in Czech can en-
tirely change the meaning.
For translation to English, the correlation
slightly increases with the increasing threshold
value for translation from French and Spanish, but
decreases for Czech and German.
There are different optimal affix distance
thresholds for different language pairs. However,
the threshold of 0.05 was used for our WMT14
submission because it had the best average cor-
relation on the WMT13 data set. Tables 1 and
2 show the results of the tBLEU for the particu-
lar language pairs for threshold 0.05. While com-
pared to the BLEU score, the correlation is slightly
higher for translation from English and approxi-
mately the same for translation to English.
The results on the WMT14 dataset did not show
any improvement over the BLEU metric. The rea-
son of the results will be further examined.
</bodyText>
<sectionHeader confidence="0.998119" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9998967">
We presented tBLEU, a language-independent MT
metric based on the standard BLEU metric. It in-
troduced the affix distance – relative edit distances
of prefixes and suffixes of two string after remov-
ing their longest common substring. Finding a
matching between translation and reference sen-
tences with respect to this matching allows a pe-
nalized substitution of words which has been most
likely wrongly inflected and therefore less penal-
izes errors in inflection.
This metric achieves a higher correlation with
the human judgment than the standard BLEU
score for translation to morphological richer lan-
guages without the necessity to employ any lan-
guage specific tools.
In future work, we would like to improve word
alignment between test and reference translations
by introducing word position and potentially other
features, and implement tBLEU in MERT to ex-
amine its impact on system tuning.
</bodyText>
<sectionHeader confidence="0.996912" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<footnote confidence="0.649223">
This research has been funded by the Czech Sci-
ence Foundation (grant n. P103/12/G084) and the
EU FP7 project Khresmoi (contract no. 257528).
</footnote>
<page confidence="0.996404">
412
</page>
<sectionHeader confidence="0.995954" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997946383333334">
Ond&amp;quot;rej Bojar, Kamil Kos, and David Mare&amp;quot;cek. 2010.
Tackling sparse data issue in machine translation
evaluation. In Proceedings of the ACL 2010 Con-
ference Short Papers, pages 86–91. Association for
Computational Linguistics.
Sherri Condon, Gregory A Sanders, Dan Parvaz, Alan
Rubenstein, Christy Doran, John Aberdeen, and
Beatrice Oshika. 2009. Normalization for auto-
mated metrics: English and arabic speech transla-
tion. Proceedings of MT Summit XII. Association
for Machine Translation in the Americas, Ottawa,
ON, Canada.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, WMT ’11, pages 85–91, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT ’02, pages 138–145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Mark Fishel, Rico Sennrich, Maja Popovi´c, and Ond&amp;quot;rej
Bojar. 2012. Terrorcat: a translation error
categorization-based mt quality metric. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 64–70. Association for Compu-
tational Linguistics.
Gregor Leusch, Nicola Ueffing, Hermann Ney, et al.
2003. A novel string-to-string distance measure
with applications to machine translation evaluation.
In Proceedings of MT Summit IX, pages 240–247.
Citeseer.
Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results of
the WMT13 metrics shared task. In Proceedings of
the Eighth Workshop on Statistical Machine Trans-
lation, pages 45–51, Sofia, Bulgaria, August. Asso-
ciation for Computational Linguistics.
James Munkres. 1957. Algorithms for the assignment
and transportation problems. Journal of the Society
for Industrial &amp; Applied Mathematics, 5(1):32–38.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160–167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th annual meeting on association for compu-
tational linguistics, pages 311–318. Association for
Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223–231.
</reference>
<page confidence="0.998973">
413
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.337380">
<title confidence="0.872301">Tolerant BLEU: a Submission to the WMT14 Metrics Task</title>
<author confidence="0.513007">Libovick´y</author>
<affiliation confidence="0.692309">Charles University in Prague, Faculty of Mathematics and Institute of Formal ad Applied</affiliation>
<abstract confidence="0.998458095238095">This paper describes a machine translation metric submitted to the WMT14 Metrics Task. It is a simple modification of the standard BLEU metric using a monolingual alignment of reference and test sentences. The alignment is computed as a minimum weighted maximum bipartite matching of the translated and the reference sentence words with respect to the relative edit distance of the word prefixes and suffixes. The aligned words are included in the n-gram precision computation with a penalty proportional to the matching distance. The proposed tBLEU metric is designed to be more tolerant to errors in inflection, which usually does not effect the understandability of a sentence, and therefore be more suitable for measuring quality of translation into morphologically richer languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Kamil Kos</author>
<author>David Marecek</author>
</authors>
<title>Tackling sparse data issue in machine translation evaluation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers,</booktitle>
<pages>86--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5706" citStr="Bojar et al., 2010" startWordPosition="938" endWordPosition="942">entence. reference translation is usually available and therefore the BLEU scores are often underestimated. The main disadvantage of BLEU is the fact that it treats words as atomic units and does not allow any partial matches. Therefore, words which are inflectional variants of each other are treated as completely different words although their meaning is similar (e.g. work, works, worked, working). Further, the n-gram precision for n &gt; 1 penalizes difference in word order between the reference and the test sentences even though in languages with free word order both sentences can be correct (Bojar et al., 2010; Condon et al., 2009). There are also other widely recognized MT evaluation metrics: The NIST score (Doddington, 2002) is also an n-gram based metric, but in addition it reflects how informative particular n-grams are. A metric that achieves a very high correlation with human judgment is METEOR (Denkowski and Lavie, 2011). It creates a monolingual alignment using language dependent tools as stemmers and synonyms dictionaries and computes weighted harmonic mean of precision and recall based on the matching. Some metrics are based on measuring the edit distance between the reference and test se</context>
</contexts>
<marker>Bojar, Kos, Marecek, 2010</marker>
<rawString>Ond&amp;quot;rej Bojar, Kamil Kos, and David Mare&amp;quot;cek. 2010. Tackling sparse data issue in machine translation evaluation. In Proceedings of the ACL 2010 Conference Short Papers, pages 86–91. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sherri Condon</author>
<author>Gregory A Sanders</author>
<author>Dan Parvaz</author>
<author>Alan Rubenstein</author>
<author>Christy Doran</author>
<author>John Aberdeen</author>
<author>Beatrice Oshika</author>
</authors>
<title>Normalization for automated metrics: English and arabic speech translation.</title>
<date>2009</date>
<booktitle>Proceedings of MT Summit XII. Association for Machine Translation in the Americas,</booktitle>
<location>Ottawa, ON,</location>
<contexts>
<context position="5728" citStr="Condon et al., 2009" startWordPosition="943" endWordPosition="946">ranslation is usually available and therefore the BLEU scores are often underestimated. The main disadvantage of BLEU is the fact that it treats words as atomic units and does not allow any partial matches. Therefore, words which are inflectional variants of each other are treated as completely different words although their meaning is similar (e.g. work, works, worked, working). Further, the n-gram precision for n &gt; 1 penalizes difference in word order between the reference and the test sentences even though in languages with free word order both sentences can be correct (Bojar et al., 2010; Condon et al., 2009). There are also other widely recognized MT evaluation metrics: The NIST score (Doddington, 2002) is also an n-gram based metric, but in addition it reflects how informative particular n-grams are. A metric that achieves a very high correlation with human judgment is METEOR (Denkowski and Lavie, 2011). It creates a monolingual alignment using language dependent tools as stemmers and synonyms dictionaries and computes weighted harmonic mean of precision and recall based on the matching. Some metrics are based on measuring the edit distance between the reference and test sentences. The Position-</context>
</contexts>
<marker>Condon, Sanders, Parvaz, Rubenstein, Doran, Aberdeen, Oshika, 2009</marker>
<rawString>Sherri Condon, Gregory A Sanders, Dan Parvaz, Alan Rubenstein, Christy Doran, John Aberdeen, and Beatrice Oshika. 2009. Normalization for automated metrics: English and arabic speech translation. Proceedings of MT Summit XII. Association for Machine Translation in the Americas, Ottawa, ON, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11,</booktitle>
<pages>85--91</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2036" citStr="Denkowski and Lavie, 2011" startWordPosition="323" endWordPosition="326">ith human judgment, we expect the system performance to be optimized also with respect to the human perception of translation quality. In this paper, we propose an MT metric called tBLEU (tolerant BLEU) that is based on the standard BLEU (Papineni et al., 2002) and designed to suit better when translation into morphologically richer languages. We aim to have a simple language independent metric that correlates with human judgment better than the standard BLEU. Several metrics try to address this problem as well and usually succeed to gain a higher correlation with human judgment (e.g. METEOR (Denkowski and Lavie, 2011), TerrorCat (Fishel et al., 2012)). However, they usually use some language-dependent tools and resources (METEOR uses stemmer and parahprasing tables, TerrorCat uses lemmatization and needs training data for each language pair) which prevent them from being widely adopted. In the next section, the previous work is briefly summarized. Section 3 describes the metric in detail. The experiments with the metric are described in Section 4 and their results are summarized in Section 5. 2 Previous Work BLEU (Papineni et al., 2002) is an established and the most widely used automatic metric for evalua</context>
<context position="6030" citStr="Denkowski and Lavie, 2011" startWordPosition="991" endWordPosition="994">different words although their meaning is similar (e.g. work, works, worked, working). Further, the n-gram precision for n &gt; 1 penalizes difference in word order between the reference and the test sentences even though in languages with free word order both sentences can be correct (Bojar et al., 2010; Condon et al., 2009). There are also other widely recognized MT evaluation metrics: The NIST score (Doddington, 2002) is also an n-gram based metric, but in addition it reflects how informative particular n-grams are. A metric that achieves a very high correlation with human judgment is METEOR (Denkowski and Lavie, 2011). It creates a monolingual alignment using language dependent tools as stemmers and synonyms dictionaries and computes weighted harmonic mean of precision and recall based on the matching. Some metrics are based on measuring the edit distance between the reference and test sentences. The Position-Independent Error Rate (PER) (Leusch et al., 2003) is computed as a length-normalized edit distance of sentences treated as bags of words. The Translation Edit Rate (TER) (Snover et al., 2006) is a number of edit operation needed to change the test sentence to the most similar reference sentence. In t</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT ’11, pages 85–91, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research, HLT ’02,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5825" citStr="Doddington, 2002" startWordPosition="959" endWordPosition="961">advantage of BLEU is the fact that it treats words as atomic units and does not allow any partial matches. Therefore, words which are inflectional variants of each other are treated as completely different words although their meaning is similar (e.g. work, works, worked, working). Further, the n-gram precision for n &gt; 1 penalizes difference in word order between the reference and the test sentences even though in languages with free word order both sentences can be correct (Bojar et al., 2010; Condon et al., 2009). There are also other widely recognized MT evaluation metrics: The NIST score (Doddington, 2002) is also an n-gram based metric, but in addition it reflects how informative particular n-grams are. A metric that achieves a very high correlation with human judgment is METEOR (Denkowski and Lavie, 2011). It creates a monolingual alignment using language dependent tools as stemmers and synonyms dictionaries and computes weighted harmonic mean of precision and recall based on the matching. Some metrics are based on measuring the edit distance between the reference and test sentences. The Position-Independent Error Rate (PER) (Leusch et al., 2003) is computed as a length-normalized edit distan</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the Second International Conference on Human Language Technology Research, HLT ’02, pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Fishel</author>
<author>Rico Sennrich</author>
<author>Maja Popovi´c</author>
<author>Ondrej Bojar</author>
</authors>
<title>Terrorcat: a translation error categorization-based mt quality metric.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>64--70</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Fishel, Sennrich, Popovi´c, Bojar, 2012</marker>
<rawString>Mark Fishel, Rico Sennrich, Maja Popovi´c, and Ond&amp;quot;rej Bojar. 2012. Terrorcat: a translation error categorization-based mt quality metric. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 64–70. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>A novel string-to-string distance measure with applications to machine translation evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of MT Summit IX,</booktitle>
<pages>240--247</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="6378" citStr="Leusch et al., 2003" startWordPosition="1045" endWordPosition="1048">recognized MT evaluation metrics: The NIST score (Doddington, 2002) is also an n-gram based metric, but in addition it reflects how informative particular n-grams are. A metric that achieves a very high correlation with human judgment is METEOR (Denkowski and Lavie, 2011). It creates a monolingual alignment using language dependent tools as stemmers and synonyms dictionaries and computes weighted harmonic mean of precision and recall based on the matching. Some metrics are based on measuring the edit distance between the reference and test sentences. The Position-Independent Error Rate (PER) (Leusch et al., 2003) is computed as a length-normalized edit distance of sentences treated as bags of words. The Translation Edit Rate (TER) (Snover et al., 2006) is a number of edit operation needed to change the test sentence to the most similar reference sentence. In this case, the allowed editing operations are insertions, deletions and substitutions and also shifting words within a sentence. A different approach is used in TerrorCat (Fishel et al., 2012). It uses frequencies of automatically obtained translation error categories as base for machine-learned pairwise comparison of translation hypotheses. In th</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2003</marker>
<rawString>Gregor Leusch, Nicola Ueffing, Hermann Ney, et al. 2003. A novel string-to-string distance measure with applications to machine translation evaluation. In Proceedings of MT Summit IX, pages 240–247. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Results of the WMT13 metrics shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Statistical Machine Translation,</booktitle>
<pages>45--51</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<marker>Mach´acek, Bojar, 2013</marker>
<rawString>Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. 2013. Results of the WMT13 metrics shared task. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 45–51, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Munkres</author>
</authors>
<title>Algorithms for the assignment and transportation problems.</title>
<date>1957</date>
<journal>Journal of the Society for Industrial &amp; Applied Mathematics,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="7667" citStr="Munkres, 1957" startWordPosition="1246" endWordPosition="1247">s compete annually (Mach´aˇcek and Bojar, 2013). In the comptetition, METEOR and TerrorCat scored better that the other mentioned metrics. 410 3 Metric Description tBLEU is computed in in two steps. Similarly to the METEOR score, we first make a monolingual alignment between the reference and the test sentences and then apply an algorithm similar to the standard BLEU but with modified n-gram precisions. The monolingual alignment is computed as a minimum weighted maximum bipartite matching between words in a reference sentence and a translation sentence1 using the Munkres assignment algorithm (Munkres, 1957). We define a weight of an alignment link as the affix distance of the test sentence word wti and the reference sentence word wrj: Let S be the longest common substring of wti and wri . We can rewrite the strings as a concatenation of a prefix, the common substring and a suffix: wt = wti,pSwti,s wr = wrj,pSwrj,s Further, we define the affix distance as: � AD(wr wt) = max 1, L(wrj,p,wti,p)+L(wrs,j ,wts,i)1 |S |J if JSJ &gt; 0 and AD(wr, wt) = 1 otherwise. L is the Levensthein distance between two strings. For example the affix distance of two Czech words vzpomenou and zapomenout (different forms o</context>
</contexts>
<marker>Munkres, 1957</marker>
<rawString>James Munkres. 1957. Algorithms for the assignment and transportation problems. Journal of the Society for Industrial &amp; Applied Mathematics, 5(1):32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1360" citStr="Och, 2003" startWordPosition="211" endWordPosition="212"> to the matching distance. The proposed tBLEU metric is designed to be more tolerant to errors in inflection, which usually does not effect the understandability of a sentence, and therefore be more suitable for measuring quality of translation into morphologically richer languages. 1 Introduction Automatic evaluation of machine translation (MT) quality is an important part of the machine translation pipeline. The possibility to run an evaluation algorithm many times while training a system enables the system to be optimized with respect to such a metric (e.g., by Minimum Error Rate Training (Och, 2003)). By achieving a high correlation of the metric with human judgment, we expect the system performance to be optimized also with respect to the human perception of translation quality. In this paper, we propose an MT metric called tBLEU (tolerant BLEU) that is based on the standard BLEU (Papineni et al., 2002) and designed to suit better when translation into morphologically richer languages. We aim to have a simple language independent metric that correlates with human judgment better than the standard BLEU. Several metrics try to address this problem as well and usually succeed to gain a hig</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th annual meeting on association for computational linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1671" citStr="Papineni et al., 2002" startWordPosition="264" endWordPosition="267">Automatic evaluation of machine translation (MT) quality is an important part of the machine translation pipeline. The possibility to run an evaluation algorithm many times while training a system enables the system to be optimized with respect to such a metric (e.g., by Minimum Error Rate Training (Och, 2003)). By achieving a high correlation of the metric with human judgment, we expect the system performance to be optimized also with respect to the human perception of translation quality. In this paper, we propose an MT metric called tBLEU (tolerant BLEU) that is based on the standard BLEU (Papineni et al., 2002) and designed to suit better when translation into morphologically richer languages. We aim to have a simple language independent metric that correlates with human judgment better than the standard BLEU. Several metrics try to address this problem as well and usually succeed to gain a higher correlation with human judgment (e.g. METEOR (Denkowski and Lavie, 2011), TerrorCat (Fishel et al., 2012)). However, they usually use some language-dependent tools and resources (METEOR uses stemmer and parahprasing tables, TerrorCat uses lemmatization and needs training data for each language pair) which </context>
<context position="3161" citStr="Papineni et al., 2002" startWordPosition="511" endWordPosition="514">EU (Papineni et al., 2002) is an established and the most widely used automatic metric for evaluation of MT quality. It is computed as a harmonic mean of the n-gram precisions multiplied by the brevity penalty coefficient which ensures also high recall. Formally: )log pn , where BP is the brevity penaly defined as follows: � 1 BP = e1− c is the length of the test sentence (number of tokens), r is the length of the reference sentence, and pn is the proportion of n-grams from the test sentence found in the reference translations. The original experiments with the English to Chinese translation (Papineni et al., 2002) reported very high correlation of BLEU with human judgments. However, these scores were computed using multiple reference translations (to capture translation variability) but in practice, only one BLEU = BP · exp 4 1 4 � n=1 ifc&gt;r otherwise , 409 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 409–413, Baltimore, Maryland USA, June 26–27, 2014. c�2014 Association for Computational Linguistics I am driving a new red car Jedu novým cerveným autem 1 Jedu s novém cerveném auto. 0 2 3 6 3 1 Source: Reference: Translation: Corrected and (Jedu, 1) (s, 1) (novým, 2/3) (ce</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311–318. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of association for machine translation in the Americas,</booktitle>
<pages>223--231</pages>
<contexts>
<context position="6520" citStr="Snover et al., 2006" startWordPosition="1068" endWordPosition="1071">e particular n-grams are. A metric that achieves a very high correlation with human judgment is METEOR (Denkowski and Lavie, 2011). It creates a monolingual alignment using language dependent tools as stemmers and synonyms dictionaries and computes weighted harmonic mean of precision and recall based on the matching. Some metrics are based on measuring the edit distance between the reference and test sentences. The Position-Independent Error Rate (PER) (Leusch et al., 2003) is computed as a length-normalized edit distance of sentences treated as bags of words. The Translation Edit Rate (TER) (Snover et al., 2006) is a number of edit operation needed to change the test sentence to the most similar reference sentence. In this case, the allowed editing operations are insertions, deletions and substitutions and also shifting words within a sentence. A different approach is used in TerrorCat (Fishel et al., 2012). It uses frequencies of automatically obtained translation error categories as base for machine-learned pairwise comparison of translation hypotheses. In the Workshop of Machine Translation (WMT) Metrics Task, several new MT metrics compete annually (Mach´aˇcek and Bojar, 2013). In the comptetitio</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of association for machine translation in the Americas, pages 223–231.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
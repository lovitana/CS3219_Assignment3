<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000092">
<title confidence="0.999461">
A Novel Two-stage Framework for Extracting Opinionated Sentences
from News Articles
</title>
<author confidence="0.997863">
Pujari Rajkumar&apos;, Swara Desai2, Niloy Ganguly&apos; and Pawan Goyal&apos;
</author>
<affiliation confidence="0.996836">
&apos;Dept. of Computer Science and Engineering,
Indian Institute of Technology Kharagpur, India – 721302
</affiliation>
<address confidence="0.478164">
2Yahoo! India
</address>
<email confidence="0.845099">
&apos;rajkumarsaikorian@gmail.com, {niloy,pawang}@cse.iitkgp.ernet.in
2swara@yahoo-inc.com
</email>
<sectionHeader confidence="0.986861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999969043478261">
This paper presents a novel two-stage
framework to extract opinionated sentences
from a given news article. In the first stage,
Naive Bayes classifier by utilizing the local
features assigns a score to each sentence
- the score signifies the probability of the
sentence to be opinionated. In the second
stage, we use this prior within the HITS
(Hyperlink-Induced Topic Search) schema to
exploit the global structure of the article and
relation between the sentences. In the HITS
schema, the opinionated sentences are treated
as Hubs and the facts around these opinions
are treated as the Authorities. The algorithm
is implemented and evaluated against a set of
manually marked data. We show that using
HITS significantly improves the precision
over the baseline Naive Bayes classifier.
We also argue that the proposed method
actually discovers the underlying structure of
the article, thus extracting various opinions,
grouped with supporting facts as well as other
supporting opinions from the article.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969290322581">
With the advertising based revenues becoming the main
source of revenue, finding novel ways to increase
focussed user engagement has become an important
research topic. A typical problem faced by web
publishing houses like Yahoo!, is understanding the
nature of the comments posted by readers of 105
articles posted at any moment on its website. A lot
of users engage in discussions in the comments section
of the articles. Each user has a different perspective
and thus comments in that genre - this many a times,
results in a situation where the discussions in the
comment section wander far away from the articles
topic. In order to assist users to discuss relevant points
in the comments section, a possible methodology can
be to generate questions from the article’s content that
seek user’s opinions about various opinions conveyed
in the article (Rokhlenko and Szpektor, 2013). It
would also direct the users into thinking about a
spectrum of various points that the article covers
and encourage users to share their unique, personal,
daily-life experience in events relevant to the article.
This would thus provide a broader view point for
readers as well as perspective questions can be created
thus catering to users with rich user generated content,
this in turn can increase user engagement on the article
pages. Generating such questions manually for huge
volume of articles is very difficult. However, if one
could identify the main opinionated sentences within
the article, it will be much easier for an editor to
generate certain questions around these. Otherwise, the
sentences themselves may also serve as the points for
discussion by the users.
Hence, in this paper we discuss a two-stage
algorithm which picks opinionated sentences from
the articles. The algorithm assumes an underlying
structure for an article, that is, each opinionated
sentence is supported by a few factual statements that
justify the opinion. We use the HITS schema to
exploit this underlying structure and pick opinionated
sentences from the article.
The main contribtutions of this papers are as follows.
First, we present a novel two-stage framework for
extracting opinionated sentences from a news article.
Secondly, we propose a new evaluation metric that
takes into account the fact that since the amount
of polarity (and thus, the number of opinionated
sentences) within documents can vary a lot and thus,
we should stress on the ratio of opinionated sentences
in the top sentences, relative to the ratio of opinionated
sentences in the article. Finally, discussions on how the
proposed algorithm captures the underlying structure
of the opinions and surrounding facts in a news article
reveal that the algorithm does much more than just
extracting opinionated sentences.
This paper has been organised as follows. Section
2 discusses related work in this field. In section 3, we
discuss our two-stage model in further details. Section
4 discusses the experimental framework and the results.
Further discussions on the underlying assumption
behind using HITS along with error analysis are carried
out in Section 5. Conclusions and future work are
detailed in Section 6.
</bodyText>
<sectionHeader confidence="0.993602" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.5118155">
Opinion mining has drawn a lot of attention in recent
years. Research works have focused on mining
</bodyText>
<page confidence="0.983064">
25
</page>
<subsubsectionHeader confidence="0.65594">
Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 25–33,
</subsubsectionHeader>
<bodyText confidence="0.99488601754386">
October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics
opinions from various information sources such as
blogs (Conrad and Schilder, 2007; Harb et al., 2008),
product reviews (Hu and Liu, 2004; Qadir, 2009; Dave
et al., 2003), news articles (Kim and Hovy, 2006;
Hu and Liu, 2006) etc. Various aspects in opinion
mining have been explored over the years (Ku et
al., 2006). One important dimension is to identify
the opinion holders as well as opinion targets. (Lu,
2010) used dependency parser to identify the opinion
holders and targets in Chinese news text. (Choi
et al., 2005) use Conditional Random Fields to
identify the sources of opinions from the sentences.
(Kobayashi et al., 2005) propose a learning based
anaphora resolution technique to extract the opinion
tuple &lt; Subject, Attribute, Value &gt;. Opinion
summarization has been another important aspect (Kim
et al., 2013).
A lot of research work has been done for opinion
mining from product reviews where most of the text
is opinion-rich. Opinion mining from news articles,
however, poses its own challenges because in contrast
with the product reviews, not all parts of news articles
present opinions (Balahur et al., 2013) and thus finding
opinionated sentences itself remains a major obstacle.
Our work mainly focus on classifying a sentence in a
news article as opinionated or factual. There have been
works on sentiment classification (Wiebe and Riloff,
2005) but the task of finding opinionated sentences is
different from finding sentiments, because sentiments
mainly convey the emotions and not the opinions.
There has been research on finding opinionated
sentences from various information sources. Some
of these works utilize a dictionary-based (Fei et al.,
2012) or regular pattern based (Brun, 2012) approach
to identify aspects in the sentences. (Kim and Hovy,
2006) utilize the presence of a single strong valence
wors as well as the total valence score of all words in
a sentence to identify opinion-bearing sentences. (Zhai
et al., 2011) work on finding ‘evaluative’ sentences in
online discussions. They exploit the inter-relationship
of aspects, evaluation words and emotion words to
reinforce each other.
Thus, while ours is not the first attempt at
opinion extraction from news articles, to the best
of our knowledge, none of the previous works has
exploited the global structure of a news article to
classify a sentence as opinionated/factual. Though
summarization algorithms (Erkan and Radev, 2004;
Goyal et al., 2013) utilize the similarity between
sentences in an article to find the important sentences,
our formulation is different in that we conceptualize
two different kinds of nodes in a document, as opposed
to the summarization algorithms, which treat all the
sentences equally.
In the next section, we describe the propsoed
two-stage algorithm in detail.
</bodyText>
<sectionHeader confidence="0.969112" genericHeader="method">
3 Our Approach
</sectionHeader>
<bodyText confidence="0.999938421052632">
Figure 1 gives a flowchart of the proposed two-stage
method for extracting opinionated sentences from news
articles. First, each news article is pre-processed to
get the dependency parse as well as the TF-IDF vector
corresponding to each of the sentences present in the
article. Then, various features are extracted from
these sentences which are used as input to the Naive
Bayes classifier, as will be described in Section 3.1.
The Naive Bayes classifier, which corresponds to the
first-stage of our method, assigns a probability score
to each sentence as being an opinionated sentence.
In the second stage, the entire article is viewed as a
complete and directed graph with edges from every
sentence to all other sentences, each edge having a
weight suitably computed. Iterative HITS algorithm
is applied to the sentence graph, with opinionated
sentences conceptualized as hubs and factual sentences
conceptualized as authorities. The two stages of our
approach are detailed below.
</bodyText>
<subsectionHeader confidence="0.992257">
3.1 Naive Bayes Classifier
</subsectionHeader>
<bodyText confidence="0.9991054">
The Naive Bayes classifier assigns the probability for
each sentence being opinionated. The classifier is
trained on 70 News articles from politics domain,
sentences of which were marked by a group
of annotators as being opinionated or factual.
Each sentence was marked by two annotators.
The inter-annotator agreement using Cohen’s kappa
coefficient was found to be 0.71.
The features utilized for the classifier are detailed
in Table 1. These features were adapted from those
reported in (Qadir, 2009; Yu and Hatzivassiloglou,
2003). A list of positive and negative polar words,
further expanded using wordnet synsets was taken
from (Kim and Hovy, 2005). Stanford dependency
parser (De Marneffe et al., 2006) was utilized to
compute the dependencies for each sentence within the
news article.
After the features are extracted from the sentences,
we used the Weka implementation of Naive Bayes to
train the classifier1.
</bodyText>
<tableCaption confidence="0.982918">
Table 1: Features List for the Naive Bayes Classifier
</tableCaption>
<bodyText confidence="0.9834996">
Count of positive polar words
Count of negative polar words
Polarity of the root verb of the sentence
Presence of aComp, xComp and advMod
dependencies in the sentence
</bodyText>
<subsectionHeader confidence="0.946587">
3.2 HITS
</subsectionHeader>
<bodyText confidence="0.999709">
The Naive Bayes classifier as discussed in Section 3.1
utilizes only the local features within a sentence. Thus,
the probability that a sentence is opinionated remains
</bodyText>
<footnote confidence="0.988118">
1http://www.cs.waikato.ac.nz/ml/weka/
</footnote>
<page confidence="0.996042">
26
</page>
<figureCaption confidence="0.999333">
Figure 1: Flow Chart of Various Stages in Our Approach
</figureCaption>
<bodyText confidence="0.999950425">
independent of its context as well as the document
structure. The main motivation behind formulating
this problem in HITS schema is to utilize the hidden
link structures among sentences. HITS stands for
‘Hyperlink-Induced Topic Search’; Originally, this
algorithm was developed to rank Web-pages, with a
particular insight that some of the webpages (Hubs)
served as catalog of information, that could lead users
directly to the other pages, which actually contained
the information (Authorities).
The intuition behind applying HITS for the task of
opinion extraction came from the following assumption
about underlying structure of an article. A news article
pertains to a specific theme and with that theme in
mind, the author presents certain opinions. These
opinions are justified with the facts present in the article
itself. We conceptualize the opinionated sentences
as Hubs and the associated facts for an opinionated
sentence as Authorities for this Hub.
To describe the formulation of HITS parameters,
let us give the notations. Let us denote a document
D using a set of sentences {S1, S2, ... , Si, ... , Sn},
where n corresponds to the number of sentences in
the document D. We construct the sentence graph
where nodes in the graph correspond to the sentences
in the document. Let Hi and Ai denote the hub
and authority scores for sentence Si. In HITS, the
edges always flow from a Hub to an Authority. In
the original HITS algorithm, each edge is given the
same weight. However, it has been reported that using
weights in HITS update improves the performance
significantly (Li et al., 2002). In our formulation,
since each node has a non-zero probablility of acting
as a hub as well as an authority, we have outgoing as
well as incoming edges for every node. Therefore, the
weights are assigned, keeping in mind the proximity
between sentences as well as the probability (of being
opinionated/factual) assigned by the classifier. The
following criteria were used for deciding the weight
function.
</bodyText>
<listItem confidence="0.954789857142857">
• An edge in the HITS graph goes from a hub
(source node) to an authority (target node). So, the
edge weight from a source node to a target node
should be higher if the source node has a high hub
score.
• A fact corresponding to an opinionated sentence
should be discussing the same topic. So, the edge
weight should be higher if the sentences are more
similar.
• It is more probable that the facts around an
opinion appear closer to that opinionated sentence
in the article. So, the edge weight from a source to
target node decreases as the distance between the
two sentences increases.
</listItem>
<bodyText confidence="0.998702">
Let W be the weight matrix such that Wij denotes
the weight for the edge from the sentence Si to the
sentence Sj. Based on the criteria outlined above, we
formulate that the weight Wij should be such that
</bodyText>
<equation confidence="0.8081708">
Wij 0c Hi
Wij 0c Simij
1
Wij 0c
distij
</equation>
<bodyText confidence="0.9997005">
where we use cosine similarity between the sentence
vectors to compute Simij. distij is simply the number
</bodyText>
<page confidence="0.987556">
27
</page>
<bodyText confidence="0.999985210526316">
of sentences separating the source and target node.
Various combinations of these factors were tried and
will be discussed in section 4. While factors like
sentence similarity and distance are symmetric, having
the weight function depend on the hub score makes it
asymmetric, consistent with the basic idea of HITS.
Thus, an edge from the sentence Si to Sj is given
a high weight if Si has a high probability score of
being opinionated (i.e., acting as hub) as obtained the
classifier.
Now, for applying the HITS algorithm iteratively,
the Hubs and Authorities scores for each sentence
are initialized using the probability scores assigned
by the classifier. That is, if Pi(Opinion) denotes
the probability that Si is an opinionated sentence as
per the Naive Bayes Classifier, Hi(0) is initialized
to Pi(Opinion) and Ai(0) is initialized to 1 −
Pi(Opinion). The iterative HITS is then applied as
follows:
</bodyText>
<equation confidence="0.999987">
Hi(k) = EjWijAi(k − 1) (1)
Ai(k) = EjWjiHi(k − 1) (2)
</equation>
<bodyText confidence="0.999848857142857">
where Hi(k) denote the hub score for the ith
sentence during the kth iteration of HITS. The iteration
is stopped once the mean squared error between the
Hub and Authority values at two different iterations
is less than a threshold e. After the HITS iteration is
over, five sentences having the highest Hub scores are
returned by the system.
</bodyText>
<sectionHeader confidence="0.996767" genericHeader="method">
4 Experimental Framework and Results
</sectionHeader>
<bodyText confidence="0.99883355">
The experiment was conducted with 90 news articles in
politics domain from Yahoo! website. The sentences
in the articles were marked as opinionated or factual
by a group of annotators. In the training set, 1393
out of 3142 sentences were found to be opinianated.
In the test set, 347 out of 830 sentences were marked
as opinionated. Out of these 90 articles, 70 articles
were used for training the Naive Bayes classifier as
well as for tuning various parameters. The rest 20
articles were used for testing. The evaluation was
done in an Information Retrieval setting. That is, the
system returns the sentences in a decreasing order of
their score (or probability in the case of Naive Bayes)
as being opinionated. We then utilize the human
judgements (provided by the annotators) to compute
precision at various points. Let op(.) be a binary
function for a given rank such that op(r) = 1 if the
sentence returned as rank r is opinionated as per the
human judgements.
A POk precision is calculated as follows:
</bodyText>
<equation confidence="0.971542666666667">
Ek
POk = �r=1 op(r) (3)
k
</equation>
<bodyText confidence="0.999268769230769">
While the precision at various points indicates how
reliable the results returned by the system are, it
does not take into account the fact that some of the
documents are opinion-rich and some are not. For
the opinion-rich documents, a high POk value might
be similar to picking sentences randomly, whereas for
the documents with a very few opinions, even a lower
POk value might be useful. We, therefore, devise
another evaluation metric MOk that indicates the ratio
of opinionated sentences at any point, normalized with
respect to the ratio of opinionated sentences in the
article.
Correspondingly, an MOk value is calculated as
</bodyText>
<equation confidence="0.957911333333333">
POk
MOk = (4)
Ratioop
</equation>
<bodyText confidence="0.9986635">
where Ratioop denotes the fraction of opinionated
sentences in the whole article. Thus
</bodyText>
<equation confidence="0.579092333333333">
Number of opinionated sentences
Ratioop = (5)
Number of sentences
</equation>
<bodyText confidence="0.999754941176471">
The parameters that we needed to fix for the HITS
algorithm were the weight function Wij and the
threshold e at which we stop the iteration. We varied
e from 0.0001 to 0.1 multiplying it by 10 in each step.
The results were not sensitive to the value of e and
we used e = 0.01. For fixing the weight function,
we tried out various combinations using the criteria
outlined in Section 3.2. Various weight functions and
the corresponding PO5 and MO5 scores are shown in
Table 2. Firstly, we varied k in Simijk and found that
the square of the similarity function gives better results.
Then, keeping it constant, we varied l in Hil and found
the best results for l = 3. Then, keeping both of these
constants, we varied α in (α + 1d). We found the best
results for α = 1.0. With this α, we tried to vary l again
but it only reduced the final score. Therefore, we fixed
the weight function to be
</bodyText>
<equation confidence="0.9406395">
Wij = Hi3(0)Simij2(1 + di 1 ) (6)
tij
</equation>
<bodyText confidence="0.9999661">
Note that Hi(0) in Equation 6 corresponds to the
probablity assigned by the classifier that the sentence
Si is opinionated.
We use the classifier results as the baseline for the
comparisons. The second-stage HITS algorithm is
then applied and we compare the performance with
respect to the classifier. Table 3 shows the comparison
results for various precision scores for the classifier
and the HITS algorithm. In practical situation, an
editor requires quick identification of 3-5 opinionated
sentences from the article, which she can then use to
formulate questions. We thus report POk and MOk
values for k = 3 and k = 5.
From the results shown in Table 3, it is clear
that applying the second-stage HITS over the Naive
Bayes Classifier improves the performance by a large
degree, both in term of POk and MOk. For
instance, the first-stage NB Classifier gives a PO5 of
0.52 and PO3 of 0.53. Using the classifier outputs
during the second-stage HITS algorithm improves the
</bodyText>
<page confidence="0.998571">
28
</page>
<tableCaption confidence="0.993391">
Table 2: Average P@5 and M@5 scores: Performance
comparison between various functions for Wij
</tableCaption>
<table confidence="0.999972105263158">
Function P@5 M@5
Simij 0.48 0.94
Sim20.57 1.16
ij
Sim3 0.53 1.11
ij
Sim2 ijHi 0.6 1.22
Sim2ijHi2 0.61 1.27
Sim2ijHi3 0.61 1.27
Sim2ijHi4 0.58 1.21
Sim2ijHi3 1 0.56 1.20
d
Sim2ijHi3(0.2 + 1d) 0.60 1.25
Sim2ijHi3(0.4+ 1d) 0.61 1.27
Sim2ijHi3(0.6 + 1d) 0.62 1.31
Sim2ijHi3(0.8 + 1 d) 0.62 1.31
Sim2 ijHi3(1 + 1 d) 0.63 1.33
Sim2ijHi3(1.2+ 1d) 0.61 1.28
Sim2ijHi2(1 + 1d) 0.6 1.23
</table>
<tableCaption confidence="0.881547333333333">
Table 3: Average P@5, M@5, P@3 and M@3 scores:
Performance comparison between the NB classifier and
HITS
</tableCaption>
<table confidence="0.99846275">
System P@5 M@5 P@3 M@3
NB Classifier 0.52 1.13 0.53 1.17
HITS 0.63 1.33 0.72 1.53
Imp. (%) +21.2 +17.7 +35.8 +30.8
</table>
<bodyText confidence="0.999111185185185">
preformance by 21.2% to 0.63 in the case of P@5. For
P@3, the improvements were much more significant
and a 35.8% improvement was obtained over the NB
classifier. M@5 and M@3 scores also improve by
17.7% and 30.8% respectively.
Strikingly, while the classifier gave nearly the same
scores for P@k and M@k for k = 3 and k = 5,
HITS gave much better results for k = 3 than k = 5.
Specially, the P@3 and M@3 scores obtained by HITS
were very encouraging, indicating that the proposed
approach helps in pushing the opinionated sentences to
the top. This clearly shows the advantage of using the
global structure of the document in contrast with the
features extracted from the sentence itself, ignoring the
context.
Figures 2 and 3 show the P@5, M@5, P@3 and
M@3 scores for individual documents as numbered
from 1 to 20 on the X-axis. The articles are
sorted as per the ratio of P@5 (and M@5) obtained
using the HITS and NB classifier. Y-axis shows the
corresponding scores. Two different lines are used to
represent the results as returned by the classifier and
the HITS algorithm. A dashed line denotes the scores
obtained by HITS while a continuous line denotes
the scores obtained by the NB classifier. A detailed
analysis of these figures can help us draw the following
conclusions:
</bodyText>
<listItem confidence="0.813992333333333">
• For 40% of the articles (numbered 13 to 20) HITS
improves over the baseline NB classifier. For
40% of the articles (numbered 5 to 12) the results
provided by HITS were the same as that of the
baseline. For 20% of the articles (numbered 1 to
4) HITS gives a performance lower than that of
the baseline. Thus, for 80% of the documents, the
second-stage performs at least as good as the first
stage. This indicates that the second-stage HITS
is quite robust.
• M@5 results are much more robust for the HITS,
with 75% of the documents having an M@5 score
&gt; 1. An M@k score &gt; 1 indicates that the ratio of
opinionated sentences in top k sentences, picked
up by the algorithm, is higher than the overall ratio
in the article.
• For 45% of the articles, (numbered 6, 9 − 11 and
15 − 20), HITS was able to achieve a P@3 = 1.0.
</listItem>
<bodyText confidence="0.988957071428571">
Thus, for these 9 articles, the top 3 sentences
picked up by the algorithm were all marked as
opinionated.
The graphs also indicate a high correlation between
the results obtained by the NB classifier and HITS.
We used Pearson’s correlation to find the correlation
strength. For the P@5 values, the correlation was
found to be 0.6021 and for the M@5 values, the
correlation was obtained as 0.5954.
In the next section, we will first attempt to further
analyze the basic assumption behind using HITS,
by looking at some actual Hub-Authority structures,
captured by the algorithm. We will also take some
cases of failure and perform error analysis.
</bodyText>
<sectionHeader confidence="0.999172" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999841653846154">
First point that we wanted to verify was, whether
HITS is really capturing the underlying structure of
the document. That is, are the sentences identified as
authorities for a given hub really correspond to the facts
supporting the particular opinion, expressed by the hub
sentence.
Figure 4 gives two examples of the Hub-Authority
structure, as captured by the HITS algorithm, for two
different articles. For each of these examples, we show
the sentence identified as Hub in the center along with
the top four sentences, identified as Authorities for that
hub. We also give the annotations as to whether the
sentences were marked as ‘opinionated’ or ‘factual’ by
the annotators.
In both of these examples, the hubs were
actually marked as ‘opinionated’ by the annotators.
Additionally, we find that all the four sentences,
identified as authorities to the hub, are very relevant to
the opinion expressed by the hub. In the first example,
top 3 authority sentences are marked as ‘factual’ by the
annotator. Although the fourth sentence is marked as
‘opinionated’, it can be seen that this sentence presents
a supporting opinion for the hub sentence.
While studying the second example, we found that
while the first authority does not present an important
fact, the fourth authority surely does. Both of these
</bodyText>
<page confidence="0.998869">
29
</page>
<figureCaption confidence="0.994888">
Figure 2: Comparison Results for 20 Test articles between the Classifier and HITS: P@5 and M@5
</figureCaption>
<figure confidence="0.90905">
(a) Comparison of P@3 values (b) Comparison of M@3 values
</figure>
<figureCaption confidence="0.985065">
Figure 3: Comparison Results for 20 Test articles between the Classifier and HITS: P@3 and M@3
</figureCaption>
<figure confidence="0.938745">
(a) Hub-Authority Structure: Example 1 (b) Hub-Authority Structure: Example 2
</figure>
<figureCaption confidence="0.997871">
Figure 4: Example from two different test articles capturing the Hub-Authority Structure
</figureCaption>
<figure confidence="0.543094">
(a) Comparison of P@5 values (b) Comparison of M@5 values
</figure>
<bodyText confidence="0.9972486">
were marked as ‘factual’ by the annotators. In this
particular example, although the second and third
authority sentences were annotated as ‘opinionated’,
these can be seen as supporting the opinion expressed
by the hub sentence. This example also gives us
an interesting idea to improve diversification in the
final results. That is, once an opinionated sentence
is identified by the algorithm, the hub score of all
its suthorities can be reduced proportional to the edge
weight. This will reduce the chances of the supporting
opinions being reurned by the system, at a later stage
as a main opinion.
We then attempted to test our tool on a
recently published article, “What’s Wrong with
a Meritocracy Rug?”2. The tool could pick up a very
</bodyText>
<footnote confidence="0.994888">
2http://news.yahoo.com/
whats-wrong-meritocracy-rug-070000354.
html
</footnote>
<page confidence="0.998039">
30
</page>
<bodyText confidence="0.999864142857143">
important opinion in the article, “Most people tend to
think that the most qualified person is someone who
looks just like them, only younger.”, which was ranked
2nd by the system. The supporting facts and opinions
for this sentence, as discovered by the algorithm
were also quite relevant. For instance, the top two
authorities corresponding to this sentence hub were:
</bodyText>
<listItem confidence="0.995624714285714">
1. And that appreciation, we learned painfully, can
easily be tinged with all kinds of gendered
elements without the person who is making the
decisions even realizing it.
2. And many of the traits we value, and how we
value them, also end up being laden with gender
overtones.
</listItem>
<subsectionHeader confidence="0.976207">
5.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999932407407407">
We then tried to analyze certain cases of failures.
Firstly, we wanted to understand why HITS was not
performing as good as the classifier for 3 articles
(Figures 2 and 3). The analysis revealed that the
supporting sentences for the opinionated sentences,
extracted by the classifier, were not very similar on
the textual level. Thus a low cosine similarity score
resulted in having lower edge weights, thereby getting
a lower hub score after applying HITS. For one of the
articles, the sentence picked up by HITS was wrongly
annotated as a factual sentence.
Then, we looked at one case of failure due to the
error introduced by the classifier prior probablities.
For instance, the sentence, “The civil war between
establishment and tea party Republicans intensified
this week when House Speaker John Boehner slammed
outside conservative groups for ridiculous pushback
against the bipartisan budget agreement which cleared
his chamber Thursday.” was classified as an
opinionanted sentence, whereas this is a factual
sentence. Looking closely, we found that the sentence
contains three polar words (marked in bold), as
well as an advMod dependency between the pair
(slammed,when). Thus the sentence got a high initial
prior by the classifier. As a result, the outgoing edges
from this node got a higher Hi3 factor. Some of the
authorities identified for this sentence were:
</bodyText>
<listItem confidence="0.999356666666667">
• For Democrats, the tea party is the gift that keeps
on giving.
• Tea party sympathetic organizations, Boehner
</listItem>
<bodyText confidence="0.981306375">
later said, “are pushing our members in places
where they don’t want to be”.
which had words, similar to the original sentence, thus
having a higher Simij factor as well. We found that
these sentences were also very close within the article.
Thus, a high hub prior along with a high outgoing
weight gave rise to this sentence having a high hub
score after the HITS iterations.
</bodyText>
<subsectionHeader confidence="0.983505">
5.2 Online Interface
</subsectionHeader>
<bodyText confidence="0.999943">
To facilitate easy usage and understanding of the
system by others, a web interface has been built for
the system3. The webpage caters for users to either
input a new article in form of text to get top opinionated
sentences or view the output analysis of the system over
manually marked test data consisting of 20 articles.
The words in green color are positive polar words,
red indicates negative polar words. Words marked in
violet are the root verbs of the sentences. The colored
graph shows top ranked opinionated sentences in
yellow box along with top supporting factual sentences
for that particluar opinionated sentence in purple boxes.
Snapshots from the online interface are provided in
Figures 5 and 6.
</bodyText>
<sectionHeader confidence="0.998463" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999968097560976">
In this paper, we presented a novel two-stage
framework for extracting the opinionated sentences
in the news articles. The problem of identifying
top opinionated sentences from news articles is very
challenging, especially because the opinions are not
as explicit in a news article as in a discussion forum.
It was also evident from the inter-annotator agreement
and the kappa coefficient was found to be 0.71.
The experiments conducted over 90 News
articles (70 for training and 20 for testing) clearly
indicate that the proposed two-stage method
almost always improves the performance of the
baseline classifier-based approach. Specifically, the
improvements are much higher for P@3 and M@3
scores (35.8% and 30.8% over the NB classifier). An
M@3 score of 1.5 and P@3 score of 0.72 indicates that
the proposed method was able to push the opinionated
sentences to the top. On an average, 2 out of top
3 sentences returned by the system were actually
opinionated. This is very much desired in a practical
scenario, where an editor requires quick identification
of 3-5 opinionated sentences, which she can then use
to formulate questions.
The examples discussed in Section 5 bring out
another important aspect of the proposed algorithm.
In addition to the main objective of extracting the
opinionated sentences within the article, the proposed
method actually discovers the underlying structure of
the article and would certainly be useful to present
various opinions, grouped with supporting facts as well
as supporting opinions in the article.
While the initial results are encouraging, there is
scope for improvement. We saw that the results
obtained via HITS were highly correlated with the
Naive Bayes classifier results, which were used in
assigning a weight to the document graph. One
direction for the future work would be to experiment
with other features to improve the precision of the
classifier. Additionally, in the current evaluation,
we are not evaluating the degree of diversity of the
opinions returned by the system. The Hub-Authority
</bodyText>
<footnote confidence="0.9992345">
3available at http://cse.iitkgp.ac.in/
resgrp/cnerg/temp2/final.php
</footnote>
<page confidence="0.999689">
31
</page>
<figureCaption confidence="0.9999905">
Figure 5: Screenshot from the Web Interface
Figure 6: Hub-Authority Structure as output on the Web Interface
</figureCaption>
<bodyText confidence="0.997888352941177">
structure of the second example gives us an interesting
idea to improve diversification and we would like to
implement that in future.
In the future, we would also like to apply this work
to track an event over time, based on the opinionated
sentences present in the articles. When an event occurs,
articles start out with more factual sentences. Over
time, opinions start surfacing on the event, and as the
event matures, opinions predominate the facts in the
articles. For example, a set of articles on a plane
crash would start out as factual, and would offer expert
opinions over time. This work can be used to plot the
maturity of the media coverage by keeping track of
facts v/s opinions on any event, and this can be used
by organizations to provide a timeline for the event.
We would also like to experiment with this model on
a different media like microblogs.
</bodyText>
<sectionHeader confidence="0.998239" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.948075545454546">
Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov,
Vanni Zavarella, Erik Van Der Goot, Matina Halkia,
Bruno Pouliquen, and Jenya Belyaeva. 2013.
Sentiment analysis in the news. arXiv preprint
arXiv:1309.6202.
Caroline Brun. 2012. Learning opinionated patterns
for contextual opinion detection. In COLING
(Posters), pages 165–174.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction
</reference>
<page confidence="0.997909">
32
</page>
<bodyText confidence="0.69331775">
patterns. In Proceedings of the conference on
Human Language Technology and Empirical
Methods in Natural Language Processing,
pages 355–362. Association for Computational
Linguistics.
Jack G Conrad and Frank Schilder. 2007. Opinion
mining in legal blogs. In Proceedings of the 11th
international conference on Artificial intelligence
and law, pages 231–236. ACM.
Kushal Dave, Steve Lawrence, and David M Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In
</bodyText>
<reference confidence="0.9944415">
Proceedings of the 12th international conference on
World Wide Web, pages 519–528. ACM.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generating
typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449–454.
G¨unes Erkan and Dragomir R Radev. 2004.
Lexrank: Graph-based lexical centrality as salience
in text summarization. J. Artif. Intell. Res.(JAIR),
22(1):457–479.
Geli Fei, Bing Liu, Meichun Hsu, Malu Castellanos,
and Riddhiman Ghosh. 2012. A dictionary-based
approach to identifying aspects im-plied by
adjectives for opinion mining. In Proceedings of
COLING 2012 (Posters).
Pawan Goyal, Laxmidhar Behera, and Thomas Martin
McGinnity. 2013. A context-based word indexing
model for document summarization. Knowledge
and Data Engineering, IEEE Transactions on,
25(8):1693–1705.
Ali Harb, Michel Planti´e, Gerard Dray, Mathieu Roche,
Franc¸ois Trousset, and Pascal Poncelet. 2008.
Web opinion mining: How to extract opinions from
blogs? In Proceedings of the 5th international
conference on Soft computing as transdisciplinary
science and technology, pages 211–217. ACM.
Minqing Hu and Bing Liu. 2004. Mining opinion
features in customer reviews. In Proceedings
of Nineteeth National Conference on Artificial
Intellgience (AAAI).
Minqing Hu and Bing Liu. 2006. Opinion extraction
and summarization on the web. In AAAI, volume 7,
pages 1621–1624.
Soo-Min Kim and Eduard Hovy. 2005. Automatic
detection of opinion bearing words and sentences.
In Proceedings of IJCNLP, volume 5.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed
in online news media text. In Proceedings of
the Workshop on Sentiment and Subjectivity in
Text, pages 1–8. Association for Computational
Linguistics.
Hyun Duk Kim, Malu Castellanos, Meichun
Hsu, ChengXiang Zhai, Umeshwar Dayal, and
Riddhiman Ghosh. 2013. Compact explanatory
opinion summarization. In Proceedings of the
22nd ACM international conference on Conference
on information &amp; knowledge management, pages
1697–1702. ACM.
Nozomi Kobayashi, Ryu Iida, Kentaro Inui, and
Yuji Matsumoto. 2005. Opinion extraction using
a learning-based anaphora resolution technique.
In The Second International Joint Conference
on Natural Language Processing (IJCNLP),
Companion Volume to the Proceeding of Conference
including Posters/Demos and Tutorial Abstracts.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006. Opinion extraction, summarization and
tracking in news and blog corpora. In AAAI
Spring Symposium: Computational Approaches to
Analyzing Weblogs, volume 100107.
Longzhuang Li, Yi Shang, and Wei Zhang. 2002.
Improvement of hits-based algorithms on web
documents. In Proceedings of the 11th international
conference on World Wide Web, pages 527–535.
ACM.
Bin Lu. 2010. Identifying opinion holders and targets
with dependency parser in chinese news texts. In
Proceedings of Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the ACL.
</reference>
<bodyText confidence="0.959276375">
Ashequl Qadir. 2009. Detecting opinion sentences
specific to product features in customer reviews
using typed dependency relations. In Proceedings
of the Workshop on Events in Emerging Text Types,
eETTs ’09, pages 38–43.
Oleg Rokhlenko and Idan Szpektor. 2013. Generating
synthetic comparable questions for news articles. In
ACL, pages 742–751.
Janyce Wiebe and Ellen Riloff. 2005. Creating
subjective and objective sentence classifiers from
unannotated texts. In Computational Linguistics
and Intelligent Text Processing, pages 486–497.
Springer.
Hong Yu and Vasileios Hatzivassiloglou. 2003.
Towards answering opinion questions: Separating
facts from opinions and identifying the polarity
</bodyText>
<reference confidence="0.950307125">
of opinion sentences. In Proceedings of the
2003 Conference on Empirical Methods in Natural
Language Processing, EMNLP ’03, pages 129–136.
Zhongwu Zhai, Bing Liu, Lei Zhang, Hua Xu,
and Peifa Jia. 2011. Identifying evaluative
sentences in online discussions. In Proceedings
of the Twenty-Fifth AAAI Conference on Artificial
Intelligence.
</reference>
<page confidence="0.999343">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.967031">
<title confidence="0.9980675">A Novel Two-stage Framework for Extracting Opinionated from News Articles</title>
<author confidence="0.996969">Swara Niloy Pawan</author>
<affiliation confidence="0.991411">of Computer Science and Indian Institute of Technology Kharagpur, India –</affiliation>
<abstract confidence="0.999362666666667">This paper presents a novel two-stage framework to extract opinionated sentences from a given news article. In the first stage, Naive Bayes classifier by utilizing the local features assigns a score to each sentence the score signifies the probability of the sentence to be opinionated. In the second stage, we use this prior within the HITS (Hyperlink-Induced Topic Search) schema to exploit the global structure of the article and relation between the sentences. In the HITS schema, the opinionated sentences are treated as Hubs and the facts around these opinions are treated as the Authorities. The algorithm is implemented and evaluated against a set of manually marked data. We show that using HITS significantly improves the precision over the baseline Naive Bayes classifier. We also argue that the proposed method actually discovers the underlying structure of the article, thus extracting various opinions, grouped with supporting facts as well as other supporting opinions from the article.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Alexandra Balahur</author>
<author>Ralf Steinberger</author>
</authors>
<institution>Mijail Kabadjov, Vanni Zavarella, Erik Van Der Goot, Matina Halkia,</institution>
<marker>Balahur, Steinberger, </marker>
<rawString>Alexandra Balahur, Ralf Steinberger, Mijail Kabadjov, Vanni Zavarella, Erik Van Der Goot, Matina Halkia,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Pouliquen</author>
<author>Jenya Belyaeva</author>
</authors>
<title>Sentiment analysis in the news. arXiv preprint arXiv:1309.6202.</title>
<date>2013</date>
<marker>Pouliquen, Belyaeva, 2013</marker>
<rawString>Bruno Pouliquen, and Jenya Belyaeva. 2013. Sentiment analysis in the news. arXiv preprint arXiv:1309.6202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Brun</author>
</authors>
<title>Learning opinionated patterns for contextual opinion detection.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>165--174</pages>
<contexts>
<context position="6568" citStr="Brun, 2012" startWordPosition="1016" endWordPosition="1017">s (Balahur et al., 2013) and thus finding opinionated sentences itself remains a major obstacle. Our work mainly focus on classifying a sentence in a news article as opinionated or factual. There have been works on sentiment classification (Wiebe and Riloff, 2005) but the task of finding opinionated sentences is different from finding sentiments, because sentiments mainly convey the emotions and not the opinions. There has been research on finding opinionated sentences from various information sources. Some of these works utilize a dictionary-based (Fei et al., 2012) or regular pattern based (Brun, 2012) approach to identify aspects in the sentences. (Kim and Hovy, 2006) utilize the presence of a single strong valence wors as well as the total valence score of all words in a sentence to identify opinion-bearing sentences. (Zhai et al., 2011) work on finding ‘evaluative’ sentences in online discussions. They exploit the inter-relationship of aspects, evaluation words and emotion words to reinforce each other. Thus, while ours is not the first attempt at opinion extraction from news articles, to the best of our knowledge, none of the previous works has exploited the global structure of a news a</context>
</contexts>
<marker>Brun, 2012</marker>
<rawString>Caroline Brun. 2012. Learning opinionated patterns for contextual opinion detection. In COLING (Posters), pages 165–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
<author>Ellen Riloff</author>
<author>Siddharth Patwardhan</author>
</authors>
<title>Identifying sources of opinions with conditional random fields and extraction</title>
<date>2005</date>
<booktitle>Proceedings of the 12th international conference on World Wide Web,</booktitle>
<pages>519--528</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5376" citStr="Choi et al., 2005" startWordPosition="831" endWordPosition="834">, pages 25–33, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics opinions from various information sources such as blogs (Conrad and Schilder, 2007; Harb et al., 2008), product reviews (Hu and Liu, 2004; Qadir, 2009; Dave et al., 2003), news articles (Kim and Hovy, 2006; Hu and Liu, 2006) etc. Various aspects in opinion mining have been explored over the years (Ku et al., 2006). One important dimension is to identify the opinion holders as well as opinion targets. (Lu, 2010) used dependency parser to identify the opinion holders and targets in Chinese news text. (Choi et al., 2005) use Conditional Random Fields to identify the sources of opinions from the sentences. (Kobayashi et al., 2005) propose a learning based anaphora resolution technique to extract the opinion tuple &lt; Subject, Attribute, Value &gt;. Opinion summarization has been another important aspect (Kim et al., 2013). A lot of research work has been done for opinion mining from product reviews where most of the text is opinion-rich. Opinion mining from news articles, however, poses its own challenges because in contrast with the product reviews, not all parts of news articles present opinions (Balahur et al., </context>
</contexts>
<marker>Choi, Cardie, Riloff, Patwardhan, 2005</marker>
<rawString>Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005. Identifying sources of opinions with conditional random fields and extraction Proceedings of the 12th international conference on World Wide Web, pages 519–528. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unes Erkan</author>
<author>Dragomir R Radev</author>
</authors>
<title>Lexrank: Graph-based lexical centrality as salience in text summarization.</title>
<date>2004</date>
<journal>J. Artif. Intell. Res.(JAIR),</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="7276" citStr="Erkan and Radev, 2004" startWordPosition="1125" endWordPosition="1128">ce of a single strong valence wors as well as the total valence score of all words in a sentence to identify opinion-bearing sentences. (Zhai et al., 2011) work on finding ‘evaluative’ sentences in online discussions. They exploit the inter-relationship of aspects, evaluation words and emotion words to reinforce each other. Thus, while ours is not the first attempt at opinion extraction from news articles, to the best of our knowledge, none of the previous works has exploited the global structure of a news article to classify a sentence as opinionated/factual. Though summarization algorithms (Erkan and Radev, 2004; Goyal et al., 2013) utilize the similarity between sentences in an article to find the important sentences, our formulation is different in that we conceptualize two different kinds of nodes in a document, as opposed to the summarization algorithms, which treat all the sentences equally. In the next section, we describe the propsoed two-stage algorithm in detail. 3 Our Approach Figure 1 gives a flowchart of the proposed two-stage method for extracting opinionated sentences from news articles. First, each news article is pre-processed to get the dependency parse as well as the TF-IDF vector c</context>
</contexts>
<marker>Erkan, Radev, 2004</marker>
<rawString>G¨unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. J. Artif. Intell. Res.(JAIR), 22(1):457–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geli Fei</author>
<author>Bing Liu</author>
<author>Meichun Hsu</author>
<author>Malu Castellanos</author>
<author>Riddhiman Ghosh</author>
</authors>
<title>A dictionary-based approach to identifying aspects im-plied by adjectives for opinion mining.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING</booktitle>
<contexts>
<context position="6530" citStr="Fei et al., 2012" startWordPosition="1008" endWordPosition="1011">t all parts of news articles present opinions (Balahur et al., 2013) and thus finding opinionated sentences itself remains a major obstacle. Our work mainly focus on classifying a sentence in a news article as opinionated or factual. There have been works on sentiment classification (Wiebe and Riloff, 2005) but the task of finding opinionated sentences is different from finding sentiments, because sentiments mainly convey the emotions and not the opinions. There has been research on finding opinionated sentences from various information sources. Some of these works utilize a dictionary-based (Fei et al., 2012) or regular pattern based (Brun, 2012) approach to identify aspects in the sentences. (Kim and Hovy, 2006) utilize the presence of a single strong valence wors as well as the total valence score of all words in a sentence to identify opinion-bearing sentences. (Zhai et al., 2011) work on finding ‘evaluative’ sentences in online discussions. They exploit the inter-relationship of aspects, evaluation words and emotion words to reinforce each other. Thus, while ours is not the first attempt at opinion extraction from news articles, to the best of our knowledge, none of the previous works has expl</context>
</contexts>
<marker>Fei, Liu, Hsu, Castellanos, Ghosh, 2012</marker>
<rawString>Geli Fei, Bing Liu, Meichun Hsu, Malu Castellanos, and Riddhiman Ghosh. 2012. A dictionary-based approach to identifying aspects im-plied by adjectives for opinion mining. In Proceedings of COLING 2012 (Posters).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pawan Goyal</author>
<author>Laxmidhar Behera</author>
<author>Thomas Martin McGinnity</author>
</authors>
<title>A context-based word indexing model for document summarization.</title>
<date>2013</date>
<journal>Knowledge and Data Engineering, IEEE Transactions on,</journal>
<volume>25</volume>
<issue>8</issue>
<contexts>
<context position="7297" citStr="Goyal et al., 2013" startWordPosition="1129" endWordPosition="1132">alence wors as well as the total valence score of all words in a sentence to identify opinion-bearing sentences. (Zhai et al., 2011) work on finding ‘evaluative’ sentences in online discussions. They exploit the inter-relationship of aspects, evaluation words and emotion words to reinforce each other. Thus, while ours is not the first attempt at opinion extraction from news articles, to the best of our knowledge, none of the previous works has exploited the global structure of a news article to classify a sentence as opinionated/factual. Though summarization algorithms (Erkan and Radev, 2004; Goyal et al., 2013) utilize the similarity between sentences in an article to find the important sentences, our formulation is different in that we conceptualize two different kinds of nodes in a document, as opposed to the summarization algorithms, which treat all the sentences equally. In the next section, we describe the propsoed two-stage algorithm in detail. 3 Our Approach Figure 1 gives a flowchart of the proposed two-stage method for extracting opinionated sentences from news articles. First, each news article is pre-processed to get the dependency parse as well as the TF-IDF vector corresponding to each </context>
</contexts>
<marker>Goyal, Behera, McGinnity, 2013</marker>
<rawString>Pawan Goyal, Laxmidhar Behera, and Thomas Martin McGinnity. 2013. A context-based word indexing model for document summarization. Knowledge and Data Engineering, IEEE Transactions on, 25(8):1693–1705.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Harb</author>
<author>Michel Planti´e</author>
<author>Gerard Dray</author>
<author>Mathieu Roche</author>
<author>Franc¸ois Trousset</author>
<author>Pascal Poncelet</author>
</authors>
<title>Web opinion mining: How to extract opinions from blogs?</title>
<date>2008</date>
<booktitle>In Proceedings of the 5th international conference on Soft computing as transdisciplinary science and technology,</booktitle>
<pages>211--217</pages>
<publisher>ACM.</publisher>
<marker>Harb, Planti´e, Dray, Roche, Trousset, Poncelet, 2008</marker>
<rawString>Ali Harb, Michel Planti´e, Gerard Dray, Mathieu Roche, Franc¸ois Trousset, and Pascal Poncelet. 2008. Web opinion mining: How to extract opinions from blogs? In Proceedings of the 5th international conference on Soft computing as transdisciplinary science and technology, pages 211–217. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining opinion features in customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of Nineteeth National Conference on Artificial Intellgience (AAAI).</booktitle>
<contexts>
<context position="4990" citStr="Hu and Liu, 2004" startWordPosition="765" endWordPosition="768"> discussions on the underlying assumption behind using HITS along with error analysis are carried out in Section 5. Conclusions and future work are detailed in Section 6. 2 Related Work Opinion mining has drawn a lot of attention in recent years. Research works have focused on mining 25 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 25–33, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics opinions from various information sources such as blogs (Conrad and Schilder, 2007; Harb et al., 2008), product reviews (Hu and Liu, 2004; Qadir, 2009; Dave et al., 2003), news articles (Kim and Hovy, 2006; Hu and Liu, 2006) etc. Various aspects in opinion mining have been explored over the years (Ku et al., 2006). One important dimension is to identify the opinion holders as well as opinion targets. (Lu, 2010) used dependency parser to identify the opinion holders and targets in Chinese news text. (Choi et al., 2005) use Conditional Random Fields to identify the sources of opinions from the sentences. (Kobayashi et al., 2005) propose a learning based anaphora resolution technique to extract the opinion tuple &lt; Subject, Attribu</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In Proceedings of Nineteeth National Conference on Artificial Intellgience (AAAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Opinion extraction and summarization on the web.</title>
<date>2006</date>
<booktitle>In AAAI,</booktitle>
<volume>7</volume>
<pages>1621--1624</pages>
<contexts>
<context position="5077" citStr="Hu and Liu, 2006" startWordPosition="781" endWordPosition="784">are carried out in Section 5. Conclusions and future work are detailed in Section 6. 2 Related Work Opinion mining has drawn a lot of attention in recent years. Research works have focused on mining 25 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 25–33, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics opinions from various information sources such as blogs (Conrad and Schilder, 2007; Harb et al., 2008), product reviews (Hu and Liu, 2004; Qadir, 2009; Dave et al., 2003), news articles (Kim and Hovy, 2006; Hu and Liu, 2006) etc. Various aspects in opinion mining have been explored over the years (Ku et al., 2006). One important dimension is to identify the opinion holders as well as opinion targets. (Lu, 2010) used dependency parser to identify the opinion holders and targets in Chinese news text. (Choi et al., 2005) use Conditional Random Fields to identify the sources of opinions from the sentences. (Kobayashi et al., 2005) propose a learning based anaphora resolution technique to extract the opinion tuple &lt; Subject, Attribute, Value &gt;. Opinion summarization has been another important aspect (Kim et al., 2013)</context>
</contexts>
<marker>Hu, Liu, 2006</marker>
<rawString>Minqing Hu and Bing Liu. 2006. Opinion extraction and summarization on the web. In AAAI, volume 7, pages 1621–1624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic detection of opinion bearing words and sentences.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<volume>5</volume>
<contexts>
<context position="9325" citStr="Kim and Hovy, 2005" startWordPosition="1445" endWordPosition="1448">he probability for each sentence being opinionated. The classifier is trained on 70 News articles from politics domain, sentences of which were marked by a group of annotators as being opinionated or factual. Each sentence was marked by two annotators. The inter-annotator agreement using Cohen’s kappa coefficient was found to be 0.71. The features utilized for the classifier are detailed in Table 1. These features were adapted from those reported in (Qadir, 2009; Yu and Hatzivassiloglou, 2003). A list of positive and negative polar words, further expanded using wordnet synsets was taken from (Kim and Hovy, 2005). Stanford dependency parser (De Marneffe et al., 2006) was utilized to compute the dependencies for each sentence within the news article. After the features are extracted from the sentences, we used the Weka implementation of Naive Bayes to train the classifier1. Table 1: Features List for the Naive Bayes Classifier Count of positive polar words Count of negative polar words Polarity of the root verb of the sentence Presence of aComp, xComp and advMod dependencies in the sentence 3.2 HITS The Naive Bayes classifier as discussed in Section 3.1 utilizes only the local features within a sentenc</context>
</contexts>
<marker>Kim, Hovy, 2005</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2005. Automatic detection of opinion bearing words and sentences. In Proceedings of IJCNLP, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Sentiment and Subjectivity in Text,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5058" citStr="Kim and Hovy, 2006" startWordPosition="777" endWordPosition="780">with error analysis are carried out in Section 5. Conclusions and future work are detailed in Section 6. 2 Related Work Opinion mining has drawn a lot of attention in recent years. Research works have focused on mining 25 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 25–33, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics opinions from various information sources such as blogs (Conrad and Schilder, 2007; Harb et al., 2008), product reviews (Hu and Liu, 2004; Qadir, 2009; Dave et al., 2003), news articles (Kim and Hovy, 2006; Hu and Liu, 2006) etc. Various aspects in opinion mining have been explored over the years (Ku et al., 2006). One important dimension is to identify the opinion holders as well as opinion targets. (Lu, 2010) used dependency parser to identify the opinion holders and targets in Chinese news text. (Choi et al., 2005) use Conditional Random Fields to identify the sources of opinions from the sentences. (Kobayashi et al., 2005) propose a learning based anaphora resolution technique to extract the opinion tuple &lt; Subject, Attribute, Value &gt;. Opinion summarization has been another important aspect</context>
<context position="6636" citStr="Kim and Hovy, 2006" startWordPosition="1025" endWordPosition="1028">ces itself remains a major obstacle. Our work mainly focus on classifying a sentence in a news article as opinionated or factual. There have been works on sentiment classification (Wiebe and Riloff, 2005) but the task of finding opinionated sentences is different from finding sentiments, because sentiments mainly convey the emotions and not the opinions. There has been research on finding opinionated sentences from various information sources. Some of these works utilize a dictionary-based (Fei et al., 2012) or regular pattern based (Brun, 2012) approach to identify aspects in the sentences. (Kim and Hovy, 2006) utilize the presence of a single strong valence wors as well as the total valence score of all words in a sentence to identify opinion-bearing sentences. (Zhai et al., 2011) work on finding ‘evaluative’ sentences in online discussions. They exploit the inter-relationship of aspects, evaluation words and emotion words to reinforce each other. Thus, while ours is not the first attempt at opinion extraction from news articles, to the best of our knowledge, none of the previous works has exploited the global structure of a news article to classify a sentence as opinionated/factual. Though summari</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyun Duk Kim</author>
<author>Malu Castellanos</author>
<author>Meichun Hsu</author>
<author>ChengXiang Zhai</author>
<author>Umeshwar Dayal</author>
<author>Riddhiman Ghosh</author>
</authors>
<title>Compact explanatory opinion summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management,</booktitle>
<pages>1697--1702</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5677" citStr="Kim et al., 2013" startWordPosition="876" endWordPosition="879">Hu and Liu, 2006) etc. Various aspects in opinion mining have been explored over the years (Ku et al., 2006). One important dimension is to identify the opinion holders as well as opinion targets. (Lu, 2010) used dependency parser to identify the opinion holders and targets in Chinese news text. (Choi et al., 2005) use Conditional Random Fields to identify the sources of opinions from the sentences. (Kobayashi et al., 2005) propose a learning based anaphora resolution technique to extract the opinion tuple &lt; Subject, Attribute, Value &gt;. Opinion summarization has been another important aspect (Kim et al., 2013). A lot of research work has been done for opinion mining from product reviews where most of the text is opinion-rich. Opinion mining from news articles, however, poses its own challenges because in contrast with the product reviews, not all parts of news articles present opinions (Balahur et al., 2013) and thus finding opinionated sentences itself remains a major obstacle. Our work mainly focus on classifying a sentence in a news article as opinionated or factual. There have been works on sentiment classification (Wiebe and Riloff, 2005) but the task of finding opinionated sentences is differ</context>
</contexts>
<marker>Kim, Castellanos, Hsu, Zhai, Dayal, Ghosh, 2013</marker>
<rawString>Hyun Duk Kim, Malu Castellanos, Meichun Hsu, ChengXiang Zhai, Umeshwar Dayal, and Riddhiman Ghosh. 2013. Compact explanatory opinion summarization. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management, pages 1697–1702. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Ryu Iida</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Opinion extraction using a learning-based anaphora resolution technique.</title>
<date>2005</date>
<booktitle>In The Second International Joint Conference on Natural Language Processing (IJCNLP), Companion Volume to the Proceeding of Conference including Posters/Demos and Tutorial Abstracts.</booktitle>
<contexts>
<context position="5487" citStr="Kobayashi et al., 2005" startWordPosition="848" endWordPosition="851">om various information sources such as blogs (Conrad and Schilder, 2007; Harb et al., 2008), product reviews (Hu and Liu, 2004; Qadir, 2009; Dave et al., 2003), news articles (Kim and Hovy, 2006; Hu and Liu, 2006) etc. Various aspects in opinion mining have been explored over the years (Ku et al., 2006). One important dimension is to identify the opinion holders as well as opinion targets. (Lu, 2010) used dependency parser to identify the opinion holders and targets in Chinese news text. (Choi et al., 2005) use Conditional Random Fields to identify the sources of opinions from the sentences. (Kobayashi et al., 2005) propose a learning based anaphora resolution technique to extract the opinion tuple &lt; Subject, Attribute, Value &gt;. Opinion summarization has been another important aspect (Kim et al., 2013). A lot of research work has been done for opinion mining from product reviews where most of the text is opinion-rich. Opinion mining from news articles, however, poses its own challenges because in contrast with the product reviews, not all parts of news articles present opinions (Balahur et al., 2013) and thus finding opinionated sentences itself remains a major obstacle. Our work mainly focus on classify</context>
</contexts>
<marker>Kobayashi, Iida, Inui, Matsumoto, 2005</marker>
<rawString>Nozomi Kobayashi, Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2005. Opinion extraction using a learning-based anaphora resolution technique. In The Second International Joint Conference on Natural Language Processing (IJCNLP), Companion Volume to the Proceeding of Conference including Posters/Demos and Tutorial Abstracts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lun-Wei Ku</author>
<author>Yu-Ting Liang</author>
<author>Hsin-Hsi Chen</author>
</authors>
<title>Opinion extraction, summarization and tracking in news and blog corpora. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs,</title>
<date>2006</date>
<volume>volume</volume>
<pages>100107</pages>
<contexts>
<context position="5168" citStr="Ku et al., 2006" startWordPosition="797" endWordPosition="800">ed Work Opinion mining has drawn a lot of attention in recent years. Research works have focused on mining 25 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 25–33, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics opinions from various information sources such as blogs (Conrad and Schilder, 2007; Harb et al., 2008), product reviews (Hu and Liu, 2004; Qadir, 2009; Dave et al., 2003), news articles (Kim and Hovy, 2006; Hu and Liu, 2006) etc. Various aspects in opinion mining have been explored over the years (Ku et al., 2006). One important dimension is to identify the opinion holders as well as opinion targets. (Lu, 2010) used dependency parser to identify the opinion holders and targets in Chinese news text. (Choi et al., 2005) use Conditional Random Fields to identify the sources of opinions from the sentences. (Kobayashi et al., 2005) propose a learning based anaphora resolution technique to extract the opinion tuple &lt; Subject, Attribute, Value &gt;. Opinion summarization has been another important aspect (Kim et al., 2013). A lot of research work has been done for opinion mining from product reviews where most o</context>
</contexts>
<marker>Ku, Liang, Chen, 2006</marker>
<rawString>Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen. 2006. Opinion extraction, summarization and tracking in news and blog corpora. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, volume 100107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longzhuang Li</author>
<author>Yi Shang</author>
<author>Wei Zhang</author>
</authors>
<title>Improvement of hits-based algorithms on web documents.</title>
<date>2002</date>
<booktitle>In Proceedings of the 11th international conference on World Wide Web,</booktitle>
<pages>527--535</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="11687" citStr="Li et al., 2002" startWordPosition="1825" endWordPosition="1828">tion of HITS parameters, let us give the notations. Let us denote a document D using a set of sentences {S1, S2, ... , Si, ... , Sn}, where n corresponds to the number of sentences in the document D. We construct the sentence graph where nodes in the graph correspond to the sentences in the document. Let Hi and Ai denote the hub and authority scores for sentence Si. In HITS, the edges always flow from a Hub to an Authority. In the original HITS algorithm, each edge is given the same weight. However, it has been reported that using weights in HITS update improves the performance significantly (Li et al., 2002). In our formulation, since each node has a non-zero probablility of acting as a hub as well as an authority, we have outgoing as well as incoming edges for every node. Therefore, the weights are assigned, keeping in mind the proximity between sentences as well as the probability (of being opinionated/factual) assigned by the classifier. The following criteria were used for deciding the weight function. • An edge in the HITS graph goes from a hub (source node) to an authority (target node). So, the edge weight from a source node to a target node should be higher if the source node has a high h</context>
</contexts>
<marker>Li, Shang, Zhang, 2002</marker>
<rawString>Longzhuang Li, Yi Shang, and Wei Zhang. 2002. Improvement of hits-based algorithms on web documents. In Proceedings of the 11th international conference on World Wide Web, pages 527–535. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Lu</author>
</authors>
<title>Identifying opinion holders and targets with dependency parser in chinese news texts.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL. of</booktitle>
<pages>129--136</pages>
<contexts>
<context position="5267" citStr="Lu, 2010" startWordPosition="815" endWordPosition="816"> 25 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 25–33, October 29, 2014, Doha, Qatar. c�2014 Association for Computational Linguistics opinions from various information sources such as blogs (Conrad and Schilder, 2007; Harb et al., 2008), product reviews (Hu and Liu, 2004; Qadir, 2009; Dave et al., 2003), news articles (Kim and Hovy, 2006; Hu and Liu, 2006) etc. Various aspects in opinion mining have been explored over the years (Ku et al., 2006). One important dimension is to identify the opinion holders as well as opinion targets. (Lu, 2010) used dependency parser to identify the opinion holders and targets in Chinese news text. (Choi et al., 2005) use Conditional Random Fields to identify the sources of opinions from the sentences. (Kobayashi et al., 2005) propose a learning based anaphora resolution technique to extract the opinion tuple &lt; Subject, Attribute, Value &gt;. Opinion summarization has been another important aspect (Kim et al., 2013). A lot of research work has been done for opinion mining from product reviews where most of the text is opinion-rich. Opinion mining from news articles, however, poses its own challenges be</context>
</contexts>
<marker>Lu, 2010</marker>
<rawString>Bin Lu. 2010. Identifying opinion holders and targets with dependency parser in chinese news texts. In Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL. of opinion sentences. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing, EMNLP ’03, pages 129–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongwu Zhai</author>
<author>Bing Liu</author>
<author>Lei Zhang</author>
<author>Hua Xu</author>
<author>Peifa Jia</author>
</authors>
<title>Identifying evaluative sentences in online discussions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6810" citStr="Zhai et al., 2011" startWordPosition="1055" endWordPosition="1058">tion (Wiebe and Riloff, 2005) but the task of finding opinionated sentences is different from finding sentiments, because sentiments mainly convey the emotions and not the opinions. There has been research on finding opinionated sentences from various information sources. Some of these works utilize a dictionary-based (Fei et al., 2012) or regular pattern based (Brun, 2012) approach to identify aspects in the sentences. (Kim and Hovy, 2006) utilize the presence of a single strong valence wors as well as the total valence score of all words in a sentence to identify opinion-bearing sentences. (Zhai et al., 2011) work on finding ‘evaluative’ sentences in online discussions. They exploit the inter-relationship of aspects, evaluation words and emotion words to reinforce each other. Thus, while ours is not the first attempt at opinion extraction from news articles, to the best of our knowledge, none of the previous works has exploited the global structure of a news article to classify a sentence as opinionated/factual. Though summarization algorithms (Erkan and Radev, 2004; Goyal et al., 2013) utilize the similarity between sentences in an article to find the important sentences, our formulation is diffe</context>
</contexts>
<marker>Zhai, Liu, Zhang, Xu, Jia, 2011</marker>
<rawString>Zhongwu Zhai, Bing Liu, Lei Zhang, Hua Xu, and Peifa Jia. 2011. Identifying evaluative sentences in online discussions. In Proceedings of the Twenty-Fifth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001684">
<title confidence="0.88972625">
Character-Cluster-Based Segmentation using Monolingual and
Bilingual Information for Statistical Machine Translation
Vipas Sutantayawalee Peerachet Porkeaw Thepchai Supnithi
Prachya Boonkwan Sitthaa Phaholphinyo
</title>
<author confidence="0.465902">
National Electronics and Computer Technology Center, Thailand
</author>
<affiliation confidence="0.198814">
{vipas.sutantayawalee, peerachet.porkeaw,prachya.boonkwan,
</affiliation>
<email confidence="0.93472">
sitthaa.phaholphinyo,thepchai}@nectec.or.th
</email>
<sectionHeader confidence="0.991537" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991176470588">
We present a novel segmentation approach for Phrase-Based Statistical Machine Translation
(PB-SMT) to languages where word boundaries are not obviously marked by using both
monolingual and bilingual information and demonstrate that (1) unsegmented corpus is able to
provide the nearly identical result compares to manually segmented corpus in PB-SMT task
when a good heuristic character clustering algorithm is applied on it, (2) the performance of
PB-SMT task has significantly increased when bilingual information are used on top of
monolingual segmented result. Our technique, instead of focusing on word separation, mainly
concentrate on character clustering. First, we cluster each character from the unsegmented
monolingual corpus by employing character co-occurrence statistics and orthographic insight.
Secondly, we enhance the segmented result by incorporating the bilingual information which
are character cluster alignment, co-occurrence frequency and alignment confidence into that
result. We evaluate the effectiveness of our method on PB-SMT task using English-Thai
language pair and report the best improvement of 8.1% increase in BLEU score. There are two
main advantages of our approach. First, our method requires less effort on developing the
corpus and can be applied to unsegmented corpus or poor-quality manually segmented corpus.
Second, this technique does not only limited to specific language pair but also capable of
automatically adjust the character cluster boundaries to be suitable for other language pairs.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.944359666666667">
Nowadays, it is admitted that word segmentation is a crucial part of Statistical Machine Translation
(SMT) especially in the languages where there are no explicit word boundaries such as Chinese,
Japanese or Thai. The writing system of these languages allow each word can be written continuously
with no space appearing between words. Consequently, word ambiguities will arise if word boundary
has been misplace which finally lead to an incorrect translation. Thus, the effective word segmentator
is required to disambiguate each word separator before processing another task in SMT. Several word
segmentators which focusing on word, character [1] or both [2] and [3] have been implemented to
accomplish this goal.
In order to retrieve a useful information to segment or cluster the word, most of word
segmentators are trained on a manually segmented monolingual corpus by using various approaches
such as dictionary-based, Hidden Markov Model (HMM), support vector machine (SVM) or
conditional random field (CRF). Although, a number of segementators are able to yield very
promising results, certain of them might be unsuitable for SMT task due to the influence of
segmentation scheme [4]. Therefore, instead of solely rely on monolingual corpus, various researches
make use of either manually segmented [4] or unsegment1ed bilingual corpus [5] as a guideline
information to perform a word segmentation task and improve the performance of SMT system.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/
</bodyText>
<page confidence="0.993603">
94
</page>
<note confidence="0.6966105">
Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 94–101,
Dublin, Ireland, August 23-29 2014.
</note>
<bodyText confidence="0.9998523125">
In this paper, we propose a novel segmentation approach for Phrase-Based Statistical Machine
Translation (PB-SMT) to languages where word boundaries are not obviously marked by using both
monolingual and bilingual information on English-Thai language pair and demonstrate that (1)
unsegmented corpus is able to provide the nearly identical result to manually segmented corpus in PB-
SMT task when the good heuristics character clustering algorithm is applied on it, (2) the performance
of PB-SMT task has significantly increased when bilingual information are used on top of
monolingual segmented result. Our technique, instead of focusing on word separation, mainly
concentrate on character clustering. First, we cluster each character from the unsegmented
monolingual corpus by employing heuristic algorithm and language insight. Secondly, we enhance the
segmented result by incorporating the bilingual information which are character cluster (CC)
alignment, CC co-occurrence frequency and alignment confidence into that result. These two tasks can
be performed repeatedly.
The remainder of this paper is organized as follows. Section 2 provides some information related
to our work. Section 3 describes the methodology of our approach. Section 4 present the experiments
setting. Section 5 present the experimental results and empirical analysis. Section 6 and 7 gives a
conclusion and future work respectively.
</bodyText>
<sectionHeader confidence="0.999934" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999157">
2.1 Thai Character Clustering
</subsectionHeader>
<bodyText confidence="0.999972916666667">
In Thai writing system, there are no explicit word boundaries as in English, and a single Thai character
does not have specific meanings like Chinese, Japanese and Korean. Thai characters could be
consonants, vowels and tone marks and a word can be formed by combining these characters. From
our observation, we found that the average length of Thai words on BEST2010 corpus (National
Electronics and Computer Technology Center, Thailand 2010) is 3.855. This makes the search space
of Thai word segmentation very large.
To alleviate this issue, the notion of Thai character cluster (TCC), is introduced [1] to reduce the
search space with predetermined unambiguious constraints for cluster formation. A cluster may not be
meaningful and has to combine with other consecutive clusters to form a word. Characters in the
cluster cannot be separated according to the Thai orthographic rules. For example, a vowel and tone
mark cannot stand alone and a tone marker is always required to be placed next to a previous character
only. [6] applied TCC to word segmentation technique which yields an interesting result.
</bodyText>
<subsectionHeader confidence="0.999568">
2.2 Bilingually Word Segmentation
</subsectionHeader>
<bodyText confidence="0.999949">
Bilingual information has also been shown beneficial for word segmentation. Several methods have
used the information from bilingual corpora to perform word segmentation. As in [5], it focuses on
unsegmented bilingual corpus and builds a self-learned dictionary using alignment statistics between
English and Chinese language pair. On the other hands, [4] is based on the manually segmented
bilingual corpus and then try to “repack” the word from existing alignment by using alignment
confidence. Both works evaluated the performance in BLEU metric and reported the promising result
of PB-SMT task.
</bodyText>
<sectionHeader confidence="0.998559" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.999984571428571">
This paper aim to compare translation quality based on SMT task between the systems trained on
bilingual corpus that contains both segmented source and target, and on the same bilingual corpus with
segmented source but unsegmented target. First, we make use of monolingual information by
employing several character cluster algorithms on unsegmented data. Second, we use bilingual-guided
alignment information retrieved from alignment extraction process for improving character cluster
segmentation. Then, we evaluate our performance based on translation accuracy by using BLEU
metric. We want to prove that (1) the result of PB-SMT task using unsegmented corpus (unsupervised)
</bodyText>
<page confidence="0.993827">
95
</page>
<bodyText confidence="0.9982635">
is nearly identical result to manually segmented (supervised) data and (2) when bilingual information
are also applied, the performance of PB-SMT is also improved.
</bodyText>
<subsectionHeader confidence="0.990893">
3.1 Notation
</subsectionHeader>
<bodyText confidence="0.9928305">
Given a target 𝑇ℎ𝑎𝑖 sentence 𝑡i consisting of 𝐽 clusters 𝑡l, . .., 𝑡I , where 𝑡I ≥ 1. If 𝑡I = 1, we
call 𝑡las a single character 𝑆. Otherwise, we call is as a character cluster 𝑇 . In addition, given a
English sentence 𝑒l consisting of 𝐼 words 𝑒, ... , 𝑒i , 𝐴E→T denotes a set of English-to-Thai language
word alignments between 𝑒l and 𝑡l. In addition, since we concentrate on one-to-many alignments,
𝐴E→T , can be rewritten as a set of pairs 𝑎i and 𝑎i = &lt; 𝑒i, 𝑡I &gt; noting a link between one single
English word and several Thai characters that are formed to one cluster 𝑇
</bodyText>
<subsectionHeader confidence="0.998819">
3.2 Monolingual Information
</subsectionHeader>
<bodyText confidence="0.999994333333333">
Due to the issue mentioned in section 2.1, we apply character clustering (CC) technique on target text
in order to reduce the search space. After performing CC, it will yield several character clusters
𝑇which can be grouped together to obtain a larger unit which approaches the notion of word.
However, for Thai and Lao, we do not only receive 𝑇 but also 𝑆 which usually has no meaning by
itself. Moreover, Thai, Burmese and Lao writing rule does not allow 𝑆 to stand alone in most case.
Thus, we are required to develop various adapted versions of CC by using orthographic insight and
heuristic algorithm to automatically pack the characters that reside in a pre-defined grammatical word
list handcrafted by linguists. Then, all of single consonants in Thai Burmese, and Lao are forced to
group with either left or right cluster due to the Thai writing system. The decision has been made by
consulting on character co-occurrence statistics (unigram and bigram frequency).
Eventually, we obtain different character cluster alignments from the system trained on various CC
approaches which effect to translation quality as shown in section 5.1
</bodyText>
<subsectionHeader confidence="0.999557">
3.3 Bilingually-Guided Alignment Information
</subsectionHeader>
<bodyText confidence="0.9992755">
We begin with the sequence of small clusters resulting from previous character clustering process.
These small clusters can be grouped together in order to form “word” using bilingually-guided
alignment information. Generally, small consecutive clusters in target side which are aligned to the
same word in source data should be grouped together. Therefore, this section describes our one-to-
many alignment extraction process.
For one-to-many alignment, we applied processes similar to those in phrase extraction
algorithm [7] which is described as follows.
With English sentence 𝑒l and a Thai character cluster 𝑇i, we apply IBM model 1-5 to extract
word-to-cluster translation probability of source-to-target 𝑃(𝑡|𝑒) and target-to-source 𝑃(𝑒|𝑡). Next,
the alignment points which have the highest probability are greedily selected from both 𝑃(𝑡|𝑒) and
𝑃(𝑒 |𝑡). Figure 1.a and 1.b show examples of alignment points of source-to-target and target-to-source
respectively. After that we selected the intersection of alignment pairs from both side. Then, additional
alignment points are added according to the growing heuristic algorithm (grow additional alignment
points, [8])
</bodyText>
<figure confidence="0.917602">
(a) (b)
96
(c) (d)
</figure>
<figureCaption confidence="0.998243">
Figure 1. The process of one-to-many alignment extraction (a) Source-to-Target word alignment (b) Target-to-Source word
alignment (c) Intersection between (a) and (b). (d) Result of (c) after applying the growing heuristic algorithm.
</figureCaption>
<bodyText confidence="0.9969475">
Finally, we select consecutive clusters which are aligned to the same English word as candidates.
From the Figure 1.d, we obtain these candidates (red, สีแou) and (bicycle, 5n 5 en u).
</bodyText>
<subsectionHeader confidence="0.907236">
3.4 Character Cluster Repacking
</subsectionHeader>
<bodyText confidence="0.99750775">
Although the alignment information obtained from the previous step is very helpful for the PB-SMT
task, there is still plenty of room to enhance the PB-SMT performance. One way of doing that is by
using word repacking [4]. However, in this paper, we perform a character cluster repacking (CCR)
instead of word. The main purpose of repacking technique is to group all small consecutive clusters
(or word) in target side that frequently align with one word in source data. Repacking approaches uses
two simple calculations which are a co-occurrence frequency (COOC (𝑒i, 𝑇i)) and alignment
confidence (𝐴𝐶( 𝑎i)). (COOC (𝑒i, 𝑇i)) is the number of times 𝑒i and 𝑇i co-occurr in the bilingual
corpus [4] [9] and 𝐴𝐶( 𝑎i) is a measure of how often the aligner aligns 𝑒i and 𝑇L when they co-occur.
</bodyText>
<equation confidence="0.924339333333333">
AC is defined as
𝐴𝐶(𝑎i) _ 𝐶(𝑎i)
COOC (𝑒i, 𝑇L)
</equation>
<bodyText confidence="0.99213725">
where 𝐶(𝑎i) denotes the number of alignments suggested by the previous-step word aligner.
Unfortunately, due to the limited memory in our experiment machine, we cannot
find COOC (𝑒i, 𝑇L)) for all possible &lt; 𝑒i, 𝑇L &gt; pairs. We, therefore, slightly modified the above
equation by finding 𝐶(𝑎i) first. Secondly, we begin searching COOC (𝑒i, 𝑇i)) from all possible
alignments in 𝑎i instead of finding all occurrences in corpus. By applying this modification, we
eliminate &lt; 𝑒i, 𝑇L &gt; pairs that co-occur together but never align to each other by previous-step aligner
(𝐴𝐶 𝑎i equals to zero) so as to reduce the search space and complexity in our algorithm. Thirdly, we
choose 𝑎i with the highest 𝐴𝐶(𝑎i) and repack all character clusters in target side that similar to 𝑇L to
be a new single cluster unit. This process can be done repeatedly. However, we have run this task less
than twice since there are few new cluster unit appear after two iterations have passed. The running
example of this algorithm is described as follows
Suppose previous step aligner (GIZA++) produce two alignments 𝑎1 = &lt; 𝑒1, 𝑇1,2 &gt; and 𝑎Z = &lt;
𝑒1, 𝑇1,2,3 &gt; CCR will find the frequency of each aligment and number of times 𝑒i and 𝑇L co-occurr in
the bilingual corpus ( COOC 𝑒l, 𝑇1,2 and COOC 𝑒l, 𝑇1,2,3 ). Then, we will have 𝐴𝐶(𝑎i) score for
each alignment and the aligment with the highest 𝐴𝐶 will be selected. The CCR will group these
cluster ( e.g. 𝑇1,2 ) to be a new single cluster unit.
</bodyText>
<page confidence="0.999263">
97
</page>
<sectionHeader confidence="0.996635" genericHeader="method">
4 Experimental Setting
</sectionHeader>
<subsectionHeader confidence="0.954062">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.994057666666667">
The bilingual corpus1 we used in our experiment is constructed from several sources and consists of
multiple domains (e.g news, travel, article, entertainment, computer, etc.). We divided this corpus into
three sets plus one additional test set as shown below
</bodyText>
<table confidence="0.9994252">
Data Set No. of sentence pairs
Train 633,589
Dev 12,568
Test #1 3,426
Test #2 500
</table>
<tableCaption confidence="0.999953">
Table 1. Information of bilingual corpus
</tableCaption>
<subsectionHeader confidence="0.9749">
4.2 Tools and Evaluation
</subsectionHeader>
<bodyText confidence="0.999983764705882">
We evaluate our system in term of translation quality based on phrase-based SMT. Source
sentences are sequence of English words while target sentences are sequences of Thai character
clusters and each cluster size depends on which approach used in the experiment.
Translation model and language model are train based on the standard phrase-based SMT.
Alignments of source (English word) and target (Thai Character Cluster) are extracted using GIZA++
[8] and the phrase extraction algorithm [7] is applied using Moses SMT package. We apply SRILM
[10] to train the 3-gram language model of target side. We use the default parameter settings for
decoding.
In testing process, we use another two test sets difference to the training data. Then we compared
the translation result with the reference in term of BLEU score instead of F-score because of two main
reasons. First, it is cumbersome to construct a reliable gold standard since their annotation schemes are
different. Second, there is no strong correlation with SMT translation quality in terms of BLEU score
[11]. Therefore, we re-segment the reference data (manually segmented) and the translation result data
based on TCC. Some may concern about using TCC will lead to over estimation (higher than actual)
due to the BLEU score is design based on word and not based on character. However, we used this
BLEU score only for comparing translation quality among our experiments. Comparing to other SMT
system still require running BLEU score based on the same segmentation guideline.
</bodyText>
<sectionHeader confidence="0.999477" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999924833333333">
We conducted all experiments on PB-SMT task and reported the performance of PB-SMT system
based on the BLEU measure. First, we use a method proposed in section 3.2 followed by the approach
in section 3.3 in order to the receive first translation result set (without CCR). Then, we perform a
method describe in 3.4 and also follow by approach in section 3.3 in order to receive another
translation result set (with CCR). Table 1 shows the number of character clusters that are decreasing
over time when several different character clustering approaches are applied.
</bodyText>
<footnote confidence="0.5609545">
1 Currently, the corpus we used is a proprietary of NECTEC and does not available to public yet due to the licensing issue.
However, for the educational purpose, this corpus is available upon by request.
</footnote>
<page confidence="0.990439">
98
</page>
<table confidence="0.999673181818182">
No. of Character Clusters (or word in original data)
Approaches Without CCR With CCR
TCC 9,862,271 7,187,862
(baseline)
TCC with language insight 8,953,437 6,636,305
(TCC-FN)
TCC with language insight 6,545,617 5,448,437
and heuristic algorithm
(TCC-FN-B)
Manually segmented corpus 5,311,648 N/A
(Upper bound)
</table>
<tableCaption confidence="0.999813">
Table 2. Number of character clusters when different character clustering approaches are applied on the bilingual corpus
</tableCaption>
<bodyText confidence="0.8801022">
Next, we present all translation results of PB-SMT task that using different character clustering
approaches. Each training set is trained with only one character clustering method which are (1) TCC
(baseline), (2) TCC with CCR, (3) TCC with only orthographic insight (TCC-FN), (4) TCC-Fn with
CCR, (5) TCC with language insight and heuristic algorithm (TCC-FN-B) and (6) TCC-FN-B with
CCR. The results are shown in Table 3.
</bodyText>
<table confidence="0.999435875">
Test #1 BLEU % of BLEU Test #2 BLEU % of BLEU
Improvement Improvement
Approaches Without With CCR Without With CCR
CCR CCR
Baseline 37.12 40.13 8.11 36.78 38.87 5.68
TCC-FN 40.23 41.90 4.15 38.36 39.09 1.90
TCC-FN-B 44.69 44.43 -0.58 40.45 40.81 0.89
Upper bound 47.04 N/A N/A 40.73 N/A N/A
</table>
<tableCaption confidence="0.9776215">
Table 3. BLEU score of each character clustering method
and the percentage of the improvement when we applied CCR to the data
</tableCaption>
<figure confidence="0.675683">
(a)
48
47
</figure>
<page confidence="0.9520295">
46
45
44
43
42
41
40
39
38
37
</page>
<figure confidence="0.963478">
No CCR
With CCR
Upperbound
36
35
Upperbound
99
(b)
</figure>
<figureCaption confidence="0.999995">
Figure 2. The BLEU score of (a) test set no.1 and (b) test set no.2
</figureCaption>
<bodyText confidence="0.999976666666667">
As seen from Table 3, when we apply the enhanced version of TCCs into the data with no CCR,
BLEU score have gradually increased and almost reached the same level as original in test set #2.
Furthermore, when CCR have been also deployed on each training dataset, the results of BLEU are
also rise in the same manner with Without CCR method. There are certain significant points that
should be noticed. First, CCR method is able to yield maximum of 8.1 % BLEU score increase.
Second, when we apply the CCR methods and reach at some point, few improvement or minor
degradation is received as shown in TCC-FN-B without and with CCR result. This is because the
number of clusters produced by this character clustering algorithm is almost equal to number of words
in original data as shown in Table 2 and this approach might suffer from the word boundary
misplacement problem. Third, character clustering that use TCC with orthographic insight and
heuristic algorithm combined with CCR approach is able to overcome the translation result from
original data for the first time.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9983362">
In this paper, we introduce a new approach for performing word segmentation task for SMT. Instead
of starting with word level, we focus on character cluster level because this approach can perform on
unsegmented corpus or multiple-guideline manually segmented corpus. First, we apply several
adapted versions of TCC on unsegmented data. Next, we use a bilingual corpus to find alignment
information for all &lt; 𝑒i, 𝑇L &gt; pairs and then employ character cluster repacking method in order to
form the large cluster of Thai characters.
We evaluate our approach on translation task on several sources and different domain corpus
and report the result in BLEU metric. Our technique demonstrates that (1) we can achieve a
dramatically improvement of BLUE as of 8.1% when we apply adapted TCC with CCR and (2) it is
possible to overcome the manually segmented corpus by using TCC with orthographic insight and
heuristic algorithm character clustering method combined with CCR. The advantage of our approach
is a reduction in time and effot for construct a billinugal corpus because we are no longer required to
manually segment all sentences in target side. In addition, our approach is able to cope with larger data
information (e.g. 1 million sentences pairs) and adaptable to other language pairs (e.g. English-
Chinese, English-Japanese or English-Lao)
</bodyText>
<figure confidence="0.978602833333333">
36
35
Baseline TCCAFn TCCAFnB
42
41
40
39
38
37
No CCR
With CCR
Upperbound
</figure>
<page confidence="0.894277">
100
</page>
<sectionHeader confidence="0.997887" genericHeader="acknowledgments">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999986666666667">
There are some tasks that can be added into this approaches. Firstly, we can make use of trigram (and
n-gram) statistics, maximum entropy or conditional random field on heuristic algorithm in adapted
version of TCC. Secondly, we might report the result from another language pair in order to confirm
our approach.Thirdly, we can modify CCR process to be able to rerank the alignment confidence by
using discriminative approach. Lastly, name entity recognition system can be integrated with our
approach in order to improve the SMT performance.
</bodyText>
<sectionHeader confidence="0.99284" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.999971379310345">
[1] T. Teeramunkong, V. Sornlertlamvanich, T. Tanhermhong and W. Chinnan, “Character cluster based Thai
information retrieval,” in IRAL &apos;00 Proceedings of the fifth international workshop on on Information
retrieval with Asian languages, 2000.
[2] C. Kruengkrai, K. Uchimoto, J. Kazama, K. Torisawa, H. Isahara and C. Jaruskulchai, “A Word and
Character-Cluster Hybrid Model for Thai Word Segmentation,” in Eighth International Symposium on
Natural Lanugage Processing, Bangkok, Thailand, 2009.
[3] Y. Liu, W. Che and T. Liu, “Enhancing Chinese Word Segmentation with Character Clustering,” in Chinese
Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data,
China, 2013.
[4] Y. Ma and A. Way, “Bilingually motivated domain-adapted word segmentation for statistical machine
translation,” in Proceeding EACL &apos;09 Proceedings of the 12th Conference of the European Chapter of the
Association for Computational Linguistics, pp. 549-557, Stroudsburg, PA, USA, 2009.
[5] J. Xu, R. Zens and H. Ney, “Do We Need Chinese Word Segmentation for Statistical Machine
Translation?,” ACL SIGHAN Workshop 2004, pp. 122-129, 2004.
[6] P. Limcharoen, C. Nattee and T. Theeramunkong, “Thai Word Segmentation based-on GLR Parsing
Technique and Word N-gram Model,” in Eighth International Symposium on Natural Lanugage
Processing, Bangkok, Thailand, 2009.
[7] P. Koehn, F. J. Och and D. Marcu, “Statistical phrase-based translation,” in NAACL &apos;03 Proceedings of the
2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology, Stroudsburg, PA, USA, 2003.
[8] F. J. Och and H. Ney, “A systematic comparison of various statistical alignment models,” Computational
Linguistics, vol. 29, no. 1, pp. 19-51, 2003.
[9] I. D. Melamed, “Models of translational equivalence among words,” Computational Linguistics, vol. 26, no.
2, pp. 221-249, 2000.
[10] “SRILM -- An extensible language modeling toolkit,” in Proceeding of the International Conference on
Spoken Language Processing, 2002.
[11] P.-C. Chang, M. Galley and C. D. Manning, “Optimizing Chinese word segmentation for machine
translation performance,” in Proceedings of the Third Workshop on Statistical Machine Translation,
Columbus, Ohio, 2008.
</reference>
<page confidence="0.998608">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.370227">
<title confidence="0.848383666666667">Character-Cluster-Based Segmentation using Monolingual Bilingual Information for Statistical Machine Translation Vipas Sutantayawalee Peerachet Porkeaw Thepchai Supnithi</title>
<author confidence="0.747431">Prachya Boonkwan Sitthaa Phaholphinyo</author>
<affiliation confidence="0.85486">National Electronics and Computer Technology Center, Thailand</affiliation>
<email confidence="0.8990945">vipas.sutantayawalee@nectec.or.th</email>
<email confidence="0.8990945">sitthaa.phaholphinyo@nectec.or.th</email>
<email confidence="0.8990945">thepchai@nectec.or.th</email>
<abstract confidence="0.999196388888889">We present a novel segmentation approach for Phrase-Based Statistical Machine Translation (PB-SMT) to languages where word boundaries are not obviously marked by using both monolingual and bilingual information and demonstrate that (1) unsegmented corpus is able to provide the nearly identical result compares to manually segmented corpus in PB-SMT task when a good heuristic character clustering algorithm is applied on it, (2) the performance of PB-SMT task has significantly increased when bilingual information are used on top of monolingual segmented result. Our technique, instead of focusing on word separation, mainly concentrate on character clustering. First, we cluster each character from the unsegmented monolingual corpus by employing character co-occurrence statistics and orthographic insight. Secondly, we enhance the segmented result by incorporating the bilingual information which are character cluster alignment, co-occurrence frequency and alignment confidence into that result. We evaluate the effectiveness of our method on PB-SMT task using English-Thai language pair and report the best improvement of 8.1% increase in BLEU score. There are two main advantages of our approach. First, our method requires less effort on developing the corpus and can be applied to unsegmented corpus or poor-quality manually segmented corpus. Second, this technique does not only limited to specific language pair but also capable of automatically adjust the character cluster boundaries to be suitable for other language pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Teeramunkong</author>
<author>V Sornlertlamvanich</author>
<author>T Tanhermhong</author>
<author>W Chinnan</author>
</authors>
<title>Character cluster based Thai information retrieval,”</title>
<date>2000</date>
<booktitle>in IRAL &apos;00 Proceedings of the fifth international workshop on on Information retrieval with Asian languages,</booktitle>
<contexts>
<context position="2587" citStr="[1]" startWordPosition="349" endWordPosition="349">ion is a crucial part of Statistical Machine Translation (SMT) especially in the languages where there are no explicit word boundaries such as Chinese, Japanese or Thai. The writing system of these languages allow each word can be written continuously with no space appearing between words. Consequently, word ambiguities will arise if word boundary has been misplace which finally lead to an incorrect translation. Thus, the effective word segmentator is required to disambiguate each word separator before processing another task in SMT. Several word segmentators which focusing on word, character [1] or both [2] and [3] have been implemented to accomplish this goal. In order to retrieve a useful information to segment or cluster the word, most of word segmentators are trained on a manually segmented monolingual corpus by using various approaches such as dictionary-based, Hidden Markov Model (HMM), support vector machine (SVM) or conditional random field (CRF). Although, a number of segementators are able to yield very promising results, certain of them might be unsuitable for SMT task due to the influence of segmentation scheme [4]. Therefore, instead of solely rely on monolingual corpus,</context>
<context position="5847" citStr="[1]" startWordPosition="832" endWordPosition="832">Thai writing system, there are no explicit word boundaries as in English, and a single Thai character does not have specific meanings like Chinese, Japanese and Korean. Thai characters could be consonants, vowels and tone marks and a word can be formed by combining these characters. From our observation, we found that the average length of Thai words on BEST2010 corpus (National Electronics and Computer Technology Center, Thailand 2010) is 3.855. This makes the search space of Thai word segmentation very large. To alleviate this issue, the notion of Thai character cluster (TCC), is introduced [1] to reduce the search space with predetermined unambiguious constraints for cluster formation. A cluster may not be meaningful and has to combine with other consecutive clusters to form a word. Characters in the cluster cannot be separated according to the Thai orthographic rules. For example, a vowel and tone mark cannot stand alone and a tone marker is always required to be placed next to a previous character only. [6] applied TCC to word segmentation technique which yields an interesting result. 2.2 Bilingually Word Segmentation Bilingual information has also been shown beneficial for word </context>
</contexts>
<marker>[1]</marker>
<rawString>T. Teeramunkong, V. Sornlertlamvanich, T. Tanhermhong and W. Chinnan, “Character cluster based Thai information retrieval,” in IRAL &apos;00 Proceedings of the fifth international workshop on on Information retrieval with Asian languages, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kruengkrai</author>
<author>K Uchimoto</author>
<author>J Kazama</author>
<author>K Torisawa</author>
<author>H Isahara</author>
<author>C Jaruskulchai</author>
</authors>
<title>A Word and Character-Cluster Hybrid Model for Thai Word Segmentation,”</title>
<date>2009</date>
<booktitle>in Eighth International Symposium on Natural Lanugage Processing,</booktitle>
<location>Bangkok, Thailand,</location>
<contexts>
<context position="2599" citStr="[2]" startWordPosition="352" endWordPosition="352">cial part of Statistical Machine Translation (SMT) especially in the languages where there are no explicit word boundaries such as Chinese, Japanese or Thai. The writing system of these languages allow each word can be written continuously with no space appearing between words. Consequently, word ambiguities will arise if word boundary has been misplace which finally lead to an incorrect translation. Thus, the effective word segmentator is required to disambiguate each word separator before processing another task in SMT. Several word segmentators which focusing on word, character [1] or both [2] and [3] have been implemented to accomplish this goal. In order to retrieve a useful information to segment or cluster the word, most of word segmentators are trained on a manually segmented monolingual corpus by using various approaches such as dictionary-based, Hidden Markov Model (HMM), support vector machine (SVM) or conditional random field (CRF). Although, a number of segementators are able to yield very promising results, certain of them might be unsuitable for SMT task due to the influence of segmentation scheme [4]. Therefore, instead of solely rely on monolingual corpus, various res</context>
</contexts>
<marker>[2]</marker>
<rawString>C. Kruengkrai, K. Uchimoto, J. Kazama, K. Torisawa, H. Isahara and C. Jaruskulchai, “A Word and Character-Cluster Hybrid Model for Thai Word Segmentation,” in Eighth International Symposium on Natural Lanugage Processing, Bangkok, Thailand, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Liu</author>
<author>W Che</author>
<author>T Liu</author>
</authors>
<title>Enhancing Chinese Word Segmentation with Character Clustering,”</title>
<date>2013</date>
<booktitle>in Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data,</booktitle>
<location>China,</location>
<contexts>
<context position="2607" citStr="[3]" startWordPosition="354" endWordPosition="354">t of Statistical Machine Translation (SMT) especially in the languages where there are no explicit word boundaries such as Chinese, Japanese or Thai. The writing system of these languages allow each word can be written continuously with no space appearing between words. Consequently, word ambiguities will arise if word boundary has been misplace which finally lead to an incorrect translation. Thus, the effective word segmentator is required to disambiguate each word separator before processing another task in SMT. Several word segmentators which focusing on word, character [1] or both [2] and [3] have been implemented to accomplish this goal. In order to retrieve a useful information to segment or cluster the word, most of word segmentators are trained on a manually segmented monolingual corpus by using various approaches such as dictionary-based, Hidden Markov Model (HMM), support vector machine (SVM) or conditional random field (CRF). Although, a number of segementators are able to yield very promising results, certain of them might be unsuitable for SMT task due to the influence of segmentation scheme [4]. Therefore, instead of solely rely on monolingual corpus, various researches </context>
</contexts>
<marker>[3]</marker>
<rawString>Y. Liu, W. Che and T. Liu, “Enhancing Chinese Word Segmentation with Character Clustering,” in Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data, China, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Ma</author>
<author>A Way</author>
</authors>
<title>Bilingually motivated domain-adapted word segmentation for statistical machine translation,”</title>
<date>2009</date>
<booktitle>in Proceeding EACL &apos;09 Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>549--557</pages>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="3129" citStr="[4]" startWordPosition="435" endWordPosition="435">veral word segmentators which focusing on word, character [1] or both [2] and [3] have been implemented to accomplish this goal. In order to retrieve a useful information to segment or cluster the word, most of word segmentators are trained on a manually segmented monolingual corpus by using various approaches such as dictionary-based, Hidden Markov Model (HMM), support vector machine (SVM) or conditional random field (CRF). Although, a number of segementators are able to yield very promising results, certain of them might be unsuitable for SMT task due to the influence of segmentation scheme [4]. Therefore, instead of solely rely on monolingual corpus, various researches make use of either manually segmented [4] or unsegment1ed bilingual corpus [5] as a guideline information to perform a word segmentation task and improve the performance of SMT system. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 94 Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguist</context>
<context position="6740" citStr="[4]" startWordPosition="968" endWordPosition="968">ple, a vowel and tone mark cannot stand alone and a tone marker is always required to be placed next to a previous character only. [6] applied TCC to word segmentation technique which yields an interesting result. 2.2 Bilingually Word Segmentation Bilingual information has also been shown beneficial for word segmentation. Several methods have used the information from bilingual corpora to perform word segmentation. As in [5], it focuses on unsegmented bilingual corpus and builds a self-learned dictionary using alignment statistics between English and Chinese language pair. On the other hands, [4] is based on the manually segmented bilingual corpus and then try to “repack” the word from existing alignment by using alignment confidence. Both works evaluated the performance in BLEU metric and reported the promising result of PB-SMT task. 3 Methodology This paper aim to compare translation quality based on SMT task between the systems trained on bilingual corpus that contains both segmented source and target, and on the same bilingual corpus with segmented source but unsegmented target. First, we make use of monolingual information by employing several character cluster algorithms on unse</context>
<context position="11507" citStr="[4]" startWordPosition="1717" endWordPosition="1717">to-Target word alignment (b) Target-to-Source word alignment (c) Intersection between (a) and (b). (d) Result of (c) after applying the growing heuristic algorithm. Finally, we select consecutive clusters which are aligned to the same English word as candidates. From the Figure 1.d, we obtain these candidates (red, สีแou) and (bicycle, 5n 5 en u). 3.4 Character Cluster Repacking Although the alignment information obtained from the previous step is very helpful for the PB-SMT task, there is still plenty of room to enhance the PB-SMT performance. One way of doing that is by using word repacking [4]. However, in this paper, we perform a character cluster repacking (CCR) instead of word. The main purpose of repacking technique is to group all small consecutive clusters (or word) in target side that frequently align with one word in source data. Repacking approaches uses two simple calculations which are a co-occurrence frequency (COOC (𝑒i, 𝑇i)) and alignment confidence (𝐴𝐶( 𝑎i)). (COOC (𝑒i, 𝑇i)) is the number of times 𝑒i and 𝑇i co-occurr in the bilingual corpus [4] [9] and 𝐴𝐶( 𝑎i) is a measure of how often the aligner aligns 𝑒i and 𝑇L when they co-occur. AC is defined as 𝐴𝐶(𝑎i) _ 𝐶(𝑎i) CO</context>
</contexts>
<marker>[4]</marker>
<rawString>Y. Ma and A. Way, “Bilingually motivated domain-adapted word segmentation for statistical machine translation,” in Proceeding EACL &apos;09 Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pp. 549-557, Stroudsburg, PA, USA, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Do We Need Chinese Word Segmentation for</title>
<date>2004</date>
<booktitle>Statistical Machine Translation?,” ACL SIGHAN Workshop</booktitle>
<pages>122--129</pages>
<contexts>
<context position="3285" citStr="[5]" startWordPosition="457" endWordPosition="457">ul information to segment or cluster the word, most of word segmentators are trained on a manually segmented monolingual corpus by using various approaches such as dictionary-based, Hidden Markov Model (HMM), support vector machine (SVM) or conditional random field (CRF). Although, a number of segementators are able to yield very promising results, certain of them might be unsuitable for SMT task due to the influence of segmentation scheme [4]. Therefore, instead of solely rely on monolingual corpus, various researches make use of either manually segmented [4] or unsegment1ed bilingual corpus [5] as a guideline information to perform a word segmentation task and improve the performance of SMT system. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/ 94 Proceedings of the 5th Workshop on South and Southeast Asian NLP, 25th International Conference on Computational Linguistics, pages 94–101, Dublin, Ireland, August 23-29 2014. In this paper, we propose a novel segmentation approach for Phrase-Based Statistical Machine Translat</context>
<context position="6565" citStr="[5]" startWordPosition="943" endWordPosition="943"> meaningful and has to combine with other consecutive clusters to form a word. Characters in the cluster cannot be separated according to the Thai orthographic rules. For example, a vowel and tone mark cannot stand alone and a tone marker is always required to be placed next to a previous character only. [6] applied TCC to word segmentation technique which yields an interesting result. 2.2 Bilingually Word Segmentation Bilingual information has also been shown beneficial for word segmentation. Several methods have used the information from bilingual corpora to perform word segmentation. As in [5], it focuses on unsegmented bilingual corpus and builds a self-learned dictionary using alignment statistics between English and Chinese language pair. On the other hands, [4] is based on the manually segmented bilingual corpus and then try to “repack” the word from existing alignment by using alignment confidence. Both works evaluated the performance in BLEU metric and reported the promising result of PB-SMT task. 3 Methodology This paper aim to compare translation quality based on SMT task between the systems trained on bilingual corpus that contains both segmented source and target, and on </context>
</contexts>
<marker>[5]</marker>
<rawString>J. Xu, R. Zens and H. Ney, “Do We Need Chinese Word Segmentation for Statistical Machine Translation?,” ACL SIGHAN Workshop 2004, pp. 122-129, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Limcharoen</author>
<author>C Nattee</author>
<author>T Theeramunkong</author>
</authors>
<title>Thai Word Segmentation based-on GLR Parsing Technique and Word N-gram Model,”</title>
<date>2009</date>
<booktitle>in Eighth International Symposium on Natural Lanugage Processing,</booktitle>
<location>Bangkok, Thailand,</location>
<contexts>
<context position="6271" citStr="[6]" startWordPosition="902" endWordPosition="902">, Thailand 2010) is 3.855. This makes the search space of Thai word segmentation very large. To alleviate this issue, the notion of Thai character cluster (TCC), is introduced [1] to reduce the search space with predetermined unambiguious constraints for cluster formation. A cluster may not be meaningful and has to combine with other consecutive clusters to form a word. Characters in the cluster cannot be separated according to the Thai orthographic rules. For example, a vowel and tone mark cannot stand alone and a tone marker is always required to be placed next to a previous character only. [6] applied TCC to word segmentation technique which yields an interesting result. 2.2 Bilingually Word Segmentation Bilingual information has also been shown beneficial for word segmentation. Several methods have used the information from bilingual corpora to perform word segmentation. As in [5], it focuses on unsegmented bilingual corpus and builds a self-learned dictionary using alignment statistics between English and Chinese language pair. On the other hands, [4] is based on the manually segmented bilingual corpus and then try to “repack” the word from existing alignment by using alignment c</context>
</contexts>
<marker>[6]</marker>
<rawString>P. Limcharoen, C. Nattee and T. Theeramunkong, “Thai Word Segmentation based-on GLR Parsing Technique and Word N-gram Model,” in Eighth International Symposium on Natural Lanugage Processing, Bangkok, Thailand, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F J Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation,”</title>
<date>2003</date>
<booktitle>in NAACL &apos;03 Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<location>Stroudsburg, PA, USA,</location>
<contexts>
<context position="10170" citStr="[7]" startWordPosition="1515" endWordPosition="1515"> quality as shown in section 5.1 3.3 Bilingually-Guided Alignment Information We begin with the sequence of small clusters resulting from previous character clustering process. These small clusters can be grouped together in order to form “word” using bilingually-guided alignment information. Generally, small consecutive clusters in target side which are aligned to the same word in source data should be grouped together. Therefore, this section describes our one-tomany alignment extraction process. For one-to-many alignment, we applied processes similar to those in phrase extraction algorithm [7] which is described as follows. With English sentence 𝑒l and a Thai character cluster 𝑇i, we apply IBM model 1-5 to extract word-to-cluster translation probability of source-to-target 𝑃(𝑡|𝑒) and target-to-source 𝑃(𝑒|𝑡). Next, the alignment points which have the highest probability are greedily selected from both 𝑃(𝑡|𝑒) and 𝑃(𝑒 |𝑡). Figure 1.a and 1.b show examples of alignment points of source-to-target and target-to-source respectively. After that we selected the intersection of alignment pairs from both side. Then, additional alignment points are added according to the growing heuristic algo</context>
<context position="14516" citStr="[7]" startWordPosition="2229" endWordPosition="2229">33,589 Dev 12,568 Test #1 3,426 Test #2 500 Table 1. Information of bilingual corpus 4.2 Tools and Evaluation We evaluate our system in term of translation quality based on phrase-based SMT. Source sentences are sequence of English words while target sentences are sequences of Thai character clusters and each cluster size depends on which approach used in the experiment. Translation model and language model are train based on the standard phrase-based SMT. Alignments of source (English word) and target (Thai Character Cluster) are extracted using GIZA++ [8] and the phrase extraction algorithm [7] is applied using Moses SMT package. We apply SRILM [10] to train the 3-gram language model of target side. We use the default parameter settings for decoding. In testing process, we use another two test sets difference to the training data. Then we compared the translation result with the reference in term of BLEU score instead of F-score because of two main reasons. First, it is cumbersome to construct a reliable gold standard since their annotation schemes are different. Second, there is no strong correlation with SMT translation quality in terms of BLEU score [11]. Therefore, we re-segment</context>
</contexts>
<marker>[7]</marker>
<rawString>P. Koehn, F. J. Och and D. Marcu, “Statistical phrase-based translation,” in NAACL &apos;03 Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, Stroudsburg, PA, USA, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models,”</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<pages>pp.</pages>
<contexts>
<context position="10814" citStr="[8]" startWordPosition="1607" endWordPosition="1607">ish sentence 𝑒l and a Thai character cluster 𝑇i, we apply IBM model 1-5 to extract word-to-cluster translation probability of source-to-target 𝑃(𝑡|𝑒) and target-to-source 𝑃(𝑒|𝑡). Next, the alignment points which have the highest probability are greedily selected from both 𝑃(𝑡|𝑒) and 𝑃(𝑒 |𝑡). Figure 1.a and 1.b show examples of alignment points of source-to-target and target-to-source respectively. After that we selected the intersection of alignment pairs from both side. Then, additional alignment points are added according to the growing heuristic algorithm (grow additional alignment points, [8]) (a) (b) 96 (c) (d) Figure 1. The process of one-to-many alignment extraction (a) Source-to-Target word alignment (b) Target-to-Source word alignment (c) Intersection between (a) and (b). (d) Result of (c) after applying the growing heuristic algorithm. Finally, we select consecutive clusters which are aligned to the same English word as candidates. From the Figure 1.d, we obtain these candidates (red, สีแou) and (bicycle, 5n 5 en u). 3.4 Character Cluster Repacking Although the alignment information obtained from the previous step is very helpful for the PB-SMT task, there is still plenty of</context>
<context position="14476" citStr="[8]" startWordPosition="2223" endWordPosition="2223">w Data Set No. of sentence pairs Train 633,589 Dev 12,568 Test #1 3,426 Test #2 500 Table 1. Information of bilingual corpus 4.2 Tools and Evaluation We evaluate our system in term of translation quality based on phrase-based SMT. Source sentences are sequence of English words while target sentences are sequences of Thai character clusters and each cluster size depends on which approach used in the experiment. Translation model and language model are train based on the standard phrase-based SMT. Alignments of source (English word) and target (Thai Character Cluster) are extracted using GIZA++ [8] and the phrase extraction algorithm [7] is applied using Moses SMT package. We apply SRILM [10] to train the 3-gram language model of target side. We use the default parameter settings for decoding. In testing process, we use another two test sets difference to the training data. Then we compared the translation result with the reference in term of BLEU score instead of F-score because of two main reasons. First, it is cumbersome to construct a reliable gold standard since their annotation schemes are different. Second, there is no strong correlation with SMT translation quality in terms of B</context>
</contexts>
<marker>[8]</marker>
<rawString>F. J. Och and H. Ney, “A systematic comparison of various statistical alignment models,” Computational Linguistics, vol. 29, no. 1, pp. 19-51, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>Models of translational equivalence among words,”</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<pages>221--249</pages>
<contexts>
<context position="11985" citStr="[9]" startWordPosition="1795" endWordPosition="1795">T task, there is still plenty of room to enhance the PB-SMT performance. One way of doing that is by using word repacking [4]. However, in this paper, we perform a character cluster repacking (CCR) instead of word. The main purpose of repacking technique is to group all small consecutive clusters (or word) in target side that frequently align with one word in source data. Repacking approaches uses two simple calculations which are a co-occurrence frequency (COOC (𝑒i, 𝑇i)) and alignment confidence (𝐴𝐶( 𝑎i)). (COOC (𝑒i, 𝑇i)) is the number of times 𝑒i and 𝑇i co-occurr in the bilingual corpus [4] [9] and 𝐴𝐶( 𝑎i) is a measure of how often the aligner aligns 𝑒i and 𝑇L when they co-occur. AC is defined as 𝐴𝐶(𝑎i) _ 𝐶(𝑎i) COOC (𝑒i, 𝑇L) where 𝐶(𝑎i) denotes the number of alignments suggested by the previous-step word aligner. Unfortunately, due to the limited memory in our experiment machine, we cannot find COOC (𝑒i, 𝑇L)) for all possible &lt; 𝑒i, 𝑇L &gt; pairs. We, therefore, slightly modified the above equation by finding 𝐶(𝑎i) first. Secondly, we begin searching COOC (𝑒i, 𝑇i)) from all possible alignments in 𝑎i instead of finding all occurrences in corpus. By applying this modification, we eliminat</context>
</contexts>
<marker>[9]</marker>
<rawString>I. D. Melamed, “Models of translational equivalence among words,” Computational Linguistics, vol. 26, no. 2, pp. 221-249, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>“SRILM</author>
</authors>
<title>An extensible language modeling toolkit,”</title>
<date>2002</date>
<booktitle>in Proceeding of the International Conference on Spoken Language Processing,</booktitle>
<contexts>
<context position="14572" citStr="[10]" startWordPosition="2239" endWordPosition="2239">formation of bilingual corpus 4.2 Tools and Evaluation We evaluate our system in term of translation quality based on phrase-based SMT. Source sentences are sequence of English words while target sentences are sequences of Thai character clusters and each cluster size depends on which approach used in the experiment. Translation model and language model are train based on the standard phrase-based SMT. Alignments of source (English word) and target (Thai Character Cluster) are extracted using GIZA++ [8] and the phrase extraction algorithm [7] is applied using Moses SMT package. We apply SRILM [10] to train the 3-gram language model of target side. We use the default parameter settings for decoding. In testing process, we use another two test sets difference to the training data. Then we compared the translation result with the reference in term of BLEU score instead of F-score because of two main reasons. First, it is cumbersome to construct a reliable gold standard since their annotation schemes are different. Second, there is no strong correlation with SMT translation quality in terms of BLEU score [11]. Therefore, we re-segment the reference data (manually segmented) and the transla</context>
</contexts>
<marker>[10]</marker>
<rawString>“SRILM -- An extensible language modeling toolkit,” in Proceeding of the International Conference on Spoken Language Processing, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-C Chang</author>
<author>M Galley</author>
<author>C D Manning</author>
</authors>
<title>Optimizing Chinese word segmentation for machine translation performance,”</title>
<date>2008</date>
<booktitle>in Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<location>Columbus, Ohio,</location>
<contexts>
<context position="15090" citStr="[11]" startWordPosition="2324" endWordPosition="2324">hrase extraction algorithm [7] is applied using Moses SMT package. We apply SRILM [10] to train the 3-gram language model of target side. We use the default parameter settings for decoding. In testing process, we use another two test sets difference to the training data. Then we compared the translation result with the reference in term of BLEU score instead of F-score because of two main reasons. First, it is cumbersome to construct a reliable gold standard since their annotation schemes are different. Second, there is no strong correlation with SMT translation quality in terms of BLEU score [11]. Therefore, we re-segment the reference data (manually segmented) and the translation result data based on TCC. Some may concern about using TCC will lead to over estimation (higher than actual) due to the BLEU score is design based on word and not based on character. However, we used this BLEU score only for comparing translation quality among our experiments. Comparing to other SMT system still require running BLEU score based on the same segmentation guideline. 5 Results and Discussion We conducted all experiments on PB-SMT task and reported the performance of PB-SMT system based on the BL</context>
</contexts>
<marker>[11]</marker>
<rawString>P.-C. Chang, M. Galley and C. D. Manning, “Optimizing Chinese word segmentation for machine translation performance,” in Proceedings of the Third Workshop on Statistical Machine Translation, Columbus, Ohio, 2008.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.950301">
Emotive or Non-emotive: That is The Question
</title>
<author confidence="0.993873">
Michal Ptaszynski Fumito Masui
</author>
<affiliation confidence="0.998634">
Department of Computer Science,
Kitami Institute of Technology
</affiliation>
<email confidence="0.3794">
{ptaszynski,f-masui}@
cs.kitami-it.ac.jp
</email>
<author confidence="0.769492">
Rafal Rzepka Kenji Araki
</author>
<affiliation confidence="0.750737">
Graduate School of Information Science
and Technology, Hokkaido University
</affiliation>
<bodyText confidence="0.359298">
{rzepka,araki}@
ist.hokudai.ac.jp
</bodyText>
<sectionHeader confidence="0.972286" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999992470588235">
In this research we focus on discriminat-
ing between emotive (emotionally loaded)
and non-emotive sentences. We define the
problem from a linguistic point of view as-
suming that emotive sentences stand out
both lexically and grammatically. We
verify this assumption experimentally by
comparing two sets of such sentences in
Japanese. The comparison is based on
words, longer n-grams as well as more so-
phisticated patterns. In the classification
we use a novel unsupervised learning algo-
rithm based on the idea of language com-
binatorics. The method reached results
comparable to the state of the art, while
the fact that it is fully automatic makes it
more efficient and language independent.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965722222222">
Recently the field of sentiment analysis has at-
tracted great interest. It has become popular to
try different methods to distinguish between sen-
tences loaded with positive and negative senti-
ments. However, a few research focused on a task
more generic, namely, discriminating whether a
sentence is even loaded with emotional content or
not. The difficulty of the task is indicated by three
facts. Firstly, the task has not been widely un-
dertaken. Secondly, in research which addresses
the challenge, the definition of the task is usually
based on subjective ad hoc assumptions. Thirdly,
in research which do tackle the problem in a sys-
tematic way, the results are usually unsatisfactory,
and satisfactory results can be obtained only with
large workload.
We decided to tackle the problem in a standard-
ized and systematic way. We defined emotionally
loaded sentences as those which in linguistics are
described as fulfilling the emotive function of lan-
guage. We assumed that there are repetitive pat-
terns which appear uniquely in emotive sentences.
We performed experiments using a novel unsu-
pervised clustering algorithm based on the idea
of language combinatorics. By using this method
we were also able to minimize human effort and
achieve F-score comparable to the state of the art
with much higher Recall rate.
The outline of the paper is as follows. We
present the background for this research in Section
2. Section 3 describes the language combinatorics
approach which we used to compare emotive and
non-emotive sentences. In section 4 we describe
our dataset and experiment settings. The results of
the experiment are presented in Section 5. Finally
the paper is concluded in Section 6.
</bodyText>
<sectionHeader confidence="0.981119" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999961352941176">
There are different linguistic means used to in-
form interlocutors of emotional states in an ev-
eryday communication. The emotive meaning is
conveyed verbally and lexically through exclama-
tions (Beijer, 2002; Ono, 2002), hypocoristics (en-
dearments) (Kamei et al., 1996), vulgarities (Crys-
tal, 1989) or, for example in Japanese, through
mimetic expressions (gitaigo) (Baba, 2003). The
function of language realized by such elements of
language conveying emotive meaning is called the
emotive function of language. It was first distin-
guished by B¨uhler (1934-1990) in his Sprachthe-
orie as one of three basic functions of languages.
B¨uhler’s theory was picked up later by Jakobson
(1960), who by distinguishing three other func-
tions laid the grounds for structural linguistics and
communication studies.
</bodyText>
<subsectionHeader confidence="0.979073">
2.1 Previous Research
</subsectionHeader>
<bodyText confidence="0.823372">
Detecting whether sentences are loaded with emo-
tional content has been undertaken by a number
&apos;The other two being descriptive and impressive.
</bodyText>
<page confidence="0.986662">
59
</page>
<note confidence="0.805202">
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 59–65,
Baltimore, Maryland, USA. June 27, 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99776406">
of researchers, most often as an additional task
in either sentiment analysis (SA) or affect analy-
sis (AA). SA, in great simplification, focuses on
determining whether a language entity (sentence,
document) was written with positive or negative
attitude toward its topic. AA on the other hand
focuses on specifying which exactly emotion type
(joy, anger, etc.) has been conveyed. The fact,
that the task was usually undertaken as a subtask,
influences the way it was formulated. Below we
present some of the most influential works on the
topic, but formulating it in slightly different terms.
Emotional vs. Neutral: Discriminating whe-
ther a sentence is emotional or neutral is to answer
the question of whether it can be interpreted as
produced in an emotional state. This way the task
was studied by Minato et al. (2006), Aman and Sz-
pakowicz (2007) or Neviarouskaya et al. (2011).
Subjective vs. Objective: Discriminating be-
tween subjective and objective sentences is to
say whether the speaker presented the sentence
contents from a first-person-centric perspective or
from no specific perspective. The research formu-
lating the problem this way include e.g, Wiebe et
al. (1999), who classified subjectivity of sentences
using naive Bayes classifier, or later Wilson and
Wiebe (2005). In other research Yu and Hatzi-
vassiloglou (2003) used supervised learning to de-
tect subjectivity and Hatzivassiloglou and Wiebe
(2012) studied the effect of gradable adjectives on
sentence subjectivity.
Emotive vs. Non-emotive: Saying that a sen-
tence is emotive means to specify the linguistic
features of language which where used to produce
a sentence uttered with emphasis. Research that
formulated and tackled the problem this way was
done by, e.g., Ptaszynski et al. (2009).
Each of the above nomenclature implies sim-
ilar, though slightly different assumptions. For
example, a sentence produced without any emo-
tive characteristics (non-emotive) could still im-
ply emotional state in some situations. Also Bing
and Zhang (2012) notice that “not all subjective
sentences express opinions and those that do are
a subgroup of opinionated sentences.” A compari-
son of the scopes and overlaps of different nomen-
clature is represented in Figure 1. In this research
we formulate the problem similarly to Ptaszynski
et al. (2009), therefore we used their system to
compare with our method.
</bodyText>
<figureCaption confidence="0.9523905">
Figure 1: Comparison of between different
nomenclature used in sentiment analysis research.
</figureCaption>
<sectionHeader confidence="0.971904" genericHeader="method">
3 Language Combinatorics
</sectionHeader>
<bodyText confidence="0.999827">
The idea of language combinatorics (LC) assumes
that patterns with disjoint elements provide bet-
ter results than the usual bag-of-words or n-gram
approach (Ptaszynski et al., 2011). Such patterns
are defined as ordered non-repeated combinations
of sentence elements. They are automatically ex-
tracted by generating all ordered combinations of
sentence elements and verifying their occurrences
within a corpus.
In particular, in every n-element sentence there
is k-number of combination clusters, such as that
1 G k G n, where k represents all k-element com-
binations being a subset of n. The number of com-
binations generated for one k-element cluster of
combinations is equal to binomial coefficient, like
in eq. 1. Thus the number of all possible combina-
tions generated for all values of k from the range
of 11, ..., n} is equal to the sum of all combina-
tions from all k-element clusters, like in eq. 2.
</bodyText>
<equation confidence="0.995185166666667">
(n n!
) = (1)
k k!(n − k)!
n! n! n!
(k) = 1!(n − 1)! + 2!(n − 2)! + ... + n!(n − n)! = 2n − 1
(2)
</equation>
<bodyText confidence="0.999872777777778">
One problem with combinatorial approach is the
phenomenon of exponential and rapid growth of
function values during combinatorial manipula-
tions, called combinatorial explosion (Krippen-
dorff, 1986). Since this phenomenon causes long
processing time, combinatorial approaches have
been often disregarded. We assumed however,
that it could be dealt with when the algorithm
is optimized to the requirements of the task. In
preliminary experiments Ptaszynski et al. (2011)
used a generic sentence pattern extraction archi-
tecture SPEC to compare the amounts of generated
sophisticated patterns with n-grams, and noticed
that it is not necessary to generate patterns of all
lengths, since the most useful ones usually appear
in the group of 2 to 5 element patterns. Follow-
ing their experience we limit the pattern length in
our research to 6 elements. All non-subsequent el-
</bodyText>
<equation confidence="0.9113755">
n
k=1
</equation>
<page confidence="0.996239">
60
</page>
<tableCaption confidence="0.998423">
Table 1: Some examples from the dataset representing emotive and non-emotive sentences close in
content, but differing in emotional load expressed in the sentence (Romanized Japanese / Translation).
</tableCaption>
<subsectionHeader confidence="0.425807">
emotive non-emotive
</subsectionHeader>
<bodyText confidence="0.994327210526316">
Takasugiru kara ne / ’Cause its just too expensive K¯ogaku na tame desu. / Due to high cost.
Un, umai, kangeki da. / Oh, so delicious, I’m impressed. Kono kar¯e wa karai. / This curry is hot.
Nanto ano hito, kekkon suru rashii yo! / Have you heard? She’s getting married! Ano hito ga kekkon suru rashii desu. / They say she is gatting married.
Ch¯o ha ga itee / Oh, how my tooth aches! Ha ga itai / A tooth aches
Sugoku kirei na umi da naaa / Oh, what a beautiful sea! Kirei na umi desu / This is a beautiful sea
ements are also separated with an asterisk (“*”) to
mark disjoint elements.
The weight wj of each pattern generated this
way is calculated, according to equation 3, as a
ratio of all occurrences of a pattern in one corpus
Opo, to the sum of occurrences in two compared
corpora Opo,+O�eg. The weights are also normal-
ized to fit in range from +1 (representing purely
emotive patterns) to -1 (representing purely non-
emotive patterns). The normalization is achieved
by subtracting 0.5 from the initial score and mul-
tiplying this intermediate product by 2. The score
of one sentence is calculated as a sum of weights
of patterns found in the sentence, like in eq. 4.
</bodyText>
<equation confidence="0.999884">
0.5) * 2 (3)
�score = wj, (1 &gt; wj &gt; −1) (4)
</equation>
<bodyText confidence="0.983781">
The weight can be further modified by either
</bodyText>
<listItem confidence="0.9991815">
• awarding length k, or
• awarding length k and occurrence O.
</listItem>
<bodyText confidence="0.999548">
The list of generated frequent patterns can also be
further modified. When two collections of sen-
tences of opposite features (such as “emotive vs.
non-emotive”) are compared, a generated list will
contain patterns appearing uniquely on only one
of the sides (e.g. uniquely emotive patterns and
uniquely non-emotive patterns) or in both (am-
biguous patterns). Therefore the pattern list can
be modified by deleting
</bodyText>
<listItem confidence="0.7958455">
• all ambiguous patterns, or
• only ambiguous patterns appearing in the same
number on both sides (later called “zero pat-
terns”, since their weight is equal 0).
</listItem>
<bodyText confidence="0.999754555555555">
Moreover, since a list of patterns will contain both
the sophisticated patterns as well usual n-grams,
the experiments were performed separately for all
patterns and n-grams only. Also, if the initial col-
lection was biased toward one of the sides (sen-
tences of one kind were longer or more numer-
ous), there will be more patterns of a certain sort.
To mitigate this bias, instead of applying a rule of
thumb, the threshold was optimized automatically.
</bodyText>
<sectionHeader confidence="0.999875" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998738">
4.1 Dataset Preparation
</subsectionHeader>
<bodyText confidence="0.99939672">
In the experiments we used a dataset developed by
Ptaszynski et al. (2009) for the needs of evaluating
their affect analysis system ML-Ask for Japanese
language. The dataset contains 50 emotive and 41
non-emotive sentences. It was created as follows.
Thirty people of different age and social groups
participated in an anonymous survey. Each partic-
ipant was to imagine or remember a conversation
with any person they know and write three sen-
tences from that conversation: one free, one emo-
tive, and one non-emotive. Additionally, the par-
ticipants were asked to make the emotive and non-
emotive sentences as close in content as possible,
so the only difference was whether a sentence was
loaded with emotion or not. The participants also
annotated on their own free utterances whether or
not they were emotive. Some examples from the
dataset are represented in Table 1.
In our research the above dataset was further
preprocessed to make the sentences separable into
elements. We did this in three ways to check how
the preprocessing influences the results. We used
MeCab2, a morphological analyzer for Japanese
to preprocess the sentences from the dataset in the
three following ways:
</bodyText>
<listItem confidence="0.996241">
• Tokenization: All words, punctuation marks,
etc. are separated by spaces.
• Parts of speech (POS): Words are replaced
with their representative parts of speech.
• Tokens with POS: Both words and POS infor-
mation is included in one element.
</listItem>
<bodyText confidence="0.99989225">
The examples of preprocessing are represented
in Table 2. In theory, the more generalized a sen-
tence is, the less unique patterns it will produce,
but the produced patterns will be more frequent.
This can be explained by comparing tokenized
sentence with its POS representation. For exam-
ple, in the sentence from Table 2 we can see that
a simple phrase kimochi ii (“feeling good”) can be
</bodyText>
<equation confidence="0.57046825">
2https://code.google.com/p/mecab/
� Opos
Opos + Oneg
wj =
</equation>
<page confidence="0.984202">
61
</page>
<bodyText confidence="0.8821495">
Table 2: Three kinds of preprocessing of a sen-
tence in Japanese; N = noun, TOP = topic marker,
ADV = adverbial particle, ADJ = adjective, COP
= copula, EXCL = exclamation mark.
Table 3: Best results for each version of the
method compared with the ML-Ask system.
</bodyText>
<figure confidence="0.9087665">
SPEC
tokenized POS token-POS
n-grams patterns n-grams patterns
n-grams patterns
ML-Ask
Sentence:
Transliteration: Ky¯owanantekimochiiihinanda!
Glossing: Today TOP what pleasant day COP EXCL
Translation: What a pleasant day it is today!
Preprocessing examples
1. Words: Ky¯o wa nante kimochi ii hi nanda !
2.POS:N TOP ADV N ADJ N COP EXCL
3.Words+POS: Ky¯o[N] wa[TOP] nante[ADV]
kimochi[N] ii[ADJ] hi[N] nanda[COP] ![EXCL]
</figure>
<bodyText confidence="0.997373333333333">
represented by a POS pattern N ADJ. We can eas-
ily assume that there will be more N ADJ patterns
than kimochi ii, because many word combinations
can be represented as N ADJ. Therefore POS pat-
terns will come in less variety but with higher oc-
currence frequency. By comparing the result of
classification using different preprocessing meth-
ods we can find out whether it is better to represent
sentences as more generalized or as more specific.
</bodyText>
<subsectionHeader confidence="0.994306">
4.2 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.999969210526316">
The experiment was performed three times, once
for each kind of preprocessing. Each time 10-
fold cross validation was performed and the results
were calculated using Precision (P), Recall (R)
and balanced F-score (F) for each threshold. We
verified which version of the algorithm achieves
the top score within the threshold span. However,
an algorithm could achieve the best score for one
certain threshold, while for others it could perform
poorly. Therefore we also looked at which ver-
sion achieves high scores for the longest threshold
span. This shows which algorithm is more bal-
anced. Finally, we checked the statistical signifi-
cance of the results. We used paired t-test because
the classification results could represent only one
of two classes (emotive or non-emotive). We also
compared the performance to the state of the art,
namely the affect analysis system ML-Ask devel-
oped by Ptaszynski et al. (2009).
</bodyText>
<sectionHeader confidence="0.999223" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999724166666667">
The overall F-score results were generally the best
for the datasets containing in order: both tokens
and POS, tokens only and POS only. The F-
scores for POS-preprocessed sentences revealed
the least constancy. For many cases n-grams
scored higher than all patterns, but almost none of
</bodyText>
<table confidence="0.839681333333333">
Precision 0.80 0.61 0.6 0.68 0.59 0.65 0.64
Recall 0.78 1.00 0.96 0.88 1.00 0.95 0.95
F-score 0.79 0.75 0.74 0.77 0.74 0.77 0.76
</table>
<bodyText confidence="0.997176204545455">
the results reached statistical significance. The F-
score results for the tokenized dataset were also
not unequivocal. For higher thresholds patterns
scored higher, while for lower thresholds the re-
sults were similar. The scores were rarely sig-
nificant, utmost at 5% level (p&lt;0.05), however,
in all situations where n-grams visibly scored
higher, the differences were not statistically sig-
nificant. Finally, for the preprocessing including
both tokens and POS information, pattern-based
approach achieved significantly better results (p-
value &lt;0.01 or &lt;0.001). The algorithm reached
its plateau at F-score around 0.73–0.74 for to-
kens and POS separately, and 0.75–0.76 for to-
kens with POS together. In the POS dataset the
elements were more abstracted, while in token-
POS dataset the elements were more specific, pro-
ducing a larger number, but less frequent patterns.
Lower scores for POS dataset could suggest that
the algorithm works better with less abstracted
preprocessing. Examples of F-score comparison
between n-grams and patterns for tokenized and
token-POS datasets are represented in Figures 2
and 3, respectively.
Results for Precision showed similar tenden-
cies. They were the most ambiguous for POS pre-
processing. For the tokenized dataset, although
there always was one or two thresholds for which
n-grams scored higher, scores for patterns were
more balanced, starting with a high score and de-
creasing slowly. As for the token-POS preprocess-
ing patterns achieved higher Precision for most of
the threshold span. The highest Precision of all
was achieved in this dataset by patterns with P =
0.87 for R = 0.50.
As for Recall, the scores were consistent for
all kinds of preprocessing, with higher scores for
patterns within most of the threshold span and
equaling while the threshold decreases. The high-
est scores achieved for each preprocessing for n-
grams and patterns are represented in Table 3.
The affect analysis system ML-Ask (Ptaszynski
et al., 2009) on the same dataset reached F = 0.79,
P = 0.8 and R = 0.78. The results were generally
</bodyText>
<page confidence="0.996319">
62
</page>
<bodyText confidence="0.999973857142857">
comparable, however slightly higher for ML-Ask
when it comes to P and F-score. R was always bet-
ter for the proposed method. However, ML-Ask is
a system requiring handcrafted lexicons, while our
method is fully automatic, learning the patterns
from data, not needing any particular preparations,
which makes it more efficient.
</bodyText>
<subsectionHeader confidence="0.993287">
5.1 Detailed Analysis of Learned Patterns
</subsectionHeader>
<bodyText confidence="0.995646">
Within some of the most frequently appearing
emotive patterns there were for example: !
(exclamation mark), n*yo, cha (emotive verb
modification), yo (exclamative sentence ending
particle), ga*yo, n*! or naa (interjection). Some
examples of sentences containing those patterns
are below (patterns underlined). Interestingly,
most elements of those patterns appear in ML-Ask
handcrafted databases, which suggests it could
be possible to improve ML-Ask performance by
extracting additional patterns with SPEC.
Ex. 1. Megane, soko ni atta nda yo. (The glasses
were over there!)
Ex. 2. Uuun, butai ga mienai yo. (Ohh, I cannot
see the stage!)
Ex. 3. Aaa, onaka ga suita yo. (Ohh, I’m so
hungry)
Another advantage of our method is the fact that
it can mark both emotive and non-emotive ele-
ments in sentence, while ML-Ask is designed to
annotate only emotive elements. Some examples
of extracted non-emotive patterns were for exam-
ple: desu, wa*desu, mashi ta, or te*masu. All of
them were patterns described in linguistic litera-
ture as typically non-emotive, consisting in copu-
las (desu), verb endings (masu, mashi ta). Some
sentence examples with those patterns include:
Ex. 4. K¯ogaku na tame desu. (Due to high cost.)
Ex. 5. Kirei na umi desu (This is a beautiful sea)
Ex. 6. Kyo wa yuki ga futte imasu. (It is snowing
today.)
</bodyText>
<sectionHeader confidence="0.993897" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999985733333333">
We presented a method for automatic extraction
of patterns from emotive sentences. We assumed
emotive sentences are distinguishable both lex-
ically and grammatically and performed experi-
ments to verify this assumption. In the experi-
ments we used a set of emotive and non-emotive
sentences preprocessed in different ways (tokens,
POS, token-POS) The patterns extracted from
sentences were applied to recognize emotionally
loaded sentences.
The algorithm reached its plateau for F-score
around 0.75–0.76 for patterns containing both to-
kens and POS information. Precision for patterns
was balanced, while for n-grams, although occa-
sionally achieving high scores, it was quickly de-
creasing. Recall scores were almost always better
for patterns. The generally lower results for POS-
represented sentences suggest that the algorithm
works better with less abstracted elements.
The results of the proposed method and the af-
fect analysis system ML-Ask were comparable.
ML-Ask achieved better Precision, but lower Re-
call. However, our method is more efficient as
it does not require handcrafted lexicons. More-
over, automatically extracted patterns overlap with
handcrafted databases of ML-Ask, which suggests
it could be possible to improve ML-Ask perfor-
mance with our method. In the near future we plan
to perform experiments on larger datasets, also in
other languages, such as English or Chinese.
</bodyText>
<figure confidence="0.9974185625">
F-score F-score
score
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
score
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
0.8
0.6
0.4
all_patterns
ngrams
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
all_patterns
ngrams
threshold threshold
</figure>
<figureCaption confidence="0.928969">
Figure 2: F-score comparison between n-grams
and patterns for tokenized detaset (p = 0.0209).
Figure 3: F-score comparison for n-grams and pat-
terns for dataset with tokens and POS (p = 0.001).
</figureCaption>
<page confidence="0.999296">
63
</page>
<sectionHeader confidence="0.966921" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.989186806818181">
Saima Aman and Stan Szpakowicz. 2007. Iden-
tifying expressions of emotion in text. In Pro-
ceedings of the 10th International Conference
on Text, Speech, and Dialogue (TSD-2007),
Lecture Notes in Computer Science (LNCS),
Springer-Verlag.
Junko Baba. 2003. Pragmatic function of Japanese
mimetics in the spoken discourse of varying
emotive intensity levels. Journal of Pragmatics,
Vol. 35, No. 12, pp. 1861-1889, Elsevier.
Fabian Beijer. 2002. The syntax and pragmatics
of exclamations and other expressive/emotional
utterances. Working Papers in Linguistics 2,
The Dept. of English in Lund.
Bing Liu, Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In Mining Text
Data, pp. 415-463. Springer.
Karl B¨uhler. 1990. Theory of Language. Represen-
tational Function of Language. John Benjamins
Publ. (reprint from Karl B¨uhler. Sprachtheorie.
Die Darstellungsfunktion der Sprache, Ullstein,
Frankfurt a. M., Berlin, Wien, 1934.)
David Crystal. 1989. The Cambridge Encyclope-
dia of Language. Cambridge University Press.
Vasileios Hatzivassiloglou and Janice Wiebe. Ef-
fects of adjective orientation and gradability on
sentence subjectivity. In Proceedings of Inter-
national Conference on Computational Linguis-
tics (COLING-2000), pp. 299-305, 2000.
Roman Jakobson. 1960. Closing Statement: Lin-
guistics and Poetics. Style in Language, pp.350-
377, The MIT Press.
Takashi Kamei, Rokuro Kouno and Eiichi Chino
(eds.). 1996. The Sanseido Encyclopedia of Lin-
guistics, Vol. VI, Sanseido.
Klaus Krippendorff. 1986. Combinatorial Explo-
sion, In: Web Dictionary of Cybernetics and
Systems. Princia Cybernetica Web.
Junko Minato, David B. Bracewell, Fuji Ren and
Shingo Kuroiwa. 2006. Statistical Analysis of a
Japanese Emotion Corpus for Natural Language
Processing. LNCS 4114, pp. 924-929.
Alena Neviarouskaya, Helmut Prendinger and
Mitsuru Ishizuka. 2011. Affect analysis model:
novel rule-based approach to affect sensing
from text. Natural Language Engineering, Vol.
17, No. 1 (2011), pp. 95-135.
Hajime Ono. 2002. An emphatic particle DA and
exclamatory sentences in Japanese. University
of California, Irvine.
Christopher Potts and Florian Schwarz. 2008. Ex-
clamatives and heightened emotion: Extracting
pragmatic generalizations from large corpora.
Ms., UMass Amherst.
Michal Ptaszynski, Pawel Dybala, Rafal Rzepka
and Kenji Araki. 2009. Affecting Corpora: Ex-
periments with Automatic Affect Annotation
System - A Case Study of the 2channel Forum
-, In Proceedings of The Conference of the Pa-
cific Association for Computational Linguistics
(PACLING-09), pp. 223-228.
Michal Ptaszynski, Rafal Rzepka, Kenji Araki and
Yoshio Momouchi. 2011. Language combina-
torics: A sentence pattern extraction architec-
ture based on combinatorial explosion. Inter-
national Journal of Computational Linguistics
(IJCL), Vol. 2, Issue 1, pp. 24-36.
Kaori Sasai. 2006. The Structure of Modern
Japanese Exclamatory Sentences: On the Struc-
ture of the Nanto-Type Sentence. Studies in the
Japanese Language, Vol, 2, No. 1, pp. 16-31.
Janyce M. Wiebe, Rebecca F. Bruce and Thomas
P. O’Hara. 1999. Development and use of a
gold-standard data set for subjectivity classi-
fications. In Proceedings of the Association
for Computational Linguistics (ACL-1999), pp.
246-253, 1999.
Theresa Wilson and Janyce Wiebe. 2005. Anno-
tating Attributions and Private States. Proceed-
ings of the ACL Workshop on Frontiers in Cor-
pus Annotation II, pp. 53-60.
Hong Yu and Vasileios Hatzivassiloglou. 2003.
Towards answering opinion questions: separat-
ing facts from opinions and identifying the po-
larity of opinion sentences. In Proceedings of
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2003), pp. 129-
136, 2003.
</bodyText>
<page confidence="0.992965">
64
</page>
<figure confidence="0.997695727272728">
Appendix: Comparison of experiment results in all experiment settings for all three ways of
dataset preprocessing.
F-score
F-score Precision Recall
Precision
Recall
0.8
0.9
1
0.9
0.8
0.7
0.8
0.7
0.6
0.7
0.6
0.5
0.6
0.5
score
score
score
0.4
0.5
all_patterns
zero_deleted
ambiguous_deleted
length_awarded
length_awarded_zero_deleted
length_awarded_ambiguous_deleted
length_and_occurrence_awarded
ngrams
ngrams_zero_deleted
ngrams_ambiguous_deleted
ngrams_length_awarded
ngrams_length_awarded_zero_deleted
ngrams_length_awarded_ambiguous_deleted
ngrams_length_and_occurrence_awarded
all_patterns
zero_deleted
ambiguous_deleted
length_awarded
length_awarded_zero_deleted
length_awarded_ambiguous_deleted
length_and_occurrence_awarded
all_patterns
zero_deleted
ambiguous_deleted
length_awarded
length_awarded_zero_deleted
length_awarded_ambiguous_deleted
length_and_occurrence_awarded
ngrams
ngrams_zero_deleted
ngrams_ambiguous_deleted
ngrams_length_awarded
ngrams_length_awarded_zero_deleted
ngrams_length_awarded_ambiguous_deleted
ngrams_length_and_occurrence_awarded
0.4
0.4
0.3
0.3
0.3
0.2
ngrams
ngrams_zero_deleted
ngrams_ambiguous_deleted
ngrams_length_awarded
ngrams_length_awarded_zero_deleted
ngrams_length_awarded_ambiguous_deleted
ngrams_length_and_occurrence_awarded
0.2
0.2
0.1
0.1
0.1
0
1
0
1
0
1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
threshold
threshold threshold threshold
threshold
threshold
(a) F-score comparison for tokenized
(b) Precision comparison for tok-
(c) Recall comparison for tokenized
(a) F-score comparison for tokenized (b) Precision comparison for tok- (c) Recall comparison for tokenized
dataset.
enized dataset.
dataset.
dataset. enized dataset. dataset.
F-score Precision Recall
0.8
0.8
1
0.9
0.7
0.7
0.8
0.6
0.6
0.7
0.5
0.5
0.6
score
score
score
0.4
0.4
0.5
all_patterns
zero_deleted
ambiguous_deleted
length_awarded
length_awarded_zero_deleted
length_awarded_ambiguous_deleted
length_and_occurrence_awarded
ngrams
ngrams_zero_deleted
ngrams_ambiguous_deleted
ngrams_length_awarded
ngrams_length_awarded_zero_deleted
ngrams_length_awarded_ambiguous_deleted
all_patterns
zero_deleted
ambiguous_deleted
length_awarded
length_awarded_zero_deleted
length_awarded_ambiguous_deleted
length_and_occurrence_awarded
all_patterns
zero_deleted
ambiguous_deleted
0.4 length_awarded
length_awarded_zero_deleted
length_awarded_ambiguous_deleted
length_and_occurrence_awarded
ngrams
ngrams_zero_deleted
ngrams_ambiguous_deleted
ngrams_length_awarded
ngrams_length_awarded_zero_deleted
ngrams_length_awarded_ambiguous_deleted
ngrams_length_and_occurrence_awarded
0.3
0.3
0.3
0.2
0.2
ngrams
ngrams_zero_deleted
ngrams_ambiguous_deleted
ngrams_length_awarded
ngrams_length_awarded_zero_deleted
ngrams_length_awarded_ambiguous_deleted
ngrams_length_and_occurrence_awarded
0.2
0.1
0.1
0.1
ngrams_length_and_occurrence_awarded
0
1
0
1
0
1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
threshold
threshold threshold threshold
threshold
threshold
(d) F-score comparison for POS-
(e) Precision comparison for POS-
(f) Recall comparison for POS-tagged
(d) F-score comparison for POS- (e) Precision comparison for POS- (f) Recall comparison for POS-tagged
tagged dataset.
tagged dataset.
dataset.
tagged dataset. tagged dataset. dataset.
F-score Precision Recall
0.8
0.9
1
0.9
0.7
0.8
0.8
0.6
0.7
0.7
0.5
0.6
0.6
score
score
score
0.4
0.5
</figure>
<bodyText confidence="0.9651606">
all_patterns
zero_deleted
ambiguous_deleted
length_awarded
length_awarded_zero_deleted
length_awarded_ambiguous_deleted
length_and_occurrence_awarded
ngrams
ngrams_zero_deleted
ngrams_ambiguous_deleted
ngrams_length_awarded
ngrams_length_awarded_zero_deleted
ngrams_length_awarded_ambiguous_deleted
ngrams_length_and_occurrence_awarded
all_patterns
zero_deleted
ambiguous_deleted
length_awarded
length_awarded_zero_deleted
length_awarded_ambiguous_deleted
</bodyText>
<figure confidence="0.919174636363636">
0.4 length_and_occurrence_awarded
all_patterns
zero_deleted
ambiguous_deleted
0.4 length_awarded
length_awarded_zero_deleted
length_awarded_ambiguous_deleted
length_and_occurrence_awarded
ngrams
ngrams_zero_deleted
ngrams_ambiguous_deleted
ngrams_length_awarded
0.5
0.3
0.3
0.2
ngrams
ngrams_zero_deleted
ngrams_ambiguous_deleted
ngrams_length_awarded
ngrams_length_awarded_zero_deleted
ngrams_length_awarded_ambiguous_deleted
ngrams_length_and_occurrence_awarded
0.2
0.3
0.1
ngrams_length_awarded_zero_deleted
0.1 ngrams_length_awarded_ambiguous_deleted
ngrams_length_and_occurrence_awarded
0
1
0.2
0
1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
0.8
0.6
0.4
0.2
0
-0.2
-0.4
-0.6
-0.8
-1
threshold
threshold threshold threshold
threshold
threshold
(g) F-score comparison for tokenized
(h) Precision comparison for tok-
(i) Recall comparison for tokenized
(g) F-score comparison for tokenized (h) Precision comparison for tok- (i) Recall comparison for tokenized
dataset with POS tags.
enized dataset with POS tags.
dataset with POS tags.
dataset with POS tags. enized dataset with POS tags. dataset with POS tags.
</figure>
<page confidence="0.990039">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.664950">
<title confidence="0.998594">Emotive or Non-emotive: That is The Question</title>
<author confidence="0.999943">Michal Ptaszynski Fumito Masui</author>
<affiliation confidence="0.9863165">Department of Computer Kitami Institute of</affiliation>
<email confidence="0.801725">cs.kitami-it.ac.jp</email>
<author confidence="0.958704">Rafal Rzepka Kenji Araki</author>
<affiliation confidence="0.9645155">Graduate School of Information and Technology, Hokkaido</affiliation>
<email confidence="0.962933">ist.hokudai.ac.jp</email>
<abstract confidence="0.999406444444444">In this research we focus on discriminating between emotive (emotionally loaded) and non-emotive sentences. We define the problem from a linguistic point of view assuming that emotive sentences stand out both lexically and grammatically. We verify this assumption experimentally by comparing two sets of such sentences in Japanese. The comparison is based on words, longer n-grams as well as more sophisticated patterns. In the classification we use a novel unsupervised learning algorithm based on the idea of language combinatorics. The method reached results comparable to the state of the art, while the fact that it is fully automatic makes it more efficient and language independent.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007675">
<title confidence="0.996812">
Semantic Parsing for Text to 3D Scene Generation
</title>
<author confidence="0.997737">
Angel X. Chang, Manolis Savva and Christopher D. Manning
</author>
<affiliation confidence="0.995559">
Computer Science Department, Stanford University
</affiliation>
<email confidence="0.967571">
angelx,msavva,manning@cs.stanford.edu
</email>
<figureCaption confidence="0.999071">
Figure 1: Generated scene for “There is a room
</figureCaption>
<bodyText confidence="0.986669333333333">
with a chair and a computer.” Note that the system
infers the presence of a desk and that the computer
should be supported by the desk.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984634920635">
We propose text-to-scene generation as an appli-
cation for semantic parsing. This is an applica-
tion that grounds semantics in a virtual world that
requires understanding of common, everyday lan-
guage. In text to scene generation, the user pro-
vides a textual description and the system gener-
ates a 3D scene. For example, Figure 1 shows the
generated scene for the input text “there is a room
with a chair and a computer”. This is a challeng-
ing, open-ended problem that prior work has only
addressed in a limited way.
Most of the technical challenges in text to
scene generation stem from the difficulty of map-
ping language to formal representations of vi-
sual scenes, as well as an overall absence of real
world spatial knowledge from current NLP sys-
tems. These issues are partly due to the omis-
sion in natural language of many facts about the
world. When people describe scenes in text, they
typically specify only important, relevant informa-
tion. Many common sense facts are unstated (e.g.,
chairs and desks are typically on the floor). There-
fore, we focus on inferring implicit relations that
are likely to hold even if they are not explicitly
stated by the input text.
Text to scene generation offers a rich, interactive
environment for grounded language that is famil-
iar to everyone. The entities are common, every-
day objects, and the knowledge necessary to ad-
dress this problem is of general use across many
domains. We present a system that leverages user
interaction with 3D scenes to generate training data
for semantic parsing approaches.
Previous semantic parsing work has dealt with
grounding text to physical attributes and rela-
tions (Matuszek et al., 2012; Krishnamurthy and
Kollar, 2013), generating text for referring to ob-
jects (FitzGerald et al., 2013) and with connect-
ing language to spatial relationships (Golland et
al., 2010; Artzi and Zettlemoyer, 2013). Seman-
tic parsing methods can also be applied to many
aspects of text to scene generation. Furthermore,
work on parsing instructions to robots (Matuszek
et al., 2013; Tellex et al., 2014) has analogues in
the context of discourse about physical scenes.
In this extended abstract, we formalize the text
to scene generation problem and describe it as a
task for semantic parsing methods. To motivate
this problem, we present a prototype system that
incorporates simple spatial knowledge, and parses
natural text to a semantic representation. By learn-
ing priors on spatial knowledge (e.g., typical posi-
tions of objects, and common spatial relations) our
system addresses inference of implicit spatial con-
straints. The user can interactively manipulate the
generated scene with textual commands, enabling
us to refine and expand learned priors.
Our current system uses deterministic rules to
map text to a scene representation but we plan to
explore training a semantic parser from data. We
can leverage our system to collect user interactions
for training data. Crowdsourcing is a promising
avenue for obtaining a large scale dataset.
</bodyText>
<page confidence="0.991194">
17
</page>
<note confidence="0.425897">
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 17–21,
Baltimore, Maryland USA, June 26 2014. c�2014 Association for Computational Linguistics
</note>
<figureCaption confidence="0.998934">
Figure 2: Illustration of our system architecture.
</figureCaption>
<sectionHeader confidence="0.955488" genericHeader="method">
2 Task Definition
</sectionHeader>
<bodyText confidence="0.999963227272727">
We define text to scene generation as the task of
taking text describing a scene as input, and gen-
erating a plausible 3D scene described by that
text as output. More concretely, we parse the
input text into a scene template, which places
constraints on what objects must be present and
relationships between them. Next, using priors
from a spatial knowledge base, the system expands
the scene template by inferring additional implicit
constraints. Based on the scene template, we select
objects from a dataset of 3D models and arrange
them to generate an output scene.
After a scene is generated, the user can interact
with the scene using both textual commands and
mouse interactions. During interaction, semantic
parsing can be used to parse the input text into
a sequence of scene interaction commands. See
Figure 2 for an illustration of the system archi-
tecture. Throughout the process, we need to ad-
dress grounding of language to: 1) actions to be
performed, 2) objects to be instantiated or manip-
ulated, and 3) constraints on the objects.
</bodyText>
<subsectionHeader confidence="0.996458">
2.1 Scene Template
</subsectionHeader>
<bodyText confidence="0.998544">
A scene template T = (O, C) consists of a set
of object descriptions O = {o1, ... , on} and con-
straints C = {c1, ... , ck} on the relationships be-
tween the objects. For each object oi, we identify
properties associated with it such as category la-
bel, basic attributes such as color and material, and
number of occurrences in the scene. Based on the
object category and attributes, and other words in
the noun phrase mentioning the object, we iden-
tify a set of associated keywords to be used later
for querying the 3D model database. Spatial rela-
tions between objects are extracted as predicates of
the form on(oi, oj) or left(oi, oj) where oi and oj
are recognized objects.
As an example, given the input “There is a room
with a desk and a red chair. The chair is to the left
of the desk.” we extract the following objects and
spatial relations:
</bodyText>
<table confidence="0.9099786">
Objects category attributes keywords
o0 room color:red room
o1 desk desk
o2 chair chair, red
Relations: left(o2, o1)
</table>
<subsectionHeader confidence="0.975429">
2.2 Scene Interaction Commands
</subsectionHeader>
<bodyText confidence="0.9984649">
During interaction, we parse textual input provided
by the user into a sequence of commands with rele-
vant parts of the scene as arguments. For example,
given a scene S, we use the input text to identify a
subset of relevant objects matching X = {Os, Cs}
where Os is the set of object descriptions and Cs
is the set of object constraints. Commands can
then be resolved against this argument to manip-
ulate the scene state: Select(X), Remove(X),
Insert(X), Replace(X,Y), Move(X, ∆X),
Scale(X, ∆X), and Orient(X, ∆X). X and Y
are semantic representations of objects, while ∆X
is a change to be applied to X, expressed as either
a target condition (“put the lamp on the table”) or
a relative change (“move the lamp to the right”).
These basic operations demonstrate possible
scene manipulations through text. This set of op-
erations can be enlarged to cover manipulation of
parts of objects (“make the seat of the chair red”),
and of the viewpoint (“zoom in on the chair”).
</bodyText>
<subsectionHeader confidence="0.996881">
2.3 Spatial Knowledge
</subsectionHeader>
<bodyText confidence="0.999544615384615">
One of the richest sources of spatial knowledge
is 3D scene data. Prior work by (Fisher et al.,
2012) collected 133 small indoor scenes created
with 1723 3D Warehouse models. Based on their
approach, we create a spatial knowledge base with
priors on the static support hierarchy of objects in
scenes1, their relative positions and orientations.
We also define a set of spatial relations such as left,
right, above, below, front, back, on top of, next to,
near, inside, and outside. Table 1 gives examples
of the definitions of these spatial relations.
We use a 3D model dataset collected from
Google 3D Warehouse by prior work in scene syn-
</bodyText>
<footnote confidence="0.568486">
1A static support hierarchy represents which objects are
likely to support which other objects on their surface (e.g.,
the floor supports tables, tables support plates).
</footnote>
<figure confidence="0.9633541">
“There is a piece of
cake on a table.”
Scene
Generation
ON(FORK, TABLE)
ON(PLATE, TABLE)
ON(CAKE, PLATE)
Scene
Inference
Objects:
CAKE, TABLE
ON(CAKE, TABLE)
Objects:
PLATE, FORK
Spatial KB 3D Models
Object
Selection
INTERACTION
Semantic
Parsing
</figure>
<page confidence="0.931414">
18
</page>
<equation confidence="0.9700735">
Relation P(relation)
V ol(AnB)
V ol(A)
V ol(An right (B))
V ol(A)
1(dist(A, B) &lt; tnear)
</equation>
<tableCaption confidence="0.9561395">
Table 1: Definitions of spatial relation using object
bounding box computations.
</tableCaption>
<bodyText confidence="0.976616111111111">
thesis and containing about 12490 mostly indoor
objects (Fisher et al., 2012). These models have
text associated with them in the form of names and
tags, and category labels. In addition, we assume
the models have been scaled to physically plausi-
ble sizes and oriented with consistent up and front
direction (Savva et al., 2014). All models are in-
dexed in a database so they can be queried at run-
time for retrieval.
</bodyText>
<sectionHeader confidence="0.996053" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.999994375">
We present how the parsed representations are
used by our system to demonstrate the key issues
that have to be addressed during text to scene gen-
eration. Our current implementation uses a sim-
ple deterministic approach to map text to the scene
template and user actions on the scene. We use the
Stanford CoreNLP pipeline2 to process the input
text and use rules to match dependency patterns.
</bodyText>
<subsectionHeader confidence="0.997241">
3.1 Scene generation
</subsectionHeader>
<bodyText confidence="0.999405789473684">
During scene generation, we want to construct the
most likely scene given the input text. We first
parse the text into a scene template and use it to
select appropriate models from the database. We
then perform object layout and arrangement given
the priors on spatial knowledge.
Scene Template Parsing We use the Stanford
coreference system to determine when the same
object is being referred to. To identify objects,
we look for noun phrases and use the head word
as the category, filtering with WordNet (Miller,
1995) to determine which objects are visualizable
(under the physical object synset, excluding loca-
tions). To identify properties of the objects, we ex-
tract other adjectives and nouns in the noun phrase.
We also match syntactic dependency patterns such
as “X is made of Y” to extract more attributes and
keywords. Finally, we use dependency patterns to
extract spatial relations between objects.
</bodyText>
<footnote confidence="0.912594">
2http://nlp.stanford.edu/software/corenlp.shtml
</footnote>
<figureCaption confidence="0.9714765">
Figure 3: Select “a blue office chair” and “a
wooden desk” from the models database
</figureCaption>
<bodyText confidence="0.995676428571428">
Object Selection Once we have the scene tem-
plate, we use the keywords associated with each
object to query the model database. We select ran-
domly from the top 10 results for variety and to
allow the user to regenerate the scene with differ-
ent models. This step can be enhanced to take into
account correlations between objects (e.g., a lamp
on a table should not be a floor lamp model). See
Figure 3 for an example of object selection.
Object Layout Given the selected models, the
source scene template, and priors on spatial rela-
tions, we find an arrangement of the objects within
the scene that maximizes the probability of the lay-
out under the given scene template.
</bodyText>
<subsectionHeader confidence="0.998467">
3.2 Scene Interaction
</subsectionHeader>
<bodyText confidence="0.999432857142857">
Here we address parsing of text after a scene has
been generated and during interaction sessions.
Command Parsing We deterministically map
verbs to possible actions as shown in Table 2.
Multiple actions are possible for some verbs (e.g.,
“place” and “put” can refer to either Move or
Insert). To differentiate between these, we as-
sume new objects are introduced with the indefi-
nite article “a” whereas old ones are modified with
the definite article “the”.
Object Resolution To allow interaction with the
scene, we must resolve references to objects within
a scene. Objects are disambiguated by category
and view-centric spatial relations. In addition to
matching objects by their categories, we use the
WordNet hierarchy to handle hyponym or hyper-
nym referents. Depending on the current view,
spatial relations such as “left” or “right” can refer
to different objects (see Figure 4).
Scene Modification Based on the action we
need to appropriately modify the current scene.
</bodyText>
<equation confidence="0.58049">
inside(A,B)
right(A,B)
near(A,B)
</equation>
<page confidence="0.99636">
19
</page>
<table confidence="0.995796125">
verb Action Example Text Example Parse
generate Generate generate a room with a desk and a lamp Generate( {room,desk,lamp} , {}) )
select Select select the chair on the right of the table Select({lamp},{right(lamp,table)})
add, insert Insert add a lamp to the table Insert({lamp},{on(lamp,table)})
delete, remove Remove remove the lamp Remove({lamp})
move Move move the chair to the left Move({chair},{left(chair)})
place, put Move, Insert put the lamp on the table Move({lamp},{on(lamp,table)})
replace Replace replace the lamp with a vase Replace({lamp},{vase})
</table>
<tableCaption confidence="0.998944">
Table 2: Mapping of verbs to possible actions.
</tableCaption>
<figureCaption confidence="0.993261714285714">
Figure 4: Left: chair is selected by “chair to the
right of the table” or “object to the right of the ta-
ble”, but not selected by “cup to the right of the
table”. Right: Different view results in a different
chair selection for “chair to the right of the table”.
Figure 5: Left: initial scene. Right: after input
“Put a lamp on the table”.
</figureCaption>
<bodyText confidence="0.999949333333333">
We do this by maximizing the probability of a new
scene template given the requested action and pre-
vious scene template (see Figure 5 for an example).
</bodyText>
<sectionHeader confidence="0.998944" genericHeader="method">
4 Future Directions
</sectionHeader>
<bodyText confidence="0.999972970588235">
We described a system prototype to motivate ap-
proaching text to scene generation as a semantic
parsing application. While this prototype illus-
trates inference of implicit constraints using prior
knowledge, it still relies on hand coded rules for
mapping text to the scene representation. This is
similar to most previous work on text to scene gen-
eration (Winograd, 1972; Coyne and Sproat, 2001)
and limits handling of natural language. More re-
cently, (Zitnick et al., 2013) used data to learn how
to ground sentences to a CRF representing 2D cli-
part scenes. Similarly, we plan to investigate using
data to learn how to ground sentences to 3D scenes.
Spatial knowledge can be helpful for resolving
ambiguities during parsing. For instance, from
spatial priors of object positions and reasoning
with physical constraints we can disambiguate the
attachment of “next to” in “there is a book on the
table next to the lamp”. The book and lamp are
likely on the table and thus next_to(book, lamp)
should be more likely.
User interaction is a natural part of text to scene
generation. We can leverage such interaction to
obtain data for training a semantic parser. Every
time the user issues a command, the user can indi-
cate whether the result of the interaction was cor-
rect or not, and optionally provide a rating. By
keeping track of these scene interactions and the
user ratings we can construct a corpus of tuples
containing: user action, parsed scene interaction,
scene operation, scene state before and after the
operation, and rating by the user. By building up
such a corpus over multiple interactions and users,
we obtain data for training semantic parsers.
</bodyText>
<sectionHeader confidence="0.99778" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9905565">
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associ-
ation for Computational Linguistics.
Bob Coyne and Richard Sproat. 2001. WordsEye: an
automatic text-to-scene conversion system. In Pro-
ceedings of the 28th annual conference on Computer
graphics and interactive techniques.
Matthew Fisher, Daniel Ritchie, Manolis Savva,
Thomas Funkhouser, and Pat Hanrahan. 2012.
Example-based synthesis of 3D object arrangements.
ACM Transactions on Graphics.
Nicholas FitzGerald, Yoav Artzi, and Luke Zettle-
moyer. 2013. Learning distributions over logical
forms for referring expression generation. In Pro-
ceedings of the Conference on EMNLP.
Dave Golland, Percy Liang, and Dan Klein. 2010.
A game-theoretic approach to generating spatial de-
scriptions. In Proceedings of the 2010 conference on
EMNLP.
Jayant Krishnamurthy and Thomas Kollar. 2013.
Jointly learning to parse and perceive: Connecting
</reference>
<page confidence="0.936994">
20
</page>
<reference confidence="0.977842928571428">
natural language to the physical world. Transactions
of the Association for Computational Linguistics.
Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In International Conference on Ma-
chine Learning.
Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2013. Learning to parse natural
language commands to a robot control system. In
Experimental Robotics.
G.A. Miller. 1995. WordNet: a lexical database for
english. CACM.
Manolis Savva, Angel X. Chang, Gilbert Bernstein,
Christopher D. Manning, and Pat Hanrahan. 2014.
On being the right scale: Sizing large collections of
3D models. Stanford University Technical Report
CSTR 2014-03.
Stefanie Tellex, Pratiksha Thaker, Joshua Joseph, and
Nicholas Roy. 2014. Learning perceptually
grounded word meanings from unaligned parallel
data. Machine Learning.
Terry Winograd. 1972. Understanding natural lan-
guage. Cognitive psychology.
C. Lawrence Zitnick, Devi Parikh, and Lucy Vander-
wende. 2013. Learning the visual interpretation
of sentences. In IEEE Intenational Conference on
Computer Vision (ICCV).
</reference>
<page confidence="0.999436">
21
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.920009">
<title confidence="0.999423">Semantic Parsing for Text to 3D Scene Generation</title>
<author confidence="0.999685">X Chang</author>
<author confidence="0.999685">Manolis Savva D Manning</author>
<affiliation confidence="0.999909">Computer Science Department, Stanford University</affiliation>
<email confidence="0.988262">angelx,msavva,manning@cs.stanford.edu</email>
<abstract confidence="0.9823035">Figure 1: Generated scene for “There is a room with a chair and a computer.” Note that the system infers the presence of a desk and that the computer should be supported by the desk.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="2263" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="368" endWordPosition="371">d language that is familiar to everyone. The entities are common, everyday objects, and the knowledge necessary to address this problem is of general use across many domains. We present a system that leverages user interaction with 3D scenes to generate training data for semantic parsing approaches. Previous semantic parsing work has dealt with grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and with connecting language to spatial relationships (Golland et al., 2010; Artzi and Zettlemoyer, 2013). Semantic parsing methods can also be applied to many aspects of text to scene generation. Furthermore, work on parsing instructions to robots (Matuszek et al., 2013; Tellex et al., 2014) has analogues in the context of discourse about physical scenes. In this extended abstract, we formalize the text to scene generation problem and describe it as a task for semantic parsing methods. To motivate this problem, we present a prototype system that incorporates simple spatial knowledge, and parses natural text to a semantic representation. By learning priors on spatial knowledge (e.g., typical posi</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coyne</author>
<author>Richard Sproat</author>
</authors>
<title>WordsEye: an automatic text-to-scene conversion system.</title>
<date>2001</date>
<booktitle>In Proceedings of the 28th annual conference on Computer graphics and interactive techniques.</booktitle>
<contexts>
<context position="13012" citStr="Coyne and Sproat, 2001" startWordPosition="2142" endWordPosition="2145">tial scene. Right: after input “Put a lamp on the table”. We do this by maximizing the probability of a new scene template given the requested action and previous scene template (see Figure 5 for an example). 4 Future Directions We described a system prototype to motivate approaching text to scene generation as a semantic parsing application. While this prototype illustrates inference of implicit constraints using prior knowledge, it still relies on hand coded rules for mapping text to the scene representation. This is similar to most previous work on text to scene generation (Winograd, 1972; Coyne and Sproat, 2001) and limits handling of natural language. More recently, (Zitnick et al., 2013) used data to learn how to ground sentences to a CRF representing 2D clipart scenes. Similarly, we plan to investigate using data to learn how to ground sentences to 3D scenes. Spatial knowledge can be helpful for resolving ambiguities during parsing. For instance, from spatial priors of object positions and reasoning with physical constraints we can disambiguate the attachment of “next to” in “there is a book on the table next to the lamp”. The book and lamp are likely on the table and thus next_to(book, lamp) shou</context>
</contexts>
<marker>Coyne, Sproat, 2001</marker>
<rawString>Bob Coyne and Richard Sproat. 2001. WordsEye: an automatic text-to-scene conversion system. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Fisher</author>
<author>Daniel Ritchie</author>
<author>Manolis Savva</author>
<author>Thomas Funkhouser</author>
<author>Pat Hanrahan</author>
</authors>
<title>Example-based synthesis of 3D object arrangements.</title>
<date>2012</date>
<journal>ACM Transactions on Graphics.</journal>
<contexts>
<context position="6776" citStr="Fisher et al., 2012" startWordPosition="1126" endWordPosition="1129">e(X, ∆X), Scale(X, ∆X), and Orient(X, ∆X). X and Y are semantic representations of objects, while ∆X is a change to be applied to X, expressed as either a target condition (“put the lamp on the table”) or a relative change (“move the lamp to the right”). These basic operations demonstrate possible scene manipulations through text. This set of operations can be enlarged to cover manipulation of parts of objects (“make the seat of the chair red”), and of the viewpoint (“zoom in on the chair”). 2.3 Spatial Knowledge One of the richest sources of spatial knowledge is 3D scene data. Prior work by (Fisher et al., 2012) collected 133 small indoor scenes created with 1723 3D Warehouse models. Based on their approach, we create a spatial knowledge base with priors on the static support hierarchy of objects in scenes1, their relative positions and orientations. We also define a set of spatial relations such as left, right, above, below, front, back, on top of, next to, near, inside, and outside. Table 1 gives examples of the definitions of these spatial relations. We use a 3D model dataset collected from Google 3D Warehouse by prior work in scene syn1A static support hierarchy represents which objects are likel</context>
</contexts>
<marker>Fisher, Ritchie, Savva, Funkhouser, Hanrahan, 2012</marker>
<rawString>Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas Funkhouser, and Pat Hanrahan. 2012. Example-based synthesis of 3D object arrangements. ACM Transactions on Graphics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas FitzGerald</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Learning distributions over logical forms for referring expression generation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on EMNLP.</booktitle>
<contexts>
<context position="2157" citStr="FitzGerald et al., 2013" startWordPosition="352" endWordPosition="355"> stated by the input text. Text to scene generation offers a rich, interactive environment for grounded language that is familiar to everyone. The entities are common, everyday objects, and the knowledge necessary to address this problem is of general use across many domains. We present a system that leverages user interaction with 3D scenes to generate training data for semantic parsing approaches. Previous semantic parsing work has dealt with grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and with connecting language to spatial relationships (Golland et al., 2010; Artzi and Zettlemoyer, 2013). Semantic parsing methods can also be applied to many aspects of text to scene generation. Furthermore, work on parsing instructions to robots (Matuszek et al., 2013; Tellex et al., 2014) has analogues in the context of discourse about physical scenes. In this extended abstract, we formalize the text to scene generation problem and describe it as a task for semantic parsing methods. To motivate this problem, we present a prototype system that incorporates simple spatial knowledge, and par</context>
</contexts>
<marker>FitzGerald, Artzi, Zettlemoyer, 2013</marker>
<rawString>Nicholas FitzGerald, Yoav Artzi, and Luke Zettlemoyer. 2013. Learning distributions over logical forms for referring expression generation. In Proceedings of the Conference on EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dave Golland</author>
<author>Percy Liang</author>
<author>Dan Klein</author>
</authors>
<title>A game-theoretic approach to generating spatial descriptions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 conference on EMNLP.</booktitle>
<contexts>
<context position="2233" citStr="Golland et al., 2010" startWordPosition="364" endWordPosition="367">nvironment for grounded language that is familiar to everyone. The entities are common, everyday objects, and the knowledge necessary to address this problem is of general use across many domains. We present a system that leverages user interaction with 3D scenes to generate training data for semantic parsing approaches. Previous semantic parsing work has dealt with grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and with connecting language to spatial relationships (Golland et al., 2010; Artzi and Zettlemoyer, 2013). Semantic parsing methods can also be applied to many aspects of text to scene generation. Furthermore, work on parsing instructions to robots (Matuszek et al., 2013; Tellex et al., 2014) has analogues in the context of discourse about physical scenes. In this extended abstract, we formalize the text to scene generation problem and describe it as a task for semantic parsing methods. To motivate this problem, we present a prototype system that incorporates simple spatial knowledge, and parses natural text to a semantic representation. By learning priors on spatial</context>
</contexts>
<marker>Golland, Liang, Klein, 2010</marker>
<rawString>Dave Golland, Percy Liang, and Dan Klein. 2010. A game-theoretic approach to generating spatial descriptions. In Proceedings of the 2010 conference on EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Thomas Kollar</author>
</authors>
<title>Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="2089" citStr="Krishnamurthy and Kollar, 2013" startWordPosition="341" endWordPosition="344"> implicit relations that are likely to hold even if they are not explicitly stated by the input text. Text to scene generation offers a rich, interactive environment for grounded language that is familiar to everyone. The entities are common, everyday objects, and the knowledge necessary to address this problem is of general use across many domains. We present a system that leverages user interaction with 3D scenes to generate training data for semantic parsing approaches. Previous semantic parsing work has dealt with grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and with connecting language to spatial relationships (Golland et al., 2010; Artzi and Zettlemoyer, 2013). Semantic parsing methods can also be applied to many aspects of text to scene generation. Furthermore, work on parsing instructions to robots (Matuszek et al., 2013; Tellex et al., 2014) has analogues in the context of discourse about physical scenes. In this extended abstract, we formalize the text to scene generation problem and describe it as a task for semantic parsing methods. To motivate this problem, we present a </context>
</contexts>
<marker>Krishnamurthy, Kollar, 2013</marker>
<rawString>Jayant Krishnamurthy and Thomas Kollar. 2013. Jointly learning to parse and perceive: Connecting natural language to the physical world. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Nicholas Fitzgerald</author>
<author>Luke Zettlemoyer</author>
<author>Liefeng Bo</author>
<author>Dieter Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2012</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<contexts>
<context position="2056" citStr="Matuszek et al., 2012" startWordPosition="337" endWordPosition="340">, we focus on inferring implicit relations that are likely to hold even if they are not explicitly stated by the input text. Text to scene generation offers a rich, interactive environment for grounded language that is familiar to everyone. The entities are common, everyday objects, and the knowledge necessary to address this problem is of general use across many domains. We present a system that leverages user interaction with 3D scenes to generate training data for semantic parsing approaches. Previous semantic parsing work has dealt with grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and with connecting language to spatial relationships (Golland et al., 2010; Artzi and Zettlemoyer, 2013). Semantic parsing methods can also be applied to many aspects of text to scene generation. Furthermore, work on parsing instructions to robots (Matuszek et al., 2013; Tellex et al., 2014) has analogues in the context of discourse about physical scenes. In this extended abstract, we formalize the text to scene generation problem and describe it as a task for semantic parsing methods. To mot</context>
</contexts>
<marker>Matuszek, Fitzgerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Nicholas Fitzgerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A joint model of language and perception for grounded attribute learning. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Evan Herbst</author>
<author>Luke Zettlemoyer</author>
<author>Dieter Fox</author>
</authors>
<title>Learning to parse natural language commands to a robot control system. In Experimental Robotics.</title>
<date>2013</date>
<contexts>
<context position="2429" citStr="Matuszek et al., 2013" startWordPosition="395" endWordPosition="398">. We present a system that leverages user interaction with 3D scenes to generate training data for semantic parsing approaches. Previous semantic parsing work has dealt with grounding text to physical attributes and relations (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013), generating text for referring to objects (FitzGerald et al., 2013) and with connecting language to spatial relationships (Golland et al., 2010; Artzi and Zettlemoyer, 2013). Semantic parsing methods can also be applied to many aspects of text to scene generation. Furthermore, work on parsing instructions to robots (Matuszek et al., 2013; Tellex et al., 2014) has analogues in the context of discourse about physical scenes. In this extended abstract, we formalize the text to scene generation problem and describe it as a task for semantic parsing methods. To motivate this problem, we present a prototype system that incorporates simple spatial knowledge, and parses natural text to a semantic representation. By learning priors on spatial knowledge (e.g., typical positions of objects, and common spatial relations) our system addresses inference of implicit spatial constraints. The user can interactively manipulate the generated sc</context>
</contexts>
<marker>Matuszek, Herbst, Zettlemoyer, Fox, 2013</marker>
<rawString>Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Dieter Fox. 2013. Learning to parse natural language commands to a robot control system. In Experimental Robotics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
</authors>
<title>WordNet: a lexical database for english.</title>
<date>1995</date>
<publisher>CACM.</publisher>
<contexts>
<context position="9271" citStr="Miller, 1995" startWordPosition="1540" endWordPosition="1541">2 to process the input text and use rules to match dependency patterns. 3.1 Scene generation During scene generation, we want to construct the most likely scene given the input text. We first parse the text into a scene template and use it to select appropriate models from the database. We then perform object layout and arrangement given the priors on spatial knowledge. Scene Template Parsing We use the Stanford coreference system to determine when the same object is being referred to. To identify objects, we look for noun phrases and use the head word as the category, filtering with WordNet (Miller, 1995) to determine which objects are visualizable (under the physical object synset, excluding locations). To identify properties of the objects, we extract other adjectives and nouns in the noun phrase. We also match syntactic dependency patterns such as “X is made of Y” to extract more attributes and keywords. Finally, we use dependency patterns to extract spatial relations between objects. 2http://nlp.stanford.edu/software/corenlp.shtml Figure 3: Select “a blue office chair” and “a wooden desk” from the models database Object Selection Once we have the scene template, we use the keywords associa</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G.A. Miller. 1995. WordNet: a lexical database for english. CACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manolis Savva</author>
<author>Angel X Chang</author>
<author>Gilbert Bernstein</author>
<author>Christopher D Manning</author>
<author>Pat Hanrahan</author>
</authors>
<title>On being the right scale: Sizing large collections of 3D models.</title>
<date>2014</date>
<tech>Technical Report CSTR 2014-03.</tech>
<institution>Stanford University</institution>
<contexts>
<context position="8230" citStr="Savva et al., 2014" startWordPosition="1360" endWordPosition="1363">s: CAKE, TABLE ON(CAKE, TABLE) Objects: PLATE, FORK Spatial KB 3D Models Object Selection INTERACTION Semantic Parsing 18 Relation P(relation) V ol(AnB) V ol(A) V ol(An right (B)) V ol(A) 1(dist(A, B) &lt; tnear) Table 1: Definitions of spatial relation using object bounding box computations. thesis and containing about 12490 mostly indoor objects (Fisher et al., 2012). These models have text associated with them in the form of names and tags, and category labels. In addition, we assume the models have been scaled to physically plausible sizes and oriented with consistent up and front direction (Savva et al., 2014). All models are indexed in a database so they can be queried at runtime for retrieval. 3 System Description We present how the parsed representations are used by our system to demonstrate the key issues that have to be addressed during text to scene generation. Our current implementation uses a simple deterministic approach to map text to the scene template and user actions on the scene. We use the Stanford CoreNLP pipeline2 to process the input text and use rules to match dependency patterns. 3.1 Scene generation During scene generation, we want to construct the most likely scene given the i</context>
</contexts>
<marker>Savva, Chang, Bernstein, Manning, Hanrahan, 2014</marker>
<rawString>Manolis Savva, Angel X. Chang, Gilbert Bernstein, Christopher D. Manning, and Pat Hanrahan. 2014. On being the right scale: Sizing large collections of 3D models. Stanford University Technical Report CSTR 2014-03.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Stefanie Tellex</author>
</authors>
<title>Pratiksha Thaker,</title>
<location>Joshua Joseph, and</location>
<marker>Tellex, </marker>
<rawString>Stefanie Tellex, Pratiksha Thaker, Joshua Joseph, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicholas Roy</author>
</authors>
<title>Learning perceptually grounded word meanings from unaligned parallel data.</title>
<date>2014</date>
<journal>Machine Learning.</journal>
<marker>Roy, 2014</marker>
<rawString>Nicholas Roy. 2014. Learning perceptually grounded word meanings from unaligned parallel data. Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Winograd</author>
</authors>
<title>Understanding natural language. Cognitive psychology.</title>
<date>1972</date>
<contexts>
<context position="12987" citStr="Winograd, 1972" startWordPosition="2140" endWordPosition="2141">ure 5: Left: initial scene. Right: after input “Put a lamp on the table”. We do this by maximizing the probability of a new scene template given the requested action and previous scene template (see Figure 5 for an example). 4 Future Directions We described a system prototype to motivate approaching text to scene generation as a semantic parsing application. While this prototype illustrates inference of implicit constraints using prior knowledge, it still relies on hand coded rules for mapping text to the scene representation. This is similar to most previous work on text to scene generation (Winograd, 1972; Coyne and Sproat, 2001) and limits handling of natural language. More recently, (Zitnick et al., 2013) used data to learn how to ground sentences to a CRF representing 2D clipart scenes. Similarly, we plan to investigate using data to learn how to ground sentences to 3D scenes. Spatial knowledge can be helpful for resolving ambiguities during parsing. For instance, from spatial priors of object positions and reasoning with physical constraints we can disambiguate the attachment of “next to” in “there is a book on the table next to the lamp”. The book and lamp are likely on the table and thus</context>
</contexts>
<marker>Winograd, 1972</marker>
<rawString>Terry Winograd. 1972. Understanding natural language. Cognitive psychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lawrence Zitnick</author>
<author>Devi Parikh</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Learning the visual interpretation of sentences.</title>
<date>2013</date>
<booktitle>In IEEE Intenational Conference on Computer Vision (ICCV).</booktitle>
<contexts>
<context position="13091" citStr="Zitnick et al., 2013" startWordPosition="2155" endWordPosition="2158">g the probability of a new scene template given the requested action and previous scene template (see Figure 5 for an example). 4 Future Directions We described a system prototype to motivate approaching text to scene generation as a semantic parsing application. While this prototype illustrates inference of implicit constraints using prior knowledge, it still relies on hand coded rules for mapping text to the scene representation. This is similar to most previous work on text to scene generation (Winograd, 1972; Coyne and Sproat, 2001) and limits handling of natural language. More recently, (Zitnick et al., 2013) used data to learn how to ground sentences to a CRF representing 2D clipart scenes. Similarly, we plan to investigate using data to learn how to ground sentences to 3D scenes. Spatial knowledge can be helpful for resolving ambiguities during parsing. For instance, from spatial priors of object positions and reasoning with physical constraints we can disambiguate the attachment of “next to” in “there is a book on the table next to the lamp”. The book and lamp are likely on the table and thus next_to(book, lamp) should be more likely. User interaction is a natural part of text to scene generati</context>
</contexts>
<marker>Zitnick, Parikh, Vanderwende, 2013</marker>
<rawString>C. Lawrence Zitnick, Devi Parikh, and Lucy Vanderwende. 2013. Learning the visual interpretation of sentences. In IEEE Intenational Conference on Computer Vision (ICCV).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
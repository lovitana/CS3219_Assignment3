<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<title confidence="0.999284">
Individuality-preserving Voice Conversion for Articulation Disorders
Using Dictionary Selective Non-negative Matrix Factorization
</title>
<author confidence="0.991621">
Ryo Aihara, Tetsuya Takiguchi, Yasuo Ariki
</author>
<affiliation confidence="0.98373">
Graduate School of System Informatics, Kobe University
</affiliation>
<address confidence="0.681002">
1-1, Rokkodai, Nada, Kobe, 6578501, Japan
</address>
<email confidence="0.591659">
aihara@me.cs.scitec.kobe-u.ac.jp,
takigu@kobe-u.ac.jp,
ariki@kobe-u.ac.jp
</email>
<sectionHeader confidence="0.992047" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999981266666667">
We present in this paper a voice conver-
sion (VC) method for a person with an ar-
ticulation disorder resulting from athetoid
cerebral palsy. The movements of such
speakers are limited by their athetoid
symptoms, and their consonants are of-
ten unstable or unclear, which makes it
difficult for them to communicate. In
this paper, exemplar-based spectral con-
version using Non-negative Matrix Factor-
ization (NMF) is applied to a voice with
an articulation disorder. In order to pre-
serve the speaker’s individuality, we use a
combined dictionary that was constructed
from the source speaker’s vowels and tar-
get speaker’s consonants. However, this
exemplar-based approach needs to hold
all the training exemplars (frames), and
it may cause mismatching of phonemes
between input signals and selected ex-
emplars. In this paper, in order to re-
duce the mismatching of phoneme align-
ment, we propose a phoneme-categorized
sub-dictionary and a dictionary selec-
tion method using NMF. The effective-
ness of this method was confirmed by
comparing its effectiveness with that of
a conventional Gaussian Mixture Model
(GMM)-based and conventional NMF-
based method.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974166666667">
In this study, we focused on a person with an
articulation disorder resulting from the athetoid
type of cerebral palsy. About two babies in 1,000
are born with cerebral palsy (Hollegaard et al.,
2013). Cerebral palsy results from damage to the
central nervous system, and the damage causes
movement disorders. Cerebral palsy is classified29
into the following types: 1)spastic, 2)athetoid,
3)ataxic, 4)atonic, 5)rigid, and a mixture of these
types (Canale and Campbell, 2002).
Athetoid symptoms develop in about 10-15% of
cerebral palsy sufferers (Hollegaard et al., 2013).
In the case of a person with this type of articulation
disorder, his/her movements are sometimes more
unstable than usual. That means their utterances
(especially their consonants) are often unstable or
unclear due to the athetoid symptoms. Athetoid
symptoms also restrict the movement of their arms
and legs. Most people suffering from athetoid
cerebral palsy cannot communicate by sign lan-
guage or writing, so there is great need for voice
systems for them.
In this paper, we propose a voice conversion
(VC) method for articulation disorders. Regard-
ing speech recognition for articulation disorders,
the recognition rate using a speaker-independent
model which is trained by well-ordered speech, is
3.5% (Matsumasa et al., 2009). This result im-
plies that the utterance of a person with an artic-
ulation disorder is difficult to understand for peo-
ple who have not communicated with them before.
In recent years, people with an articulation dis-
order may use slideshows and a previously syn-
thesized voice when they give a lecture. How-
ever, because their movement is restricted by their
athetoid symptoms, to make slides or synthesize
their voice in advance is hard for them. Peo-
ple with articulation disorders desire a VC sys-
tem that converts their voice into a clear voice
that preserves their voice’s individuality. Rudz-
icz et al. (Rudzicz, 2011; Rudzicz, 2014) proposed
speech adjustment method for people with articu-
lation disorders based on the observations from the
database.
In (Aihara et al., 2014), we proposed
individuality-preserving VC for articulation
disorders. In our VC, source exemplars and
target exemplars are extracted from the parallel
</bodyText>
<note confidence="0.8028785">
Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 29–37,
Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.996164416666667">
training data, having the same texts uttered by
the source and target speakers. The input source
signal is expressed with a sparse representation
of the source exemplars using Non-negative
Matrix Factorization (NMF). By replacing a
source speaker’s exemplar with a target speaker’s
exemplar, the original speech spectrum is replaced
with the target speaker’s spectrum. People with
articulation disorders wish to communicate by
their own voice if they can; therefore, we pro-
posed a combined-dictionary, which consists of
a source speaker’s vowels and target speaker’s
well-ordered consonants. In the voice of a person
with an articulation disorder, their consonants are
often unstable and that makes their voices unclear.
Their vowels are relatively stable compared
to their consonants. Hence, by replacing the
articulation-disordered basis of consonants only,
a voice with an articulation disorder is converted
into a non-disordered voice that preserves the
individuality of the speaker’s voice.
In this paper, we propose advanced
individuality-preserving VC using NMF. In
order to avoid a mixture of the source and target
spectra in a converted phoneme, we applied a
phoneme-categorized dictionary and a dictionary
selection method to our VC using NMF. In
conventional NMF-based VC, the number of
dictionary frames becomes large because the
dictionary holds all the training exemplar frames.
Therefore, it may cause phoneme mismatching
between input signals and selected exemplars and
some frames of converted spectra might be mixed
with the source and target spectra. In this paper,
a training exemplar is divided into a phoneme-
categorized sub-dictionary, and an input signal is
converted by using the selected sub-dictionary.
The effectiveness of this method was confirmed
by comparing it with a conventional NMF-based
method and a conventional Gaussian Mixture
Model (GMM)-based method.
The rest of this paper is organized as follows:
In Section 2, related works are introduced. In Sec-
tion 3, the basic idea of NMF-based VC is de-
scribed. In Section 4, our proposed method is de-
scribed. In Section 5, the experimental data are
evaluated, and the final section is devoted to our
conclusions. 30
</bodyText>
<sectionHeader confidence="0.998877" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.999967415841585">
Voice conversion (VC) is a technique for convert-
ing specific information in speech while maintain-
ing the other information in the utterance. One of
the most popular VC applications is speaker con-
version (Stylianou et al., 1998). In speaker con-
version, a source speaker’s voice individuality is
changed to a specified target speaker’s so that the
input utterance sounds as though a specified target
speaker had spoken it.
There have also been studies on several tasks
that make use of VC. Emotion conversion is a
technique for changing emotional information in
input speech while maintaining linguistic informa-
tion and speaker individuality (Veaux and Robet,
2011). In recent years, VC has been used for auto-
matic speech recognition (ASR) or speaker adap-
tation in text-to-speech (TTS) systems (Kain and
Macon, 1998). These studies show the varied uses
of VC.
Many statistical approaches to VC have been
studied (Valbret et al., 1992). Among these ap-
proaches, the Gaussian mixture model (GMM)-
based mapping approach (Stylianou et al., 1998)
is widely used. In this approach, the conversion
function is interpreted as the expectation value
of the target spectral envelope. The conversion
parameters are evaluated using Minimum Mean-
Square Error (MMSE) on a parallel training set.
A number of improvements in this approach have
been proposed. Toda et al. (Toda et al., 2007)
introduced dynamic features and the global vari-
ance (GV) of the converted spectra over a time
sequence. Helander et al. (Helander et al., 2010)
proposed transforms based on partial least squares
(PLS) in order to prevent the over-fitting problem
associated with standard multivariate regression.
There have also been approaches that do not re-
quire parallel data that make use of GMM adapta-
tion techniques (Lee and Wu, 2006) or eigen-voice
GMM (EV-GMM) (Toda et al., 2006).
In the field of assistive technology, Nakamura
et al. (Nakamura et al., 2012; Nakamura et al.,
2006) proposed GMM-based VC systems that re-
construct a speaker’s individuality in electrolaryn-
geal speech and speech recorded by NAM micro-
phones. These systems are effective for electrola-
ryngeal speech and speech recorded by NAM mi-
crophones however, because these statistical ap-
proaches are mainly proposed for speaker con-
version, the target speaker’s individuality will be
changed to the source speaker’s individuality. Peo-
ple with articulation disorders wish to communi-
cate by their own voice if they can and there is a
needs for individuality-preserving VC.
Text-to-speech synthesis (TTS) is a famous
voice application that is widely researched. Veaux
et al. (Veaux et al., 2012) used HMM-based speech
synthesis to reconstruct the voice of individu-
als with degenerative speech disorders resulting
from Amyotrophic Lateral Sclerosis (ALS). Ya-
magishi et al. (Yamagishi et al., 2013) proposed
a project named “Voice Banking and Reconstruc-
tion”. In that project, various types of voices are
collected and they proposed TTS for ALS using
that database. The difference between TTS and
VC is that TTS needs text input to synthesize
speech, whereas VC does not need text input. In
the case of people with articulation disorders re-
sulting from athetoid cerebral palsy, it is difficult
for them to input text because of their athetoid
symptoms.
Our proposed NMF-based VC (Takashima et
al., 2012) is an exemplar-based method using
sparse representation, which is different from the
conventional statistical method. In recent years,
approaches based on sparse representations have
gained interest in a broad range of signal process-
ing. In approaches based on sparse representa-
tions, the observed signal is represented by a lin-
ear combination of a small number of bases. In
some approaches for source separation, the atoms
are grouped for each source, and the mixed sig-
nals are expressed with a sparse representation of
these atoms. By using only the weights of the
atoms related to the target signal, the target sig-
nal can be reconstructed. Gemmeke et al. (Gem-
meke et al., 2011) also propose an exemplar-based
method for noise-robust speech recognition. In
that method, the observed speech is decomposed
into the speech atoms, noise atoms, and their
weights. Then the weights of the speech atoms are
used as phonetic scores (instead of the likelihoods
of hidden Markov models) for speech recognition.
In (Takashima et al., 2012), we proposed noise-
robust VC using NMF. The noise exemplars,
which are extracted from the before- and after-
utterance sections in an observed signal, are used
as the noise-dictionary, and the VC process is
combined with an NMF-based noise-reduction
method. On the other hand, NMF is one of the
clustering methods. In our exemplar-based VC, if
</bodyText>
<page confidence="0.998454">
31
</page>
<bodyText confidence="0.999893">
the phoneme label of the source exemplar is given,
we can discriminate the phoneme of the input sig-
nal by using NMF. In this paper, we proposed a
dictionary selection method using this property of
NMF.
</bodyText>
<sectionHeader confidence="0.885855" genericHeader="method">
3 Voice Conversion Based on
Non-negative Matrix Factorization
</sectionHeader>
<subsectionHeader confidence="0.999959">
3.1 Basic Idea
</subsectionHeader>
<bodyText confidence="0.999923666666667">
In the exemplar-based approach, the observed sig-
nal is represented by a linear combination of a
small number of bases.
</bodyText>
<equation confidence="0.97811">
J
Xl � EJ, ajhj,l = Ahl (1)
</equation>
<bodyText confidence="0.9980795">
xl represents the l-th frame of the observation.
aj and hj,l represent the j-th basis and the
weight, respectively. A = [a1 ... aJ] and hl =
[h1,l ... hJ,l]T are the collection of the bases and
the stack of weights. In this paper, each basis de-
notes the exemplar of the spectrum, and the col-
lection of exemplar A and the weight vector hl are
called the ‘dictionary’ and ‘activity’, respectively.
When the weight vector hl is sparse, the observed
signal can be represented by a linear combination
of a small number of bases that have non-zero
weights. Eq. (1) is expressed as the inner product
of two matrices using the collection of the frames
or bases.
</bodyText>
<equation confidence="0.9999145">
X AH (2)
X = [x1, ... , xL], H = [h1,. . . , hL]. (3)
</equation>
<bodyText confidence="0.999145842105263">
L represents the number of the frames.
Fig. 1 shows the basic approach of our
exemplar-based VC, where D, L, and J represent
the numbers of dimensions, frames, and bases,
respectively. Our VC method needs two dictio-
naries that are phonemically parallel. As repre-
sents a source dictionary that consists of the source
speaker’s exemplars and At represents a target
dictionary that consists of the target speaker’s ex-
emplars. These two dictionaries consist of the
same words and are aligned with dynamic time
warping (DTW) just as conventional GMM-based
VC is. Hence, these dictionaries have the same
number of bases.
This method assumes that when the source sig-
nal and the target signal (which are the same words
but spoken by different speakers) are expressed
with sparse representations of the source dictio-
nary and the target dictionary, respectively, the ob-
tained activity matrices are approximately equiv-
alent. Fig. 2 shows an example of the activity
matrices estimated from a Japanese word “ikioi”
(“vigor” in English), where one is uttered by a
male, the other is uttered by a female, and each
dictionary is structured from just one word “ikioi”
as the simple example.
As shown in Fig. 2, these activities have high
energies at similar elements. For this reason, we
assume that when there are parallel dictionaries,
the activity of the source features estimated with
the source dictionary may be able to be substi-
tuted with that of the target features. Therefore,
the target speech can be constructed using the tar-
get dictionary and the activity of the source signal
as shown in Fig. 1. In this paper, we use Non-
negative Matrix Factorization (NMF), which is a
sparse coding method in order to estimate the ac-
tivity matrix.
</bodyText>
<figure confidence="0.622812">
Copy
</figure>
<figureCaption confidence="0.980053">
Figure 1: Basic approach of NMF-based voice
conversion
</figureCaption>
<figure confidence="0.644411">
Frame of source speech Frame of target speech
</figure>
<figureCaption confidence="0.99491">
Figure 2: Activity matrices for parallel utterances
</figureCaption>
<subsectionHeader confidence="0.8291175">
3.2 Individuality-preserving Voice
Conversion Using Combined Dictionary
</subsectionHeader>
<bodyText confidence="0.9999605">
In order to make a parallel dictionary, some pairs
of parallel utterances are needed, where each pair
consists of the same text. One is spoken by a per-
son with an articulation disorder (source speaker),
</bodyText>
<page confidence="0.990715">
32
</page>
<bodyText confidence="0.999980711111111">
and the other is spoken by a physically unim-
paired person (target speaker). Spectrum en-
velopes, which are extracted from parallel utter-
ances, are phonemically aligned by using DTW.
In order to estimate activities of source features
precisely, segment features, which consist of some
consecutive frames, are constructed. Target fea-
tures are constructed from consonant frames of the
target’s aligned spectrum and vowel frames of the
source’s aligned spectrum. Source and target dic-
tionaries are constructed by lining up each of the
features extracted from parallel utterances.
The vowels voiced by a speaker strongly indi-
cate the speaker’s individuality. On the other hand,
consonants of people with articulation disorders
are often unstable. Fig. 3(a) shows an example
of the spectrogram for the word “ikioi” (“vigor”
in English) of a person with an articulation dis-
order. The spectrogram of a physically unim-
paired person speaking the same word is shown in
Fig. 3(b). In Fig. 3(a), the area labeled “k” is not
clear, compared to the same region in to Fig. 3(b).
These figures indicate that consonants of people
with articulation disorders are often unstable and
this deteriorates their voice intelligibility. In or-
der to preserve their voice individuality, we use
a “combined-dictionary” that consists of a source
speaker’s vowels and target speaker’s consonants.
We replace the target dictionary A3 in Fig. 1
with the “combined-dictionary”. Input source
features X3, which consist of an articulation-
disordered spectrum and its segment features, are
decomposed into a linear combination of bases
from the source dictionary A3 by NMF. The
weights of the bases are estimated as an activity
H3. Therefore, the activity includes the weight in-
formation of input features for each basis. Then,
the activity is multiplied by a combined-dictionary
in order to obtain converted spectral features ˆXt,
which are represented by a linear combination of
bases from the source speaker’s vowels and tar-
get speaker’s consonants. Because the source and
target are parallel phonemically, the bases used in
the converted features are phonemically the same
as that of the source features.
</bodyText>
<subsectionHeader confidence="0.988668">
3.3 Problems
</subsectionHeader>
<bodyText confidence="0.99988425">
In the NMF-based approach described in Sec. 3.2,
the parallel dictionary consists of the parallel train-
ing data themselves. Therefore, as the number
of the bases in the dictionary increases, the input
</bodyText>
<figure confidence="0.9925809">
Basis ID in source dictionary
200
250
100
150
50
20 40 60 80 100 120 140
Basis ID in target dictionary
200
250
100
150
50
20 40 60 80 100 120
Source
spectral features
(D x L)
D
Converted Source and target
spectral features dictionaries
(D x L) Activity of
(D x J)
source signal
(J x L)
X
Xs
ˆ
L
t
Construction
Activity
estimation
Parallel data
As
At
J
H
H
s
s
</figure>
<bodyText confidence="0.99985494117647">
signal comes to be represented by a linear com-
bination of a large number of bases rather than a
small number of bases. When the number of bases
that represent the input signal becomes large, the
assumption of similarity between source and tar-
get activities may be weak due to the influence of
the mismatch between the input signal and the se-
lected bases. Moreover, in the case of a combined-
dictionary, the input articulation-disordered spec-
trum may come to be represented by a combi-
nation of vowels and consonants. We assume
that this problem degrades the performance of our
exemplar-based VC. Hence, we use a phoneme-
categorized sub-dictionary in place of the large
dictionary in order to reduce the number of the
bases that represent the input signal and avoid the
mixture of vowels and consonants.
</bodyText>
<figure confidence="0.9248466">
i k i oi
Time [Sec]
(a) Spoken by a person with an articulation disorder
Time [Sec]
(b) Spoken by a physically unimpaired person
</figure>
<sectionHeader confidence="0.931710666666667" genericHeader="method">
4 Non-negative Matrix Factorization
Using a Phoneme-categorized
Dictionary
</sectionHeader>
<subsectionHeader confidence="0.995611">
4.1 Phoneme-categorized Dictionary
</subsectionHeader>
<bodyText confidence="0.999964076923077">
Fig. 4 shows how to construct the sub-dictionary.
As and At imply the source and target dictionary
which hold all the bases from training data. These
dictionaries are divided into K dictionaries. In
this paper, the dictionaries are divided into 10 cat-
egories according to the Japanese phoneme cate-
gories shown in Table 1.
In order to select the sub-dictionary, a
“categorizing-dictionary”, which consists of the
representative vector from each sub-dictionary, is
constructed. The representative vectors for each
phoneme category consist of the mean vectors of
the Gaussian Mixture Model (GMM).
</bodyText>
<equation confidence="0.991350125">
α(k)
m N(x(k)
n , µ(k)
m , Σ(k)
m ) (4)
Mk, α(k)
m , µ(k)
m and Σ(k)
</equation>
<bodyText confidence="0.9998953">
m represent the number
of the Gaussian mixture, the weights of mixture,
mean and variance of the m-th mixture of the
Gaussian, in the k-th sub-dictionary, respectively.
Each parameter is estimated by using an EM algo-
rithm.
The basis of the categorizing-dictionary, which
corresponds to the k-th sub-dictionary Φsk, is rep-
resented using the estimated phoneme GMM as
follows:
</bodyText>
<equation confidence="0.999216">
θk = [µ(k)
1 ,... , µ(k) Mk] (5)
Φsk = [x(k)
1 , ... , x(k)
Nk] (6)
</equation>
<bodyText confidence="0.998944">
Nk represents the number of frames of the k-th
sub-dictionary. The categorizing-dictionary Θ is
given as follows:
</bodyText>
<equation confidence="0.991868">
Θ = [θ1,...,θK] (7)
</equation>
<subsectionHeader confidence="0.765437">
4.2 Dictionary Selection and Voice
</subsectionHeader>
<figure confidence="0.974403666666667">
Conversion
Frequency [Hz]
5000
4000
3000
6000
2000
1000
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Frequency [Hz]
5000
4000
3000
6000
2000
1000
0
0.2 0.4 0.6 0.8 1
i
k
i
oi
6000
5000
Frequency [Hz]
4000
3000
2000
1000
0
i
k
i
oi
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Mk
m=1
p(x(k)
n ) =
Time [Sec] Fig. 5 shows the flow of the dictionary selection
(c) Converted by NMF-based VC and VC. The input spectral features Xs are rep-
</figure>
<figureCaption confidence="0.8615235">
resented by a linear combination of bases from
Figure 3: Examples of spectrogram //i k i oi 33 the categorizing-dictionary Θ. The weights of the
</figureCaption>
<figure confidence="0.999637073170732">
Spectral
envelope
Source
training speech
phoneme categorized
sub-dictionaries
Vowels are replaced with
source speaker’s spectrum
As
STRAIGHT
Target
training speech
Articulation-
disordered spectrum
DP-matching
At
STRAIGHT
Well-ordered
spectrum
Parallel Dictionaries
Categorization
Representative
vectors
Vowel sub-dictionaries Consonant sub-dictionaries
Θ
Φ1 s
...
Φk s
Φ k+1
s
...
ΦKs
Categorizing
Dictionary
Φ1 s
...
Φk s
Φ k+1
t
...
ΦKt
</figure>
<figureCaption confidence="0.999795">
Figure 4: Making a sub-dictionary
</figureCaption>
<bodyText confidence="0.9993444">
On the other hand, if the selected sub-dictionary
Φskˆ is related to vowels, the l-th frame of the con-
verted spectral feature ˆyl is constructed by using
the activity and the sub-dictionary of the source
speaker Φsˆk.
</bodyText>
<equation confidence="0.884156">
ˆyl = Φsˆkhˆk,l (16)
</equation>
<tableCaption confidence="0.99637">
Table 1: Japanese phoneme categories
</tableCaption>
<figure confidence="0.924195">
Category phoneme
vowels a a
e e
i i
o o
u u
</figure>
<figureCaption confidence="0.8892936">
consonants plosives Q, b, d, dy, g, gy, k, ky, p, t
fricatives ch, f, h, hy, j, s, sh, ts, z
nasals m, my ny, N
semivowels w,y
liquid r, ry
</figureCaption>
<bodyText confidence="0.707656">
bases are represented as activities Hs�.
</bodyText>
<equation confidence="0.9846465">
Xs pz ΘHs� s.t. Hs� &gt; 0 (8)
Xs = [xs1, ... , xsL] (9)
Hs � = [hs �1, . . . , hs �L] (10)
hs�l = [hsθ1l, ... , hsθ�l]T (11)
T
hs θkl = [hsθ1l, ... , hs θMkl] (12)
</equation>
<bodyText confidence="0.9998806">
Then, the l-th frame of input feature xsl is rep-
resented by a linear combination of bases from the
sub-dictionary of the source speaker. The sub-
dictionary Φsˆk, which corresponds to xl, is se-
lected as follows:
</bodyText>
<equation confidence="0.9991276">
kˆ = arg max
k
hs (13)
θal
xl = Φsˆkhˆk,l (14)
</equation>
<bodyText confidence="0.9997585">
The activity hl,ˆk in Eq. (14) is estimated from the
selected source speaker sub-dictionary.
If the selected sub-dictionary Φskˆ is related to
consonants, the l-th frame of the converted spec-
tral feature ˆyl is constructed by using the activity
and the sub-dictionary of the target speaker Φtˆk.
</bodyText>
<equation confidence="0.589138">
ˆyl =Φt 34
ˆkhˆk,l (15)
</equation>
<figureCaption confidence="0.972217">
Figure 5: NMF-based voice conversion using cat-
egorized dictionary
</figureCaption>
<sectionHeader confidence="0.984976" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.942514">
5.1 Experimental Conditions
</subsectionHeader>
<bodyText confidence="0.999419625">
The proposed VC technique was evaluated by
comparing it with the conventional NMF-based
method (Aihara et al., 2014) (referred to as the
“sample-based method” in this paper) and the
conventional GMM-based method (Stylianou et
al., 1998) using clean speech data. We recorded
432 utterances (216 words, each repeated two
times) included in the ATR Japanese speech
</bodyText>
<figure confidence="0.99014352">
l-th frame of
input spectral features
Categorizing
Dictionary
Activityh
h
...
s
...ekl
l-th frame of converted
spectral features
y l
xl
ˆ
Select the sub-dictionary
Φ1 s
Φ1 s
phoneme-categorized sub-dictionaries
k = arg max1ixMk hek
k
...
...
Φ k +1
t
Φ k+1
s
Activity h ˆ
k, l
l
...
...
Copy
h
Φs
K
Φt
K
s
eK
l
xl
O
h
s
l
s
e1
l
11xMkhs
θkl
</figure>
<equation confidence="0.640965666666667">
Mk
= arg max
k m=1
</equation>
<bodyText confidence="0.99990037254902">
database (Kurematsu et al., 1990). The speech sig-
nals were sampled at 12 kHz and windowed with
a 25-msec Hamming window every 10 msec. A
physically unimpaired Japanese male in the ATR
Japanese speech database, was chosen as a target
speaker.
In the proposed and sample-based methods,
the number of dimensions of the spectral fea-
ture is 2,565. It consists of a 513-dimensional
STRAIGHT spectrum (Kawahara et al., 1999)
and its consecutive frames (the 2 frames com-
ing before and the 2 frames coming after). The
Gaussian mixture, which is used to construct
a categorizing-dictionary, is 1/500 of the num-
ber of bases of each sub-dictionary. The num-
ber of iterations for estimating the activity in
the proposed and sample-based methods was
300. In the conventional GMM-based method,
MFCC+∆MFCC+∆∆MFCC is used as a spectral
feature. Its number of dimensions is 74. The num-
ber of Gaussian mixtures is set to 64, which is ex-
perimentally selected.
In this paper, F0 information is converted using
a conventional linear regression based on the mean
and standard deviation (Toda et al., 2007). The
other information such as aperiodic components,
is synthesized without any conversion.
We conducted a subjective evaluation of 3 top-
ics. A total of 10 Japanese speakers took part
in the test using headphones. For the “listening
intelligibility” evaluation, we performed a MOS
(Mean Opinion Score) test (”INTERNATIONAL
TELECOMMUNICATION UNION”, 2003). The
opinion score was set to a 5-point scale (5: excel-
lent, 4: good, 3: fair, 2: poor, 1: bad). Twenty-two
words that are difficult for a person with an artic-
ulation disorder to utter were evaluated. The sub-
jects were asked about the listening intelligibility
in the articulation-disordered voice, the voice con-
verted by our proposed method, and the GMM-
based converted voice.
On the “similarity” evaluation, the XAB test
was carried out. In the XAB test, each subject lis-
tened to the articulation-disordered voice. Then
the subject listened to the voice converted by the
two methods and selected which sample sounded
most similar to the articulation-disordered voice.
On the “naturalness” evaluation, a paired com-
parison test was carried out, where each subject
listened to pairs of speech converted by the two
methods and selected which sample sounded more35
</bodyText>
<figure confidence="0.873441833333333">
natural.
i k i oi
Time [Sec]
(a) Converted by proposed method
Time [Sec]
(b) Converted by GMM-based VC
</figure>
<figureCaption confidence="0.9396395">
Figure 6: Examples of converted spectrogram //i k
i oi
</figureCaption>
<subsectionHeader confidence="0.872315">
5.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.990301846153846">
Fig. 6(a) and 6(b) show examples of converted
spectrograms using our proposed method and the
conventional GMM-based method, respectively.
In Fig. 6(a), there are fewer misconversions in the
vowel part compared to Fig. 3(c). Moreover, by
using GMM-based conversion, the area labeled
“oi” becomes unclear compared to NMF-based
conversion.
Fig. 7 shows the results of the MOS test for lis-
tening intelligibility. The error bars show a 95%
confidence score; thus, our proposed VC method
is shown to be able to improve the listening intel-
ligibility and clarity of consonants. On the other
hand, GMM-based conversion can improve the
clarity of consonants, but it deteriorates the lis-
tening intelligibility. This is because GMM-based
conversion has the effect of noise resulting from
measurement error. Our proposed VC method also
has the effect of noise, but it is less than that cre-
ated by GMM-based conversion.
Fig. 8 shows the results of the XAB test on
the similarity to the source speaker and natural-
ness of the converted voice. The error bars show a
95% confidence score. Our proposed VC method
obtained a higher score than Sample-based and
GMM-based conversion on similarity. Fig. 9
</bodyText>
<figure confidence="0.998033727272727">
Frequency [Hz]
5000
4000
3000
6000
2000
1000
0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
6000
5000
Frequency [Hz]
4000
3000
2000
1000
0
i
k
i
oi
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
</figure>
<bodyText confidence="0.9938694">
shows the preference score on the naturalness. The
error bars show a 95% confidence score. Our pro-
posed VC also method obtained a higher score
than Sample-based and GMM-based conversion
methods in regard to naturalness.
</bodyText>
<figureCaption confidence="0.9264265">
Figure 7: Results of MOS test on listening intelli-
gibility
</figureCaption>
<figure confidence="0.981024333333333">
Preference Score 1 1
0.9 0.9
0.8 0.8
0.7 0.7
0.6 0.6
0.5 0.5
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
0 0
NMF GMM Proposed NMF Proposed GMM
</figure>
<figureCaption confidence="0.980356">
Figure 8: Preference scores for the individuality
</figureCaption>
<figure confidence="0.994181736842105">
Score 1
Preference
1
0.9
0.9
0.8
0.8
0.7
0.7
0.6
0.6
0.5 0.5
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
0
NMF GMM Proposed NMF 0 GMM
Proposed
</figure>
<figureCaption confidence="0.999229">
Figure 9: Preference scores for the naturalness
</figureCaption>
<sectionHeader confidence="0.999224" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999918">
We proposed a spectral conversion method based
on NMF for a voice with an articulation disorder.
Our proposed method introduced a dictionary-
selection method for conventional NMF-based
VC. Experimental results demonstrated that our
VC method can improve the listening intelligibil-
ity of words uttered by a person with an articu-
lation disorder. Moreover, compared to conven-
tional GMM-based VC and conventional NMF-
based VC, our proposed VC method can preserve
the individuality of the source speaker’s voice and36
the naturalness of the voice. In this study, there
was only one subject person, so in future experi-
ments, we will increase the number of subjects and
further examine the effectiveness of our method.
</bodyText>
<sectionHeader confidence="0.99848" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9996871">
R. Aihara, R. Takashima, T. Takiguchi, and Y. Ariki.
2014. A preliminary demonstration of exemplar-
based voice conversion for articulation disor-
ders using an individuality-preserving dictionary.
EURASIP Journal on Audio, Speech, and Music
Processing, 2014:5, doi:10.1186/1687-4722-2014-
5.
S. T. Canale and W. C. Campbell. 2002. Campbell’s
operative orthopaedics. Technical report, Mosby-
Year Book.
</reference>
<equation confidence="0.58634">
0 0 0 0 0 0 0 0 0
0 .1 2 3 4 5 6 7 .8 .9 1
</equation>
<reference confidence="0.971875105263158">
J. F. Gemmeke, T. Viratnen, and A. Hurmalainen.
2011. Exemplar-based sparse representations for
noise robust automatic speech recognition. IEEE
Trans. Audio, Speech and Language Processing, vol.
19, no. 7, pages 2067–2080.
E. Helander, T. Virtanen, J. Nurminen, and M. Gab-
bouj. 2010. Voice conversion using partial least
squares regression. IEEE Trans. Audio, Speech,
Lang. Process., vol. 18, Issue:5, pages 912–921.
Mads Vilhelm Hollegaard, Kristin Skogstrand, Poul
Thorsen, Bent Norgaard-Pedersen, David Michael
Hougaard, and Jakob Grove. 2013. Joint analysis
of SNPs and proteins identifies regulatory IL18 gene
variations decreasing the chance of spastic cerebral
palsy. Human Mutation, Vol. 34, pages 143–148.
0 0 0 0 0 0 0 0 0
0 .1 2 3 4 .5 6 7 .8 .9 1
INTERNATIONAL TELECOMMUNICATION
UNION. 2003. Methods for objective and subjec-
tive assessment of quality. ITU-T Recommendation
P.800.
A. Kain and M. W. Macon. 1998. Spectral voice con-
version for text-to-speech synthesis. in ICASSP, vol.
1, pages 285–288.
H. Kawahara, I. Masuda-Katsuse, and A.de Cheveigne.
1999. Restructuring speech representations using
a pitch-adaptive time-frequency smoothing and an
instantaneous-frequencybased F0 extraction: possi-
ble role of a repetitive structure in sounds. Speech
Communication, 27(3-4):187–207.
A. Kurematsu, K. Takeda, Y. Sagisaka, S. Katagiri,
H. Kuwabara, and K. Shikano. 1990. ATR Japanese
speech database as a tool of speech recognition and
synthesis. Speech Communication, 9:357–363.
C. H. Lee and C. H. Wu. 2006. Map-based adaptation
for speech conversion using adaptation data selec-
tion and non-parallel training. in Interspeech, pages
2254–2257.
</reference>
<figure confidence="0.9451604">
Proposed GMM source
MOS
2.7
2.6
2.5
2.4
2.3
2.2
2.1
2
</figure>
<reference confidence="0.999516857142857">
H. Matsumasa, T. Takiguchi, Y. Ariki, I. Li, and
T. Nakabayachi. 2009. Integration of metamodel
and acoustic model for dysarthric speech recogni-
tion. Journal of Multimedia, Volume 4, Issue 4,
pages 254–261.
K. Nakamura, T. Toda, H. Saruwatari, and K. Shikano.
2006. Speaking aid system for total laryngectomees
using voice conversion of body transmitted artificial
speech. in Interspeech, pages 148–151.
K. Nakamura, T. Toda, H. Saruwatari, and K. Shikano.
2012. Speaking-aid systems using GMM-
based voice conversion for electrolaryngeal speech.
Speech Communication, 54(1):134–146.
F. Rudzicz. 2011. Acoustic transformations to im-
prove the intelligibility of dysarthric speech. in
proc. the Second Workshop on Speech and Language
Processing for Assistive Technologies.
F. Rudzicz. 2014. Adjusting dysarthric speech sig-
nals to be more intelligible. in Computer Speech and
Language, 27(6), September, pages 1163–1177.
Y. Stylianou, O. Cappe, and E. Moilines. 1998. Con-
tinuous probabilistic transform for voice conver-
sion. IEEE Trans. Speech and Audio Processing,
6(2):131–142.
R. Takashima, T. Takiguchi, and Y. Ariki. 2012.
Exemplar-based voice conversion in noisy environ-
ment. in SLT, pages 313–317.
T. Toda, Y. Ohtani, and K. Shikano. 2006. Eigenvoice
conversion based on Gaussian mixture model. in In-
terspeech, pages 2446–2449.
T. Toda, A. Black, and K. Tokuda. 2007. Voice con-
version based on maximum likelihood estimation of
spectral parameter trajectory. IEEE Trans. Audio,
Speech, Lang. Process., 15(8):2222–2235.
H. Valbret, E. Moulines, and J. P. Tubach. 1992. Voice
transformation using PSOLA technique. Speech
Communication, vol. 11, no. 2-3, pp. 175-187.
C. Veaux and X. Robet. 2011. Intonation conversion
from neutral to expressive speech. in Interspeech,
pages 2765–2768.
C. Veaux, J. Yamagishi, and S. King. 2012. Us-
ing HMM-based speech synthesis to reconstruct the
voice of individuals with degenerative speech disor-
ders. in Interspeech, pages 1–4.
J. Yamagishi, Christophe Veaux, Simon King, and
Steve Renals. 2013. Speech synthesis technolo-
gies for individuals with vocal disabilities: Voice
banking and reconstruction. Acoustical Science and
Technology, Vol. 33 (2012) No. 1, pages 1–5.
</reference>
<page confidence="0.999609">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.900771">
<title confidence="0.999737">Individuality-preserving Voice Conversion for Articulation Using Dictionary Selective Non-negative Matrix Factorization</title>
<author confidence="0.993122">Ryo Aihara</author>
<author confidence="0.993122">Tetsuya Takiguchi</author>
<author confidence="0.993122">Yasuo</author>
<affiliation confidence="0.984242">Graduate School of System Informatics, Kobe</affiliation>
<address confidence="0.97985">1-1, Rokkodai, Nada, Kobe, 6578501,</address>
<email confidence="0.974419">ariki@kobe-u.ac.jp</email>
<abstract confidence="0.998527419354839">We present in this paper a voice conversion (VC) method for a person with an articulation disorder resulting from athetoid cerebral palsy. The movements of such speakers are limited by their athetoid symptoms, and their consonants are often unstable or unclear, which makes it difficult for them to communicate. In this paper, exemplar-based spectral conversion using Non-negative Matrix Factorization (NMF) is applied to a voice with an articulation disorder. In order to preserve the speaker’s individuality, we use a combined dictionary that was constructed from the source speaker’s vowels and target speaker’s consonants. However, this exemplar-based approach needs to hold all the training exemplars (frames), and it may cause mismatching of phonemes between input signals and selected exemplars. In this paper, in order to reduce the mismatching of phoneme alignment, we propose a phoneme-categorized sub-dictionary and a dictionary selection method using NMF. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based and conventional NMFbased method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Aihara</author>
<author>R Takashima</author>
<author>T Takiguchi</author>
<author>Y Ariki</author>
</authors>
<title>A preliminary demonstration of exemplarbased voice conversion for articulation disorders using an individuality-preserving dictionary.</title>
<date>2014</date>
<journal>EURASIP Journal on Audio, Speech, and Music Processing,</journal>
<volume>2014</volume>
<pages>10--1186</pages>
<contexts>
<context position="3589" citStr="Aihara et al., 2014" startWordPosition="540" endWordPosition="543">em before. In recent years, people with an articulation disorder may use slideshows and a previously synthesized voice when they give a lecture. However, because their movement is restricted by their athetoid symptoms, to make slides or synthesize their voice in advance is hard for them. People with articulation disorders desire a VC system that converts their voice into a clear voice that preserves their voice’s individuality. Rudzicz et al. (Rudzicz, 2011; Rudzicz, 2014) proposed speech adjustment method for people with articulation disorders based on the observations from the database. In (Aihara et al., 2014), we proposed individuality-preserving VC for articulation disorders. In our VC, source exemplars and target exemplars are extracted from the parallel Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 29–37, Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics training data, having the same texts uttered by the source and target speakers. The input source signal is expressed with a sparse representation of the source exemplars using Non-negative Matrix Factorization (NMF). By replacing a source speak</context>
<context position="21722" citStr="Aihara et al., 2014" startWordPosition="3537" endWordPosition="3540">elected as follows: kˆ = arg max k hs (13) θal xl = Φsˆkhˆk,l (14) The activity hl,ˆk in Eq. (14) is estimated from the selected source speaker sub-dictionary. If the selected sub-dictionary Φskˆ is related to consonants, the l-th frame of the converted spectral feature ˆyl is constructed by using the activity and the sub-dictionary of the target speaker Φtˆk. ˆyl =Φt 34 ˆkhˆk,l (15) Figure 5: NMF-based voice conversion using categorized dictionary 5 Experimental Results 5.1 Experimental Conditions The proposed VC technique was evaluated by comparing it with the conventional NMF-based method (Aihara et al., 2014) (referred to as the “sample-based method” in this paper) and the conventional GMM-based method (Stylianou et al., 1998) using clean speech data. We recorded 432 utterances (216 words, each repeated two times) included in the ATR Japanese speech l-th frame of input spectral features Categorizing Dictionary Activityh h ... s ...ekl l-th frame of converted spectral features y l xl ˆ Select the sub-dictionary Φ1 s Φ1 s phoneme-categorized sub-dictionaries k = arg max1ixMk hek k ... ... Φ k +1 t Φ k+1 s Activity h ˆ k, l l ... ... Copy h Φs K Φt K s eK l xl O h s l s e1 l 11xMkhs θkl Mk = arg max </context>
</contexts>
<marker>Aihara, Takashima, Takiguchi, Ariki, 2014</marker>
<rawString>R. Aihara, R. Takashima, T. Takiguchi, and Y. Ariki. 2014. A preliminary demonstration of exemplarbased voice conversion for articulation disorders using an individuality-preserving dictionary. EURASIP Journal on Audio, Speech, and Music Processing, 2014:5, doi:10.1186/1687-4722-2014-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S T Canale</author>
<author>W C Campbell</author>
</authors>
<title>Campbell’s operative orthopaedics.</title>
<date>2002</date>
<tech>Technical report,</tech>
<institution>MosbyYear Book.</institution>
<contexts>
<context position="1986" citStr="Canale and Campbell, 2002" startWordPosition="285" endWordPosition="288">nfirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based and conventional NMFbased method. 1 Introduction In this study, we focused on a person with an articulation disorder resulting from the athetoid type of cerebral palsy. About two babies in 1,000 are born with cerebral palsy (Hollegaard et al., 2013). Cerebral palsy results from damage to the central nervous system, and the damage causes movement disorders. Cerebral palsy is classified29 into the following types: 1)spastic, 2)athetoid, 3)ataxic, 4)atonic, 5)rigid, and a mixture of these types (Canale and Campbell, 2002). Athetoid symptoms develop in about 10-15% of cerebral palsy sufferers (Hollegaard et al., 2013). In the case of a person with this type of articulation disorder, his/her movements are sometimes more unstable than usual. That means their utterances (especially their consonants) are often unstable or unclear due to the athetoid symptoms. Athetoid symptoms also restrict the movement of their arms and legs. Most people suffering from athetoid cerebral palsy cannot communicate by sign language or writing, so there is great need for voice systems for them. In this paper, we propose a voice convers</context>
</contexts>
<marker>Canale, Campbell, 2002</marker>
<rawString>S. T. Canale and W. C. Campbell. 2002. Campbell’s operative orthopaedics. Technical report, MosbyYear Book.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F Gemmeke</author>
<author>T Viratnen</author>
<author>A Hurmalainen</author>
</authors>
<title>Exemplar-based sparse representations for noise robust automatic speech recognition.</title>
<date>2011</date>
<journal>IEEE Trans. Audio, Speech and Language Processing,</journal>
<volume>19</volume>
<pages>2067--2080</pages>
<contexts>
<context position="10166" citStr="Gemmeke et al., 2011" startWordPosition="1566" endWordPosition="1570">ich is different from the conventional statistical method. In recent years, approaches based on sparse representations have gained interest in a broad range of signal processing. In approaches based on sparse representations, the observed signal is represented by a linear combination of a small number of bases. In some approaches for source separation, the atoms are grouped for each source, and the mixed signals are expressed with a sparse representation of these atoms. By using only the weights of the atoms related to the target signal, the target signal can be reconstructed. Gemmeke et al. (Gemmeke et al., 2011) also propose an exemplar-based method for noise-robust speech recognition. In that method, the observed speech is decomposed into the speech atoms, noise atoms, and their weights. Then the weights of the speech atoms are used as phonetic scores (instead of the likelihoods of hidden Markov models) for speech recognition. In (Takashima et al., 2012), we proposed noiserobust VC using NMF. The noise exemplars, which are extracted from the before- and afterutterance sections in an observed signal, are used as the noise-dictionary, and the VC process is combined with an NMF-based noise-reduction me</context>
</contexts>
<marker>Gemmeke, Viratnen, Hurmalainen, 2011</marker>
<rawString>J. F. Gemmeke, T. Viratnen, and A. Hurmalainen. 2011. Exemplar-based sparse representations for noise robust automatic speech recognition. IEEE Trans. Audio, Speech and Language Processing, vol. 19, no. 7, pages 2067–2080.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Helander</author>
<author>T Virtanen</author>
<author>J Nurminen</author>
<author>M Gabbouj</author>
</authors>
<title>Voice conversion using partial least squares regression.</title>
<date>2010</date>
<journal>IEEE Trans. Audio, Speech, Lang. Process.,</journal>
<volume>18</volume>
<pages>912--921</pages>
<contexts>
<context position="7672" citStr="Helander et al., 2010" startWordPosition="1169" endWordPosition="1172">have been studied (Valbret et al., 1992). Among these approaches, the Gaussian mixture model (GMM)- based mapping approach (Stylianou et al., 1998) is widely used. In this approach, the conversion function is interpreted as the expectation value of the target spectral envelope. The conversion parameters are evaluated using Minimum MeanSquare Error (MMSE) on a parallel training set. A number of improvements in this approach have been proposed. Toda et al. (Toda et al., 2007) introduced dynamic features and the global variance (GV) of the converted spectra over a time sequence. Helander et al. (Helander et al., 2010) proposed transforms based on partial least squares (PLS) in order to prevent the over-fitting problem associated with standard multivariate regression. There have also been approaches that do not require parallel data that make use of GMM adaptation techniques (Lee and Wu, 2006) or eigen-voice GMM (EV-GMM) (Toda et al., 2006). In the field of assistive technology, Nakamura et al. (Nakamura et al., 2012; Nakamura et al., 2006) proposed GMM-based VC systems that reconstruct a speaker’s individuality in electrolaryngeal speech and speech recorded by NAM microphones. These systems are effective f</context>
</contexts>
<marker>Helander, Virtanen, Nurminen, Gabbouj, 2010</marker>
<rawString>E. Helander, T. Virtanen, J. Nurminen, and M. Gabbouj. 2010. Voice conversion using partial least squares regression. IEEE Trans. Audio, Speech, Lang. Process., vol. 18, Issue:5, pages 912–921.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mads Vilhelm Hollegaard</author>
<author>Kristin Skogstrand</author>
<author>Poul Thorsen</author>
<author>Bent Norgaard-Pedersen</author>
<author>David Michael Hougaard</author>
<author>Jakob Grove</author>
</authors>
<title>Joint analysis of SNPs and proteins identifies regulatory IL18 gene variations decreasing the chance of spastic cerebral palsy.</title>
<date>2013</date>
<journal>Human Mutation,</journal>
<volume>34</volume>
<pages>143--148</pages>
<contexts>
<context position="1711" citStr="Hollegaard et al., 2013" startWordPosition="246" endWordPosition="249">ismatching of phonemes between input signals and selected exemplars. In this paper, in order to reduce the mismatching of phoneme alignment, we propose a phoneme-categorized sub-dictionary and a dictionary selection method using NMF. The effectiveness of this method was confirmed by comparing its effectiveness with that of a conventional Gaussian Mixture Model (GMM)-based and conventional NMFbased method. 1 Introduction In this study, we focused on a person with an articulation disorder resulting from the athetoid type of cerebral palsy. About two babies in 1,000 are born with cerebral palsy (Hollegaard et al., 2013). Cerebral palsy results from damage to the central nervous system, and the damage causes movement disorders. Cerebral palsy is classified29 into the following types: 1)spastic, 2)athetoid, 3)ataxic, 4)atonic, 5)rigid, and a mixture of these types (Canale and Campbell, 2002). Athetoid symptoms develop in about 10-15% of cerebral palsy sufferers (Hollegaard et al., 2013). In the case of a person with this type of articulation disorder, his/her movements are sometimes more unstable than usual. That means their utterances (especially their consonants) are often unstable or unclear due to the athe</context>
</contexts>
<marker>Hollegaard, Skogstrand, Thorsen, Norgaard-Pedersen, Hougaard, Grove, 2013</marker>
<rawString>Mads Vilhelm Hollegaard, Kristin Skogstrand, Poul Thorsen, Bent Norgaard-Pedersen, David Michael Hougaard, and Jakob Grove. 2013. Joint analysis of SNPs and proteins identifies regulatory IL18 gene variations decreasing the chance of spastic cerebral palsy. Human Mutation, Vol. 34, pages 143–148.</rawString>
</citation>
<citation valid="false">
<volume>0</volume>
<marker></marker>
<rawString>0 0 0 0 0 0 0 0 0 0 .1 2 3 4 .5 6 7 .8 .9 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>INTERNATIONAL TELECOMMUNICATION UNION</author>
</authors>
<title>Methods for objective and subjective assessment of quality. ITU-T Recommendation P.800.</title>
<date>2003</date>
<marker>UNION, 2003</marker>
<rawString>INTERNATIONAL TELECOMMUNICATION UNION. 2003. Methods for objective and subjective assessment of quality. ITU-T Recommendation P.800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kain</author>
<author>M W Macon</author>
</authors>
<title>Spectral voice conversion for text-to-speech synthesis.</title>
<date>1998</date>
<journal>in ICASSP,</journal>
<volume>1</volume>
<pages>285--288</pages>
<contexts>
<context position="6972" citStr="Kain and Macon, 1998" startWordPosition="1056" endWordPosition="1059">onversion (Stylianou et al., 1998). In speaker conversion, a source speaker’s voice individuality is changed to a specified target speaker’s so that the input utterance sounds as though a specified target speaker had spoken it. There have also been studies on several tasks that make use of VC. Emotion conversion is a technique for changing emotional information in input speech while maintaining linguistic information and speaker individuality (Veaux and Robet, 2011). In recent years, VC has been used for automatic speech recognition (ASR) or speaker adaptation in text-to-speech (TTS) systems (Kain and Macon, 1998). These studies show the varied uses of VC. Many statistical approaches to VC have been studied (Valbret et al., 1992). Among these approaches, the Gaussian mixture model (GMM)- based mapping approach (Stylianou et al., 1998) is widely used. In this approach, the conversion function is interpreted as the expectation value of the target spectral envelope. The conversion parameters are evaluated using Minimum MeanSquare Error (MMSE) on a parallel training set. A number of improvements in this approach have been proposed. Toda et al. (Toda et al., 2007) introduced dynamic features and the global </context>
</contexts>
<marker>Kain, Macon, 1998</marker>
<rawString>A. Kain and M. W. Macon. 1998. Spectral voice conversion for text-to-speech synthesis. in ICASSP, vol. 1, pages 285–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kawahara</author>
<author>I Masuda-Katsuse</author>
<author>A de Cheveigne</author>
</authors>
<title>Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequencybased F0 extraction: possible role of a repetitive structure in sounds.</title>
<date>1999</date>
<journal>Speech Communication,</journal>
<pages>27--3</pages>
<marker>Kawahara, Masuda-Katsuse, de Cheveigne, 1999</marker>
<rawString>H. Kawahara, I. Masuda-Katsuse, and A.de Cheveigne. 1999. Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequencybased F0 extraction: possible role of a repetitive structure in sounds. Speech Communication, 27(3-4):187–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kurematsu</author>
<author>K Takeda</author>
<author>Y Sagisaka</author>
<author>S Katagiri</author>
<author>H Kuwabara</author>
<author>K Shikano</author>
</authors>
<title>ATR Japanese speech database as a tool of speech recognition and synthesis.</title>
<date>1990</date>
<journal>Speech Communication,</journal>
<pages>9--357</pages>
<contexts>
<context position="22361" citStr="Kurematsu et al., 1990" startWordPosition="3660" endWordPosition="3663"> the “sample-based method” in this paper) and the conventional GMM-based method (Stylianou et al., 1998) using clean speech data. We recorded 432 utterances (216 words, each repeated two times) included in the ATR Japanese speech l-th frame of input spectral features Categorizing Dictionary Activityh h ... s ...ekl l-th frame of converted spectral features y l xl ˆ Select the sub-dictionary Φ1 s Φ1 s phoneme-categorized sub-dictionaries k = arg max1ixMk hek k ... ... Φ k +1 t Φ k+1 s Activity h ˆ k, l l ... ... Copy h Φs K Φt K s eK l xl O h s l s e1 l 11xMkhs θkl Mk = arg max k m=1 database (Kurematsu et al., 1990). The speech signals were sampled at 12 kHz and windowed with a 25-msec Hamming window every 10 msec. A physically unimpaired Japanese male in the ATR Japanese speech database, was chosen as a target speaker. In the proposed and sample-based methods, the number of dimensions of the spectral feature is 2,565. It consists of a 513-dimensional STRAIGHT spectrum (Kawahara et al., 1999) and its consecutive frames (the 2 frames coming before and the 2 frames coming after). The Gaussian mixture, which is used to construct a categorizing-dictionary, is 1/500 of the number of bases of each sub-dictiona</context>
</contexts>
<marker>Kurematsu, Takeda, Sagisaka, Katagiri, Kuwabara, Shikano, 1990</marker>
<rawString>A. Kurematsu, K. Takeda, Y. Sagisaka, S. Katagiri, H. Kuwabara, and K. Shikano. 1990. ATR Japanese speech database as a tool of speech recognition and synthesis. Speech Communication, 9:357–363.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C H Lee</author>
<author>C H Wu</author>
</authors>
<title>Map-based adaptation for speech conversion using adaptation data selection and non-parallel training. in Interspeech,</title>
<date>2006</date>
<pages>2254--2257</pages>
<contexts>
<context position="7952" citStr="Lee and Wu, 2006" startWordPosition="1213" endWordPosition="1216">nversion parameters are evaluated using Minimum MeanSquare Error (MMSE) on a parallel training set. A number of improvements in this approach have been proposed. Toda et al. (Toda et al., 2007) introduced dynamic features and the global variance (GV) of the converted spectra over a time sequence. Helander et al. (Helander et al., 2010) proposed transforms based on partial least squares (PLS) in order to prevent the over-fitting problem associated with standard multivariate regression. There have also been approaches that do not require parallel data that make use of GMM adaptation techniques (Lee and Wu, 2006) or eigen-voice GMM (EV-GMM) (Toda et al., 2006). In the field of assistive technology, Nakamura et al. (Nakamura et al., 2012; Nakamura et al., 2006) proposed GMM-based VC systems that reconstruct a speaker’s individuality in electrolaryngeal speech and speech recorded by NAM microphones. These systems are effective for electrolaryngeal speech and speech recorded by NAM microphones however, because these statistical approaches are mainly proposed for speaker conversion, the target speaker’s individuality will be changed to the source speaker’s individuality. People with articulation disorders</context>
</contexts>
<marker>Lee, Wu, 2006</marker>
<rawString>C. H. Lee and C. H. Wu. 2006. Map-based adaptation for speech conversion using adaptation data selection and non-parallel training. in Interspeech, pages 2254–2257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Matsumasa</author>
<author>T Takiguchi</author>
<author>Y Ariki</author>
<author>I Li</author>
<author>T Nakabayachi</author>
</authors>
<title>Integration of metamodel and acoustic model for dysarthric speech recognition.</title>
<date>2009</date>
<journal>Journal of Multimedia,</journal>
<volume>4</volume>
<pages>254--261</pages>
<contexts>
<context position="2815" citStr="Matsumasa et al., 2009" startWordPosition="411" endWordPosition="414">ble than usual. That means their utterances (especially their consonants) are often unstable or unclear due to the athetoid symptoms. Athetoid symptoms also restrict the movement of their arms and legs. Most people suffering from athetoid cerebral palsy cannot communicate by sign language or writing, so there is great need for voice systems for them. In this paper, we propose a voice conversion (VC) method for articulation disorders. Regarding speech recognition for articulation disorders, the recognition rate using a speaker-independent model which is trained by well-ordered speech, is 3.5% (Matsumasa et al., 2009). This result implies that the utterance of a person with an articulation disorder is difficult to understand for people who have not communicated with them before. In recent years, people with an articulation disorder may use slideshows and a previously synthesized voice when they give a lecture. However, because their movement is restricted by their athetoid symptoms, to make slides or synthesize their voice in advance is hard for them. People with articulation disorders desire a VC system that converts their voice into a clear voice that preserves their voice’s individuality. Rudzicz et al.</context>
</contexts>
<marker>Matsumasa, Takiguchi, Ariki, Li, Nakabayachi, 2009</marker>
<rawString>H. Matsumasa, T. Takiguchi, Y. Ariki, I. Li, and T. Nakabayachi. 2009. Integration of metamodel and acoustic model for dysarthric speech recognition. Journal of Multimedia, Volume 4, Issue 4, pages 254–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nakamura</author>
<author>T Toda</author>
<author>H Saruwatari</author>
<author>K Shikano</author>
</authors>
<title>Speaking aid system for total laryngectomees using voice conversion of body transmitted artificial speech. in Interspeech,</title>
<date>2006</date>
<pages>148--151</pages>
<contexts>
<context position="8102" citStr="Nakamura et al., 2006" startWordPosition="1238" endWordPosition="1241">e been proposed. Toda et al. (Toda et al., 2007) introduced dynamic features and the global variance (GV) of the converted spectra over a time sequence. Helander et al. (Helander et al., 2010) proposed transforms based on partial least squares (PLS) in order to prevent the over-fitting problem associated with standard multivariate regression. There have also been approaches that do not require parallel data that make use of GMM adaptation techniques (Lee and Wu, 2006) or eigen-voice GMM (EV-GMM) (Toda et al., 2006). In the field of assistive technology, Nakamura et al. (Nakamura et al., 2012; Nakamura et al., 2006) proposed GMM-based VC systems that reconstruct a speaker’s individuality in electrolaryngeal speech and speech recorded by NAM microphones. These systems are effective for electrolaryngeal speech and speech recorded by NAM microphones however, because these statistical approaches are mainly proposed for speaker conversion, the target speaker’s individuality will be changed to the source speaker’s individuality. People with articulation disorders wish to communicate by their own voice if they can and there is a needs for individuality-preserving VC. Text-to-speech synthesis (TTS) is a famous v</context>
</contexts>
<marker>Nakamura, Toda, Saruwatari, Shikano, 2006</marker>
<rawString>K. Nakamura, T. Toda, H. Saruwatari, and K. Shikano. 2006. Speaking aid system for total laryngectomees using voice conversion of body transmitted artificial speech. in Interspeech, pages 148–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Nakamura</author>
<author>T Toda</author>
<author>H Saruwatari</author>
<author>K Shikano</author>
</authors>
<title>Speaking-aid systems using GMMbased voice conversion for electrolaryngeal speech.</title>
<date>2012</date>
<journal>Speech Communication,</journal>
<volume>54</volume>
<issue>1</issue>
<contexts>
<context position="8078" citStr="Nakamura et al., 2012" startWordPosition="1234" endWordPosition="1237">ts in this approach have been proposed. Toda et al. (Toda et al., 2007) introduced dynamic features and the global variance (GV) of the converted spectra over a time sequence. Helander et al. (Helander et al., 2010) proposed transforms based on partial least squares (PLS) in order to prevent the over-fitting problem associated with standard multivariate regression. There have also been approaches that do not require parallel data that make use of GMM adaptation techniques (Lee and Wu, 2006) or eigen-voice GMM (EV-GMM) (Toda et al., 2006). In the field of assistive technology, Nakamura et al. (Nakamura et al., 2012; Nakamura et al., 2006) proposed GMM-based VC systems that reconstruct a speaker’s individuality in electrolaryngeal speech and speech recorded by NAM microphones. These systems are effective for electrolaryngeal speech and speech recorded by NAM microphones however, because these statistical approaches are mainly proposed for speaker conversion, the target speaker’s individuality will be changed to the source speaker’s individuality. People with articulation disorders wish to communicate by their own voice if they can and there is a needs for individuality-preserving VC. Text-to-speech synth</context>
</contexts>
<marker>Nakamura, Toda, Saruwatari, Shikano, 2012</marker>
<rawString>K. Nakamura, T. Toda, H. Saruwatari, and K. Shikano. 2012. Speaking-aid systems using GMMbased voice conversion for electrolaryngeal speech. Speech Communication, 54(1):134–146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rudzicz</author>
</authors>
<title>Acoustic transformations to improve the intelligibility of dysarthric speech.</title>
<date>2011</date>
<booktitle>in proc. the Second Workshop on Speech and Language Processing for Assistive Technologies.</booktitle>
<contexts>
<context position="3430" citStr="Rudzicz, 2011" startWordPosition="518" endWordPosition="519">This result implies that the utterance of a person with an articulation disorder is difficult to understand for people who have not communicated with them before. In recent years, people with an articulation disorder may use slideshows and a previously synthesized voice when they give a lecture. However, because their movement is restricted by their athetoid symptoms, to make slides or synthesize their voice in advance is hard for them. People with articulation disorders desire a VC system that converts their voice into a clear voice that preserves their voice’s individuality. Rudzicz et al. (Rudzicz, 2011; Rudzicz, 2014) proposed speech adjustment method for people with articulation disorders based on the observations from the database. In (Aihara et al., 2014), we proposed individuality-preserving VC for articulation disorders. In our VC, source exemplars and target exemplars are extracted from the parallel Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 29–37, Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics training data, having the same texts uttered by the source and target speakers. The i</context>
</contexts>
<marker>Rudzicz, 2011</marker>
<rawString>F. Rudzicz. 2011. Acoustic transformations to improve the intelligibility of dysarthric speech. in proc. the Second Workshop on Speech and Language Processing for Assistive Technologies.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Rudzicz</author>
</authors>
<title>Adjusting dysarthric speech signals to be more intelligible.</title>
<date>2014</date>
<journal>in Computer Speech and Language,</journal>
<volume>27</volume>
<issue>6</issue>
<pages>1163--1177</pages>
<contexts>
<context position="3446" citStr="Rudzicz, 2014" startWordPosition="520" endWordPosition="521">lies that the utterance of a person with an articulation disorder is difficult to understand for people who have not communicated with them before. In recent years, people with an articulation disorder may use slideshows and a previously synthesized voice when they give a lecture. However, because their movement is restricted by their athetoid symptoms, to make slides or synthesize their voice in advance is hard for them. People with articulation disorders desire a VC system that converts their voice into a clear voice that preserves their voice’s individuality. Rudzicz et al. (Rudzicz, 2011; Rudzicz, 2014) proposed speech adjustment method for people with articulation disorders based on the observations from the database. In (Aihara et al., 2014), we proposed individuality-preserving VC for articulation disorders. In our VC, source exemplars and target exemplars are extracted from the parallel Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 29–37, Baltimore, Maryland USA, August 26 2014. c�2014 Association for Computational Linguistics training data, having the same texts uttered by the source and target speakers. The input source sign</context>
</contexts>
<marker>Rudzicz, 2014</marker>
<rawString>F. Rudzicz. 2014. Adjusting dysarthric speech signals to be more intelligible. in Computer Speech and Language, 27(6), September, pages 1163–1177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Stylianou</author>
<author>O Cappe</author>
<author>E Moilines</author>
</authors>
<title>Continuous probabilistic transform for voice conversion.</title>
<date>1998</date>
<journal>IEEE Trans. Speech and Audio Processing,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="6385" citStr="Stylianou et al., 1998" startWordPosition="963" endWordPosition="966">ased method and a conventional Gaussian Mixture Model (GMM)-based method. The rest of this paper is organized as follows: In Section 2, related works are introduced. In Section 3, the basic idea of NMF-based VC is described. In Section 4, our proposed method is described. In Section 5, the experimental data are evaluated, and the final section is devoted to our conclusions. 30 2 Related Works Voice conversion (VC) is a technique for converting specific information in speech while maintaining the other information in the utterance. One of the most popular VC applications is speaker conversion (Stylianou et al., 1998). In speaker conversion, a source speaker’s voice individuality is changed to a specified target speaker’s so that the input utterance sounds as though a specified target speaker had spoken it. There have also been studies on several tasks that make use of VC. Emotion conversion is a technique for changing emotional information in input speech while maintaining linguistic information and speaker individuality (Veaux and Robet, 2011). In recent years, VC has been used for automatic speech recognition (ASR) or speaker adaptation in text-to-speech (TTS) systems (Kain and Macon, 1998). These studi</context>
<context position="21842" citStr="Stylianou et al., 1998" startWordPosition="3555" endWordPosition="3558">he selected source speaker sub-dictionary. If the selected sub-dictionary Φskˆ is related to consonants, the l-th frame of the converted spectral feature ˆyl is constructed by using the activity and the sub-dictionary of the target speaker Φtˆk. ˆyl =Φt 34 ˆkhˆk,l (15) Figure 5: NMF-based voice conversion using categorized dictionary 5 Experimental Results 5.1 Experimental Conditions The proposed VC technique was evaluated by comparing it with the conventional NMF-based method (Aihara et al., 2014) (referred to as the “sample-based method” in this paper) and the conventional GMM-based method (Stylianou et al., 1998) using clean speech data. We recorded 432 utterances (216 words, each repeated two times) included in the ATR Japanese speech l-th frame of input spectral features Categorizing Dictionary Activityh h ... s ...ekl l-th frame of converted spectral features y l xl ˆ Select the sub-dictionary Φ1 s Φ1 s phoneme-categorized sub-dictionaries k = arg max1ixMk hek k ... ... Φ k +1 t Φ k+1 s Activity h ˆ k, l l ... ... Copy h Φs K Φt K s eK l xl O h s l s e1 l 11xMkhs θkl Mk = arg max k m=1 database (Kurematsu et al., 1990). The speech signals were sampled at 12 kHz and windowed with a 25-msec Hamming w</context>
</contexts>
<marker>Stylianou, Cappe, Moilines, 1998</marker>
<rawString>Y. Stylianou, O. Cappe, and E. Moilines. 1998. Continuous probabilistic transform for voice conversion. IEEE Trans. Speech and Audio Processing, 6(2):131–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Takashima</author>
<author>T Takiguchi</author>
<author>Y Ariki</author>
</authors>
<title>Exemplar-based voice conversion in noisy environment. in SLT,</title>
<date>2012</date>
<pages>313--317</pages>
<contexts>
<context position="9485" citStr="Takashima et al., 2012" startWordPosition="1455" endWordPosition="1458">erative speech disorders resulting from Amyotrophic Lateral Sclerosis (ALS). Yamagishi et al. (Yamagishi et al., 2013) proposed a project named “Voice Banking and Reconstruction”. In that project, various types of voices are collected and they proposed TTS for ALS using that database. The difference between TTS and VC is that TTS needs text input to synthesize speech, whereas VC does not need text input. In the case of people with articulation disorders resulting from athetoid cerebral palsy, it is difficult for them to input text because of their athetoid symptoms. Our proposed NMF-based VC (Takashima et al., 2012) is an exemplar-based method using sparse representation, which is different from the conventional statistical method. In recent years, approaches based on sparse representations have gained interest in a broad range of signal processing. In approaches based on sparse representations, the observed signal is represented by a linear combination of a small number of bases. In some approaches for source separation, the atoms are grouped for each source, and the mixed signals are expressed with a sparse representation of these atoms. By using only the weights of the atoms related to the target sign</context>
</contexts>
<marker>Takashima, Takiguchi, Ariki, 2012</marker>
<rawString>R. Takashima, T. Takiguchi, and Y. Ariki. 2012. Exemplar-based voice conversion in noisy environment. in SLT, pages 313–317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Toda</author>
<author>Y Ohtani</author>
<author>K Shikano</author>
</authors>
<title>Eigenvoice conversion based on Gaussian mixture model. in Interspeech,</title>
<date>2006</date>
<pages>2446--2449</pages>
<contexts>
<context position="8000" citStr="Toda et al., 2006" startWordPosition="1221" endWordPosition="1224"> MeanSquare Error (MMSE) on a parallel training set. A number of improvements in this approach have been proposed. Toda et al. (Toda et al., 2007) introduced dynamic features and the global variance (GV) of the converted spectra over a time sequence. Helander et al. (Helander et al., 2010) proposed transforms based on partial least squares (PLS) in order to prevent the over-fitting problem associated with standard multivariate regression. There have also been approaches that do not require parallel data that make use of GMM adaptation techniques (Lee and Wu, 2006) or eigen-voice GMM (EV-GMM) (Toda et al., 2006). In the field of assistive technology, Nakamura et al. (Nakamura et al., 2012; Nakamura et al., 2006) proposed GMM-based VC systems that reconstruct a speaker’s individuality in electrolaryngeal speech and speech recorded by NAM microphones. These systems are effective for electrolaryngeal speech and speech recorded by NAM microphones however, because these statistical approaches are mainly proposed for speaker conversion, the target speaker’s individuality will be changed to the source speaker’s individuality. People with articulation disorders wish to communicate by their own voice if they </context>
</contexts>
<marker>Toda, Ohtani, Shikano, 2006</marker>
<rawString>T. Toda, Y. Ohtani, and K. Shikano. 2006. Eigenvoice conversion based on Gaussian mixture model. in Interspeech, pages 2446–2449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Toda</author>
<author>A Black</author>
<author>K Tokuda</author>
</authors>
<title>Voice conversion based on maximum likelihood estimation of spectral parameter trajectory.</title>
<date>2007</date>
<journal>IEEE Trans. Audio, Speech, Lang. Process.,</journal>
<volume>15</volume>
<issue>8</issue>
<contexts>
<context position="7528" citStr="Toda et al., 2007" startWordPosition="1145" endWordPosition="1148">ptation in text-to-speech (TTS) systems (Kain and Macon, 1998). These studies show the varied uses of VC. Many statistical approaches to VC have been studied (Valbret et al., 1992). Among these approaches, the Gaussian mixture model (GMM)- based mapping approach (Stylianou et al., 1998) is widely used. In this approach, the conversion function is interpreted as the expectation value of the target spectral envelope. The conversion parameters are evaluated using Minimum MeanSquare Error (MMSE) on a parallel training set. A number of improvements in this approach have been proposed. Toda et al. (Toda et al., 2007) introduced dynamic features and the global variance (GV) of the converted spectra over a time sequence. Helander et al. (Helander et al., 2010) proposed transforms based on partial least squares (PLS) in order to prevent the over-fitting problem associated with standard multivariate regression. There have also been approaches that do not require parallel data that make use of GMM adaptation techniques (Lee and Wu, 2006) or eigen-voice GMM (EV-GMM) (Toda et al., 2006). In the field of assistive technology, Nakamura et al. (Nakamura et al., 2012; Nakamura et al., 2006) proposed GMM-based VC sys</context>
<context position="23409" citStr="Toda et al., 2007" startWordPosition="3834" endWordPosition="3837">ng before and the 2 frames coming after). The Gaussian mixture, which is used to construct a categorizing-dictionary, is 1/500 of the number of bases of each sub-dictionary. The number of iterations for estimating the activity in the proposed and sample-based methods was 300. In the conventional GMM-based method, MFCC+∆MFCC+∆∆MFCC is used as a spectral feature. Its number of dimensions is 74. The number of Gaussian mixtures is set to 64, which is experimentally selected. In this paper, F0 information is converted using a conventional linear regression based on the mean and standard deviation (Toda et al., 2007). The other information such as aperiodic components, is synthesized without any conversion. We conducted a subjective evaluation of 3 topics. A total of 10 Japanese speakers took part in the test using headphones. For the “listening intelligibility” evaluation, we performed a MOS (Mean Opinion Score) test (”INTERNATIONAL TELECOMMUNICATION UNION”, 2003). The opinion score was set to a 5-point scale (5: excellent, 4: good, 3: fair, 2: poor, 1: bad). Twenty-two words that are difficult for a person with an articulation disorder to utter were evaluated. The subjects were asked about the listening</context>
</contexts>
<marker>Toda, Black, Tokuda, 2007</marker>
<rawString>T. Toda, A. Black, and K. Tokuda. 2007. Voice conversion based on maximum likelihood estimation of spectral parameter trajectory. IEEE Trans. Audio, Speech, Lang. Process., 15(8):2222–2235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Valbret</author>
<author>E Moulines</author>
<author>J P Tubach</author>
</authors>
<title>Voice transformation using PSOLA technique.</title>
<date>1992</date>
<journal>Speech Communication,</journal>
<volume>11</volume>
<pages>2--3</pages>
<contexts>
<context position="7090" citStr="Valbret et al., 1992" startWordPosition="1076" endWordPosition="1079">ified target speaker’s so that the input utterance sounds as though a specified target speaker had spoken it. There have also been studies on several tasks that make use of VC. Emotion conversion is a technique for changing emotional information in input speech while maintaining linguistic information and speaker individuality (Veaux and Robet, 2011). In recent years, VC has been used for automatic speech recognition (ASR) or speaker adaptation in text-to-speech (TTS) systems (Kain and Macon, 1998). These studies show the varied uses of VC. Many statistical approaches to VC have been studied (Valbret et al., 1992). Among these approaches, the Gaussian mixture model (GMM)- based mapping approach (Stylianou et al., 1998) is widely used. In this approach, the conversion function is interpreted as the expectation value of the target spectral envelope. The conversion parameters are evaluated using Minimum MeanSquare Error (MMSE) on a parallel training set. A number of improvements in this approach have been proposed. Toda et al. (Toda et al., 2007) introduced dynamic features and the global variance (GV) of the converted spectra over a time sequence. Helander et al. (Helander et al., 2010) proposed transfor</context>
</contexts>
<marker>Valbret, Moulines, Tubach, 1992</marker>
<rawString>H. Valbret, E. Moulines, and J. P. Tubach. 1992. Voice transformation using PSOLA technique. Speech Communication, vol. 11, no. 2-3, pp. 175-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Veaux</author>
<author>X Robet</author>
</authors>
<title>Intonation conversion from neutral to expressive speech. in Interspeech,</title>
<date>2011</date>
<pages>2765--2768</pages>
<contexts>
<context position="6821" citStr="Veaux and Robet, 2011" startWordPosition="1031" endWordPosition="1034">converting specific information in speech while maintaining the other information in the utterance. One of the most popular VC applications is speaker conversion (Stylianou et al., 1998). In speaker conversion, a source speaker’s voice individuality is changed to a specified target speaker’s so that the input utterance sounds as though a specified target speaker had spoken it. There have also been studies on several tasks that make use of VC. Emotion conversion is a technique for changing emotional information in input speech while maintaining linguistic information and speaker individuality (Veaux and Robet, 2011). In recent years, VC has been used for automatic speech recognition (ASR) or speaker adaptation in text-to-speech (TTS) systems (Kain and Macon, 1998). These studies show the varied uses of VC. Many statistical approaches to VC have been studied (Valbret et al., 1992). Among these approaches, the Gaussian mixture model (GMM)- based mapping approach (Stylianou et al., 1998) is widely used. In this approach, the conversion function is interpreted as the expectation value of the target spectral envelope. The conversion parameters are evaluated using Minimum MeanSquare Error (MMSE) on a parallel </context>
</contexts>
<marker>Veaux, Robet, 2011</marker>
<rawString>C. Veaux and X. Robet. 2011. Intonation conversion from neutral to expressive speech. in Interspeech, pages 2765–2768.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Veaux</author>
<author>J Yamagishi</author>
<author>S King</author>
</authors>
<title>Using HMM-based speech synthesis to reconstruct the voice of individuals with degenerative speech disorders. in Interspeech,</title>
<date>2012</date>
<pages>1--4</pages>
<contexts>
<context position="8779" citStr="Veaux et al., 2012" startWordPosition="1342" endWordPosition="1345">s individuality in electrolaryngeal speech and speech recorded by NAM microphones. These systems are effective for electrolaryngeal speech and speech recorded by NAM microphones however, because these statistical approaches are mainly proposed for speaker conversion, the target speaker’s individuality will be changed to the source speaker’s individuality. People with articulation disorders wish to communicate by their own voice if they can and there is a needs for individuality-preserving VC. Text-to-speech synthesis (TTS) is a famous voice application that is widely researched. Veaux et al. (Veaux et al., 2012) used HMM-based speech synthesis to reconstruct the voice of individuals with degenerative speech disorders resulting from Amyotrophic Lateral Sclerosis (ALS). Yamagishi et al. (Yamagishi et al., 2013) proposed a project named “Voice Banking and Reconstruction”. In that project, various types of voices are collected and they proposed TTS for ALS using that database. The difference between TTS and VC is that TTS needs text input to synthesize speech, whereas VC does not need text input. In the case of people with articulation disorders resulting from athetoid cerebral palsy, it is difficult for</context>
</contexts>
<marker>Veaux, Yamagishi, King, 2012</marker>
<rawString>C. Veaux, J. Yamagishi, and S. King. 2012. Using HMM-based speech synthesis to reconstruct the voice of individuals with degenerative speech disorders. in Interspeech, pages 1–4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yamagishi</author>
<author>Christophe Veaux</author>
<author>Simon King</author>
<author>Steve Renals</author>
</authors>
<title>Speech synthesis technologies for individuals with vocal disabilities: Voice banking and reconstruction.</title>
<date>2013</date>
<journal>Acoustical Science and Technology,</journal>
<volume>33</volume>
<pages>1--5</pages>
<contexts>
<context position="8980" citStr="Yamagishi et al., 2013" startWordPosition="1371" endWordPosition="1374">ese statistical approaches are mainly proposed for speaker conversion, the target speaker’s individuality will be changed to the source speaker’s individuality. People with articulation disorders wish to communicate by their own voice if they can and there is a needs for individuality-preserving VC. Text-to-speech synthesis (TTS) is a famous voice application that is widely researched. Veaux et al. (Veaux et al., 2012) used HMM-based speech synthesis to reconstruct the voice of individuals with degenerative speech disorders resulting from Amyotrophic Lateral Sclerosis (ALS). Yamagishi et al. (Yamagishi et al., 2013) proposed a project named “Voice Banking and Reconstruction”. In that project, various types of voices are collected and they proposed TTS for ALS using that database. The difference between TTS and VC is that TTS needs text input to synthesize speech, whereas VC does not need text input. In the case of people with articulation disorders resulting from athetoid cerebral palsy, it is difficult for them to input text because of their athetoid symptoms. Our proposed NMF-based VC (Takashima et al., 2012) is an exemplar-based method using sparse representation, which is different from the conventio</context>
</contexts>
<marker>Yamagishi, Veaux, King, Renals, 2013</marker>
<rawString>J. Yamagishi, Christophe Veaux, Simon King, and Steve Renals. 2013. Speech synthesis technologies for individuals with vocal disabilities: Voice banking and reconstruction. Acoustical Science and Technology, Vol. 33 (2012) No. 1, pages 1–5.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
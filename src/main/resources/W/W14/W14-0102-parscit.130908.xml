<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.915682">
WN-Toolkit:
Automatic generation of WordNets following the expand model
</title>
<author confidence="0.545559">
Antoni Oliver
</author>
<affiliation confidence="0.333398">
Universitat Oberta de Catalunya
</affiliation>
<address confidence="0.471008">
Barcelona - Catalonia - Spain
</address>
<email confidence="0.991572">
aoliverg@uoc.edu
</email>
<sectionHeader confidence="0.993714" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999889">
This paper presents a set of methodolo-
gies and algorithms to create WordNets
following the expand model. We explore
dictionary and BabelNet based strategies,
as well as methodologies based on the
use of parallel corpora. Evaluation results
for six languages are presented: Catalan,
Spanish, French, German, Italian and Por-
tuguese. Along with the methodologies
and evaluation we present an implemen-
tation of all the algorithms grouped in a
set of programs or toolkit. These programs
have been successfully used in the Know2
Project for the creation of Catalan and
Spanish WordNet 3.0. The toolkit is pub-
lished under the GNU-GPL license and
can be freely downloaded from http:
//lpg.uoc.edu/wn-toolkit.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816555555555">
WordNet (Fellbaum, 1998) is a lexical database
that has become a standard resource in Natural
Language Processing research and applications.
The English WordNet (PWN - Princeton Word-
Net) is being updated regularly, so that its num-
ber of synsets increases with every new version.
The current version of PWN is 3.1, but in our ex-
periments we are using the 3.0 version because is
the latest one available for download at the time of
performing the experiments.
WordNet versions in other languages are also
available. On the Global WordNet Association1
website, a comprehensive list of WordNets avail-
able for different languages can be found. The
Open Multilingual WordNet project (Bond and
Kyonghee, 2012) provides free access to Word-
Nets in several languages in a common format.
We have used the WordNets from this project for
</bodyText>
<footnote confidence="0.959666">
1www.globalwordnet.org
</footnote>
<bodyText confidence="0.863675888888889">
Catalan (Gonzalez-Agirre et al., 2012) , Spanish
(Gonzalez-Agirre et al., 2012) , French (WOLF)
(Sagot and Fiˇser, 2008) , Italian (Multiwordnet)
(Pianta et al., 2002) and Portuguese (OpenWN-
PT) (de Paiva and Rademaker, 2012) . For Ger-
man we have used the GermaNet 7.0 (Hamp and
Feldweg, 1997), freely available for research. In
Table 1, the sizes of all these WordNets are pre-
sented along with the size of the PWN.
</bodyText>
<table confidence="0.935515125">
Synsets Words
English 118.695 206.979
Catalan 45.826 46.531
Spanish 38.512 36.681
French 59.091 55.373
Italian 34.728 40.343
Portuguese 41.810 52.220
German 74.612 99.529
</table>
<tableCaption confidence="0.999242">
Table 1: Size of the WordNets
</tableCaption>
<sectionHeader confidence="0.893721" genericHeader="introduction">
2 The expand model
</sectionHeader>
<bodyText confidence="0.999503666666667">
According to (Vossen, 1998), we can distinguish
two general methodologies for WordNet construc-
tion: (i) the merge model, where a new ontology is
constructed for the target language; and (ii) the ex-
pand model, where variants associated with PWN
synsets are translated using different strategies.
</bodyText>
<subsectionHeader confidence="0.909674">
2.1 Dictionary-based strategies
</subsectionHeader>
<bodyText confidence="0.983132846153846">
The most commonly used strategy within the ex-
pand model is the use of bilingual dictionaries.
The main difficulty faced is polysemy. If all the
variants were monosemic, i.e., if they were as-
signed to a single synset, the problem would be
simple, as we would only need to find one or more
translations for the English variant. In Table 2 we
can see the degree of polysemy in PWN 3.0. As
we can see, 82.32% of the variants of the PWN
are monosemic, as they are assigned to a single
synset.
It is also worth observing the percentage of
monosemic variants that are written with the first
</bodyText>
<note confidence="0.594459333333333">
N. synsets variants %
1 123.228 82.32
2 15.577 10.41
3 5.027 3.36
4 2.199 1.47
5+ 3.659 2.44
</note>
<tableCaption confidence="0.986418">
Table 2: Degree of polysemy in PWN 3.0
</tableCaption>
<bodyText confidence="0.987904666666667">
letter in upper case (probably corresponding to
proper names) and in lower case. In Table 3, we
can see the figures.
</bodyText>
<figure confidence="0.331836">
variants %
upper case 84.714 68.75
lower case 38.514 31.25
</figure>
<tableCaption confidence="0.70845">
Table 3: Number of monosemic variants with the
first letter in uppercase or lowercase
</tableCaption>
<bodyText confidence="0.9997105">
These figures show us that a large percentage of
a target WordNet can be implemented using this
strategy. We must bear in mind, however, that us-
ing this methodology, we would probably not be
able to obtain the most frequent variants, as com-
mon words are usually polysemic.
The Spanish WordNet (Atserias et al., 1997) in
the EuroWordNet project and the Catalan Word-
Net (Benitez et al., 1998) were constructed using
dictionaries.
With the dictionary-based strategy we will only
be able to get target language variants for synsets
having monosemic English variants, i.e. English
words assigned to a single synset.
</bodyText>
<subsectionHeader confidence="0.997356">
2.2 Babelnet
</subsectionHeader>
<bodyText confidence="0.999961702702703">
BabelNet (Navigli and Ponzetto, 2010) is a se-
mantic network and ontology created by linking
Wikipedia entries to WordNet synsets. These rela-
tions are multilingual through the interlingual rela-
tions in Wikipedia. For languages lacking the cor-
responding Wikipedia entry a statistical machine
translation system is used to translate a set of En-
glish sentences containing the synset in the Sem-
cor corpus and in sentences from Wikipedia con-
taining a link to the English Wikipedia version.
After that, the most frequent translation is detected
and included as a variant for the synset in the given
language.
Similarly to WordNet, BabelNet groups words
in different languages into sets of synonyms,
called Babel synsets. Babelnet also provides def-
initions or glosses collected from WordNet and
Wikipedia. For cases where the sense is also avail-
able in WordNet, the WordNet synset is also pro-
vided. We can use Babelnet directly for the cre-
ation of WordNets for the languages included in
Babelnet (English, Catalan, Spanish, Italian, Ger-
man and French). For other languages, we can also
exploit Babelnet through the Wikipedia’s interlin-
gual index.
Recently Babelnet 2.0 was released. This ver-
sion includes 50 languages and uses informa-
tion from the following sources: (i) Princeton
WordNet, (ii) Open Multilingual WordNet, (iii)
Wikipedia and (iv) OmegaWiki. a large collabo-
rative multilingual dictionary.
Prelimiary results using this new version of Ba-
belnet will be also shown in section 3.3.4.
With the Babelnet-based strategy we can get the
target language variants for synsyets having both
monosemic and polisemic English variants, that is,
English words assigned to one or more synsets.
</bodyText>
<subsectionHeader confidence="0.99815">
2.3 Parallel corpus based strategies
</subsectionHeader>
<bodyText confidence="0.999974888888889">
In some previous works we presented a method-
ology for the construction of WordNets based on
the use of parallel bilingual corpora. These cor-
pora need to be semantically tagged, the tags be-
ing PWN synsets, at least in the English part. As
this kind of corpus is not easily available we ex-
plored two strategies for the automatic construc-
tion of these corpora: (i) by machine translation of
sense-tagged corpora (Oliver and Climent, 2011),
(Oliver and Climent, 2012a) and (ii) by automatic
sense tagging of bilingual corpora (Oliver and Cli-
ment, 2012b).
Once we have created the parallel corpus, we
need a word alignment algorithm in order to create
the target WordNet. Fortunately, word alignment
is a well-known task and several freely available
algorithms are available. In previous works we
have used Berkeley Aligner (Liang et al., 2006). In
this paper we present the results using a very sim-
ple word alignment algorithm based on the most
frequent translation. This algorithm is available in
the WN-Toolkit.
With the parallel corpus based strategy we can
get the target language variants for synsyets hav-
ing both monosemic and polisemic English vari-
ants, that is, English words assigned to one or
more synsets.
</bodyText>
<sectionHeader confidence="0.7800075" genericHeader="method">
2.3.1 Machine translation of sense-tagged
corpora
</sectionHeader>
<bodyText confidence="0.999396625">
For the creation of the parallel corpus from a
monolingual sense-tagged corpus, we use a ma-
chine translation system to get the target sen-
tences. The machine translation system must be
capable of performing a good lexical selection,
that is, it should select the correct target words for
the source English words. Other kinds of transla-
tion errors are less important for this strategy.
</bodyText>
<subsectionHeader confidence="0.950553">
2.3.2 Automatic sense-tagging of parallel
corpora
</subsectionHeader>
<bodyText confidence="0.999903458333333">
The second strategy for the creation of the cor-
pora is to use a parallel corpus between English
and the target language and perform an automatic
sense tagging of the English sentences. Unfor-
tunately word sense disambiguation is a highly
error-prone task. The best WSD systems for En-
glish using WordNet synsets achieve a precision
score of about 60-65% (Snyder and Palmer, 2004;
Palmer et al., 2001). In our experiments we have
explored two options: (i) the use of Freeling and
UKB (Padr´o et al., 2010b) and (ii) Word Sense
Disambiguation of multilingual corpora based on
the sense information of all the languages (Shahid
and Kazakov, 2010).
We have used Freeling (Padr´o et al., 2010a)
and the integrated UKB module (Agirre and Soroa,
2009) to add sense tags to a fragment of the DGT-
TM corpus (Steinberger et al., 2012). Before using
this algorithm we have evaluated its the precision
by means of automatically sense tag some sense
tagged corpora: Semcor, Semeval2, Semeval3 and
the Princeton WordNet Gloss Corpus (PWGC).
After the automatic sense-tagging is performed,
the tags are compared with those in the manu-
ally sense tagged-version. In Table 4 we can see
the precision figure for each corpus and pos. As
we can see, there is a great difference in preci-
sion. This difference can be explained by the com-
plimentary values given in the table: the degree
of ambiguity in the corpus and the percentage of
open class words that are tagged in the corpus.
As we can observe, the better precision value is
achieved by the PWGC, having the smaller de-
gree of ambiguity and the smaller percentage of
tagged words. By contrast, the worse precision is
achieved by the Semeval3 corpus, which has the
highest degree of ambiguity and the highest per-
centage of tagged words.
We have also explored a word sense disam-
biguation strategy based on the sense information
provided by a multilingual corpus, following the
idea of (Ide et al., 2002). We have used the DGT-
TM Corpus (Steinberger et al., 2012) in six lan-
guages: English, Spanish, French, German, Italian
and Portuguese. We have sense tagged all the lan-
guages with no sense disambiguation, that is, giv-
ing all the possible senses to all the words in the
corpus present in the WordNet versions for these
languages. With all this sense information the
Word Sense Disambiguation task consists of com-
paring the synsets in all languages for the same
sentence, and taking the sense appearing the most
times. Using this strategy some degree of ambi-
guity is still present after disambiguation. For ex-
ample, for English the average number of synsets
for tagged words before disambiguation is 5.96
(16.05% of the tagged words are unambiguous),
and, after disambiguation, this figure is reduced to
2.46 (55.5% of the tagged words are unambigu-
ous).
We have manually evaluated a small portion of
this disambiguation strategy for the English DTG-
TM corpus, obtaining a precision of 51.25%, very
similar to the worst results for the Freeling+UKB
strategy. One of the problems of the practical
use of the multilingual word sense disambiguation
strategy is the sensitivity of the methodology on
the degree of development of the target WordNets.
It is very important that the target WordNets used
for tagging the target language corpora have regis-
tered all the senses for a given word. If this is not
the case, we will get the wrong results.
</bodyText>
<sectionHeader confidence="0.993947" genericHeader="method">
3 The WN-Toolkit
</sectionHeader>
<subsectionHeader confidence="0.998446">
3.1 Toolkit description
</subsectionHeader>
<bodyText confidence="0.999913">
The toolkit we present in this paper collects sev-
eral programs written in Python. All programs
must be run in a command line and several pa-
rameters must be given. All programs have the
option -h to get the required and optional param-
eters. The toolkit also provides some free lan-
guage resources. The toolkit is divided in the
following parts: (i) Dictionary-based strategies;
(ii) Babelnet-based strategies, (iii) Parallel corpus
based strategies and (iv) Resources, such as freely
available lexical resources, pre-processed corpora,
etc.
The toolkit can be freely downloaded from
http://lpg.uoc.edu/wn-toolkit.
In the rest of this section, each of these parts of
the toolkit are presented, along with the results of
the experiments of WordNet extraction for the fol-
lowing languages: Catalan, Spanish, French, Ger-
man, Italian and Portuguese. The evaluation of the
</bodyText>
<table confidence="0.992953">
Ambiguity % tagged w. Global Nouns Verbs Adjectives Adverbs
Semcor 7.61 84.24 51.99 58.64 40.68 61.57 68.91
Senseval2 5.48 88.88 59.77 70.55 31.49 62.82 66.28
Senseval3 7.84 89.44 51.82 57.08 42.46 59.72 100
PWGC 4.72 65.9 85.56 84.74 80.09 89.74 92.16
</table>
<tableCaption confidence="0.999711">
Table 4: Precision figures of the Freeling’s implementation of UKB algorithm for four English Corpora
</tableCaption>
<bodyText confidence="0.999924823529412">
results is performed automatically using the ex-
isting versions of these WordNets. We compare
the variants obtained for each synset in the target
languages. If the existing version of WordNet for
the given languages has the same variant for this
synset, the result is evaluated as correct. If the ex-
isting WordNet does not have any variant for the
synset, this result is not evaluated. This evalu-
ation method has a major drawback: as the ex-
isting WordNets for the target languages are not
complete (some variants for a given synset are not
registered), some correct proposals can be evalu-
ated as incorrect. For each strategy we have man-
ually evaluated a subset of the variants evaluated
as incorrect and those not evaluated for Catalan or
Spanish. Crrected precision figures are presented
for these languages.
</bodyText>
<subsectionHeader confidence="0.948198">
3.2 Dictionary-based strategies
3.2.1 Introduction
</subsectionHeader>
<bodyText confidence="0.997500111111111">
Using this strategy we can obtain variants only for
the synsets having monosemic English variants.
We can translate the English variants using dif-
ferent kinds of dictionaries (general, encyclopedic
and terminological dictionaries). We then assign
the translations to the synset of the target language
WordNet.
The WN-Toolkit provides several programs for
the use of this strategy:
</bodyText>
<listItem confidence="0.998435833333333">
• createmonosemicwordlist.py: for the cre-
ation of the lists of monosemic words of the
PWN. Alternatively, it is possible to use the
monosemic word lists corresponding to the
PWN version 3.0 distributed with the toolkit.
•
</listItem>
<bodyText confidence="0.9293522">
wndictionary.py: using the monosemic
word list of the PWN and a bilingual dictio-
nary this program is able to create a list of
synsets and the corresponding variants in the
target language.
</bodyText>
<listItem confidence="0.99147">
• wiktionary2bildic.py: this program creates
a bilingual dictionary suitable for use with the
program wndictionary.py from the xml dump
files of Wiktionary2.
• wikipedia2bildic.py: this program creates a
bilingual dictionary suitable for the use with
the program wndictionary.py from the xml
dump files of the Wikipedia3.
• apertium2bildic.py: this program creates a
bilingual dictionary suitable for the use with
the program wndictionary.py from the trans-
fer dictionaries of the open source machine
translation system Apertium4 (Forcada et al.,
2009). This resource is useful for Basque,
Catalan, Esperanto, Galician, Haitian Cre-
ole, Icelandic, Macedonian, Spanish, Welsh
and Icelandic, as there are available linguistic
data for the translation system between En-
glish and these languages.
• combinedictionary.py: this program allows
for the combination of several dictionaries,
creating a dictionary with all the informa-
tion from every dictionary, eliminating the re-
peated entries.
</listItem>
<subsectionHeader confidence="0.55094">
3.2.2 Experimental settings
</subsectionHeader>
<bodyText confidence="0.998061222222222">
We have used this strategy for the creation of
WordNets for the following 6 languages: Catalan,
Spanish, French, German, Italian and Portuguese.
We have used Wiktionary and Wikipedia for all
these languages and we have explored the use of
additional resources for Catalan and Spanish. In
Table 5 we can see the number of entries of the
dictionaries created with the toolkit for all six lan-
guages using Wiktionary and Wikipedia.
</bodyText>
<table confidence="0.997837714285714">
Wiktionary Wikipedia
cat 9,979 31,578
spa 26,064 106,665
fre 30,708 142,142
deu 29,808 164,463
ita 20,542 77,736
por 15,280 42,653
</table>
<tableCaption confidence="0.99934">
Table 5: Size of the dictionaries
</tableCaption>
<footnote confidence="0.999988">
2www.wiktionary.org
3www.wikipedia.org
4http://apertium.org
</footnote>
<subsubsectionHeader confidence="0.444567">
3.2.3 Results and evaluation
</subsubsectionHeader>
<bodyText confidence="0.981962769230769">
In Table 6 we can see the results of the evaluation
of the dictionary-based strategy using Wiktionary.
The number of variants obtained depends on the
Wiktionary size for each of the languages and
ranges from 5,081 for Catalan to 18,092 for Ger-
man. The automatic calculated precision ranges
from 48.09% for German to 84.8% for French.
This precision figure can be strongly influenced by
the size of the reference WordNets, and more pre-
cisely on the number of variants for each synset.
In the column New variants we can see the num-
ber of obtained variants for synsets not present in
the target reference WordNet.
</bodyText>
<table confidence="0.997627714285714">
Var. Precision New var.
cat 5,081 78.36 1,588
spa 14,990 50.93 8,570
fre 16,424 84.80 1,799
deu 18,092 48.09 12,405
ita 10,209 75.45 3,369
por 7,820 80.71 1,104
</table>
<tableCaption confidence="0.933288">
Table 6: Evaluation of the dictionary based strat-
egy using Wiktionary
</tableCaption>
<bodyText confidence="0.999409166666667">
In Table 7 the results for the acquisition of
WordNets from the Wikipedia as a dictionary are
presented. The precision values are calculated au-
tomatically. The number of obtained variants is
lower than the previous results from the Wiki-
tionary.
</bodyText>
<table confidence="0.999273">
Var. Precision New var.
cat 290 63.29 132
spa 607 63.19 463
fre 654 71.49 177
deu 766 24.14 737
ita 361 52.17 292
por 315 72.93 85
</table>
<tableCaption confidence="0.9009305">
Table 7: Evaluation of the dictionary based strat-
egy using Wikipedia
</tableCaption>
<bodyText confidence="0.999984565217391">
We have extended the dictionary-based strategy
for Catalan using the transfer dictionary of the
open source machine translation system Apertium
along with Wikipedia and Wiktionary. The result-
ing combined dictionary has 65,937 entries. This
made it possible to create a new WordNet with
11,970 entries with an automatic calculated preci-
sion of 75.75%. We have manually revised 10% of
the results for Catalan and calculated a corrected
precision of 92.86% (most of the non-evaluated
variants were correct and some of those evaluated
as incorrect were correct too).
As we can see from Tables 6 and 7 the num-
ber of extracted variants from Wikipedia is smaller
than the extracted from Wiktionary, although the
dictionary extracted from Wikipedia is 3 or 4
times larger. This can be explained by the percent
of encyclopedic-like variants in English Word-
Net, that can be calculated counting the number
of noun variants starting by a upper-case letter.
Roughly 30% of the nouns in WordNet are ency-
clopaedic variants, and this means about the 20%
of the overall variants.
</bodyText>
<subsectionHeader confidence="0.918415">
3.3 Babelnet-based strategies
3.3.1 Introduction
</subsectionHeader>
<bodyText confidence="0.999986375">
The program babel2wordnet.py allows us to cre-
ate WordNets from the Babelnet glosses file. This
program needs as parameters the two-letter code
of the target language and the path to the Babel-
net glosses file. With these two parameters, the
program is able to create WordNets only for the
languages present in Babelnet (in fact the pro-
gram simply changes the format of the output).
The program also accepts an English-target lan-
guage dictionary created from Wikipedia (using
the program wikipedia2bildic.py). This parameter
is mandatory for target languages not present in
Babelnet, and optional for languages included in
Babelnet. The program also accepts as a parameter
the data.noun file of PWN, useful for performing
caps normalization.
</bodyText>
<subsectionHeader confidence="0.896083">
3.3.2 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999851833333333">
For our experiments we have used the 1.1.1 ver-
sion of Babelnet, along with the dictionaries ex-
tracted from Wikipedia as explained in section
3.2.2. We used the babel2wordnet.py program us-
ing the above-mentioned dictionary and the caps
normalization option.
</bodyText>
<sectionHeader confidence="0.5639" genericHeader="method">
3.3.3 Results and evaluation
</sectionHeader>
<bodyText confidence="0.998002">
In Table 8 we can see the results obtained for Cata-
lan, Spanish, French, German and Italian with-
out the use of a complementary Wikipedia dictio-
nary. Note that no values are presented for Por-
tuguese, as this language is not included in Ba-
belnet. For all languages, the precision values are
calculated automatically taking the existing Word-
Nets for these languages described in Table 1 as
references.
Table 9 shows the results using the optional
Wikipedia dictionary. Note that now results are
presented for Portuguese, although this language
</bodyText>
<table confidence="0.979091714285714">
Var. Precision New var.
cat 23,115 70.95 9,129
spa 31,351 76.80 19,107
fre 32,594 80.71 8,291
deu 32,972 52.10 27,243
ita 27,481 66.78 16.945
por - - -
</table>
<tableCaption confidence="0.998602">
Table 8: Evaluation of the Babelnet-based strategy
</tableCaption>
<bodyText confidence="0.998079166666667">
is not present in Babelnet. These results are very
similar with the results with no Wikipedia dictio-
nary, except for Portuguese. This can be explained
by the fact that Babelnet itself uses Wikipedia, so
adding the same resource again (although a differ-
ent version) leads to a very little improvements.
</bodyText>
<table confidence="0.861361714285714">
Var. Precision New var.
cat 23,307 70.85 9,244
spa 31,604 76.61 19,301
fre 32,880 80.60 8,415
deu 33,455 51.79 27,651
ita 27,695 66.53 17,069
por 1,392 75.23 532
</table>
<tableCaption confidence="0.890489">
Table 9: Evaluation of the Babelnet-based strategy
with Wikipedia dictionary
</tableCaption>
<bodyText confidence="0.999578666666667">
We have manually evaluated 1% of the results
for Catalan and we obtained a corrected precision
value of 89.17%
</bodyText>
<subsectionHeader confidence="0.797809">
3.3.4 Preliminary results using Babelnet 2.0
</subsectionHeader>
<bodyText confidence="0.9921154">
In Table 10 preliminary results using the Babel-
net 2.0 are shown. Please, note that precision val-
ues for Catalan, Spanish, French, Italian and Por-
tuguese are marked with an asterisk, indicating
that these values can not be considered as correct.
The reason is simple, we are automatically eval-
uating the results with one of the resources used
for constructing the Babelnet 2.0. Remember than
one of the resoures for the construction of Babel-
net 2.0 are the WordNet included in the Open Mul-
tilingual WordNet, the same WordNet used for au-
tomatic evaluation. Figures of new variants are
comparable with the results obtained with the pre-
vious version of Babelnet.
Var. Precision New var.
cat 84,519 *94.12 9,453
spa 81,160 *94.58 20,132
fre 34,746 *79,03 8,660
deu 35,905 49,43 29,522
ita 64,504 *93,83 17.782
por 28,670 *86.88 7,734
Table 10: Evaluation of the Babelnet-based strat-
egy using Babelnet 2.0
Anyway, Babelnet 2.0 can be a good starting
point for constructing WordNets for 50 languages.
The algorithm for exploiting the Babelnet 2.0 for
WordNet construction is also included in the WN-
Toolkit. Please, note that this algorithm simply
changes the format of the Babelnet file into the
Open Multilingual Wordnet format.
</bodyText>
<subsectionHeader confidence="0.6263205">
3.4 Parallel corpus based strategies
3.4.1 Introduction
</subsectionHeader>
<bodyText confidence="0.998387076923077">
The WN-Toolkit implements a simple word align-
ment algorithm useful for the creation of Word-
Nets from parallel corpora. The program, called
synset-word-alignement.py, calculates the most
frequent translation found in the corpus for each
synset. We must bear in mind that the parallel cor-
pus must be tagged with PWN synsets in the En-
glish part. The target corpus must be lemmatized
and tagged with very simple tags (n for nouns; v
for verbs; a for adjectives; r for adverbs and any
other letter for other pos).
The synset-word-alignment program uses two
parameters to tune its behaviour:
</bodyText>
<listItem confidence="0.92403625">
• The i parameter forces the first translation
equivalent to have a frequency at least i times
greater than the frequency of the second can-
didate. If this condition is not achieved, the
translation candidate is rejected and the pro-
gram fails to give a target variant for the given
synset.
• The f parameter is the greater value for the
ratio between the frequency of the transla-
tion candidate in the target part of the parallel
corpus and the frequency of the synset in the
source part of the parallel corpus.
</listItem>
<subsectionHeader confidence="0.963536">
3.4.2 Experimental settings
</subsectionHeader>
<bodyText confidence="0.999874">
For our experiments we have used two strategies
for the creation of the parallel corpus with sense
tags in the English part.
</bodyText>
<listItem confidence="0.9892247">
• Machine translation of sense-tagged corpora.
We have used two corpora: Semcor and
Princeton WordNet Gloss Corpus. We have
used Google Translate to machine translate
these corpora to Catalan, Spanish, French,
German, Italian and Portuguese.
• Automatic sense tagging of parallel corpora,
using two WSD techniques: (i) WSD us-
ing multilingual information and (ii) Freel-
ing + UKB. We have used a 118K sentences
</listItem>
<bodyText confidence="0.998163375">
fragment of the DGT-TM multilingual corpus
(available in English, Spanish, French, Ger-
man, Italian and Portuguese, but not in Cata-
lan). We have chosen this number of sen-
tences to have a corpus of a similar size to
the Princeton WordNet Gloss Corpus
For our experiments we have set the parameter
i to 2.5 and the parameter f to 5.
</bodyText>
<subsectionHeader confidence="0.594174">
3.4.3 Results and evaluation
</subsectionHeader>
<bodyText confidence="0.9993854">
In Table 11 and 12 we can see the results for the
use of machine translation of Semcor an PWGC.
As we can see, the precision figures are very sim-
ilar for both corpora, but the number of extracted
variants is greater for the PWGC, due to the larger
size of the corpus. We have manually evaluated
20% of the results for Catalan. In the case of
Semcor we have calculated a corrected value of
94.74%, whereas for PWGC corpus we have ob-
tained a corrected value of 96.18%.
</bodyText>
<table confidence="0.996334285714286">
Var. Precision New var.
cat 2,001 87.63 449
spa 2,076 88.93 504
fre 1,844 91.83 142
deu 2,657 70.26 1,285
ita 858 93.81 66
por 2,064 84.14 324
</table>
<tableCaption confidence="0.994981">
Table 11: Evaluation of the parallel corpus based
strategy: machine translation of Semcor corpus
</tableCaption>
<table confidence="0.975271142857143">
Var. Precision New var.
cat 4,744 87.87 1,125
spa 4,959 84.28 2,102
fre 4,598 91.63 510
deu 5,055 71.11 2,559
ita 4,870 88.68 904
por 4,845 86.26 871
</table>
<tableCaption confidence="0.961097">
Table 12: Evaluation of the parallel corpus based
strategy: machine translation of PWGC corpus
</tableCaption>
<bodyText confidence="0.999453769230769">
In Table 13 and 14 we can see the results for
the use of automatic sense tagging for the DGT-
TM corpus using a multilingual strategy and Freel-
ing+UKB. Here the precision figures are also sim-
ilar for both strategies, but the number of extracted
variants is greater for the Freeling+UKB strategy.
The reason is that using Freeling and UKB we can
disambiguate all the ambiguous words, while us-
ing the multilingual strategy we are not able to
disambiguate all of them and in some cases some
degree of ambiguity remains. For the extraction
process we have only considered the fully disam-
biguated words.
</bodyText>
<table confidence="0.993582">
Var. Precision New var.
spa 313 75.35 171
fre 173 75.89 32
deu 207 36.54 155
ita 266 82.44 61
por 302 79.20 52
</table>
<tableCaption confidence="0.965849">
Table 13: Multilingual WSD of 118K sentences
fragment of the DGT-TM corpus
</tableCaption>
<table confidence="0.999793">
Var. Precision New var.
spa 1,155 79.71 386
fre 484 68.66 82
deu 609 24.72 431
ita 1,031 78.31 252
por 1,075 74.23 194
</table>
<tableCaption confidence="0.998431">
Table 14: Freeling + UKB of 118K sentences frag-
</tableCaption>
<bodyText confidence="0.981957142857143">
ment of the DGT-TM corpus
In this case we have manually evaluated the re-
sults for Spanish as Catalan is not available in
this corpus. For the multilingual strategy we have
manually evaluated 100% of the results and cal-
culated a corrected precision figure of 91.67%.
For the Freeling + UKB results we have manually
evaluated 25% of the results, obtaining a corrected
precision of 88.94%.
If we analyse the results, we see that the ex-
traction task has a much higher precision than
the Word Sense Disambiguation strategies used to
process the corpora. This may seem a little odd
but we must bear in mind that we have used very
restrictive values for the parameters i and f of the
extraction program. These parameters allow us to
extract only the best candidates, ensuring a good
precision value for the extraction process, but a
very poor recall value. It should be noted than for
Spanish with the machine translation strategy we
are getting 2,076 candidatesfor the Semcor Corpus
and 4,959 for the Princeton Gloss Corpus, and we
are now getting 313 candidates for the multilin-
gual WSD strategy and 1,155 for the UKB WSD.
If we force the extraction process to get 2,076 can-
didates, we obtain a precision value of 43.77%
for the multilingual WSD strategy and 58.12% for
UKB.
</bodyText>
<sectionHeader confidence="0.99981" genericHeader="method">
4 Resources
</sectionHeader>
<bodyText confidence="0.9995985">
We are distributing some resources for several lan-
guages with the hope they can be useful to use the
toolkit to create new WordNets or extend existing
ones.
</bodyText>
<listItem confidence="0.945050666666667">
• Lexical resources: dictionaries created from
Wiktionary, Wikipedia and Apertium transfer
dictionaries.
• Preprocessed corpora: DGT-TM, Emea and
United Nations Corpus from Opus5 (Tiede-
mann, 2012). We have semantically-tagged
</listItem>
<bodyText confidence="0.703682">
the English part of the corpora with Freeling
and UKB and lemmatized and tagged some
of the target languages. We plan to prepro-
cess other parallel corpora in the future.
</bodyText>
<sectionHeader confidence="0.999245" genericHeader="method">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9998776">
We have presented the results of the automatic cre-
ation of WordNets for six languages using several
techniques following the expand model. All these
techniques are implemented in the freely available
WN-Toolkit and have been successfully used for
the expansion of the Catalan and Spanish Word-
Nets under the Know2 project. The WordNets
and the toolkit itself are being improved under the
Skater Project. The successful use of this toolkit
has also been reported for the Galician WordNet
(G´omez Guinovart and Sim˜oes, 2013).
We can analyse the coincident extracted synsets
and their associated precision for Catalan in Table
15. Here we have mixed the results for extended
dictionary, Babelnet, translated PWGC and trans-
lated Semcor. The overall precision is 71.06% but,
if we take into account the variants extracted using
2 or more methodologies, this precision rises up to
91.35%, although the number of extracted variants
is drastically reduced.
</bodyText>
<table confidence="0.9950864">
Freq. Var. Precision New var.
1+ 35,142 71.06 13,997
2+ 5,661 91.35 1,062
3+ 1052 94.92 87
4+ 135 96.06 8
</table>
<tableCaption confidence="0.987277">
Table 15: Evaluation of the repetition of the results
</tableCaption>
<bodyText confidence="0.929236583333333">
for different strategies for Catalan
This combination of methodologies allows us to
classify the extracted variants with an estimated
precision value so we can obtain variants and give
each variant a score. This score can be updated
if the variant is obtained again using a different
methodology or resource.
It’s important to take into account the fact
that the automatically-calculated precision value
is very prone to errors, as, if a given synset hav-
ing a variant lacks other possible variants and if
those unregistered correct variants are extracted,
</bodyText>
<footnote confidence="0.889872">
5http://opus.lingfil.uu.se/
</footnote>
<bodyText confidence="0.98860775">
the evaluation algorithm will consider them as in-
correct. In Table 16 we can see the comparison
between the automatic and corrected values of pre-
cision.
</bodyText>
<table confidence="0.999641">
Strategy Lang. % rev. Pauto. Pcorr.
Dictionaries cat 10 75.75 92.86
Babelnet cat 1 70.85 89.17
Semcor trad. cat 20 87.63 94.75
PWGC trad. cat 20 87.87 96.18
DGT-TM mult. spa 100 75.35 91.67
DGT-TM UKB spa 25 79.71 88.94
</table>
<tableCaption confidence="0.989036">
Table 16: Comparison of automatic and corrected
precision figures
</tableCaption>
<sectionHeader confidence="0.999259" genericHeader="discussions">
6 Future work
</sectionHeader>
<bodyText confidence="0.999960285714286">
We plan to follow the development of the WN-
Toolkit in the following directions: (i) change the
script-oriented implementation of the current ver-
sion to a class-oriented implementation allowing
easy integration into another applications; (ii) in-
creasing the number of integrated freely available
resources and implementing a web query based
use of some resources; (iii) developing a simple
graphical user interface to facilitate its use and (iv)
pre-processing and distributing more freely avail-
able corpora.
We also plan to use the toolkit to develop
preliminary versions of WordNets for other lan-
guages.
</bodyText>
<sectionHeader confidence="0.997654" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.976817333333333">
This research has been carried out thanks to the
Project SKATER, (TIN2012-38548-C06-01) of the
Spanish Ministry of Science and Innovation
</bodyText>
<sectionHeader confidence="0.995089" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995948606837606">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 33–41.
Jordi Atserias, Salvador Climent, Xavier Farreres, Ger-
man Rigau, and Horacio Rodriguez. 1997. Com-
bining multiple methods for the automatic construc-
tion of multi-lingual WordNets. In Recent Advances
in Natural Language Processing II. Selected papers
from RANLP, volume 97, pages 327–338.
Laura Ben´ıtez, Sergi Cervell, Gerard Escudero, M`onica
L´opez, German Rigau, and Mariona Taul´e. 1998.
Methods and Tools for Building the Catalan Word-
Net. In In Proceedings of the ELRA Workshop on
Language Resources for European Minority Lan-
guages.
Francis Bond and Paik Kyonghee. 2012. A survey
of wordnets and their licenses. In Proceedings of
the 6th International Global WordNet Conference,
Matsue (Japan), pages 64–71.
Valeria de Paiva and Alexandre Rademaker. 2012. Re-
visiting a brazilian WordNet. In Proceedings of the
6th Global Wordnet Conference, Matsue (Japan).
Christiane Fellbaum. 1998. WordNet: An electronic
lexical database. The MIT press.
Mikel L. Forcada, Francis M. Tyers, and Gema
Ram´ırez-S´anchez. 2009. The apertium machine
translation platform: five years on. In Proceedings
of the First International Workshop on Free/Open-
Source Rule-Based Machine Translation, pages 3–
10.
Aitor Gonzalez-Agirre, Egoitz Laparra, and German
Rigau. 2012. Multilingual central repository ver-
sion 3.0. In Proceedings of the 6th Global WordNet
Conference.
Xavier G´omez Guinovart and Alberto Sim˜oes. 2013.
Retreading dictionaries for the 21st century. In
Proceedings of the 2nd Symposium on Languages,
Applications and Technologies (SLATE’13), pages
115–126.
Birgit Hamp and Helmut Feldweg. 1997. GermaNet
- a Lexical-Semantic net for german. In Proceed-
ings of the ACL Workshor on Automatic Information
Extraction and Building of Lexical and Sematic Re-
sources for NLP Applications, pages 9—15.
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002.
Sense discrimination with parallel corpora. In
Proceedings of the ACL-02 workshop on Word
sense disambiguation: recent successes and future
directions-Volume 8, pages 61–66.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the HLT-
NAACL ’06.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: building a very large multilingual se-
mantic network. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ’10, pages 216–225, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics. ACM ID: 1858704.
Antoni Oliver and Salvador Climent. 2011. Con-
strucci´on de los wordnets 3.0 para castellano y
catal´an mediante traducci´on autom´atica de corpus
anotados sem´anticamente. In Proceedings of the
27th Conference of the SEPLN, Huelva Spain.
Antoni Oliver and Salvador Climent. 2012a. Build-
ing wordnets by machine translation of sense tagged
corpora. In Proceedings of the Global WordNet
Conference, Matsue, Japan.
Antoni Oliver and Salvador Climent. 2012b. Parallel
corpora for wordnet construction. In Proceedings of
the 13th International Conference on Intelligent Text
Processing and Computational Linguistics (Cicling
2012). New Delhi (India).
Llu´ıs Padr´o, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castell´on. 2010a. FreeLing 2.1:
Five years of open-source language processing tools.
In LREC, volume 10, pages 931–936.
Llu´ıs Padr´o, Samuel Reese, Eneko Agirre, and Aitor
Soroa. 2010b. Semantic services in freeling 2.1:
Wordnet and UKB. In Proceedings of the 5th Inter-
national Conference of the Global WordNet Associ-
ation (GWC-2010).
Martha Palmer, Christiane Fellbaum, Scott Cotton,
Lauren Delfs, and Hoa Trang Dang. 2001. English
tasks: All-words and verb lexical sample. In The
Proceedings of the Second International Workshop
on Evaluating Word Sense Disambiguation Systems,
pages 21–24.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. Developing an aligned multilingual
database. In Proc. 1st Int’l Conference on Global
WordNet.
Benoit Sagot and Darja Fiˇser. 2008. Building a free
french wordnet from multilingual resources. In Pro-
ceedings of OntoLex 2008, Marrackech (Morocco).
Ahmad R. Shahid and Dimitar Kazakov. 2010. Re-
trieving lexical semantics from multilingual corpora.
Polibits, 41:25–28.
Benjamin Snyder and Martha Palmer. 2004. The En-
glish all-words task. In Proceedings of the 3rd In-
ternational Workshop on the Evaluation of Systems
for the Semantic Analysis of Text (Senseval-3), pages
41–43, Barcelona (Spain).
Ralf Steinberger, Andreas Eisele, Szymon Klocek,
Spyridon Pilos, and Patrick Schl¨uter. 2012. Dgt-
tm: A freely available translation memory in 22
languages. In Proceedings of the 8th International
Conference on Language Resources and Evaluation,
pages 454–459.
J¨org Tiedemann. 2012. Parallel data, tools and inter-
faces in OPUS. In In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation (LREC’2012), pages 2214–2218.
Piek Vossen. 1998. Introduction to Eurowordnet.
Computers and the Humanities, 32(2):73–89.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.237746">
<title confidence="0.969212">WN-Toolkit: Automatic generation of WordNets following the expand model</title>
<author confidence="0.8987">Antoni</author>
<affiliation confidence="0.770385">Universitat Oberta de</affiliation>
<author confidence="0.948585">Catalonia</author>
<email confidence="0.999312">aoliverg@uoc.edu</email>
<abstract confidence="0.947497222222222">This paper presents a set of methodologies and algorithms to create WordNets following the expand model. We explore dictionary and BabelNet based strategies, as well as methodologies based on the use of parallel corpora. Evaluation results for six languages are presented: Catalan, Spanish, French, German, Italian and Portuguese. Along with the methodologies and evaluation we present an implementation of all the algorithms grouped in a set of programs or toolkit. These programs have been successfully used in the Know2 Project for the creation of Catalan and Spanish WordNet 3.0. The toolkit is published under the GNU-GPL license and be freely downloaded from</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing pagerank for word sense disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="8463" citStr="Agirre and Soroa, 2009" startWordPosition="1370" endWordPosition="1373"> and perform an automatic sense tagging of the English sentences. Unfortunately word sense disambiguation is a highly error-prone task. The best WSD systems for English using WordNet synsets achieve a precision score of about 60-65% (Snyder and Palmer, 2004; Palmer et al., 2001). In our experiments we have explored two options: (i) the use of Freeling and UKB (Padr´o et al., 2010b) and (ii) Word Sense Disambiguation of multilingual corpora based on the sense information of all the languages (Shahid and Kazakov, 2010). We have used Freeling (Padr´o et al., 2010a) and the integrated UKB module (Agirre and Soroa, 2009) to add sense tags to a fragment of the DGTTM corpus (Steinberger et al., 2012). Before using this algorithm we have evaluated its the precision by means of automatically sense tag some sense tagged corpora: Semcor, Semeval2, Semeval3 and the Princeton WordNet Gloss Corpus (PWGC). After the automatic sense-tagging is performed, the tags are compared with those in the manually sense tagged-version. In Table 4 we can see the precision figure for each corpus and pos. As we can see, there is a great difference in precision. This difference can be explained by the complimentary values given in the </context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordi Atserias</author>
<author>Salvador Climent</author>
<author>Xavier Farreres</author>
<author>German Rigau</author>
<author>Horacio Rodriguez</author>
</authors>
<title>Combining multiple methods for the automatic construction of multi-lingual WordNets.</title>
<date>1997</date>
<booktitle>In Recent Advances in Natural Language Processing II. Selected papers from RANLP,</booktitle>
<volume>97</volume>
<pages>327--338</pages>
<contexts>
<context position="3994" citStr="Atserias et al., 1997" startWordPosition="652" endWordPosition="655">2.44 Table 2: Degree of polysemy in PWN 3.0 letter in upper case (probably corresponding to proper names) and in lower case. In Table 3, we can see the figures. variants % upper case 84.714 68.75 lower case 38.514 31.25 Table 3: Number of monosemic variants with the first letter in uppercase or lowercase These figures show us that a large percentage of a target WordNet can be implemented using this strategy. We must bear in mind, however, that using this methodology, we would probably not be able to obtain the most frequent variants, as common words are usually polysemic. The Spanish WordNet (Atserias et al., 1997) in the EuroWordNet project and the Catalan WordNet (Benitez et al., 1998) were constructed using dictionaries. With the dictionary-based strategy we will only be able to get target language variants for synsets having monosemic English variants, i.e. English words assigned to a single synset. 2.2 Babelnet BabelNet (Navigli and Ponzetto, 2010) is a semantic network and ontology created by linking Wikipedia entries to WordNet synsets. These relations are multilingual through the interlingual relations in Wikipedia. For languages lacking the corresponding Wikipedia entry a statistical machine tr</context>
</contexts>
<marker>Atserias, Climent, Farreres, Rigau, Rodriguez, 1997</marker>
<rawString>Jordi Atserias, Salvador Climent, Xavier Farreres, German Rigau, and Horacio Rodriguez. 1997. Combining multiple methods for the automatic construction of multi-lingual WordNets. In Recent Advances in Natural Language Processing II. Selected papers from RANLP, volume 97, pages 327–338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Ben´ıtez</author>
<author>Sergi Cervell</author>
<author>Gerard Escudero</author>
<author>M`onica L´opez</author>
<author>German Rigau</author>
<author>Mariona Taul´e</author>
</authors>
<title>Methods and Tools for Building the Catalan WordNet. In</title>
<date>1998</date>
<booktitle>In Proceedings of the ELRA Workshop on Language Resources for European Minority Languages.</booktitle>
<marker>Ben´ıtez, Cervell, Escudero, L´opez, Rigau, Taul´e, 1998</marker>
<rawString>Laura Ben´ıtez, Sergi Cervell, Gerard Escudero, M`onica L´opez, German Rigau, and Mariona Taul´e. 1998. Methods and Tools for Building the Catalan WordNet. In In Proceedings of the ELRA Workshop on Language Resources for European Minority Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Paik Kyonghee</author>
</authors>
<title>A survey of wordnets and their licenses.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Global WordNet Conference,</booktitle>
<pages>64--71</pages>
<location>Matsue</location>
<contexts>
<context position="1592" citStr="Bond and Kyonghee, 2012" startWordPosition="242" endWordPosition="245">urce in Natural Language Processing research and applications. The English WordNet (PWN - Princeton WordNet) is being updated regularly, so that its number of synsets increases with every new version. The current version of PWN is 3.1, but in our experiments we are using the 3.0 version because is the latest one available for download at the time of performing the experiments. WordNet versions in other languages are also available. On the Global WordNet Association1 website, a comprehensive list of WordNets available for different languages can be found. The Open Multilingual WordNet project (Bond and Kyonghee, 2012) provides free access to WordNets in several languages in a common format. We have used the WordNets from this project for 1www.globalwordnet.org Catalan (Gonzalez-Agirre et al., 2012) , Spanish (Gonzalez-Agirre et al., 2012) , French (WOLF) (Sagot and Fiˇser, 2008) , Italian (Multiwordnet) (Pianta et al., 2002) and Portuguese (OpenWNPT) (de Paiva and Rademaker, 2012) . For German we have used the GermaNet 7.0 (Hamp and Feldweg, 1997), freely available for research. In Table 1, the sizes of all these WordNets are presented along with the size of the PWN. Synsets Words English 118.695 206.979 C</context>
</contexts>
<marker>Bond, Kyonghee, 2012</marker>
<rawString>Francis Bond and Paik Kyonghee. 2012. A survey of wordnets and their licenses. In Proceedings of the 6th International Global WordNet Conference, Matsue (Japan), pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valeria de Paiva</author>
<author>Alexandre Rademaker</author>
</authors>
<title>Revisiting a brazilian WordNet.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th Global Wordnet Conference,</booktitle>
<location>Matsue</location>
<marker>de Paiva, Rademaker, 2012</marker>
<rawString>Valeria de Paiva and Alexandre Rademaker. 2012. Revisiting a brazilian WordNet. In Proceedings of the 6th Global Wordnet Conference, Matsue (Japan).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An electronic lexical database.</title>
<date>1998</date>
<publisher>The MIT press.</publisher>
<contexts>
<context position="914" citStr="Fellbaum, 1998" startWordPosition="134" endWordPosition="135">y and BabelNet based strategies, as well as methodologies based on the use of parallel corpora. Evaluation results for six languages are presented: Catalan, Spanish, French, German, Italian and Portuguese. Along with the methodologies and evaluation we present an implementation of all the algorithms grouped in a set of programs or toolkit. These programs have been successfully used in the Know2 Project for the creation of Catalan and Spanish WordNet 3.0. The toolkit is published under the GNU-GPL license and can be freely downloaded from http: //lpg.uoc.edu/wn-toolkit. 1 Introduction WordNet (Fellbaum, 1998) is a lexical database that has become a standard resource in Natural Language Processing research and applications. The English WordNet (PWN - Princeton WordNet) is being updated regularly, so that its number of synsets increases with every new version. The current version of PWN is 3.1, but in our experiments we are using the 3.0 version because is the latest one available for download at the time of performing the experiments. WordNet versions in other languages are also available. On the Global WordNet Association1 website, a comprehensive list of WordNets available for different languages</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An electronic lexical database. The MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikel L Forcada</author>
<author>Francis M Tyers</author>
<author>Gema Ram´ırez-S´anchez</author>
</authors>
<title>The apertium machine translation platform: five years on.</title>
<date>2009</date>
<booktitle>In Proceedings of the First International Workshop on Free/OpenSource Rule-Based Machine Translation,</booktitle>
<pages>3--10</pages>
<marker>Forcada, Tyers, Ram´ırez-S´anchez, 2009</marker>
<rawString>Mikel L. Forcada, Francis M. Tyers, and Gema Ram´ırez-S´anchez. 2009. The apertium machine translation platform: five years on. In Proceedings of the First International Workshop on Free/OpenSource Rule-Based Machine Translation, pages 3– 10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitor Gonzalez-Agirre</author>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Multilingual central repository version 3.0.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th Global WordNet Conference.</booktitle>
<contexts>
<context position="1776" citStr="Gonzalez-Agirre et al., 2012" startWordPosition="270" endWordPosition="273">th every new version. The current version of PWN is 3.1, but in our experiments we are using the 3.0 version because is the latest one available for download at the time of performing the experiments. WordNet versions in other languages are also available. On the Global WordNet Association1 website, a comprehensive list of WordNets available for different languages can be found. The Open Multilingual WordNet project (Bond and Kyonghee, 2012) provides free access to WordNets in several languages in a common format. We have used the WordNets from this project for 1www.globalwordnet.org Catalan (Gonzalez-Agirre et al., 2012) , Spanish (Gonzalez-Agirre et al., 2012) , French (WOLF) (Sagot and Fiˇser, 2008) , Italian (Multiwordnet) (Pianta et al., 2002) and Portuguese (OpenWNPT) (de Paiva and Rademaker, 2012) . For German we have used the GermaNet 7.0 (Hamp and Feldweg, 1997), freely available for research. In Table 1, the sizes of all these WordNets are presented along with the size of the PWN. Synsets Words English 118.695 206.979 Catalan 45.826 46.531 Spanish 38.512 36.681 French 59.091 55.373 Italian 34.728 40.343 Portuguese 41.810 52.220 German 74.612 99.529 Table 1: Size of the WordNets 2 The expand model Acc</context>
</contexts>
<marker>Gonzalez-Agirre, Laparra, Rigau, 2012</marker>
<rawString>Aitor Gonzalez-Agirre, Egoitz Laparra, and German Rigau. 2012. Multilingual central repository version 3.0. In Proceedings of the 6th Global WordNet Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier G´omez Guinovart</author>
<author>Alberto Sim˜oes</author>
</authors>
<title>Retreading dictionaries for the 21st century.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Symposium on Languages, Applications and Technologies (SLATE’13),</booktitle>
<pages>115--126</pages>
<marker>Guinovart, Sim˜oes, 2013</marker>
<rawString>Xavier G´omez Guinovart and Alberto Sim˜oes. 2013. Retreading dictionaries for the 21st century. In Proceedings of the 2nd Symposium on Languages, Applications and Technologies (SLATE’13), pages 115–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Birgit Hamp</author>
<author>Helmut Feldweg</author>
</authors>
<title>GermaNet - a Lexical-Semantic net for german.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Workshor on Automatic Information Extraction and Building of Lexical and Sematic Resources for NLP Applications,</booktitle>
<pages>9--15</pages>
<contexts>
<context position="2030" citStr="Hamp and Feldweg, 1997" startWordPosition="313" endWordPosition="316">the Global WordNet Association1 website, a comprehensive list of WordNets available for different languages can be found. The Open Multilingual WordNet project (Bond and Kyonghee, 2012) provides free access to WordNets in several languages in a common format. We have used the WordNets from this project for 1www.globalwordnet.org Catalan (Gonzalez-Agirre et al., 2012) , Spanish (Gonzalez-Agirre et al., 2012) , French (WOLF) (Sagot and Fiˇser, 2008) , Italian (Multiwordnet) (Pianta et al., 2002) and Portuguese (OpenWNPT) (de Paiva and Rademaker, 2012) . For German we have used the GermaNet 7.0 (Hamp and Feldweg, 1997), freely available for research. In Table 1, the sizes of all these WordNets are presented along with the size of the PWN. Synsets Words English 118.695 206.979 Catalan 45.826 46.531 Spanish 38.512 36.681 French 59.091 55.373 Italian 34.728 40.343 Portuguese 41.810 52.220 German 74.612 99.529 Table 1: Size of the WordNets 2 The expand model According to (Vossen, 1998), we can distinguish two general methodologies for WordNet construction: (i) the merge model, where a new ontology is constructed for the target language; and (ii) the expand model, where variants associated with PWN synsets are t</context>
</contexts>
<marker>Hamp, Feldweg, 1997</marker>
<rawString>Birgit Hamp and Helmut Feldweg. 1997. GermaNet - a Lexical-Semantic net for german. In Proceedings of the ACL Workshor on Automatic Information Extraction and Building of Lexical and Sematic Resources for NLP Applications, pages 9—15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Ide</author>
<author>Tomaz Erjavec</author>
<author>Dan Tufis</author>
</authors>
<title>Sense discrimination with parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Word</booktitle>
<pages>61--66</pages>
<contexts>
<context position="9652" citStr="Ide et al., 2002" startWordPosition="1575" endWordPosition="1578">mentary values given in the table: the degree of ambiguity in the corpus and the percentage of open class words that are tagged in the corpus. As we can observe, the better precision value is achieved by the PWGC, having the smaller degree of ambiguity and the smaller percentage of tagged words. By contrast, the worse precision is achieved by the Semeval3 corpus, which has the highest degree of ambiguity and the highest percentage of tagged words. We have also explored a word sense disambiguation strategy based on the sense information provided by a multilingual corpus, following the idea of (Ide et al., 2002). We have used the DGTTM Corpus (Steinberger et al., 2012) in six languages: English, Spanish, French, German, Italian and Portuguese. We have sense tagged all the languages with no sense disambiguation, that is, giving all the possible senses to all the words in the corpus present in the WordNet versions for these languages. With all this sense information the Word Sense Disambiguation task consists of comparing the synsets in all languages for the same sentence, and taking the sense appearing the most times. Using this strategy some degree of ambiguity is still present after disambiguation. </context>
</contexts>
<marker>Ide, Erjavec, Tufis, 2002</marker>
<rawString>Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense discrimination with parallel corpora. In Proceedings of the ACL-02 workshop on Word sense disambiguation: recent successes and future directions-Volume 8, pages 61–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLTNAACL ’06.</booktitle>
<contexts>
<context position="6862" citStr="Liang et al., 2006" startWordPosition="1108" endWordPosition="1111"> English part. As this kind of corpus is not easily available we explored two strategies for the automatic construction of these corpora: (i) by machine translation of sense-tagged corpora (Oliver and Climent, 2011), (Oliver and Climent, 2012a) and (ii) by automatic sense tagging of bilingual corpora (Oliver and Climent, 2012b). Once we have created the parallel corpus, we need a word alignment algorithm in order to create the target WordNet. Fortunately, word alignment is a well-known task and several freely available algorithms are available. In previous works we have used Berkeley Aligner (Liang et al., 2006). In this paper we present the results using a very simple word alignment algorithm based on the most frequent translation. This algorithm is available in the WN-Toolkit. With the parallel corpus based strategy we can get the target language variants for synsyets having both monosemic and polisemic English variants, that is, English words assigned to one or more synsets. 2.3.1 Machine translation of sense-tagged corpora For the creation of the parallel corpus from a monolingual sense-tagged corpus, we use a machine translation system to get the target sentences. The machine translation system </context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In Proceedings of the HLTNAACL ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: building a very large multilingual semantic network.</title>
<date>2010</date>
<journal>ACM ID:</journal>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10,</booktitle>
<pages>216--225</pages>
<publisher>Association</publisher>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4339" citStr="Navigli and Ponzetto, 2010" startWordPosition="704" endWordPosition="707"> percentage of a target WordNet can be implemented using this strategy. We must bear in mind, however, that using this methodology, we would probably not be able to obtain the most frequent variants, as common words are usually polysemic. The Spanish WordNet (Atserias et al., 1997) in the EuroWordNet project and the Catalan WordNet (Benitez et al., 1998) were constructed using dictionaries. With the dictionary-based strategy we will only be able to get target language variants for synsets having monosemic English variants, i.e. English words assigned to a single synset. 2.2 Babelnet BabelNet (Navigli and Ponzetto, 2010) is a semantic network and ontology created by linking Wikipedia entries to WordNet synsets. These relations are multilingual through the interlingual relations in Wikipedia. For languages lacking the corresponding Wikipedia entry a statistical machine translation system is used to translate a set of English sentences containing the synset in the Semcor corpus and in sentences from Wikipedia containing a link to the English Wikipedia version. After that, the most frequent translation is detected and included as a variant for the synset in the given language. Similarly to WordNet, BabelNet grou</context>
</contexts>
<marker>Navigli, Ponzetto, 2010</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2010. BabelNet: building a very large multilingual semantic network. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ’10, pages 216–225, Stroudsburg, PA, USA. Association for Computational Linguistics. ACM ID: 1858704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoni Oliver</author>
<author>Salvador Climent</author>
</authors>
<title>Construcci´on de los wordnets 3.0 para castellano y catal´an mediante traducci´on autom´atica de corpus anotados sem´anticamente.</title>
<date>2011</date>
<booktitle>In Proceedings of the 27th Conference of the SEPLN,</booktitle>
<location>Huelva</location>
<contexts>
<context position="6458" citStr="Oliver and Climent, 2011" startWordPosition="1044" endWordPosition="1047"> the target language variants for synsyets having both monosemic and polisemic English variants, that is, English words assigned to one or more synsets. 2.3 Parallel corpus based strategies In some previous works we presented a methodology for the construction of WordNets based on the use of parallel bilingual corpora. These corpora need to be semantically tagged, the tags being PWN synsets, at least in the English part. As this kind of corpus is not easily available we explored two strategies for the automatic construction of these corpora: (i) by machine translation of sense-tagged corpora (Oliver and Climent, 2011), (Oliver and Climent, 2012a) and (ii) by automatic sense tagging of bilingual corpora (Oliver and Climent, 2012b). Once we have created the parallel corpus, we need a word alignment algorithm in order to create the target WordNet. Fortunately, word alignment is a well-known task and several freely available algorithms are available. In previous works we have used Berkeley Aligner (Liang et al., 2006). In this paper we present the results using a very simple word alignment algorithm based on the most frequent translation. This algorithm is available in the WN-Toolkit. With the parallel corpus </context>
</contexts>
<marker>Oliver, Climent, 2011</marker>
<rawString>Antoni Oliver and Salvador Climent. 2011. Construcci´on de los wordnets 3.0 para castellano y catal´an mediante traducci´on autom´atica de corpus anotados sem´anticamente. In Proceedings of the 27th Conference of the SEPLN, Huelva Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoni Oliver</author>
<author>Salvador Climent</author>
</authors>
<title>Building wordnets by machine translation of sense tagged corpora.</title>
<date>2012</date>
<booktitle>In Proceedings of the Global WordNet Conference,</booktitle>
<location>Matsue, Japan.</location>
<contexts>
<context position="6485" citStr="Oliver and Climent, 2012" startWordPosition="1048" endWordPosition="1051">s for synsyets having both monosemic and polisemic English variants, that is, English words assigned to one or more synsets. 2.3 Parallel corpus based strategies In some previous works we presented a methodology for the construction of WordNets based on the use of parallel bilingual corpora. These corpora need to be semantically tagged, the tags being PWN synsets, at least in the English part. As this kind of corpus is not easily available we explored two strategies for the automatic construction of these corpora: (i) by machine translation of sense-tagged corpora (Oliver and Climent, 2011), (Oliver and Climent, 2012a) and (ii) by automatic sense tagging of bilingual corpora (Oliver and Climent, 2012b). Once we have created the parallel corpus, we need a word alignment algorithm in order to create the target WordNet. Fortunately, word alignment is a well-known task and several freely available algorithms are available. In previous works we have used Berkeley Aligner (Liang et al., 2006). In this paper we present the results using a very simple word alignment algorithm based on the most frequent translation. This algorithm is available in the WN-Toolkit. With the parallel corpus based strategy we can get t</context>
</contexts>
<marker>Oliver, Climent, 2012</marker>
<rawString>Antoni Oliver and Salvador Climent. 2012a. Building wordnets by machine translation of sense tagged corpora. In Proceedings of the Global WordNet Conference, Matsue, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoni Oliver</author>
<author>Salvador Climent</author>
</authors>
<title>Parallel corpora for wordnet construction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th International Conference on Intelligent Text Processing and Computational Linguistics (Cicling</booktitle>
<location>New Delhi</location>
<contexts>
<context position="6485" citStr="Oliver and Climent, 2012" startWordPosition="1048" endWordPosition="1051">s for synsyets having both monosemic and polisemic English variants, that is, English words assigned to one or more synsets. 2.3 Parallel corpus based strategies In some previous works we presented a methodology for the construction of WordNets based on the use of parallel bilingual corpora. These corpora need to be semantically tagged, the tags being PWN synsets, at least in the English part. As this kind of corpus is not easily available we explored two strategies for the automatic construction of these corpora: (i) by machine translation of sense-tagged corpora (Oliver and Climent, 2011), (Oliver and Climent, 2012a) and (ii) by automatic sense tagging of bilingual corpora (Oliver and Climent, 2012b). Once we have created the parallel corpus, we need a word alignment algorithm in order to create the target WordNet. Fortunately, word alignment is a well-known task and several freely available algorithms are available. In previous works we have used Berkeley Aligner (Liang et al., 2006). In this paper we present the results using a very simple word alignment algorithm based on the most frequent translation. This algorithm is available in the WN-Toolkit. With the parallel corpus based strategy we can get t</context>
</contexts>
<marker>Oliver, Climent, 2012</marker>
<rawString>Antoni Oliver and Salvador Climent. 2012b. Parallel corpora for wordnet construction. In Proceedings of the 13th International Conference on Intelligent Text Processing and Computational Linguistics (Cicling 2012). New Delhi (India).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs Padr´o</author>
<author>Miquel Collado</author>
<author>Samuel Reese</author>
<author>Marina Lloberes</author>
<author>Irene Castell´on</author>
</authors>
<title>FreeLing 2.1: Five years of open-source language processing tools.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<volume>10</volume>
<pages>931--936</pages>
<marker>Padr´o, Collado, Reese, Lloberes, Castell´on, 2010</marker>
<rawString>Llu´ıs Padr´o, Miquel Collado, Samuel Reese, Marina Lloberes, and Irene Castell´on. 2010a. FreeLing 2.1: Five years of open-source language processing tools. In LREC, volume 10, pages 931–936.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Llu´ıs Padr´o</author>
<author>Samuel Reese</author>
</authors>
<title>Eneko Agirre, and Aitor Soroa. 2010b. Semantic services in freeling 2.1: Wordnet and UKB.</title>
<booktitle>In Proceedings of the 5th International Conference of the Global WordNet Association (GWC-2010).</booktitle>
<marker>Padr´o, Reese, </marker>
<rawString>Llu´ıs Padr´o, Samuel Reese, Eneko Agirre, and Aitor Soroa. 2010b. Semantic services in freeling 2.1: Wordnet and UKB. In Proceedings of the 5th International Conference of the Global WordNet Association (GWC-2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Christiane Fellbaum</author>
<author>Scott Cotton</author>
<author>Lauren Delfs</author>
<author>Hoa Trang Dang</author>
</authors>
<title>English tasks: All-words and verb lexical sample.</title>
<date>2001</date>
<booktitle>In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>21--24</pages>
<contexts>
<context position="8119" citStr="Palmer et al., 2001" startWordPosition="1313" endWordPosition="1316">lexical selection, that is, it should select the correct target words for the source English words. Other kinds of translation errors are less important for this strategy. 2.3.2 Automatic sense-tagging of parallel corpora The second strategy for the creation of the corpora is to use a parallel corpus between English and the target language and perform an automatic sense tagging of the English sentences. Unfortunately word sense disambiguation is a highly error-prone task. The best WSD systems for English using WordNet synsets achieve a precision score of about 60-65% (Snyder and Palmer, 2004; Palmer et al., 2001). In our experiments we have explored two options: (i) the use of Freeling and UKB (Padr´o et al., 2010b) and (ii) Word Sense Disambiguation of multilingual corpora based on the sense information of all the languages (Shahid and Kazakov, 2010). We have used Freeling (Padr´o et al., 2010a) and the integrated UKB module (Agirre and Soroa, 2009) to add sense tags to a fragment of the DGTTM corpus (Steinberger et al., 2012). Before using this algorithm we have evaluated its the precision by means of automatically sense tag some sense tagged corpora: Semcor, Semeval2, Semeval3 and the Princeton Wor</context>
</contexts>
<marker>Palmer, Fellbaum, Cotton, Delfs, Dang, 2001</marker>
<rawString>Martha Palmer, Christiane Fellbaum, Scott Cotton, Lauren Delfs, and Hoa Trang Dang. 2001. English tasks: All-words and verb lexical sample. In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems, pages 21–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emanuele Pianta</author>
<author>Luisa Bentivogli</author>
<author>Christian Girardi</author>
</authors>
<title>Developing an aligned multilingual database.</title>
<date>2002</date>
<booktitle>In Proc. 1st Int’l Conference on Global WordNet.</booktitle>
<contexts>
<context position="1905" citStr="Pianta et al., 2002" startWordPosition="290" endWordPosition="293">ilable for download at the time of performing the experiments. WordNet versions in other languages are also available. On the Global WordNet Association1 website, a comprehensive list of WordNets available for different languages can be found. The Open Multilingual WordNet project (Bond and Kyonghee, 2012) provides free access to WordNets in several languages in a common format. We have used the WordNets from this project for 1www.globalwordnet.org Catalan (Gonzalez-Agirre et al., 2012) , Spanish (Gonzalez-Agirre et al., 2012) , French (WOLF) (Sagot and Fiˇser, 2008) , Italian (Multiwordnet) (Pianta et al., 2002) and Portuguese (OpenWNPT) (de Paiva and Rademaker, 2012) . For German we have used the GermaNet 7.0 (Hamp and Feldweg, 1997), freely available for research. In Table 1, the sizes of all these WordNets are presented along with the size of the PWN. Synsets Words English 118.695 206.979 Catalan 45.826 46.531 Spanish 38.512 36.681 French 59.091 55.373 Italian 34.728 40.343 Portuguese 41.810 52.220 German 74.612 99.529 Table 1: Size of the WordNets 2 The expand model According to (Vossen, 1998), we can distinguish two general methodologies for WordNet construction: (i) the merge model, where a new</context>
</contexts>
<marker>Pianta, Bentivogli, Girardi, 2002</marker>
<rawString>Emanuele Pianta, Luisa Bentivogli, and Christian Girardi. 2002. Developing an aligned multilingual database. In Proc. 1st Int’l Conference on Global WordNet.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
<author>Darja Fiˇser</author>
</authors>
<title>Building a free french wordnet from multilingual resources.</title>
<date>2008</date>
<booktitle>In Proceedings of OntoLex</booktitle>
<location>Marrackech</location>
<marker>Sagot, Fiˇser, 2008</marker>
<rawString>Benoit Sagot and Darja Fiˇser. 2008. Building a free french wordnet from multilingual resources. In Proceedings of OntoLex 2008, Marrackech (Morocco).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ahmad R Shahid</author>
<author>Dimitar Kazakov</author>
</authors>
<title>Retrieving lexical semantics from multilingual corpora.</title>
<date>2010</date>
<journal>Polibits,</journal>
<pages>41--25</pages>
<contexts>
<context position="8362" citStr="Shahid and Kazakov, 2010" startWordPosition="1353" endWordPosition="1356">egy for the creation of the corpora is to use a parallel corpus between English and the target language and perform an automatic sense tagging of the English sentences. Unfortunately word sense disambiguation is a highly error-prone task. The best WSD systems for English using WordNet synsets achieve a precision score of about 60-65% (Snyder and Palmer, 2004; Palmer et al., 2001). In our experiments we have explored two options: (i) the use of Freeling and UKB (Padr´o et al., 2010b) and (ii) Word Sense Disambiguation of multilingual corpora based on the sense information of all the languages (Shahid and Kazakov, 2010). We have used Freeling (Padr´o et al., 2010a) and the integrated UKB module (Agirre and Soroa, 2009) to add sense tags to a fragment of the DGTTM corpus (Steinberger et al., 2012). Before using this algorithm we have evaluated its the precision by means of automatically sense tag some sense tagged corpora: Semcor, Semeval2, Semeval3 and the Princeton WordNet Gloss Corpus (PWGC). After the automatic sense-tagging is performed, the tags are compared with those in the manually sense tagged-version. In Table 4 we can see the precision figure for each corpus and pos. As we can see, there is a grea</context>
</contexts>
<marker>Shahid, Kazakov, 2010</marker>
<rawString>Ahmad R. Shahid and Dimitar Kazakov. 2010. Retrieving lexical semantics from multilingual corpora. Polibits, 41:25–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The English all-words task.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3),</booktitle>
<pages>41--43</pages>
<location>Barcelona</location>
<contexts>
<context position="8097" citStr="Snyder and Palmer, 2004" startWordPosition="1309" endWordPosition="1312">ble of performing a good lexical selection, that is, it should select the correct target words for the source English words. Other kinds of translation errors are less important for this strategy. 2.3.2 Automatic sense-tagging of parallel corpora The second strategy for the creation of the corpora is to use a parallel corpus between English and the target language and perform an automatic sense tagging of the English sentences. Unfortunately word sense disambiguation is a highly error-prone task. The best WSD systems for English using WordNet synsets achieve a precision score of about 60-65% (Snyder and Palmer, 2004; Palmer et al., 2001). In our experiments we have explored two options: (i) the use of Freeling and UKB (Padr´o et al., 2010b) and (ii) Word Sense Disambiguation of multilingual corpora based on the sense information of all the languages (Shahid and Kazakov, 2010). We have used Freeling (Padr´o et al., 2010a) and the integrated UKB module (Agirre and Soroa, 2009) to add sense tags to a fragment of the DGTTM corpus (Steinberger et al., 2012). Before using this algorithm we have evaluated its the precision by means of automatically sense tag some sense tagged corpora: Semcor, Semeval2, Semeval3</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The English all-words task. In Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (Senseval-3), pages 41–43, Barcelona (Spain).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Andreas Eisele</author>
<author>Szymon Klocek</author>
<author>Spyridon Pilos</author>
<author>Patrick Schl¨uter</author>
</authors>
<title>Dgttm: A freely available translation memory in 22 languages.</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation,</booktitle>
<pages>454--459</pages>
<marker>Steinberger, Eisele, Klocek, Pilos, Schl¨uter, 2012</marker>
<rawString>Ralf Steinberger, Andreas Eisele, Szymon Klocek, Spyridon Pilos, and Patrick Schl¨uter. 2012. Dgttm: A freely available translation memory in 22 languages. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 454–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Parallel data, tools and interfaces in OPUS. In</title>
<date>2012</date>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’2012),</booktitle>
<pages>2214--2218</pages>
<contexts>
<context position="27422" citStr="Tiedemann, 2012" startWordPosition="4498" endWordPosition="4500">and we are now getting 313 candidates for the multilingual WSD strategy and 1,155 for the UKB WSD. If we force the extraction process to get 2,076 candidates, we obtain a precision value of 43.77% for the multilingual WSD strategy and 58.12% for UKB. 4 Resources We are distributing some resources for several languages with the hope they can be useful to use the toolkit to create new WordNets or extend existing ones. • Lexical resources: dictionaries created from Wiktionary, Wikipedia and Apertium transfer dictionaries. • Preprocessed corpora: DGT-TM, Emea and United Nations Corpus from Opus5 (Tiedemann, 2012). We have semantically-tagged the English part of the corpora with Freeling and UKB and lemmatized and tagged some of the target languages. We plan to preprocess other parallel corpora in the future. 5 Conclusions We have presented the results of the automatic creation of WordNets for six languages using several techniques following the expand model. All these techniques are implemented in the freely available WN-Toolkit and have been successfully used for the expansion of the Catalan and Spanish WordNets under the Know2 project. The WordNets and the toolkit itself are being improved under the</context>
</contexts>
<marker>Tiedemann, 2012</marker>
<rawString>J¨org Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC’2012), pages 2214–2218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Piek Vossen</author>
</authors>
<date>1998</date>
<booktitle>Introduction to Eurowordnet. Computers and the Humanities,</booktitle>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="2400" citStr="Vossen, 1998" startWordPosition="375" endWordPosition="376">(Gonzalez-Agirre et al., 2012) , French (WOLF) (Sagot and Fiˇser, 2008) , Italian (Multiwordnet) (Pianta et al., 2002) and Portuguese (OpenWNPT) (de Paiva and Rademaker, 2012) . For German we have used the GermaNet 7.0 (Hamp and Feldweg, 1997), freely available for research. In Table 1, the sizes of all these WordNets are presented along with the size of the PWN. Synsets Words English 118.695 206.979 Catalan 45.826 46.531 Spanish 38.512 36.681 French 59.091 55.373 Italian 34.728 40.343 Portuguese 41.810 52.220 German 74.612 99.529 Table 1: Size of the WordNets 2 The expand model According to (Vossen, 1998), we can distinguish two general methodologies for WordNet construction: (i) the merge model, where a new ontology is constructed for the target language; and (ii) the expand model, where variants associated with PWN synsets are translated using different strategies. 2.1 Dictionary-based strategies The most commonly used strategy within the expand model is the use of bilingual dictionaries. The main difficulty faced is polysemy. If all the variants were monosemic, i.e., if they were assigned to a single synset, the problem would be simple, as we would only need to find one or more translations</context>
</contexts>
<marker>Vossen, 1998</marker>
<rawString>Piek Vossen. 1998. Introduction to Eurowordnet. Computers and the Humanities, 32(2):73–89.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
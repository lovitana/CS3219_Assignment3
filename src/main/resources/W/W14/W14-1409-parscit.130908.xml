<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996955">
A Probabilistic Rich Type Theory for Semantic Interpretation
</title>
<author confidence="0.999695">
Robin Cooper&apos;, Simon Dobnik&apos;, Shalom Lapping, and Staffan Larsson&apos;
</author>
<affiliation confidence="0.998712">
&apos;University of Gothenburg, gKing’s College London
</affiliation>
<email confidence="0.962801">
{cooper,sl}@ling.gu.se, simon.dobnik@gu.se, shalom.lappin@kcl.ac.uk
</email>
<sectionHeader confidence="0.997309" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990225">
We propose a probabilistic type theory in which a
situation s is judged to be of a type T with probabil-
ity p. In addition to basic and functional types it in-
cludes, inter alia, record types and a notion of typ-
ing based on them. The type system is intensional
in that types of situations are not reduced to sets
of situations. We specify the fragment of a com-
positional semantics in which truth conditions are
replaced by probability conditions. The type sys-
tem is the interface between classifying situations
in perception and computing the semantic interpre-
tations of phrases in natural language.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999969238805971">
Classical semantic theories (Montague, 1974), as
well as dynamic (Kamp and Reyle, 1993) and un-
derspecified (Fox and Lappin, 2010) frameworks
use categorical type systems. A type T identifies
a set of possible denotations for expressions in T,
and the system specifies combinatorial operations
for deriving the denotation of an expression from
the values of its constituents.
These theories cannot represent the gradience
of semantic properties that is pervasive in speak-
ers’ judgements concerning truth, predication, and
meaning relations. In general, predicates do not
have determinate extensions (or intensions), and
so, in many cases, speakers do not make categor-
ical judgements about the interpretation of an ex-
pression. Attributing gradience effects to perfor-
mance mechanisms offers no help, unless one can
show precisely how these mechanisms produce the
observed effects.
Moreover, there is a fair amount of evidence in-
dicating that language acquisition in general cru-
cially relies on probabilistic learning (Clark and
Lappin, 2011). It is not clear how a reasonable
account of semantic learning could be constructed
on the basis of the categorical type systems that ei-
ther classical or revised semantic theories assume.
Such systems do not appear to be efficiently learn-
able from the primary linguistic data (with weak
learning biases), nor is there much psychological
data to suggest that they provide biologically de-
termined constraints on semantic learning.
A semantic theory that assigns probability
rather than truth conditions to sentences is in a
better position to deal with both of these issues.
Gradience is intrinsic to the theory by virtue of
the fact that speakers assign values to declarative
sentences in the continuum of real numbers [0,1],
rather than Boolean values in {0,1}. In addition,
a probabilistic account of semantic learning is fa-
cilitated if the target of learning is a probabilistic
representation of meaning. Both semantic repre-
sentation and learning are instances of reasoning
under uncertainty.
Probability theorists working in AI often de-
scribe probability judgements as involving distri-
butions over worlds. In fact, they tend to limit
such judgements to a restricted set of outcomes
or events, each of which corresponds to a par-
tial world which is, effectively, a type of situa-
tion (Halpern, 2003). A classic example of the re-
duction of worlds to situation types in probability
theory is the estimation of the likelihood of heads
vs tails in a series of coin tosses. Here the world
is held constant except along the dimension of a
binary choice between a particular set of possi-
ble outcomes. A slightly more complex case is
the probability distribution for possible results of
throwing a single die, which allows for six pos-
sibilities corresponding to each of its numbered
faces. This restricted range of outcomes consti-
tutes the sample space.
We are making explicit the assumption, com-
mon to most probability theories used in AI, with
clearly defined sample spaces, that probability
is distributed over situation types (Barwise and
Perry, 1983), rather than over sets of entire worlds.
An Austinian proposition is a judgement that a
</bodyText>
<page confidence="0.981004">
72
</page>
<note confidence="0.9917505">
Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 72–79,
Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999943761904762">
situation is of a particular type, and we treat it
as probabilistic. In fact, it expresses a subjec-
tive probability in that it encodes the belief of an
agent concerning the likelihood that a situation is
of that type. The core of an Austinian proposi-
tion is a type judgement of the form s : T, which
states that a situation s is of type T. On our ac-
count this judgement is expressed probabilistically
as p(s : T) = r, where r E [0,1].1
On the probabilistic type system that we pro-
pose situation types are intensional objects over
which probability distributions are specified. This
allows us to reason about the likelihood of alter-
native states of affairs without invoking possible
worlds.
Complete worlds are not tractably repre-
sentable. Assume that worlds are maximal con-
sistent sets of propositions (Carnap, 1947). If
the logic of propositions is higher-order, then the
problem of determining membership in such a set
is not complete. If the logic is classically first-
order, then the membership problem is complete,
but undecidable.
Alternatively, we could limit ourselves to
propositional logic, and try to generate a maxi-
mally consistent set of propositions from a single
finite proposition P in Conjunctive Normal Form
(CNF, a conjunction of disjunctions), by simply
adding conjuncts to P. But it is not clear what
(finite) set of rules or procedures we could use to
decide which propositions to add in order to gen-
erate a full description of a world in a systematic
way. Nor is it obvious at what point the conjunc-
tion will constitute a complete description of the
world.
Moreover, all the propositions that P entails
must be added to it, and all the propositions with
which P is inconsistent must be excluded, in or-
der to obtain the maximal consistent set of propo-
sitions that describe a world. But then testing the
satisfiability of P is an instance of the ksat prob-
lem, which, in the general case, is NP-complete.2
</bodyText>
<footnote confidence="0.8878835">
1Beltagy et al. (2013) propose an approach on which clas-
sical logic-based representations are combined with distribu-
tional lexical semantics and a probabilistic Markov logic, in
order to select among the set of possible inferences from a
sentence. Our concern here is more foundational. We seek to
replace classical semantic representations with a rich proba-
bilistic type theory as the basis of both lexical and composi-
tional interpretation.
2The ksat problem is to determine whether a formula in
propositional logic has a satisfying set of truth-value assign-
ments. For the complexity results of different types of ksat
problem see Papadimitriou (1995).
</footnote>
<bodyText confidence="0.999761333333333">
By contrast situation types can be as large or as
small as we need them to be. They are not max-
imal in the way that worlds are, and so the issue
of completeness of specification does not arise.
Therefore, they can, in principle, be tractably rep-
resented.
</bodyText>
<sectionHeader confidence="0.877646" genericHeader="method">
2 Rich Type Theory and Probability
</sectionHeader>
<bodyText confidence="0.96464985">
Central to standard formulations of rich type the-
ories (for example, (Martin-L¨of, 1984)) is the no-
tion of a judgement a : T, that object a is of type
T. We represent the probability of this judgement
as p(a : T). Our system (based on Cooper (2012))
includes the following types.
Basic Types are not constructed out of other ob-
jects introduced in the theory. If T is a basic type,
p(a : T) for any object a is provided by a probabil-
ity model, an assignment of probabilities to judge-
ments involving basic types.
PTypes are constructed from a predicate and
an appropriate sequence of arguments. An exam-
ple is the predicate ‘man’ with arity (Ind, Time)
where the types Ind and Time are the basic type
of individuals and of time points respectively.
Thus man(john,18:10) is the type of situation (or
eventuality) where John is a man at time 18:10.
A probability model provides probabilities p(e :
r(a1, ... , an)) for ptypes r(a1, ... , an). We take
both common nouns and verbs to provide the com-
ponents out of which PTypes are constructed.
Meets and Joins give, for T1 and T2, the meet,
T1 n T2 and the join T1 V T2, respectively. a :
T1 n T2 just in case a : T1 and a : T2. a : T1 V
T2 just in case either a : T1 or a : T2 (possibly
both).3 The probabilities for meet and joint types
are defined by the classical (Kolmogorov, 1950)
equations p(a : T1 n T2) = p(a : T1)p(a : T2  |a : T1)
(equivalently, p(a : T1 n T2) = p(a : T1, a : T2)), and
p(a : T1 V T2) = p(a : T1) + p(a : T2) − p(a : T1 n T2),
respectively.
Subtypes A type T1 is a subtype of type T2,
T1 C T2, just in case a : T1 implies a : T2 no mat-
ter what we assign to the basic types. If T1 C T2
then a : T1nT2 iff a : T1 and a : T1VT2 iff a : T2.
Similarly, if T2 C T1 then a : T1 n T2 iff a : T2
and a : T1 V T2 iff a : T1.
If T2 C T1, then p(a : T1 n T2) = p(a : T2),
and p(a : T1 V T2) = p(a : T1). If T1 C T2,
</bodyText>
<footnote confidence="0.990786">
3This use of intersection and union types is not standard in
rich type theories, where product and disjoint union are pre-
ferred following the Curry-Howard correspondence for con-
junction and disjunction.
</footnote>
<page confidence="0.995363">
73
</page>
<bodyText confidence="0.4795965">
then p(a : T1) G p(a : T2). These definitions
also entail that p(a : T1 n T2) G p(a : T1), and
</bodyText>
<equation confidence="0.856984">
p(a : T1) G p(a : T1 V T2). —
</equation>
<bodyText confidence="0.99975375">
We generalize probabilistic meet and join types
to probabilities for unbounded conjunctive and
disjunctive type judgements, again using the clas-
sical equations.
</bodyText>
<equation confidence="0.926374">
Let ^ p (a0 : T0, ... , an : Tn) be the conjunctive
probability of judgements a0 : T0, ... , a,,, : T,,,.
^Then ^ p (a0 : T0, ... , an : Tn) = p (a0 : T0, ..., an_1 :
Tn_1)p(an : Tn  |a0 : T0,..., an_1 : Tn_1). If n = 0,
^ p(a0 : T0, ... , an : Tn) = 1.
</equation>
<bodyText confidence="0.874093833333333">
We interpret universal quantification as an un-
bounded conjunctive probability, which is true if
it is vacuously satisfied (n = 0) (Paris, 2010).
Let _p (a0 : T0, a1 : T1, ...) be the disjunctive
probability of judgements a0 : T0, a1 : T1,....
It is computed by _p (a0 : T0, ... , an : Tn) =
</bodyText>
<equation confidence="0.9102325">
_p (a0 : T0,..., an_1 : Tn_1) + p(an : Tn) − ^(a0
p :
T0, ... , an_1 : Tn_1)p(an : Tn  |a0 : T0, ... , an_1 :
Tn_1). If n = 0,_p (a0 : T0,..., an : Tn) = 0.
</equation>
<bodyText confidence="0.86806925">
We take existential quantification to be an un-
bounded disjunctive probability, which is false if it
lacks a single non-nil probability instance (n = 0).
Conditional Conjunctive Probabilities are
</bodyText>
<equation confidence="0.886909666666667">
computed by ^ (a0 : T0, ... , an : Tn  |a : T) =
p
^p (a0 : T0,..., an_1 : Tn_1  |a : T)p(an : Tn |
a0 : T0,..., an_1 : Tn_1, a : T)). If n = 0, ^(a0
p :
T0,..., an : Tn  |a : T) = 1.
</equation>
<bodyText confidence="0.999772">
Function Types give, for any types T1 and T2,
the type (T1 —* T2). This is the type of total func-
tions with domain the set of all objects of type
T1 and range included in objects of type T2. The
probability that a function f is of type (T1 —* T2)
is the probability that everything in its domain is of
type T1 and that everything in its range is of type
T2, and furthermore that everything not in its do-
main which has some probability of being of type
T1 is not in fact of type T1. p(f : (T1 --+ T2)) =
</bodyText>
<equation confidence="0.9963285">
^ _ p (a : T1))
(a : T1, f(a) : T2)(1 −
p
aEdom(f) a0dom(f)
</equation>
<bodyText confidence="0.992011254545455">
Suppose that T1 is the type of event where there
is a flash of lightning and T2 is the type of event
where there is a clap of thunder. Suppose that f
maps lightning events to thunder events, and that
it has as its domain all events which have been
judged to have probability greater than 0 of being
lightning events. Let us consider that all the puta-
tive lightning events were clear examples of light-
ning (i.e. judged with probability 1 to be of type
T1) and are furthermore associated by f with clear
events of thunder (i.e. judged with probability 1 to
be of type T2). Suppose there were four such pairs
of events. Then the probability of f being of type
(T1 —* T2) is (1 x 1)4, that is, 1.
Suppose, alternatively, that for one of the four
events f associates the lightning event with a silent
event, that is, one whose probability of being of
T2 is 0. Then the probability of f being of type
(T1 —* T2) is (1 x 1)3 x (1 x 0) = 0. One clear
counterexample is sufficient to show that the func-
tion is definitely not of the type.
In cases where the probabilities of the an-
tecedent and the consequent type judgements are
higher than 0, the probability of the entire judge-
ment on the existence of a functional type f will
decline in proportion to the size of dom(f). As-
sume, for example that there are k elements a E
dom(f), where for each such a p(a : T1) =
p(f(a) : T2) &gt; .5. Every az that is added to
dom(f) will reduce the value of p(f : (T1 —*
T2)), even if it yields higher values for p(a : T1)
and p(f(a) : T2). This is due to the fact that we
are treating the probability of p(f : (T1 —* T2))
as the likelihood of there being a function that is
satisfied by all objects in its domain. The larger
the domain, the less probable that all elements in
it fulfill the functional relation.
We are, then, interpreting a functional type
judgement of this kind as a universally quantified
assertion over the pairing of objects in dom(f)
and range(f). The probability of such an asser-
tion is given by the conjunction of assertions cor-
responding to the co-occurrence of each element a
in f’s domain as an instance of T1 with f(a) as an
instance of T2. This probability is the product of
the probabilities of these individual assertions.
This seems reasonable, but it only deals with
functions whose domain is all objects which have
been judged to have some probability, however
low, of being of type T1. Intuitively, functions
which leave out some of the objects with lower
likelihood of being of type T1 should also have a
probability of being of type (T1 —* T2). This fac-
tor in the probability is represented by the second
element of the product in the formula.
</bodyText>
<page confidence="0.991938">
74
</page>
<bodyText confidence="0.996503172413793">
Negation ¬T, of type T, is the function type
(T → ⊥), where ⊥ is a necessarily empty type
and p(⊥) = 0. It follows from our rules for func-
tion types that p(f : ¬T) = 1 if dom(f) = ∅, that
is T is empty, and 0 otherwise.
We also assign probabilities to judgements con-
cerning the (non-)emptiness of a type, p(T). we
pass over the details of how we compute the prob-
abilities of such judgements, but we note that our
account of negation entails that p(T ∨ ¬T) = 1,
and (ii) p(¬¬T) = p(T). Therefore, we sustain
classical Boolean negation and disjunction, in con-
trast to Martin-L¨of’s (1984) intuitionistic type the-
ory.
Dependent Types are functions from objects to
types. Given appropriate arguments as functions
they will return a type. Therefore, the account of
probabilities associated with functions above ap-
plies to dependent types.
Record Types A record in a type system asso-
ciated with a set of labels is a set of ordered pairs
(fields) whose first member is a label and whose
second member is an object of some type (possibly
a record). Records are required to be functional on
labels (each label in a record can only occur once
in the record’s left projection).
A dependent record type is a set of fields (or-
dered pairs) consisting of a label E followed by T
as above. The set of record types is defined by:
</bodyText>
<listItem confidence="0.984741666666667">
1. [], that is the empty set or Rec, is a record type. r : Rec
just in case r is a record.
2. If T1 is a record type, f is a label not occurring in T1,
and T2 is a type, then T1 U {(f, T2)} is a record type.
r : T1 U {(f, T2)} just in case r : T1, r.f is defined (f
occurs as a label in r) and r.f : T2.
3. If T is a record type, f is a label not occuring in
T, T is a dependent type requiring n arguments, and
(Ir1, ... , Irn) is an n-place sequence of paths in T,4
then T U {(f, (T , (Ir1, ... , Irn)))} is a record type.
r : T U {(f, (T , (Ir1, ... , Irn)))} just in case r : T,
r.f is defined and r.f : T (r.Ir1, ... , r.Irn).
</listItem>
<bodyText confidence="0.887892">
The probability that an object r is of a record
type T is given by the following clauses:
</bodyText>
<listItem confidence="0.8844155">
1. p(r : Rec) = 1 if r is a record, 0 otherwise
^
2. p(r : T1 U {(f, T2)}) = p (r : T1, r.f : T2)
3. If// T : (T1 --+ (... --+ (Tn --+ Type) ...)), then
plr : T U { (f, (T , (Ir1, ... , Irn)))}) = ^ p (r : T, r.f :
T (r.Ir1, ... , r.Irn) I r.Ir1 : T1, ... , r.Irn : Tn)
</listItem>
<footnote confidence="0.974837333333333">
4In the full version of TTR we also allow absolute paths
which point to particular records, but we will not include
them here.
</footnote>
<sectionHeader confidence="0.989857" genericHeader="method">
3 Compositional Semantics
</sectionHeader>
<bodyText confidence="0.999259820512821">
Montague (1974) determines the denotation of a
complex expression by applying a function to an
intensional argument (as in Q NP ](Q ∧VP ])). We
employ a variant of this general strategy by ap-
plying a probabilistic evaluation function Q · ]p to
a categorical (non-probabilistic) semantic value.
For semantic categories that are interpreted as
functions, Q · ]p yields functions from categorical
values to probabilities. For sentences it produces
probability values.
The probabilistic evaluation function Q · ]p pro-
duces a probabilistic interpretation based on a
classical compositional semantics. For sentences
it will return the probability that the sentence is
true. For categories that are interpreted as func-
tions it will return functions from (categorical) in-
terpretations to probabilities. We are not propos-
ing strict compositionality in terms of probabili-
ties. Probabilities are like truth-values (or rather,
truth-values are the limit cases of probabilities).
We would not expect to be able to compute the
probability associated with a complex constituent
on the basis of the probabilities associated with its
immediate constituents, any more than we would
expect to be able to compute a categorical inter-
pretation entirely in terms of truth-functions and
extensions. However, the simultaneous computa-
tion of categorical and probabilistic interpretations
provides us with a compositional semantic system
that is closely related to the simultaneous com-
putation of intensions and extensions in classical
Montague semantics.
The following definition of Q · ]p for a fragment
of English is specified on the basis of our proba-
bilistic type system and a non-probabilistic inter-
pretation function Q · ], which we do not give in
this version of the paper. (It’s definition is given
by removing the probability p from the definition
below.)
</bodyText>
<equation confidence="0.9135747">
~e1:[ S1 ] �
[ [S S1 and S2] ]p = p( )
e2:[ S2 ]
[ [S S1 or S2] ]p =p(�e:[ S1 ]�[ S2 ]�)
[ [S Neg S] ]p = [ Neg ]p([ S ])
[ [S NP VP] ]p = [ NP ]p([ VP ])
[ [NP Det N] ]p = [ Det ]p([ N ])
[ [NP Nprop] ]p = [ Nprop ]p
[ [VP Vt NP] ]p = [ Vt ]p([ NP ])
[ [VP Vi] ]p = [ Vi ]p
</equation>
<listItem confidence="0.491063">
[ [Neg “it’s not true that”] ]p = AT:RecType(p([e:-T]))
[ [Det “some”] ]p = AQ:Ppty(AP:Ppty(p([e:some(Q, P)])))
[ [Det “every”] ]p = AQ:Ppty(AP:Ppty(p([e:every(Q, P)~)))
[ [Det “most”] ]p = AQ:Ppty(AP:Ppty(p([e:most(Q, P)])))
</listItem>
<page confidence="0.763887">
75
</page>
<table confidence="0.941382411764706">
�
�
[ [N “boy”] ]p = Ar:[x:Ind](p([e:boy(r.x)]))
[
[N “girl”] ]p = Ar:[x:Ind](p([e:girl(r.x)]))
[ [Adj “green”] ]p =
AP:Ppty(Ar:[x:Ind](p(([e:green(r.x,P)])))))
[ [Adj “imaginary”] ]p =
AP:Ppty(Ar:[x:Ind](p(([e:imaginary(r.x,P)])))))5
[ [Nprop “Kim”] ]p = AP:Ppty(p(P([x=kim])))
[ [Nprop “Sandy”] ]p = AP:Ppty(p(P([x=sandy])))
[ [Vt “knows”] ]p =
AP:Quant(Ar1:[x:Ind](p(P(Ar2:([e:know(r1.x,r2.x)])))))
[ [Vt “sees”] ]p =
AP:Quant(Ar1:[x:Ind](p(P(Ar2:([e:see(r1.x,r2.x)])))))
[ [Vz “smiles”] ]p = Ar:[x:Ind](p([e:smile(r.x)]))
[ [Vz “laughs”] ]p = Ar:[x:Ind](p([e:laugh(r.x)]))
</table>
<bodyText confidence="0.958475">
A probability distribution d for this fragment,
based on a set of situations S, is such that:
</bodyText>
<equation confidence="0.998583888888889">
pd(a : Ind) = 1 if a is kim or sandy6
pd(s : T) E [0, 1] if s E S and T is a ptype
pd(s : T) = 0 if s E� S and T is a ptype7
pd(a : [TP]) = pd(P([x=a]))
pd(some(P, Q)) = pd([TP] n [TQ])
pd(every(P, Q)) = pd([T P] — [T Q])
mos
pd(t(P Q)) = min/1, pd([T P]∧[T Q] )
(0most pd([T P])
</equation>
<bodyText confidence="0.999038888888889">
The probability that an event e is of the type in
which the relation some holds of the properties P
and Q is the probability that e is of the conjunctive
type P ∧ Q. The probability that e is of the every
type for P and Q is the likelihood that it instanti-
ates the functional type P —* Q. As we have de-
fined the probabilities associated with functional
types in terms of universal quantification (an un-
bounded conjunction of the pairings between the
elements of the domain P of the function and its
range Q), this definition sustains the desired read-
ing of every. The likelihood that e is of the type
most for P and Q is the likelihood that e is of
type P ∧ Q, factored by the product of the contex-
tually determined parameter Bmost and the likeli-
hood that e is of type P, where this fraction is less
than 1, and 1 otherwise.
Consider a simple example.
</bodyText>
<equation confidence="0.9781652">
[
[S [NP [Nprop Kim]] [VP [Vz smiles]]] ]p =
AP:Ppty(p(P([x=kim])))(Ar:[x:Ind]([e:smile(r.x)])) =
p(Ar:[x:Ind]([e:smile(r.x)])([x=kim])) =
p([e:smile(kim)])
</equation>
<bodyText confidence="0.572141">
5Notice that we characterize adjectival modifiers as rela-
tions between records of individuals and properties. We can
then invoke subtyping to capture the distinction between in-
tersective and non-intersective modifier relations.
</bodyText>
<footnote confidence="0.977145">
6This seems an intuitive assumption, though not a neces-
sary one.
7Again this seems an intuitive, though not a necessary as-
sumption.
</footnote>
<bodyText confidence="0.96837775">
Suppose that pd(s1:smile(kim)) = .7,
pd(s2:smile(kim)) = .3, pd(s3:smile(kim)) =
.4, and there are no other situations si such
that pd(si:smile(kim)) &gt; 0. Furthermore, let
us assume that these probabilities are indepen-
dent of each other, that is, pd(s3:smile(kim)) =
pd(s3:smile(kim)  |s1:smile(kim), s2:smile(kim))
and so on. Then
</bodyText>
<equation confidence="0.9850535">
pd(smile(kim))=
/ d(s1 : smile(kim), s2 : smile(kim), s3 : smile(kim)) =
/ d(s1 : smile(kim), s2 : smile(kim)) + .4 − .4YP/ d(s1 :
smile(kim), s2 : smile(kim)) =
(.7 + .3 − .7 x .3) + .4 − .4(.7 + .3 − .7 x .3) =
.874
</equation>
<bodyText confidence="0.995279555555556">
This means that pd([e:smile(kim)]) = .874.
Hence [ [S [NP [Nprop Kim]] [VP [Vz smiles]]] ]pd = .874
(where Q α ]pd is the result of computing Q α ]p
with respect to the probability distribution d).
Just as for categorical semantics, we can con-
struct type theoretic objects corresponding to
probabilistic judgements. We call these proba-
bilistic Austinian propositions. These are records
of type
</bodyText>
<equation confidence="0.991252">
sit : Sit
sit-type : Type
prob : [0,1]
</equation>
<bodyText confidence="0.999982625">
where [0,1] is used to represent the type of real
numbers between 0 and 1. They assert that the
probability that a situation s is of type Type is the
value of prob.
The definition of Q ]p specifies a compositional
procedure for generating an Austinian proposition
(record) of this type from the meanings of the syn-
tactic constituents of a sentence.
</bodyText>
<sectionHeader confidence="0.960091" genericHeader="method">
4 An Outline of Semantic Learning
</sectionHeader>
<bodyText confidence="0.990201214285714">
We outline a schematic theory of semantic learn-
ing on which agents acquire classifiers that form
the basis for our probabilistic type system. For
simplicity and ease of presentation we take these
to be Naive Bayes classifiers, which an agent ac-
quires from observation. In future developments
of this theory we will seek to extend the approach
to Bayesian networks (Pearl, 1990).
We assume that agents keep records of observed
situations and their types, modelled as probabilis-
tic Austinian propositions. For example, an obser-
vation of a man running might yield the following
Austinian proposition for some a:Ind, s1:man(a),
s2:run(a):
</bodyText>
<equation confidence="0.397448">
1
</equation>
<page confidence="0.92254">
76
</page>
<bodyText confidence="0.9998034">
An agent, A, makes judgements based on a
finite string of probabilistic Austinian proposi-
tions, J, corresponding to prior judgements held
in memory. For a type, T, JT represents that set of
Austinian propositions j such that j.sit-type C T.
If T is a type and J a finite string of probabilis-
tic Austinian propositions, then  ||T ||J represents
the sum of all probabilities associated with T in J
(PjEJT j.prob). P(J) is the sum of all probabilities
in J (PjEJ j.prob).
We use priorJ(T) to represent the prior proba-
bility that anything is of type T given J, that is
||T||J P(J) if P(J) &gt; 0, and 0 otherwise.
pA,J(s : T) denotes the probability that agent A
assigns with respect to prior judgements J to s be-
ing of type T. Similarly, pA,J(s : T1  |s : T2) is
the probability that agent A assigns with respect
to prior judgements J to s being of type T1, given
that A judges s to be of type T2.
When an agent A encounters a new situation
s and considers whether it is of type T, he/she
uses probabilistic reasoning to determine the value
of pA,J(s : T). A uses conditional probabilities
to calculate this value, where A computes these
conditional probabilities with the equation pA,J(s :
</bodyText>
<equation confidence="0.989567666666667">
T1  |s : T2) = ||T1∧T2||J if  ||T2 ||J# 0. Otherwise,
 ||T2||J
pA,J(s : T1  |s : T2) = 0.
</equation>
<bodyText confidence="0.899955454545455">
This is our type theoretic variant of the stan-
dard Bayesian formula for conditional probabili-
ties: p(A  |B) = |A&amp;B|
|B |. But instead of counting
categorical instances, we sum the probabilities of
judgements. This is because our “training data” is
not limited to categorical observations. Instead it
consists of probabilistic observational judgements
that situations are of particular types.8
Assume that we have the following types:
� ref : Ind
</bodyText>
<equation confidence="0.965289333333333">
~Tman = cman : man(ref) �
ref : Ind
crun : run(ref)
</equation>
<bodyText confidence="0.988251047619048">
8As a reviewer observes, by using an observer’s previous
judgements for the probability of an event being of a partic-
ular type, as the prior for the rule that computes the proba-
bility of a new event being of that type, we have, in effect,
compressed information that properly belongs in a Bayesian
network into our specification of a naive Bayesian classifier.
This is a simplification that we adopt here for ease of expo-
sition. In future work, we will characterise classifier learning
through full Bayesian networks.
Assume also that JTman∧Trun has three members,
corresponding to judgements by A that a man was
running in three observed situations s1, s3, and
s4, and that these Austinian propositions have the
probabilities 0.6, 0.6. and 0.5 respectively.
Take JTman to have five members correspond-
ing to judgements by A that there was a man in
s1, ... , s5, and that the Austinian propositions as-
signing Tman to s1, ... , s5 all have probability 0.7.
Given these assumptions, the conditional probabil-
ity that A will assign on the basis of J to someone
runs, given that he is a man is pA,J(r : Trun  |r :
</bodyText>
<equation confidence="0.962617">
Tman) = ||Tman∧Trun||J ||Tman||J = 0.7+0.7+0.7+0.7+0.7 = .486
0.6+0.6+0.5
</equation>
<bodyText confidence="0.99995175">
We use conditional probabilities to construct a
Naive Bayes classifier. A classifies a new situa-
tion s based on the prior judgements J, and what-
ever evidence A can acquire about s. This evi-
dence has the form pA,J(s : Te1), ..., pA,J(s : Ten),
where Te1, ... , Ten are the evidence types. The
Naive Bayes classifier assumes that the evidence is
independent, in that the probability of each piece
of evidence is independent of every other piece of
evidence.
We first formulate Bayes’ rule of conditional
probability. This rule defines the conditional prob-
ability of a conclusion r : Tc, given evidence r :
Te1, r : Te2, ... , r : Ten, in terms of conditional prob-
abilities of the form p(si : Tei  |si : Tc), 1 &lt; i &lt; n,
and priors for conclusion and evidence:
</bodyText>
<equation confidence="0.857027">
pA,J(r : Tc  |r : Te1,..., r : Ten) =
</equation>
<bodyText confidence="0.999771785714286">
The conditional probabilities are computed
from observations as indicated above. The rule of
conditional probability allows the combination of
several pieces of evidence, without requiring pre-
vious observation of a situation involving all the
evidence types.
We formulate a Naive Bayes classifier as a func-
tion from evidence types Te1, Te2, ... ,Ten (i.e. from
a record of type Te1 n Te2 n ... n Ten) to conclusion
types Tc1, Tc2, ... , Tcm. The conclusion is a disjunc-
tion of one or more T E {Tc1, Tc2, ..., Tcm}, where
m ranges over all possible non-disjunctive conclu-
sions distinguished by the classifier. This function
is specified as follows.
</bodyText>
<equation confidence="0.989832666666667">
κ : (Te1 n ... nTen) --+ (Tc1 V ... V Tcm) such that κ(r) =
(W argmax pA,J(r : T  |r : Te1,..., r : Ten)
TE(Tc1 ,...,Tcm )
</equation>
<bodyText confidence="0.9981835">
The classifier returns the type T which max-
imises the conditional probability of r : T given
</bodyText>
<equation confidence="0.9376645">
ref = a
cman = s1
crun = s2
⎤
ref : Ind
cman : man(ref) ⎦
crun : run(ref)
prob = 0.7
</equation>
<figure confidence="0.939263842105263">
⎡
⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣
⎡
sit = ⎣
⎡
sit-type =
⎣
⎤
⎦
⎤
⎦⎥⎥⎥⎥⎥⎥⎥
J and
Trun =
priorJ(Tc)
||Te1 ∧Tc||J
||Tc||J ...
||Ten ∧Tc||J
||Tc||J
priorJ(Te1 )...priorJ(Ten )
</figure>
<page confidence="0.637809">
77
</page>
<bodyText confidence="0.944549526315789">
�
�
the evidence provided by r. The argmax operator
here takes a sequence of arguments and a func-
tion and yields a sequence containing the argu-
ments which maximise the function (if there are
more than one).
The classifier will output a disjunction in case
both possibilities have the same probability. The
V operator takes a sequence and returns the dis-
junction of all elements of the sequence.
In addition to computing the conclusion which
receives the highest probability given the evi-
dence, we also want the posterior probability of
the judgement above, i.e. the probability of the
judgement in light of the evidence. We obtain the
non-normalised probabilities (pnnA,J) of the different
possible conclusions by factoring in the probabili-
ties of the evidence:
</bodyText>
<equation confidence="0.99938325">
pnnA,J(r : κ(r)) =
E
T∈V−1 κ(r) pA,J(r : T I r : Te1,..., r : Ten)pA,J(r :
Te1) ... pA,J(r : Ten)
</equation>
<bodyText confidence="0.962438714285714">
where V−1 is the inverse of V, i.e. a function that
takes a disjunction and returns the set of disjuncts.
We then take the probability of r : κ(r) and
normalise over the sum of the probabilities of
all the possible conclusions. This gives us the
normalised probability of the judgement resulting
pnn A,J(r:κ(r))
from classification p(r : κ(r)) = E1&lt;i&lt;m pnnA,J(r:Tci) .
However, since the probabilities of the evidence
are identical for all possible conclusions, we can
ignore them and instead compute the normalised
probability with the following equation (where m
ranges over all possible non-disjunctive conclu-
sions distinguished by the classifier, as above).
</bodyText>
<equation confidence="0.9953465">
/ ET EV−1 κ(r) pA,J (r:T |r:Te1 ,...,r:Ten )
pA,Jlr : κ(r)) = E1&lt;i&lt;m pA,J(r:Tci |r:Te1,...,r:Ten)
</equation>
<bodyText confidence="0.9983485">
The result of classification can be represented as
an Austinian proposition
</bodyText>
<equation confidence="0.943622">
sit = s
sit-type = κ(s)
prob = pA,J(s : κ(s))
</equation>
<bodyText confidence="0.999515666666667">
which A adds to ;� as a result of observing and
classifying s, and is thus made available for sub-
sequent probabilistic reasoning.
</bodyText>
<sectionHeader confidence="0.999194" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999912203703704">
We have presented a probabilistic version of a rich
type theory with records, relying heavily on classi-
cal equations for types formed with meet, join, and
negation. This has permitted us to sustain classi-
cal equivalences and Boolean negation for com-
plex types within an intensional type theory. We
have replaced the truth of a type judgement with
the probability of it being the case, and we have
applied this approach to judgements that a situa-
tion if of type T.
Our probabilistic formulation of a rich type the-
ory with records provides the basis for a compo-
sitional semantics in which functions apply to cat-
egorical semantic objects in order to return either
functions from categorical interpretations to prob-
abilistic judgements, or, for sentences, to proba-
bilistic Austinian propositions. One of the inter-
esting ways in which this framework differs from
classical model theoretic semantics is that the ba-
sic types and type judgements at the foundation of
the type system correspond to perceptual judge-
ments concerning objects and events in the world,
rather than to entities in a model and set theoretic
constructions defined on them.
We have offered a schematic view of semantic
learning. On this account observations of situa-
tions in the world support the acquisition of naive
Bayesian classifiers from which the basic proba-
bilistic types of our type theoretical semantics are
extracted. Our type theory is, then, the interface
between observation-based learning of classifiers
for objects and the situations in which they figure
on one hand, and the computation of complex se-
mantic values for the expressions of a natural lan-
guage from these simple probabilistic types and
type judgements on the other. Therefore our gen-
eral model of interpretation achieves a highly in-
tegrated bottom-up treatment of linguistic mean-
ing and perceptually-based cognition that situates
meaning in learning how to make observational
judgements concerning the likelihood of situations
obtaining in the world.
The types of our semantic theory are inten-
sional. They constitute ways of classifying situ-
ations, and they cannot be reduced to set of situa-
tions. The theory achieves fine-grained intension-
ality through a rich and articulated type system,
where the foundation of this system is anchored in
perceptual observation.
The meanings of expressions are acquired on
the basis of speakers’ experience in the applica-
tion of classifiers to objects and events that they
encounter. Meanings are dynamic and updated in
light of subsequent experience.
</bodyText>
<page confidence="0.6643025">
1
78
</page>
<bodyText confidence="0.9998155">
Probability is distributed over alternative situ-
ation types. Possible worlds, construed as maxi-
mal consistent sets of propositions (ultrafilters in a
proof theoretic lattice of propositions) play no role
in this framework.
Bayesian reasoning from observation provides
the incremental basis for learning and refining
predicative types. These types feed the combina-
torial semantic procedures for interpreting the sen-
tences of a natural language.
In future work we will explore implementations
of our learning theory in order to study the viabil-
ity of our probabilistic type theory as an interface
between perceptual judgement and compositional
semantics. We hope to show that, in addition to
its cognitive and theoretical interest, our proposed
framework will yield results in robotic language
learning, and dialogue modelling.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999791833333334">
We are grateful to two anonymous reviewers for
very helpful comments on an earlier draft of this
paper. We also thank Alex Clark, Jekaterina
Denissova, Raquel Fern´andez, Jonathan Ginzburg,
Noah Goodman, Dan Lassiter, Michiel van Lam-
balgen, Poppy Mankowitz, Aarne Ranta, and Pe-
ter Sutton for useful discussion of ideas presented
in this paper. Shalom Lappin’s participation in
the research reported here was funded by grant
ES/J022969/1 from the Economic and Social Re-
search Council of the UK, and a grant from the
Wenner-Gren Foundations. We also gratefully ac-
knowledge the support of Vetenskapsr˚adet, project
2009-1569, Semantic analysis of interaction and
coordination in dialogue (SAICD); the Depart-
ment of Philosophy, Linguistics, and Theory of
Science; and the Centre for Language Technology
at the University of Gothenburg.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999879644444445">
Jon Barwise and John Perry. 1983. Situations and
Attitudes. Bradford Books. MIT Press, Cambridge,
Mass.
I. Beltagy, C. Chau, G. Boleda, D. Garrette, K. Erk,
and R. Mooney. 2013. Montague meets markov:
Deep semantics with probabilistic logical form. In
Second Joint Conference on Lexical and Computa-
tional Semantics, Vol. 1, pages 11–21. Association
of Computational Linguistics, Atlanta, GA.
R. Carnap. 1947. Meaning and Necessity. University
of Chicago Press, Chicago.
A. Clark and S. Lappin. 2011. Linguistic Nativism
and the Poverty of the Stimulus. Wiley-Blackwell,
Chichester, West Sussex, and Malden, MA.
Robin Cooper. 2012. Type theory and semantics in
flux. In Ruth Kempson, Nicholas Asher, and Tim
Fernando, editors, Handbook of the Philosophy of
Science, volume 14: Philosophy of Linguistics. El-
sevier BV, 271–323. General editors: Dov M. Gab-
bay, Paul Thagard and John Woods.
C. Fox and S. Lappin. 2010. Expressiveness and
complexity in underspecified semantics. Linguistic
Analysis, Festschrift for Joachim Lambek, 36:385–
417.
J. Halpern. 2003. Reasoning About Uncertainty. MIT
Press, Cambridge MA.
H. Kamp and U. Reyle. 1993. From Discourse to
Logic: Introduction to Modeltheoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Kluwer, Dordrecht.
A.N. Kolmogorov. 1950. Foundations of Probability.
Chelsea Publishing, New York.
Per Martin-L¨of. 1984. Intuitionistic Type Theory. Bib-
liopolis, Naples.
Richard Montague. 1974. Formal Philosophy: Se-
lected Papers of Richard Montague. Yale University
Press, New Haven. ed. and with an introduction by
Richmond H. Thomason.
C. Papadimitriou. 1995. Computational Complexity.
Addison-Wesley Publishing Co., Readin, MA.
J. Paris. 2010. Pure inductive logic. Winter School in
Logic, Guangzhou, China.
J. Pearl. 1990. Bayesian decision methods. In
G. Shafer and J. Pearl, editors, Readings in Uncer-
tain Reasoning, pages 345–352. Morgan Kaufmann.
</reference>
<page confidence="0.999049">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.856026">
<title confidence="0.999957">A Probabilistic Rich Type Theory for Semantic Interpretation</title>
<author confidence="0.99709">Simon Shalom</author>
<author confidence="0.99709">Staffan</author>
<affiliation confidence="0.907725">of Gothenburg, College London</affiliation>
<email confidence="0.992126">simon.dobnik@gu.se,shalom.lappin@kcl.ac.uk</email>
<abstract confidence="0.996237076923077">We propose a probabilistic type theory in which a judged to be of a type probabil- In addition to basic and functional types it inrecord types and a notion of typing based on them. The type system is intensional in that types of situations are not reduced to sets of situations. We specify the fragment of a compositional semantics in which truth conditions are replaced by probability conditions. The type system is the interface between classifying situations in perception and computing the semantic interpretations of phrases in natural language.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jon Barwise</author>
<author>John Perry</author>
</authors>
<title>Situations and Attitudes.</title>
<date>1983</date>
<publisher>Bradford Books. MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="3954" citStr="Barwise and Perry, 1983" startWordPosition="616" endWordPosition="619">of heads vs tails in a series of coin tosses. Here the world is held constant except along the dimension of a binary choice between a particular set of possible outcomes. A slightly more complex case is the probability distribution for possible results of throwing a single die, which allows for six possibilities corresponding to each of its numbered faces. This restricted range of outcomes constitutes the sample space. We are making explicit the assumption, common to most probability theories used in AI, with clearly defined sample spaces, that probability is distributed over situation types (Barwise and Perry, 1983), rather than over sets of entire worlds. An Austinian proposition is a judgement that a 72 Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 72–79, Gothenburg, Sweden, April 26-30 2014. c�2014 Association for Computational Linguistics situation is of a particular type, and we treat it as probabilistic. In fact, it expresses a subjective probability in that it encodes the belief of an agent concerning the likelihood that a situation is of that type. The core of an Austinian proposition is a type judgement of the form s : T, which states that a s</context>
</contexts>
<marker>Barwise, Perry, 1983</marker>
<rawString>Jon Barwise and John Perry. 1983. Situations and Attitudes. Bradford Books. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Beltagy</author>
<author>C Chau</author>
<author>G Boleda</author>
<author>D Garrette</author>
<author>K Erk</author>
<author>R Mooney</author>
</authors>
<title>Montague meets markov: Deep semantics with probabilistic logical form.</title>
<date>2013</date>
<journal>Association of Computational Linguistics,</journal>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics,</booktitle>
<volume>1</volume>
<pages>11--21</pages>
<location>Atlanta, GA.</location>
<contexts>
<context position="6186" citStr="Beltagy et al. (2013)" startWordPosition="995" endWordPosition="998">te) set of rules or procedures we could use to decide which propositions to add in order to generate a full description of a world in a systematic way. Nor is it obvious at what point the conjunction will constitute a complete description of the world. Moreover, all the propositions that P entails must be added to it, and all the propositions with which P is inconsistent must be excluded, in order to obtain the maximal consistent set of propositions that describe a world. But then testing the satisfiability of P is an instance of the ksat problem, which, in the general case, is NP-complete.2 1Beltagy et al. (2013) propose an approach on which classical logic-based representations are combined with distributional lexical semantics and a probabilistic Markov logic, in order to select among the set of possible inferences from a sentence. Our concern here is more foundational. We seek to replace classical semantic representations with a rich probabilistic type theory as the basis of both lexical and compositional interpretation. 2The ksat problem is to determine whether a formula in propositional logic has a satisfying set of truth-value assignments. For the complexity results of different types of ksat pr</context>
</contexts>
<marker>Beltagy, Chau, Boleda, Garrette, Erk, Mooney, 2013</marker>
<rawString>I. Beltagy, C. Chau, G. Boleda, D. Garrette, K. Erk, and R. Mooney. 2013. Montague meets markov: Deep semantics with probabilistic logical form. In Second Joint Conference on Lexical and Computational Semantics, Vol. 1, pages 11–21. Association of Computational Linguistics, Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Carnap</author>
</authors>
<title>Meaning and Necessity.</title>
<date>1947</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="5055" citStr="Carnap, 1947" startWordPosition="802" endWordPosition="803"> of that type. The core of an Austinian proposition is a type judgement of the form s : T, which states that a situation s is of type T. On our account this judgement is expressed probabilistically as p(s : T) = r, where r E [0,1].1 On the probabilistic type system that we propose situation types are intensional objects over which probability distributions are specified. This allows us to reason about the likelihood of alternative states of affairs without invoking possible worlds. Complete worlds are not tractably representable. Assume that worlds are maximal consistent sets of propositions (Carnap, 1947). If the logic of propositions is higher-order, then the problem of determining membership in such a set is not complete. If the logic is classically firstorder, then the membership problem is complete, but undecidable. Alternatively, we could limit ourselves to propositional logic, and try to generate a maximally consistent set of propositions from a single finite proposition P in Conjunctive Normal Form (CNF, a conjunction of disjunctions), by simply adding conjuncts to P. But it is not clear what (finite) set of rules or procedures we could use to decide which propositions to add in order t</context>
</contexts>
<marker>Carnap, 1947</marker>
<rawString>R. Carnap. 1947. Meaning and Necessity. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Clark</author>
<author>S Lappin</author>
</authors>
<date>2011</date>
<booktitle>Linguistic Nativism and the Poverty of the Stimulus.</booktitle>
<location>Wiley-Blackwell, Chichester, West Sussex, and Malden, MA.</location>
<contexts>
<context position="1906" citStr="Clark and Lappin, 2011" startWordPosition="284" endWordPosition="287">ience of semantic properties that is pervasive in speakers’ judgements concerning truth, predication, and meaning relations. In general, predicates do not have determinate extensions (or intensions), and so, in many cases, speakers do not make categorical judgements about the interpretation of an expression. Attributing gradience effects to performance mechanisms offers no help, unless one can show precisely how these mechanisms produce the observed effects. Moreover, there is a fair amount of evidence indicating that language acquisition in general crucially relies on probabilistic learning (Clark and Lappin, 2011). It is not clear how a reasonable account of semantic learning could be constructed on the basis of the categorical type systems that either classical or revised semantic theories assume. Such systems do not appear to be efficiently learnable from the primary linguistic data (with weak learning biases), nor is there much psychological data to suggest that they provide biologically determined constraints on semantic learning. A semantic theory that assigns probability rather than truth conditions to sentences is in a better position to deal with both of these issues. Gradience is intrinsic to </context>
</contexts>
<marker>Clark, Lappin, 2011</marker>
<rawString>A. Clark and S. Lappin. 2011. Linguistic Nativism and the Poverty of the Stimulus. Wiley-Blackwell, Chichester, West Sussex, and Malden, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robin Cooper</author>
</authors>
<title>Type theory and semantics in flux.</title>
<date>2012</date>
<booktitle>Handbook of the Philosophy of Science, volume 14: Philosophy of Linguistics. Elsevier BV,</booktitle>
<pages>271--323</pages>
<editor>In Ruth Kempson, Nicholas Asher, and Tim Fernando, editors,</editor>
<contexts>
<context position="7356" citStr="Cooper (2012)" startWordPosition="1195" endWordPosition="1196">exity results of different types of ksat problem see Papadimitriou (1995). By contrast situation types can be as large or as small as we need them to be. They are not maximal in the way that worlds are, and so the issue of completeness of specification does not arise. Therefore, they can, in principle, be tractably represented. 2 Rich Type Theory and Probability Central to standard formulations of rich type theories (for example, (Martin-L¨of, 1984)) is the notion of a judgement a : T, that object a is of type T. We represent the probability of this judgement as p(a : T). Our system (based on Cooper (2012)) includes the following types. Basic Types are not constructed out of other objects introduced in the theory. If T is a basic type, p(a : T) for any object a is provided by a probability model, an assignment of probabilities to judgements involving basic types. PTypes are constructed from a predicate and an appropriate sequence of arguments. An example is the predicate ‘man’ with arity (Ind, Time) where the types Ind and Time are the basic type of individuals and of time points respectively. Thus man(john,18:10) is the type of situation (or eventuality) where John is a man at time 18:10. A pr</context>
</contexts>
<marker>Cooper, 2012</marker>
<rawString>Robin Cooper. 2012. Type theory and semantics in flux. In Ruth Kempson, Nicholas Asher, and Tim Fernando, editors, Handbook of the Philosophy of Science, volume 14: Philosophy of Linguistics. Elsevier BV, 271–323. General editors: Dov M. Gabbay, Paul Thagard and John Woods.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fox</author>
<author>S Lappin</author>
</authors>
<title>Expressiveness and complexity in underspecified semantics. Linguistic Analysis, Festschrift for Joachim Lambek,</title>
<date>2010</date>
<pages>36--385</pages>
<contexts>
<context position="997" citStr="Fox and Lappin, 2010" startWordPosition="149" endWordPosition="152"> to basic and functional types it includes, inter alia, record types and a notion of typing based on them. The type system is intensional in that types of situations are not reduced to sets of situations. We specify the fragment of a compositional semantics in which truth conditions are replaced by probability conditions. The type system is the interface between classifying situations in perception and computing the semantic interpretations of phrases in natural language. 1 Introduction Classical semantic theories (Montague, 1974), as well as dynamic (Kamp and Reyle, 1993) and underspecified (Fox and Lappin, 2010) frameworks use categorical type systems. A type T identifies a set of possible denotations for expressions in T, and the system specifies combinatorial operations for deriving the denotation of an expression from the values of its constituents. These theories cannot represent the gradience of semantic properties that is pervasive in speakers’ judgements concerning truth, predication, and meaning relations. In general, predicates do not have determinate extensions (or intensions), and so, in many cases, speakers do not make categorical judgements about the interpretation of an expression. Attr</context>
</contexts>
<marker>Fox, Lappin, 2010</marker>
<rawString>C. Fox and S. Lappin. 2010. Expressiveness and complexity in underspecified semantics. Linguistic Analysis, Festschrift for Joachim Lambek, 36:385– 417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Halpern</author>
</authors>
<title>Reasoning About Uncertainty.</title>
<date>2003</date>
<publisher>MIT Press,</publisher>
<location>Cambridge MA.</location>
<contexts>
<context position="3206" citStr="Halpern, 2003" startWordPosition="494" endWordPosition="495">in the continuum of real numbers [0,1], rather than Boolean values in {0,1}. In addition, a probabilistic account of semantic learning is facilitated if the target of learning is a probabilistic representation of meaning. Both semantic representation and learning are instances of reasoning under uncertainty. Probability theorists working in AI often describe probability judgements as involving distributions over worlds. In fact, they tend to limit such judgements to a restricted set of outcomes or events, each of which corresponds to a partial world which is, effectively, a type of situation (Halpern, 2003). A classic example of the reduction of worlds to situation types in probability theory is the estimation of the likelihood of heads vs tails in a series of coin tosses. Here the world is held constant except along the dimension of a binary choice between a particular set of possible outcomes. A slightly more complex case is the probability distribution for possible results of throwing a single die, which allows for six possibilities corresponding to each of its numbered faces. This restricted range of outcomes constitutes the sample space. We are making explicit the assumption, common to most</context>
</contexts>
<marker>Halpern, 2003</marker>
<rawString>J. Halpern. 2003. Reasoning About Uncertainty. MIT Press, Cambridge MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kamp</author>
<author>U Reyle</author>
</authors>
<title>From Discourse to Logic: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="955" citStr="Kamp and Reyle, 1993" startWordPosition="142" endWordPosition="145">f a type T with probability p. In addition to basic and functional types it includes, inter alia, record types and a notion of typing based on them. The type system is intensional in that types of situations are not reduced to sets of situations. We specify the fragment of a compositional semantics in which truth conditions are replaced by probability conditions. The type system is the interface between classifying situations in perception and computing the semantic interpretations of phrases in natural language. 1 Introduction Classical semantic theories (Montague, 1974), as well as dynamic (Kamp and Reyle, 1993) and underspecified (Fox and Lappin, 2010) frameworks use categorical type systems. A type T identifies a set of possible denotations for expressions in T, and the system specifies combinatorial operations for deriving the denotation of an expression from the values of its constituents. These theories cannot represent the gradience of semantic properties that is pervasive in speakers’ judgements concerning truth, predication, and meaning relations. In general, predicates do not have determinate extensions (or intensions), and so, in many cases, speakers do not make categorical judgements about</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>H. Kamp and U. Reyle. 1993. From Discourse to Logic: Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A N Kolmogorov</author>
</authors>
<title>Foundations of Probability.</title>
<date>1950</date>
<publisher>Chelsea Publishing,</publisher>
<location>New York.</location>
<contexts>
<context position="8437" citStr="Kolmogorov, 1950" startWordPosition="1402" endWordPosition="1403"> and of time points respectively. Thus man(john,18:10) is the type of situation (or eventuality) where John is a man at time 18:10. A probability model provides probabilities p(e : r(a1, ... , an)) for ptypes r(a1, ... , an). We take both common nouns and verbs to provide the components out of which PTypes are constructed. Meets and Joins give, for T1 and T2, the meet, T1 n T2 and the join T1 V T2, respectively. a : T1 n T2 just in case a : T1 and a : T2. a : T1 V T2 just in case either a : T1 or a : T2 (possibly both).3 The probabilities for meet and joint types are defined by the classical (Kolmogorov, 1950) equations p(a : T1 n T2) = p(a : T1)p(a : T2 |a : T1) (equivalently, p(a : T1 n T2) = p(a : T1, a : T2)), and p(a : T1 V T2) = p(a : T1) + p(a : T2) − p(a : T1 n T2), respectively. Subtypes A type T1 is a subtype of type T2, T1 C T2, just in case a : T1 implies a : T2 no matter what we assign to the basic types. If T1 C T2 then a : T1nT2 iff a : T1 and a : T1VT2 iff a : T2. Similarly, if T2 C T1 then a : T1 n T2 iff a : T2 and a : T1 V T2 iff a : T1. If T2 C T1, then p(a : T1 n T2) = p(a : T2), and p(a : T1 V T2) = p(a : T1). If T1 C T2, 3This use of intersection and union types is not standa</context>
</contexts>
<marker>Kolmogorov, 1950</marker>
<rawString>A.N. Kolmogorov. 1950. Foundations of Probability. Chelsea Publishing, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Per Martin-L¨of</author>
</authors>
<title>Intuitionistic Type Theory. Bibliopolis,</title>
<date>1984</date>
<location>Naples.</location>
<marker>Martin-L¨of, 1984</marker>
<rawString>Per Martin-L¨of. 1984. Intuitionistic Type Theory. Bibliopolis, Naples.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Montague</author>
</authors>
<title>Formal Philosophy: Selected Papers of Richard Montague.</title>
<date>1974</date>
<editor>Haven. ed.</editor>
<publisher>Yale University Press,</publisher>
<location>New</location>
<contexts>
<context position="912" citStr="Montague, 1974" startWordPosition="136" endWordPosition="137">which a situation s is judged to be of a type T with probability p. In addition to basic and functional types it includes, inter alia, record types and a notion of typing based on them. The type system is intensional in that types of situations are not reduced to sets of situations. We specify the fragment of a compositional semantics in which truth conditions are replaced by probability conditions. The type system is the interface between classifying situations in perception and computing the semantic interpretations of phrases in natural language. 1 Introduction Classical semantic theories (Montague, 1974), as well as dynamic (Kamp and Reyle, 1993) and underspecified (Fox and Lappin, 2010) frameworks use categorical type systems. A type T identifies a set of possible denotations for expressions in T, and the system specifies combinatorial operations for deriving the denotation of an expression from the values of its constituents. These theories cannot represent the gradience of semantic properties that is pervasive in speakers’ judgements concerning truth, predication, and meaning relations. In general, predicates do not have determinate extensions (or intensions), and so, in many cases, speake</context>
<context position="16245" citStr="Montague (1974)" startWordPosition="3040" endWordPosition="3041">r1, ... , Irn)))} just in case r : T, r.f is defined and r.f : T (r.Ir1, ... , r.Irn). The probability that an object r is of a record type T is given by the following clauses: 1. p(r : Rec) = 1 if r is a record, 0 otherwise ^ 2. p(r : T1 U {(f, T2)}) = p (r : T1, r.f : T2) 3. If// T : (T1 --+ (... --+ (Tn --+ Type) ...)), then plr : T U { (f, (T , (Ir1, ... , Irn)))}) = ^ p (r : T, r.f : T (r.Ir1, ... , r.Irn) I r.Ir1 : T1, ... , r.Irn : Tn) 4In the full version of TTR we also allow absolute paths which point to particular records, but we will not include them here. 3 Compositional Semantics Montague (1974) determines the denotation of a complex expression by applying a function to an intensional argument (as in Q NP ](Q ∧VP ])). We employ a variant of this general strategy by applying a probabilistic evaluation function Q · ]p to a categorical (non-probabilistic) semantic value. For semantic categories that are interpreted as functions, Q · ]p yields functions from categorical values to probabilities. For sentences it produces probability values. The probabilistic evaluation function Q · ]p produces a probabilistic interpretation based on a classical compositional semantics. For sentences it wi</context>
</contexts>
<marker>Montague, 1974</marker>
<rawString>Richard Montague. 1974. Formal Philosophy: Selected Papers of Richard Montague. Yale University Press, New Haven. ed. and with an introduction by Richmond H. Thomason.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Papadimitriou</author>
</authors>
<title>Computational Complexity.</title>
<date>1995</date>
<publisher>Addison-Wesley Publishing Co.,</publisher>
<location>Readin, MA.</location>
<contexts>
<context position="6816" citStr="Papadimitriou (1995)" startWordPosition="1094" endWordPosition="1095">n approach on which classical logic-based representations are combined with distributional lexical semantics and a probabilistic Markov logic, in order to select among the set of possible inferences from a sentence. Our concern here is more foundational. We seek to replace classical semantic representations with a rich probabilistic type theory as the basis of both lexical and compositional interpretation. 2The ksat problem is to determine whether a formula in propositional logic has a satisfying set of truth-value assignments. For the complexity results of different types of ksat problem see Papadimitriou (1995). By contrast situation types can be as large or as small as we need them to be. They are not maximal in the way that worlds are, and so the issue of completeness of specification does not arise. Therefore, they can, in principle, be tractably represented. 2 Rich Type Theory and Probability Central to standard formulations of rich type theories (for example, (Martin-L¨of, 1984)) is the notion of a judgement a : T, that object a is of type T. We represent the probability of this judgement as p(a : T). Our system (based on Cooper (2012)) includes the following types. Basic Types are not construc</context>
</contexts>
<marker>Papadimitriou, 1995</marker>
<rawString>C. Papadimitriou. 1995. Computational Complexity. Addison-Wesley Publishing Co., Readin, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Paris</author>
</authors>
<title>Pure inductive logic. Winter School in Logic,</title>
<date>2010</date>
<location>Guangzhou, China.</location>
<contexts>
<context position="9870" citStr="Paris, 2010" startWordPosition="1735" endWordPosition="1736">1 n T2) G p(a : T1), and p(a : T1) G p(a : T1 V T2). — We generalize probabilistic meet and join types to probabilities for unbounded conjunctive and disjunctive type judgements, again using the classical equations. Let ^ p (a0 : T0, ... , an : Tn) be the conjunctive probability of judgements a0 : T0, ... , a,,, : T,,,. ^Then ^ p (a0 : T0, ... , an : Tn) = p (a0 : T0, ..., an_1 : Tn_1)p(an : Tn |a0 : T0,..., an_1 : Tn_1). If n = 0, ^ p(a0 : T0, ... , an : Tn) = 1. We interpret universal quantification as an unbounded conjunctive probability, which is true if it is vacuously satisfied (n = 0) (Paris, 2010). Let _p (a0 : T0, a1 : T1, ...) be the disjunctive probability of judgements a0 : T0, a1 : T1,.... It is computed by _p (a0 : T0, ... , an : Tn) = _p (a0 : T0,..., an_1 : Tn_1) + p(an : Tn) − ^(a0 p : T0, ... , an_1 : Tn_1)p(an : Tn |a0 : T0, ... , an_1 : Tn_1). If n = 0,_p (a0 : T0,..., an : Tn) = 0. We take existential quantification to be an unbounded disjunctive probability, which is false if it lacks a single non-nil probability instance (n = 0). Conditional Conjunctive Probabilities are computed by ^ (a0 : T0, ... , an : Tn |a : T) = p ^p (a0 : T0,..., an_1 : Tn_1 |a : T)p(an : Tn | a0 </context>
</contexts>
<marker>Paris, 2010</marker>
<rawString>J. Paris. 2010. Pure inductive logic. Winter School in Logic, Guangzhou, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
</authors>
<title>Bayesian decision methods.</title>
<date>1990</date>
<booktitle>Readings in Uncertain Reasoning,</booktitle>
<pages>345--352</pages>
<editor>In G. Shafer and J. Pearl, editors,</editor>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="22635" citStr="Pearl, 1990" startWordPosition="4124" endWordPosition="4125">e value of prob. The definition of Q ]p specifies a compositional procedure for generating an Austinian proposition (record) of this type from the meanings of the syntactic constituents of a sentence. 4 An Outline of Semantic Learning We outline a schematic theory of semantic learning on which agents acquire classifiers that form the basis for our probabilistic type system. For simplicity and ease of presentation we take these to be Naive Bayes classifiers, which an agent acquires from observation. In future developments of this theory we will seek to extend the approach to Bayesian networks (Pearl, 1990). We assume that agents keep records of observed situations and their types, modelled as probabilistic Austinian propositions. For example, an observation of a man running might yield the following Austinian proposition for some a:Ind, s1:man(a), s2:run(a): 1 76 An agent, A, makes judgements based on a finite string of probabilistic Austinian propositions, J, corresponding to prior judgements held in memory. For a type, T, JT represents that set of Austinian propositions j such that j.sit-type C T. If T is a type and J a finite string of probabilistic Austinian propositions, then ||T ||J repre</context>
</contexts>
<marker>Pearl, 1990</marker>
<rawString>J. Pearl. 1990. Bayesian decision methods. In G. Shafer and J. Pearl, editors, Readings in Uncertain Reasoning, pages 345–352. Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
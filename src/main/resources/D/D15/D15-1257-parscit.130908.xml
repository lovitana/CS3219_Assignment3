<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000093">
<title confidence="0.989892">
Modeling Reportable Events as Turning Points in Narrative
</title>
<author confidence="0.997709">
Jessica Ouyang
</author>
<affiliation confidence="0.9967575">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.985933">
New York, NY 10027
</address>
<email confidence="0.999391">
ouyangj@cs.columbia.edu
</email>
<author confidence="0.996584">
Kathleen McKeown
</author>
<affiliation confidence="0.997021">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.994246">
New York, NY 10027
</address>
<email confidence="0.999665">
kathy@cs.columbia.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987">
We present novel experiments in model-
ing the rise and fall of story characteristics
within narrative, leading up to the Most
Reportable Event (MRE), the compelling
event that is the nucleus of the story. We
construct a corpus of personal narratives
from the bulletin board website Reddit,
using the organization of Reddit content
into topic-specific communities to auto-
matically identify narratives. Leveraging
the structure of Reddit comment threads,
we automatically label a large dataset of
narratives. We present a change-based
model of narrative that tracks changes in
formality, affect, and other characteristics
over the course of a story, and we use
this model in distant supervision and self-
training experiments that achieve signifi-
cant improvements over the baselines at
the task of identifying MREs.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999776357142857">
What is a narrative? In one of the early linguis-
tic analyses of storytelling, Prince (1973) defines
a story as describing an event that causes a change
of state. Prince’s minimal story has three parts: the
starting state, the ending state, and the event that
transforms the stating state into the ending state.
An example of a minimal story is as follows:
A man was unhappy, then he fell in love,
then as a result, he was happy.
Polanyi (1976) notes that minimal stories are
toy examples that would never hold an audience’s
interest. So what makes a story interesting?
Labov (1967; 1997) defines a well-formed nar-
rative as a series of actions leading to a Most Re-
portable Event (MRE). The MRE is the point of
the story – the most unusual event that has the
greatest emotional impact on the narrator and the
audience. For a story to be interesting, Prince’s
change-of-state event should be an MRE.
The following is an example of a narrative from
the corpus we create in this work, with the sen-
tence containing the MRE emphasized:
This isn’t exactly creepy, but it’s one of
the scariest things that’s ever happened
to me. I was driving down the motor-
way with my boyfriend in the passenger
seat, and my dad in the seat behind my
own. My dad is an epileptic and his fits
are extremely sporadic. Sometimes he
goes extremely stiff and other times he
will try to get out of places or grab and
punch people. Mid-conversation I felt
his hands wrap around my throat as I
was driving, pulling my head back and
making it increasingly difficult to drive.
My boyfriend managed to help steer the
car into the hard shoulder but it was one
of the scariest experiences in my life.
The MRE is the shortest possible summary of a
story; it is what we would say about the story if
we could only say one thing. If we could identify
the MRE of a narrative, we could automatically
generate summaries or headline-style titles for on-
line stories. Detecting MREs could also allow us
to explore how storytellers build emotional impact
as they lead up to the climaxes of their stories.
In this work, we present a novel approach to
modeling narrative in order to automatically iden-
tify the MRE. The MRE is a real world event un-
derlying the story and thus is difficult to infer; in-
stead, we identify sentences that describe or re-
fer to it. We incorporate Prince’s change-of-state
formalization as well as Labov’s definition of the
MRE by modeling changes in story characteristics
suggested by Prince, Polanyi, and Labov, such as
measures of syntactic complexity and emotional
</bodyText>
<page confidence="0.968582">
2149
</page>
<note confidence="0.9851855">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2149–2158,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.998347619047619">
content. If Prince and Labov are both correct, we
should find the MRE at a point of change in the
story and in our story characteristics.
We create a corpus of thousands of personal
narratives collected from Reddit, a social bul-
letin board website organized into topic-specific
‘subreddit’ communities. We automatically label
most of this data using heuristics based on the
comment-thread structure of Reddit content. Us-
ing this corpus, we conduct two experiments in
classifying sentences of a story as containing the
MRE or not: the first using distant supervision,
and the second using self-training.
In Section 2, we discuss prior work on automati-
cally identifying personal narratives, as well as re-
lated experiments using Labov’s theory of narra-
tive analysis. Section 3 discusses data collection
and labeling. Sections 4-5 present our change-
based model of narrative and our experiments. Fi-
nally, Section 6 discusses our experimental results
and proposes directions for future work.
</bodyText>
<sectionHeader confidence="0.999926" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999951352112676">
Prior work using Labov’s theory of narrative has
focused on classifying clauses by their function.
Rahimtoroghi et al. (2013)worked on 20 of
Aesop’s fables. The 315 clauses were manu-
ally annotated with the three labels of Labov
and Waltezky (1967), Orientation (background in-
formation), Action (events), and Evaluation (au-
thor’s perspective), which we discuss in Section 4.
Rahimtoroghi et al. used two annotators with high
agreement and achieved accuracy and precision
around 0.9 on all three labels, as well as recall
above 0.9 on all but Orientation. They noted that
their data set was very clean: interannotator agree-
ment was nearly perfect, the language was simple,
and each clause served a clear narrative purpose.
Ouyang and McKeown (2014) explored iden-
tifying the Action chain of the oral narratives in
Labov (2013). They used a dataset of 49 narra-
tives (1,277 clauses), transcribed from recordings
of speech and annotated by Labov and achieved
0.72 f-score on classifying clauses as Action or
not. This task is easier than our proposed task
of identifying sentences containing MREs. Ac-
tions account for nearly half the clauses in the
Labov (2013) dataset, while there are only and
average of 2.5 MRE sentences per story. Addi-
tionally, identifying Labov’s Actions is a problem
of detecting causal and temporal relations among
events; identifying the MRE is a problem of mea-
suring how impactful and shocking an event is.
Swanson et al. (2014) used 50 stories, which
were annotated with an extended label set by three
annotators, and each of the 1,602 clauses was as-
signed the label given by the majority of annota-
tors. The extended label set was then mapped to
Labov and Waletzky’s three labels. Nearly half
of the clauses in this dataset are Evaluations, and
Orientations and Actions each make up nearly one
quarter of the dataset. Swanson et al. achieved
0.69 overall f-score on three-way classification of
clauses. Again, this task is less difficult than our
proposed task. The three labels, Orientation, Ac-
tion, and Evaluation have distinct functions that
are reflected in tense, mood, and a clause’s posi-
tion in the narrative. The MRE is not a sentence
or clause but an event that may be described or re-
ferred to by any sentence in a narrative; it is distin-
guished from the other events only by its surpris-
ingness and emotional impact, dimensions that are
difficult to model computationally without a deep
semantic understanding of the story.
The stories that Swanson et al. used were drawn
from a corpus drawn from weblog posts (Gordon
and Swanson, 2009). Gordon and Swanson used
unigram features to classify posts as either stories
or not, achieving 75% precision. They note that
only about 17% of weblog text consists of stories.
In contrast to the relatively small datasets used
by Rahimtoroghi et al., Ouyang and McKeown,
and Swanson et al., we use a larger dataset au-
tomatically collected from Reddit. Our collection
method achieved 94% precision in identifying nar-
ratives. A number of researchers have character-
ized the structure and use of Reddit, currently the
26th most popular website in the world1. Weninger
et al. (2013) described the structure of Reddit com-
ment threads. Gilbert (2013) measured user par-
ticipation in the voting process that ranks Reddit
content. Singer et al. (2014) conducted a longitu-
dinal study of the Reddit user community, finding
a trend favoring original, user-generated content.
</bodyText>
<sectionHeader confidence="0.999677" genericHeader="method">
3 Data
</sectionHeader>
<subsectionHeader confidence="0.999874">
3.1 Collection
</subsectionHeader>
<bodyText confidence="0.9997895">
We collected data from the AskReddit subreddit,
where users post questions for other members of
the community, who reply with comments answer-
ing the questions. Table 1 shows some examples
</bodyText>
<footnote confidence="0.99799">
1http://www.alexa.com/siteinfo/reddit.com
</footnote>
<page confidence="0.995642">
2150
</page>
<bodyText confidence="0.8420785">
of these posts, and we can see some of the wide
variety of story topics found on AskReddit.
Post Title
Whats your creepiest (REAL LIFE) story?
Your best “Accidentally Racist” story?
What are your stories of petty revenge?
</bodyText>
<tableCaption confidence="0.989327">
Table 1: Examples of AskReddit posts.
</tableCaption>
<bodyText confidence="0.999865923076923">
Using PRAW2, we scraped the top 50 AskRed-
dit posts containing the keyword ‘story.’ Of these
posts, 10 were tagged as NSFW (‘not safe for
work’), indicating they contained adult content;
we did not use these posts in this work, as we
felt the language would be too different from that
used in posts without the tag. Another 3 posts did
not contain personal narratives, and instead were
about fictional stories in movies or music.
With the 37 remaining posts, we treated each
top-level comment (those that replied directly to
the posted question) as a story. The example given
in Section 1 is one such story. We collected 6,000
top-level comments and discarded those without
comment threads replying to them. As we discuss
in Section 3.2, we use comment threads to auto-
matically label our training data. We tokenized the
top-level comments by sentence (Bird et al., 2009)
and removed all sentences following any varia-
tion of the word ‘EDIT’, as these were usually re-
sponses to readers’ comments. We discarded texts
with fewer than three sentences, based on Prince’s
definition of a minimal story as consisting of a
starting state, an event, and an ending state. We
are left with 4,896 stories, with an average length
of 16 sentences and a maxiumum of 198.
</bodyText>
<subsectionHeader confidence="0.999703">
3.2 Labeling
</subsectionHeader>
<bodyText confidence="0.9996529">
We partitioned our data into development, seed,
and tuning sets of 100 stories each; a testing set of
200 stories; and a training set of 4,178 stories. The
development, seed, tuning, and testing sets were
manually annotated by a native English speaker
(not one of the authors), who was instructed to
label all sentences that contained or referred to
the MRE. For convenience, from here on, we will
use the term ‘MRE’ to refer to both the Most Re-
portable Event itself (of which there can only be
</bodyText>
<footnote confidence="0.9634935">
2https://praw.readthedocs.org/en/v2.1.20/, Python Reddit
API Wrapper
</footnote>
<bodyText confidence="0.995627895833333">
one per narrative) and to sentences that contain or
refer to it (of which there can be more than one).
To measure interannotator agreement, we also
had a second annotator (also a native English
speaker and not one of the authors) label MREs
in the 100 narratives in our development set. We
found substantial agreement (Cohen’s κ = 0.729);
the two classes, MRE and not-MRE, are highly
unbalanced, so percent agreement between the two
annotators was extremely high (95%).
In addition to labeling the MREs, our first an-
notator identified and discarded 31 texts that were
not true stories, but rather Reddit-specific inside
jokes or comments on how cool the stories in the
thread were. From this, we can see that the pre-
cision of our story collection method is very high.
Gordon et al. (2007) found that stories were 17%
of the weblog text that they collected; of the 500
texts given to our annotators, 94% were stories.
Using the development set, we experimented
with seven heuristics, defined below, for automat-
ically labeling the training set. Each predicts a
sentence index sh to be the index of an MRE. We
measured the performance of each heuristic using
root-mean-square error (RMSE), which measures
the standard deviation of how far the heuristic’s
predictions fall from a true MRE.
Let N = number of narratives
sMRE = index of a true MRE
(sMREi − shi)2
We used a linear combination of three heuristics
with the lowest RMSE to label our training set.
Similarity to comment. The bag-of-words co-
sine similarity between a sentence and comments
replying to the story. We expect comments to re-
fer to the MRE because of its shocking nature and
importance to the story. This heuristic achieved
RMSE of 5.5 sentences on the development set.
Similarity to tl;dr. The latent semantic simi-
larity between a sentence and the tl;dr. The tl;dr
(too long; didn’t read) is a very short paraphrase
of a post given by its author. They are relatively
rare – 663 stories, or 14% of our data, had tl;drs.
Since the MRE is the central event of the story, we
expect it to be included in the tl;dr. We calculated
the similarity using the weighted matrix factoriza-
tion algorithm described by Guo and Diab (2012).
This heuristic achieved RMSE of 5.8 sentences.
</bodyText>
<equation confidence="0.983218">
1
RMSE =
N
N
Z=1
</equation>
<page confidence="0.947455">
2151
</page>
<bodyText confidence="0.890203254901961">
In contrast, bag-of-words cosine similarity to
the tl;dr performed poorly (RMSE of 13.2). This
is due to the tl;dr being both short and a paraphrase
of its story. There are few words in the tl;dr, and
those words are often synonyms of, but not the
same as, words in the story. Guo and Diab’s la-
tent semantic similarity score addresses this word
sparsity problem by modeling words that are not
present in the input text. We also experimented
with latent semantic similarity for the similarity-
to-comment and similarity-to-prompt heuristics,
but in these two cases, it did not perform as well
as the bag-of-words cosine similarity.
Similarity to prompt. The bag-of-words
cosine similarity between a sentence and the
AskReddit post that prompted the story. The story
should be relevant to the prompt, so we expect the
MRE to be similar to the prompt text. This heuris-
tic achieved RMSE of 6.3 sentences.
We used the heuristic with the fourth lowest
RMSE as one of the baselines in our experiments:
Last sentence. The last sentence in the story.
Since the events of a story build up to the MRE,
the MRE should occur near the end of the story.
This heuristic achieved RMSE of 6.9 sentences.
Other heuristics. We also tried the following:
• Single-sentence paragraph (RMSE of 8.7).
This heuristic was meant to capture empha-
sis, as an MRE might be placed in its own,
separate paragraph to draw attention to it.
• First sentence (RMSE of 13.7). Narra-
tives occasionally open with a brief introduc-
tory paragraph that summarizes the events to
come. This heuristic was meant to capture a
reference to the MRE in this introduction.
The training set was automatically labeled using
a linear combination of the three best-performing
heuristics: similarity to comment, similarity to
tl;dr, and similarity to prompt.
hlabel = 0.2 * hcomment + 0.5 * htldr + 0.3 * hprompt
This outperformed each of the three alone, achiev-
ing an RMSE of 5.1 sentences. The weights for
each heuristic were tuned on the development set.
For stories without a tl;dr, that heuristic was set
to 0. The sentence in the story with the highest
heuristic score was selected as the MRE.
In 52 of the 99 stories in the development
set, we found that multiple, consecutive sentences
were labeled by our annotator as MREs. The av-
erage number of consecutive MREs was 2.5 sen-
tences. To reflect this, we labeled our training set
</bodyText>
<table confidence="0.998352285714286">
Number of Sentences
Data Set Stories MRE Total
dev* 99 169 1528
seed* 82 184 958
tuning* 95 212 1301
testing* 193 444 2771
training 4178 11205 67954
</table>
<tableCaption confidence="0.998355">
Table 2: Distribution of labels (*manual).
</tableCaption>
<bodyText confidence="0.9998904">
in three-sentence blocks. The sentence selected
by our labeling heuristic, along with the imme-
diately preceding and following sentences, were
all labeled as MREs. The result was the weakly-
labeled training set in Table 2.
</bodyText>
<sectionHeader confidence="0.961084" genericHeader="method">
4 Modeling Narrative
</sectionHeader>
<bodyText confidence="0.999878333333333">
Our approach to modeling narrative is based on
both Labov (2013) and Prince (1973). We claim
that Labov’s MRE is Prince’s change of state with
the added requirement of reportability or interest-
ingness – in fact, all three components of Prince’s
minimal story have equivalences in Labov.
Labov and Waletzky (1967) proposed three
components of narrative: the Orientation, which
we equate with Prince’s starting state; the Ac-
tion, the chain of events culminating in the MRE;
and the Evaluation, the author’s perspective on the
story. Labov (2013) adds three more components:
the Resolution, equivalent to Prince’s ending state,
and the Abstract and Coda, where the author intro-
duces and concludes the story.
We focus on Prince’s claim that stories are about
change. Polanyi (1985) observes that the turning
point of a story is marked by a change in style,
formality of language, or emphasis in the telling
of the story. Labov (2013) likewise observes that
a change in verb tense often accompanies MREs.
We hypothesize that the MRE should be found at
a point of change in the story.
We score each sentence according to three
views of narrative: syntax, semantics, and affect.
Syntax. We model Polanyi’s claim that a
change in formality marks the changing point by
including metrics of sentential syntax; we use
the syntactic complexity of a sentence as an ap-
proximation for formality. The complexity of a
sentence also reflects emphasis – short, staccato
sentences bear more emphasis than long, com-
plicated ones. We use the length of the sen-
</bodyText>
<page confidence="0.97581">
2152
</page>
<bodyText confidence="0.999906914285714">
tence, the length of its verb phrase, and the ra-
tio of these two lengths; the depth of the sen-
tence’s parse tree (Klein and Manning, 2003), the
depth of its verb phrase’s subtree, and the ra-
tio of these two depths. We also use the aver-
age word length for the sentence and the syntactic
complexity formula proposed by Botel and Gra-
nowsky (1972), which scores sentences on spe-
cific structures, such as passives, appositives, and
clausal subjects. Finally, we use the formality and
complexity dictionaries described in Pavlick and
Nenkova (2015), which provide human formal-
ity judgments for 7,794 words and short phrases
and complexity judgments for 5,699 words and
phrases. We score each sentence by averaging
across all words and phrases in the sentence.
Semantics. As the MRE is surprising and
shocking, we expect it to be dissimilar from the
surrounding sentences; we use semantic similarity
to surrounding sentences as a measure of shock.
Our semantic scores are the bag-of-words cosine
and the latent semantic similarity scores for adja-
cent sentences (Guo and Diab, 2012).
Affect. A change in affect reflects a change
in style, and we expect the MRE to occur at an
emotional peak. We use the Dictionary of Af-
fect in Language (DAL) (Whissell, 1989), aug-
mented with WordNet for coverage (Miller, 1995).
The DAL represents lexical affect with three
scores: evaluation (ee, hereafter ‘pleasantness’ to
avoid confusion with Labov’s Evaluation), activa-
tion (aa, activeness), and imagery (ii, concrete-
ness). We also use a fourth score, the activation-
evaluation (AE) norm, a measure of subjectivity
defined by Agarwal et al. (2009):
</bodyText>
<equation confidence="0.853076">
√
ee2 + aa2
ii
</equation>
<bodyText confidence="0.999851428571429">
For each of these four word-level scores, we cal-
culate a sentence-level score by averaging across
the words in the sentence using the finite state ma-
chine described by Agarwal et al. We expect the
sentences surrounding an MRE to be more sub-
jective and emotional as the impact of the MRE
becomes clear. We also expect a build-up in ac-
tiveness and intensity, peaking at the MRE.
To model change over the course of a narrative,
we look for changes in the syntactic, semantic, and
affectual scores. To illustrate this, Figure 1 shows
the activeness and pleasantness DAL scores for the
example narrative given in Section 1. We can see
how the MRE is the most exciting sentence in the
</bodyText>
<figureCaption confidence="0.999617">
Figure 1: Activeness and pleasantness scores.
</figureCaption>
<bodyText confidence="0.999942857142857">
story – global maximum in activation – as well as
the most horrifying – global minimum in pleasant-
ness. The overall shape of the activeness scores
reflects Prince’s three components of a minimal
story: low initial activation (starting state) and low
final activation (ending state) with a build up to a
peak at the MRE (change in state) between them.
</bodyText>
<sectionHeader confidence="0.999739" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999876714285714">
Using our Reddit dataset and change-based model
of narrative, we conducted two experiments on au-
tomatically identifying MREs. We compare our
results with three baselines: random, our labeling
heuristic, and the last sentence of the story ( best-
performing heuristic not used in labeling).
As described in Section 3.2, we labeled our
training set in blocks of three consecutive MREs,
centered on the sentence from each narrative that
was selected by our heuristics. To account for this,
in our experiments and baselines, we predicted the
presence of an MRE in a three-sentence block. In
testing, we considered a predicted block to be cor-
rect if it contained at least one gold-label MRE.
</bodyText>
<subsectionHeader confidence="0.838169">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.9998614">
Change-based Features. For each of the fif-
teen metrics in Section 4, shown in Table 3, we
first smooth the scores by applying a Gaussian fil-
ter. We also tried weighted and exponential mov-
ing averages, as well as a Hamming window, but
the Gaussian performed best in experiments on
our tuning set. We then generate 11 features for
each sentence: the metric score at the sentence;
whether or not the sentence is a local maximum or
minimum; the sentence’s distance from the global
</bodyText>
<figure confidence="0.9971399375">
0.6
0.4
Activeness
Pleasantness
MRE
0.2
0.0
0.2
0.4
DAL Score
0.6
0.8
1.0
1 2 3 4 5 6
Sentence Index
norm =
</figure>
<page confidence="0.94372">
2153
</page>
<bodyText confidence="0.61140825">
Type Metric Names
sentlength, vplength, lengthratio,
sentdepth, vpdepth, depthratio
wordlength, structcomplexity,
wordformality, wordcomplexity
Semantic cossimilarity, lssimilarity
Affectual pleasantness, activation,
imagery, subjectivity
</bodyText>
<tableCaption confidence="0.998923">
Table 3: The fifteen metrics for change.
</tableCaption>
<table confidence="0.8932883">
γ = 0.001, chosen using grid search on our tuning
set (Pedregosa et al., 2011).
Trial Precision Recall F-Score
Last sent. baseline 0.208 0.112 0.146
Heuristic baseline 0.107 0.333 0.162
No change* 0.146 0.378 0.211
Random baseline 0.185 0.586 0.281
Change only* 0.351 0.685 0.466
All features* 0.398 0.745 0.519
Syntactic
</table>
<tableCaption confidence="0.997894">
Table 4: Distant supervision results (*p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.937708555555556">
maximum and minimum; the difference in score
between the sentence and the preceding sentence,
the difference between the sentence and the fol-
lowing sentence, and the average of these differ-
ences (approximating the incoming, outgoing, and
self- slopes for the metric); and the incoming,
outgoing, and self- differences of differences (ap-
proximating the second derivative).
Other Features.
</bodyText>
<listItem confidence="0.728614">
• The tense of the main verb and whether or not
there is a shift from the previous sentence.
Labov (2013) suggests a shift between the
past and the historical present near the MRE.
• The position of the sentence in the narrative.
• The bag-of-words cosine similarity and latent
semantic similarity between the sentence and
the first and second sentences in the narra-
tive. The MRE usually appears near the end
of a story, but Labov (2013) notes that the
Abstract, a short introduction that occurs in
some narratives, often refers to the MRE.
</listItem>
<subsectionHeader confidence="0.997865">
5.2 Distant Supervision
</subsectionHeader>
<bodyText confidence="0.999990283018868">
Our first experiment used a distant supervision ap-
proach with our automatically-labeled training set.
Distant supervision has previously been applied
to NLP problems such as sentiment analysis (Go
et al., 2009; Purver and Battersby, 2012; Suttles
and Ide, 2013) and relation extraction (Mintz et
al., 2009; Yao et al., 2010; Hoffmann et al., 2011;
Nguyen and Moschitti, 2011; Krause et al., 2012;
Min et al., 2013; Xu et al., 2013).
We classify blocks of three sentences as con-
taining the MRE or not. The two classes, MRE
and not-MRE, were weighted inversely to their fre-
quencies in the weakly-labeled set, and all features
were normalized to the range [0, 1]. We trained an
SVM with margin C = 1 and an RBF kernel with
The results of the distant supervision experi-
ment are shown in Table 4. Our best results use
all features, but, notably, using the change-based
features alone achieves significant improvement
over the three baselines (p &lt; 0.00005). The ‘no
change’ trial used the metric scores themselves
and the ‘other’ features but none of the change-
based features, such as slopes and proximity to
global extremes. This feature set was outper-
formed by the random baseline (p &lt; 0.0024), sup-
porting our hypothesis that it is change in a metric,
rather than the score itself, that predicts MREs.
Because we used an non-linear kernel, we were
not able to examine feature weights directly. In-
stead, Table 5 shows the results of a logistic re-
gression model trained on our features. The 10
best features are shown, along with their weights
and 95% confidence intervals. From feature 8, we
see that the MRE is found in sentences near the
narrative’s global minimum in imagery (the Eval-
uation), but feature 1 indicates that sentences con-
taining the MRE show a sharp increase in imagery
compared to the previous sentences. The MRE is
described in a burst of vivid language, followed by
more abstract author opinions .
Features 2 and 9 indicate that the MRE tends to
be described using informal language – a textual
echo to Labov’s observation that the subjects of his
sociolinguistic interviews spoke less formally and
more colloquially as they relived the climaxes of
their stories (Labov, 2013). Feature 3 suggests that
sentences containing the MRE are similar to the
surrounding sentences. While we expected MRE
sentences to be different from their neighbors due
to the unusual and shocking nature of the MRE,
this feature seems instead to reinforce the idea that
MREs tend to described over the course of multi-
ple, consecutive sentences, rather than in a single
</bodyText>
<page confidence="0.988936">
2154
</page>
<bodyText confidence="0.83163">
Feature Name Weight Confidence Interval
</bodyText>
<listItem confidence="0.9910073">
1. incomingd2 imagery 4.174 (4.062, 4.287)
2. distancefrommin wordformality neg 4.109 (3.952, 4.265)
3. cossimilarity adjacent 3.618 (3.425, 3.812)
4. distancefrommin activeness 3.377 (2.855, 3.298)
5. sentdepth 3.364 (3.138, 3.590)
6. distancefrommin wordlength neg 3.321 (3.018, 3.624)
7. distancefrommin vpdepth 3.034 (2.823, 3.247)
8. distancefrommin imagery neg 2.790 (2.524, 3.056)
9. wordformality neg 2.329 (2.226, 2.432)
10. incomingd2 vplen 2.128 (1.938, 2.318)
</listItem>
<tableCaption confidence="0.990617">
Table 5: Top 10 features.
</tableCaption>
<bodyText confidence="0.999557777777778">
sentence. From feature 4, we see, as expected, that
the MRE is far from the narrative’s global mini-
mum in activeness, as it is the end of a chain of
events, far away from the stative Orientation.
Finally, features 5 and 10 suggest that MRE sen-
tences are not only long, but much longer than the
preceding sentences, and feature 6 indicates that
MRE sentences are close to the global minimum in
average word length. Shorter average word length
is expected, as an indicator of both informal word
choice and emphasis. Long sentences, however,
suggest a domain difference between our work
on text and Labov’s work on transcribed speech.
Looking over our development set, we find that
many authors combine the description of the MRE
with evaluative material in a single sentence, re-
sulting in a longer and more syntactically complex
MRE sentence than is found in Labov’s data.
</bodyText>
<subsectionHeader confidence="0.999925">
5.3 Self-Training
</subsectionHeader>
<bodyText confidence="0.99986795">
Our second experiment used a self-training ap-
proach, where a classifier uses a small, labeled
seed set to label a larger training set. Self-training
has been applied to parsing (McClosky et al.,
2006; Reichart and Rappoport, 2007; McClosky
and Charniak, 2008; Huang and Harper, 2009;
Sagae, 2010) and word sense disambiguation (Mi-
halcea, 2004). With the same parameters as in
the distant supervision experiment, we trained an
SVM on our hand-labeled seed set of 958 sen-
tences. We used this initial model to relabel the
training set. All sentences where this labeling
agreed with our automatically-generated heuristic
labels were added to the seed set and used to train a
new model, which was in turn used to label the re-
maining sentences, and so on until none of the cur-
rent model’s labels agreed with any of the remain-
ing heuristic labels. Figure 2 shows the learning
curve for the self-training experiment, along with
the growth of the self-training set.
</bodyText>
<figureCaption confidence="0.997408">
Figure 2: Learning and training set size curves.
</figureCaption>
<bodyText confidence="0.99909">
The results of the self-training experiment are
shown in Table 6. We achieve the best perfor-
mance, f1 = 0.635, after 9 rounds of self-training.
Self-training terminated after 10 rounds, but the
10th round had no effect on performance.
</bodyText>
<table confidence="0.9995196">
Trial Precision Recall F-Score
Random baseline 0.185 0.586 0.281
Seed only* 0.374 0.617 0.466
Dist. supervision* 0.398 0.745 0.519
Self-training* 0.478 0.946 0.635
</table>
<tableCaption confidence="0.989658">
Table 6: Best self-training results (*p &lt; 0.01).
</tableCaption>
<bodyText confidence="0.6906105">
The initial model, trained only on the seed set,
performed nearly as well as our distant supervi-
</bodyText>
<figure confidence="0.997775043478261">
0.64
0.62
50
0.60
0.58
0.56
30
0.54
0.52
0.50
10
Performance
Training Size
0
0 2 4 6 8 10
Training Round
F-Score
Sentences (thousands)
0.48
0.46
40
60
20
</figure>
<page confidence="0.99487">
2155
</page>
<bodyText confidence="0.999980727272727">
sion experiment. This illustrates that quantity of
data does not overcome the use of accurate man-
ual labels on a small dataset. As described in Sec-
tion 3.2, the distant supervision labels were based
on a linear combination of three heuristics that
achieved at best an RMSE of 5.1 sentences. How-
ever, with self-training, we can exploit the noisy
heuristic labels by using only those labels that
agree with the seed-trained model, thus reducing
the amount of noise. 52,147 of the 67,954 weakly-
labeled sentences were used in self-training.
</bodyText>
<sectionHeader confidence="0.996134" genericHeader="evaluation">
6 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999990310344828">
Identifying MREs is a hard problem. A human
annotator can rely on world knowledge to find the
most shocking and impactful event in a story, but
we do not have access to that knowledge. Addi-
tionally, MREs are rare, comprising 15% of the
sentences in our hand-annotated datasets. MREs
comprise just over 16% of our weakly-labeled
training set, but as we discuss below, there is too
much noise in the automatically-generated labels.
Despite the difficulty of the task, our ex-
periments show that our change-based model
of narrative is effective for identifying MREs,
and this model provides evidence supporting the
change-in-state view of narrative suggested by
Prince (1973), Polanyi (1985), and Labov (1997).
We achieve high recall with self-training (95%),
but precision is low across the board. This sug-
gests that, while MREs do occur at extremes
in syntactic complexity, semantic similarity, and
emotional activation, there may be many non-
MRE local extremes throughout a narrative.
Examining our results, we find a few common
sources of error. False positive sentences tend to
have high imagery and activeness. In Table 5, we
saw that imagery and activeness alone do not indi-
cate the presence of the MRE. An MRE sentence
is not just active; it is separated from the stative in-
troduction by the other events of the story. Nor is
it enough for a sentence to have high imagery; the
MRE is more vividly described than the preceding
events – we see again the importance of change
in our model of narrative. False negatives tend to
have high scores in syntactic complexity and for-
mality. As low formality was one of our stronger
predictors of MRE sentences, we may need to ad-
just these features in future work.
We also hope to refine our automatically-
generated labels in future work. Our self-training
experiment showed that 27% of our automatically-
generated labels were too noisy to use. We also
hope to improve our filters for automatically dis-
carding non-story text. We currently reject texts
shorter than three sentences, based on Prince’s
three-part definition of a story. In spite of this fil-
tering, 7% of our 500 manually-labeled texts were
identified as non-stories by our annotator. Extrap-
olating to our training set, we suspect that over 300
of our training ‘narratives’ are not narratives at all.
Finally, we hope to explore other theories of
narrative analysis that could suggest new ways
to quantify change in narrative. Prince, Polanyi,
and Labov propose a high-level view of personal
narrative: stories are centered around reportable
events that cause a change in state for the author.
This work tested fifteen surface-level features that
reflect this change in state. Are there others? Or
is a deeper semantic understanding of the starting
and ending states of stories required?
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999595206896552">
We have described a new model of narrative
based on Prince (1973), Polanyi (1985), and
Labov (1997). Our model tracks story charac-
terstics over the course of a narrative, capturing
change in complexity, meaning, and emotion.
We have created a corpus of 4,896 personal nar-
ratives, taking advantage of AskReddit, a com-
munity where members often prompt each other
for stories. Our experiments on this corpus show
that our change-based model is able to identify
MREs. They also demonstrate that large quanti-
ties of hand-labeled data are not required for this
task. Our distant supervision and self-training
approaches successfully use data weakly labeled
using heuristic rules that leverage the comment
thread structure of Reddit content. We believe
these Reddit stories are representative of the short,
personal narratives found online in blogs or dis-
cussion forums, and so this work should be use-
ful for finding MREs in a variety of online per-
sonal narratives. The one difference between this
data and stories from other online sources is the
prompt. A personal narrative posted to someone’s
personal blog is unlikely to have a prompt. We
use the prompt for our heuristic labeling, so our
automatic labels on non-Reddit data may be nois-
ier, but many blog posts also have titles or tags that
may be just as useful.
Identifying MREs is a hard problem that has not
</bodyText>
<page confidence="0.972743">
2156
</page>
<bodyText confidence="0.999931714285714">
previously been addressed in work on computa-
tional narrative. We have shown that the high-level
view proposed by linguistic theories of narrative –
that stories are about change – holds true. Mea-
suring change over the course of a narrative yields
better results than other features and baselines.
Why do we care about MREs? Polanyi (1976)
asserts that “one does not produce a narrative text
for no reason at all.” The Most Reportable Event is
that reason. It is the point of the story; the shortest
possible summary; the answer to the question, “So
what?”. It could be used to generate titles or sum-
maries to be used in organizing stories for readers
to browse, or it could be used in recommendation
systems to help readers find related stories. In fu-
ture work we hope to be able to generate a text
description the full MRE, which would be better
suited to summarization or generating headlines,
rather than identifying sentences that refer to it.
We hope this work will encourage others to fur-
ther investigate the Most Reportable Event.
</bodyText>
<sectionHeader confidence="0.998389" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9954755">
This work was partially supported by NSF Con-
tract No. IIS-1422863.
</bodyText>
<sectionHeader confidence="0.998564" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993401025974026">
Apoorv Agarwal, Fadi Biadsy, and Kathleen McKe-
own. 2009. Contextual phrase-level polarity anal-
ysis using lexical affect scoring and syntactic n-
grams. Proceedings of the 12th Conference of the
European Chapter of the Association for Computa-
tional Linguistics.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Langauge Processing with Python.
O’Reilly Media Inc., Sebastopol, CA.
Morton Botel and Alvin Granowsky. 1972. A formula
for measuring syntactic complexity: A directional
effort. Elementary English.
Eric Gilbert. 2013. Widespread underprovision on red-
dit. Proceedings of the 2013 conference on Com-
puter supported cooperative work.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford.
Andrew S. Gordon, Qun Cao, and Reid Swanson.
2007. Automated Story Capture From Internet We-
blogs. Proceedings of the Fourth International Con-
ference on Knowledge Capture.
Andrew S. Gordon and Reid Swanson 2009. Identify-
ing Personal Stories in Millions of Weblog Entries.
Third International Conference on Weblogs and So-
cial Media, Data Challenge Workshop.
Weiwei Guo and Mona Diab. 2012. Modeling sen-
tences in the latent space. Proceedings of the 50th
Annual Meeting of the Association for Computa-
tional Linguistics: Long Papers – Volume 1
Raphael Hoffmann, Congle Zhang, Xiao Ling,
Luke Zettlemoyer, and Daniel S. Weld. 2011.
Knowledge-based weak supervision for informa-
tion extraction of overlapping relations. Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies-Volume 1.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2.
Eric Jones, Travis Oliphant, Pearu Peterson, and oth-
ers. 2001. SciPy: Open source scientific tools for
Python. http://www.scipy.org/ Online; accessed 26
Jan. 2015.
Dan Klein and Christopher Manning. 2003. Accu-
rate Unlexicalized Parsing. Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics.
Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu
Xu. 2012. Large-scale learning of relation-
extraction rules with distant supervision from the
web. The Semantic WebISWC 2012, 263-278.
Springer Berlin Heidelberg, Berlin.
William Labov. 1997. Some further steps in narra-
tive analysis. Journal of Narrative and Life History,
7:395-415.
William Labov. 2013. The Language of Life and
Death. Cambridge University Press, Cambridge,
UK.
William Labov and Joshua Waletzky. 1967. Narra-
tive Analysis: Oral Versions of Personal Experience.
Essays on the Verbal and Visual Arts, 12-44. June
Helm (Ed.). University of Washington Press, Seat-
tle, WA.
Annie Louis and Ani Nenkova. 2013. What Makes
Writing Great? First Experiments on Article Qual-
ity Prediction in the Science Journalism Domain.
Transactions of ACL.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. Pro-
ceedings of the main conference on human language
technology conference of the North American Chap-
ter of the Association of Computational Linguistics.
David McClosky and Eugene Charniak 2008. Self-
training for biomedical parsing. Proceedings of the
</reference>
<page confidence="0.832891">
2157
</page>
<reference confidence="0.999764896226415">
46th Annual Meeting of the Association for Compu-
tational Linguistics on Human Language Technolo-
gies: Short Papers.
Neil McIntyre and Mirella Lapata. 2009. Learning to
Tell Tales: A Data-driven Approach to Story Gen-
eration. Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP.
Rada Mihalcea. 2004. Co-training and self-training
for word sense disambiguation. Proceedings of the
Conference on Computational Natural Language
Learning (CoNLL-2004).
George Miller. 1995. WordNet: A Lexical Database
for English. Communications of the ACM Vol. 38.
Bonan Min, Ralph Grishman, Li Wan, Chang Wang,
and David Gondek. 2013. Distant Supervision for
Relation Extraction with an Incomplete Knowledge
Base. Proceedings of NAACL-HLT 2013.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP.
Truc-Vien T. Nguyen and Alessandro Moschitti. 2011.
End-to-end relation extraction using distant supervi-
sion from external semantic repositories. Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies: short papers.
Jessica Ouyang and Kathleen McKeown. 2014. To-
wards automatic detection of narrative structure.
Proceedings of the Ninth International Conference
on Language Resources and Evaluation (LREC’14).
Ellie Pavlick and Ani Nenkova. 2015. Inducing Lex-
ical Style Properties for Paraphrase and Genre Dif-
ferentiation. Proceedings of NAACL-HLT 2015.
Fabian Pedregosa, Gal Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel et al. 2011. Scikit-learn:
Machine learning in Python. The Journal of Ma-
chine Learning Research, 12: 2825-2830.
Livia Polanyi. 1976. Why the Whats are When: Mu-
tually Contextualizing Realms of Narratives. Pro-
ceedings of the second Annual Meeting of the Berke-
ley Linguistic Society.
Livia Polanyi. 1985. Telling the American story :
a structural and cultural analysis of conversational
storytelling Ablex Publishing, Norwood, NJ.
Gerald Prince. 1973. A Grammar of Stories: An Intro-
duction. Mouton, The Hague.
Matthew Purver and Stuart Battersby. 2012. Experi-
menting with distant supervision for emotion classi-
fication. Proceedings of the 13th Conference of the
European Chapter of the Association for Computa-
tional Linguistics.
Elahe Rahimtoroghi, Reid Swanson, Marilyn A.
Walker, and Thomas Corcoran. 2013. Evaluation,
Orientation, and Action in Interactive StoryTelling.
Proceedings of Intelligent Narrative Technologies 6.
Roi Reichart and Ari Rappoport. 2007. Self-training
for enhancement and domain adaptation of statisti-
cal parsers trained on small datasets. Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics.
Kenji Sagae. 2010. Self-training without reranking for
parser domain adaptation and its impact on semantic
role labeling. Proceedings of the 2010 Workshop on
Domain Adaptation for Natural Language Process-
ing.
Philipp Singer, Fabian Flck, Clemens Meinhart, Elias
Zeitfogel, and Markus Strohmaier. 2014. Evolu-
tion of Reddit: From the Front Page of the Internet
to a Self-referential Community? Proceedings of
the companion publication of the 23rd international
conference on World wide web companion.
Jared Suttles and Nancy Ide. 2013. Distant supervision
for emotion classification with discrete binary val-
ues. Computational Linguistics and Intelligent Text
Processing, 121-136. Springer Berlin Heidelberg,
Berlin.
Reid Swanson, Elahe Rahimtoroghi, Thomas Corcoran
and Marilyn A. Walker. 2014. Identifying Narrative
Clause Types in Personal Stories. Proceedings of the
15th Annual Meeting of the Special Interest Group
on Discourse and Dialogue (SIGDIAL).
Tim Weninger, Xihao Avi Zhu, and Jiawei Han. 2013.
An Exploration of Discussion Threads in Social
News Sites: A Case Study of the Reddit Commu-
nity. 2013 IEEE/ACM International Conference on
Advances in Social Networks Analysis and Mining
(ASONAM).
Cynthia Whissell. 1989. The dictionary of affect in
language. Emotion: Theory, research, and experi-
ence, 4:113-131. Academic Press, London.
Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Gr-
ishman. 2013. Filling Knowledge Base Gaps for
Distant Supervision of Relation Extraction. Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing.
</reference>
<page confidence="0.994502">
2158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.985273">
<title confidence="0.999967">Modeling Reportable Events as Turning Points in Narrative</title>
<author confidence="0.999982">Jessica Ouyang</author>
<affiliation confidence="0.999892">Department of Computer Science Columbia University</affiliation>
<address confidence="0.999475">New York, NY 10027</address>
<email confidence="0.996695">ouyangj@cs.columbia.edu</email>
<author confidence="0.999971">Kathleen McKeown</author>
<affiliation confidence="0.999902">Department of Computer Science Columbia University</affiliation>
<address confidence="0.999485">New York, NY 10027</address>
<email confidence="0.999617">kathy@cs.columbia.edu</email>
<abstract confidence="0.999515047619048">We present novel experiments in modeling the rise and fall of story characteristics narrative, leading up to the Event the compelling event that is the nucleus of the story. We construct a corpus of personal narratives from the bulletin board website Reddit, using the organization of Reddit content into topic-specific communities to automatically identify narratives. Leveraging the structure of Reddit comment threads, we automatically label a large dataset of narratives. We present a change-based model of narrative that tracks changes in formality, affect, and other characteristics over the course of a story, and we use this model in distant supervision and selftraining experiments that achieve significant improvements over the baselines at the task of identifying MREs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Fadi Biadsy</author>
<author>Kathleen McKeown</author>
</authors>
<title>Contextual phrase-level polarity analysis using lexical affect scoring and syntactic ngrams.</title>
<date>2009</date>
<booktitle>Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18870" citStr="Agarwal et al. (2009)" startWordPosition="3140" endWordPosition="3143">emantic similarity scores for adjacent sentences (Guo and Diab, 2012). Affect. A change in affect reflects a change in style, and we expect the MRE to occur at an emotional peak. We use the Dictionary of Affect in Language (DAL) (Whissell, 1989), augmented with WordNet for coverage (Miller, 1995). The DAL represents lexical affect with three scores: evaluation (ee, hereafter ‘pleasantness’ to avoid confusion with Labov’s Evaluation), activation (aa, activeness), and imagery (ii, concreteness). We also use a fourth score, the activationevaluation (AE) norm, a measure of subjectivity defined by Agarwal et al. (2009): √ ee2 + aa2 ii For each of these four word-level scores, we calculate a sentence-level score by averaging across the words in the sentence using the finite state machine described by Agarwal et al. We expect the sentences surrounding an MRE to be more subjective and emotional as the impact of the MRE becomes clear. We also expect a build-up in activeness and intensity, peaking at the MRE. To model change over the course of a narrative, we look for changes in the syntactic, semantic, and affectual scores. To illustrate this, Figure 1 shows the activeness and pleasantness DAL scores for the ex</context>
</contexts>
<marker>Agarwal, Biadsy, McKeown, 2009</marker>
<rawString>Apoorv Agarwal, Fadi Biadsy, and Kathleen McKeown. 2009. Contextual phrase-level polarity analysis using lexical affect scoring and syntactic ngrams. Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Ewan Klein</author>
</authors>
<date>2009</date>
<booktitle>Natural Langauge Processing with Python. O’Reilly Media Inc.,</booktitle>
<location>Sebastopol, CA.</location>
<contexts>
<context position="9672" citStr="Bird et al., 2009" startWordPosition="1583" endWordPosition="1586">would be too different from that used in posts without the tag. Another 3 posts did not contain personal narratives, and instead were about fictional stories in movies or music. With the 37 remaining posts, we treated each top-level comment (those that replied directly to the posted question) as a story. The example given in Section 1 is one such story. We collected 6,000 top-level comments and discarded those without comment threads replying to them. As we discuss in Section 3.2, we use comment threads to automatically label our training data. We tokenized the top-level comments by sentence (Bird et al., 2009) and removed all sentences following any variation of the word ‘EDIT’, as these were usually responses to readers’ comments. We discarded texts with fewer than three sentences, based on Prince’s definition of a minimal story as consisting of a starting state, an event, and an ending state. We are left with 4,896 stories, with an average length of 16 sentences and a maxiumum of 198. 3.2 Labeling We partitioned our data into development, seed, and tuning sets of 100 stories each; a testing set of 200 stories; and a training set of 4,178 stories. The development, seed, tuning, and testing sets we</context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Steven Bird, Edward Loper, and Ewan Klein. 2009. Natural Langauge Processing with Python. O’Reilly Media Inc., Sebastopol, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morton Botel</author>
<author>Alvin Granowsky</author>
</authors>
<title>A formula for measuring syntactic complexity: A directional effort. Elementary English.</title>
<date>1972</date>
<contexts>
<context position="17587" citStr="Botel and Granowsky (1972)" startWordPosition="2937" endWordPosition="2941"> including metrics of sentential syntax; we use the syntactic complexity of a sentence as an approximation for formality. The complexity of a sentence also reflects emphasis – short, staccato sentences bear more emphasis than long, complicated ones. We use the length of the sen2152 tence, the length of its verb phrase, and the ratio of these two lengths; the depth of the sentence’s parse tree (Klein and Manning, 2003), the depth of its verb phrase’s subtree, and the ratio of these two depths. We also use the average word length for the sentence and the syntactic complexity formula proposed by Botel and Granowsky (1972), which scores sentences on specific structures, such as passives, appositives, and clausal subjects. Finally, we use the formality and complexity dictionaries described in Pavlick and Nenkova (2015), which provide human formality judgments for 7,794 words and short phrases and complexity judgments for 5,699 words and phrases. We score each sentence by averaging across all words and phrases in the sentence. Semantics. As the MRE is surprising and shocking, we expect it to be dissimilar from the surrounding sentences; we use semantic similarity to surrounding sentences as a measure of shock. Ou</context>
</contexts>
<marker>Botel, Granowsky, 1972</marker>
<rawString>Morton Botel and Alvin Granowsky. 1972. A formula for measuring syntactic complexity: A directional effort. Elementary English.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Gilbert</author>
</authors>
<title>Widespread underprovision on reddit.</title>
<date>2013</date>
<booktitle>Proceedings of the 2013 conference on Computer supported cooperative work.</booktitle>
<contexts>
<context position="8067" citStr="Gilbert (2013)" startWordPosition="1324" endWordPosition="1325">ram features to classify posts as either stories or not, achieving 75% precision. They note that only about 17% of weblog text consists of stories. In contrast to the relatively small datasets used by Rahimtoroghi et al., Ouyang and McKeown, and Swanson et al., we use a larger dataset automatically collected from Reddit. Our collection method achieved 94% precision in identifying narratives. A number of researchers have characterized the structure and use of Reddit, currently the 26th most popular website in the world1. Weninger et al. (2013) described the structure of Reddit comment threads. Gilbert (2013) measured user participation in the voting process that ranks Reddit content. Singer et al. (2014) conducted a longitudinal study of the Reddit user community, finding a trend favoring original, user-generated content. 3 Data 3.1 Collection We collected data from the AskReddit subreddit, where users post questions for other members of the community, who reply with comments answering the questions. Table 1 shows some examples 1http://www.alexa.com/siteinfo/reddit.com 2150 of these posts, and we can see some of the wide variety of story topics found on AskReddit. Post Title Whats your creepiest </context>
</contexts>
<marker>Gilbert, 2013</marker>
<rawString>Eric Gilbert. 2013. Widespread underprovision on reddit. Proceedings of the 2013 conference on Computer supported cooperative work.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<location>Stanford.</location>
<contexts>
<context position="23079" citStr="Go et al., 2009" startWordPosition="3830" endWordPosition="3833">present near the MRE. • The position of the sentence in the narrative. • The bag-of-words cosine similarity and latent semantic similarity between the sentence and the first and second sentences in the narrative. The MRE usually appears near the end of a story, but Labov (2013) notes that the Abstract, a short introduction that occurs in some narratives, often refers to the MRE. 5.2 Distant Supervision Our first experiment used a distant supervision approach with our automatically-labeled training set. Distant supervision has previously been applied to NLP problems such as sentiment analysis (Go et al., 2009; Purver and Battersby, 2012; Suttles and Ide, 2013) and relation extraction (Mintz et al., 2009; Yao et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013; Xu et al., 2013). We classify blocks of three sentences as containing the MRE or not. The two classes, MRE and not-MRE, were weighted inversely to their frequencies in the weakly-labeled set, and all features were normalized to the range [0, 1]. We trained an SVM with margin C = 1 and an RBF kernel with The results of the distant supervision experiment are shown in Table 4. Our best results </context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew S Gordon</author>
<author>Qun Cao</author>
<author>Reid Swanson</author>
</authors>
<title>Automated Story Capture From Internet Weblogs.</title>
<date>2007</date>
<booktitle>Proceedings of the Fourth International Conference on Knowledge Capture.</booktitle>
<contexts>
<context position="11422" citStr="Gordon et al. (2007)" startWordPosition="1881" endWordPosition="1884">or (also a native English speaker and not one of the authors) label MREs in the 100 narratives in our development set. We found substantial agreement (Cohen’s κ = 0.729); the two classes, MRE and not-MRE, are highly unbalanced, so percent agreement between the two annotators was extremely high (95%). In addition to labeling the MREs, our first annotator identified and discarded 31 texts that were not true stories, but rather Reddit-specific inside jokes or comments on how cool the stories in the thread were. From this, we can see that the precision of our story collection method is very high. Gordon et al. (2007) found that stories were 17% of the weblog text that they collected; of the 500 texts given to our annotators, 94% were stories. Using the development set, we experimented with seven heuristics, defined below, for automatically labeling the training set. Each predicts a sentence index sh to be the index of an MRE. We measured the performance of each heuristic using root-mean-square error (RMSE), which measures the standard deviation of how far the heuristic’s predictions fall from a true MRE. Let N = number of narratives sMRE = index of a true MRE (sMREi − shi)2 We used a linear combination of</context>
</contexts>
<marker>Gordon, Cao, Swanson, 2007</marker>
<rawString>Andrew S. Gordon, Qun Cao, and Reid Swanson. 2007. Automated Story Capture From Internet Weblogs. Proceedings of the Fourth International Conference on Knowledge Capture.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew S Gordon</author>
<author>Reid Swanson</author>
</authors>
<title>Identifying Personal Stories in Millions of Weblog Entries.</title>
<date>2009</date>
<booktitle>Third International Conference on Weblogs and Social Media, Data Challenge Workshop.</booktitle>
<contexts>
<context position="7423" citStr="Gordon and Swanson, 2009" startWordPosition="1218" endWordPosition="1221">is less difficult than our proposed task. The three labels, Orientation, Action, and Evaluation have distinct functions that are reflected in tense, mood, and a clause’s position in the narrative. The MRE is not a sentence or clause but an event that may be described or referred to by any sentence in a narrative; it is distinguished from the other events only by its surprisingness and emotional impact, dimensions that are difficult to model computationally without a deep semantic understanding of the story. The stories that Swanson et al. used were drawn from a corpus drawn from weblog posts (Gordon and Swanson, 2009). Gordon and Swanson used unigram features to classify posts as either stories or not, achieving 75% precision. They note that only about 17% of weblog text consists of stories. In contrast to the relatively small datasets used by Rahimtoroghi et al., Ouyang and McKeown, and Swanson et al., we use a larger dataset automatically collected from Reddit. Our collection method achieved 94% precision in identifying narratives. A number of researchers have characterized the structure and use of Reddit, currently the 26th most popular website in the world1. Weninger et al. (2013) described the structu</context>
</contexts>
<marker>Gordon, Swanson, 2009</marker>
<rawString>Andrew S. Gordon and Reid Swanson 2009. Identifying Personal Stories in Millions of Weblog Entries. Third International Conference on Weblogs and Social Media, Data Challenge Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers –</booktitle>
<volume>1</volume>
<contexts>
<context position="12822" citStr="Guo and Diab (2012)" startWordPosition="2124" endWordPosition="2127"> expect comments to refer to the MRE because of its shocking nature and importance to the story. This heuristic achieved RMSE of 5.5 sentences on the development set. Similarity to tl;dr. The latent semantic similarity between a sentence and the tl;dr. The tl;dr (too long; didn’t read) is a very short paraphrase of a post given by its author. They are relatively rare – 663 stories, or 14% of our data, had tl;drs. Since the MRE is the central event of the story, we expect it to be included in the tl;dr. We calculated the similarity using the weighted matrix factorization algorithm described by Guo and Diab (2012). This heuristic achieved RMSE of 5.8 sentences. 1 RMSE = N N Z=1 2151 In contrast, bag-of-words cosine similarity to the tl;dr performed poorly (RMSE of 13.2). This is due to the tl;dr being both short and a paraphrase of its story. There are few words in the tl;dr, and those words are often synonyms of, but not the same as, words in the story. Guo and Diab’s latent semantic similarity score addresses this word sparsity problem by modeling words that are not present in the input text. We also experimented with latent semantic similarity for the similarityto-comment and similarity-to-prompt he</context>
<context position="18318" citStr="Guo and Diab, 2012" startWordPosition="3051" endWordPosition="3054">e use the formality and complexity dictionaries described in Pavlick and Nenkova (2015), which provide human formality judgments for 7,794 words and short phrases and complexity judgments for 5,699 words and phrases. We score each sentence by averaging across all words and phrases in the sentence. Semantics. As the MRE is surprising and shocking, we expect it to be dissimilar from the surrounding sentences; we use semantic similarity to surrounding sentences as a measure of shock. Our semantic scores are the bag-of-words cosine and the latent semantic similarity scores for adjacent sentences (Guo and Diab, 2012). Affect. A change in affect reflects a change in style, and we expect the MRE to occur at an emotional peak. We use the Dictionary of Affect in Language (DAL) (Whissell, 1989), augmented with WordNet for coverage (Miller, 1995). The DAL represents lexical affect with three scores: evaluation (ee, hereafter ‘pleasantness’ to avoid confusion with Labov’s Evaluation), activation (aa, activeness), and imagery (ii, concreteness). We also use a fourth score, the activationevaluation (AE) norm, a measure of subjectivity defined by Agarwal et al. (2009): √ ee2 + aa2 ii For each of these four word-lev</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012. Modeling sentences in the latent space. Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers – Volume 1</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raphael Hoffmann</author>
<author>Congle Zhang</author>
<author>Xiao Ling</author>
<author>Luke Zettlemoyer</author>
<author>Daniel S Weld</author>
</authors>
<title>Knowledge-based weak supervision for information extraction of overlapping relations.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1.</booktitle>
<contexts>
<context position="23216" citStr="Hoffmann et al., 2011" startWordPosition="3853" endWordPosition="3856">ilarity between the sentence and the first and second sentences in the narrative. The MRE usually appears near the end of a story, but Labov (2013) notes that the Abstract, a short introduction that occurs in some narratives, often refers to the MRE. 5.2 Distant Supervision Our first experiment used a distant supervision approach with our automatically-labeled training set. Distant supervision has previously been applied to NLP problems such as sentiment analysis (Go et al., 2009; Purver and Battersby, 2012; Suttles and Ide, 2013) and relation extraction (Mintz et al., 2009; Yao et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013; Xu et al., 2013). We classify blocks of three sentences as containing the MRE or not. The two classes, MRE and not-MRE, were weighted inversely to their frequencies in the weakly-labeled set, and all features were normalized to the range [0, 1]. We trained an SVM with margin C = 1 and an RBF kernel with The results of the distant supervision experiment are shown in Table 4. Our best results use all features, but, notably, using the change-based features alone achieves significant improvement over the three baselines (p &lt; 0.00</context>
</contexts>
<marker>Hoffmann, Zhang, Ling, Zettlemoyer, Weld, 2011</marker>
<rawString>Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S. Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongqiang Huang</author>
<author>Mary Harper</author>
</authors>
<title>SelfTraining PCFG grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<contexts>
<context position="27145" citStr="Huang and Harper, 2009" startWordPosition="4500" endWordPosition="4503">domain difference between our work on text and Labov’s work on transcribed speech. Looking over our development set, we find that many authors combine the description of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 5.3 Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the current model’s labels agreed with any of the remaining heuristic labels. Figure 2 shows the learning curve for the</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Zhongqiang Huang and Mary Harper. 2009. SelfTraining PCFG grammars with latent annotations across languages. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Jones</author>
<author>Travis Oliphant</author>
<author>Pearu Peterson</author>
<author>others</author>
</authors>
<title>SciPy: Open source scientific tools for Python. http://www.scipy.org/ Online; accessed 26</title>
<date>2001</date>
<marker>Jones, Oliphant, Peterson, others, 2001</marker>
<rawString>Eric Jones, Travis Oliphant, Pearu Peterson, and others. 2001. SciPy: Open source scientific tools for Python. http://www.scipy.org/ Online; accessed 26 Jan. 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="17382" citStr="Klein and Manning, 2003" startWordPosition="2900" endWordPosition="2903">f change in the story. We score each sentence according to three views of narrative: syntax, semantics, and affect. Syntax. We model Polanyi’s claim that a change in formality marks the changing point by including metrics of sentential syntax; we use the syntactic complexity of a sentence as an approximation for formality. The complexity of a sentence also reflects emphasis – short, staccato sentences bear more emphasis than long, complicated ones. We use the length of the sen2152 tence, the length of its verb phrase, and the ratio of these two lengths; the depth of the sentence’s parse tree (Klein and Manning, 2003), the depth of its verb phrase’s subtree, and the ratio of these two depths. We also use the average word length for the sentence and the syntactic complexity formula proposed by Botel and Granowsky (1972), which scores sentences on specific structures, such as passives, appositives, and clausal subjects. Finally, we use the formality and complexity dictionaries described in Pavlick and Nenkova (2015), which provide human formality judgments for 7,794 words and short phrases and complexity judgments for 5,699 words and phrases. We score each sentence by averaging across all words and phrases i</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher Manning. 2003. Accurate Unlexicalized Parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Krause</author>
<author>Hong Li</author>
<author>Hans Uszkoreit</author>
<author>Feiyu Xu</author>
</authors>
<title>Large-scale learning of relationextraction rules with distant supervision from the web. The Semantic WebISWC</title>
<date>2012</date>
<pages>263--278</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg, Berlin.</location>
<contexts>
<context position="23265" citStr="Krause et al., 2012" startWordPosition="3861" endWordPosition="3864">nd sentences in the narrative. The MRE usually appears near the end of a story, but Labov (2013) notes that the Abstract, a short introduction that occurs in some narratives, often refers to the MRE. 5.2 Distant Supervision Our first experiment used a distant supervision approach with our automatically-labeled training set. Distant supervision has previously been applied to NLP problems such as sentiment analysis (Go et al., 2009; Purver and Battersby, 2012; Suttles and Ide, 2013) and relation extraction (Mintz et al., 2009; Yao et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013; Xu et al., 2013). We classify blocks of three sentences as containing the MRE or not. The two classes, MRE and not-MRE, were weighted inversely to their frequencies in the weakly-labeled set, and all features were normalized to the range [0, 1]. We trained an SVM with margin C = 1 and an RBF kernel with The results of the distant supervision experiment are shown in Table 4. Our best results use all features, but, notably, using the change-based features alone achieves significant improvement over the three baselines (p &lt; 0.00005). The ‘no change’ trial used the metric score</context>
</contexts>
<marker>Krause, Li, Uszkoreit, Xu, 2012</marker>
<rawString>Sebastian Krause, Hong Li, Hans Uszkoreit, and Feiyu Xu. 2012. Large-scale learning of relationextraction rules with distant supervision from the web. The Semantic WebISWC 2012, 263-278. Springer Berlin Heidelberg, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Labov</author>
</authors>
<title>Some further steps in narrative analysis.</title>
<date>1997</date>
<journal>Journal of Narrative and Life History,</journal>
<pages>7--395</pages>
<contexts>
<context position="29843" citStr="Labov (1997)" startWordPosition="4951" endWordPosition="4952"> most shocking and impactful event in a story, but we do not have access to that knowledge. Additionally, MREs are rare, comprising 15% of the sentences in our hand-annotated datasets. MREs comprise just over 16% of our weakly-labeled training set, but as we discuss below, there is too much noise in the automatically-generated labels. Despite the difficulty of the task, our experiments show that our change-based model of narrative is effective for identifying MREs, and this model provides evidence supporting the change-in-state view of narrative suggested by Prince (1973), Polanyi (1985), and Labov (1997). We achieve high recall with self-training (95%), but precision is low across the board. This suggests that, while MREs do occur at extremes in syntactic complexity, semantic similarity, and emotional activation, there may be many nonMRE local extremes throughout a narrative. Examining our results, we find a few common sources of error. False positive sentences tend to have high imagery and activeness. In Table 5, we saw that imagery and activeness alone do not indicate the presence of the MRE. An MRE sentence is not just active; it is separated from the stative introduction by the other even</context>
<context position="32040" citStr="Labov (1997)" startWordPosition="5317" endWordPosition="5318">ives at all. Finally, we hope to explore other theories of narrative analysis that could suggest new ways to quantify change in narrative. Prince, Polanyi, and Labov propose a high-level view of personal narrative: stories are centered around reportable events that cause a change in state for the author. This work tested fifteen surface-level features that reflect this change in state. Are there others? Or is a deeper semantic understanding of the starting and ending states of stories required? 7 Conclusion We have described a new model of narrative based on Prince (1973), Polanyi (1985), and Labov (1997). Our model tracks story characterstics over the course of a narrative, capturing change in complexity, meaning, and emotion. We have created a corpus of 4,896 personal narratives, taking advantage of AskReddit, a community where members often prompt each other for stories. Our experiments on this corpus show that our change-based model is able to identify MREs. They also demonstrate that large quantities of hand-labeled data are not required for this task. Our distant supervision and self-training approaches successfully use data weakly labeled using heuristic rules that leverage the comment </context>
</contexts>
<marker>Labov, 1997</marker>
<rawString>William Labov. 1997. Some further steps in narrative analysis. Journal of Narrative and Life History, 7:395-415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Labov</author>
</authors>
<title>The Language of Life and Death.</title>
<date>2013</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="5679" citStr="Labov (2013)" startWordPosition="925" endWordPosition="926"> labels of Labov and Waltezky (1967), Orientation (background information), Action (events), and Evaluation (author’s perspective), which we discuss in Section 4. Rahimtoroghi et al. used two annotators with high agreement and achieved accuracy and precision around 0.9 on all three labels, as well as recall above 0.9 on all but Orientation. They noted that their data set was very clean: interannotator agreement was nearly perfect, the language was simple, and each clause served a clear narrative purpose. Ouyang and McKeown (2014) explored identifying the Action chain of the oral narratives in Labov (2013). They used a dataset of 49 narratives (1,277 clauses), transcribed from recordings of speech and annotated by Labov and achieved 0.72 f-score on classifying clauses as Action or not. This task is easier than our proposed task of identifying sentences containing MREs. Actions account for nearly half the clauses in the Labov (2013) dataset, while there are only and average of 2.5 MRE sentences per story. Additionally, identifying Labov’s Actions is a problem of detecting causal and temporal relations among events; identifying the MRE is a problem of measuring how impactful and shocking an event</context>
<context position="15766" citStr="Labov (2013)" startWordPosition="2631" endWordPosition="2632">r as MREs. The average number of consecutive MREs was 2.5 sentences. To reflect this, we labeled our training set Number of Sentences Data Set Stories MRE Total dev* 99 169 1528 seed* 82 184 958 tuning* 95 212 1301 testing* 193 444 2771 training 4178 11205 67954 Table 2: Distribution of labels (*manual). in three-sentence blocks. The sentence selected by our labeling heuristic, along with the immediately preceding and following sentences, were all labeled as MREs. The result was the weaklylabeled training set in Table 2. 4 Modeling Narrative Our approach to modeling narrative is based on both Labov (2013) and Prince (1973). We claim that Labov’s MRE is Prince’s change of state with the added requirement of reportability or interestingness – in fact, all three components of Prince’s minimal story have equivalences in Labov. Labov and Waletzky (1967) proposed three components of narrative: the Orientation, which we equate with Prince’s starting state; the Action, the chain of events culminating in the MRE; and the Evaluation, the author’s perspective on the story. Labov (2013) adds three more components: the Resolution, equivalent to Prince’s ending state, and the Abstract and Coda, where the au</context>
<context position="22410" citStr="Labov (2013)" startWordPosition="3724" endWordPosition="3725">ge only* 0.351 0.685 0.466 All features* 0.398 0.745 0.519 Syntactic Table 4: Distant supervision results (*p &lt; 0.01). maximum and minimum; the difference in score between the sentence and the preceding sentence, the difference between the sentence and the following sentence, and the average of these differences (approximating the incoming, outgoing, and self- slopes for the metric); and the incoming, outgoing, and self- differences of differences (approximating the second derivative). Other Features. • The tense of the main verb and whether or not there is a shift from the previous sentence. Labov (2013) suggests a shift between the past and the historical present near the MRE. • The position of the sentence in the narrative. • The bag-of-words cosine similarity and latent semantic similarity between the sentence and the first and second sentences in the narrative. The MRE usually appears near the end of a story, but Labov (2013) notes that the Abstract, a short introduction that occurs in some narratives, often refers to the MRE. 5.2 Distant Supervision Our first experiment used a distant supervision approach with our automatically-labeled training set. Distant supervision has previously bee</context>
<context position="25061" citStr="Labov, 2013" startWordPosition="4172" endWordPosition="4173">e 8, we see that the MRE is found in sentences near the narrative’s global minimum in imagery (the Evaluation), but feature 1 indicates that sentences containing the MRE show a sharp increase in imagery compared to the previous sentences. The MRE is described in a burst of vivid language, followed by more abstract author opinions . Features 2 and 9 indicate that the MRE tends to be described using informal language – a textual echo to Labov’s observation that the subjects of his sociolinguistic interviews spoke less formally and more colloquially as they relived the climaxes of their stories (Labov, 2013). Feature 3 suggests that sentences containing the MRE are similar to the surrounding sentences. While we expected MRE sentences to be different from their neighbors due to the unusual and shocking nature of the MRE, this feature seems instead to reinforce the idea that MREs tend to described over the course of multiple, consecutive sentences, rather than in a single 2154 Feature Name Weight Confidence Interval 1. incomingd2 imagery 4.174 (4.062, 4.287) 2. distancefrommin wordformality neg 4.109 (3.952, 4.265) 3. cossimilarity adjacent 3.618 (3.425, 3.812) 4. distancefrommin activeness 3.377 (</context>
</contexts>
<marker>Labov, 2013</marker>
<rawString>William Labov. 2013. The Language of Life and Death. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Labov</author>
<author>Joshua Waletzky</author>
</authors>
<title>Narrative Analysis: Oral Versions of Personal Experience.</title>
<date>1967</date>
<booktitle>Essays on the Verbal and Visual Arts,</booktitle>
<pages>12--44</pages>
<institution>Helm (Ed.). University of Washington Press,</institution>
<location>Seattle, WA.</location>
<contexts>
<context position="16014" citStr="Labov and Waletzky (1967)" startWordPosition="2669" endWordPosition="2672">1 training 4178 11205 67954 Table 2: Distribution of labels (*manual). in three-sentence blocks. The sentence selected by our labeling heuristic, along with the immediately preceding and following sentences, were all labeled as MREs. The result was the weaklylabeled training set in Table 2. 4 Modeling Narrative Our approach to modeling narrative is based on both Labov (2013) and Prince (1973). We claim that Labov’s MRE is Prince’s change of state with the added requirement of reportability or interestingness – in fact, all three components of Prince’s minimal story have equivalences in Labov. Labov and Waletzky (1967) proposed three components of narrative: the Orientation, which we equate with Prince’s starting state; the Action, the chain of events culminating in the MRE; and the Evaluation, the author’s perspective on the story. Labov (2013) adds three more components: the Resolution, equivalent to Prince’s ending state, and the Abstract and Coda, where the author introduces and concludes the story. We focus on Prince’s claim that stories are about change. Polanyi (1985) observes that the turning point of a story is marked by a change in style, formality of language, or emphasis in the telling of the st</context>
</contexts>
<marker>Labov, Waletzky, 1967</marker>
<rawString>William Labov and Joshua Waletzky. 1967. Narrative Analysis: Oral Versions of Personal Experience. Essays on the Verbal and Visual Arts, 12-44. June Helm (Ed.). University of Washington Press, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Annie Louis</author>
<author>Ani Nenkova</author>
</authors>
<title>What Makes Writing Great? First Experiments on Article Quality Prediction in the Science Journalism Domain.</title>
<date>2013</date>
<journal>Transactions of ACL.</journal>
<marker>Louis, Nenkova, 2013</marker>
<rawString>Annie Louis and Ani Nenkova. 2013. What Makes Writing Great? First Experiments on Article Quality Prediction in the Science Journalism Domain. Transactions of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="27062" citStr="McClosky et al., 2006" startWordPosition="4488" endWordPosition="4491">tor of both informal word choice and emphasis. Long sentences, however, suggest a domain difference between our work on text and Labov’s work on transcribed speech. Looking over our development set, we find that many authors combine the description of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 5.3 Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the current model’s labels agreed wi</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
</authors>
<title>Selftraining for biomedical parsing.</title>
<date>2008</date>
<booktitle>Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers.</booktitle>
<contexts>
<context position="27121" citStr="McClosky and Charniak, 2008" startWordPosition="4496" endWordPosition="4499">entences, however, suggest a domain difference between our work on text and Labov’s work on transcribed speech. Looking over our development set, we find that many authors combine the description of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 5.3 Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the current model’s labels agreed with any of the remaining heuristic labels. Figure 2 shows th</context>
</contexts>
<marker>McClosky, Charniak, 2008</marker>
<rawString>David McClosky and Eugene Charniak 2008. Selftraining for biomedical parsing. Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Neil McIntyre</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning to Tell Tales: A Data-driven Approach to Story Generation.</title>
<date>2009</date>
<booktitle>Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<marker>McIntyre, Lapata, 2009</marker>
<rawString>Neil McIntyre and Mirella Lapata. 2009. Learning to Tell Tales: A Data-driven Approach to Story Generation. Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Co-training and self-training for word sense disambiguation.</title>
<date>2004</date>
<booktitle>Proceedings of the Conference on Computational Natural Language Learning (CoNLL-2004).</booktitle>
<contexts>
<context position="27206" citStr="Mihalcea, 2004" startWordPosition="4510" endWordPosition="4512">cribed speech. Looking over our development set, we find that many authors combine the description of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 5.3 Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the current model’s labels agreed with any of the remaining heuristic labels. Figure 2 shows the learning curve for the self-training experiment, along with the growth of the self-</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Rada Mihalcea. 2004. Co-training and self-training for word sense disambiguation. Proceedings of the Conference on Computational Natural Language Learning (CoNLL-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM</journal>
<volume>38</volume>
<contexts>
<context position="18546" citStr="Miller, 1995" startWordPosition="3094" endWordPosition="3095">tence by averaging across all words and phrases in the sentence. Semantics. As the MRE is surprising and shocking, we expect it to be dissimilar from the surrounding sentences; we use semantic similarity to surrounding sentences as a measure of shock. Our semantic scores are the bag-of-words cosine and the latent semantic similarity scores for adjacent sentences (Guo and Diab, 2012). Affect. A change in affect reflects a change in style, and we expect the MRE to occur at an emotional peak. We use the Dictionary of Affect in Language (DAL) (Whissell, 1989), augmented with WordNet for coverage (Miller, 1995). The DAL represents lexical affect with three scores: evaluation (ee, hereafter ‘pleasantness’ to avoid confusion with Labov’s Evaluation), activation (aa, activeness), and imagery (ii, concreteness). We also use a fourth score, the activationevaluation (AE) norm, a measure of subjectivity defined by Agarwal et al. (2009): √ ee2 + aa2 ii For each of these four word-level scores, we calculate a sentence-level score by averaging across the words in the sentence using the finite state machine described by Agarwal et al. We expect the sentences surrounding an MRE to be more subjective and emotion</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM Vol. 38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonan Min</author>
<author>Ralph Grishman</author>
<author>Li Wan</author>
<author>Chang Wang</author>
<author>David Gondek</author>
</authors>
<title>Distant Supervision for Relation Extraction with an Incomplete Knowledge Base.</title>
<date>2013</date>
<booktitle>Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="23283" citStr="Min et al., 2013" startWordPosition="3865" endWordPosition="3868">arrative. The MRE usually appears near the end of a story, but Labov (2013) notes that the Abstract, a short introduction that occurs in some narratives, often refers to the MRE. 5.2 Distant Supervision Our first experiment used a distant supervision approach with our automatically-labeled training set. Distant supervision has previously been applied to NLP problems such as sentiment analysis (Go et al., 2009; Purver and Battersby, 2012; Suttles and Ide, 2013) and relation extraction (Mintz et al., 2009; Yao et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013; Xu et al., 2013). We classify blocks of three sentences as containing the MRE or not. The two classes, MRE and not-MRE, were weighted inversely to their frequencies in the weakly-labeled set, and all features were normalized to the range [0, 1]. We trained an SVM with margin C = 1 and an RBF kernel with The results of the distant supervision experiment are shown in Table 4. Our best results use all features, but, notably, using the change-based features alone achieves significant improvement over the three baselines (p &lt; 0.00005). The ‘no change’ trial used the metric scores themselves and t</context>
</contexts>
<marker>Min, Grishman, Wan, Wang, Gondek, 2013</marker>
<rawString>Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. 2013. Distant Supervision for Relation Extraction with an Incomplete Knowledge Base. Proceedings of NAACL-HLT 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Mintz</author>
<author>Steven Bills</author>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data.</title>
<date>2009</date>
<booktitle>Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</booktitle>
<contexts>
<context position="23175" citStr="Mintz et al., 2009" startWordPosition="3845" endWordPosition="3848">ine similarity and latent semantic similarity between the sentence and the first and second sentences in the narrative. The MRE usually appears near the end of a story, but Labov (2013) notes that the Abstract, a short introduction that occurs in some narratives, often refers to the MRE. 5.2 Distant Supervision Our first experiment used a distant supervision approach with our automatically-labeled training set. Distant supervision has previously been applied to NLP problems such as sentiment analysis (Go et al., 2009; Purver and Battersby, 2012; Suttles and Ide, 2013) and relation extraction (Mintz et al., 2009; Yao et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013; Xu et al., 2013). We classify blocks of three sentences as containing the MRE or not. The two classes, MRE and not-MRE, were weighted inversely to their frequencies in the weakly-labeled set, and all features were normalized to the range [0, 1]. We trained an SVM with margin C = 1 and an RBF kernel with The results of the distant supervision experiment are shown in Table 4. Our best results use all features, but, notably, using the change-based features alone achieves significant impro</context>
</contexts>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Truc-Vien T Nguyen</author>
<author>Alessandro Moschitti</author>
</authors>
<title>End-to-end relation extraction using distant supervision from external semantic repositories.</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association</booktitle>
<contexts>
<context position="23244" citStr="Nguyen and Moschitti, 2011" startWordPosition="3857" endWordPosition="3860">tence and the first and second sentences in the narrative. The MRE usually appears near the end of a story, but Labov (2013) notes that the Abstract, a short introduction that occurs in some narratives, often refers to the MRE. 5.2 Distant Supervision Our first experiment used a distant supervision approach with our automatically-labeled training set. Distant supervision has previously been applied to NLP problems such as sentiment analysis (Go et al., 2009; Purver and Battersby, 2012; Suttles and Ide, 2013) and relation extraction (Mintz et al., 2009; Yao et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013; Xu et al., 2013). We classify blocks of three sentences as containing the MRE or not. The two classes, MRE and not-MRE, were weighted inversely to their frequencies in the weakly-labeled set, and all features were normalized to the range [0, 1]. We trained an SVM with margin C = 1 and an RBF kernel with The results of the distant supervision experiment are shown in Table 4. Our best results use all features, but, notably, using the change-based features alone achieves significant improvement over the three baselines (p &lt; 0.00005). The ‘no change’ trial </context>
</contexts>
<marker>Nguyen, Moschitti, 2011</marker>
<rawString>Truc-Vien T. Nguyen and Alessandro Moschitti. 2011. End-to-end relation extraction using distant supervision from external semantic repositories. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jessica Ouyang</author>
<author>Kathleen McKeown</author>
</authors>
<title>Towards automatic detection of narrative structure.</title>
<date>2014</date>
<booktitle>Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14).</booktitle>
<contexts>
<context position="5602" citStr="Ouyang and McKeown (2014)" startWordPosition="910" endWordPosition="913">013)worked on 20 of Aesop’s fables. The 315 clauses were manually annotated with the three labels of Labov and Waltezky (1967), Orientation (background information), Action (events), and Evaluation (author’s perspective), which we discuss in Section 4. Rahimtoroghi et al. used two annotators with high agreement and achieved accuracy and precision around 0.9 on all three labels, as well as recall above 0.9 on all but Orientation. They noted that their data set was very clean: interannotator agreement was nearly perfect, the language was simple, and each clause served a clear narrative purpose. Ouyang and McKeown (2014) explored identifying the Action chain of the oral narratives in Labov (2013). They used a dataset of 49 narratives (1,277 clauses), transcribed from recordings of speech and annotated by Labov and achieved 0.72 f-score on classifying clauses as Action or not. This task is easier than our proposed task of identifying sentences containing MREs. Actions account for nearly half the clauses in the Labov (2013) dataset, while there are only and average of 2.5 MRE sentences per story. Additionally, identifying Labov’s Actions is a problem of detecting causal and temporal relations among events; iden</context>
</contexts>
<marker>Ouyang, McKeown, 2014</marker>
<rawString>Jessica Ouyang and Kathleen McKeown. 2014. Towards automatic detection of narrative structure. Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellie Pavlick</author>
<author>Ani Nenkova</author>
</authors>
<title>Inducing Lexical Style Properties for Paraphrase and Genre Differentiation.</title>
<date>2015</date>
<booktitle>Proceedings of NAACL-HLT</booktitle>
<contexts>
<context position="17786" citStr="Pavlick and Nenkova (2015)" startWordPosition="2966" endWordPosition="2969">es bear more emphasis than long, complicated ones. We use the length of the sen2152 tence, the length of its verb phrase, and the ratio of these two lengths; the depth of the sentence’s parse tree (Klein and Manning, 2003), the depth of its verb phrase’s subtree, and the ratio of these two depths. We also use the average word length for the sentence and the syntactic complexity formula proposed by Botel and Granowsky (1972), which scores sentences on specific structures, such as passives, appositives, and clausal subjects. Finally, we use the formality and complexity dictionaries described in Pavlick and Nenkova (2015), which provide human formality judgments for 7,794 words and short phrases and complexity judgments for 5,699 words and phrases. We score each sentence by averaging across all words and phrases in the sentence. Semantics. As the MRE is surprising and shocking, we expect it to be dissimilar from the surrounding sentences; we use semantic similarity to surrounding sentences as a measure of shock. Our semantic scores are the bag-of-words cosine and the latent semantic similarity scores for adjacent sentences (Guo and Diab, 2012). Affect. A change in affect reflects a change in style, and we expe</context>
</contexts>
<marker>Pavlick, Nenkova, 2015</marker>
<rawString>Ellie Pavlick and Ani Nenkova. 2015. Inducing Lexical Style Properties for Paraphrase and Genre Differentiation. Proceedings of NAACL-HLT 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Gal Varoquaux</author>
<author>Alexandre Gramfort</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<pages>2825--2830</pages>
<contexts>
<context position="21623" citStr="Pedregosa et al., 2011" startWordPosition="3600" endWordPosition="3603">for each sentence: the metric score at the sentence; whether or not the sentence is a local maximum or minimum; the sentence’s distance from the global 0.6 0.4 Activeness Pleasantness MRE 0.2 0.0 0.2 0.4 DAL Score 0.6 0.8 1.0 1 2 3 4 5 6 Sentence Index norm = 2153 Type Metric Names sentlength, vplength, lengthratio, sentdepth, vpdepth, depthratio wordlength, structcomplexity, wordformality, wordcomplexity Semantic cossimilarity, lssimilarity Affectual pleasantness, activation, imagery, subjectivity Table 3: The fifteen metrics for change. γ = 0.001, chosen using grid search on our tuning set (Pedregosa et al., 2011). Trial Precision Recall F-Score Last sent. baseline 0.208 0.112 0.146 Heuristic baseline 0.107 0.333 0.162 No change* 0.146 0.378 0.211 Random baseline 0.185 0.586 0.281 Change only* 0.351 0.685 0.466 All features* 0.398 0.745 0.519 Syntactic Table 4: Distant supervision results (*p &lt; 0.01). maximum and minimum; the difference in score between the sentence and the preceding sentence, the difference between the sentence and the following sentence, and the average of these differences (approximating the incoming, outgoing, and self- slopes for the metric); and the incoming, outgoing, and self- </context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, 2011</marker>
<rawString>Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel et al. 2011. Scikit-learn: Machine learning in Python. The Journal of Machine Learning Research, 12: 2825-2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
</authors>
<title>Why the Whats are When: Mutually Contextualizing Realms of Narratives.</title>
<date>1976</date>
<booktitle>Proceedings of the second Annual Meeting of the Berkeley Linguistic Society.</booktitle>
<contexts>
<context position="1554" citStr="Polanyi (1976)" startWordPosition="239" endWordPosition="240">se this model in distant supervision and selftraining experiments that achieve significant improvements over the baselines at the task of identifying MREs. 1 Introduction What is a narrative? In one of the early linguistic analyses of storytelling, Prince (1973) defines a story as describing an event that causes a change of state. Prince’s minimal story has three parts: the starting state, the ending state, and the event that transforms the stating state into the ending state. An example of a minimal story is as follows: A man was unhappy, then he fell in love, then as a result, he was happy. Polanyi (1976) notes that minimal stories are toy examples that would never hold an audience’s interest. So what makes a story interesting? Labov (1967; 1997) defines a well-formed narrative as a series of actions leading to a Most Reportable Event (MRE). The MRE is the point of the story – the most unusual event that has the greatest emotional impact on the narrator and the audience. For a story to be interesting, Prince’s change-of-state event should be an MRE. The following is an example of a narrative from the corpus we create in this work, with the sentence containing the MRE emphasized: This isn’t exa</context>
<context position="33644" citStr="Polanyi (1976)" startWordPosition="5583" endWordPosition="5584"> personal blog is unlikely to have a prompt. We use the prompt for our heuristic labeling, so our automatic labels on non-Reddit data may be noisier, but many blog posts also have titles or tags that may be just as useful. Identifying MREs is a hard problem that has not 2156 previously been addressed in work on computational narrative. We have shown that the high-level view proposed by linguistic theories of narrative – that stories are about change – holds true. Measuring change over the course of a narrative yields better results than other features and baselines. Why do we care about MREs? Polanyi (1976) asserts that “one does not produce a narrative text for no reason at all.” The Most Reportable Event is that reason. It is the point of the story; the shortest possible summary; the answer to the question, “So what?”. It could be used to generate titles or summaries to be used in organizing stories for readers to browse, or it could be used in recommendation systems to help readers find related stories. In future work we hope to be able to generate a text description the full MRE, which would be better suited to summarization or generating headlines, rather than identifying sentences that ref</context>
</contexts>
<marker>Polanyi, 1976</marker>
<rawString>Livia Polanyi. 1976. Why the Whats are When: Mutually Contextualizing Realms of Narratives. Proceedings of the second Annual Meeting of the Berkeley Linguistic Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
</authors>
<title>Telling the American story : a structural and cultural analysis of conversational storytelling Ablex Publishing,</title>
<date>1985</date>
<location>Norwood, NJ.</location>
<contexts>
<context position="16479" citStr="Polanyi (1985)" startWordPosition="2744" endWordPosition="2745">t of reportability or interestingness – in fact, all three components of Prince’s minimal story have equivalences in Labov. Labov and Waletzky (1967) proposed three components of narrative: the Orientation, which we equate with Prince’s starting state; the Action, the chain of events culminating in the MRE; and the Evaluation, the author’s perspective on the story. Labov (2013) adds three more components: the Resolution, equivalent to Prince’s ending state, and the Abstract and Coda, where the author introduces and concludes the story. We focus on Prince’s claim that stories are about change. Polanyi (1985) observes that the turning point of a story is marked by a change in style, formality of language, or emphasis in the telling of the story. Labov (2013) likewise observes that a change in verb tense often accompanies MREs. We hypothesize that the MRE should be found at a point of change in the story. We score each sentence according to three views of narrative: syntax, semantics, and affect. Syntax. We model Polanyi’s claim that a change in formality marks the changing point by including metrics of sentential syntax; we use the syntactic complexity of a sentence as an approximation for formali</context>
<context position="29825" citStr="Polanyi (1985)" startWordPosition="4948" endWordPosition="4949">nowledge to find the most shocking and impactful event in a story, but we do not have access to that knowledge. Additionally, MREs are rare, comprising 15% of the sentences in our hand-annotated datasets. MREs comprise just over 16% of our weakly-labeled training set, but as we discuss below, there is too much noise in the automatically-generated labels. Despite the difficulty of the task, our experiments show that our change-based model of narrative is effective for identifying MREs, and this model provides evidence supporting the change-in-state view of narrative suggested by Prince (1973), Polanyi (1985), and Labov (1997). We achieve high recall with self-training (95%), but precision is low across the board. This suggests that, while MREs do occur at extremes in syntactic complexity, semantic similarity, and emotional activation, there may be many nonMRE local extremes throughout a narrative. Examining our results, we find a few common sources of error. False positive sentences tend to have high imagery and activeness. In Table 5, we saw that imagery and activeness alone do not indicate the presence of the MRE. An MRE sentence is not just active; it is separated from the stative introduction</context>
<context position="32022" citStr="Polanyi (1985)" startWordPosition="5314" endWordPosition="5315">ives’ are not narratives at all. Finally, we hope to explore other theories of narrative analysis that could suggest new ways to quantify change in narrative. Prince, Polanyi, and Labov propose a high-level view of personal narrative: stories are centered around reportable events that cause a change in state for the author. This work tested fifteen surface-level features that reflect this change in state. Are there others? Or is a deeper semantic understanding of the starting and ending states of stories required? 7 Conclusion We have described a new model of narrative based on Prince (1973), Polanyi (1985), and Labov (1997). Our model tracks story characterstics over the course of a narrative, capturing change in complexity, meaning, and emotion. We have created a corpus of 4,896 personal narratives, taking advantage of AskReddit, a community where members often prompt each other for stories. Our experiments on this corpus show that our change-based model is able to identify MREs. They also demonstrate that large quantities of hand-labeled data are not required for this task. Our distant supervision and self-training approaches successfully use data weakly labeled using heuristic rules that lev</context>
</contexts>
<marker>Polanyi, 1985</marker>
<rawString>Livia Polanyi. 1985. Telling the American story : a structural and cultural analysis of conversational storytelling Ablex Publishing, Norwood, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Prince</author>
</authors>
<title>A Grammar of Stories: An Introduction. Mouton, The Hague.</title>
<date>1973</date>
<contexts>
<context position="1202" citStr="Prince (1973)" startWordPosition="175" endWordPosition="176">ization of Reddit content into topic-specific communities to automatically identify narratives. Leveraging the structure of Reddit comment threads, we automatically label a large dataset of narratives. We present a change-based model of narrative that tracks changes in formality, affect, and other characteristics over the course of a story, and we use this model in distant supervision and selftraining experiments that achieve significant improvements over the baselines at the task of identifying MREs. 1 Introduction What is a narrative? In one of the early linguistic analyses of storytelling, Prince (1973) defines a story as describing an event that causes a change of state. Prince’s minimal story has three parts: the starting state, the ending state, and the event that transforms the stating state into the ending state. An example of a minimal story is as follows: A man was unhappy, then he fell in love, then as a result, he was happy. Polanyi (1976) notes that minimal stories are toy examples that would never hold an audience’s interest. So what makes a story interesting? Labov (1967; 1997) defines a well-formed narrative as a series of actions leading to a Most Reportable Event (MRE). The MR</context>
<context position="15784" citStr="Prince (1973)" startWordPosition="2634" endWordPosition="2635">erage number of consecutive MREs was 2.5 sentences. To reflect this, we labeled our training set Number of Sentences Data Set Stories MRE Total dev* 99 169 1528 seed* 82 184 958 tuning* 95 212 1301 testing* 193 444 2771 training 4178 11205 67954 Table 2: Distribution of labels (*manual). in three-sentence blocks. The sentence selected by our labeling heuristic, along with the immediately preceding and following sentences, were all labeled as MREs. The result was the weaklylabeled training set in Table 2. 4 Modeling Narrative Our approach to modeling narrative is based on both Labov (2013) and Prince (1973). We claim that Labov’s MRE is Prince’s change of state with the added requirement of reportability or interestingness – in fact, all three components of Prince’s minimal story have equivalences in Labov. Labov and Waletzky (1967) proposed three components of narrative: the Orientation, which we equate with Prince’s starting state; the Action, the chain of events culminating in the MRE; and the Evaluation, the author’s perspective on the story. Labov (2013) adds three more components: the Resolution, equivalent to Prince’s ending state, and the Abstract and Coda, where the author introduces an</context>
<context position="29809" citStr="Prince (1973)" startWordPosition="4946" endWordPosition="4947">rely on world knowledge to find the most shocking and impactful event in a story, but we do not have access to that knowledge. Additionally, MREs are rare, comprising 15% of the sentences in our hand-annotated datasets. MREs comprise just over 16% of our weakly-labeled training set, but as we discuss below, there is too much noise in the automatically-generated labels. Despite the difficulty of the task, our experiments show that our change-based model of narrative is effective for identifying MREs, and this model provides evidence supporting the change-in-state view of narrative suggested by Prince (1973), Polanyi (1985), and Labov (1997). We achieve high recall with self-training (95%), but precision is low across the board. This suggests that, while MREs do occur at extremes in syntactic complexity, semantic similarity, and emotional activation, there may be many nonMRE local extremes throughout a narrative. Examining our results, we find a few common sources of error. False positive sentences tend to have high imagery and activeness. In Table 5, we saw that imagery and activeness alone do not indicate the presence of the MRE. An MRE sentence is not just active; it is separated from the stat</context>
<context position="32006" citStr="Prince (1973)" startWordPosition="5312" endWordPosition="5313">raining ‘narratives’ are not narratives at all. Finally, we hope to explore other theories of narrative analysis that could suggest new ways to quantify change in narrative. Prince, Polanyi, and Labov propose a high-level view of personal narrative: stories are centered around reportable events that cause a change in state for the author. This work tested fifteen surface-level features that reflect this change in state. Are there others? Or is a deeper semantic understanding of the starting and ending states of stories required? 7 Conclusion We have described a new model of narrative based on Prince (1973), Polanyi (1985), and Labov (1997). Our model tracks story characterstics over the course of a narrative, capturing change in complexity, meaning, and emotion. We have created a corpus of 4,896 personal narratives, taking advantage of AskReddit, a community where members often prompt each other for stories. Our experiments on this corpus show that our change-based model is able to identify MREs. They also demonstrate that large quantities of hand-labeled data are not required for this task. Our distant supervision and self-training approaches successfully use data weakly labeled using heuristi</context>
</contexts>
<marker>Prince, 1973</marker>
<rawString>Gerald Prince. 1973. A Grammar of Stories: An Introduction. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Stuart Battersby</author>
</authors>
<title>Experimenting with distant supervision for emotion classification.</title>
<date>2012</date>
<booktitle>Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="23107" citStr="Purver and Battersby, 2012" startWordPosition="3834" endWordPosition="3837">MRE. • The position of the sentence in the narrative. • The bag-of-words cosine similarity and latent semantic similarity between the sentence and the first and second sentences in the narrative. The MRE usually appears near the end of a story, but Labov (2013) notes that the Abstract, a short introduction that occurs in some narratives, often refers to the MRE. 5.2 Distant Supervision Our first experiment used a distant supervision approach with our automatically-labeled training set. Distant supervision has previously been applied to NLP problems such as sentiment analysis (Go et al., 2009; Purver and Battersby, 2012; Suttles and Ide, 2013) and relation extraction (Mintz et al., 2009; Yao et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013; Xu et al., 2013). We classify blocks of three sentences as containing the MRE or not. The two classes, MRE and not-MRE, were weighted inversely to their frequencies in the weakly-labeled set, and all features were normalized to the range [0, 1]. We trained an SVM with margin C = 1 and an RBF kernel with The results of the distant supervision experiment are shown in Table 4. Our best results use all features, but, notab</context>
</contexts>
<marker>Purver, Battersby, 2012</marker>
<rawString>Matthew Purver and Stuart Battersby. 2012. Experimenting with distant supervision for emotion classification. Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elahe Rahimtoroghi</author>
<author>Reid Swanson</author>
<author>Marilyn A Walker</author>
<author>Thomas Corcoran</author>
</authors>
<date>2013</date>
<booktitle>Evaluation, Orientation, and Action in Interactive StoryTelling. Proceedings of Intelligent Narrative Technologies 6.</booktitle>
<contexts>
<context position="4981" citStr="Rahimtoroghi et al. (2013)" startWordPosition="811" endWordPosition="814">as containing the MRE or not: the first using distant supervision, and the second using self-training. In Section 2, we discuss prior work on automatically identifying personal narratives, as well as related experiments using Labov’s theory of narrative analysis. Section 3 discusses data collection and labeling. Sections 4-5 present our changebased model of narrative and our experiments. Finally, Section 6 discusses our experimental results and proposes directions for future work. 2 Related Work Prior work using Labov’s theory of narrative has focused on classifying clauses by their function. Rahimtoroghi et al. (2013)worked on 20 of Aesop’s fables. The 315 clauses were manually annotated with the three labels of Labov and Waltezky (1967), Orientation (background information), Action (events), and Evaluation (author’s perspective), which we discuss in Section 4. Rahimtoroghi et al. used two annotators with high agreement and achieved accuracy and precision around 0.9 on all three labels, as well as recall above 0.9 on all but Orientation. They noted that their data set was very clean: interannotator agreement was nearly perfect, the language was simple, and each clause served a clear narrative purpose. Ouya</context>
</contexts>
<marker>Rahimtoroghi, Swanson, Walker, Corcoran, 2013</marker>
<rawString>Elahe Rahimtoroghi, Reid Swanson, Marilyn A. Walker, and Thomas Corcoran. 2013. Evaluation, Orientation, and Action in Interactive StoryTelling. Proceedings of Intelligent Narrative Technologies 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Ari Rappoport</author>
</authors>
<title>Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets.</title>
<date>2007</date>
<booktitle>Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="27092" citStr="Reichart and Rappoport, 2007" startWordPosition="4492" endWordPosition="4495">rd choice and emphasis. Long sentences, however, suggest a domain difference between our work on text and Labov’s work on transcribed speech. Looking over our development set, we find that many authors combine the description of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 5.3 Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the current model’s labels agreed with any of the remaining heuris</context>
</contexts>
<marker>Reichart, Rappoport, 2007</marker>
<rawString>Roi Reichart and Ari Rappoport. 2007. Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
</authors>
<title>Self-training without reranking for parser domain adaptation and its impact on semantic role labeling.</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing.</booktitle>
<contexts>
<context position="27159" citStr="Sagae, 2010" startWordPosition="4504" endWordPosition="4505">n our work on text and Labov’s work on transcribed speech. Looking over our development set, we find that many authors combine the description of the MRE with evaluative material in a single sentence, resulting in a longer and more syntactically complex MRE sentence than is found in Labov’s data. 5.3 Self-Training Our second experiment used a self-training approach, where a classifier uses a small, labeled seed set to label a larger training set. Self-training has been applied to parsing (McClosky et al., 2006; Reichart and Rappoport, 2007; McClosky and Charniak, 2008; Huang and Harper, 2009; Sagae, 2010) and word sense disambiguation (Mihalcea, 2004). With the same parameters as in the distant supervision experiment, we trained an SVM on our hand-labeled seed set of 958 sentences. We used this initial model to relabel the training set. All sentences where this labeling agreed with our automatically-generated heuristic labels were added to the seed set and used to train a new model, which was in turn used to label the remaining sentences, and so on until none of the current model’s labels agreed with any of the remaining heuristic labels. Figure 2 shows the learning curve for the self-training</context>
</contexts>
<marker>Sagae, 2010</marker>
<rawString>Kenji Sagae. 2010. Self-training without reranking for parser domain adaptation and its impact on semantic role labeling. Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Singer</author>
<author>Fabian Flck</author>
<author>Clemens Meinhart</author>
<author>Elias Zeitfogel</author>
<author>Markus Strohmaier</author>
</authors>
<title>Evolution of Reddit: From the Front Page of the Internet to a Self-referential Community?</title>
<date>2014</date>
<booktitle>Proceedings of the companion publication of the 23rd international conference on World wide web companion.</booktitle>
<contexts>
<context position="8165" citStr="Singer et al. (2014)" startWordPosition="1338" endWordPosition="1341">that only about 17% of weblog text consists of stories. In contrast to the relatively small datasets used by Rahimtoroghi et al., Ouyang and McKeown, and Swanson et al., we use a larger dataset automatically collected from Reddit. Our collection method achieved 94% precision in identifying narratives. A number of researchers have characterized the structure and use of Reddit, currently the 26th most popular website in the world1. Weninger et al. (2013) described the structure of Reddit comment threads. Gilbert (2013) measured user participation in the voting process that ranks Reddit content. Singer et al. (2014) conducted a longitudinal study of the Reddit user community, finding a trend favoring original, user-generated content. 3 Data 3.1 Collection We collected data from the AskReddit subreddit, where users post questions for other members of the community, who reply with comments answering the questions. Table 1 shows some examples 1http://www.alexa.com/siteinfo/reddit.com 2150 of these posts, and we can see some of the wide variety of story topics found on AskReddit. Post Title Whats your creepiest (REAL LIFE) story? Your best “Accidentally Racist” story? What are your stories of petty revenge? </context>
</contexts>
<marker>Singer, Flck, Meinhart, Zeitfogel, Strohmaier, 2014</marker>
<rawString>Philipp Singer, Fabian Flck, Clemens Meinhart, Elias Zeitfogel, and Markus Strohmaier. 2014. Evolution of Reddit: From the Front Page of the Internet to a Self-referential Community? Proceedings of the companion publication of the 23rd international conference on World wide web companion.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jared Suttles</author>
<author>Nancy Ide</author>
</authors>
<title>Distant supervision for emotion classification with discrete binary values.</title>
<date>2013</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>121--136</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg, Berlin.</location>
<contexts>
<context position="23131" citStr="Suttles and Ide, 2013" startWordPosition="3838" endWordPosition="3841">entence in the narrative. • The bag-of-words cosine similarity and latent semantic similarity between the sentence and the first and second sentences in the narrative. The MRE usually appears near the end of a story, but Labov (2013) notes that the Abstract, a short introduction that occurs in some narratives, often refers to the MRE. 5.2 Distant Supervision Our first experiment used a distant supervision approach with our automatically-labeled training set. Distant supervision has previously been applied to NLP problems such as sentiment analysis (Go et al., 2009; Purver and Battersby, 2012; Suttles and Ide, 2013) and relation extraction (Mintz et al., 2009; Yao et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013; Xu et al., 2013). We classify blocks of three sentences as containing the MRE or not. The two classes, MRE and not-MRE, were weighted inversely to their frequencies in the weakly-labeled set, and all features were normalized to the range [0, 1]. We trained an SVM with margin C = 1 and an RBF kernel with The results of the distant supervision experiment are shown in Table 4. Our best results use all features, but, notably, using the change-bas</context>
</contexts>
<marker>Suttles, Ide, 2013</marker>
<rawString>Jared Suttles and Nancy Ide. 2013. Distant supervision for emotion classification with discrete binary values. Computational Linguistics and Intelligent Text Processing, 121-136. Springer Berlin Heidelberg, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reid Swanson</author>
<author>Elahe Rahimtoroghi</author>
<author>Thomas Corcoran</author>
<author>Marilyn A Walker</author>
</authors>
<title>Identifying Narrative Clause Types in Personal Stories.</title>
<date>2014</date>
<booktitle>Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL).</booktitle>
<contexts>
<context position="6305" citStr="Swanson et al. (2014)" startWordPosition="1027" endWordPosition="1030">y used a dataset of 49 narratives (1,277 clauses), transcribed from recordings of speech and annotated by Labov and achieved 0.72 f-score on classifying clauses as Action or not. This task is easier than our proposed task of identifying sentences containing MREs. Actions account for nearly half the clauses in the Labov (2013) dataset, while there are only and average of 2.5 MRE sentences per story. Additionally, identifying Labov’s Actions is a problem of detecting causal and temporal relations among events; identifying the MRE is a problem of measuring how impactful and shocking an event is. Swanson et al. (2014) used 50 stories, which were annotated with an extended label set by three annotators, and each of the 1,602 clauses was assigned the label given by the majority of annotators. The extended label set was then mapped to Labov and Waletzky’s three labels. Nearly half of the clauses in this dataset are Evaluations, and Orientations and Actions each make up nearly one quarter of the dataset. Swanson et al. achieved 0.69 overall f-score on three-way classification of clauses. Again, this task is less difficult than our proposed task. The three labels, Orientation, Action, and Evaluation have distin</context>
</contexts>
<marker>Swanson, Rahimtoroghi, Corcoran, Walker, 2014</marker>
<rawString>Reid Swanson, Elahe Rahimtoroghi, Thomas Corcoran and Marilyn A. Walker. 2014. Identifying Narrative Clause Types in Personal Stories. Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Weninger</author>
<author>Xihao Avi Zhu</author>
<author>Jiawei Han</author>
</authors>
<title>An Exploration of Discussion Threads in Social News Sites: A Case Study of the Reddit Community.</title>
<date>2013</date>
<booktitle>IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM).</booktitle>
<contexts>
<context position="8001" citStr="Weninger et al. (2013)" startWordPosition="1312" endWordPosition="1315">from weblog posts (Gordon and Swanson, 2009). Gordon and Swanson used unigram features to classify posts as either stories or not, achieving 75% precision. They note that only about 17% of weblog text consists of stories. In contrast to the relatively small datasets used by Rahimtoroghi et al., Ouyang and McKeown, and Swanson et al., we use a larger dataset automatically collected from Reddit. Our collection method achieved 94% precision in identifying narratives. A number of researchers have characterized the structure and use of Reddit, currently the 26th most popular website in the world1. Weninger et al. (2013) described the structure of Reddit comment threads. Gilbert (2013) measured user participation in the voting process that ranks Reddit content. Singer et al. (2014) conducted a longitudinal study of the Reddit user community, finding a trend favoring original, user-generated content. 3 Data 3.1 Collection We collected data from the AskReddit subreddit, where users post questions for other members of the community, who reply with comments answering the questions. Table 1 shows some examples 1http://www.alexa.com/siteinfo/reddit.com 2150 of these posts, and we can see some of the wide variety of</context>
</contexts>
<marker>Weninger, Zhu, Han, 2013</marker>
<rawString>Tim Weninger, Xihao Avi Zhu, and Jiawei Han. 2013. An Exploration of Discussion Threads in Social News Sites: A Case Study of the Reddit Community. 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Whissell</author>
</authors>
<title>The dictionary of affect in language. Emotion: Theory, research, and experience,</title>
<date>1989</date>
<pages>4--113</pages>
<publisher>Academic Press,</publisher>
<location>London.</location>
<contexts>
<context position="18494" citStr="Whissell, 1989" startWordPosition="3086" endWordPosition="3087">dgments for 5,699 words and phrases. We score each sentence by averaging across all words and phrases in the sentence. Semantics. As the MRE is surprising and shocking, we expect it to be dissimilar from the surrounding sentences; we use semantic similarity to surrounding sentences as a measure of shock. Our semantic scores are the bag-of-words cosine and the latent semantic similarity scores for adjacent sentences (Guo and Diab, 2012). Affect. A change in affect reflects a change in style, and we expect the MRE to occur at an emotional peak. We use the Dictionary of Affect in Language (DAL) (Whissell, 1989), augmented with WordNet for coverage (Miller, 1995). The DAL represents lexical affect with three scores: evaluation (ee, hereafter ‘pleasantness’ to avoid confusion with Labov’s Evaluation), activation (aa, activeness), and imagery (ii, concreteness). We also use a fourth score, the activationevaluation (AE) norm, a measure of subjectivity defined by Agarwal et al. (2009): √ ee2 + aa2 ii For each of these four word-level scores, we calculate a sentence-level score by averaging across the words in the sentence using the finite state machine described by Agarwal et al. We expect the sentences </context>
</contexts>
<marker>Whissell, 1989</marker>
<rawString>Cynthia Whissell. 1989. The dictionary of affect in language. Emotion: Theory, research, and experience, 4:113-131. Academic Press, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Raphael Hoffmann</author>
<author>Le Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction.</title>
<date>2013</date>
<booktitle>Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</booktitle>
<marker>Xu, Hoffmann, Le Zhao, Grishman, 2013</marker>
<rawString>Wei Xu, Raphael Hoffmann, Le Zhao, and Ralph Grishman. 2013. Filling Knowledge Base Gaps for Distant Supervision of Relation Extraction. Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</rawString>
</citation>
<citation valid="true">
<authors>
<author>Limin Yao</author>
<author>Sebastian Riedel</author>
<author>Andrew McCallum</author>
</authors>
<title>Collective cross-document relation extraction without labelled data.</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="23193" citStr="Yao et al., 2010" startWordPosition="3849" endWordPosition="3852">atent semantic similarity between the sentence and the first and second sentences in the narrative. The MRE usually appears near the end of a story, but Labov (2013) notes that the Abstract, a short introduction that occurs in some narratives, often refers to the MRE. 5.2 Distant Supervision Our first experiment used a distant supervision approach with our automatically-labeled training set. Distant supervision has previously been applied to NLP problems such as sentiment analysis (Go et al., 2009; Purver and Battersby, 2012; Suttles and Ide, 2013) and relation extraction (Mintz et al., 2009; Yao et al., 2010; Hoffmann et al., 2011; Nguyen and Moschitti, 2011; Krause et al., 2012; Min et al., 2013; Xu et al., 2013). We classify blocks of three sentences as containing the MRE or not. The two classes, MRE and not-MRE, were weighted inversely to their frequencies in the weakly-labeled set, and all features were normalized to the range [0, 1]. We trained an SVM with margin C = 1 and an RBF kernel with The results of the distant supervision experiment are shown in Table 4. Our best results use all features, but, notably, using the change-based features alone achieves significant improvement over the th</context>
</contexts>
<marker>Yao, Riedel, McCallum, 2010</marker>
<rawString>Limin Yao, Sebastian Riedel, and Andrew McCallum. 2010. Collective cross-document relation extraction without labelled data. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002412">
<title confidence="0.984727">
Context-Dependent Knowledge Graph Embedding
</title>
<author confidence="0.988262">
Yuanfei Luo&apos; 2, Quan Wang&apos;∗, Bin Wang&apos;, Li Guo&apos;
</author>
<affiliation confidence="0.985123">
&apos;Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China
</affiliation>
<email confidence="0.94729">
{luoyuanfei,wangquan,wangbin,guoli}@iie.ac.cn
</email>
<affiliation confidence="0.574554">
2University of Chinese Academy of Sciences, Beijing, China
</affiliation>
<sectionHeader confidence="0.977239" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998316875">
We consider the problem of embedding
knowledge graphs (KGs) into continuous
vector spaces. Existing methods can on-
ly deal with explicit relationships within
each triple, i.e., local connectivity pattern-
s, but cannot handle implicit relationship-
s across different triples, i.e., contextual
connectivity patterns. This paper proposes
context-dependent KG embedding, a two-
stage scheme that takes into account both
types of connectivity patterns and obtain-
s more accurate embeddings. We evaluate
our approach on the tasks of link predic-
tion and triple classification, and achieve
significant and consistent improvements
over state-of-the-art methods.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999822421052632">
Knowledge Graphs (KGs) like WordNet (Miller,
1995), Freebase (Bollacker et al., 2008), and DB-
pedia (Lehmann et al., 2014) have become ex-
tremely useful resources for many NLP-related ap-
plications. A KG is a directed graph whose nodes
correspond to entities and edges to relations. Each
edge is a triple of the form (h, r, t), indicating that
entities h and t are connected by relation r. Al-
though powerful in representing complex data, the
symbolic nature makes KGs hard to manipulate.
Recently, knowledge graph embedding has at-
tracted much attention (Bordes et al., 2011; Bor-
des et al., 2013; Socher et al., 2013; Wang et al.,
2015). It attempts to embed entities and relations
in a KG into a continuous vector space, so as to
simplify the manipulation while preserving the in-
herent structure of the original graph.
Most of the existing KG embedding methods
model triples individually, ignoring the fact that
</bodyText>
<note confidence="0.8531055">
∗Corresponding author: Quan Wang.
Shaqullle_0_Neal NBA Nevada Utah
</note>
<figureCaption confidence="0.999913">
Figure 1: LCPs and CCPs.
</figureCaption>
<bodyText confidence="0.997879033333333">
entities connected to a same node are usually im-
plicitly related to each other, even if they are not
directly connected. Figure 1 gives two examples.
Shaquille O Neal and NBA in the former ex-
ample and Nevada and Utah in the latter exam-
ple are implicitly related to each other, through the
intermediate nodes Phoenix Suns and USA re-
spectively. We refer to such implicit relationships
as contextual connectivity patterns (CCPs). Re-
lationships explicitly represented in triples are re-
ferred to as local connectivity patterns (LCPs). In
most of the existing methods, only LCPs are ex-
plicitly modeled.
This paper proposes a two-stage embedding
scheme that explicitly takes into account both C-
CPs and LCPs, called context-dependent KG em-
bedding. In the first stage, each CCP is formalized
as a knowledge path, i.e., a sequence of entities
and relations occurring in the pattern. A word em-
bedding model is adopted to learn embeddings of
entities and relations, by taking them as pseudo-
words. The embeddings are enforced compatible
within each knowledge path, and hence can cap-
ture CCPs. In the second stage, the learned em-
beddings are fine-tuned by an existing KG embed-
ding technique. Since such a technique requires
the embeddings to be compatible on each individ-
ual triple, LCPs are also encoded.
The advantages of our approach are three-fold.
1) It fully exploits both CCPs and LCPs, and can
</bodyText>
<figure confidence="0.9351165">
LCPs
CCPs
Phoenix_Suns
USA
</figure>
<page confidence="0.920129">
1656
</page>
<note confidence="0.656323">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1656–1661,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999976916666667">
obtain more accurate embeddings. 2) It is a gen-
eral scheme, applicable to a wide variety of word
embedding models in the first stage and KG em-
bedding models in the second. 3) No auxiliary
data is further required in the two-stage process,
except for the original graph.
We evaluate our approach on two publicly avail-
able data sets, and achieve significant and consis-
tent improvements over state-of-the-art methods in
the link prediction and triple classification tasks.
The learned embeddings are not only more accu-
rate but also more stable.
</bodyText>
<sectionHeader confidence="0.918279" genericHeader="method">
2 Context-Dependent KG Embedding
</sectionHeader>
<bodyText confidence="0.999976888888889">
We are given a KG with nodes corresponding to
entities and edges to relations. Each edge is denot-
ed by a triple (h, r, t), where h is the head entity,
t the tail entity, and r the relation between them.
Entities and relations are represented as vectors,
matrices, or tensors in a continuous vector space.
Context-dependent KG embedding aims to auto-
matically learn entity and relation embeddings, by
using observed triples O in a two-stage process.
</bodyText>
<subsectionHeader confidence="0.998411">
2.1 Modeling CCPs
</subsectionHeader>
<bodyText confidence="0.999854">
The first stage models CCPs conveyed in the KG.
Each CCP is formalized as a knowledge path, i.e.,
a sequence of entities and relations occurring in
the pattern. For the CCPs in Figure 1, the associ-
ated knowledge paths are:
</bodyText>
<subsectionHeader confidence="0.2541985">
“Shaquille O Neal, AthletePlaysForTeam,
Phoenix Suns, TeamPlaysInLeague, NBA”
</subsectionHeader>
<bodyText confidence="0.959858411764706">
“Nevada, StateLocatedInCountry, USA,
StateLocatedInCountry, Utah”.
We fix the length of knowledge paths to 5. Dur-
ing path extraction, we ignore the directionality of
edges, and treat the KG as an undirected graph.1
Given the extracted knowledge paths, we em-
ploy word embedding models to pre-train the em-
beddings of entities and relations, by taking them
as pseudo-words. We use two word embedding
models: CBOW and Skip-gram (Mikolov et al.,
2013a; Mikolov et al., 2013b). In CBOW, words in
the context are projected to their embeddings and
then summed. Based on the summed embedding,
log-linear classifiers are employed to predict the
current word. In Skip-gram, the current word is
projected to its embedding, and log-linear classi-
fiers are further adopted to predict its context. We
</bodyText>
<footnote confidence="0.984528">
1Two entities connected to a same node are always expect-
ed to have some implicit relationships, no matter how they are
connected to the intermediate node.
</footnote>
<bodyText confidence="0.999430368421053">
restrain the context of a word (i.e. entity/relation)
within each knowledge path. The entity and re-
lation embeddings pre-trained in this way are re-
quired to be compatible within each knowledge
path, and thus can encode CCPs.
Perozzi et al. (2014) and Goikoetxea et al.
(2015) have proposed similar ideas, i.e., to gener-
ate random walks from online social networks or
from the WordNet knowledge base, and then em-
ploy word embedding techniques on these random
walks. But our approach has two differences. 1)
It deals with heterogeneous graphs with differen-
t types of edges. Both nodes (entities) and edges
(relations) are included during knowledge path ex-
traction. However, the previous studies focus only
on nodes. 2) We devise a two-stage scheme where
the embeddings learned in the first stage will be
fine-tuned in the second one, while the previous
studies take such embeddings as final output.
</bodyText>
<subsectionHeader confidence="0.999703">
2.2 Modeling LCPs
</subsectionHeader>
<bodyText confidence="0.999832285714286">
The second stage models LCPs conveyed in the
KG. We employ three state-of-the-art KG embed-
ding models, namely SME (Bordes et al., 2014),
TransE (Bordes et al., 2013), and SE (Bordes et
al., 2011) to fine-tune the pre-trained embeddings.
These three models work in the following way.
First, entities are represented as vectors, and re-
lations as operators in an embedding space, char-
acterized by vectors (SME and TransE) or matri-
ces (SE). Then, for each triple (h, r, t), an energy
function fr(h, t) is defined to measure its plausi-
bility. Plausible triples are assumed to have low
energies. Finally, to obtain entity and relation em-
beddings, a margin-based ranking loss, i.e.,
</bodyText>
<equation confidence="0.9991695">
L = � � [γ + fr(h, t) − fr(h&apos;, t&apos;)]+ ,
t+EO t−ENt+
</equation>
<bodyText confidence="0.990855230769231">
is minimized. Here, t+ = (h, r, t) E O is an ob-
served (positive) triple; Nt+ is the set of negative
triples constructed by replacing entities in t+, and
t− = (h&apos;, r, t&apos;) E Nt+; γ is a margin separating
positive and negative triples; [x]+ = max(0, x).
Table 1 summarizes the entity/relation embed-
dings and the energy functions used in SME,
TansE, and SE. For other KG embedding models,
please refer to (Nickel et al., 2011; Riedel et al.,
2013; Wang et al., 2014; Chang et al., 2014).
We adopt stochastic gradient descent to solve
the minimization problem, by taking entity and re-
lation embeddings pre-trained in the first stage as
</bodyText>
<page confidence="0.86016">
1657
</page>
<bodyText confidence="0.496241">
Method Entity/Relation embedding Energy function
</bodyText>
<equation confidence="0.9255415">
h,tERk,rERk
fr (h, t) = (Wu1r + Wu2h + bu)T (Wv1r + Wv2t + bv)
SME (linear) (Bordes et al., 2014)
SME (bilinear) (Bordes et al., 2014)
h, t E Rk, r E Rk fr (h, t) = ((Wu ¯X3r) h + bu)T ((Wv¯X3r) t + bv)
h, t E Rk, r E Rk fr (h, t) = Ilh + r − tIl`,
</equation>
<note confidence="0.833042">
TransE (Bordes et al., 2013)
SE (Bordes et al., 2011)
</note>
<tableCaption confidence="0.8440325">
h, t E Rk, Ru, Rv E Rk×k fr (h, t) = IlRuh − RvtIl`,
Table 1: Entity/Relation embeddings and energy functions used in KG embedding methods.
</tableCaption>
<table confidence="0.952867666666667">
# rel. # ent. # trip. (train/valid/test) # path
WN18 18 40,943 141,442 5,000 5,000 5,674,308
NELL186 186 14,463 31,134 5,000 5,000 1,914,475
</table>
<tableCaption confidence="0.99942">
Table 2: Statistics of the data sets.
</tableCaption>
<bodyText confidence="0.9996597">
initial values.2 The entity and relation embeddings
fine-tuned in this way are required to be compati-
ble within each triple, and thus can encode LCPs.
Socher et al. (2013) have proposed a similar
idea, i.e., to use embeddings learned from an aux-
iliary corpus as initial values. However, linking
entities recognized in an auxiliary corpus to those
occurring in the KG is always a non-trivial task.
Our approach requires no auxiliary data, and nat-
urally avoids the entity linking task.
</bodyText>
<sectionHeader confidence="0.999263" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999888227272727">
We test our approach on the tasks of link predic-
tion and triple classification. Two publicly avail-
able data sets are used. The first is WN18 released
by Bordes et al. (2013)3. It is a subset of Word-
Net, consisting of 18 relations and the entities con-
nected by them. The second is NELL186 released
by Guo et al. (2015)4, containing the most fre-
quent 186 relations in NELL (Carlson et al., 2010)
and the associated entities. Triples are split into
training/validation/test sets, used for model train-
ing, parameter tuning, and evaluation respectively.
Knowledge paths are extracted from training sets.
Table 2 gives some statistics of the data sets.
To perform context-dependent KG embedding,
we use CBOW and Skip-gram in the pre-training
stage, and SME, TransE, and SE in the fine-tuning
stage. We take randomly initialized SME, TransE,
and SE as baselines, denoted as *-Random. We
do not compare to the setting that employs only
CBOW or Skip-gram, since it does not provide
an energy function to calculate triple plausibility,
which hinders the evaluation of both tasks.
</bodyText>
<footnote confidence="0.99992025">
2For SE, only entity vectors are initialized by pre-trained
embeddings. Relation matrices are randomly initialized.
3https://everest.hds.utc.fr/doku.php?id=en:smemlj12
4http://www.aclweb.org/anthology/P/P15/
</footnote>
<subsectionHeader confidence="0.998666">
3.1 Link Prediction
</subsectionHeader>
<bodyText confidence="0.9998444">
Link prediction is to predict whether there is a spe-
cific relation between two entities.
Evaluation Protocol. For each test triple, the
head is replaced by every entity in the KG, and
the energy is calculated for each corrupted triple.
Ranking the energies in ascending order, we get
the rank of the correct answer. We can get another
rank by corrupting the tail. We report two metrics
on the test sets: Mean (averaged rank) and Hit-
s@10 (proportion of ranks no larger than 10).
Implementation Details. To train CBOW and
Skip-gram, we use the word2vec implementation-
s5. 20 negative samples are drawn for each pos-
itive one. The context size is fixed to 5. To train
SME, TransE, and SE, we use the implementation-
s provided by the authors6, with 100 mini-batches.
We vary the learning rate in {0.01, 0.1,1,10}, the
dimension k in {20, 50}, and the margin γ in
{1, 2, 4}. The best model is selected by monitor-
ing Hits@10 on the validation sets, with a total of
at most 1000 iterations over the training sets.
Results. Table 3 reports the results on the test
sets of WN18 and NELL186. The improvements
of CBOW/Skip-gram over Random are also given.
Statistically significant improvements are marked
by ‡ (sign test, significance level 0.05). The result-
s show that a pre-training stage consistently im-
proves over the baselines for all the methods on
both data sets. Almost all of the improvements are
statistically significant.
</bodyText>
<subsectionHeader confidence="0.996821">
3.2 Triple Classification
</subsectionHeader>
<bodyText confidence="0.998397833333333">
Triple classification aims to verify whether an un-
seen triple is correct or not.
Evaluation Protocol. Triples in the validation
and test sets are labeled as positive instances. For
each positive instance, we construct a negative in-
stance by randomly corrupting the entities. During
</bodyText>
<footnote confidence="0.999921">
5https://code.google.com/p/word2vec/
6https://github.com/glorotxa/SME
</footnote>
<page confidence="0.835735">
1658
</page>
<table confidence="0.9998567">
Random Mean Skip-gram Random Hits@10 (%) Skip-gram
CBOW CBOW
SME (linear) 463.2 $286.5 (138%) 226.9 (151%) 63.98 $68.65 (17%) $70.01 (19%)
SME (bilinear) 551.8 $308.8 (144%) $279.2 (149%) 63.83 $67.65 (16%) $67.53 (16%)
TransE 723.1 $293.0 (159%) $290.0 (160%) 78.50 $79.67 (11%) $79.87 (12%)
SE 960.0 $426.2 (156%) $289.4 (170%) 71.53 $76.05 (16%) $75.89 (16%)
SME (linear) 595.5 $371.9 (138%) $340.3 (143%) 29.82 $34.22 (115%) $35.57 (119%)
SME (bilinear) 375.2 $305.0 (119%) $292.9 (122%) 37.45 $39.31 (1 5%) $39.70 (1 6%)
TransE 732.6 $384.6 (148%) $384.6 (148%) 27.60 $28.71 (1 4%) $30.52 (111%)
SE 2307.0 $1314.7 (143%) $412.2 (182%) 19.53 $26.15 (134%) $31.12 (159%)
</table>
<tableCaption confidence="0.98189">
Table 3: Link prediction results on the test sets of WN18 and NELL186.
</tableCaption>
<table confidence="0.984399083333333">
WN18
NELL186
Random Micro-ACC (%) Random Macro-ACC (%) Skip-gram
CBOW Skip-gram CBOW
SME (linear) 84.70 89.54 (16%) 89.16 (15%) 85.11 89.11 (15%) 90.57 (16%)
SME (bilinear) 84.30 91.83 (19%) 90.68 (18%) 85.36 90.49 (16%) 89.89 (15%)
TransE 94.60 96.98 (13%) 97.23 (13%) 86.74 93.46 (18%) 94.49 (19%)
SE 94.71 96.46 (12%) 96.42 (12%) 87.99 92.05 (15%) 91.70 (14%)
SME (linear) 88.59 89.95 (12%) 91.19 (13%) 84.42 85.70 (12%) 86.67 (13%)
SME (bilinear) 88.74 93.22 (15%) 92.86 (15%) 83.41 89.70 (18%) 89.65 (17%)
TransE 82.54 85.65 (14%) 85.33 (13%) 76.74 80.06 (14%) 80.06 (14%)
SE 89.00 93.37 (15%) 93.07 (15%) 83.01 87.89 (16%) 87.98 (16%)
</table>
<tableCaption confidence="0.999653">
Table 4: Triple classification results on the test sets of WN18 and NELL186.
</tableCaption>
<bodyText confidence="0.999269142857143">
classification, a triple is predicted to be positive
if the energy is below a relation-specific thresh-
old δr; otherwise negative. We report two metric-
s on the test sets: micro-averaged accuracy (per-
instance average) and macro-averaged accuracy
(per-relation average).
Implementation Details. We use the same pa-
rameter settings as in the link prediction task.
The relation-specific threshold δr is determined by
maximizing Micro-ACC on the validation sets.
Results. Table 4 reports the results on the test
sets of WN18 and NELL186. The results again
demonstrate both the superiority and the generali-
ty of our approach.
</bodyText>
<subsectionHeader confidence="0.995362">
3.3 Discussions
</subsectionHeader>
<bodyText confidence="0.9989329">
This section is to explore why pre-training helps
in KG embedding, specifically in link prediction.
We first test different random initializations in
traditional KG embedding models. We run SME
(linear) twice on WN18, with two different initial-
ization settings. Both are randomly sampled from
the same uniform distribution, but with differen-
t seeds, referred to as Random-I and Random-
II. Each setting finally gets 10,000 ranks on the
test set.7 To better understand the difference be-
</bodyText>
<footnote confidence="0.765904">
7For each of the 5,000 test triples, both the head and the
</footnote>
<bodyText confidence="0.990069892857143">
tween the two settings, we analyze the ranks indi-
vidually, rather than reporting aggregated metrics
(Mean and Hits@10). Specifically, we distribute
the 10,000 instances into different bins according
to the ranks given by one setting (e.g. Random-
I). Instances assigned to the i-th bin have the same
rank of i, that means, they are all ranked in the i-th
position by this setting. Then, within each bin, we
calculate the average rank of the instances given
by the other setting (e.g. Random-II). If the av-
erage rank differs drastically from the bin ID, the
instances in this bin are ranked significantly dif-
ferently by the two settings. Figures 2(a) and 2(b)
show the results, with the instances distributed ac-
cording to Random-I and Random-II respectively.
In both cases, we retain the bins with ID no larger
than 50, covering about 85% of the instances. In
most of the bins, the average rank (red bars in the
figures) differs drastically from the bin ID (black
bars in the figures), indicating that the ranks giv-
en by Random-I and Random-II are significantly
different at the instance level. The results demon-
strate the non-convexity of SME (linear): different
initial values lead to different local minimum.
We further compare the settings of initial val-
ues 1) randomly sampled from a uniform distri-
bution (Random) and 2) pre-trained by Skip-gram
tail are corrupted and ranked.
</bodyText>
<page confidence="0.98315">
1659
</page>
<figure confidence="0.999765949367089">
WN18
NELL186
100
80
60
40
Average Rank
20
0
100
80
60
40
Average Rank
20
0
100
80
60
40
Average Rank
20
0
100
80
60
40
Average Rank
20
0
10 20
30 40 50
10 20
30 40 50
10 20
30 40 50
Random−I
Random−II
Bin ID
(a) Distributed by Random-I.
Random−II
Random−I
0 20 30 40 50
Bin ID
(b) Distributed by Random-II.
Random
Skip−gram
10 20
30 40 50
100
80
60
40
20
0
Bin ID
(c) Distributed by Random.
Bin ID
(d) Distributed by Skip-gram.
Skip−gram−I
Skip−gram−II
10 20
30 40 50
100
80
60
40
20
0
Bin ID
(e) Distributed by Skip-gram-I.
Skip−gram−II
Skip−gram−I
Bin ID
(f) Distributed by Skip-gram-II.
Average Rank
Average Rank
Skip−gram
Random
</figure>
<figureCaption confidence="0.999854">
Figure 2: Ranks obtained by different initialization strategies (best viewed in color).
</figureCaption>
<bodyText confidence="0.999965">
(Skip-gram). The results are given in Figures 2(c)
and 2(d). In most of the bins Skip-gram has an
average rank lower than the bin ID (Figure 2(c)),
while Random has an average rank much higher
than the bin ID (Figure 2(d)), implying that Skip-
gram performs better than Random-I at the in-
stance level. The results indicate that pre-training
might help in finding better initial values which
lead to better local minimum.
Finally we test our two-stage KG embedding
scheme where the skip-gram model itself is giv-
en two different initialization settings, say Skip-
gram-I and Skip-gram-II. The results are given in
Figures 2(e) and 2(f). In each of the first 20 bins,
Skip-gram-I and Skip-gram-II get an average rank
almost the same with the bin ID, implying that the
two settings perform quite similarly, particularly
at the highest ranking levels. The results indicate
that a pre-training stage might help in obtaining
more stable embeddings.
</bodyText>
<sectionHeader confidence="0.999535" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999981391304348">
We have proposed a novel two-stage scheme for
KG embedding, called context-dependent KG em-
bedding. In the pre-training stage CCPs are encod-
ed by a word embedding model, and in the fine-
tuning stage LCPs are encoded by a traditional KG
embedding model. Since both types of connectiv-
ity patterns are explicitly taken into account, our
approach can obtain more accurate embeddings.
Moreover, our approach is quite general, applica-
ble to various word embedding and KG embed-
ding models. Experimental results on link predic-
tion and triple classification demonstrate the supe-
riority, generality, and stability of our approach.
As future work, we plan to 1) Investigate the ef-
ficacy of longer CCPs (i.e. knowledge paths with
lengths longer than 5). 2) Design a joint model that
encodes LCPs and CCPs simultaneously. More-
over, our approach actually reveals the possibili-
ty of a broad idea, i.e., initializing an embedding
model by another embedding model. We would
also like to test the feasibility of other such strate-
gies, e.g., initializing SME by TransE, so as to
combine the benefits of both models.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999765875">
We would like to thank the anonymous reviewers
for their valuable comments and suggestions. This
work is supported by the National Natural Science
Foundation of China (grant No. 61402465), the S-
trategic Priority Research Program of the Chinese
Academy of Sciences (grant No. XDA06030200),
and the National Key Technology R&amp;D Program
(grant No. 201213AH461303).
</bodyText>
<page confidence="0.987368">
1660
</page>
<sectionHeader confidence="0.990177" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999780223404255">
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim S-
turge, and Jamie Taylor. 2008. Freebase: A collab-
oratively created graph database for structuring hu-
man knowledge. In Proceedings of the 2008 ACM
SIGMOD International Conference on Management
of Data, pages 1247–1250.
Antoine Bordes, Jason Weston, Ronan Collobert, and
Yoshua Bengio. 2011. Learning structured em-
beddings of knowledge bases. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence,
pages 301–306.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-
Duran, Jason Weston, and Oksana Yakhnenko.
2013. Translating embeddings for modeling multi-
relational data. In Advances in Neural Information
Processing Systems, pages 2787–2795.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2014. A semantic matching en-
ergy function for learning with multi-relational data.
Machine Learning, 94(2):233–259.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Bur-
r Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the
24th AAAI Conference on Artificial Intelligence,
pages 1306–1313.
Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and
Christopher Meek. 2014. Typed tensor decompo-
sition of knowledge bases for relation extraction. In
Proceedings of the 2014 Conference on Empirical
Methods on Natural Language Processing, pages
1568–1579.
Josu Goikoetxea, Aitor Soroa, and Eneko Agirre.
2015. Random walks and neural network language
models on knowledge bases. In Proceedings of the
2015 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 1434–1439.
Shu Guo, Quan Wang, Bin Wang, Lihong Wang, and
Li Guo. 2015. Semantically smooth knowledge
graph embedding. In Proceedings of the 53rd An-
nual Meeting of the Association for Computational
Linguistics and the 7th International Joint Confer-
ence on Natural Language Processing, pages 84–94.
Jens Lehmann, Robert Isele, Max Jakob, Anja
Jentzsch, Dimitris Kontokostas, Pablo N. Mendes,
Sebastian Hellmann, Mohamed Morsey, Patrick van
Kleef, S¨oren Auer, et al. 2014. Dbpedia: A large-
scale, multilingual knowledge base extracted from
wikipedia. Semantic Web Journal.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at International Conference on Learning Represen-
tations.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
George A Miller. 1995. Wordnet: A lexical
database for english. Communications of the ACM,
38(11):39–41.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In Proceedings
of the 28th International Conference on Machine
Learning, pages 809–816.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.
2014. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKD-
D International Conference on Knowledge Discov-
ery and Data Mining, pages 701–710.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
Proceedings of the 2013 Conference on North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
74–84.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Ng. 2013. Reasoning with neural ten-
sor networks for knowledge base completion. In Ad-
vances in Neural Information Processing Systems,
pages 926–934.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng
Chen. 2014. Knowledge graph embedding by trans-
lating on hyperplanes. In Proceedings of the 28th
AAAI Conference on Artificial Intelligence, pages
1112–1119.
Quan Wang, Bin Wang, and Li Guo. 2015. Knowl-
edge base completion using embeddings and rules.
In Proceedings of the 24th International Joint Con-
ference on Artificial Intelligence, pages 1859–1865.
</reference>
<page confidence="0.992668">
1661
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.963385">
<title confidence="0.99979">Context-Dependent Knowledge Graph Embedding</title>
<author confidence="0.999409">Quan Bin Li</author>
<affiliation confidence="0.9986">of Information Engineering, Chinese Academy of Sciences, Beijing,</affiliation>
<address confidence="0.975676">of Chinese Academy of Sciences, Beijing, China</address>
<abstract confidence="0.999372647058824">We consider the problem of embedding knowledge graphs (KGs) into continuous vector spaces. Existing methods can only deal with explicit relationships within each triple, i.e., local connectivity patterns, but cannot handle implicit relationships across different triples, i.e., contextual connectivity patterns. This paper proposes context-dependent KG embedding, a twostage scheme that takes into account both types of connectivity patterns and obtains more accurate embeddings. We evaluate our approach on the tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: A collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data,</booktitle>
<pages>1247--1250</pages>
<contexts>
<context position="1036" citStr="Bollacker et al., 2008" startWordPosition="134" endWordPosition="137">th explicit relationships within each triple, i.e., local connectivity patterns, but cannot handle implicit relationships across different triples, i.e., contextual connectivity patterns. This paper proposes context-dependent KG embedding, a twostage scheme that takes into account both types of connectivity patterns and obtains more accurate embeddings. We evaluate our approach on the tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods. 1 Introduction Knowledge Graphs (KGs) like WordNet (Miller, 1995), Freebase (Bollacker et al., 2008), and DBpedia (Lehmann et al., 2014) have become extremely useful resources for many NLP-related applications. A KG is a directed graph whose nodes correspond to entities and edges to relations. Each edge is a triple of the form (h, r, t), indicating that entities h and t are connected by relation r. Although powerful in representing complex data, the symbolic nature makes KGs hard to manipulate. Recently, knowledge graph embedding has attracted much attention (Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2015). It attempts to embed entities and relations in a KG</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pages 1247–1250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Jason Weston</author>
<author>Ronan Collobert</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning structured embeddings of knowledge bases.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th AAAI Conference on Artificial Intelligence,</booktitle>
<pages>301--306</pages>
<contexts>
<context position="1521" citStr="Bordes et al., 2011" startWordPosition="217" endWordPosition="220">ts over state-of-the-art methods. 1 Introduction Knowledge Graphs (KGs) like WordNet (Miller, 1995), Freebase (Bollacker et al., 2008), and DBpedia (Lehmann et al., 2014) have become extremely useful resources for many NLP-related applications. A KG is a directed graph whose nodes correspond to entities and edges to relations. Each edge is a triple of the form (h, r, t), indicating that entities h and t are connected by relation r. Although powerful in representing complex data, the symbolic nature makes KGs hard to manipulate. Recently, knowledge graph embedding has attracted much attention (Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2015). It attempts to embed entities and relations in a KG into a continuous vector space, so as to simplify the manipulation while preserving the inherent structure of the original graph. Most of the existing KG embedding methods model triples individually, ignoring the fact that ∗Corresponding author: Quan Wang. Shaqullle_0_Neal NBA Nevada Utah Figure 1: LCPs and CCPs. entities connected to a same node are usually implicitly related to each other, even if they are not directly connected. Figure 1 gives two examples. Shaquille O Neal an</context>
<context position="6959" citStr="Bordes et al., 2011" startWordPosition="1105" endWordPosition="1108">differences. 1) It deals with heterogeneous graphs with different types of edges. Both nodes (entities) and edges (relations) are included during knowledge path extraction. However, the previous studies focus only on nodes. 2) We devise a two-stage scheme where the embeddings learned in the first stage will be fine-tuned in the second one, while the previous studies take such embeddings as final output. 2.2 Modeling LCPs The second stage models LCPs conveyed in the KG. We employ three state-of-the-art KG embedding models, namely SME (Bordes et al., 2014), TransE (Bordes et al., 2013), and SE (Bordes et al., 2011) to fine-tune the pre-trained embeddings. These three models work in the following way. First, entities are represented as vectors, and relations as operators in an embedding space, characterized by vectors (SME and TransE) or matrices (SE). Then, for each triple (h, r, t), an energy function fr(h, t) is defined to measure its plausibility. Plausible triples are assumed to have low energies. Finally, to obtain entity and relation embeddings, a margin-based ranking loss, i.e., L = � � [γ + fr(h, t) − fr(h&apos;, t&apos;)]+ , t+EO t−ENt+ is minimized. Here, t+ = (h, r, t) E O is an observed (positive) tri</context>
<context position="8478" citStr="Bordes et al., 2011" startWordPosition="1391" endWordPosition="1394">embedding models, please refer to (Nickel et al., 2011; Riedel et al., 2013; Wang et al., 2014; Chang et al., 2014). We adopt stochastic gradient descent to solve the minimization problem, by taking entity and relation embeddings pre-trained in the first stage as 1657 Method Entity/Relation embedding Energy function h,tERk,rERk fr (h, t) = (Wu1r + Wu2h + bu)T (Wv1r + Wv2t + bv) SME (linear) (Bordes et al., 2014) SME (bilinear) (Bordes et al., 2014) h, t E Rk, r E Rk fr (h, t) = ((Wu ¯X3r) h + bu)T ((Wv¯X3r) t + bv) h, t E Rk, r E Rk fr (h, t) = Ilh + r − tIl`, TransE (Bordes et al., 2013) SE (Bordes et al., 2011) h, t E Rk, Ru, Rv E Rk×k fr (h, t) = IlRuh − RvtIl`, Table 1: Entity/Relation embeddings and energy functions used in KG embedding methods. # rel. # ent. # trip. (train/valid/test) # path WN18 18 40,943 141,442 5,000 5,000 5,674,308 NELL186 186 14,463 31,134 5,000 5,000 1,914,475 Table 2: Statistics of the data sets. initial values.2 The entity and relation embeddings fine-tuned in this way are required to be compatible within each triple, and thus can encode LCPs. Socher et al. (2013) have proposed a similar idea, i.e., to use embeddings learned from an auxiliary corpus as initial values. Ho</context>
</contexts>
<marker>Bordes, Weston, Collobert, Bengio, 2011</marker>
<rawString>Antoine Bordes, Jason Weston, Ronan Collobert, and Yoshua Bengio. 2011. Learning structured embeddings of knowledge bases. In Proceedings of the 25th AAAI Conference on Artificial Intelligence, pages 301–306.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Nicolas Usunier</author>
<author>Alberto GarciaDuran</author>
<author>Jason Weston</author>
<author>Oksana Yakhnenko</author>
</authors>
<title>Translating embeddings for modeling multirelational data.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2787--2795</pages>
<contexts>
<context position="1542" citStr="Bordes et al., 2013" startWordPosition="221" endWordPosition="225">art methods. 1 Introduction Knowledge Graphs (KGs) like WordNet (Miller, 1995), Freebase (Bollacker et al., 2008), and DBpedia (Lehmann et al., 2014) have become extremely useful resources for many NLP-related applications. A KG is a directed graph whose nodes correspond to entities and edges to relations. Each edge is a triple of the form (h, r, t), indicating that entities h and t are connected by relation r. Although powerful in representing complex data, the symbolic nature makes KGs hard to manipulate. Recently, knowledge graph embedding has attracted much attention (Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2015). It attempts to embed entities and relations in a KG into a continuous vector space, so as to simplify the manipulation while preserving the inherent structure of the original graph. Most of the existing KG embedding methods model triples individually, ignoring the fact that ∗Corresponding author: Quan Wang. Shaqullle_0_Neal NBA Nevada Utah Figure 1: LCPs and CCPs. entities connected to a same node are usually implicitly related to each other, even if they are not directly connected. Figure 1 gives two examples. Shaquille O Neal and NBA in the former e</context>
<context position="6929" citStr="Bordes et al., 2013" startWordPosition="1099" endWordPosition="1102">lks. But our approach has two differences. 1) It deals with heterogeneous graphs with different types of edges. Both nodes (entities) and edges (relations) are included during knowledge path extraction. However, the previous studies focus only on nodes. 2) We devise a two-stage scheme where the embeddings learned in the first stage will be fine-tuned in the second one, while the previous studies take such embeddings as final output. 2.2 Modeling LCPs The second stage models LCPs conveyed in the KG. We employ three state-of-the-art KG embedding models, namely SME (Bordes et al., 2014), TransE (Bordes et al., 2013), and SE (Bordes et al., 2011) to fine-tune the pre-trained embeddings. These three models work in the following way. First, entities are represented as vectors, and relations as operators in an embedding space, characterized by vectors (SME and TransE) or matrices (SE). Then, for each triple (h, r, t), an energy function fr(h, t) is defined to measure its plausibility. Plausible triples are assumed to have low energies. Finally, to obtain entity and relation embeddings, a margin-based ranking loss, i.e., L = � � [γ + fr(h, t) − fr(h&apos;, t&apos;)]+ , t+EO t−ENt+ is minimized. Here, t+ = (h, r, t) E O</context>
<context position="8453" citStr="Bordes et al., 2013" startWordPosition="1386" endWordPosition="1389">sE, and SE. For other KG embedding models, please refer to (Nickel et al., 2011; Riedel et al., 2013; Wang et al., 2014; Chang et al., 2014). We adopt stochastic gradient descent to solve the minimization problem, by taking entity and relation embeddings pre-trained in the first stage as 1657 Method Entity/Relation embedding Energy function h,tERk,rERk fr (h, t) = (Wu1r + Wu2h + bu)T (Wv1r + Wv2t + bv) SME (linear) (Bordes et al., 2014) SME (bilinear) (Bordes et al., 2014) h, t E Rk, r E Rk fr (h, t) = ((Wu ¯X3r) h + bu)T ((Wv¯X3r) t + bv) h, t E Rk, r E Rk fr (h, t) = Ilh + r − tIl`, TransE (Bordes et al., 2013) SE (Bordes et al., 2011) h, t E Rk, Ru, Rv E Rk×k fr (h, t) = IlRuh − RvtIl`, Table 1: Entity/Relation embeddings and energy functions used in KG embedding methods. # rel. # ent. # trip. (train/valid/test) # path WN18 18 40,943 141,442 5,000 5,000 5,674,308 NELL186 186 14,463 31,134 5,000 5,000 1,914,475 Table 2: Statistics of the data sets. initial values.2 The entity and relation embeddings fine-tuned in this way are required to be compatible within each triple, and thus can encode LCPs. Socher et al. (2013) have proposed a similar idea, i.e., to use embeddings learned from an auxiliary cor</context>
</contexts>
<marker>Bordes, Usunier, GarciaDuran, Weston, Yakhnenko, 2013</marker>
<rawString>Antoine Bordes, Nicolas Usunier, Alberto GarciaDuran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multirelational data. In Advances in Neural Information Processing Systems, pages 2787–2795.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antoine Bordes</author>
<author>Xavier Glorot</author>
<author>Jason Weston</author>
<author>Yoshua Bengio</author>
</authors>
<title>A semantic matching energy function for learning with multi-relational data.</title>
<date>2014</date>
<booktitle>Machine Learning,</booktitle>
<volume>94</volume>
<issue>2</issue>
<contexts>
<context position="6899" citStr="Bordes et al., 2014" startWordPosition="1094" endWordPosition="1097"> techniques on these random walks. But our approach has two differences. 1) It deals with heterogeneous graphs with different types of edges. Both nodes (entities) and edges (relations) are included during knowledge path extraction. However, the previous studies focus only on nodes. 2) We devise a two-stage scheme where the embeddings learned in the first stage will be fine-tuned in the second one, while the previous studies take such embeddings as final output. 2.2 Modeling LCPs The second stage models LCPs conveyed in the KG. We employ three state-of-the-art KG embedding models, namely SME (Bordes et al., 2014), TransE (Bordes et al., 2013), and SE (Bordes et al., 2011) to fine-tune the pre-trained embeddings. These three models work in the following way. First, entities are represented as vectors, and relations as operators in an embedding space, characterized by vectors (SME and TransE) or matrices (SE). Then, for each triple (h, r, t), an energy function fr(h, t) is defined to measure its plausibility. Plausible triples are assumed to have low energies. Finally, to obtain entity and relation embeddings, a margin-based ranking loss, i.e., L = � � [γ + fr(h, t) − fr(h&apos;, t&apos;)]+ , t+EO t−ENt+ is minim</context>
<context position="8273" citStr="Bordes et al., 2014" startWordPosition="1339" endWordPosition="1342"> r, t&apos;) E Nt+; γ is a margin separating positive and negative triples; [x]+ = max(0, x). Table 1 summarizes the entity/relation embeddings and the energy functions used in SME, TansE, and SE. For other KG embedding models, please refer to (Nickel et al., 2011; Riedel et al., 2013; Wang et al., 2014; Chang et al., 2014). We adopt stochastic gradient descent to solve the minimization problem, by taking entity and relation embeddings pre-trained in the first stage as 1657 Method Entity/Relation embedding Energy function h,tERk,rERk fr (h, t) = (Wu1r + Wu2h + bu)T (Wv1r + Wv2t + bv) SME (linear) (Bordes et al., 2014) SME (bilinear) (Bordes et al., 2014) h, t E Rk, r E Rk fr (h, t) = ((Wu ¯X3r) h + bu)T ((Wv¯X3r) t + bv) h, t E Rk, r E Rk fr (h, t) = Ilh + r − tIl`, TransE (Bordes et al., 2013) SE (Bordes et al., 2011) h, t E Rk, Ru, Rv E Rk×k fr (h, t) = IlRuh − RvtIl`, Table 1: Entity/Relation embeddings and energy functions used in KG embedding methods. # rel. # ent. # trip. (train/valid/test) # path WN18 18 40,943 141,442 5,000 5,000 5,674,308 NELL186 186 14,463 31,134 5,000 5,000 1,914,475 Table 2: Statistics of the data sets. initial values.2 The entity and relation embeddings fine-tuned in this way </context>
</contexts>
<marker>Bordes, Glorot, Weston, Bengio, 2014</marker>
<rawString>Antoine Bordes, Xavier Glorot, Jason Weston, and Yoshua Bengio. 2014. A semantic matching energy function for learning with multi-relational data. Machine Learning, 94(2):233–259.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Carlson</author>
<author>Justin Betteridge</author>
<author>Bryan Kisiel</author>
<author>Burr Settles</author>
<author>Estevam R Hruschka Jr</author>
<author>Tom M Mitchell</author>
</authors>
<title>Toward an architecture for neverending language learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 24th AAAI Conference on Artificial Intelligence,</booktitle>
<pages>1306--1313</pages>
<contexts>
<context position="9689" citStr="Carlson et al., 2010" startWordPosition="1603" endWordPosition="1606">values. However, linking entities recognized in an auxiliary corpus to those occurring in the KG is always a non-trivial task. Our approach requires no auxiliary data, and naturally avoids the entity linking task. 3 Experiments We test our approach on the tasks of link prediction and triple classification. Two publicly available data sets are used. The first is WN18 released by Bordes et al. (2013)3. It is a subset of WordNet, consisting of 18 relations and the entities connected by them. The second is NELL186 released by Guo et al. (2015)4, containing the most frequent 186 relations in NELL (Carlson et al., 2010) and the associated entities. Triples are split into training/validation/test sets, used for model training, parameter tuning, and evaluation respectively. Knowledge paths are extracted from training sets. Table 2 gives some statistics of the data sets. To perform context-dependent KG embedding, we use CBOW and Skip-gram in the pre-training stage, and SME, TransE, and SE in the fine-tuning stage. We take randomly initialized SME, TransE, and SE as baselines, denoted as *-Random. We do not compare to the setting that employs only CBOW or Skip-gram, since it does not provide an energy function t</context>
</contexts>
<marker>Carlson, Betteridge, Kisiel, Settles, Jr, Mitchell, 2010</marker>
<rawString>Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr., and Tom M. Mitchell. 2010. Toward an architecture for neverending language learning. In Proceedings of the 24th AAAI Conference on Artificial Intelligence, pages 1306–1313.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Wen-tau Yih</author>
<author>Bishan Yang</author>
<author>Christopher Meek</author>
</authors>
<title>Typed tensor decomposition of knowledge bases for relation extraction.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods on Natural Language Processing,</booktitle>
<pages>1568--1579</pages>
<contexts>
<context position="7973" citStr="Chang et al., 2014" startWordPosition="1289" endWordPosition="1292">Finally, to obtain entity and relation embeddings, a margin-based ranking loss, i.e., L = � � [γ + fr(h, t) − fr(h&apos;, t&apos;)]+ , t+EO t−ENt+ is minimized. Here, t+ = (h, r, t) E O is an observed (positive) triple; Nt+ is the set of negative triples constructed by replacing entities in t+, and t− = (h&apos;, r, t&apos;) E Nt+; γ is a margin separating positive and negative triples; [x]+ = max(0, x). Table 1 summarizes the entity/relation embeddings and the energy functions used in SME, TansE, and SE. For other KG embedding models, please refer to (Nickel et al., 2011; Riedel et al., 2013; Wang et al., 2014; Chang et al., 2014). We adopt stochastic gradient descent to solve the minimization problem, by taking entity and relation embeddings pre-trained in the first stage as 1657 Method Entity/Relation embedding Energy function h,tERk,rERk fr (h, t) = (Wu1r + Wu2h + bu)T (Wv1r + Wv2t + bv) SME (linear) (Bordes et al., 2014) SME (bilinear) (Bordes et al., 2014) h, t E Rk, r E Rk fr (h, t) = ((Wu ¯X3r) h + bu)T ((Wv¯X3r) t + bv) h, t E Rk, r E Rk fr (h, t) = Ilh + r − tIl`, TransE (Bordes et al., 2013) SE (Bordes et al., 2011) h, t E Rk, Ru, Rv E Rk×k fr (h, t) = IlRuh − RvtIl`, Table 1: Entity/Relation embeddings and e</context>
</contexts>
<marker>Chang, Yih, Yang, Meek, 2014</marker>
<rawString>Kai-Wei Chang, Wen-tau Yih, Bishan Yang, and Christopher Meek. 2014. Typed tensor decomposition of knowledge bases for relation extraction. In Proceedings of the 2014 Conference on Empirical Methods on Natural Language Processing, pages 1568–1579.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josu Goikoetxea</author>
<author>Aitor Soroa</author>
<author>Eneko Agirre</author>
</authors>
<title>Random walks and neural network language models on knowledge bases.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1434--1439</pages>
<contexts>
<context position="6124" citStr="Goikoetxea et al. (2015)" startWordPosition="967" endWordPosition="970">ear classifiers are employed to predict the current word. In Skip-gram, the current word is projected to its embedding, and log-linear classifiers are further adopted to predict its context. We 1Two entities connected to a same node are always expected to have some implicit relationships, no matter how they are connected to the intermediate node. restrain the context of a word (i.e. entity/relation) within each knowledge path. The entity and relation embeddings pre-trained in this way are required to be compatible within each knowledge path, and thus can encode CCPs. Perozzi et al. (2014) and Goikoetxea et al. (2015) have proposed similar ideas, i.e., to generate random walks from online social networks or from the WordNet knowledge base, and then employ word embedding techniques on these random walks. But our approach has two differences. 1) It deals with heterogeneous graphs with different types of edges. Both nodes (entities) and edges (relations) are included during knowledge path extraction. However, the previous studies focus only on nodes. 2) We devise a two-stage scheme where the embeddings learned in the first stage will be fine-tuned in the second one, while the previous studies take such embedd</context>
</contexts>
<marker>Goikoetxea, Soroa, Agirre, 2015</marker>
<rawString>Josu Goikoetxea, Aitor Soroa, and Eneko Agirre. 2015. Random walks and neural network language models on knowledge bases. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1434–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shu Guo</author>
<author>Quan Wang</author>
<author>Bin Wang</author>
<author>Lihong Wang</author>
<author>Li Guo</author>
</authors>
<title>Semantically smooth knowledge graph embedding.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,</booktitle>
<pages>84--94</pages>
<contexts>
<context position="9613" citStr="Guo et al. (2015)" startWordPosition="1590" endWordPosition="1593">ea, i.e., to use embeddings learned from an auxiliary corpus as initial values. However, linking entities recognized in an auxiliary corpus to those occurring in the KG is always a non-trivial task. Our approach requires no auxiliary data, and naturally avoids the entity linking task. 3 Experiments We test our approach on the tasks of link prediction and triple classification. Two publicly available data sets are used. The first is WN18 released by Bordes et al. (2013)3. It is a subset of WordNet, consisting of 18 relations and the entities connected by them. The second is NELL186 released by Guo et al. (2015)4, containing the most frequent 186 relations in NELL (Carlson et al., 2010) and the associated entities. Triples are split into training/validation/test sets, used for model training, parameter tuning, and evaluation respectively. Knowledge paths are extracted from training sets. Table 2 gives some statistics of the data sets. To perform context-dependent KG embedding, we use CBOW and Skip-gram in the pre-training stage, and SME, TransE, and SE in the fine-tuning stage. We take randomly initialized SME, TransE, and SE as baselines, denoted as *-Random. We do not compare to the setting that em</context>
</contexts>
<marker>Guo, Wang, Wang, Wang, Guo, 2015</marker>
<rawString>Shu Guo, Quan Wang, Bin Wang, Lihong Wang, and Li Guo. 2015. Semantically smooth knowledge graph embedding. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 84–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jens Lehmann</author>
<author>Robert Isele</author>
<author>Max Jakob</author>
<author>Anja Jentzsch</author>
<author>Dimitris Kontokostas</author>
<author>Pablo N Mendes</author>
<author>Sebastian Hellmann</author>
<author>Mohamed Morsey</author>
<author>Patrick van Kleef</author>
<author>S¨oren Auer</author>
</authors>
<title>Dbpedia: A largescale, multilingual knowledge base extracted from wikipedia. Semantic Web Journal.</title>
<date>2014</date>
<marker>Lehmann, Isele, Jakob, Jentzsch, Kontokostas, Mendes, Hellmann, Morsey, van Kleef, Auer, 2014</marker>
<rawString>Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann, Mohamed Morsey, Patrick van Kleef, S¨oren Auer, et al. 2014. Dbpedia: A largescale, multilingual knowledge base extracted from wikipedia. Semantic Web Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at International Conference on Learning Representations.</booktitle>
<contexts>
<context position="5354" citStr="Mikolov et al., 2013" startWordPosition="841" endWordPosition="844">elations occurring in the pattern. For the CCPs in Figure 1, the associated knowledge paths are: “Shaquille O Neal, AthletePlaysForTeam, Phoenix Suns, TeamPlaysInLeague, NBA” “Nevada, StateLocatedInCountry, USA, StateLocatedInCountry, Utah”. We fix the length of knowledge paths to 5. During path extraction, we ignore the directionality of edges, and treat the KG as an undirected graph.1 Given the extracted knowledge paths, we employ word embedding models to pre-train the embeddings of entities and relations, by taking them as pseudo-words. We use two word embedding models: CBOW and Skip-gram (Mikolov et al., 2013a; Mikolov et al., 2013b). In CBOW, words in the context are projected to their embeddings and then summed. Based on the summed embedding, log-linear classifiers are employed to predict the current word. In Skip-gram, the current word is projected to its embedding, and log-linear classifiers are further adopted to predict its context. We 1Two entities connected to a same node are always expected to have some implicit relationships, no matter how they are connected to the intermediate node. restrain the context of a word (i.e. entity/relation) within each knowledge path. The entity and relation</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. In Proceedings of Workshop at International Conference on Learning Representations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="5354" citStr="Mikolov et al., 2013" startWordPosition="841" endWordPosition="844">elations occurring in the pattern. For the CCPs in Figure 1, the associated knowledge paths are: “Shaquille O Neal, AthletePlaysForTeam, Phoenix Suns, TeamPlaysInLeague, NBA” “Nevada, StateLocatedInCountry, USA, StateLocatedInCountry, Utah”. We fix the length of knowledge paths to 5. During path extraction, we ignore the directionality of edges, and treat the KG as an undirected graph.1 Given the extracted knowledge paths, we employ word embedding models to pre-train the embeddings of entities and relations, by taking them as pseudo-words. We use two word embedding models: CBOW and Skip-gram (Mikolov et al., 2013a; Mikolov et al., 2013b). In CBOW, words in the context are projected to their embeddings and then summed. Based on the summed embedding, log-linear classifiers are employed to predict the current word. In Skip-gram, the current word is projected to its embedding, and log-linear classifiers are further adopted to predict its context. We 1Two entities connected to a same node are always expected to have some implicit relationships, no matter how they are connected to the intermediate node. restrain the context of a word (i.e. entity/relation) within each knowledge path. The entity and relation</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: A lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1001" citStr="Miller, 1995" startWordPosition="131" endWordPosition="132"> methods can only deal with explicit relationships within each triple, i.e., local connectivity patterns, but cannot handle implicit relationships across different triples, i.e., contextual connectivity patterns. This paper proposes context-dependent KG embedding, a twostage scheme that takes into account both types of connectivity patterns and obtains more accurate embeddings. We evaluate our approach on the tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods. 1 Introduction Knowledge Graphs (KGs) like WordNet (Miller, 1995), Freebase (Bollacker et al., 2008), and DBpedia (Lehmann et al., 2014) have become extremely useful resources for many NLP-related applications. A KG is a directed graph whose nodes correspond to entities and edges to relations. Each edge is a triple of the form (h, r, t), indicating that entities h and t are connected by relation r. Although powerful in representing complex data, the symbolic nature makes KGs hard to manipulate. Recently, knowledge graph embedding has attracted much attention (Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2015). It attempts to e</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: A lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maximilian Nickel</author>
<author>Volker Tresp</author>
<author>Hans-Peter Kriegel</author>
</authors>
<title>A three-way model for collective learning on multi-relational data.</title>
<date>2011</date>
<booktitle>In Proceedings of the 28th International Conference on Machine Learning,</booktitle>
<pages>809--816</pages>
<contexts>
<context position="7912" citStr="Nickel et al., 2011" startWordPosition="1277" endWordPosition="1280">ibility. Plausible triples are assumed to have low energies. Finally, to obtain entity and relation embeddings, a margin-based ranking loss, i.e., L = � � [γ + fr(h, t) − fr(h&apos;, t&apos;)]+ , t+EO t−ENt+ is minimized. Here, t+ = (h, r, t) E O is an observed (positive) triple; Nt+ is the set of negative triples constructed by replacing entities in t+, and t− = (h&apos;, r, t&apos;) E Nt+; γ is a margin separating positive and negative triples; [x]+ = max(0, x). Table 1 summarizes the entity/relation embeddings and the energy functions used in SME, TansE, and SE. For other KG embedding models, please refer to (Nickel et al., 2011; Riedel et al., 2013; Wang et al., 2014; Chang et al., 2014). We adopt stochastic gradient descent to solve the minimization problem, by taking entity and relation embeddings pre-trained in the first stage as 1657 Method Entity/Relation embedding Energy function h,tERk,rERk fr (h, t) = (Wu1r + Wu2h + bu)T (Wv1r + Wv2t + bv) SME (linear) (Bordes et al., 2014) SME (bilinear) (Bordes et al., 2014) h, t E Rk, r E Rk fr (h, t) = ((Wu ¯X3r) h + bu)T ((Wv¯X3r) t + bv) h, t E Rk, r E Rk fr (h, t) = Ilh + r − tIl`, TransE (Bordes et al., 2013) SE (Bordes et al., 2011) h, t E Rk, Ru, Rv E Rk×k fr (h, t</context>
</contexts>
<marker>Nickel, Tresp, Kriegel, 2011</marker>
<rawString>Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on Machine Learning, pages 809–816.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bryan Perozzi</author>
<author>Rami Al-Rfou</author>
<author>Steven Skiena</author>
</authors>
<title>Deepwalk: Online learning of social representations.</title>
<date>2014</date>
<booktitle>In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>701--710</pages>
<contexts>
<context position="6095" citStr="Perozzi et al. (2014)" startWordPosition="962" endWordPosition="965"> summed embedding, log-linear classifiers are employed to predict the current word. In Skip-gram, the current word is projected to its embedding, and log-linear classifiers are further adopted to predict its context. We 1Two entities connected to a same node are always expected to have some implicit relationships, no matter how they are connected to the intermediate node. restrain the context of a word (i.e. entity/relation) within each knowledge path. The entity and relation embeddings pre-trained in this way are required to be compatible within each knowledge path, and thus can encode CCPs. Perozzi et al. (2014) and Goikoetxea et al. (2015) have proposed similar ideas, i.e., to generate random walks from online social networks or from the WordNet knowledge base, and then employ word embedding techniques on these random walks. But our approach has two differences. 1) It deals with heterogeneous graphs with different types of edges. Both nodes (entities) and edges (relations) are included during knowledge path extraction. However, the previous studies focus only on nodes. 2) We devise a two-stage scheme where the embeddings learned in the first stage will be fine-tuned in the second one, while the prev</context>
</contexts>
<marker>Perozzi, Al-Rfou, Skiena, 2014</marker>
<rawString>Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning of social representations. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 701–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>Limin Yao</author>
<author>Andrew McCallum</author>
<author>Benjamin M Marlin</author>
</authors>
<title>Relation extraction with matrix factorization and universal schemas.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>74--84</pages>
<contexts>
<context position="7933" citStr="Riedel et al., 2013" startWordPosition="1281" endWordPosition="1284">iples are assumed to have low energies. Finally, to obtain entity and relation embeddings, a margin-based ranking loss, i.e., L = � � [γ + fr(h, t) − fr(h&apos;, t&apos;)]+ , t+EO t−ENt+ is minimized. Here, t+ = (h, r, t) E O is an observed (positive) triple; Nt+ is the set of negative triples constructed by replacing entities in t+, and t− = (h&apos;, r, t&apos;) E Nt+; γ is a margin separating positive and negative triples; [x]+ = max(0, x). Table 1 summarizes the entity/relation embeddings and the energy functions used in SME, TansE, and SE. For other KG embedding models, please refer to (Nickel et al., 2011; Riedel et al., 2013; Wang et al., 2014; Chang et al., 2014). We adopt stochastic gradient descent to solve the minimization problem, by taking entity and relation embeddings pre-trained in the first stage as 1657 Method Entity/Relation embedding Energy function h,tERk,rERk fr (h, t) = (Wu1r + Wu2h + bu)T (Wv1r + Wv2t + bv) SME (linear) (Bordes et al., 2014) SME (bilinear) (Bordes et al., 2014) h, t E Rk, r E Rk fr (h, t) = ((Wu ¯X3r) h + bu)T ((Wv¯X3r) t + bv) h, t E Rk, r E Rk fr (h, t) = Ilh + r − tIl`, TransE (Bordes et al., 2013) SE (Bordes et al., 2011) h, t E Rk, Ru, Rv E Rk×k fr (h, t) = IlRuh − RvtIl`, T</context>
</contexts>
<marker>Riedel, Yao, McCallum, Marlin, 2013</marker>
<rawString>Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. In Proceedings of the 2013 Conference on North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 74–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Reasoning with neural tensor networks for knowledge base completion.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>926--934</pages>
<contexts>
<context position="1563" citStr="Socher et al., 2013" startWordPosition="226" endWordPosition="229">uction Knowledge Graphs (KGs) like WordNet (Miller, 1995), Freebase (Bollacker et al., 2008), and DBpedia (Lehmann et al., 2014) have become extremely useful resources for many NLP-related applications. A KG is a directed graph whose nodes correspond to entities and edges to relations. Each edge is a triple of the form (h, r, t), indicating that entities h and t are connected by relation r. Although powerful in representing complex data, the symbolic nature makes KGs hard to manipulate. Recently, knowledge graph embedding has attracted much attention (Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2015). It attempts to embed entities and relations in a KG into a continuous vector space, so as to simplify the manipulation while preserving the inherent structure of the original graph. Most of the existing KG embedding methods model triples individually, ignoring the fact that ∗Corresponding author: Quan Wang. Shaqullle_0_Neal NBA Nevada Utah Figure 1: LCPs and CCPs. entities connected to a same node are usually implicitly related to each other, even if they are not directly connected. Figure 1 gives two examples. Shaquille O Neal and NBA in the former example and Nevada and</context>
<context position="8969" citStr="Socher et al. (2013)" startWordPosition="1477" endWordPosition="1480">r) h + bu)T ((Wv¯X3r) t + bv) h, t E Rk, r E Rk fr (h, t) = Ilh + r − tIl`, TransE (Bordes et al., 2013) SE (Bordes et al., 2011) h, t E Rk, Ru, Rv E Rk×k fr (h, t) = IlRuh − RvtIl`, Table 1: Entity/Relation embeddings and energy functions used in KG embedding methods. # rel. # ent. # trip. (train/valid/test) # path WN18 18 40,943 141,442 5,000 5,000 5,674,308 NELL186 186 14,463 31,134 5,000 5,000 1,914,475 Table 2: Statistics of the data sets. initial values.2 The entity and relation embeddings fine-tuned in this way are required to be compatible within each triple, and thus can encode LCPs. Socher et al. (2013) have proposed a similar idea, i.e., to use embeddings learned from an auxiliary corpus as initial values. However, linking entities recognized in an auxiliary corpus to those occurring in the KG is always a non-trivial task. Our approach requires no auxiliary data, and naturally avoids the entity linking task. 3 Experiments We test our approach on the tasks of link prediction and triple classification. Two publicly available data sets are used. The first is WN18 released by Bordes et al. (2013)3. It is a subset of WordNet, consisting of 18 relations and the entities connected by them. The sec</context>
</contexts>
<marker>Socher, Chen, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Danqi Chen, Christopher D. Manning, and Andrew Ng. 2013. Reasoning with neural tensor networks for knowledge base completion. In Advances in Neural Information Processing Systems, pages 926–934.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhen Wang</author>
<author>Jianwen Zhang</author>
<author>Jianlin Feng</author>
<author>Zheng Chen</author>
</authors>
<title>Knowledge graph embedding by translating on hyperplanes.</title>
<date>2014</date>
<booktitle>In Proceedings of the 28th AAAI Conference on Artificial Intelligence,</booktitle>
<pages>1112--1119</pages>
<contexts>
<context position="7952" citStr="Wang et al., 2014" startWordPosition="1285" endWordPosition="1288">have low energies. Finally, to obtain entity and relation embeddings, a margin-based ranking loss, i.e., L = � � [γ + fr(h, t) − fr(h&apos;, t&apos;)]+ , t+EO t−ENt+ is minimized. Here, t+ = (h, r, t) E O is an observed (positive) triple; Nt+ is the set of negative triples constructed by replacing entities in t+, and t− = (h&apos;, r, t&apos;) E Nt+; γ is a margin separating positive and negative triples; [x]+ = max(0, x). Table 1 summarizes the entity/relation embeddings and the energy functions used in SME, TansE, and SE. For other KG embedding models, please refer to (Nickel et al., 2011; Riedel et al., 2013; Wang et al., 2014; Chang et al., 2014). We adopt stochastic gradient descent to solve the minimization problem, by taking entity and relation embeddings pre-trained in the first stage as 1657 Method Entity/Relation embedding Energy function h,tERk,rERk fr (h, t) = (Wu1r + Wu2h + bu)T (Wv1r + Wv2t + bv) SME (linear) (Bordes et al., 2014) SME (bilinear) (Bordes et al., 2014) h, t E Rk, r E Rk fr (h, t) = ((Wu ¯X3r) h + bu)T ((Wv¯X3r) t + bv) h, t E Rk, r E Rk fr (h, t) = Ilh + r − tIl`, TransE (Bordes et al., 2013) SE (Bordes et al., 2011) h, t E Rk, Ru, Rv E Rk×k fr (h, t) = IlRuh − RvtIl`, Table 1: Entity/Rela</context>
</contexts>
<marker>Wang, Zhang, Feng, Chen, 2014</marker>
<rawString>Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge graph embedding by translating on hyperplanes. In Proceedings of the 28th AAAI Conference on Artificial Intelligence, pages 1112–1119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quan Wang</author>
<author>Bin Wang</author>
<author>Li Guo</author>
</authors>
<title>Knowledge base completion using embeddings and rules.</title>
<date>2015</date>
<booktitle>In Proceedings of the 24th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1859--1865</pages>
<contexts>
<context position="1583" citStr="Wang et al., 2015" startWordPosition="230" endWordPosition="233">hs (KGs) like WordNet (Miller, 1995), Freebase (Bollacker et al., 2008), and DBpedia (Lehmann et al., 2014) have become extremely useful resources for many NLP-related applications. A KG is a directed graph whose nodes correspond to entities and edges to relations. Each edge is a triple of the form (h, r, t), indicating that entities h and t are connected by relation r. Although powerful in representing complex data, the symbolic nature makes KGs hard to manipulate. Recently, knowledge graph embedding has attracted much attention (Bordes et al., 2011; Bordes et al., 2013; Socher et al., 2013; Wang et al., 2015). It attempts to embed entities and relations in a KG into a continuous vector space, so as to simplify the manipulation while preserving the inherent structure of the original graph. Most of the existing KG embedding methods model triples individually, ignoring the fact that ∗Corresponding author: Quan Wang. Shaqullle_0_Neal NBA Nevada Utah Figure 1: LCPs and CCPs. entities connected to a same node are usually implicitly related to each other, even if they are not directly connected. Figure 1 gives two examples. Shaquille O Neal and NBA in the former example and Nevada and Utah in the latter </context>
</contexts>
<marker>Wang, Wang, Guo, 2015</marker>
<rawString>Quan Wang, Bin Wang, and Li Guo. 2015. Knowledge base completion using embeddings and rules. In Proceedings of the 24th International Joint Conference on Artificial Intelligence, pages 1859–1865.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
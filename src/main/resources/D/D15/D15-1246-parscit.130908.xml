<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006701">
<title confidence="0.998253">
What’s in an Embedding?
Analyzing Word Embeddings through Multilingual Evaluation
</title>
<author confidence="0.996445">
Arne Köhn
</author>
<affiliation confidence="0.997938">
Department of Informatics
</affiliation>
<address confidence="0.643949">
Universität Hamburg
</address>
<email confidence="0.99692">
koehn@informatik.uni-hamburg.de
</email>
<sectionHeader confidence="0.993847" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999910789473684">
In the last two years, there has been a
surge of word embedding algorithms and
research on them. However, evaluation has
mostly been carried out on a narrow set of
tasks, mainly word similarity/relatedness
and word relation similarity and on a single
language, namely English.
We propose an approach to evaluate embed-
dings on a variety of languages that also
yields insights into the structure of the em-
bedding space by investigating how well
word embeddings cluster along different
syntactic features.
We show that all embedding approaches
behave similarly in this task, with
dependency-based embeddings performing
best. This effect is even more pronounced
when generating low dimensional embed-
dings.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9996383125">
Word embeddings map words into a vector space,
allowing to reason about words in this space. They
have been shown to be beneficial for several tasks
such as machine translation (Botha and Blunsom,
2014), parsing (Lei et al., 2014), and named en-
tity recognition (Passos et al., 2014). Recently,
word embedding techniques have been studied for
their mathematical properties (Levy and Goldberg,
2014b; Stratos et al., 2015), yielding a better un-
derstanding of the underlying optimization criteria.
However, word embeddings have mostly been stud-
ied and evaluated on a single language (English).
Therefore, validation on languages other than En-
glish is lacking and the question whether word
embeddings work the same way across languages
has not been empirically evaluated. Evaluations of
complex systems – such as parsers – employing
word embeddings generally give only little insight
into the type of contribution to the result and the
structure of word embeddings.
We aim to fill these gaps by evaluating several
word embedding algorithms on a set of different
languages using tasks that enable additional insight
into the learned structures using easily obtainable
data. At the same time, we provide baseline results
for using word embeddings in several syntax-based
classification tasks.
We focus on syntax-related measures because
data is available for several languages and we ex-
pect a correlation with usefulness of word embed-
dings for syntax-related tasks such as named entity
recognition, parsing, and morphological analysis.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99993132">
Previous approaches to word embedding evaluation
have either used relatively basic word finding and
classification tasks (as this paper also proposes)
or application-oriented end-to-end evaluations as
part of a larger system. Word finding tasks are of
the form “given a pair of words (x, y), find a y&apos;
for a given x&apos;”, e.g. given (Rome, Italy), find a
word for Oslo. These tasks have been introduced
by Mikolov et al. (2013a). The downside of this
kind of task is that the data is not readily available
and has to be constructed for each language. This
type of evaluation primarily describes the similar-
ity between vector differences and not similarity
between vectors. In addition, Levy et al. (2015)
showed for this task that word embedding-based
classifiers actually mostly learn whether a word is
a general hypernym and not, as would be expected,
the relation between two words.
Another approach to evaluate embeddings, used
by Pennington et al. (2014) amongst others, is to
rank a fixed set of words relative to a reference
word. The results are then compared to human
judgments, e.g. from the WS353 corpus (Finkel-
stein et al., 2002). This approach has a limited
coverage and additional data is expensive to obtain.
</bodyText>
<page confidence="0.944039">
2067
</page>
<note confidence="0.651382">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.993849518518518">
Botha and Blunsom (2014) propose to factorize
word vectors into morpheme vectors to better cap-
ture similarities between morphologically related
words and evaluate their word representations us-
ing log-bilinear language models based on their
word vectors.1 They measure model perplexity re-
duction relative to n-gram language models and
include their model into a machine translation sys-
tem, gaining between 0 (English → German) and
1.2 (English → Russian) BLEU points.
Lei et al. (2014) introduce a syntactic depen-
dency parser using (amongst others) a low-rank
tensor component for scoring dependency edges.
This scoring can employ word embeddings. Doing
so yields an improvement of 0.2 to 0.5 percentage
points. If no Part-of-Speech (PoS) tags are avail-
able, this difference rises to up to four percentage
points. Köhn et al. (2014) show that this gain from
using word embeddings is even more pronounced
in complete absence of morphological information
(including PoS tags), reporting a difference of five
to seven percentage points,depending on the lan-
guage, using the same parser. With these findings,
it can be assumed that word embeddings encode
some kind of morphological information. Neither,
however, investigated what kind of information the
word embeddings actually contain.
</bodyText>
<sectionHeader confidence="0.984536" genericHeader="method">
3 The Embedding Algorithms
</sectionHeader>
<bodyText confidence="0.99978815">
To assess the differences between embedding algo-
rithms, we will evaluate six different approaches.
The continuous bag-of-words (cbow) approach de-
scribed by Mikolov et al. (2013a) is learned by
predicting the word vector based on the context
vectors. The skip-gram approach (skip) from the
same authors is doing the reverse: it predicts the
context word vectors based on the embedding of
the current word. We use the version of cbow and
skip as described in (Mikolov et al., 2013b) which
use negative sampling, i.e. they train by distinguish-
ing the correct word in its context against words
not occurring in that context.
Levy and Goldberg (2014a) alter the skipgram
approach by not using the neighboring words wrt.
the sentence’s word sequence but wrt. the depen-
dency tree of the sentence. Therefore, the context
of w is defined as all words that are either the head
or dependents of w. We will call this approach
dep.
</bodyText>
<footnote confidence="0.9984015">
1Their approach has not been evaluated in this paper as the
corresponding code is not available as of now.
</footnote>
<bodyText confidence="0.99996555">
GloVe, introduced by Pennington et al. (2014),
optimizes the ratio of co-occurrence probabilities
instead of the co-occurrence probabilities them-
selves, getting rid of the negative sampling used
for the approaches previously mentioned.
Stratos et al. (2015) describe a method to de-
rive word embeddings using canonical correlation
analysis. We will call this approach cca.
brown clusters (Brown et al., 1992) are con-
structed by clustering words hierarchically into a
binary search tree in a way that maximizes mutual
information for a language model. To construct an
embedding for a cluster c, we use the following
procedure: For each edge on the path from the root
to c, add either 1 or −1, depending on the direc-
tion of descent. Because not every path has the
same depth, we pad missing dimensions with 0.
This way, we obtain an embedding interpretation
of the clusters. Note that, in contrast to clustering
embeddings, no information is lost.
</bodyText>
<sectionHeader confidence="0.929123" genericHeader="method">
4 Our Evaluation by Classification Tasks
</sectionHeader>
<bodyText confidence="0.999930310344828">
We classify words separately according to several
tasks with an L2-regularized linear classifier. All
classification tasks are based on the word embed-
ding of a single word alone, without any other infor-
mation about the word or its context; in particular,
the word’s lexicalization is not used as a feature.
By using the continuous features directly instead
of clustering them (as e.g. done by Bansal et al.
(2014)), we ensure that no information is lost dur-
ing preprocessing.
All tasks can be carried out on dependency tree-
banks with morphological annotation. From each
word in the treebank, we extract a data point (word
embedding, class) for training/testing, where class
is of one of the following, depending on the task:
pos The Part-of-Speech of the word
headpos The PoS of the word’s head
label The label of the word’s dependency edge
gender* The gender of the word
case* The case of the word
number* The number of the word
tense* The tense of the word
Tasks marked with an asterisk are only carried out
on words with a corresponding feature. Some of
these features are absent in some languages, e.g.
Basque is mostly genderless and the corpus of En-
glish we used is not annotated with morphological
information. These combinations of language and
feature have been omitted.
</bodyText>
<page confidence="0.959818">
2068
</page>
<bodyText confidence="0.99867375">
We use a one-versus-all linear classifier for two
reasons: First, the feature dimensionality is rela-
tively high. Second, and more importantly, training
a linear classifier yields insights into the structure
of the vector space because the classifier also serves
as a tool to obtain a supervised clustering of the
vector space.
Let C be set set of classes. A one-versus-all
linear classifier learns a linear function fc ∈ Rn →
R for each class c ∈ C. The classifier assigns to
a vector X the best matching class based on these
functions:
</bodyText>
<equation confidence="0.981594">
class(X) = arg max fc(X)
c∈C
</equation>
<bodyText confidence="0.999992351351351">
Due to the linearity of the functions fc, the vector
space is partitioned into convex polytopes, which
each represent exactly one class (see Appendix A).
Therefore, the classification accuracies can also
be interpreted as supervised clustering accuracies.
This means that if the classifier yields a high ac-
curacy, the members of each class are clustered
in a single convex region of the vector space. We
think that this is a fairly strong statement about the
structure of the vector space.
To better gauge how well the embeddings are ac-
tually clustered, we use a majority baseline which
classifies all elements as the one class that occurred
most often during training. This is the accuracy a
classifier would yield without any information and
therefore the information gain obtainable by us-
ing word embeddings as features is the difference
between the achieved accuracy and the baseline
accuracy.
In addition to the lower bound described above,
we also provide an approximate upper bound for
the accuracy. Because no context information is
used during classification, the word vector corre-
sponding to a word will always be classified the
same, even though the correct classification might
depend on the context, e. g. the word put can be-
long to different tense classes depending on the
context. Therefore, an upper bound for the classifi-
cation task is to assign each word the most probable
class for that word (computed on the training set).
Assuming that no sparsity issues exist, embedding-
based classification can yield at most accuracies as
high as this approach. Note that because in reality
data sparsity unfortunately does exist, this is only
an approximation of the upper bound. We call this
approximation up-approx and compute it omitting
words not seen during training.
</bodyText>
<sectionHeader confidence="0.998304" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999913923076923">
Evaluation was carried out on Basque, English,
French, German, Hungarian, Polish, and Swedish
datasets. For English, automatically labeled data
was obtained by tagging and parsing a subset of
the English Wikipedia dump provided by Al-Rfou
et al. (2013) using TurboTagger and TurboParser
(Martins et al., 2013). The Penn Treebank (Marcus
et al., 1994), converted using the LTH converter
(Johansson and Nugues, 2007), was used as the
corresponding manually annotated resource.
For all other languages, datasets including both
automatically and manually annotated data pro-
vided as part of the Shared Task on parsing mor-
phologically rich languages (Seddah et al., 2014)
were used.2
For all languages, we trained embeddings on the
automatically labeled data using the approaches
described in Section 3, with different window sizes
(5 and 11, where applicable) and dimensions (10,
100, 200). The rare word limit was set to five words
occurrences. brown was only trained with 1024
clusters equaling about 10 dimensions, as the num-
ber of clusters can not be increased to generate
higher-dimensional embeddings. dep was not eval-
uated on French because the French automatically
labeled dataset lacks dependency information.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9849906875">
Figure 1 a) shows the accuracies for the evaluated
word embeddings on all tasks for the different lan-
guages. The results were obtained using the best-
performing hyperparameters (200 dimensions for
all, window size = 5 for cca, cbow and skip, win-
dow size = 11 for GloVe, compare Table 1).
All embeddings capture the PoS well. To a lesser
degree, the dependency label and head PoS can also
be recovered. The better-performing embeddings
achieve results near the approximate upper bound
for all tasks.
The embeddings also mostly cluster well with re-
spect to tense, number, gender, and case, with tense
showing the best correlations. For some of these
tasks, the baseline is however fairly high because
the number of classes is lower.
</bodyText>
<note confidence="0.961328666666667">
2Basque: (Aduriz et al., 2003; Aldezabal et al., 2008),
French: (Abeillé et al., 2003; Candito et al., 2010), German:
(Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian:
(Csendes et al., 2005; Vincze et al., 2010), Polish: (Woli´nski
et al., 2011; ´Swidzi´nski and Woli´nski, 2010; Wróblewska,
2012), Swedish: (Nivre et al., 2006)
</note>
<page confidence="0.974517">
2069
</page>
<figure confidence="0.984534615384615">
a) dimensionality 200
B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S
100
90
80
70
60
50
40
30
20
pos label headpos case gender tense number
b) dimensionality 10
B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S
100
90
80
70
60
50
40
30
20
pos label headpos case gender tense number
cca skip glove baseline
dep cbow brown approx-up
</figure>
<figureCaption confidence="0.962214">
Figure 1: Results with window = 5 (for cbow, cca &amp; skip) / 11 (for GloVe) for Basque, English, French,
German, Hungarian, Polish, Swedish. Note: brown is only present in b).
</figureCaption>
<table confidence="0.998039571428571">
w d cca skip cbow dep GloVe
5 200 80.41 80.69 80.42 82.35 70.05
100 −1.38 −1.16 −3.31 −0.39 −2.24
10 −18.06 −22.92 −16.18 −8.38 −16.12
11 200 −1.31 −0.04 −0.05 n/a +0.57
100 −3.56 −1.16 −1.17 n/a −1.73
10 −23.51 −22.94 −16.34 n/a −15.64
</table>
<tableCaption confidence="0.998971">
Table 1: Mean accuracy across tasks for
</tableCaption>
<bodyText confidence="0.991483666666667">
dimension=200 and window=5, and change in
mean accuracy when deviating, measured in per-
centage points. dep has no window parameter.
cbow, cca and skip perform nearly identical,
while dep performs slightly better. Interestingly,
GloVe performs consistently worse than all other
embeddings, contrary to the findings published in
Pennington et al. (2014), but in line with Stratos et
al. (2015). dep performs best on nearly all tasks,
which may indicate that dependency-based context
is not only beneficial for preserving dependency-
related information, but also for morphology.
This finding is even more pronounced in the
evaluation using only ten dimensions (Figure 1 b)):
While dep can capture the different aspects tested
for nearly as well as with 200 dimensions, the other
embeddings suffer larger degradations, especially
for PoS and label prediction. cbow seems to be
able to cope better with low dimensionality than
skip, although they perform nearly identical on the
high dimensionality tasks. brown behaves similar
to the other approaches despite being quite different
algorithmically and only producing low-granular
data (with values for each dimension being either
1, 0, or −1). Note that results near the baseline
signify that the embeddings yield only minimal
benefit since the baseline does not use any features
at all.
Table 1 gives an overview of the average change
in accuracy when changing hyperparameters. Us-
ing 200 dimensions instead of 100 is beneficial for
all word embeddings. The difference is however
not nearly as pronounced as between ten and 100
dimensions. skip and cbow yield slightly better
results with a window of five, whereas for GloVe
a larger window is advantageous. dep achieves
both the highest average score and has the lowest
degradation when lowering the dimensionality.
Bansal et al. (2014) evaluate word embeddings
</bodyText>
<page confidence="0.979636">
2070
</page>
<bodyText confidence="0.999904181818182">
wrt. how they cluster along PoS tags. They first
divide the embeddings into 1000 clusters using k-
means and then associate each cluster with a PoS
tag. They report a clustering accuracy of 81.1%
for w = 11 and 85.8% for w = 5 using skip. Our
results, however, show an accuracy of 94.4% and
94.4%, respectively, i.e. no such difference. That
means that the PoS are still mostly linearly sepa-
rable with larger window sizes. The differences
observed by them could result from information
getting lost during clustering.
</bodyText>
<sectionHeader confidence="0.998041" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.984773235294118">
Word embeddings are able to capture a range of
syntactic and morphological information. They
align especially well with the word’s part of speech.
With a high dimensionality, most embeddings per-
form similarly, with GloVe performing on average
ten percentage points worse. With a low dimen-
sionality, the differences become more pronounced
and dep is the clear choice for applications where
using high-dimensional vectors is not feasible and
a correlation to the features tested in this paper is
wanted.
We have shown that the different word embed-
ding algorithms behave similar over a variety of
languages and perform well relative to the task’s
upper bounds.
The evaluation approach proposed yields in-
sights into the usefulness of embeddings for syntax-
related tasks, works on a wide variety of languages
and avoids inaccuracies introduced when employ-
ing unsupervised clustering for evaluation. We
hope that this evaluation approach will be useful
for evaluating future embedding techniques.
The software to replicate the experiments for this
paper is available on
http://arne.chark.eu/emnlp2015.
A Proof: Convexity of regions
To show that a one-versus-all classifier generates
exactly one convex polytope for each class, we
have to show that for any two points belonging to a
class, each point between them belongs to the same
class.
Let c E C be a class and rc C Rn be the re-
gion(s) of c in the vector space3 , i. e. where the
following holds true:
</bodyText>
<footnote confidence="0.911164666666667">
fc &gt; foboEC\c
3the vector space is assumed to have one dimension for the
bias.
</footnote>
<bodyText confidence="0.997034">
Let x, y E rc be two points classified into c. Then
the following statement needs to be true:
</bodyText>
<equation confidence="0.502257">
z E rc, z := (1 − A)x + Ay bA E [0, 1]
</equation>
<bodyText confidence="0.8683896">
Assume that z E/ rc. Then, by definition, fo(z) &gt;
fc(z) for some o E C \ c. We can substitute z with
its definition:
fo((1 − A)x + Ay) &gt; fc((1 − A)x + Ay)
And therefore due to the linearity of fo and fc:
</bodyText>
<equation confidence="0.936447">
(1 − A)fo(x) + Afo(y) &gt; (1 − A)fc(x) + Afc(y)
</equation>
<bodyText confidence="0.999434666666667">
But this cannot be, as by definition, fo(x) &lt; fc(x)
and fo(y) &lt; fc(y). Therefore, there is only one
region for c and that region is convex. ❑
</bodyText>
<sectionHeader confidence="0.994695" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998976121212121">
Anne Abeillé, Lionel Clément, and François Toussenel.
2003. Building a treebank for french. In Anne
Abeillé, editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Díaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency tree-
bank. In TLT-03, pages 201–204.
Rami Al-Rfou, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual nlp. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning, pages 183–192, Sofia, Bulgaria,
August. Association for Computational Linguistics.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fernández. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, nž
41 (2008), pages 147–154. XXIV edición del Con-
greso Anual de la Sociedad Española para el Proce-
samiento del Lenguaje Natural (SEPLN).
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the 52nd An-
nual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages 809–
815, Baltimore, Maryland, June. Association for
Computational Linguistics.
Jan Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In Eric P. Xing and Tony Jebara, editors,
Proceedings of The 31st International Conference
on Machine Learning, JMLR Workshop and Confer-
ence Proceedings, pages 1899–1907.
</reference>
<page confidence="0.921812">
2071
</page>
<reference confidence="0.998865196428572">
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, ed-
itors, Proceedings of the First Workshop on Tree-
banks and Linguistic Theories (TLT 2002), pages
24–41, Sozopol, Bulgaria.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deS-
ouza, Jenifer C. Lai, and Robert L. Mercer. 1992.
Class-based n-gram models of natural language.
Computational Linguistics, 18(4):467–479, Decem-
ber.
Marie Candito, Benoit Crabbé, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank
conversion and first results. In Proceedings of
LREC, Valletta, Malta.
Dóra Csendes, János Csirik, Tibor Gyimóthy, and An-
drás Kocsor. 2005. The Szeged treebank. In Pro-
ceedings of the 8th International Conference on Text,
Speech and Dialogue (TSD), Lecture Notes in Com-
puter Science, pages 123–132, Berlin / Heidelberg.
Springer.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116–131, January.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for En-
glish. In Kadri Muischnek Joakim Nivre, Heiki-
Jaan Kaalep and Mare Koit, editors, Proceedings of
the 16th Nordic Conference of Computational Lin-
guistics NODALIDA-2007, pages 105–112, Univer-
sity of Tartu, Tartu.
Arne Köhn, U Chun Lao, AmirAli B Zadeh, and
Kenji Sagae. 2014. Parsing morphologically rich
languages with (mostly) off-the-shelf software and
word vectors. In Proceedings of the 2014 Shared
Task of the COLING Workshop on Statistical Pars-
ing of Morphologically Rich Languages.
Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and
Tommi Jaakkola. 2014. Low-rank tensors for scor-
ing dependency structures. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1381–1391, Baltimore, Maryland, June. Association
for Computational Linguistics.
Omer Levy and Yoav Goldberg. 2014a. Dependency-
based word embeddings. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages
302–308, Baltimore, Maryland, June. Association
for Computational Linguistics.
Omer Levy and Yoav Goldberg. 2014b. Neural
word embedding as implicit matrix factorization.
In Z. Ghahramani, M. Welling, C. Cortes, N.D.
Lawrence, and K.Q. Weinberger, editors, Advances
in Neural Information Processing Systems 27, pages
2177–2185. Curran Associates, Inc.
Omer Levy, Steffen Remus, Chris Biemann, and Ido
Dagan. 2015. Do supervised distributional meth-
ods really learn lexical inference relations? In Pro-
ceedings of the 2015 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
970–976, Denver, Colorado, May–June. Association
for Computational Linguistics.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating predicate
argument structure. In Proceedings of the Workshop
on Human Language Technology, HLT ’94, pages
114–119, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Andre Martins, Miguel Almeida, and Noah A. Smith.
2013. Turning on the turbo: Fast third-order non-
projective turbo parsers. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
617–622, Sofia, Bulgaria, August.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed repre-
sentations of words and phrases and their composi-
tionality. In C.J.C. Burges, L. Bottou, M. Welling,
Z. Ghahramani, and K.Q. Weinberger, editors, Ad-
vances in Neural Information Processing Systems
26, pages 3111–3119. Curran Associates, Inc.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase struc-
ture and dependency annotation. In Proceedings of
LREC, pages 1392–1395, Genoa, Italy.
Alexandre Passos, Vineet Kumar, and Andrew McCal-
lum. 2014. Lexicon infused phrase embeddings for
named entity resolution. In Proceedings of the Eigh-
teenth Conference on Computational Natural Lan-
guage Learning, pages 78–86, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Jeffrey Pennington, Richard Socher, and Christo-
pher Manning. 2014. GloVe: Global vectors
for word representation. In Proceedings of the
2014 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 1532–
1543, Doha, Qatar, October. Association for Com-
putational Linguistics.
Djamé Seddah, Reut Tsarfaty, Sandra Kübler, Marie
Candito, Jinho Choi, Matthieu Constant, Richárd
Farkas, Iakes Goenaga, Koldo Gojenola, Yoav
Goldberg, Spence Green, Nizar Habash, Marco
</reference>
<page confidence="0.856796">
2072
</page>
<reference confidence="0.999592242424242">
Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam
Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yan-
nick Versley, Veronika Vincze, Marcin Woli´nski,
Alina Wróblewska, and Eric Villemonte de la Clérg-
erie. 2014. Overview of the SPMRL 2014 shared
task on parsing morphologically rich languages. In
Notes of the SPMRL 2014 Shared Task on Parsing
Morphologically-Rich Languages, Dublin, Ireland.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a ger-
man treebank. In Proceedings of the 8th Interna-
tional Conference on Language Resources and Eval-
uation, pages 3132–3139, Istanbul, Turkey. Euro-
pean Language Resources Association (ELRA).
Karl Stratos, Michael Collins, and Daniel Hsu. 2015.
Model-based word embeddings from decomposi-
tions of count matrices. In Proceedings of ACL.
Marek ´Swidzi´nski and Marcin Woli´nski. 2010. To-
wards a bank of constituent parse trees for Polish.
In Proceedings of Text, Speech and Dialogue, pages
197–204, Brno, Czech Republic.
Veronika Vincze, Dóra Szauter, Attila Almási, György
Móra, Zoltán Alexin, and János Csirik. 2010. Hun-
garian dependency treebank. In Proceedings of
LREC, Valletta, Malta.
Marcin Woli´nski, Katarzyna Głowi´nska, and Marek
´Swidzi´nski. 2011. A preliminary version of
Składnica—a treebank of Polish. In Proceedings of
the 5th Language &amp; Technology Conference, pages
299–303, Pozna´n, Poland.
Alina Wróblewska. 2012. Polish Dependency Bank.
Linguistic Issues in Language Technology, 7(1):1–
15.
</reference>
<page confidence="0.986121">
2073
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.775629">
<title confidence="0.9974895">What’s in an Embedding? Analyzing Word Embeddings through Multilingual Evaluation</title>
<author confidence="0.995309">Arne</author>
<affiliation confidence="0.9542455">Department of Universität</affiliation>
<email confidence="0.966427">koehn@informatik.uni-hamburg.de</email>
<abstract confidence="0.9939703">In the last two years, there has been a surge of word embedding algorithms and research on them. However, evaluation has mostly been carried out on a narrow set of tasks, mainly word similarity/relatedness and word relation similarity and on a single language, namely English. We propose an approach to evaluate embeddings on a variety of languages that also yields insights into the structure of the embedding space by investigating how well word embeddings cluster along different syntactic features. We show that all embedding approaches behave similarly in this task, with dependency-based embeddings performing best. This effect is even more pronounced when generating low dimensional embeddings.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeillé</author>
</authors>
<title>Lionel Clément, and François Toussenel.</title>
<date>2003</date>
<editor>In Anne Abeillé, editor, Treebanks.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<marker>Abeillé, 2003</marker>
<rawString>Anne Abeillé, Lionel Clément, and François Toussenel. 2003. Building a treebank for french. In Anne Abeillé, editor, Treebanks. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Aduriz</author>
<author>M J Aranzabe</author>
<author>J M Arriola</author>
<author>A Atutxa</author>
<author>A Díaz de Ilarraza</author>
<author>A Garmendia</author>
<author>M Oronoz</author>
</authors>
<title>Construction of a Basque dependency treebank.</title>
<date>2003</date>
<booktitle>In TLT-03,</booktitle>
<pages>201--204</pages>
<marker>Aduriz, Aranzabe, Arriola, Atutxa, de Ilarraza, Garmendia, Oronoz, 2003</marker>
<rawString>I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. Díaz de Ilarraza, A. Garmendia, and M. Oronoz. 2003. Construction of a Basque dependency treebank. In TLT-03, pages 201–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rami Al-Rfou</author>
<author>Bryan Perozzi</author>
<author>Steven Skiena</author>
</authors>
<title>Polyglot: Distributed word representations for multilingual nlp.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning,</booktitle>
<pages>183--192</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="11088" citStr="Al-Rfou et al. (2013)" startWordPosition="1770" endWordPosition="1773">ning set). Assuming that no sparsity issues exist, embeddingbased classification can yield at most accuracies as high as this approach. Note that because in reality data sparsity unfortunately does exist, this is only an approximation of the upper bound. We call this approximation up-approx and compute it omitting words not seen during training. 5 Experimental Setup Evaluation was carried out on Basque, English, French, German, Hungarian, Polish, and Swedish datasets. For English, automatically labeled data was obtained by tagging and parsing a subset of the English Wikipedia dump provided by Al-Rfou et al. (2013) using TurboTagger and TurboParser (Martins et al., 2013). The Penn Treebank (Marcus et al., 1994), converted using the LTH converter (Johansson and Nugues, 2007), was used as the corresponding manually annotated resource. For all other languages, datasets including both automatically and manually annotated data provided as part of the Shared Task on parsing morphologically rich languages (Seddah et al., 2014) were used.2 For all languages, we trained embeddings on the automatically labeled data using the approaches described in Section 3, with different window sizes (5 and 11, where applicabl</context>
</contexts>
<marker>Al-Rfou, Perozzi, Skiena, 2013</marker>
<rawString>Rami Al-Rfou, Bryan Perozzi, and Steven Skiena. 2013. Polyglot: Distributed word representations for multilingual nlp. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183–192, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Aldezabal</author>
<author>M J Aranzabe</author>
<author>A Diaz de Ilarraza</author>
<author>K Fernández</author>
</authors>
<title>From dependencies to constituents in the reference corpus for the processing of Basque.</title>
<date>2008</date>
<booktitle>In Procesamiento del Lenguaje Natural, nž</booktitle>
<volume>41</volume>
<pages>147--154</pages>
<marker>Aldezabal, Aranzabe, de Ilarraza, Fernández, 2008</marker>
<rawString>I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and K. Fernández. 2008. From dependencies to constituents in the reference corpus for the processing of Basque. In Procesamiento del Lenguaje Natural, nž 41 (2008), pages 147–154. XXIV edición del Congreso Anual de la Sociedad Española para el Procesamiento del Lenguaje Natural (SEPLN).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>809--815</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="7597" citStr="Bansal et al. (2014)" startWordPosition="1193" endWordPosition="1196"> missing dimensions with 0. This way, we obtain an embedding interpretation of the clusters. Note that, in contrast to clustering embeddings, no information is lost. 4 Our Evaluation by Classification Tasks We classify words separately according to several tasks with an L2-regularized linear classifier. All classification tasks are based on the word embedding of a single word alone, without any other information about the word or its context; in particular, the word’s lexicalization is not used as a feature. By using the continuous features directly instead of clustering them (as e.g. done by Bansal et al. (2014)), we ensure that no information is lost during preprocessing. All tasks can be carried out on dependency treebanks with morphological annotation. From each word in the treebank, we extract a data point (word embedding, class) for training/testing, where class is of one of the following, depending on the task: pos The Part-of-Speech of the word headpos The PoS of the word’s head label The label of the word’s dependency edge gender* The gender of the word case* The case of the word number* The number of the word tense* The tense of the word Tasks marked with an asterisk are only carried out on </context>
<context position="15815" citStr="Bansal et al. (2014)" startWordPosition="2593" endWordPosition="2596">the baseline signify that the embeddings yield only minimal benefit since the baseline does not use any features at all. Table 1 gives an overview of the average change in accuracy when changing hyperparameters. Using 200 dimensions instead of 100 is beneficial for all word embeddings. The difference is however not nearly as pronounced as between ten and 100 dimensions. skip and cbow yield slightly better results with a window of five, whereas for GloVe a larger window is advantageous. dep achieves both the highest average score and has the lowest degradation when lowering the dimensionality. Bansal et al. (2014) evaluate word embeddings 2070 wrt. how they cluster along PoS tags. They first divide the embeddings into 1000 clusters using kmeans and then associate each cluster with a PoS tag. They report a clustering accuracy of 81.1% for w = 11 and 85.8% for w = 5 using skip. Our results, however, show an accuracy of 94.4% and 94.4%, respectively, i.e. no such difference. That means that the PoS are still mostly linearly separable with larger window sizes. The differences observed by them could result from information getting lost during clustering. 7 Conclusions Word embeddings are able to capture a r</context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 809– 815, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Botha</author>
<author>Phil Blunsom</author>
</authors>
<title>Compositional morphology for word representations and language modelling.</title>
<date>2014</date>
<booktitle>Proceedings of The 31st International Conference on Machine Learning, JMLR Workshop and Conference Proceedings,</booktitle>
<pages>1899--1907</pages>
<editor>In Eric P. Xing and Tony Jebara, editors,</editor>
<contexts>
<context position="1098" citStr="Botha and Blunsom, 2014" startWordPosition="160" endWordPosition="163">oach to evaluate embeddings on a variety of languages that also yields insights into the structure of the embedding space by investigating how well word embeddings cluster along different syntactic features. We show that all embedding approaches behave similarly in this task, with dependency-based embeddings performing best. This effect is even more pronounced when generating low dimensional embeddings. 1 Introduction Word embeddings map words into a vector space, allowing to reason about words in this space. They have been shown to be beneficial for several tasks such as machine translation (Botha and Blunsom, 2014), parsing (Lei et al., 2014), and named entity recognition (Passos et al., 2014). Recently, word embedding techniques have been studied for their mathematical properties (Levy and Goldberg, 2014b; Stratos et al., 2015), yielding a better understanding of the underlying optimization criteria. However, word embeddings have mostly been studied and evaluated on a single language (English). Therefore, validation on languages other than English is lacking and the question whether word embeddings work the same way across languages has not been empirically evaluated. Evaluations of complex systems – s</context>
<context position="3886" citStr="Botha and Blunsom (2014)" startWordPosition="596" endWordPosition="599">nd not, as would be expected, the relation between two words. Another approach to evaluate embeddings, used by Pennington et al. (2014) amongst others, is to rank a fixed set of words relative to a reference word. The results are then compared to human judgments, e.g. from the WS353 corpus (Finkelstein et al., 2002). This approach has a limited coverage and additional data is expensive to obtain. 2067 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Botha and Blunsom (2014) propose to factorize word vectors into morpheme vectors to better capture similarities between morphologically related words and evaluate their word representations using log-bilinear language models based on their word vectors.1 They measure model perplexity reduction relative to n-gram language models and include their model into a machine translation system, gaining between 0 (English → German) and 1.2 (English → Russian) BLEU points. Lei et al. (2014) introduce a syntactic dependency parser using (amongst others) a low-rank tensor component for scoring dependency edges. This scoring can e</context>
</contexts>
<marker>Botha, Blunsom, 2014</marker>
<rawString>Jan Botha and Phil Blunsom. 2014. Compositional morphology for word representations and language modelling. In Eric P. Xing and Tony Jebara, editors, Proceedings of The 31st International Conference on Machine Learning, JMLR Workshop and Conference Proceedings, pages 1899–1907.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Brants</author>
<author>Stefanie Dipper</author>
<author>Silvia Hansen</author>
<author>Wolfgang Lezius</author>
<author>George Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Erhard Hinrichs and Kiril Simov, editors, Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT 2002),</booktitle>
<pages>24--41</pages>
<location>Sozopol, Bulgaria.</location>
<contexts>
<context position="12928" citStr="Brants et al., 2002" startWordPosition="2065" endWordPosition="2068">ize = 11 for GloVe, compare Table 1). All embeddings capture the PoS well. To a lesser degree, the dependency label and head PoS can also be recovered. The better-performing embeddings achieve results near the approximate upper bound for all tasks. The embeddings also mostly cluster well with respect to tense, number, gender, and case, with tense showing the best correlations. For some of these tasks, the baseline is however fairly high because the number of classes is lower. 2Basque: (Aduriz et al., 2003; Aldezabal et al., 2008), French: (Abeillé et al., 2003; Candito et al., 2010), German: (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian: (Csendes et al., 2005; Vincze et al., 2010), Polish: (Woli´nski et al., 2011; ´Swidzi´nski and Woli´nski, 2010; Wróblewska, 2012), Swedish: (Nivre et al., 2006) 2069 a) dimensionality 200 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number b) dimensionality 10 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number cca skip glove baseline dep cbow brown appro</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank. In Erhard Hinrichs and Kiril Simov, editors, Proceedings of the First Workshop on Treebanks and Linguistic Theories (TLT 2002), pages 24–41, Sozopol, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="6605" citStr="Brown et al., 1992" startWordPosition="1025" endWordPosition="1028">f w is defined as all words that are either the head or dependents of w. We will call this approach dep. 1Their approach has not been evaluated in this paper as the corresponding code is not available as of now. GloVe, introduced by Pennington et al. (2014), optimizes the ratio of co-occurrence probabilities instead of the co-occurrence probabilities themselves, getting rid of the negative sampling used for the approaches previously mentioned. Stratos et al. (2015) describe a method to derive word embeddings using canonical correlation analysis. We will call this approach cca. brown clusters (Brown et al., 1992) are constructed by clustering words hierarchically into a binary search tree in a way that maximizes mutual information for a language model. To construct an embedding for a cluster c, we use the following procedure: For each edge on the path from the root to c, add either 1 or −1, depending on the direction of descent. Because not every path has the same depth, we pad missing dimensions with 0. This way, we obtain an embedding interpretation of the clusters. Note that, in contrast to clustering embeddings, no information is lost. 4 Our Evaluation by Classification Tasks We classify words sep</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benoit Crabbé</author>
<author>Pascal Denis</author>
</authors>
<title>Statistical French dependency parsing: Treebank conversion and first results.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Valletta,</location>
<contexts>
<context position="12898" citStr="Candito et al., 2010" startWordPosition="2060" endWordPosition="2063">for cca, cbow and skip, window size = 11 for GloVe, compare Table 1). All embeddings capture the PoS well. To a lesser degree, the dependency label and head PoS can also be recovered. The better-performing embeddings achieve results near the approximate upper bound for all tasks. The embeddings also mostly cluster well with respect to tense, number, gender, and case, with tense showing the best correlations. For some of these tasks, the baseline is however fairly high because the number of classes is lower. 2Basque: (Aduriz et al., 2003; Aldezabal et al., 2008), French: (Abeillé et al., 2003; Candito et al., 2010), German: (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian: (Csendes et al., 2005; Vincze et al., 2010), Polish: (Woli´nski et al., 2011; ´Swidzi´nski and Woli´nski, 2010; Wróblewska, 2012), Swedish: (Nivre et al., 2006) 2069 a) dimensionality 200 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number b) dimensionality 10 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number cca skip glove</context>
</contexts>
<marker>Candito, Crabbé, Denis, 2010</marker>
<rawString>Marie Candito, Benoit Crabbé, and Pascal Denis. 2010. Statistical French dependency parsing: Treebank conversion and first results. In Proceedings of LREC, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dóra Csendes</author>
<author>János Csirik</author>
<author>Tibor Gyimóthy</author>
<author>András Kocsor</author>
</authors>
<title>The Szeged treebank.</title>
<date>2005</date>
<booktitle>In Proceedings of the 8th International Conference on Text, Speech and Dialogue (TSD), Lecture Notes in Computer Science,</booktitle>
<pages>123--132</pages>
<publisher>Springer.</publisher>
<location>Berlin / Heidelberg.</location>
<contexts>
<context position="12986" citStr="Csendes et al., 2005" startWordPosition="2074" endWordPosition="2077">ure the PoS well. To a lesser degree, the dependency label and head PoS can also be recovered. The better-performing embeddings achieve results near the approximate upper bound for all tasks. The embeddings also mostly cluster well with respect to tense, number, gender, and case, with tense showing the best correlations. For some of these tasks, the baseline is however fairly high because the number of classes is lower. 2Basque: (Aduriz et al., 2003; Aldezabal et al., 2008), French: (Abeillé et al., 2003; Candito et al., 2010), German: (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian: (Csendes et al., 2005; Vincze et al., 2010), Polish: (Woli´nski et al., 2011; ´Swidzi´nski and Woli´nski, 2010; Wróblewska, 2012), Swedish: (Nivre et al., 2006) 2069 a) dimensionality 200 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number b) dimensionality 10 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number cca skip glove baseline dep cbow brown approx-up Figure 1: Results with window = 5 (for cbow, cca &amp; sk</context>
</contexts>
<marker>Csendes, Csirik, Gyimóthy, Kocsor, 2005</marker>
<rawString>Dóra Csendes, János Csirik, Tibor Gyimóthy, and András Kocsor. 2005. The Szeged treebank. In Proceedings of the 8th International Conference on Text, Speech and Dialogue (TSD), Lecture Notes in Computer Science, pages 123–132, Berlin / Heidelberg. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="3579" citStr="Finkelstein et al., 2002" startWordPosition="553" endWordPosition="557"> be constructed for each language. This type of evaluation primarily describes the similarity between vector differences and not similarity between vectors. In addition, Levy et al. (2015) showed for this task that word embedding-based classifiers actually mostly learn whether a word is a general hypernym and not, as would be expected, the relation between two words. Another approach to evaluate embeddings, used by Pennington et al. (2014) amongst others, is to rank a fixed set of words relative to a reference word. The results are then compared to human judgments, e.g. from the WS353 corpus (Finkelstein et al., 2002). This approach has a limited coverage and additional data is expensive to obtain. 2067 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Botha and Blunsom (2014) propose to factorize word vectors into morpheme vectors to better capture similarities between morphologically related words and evaluate their word representations using log-bilinear language models based on their word vectors.1 They measure model perplexity reduction relative to n-gram lan</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116–131, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for English.</title>
<date>2007</date>
<booktitle>In Kadri Muischnek Joakim Nivre, HeikiJaan Kaalep and Mare Koit, editors, Proceedings of the 16th Nordic Conference of Computational Linguistics NODALIDA-2007,</booktitle>
<pages>105--112</pages>
<institution>University of Tartu,</institution>
<location>Tartu.</location>
<contexts>
<context position="11250" citStr="Johansson and Nugues, 2007" startWordPosition="1794" endWordPosition="1797"> reality data sparsity unfortunately does exist, this is only an approximation of the upper bound. We call this approximation up-approx and compute it omitting words not seen during training. 5 Experimental Setup Evaluation was carried out on Basque, English, French, German, Hungarian, Polish, and Swedish datasets. For English, automatically labeled data was obtained by tagging and parsing a subset of the English Wikipedia dump provided by Al-Rfou et al. (2013) using TurboTagger and TurboParser (Martins et al., 2013). The Penn Treebank (Marcus et al., 1994), converted using the LTH converter (Johansson and Nugues, 2007), was used as the corresponding manually annotated resource. For all other languages, datasets including both automatically and manually annotated data provided as part of the Shared Task on parsing morphologically rich languages (Seddah et al., 2014) were used.2 For all languages, we trained embeddings on the automatically labeled data using the approaches described in Section 3, with different window sizes (5 and 11, where applicable) and dimensions (10, 100, 200). The rare word limit was set to five words occurrences. brown was only trained with 1024 clusters equaling about 10 dimensions, a</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for English. In Kadri Muischnek Joakim Nivre, HeikiJaan Kaalep and Mare Koit, editors, Proceedings of the 16th Nordic Conference of Computational Linguistics NODALIDA-2007, pages 105–112, University of Tartu, Tartu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arne Köhn</author>
<author>U Chun Lao</author>
<author>AmirAli B Zadeh</author>
<author>Kenji Sagae</author>
</authors>
<title>Parsing morphologically rich languages with (mostly) off-the-shelf software and word vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Shared Task of the COLING Workshop on Statistical Parsing of Morphologically Rich Languages.</booktitle>
<contexts>
<context position="4693" citStr="Köhn et al. (2014)" startWordPosition="721" endWordPosition="724">uage models based on their word vectors.1 They measure model perplexity reduction relative to n-gram language models and include their model into a machine translation system, gaining between 0 (English → German) and 1.2 (English → Russian) BLEU points. Lei et al. (2014) introduce a syntactic dependency parser using (amongst others) a low-rank tensor component for scoring dependency edges. This scoring can employ word embeddings. Doing so yields an improvement of 0.2 to 0.5 percentage points. If no Part-of-Speech (PoS) tags are available, this difference rises to up to four percentage points. Köhn et al. (2014) show that this gain from using word embeddings is even more pronounced in complete absence of morphological information (including PoS tags), reporting a difference of five to seven percentage points,depending on the language, using the same parser. With these findings, it can be assumed that word embeddings encode some kind of morphological information. Neither, however, investigated what kind of information the word embeddings actually contain. 3 The Embedding Algorithms To assess the differences between embedding algorithms, we will evaluate six different approaches. The continuous bag-of-</context>
</contexts>
<marker>Köhn, Lao, Zadeh, Sagae, 2014</marker>
<rawString>Arne Köhn, U Chun Lao, AmirAli B Zadeh, and Kenji Sagae. 2014. Parsing morphologically rich languages with (mostly) off-the-shelf software and word vectors. In Proceedings of the 2014 Shared Task of the COLING Workshop on Statistical Parsing of Morphologically Rich Languages.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Yu Xin</author>
<author>Yuan Zhang</author>
<author>Regina Barzilay</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Low-rank tensors for scoring dependency structures.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1381--1391</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="1126" citStr="Lei et al., 2014" startWordPosition="165" endWordPosition="168">riety of languages that also yields insights into the structure of the embedding space by investigating how well word embeddings cluster along different syntactic features. We show that all embedding approaches behave similarly in this task, with dependency-based embeddings performing best. This effect is even more pronounced when generating low dimensional embeddings. 1 Introduction Word embeddings map words into a vector space, allowing to reason about words in this space. They have been shown to be beneficial for several tasks such as machine translation (Botha and Blunsom, 2014), parsing (Lei et al., 2014), and named entity recognition (Passos et al., 2014). Recently, word embedding techniques have been studied for their mathematical properties (Levy and Goldberg, 2014b; Stratos et al., 2015), yielding a better understanding of the underlying optimization criteria. However, word embeddings have mostly been studied and evaluated on a single language (English). Therefore, validation on languages other than English is lacking and the question whether word embeddings work the same way across languages has not been empirically evaluated. Evaluations of complex systems – such as parsers – employing w</context>
<context position="4346" citStr="Lei et al. (2014)" startWordPosition="666" endWordPosition="669">ural Language Processing, pages 2067–2073, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Botha and Blunsom (2014) propose to factorize word vectors into morpheme vectors to better capture similarities between morphologically related words and evaluate their word representations using log-bilinear language models based on their word vectors.1 They measure model perplexity reduction relative to n-gram language models and include their model into a machine translation system, gaining between 0 (English → German) and 1.2 (English → Russian) BLEU points. Lei et al. (2014) introduce a syntactic dependency parser using (amongst others) a low-rank tensor component for scoring dependency edges. This scoring can employ word embeddings. Doing so yields an improvement of 0.2 to 0.5 percentage points. If no Part-of-Speech (PoS) tags are available, this difference rises to up to four percentage points. Köhn et al. (2014) show that this gain from using word embeddings is even more pronounced in complete absence of morphological information (including PoS tags), reporting a difference of five to seven percentage points,depending on the language, using the same parser. Wi</context>
</contexts>
<marker>Lei, Xin, Zhang, Barzilay, Jaakkola, 2014</marker>
<rawString>Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, and Tommi Jaakkola. 2014. Low-rank tensors for scoring dependency structures. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1381–1391, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Dependencybased word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>302--308</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="1292" citStr="Levy and Goldberg, 2014" startWordPosition="189" endWordPosition="192">ic features. We show that all embedding approaches behave similarly in this task, with dependency-based embeddings performing best. This effect is even more pronounced when generating low dimensional embeddings. 1 Introduction Word embeddings map words into a vector space, allowing to reason about words in this space. They have been shown to be beneficial for several tasks such as machine translation (Botha and Blunsom, 2014), parsing (Lei et al., 2014), and named entity recognition (Passos et al., 2014). Recently, word embedding techniques have been studied for their mathematical properties (Levy and Goldberg, 2014b; Stratos et al., 2015), yielding a better understanding of the underlying optimization criteria. However, word embeddings have mostly been studied and evaluated on a single language (English). Therefore, validation on languages other than English is lacking and the question whether word embeddings work the same way across languages has not been empirically evaluated. Evaluations of complex systems – such as parsers – employing word embeddings generally give only little insight into the type of contribution to the result and the structure of word embeddings. We aim to fill these gaps by evalu</context>
<context position="5816" citStr="Levy and Goldberg (2014" startWordPosition="897" endWordPosition="900">nces between embedding algorithms, we will evaluate six different approaches. The continuous bag-of-words (cbow) approach described by Mikolov et al. (2013a) is learned by predicting the word vector based on the context vectors. The skip-gram approach (skip) from the same authors is doing the reverse: it predicts the context word vectors based on the embedding of the current word. We use the version of cbow and skip as described in (Mikolov et al., 2013b) which use negative sampling, i.e. they train by distinguishing the correct word in its context against words not occurring in that context. Levy and Goldberg (2014a) alter the skipgram approach by not using the neighboring words wrt. the sentence’s word sequence but wrt. the dependency tree of the sentence. Therefore, the context of w is defined as all words that are either the head or dependents of w. We will call this approach dep. 1Their approach has not been evaluated in this paper as the corresponding code is not available as of now. GloVe, introduced by Pennington et al. (2014), optimizes the ratio of co-occurrence probabilities instead of the co-occurrence probabilities themselves, getting rid of the negative sampling used for the approaches prev</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014a. Dependencybased word embeddings. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 302–308, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Yoav Goldberg</author>
</authors>
<title>Neural word embedding as implicit matrix factorization. In</title>
<date>2014</date>
<booktitle>Advances in Neural Information Processing Systems 27,</booktitle>
<pages>2177--2185</pages>
<editor>Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="1292" citStr="Levy and Goldberg, 2014" startWordPosition="189" endWordPosition="192">ic features. We show that all embedding approaches behave similarly in this task, with dependency-based embeddings performing best. This effect is even more pronounced when generating low dimensional embeddings. 1 Introduction Word embeddings map words into a vector space, allowing to reason about words in this space. They have been shown to be beneficial for several tasks such as machine translation (Botha and Blunsom, 2014), parsing (Lei et al., 2014), and named entity recognition (Passos et al., 2014). Recently, word embedding techniques have been studied for their mathematical properties (Levy and Goldberg, 2014b; Stratos et al., 2015), yielding a better understanding of the underlying optimization criteria. However, word embeddings have mostly been studied and evaluated on a single language (English). Therefore, validation on languages other than English is lacking and the question whether word embeddings work the same way across languages has not been empirically evaluated. Evaluations of complex systems – such as parsers – employing word embeddings generally give only little insight into the type of contribution to the result and the structure of word embeddings. We aim to fill these gaps by evalu</context>
<context position="5816" citStr="Levy and Goldberg (2014" startWordPosition="897" endWordPosition="900">nces between embedding algorithms, we will evaluate six different approaches. The continuous bag-of-words (cbow) approach described by Mikolov et al. (2013a) is learned by predicting the word vector based on the context vectors. The skip-gram approach (skip) from the same authors is doing the reverse: it predicts the context word vectors based on the embedding of the current word. We use the version of cbow and skip as described in (Mikolov et al., 2013b) which use negative sampling, i.e. they train by distinguishing the correct word in its context against words not occurring in that context. Levy and Goldberg (2014a) alter the skipgram approach by not using the neighboring words wrt. the sentence’s word sequence but wrt. the dependency tree of the sentence. Therefore, the context of w is defined as all words that are either the head or dependents of w. We will call this approach dep. 1Their approach has not been evaluated in this paper as the corresponding code is not available as of now. GloVe, introduced by Pennington et al. (2014), optimizes the ratio of co-occurrence probabilities instead of the co-occurrence probabilities themselves, getting rid of the negative sampling used for the approaches prev</context>
</contexts>
<marker>Levy, Goldberg, 2014</marker>
<rawString>Omer Levy and Yoav Goldberg. 2014b. Neural word embedding as implicit matrix factorization. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 2177–2185. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omer Levy</author>
<author>Steffen Remus</author>
<author>Chris Biemann</author>
<author>Ido Dagan</author>
</authors>
<title>Do supervised distributional methods really learn lexical inference relations?</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>970--976</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado, May–June.</location>
<contexts>
<context position="3142" citStr="Levy et al. (2015)" startWordPosition="481" endWordPosition="484">word finding and classification tasks (as this paper also proposes) or application-oriented end-to-end evaluations as part of a larger system. Word finding tasks are of the form “given a pair of words (x, y), find a y&apos; for a given x&apos;”, e.g. given (Rome, Italy), find a word for Oslo. These tasks have been introduced by Mikolov et al. (2013a). The downside of this kind of task is that the data is not readily available and has to be constructed for each language. This type of evaluation primarily describes the similarity between vector differences and not similarity between vectors. In addition, Levy et al. (2015) showed for this task that word embedding-based classifiers actually mostly learn whether a word is a general hypernym and not, as would be expected, the relation between two words. Another approach to evaluate embeddings, used by Pennington et al. (2014) amongst others, is to rank a fixed set of words relative to a reference word. The results are then compared to human judgments, e.g. from the WS353 corpus (Finkelstein et al., 2002). This approach has a limited coverage and additional data is expensive to obtain. 2067 Proceedings of the 2015 Conference on Empirical Methods in Natural Language</context>
</contexts>
<marker>Levy, Remus, Biemann, Dagan, 2015</marker>
<rawString>Omer Levy, Steffen Remus, Chris Biemann, and Ido Dagan. 2015. Do supervised distributional methods really learn lexical inference relations? In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 970–976, Denver, Colorado, May–June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Grace Kim</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Robert MacIntyre</author>
<author>Ann Bies</author>
<author>Mark Ferguson</author>
<author>Karen Katz</author>
<author>Britta Schasberger</author>
</authors>
<title>The Penn Treebank: Annotating predicate argument structure.</title>
<date>1994</date>
<booktitle>In Proceedings of the Workshop on Human Language Technology, HLT ’94,</booktitle>
<pages>114--119</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="11186" citStr="Marcus et al., 1994" startWordPosition="1785" endWordPosition="1788">accuracies as high as this approach. Note that because in reality data sparsity unfortunately does exist, this is only an approximation of the upper bound. We call this approximation up-approx and compute it omitting words not seen during training. 5 Experimental Setup Evaluation was carried out on Basque, English, French, German, Hungarian, Polish, and Swedish datasets. For English, automatically labeled data was obtained by tagging and parsing a subset of the English Wikipedia dump provided by Al-Rfou et al. (2013) using TurboTagger and TurboParser (Martins et al., 2013). The Penn Treebank (Marcus et al., 1994), converted using the LTH converter (Johansson and Nugues, 2007), was used as the corresponding manually annotated resource. For all other languages, datasets including both automatically and manually annotated data provided as part of the Shared Task on parsing morphologically rich languages (Seddah et al., 2014) were used.2 For all languages, we trained embeddings on the automatically labeled data using the approaches described in Section 3, with different window sizes (5 and 11, where applicable) and dimensions (10, 100, 200). The rare word limit was set to five words occurrences. brown was</context>
</contexts>
<marker>Marcus, Kim, Marcinkiewicz, MacIntyre, Bies, Ferguson, Katz, Schasberger, 1994</marker>
<rawString>Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Proceedings of the Workshop on Human Language Technology, HLT ’94, pages 114–119, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Miguel Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order nonprojective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>617--622</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="11145" citStr="Martins et al., 2013" startWordPosition="1778" endWordPosition="1781">ingbased classification can yield at most accuracies as high as this approach. Note that because in reality data sparsity unfortunately does exist, this is only an approximation of the upper bound. We call this approximation up-approx and compute it omitting words not seen during training. 5 Experimental Setup Evaluation was carried out on Basque, English, French, German, Hungarian, Polish, and Swedish datasets. For English, automatically labeled data was obtained by tagging and parsing a subset of the English Wikipedia dump provided by Al-Rfou et al. (2013) using TurboTagger and TurboParser (Martins et al., 2013). The Penn Treebank (Marcus et al., 1994), converted using the LTH converter (Johansson and Nugues, 2007), was used as the corresponding manually annotated resource. For all other languages, datasets including both automatically and manually annotated data provided as part of the Shared Task on parsing morphologically rich languages (Seddah et al., 2014) were used.2 For all languages, we trained embeddings on the automatically labeled data using the approaches described in Section 3, with different window sizes (5 and 11, where applicable) and dimensions (10, 100, 200). The rare word limit was</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andre Martins, Miguel Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order nonprojective turbo parsers. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 617–622, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<contexts>
<context position="2864" citStr="Mikolov et al. (2013" startWordPosition="435" endWordPosition="438"> for several languages and we expect a correlation with usefulness of word embeddings for syntax-related tasks such as named entity recognition, parsing, and morphological analysis. 2 Related Work Previous approaches to word embedding evaluation have either used relatively basic word finding and classification tasks (as this paper also proposes) or application-oriented end-to-end evaluations as part of a larger system. Word finding tasks are of the form “given a pair of words (x, y), find a y&apos; for a given x&apos;”, e.g. given (Rome, Italy), find a word for Oslo. These tasks have been introduced by Mikolov et al. (2013a). The downside of this kind of task is that the data is not readily available and has to be constructed for each language. This type of evaluation primarily describes the similarity between vector differences and not similarity between vectors. In addition, Levy et al. (2015) showed for this task that word embedding-based classifiers actually mostly learn whether a word is a general hypernym and not, as would be expected, the relation between two words. Another approach to evaluate embeddings, used by Pennington et al. (2014) amongst others, is to rank a fixed set of words relative to a refe</context>
<context position="5348" citStr="Mikolov et al. (2013" startWordPosition="817" endWordPosition="820">d embeddings is even more pronounced in complete absence of morphological information (including PoS tags), reporting a difference of five to seven percentage points,depending on the language, using the same parser. With these findings, it can be assumed that word embeddings encode some kind of morphological information. Neither, however, investigated what kind of information the word embeddings actually contain. 3 The Embedding Algorithms To assess the differences between embedding algorithms, we will evaluate six different approaches. The continuous bag-of-words (cbow) approach described by Mikolov et al. (2013a) is learned by predicting the word vector based on the context vectors. The skip-gram approach (skip) from the same authors is doing the reverse: it predicts the context word vectors based on the embedding of the current word. We use the version of cbow and skip as described in (Mikolov et al., 2013b) which use negative sampling, i.e. they train by distinguishing the correct word in its context against words not occurring in that context. Levy and Goldberg (2014a) alter the skipgram approach by not using the neighboring words wrt. the sentence’s word sequence but wrt. the dependency tree of </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<editor>In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,</editor>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="2864" citStr="Mikolov et al. (2013" startWordPosition="435" endWordPosition="438"> for several languages and we expect a correlation with usefulness of word embeddings for syntax-related tasks such as named entity recognition, parsing, and morphological analysis. 2 Related Work Previous approaches to word embedding evaluation have either used relatively basic word finding and classification tasks (as this paper also proposes) or application-oriented end-to-end evaluations as part of a larger system. Word finding tasks are of the form “given a pair of words (x, y), find a y&apos; for a given x&apos;”, e.g. given (Rome, Italy), find a word for Oslo. These tasks have been introduced by Mikolov et al. (2013a). The downside of this kind of task is that the data is not readily available and has to be constructed for each language. This type of evaluation primarily describes the similarity between vector differences and not similarity between vectors. In addition, Levy et al. (2015) showed for this task that word embedding-based classifiers actually mostly learn whether a word is a general hypernym and not, as would be expected, the relation between two words. Another approach to evaluate embeddings, used by Pennington et al. (2014) amongst others, is to rank a fixed set of words relative to a refe</context>
<context position="5348" citStr="Mikolov et al. (2013" startWordPosition="817" endWordPosition="820">d embeddings is even more pronounced in complete absence of morphological information (including PoS tags), reporting a difference of five to seven percentage points,depending on the language, using the same parser. With these findings, it can be assumed that word embeddings encode some kind of morphological information. Neither, however, investigated what kind of information the word embeddings actually contain. 3 The Embedding Algorithms To assess the differences between embedding algorithms, we will evaluate six different approaches. The continuous bag-of-words (cbow) approach described by Mikolov et al. (2013a) is learned by predicting the word vector based on the context vectors. The skip-gram approach (skip) from the same authors is doing the reverse: it predicts the context word vectors based on the embedding of the current word. We use the version of cbow and skip as described in (Mikolov et al., 2013b) which use negative sampling, i.e. they train by distinguishing the correct word in its context against words not occurring in that context. Levy and Goldberg (2014a) alter the skipgram approach by not using the neighboring words wrt. the sentence’s word sequence but wrt. the dependency tree of </context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In C.J.C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 26, pages 3111–3119. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
<author>Johan Hall</author>
</authors>
<title>Talbanken05: A Swedish treebank with phrase structure and dependency annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<pages>1392--1395</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="13125" citStr="Nivre et al., 2006" startWordPosition="2094" endWordPosition="2097">lts near the approximate upper bound for all tasks. The embeddings also mostly cluster well with respect to tense, number, gender, and case, with tense showing the best correlations. For some of these tasks, the baseline is however fairly high because the number of classes is lower. 2Basque: (Aduriz et al., 2003; Aldezabal et al., 2008), French: (Abeillé et al., 2003; Candito et al., 2010), German: (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian: (Csendes et al., 2005; Vincze et al., 2010), Polish: (Woli´nski et al., 2011; ´Swidzi´nski and Woli´nski, 2010; Wróblewska, 2012), Swedish: (Nivre et al., 2006) 2069 a) dimensionality 200 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number b) dimensionality 10 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number cca skip glove baseline dep cbow brown approx-up Figure 1: Results with window = 5 (for cbow, cca &amp; skip) / 11 (for GloVe) for Basque, English, French, German, Hungarian, Polish, Swedish. Note: brown is only present in b). w d cca skip cbow </context>
</contexts>
<marker>Nivre, Nilsson, Hall, 2006</marker>
<rawString>Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Talbanken05: A Swedish treebank with phrase structure and dependency annotation. In Proceedings of LREC, pages 1392–1395, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Passos</author>
<author>Vineet Kumar</author>
<author>Andrew McCallum</author>
</authors>
<title>Lexicon infused phrase embeddings for named entity resolution.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning,</booktitle>
<pages>78--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="1178" citStr="Passos et al., 2014" startWordPosition="174" endWordPosition="177"> the structure of the embedding space by investigating how well word embeddings cluster along different syntactic features. We show that all embedding approaches behave similarly in this task, with dependency-based embeddings performing best. This effect is even more pronounced when generating low dimensional embeddings. 1 Introduction Word embeddings map words into a vector space, allowing to reason about words in this space. They have been shown to be beneficial for several tasks such as machine translation (Botha and Blunsom, 2014), parsing (Lei et al., 2014), and named entity recognition (Passos et al., 2014). Recently, word embedding techniques have been studied for their mathematical properties (Levy and Goldberg, 2014b; Stratos et al., 2015), yielding a better understanding of the underlying optimization criteria. However, word embeddings have mostly been studied and evaluated on a single language (English). Therefore, validation on languages other than English is lacking and the question whether word embeddings work the same way across languages has not been empirically evaluated. Evaluations of complex systems – such as parsers – employing word embeddings generally give only little insight in</context>
</contexts>
<marker>Passos, Kumar, McCallum, 2014</marker>
<rawString>Alexandre Passos, Vineet Kumar, and Andrew McCallum. 2014. Lexicon infused phrase embeddings for named entity resolution. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning, pages 78–86, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>GloVe: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1532--1543</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="3397" citStr="Pennington et al. (2014)" startWordPosition="521" endWordPosition="524">ome, Italy), find a word for Oslo. These tasks have been introduced by Mikolov et al. (2013a). The downside of this kind of task is that the data is not readily available and has to be constructed for each language. This type of evaluation primarily describes the similarity between vector differences and not similarity between vectors. In addition, Levy et al. (2015) showed for this task that word embedding-based classifiers actually mostly learn whether a word is a general hypernym and not, as would be expected, the relation between two words. Another approach to evaluate embeddings, used by Pennington et al. (2014) amongst others, is to rank a fixed set of words relative to a reference word. The results are then compared to human judgments, e.g. from the WS353 corpus (Finkelstein et al., 2002). This approach has a limited coverage and additional data is expensive to obtain. 2067 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2067–2073, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. Botha and Blunsom (2014) propose to factorize word vectors into morpheme vectors to better capture similarities between morphologically</context>
<context position="6243" citStr="Pennington et al. (2014)" startWordPosition="972" endWordPosition="975">cribed in (Mikolov et al., 2013b) which use negative sampling, i.e. they train by distinguishing the correct word in its context against words not occurring in that context. Levy and Goldberg (2014a) alter the skipgram approach by not using the neighboring words wrt. the sentence’s word sequence but wrt. the dependency tree of the sentence. Therefore, the context of w is defined as all words that are either the head or dependents of w. We will call this approach dep. 1Their approach has not been evaluated in this paper as the corresponding code is not available as of now. GloVe, introduced by Pennington et al. (2014), optimizes the ratio of co-occurrence probabilities instead of the co-occurrence probabilities themselves, getting rid of the negative sampling used for the approaches previously mentioned. Stratos et al. (2015) describe a method to derive word embeddings using canonical correlation analysis. We will call this approach cca. brown clusters (Brown et al., 1992) are constructed by clustering words hierarchically into a binary search tree in a way that maximizes mutual information for a language model. To construct an embedding for a cluster c, we use the following procedure: For each edge on the</context>
<context position="14335" citStr="Pennington et al. (2014)" startWordPosition="2359" endWordPosition="2362">cca skip cbow dep GloVe 5 200 80.41 80.69 80.42 82.35 70.05 100 −1.38 −1.16 −3.31 −0.39 −2.24 10 −18.06 −22.92 −16.18 −8.38 −16.12 11 200 −1.31 −0.04 −0.05 n/a +0.57 100 −3.56 −1.16 −1.17 n/a −1.73 10 −23.51 −22.94 −16.34 n/a −15.64 Table 1: Mean accuracy across tasks for dimension=200 and window=5, and change in mean accuracy when deviating, measured in percentage points. dep has no window parameter. cbow, cca and skip perform nearly identical, while dep performs slightly better. Interestingly, GloVe performs consistently worse than all other embeddings, contrary to the findings published in Pennington et al. (2014), but in line with Stratos et al. (2015). dep performs best on nearly all tasks, which may indicate that dependency-based context is not only beneficial for preserving dependencyrelated information, but also for morphology. This finding is even more pronounced in the evaluation using only ten dimensions (Figure 1 b)): While dep can capture the different aspects tested for nearly as well as with 200 dimensions, the other embeddings suffer larger degradations, especially for PoS and label prediction. cbow seems to be able to cope better with low dimensionality than skip, although they perform ne</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532– 1543, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Djamé Seddah</author>
</authors>
<title>Reut Tsarfaty, Sandra Kübler, Marie Candito, Jinho Choi, Matthieu Constant, Richárd Farkas, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green,</title>
<location>Nizar Habash, Marco</location>
<marker>Seddah, </marker>
<rawString>Djamé Seddah, Reut Tsarfaty, Sandra Kübler, Marie Candito, Jinho Choi, Matthieu Constant, Richárd Farkas, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier Kuhlmann</author>
<author>Joakim Nivre</author>
<author>Adam Przepiorkowski</author>
<author>Ryan Roth</author>
</authors>
<title>Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, Alina Wróblewska, and Eric Villemonte de la Clérgerie.</title>
<date>2014</date>
<journal>Overview of the SPMRL</journal>
<booktitle>In Notes of the SPMRL 2014 Shared Task on Parsing Morphologically-Rich Languages,</booktitle>
<location>Dublin,</location>
<marker>Kuhlmann, Nivre, Przepiorkowski, Roth, 2014</marker>
<rawString>Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, Alina Wróblewska, and Eric Villemonte de la Clérgerie. 2014. Overview of the SPMRL 2014 shared task on parsing morphologically rich languages. In Notes of the SPMRL 2014 Shared Task on Parsing Morphologically-Rich Languages, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Seeker</author>
<author>Jonas Kuhn</author>
</authors>
<title>Making ellipses explicit in dependency conversion for a german treebank.</title>
<date>2012</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the 8th International Conference on Language Resources and Evaluation,</booktitle>
<pages>3132--3139</pages>
<location>Istanbul,</location>
<contexts>
<context position="12952" citStr="Seeker and Kuhn, 2012" startWordPosition="2069" endWordPosition="2072">ompare Table 1). All embeddings capture the PoS well. To a lesser degree, the dependency label and head PoS can also be recovered. The better-performing embeddings achieve results near the approximate upper bound for all tasks. The embeddings also mostly cluster well with respect to tense, number, gender, and case, with tense showing the best correlations. For some of these tasks, the baseline is however fairly high because the number of classes is lower. 2Basque: (Aduriz et al., 2003; Aldezabal et al., 2008), French: (Abeillé et al., 2003; Candito et al., 2010), German: (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian: (Csendes et al., 2005; Vincze et al., 2010), Polish: (Woli´nski et al., 2011; ´Swidzi´nski and Woli´nski, 2010; Wróblewska, 2012), Swedish: (Nivre et al., 2006) 2069 a) dimensionality 200 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number b) dimensionality 10 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number cca skip glove baseline dep cbow brown approx-up Figure 1: Results w</context>
</contexts>
<marker>Seeker, Kuhn, 2012</marker>
<rawString>Wolfgang Seeker and Jonas Kuhn. 2012. Making ellipses explicit in dependency conversion for a german treebank. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 3132–3139, Istanbul, Turkey. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Daniel Hsu</author>
</authors>
<title>Model-based word embeddings from decompositions of count matrices.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1316" citStr="Stratos et al., 2015" startWordPosition="193" endWordPosition="196">all embedding approaches behave similarly in this task, with dependency-based embeddings performing best. This effect is even more pronounced when generating low dimensional embeddings. 1 Introduction Word embeddings map words into a vector space, allowing to reason about words in this space. They have been shown to be beneficial for several tasks such as machine translation (Botha and Blunsom, 2014), parsing (Lei et al., 2014), and named entity recognition (Passos et al., 2014). Recently, word embedding techniques have been studied for their mathematical properties (Levy and Goldberg, 2014b; Stratos et al., 2015), yielding a better understanding of the underlying optimization criteria. However, word embeddings have mostly been studied and evaluated on a single language (English). Therefore, validation on languages other than English is lacking and the question whether word embeddings work the same way across languages has not been empirically evaluated. Evaluations of complex systems – such as parsers – employing word embeddings generally give only little insight into the type of contribution to the result and the structure of word embeddings. We aim to fill these gaps by evaluating several word embed</context>
<context position="6455" citStr="Stratos et al. (2015)" startWordPosition="1001" endWordPosition="1004">ram approach by not using the neighboring words wrt. the sentence’s word sequence but wrt. the dependency tree of the sentence. Therefore, the context of w is defined as all words that are either the head or dependents of w. We will call this approach dep. 1Their approach has not been evaluated in this paper as the corresponding code is not available as of now. GloVe, introduced by Pennington et al. (2014), optimizes the ratio of co-occurrence probabilities instead of the co-occurrence probabilities themselves, getting rid of the negative sampling used for the approaches previously mentioned. Stratos et al. (2015) describe a method to derive word embeddings using canonical correlation analysis. We will call this approach cca. brown clusters (Brown et al., 1992) are constructed by clustering words hierarchically into a binary search tree in a way that maximizes mutual information for a language model. To construct an embedding for a cluster c, we use the following procedure: For each edge on the path from the root to c, add either 1 or −1, depending on the direction of descent. Because not every path has the same depth, we pad missing dimensions with 0. This way, we obtain an embedding interpretation of</context>
<context position="14375" citStr="Stratos et al. (2015)" startWordPosition="2367" endWordPosition="2370">0.42 82.35 70.05 100 −1.38 −1.16 −3.31 −0.39 −2.24 10 −18.06 −22.92 −16.18 −8.38 −16.12 11 200 −1.31 −0.04 −0.05 n/a +0.57 100 −3.56 −1.16 −1.17 n/a −1.73 10 −23.51 −22.94 −16.34 n/a −15.64 Table 1: Mean accuracy across tasks for dimension=200 and window=5, and change in mean accuracy when deviating, measured in percentage points. dep has no window parameter. cbow, cca and skip perform nearly identical, while dep performs slightly better. Interestingly, GloVe performs consistently worse than all other embeddings, contrary to the findings published in Pennington et al. (2014), but in line with Stratos et al. (2015). dep performs best on nearly all tasks, which may indicate that dependency-based context is not only beneficial for preserving dependencyrelated information, but also for morphology. This finding is even more pronounced in the evaluation using only ten dimensions (Figure 1 b)): While dep can capture the different aspects tested for nearly as well as with 200 dimensions, the other embeddings suffer larger degradations, especially for PoS and label prediction. cbow seems to be able to cope better with low dimensionality than skip, although they perform nearly identical on the high dimensionalit</context>
</contexts>
<marker>Stratos, Collins, Hsu, 2015</marker>
<rawString>Karl Stratos, Michael Collins, and Daniel Hsu. 2015. Model-based word embeddings from decompositions of count matrices. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marek ´Swidzi´nski</author>
<author>Marcin Woli´nski</author>
</authors>
<title>Towards a bank of constituent parse trees for Polish.</title>
<date>2010</date>
<booktitle>In Proceedings of Text, Speech and Dialogue,</booktitle>
<pages>197--204</pages>
<location>Brno, Czech Republic.</location>
<marker>´Swidzi´nski, Woli´nski, 2010</marker>
<rawString>Marek ´Swidzi´nski and Marcin Woli´nski. 2010. Towards a bank of constituent parse trees for Polish. In Proceedings of Text, Speech and Dialogue, pages 197–204, Brno, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
</authors>
<title>Dóra Szauter, Attila Almási, György Móra, Zoltán Alexin, and János Csirik.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Valletta,</location>
<marker>Vincze, 2010</marker>
<rawString>Veronika Vincze, Dóra Szauter, Attila Almási, György Móra, Zoltán Alexin, and János Csirik. 2010. Hungarian dependency treebank. In Proceedings of LREC, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcin Woli´nski</author>
<author>Katarzyna Głowi´nska</author>
<author>Marek ´Swidzi´nski</author>
</authors>
<title>A preliminary version of Składnica—a treebank of Polish.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th Language &amp; Technology Conference,</booktitle>
<pages>299--303</pages>
<location>Pozna´n,</location>
<marker>Woli´nski, Głowi´nska, ´Swidzi´nski, 2011</marker>
<rawString>Marcin Woli´nski, Katarzyna Głowi´nska, and Marek ´Swidzi´nski. 2011. A preliminary version of Składnica—a treebank of Polish. In Proceedings of the 5th Language &amp; Technology Conference, pages 299–303, Pozna´n, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alina Wróblewska</author>
</authors>
<title>Polish Dependency Bank.</title>
<date>2012</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>7</volume>
<issue>1</issue>
<pages>15</pages>
<contexts>
<context position="13094" citStr="Wróblewska, 2012" startWordPosition="2091" endWordPosition="2092">rming embeddings achieve results near the approximate upper bound for all tasks. The embeddings also mostly cluster well with respect to tense, number, gender, and case, with tense showing the best correlations. For some of these tasks, the baseline is however fairly high because the number of classes is lower. 2Basque: (Aduriz et al., 2003; Aldezabal et al., 2008), French: (Abeillé et al., 2003; Candito et al., 2010), German: (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian: (Csendes et al., 2005; Vincze et al., 2010), Polish: (Woli´nski et al., 2011; ´Swidzi´nski and Woli´nski, 2010; Wróblewska, 2012), Swedish: (Nivre et al., 2006) 2069 a) dimensionality 200 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number b) dimensionality 10 B E F G H P S B E F G H P S B E F G H P S B G H P S F G P S B F G H P S B F G H P S 100 90 80 70 60 50 40 30 20 pos label headpos case gender tense number cca skip glove baseline dep cbow brown approx-up Figure 1: Results with window = 5 (for cbow, cca &amp; skip) / 11 (for GloVe) for Basque, English, French, German, Hungarian, Polish, Swedish. Note: brown is only pr</context>
</contexts>
<marker>Wróblewska, 2012</marker>
<rawString>Alina Wróblewska. 2012. Polish Dependency Bank. Linguistic Issues in Language Technology, 7(1):1– 15.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
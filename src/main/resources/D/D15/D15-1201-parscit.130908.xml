<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000075">
<title confidence="0.999053">
Learning Semantic Composition to Detect Non-compositionality
of Multiword Expressions
</title>
<author confidence="0.925288">
Majid Yazdani Meghdad Farahmand James Henderson
</author>
<affiliation confidence="0.98128">
Computer Science Computer Science Xerox Research Center Europe
Department Department james.henderson@
University of Geneva University of Geneva xrce.xerox.com
</affiliation>
<email confidence="0.799212">
majid.yazdani@unige.ch meghdad.farahmand@
</email>
<note confidence="0.543561">
unige.ch
</note>
<sectionHeader confidence="0.974335" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999698">
Non-compositionality of multiword ex-
pressions is an intriguing problem that
can be the source of error in a variety
of NLP tasks such as language genera-
tion, machine translation and word sense
disambiguation. We present methods
of non-compositionality detection for En-
glish noun compounds using the unsu-
pervised learning of a semantic compo-
sition function. Compounds which are
not well modeled by the learned semantic
composition function are considered non-
compositional. We explore a range of dis-
tributional vector-space models for seman-
tic composition, empirically evaluate these
models, and propose additional methods
which improve results further. We show
that a complex function such as polyno-
mial projection can learn semantic compo-
sition and identify non-compositionality
in an unsupervised way, beating all other
baselines ranging from simple to complex.
We show that enforcing sparsity is a useful
regularizer in learning complex composi-
tion functions. We show further improve-
ments by training a decomposition func-
tion in addition to the composition func-
tion. Finally, we propose an EM algo-
rithm over latent compositionality annota-
tions that also improves the performance.
</bodyText>
<sectionHeader confidence="0.99831" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.8762955">
Multiword Expressions (MWEs) are sequences of
words that exhibit some kind of idiosyncrasy. This
idiosyncrasy can be semantic, statistical, or syn-
tactic1. Ivory tower, speed limit, and at large
</bodyText>
<footnote confidence="0.986245">
1MWEs can have other less significant kinds of idiosyn-
crasies. For instance lexical idiosyncrasy as in ad hoc, and
pragmatic idiosyncrasy as in good morning (Baldwin and
Kim, 2010)
</footnote>
<bodyText confidence="0.99972927027027">
are examples of semantically, statistically and syn-
tactically idiosyncratic MWEs respectively. Note
that an MWE can be idiosyncratic at several lev-
els. In general, semantically idiosyncratic MWEs
are commonly referred to as non-compositional
(Baldwin and Kim, 2010) and statistically idiosyn-
cratic MWEs are commonly referred to as col-
locations (Sag et al., 2002). Non-compositional
MWEs are those whose meaning can not be read-
ily inferred from the meaning of their constituents
and collocations are those MWEs whose con-
stituents co-occur more than expected by chance.
Collocations constitute the largest subset of all
kinds of MWEs, however, non-compositional ones
cause more problems in various NLP tasks, for ex-
ample word sense disambiguation (McCarthy et
al., 2003) and machine translation (Lin, 1999).
It may also be more challenging to model non-
compositionality than collocational weight as the
former has to do with modelling the semantics and
the latter can to some extent be modeled by con-
ventional statistical measures such as mutual in-
formation. Detecting non-compositionality in an
automatic fashion has been the aim of much pre-
vious research.
In this paper, we capture non-compositionality
of English Noun Compounds (NCs)2 based on the
assumption that the majority of the compounds
are compositional, for which a composition func-
tion can be learned. This implies that the com-
pounds for which a composition function cannot
be learned with a relatively low error are non-
compositional.
In previous work on vector-space models of dis-
tributional semantics, semantic composition has
been commonly assumed to be a trivial predeter-
mined function such as addition, multiplication,
</bodyText>
<footnote confidence="0.94753225">
2MWEs have various syntactic categories such as noun
compounds, verb particle constructions, light verb construc-
tions, etc., with noun compounds and verb particle construc-
tions constituting the most prominent categories of MWEs.
</footnote>
<page confidence="0.648666">
1733
</page>
<note confidence="0.9879355">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9966149">
and their weighted variations (Mitchell and Lap-
ata, 2008; Reddy et al., 2011; Salehi et al., 2015).
Nevertheless there is some work that regards com-
position as a more complex function. For in-
stance Widdows (2008) who propose (but doesn’t
empirically test) the use of Tensor and Convolu-
tion products for modelling non-compositionality,
Baroni and Zamparelli (2010) who regard adjec-
tives in adjectival-noun compositions as matri-
ces that can be learned by linear regression, and
Socher et al. (2012) who present a model that
learns phrase composition by means of a recursive
neural network. The two latter works show that
complex composition models significantly outper-
form additive and multiplicative functions. In
this work, we too assume that composition is ar-
guably a complex function. We believe simpli-
fied composition functions, such as additive and
multiplicative functions and their weighted varia-
tions, while having advantages such as being im-
pervious to overfitting, can not completely cap-
ture semantic composition. Nevertheless mod-
elling composition by means of a powerful func-
tion can be equally inadequate for our purposes.
An overly powerful composition function mem-
orizes all compositional and non-compositional
compounds, resulting in overfitting and low learn-
ing error that hinders discrimination between com-
positional and non-compositional compounds. We
examine various classes of composition functions,
ranging from the least to the most powerful (in
terms of learning capacity). We show that com-
plex functions clearly do a better job in mod-
elling semantic composition and in detecting non-
compositionality compared to commonly used ad-
ditive and multiplicative functions.
Compositional compounds are also decompos-
able; intuitively, their semantics is the union of
the semantics of their components. More for-
mally, conditioned on the vector of the com-
pound, vectors of the component words should
be independently predictable. This principle, to-
gether with the assumption that most of the com-
pounds are compositional, leads to the conclusion
that a model of composition should be able to
be auto-reconstructive: the composition function
that maps component-words’ vectors to their com-
pound vector should have an associated decom-
position function that independently predicts each
of the component-words’ vectors from this com-
pound vector. An auto-reconstructive model en-
ables us to exploit more data in order to learn se-
mantic composition and predict compositionality.
We show that auto-reconstruction can improve the
accuracy of composition functions and improve
detecting non-compositionality.
To further improve non-compositionality de-
tection, we propose an EM-like detection algo-
rithm based on hidden compositionality annota-
tions. The best composition is the one that is
the best fit on all the data points except the non-
compositional ones. Since we don’t use annotated
data at training time, we assume annotations to be
hidden variables and iteratively alternate between
optimizing the composition function and optimiz-
ing the hidden compositionality annotations. We
show that this iterative algorithm increases the
accuracy of non-compositionality detection com-
pared to the case when training is done on all ex-
amples.
We run our experiments on the data set of
Farahmand et al. (2015) who provide a set of
English NCs which are annotated with non-
compositionality judgments. We show that
quadratic regression significantly outperforms ad-
ditive and multiplicative baselines and all other
models in modelling semantic composition and
identifying the non-compositional NCs. In short,
the contributions of our work are: to empirically
evaluate various composition functions ranging
from simple to overly complex in order to find the
most accurate function; to propose, to the best of
our knowledge for the first time, a method of iden-
tifying non-compositional phrases as phrases for
which a composition function cannot be readily
learned; to propose learning decomposability as
another criterion to detect non-compositionality;
and to examine possible ways of improving the ac-
curacy of the models by means of EM on hidden
compositionality annotations.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999465">
To the best of our knowledge, attempts to extract
non-compositionality in computational linguistics
go back to 1998. Tapanainen et al. (1998) pro-
pose a method to identify non-compositional verb-
object collocations based on the semantic asym-
metry of verb-object relation. They assume that
in a verb-object idiomatic expression, the object
is a more interesting element in the sense that
if the object appears with one (or only a few)
verbs in a large corpus, it presumably has an id-
</bodyText>
<page confidence="0.989959">
1734
</page>
<bodyText confidence="0.999813257425743">
iomatic nature. Lin (1999) argues that the mu-
tual information between the constituents of a
non-compositional phrase is significantly differ-
ent from that of a phrase created by substitut-
ing the constituents of that phrase with their sim-
ilar words. Their evaluation reveals a low pre-
cision (16 − 39%) and recall (14 − 21%). In
any case this method is not able to discrimi-
nate non-compositional MWEs from collocational
MWEs as they share the same property of non-
substitutability (their constituents cannot be re-
placed with their synonyms). Baldwin et al.
(2003) present a method that decides about the
non-compositionality of English NCs and verb
particle constructions by using latent semantic
analysis to calculate the similarity between a
MWE and its components. They argue that a
higher similarity indicates a higher degree of com-
positionality. McCarthy et al. (2003) devise a
number of measures based on comparison of the
neighbors of phrasal verbs and their correspond-
ing simplex verbs. They evaluate these measures
by calculating their correlation with human com-
positionality judgments on a set of phrasal verbs.
They show that some of the measures have signifi-
cant correlations with human judgments. Venkata-
pathy and Joshi (2005) present a supervised model
that benefits from both collocational and con-
textual information and ranks the MWE candi-
dates based on their non-compositionality. Katz
and Giesbrecht (2006) use distributional semantics
and LSA as a model of context similarity to test
whether the local context of a MWE can distin-
guish its idiomatic use from literal use. They fur-
ther compare the context of a MWE with the con-
text of its components and show that this can be
used to decide whether the expression is idiomatic
or not. Cook et al. (2007) is a relatively differ-
ent work where the authors propose a syntactic ap-
proach to identify semantic non-compositionality
of verb-noun MWEs. McCarthy et al. (2007) use
various models of selectional preferences for de-
tecting non-compositional verb-object pairs.
Reddy et al. (2011) employ the additive and
multiplicative composition functions presented by
Mitchell and Lapata (2008)3 and several similar-
3Mitchell and Lapata (2008) present an analysis of vector-
based additive and multiplicative semantic composition mod-
els where each words is represented by its distributional vec-
tor. They conclude that multiplicative and combined models
do a better job in modelling vector-based semantic composi-
tion than other models.
ity based models to measure the compositionality
of MWEs. Similarity based models measure the
similarity of a MWE vector and sum/product of
its constituents’ vectors. Their evaluation (which
is carried out on a set of 90 annotated NCs) shows
that there is a relatively high correlation (Spear-
man p of between 0.51 and 0.71) between their
models’ predictions and human judgments on non-
compositionality of English NCs, with weighted
additive function outperforming all the other mod-
els. Kiela and Clark (2013) present a model of de-
tecting non-compositionality based on the hypoth-
esis that the average distance between a phrase
vector and its substituted phrase vectors is related
to its compositionality. In particular composi-
tional phrases are less similar to their neighbors in
semantic space. The distributional vectors repre-
senting the semantics of words were created using
the standard window method and 50,000 most fre-
quent context words. They show that their model
slightly (+0.014 and +0.007) outperforms their
baselines (Venkatapathy and Joshi, 2005; Mc-
Carthy et al., 2007).
All of the models mentioned so far are based on
conventional4 or count based vector space repre-
sentation of the words. More recent works how-
ever are based on representation learning of word
embeddings. Baroni and Zamparelli (2010) regard
adjective as matrices and nouns as real-valued vec-
tors for Italian adjective noun composition. They
learn the adjective matrices by linear regression.
In this work, however, every adjective is presented
by a new matrix which leads to a large number
of parameters. Socher et al. (2012) suggest that
composition function is a matrix that multiplies
on the word vectors, and Mikolov et al. (2013b)
present a model of learning non-compositional
phrases by calculating a data-driven score for cer-
tain frequent phrases (up to size two) and learn
them as a whole. Salehi et al. (2015) borrow the
word embeddings from (Mikolov et al., 2013a)
to model the semantics of words and use sev-
eral composition functions from (Mitchell and La-
pata, 2008; Reddy et al., 2011) to predict the
non-compositionality of MWEs. They compare
the performance of word embeddings with con-
ventional distributional vector representations and
discover the superiority of word embeddings in
predicting non-compositionality of MWEs.
</bodyText>
<footnote confidence="0.975977333333333">
4Conventional or count based models of distributional
similarity as oppose to word embeddings (Salehi et al., 2015;
Baroni et al., 2014).
</footnote>
<page confidence="0.993231">
1735
</page>
<sectionHeader confidence="0.957093" genericHeader="method">
3 Representation of Words and
Compounds
</sectionHeader>
<bodyText confidence="0.99998875">
In order to represent words and compounds we
use word embeddings, which are a form of vec-
tor space models. Vector space models represent
the semantics of words and phrases with real val-
ued vectors. Word embeddings have proven to
be effective models of semantic representation of
words and outperform the count-based models in
various NLP tasks (Baroni et al., 2014; Collobert
et al., 2011; Collobert and Weston, 2008; Yaz-
dani and Popescu-Belis, 2013; Huang et al., 2012;
Mikolov et al., 2013c). They have been success-
fully applied to semantic composition (Mikolov
et al., 2013b) and outperformed the conventional
count based contextual models in predicting non-
compositionality of MWEs (Salehi et al., 2015).
In this work we use word embeddings of
Mikolov et al. (2013a) to represent the seman-
tics of words and compounds. We chose an En-
glish Wikipedia dump as our corpus. After fil-
tering HTML tags and noise we POS-tagged the
corpus and extracted ≈ 70k compounds whose
frequency of occurrence was above 50. We learn
the embeddings of these compounds as single to-
kens using the word2vec5 bag-of-word model. We
also learn the embeddings of the compounds of the
evaluation set, plus the embeddings of all the com-
pounds’ component words. Compounds’ sizes are
restricted to two (i.e. bigrams) for the sake of sim-
plicity and to respect the evaluation set standards.
The compounds and word embeddings are then
used as supervised signals to learn a composition
function.
</bodyText>
<sectionHeader confidence="0.746361" genericHeader="method">
4 Supervised Models of Composition on
Word Embeddings
</sectionHeader>
<bodyText confidence="0.999964833333333">
After the unsupervised learning of word embed-
dings and candidate compound embeddings (see
section 3), we use these embeddings as supervised
signals in order to train our composition func-
tions. The term supervised might be misleading
as the models do not have any information about
the compositionality of the compounds during the
training phase, and in that respect it is unsuper-
vised. To describe the models in a formal way,
throughout the paper we use the following nota-
tions: d represents the size of embeddings, φ(wi)
represents embedding of wi, and ˜φ(wi−wj) _
</bodyText>
<footnote confidence="0.781608">
5https://code.google.com/p/word2vec/
</footnote>
<bodyText confidence="0.999556">
f(φ(wi), φ(wj)) represents the learned embed-
ding of bigram wi−wj by the composition func-
tion f. The training error of bigram wi−wj by f
is eij _ k˜φ(wi−wj)−φ(wi, wj)k, and kk is norm
2. The composition functions are described in the
following sections.
Given unsupervised embeddings for both words
and compounds, a composition model is trained
to map the word embeddings to the compound
embeddings, with norm 2 error eij defined
above. Then this same error for this same
task (norm 2 between predicted and unsupervised
compound embeddings) is used to measure non-
compositionality. In other words, we learn a com-
position function (with several models) and iden-
tify non-compositional expressions as those for
which the error of this composition function is
high.
We explore various classes of composition
functions of word embeddings, ranging from sim-
ple to complex, to find the most effective one.
We want a composition function that is pow-
erful enough to learn composition for composi-
tional compounds, but simple enough that it fails
to learn composition for non-compositional com-
pounds. To this end, we investigate linear pro-
jections, polynomial projections, and neural net-
works. We try these models with and without spar-
sity regularisation, which reduces the number of
non-zero parameters while otherwise keeping the
complexity of the function that can be learned.
</bodyText>
<subsectionHeader confidence="0.983023">
4.1 Linear Projection
</subsectionHeader>
<bodyText confidence="0.9320893125">
In this model we assume that the embedding of
a bigram is a linear projection of its component
words’ embeddings.
f(φ(wi), φ(wj)) _ [φ(wi), φ(wj)1θ2d×d
To train this function we optimize the least square
error, which gives us a multi-variant linear regres-
sion.
min k[φ(wi), φ(wj)1θ2d×d − φ(wi, wj)k
θ
As mentioned earlier, a composition function
that doesn’t overfit the training data and induces
a more meaningful error is more suitable for our
purpose. One effective way of reducing overfitting
and increasing generalization is by keeping only
the important parameters of the model, which is
done by enforcing sparsity on model parameters.
</bodyText>
<page confidence="0.963419">
1736
</page>
<figure confidence="0.987188">
(a) Linear Projection (b) Sparse Linear Projection
</figure>
<figureCaption confidence="0.997527">
Figure 1: Linear transformation matrix of compositionality for embeddings of size 50
</figureCaption>
<figure confidence="0.979512">
(a) Pure Quadratic (b) Sparse Pure Quadratic
</figure>
<figureCaption confidence="0.999887">
Figure 2: Pure quadratic transformation matrix of compositionality for embeddings of size 50
</figureCaption>
<bodyText confidence="0.9760715">
In case of sparse linear projections, only a few el-
ements of the projection matrix θ are non-zero.
This means that not all dimensions of the latent
space has a role in all dimensions of the compound
embedding.
To apply sparsity on θ, we add a norm 1 penalty
on it and add that to the least square optimization.
This forms a multi-variant lasso regression (Tib-
shirani, 2011).
min II[φ(wi), φ(wj)]θ − φ(wi, wj)II + λ|θ|
θ
Figure 1 shows the transformation matrices of
linear projection and sparse linear projection. The
two diagonals of the matrices correspond to the
sum of the two embeddings, which we can see are
the main component of the sparse function, and
play an important role in the non-sparse one. We
will see that despite being an important compo-
nent of these functions, sum alone is not capable
of accurately modelling semantic composition.
</bodyText>
<subsectionHeader confidence="0.926354">
4.2 Polynomial Projection
</subsectionHeader>
<bodyText confidence="0.999671444444444">
Polynomial projection is a non-linear projection
that assumes the relation between compound em-
bedding and the component words’ embeddings
should be a polynomial of degree n. This can be
viewed as a form of linear regression where first
a polynomial transformation is applied to the in-
put vector and then a linear projection is fitted. If
ψ shows the polynomial transformation then we
have:
</bodyText>
<equation confidence="0.811893">
f(φ(wi),φ(wj)) = ψ([φ(wi),φ(wj)])θ
</equation>
<bodyText confidence="0.999825">
We couldn’t successfully apply any polynomial
beyond quadratic transformation without overfit-
ting. The case of a quadratic ψ transformation is:
</bodyText>
<equation confidence="0.885065333333333">
ψ(x) = x2 1, · · · x2
� �� n �
Pure quadratic
</equation>
<bodyText confidence="0.998165181818182">
Similar to the linear case we can have sparse
version of the polynomial regression in which we
allow the presence of only a few non-zero ele-
ments in the θ matrix. The sparsity regularizer
is more important in the case of polynomial re-
gression as we have many more parameters. The
quadratic model is similar to Recursive Neural
Tensor compositionality model of Socher et al.
(2013). But in our model the tensor is symmet-
ric around the diagonal. Figure 2 shows the pure
quadratic transformation matrices.
</bodyText>
<table confidence="0.96825">
, x1x2, ··· xn−1xn ,`1,···x:
� �� � Y
interaction terms linear terms
</table>
<page confidence="0.875326">
1737
</page>
<subsectionHeader confidence="0.97615">
4.3 Neural Networks
</subsectionHeader>
<bodyText confidence="0.982406642857143">
A feed forward neural network is a universal ap-
proximator (Cybenko, 1989): feed-forward net-
work with a single hidden layer can approximate
any continuous function, provided it has enough
hidden units. Therefore we use neural networks
as a powerful class of learning models to learn se-
mantic composition. The number of hidden units
gives us a measure to control expressiveness of our
model.
f(φ(wi), φ(wj)) = σ([φ(wi), φ(wj)]Wih)Who
Similar to the previous models, we optionally
impose sparsity over weight matrices of the neu-
ral network to be able to induce more meaningful
learning errors.
</bodyText>
<subsectionHeader confidence="0.986931">
4.4 Experimental Results
</subsectionHeader>
<bodyText confidence="0.99990703030303">
We evaluated the above models on the data set of
Farahmand et al. (2015). They provide a set of
1042 English NCs with four non-compositionality
judgments. The judgments are binary decisions
taken by four experts about whether or not a com-
pound is non-compositional. We calculate a vote-
based non-compositionality score for each of the
data set compounds by summing over its non-
compositionality judgments. The neural network
models are trained using stochastic gradient de-
scent. We use the additive and multiplicative mod-
els of modelling composition and detecting non-
compositionality presented by Salehi et al. (2015)
and (Reddy et al., 2011) as state of the art base-
lines.
The results are shown in Table 1. The sec-
ond column shows the correlation between dif-
ferent models’ predictions and the annotated
data in terms of Spearman p. The last three
columns show the performance of different mod-
els in terms of Normalized Discounted Cumu-
lative Gain (NDCG), F1 score and and Preci-
sion at 100 (P@100). For these three scores
we consider the problem of predicting non-
compositional NCs a problem with a binary so-
lution where we assume compounds (of the evalu-
ation set) with at least two non-compositionality
votes are non-compositional. NDCG assigns a
higher score to a ranked list of compounds if
the non-compositional ones are ranked higher in
the list. F1 column represents the maximum F1
score on the top-n elements of the ranked list re-
turned by the corresponding model for all n in
</bodyText>
<table confidence="0.999675733333333">
Model Spearman ρ NDCG F1 P@100
Additive model 20.83 81.39 36.95 43
(Salehi et al., 2015);
(Reddy et al., 2011)
Multiplicative model 9.18 76 35.61 22
(Reddy et al., 2011)
Sparse Linear 37.58 84.25 46.40 48
Linear 38.09 84.25 46.41 49
Sparse Pure Quad. 37.85 84.11 47.05 48
Pure Quad. 38.57 84.68 47.01 47
Sparse Interaction 41.03 85.82 48.71 54
Interaction 40.25 85.69 48.64 50
Quadratic 40.25 85.59 48.34 49
Sparse NN (14=1000) 37.08 85.04 46.35 52
NN (14=1000) 37.51 84.97 45.47 51
</table>
<tableCaption confidence="0.8447065">
Table 1: Results for each model’s ability to predict
non-compositionality.
</tableCaption>
<bodyText confidence="0.9994505">
[1 − size-of-ranked-list]. P@100 shows the pre-
cision at the first 100 compounds ranked as non-
compositional. The models are listed in the or-
der of complexity of the composition function.
The addition-based baseline which was explored
in a variety of previous work does not seem to
be as powerful as the other models. It is outper-
formed by almost all learned models. In general,
we can see that more complex functions tend to
learn compositionality in a more effective way.
As mentioned earlier, overly powerful learners
overfit and do not produce meaningful errors for
the detection task. Sparsity seems to address this
issue by reducing the number of non-zero param-
eters while the function can still keep the complex
terms if needed. In general sparse models show
improvement over their non-sparse counterparts,
specifically for more powerful models.
</bodyText>
<sectionHeader confidence="0.999424" genericHeader="method">
5 Auto-reconstructive Models
</sectionHeader>
<bodyText confidence="0.999964461538462">
In this section, we investigate the hypothesis that
we can detect non-compositionality better by not
only modelling a composition function, but also
modelling a decomposition function. For com-
positional compounds, given the meaning of the
compound, the meaning of the two component
words should be conditionally independent. We
therefore assume that the decomposition function
predicts the component words’ vectors indepen-
dently. Let us illustrate this assumption by exam-
ining the non-compositional compound flag stop.
Given the semantics of this compound (a point at
which a vehicle in public transportation stops only
</bodyText>
<page confidence="0.98279">
1738
</page>
<bodyText confidence="0.9999625">
on prearrangement or signal6), we can not readily
predict one of its component words without know-
ing the other. Now consider the compositional
compound hip injury. Given the semantics of this
compound it is much easier to predict each of its
component words independently.
In the previous section, the training signals
came from the embeddings of the candidate com-
pounds and their component words. In this section
we extend our model such that it can benefit from
more training signals. To this end, we formal-
ize the assumption that a compositional compound
is also decomposable as an auto-reconsructive
model. We thus add this hypothesis to the learn-
ing process: a good composition function not only
builds the semantics of the compound from the
semantics of its component words, but it also al-
lows the independent prediction of the semantics
of its component words from the compound se-
mantics. In the following sections we add this as-
sumption to both linear projection (which encom-
passes polynomial) and Neural Network models.
</bodyText>
<subsectionHeader confidence="0.995498">
5.1 Auto-reconstructive Linear Models
</subsectionHeader>
<bodyText confidence="0.937673913043478">
Let YM×d be a matrix whose rows are the pre-
computed compound embeddings, and XM×2d be
a matrix whose rows are the concatenation of the
embeddings for the words of these compounds.
Let AN×2d be another matrix where every row
contains the concatenation of the embeddings for
the words of a compound, but this matrix in-
cludes many compounds for which we did not pre-
compute compound embeddings. We assume that
the rows of matrix A include the rows of matrix X.
In linear models the auto-reconstructive objective
function is as follows:
minIIXθ − Y II + λIIAθθ0 − AII
θ,θ�
where λ is a meta-parameter for the importance
of the auto-reconstruction in the objective. A
schematic of this model is shown in Figure 3a.
We can look at this problem as the following
weighted least square problem:
(1) min II CA/ θ − (Aθ0T (θ0θ0T)−1) II
λ
In the above matrix formula we transformed the
auto-reconstruction part of the objective to a
</bodyText>
<footnote confidence="0.565629">
6Definition taken from Merriam-Webster Dictionary
</footnote>
<figure confidence="0.967367">
(a) Linear auto-reconstructive (b) NN Auto-reconstructive
</figure>
<figureCaption confidence="0.9322025">
Figure 3: Auto-reconstructive linear and neural
network models
</figureCaption>
<bodyText confidence="0.990096294117647">
pseudo regressand of the least square. To solve
this optimization we design an efficient alternat-
ing least squares algorithm.
First we initialize θ0 to be the answer of the
original multi-variant linear regression, θ0=X \ Y
where X \ =(XTX)−1XT is the pseudoinverse
of X. Let us assume W is the diagonal matrix
with first M elements of the diagonal being 1 and
the remaining N being λ. We alternate between
the following formulas until the algorithm con-
verges. First we approximate the next θ0 based on
the current approximation of θ, then we use this
value of θ0 to calculate the pseudo regressand part
of the least square. In the final step we solve the
weighted least square for this new regressand ma-
trix and continue iterating these stages until the al-
gorithm converges.
</bodyText>
<equation confidence="0.999336285714286">
θ0t = (Aθt) \ A (1)
~X �
X2 = A
�
Y (2)
Aθ0T t−1(θ0 t−1θ0T t−1)−1
θt = (XT2 WX2)−1(X2)TWY2 (3)
</equation>
<bodyText confidence="0.999855">
The above algorithm can also be used in the
case of polynomial regression. The only thing that
needs to be done is to replace X and A by their
polynomial transformations.
</bodyText>
<subsectionHeader confidence="0.999708">
5.2 Auto-reconstructive Neural Networks
</subsectionHeader>
<bodyText confidence="0.9999355">
The auto-reconstructive neural network follows
the same idea. The objective function changes to:
</bodyText>
<equation confidence="0.95466">
min IIσ(XWih)Who − Y II+
Wih,Whi,Woh
λIIσ(AWih)Whi − AII (4)
�Y2 =
</equation>
<page confidence="0.976781">
1739
</page>
<table confidence="0.999874285714286">
Composition Spearman p NDCG F1 P@100
Linear 38.09 84.25 46.41 49
Linear+ auto 37.52 84.55 46.55 49
Interaction 40.25 85.69 48.64 50
Interaction+ auto 39.29 85.71 48.95 56
NN (H=1000) 37.40 84.50 46.34 49
NN (H=1000) + auto 39.98 85.17 49.12 55
</table>
<tableCaption confidence="0.996119">
Table 2: Results comparing the auto-
</tableCaption>
<bodyText confidence="0.967311761904762">
reconstructive models’ ability to predict non-
compositionality.
Figure 3b shows the schematic of this model.
We optimize this objective using stochastic gradi-
ent descent with early stopping. The results are
shown in Table 2. We choose the first 300K fre-
quent noun-noun compounds from the corpus in
order to build matrix A. Each row of A cre-
ated by concatenating the component words vec-
tors. The results show that the auto-reconstructive
models generally improve over their counterparts.
As mentioned earlier, the improvement comes
from two facts. On the one hand we increase
the training signals by implementing the decom-
posability hypothesis. On the other hand, the
auto-reconstructive model enables us to exploit
more data in addition to the candidate compounds.
There is almost no improvement in the case of
linear model because this model does not have
enough learning capacity to benefit from a higher
number of training signals.
</bodyText>
<sectionHeader confidence="0.911919" genericHeader="method">
6 Non-compositionality Detection Using
</sectionHeader>
<subsectionHeader confidence="0.932856">
Latent Annotations
</subsectionHeader>
<bodyText confidence="0.999945222222222">
All the models discussed in this paper are unsu-
pervised since they don’t have any access to la-
bels specifying compositionality of compounds.
The above models simply assume that most com-
pounds are compositional, and therefore train their
composition and decomposition functions on all
compounds. In this section we incorporate in the
models an intrinsic uncertainty about the compo-
sitionality annotation of the training set.
The best (optimum) composition function is
the one that fits well all the compositional com-
pounds and does not fit the non-compositional
ones. But we assume that we do not have train-
ing labels indicating compositionality. To over-
come this uncertainty and improve the learning
process, we introduce latent compositionality la-
bels to the model. We assume each candidate com-
pound has a latent annotation, 1 or 0, showing
</bodyText>
<table confidence="0.999765428571428">
Composition Spearman p NDCG F1 P@100
Linear 38.09 84.25 46.41 49
Linear+ LA 37.80 84.60 46.29 48
Interaction 40.25 85.69 48.64 50
Interaction+ LA 40.56 86.30 48.34 51
NN (H=1000) 37.40 84.50 46.34 49
NN (H=1000) + LA 39.23 85.36 48.15 55
</table>
<tableCaption confidence="0.958374">
Table 3: Results comparing the latent annotation
models’ ability to predict non-compositionality.
</tableCaption>
<bodyText confidence="0.996452333333333">
whether or not it is compositional. Let us assume
a non-compositionality detection system that re-
turns B non-compositional candidates that should
have their own lexical unit and parameters. The
objective of this composition function training is
to minimize the error of compositional compounds
and not the error of non-compositional ones. In
order to implement this objective we use the fol-
lowing loss function:
</bodyText>
<equation confidence="0.9616052">
min λije2ij
λij,θ ij
s.t λij E {0, 1},
λij = N − B
ij
</equation>
<bodyText confidence="0.9997344">
where λij represents the hidden compositionality
annotation and eij is again the learning error for
the pair wi−wj. We want to find the B points such
that annotating them as non-compositional results
in the minimum error of this objective. The algo-
rithm that alternates between optimizing the com-
position learning and the hidden annotations even-
tually converges to this solution.
If the errors are fixed, the B compounds with
the biggest errors are the answers to the non-
compositional annotation optimization of that it-
eration. Therefore to solve this optimization we
follow an EM-like algorithm: First we set all λij
to 1 and perform the optimization on the compo-
sition function. Then we sort the compounds by
their error and set the λij of the biggest B el-
ements to 0, and the rest to 1. In other words
we assume the compounds with big error are pre-
sumably non-compositional according to what we
know until that iteration. We continue alternat-
ing between training the composition function and
annotating high error points until the algorithm
reaches convergence. The results are shown in Ta-
ble 3. Models that use latent annotations clearly
outperform their counterparts, especially in terms
</bodyText>
<page confidence="0.955954">
1740
</page>
<bodyText confidence="0.999970333333333">
of precision at 100. This is expected since at train-
ing time we consider a model that returns B non-
compositional compounds and therefore precision
at 100 is optimized. The latent annotations do not
improve the linear model since the model is simple
and there is not much room to improve its learning.
</bodyText>
<sectionHeader confidence="0.99876" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99999375">
We proposed a framework to detect non-
compositional compounds as the compounds that
stand out as outliers in the process of learning
compositionality of English noun compounds. We
proposed and evaluated a range of functions with a
variety of complexities that model semantic com-
position. We showed that learners such as poly-
nomial projection and neural networks which are
distinctly more complex than commonly used ad-
ditive and multiplicative functions can model se-
mantic composition more effectively. We showed
that a function as complex as quadratic projection
is a better learner of compositionality than simpler
models. We further showed that enforcing sparsity
is an effective way of learning a complex compo-
sition function while avoiding overfitting and pro-
ducing meaningful learning errors. Furthermore,
we improved our models by incorporating an auto-
reconstructive loss function that enables us to ben-
efit from more training signals and cover more
data. Finally, we addressed the intrinsic label un-
certainty in training data by considering latent an-
notations, and showed that it can further improve
the results.
</bodyText>
<sectionHeader confidence="0.99756" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9905765">
This research was partially funded by Hasler foun-
dation project no. 15019, “Deep Neural Network
Dependency Parser for Context-aware Represen-
tation Learning”.
</bodyText>
<sectionHeader confidence="0.997652" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9986865">
Timothy Baldwin and Su Nam Kim. 2010. Multiword
expressions. Handbook of Natural Language Pro-
cessing, second edition. Morgan and Claypool.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 workshop on Multiword
expressions: analysis, acquisition and treatment-
Volume 18, pages 89–96. Association for Computa-
tional Linguistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183–1193. Association for Computational Linguis-
tics.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! a
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
of the 52nd Annual Meeting of the Association
for Computational Linguistics, volume 1, pages
238–247.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning, ICML ’08, pages 160–167, New
York, NY, USA. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.
2007. Pulling their weight: Exploiting syntactic
forms for the automatic identification of idiomatic
expressions in context. In Proceedings of the work-
shop on a broader perspective on multiword expres-
sions, pages 41–48. Association for Computational
Linguistics.
G. Cybenko. 1989. Approximation by superpositions
of a sigmoidal function. Mathematics of Control,
Signals and Systems, 2(4):303–314.
Meghdad Farahmand, Aaron Smith, and Joakim Nivre.
2015. A multiword expression data set: Anno-
tating non-compositionality and conventionalization
for english noun compounds. In Proceedings of the
11th Workshop on Multiword Expressions (MWE-
NAACL 2015). Association for Computational Lin-
guistics.
Eric H. Huang, Richard Socher, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Graham Katz and Eugenie Giesbrecht. 2006. Au-
tomatic identification of non-compositional multi-
word expressions using latent semantic analysis. In
Proceedings of the Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties, MWE ’06, pages 12–19, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Douwe Kiela and Stephen Clark. 2013. Detect-
ing compositionality of multi-word expressions us-
ing nearest neighbours in vector space models. In
EMNLP, pages 1427–1432.
</reference>
<page confidence="0.801424">
1741
</page>
<reference confidence="0.99948032967033">
Dekang Lin. 1999. Automatic identification of
non-compositional phrases. In Proceedings of the
37th Annual Meeting of the Association for Compu-
tational Linguistics on Computational Linguistics,
ACL ’99, pages 317–324, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL-SIGLEX
Workshop on Multiword Expressions: Analysis, Ac-
qusation and Treatment, pages 73–80.
Diana McCarthy, Sriram Venkatapathy, and Aravind K
Joshi. 2007. Detecting compositionality of verb-
object combinations using selectional preferences.
In EMNLP-CoNLL, pages 369–379.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.
2013c. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies (NAACL-HLT-2013). Asso-
ciation for Computational Linguistics, May.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236–244.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in
compound nouns. In IJCNLP, pages 210–218.
Ivan A Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for nlp. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 1–15. Springer.
Bahar Salehi, Paul Cook, and Timothy Baldwin. 2015.
A word embedding approach to predicting the com-
positionality of multiword expressions. In Proceed-
ings of NAACL HLT. Association for Computational
Linguistics.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642, Stroudsburg, PA, October.
Association for Computational Linguistics.
Pasi Tapanainen, Jussi Piitulainen, and Timo J¨arvinen.
1998. Idiomatic object usage and support verbs.
In Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics and 17th
International Conference on Computational Lin-
guistics - Volume 2, ACL ’98, pages 1289–1293,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Robert Tibshirani. 2011. Regression shrinkage and
selection via the lasso: a retrospective. Journal of
the Royal Statistical Society: Series B (Statistical
Methodology), 73(3):273–282.
Sriram Venkatapathy and Aravind K. Joshi. 2005.
Measuring the relative compositionality of verb-
noun (v-n) collocations by integrating features. In
Proceedings of the Conference on Human Lan-
guage Technology and Empirical Methods in Natu-
ral Language Processing, HLT ’05, pages 899–906,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second AAAI Sym-
posium on Quantum Interaction, volume 26, page
28th.
Majid Yazdani and Andrei Popescu-Belis. 2013. Com-
puting text semantic relatedness using the contents
and links of a hypertext encyclopedia. Artif. Intell.,
194:176–202.
</reference>
<page confidence="0.99369">
1742
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.407587">
<title confidence="0.999489">Learning Semantic Composition to Detect of Multiword Expressions</title>
<author confidence="0.989764">Majid Meghdad James</author>
<affiliation confidence="0.996652">Computer Computer Xerox Research Center University of University of xrce.xerox.com</affiliation>
<abstract confidence="0.98144509375">majid.yazdani@unige.ch unige.ch Abstract Non-compositionality of multiword expressions is an intriguing problem that can be the source of error in a variety of NLP tasks such as language generation, machine translation and word sense disambiguation. We present methods of non-compositionality detection for English noun compounds using the unsupervised learning of a semantic composition function. Compounds which are not well modeled by the learned semantic composition function are considered noncompositional. We explore a range of distributional vector-space models for semantic composition, empirically evaluate these models, and propose additional methods which improve results further. We show that a complex function such as polynomial projection can learn semantic composition and identify non-compositionality in an unsupervised way, beating all other baselines ranging from simple to complex. We show that enforcing sparsity is a useful regularizer in learning complex composition functions. We show further improvements by training a decomposition function in addition to the composition function. Finally, we propose an EM algorithm over latent compositionality annotations that also improves the performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Su Nam Kim</author>
</authors>
<title>Multiword expressions. Handbook of Natural Language Processing, second edition.</title>
<date>2010</date>
<publisher>Morgan</publisher>
<contexts>
<context position="1924" citStr="Baldwin and Kim, 2010" startWordPosition="271" endWordPosition="274">n functions. We show further improvements by training a decomposition function in addition to the composition function. Finally, we propose an EM algorithm over latent compositionality annotations that also improves the performance. 1 Introduction Multiword Expressions (MWEs) are sequences of words that exhibit some kind of idiosyncrasy. This idiosyncrasy can be semantic, statistical, or syntactic1. Ivory tower, speed limit, and at large 1MWEs can have other less significant kinds of idiosyncrasies. For instance lexical idiosyncrasy as in ad hoc, and pragmatic idiosyncrasy as in good morning (Baldwin and Kim, 2010) are examples of semantically, statistically and syntactically idiosyncratic MWEs respectively. Note that an MWE can be idiosyncratic at several levels. In general, semantically idiosyncratic MWEs are commonly referred to as non-compositional (Baldwin and Kim, 2010) and statistically idiosyncratic MWEs are commonly referred to as collocations (Sag et al., 2002). Non-compositional MWEs are those whose meaning can not be readily inferred from the meaning of their constituents and collocations are those MWEs whose constituents co-occur more than expected by chance. Collocations constitute the lar</context>
</contexts>
<marker>Baldwin, Kim, 2010</marker>
<rawString>Timothy Baldwin and Su Nam Kim. 2010. Multiword expressions. Handbook of Natural Language Processing, second edition. Morgan and Claypool.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Colin Bannard</author>
<author>Takaaki Tanaka</author>
<author>Dominic Widdows</author>
</authors>
<title>An empirical model of multiword expression decomposability.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>89--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9260" citStr="Baldwin et al. (2003)" startWordPosition="1389" endWordPosition="1392">y a few) verbs in a large corpus, it presumably has an id1734 iomatic nature. Lin (1999) argues that the mutual information between the constituents of a non-compositional phrase is significantly different from that of a phrase created by substituting the constituents of that phrase with their similar words. Their evaluation reveals a low precision (16 − 39%) and recall (14 − 21%). In any case this method is not able to discriminate non-compositional MWEs from collocational MWEs as they share the same property of nonsubstitutability (their constituents cannot be replaced with their synonyms). Baldwin et al. (2003) present a method that decides about the non-compositionality of English NCs and verb particle constructions by using latent semantic analysis to calculate the similarity between a MWE and its components. They argue that a higher similarity indicates a higher degree of compositionality. McCarthy et al. (2003) devise a number of measures based on comparison of the neighbors of phrasal verbs and their corresponding simplex verbs. They evaluate these measures by calculating their correlation with human compositionality judgments on a set of phrasal verbs. They show that some of the measures have </context>
</contexts>
<marker>Baldwin, Bannard, Tanaka, Widdows, 2003</marker>
<rawString>Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An empirical model of multiword expression decomposability. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatmentVolume 18, pages 89–96. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1183--1193</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4401" citStr="Baroni and Zamparelli (2010)" startWordPosition="643" endWordPosition="646">onstructions constituting the most prominent categories of MWEs. 1733 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significantly outperform additive and multiplicative functions. In this work, we too assume that composition is arguably a complex function. We believe simplified composition functions, such as additive and multiplicative functions and their weighted variations, while having advantages such as being impervious to ov</context>
<context position="12497" citStr="Baroni and Zamparelli (2010)" startWordPosition="1897" endWordPosition="1900"> compositionality. In particular compositional phrases are less similar to their neighbors in semantic space. The distributional vectors representing the semantics of words were created using the standard window method and 50,000 most frequent context words. They show that their model slightly (+0.014 and +0.007) outperforms their baselines (Venkatapathy and Joshi, 2005; McCarthy et al., 2007). All of the models mentioned so far are based on conventional4 or count based vector space representation of the words. More recent works however are based on representation learning of word embeddings. Baroni and Zamparelli (2010) regard adjective as matrices and nouns as real-valued vectors for Italian adjective noun composition. They learn the adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word vectors, and Mikolov et al. (2013b) present a model of learning non-compositional phrases by calculating a data-driven score for certain frequent phrases (up to size two) and learn them as a whole. Salehi et al. (2015) borrow the wor</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1183–1193. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>238--247</pages>
<contexts>
<context position="13639" citStr="Baroni et al., 2014" startWordPosition="2076" endWordPosition="2079">up to size two) and learn them as a whole. Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with conventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of MWEs. 4Conventional or count based models of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic compositio</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, volume 1, pages 238–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning, ICML ’08,</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="14102" citStr="Collobert and Weston, 2008" startWordPosition="2152" endWordPosition="2155">-compositionality of MWEs. 4Conventional or count based models of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic composition (Mikolov et al., 2013b) and outperformed the conventional count based contextual models in predicting noncompositionality of MWEs (Salehi et al., 2015). In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds. We chose an English Wikipedia dump as our corpus. After filtering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50. We learn t</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning, ICML ’08, pages 160–167, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="14074" citStr="Collobert et al., 2011" startWordPosition="2148" endWordPosition="2151">ddings in predicting non-compositionality of MWEs. 4Conventional or count based models of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic composition (Mikolov et al., 2013b) and outperformed the conventional count based contextual models in predicting noncompositionality of MWEs (Salehi et al., 2015). In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds. We chose an English Wikipedia dump as our corpus. After filtering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurre</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Cook</author>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context.</title>
<date>2007</date>
<booktitle>In Proceedings of the workshop on</booktitle>
<pages>41--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10467" citStr="Cook et al. (2007)" startWordPosition="1586" endWordPosition="1589">sures have significant correlations with human judgments. Venkatapathy and Joshi (2005) present a supervised model that benefits from both collocational and contextual information and ranks the MWE candidates based on their non-compositionality. Katz and Giesbrecht (2006) use distributional semantics and LSA as a model of context similarity to test whether the local context of a MWE can distinguish its idiomatic use from literal use. They further compare the context of a MWE with the context of its components and show that this can be used to decide whether the expression is idiomatic or not. Cook et al. (2007) is a relatively different work where the authors propose a syntactic approach to identify semantic non-compositionality of verb-noun MWEs. McCarthy et al. (2007) use various models of selectional preferences for detecting non-compositional verb-object pairs. Reddy et al. (2011) employ the additive and multiplicative composition functions presented by Mitchell and Lapata (2008)3 and several similar3Mitchell and Lapata (2008) present an analysis of vectorbased additive and multiplicative semantic composition models where each words is represented by its distributional vector. They conclude that</context>
</contexts>
<marker>Cook, Fazly, Stevenson, 2007</marker>
<rawString>Paul Cook, Afsaneh Fazly, and Suzanne Stevenson. 2007. Pulling their weight: Exploiting syntactic forms for the automatic identification of idiomatic expressions in context. In Proceedings of the workshop on a broader perspective on multiword expressions, pages 41–48. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Cybenko</author>
</authors>
<title>Approximation by superpositions of a sigmoidal function.</title>
<date>1989</date>
<journal>Mathematics of Control, Signals and Systems,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="20277" citStr="Cybenko, 1989" startWordPosition="3159" endWordPosition="3160"> the polynomial regression in which we allow the presence of only a few non-zero elements in the θ matrix. The sparsity regularizer is more important in the case of polynomial regression as we have many more parameters. The quadratic model is similar to Recursive Neural Tensor compositionality model of Socher et al. (2013). But in our model the tensor is symmetric around the diagonal. Figure 2 shows the pure quadratic transformation matrices. , x1x2, ··· xn−1xn ,`1,···x: � �� � Y interaction terms linear terms 1737 4.3 Neural Networks A feed forward neural network is a universal approximator (Cybenko, 1989): feed-forward network with a single hidden layer can approximate any continuous function, provided it has enough hidden units. Therefore we use neural networks as a powerful class of learning models to learn semantic composition. The number of hidden units gives us a measure to control expressiveness of our model. f(φ(wi), φ(wj)) = σ([φ(wi), φ(wj)]Wih)Who Similar to the previous models, we optionally impose sparsity over weight matrices of the neural network to be able to induce more meaningful learning errors. 4.4 Experimental Results We evaluated the above models on the data set of Farahman</context>
</contexts>
<marker>Cybenko, 1989</marker>
<rawString>G. Cybenko. 1989. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4):303–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Meghdad Farahmand</author>
<author>Aaron Smith</author>
<author>Joakim Nivre</author>
</authors>
<title>A multiword expression data set: Annotating non-compositionality and conventionalization for english noun compounds.</title>
<date>2015</date>
<booktitle>In Proceedings of the 11th Workshop on Multiword Expressions (MWENAACL 2015). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7338" citStr="Farahmand et al. (2015)" startWordPosition="1085" endWordPosition="1088">ike detection algorithm based on hidden compositionality annotations. The best composition is the one that is the best fit on all the data points except the noncompositional ones. Since we don’t use annotated data at training time, we assume annotations to be hidden variables and iteratively alternate between optimizing the composition function and optimizing the hidden compositionality annotations. We show that this iterative algorithm increases the accuracy of non-compositionality detection compared to the case when training is done on all examples. We run our experiments on the data set of Farahmand et al. (2015) who provide a set of English NCs which are annotated with noncompositionality judgments. We show that quadratic regression significantly outperforms additive and multiplicative baselines and all other models in modelling semantic composition and identifying the non-compositional NCs. In short, the contributions of our work are: to empirically evaluate various composition functions ranging from simple to overly complex in order to find the most accurate function; to propose, to the best of our knowledge for the first time, a method of identifying non-compositional phrases as phrases for which </context>
<context position="20892" citStr="Farahmand et al. (2015)" startWordPosition="3256" endWordPosition="3259">o, 1989): feed-forward network with a single hidden layer can approximate any continuous function, provided it has enough hidden units. Therefore we use neural networks as a powerful class of learning models to learn semantic composition. The number of hidden units gives us a measure to control expressiveness of our model. f(φ(wi), φ(wj)) = σ([φ(wi), φ(wj)]Wih)Who Similar to the previous models, we optionally impose sparsity over weight matrices of the neural network to be able to induce more meaningful learning errors. 4.4 Experimental Results We evaluated the above models on the data set of Farahmand et al. (2015). They provide a set of 1042 English NCs with four non-compositionality judgments. The judgments are binary decisions taken by four experts about whether or not a compound is non-compositional. We calculate a votebased non-compositionality score for each of the data set compounds by summing over its noncompositionality judgments. The neural network models are trained using stochastic gradient descent. We use the additive and multiplicative models of modelling composition and detecting noncompositionality presented by Salehi et al. (2015) and (Reddy et al., 2011) as state of the art baselines. </context>
</contexts>
<marker>Farahmand, Smith, Nivre, 2015</marker>
<rawString>Meghdad Farahmand, Aaron Smith, and Joakim Nivre. 2015. A multiword expression data set: Annotating non-compositionality and conventionalization for english noun compounds. In Proceedings of the 11th Workshop on Multiword Expressions (MWENAACL 2015). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving Word Representations via Global Context and Multiple Word Prototypes.</title>
<date>2012</date>
<booktitle>In Annual Meeting of the Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="14155" citStr="Huang et al., 2012" startWordPosition="2161" endWordPosition="2164">s of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic composition (Mikolov et al., 2013b) and outperformed the conventional count based contextual models in predicting noncompositionality of MWEs (Salehi et al., 2015). In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds. We chose an English Wikipedia dump as our corpus. After filtering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single tokens usi</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving Word Representations via Global Context and Multiple Word Prototypes. In Annual Meeting of the Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Katz</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multiword expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, MWE ’06,</booktitle>
<pages>12--19</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10121" citStr="Katz and Giesbrecht (2006)" startWordPosition="1521" endWordPosition="1524">arity indicates a higher degree of compositionality. McCarthy et al. (2003) devise a number of measures based on comparison of the neighbors of phrasal verbs and their corresponding simplex verbs. They evaluate these measures by calculating their correlation with human compositionality judgments on a set of phrasal verbs. They show that some of the measures have significant correlations with human judgments. Venkatapathy and Joshi (2005) present a supervised model that benefits from both collocational and contextual information and ranks the MWE candidates based on their non-compositionality. Katz and Giesbrecht (2006) use distributional semantics and LSA as a model of context similarity to test whether the local context of a MWE can distinguish its idiomatic use from literal use. They further compare the context of a MWE with the context of its components and show that this can be used to decide whether the expression is idiomatic or not. Cook et al. (2007) is a relatively different work where the authors propose a syntactic approach to identify semantic non-compositionality of verb-noun MWEs. McCarthy et al. (2007) use various models of selectional preferences for detecting non-compositional verb-object p</context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>Graham Katz and Eugenie Giesbrecht. 2006. Automatic identification of non-compositional multiword expressions using latent semantic analysis. In Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, MWE ’06, pages 12–19, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Stephen Clark</author>
</authors>
<title>Detecting compositionality of multi-word expressions using nearest neighbours in vector space models.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1427--1432</pages>
<contexts>
<context position="11692" citStr="Kiela and Clark (2013)" startWordPosition="1771" endWordPosition="1774">ltiplicative and combined models do a better job in modelling vector-based semantic composition than other models. ity based models to measure the compositionality of MWEs. Similarity based models measure the similarity of a MWE vector and sum/product of its constituents’ vectors. Their evaluation (which is carried out on a set of 90 annotated NCs) shows that there is a relatively high correlation (Spearman p of between 0.51 and 0.71) between their models’ predictions and human judgments on noncompositionality of English NCs, with weighted additive function outperforming all the other models. Kiela and Clark (2013) present a model of detecting non-compositionality based on the hypothesis that the average distance between a phrase vector and its substituted phrase vectors is related to its compositionality. In particular compositional phrases are less similar to their neighbors in semantic space. The distributional vectors representing the semantics of words were created using the standard window method and 50,000 most frequent context words. They show that their model slightly (+0.014 and +0.007) outperforms their baselines (Venkatapathy and Joshi, 2005; McCarthy et al., 2007). All of the models mention</context>
</contexts>
<marker>Kiela, Clark, 2013</marker>
<rawString>Douwe Kiela and Stephen Clark. 2013. Detecting compositionality of multi-word expressions using nearest neighbours in vector space models. In EMNLP, pages 1427–1432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic identification of non-compositional phrases.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99,</booktitle>
<pages>317--324</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2729" citStr="Lin, 1999" startWordPosition="393" endWordPosition="394">s are commonly referred to as non-compositional (Baldwin and Kim, 2010) and statistically idiosyncratic MWEs are commonly referred to as collocations (Sag et al., 2002). Non-compositional MWEs are those whose meaning can not be readily inferred from the meaning of their constituents and collocations are those MWEs whose constituents co-occur more than expected by chance. Collocations constitute the largest subset of all kinds of MWEs, however, non-compositional ones cause more problems in various NLP tasks, for example word sense disambiguation (McCarthy et al., 2003) and machine translation (Lin, 1999). It may also be more challenging to model noncompositionality than collocational weight as the former has to do with modelling the semantics and the latter can to some extent be modeled by conventional statistical measures such as mutual information. Detecting non-compositionality in an automatic fashion has been the aim of much previous research. In this paper, we capture non-compositionality of English Noun Compounds (NCs)2 based on the assumption that the majority of the compounds are compositional, for which a composition function can be learned. This implies that the compounds for which </context>
<context position="8727" citStr="Lin (1999)" startWordPosition="1302" endWordPosition="1303">g the accuracy of the models by means of EM on hidden compositionality annotations. 2 Related Work To the best of our knowledge, attempts to extract non-compositionality in computational linguistics go back to 1998. Tapanainen et al. (1998) propose a method to identify non-compositional verbobject collocations based on the semantic asymmetry of verb-object relation. They assume that in a verb-object idiomatic expression, the object is a more interesting element in the sense that if the object appears with one (or only a few) verbs in a large corpus, it presumably has an id1734 iomatic nature. Lin (1999) argues that the mutual information between the constituents of a non-compositional phrase is significantly different from that of a phrase created by substituting the constituents of that phrase with their similar words. Their evaluation reveals a low precision (16 − 39%) and recall (14 − 21%). In any case this method is not able to discriminate non-compositional MWEs from collocational MWEs as they share the same property of nonsubstitutability (their constituents cannot be replaced with their synonyms). Baldwin et al. (2003) present a method that decides about the non-compositionality of En</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. Automatic identification of non-compositional phrases. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 317–324, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Bill Keller</author>
<author>John Carroll</author>
</authors>
<title>Detecting a continuum of compositionality in phrasal verbs.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acqusation and Treatment,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="2693" citStr="McCarthy et al., 2003" startWordPosition="386" endWordPosition="389">vels. In general, semantically idiosyncratic MWEs are commonly referred to as non-compositional (Baldwin and Kim, 2010) and statistically idiosyncratic MWEs are commonly referred to as collocations (Sag et al., 2002). Non-compositional MWEs are those whose meaning can not be readily inferred from the meaning of their constituents and collocations are those MWEs whose constituents co-occur more than expected by chance. Collocations constitute the largest subset of all kinds of MWEs, however, non-compositional ones cause more problems in various NLP tasks, for example word sense disambiguation (McCarthy et al., 2003) and machine translation (Lin, 1999). It may also be more challenging to model noncompositionality than collocational weight as the former has to do with modelling the semantics and the latter can to some extent be modeled by conventional statistical measures such as mutual information. Detecting non-compositionality in an automatic fashion has been the aim of much previous research. In this paper, we capture non-compositionality of English Noun Compounds (NCs)2 based on the assumption that the majority of the compounds are compositional, for which a composition function can be learned. This i</context>
<context position="9570" citStr="McCarthy et al. (2003)" startWordPosition="1436" endWordPosition="1439"> Their evaluation reveals a low precision (16 − 39%) and recall (14 − 21%). In any case this method is not able to discriminate non-compositional MWEs from collocational MWEs as they share the same property of nonsubstitutability (their constituents cannot be replaced with their synonyms). Baldwin et al. (2003) present a method that decides about the non-compositionality of English NCs and verb particle constructions by using latent semantic analysis to calculate the similarity between a MWE and its components. They argue that a higher similarity indicates a higher degree of compositionality. McCarthy et al. (2003) devise a number of measures based on comparison of the neighbors of phrasal verbs and their corresponding simplex verbs. They evaluate these measures by calculating their correlation with human compositionality judgments on a set of phrasal verbs. They show that some of the measures have significant correlations with human judgments. Venkatapathy and Joshi (2005) present a supervised model that benefits from both collocational and contextual information and ranks the MWE candidates based on their non-compositionality. Katz and Giesbrecht (2006) use distributional semantics and LSA as a model </context>
</contexts>
<marker>McCarthy, Keller, Carroll, 2003</marker>
<rawString>Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a continuum of compositionality in phrasal verbs. In Proceedings of the ACL-SIGLEX Workshop on Multiword Expressions: Analysis, Acqusation and Treatment, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Sriram Venkatapathy</author>
<author>Aravind K Joshi</author>
</authors>
<title>Detecting compositionality of verbobject combinations using selectional preferences. In EMNLP-CoNLL,</title>
<date>2007</date>
<pages>369--379</pages>
<contexts>
<context position="10629" citStr="McCarthy et al. (2007)" startWordPosition="1611" endWordPosition="1614">ontextual information and ranks the MWE candidates based on their non-compositionality. Katz and Giesbrecht (2006) use distributional semantics and LSA as a model of context similarity to test whether the local context of a MWE can distinguish its idiomatic use from literal use. They further compare the context of a MWE with the context of its components and show that this can be used to decide whether the expression is idiomatic or not. Cook et al. (2007) is a relatively different work where the authors propose a syntactic approach to identify semantic non-compositionality of verb-noun MWEs. McCarthy et al. (2007) use various models of selectional preferences for detecting non-compositional verb-object pairs. Reddy et al. (2011) employ the additive and multiplicative composition functions presented by Mitchell and Lapata (2008)3 and several similar3Mitchell and Lapata (2008) present an analysis of vectorbased additive and multiplicative semantic composition models where each words is represented by its distributional vector. They conclude that multiplicative and combined models do a better job in modelling vector-based semantic composition than other models. ity based models to measure the compositiona</context>
<context position="12265" citStr="McCarthy et al., 2007" startWordPosition="1858" endWordPosition="1862">ming all the other models. Kiela and Clark (2013) present a model of detecting non-compositionality based on the hypothesis that the average distance between a phrase vector and its substituted phrase vectors is related to its compositionality. In particular compositional phrases are less similar to their neighbors in semantic space. The distributional vectors representing the semantics of words were created using the standard window method and 50,000 most frequent context words. They show that their model slightly (+0.014 and +0.007) outperforms their baselines (Venkatapathy and Joshi, 2005; McCarthy et al., 2007). All of the models mentioned so far are based on conventional4 or count based vector space representation of the words. More recent works however are based on representation learning of word embeddings. Baroni and Zamparelli (2010) regard adjective as matrices and nouns as real-valued vectors for Italian adjective noun composition. They learn the adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word v</context>
</contexts>
<marker>McCarthy, Venkatapathy, Joshi, 2007</marker>
<rawString>Diana McCarthy, Sriram Venkatapathy, and Aravind K Joshi. 2007. Detecting compositionality of verbobject combinations using selectional preferences. In EMNLP-CoNLL, pages 369–379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="12897" citStr="Mikolov et al. (2013" startWordPosition="1962" endWordPosition="1965"> models mentioned so far are based on conventional4 or count based vector space representation of the words. More recent works however are based on representation learning of word embeddings. Baroni and Zamparelli (2010) regard adjective as matrices and nouns as real-valued vectors for Italian adjective noun composition. They learn the adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word vectors, and Mikolov et al. (2013b) present a model of learning non-compositional phrases by calculating a data-driven score for certain frequent phrases (up to size two) and learn them as a whole. Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with conventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of </context>
<context position="14177" citStr="Mikolov et al., 2013" startWordPosition="2165" endWordPosition="2168">similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic composition (Mikolov et al., 2013b) and outperformed the conventional count based contextual models in predicting noncompositionality of MWEs (Salehi et al., 2015). In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds. We chose an English Wikipedia dump as our corpus. After filtering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single tokens using the word2vec5 bag-o</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="12897" citStr="Mikolov et al. (2013" startWordPosition="1962" endWordPosition="1965"> models mentioned so far are based on conventional4 or count based vector space representation of the words. More recent works however are based on representation learning of word embeddings. Baroni and Zamparelli (2010) regard adjective as matrices and nouns as real-valued vectors for Italian adjective noun composition. They learn the adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word vectors, and Mikolov et al. (2013b) present a model of learning non-compositional phrases by calculating a data-driven score for certain frequent phrases (up to size two) and learn them as a whole. Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with conventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of </context>
<context position="14177" citStr="Mikolov et al., 2013" startWordPosition="2165" endWordPosition="2168">similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic composition (Mikolov et al., 2013b) and outperformed the conventional count based contextual models in predicting noncompositionality of MWEs (Salehi et al., 2015). In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds. We chose an English Wikipedia dump as our corpus. After filtering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single tokens using the word2vec5 bag-o</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013). Association for Computational Linguistics,</booktitle>
<contexts>
<context position="12897" citStr="Mikolov et al. (2013" startWordPosition="1962" endWordPosition="1965"> models mentioned so far are based on conventional4 or count based vector space representation of the words. More recent works however are based on representation learning of word embeddings. Baroni and Zamparelli (2010) regard adjective as matrices and nouns as real-valued vectors for Italian adjective noun composition. They learn the adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word vectors, and Mikolov et al. (2013b) present a model of learning non-compositional phrases by calculating a data-driven score for certain frequent phrases (up to size two) and learn them as a whole. Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with conventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of </context>
<context position="14177" citStr="Mikolov et al., 2013" startWordPosition="2165" endWordPosition="2168">similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic composition (Mikolov et al., 2013b) and outperformed the conventional count based contextual models in predicting noncompositionality of MWEs (Salehi et al., 2015). In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds. We chose an English Wikipedia dump as our corpus. After filtering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single tokens using the word2vec5 bag-o</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig. 2013c. Linguistic regularities in continuous space word representations. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013). Association for Computational Linguistics, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>236--244</pages>
<contexts>
<context position="4094" citStr="Mitchell and Lapata, 2008" startWordPosition="594" endWordPosition="598">butional semantics, semantic composition has been commonly assumed to be a trivial predetermined function such as addition, multiplication, 2MWEs have various syntactic categories such as noun compounds, verb particle constructions, light verb constructions, etc., with noun compounds and verb particle constructions constituting the most prominent categories of MWEs. 1733 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significan</context>
<context position="10847" citStr="Mitchell and Lapata (2008)" startWordPosition="1640" endWordPosition="1643">text of a MWE can distinguish its idiomatic use from literal use. They further compare the context of a MWE with the context of its components and show that this can be used to decide whether the expression is idiomatic or not. Cook et al. (2007) is a relatively different work where the authors propose a syntactic approach to identify semantic non-compositionality of verb-noun MWEs. McCarthy et al. (2007) use various models of selectional preferences for detecting non-compositional verb-object pairs. Reddy et al. (2011) employ the additive and multiplicative composition functions presented by Mitchell and Lapata (2008)3 and several similar3Mitchell and Lapata (2008) present an analysis of vectorbased additive and multiplicative semantic composition models where each words is represented by its distributional vector. They conclude that multiplicative and combined models do a better job in modelling vector-based semantic composition than other models. ity based models to measure the compositionality of MWEs. Similarity based models measure the similarity of a MWE vector and sum/product of its constituents’ vectors. Their evaluation (which is carried out on a set of 90 annotated NCs) shows that there is a rela</context>
<context position="13240" citStr="Mitchell and Lapata, 2008" startWordPosition="2019" endWordPosition="2023">adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word vectors, and Mikolov et al. (2013b) present a model of learning non-compositional phrases by calculating a data-driven score for certain frequent phrases (up to size two) and learn them as a whole. Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with conventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of MWEs. 4Conventional or count based models of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, pages 236–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>An empirical study on compositionality in compound nouns.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>210--218</pages>
<contexts>
<context position="4114" citStr="Reddy et al., 2011" startWordPosition="599" endWordPosition="602">c composition has been commonly assumed to be a trivial predetermined function such as addition, multiplication, 2MWEs have various syntactic categories such as noun compounds, verb particle constructions, light verb constructions, etc., with noun compounds and verb particle constructions constituting the most prominent categories of MWEs. 1733 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significantly outperform addit</context>
<context position="10746" citStr="Reddy et al. (2011)" startWordPosition="1627" endWordPosition="1630">istributional semantics and LSA as a model of context similarity to test whether the local context of a MWE can distinguish its idiomatic use from literal use. They further compare the context of a MWE with the context of its components and show that this can be used to decide whether the expression is idiomatic or not. Cook et al. (2007) is a relatively different work where the authors propose a syntactic approach to identify semantic non-compositionality of verb-noun MWEs. McCarthy et al. (2007) use various models of selectional preferences for detecting non-compositional verb-object pairs. Reddy et al. (2011) employ the additive and multiplicative composition functions presented by Mitchell and Lapata (2008)3 and several similar3Mitchell and Lapata (2008) present an analysis of vectorbased additive and multiplicative semantic composition models where each words is represented by its distributional vector. They conclude that multiplicative and combined models do a better job in modelling vector-based semantic composition than other models. ity based models to measure the compositionality of MWEs. Similarity based models measure the similarity of a MWE vector and sum/product of its constituents’ vec</context>
<context position="13261" citStr="Reddy et al., 2011" startWordPosition="2024" endWordPosition="2027">r regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word vectors, and Mikolov et al. (2013b) present a model of learning non-compositional phrases by calculating a data-driven score for certain frequent phrases (up to size two) and learn them as a whole. Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with conventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of MWEs. 4Conventional or count based models of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases wit</context>
<context position="21460" citStr="Reddy et al., 2011" startWordPosition="3344" endWordPosition="3347">e models on the data set of Farahmand et al. (2015). They provide a set of 1042 English NCs with four non-compositionality judgments. The judgments are binary decisions taken by four experts about whether or not a compound is non-compositional. We calculate a votebased non-compositionality score for each of the data set compounds by summing over its noncompositionality judgments. The neural network models are trained using stochastic gradient descent. We use the additive and multiplicative models of modelling composition and detecting noncompositionality presented by Salehi et al. (2015) and (Reddy et al., 2011) as state of the art baselines. The results are shown in Table 1. The second column shows the correlation between different models’ predictions and the annotated data in terms of Spearman p. The last three columns show the performance of different models in terms of Normalized Discounted Cumulative Gain (NDCG), F1 score and and Precision at 100 (P@100). For these three scores we consider the problem of predicting noncompositional NCs a problem with a binary solution where we assume compounds (of the evaluation set) with at least two non-compositionality votes are non-compositional. NDCG assign</context>
</contexts>
<marker>Reddy, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011. An empirical study on compositionality in compound nouns. In IJCNLP, pages 210–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for nlp.</title>
<date>2002</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>1--15</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2287" citStr="Sag et al., 2002" startWordPosition="324" endWordPosition="327">antic, statistical, or syntactic1. Ivory tower, speed limit, and at large 1MWEs can have other less significant kinds of idiosyncrasies. For instance lexical idiosyncrasy as in ad hoc, and pragmatic idiosyncrasy as in good morning (Baldwin and Kim, 2010) are examples of semantically, statistically and syntactically idiosyncratic MWEs respectively. Note that an MWE can be idiosyncratic at several levels. In general, semantically idiosyncratic MWEs are commonly referred to as non-compositional (Baldwin and Kim, 2010) and statistically idiosyncratic MWEs are commonly referred to as collocations (Sag et al., 2002). Non-compositional MWEs are those whose meaning can not be readily inferred from the meaning of their constituents and collocations are those MWEs whose constituents co-occur more than expected by chance. Collocations constitute the largest subset of all kinds of MWEs, however, non-compositional ones cause more problems in various NLP tasks, for example word sense disambiguation (McCarthy et al., 2003) and machine translation (Lin, 1999). It may also be more challenging to model noncompositionality than collocational weight as the former has to do with modelling the semantics and the latter c</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2002</marker>
<rawString>Ivan A Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for nlp. In Computational Linguistics and Intelligent Text Processing, pages 1–15. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bahar Salehi</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>A word embedding approach to predicting the compositionality of multiword expressions.</title>
<date>2015</date>
<booktitle>In Proceedings of NAACL HLT. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4136" citStr="Salehi et al., 2015" startWordPosition="603" endWordPosition="606">en commonly assumed to be a trivial predetermined function such as addition, multiplication, 2MWEs have various syntactic categories such as noun compounds, verb particle constructions, light verb constructions, etc., with noun compounds and verb particle constructions constituting the most prominent categories of MWEs. 1733 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significantly outperform additive and multiplicative</context>
<context position="13082" citStr="Salehi et al. (2015)" startWordPosition="1993" endWordPosition="1996">dings. Baroni and Zamparelli (2010) regard adjective as matrices and nouns as real-valued vectors for Italian adjective noun composition. They learn the adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word vectors, and Mikolov et al. (2013b) present a model of learning non-compositional phrases by calculating a data-driven score for certain frequent phrases (up to size two) and learn them as a whole. Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with conventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of MWEs. 4Conventional or count based models of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compou</context>
<context position="14392" citStr="Salehi et al., 2015" startWordPosition="2196" endWordPosition="2199">vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic composition (Mikolov et al., 2013b) and outperformed the conventional count based contextual models in predicting noncompositionality of MWEs (Salehi et al., 2015). In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds. We chose an English Wikipedia dump as our corpus. After filtering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single tokens using the word2vec5 bag-of-word model. We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the compounds’ component words. Compounds’ sizes are restricted to two (i.e. bigrams) for the sake of sim</context>
<context position="21435" citStr="Salehi et al. (2015)" startWordPosition="3339" endWordPosition="3342">ults We evaluated the above models on the data set of Farahmand et al. (2015). They provide a set of 1042 English NCs with four non-compositionality judgments. The judgments are binary decisions taken by four experts about whether or not a compound is non-compositional. We calculate a votebased non-compositionality score for each of the data set compounds by summing over its noncompositionality judgments. The neural network models are trained using stochastic gradient descent. We use the additive and multiplicative models of modelling composition and detecting noncompositionality presented by Salehi et al. (2015) and (Reddy et al., 2011) as state of the art baselines. The results are shown in Table 1. The second column shows the correlation between different models’ predictions and the annotated data in terms of Spearman p. The last three columns show the performance of different models in terms of Normalized Discounted Cumulative Gain (NDCG), F1 score and and Precision at 100 (P@100). For these three scores we consider the problem of predicting noncompositional NCs a problem with a binary solution where we assume compounds (of the evaluation set) with at least two non-compositionality votes are non-c</context>
</contexts>
<marker>Salehi, Cook, Baldwin, 2015</marker>
<rawString>Bahar Salehi, Paul Cook, and Timothy Baldwin. 2015. A word embedding approach to predicting the compositionality of multiword expressions. In Proceedings of NAACL HLT. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4534" citStr="Socher et al. (2012)" startWordPosition="665" endWordPosition="668">uage Processing, pages 1733–1742, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significantly outperform additive and multiplicative functions. In this work, we too assume that composition is arguably a complex function. We believe simplified composition functions, such as additive and multiplicative functions and their weighted variations, while having advantages such as being impervious to overfitting, can not completely capture semantic composition. Nevertheless modelling composition by means of a powerful function can be</context>
<context position="12789" citStr="Socher et al. (2012)" startWordPosition="1944" endWordPosition="1947">14 and +0.007) outperforms their baselines (Venkatapathy and Joshi, 2005; McCarthy et al., 2007). All of the models mentioned so far are based on conventional4 or count based vector space representation of the words. More recent works however are based on representation learning of word embeddings. Baroni and Zamparelli (2010) regard adjective as matrices and nouns as real-valued vectors for Italian adjective noun composition. They learn the adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word vectors, and Mikolov et al. (2013b) present a model of learning non-compositional phrases by calculating a data-driven score for certain frequent phrases (up to size two) and learn them as a whole. Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with conventional distributional v</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA,</location>
<contexts>
<context position="19987" citStr="Socher et al. (2013)" startWordPosition="3108" endWordPosition="3111"> then we have: f(φ(wi),φ(wj)) = ψ([φ(wi),φ(wj)])θ We couldn’t successfully apply any polynomial beyond quadratic transformation without overfitting. The case of a quadratic ψ transformation is: ψ(x) = x2 1, · · · x2 � �� n � Pure quadratic Similar to the linear case we can have sparse version of the polynomial regression in which we allow the presence of only a few non-zero elements in the θ matrix. The sparsity regularizer is more important in the case of polynomial regression as we have many more parameters. The quadratic model is similar to Recursive Neural Tensor compositionality model of Socher et al. (2013). But in our model the tensor is symmetric around the diagonal. Figure 2 shows the pure quadratic transformation matrices. , x1x2, ··· xn−1xn ,`1,···x: � �� � Y interaction terms linear terms 1737 4.3 Neural Networks A feed forward neural network is a universal approximator (Cybenko, 1989): feed-forward network with a single hidden layer can approximate any continuous function, provided it has enough hidden units. Therefore we use neural networks as a powerful class of learning models to learn semantic composition. The number of hidden units gives us a measure to control expressiveness of our </context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Stroudsburg, PA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Jussi Piitulainen</author>
<author>Timo J¨arvinen</author>
</authors>
<title>Idiomatic object usage and support verbs.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98,</booktitle>
<pages>1289--1293</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Tapanainen, Piitulainen, J¨arvinen, 1998</marker>
<rawString>Pasi Tapanainen, Jussi Piitulainen, and Timo J¨arvinen. 1998. Idiomatic object usage and support verbs. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL ’98, pages 1289–1293, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
</authors>
<title>Regression shrinkage and selection via the lasso: a retrospective.</title>
<date>2011</date>
<journal>Journal of the Royal Statistical Society: Series B (Statistical Methodology),</journal>
<volume>73</volume>
<issue>3</issue>
<contexts>
<context position="18487" citStr="Tibshirani, 2011" startWordPosition="2859" endWordPosition="2861">se Linear Projection Figure 1: Linear transformation matrix of compositionality for embeddings of size 50 (a) Pure Quadratic (b) Sparse Pure Quadratic Figure 2: Pure quadratic transformation matrix of compositionality for embeddings of size 50 In case of sparse linear projections, only a few elements of the projection matrix θ are non-zero. This means that not all dimensions of the latent space has a role in all dimensions of the compound embedding. To apply sparsity on θ, we add a norm 1 penalty on it and add that to the least square optimization. This forms a multi-variant lasso regression (Tibshirani, 2011). min II[φ(wi), φ(wj)]θ − φ(wi, wj)II + λ|θ| θ Figure 1 shows the transformation matrices of linear projection and sparse linear projection. The two diagonals of the matrices correspond to the sum of the two embeddings, which we can see are the main component of the sparse function, and play an important role in the non-sparse one. We will see that despite being an important component of these functions, sum alone is not capable of accurately modelling semantic composition. 4.2 Polynomial Projection Polynomial projection is a non-linear projection that assumes the relation between compound emb</context>
</contexts>
<marker>Tibshirani, 2011</marker>
<rawString>Robert Tibshirani. 2011. Regression shrinkage and selection via the lasso: a retrospective. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 73(3):273–282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriram Venkatapathy</author>
<author>Aravind K Joshi</author>
</authors>
<title>Measuring the relative compositionality of verbnoun (v-n) collocations by integrating features.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>899--906</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9936" citStr="Venkatapathy and Joshi (2005)" startWordPosition="1493" endWordPosition="1497">mpositionality of English NCs and verb particle constructions by using latent semantic analysis to calculate the similarity between a MWE and its components. They argue that a higher similarity indicates a higher degree of compositionality. McCarthy et al. (2003) devise a number of measures based on comparison of the neighbors of phrasal verbs and their corresponding simplex verbs. They evaluate these measures by calculating their correlation with human compositionality judgments on a set of phrasal verbs. They show that some of the measures have significant correlations with human judgments. Venkatapathy and Joshi (2005) present a supervised model that benefits from both collocational and contextual information and ranks the MWE candidates based on their non-compositionality. Katz and Giesbrecht (2006) use distributional semantics and LSA as a model of context similarity to test whether the local context of a MWE can distinguish its idiomatic use from literal use. They further compare the context of a MWE with the context of its components and show that this can be used to decide whether the expression is idiomatic or not. Cook et al. (2007) is a relatively different work where the authors propose a syntactic</context>
<context position="12241" citStr="Venkatapathy and Joshi, 2005" startWordPosition="1854" endWordPosition="1857">ed additive function outperforming all the other models. Kiela and Clark (2013) present a model of detecting non-compositionality based on the hypothesis that the average distance between a phrase vector and its substituted phrase vectors is related to its compositionality. In particular compositional phrases are less similar to their neighbors in semantic space. The distributional vectors representing the semantics of words were created using the standard window method and 50,000 most frequent context words. They show that their model slightly (+0.014 and +0.007) outperforms their baselines (Venkatapathy and Joshi, 2005; McCarthy et al., 2007). All of the models mentioned so far are based on conventional4 or count based vector space representation of the words. More recent works however are based on representation learning of word embeddings. Baroni and Zamparelli (2010) regard adjective as matrices and nouns as real-valued vectors for Italian adjective noun composition. They learn the adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that </context>
</contexts>
<marker>Venkatapathy, Joshi, 2005</marker>
<rawString>Sriram Venkatapathy and Aravind K. Joshi. 2005. Measuring the relative compositionality of verbnoun (v-n) collocations by integrating features. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 899–906, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Second AAAI Symposium on Quantum Interaction,</booktitle>
<volume>26</volume>
<pages>28</pages>
<contexts>
<context position="4250" citStr="Widdows (2008)" startWordPosition="624" endWordPosition="625">c categories such as noun compounds, verb particle constructions, light verb constructions, etc., with noun compounds and verb particle constructions constituting the most prominent categories of MWEs. 1733 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significantly outperform additive and multiplicative functions. In this work, we too assume that composition is arguably a complex function. We believe simplified com</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Second AAAI Symposium on Quantum Interaction, volume 26, page 28th.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Yazdani</author>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Computing text semantic relatedness using the contents and links of a hypertext encyclopedia.</title>
<date>2013</date>
<journal>Artif. Intell.,</journal>
<pages>194--176</pages>
<contexts>
<context position="14135" citStr="Yazdani and Popescu-Belis, 2013" startWordPosition="2156" endWordPosition="2160">Conventional or count based models of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic composition (Mikolov et al., 2013b) and outperformed the conventional count based contextual models in predicting noncompositionality of MWEs (Salehi et al., 2015). In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds. We chose an English Wikipedia dump as our corpus. After filtering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds </context>
</contexts>
<marker>Yazdani, Popescu-Belis, 2013</marker>
<rawString>Majid Yazdani and Andrei Popescu-Belis. 2013. Computing text semantic relatedness using the contents and links of a hypertext encyclopedia. Artif. Intell., 194:176–202.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
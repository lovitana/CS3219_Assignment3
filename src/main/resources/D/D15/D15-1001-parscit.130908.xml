<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000006">
<title confidence="0.9974235">
Language Understanding for Text-based Games using Deep
Reinforcement Learning
</title>
<author confidence="0.887978">
Karthik Narasimhan* Tejas D Kulkarni* Regina Barzilay
</author>
<affiliation confidence="0.768371">
CSAIL, MIT CSAIL, BCS, MIT CSAIL, MIT
</affiliation>
<email confidence="0.987602">
karthikn@csail.mit.edu tejask@mit.edu regina@csail.mit.edu
</email>
<sectionHeader confidence="0.993489" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999957761904762">
In this paper, we consider the task of learn-
ing control policies for text-based games.
In these games, all interactions in the vir-
tual world are through text and the un-
derlying state is not observed. The re-
sulting language barrier makes such envi-
ronments challenging for automatic game
players. We employ a deep reinforcement
learning framework to jointly learn state
representations and action policies using
game rewards as feedback. This frame-
work enables us to map text descriptions
into vector representations that capture the
semantics of the game states. We eval-
uate our approach on two game worlds,
comparing against baselines using bag-of-
words and bag-of-bigrams for state rep-
resentations. Our algorithm outperforms
the baselines on both worlds demonstrat-
ing the importance of learning expressive
representations. 1
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952272727273">
In this paper, we address the task of learning con-
trol policies for text-based strategy games. These
games, predecessors to modern graphical ones,
still enjoy a large following worldwide.2 They of-
ten involve complex worlds with rich interactions
and elaborate textual descriptions of the underly-
ing states (see Figure 1). Players read descriptions
of the current world state and respond with natural
language commands to take actions. Since the un-
derlying state is not directly observable, the player
has to understand the text in order to act, making it
</bodyText>
<note confidence="0.962664">
∗Both authors contributed equally to this work.
</note>
<footnote confidence="0.999898">
1Code is available at http://people.csail.mit.
edu/karthikn/mud-play.
2http://mudstats.com/
</footnote>
<figureCaption confidence="0.7465664">
Figure 1: Sample gameplay from a Fantasy World.
The player with the quest of finding a secret tomb,
is currently located on an old bridge. She then
chooses an action to go east that brings her to a
ruined gatehouse (State 2).
</figureCaption>
<bodyText confidence="0.9985885">
challenging for existing AI programs to play these
games (DePristo and Zubek, 2001).
In designing an autonomous game player, we
have considerable latitude when selecting an ad-
equate state representation to use. The simplest
method is to use a bag-of-words representation
derived from the text description. However, this
scheme disregards the ordering of words and the
finer nuances of meaning that evolve from com-
posing words into sentences and paragraphs. For
instance, in State 2 in Figure 1, the agent has to
understand that going east will lead it to the cas-
tle whereas moving south will take it to the stand-
ing archway. An alternative approach is to convert
text descriptions to pre-specified representations
using annotated training data, commonly used in
</bodyText>
<note confidence="0.585777">
Command: Go east
</note>
<figureCaption confidence="0.269695">
State 2: Ruined gatehouse
</figureCaption>
<bodyText confidence="0.947891666666667">
The old gatehouse is near collapse. Part of
its northern wall has already fallen down ...
East of the gatehouse leads out to a small
open area surrounded by the remains of the
castle. There is also a standing archway of-
fering passage to a path along the old south-
ern inner wall.
Exits: Standing archway, castle corner,
Bridge over the abyss
</bodyText>
<figureCaption confidence="0.642829">
State 1: The old bridge
</figureCaption>
<bodyText confidence="0.79687925">
You are standing very close to the bridge’s
eastern foundation. If you go east you will
be back on solid ground ... The bridge
sways in the wind.
</bodyText>
<page confidence="0.80943">
1
</page>
<note confidence="0.9855625">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1–11,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.998215380952381">
language grounding tasks (Matuszek et al., 2013;
Kushman et al., 2014).
In contrast, our goal is to learn useful represen-
tations in conjunction with control policies. We
adopt a reinforcement learning framework and for-
mulate game sequences as Markov Decision Pro-
cesses. An agent playing the game aims to maxi-
mize rewards that it obtains from the game engine
upon the occurrence of certain events. The agent
learns a policy in the form of an action-value func-
tion Q(s, a) which denotes the long-term merit of
an action a in state s.
The action-value function is parametrized us-
ing a deep recurrent neural network, trained us-
ing the game feedback. The network contains two
modules. The first one converts textual descrip-
tions into vector representations that act as prox-
ies for states. This component is implemented us-
ing Long Short-Term Memory (LSTM) networks
(Hochreiter and Schmidhuber, 1997). The second
module of the network scores the actions given the
vector representation computed by the first.
We evaluate our model using two Multi-User
Dungeon (MUD) games (Curtis, 1992; Amir and
Doyle, 2002). The first game is designed to pro-
vide a controlled setup for the task, while the sec-
ond is a publicly available one and contains hu-
man generated text descriptions with significant
language variability. We compare our algorithm
against baselines of a random player and mod-
els that use bag-of-words or bag-of-bigrams rep-
resentations for a state. We demonstrate that our
model LSTM-DQN significantly outperforms the
baselines in terms of number of completed quests
and accumulated rewards. For instance, on a fan-
tasy MUD game, our model learns to complete
96% of the quests, while the bag-of-words model
and a random baseline solve only 82% and 5% of
the quests, respectively. Moreover, we show that
the acquired representation can be reused across
games, speeding up learning and leading to faster
convergence of Q-values.
</bodyText>
<sectionHeader confidence="0.999801" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999824851851852">
Learning control policies from text is gaining in-
creasing interest in the NLP community. Example
applications include interpreting help documenta-
tion for software (Branavan et al., 2010), navi-
gating with directions (Vogel and Jurafsky, 2010;
Kollar et al., 2010; Artzi and Zettlemoyer, 2013;
Matuszek et al., 2013; Andreas and Klein, 2015)
and playing computer games (Eisenstein et al.,
2009; Branavan et al., 2011a).
Games provide a rich domain for grounded lan-
guage analysis. Prior work has assumed perfect
knowledge of the underlying state of the game to
learn policies. Gorniak and Roy (2005) developed
a game character that can be controlled by spoken
instructions adaptable to the game situation. The
grounding of commands to actions is learned from
a transcript manually annotated with actions and
state attributes. Eisenstein et al. (2009) learn game
rules by analyzing a collection of game-related
documents and precompiled traces of the game. In
contrast to the above work, our model combines
text interpretation and strategy learning in a single
framework. As a result, textual analysis is guided
by the received control feedback, and the learned
strategy directly builds on the text interpretation.
Our work closely relates to an automatic game
player that utilizes text manuals to learn strategies
for Civilization (Branavan et al., 2011a). Similar
to our approach, text analysis and control strate-
gies are learned jointly using feedback provided
by the game simulation. In their setup, states are
fully observable, and the model learns a strategy
by combining state/action features and features
extracted from text. However, in our application,
the state representation is not provided, but has to
be inferred from a textual description. Therefore,
it is not sufficient to extract features from text to
supplement a simulation-based player.
Another related line of work consists of auto-
matic video game players that infer state repre-
sentations directly from raw pixels (Koutn´ık et al.,
2013; Mnih et al., 2015). For instance, Mnih et
al. (2015) learn control strategies using convolu-
tional neural networks, trained with a variant of
Q-learning (Watkins and Dayan, 1992). While
both approaches use deep reinforcement learning
for training, our work has important differences.
In order to handle the sequential nature of text, we
use Long Short-Term Memory networks to auto-
matically learn useful representations for arbitrary
text descriptions. Additionally, we show that de-
composing the network into a representation layer
and an action selector is useful for transferring the
learnt representations to new game scenarios.
</bodyText>
<sectionHeader confidence="0.994933" genericHeader="method">
3 Background
</sectionHeader>
<bodyText confidence="0.7688155">
Game Representation We represent a game by
the tuple (H, A, T, R, Ψ), where H is the set of
</bodyText>
<page confidence="0.990248">
2
</page>
<bodyText confidence="0.999864777777778">
all possible game states, A = {(a, o)} is the set of
all commands (action-object pairs), T (h&apos;  |h, a, o)
is the stochastic transition function between states
and R(h, a, o) is the reward function. The game
state H is hidden from the player, who only re-
ceives a varying textual description, produced by
a stochastic function Ψ : H → S. Specifically,
the underlying state h in the game engine keeps
track of attributes such as the player’s location,
her health points, time of day, etc. The function
Ψ (also part of the game framework) then converts
this state into a textual description of the location
the player is at or a message indicating low health.
We do not assume access to either H or Ψ for our
agent during both training and testing phases of
our experiments. We denote the space of all possi-
ble text descriptions s to be S. Rewards are gener-
ated using R and are only given to the player upon
completion of in-game quests.
Q-Learning Reinforcement Learning is a com-
monly used framework for learning control poli-
cies in game environments (Silver et al., 2007;
Amato and Shani, 2010; Branavan et al., 2011b;
Szita, 2012). The game environment can be
formulated as a sequence of state transitions
(s, a, r, s&apos;) of a Markov Decision Process (MDP).
The agent takes an action a in state s by consult-
ing a state-action value function Q(s, a), which is
a measure of the action’s expected long-term re-
ward. Q-Learning (Watkins and Dayan, 1992) is
a model-free technique which is used to learn an
optimal Q(s, a) for the agent. Starting from a ran-
dom Q-function, the agent continuously updates
its Q-values by playing the game and obtaining re-
wards. The iterative updates are derived from the
Bellman equation (Sutton and Barto, 1998):
</bodyText>
<equation confidence="0.8906875">
Qi+1(s, a) = E[r + -y max Qi(s&apos;, a&apos;)  |s, a] (1)
a
</equation>
<bodyText confidence="0.995211583333333">
where -y is a discount factor for future rewards and
the expectation is over all game transitions that in-
volved the agent taking action a in state s.
Using these evolving Q-values, the agent
chooses the action with the highest Q(s, a) to
maximize its expected future rewards. In practice,
the trade-off between exploration and exploitation
can be achieved following an E-greedy policy (Sut-
ton and Barto, 1998), where the agent performs a
random action with probability E.
Deep Q-Network In large games, it is often im-
practical to maintain the Q-value for all possible
</bodyText>
<figureCaption confidence="0.9092515">
Figure 2: Architecture of LSTM-DQN: The Rep-
resentation Generator (φR) (bottom) takes as input
a stream of words observed in state s and produces
a vector representation vs, which is fed into the
action scorer (φA) (top) to produce scores for all
actions and argument objects.
</figureCaption>
<bodyText confidence="0.998231866666667">
state-action pairs. One solution to this problem
is to approximate Q(s, a) using a parametrized
function Q(s, a; θ), which can generalize over
states and actions by considering higher-level at-
tributes (Sutton and Barto, 1998; Branavan et al.,
2011a). However, creating a good parametrization
requires knowledge of the state and action spaces.
One way to bypass this feature engineering is to
use a Deep Q-Network (DQN) (Mnih et al., 2015).
The DQN approximates the Q-value function with
a deep neural network to predict Q(s, a) for all
possible actions a simultaneously given the cur-
rent state s. The non-linear function layers of the
DQN also enable it to learn better value functions
than linear approximators.
</bodyText>
<subsectionHeader confidence="0.578526">
4 Learning Representations and Control
Policies
</subsectionHeader>
<bodyText confidence="0.999711166666667">
In this section, we describe our model (DQN) and
describe its use in learning good Q-value approxi-
mations for games with stochastic textual descrip-
tions. We divide our model into two parts. The
first module is a representation generator that con-
verts the textual description of the current state
into a vector. This vector is then input into the
second module which is an action scorer. Fig-
ure 2 shows the overall architecture of our model.
We learn the parameters of both the representation
generator and the action scorer jointly, using the
in-game reward feedback.
</bodyText>
<figure confidence="0.994930384615385">
Q(s, a) Q(s, o)
LSTM
w1 w2 w3 wn
Linear
LSTM
Mean Pooling
Linear
ReLU
LSTM
Linear
LSTM
OA
OR
</figure>
<page confidence="0.981433">
3
</page>
<bodyText confidence="0.988423022988506">
Representation Generator (φR) The represen-
tation generator reads raw text displayed to the
agent and converts it to a vector representation vs.
A bag-of-words (BOW) representation is not suf-
ficient to capture higher-order structures of sen-
tences and paragraphs. The need for a better se-
mantic representation of the text is evident from
the average performance of this representation in
playing MUD-games (as we show in Section 6).
In order to assimilate better representations,
we utilize a Long Short-Term Memory network
(LSTM) (Hochreiter and Schmidhuber, 1997) as
a representation generator. LSTMs are recurrent
neural networks with the ability to connect and
recognize long-range patterns between words in
text. They are more robust than BOW to small
variations in word usage and are able to capture
underlying semantics of sentences to some ex-
tent. In recent work, LSTMs have been used suc-
cessfully in NLP tasks such as machine transla-
tion (Sutskever et al., 2014) and sentiment anal-
ysis (Tai et al., 2015) to compose vector repre-
sentations of sentences from word-level embed-
dings (Mikolov et al., 2013; Pennington et al.,
2014). In our setup, the LSTM network takes in
word embeddings wk from the words in a descrip-
tion s and produces output vectors xk at each step.
To get the final state representation vs, we add a
mean pooling layer which computes the element-
wise mean over the output vectors xk.3
vs = 1 n xk (2)
n k=1
Action Scorer (φA) The action scorer module
produces scores for the set of possible actions
given the current state representation. We use a
multi-layered neural network for this purpose (see
Figure 2). The input to this module is the vec-
tor from the representation generator, vs = φR(s)
and the outputs are scores for actions a E A.
Scores for all actions are predicted simultaneously,
which is computationally more efficient than scor-
ing each state-action pair separately. Thus, by
combining the representation generator and action
scorer, we can obtain the approximation for the Q-
function as Q(s, a) ≈ φA(φR(s))[a].
An additional complexity in playing MUD-
games is that the actions taken by the player are
3We also experimented with considering just the output
vector of the LSTM after processing the last word. Empiri-
cally, we find that mean pooling leads to faster learning, so
we use it in all our experiments.
multi-word natural language commands such as
eat apple or go east. Due to computational con-
straints, in this work we limit ourselves to con-
sider commands to consist of one action (e.g. eat)
and one argument object (e.g. apple). This as-
sumption holds for the majority of the commands
in our worlds, with the exception of one class of
commands that require two arguments (e.g. move
red-root right, move blue-root up). We consider all
possible actions and objects available in the game
and predict both for each state using the same net-
work (Figure 2). We consider the Q-value of the
entire command (a, o) to be the average of the Q-
values of the action a and the object o. For the rest
of this section, we only show equations for Q(s, a)
but similar ones hold for Q(s, o).
Parameter Learning We learn the parameters
θR of the representation generator and θA of the
action scorer using stochastic gradient descent
with RMSprop (Tieleman and Hinton, 2012). The
complete training procedure is shown in Algo-
rithm 1. In each iteration i, we update the pa-
rameters to reduce the discrepancy between the
predicted value of the current state Q(st, at; θi)
(where θi = [θR; θA]i) and the expected Q-value
given the reward rt and the value of the next state
maxa Q(st+1, a; θi−1).
We keep track of the agent’s previous experi-
ences in a memory D.4 Instead of performing
updates to the Q-value using transitions from the
current episode, we sample a random transition
(ˆs, ˆa, s&apos;, r) from D. Updating the parameters in
this way avoids issues due to strong correlation
when using transitions of the same episode (Mnih
et al., 2015). Using the sampled transition and (1),
we obtain the following loss function to minimize:
</bodyText>
<equation confidence="0.997975">
Li(θi) = Eˆs,ˆa[(yi − Q(ˆs, ˆa; θi))2] (3)
</equation>
<bodyText confidence="0.9996292">
where yi = Eˆs,ˆa[r + γ maxa&apos; Q(s&apos;, a&apos;; θi−1)  |ˆs, ˆa]
is the target Q-value with parameters θi−1 fixed
from the previous iteration.
The updates on the parameters θ can be per-
formed( using the following gradient of Li (θi):
</bodyText>
<equation confidence="0.991355">
VθiLi(Bi) = Eˆs,ˆa[2(yi − Q(ˆs, ˆa; θi))VθiQ(ˆs, ˆa; θi)]
</equation>
<bodyText confidence="0.996874">
For each epoch of training, the agent plays several
episodes of the game, which is restarted after ev-
ery terminal state.
</bodyText>
<footnote confidence="0.995753">
4The memory is limited and rewritten in a first-in-first-out
(FIFO) fashion.
</footnote>
<page confidence="0.980272">
4
</page>
<bodyText confidence="0.256111">
Algorithm 1 Training Procedure for DQN with prioritized sampling
</bodyText>
<listItem confidence="0.8677564">
1: Initialize experience memory D
2: Initialize parameters of representation generator (OR) and action scorer (OA) randomly
3: for episode = 1, M do
4: Initialize game and get start state description s1
5: for t = 1, T do
6: Convert st (text) to representation vst using OR
7: if random() &lt; c then
8: Select a random action at
9: else
10: Compute Q(st, a) for all actions using OA(vst)
11: Select at = argmax Q(st, a)
12: Execute action at and observe reward rt and new state st+1
13: Set priority pt = 1 if rt &gt; 0, else pt = 0
14: Store transition (st, at, rt, st+1,pt) in D
15: Sample random mini batch of transitions (sj, aj, rj, sj+1, pj) from D,
with fraction ρ having pj = 1
� rj if sj+1 is terminal
16: Set yj =
rj + ry maxal Q(sj+1, a&apos;; 0) if sj+1 is non-terminal
17: Perform gradient descent step on the loss G(0) = (yj − Q(sj, aj; 0))2
</listItem>
<bodyText confidence="0.909043052631579">
Mini-batch Sampling In practice, online up-
dates to the parameters 0 are performed over a
mini batch of state transitions, instead of a single
transition. This increases the number of experi-
ences used per step and is also more efficient due
to optimized matrix operations.
The simplest method to create these mini-
batches from the experience memory D is to sam-
ple uniformly at random. However, certain ex-
periences are more valuable than others for the
agent to learn from. For instance, rare transitions
that provide positive rewards can be used more of-
ten to learn optimal Q-values faster. In our ex-
periments, we consider such positive-reward tran-
sitions to have higher priority and keep track of
them in D. We use prioritized sampling (inspired
by Moore and Atkeson (1993)) to sample a frac-
tion ρ of transitions from the higher priority pool
and a fraction 1 − ρ from the rest.
</bodyText>
<sectionHeader confidence="0.999213" genericHeader="method">
5 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999899333333333">
Game Environment For our game environ-
ment, we modify Evennia,5 an open-source library
for building online textual MUD games. Evennia
is a Python-based framework that allows one to
easily create new games by writing a batch file
describing the environment with details of rooms,
</bodyText>
<footnote confidence="0.963249">
5http://www.evennia.com/
</footnote>
<table confidence="0.999398">
Stats Home World Fantasy World
Vocabulary size 84 1340
Avg. words / description 10.5 65.21
Max descriptions / room 3 100
# diff. quest descriptions 12 -
State transitions Deterministic Stochastic
# states (underlying) 16 ≥ 56
Branching factor
(# commands / state) 40 222
</table>
<tableCaption confidence="0.999931">
Table 1: Various statistics of the two game worlds
</tableCaption>
<bodyText confidence="0.998902157894737">
objects and actions. The game engine keeps
track of the game state internally, presenting tex-
tual descriptions to the player and receiving text
commands from the player. We conduct exper-
iments on two worlds - a smaller Home world
we created ourselves, and a larger, more com-
plex Fantasy world created by Evennia’s develop-
ers. The motivation behind Home world is to ab-
stract away high-level planning and focus on the
language understanding requirements of the game.
Table 1 provides statistics of the game worlds.
We observe that the Fantasy world is moderately
sized with a vocabulary of 1340 words and up to
100 different descriptions for a room. These de-
scriptions were created manually by the game de-
velopers. These diverse, engaging descriptions are
designed to make it interesting and exciting for hu-
man players. Several rooms have many alternative
descriptions, invoked randomly on each visit by
</bodyText>
<page confidence="0.965134">
5
</page>
<bodyText confidence="0.998433125">
the player.
Comparatively, the Home world is smaller: it
has a very restricted vocabulary of 84 words and
the room descriptions are relatively structured.
However, both the room descriptions (which are
also varied and randomly provided to the agent)
and the quest descriptions were adversarially cre-
ated with negation and conjunction of facts to
force an agent to actually understand the state in
order to play well. Therefore, this domain pro-
vides an interesting challenge for language under-
standing.
In both worlds, the agent receives a positive
reward on completing a quest, and negative re-
wards for getting into bad situations like falling
off a bridge, or losing a battle. We also add
small deterministic negative rewards for each non-
terminating step. This incentivizes the agent to
learn policies that solve quests in fewer steps. The
supplementary material has details on the reward
structure.
Home World We created Home world to mimic
the environment of a typical house.6 The world
consists of four rooms - a living room, a bedroom,
a kitchen and a garden with connecting pathways.
Every room is reachable from every other room.
Each room contains a representative object that the
agent can interact with. For instance, the kitchen
has an apple that the player can eat. Transitions
between the rooms are deterministic. At the start
of each game episode, the player is placed in a ran-
dom room and provided with a randomly selected
quest. The text provided to the player contains
both the description of her current state and that
of the quest. Thus, the player can begin in one
of 16 different states (4 rooms × 4 quests), which
adds to the world’s complexity.
An example of a quest given to the player in
text is Not you are sleepy now but you are hun-
gry now. To complete this quest and obtain a re-
ward, the player has to navigate through the house
to reach the kitchen and eat the apple (i.e type in
the command eat apple). More importantly, the
player should interpret that the quest does not re-
quire her to take a nap in the bedroom. We cre-
ated such misguiding quests to make it hard for
agents to succeed without having an adequate level
of language understanding.
</bodyText>
<footnote confidence="0.724248">
6An illustration is provided in the supplementary material.
</footnote>
<bodyText confidence="0.999523826086957">
Fantasy World The Fantasy world is consider-
ably more complex and involves quests such as
navigating through a broken bridge or finding the
secret tomb of an ancient hero. This game also has
stochastic transitions in addition to varying state
descriptions provided to the player. For instance,
there is a possibility of the player falling from the
bridge if she lingers too long on it.
Due to the large command space in this game,7
we make use of cues provided by the game itself to
narrow down the set of possible objects to consider
in each state. For instance, in the MUD example in
Figure 1, the game provides a list of possible exits.
If the game does not provide such clues for the
current state, we consider all objects in the game.
Evaluation We use two metrics for measuring
an agent’s performance: (1) the cumulative reward
obtained per episode averaged over the episodes
and (2) the fraction of quests completed by the
agent. The evaluation procedure is as follows. In
each epoch, we first train the agent on M episodes
of T steps each. At the end of this training, we
have a testing phase of running M episodes of the
game for T steps. We use M = 50, T = 20 for the
Home world and M = 20, T = 250 for the Fan-
tasy world. For all evaluation episodes, we run the
agent following an c-greedy policy with c = 0.05,
which makes the agent choose the best action ac-
cording to its Q-values 95% of the time. We report
the agent’s performance at each epoch.
Baselines We compare our LSTM-DQN model
with three baselines. The first is a Random agent
that chooses both actions and objects uniformly at
random from all available choices.8 The other two
are BOW-DQN and BI-DQN, which use a bag-
of-words and a bag-of-bigrams representation of
the text, respectively, as input to the DQN action
scorer. These baselines serve to illustrate the im-
portance of having a good representation layer for
the task.
Settings For our DQN models, we used D =
100000, γ = 0.5. We use a learning rate of 0.0005
for RMSprop. We anneal the c for c-greedy from
1 to 0.2 over 100000 transitions. A mini-batch
gradient update is performed every 4 steps of the
gameplay. We roll out the LSTM (over words) for
</bodyText>
<footnote confidence="0.988175">
7We consider 222 possible command combinations of 6
actions and 37 object arguments.
8In the case of the Fantasy world, the object choices are
narrowed down using game clues as described earlier.
</footnote>
<page confidence="0.992969">
6
</page>
<figure confidence="0.999846444444444">
1.0
0.5
0.0
0.5
1.0
1.5
2.00 20 40 60 80 100
Epochs
Reward (Home)
Quest completion (Home)
1.2
LSTM-DQN
BI-DQN
BOW-DQN
Random
0.00 20 40 60 80 100
Epochs
Quest Completion
1.0
0.8
0.6
0.4
0.2
Reward
LSTM-DQN
BI-DQN
BOW-DQN
Random
No Transfer
Transfer
1.0
0.5
0.0
0.5
1.0
1.50 10 20 30 40 50
Epochs
Transfer Learning (Home)
Reward
Reward (log scale)
Quest Completion
1.5
2.0
2.5
3.0
3.5
4.0
4.5
5.0
5.50 10 20 30 40 50 60 70
Epochs
Reward (Fantasy)
1.2
1.0
LSTM-DQN
BI-DQN
BOW-DQN
Random
0.00 10 20 30 40 50 60 70
Epochs
Quest completion (Fantasy)
Reward
0.8
0.6
0.4
0.2
LSTM-DQN
BI-DQN
BOW-DQN
Random
1.0
0.5
0.0
0.5
1.0
1.5
2.00 20 40 60 80 100
Epochs
Uniform
Prioritized
Prioritized Sampling (Home)
</figure>
<figureCaption confidence="0.997215">
Figure 3: Left: Graphs showing the evolution of average reward and quest completion rate for BOW-
</figureCaption>
<bodyText confidence="0.979435625">
DQN, LSTM-DQN and a Random baseline on the Home world (top) and Fantasy world (bottom). Note
that the reward is shown in log scale for the Fantasy world. Right: Graphs showing effects of transfer
learning and prioritized sampling on the Home world.
a maximum of 30 steps on the Home world and for
100 steps on the Fantasy world. For the prioritized
sampling, we used p = 0.25 for both worlds. We
employed a mini-batch size of 64 and word em-
bedding size d = 20 in all experiments.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.992620294117647">
Home World Figure 3 illustrates the perfor-
mance of LSTM-DQN compared to the baselines.
We can observe that the Random baseline per-
forms quite poorly, completing only around 10%
of quests on average9 obtaining a low reward of
around −1.58. The BOW-DQN model performs
significantly better and is able to complete around
46% of the quests, with an average reward of 0.20.
The improvement in reward is due to both greater
quest success rate and a lower rate of issuing in-
valid commands (e.g. eat apple would be invalid
in the bedroom since there is no apple). We no-
tice that both the reward and quest completion
graphs of this model are volatile. This is because
the model fails to pick out differences between
quests like Not you are hungry now but you are
sleepy now and Not you are sleepy now but you
</bodyText>
<footnote confidence="0.777188">
9Averaged over the last 10 epochs.
</footnote>
<bodyText confidence="0.98253896">
are hungry now. The BI-DQN model suffers from
the same issue although it performs slightly bet-
ter than BOW-DQN by completing 48% of quests.
In contrast, the LSTM-DQN model does not suf-
fer from this issue and is able to complete 100%
of the quests after around 50 epochs of training,
achieving close to the optimal reward possible.10
This demonstrates that having an expressive rep-
resentation for text is crucial to understanding the
game states and choosing intelligent actions.
In addition, we also investigated the impact of
using a deep neural network for modeling the ac-
tion scorer OA. Figure 4 illustrates the perfor-
mance of the BOW-DQN and BI-DQN models
along with their simpler versions BOW-LIN and
BI-LIN, which use a single linear layer for OA. It
can be seen that the DQN models clearly achieve
better performance than their linear counterparts,
which points to them modeling the control policy
better.
Fantasy World We evaluate all the models on
the Fantasy world in the same manner as before
and report reward, quest completion rates and Q-
10Note that since each step incurs a penalty of −0.01, the
best reward (on average) a player can get is around 0.98.
</bodyText>
<page confidence="0.997453">
7
</page>
<figure confidence="0.999857722222222">
BI-DQN
BOW-DQN
BI-LIN
BOW-LIN
10 20 30 40 50 60
Epochs
0.10
0.7
0.6
Quest Completion
0.5
0.4
0.3
0.2
“Living room”
“Garden”
“Kitchen”
“Bedroom”
</figure>
<figureCaption confidence="0.9761765">
Figure 4: Quest completion rates of DQN vs. Lin-
ear models on Home world.
</figureCaption>
<bodyText confidence="0.999765828571428">
values. The quest we evaluate on involves crossing
the broken bridge (which takes a minimum of five
steps), with the possibility of falling off at random
(a 5% chance) when the player is on the bridge.
The game has an additional quest of reaching a
secret tomb. However, this is a complex quest that
requires the player to memorize game events and
perform high-level planning which are beyond the
scope of this current work. Therefore, we focus
only on the first quest.
From Figure 3 (bottom), we can see that the
Random baseline does poorly in terms of both av-
erage per-episode reward11 and quest completion
rates. BOW-DQN converges to a much higher av-
erage reward of −12.68 and achieves around 82%
quest completion. Again, the BOW-DQN is often
confused by varying (10 different) descriptions of
the portions of the bridge, which reflects in its er-
ratic performance on the quest. The BI-DQN per-
forms very well on quest completion by finishing
97% of quests. However, this model tends to find
sub-optimal solutions and gets an average reward
of −26.68, even worse than BOW-DQN. One rea-
son for this is the negative rewards the agent ob-
tains after falling off the bridge. The LSTM-DQN
model again performs best, achieving an average
reward of −11.33 and completing 96% of quests
on average. Though this world does not con-
tain descriptions adversarial to BOW-DQN or BI-
DQN, the LSTM-DQN obtains higher average re-
ward by completing the quest in fewer steps and
showing more resilience to variations in the state
descriptions.
Transfer Learning We would like the represen-
tations learnt by φR to be generic enough and
</bodyText>
<footnote confidence="0.423703">
11Note that the rewards graph is in log scale.
</footnote>
<figureCaption confidence="0.6834035">
Figure 5: t-SNE visualization of the word embed-
dings (except stopwords) after training on Home
world. The embedding values are initialized ran-
domly.
</figureCaption>
<bodyText confidence="0.9999153">
transferable to new game worlds. To test this,
we created a second Home world with the same
rooms, but a completely different map, changing
the locations of the rooms and the pathways be-
tween them. The main differentiating factor of
this world from the original home world lies in the
high-level planning required to complete quests.
We initialized the LSTM part of an LSTM-
DQN agent with parameters θR learnt from the
original home world and trained it on the new
world.12 Figure 3 (top right) demonstrates that
the agent with transferred parameters is able to
learn quicker than an agent starting from scratch
initialized with random parameters (No Transfer),
reaching the optimal policy almost 20 epochs ear-
lier. This indicates that these simulated worlds can
be used to learn good representations for language
that transfer across worlds.
Prioritized sampling We also investigate the ef-
fects of different minibatch sampling procedures
on the parameter learning. From Figure 3 (bottom
right), we observe that using prioritized sampling
significantly speeds up learning, with the agent
achieving the optimal policy around 50 epochs
faster than using uniform sampling. This shows
promise for further research into different schemes
of assigning priority to transitions.
Representation Analysis We analyzed the rep-
resentations learnt by the LSTM-DQN model on
the Home world. Figure 5 shows a visualization
</bodyText>
<footnote confidence="0.952618">
12The parameters for the Action Scorer (θA) are initialized
randomly.
</footnote>
<page confidence="0.996882">
8
</page>
<bodyText confidence="0.9851365">
Description Nearest neighbor
You are halfways out on the unstable bridge. From the castle The bridge slopes precariously where it extends westwards to-
you hear a distant howling sound, like that of a large dog or wards the lowest point - the center point of the hang bridge. You
other beast. clasp the ropes firmly as the bridge sways and creaks under you.
The ruins opens up to the sky in a small open area, lined by The old gatehouse is near collapse..... East the gatehouse leads
columns. ... To the west is the gatehouse and entrance to the out to a small open area surrounded by the remains of the cas-
castle, whereas southwards the columns make way for a wide tle. There is also a standing archway offering passage to a path
open courtyard. along the old southern inner wall.
Table 2: Sample descriptions from the Fantasy world and their nearest neighbors (NN) according to their
vector representations from the LSTM representation generator. The NNs are often descriptions of the
same or similar (nearby) states in the game.
of learnt word embeddings, reduced to two di-
mensions using t-SNE (Van der Maaten and Hin-
ton, 2008). All the vectors were initialized ran-
domly before training. We can see that semanti-
cally similar words appear close together to form
coherent subspaces. In fact, we observe four dif-
ferent subspaces, each for one type of room along
with its corresponding object(s) and quest words.
For instance, food items like pizza and rooms like
kitchen are very close to the word hungry which
appears in a quest description. This shows that
the agent learns to form meaningful associations
between the semantics of the quest and the envi-
ronment. Table 2 shows some examples of de-
scriptions from Fantasy world and their nearest
neighbors using cosine similarity between their
corresponding vector representations produced by
LSTM-DQN. The model is able to correlate de-
scriptions of the same (or similar) underlying
states and project them onto nearby points in the
representation subspace.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999836411764706">
We address the task of end-to-end learning of con-
trol policies for text-based games. In these games,
all interactions in the virtual world are through
text and the underlying state is not observed. The
resulting language variability makes such envi-
ronments challenging for automatic game play-
ers. We employ a deep reinforcement learning
framework to jointly learn state representations
and action policies using game rewards as feed-
back. This framework enables us to map text de-
scriptions into vector representations that capture
the semantics of the game states. Our experiments
demonstrate the importance of learning good rep-
resentations of text in order to play these games
well. Future directions include tackling high-level
planning and strategy learning to improve the per-
formance of intelligent agents.
</bodyText>
<sectionHeader confidence="0.996934" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999566">
We are grateful to the developers of Evennia, the
game framework upon which this work is based.
We also thank Nate Kushman, Clement Gehring,
Gustavo Goretkin, members of MIT’s NLP group
and the anonymous EMNLP reviewers for insight-
ful comments and feedback. T. Kulkarni was gra-
ciously supported by the Leventhal Fellowship.
We would also like to acknowledge MIT’s Cen-
ter for Brains, Minds and Machines (CBMM) for
support.
</bodyText>
<sectionHeader confidence="0.998514" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998743913043478">
Christopher Amato and Guy Shani. 2010. High-level
reinforcement learning in strategy games. In Pro-
ceedings of the 9th International Conference on Au-
tonomous Agents and Multiagent Systems: Volume
1, pages 75–82. International Foundation for Au-
tonomous Agents and Multiagent Systems.
Eyal Amir and Patrick Doyle. 2002. Adventure games:
A challenge for cognitive robotics. In Proc. Int.
Cognitive Robotics Workshop, pages 148–155.
Jacob Andreas and Dan Klein. 2015. Alignment-
based compositional semantics for instruction fol-
lowing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics, 1(1):49–62.
SRK Branavan, Luke S Zettlemoyer, and Regina Barzi-
lay. 2010. Reading between the lines: Learning to
map high-level instructions to commands. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1268–
1277. Association for Computational Linguistics.
</reference>
<page confidence="0.945445">
9
</page>
<reference confidence="0.999725229357799">
SRK Branavan, David Silver, and Regina Barzilay.
2011a. Learning to win by reading manuals in a
monte-carlo framework. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1, pages 268–277. Association for Compu-
tational Linguistics.
SRK Branavan, David Silver, and Regina Barzilay.
2011b. Non-linear monte-carlo search in Civiliza-
tion II. AAAI Press/International Joint Conferences
on Artificial Intelligence.
Pavel Curtis. 1992. Mudding: Social phenomena in
text-based virtual realities. High noon on the elec-
tronic frontier: Conceptual issues in cyberspace,
pages 347–374.
Mark A DePristo and Robert Zubek. 2001. being-in-
the-world. In Proceedings of the 2001 AAAI Spring
Symposium on Artificial Intelligence and Interactive
Entertainment, pages 31–34.
Jacob Eisenstein, James Clarke, Dan Goldwasser, and
Dan Roth. 2009. Reading to learn: Constructing
features from semantic abstracts. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 958–967, Singapore,
August. Association for Computational Linguistics.
Peter Gorniak and Deb Roy. 2005. Speaking with your
sidekick: Understanding situated speech in com-
puter role playing games. In R. Michael Young and
John E. Laird, editors, Proceedings of the First Arti-
ficial Intelligence and Interactive Digital Entertain-
ment Conference, June 1-5, 2005, Marina del Rey,
California, USA, pages 57–62. AAAI Press.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas
Roy. 2010. Toward understanding natural language
directions. In Human-Robot Interaction (HRI),
2010 5th ACM/IEEE International Conference on,
pages 259–266. IEEE.
Jan Koutnik, Giuseppe Cuccu, J¨urgen Schmidhuber,
and Faustino Gomez. 2013. Evolving large-
scale neural networks for vision-based reinforce-
ment learning. In Proceedings of the 15th annual
conference on Genetic and evolutionary computa-
tion, pages 1061–1068. ACM.
Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and
Regina Barzilay. 2014. Learning to automatically
solve algebra word problems. ACL (1), pages 271–
281.
Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer,
and Dieter Fox. 2013. Learning to parse natural
language commands to a robot control system. In
Experimental Robotics, pages 403–415. Springer.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A. Rusu, Joel Veness, Marc G. Bellemare,
Alex Graves, Martin Riedmiller, Andreas K. Fidje-
land, Georg Ostrovski, Stig Petersen, Charles Beat-
tie, Amir Sadik, Ioannis Antonoglou, Helen King,
Dharshan Kumaran, Daan Wierstra, Shane Legg,
and Demis Hassabis. 2015. Human-level con-
trol through deep reinforcement learning. Nature,
518(7540):529–533, 02.
Andrew W Moore and Christopher G Atkeson. 1993.
Prioritized sweeping: Reinforcement learning with
less data and less time. Machine Learning,
13(1):103–130.
Jeffrey Pennington, Richard Socher, and Christopher D
Manning. 2014. Glove: Global vectors for
word representation. Proceedings of the Empiricial
Methods in Natural Language Processing (EMNLP
2014), 12.
David Silver, Richard S Sutton, and Martin M¨uller.
2007. Reinforcement learning of local shape in the
game of go. In IJCAI, volume 7, pages 1053–1058.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.
Richard S Sutton and Andrew G Barto. 1998. Intro-
duction to reinforcement learning. MIT Press.
Istv´an Szita. 2012. Reinforcement learning in
games. In Reinforcement Learning, pages 539–577.
Springer.
Kai Sheng Tai, Richard Socher, and Christopher D.
Manning. 2015. Improved semantic representa-
tions from tree-structured long short-term memory
networks. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 1: Long Papers),
pages 1556–1566, Beijing, China, July. Association
for Computational Linguistics.
Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture
6.5-rmsprop: Divide the gradient by a running av-
erage of its recent magnitude. COURSERA: Neural
Networks for Machine Learning, 4.
Laurens Van der Maaten and Geoffrey Hinton. 2008.
Visualizing data using t-sne. Journal of Machine
Learning Research, 9(2579-2605):85.
Adam Vogel and Dan Jurafsky. 2010. Learning to fol-
low navigational directions. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 806–814. Association for
Computational Linguistics.
</reference>
<page confidence="0.952872">
10
</page>
<reference confidence="0.9254575">
Christopher JCH Watkins and Peter Dayan. 1992. Q-
learning. Machine learning, 8(3-4):279–292.
</reference>
<page confidence="0.999418">
11
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.688586">
<title confidence="0.9974465">Language Understanding for Text-based Games using Deep Reinforcement Learning</title>
<author confidence="0.958843">D Barzilay</author>
<affiliation confidence="0.956542">CSAIL, MIT CSAIL, BCS, MIT CSAIL,</affiliation>
<email confidence="0.999097">karthikn@csail.mit.edutejask@mit.eduregina@csail.mit.edu</email>
<abstract confidence="0.987672428571428">In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-ofwords and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher Amato</author>
<author>Guy Shani</author>
</authors>
<title>High-level reinforcement learning in strategy games.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems:</booktitle>
<volume>1</volume>
<pages>75--82</pages>
<contexts>
<context position="9280" citStr="Amato and Shani, 2010" startWordPosition="1487" endWordPosition="1490">me of day, etc. The function Ψ (also part of the game framework) then converts this state into a textual description of the location the player is at or a message indicating low health. We do not assume access to either H or Ψ for our agent during both training and testing phases of our experiments. We denote the space of all possible text descriptions s to be S. Rewards are generated using R and are only given to the player upon completion of in-game quests. Q-Learning Reinforcement Learning is a commonly used framework for learning control policies in game environments (Silver et al., 2007; Amato and Shani, 2010; Branavan et al., 2011b; Szita, 2012). The game environment can be formulated as a sequence of state transitions (s, a, r, s&apos;) of a Markov Decision Process (MDP). The agent takes an action a in state s by consulting a state-action value function Q(s, a), which is a measure of the action’s expected long-term reward. Q-Learning (Watkins and Dayan, 1992) is a model-free technique which is used to learn an optimal Q(s, a) for the agent. Starting from a random Q-function, the agent continuously updates its Q-values by playing the game and obtaining rewards. The iterative updates are derived from t</context>
</contexts>
<marker>Amato, Shani, 2010</marker>
<rawString>Christopher Amato and Guy Shani. 2010. High-level reinforcement learning in strategy games. In Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: Volume 1, pages 75–82. International Foundation for Autonomous Agents and Multiagent Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eyal Amir</author>
<author>Patrick Doyle</author>
</authors>
<title>Adventure games: A challenge for cognitive robotics.</title>
<date>2002</date>
<booktitle>In Proc. Int. Cognitive Robotics Workshop,</booktitle>
<pages>148--155</pages>
<contexts>
<context position="4608" citStr="Amir and Doyle, 2002" startWordPosition="727" endWordPosition="730">otes the long-term merit of an action a in state s. The action-value function is parametrized using a deep recurrent neural network, trained using the game feedback. The network contains two modules. The first one converts textual descriptions into vector representations that act as proxies for states. This component is implemented using Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997). The second module of the network scores the actions given the vector representation computed by the first. We evaluate our model using two Multi-User Dungeon (MUD) games (Curtis, 1992; Amir and Doyle, 2002). The first game is designed to provide a controlled setup for the task, while the second is a publicly available one and contains human generated text descriptions with significant language variability. We compare our algorithm against baselines of a random player and models that use bag-of-words or bag-of-bigrams representations for a state. We demonstrate that our model LSTM-DQN significantly outperforms the baselines in terms of number of completed quests and accumulated rewards. For instance, on a fantasy MUD game, our model learns to complete 96% of the quests, while the bag-of-words mod</context>
</contexts>
<marker>Amir, Doyle, 2002</marker>
<rawString>Eyal Amir and Patrick Doyle. 2002. Adventure games: A challenge for cognitive robotics. In Proc. Int. Cognitive Robotics Workshop, pages 148–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Dan Klein</author>
</authors>
<title>Alignmentbased compositional semantics for instruction following.</title>
<date>2015</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5785" citStr="Andreas and Klein, 2015" startWordPosition="914" endWordPosition="917">e 96% of the quests, while the bag-of-words model and a random baseline solve only 82% and 5% of the quests, respectively. Moreover, we show that the acquired representation can be reused across games, speeding up learning and leading to faster convergence of Q-values. 2 Related Work Learning control policies from text is gaining increasing interest in the NLP community. Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Andreas and Klein, 2015) and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a). Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotated with actions and state attributes. Eisenstein et al. (2009) learn game rules by analyzing a collection of game-related documents and precompiled traces</context>
</contexts>
<marker>Andreas, Klein, 2015</marker>
<rawString>Jacob Andreas and Dan Klein. 2015. Alignmentbased compositional semantics for instruction following. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="5736" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="906" endWordPosition="909">, on a fantasy MUD game, our model learns to complete 96% of the quests, while the bag-of-words model and a random baseline solve only 82% and 5% of the quests, respectively. Moreover, we show that the acquired representation can be reused across games, speeding up learning and leading to faster convergence of Q-values. 2 Related Work Learning control policies from text is gaining increasing interest in the NLP community. Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Andreas and Klein, 2015) and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a). Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotated with actions and state attributes. Eisenstein et al. (2009) learn game rules by analyzing a collection</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics, 1(1):49–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>Luke S Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reading between the lines: Learning to map high-level instructions to commands.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1268--1277</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="5632" citStr="Branavan et al., 2010" startWordPosition="890" endWordPosition="893">performs the baselines in terms of number of completed quests and accumulated rewards. For instance, on a fantasy MUD game, our model learns to complete 96% of the quests, while the bag-of-words model and a random baseline solve only 82% and 5% of the quests, respectively. Moreover, we show that the acquired representation can be reused across games, speeding up learning and leading to faster convergence of Q-values. 2 Related Work Learning control policies from text is gaining increasing interest in the NLP community. Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Andreas and Klein, 2015) and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a). Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotate</context>
</contexts>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>SRK Branavan, Luke S Zettlemoyer, and Regina Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1268– 1277. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SRK Branavan</author>
<author>David Silver</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning to win by reading manuals in a monte-carlo framework.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1,</booktitle>
<pages>268--277</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5860" citStr="Branavan et al., 2011" startWordPosition="926" endWordPosition="929">only 82% and 5% of the quests, respectively. Moreover, we show that the acquired representation can be reused across games, speeding up learning and leading to faster convergence of Q-values. 2 Related Work Learning control policies from text is gaining increasing interest in the NLP community. Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Andreas and Klein, 2015) and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a). Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotated with actions and state attributes. Eisenstein et al. (2009) learn game rules by analyzing a collection of game-related documents and precompiled traces of the game. In contrast to the above work, our model combines text interp</context>
<context position="9303" citStr="Branavan et al., 2011" startWordPosition="1491" endWordPosition="1494">ction Ψ (also part of the game framework) then converts this state into a textual description of the location the player is at or a message indicating low health. We do not assume access to either H or Ψ for our agent during both training and testing phases of our experiments. We denote the space of all possible text descriptions s to be S. Rewards are generated using R and are only given to the player upon completion of in-game quests. Q-Learning Reinforcement Learning is a commonly used framework for learning control policies in game environments (Silver et al., 2007; Amato and Shani, 2010; Branavan et al., 2011b; Szita, 2012). The game environment can be formulated as a sequence of state transitions (s, a, r, s&apos;) of a Markov Decision Process (MDP). The agent takes an action a in state s by consulting a state-action value function Q(s, a), which is a measure of the action’s expected long-term reward. Q-Learning (Watkins and Dayan, 1992) is a model-free technique which is used to learn an optimal Q(s, a) for the agent. Starting from a random Q-function, the agent continuously updates its Q-values by playing the game and obtaining rewards. The iterative updates are derived from the Bellman equation (Su</context>
<context position="11067" citStr="Branavan et al., 2011" startWordPosition="1791" endWordPosition="1794">. Deep Q-Network In large games, it is often impractical to maintain the Q-value for all possible Figure 2: Architecture of LSTM-DQN: The Representation Generator (φR) (bottom) takes as input a stream of words observed in state s and produces a vector representation vs, which is fed into the action scorer (φA) (top) to produce scores for all actions and argument objects. state-action pairs. One solution to this problem is to approximate Q(s, a) using a parametrized function Q(s, a; θ), which can generalize over states and actions by considering higher-level attributes (Sutton and Barto, 1998; Branavan et al., 2011a). However, creating a good parametrization requires knowledge of the state and action spaces. One way to bypass this feature engineering is to use a Deep Q-Network (DQN) (Mnih et al., 2015). The DQN approximates the Q-value function with a deep neural network to predict Q(s, a) for all possible actions a simultaneously given the current state s. The non-linear function layers of the DQN also enable it to learn better value functions than linear approximators. 4 Learning Representations and Control Policies In this section, we describe our model (DQN) and describe its use in learning good Q-v</context>
</contexts>
<marker>Branavan, Silver, Barzilay, 2011</marker>
<rawString>SRK Branavan, David Silver, and Regina Barzilay. 2011a. Learning to win by reading manuals in a monte-carlo framework. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language TechnologiesVolume 1, pages 268–277. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>SRK Branavan</author>
<author>David Silver</author>
<author>Regina Barzilay</author>
</authors>
<booktitle>2011b. Non-linear monte-carlo search in Civilization II. AAAI Press/International Joint Conferences on Artificial Intelligence.</booktitle>
<marker>Branavan, Silver, Barzilay, </marker>
<rawString>SRK Branavan, David Silver, and Regina Barzilay. 2011b. Non-linear monte-carlo search in Civilization II. AAAI Press/International Joint Conferences on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Curtis</author>
</authors>
<title>Mudding: Social phenomena in text-based virtual realities. High noon on the electronic frontier: Conceptual issues in cyberspace,</title>
<date>1992</date>
<pages>347--374</pages>
<contexts>
<context position="4585" citStr="Curtis, 1992" startWordPosition="725" endWordPosition="726">, a) which denotes the long-term merit of an action a in state s. The action-value function is parametrized using a deep recurrent neural network, trained using the game feedback. The network contains two modules. The first one converts textual descriptions into vector representations that act as proxies for states. This component is implemented using Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997). The second module of the network scores the actions given the vector representation computed by the first. We evaluate our model using two Multi-User Dungeon (MUD) games (Curtis, 1992; Amir and Doyle, 2002). The first game is designed to provide a controlled setup for the task, while the second is a publicly available one and contains human generated text descriptions with significant language variability. We compare our algorithm against baselines of a random player and models that use bag-of-words or bag-of-bigrams representations for a state. We demonstrate that our model LSTM-DQN significantly outperforms the baselines in terms of number of completed quests and accumulated rewards. For instance, on a fantasy MUD game, our model learns to complete 96% of the quests, whi</context>
</contexts>
<marker>Curtis, 1992</marker>
<rawString>Pavel Curtis. 1992. Mudding: Social phenomena in text-based virtual realities. High noon on the electronic frontier: Conceptual issues in cyberspace, pages 347–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A DePristo</author>
<author>Robert Zubek</author>
</authors>
<title>being-inthe-world.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 AAAI Spring Symposium on Artificial Intelligence and Interactive Entertainment,</booktitle>
<pages>31--34</pages>
<contexts>
<context position="2082" citStr="DePristo and Zubek, 2001" startWordPosition="311" endWordPosition="314">respond with natural language commands to take actions. Since the underlying state is not directly observable, the player has to understand the text in order to act, making it ∗Both authors contributed equally to this work. 1Code is available at http://people.csail.mit. edu/karthikn/mud-play. 2http://mudstats.com/ Figure 1: Sample gameplay from a Fantasy World. The player with the quest of finding a secret tomb, is currently located on an old bridge. She then chooses an action to go east that brings her to a ruined gatehouse (State 2). challenging for existing AI programs to play these games (DePristo and Zubek, 2001). In designing an autonomous game player, we have considerable latitude when selecting an adequate state representation to use. The simplest method is to use a bag-of-words representation derived from the text description. However, this scheme disregards the ordering of words and the finer nuances of meaning that evolve from composing words into sentences and paragraphs. For instance, in State 2 in Figure 1, the agent has to understand that going east will lead it to the castle whereas moving south will take it to the standing archway. An alternative approach is to convert text descriptions to</context>
</contexts>
<marker>DePristo, Zubek, 2001</marker>
<rawString>Mark A DePristo and Robert Zubek. 2001. being-inthe-world. In Proceedings of the 2001 AAAI Spring Symposium on Artificial Intelligence and Interactive Entertainment, pages 31–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Eisenstein</author>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Dan Roth</author>
</authors>
<title>Reading to learn: Constructing features from semantic abstracts.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>958--967</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5837" citStr="Eisenstein et al., 2009" startWordPosition="922" endWordPosition="925"> a random baseline solve only 82% and 5% of the quests, respectively. Moreover, we show that the acquired representation can be reused across games, speeding up learning and leading to faster convergence of Q-values. 2 Related Work Learning control policies from text is gaining increasing interest in the NLP community. Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Andreas and Klein, 2015) and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a). Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotated with actions and state attributes. Eisenstein et al. (2009) learn game rules by analyzing a collection of game-related documents and precompiled traces of the game. In contrast to the above work, our mod</context>
</contexts>
<marker>Eisenstein, Clarke, Goldwasser, Roth, 2009</marker>
<rawString>Jacob Eisenstein, James Clarke, Dan Goldwasser, and Dan Roth. 2009. Reading to learn: Constructing features from semantic abstracts. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 958–967, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gorniak</author>
<author>Deb Roy</author>
</authors>
<title>Speaking with your sidekick: Understanding situated speech in computer role playing games. In</title>
<date>2005</date>
<booktitle>Proceedings of the First Artificial Intelligence and Interactive Digital Entertainment Conference,</booktitle>
<pages>57--62</pages>
<editor>R. Michael Young and John E. Laird, editors,</editor>
<publisher>AAAI Press.</publisher>
<location>Marina</location>
<contexts>
<context position="6042" citStr="Gorniak and Roy (2005)" startWordPosition="956" endWordPosition="959"> Q-values. 2 Related Work Learning control policies from text is gaining increasing interest in the NLP community. Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Andreas and Klein, 2015) and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a). Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotated with actions and state attributes. Eisenstein et al. (2009) learn game rules by analyzing a collection of game-related documents and precompiled traces of the game. In contrast to the above work, our model combines text interpretation and strategy learning in a single framework. As a result, textual analysis is guided by the received control feedback, and the learned strategy directly builds on the text i</context>
</contexts>
<marker>Gorniak, Roy, 2005</marker>
<rawString>Peter Gorniak and Deb Roy. 2005. Speaking with your sidekick: Understanding situated speech in computer role playing games. In R. Michael Young and John E. Laird, editors, Proceedings of the First Artificial Intelligence and Interactive Digital Entertainment Conference, June 1-5, 2005, Marina del Rey, California, USA, pages 57–62. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="4400" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="694" endWordPosition="697">rocesses. An agent playing the game aims to maximize rewards that it obtains from the game engine upon the occurrence of certain events. The agent learns a policy in the form of an action-value function Q(s, a) which denotes the long-term merit of an action a in state s. The action-value function is parametrized using a deep recurrent neural network, trained using the game feedback. The network contains two modules. The first one converts textual descriptions into vector representations that act as proxies for states. This component is implemented using Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997). The second module of the network scores the actions given the vector representation computed by the first. We evaluate our model using two Multi-User Dungeon (MUD) games (Curtis, 1992; Amir and Doyle, 2002). The first game is designed to provide a controlled setup for the task, while the second is a publicly available one and contains human generated text descriptions with significant language variability. We compare our algorithm against baselines of a random player and models that use bag-of-words or bag-of-bigrams representations for a state. We demonstrate that our model LSTM-DQN signifi</context>
<context position="12807" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="2075" endWordPosition="2078">2 w3 wn Linear LSTM Mean Pooling Linear ReLU LSTM Linear LSTM OA OR 3 Representation Generator (φR) The representation generator reads raw text displayed to the agent and converts it to a vector representation vs. A bag-of-words (BOW) representation is not sufficient to capture higher-order structures of sentences and paragraphs. The need for a better semantic representation of the text is evident from the average performance of this representation in playing MUD-games (as we show in Section 6). In order to assimilate better representations, we utilize a Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997) as a representation generator. LSTMs are recurrent neural networks with the ability to connect and recognize long-range patterns between words in text. They are more robust than BOW to small variations in word usage and are able to capture underlying semantics of sentences to some extent. In recent work, LSTMs have been used successfully in NLP tasks such as machine translation (Sutskever et al., 2014) and sentiment analysis (Tai et al., 2015) to compose vector representations of sentences from word-level embeddings (Mikolov et al., 2013; Pennington et al., 2014). In our setup, the LSTM netwo</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Kollar</author>
<author>Stefanie Tellex</author>
<author>Deb Roy</author>
<author>Nicholas Roy</author>
</authors>
<title>Toward understanding natural language directions.</title>
<date>2010</date>
<booktitle>In Human-Robot Interaction (HRI), 2010 5th ACM/IEEE International Conference on,</booktitle>
<pages>259--266</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="5707" citStr="Kollar et al., 2010" startWordPosition="902" endWordPosition="905">rewards. For instance, on a fantasy MUD game, our model learns to complete 96% of the quests, while the bag-of-words model and a random baseline solve only 82% and 5% of the quests, respectively. Moreover, we show that the acquired representation can be reused across games, speeding up learning and leading to faster convergence of Q-values. 2 Related Work Learning control policies from text is gaining increasing interest in the NLP community. Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Andreas and Klein, 2015) and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a). Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotated with actions and state attributes. Eisenstein et al. (2009) learn game ru</context>
</contexts>
<marker>Kollar, Tellex, Roy, Roy, 2010</marker>
<rawString>Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas Roy. 2010. Toward understanding natural language directions. In Human-Robot Interaction (HRI), 2010 5th ACM/IEEE International Conference on, pages 259–266. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Koutnik</author>
<author>Giuseppe Cuccu</author>
<author>J¨urgen Schmidhuber</author>
<author>Faustino Gomez</author>
</authors>
<title>Evolving largescale neural networks for vision-based reinforcement learning.</title>
<date>2013</date>
<booktitle>In Proceedings of the 15th annual conference on Genetic and evolutionary computation,</booktitle>
<pages>1061--1068</pages>
<publisher>ACM.</publisher>
<marker>Koutnik, Cuccu, Schmidhuber, Gomez, 2013</marker>
<rawString>Jan Koutnik, Giuseppe Cuccu, J¨urgen Schmidhuber, and Faustino Gomez. 2013. Evolving largescale neural networks for vision-based reinforcement learning. In Proceedings of the 15th annual conference on Genetic and evolutionary computation, pages 1061–1068. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nate Kushman</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Learning to automatically solve algebra word problems.</title>
<date>2014</date>
<journal>ACL</journal>
<volume>1</volume>
<pages>271--281</pages>
<contexts>
<context position="3577" citStr="Kushman et al., 2014" startWordPosition="558" endWordPosition="561">d by the remains of the castle. There is also a standing archway offering passage to a path along the old southern inner wall. Exits: Standing archway, castle corner, Bridge over the abyss State 1: The old bridge You are standing very close to the bridge’s eastern foundation. If you go east you will be back on solid ground ... The bridge sways in the wind. 1 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1–11, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. language grounding tasks (Matuszek et al., 2013; Kushman et al., 2014). In contrast, our goal is to learn useful representations in conjunction with control policies. We adopt a reinforcement learning framework and formulate game sequences as Markov Decision Processes. An agent playing the game aims to maximize rewards that it obtains from the game engine upon the occurrence of certain events. The agent learns a policy in the form of an action-value function Q(s, a) which denotes the long-term merit of an action a in state s. The action-value function is parametrized using a deep recurrent neural network, trained using the game feedback. The network contains two</context>
</contexts>
<marker>Kushman, Artzi, Zettlemoyer, Barzilay, 2014</marker>
<rawString>Nate Kushman, Yoav Artzi, Luke Zettlemoyer, and Regina Barzilay. 2014. Learning to automatically solve algebra word problems. ACL (1), pages 271– 281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Evan Herbst</author>
<author>Luke Zettlemoyer</author>
<author>Dieter Fox</author>
</authors>
<title>Learning to parse natural language commands to a robot control system.</title>
<date>2013</date>
<booktitle>In Experimental Robotics,</booktitle>
<pages>403--415</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="3554" citStr="Matuszek et al., 2013" startWordPosition="554" endWordPosition="557">all open area surrounded by the remains of the castle. There is also a standing archway offering passage to a path along the old southern inner wall. Exits: Standing archway, castle corner, Bridge over the abyss State 1: The old bridge You are standing very close to the bridge’s eastern foundation. If you go east you will be back on solid ground ... The bridge sways in the wind. 1 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1–11, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. language grounding tasks (Matuszek et al., 2013; Kushman et al., 2014). In contrast, our goal is to learn useful representations in conjunction with control policies. We adopt a reinforcement learning framework and formulate game sequences as Markov Decision Processes. An agent playing the game aims to maximize rewards that it obtains from the game engine upon the occurrence of certain events. The agent learns a policy in the form of an action-value function Q(s, a) which denotes the long-term merit of an action a in state s. The action-value function is parametrized using a deep recurrent neural network, trained using the game feedback. T</context>
<context position="5759" citStr="Matuszek et al., 2013" startWordPosition="910" endWordPosition="913">model learns to complete 96% of the quests, while the bag-of-words model and a random baseline solve only 82% and 5% of the quests, respectively. Moreover, we show that the acquired representation can be reused across games, speeding up learning and leading to faster convergence of Q-values. 2 Related Work Learning control policies from text is gaining increasing interest in the NLP community. Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Andreas and Klein, 2015) and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a). Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotated with actions and state attributes. Eisenstein et al. (2009) learn game rules by analyzing a collection of game-related docume</context>
</contexts>
<marker>Matuszek, Herbst, Zettlemoyer, Fox, 2013</marker>
<rawString>Cynthia Matuszek, Evan Herbst, Luke Zettlemoyer, and Dieter Fox. 2013. Learning to parse natural language commands to a robot control system. In Experimental Robotics, pages 403–415. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="13351" citStr="Mikolov et al., 2013" startWordPosition="2166" endWordPosition="2169">e a Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997) as a representation generator. LSTMs are recurrent neural networks with the ability to connect and recognize long-range patterns between words in text. They are more robust than BOW to small variations in word usage and are able to capture underlying semantics of sentences to some extent. In recent work, LSTMs have been used successfully in NLP tasks such as machine translation (Sutskever et al., 2014) and sentiment analysis (Tai et al., 2015) to compose vector representations of sentences from word-level embeddings (Mikolov et al., 2013; Pennington et al., 2014). In our setup, the LSTM network takes in word embeddings wk from the words in a description s and produces output vectors xk at each step. To get the final state representation vs, we add a mean pooling layer which computes the elementwise mean over the output vectors xk.3 vs = 1 n xk (2) n k=1 Action Scorer (φA) The action scorer module produces scores for the set of possible actions given the current state representation. We use a multi-layered neural network for this purpose (see Figure 2). The input to this module is the vector from the representation generator, </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Volodymyr Mnih</author>
<author>Koray Kavukcuoglu</author>
<author>David Silver</author>
<author>Andrei A Rusu</author>
<author>Joel Veness</author>
<author>Marc G Bellemare</author>
<author>Alex Graves</author>
<author>Martin Riedmiller</author>
<author>Andreas K Fidjeland</author>
</authors>
<title>Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.</title>
<date>2015</date>
<journal>Nature,</journal>
<volume>518</volume>
<issue>7540</issue>
<pages>02</pages>
<contexts>
<context position="7471" citStr="Mnih et al., 2015" startWordPosition="1177" endWordPosition="1180">rategies are learned jointly using feedback provided by the game simulation. In their setup, states are fully observable, and the model learns a strategy by combining state/action features and features extracted from text. However, in our application, the state representation is not provided, but has to be inferred from a textual description. Therefore, it is not sufficient to extract features from text to supplement a simulation-based player. Another related line of work consists of automatic video game players that infer state representations directly from raw pixels (Koutn´ık et al., 2013; Mnih et al., 2015). For instance, Mnih et al. (2015) learn control strategies using convolutional neural networks, trained with a variant of Q-learning (Watkins and Dayan, 1992). While both approaches use deep reinforcement learning for training, our work has important differences. In order to handle the sequential nature of text, we use Long Short-Term Memory networks to automatically learn useful representations for arbitrary text descriptions. Additionally, we show that decomposing the network into a representation layer and an action selector is useful for transferring the learnt representations to new game</context>
<context position="11258" citStr="Mnih et al., 2015" startWordPosition="1822" endWordPosition="1825">stream of words observed in state s and produces a vector representation vs, which is fed into the action scorer (φA) (top) to produce scores for all actions and argument objects. state-action pairs. One solution to this problem is to approximate Q(s, a) using a parametrized function Q(s, a; θ), which can generalize over states and actions by considering higher-level attributes (Sutton and Barto, 1998; Branavan et al., 2011a). However, creating a good parametrization requires knowledge of the state and action spaces. One way to bypass this feature engineering is to use a Deep Q-Network (DQN) (Mnih et al., 2015). The DQN approximates the Q-value function with a deep neural network to predict Q(s, a) for all possible actions a simultaneously given the current state s. The non-linear function layers of the DQN also enable it to learn better value functions than linear approximators. 4 Learning Representations and Control Policies In this section, we describe our model (DQN) and describe its use in learning good Q-value approximations for games with stochastic textual descriptions. We divide our model into two parts. The first module is a representation generator that converts the textual description of</context>
<context position="16204" citStr="Mnih et al., 2015" startWordPosition="2667" endWordPosition="2670">Algorithm 1. In each iteration i, we update the parameters to reduce the discrepancy between the predicted value of the current state Q(st, at; θi) (where θi = [θR; θA]i) and the expected Q-value given the reward rt and the value of the next state maxa Q(st+1, a; θi−1). We keep track of the agent’s previous experiences in a memory D.4 Instead of performing updates to the Q-value using transitions from the current episode, we sample a random transition (ˆs, ˆa, s&apos;, r) from D. Updating the parameters in this way avoids issues due to strong correlation when using transitions of the same episode (Mnih et al., 2015). Using the sampled transition and (1), we obtain the following loss function to minimize: Li(θi) = Eˆs,ˆa[(yi − Q(ˆs, ˆa; θi))2] (3) where yi = Eˆs,ˆa[r + γ maxa&apos; Q(s&apos;, a&apos;; θi−1) |ˆs, ˆa] is the target Q-value with parameters θi−1 fixed from the previous iteration. The updates on the parameters θ can be performed( using the following gradient of Li (θi): VθiLi(Bi) = Eˆs,ˆa[2(yi − Q(ˆs, ˆa; θi))VθiQ(ˆs, ˆa; θi)] For each epoch of training, the agent plays several episodes of the game, which is restarted after every terminal state. 4The memory is limited and rewritten in a first-in-first-out (F</context>
</contexts>
<marker>Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, 2015</marker>
<rawString>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. 2015. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew W Moore</author>
<author>Christopher G Atkeson</author>
</authors>
<title>Prioritized sweeping: Reinforcement learning with less data and less time.</title>
<date>1993</date>
<booktitle>Machine Learning,</booktitle>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="18503" citStr="Moore and Atkeson (1993)" startWordPosition="3082" endWordPosition="3085">tion. This increases the number of experiences used per step and is also more efficient due to optimized matrix operations. The simplest method to create these minibatches from the experience memory D is to sample uniformly at random. However, certain experiences are more valuable than others for the agent to learn from. For instance, rare transitions that provide positive rewards can be used more often to learn optimal Q-values faster. In our experiments, we consider such positive-reward transitions to have higher priority and keep track of them in D. We use prioritized sampling (inspired by Moore and Atkeson (1993)) to sample a fraction ρ of transitions from the higher priority pool and a fraction 1 − ρ from the rest. 5 Experimental Setup Game Environment For our game environment, we modify Evennia,5 an open-source library for building online textual MUD games. Evennia is a Python-based framework that allows one to easily create new games by writing a batch file describing the environment with details of rooms, 5http://www.evennia.com/ Stats Home World Fantasy World Vocabulary size 84 1340 Avg. words / description 10.5 65.21 Max descriptions / room 3 100 # diff. quest descriptions 12 - State transitions</context>
</contexts>
<marker>Moore, Atkeson, 1993</marker>
<rawString>Andrew W Moore and Christopher G Atkeson. 1993. Prioritized sweeping: Reinforcement learning with less data and less time. Machine Learning, 13(1):103–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014),</booktitle>
<pages>12</pages>
<contexts>
<context position="13377" citStr="Pennington et al., 2014" startWordPosition="2170" endWordPosition="2173">mory network (LSTM) (Hochreiter and Schmidhuber, 1997) as a representation generator. LSTMs are recurrent neural networks with the ability to connect and recognize long-range patterns between words in text. They are more robust than BOW to small variations in word usage and are able to capture underlying semantics of sentences to some extent. In recent work, LSTMs have been used successfully in NLP tasks such as machine translation (Sutskever et al., 2014) and sentiment analysis (Tai et al., 2015) to compose vector representations of sentences from word-level embeddings (Mikolov et al., 2013; Pennington et al., 2014). In our setup, the LSTM network takes in word embeddings wk from the words in a description s and produces output vectors xk at each step. To get the final state representation vs, we add a mean pooling layer which computes the elementwise mean over the output vectors xk.3 vs = 1 n xk (2) n k=1 Action Scorer (φA) The action scorer module produces scores for the set of possible actions given the current state representation. We use a multi-layered neural network for this purpose (see Figure 2). The input to this module is the vector from the representation generator, vs = φR(s) and the outputs</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014), 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Silver</author>
<author>Richard S Sutton</author>
<author>Martin M¨uller</author>
</authors>
<title>Reinforcement learning of local shape in the game of go.</title>
<date>2007</date>
<booktitle>In IJCAI,</booktitle>
<volume>7</volume>
<pages>1053--1058</pages>
<marker>Silver, Sutton, M¨uller, 2007</marker>
<rawString>David Silver, Richard S Sutton, and Martin M¨uller. 2007. Reinforcement learning of local shape in the game of go. In IJCAI, volume 7, pages 1053–1058.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="13213" citStr="Sutskever et al., 2014" startWordPosition="2143" endWordPosition="2146">e performance of this representation in playing MUD-games (as we show in Section 6). In order to assimilate better representations, we utilize a Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997) as a representation generator. LSTMs are recurrent neural networks with the ability to connect and recognize long-range patterns between words in text. They are more robust than BOW to small variations in word usage and are able to capture underlying semantics of sentences to some extent. In recent work, LSTMs have been used successfully in NLP tasks such as machine translation (Sutskever et al., 2014) and sentiment analysis (Tai et al., 2015) to compose vector representations of sentences from word-level embeddings (Mikolov et al., 2013; Pennington et al., 2014). In our setup, the LSTM network takes in word embeddings wk from the words in a description s and produces output vectors xk at each step. To get the final state representation vs, we add a mean pooling layer which computes the elementwise mean over the output vectors xk.3 vs = 1 n xk (2) n k=1 Action Scorer (φA) The action scorer module produces scores for the set of possible actions given the current state representation. We use </context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard S Sutton</author>
<author>Andrew G Barto</author>
</authors>
<title>Introduction to reinforcement learning.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9924" citStr="Sutton and Barto, 1998" startWordPosition="1598" endWordPosition="1601">11b; Szita, 2012). The game environment can be formulated as a sequence of state transitions (s, a, r, s&apos;) of a Markov Decision Process (MDP). The agent takes an action a in state s by consulting a state-action value function Q(s, a), which is a measure of the action’s expected long-term reward. Q-Learning (Watkins and Dayan, 1992) is a model-free technique which is used to learn an optimal Q(s, a) for the agent. Starting from a random Q-function, the agent continuously updates its Q-values by playing the game and obtaining rewards. The iterative updates are derived from the Bellman equation (Sutton and Barto, 1998): Qi+1(s, a) = E[r + -y max Qi(s&apos;, a&apos;) |s, a] (1) a where -y is a discount factor for future rewards and the expectation is over all game transitions that involved the agent taking action a in state s. Using these evolving Q-values, the agent chooses the action with the highest Q(s, a) to maximize its expected future rewards. In practice, the trade-off between exploration and exploitation can be achieved following an E-greedy policy (Sutton and Barto, 1998), where the agent performs a random action with probability E. Deep Q-Network In large games, it is often impractical to maintain the Q-val</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>Richard S Sutton and Andrew G Barto. 1998. Introduction to reinforcement learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Istv´an Szita</author>
</authors>
<title>Reinforcement learning in games.</title>
<date>2012</date>
<booktitle>In Reinforcement Learning,</booktitle>
<pages>539--577</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="9318" citStr="Szita, 2012" startWordPosition="1495" endWordPosition="1496">e game framework) then converts this state into a textual description of the location the player is at or a message indicating low health. We do not assume access to either H or Ψ for our agent during both training and testing phases of our experiments. We denote the space of all possible text descriptions s to be S. Rewards are generated using R and are only given to the player upon completion of in-game quests. Q-Learning Reinforcement Learning is a commonly used framework for learning control policies in game environments (Silver et al., 2007; Amato and Shani, 2010; Branavan et al., 2011b; Szita, 2012). The game environment can be formulated as a sequence of state transitions (s, a, r, s&apos;) of a Markov Decision Process (MDP). The agent takes an action a in state s by consulting a state-action value function Q(s, a), which is a measure of the action’s expected long-term reward. Q-Learning (Watkins and Dayan, 1992) is a model-free technique which is used to learn an optimal Q(s, a) for the agent. Starting from a random Q-function, the agent continuously updates its Q-values by playing the game and obtaining rewards. The iterative updates are derived from the Bellman equation (Sutton and Barto,</context>
</contexts>
<marker>Szita, 2012</marker>
<rawString>Istv´an Szita. 2012. Reinforcement learning in games. In Reinforcement Learning, pages 539–577. Springer.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),</booktitle>
<pages>1556--1566</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="13255" citStr="Tai et al., 2015" startWordPosition="2151" endWordPosition="2154">MUD-games (as we show in Section 6). In order to assimilate better representations, we utilize a Long Short-Term Memory network (LSTM) (Hochreiter and Schmidhuber, 1997) as a representation generator. LSTMs are recurrent neural networks with the ability to connect and recognize long-range patterns between words in text. They are more robust than BOW to small variations in word usage and are able to capture underlying semantics of sentences to some extent. In recent work, LSTMs have been used successfully in NLP tasks such as machine translation (Sutskever et al., 2014) and sentiment analysis (Tai et al., 2015) to compose vector representations of sentences from word-level embeddings (Mikolov et al., 2013; Pennington et al., 2014). In our setup, the LSTM network takes in word embeddings wk from the words in a description s and produces output vectors xk at each step. To get the final state representation vs, we add a mean pooling layer which computes the elementwise mean over the output vectors xk.3 vs = 1 n xk (2) n k=1 Action Scorer (φA) The action scorer module produces scores for the set of possible actions given the current state representation. We use a multi-layered neural network for this pu</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory networks. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1556–1566, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tijmen Tieleman</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.</title>
<date>2012</date>
<booktitle>COURSERA: Neural Networks for Machine Learning,</booktitle>
<volume>4</volume>
<contexts>
<context position="15540" citStr="Tieleman and Hinton, 2012" startWordPosition="2549" endWordPosition="2552">class of commands that require two arguments (e.g. move red-root right, move blue-root up). We consider all possible actions and objects available in the game and predict both for each state using the same network (Figure 2). We consider the Q-value of the entire command (a, o) to be the average of the Qvalues of the action a and the object o. For the rest of this section, we only show equations for Q(s, a) but similar ones hold for Q(s, o). Parameter Learning We learn the parameters θR of the representation generator and θA of the action scorer using stochastic gradient descent with RMSprop (Tieleman and Hinton, 2012). The complete training procedure is shown in Algorithm 1. In each iteration i, we update the parameters to reduce the discrepancy between the predicted value of the current state Q(st, at; θi) (where θi = [θR; θA]i) and the expected Q-value given the reward rt and the value of the next state maxa Q(st+1, a; θi−1). We keep track of the agent’s previous experiences in a memory D.4 Instead of performing updates to the Q-value using transitions from the current episode, we sample a random transition (ˆs, ˆa, s&apos;, r) from D. Updating the parameters in this way avoids issues due to strong correlatio</context>
</contexts>
<marker>Tieleman, Hinton, 2012</marker>
<rawString>Tijmen Tieleman and Geoffrey Hinton. 2012. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens Van der Maaten</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Visualizing data using t-sne.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--2579</pages>
<marker>Van der Maaten, Hinton, 2008</marker>
<rawString>Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Dan Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>806--814</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5686" citStr="Vogel and Jurafsky, 2010" startWordPosition="898" endWordPosition="901">ed quests and accumulated rewards. For instance, on a fantasy MUD game, our model learns to complete 96% of the quests, while the bag-of-words model and a random baseline solve only 82% and 5% of the quests, respectively. Moreover, we show that the acquired representation can be reused across games, speeding up learning and leading to faster convergence of Q-values. 2 Related Work Learning control policies from text is gaining increasing interest in the NLP community. Example applications include interpreting help documentation for software (Branavan et al., 2010), navigating with directions (Vogel and Jurafsky, 2010; Kollar et al., 2010; Artzi and Zettlemoyer, 2013; Matuszek et al., 2013; Andreas and Klein, 2015) and playing computer games (Eisenstein et al., 2009; Branavan et al., 2011a). Games provide a rich domain for grounded language analysis. Prior work has assumed perfect knowledge of the underlying state of the game to learn policies. Gorniak and Roy (2005) developed a game character that can be controlled by spoken instructions adaptable to the game situation. The grounding of commands to actions is learned from a transcript manually annotated with actions and state attributes. Eisenstein et al.</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Dan Jurafsky. 2010. Learning to follow navigational directions. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 806–814. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher JCH Watkins</author>
<author>Peter Dayan</author>
</authors>
<date>1992</date>
<booktitle>Qlearning. Machine learning,</booktitle>
<pages>8--3</pages>
<contexts>
<context position="7630" citStr="Watkins and Dayan, 1992" startWordPosition="1201" endWordPosition="1204"> by combining state/action features and features extracted from text. However, in our application, the state representation is not provided, but has to be inferred from a textual description. Therefore, it is not sufficient to extract features from text to supplement a simulation-based player. Another related line of work consists of automatic video game players that infer state representations directly from raw pixels (Koutn´ık et al., 2013; Mnih et al., 2015). For instance, Mnih et al. (2015) learn control strategies using convolutional neural networks, trained with a variant of Q-learning (Watkins and Dayan, 1992). While both approaches use deep reinforcement learning for training, our work has important differences. In order to handle the sequential nature of text, we use Long Short-Term Memory networks to automatically learn useful representations for arbitrary text descriptions. Additionally, we show that decomposing the network into a representation layer and an action selector is useful for transferring the learnt representations to new game scenarios. 3 Background Game Representation We represent a game by the tuple (H, A, T, R, Ψ), where H is the set of 2 all possible game states, A = {(a, o)} i</context>
<context position="9634" citStr="Watkins and Dayan, 1992" startWordPosition="1549" endWordPosition="1552">ons s to be S. Rewards are generated using R and are only given to the player upon completion of in-game quests. Q-Learning Reinforcement Learning is a commonly used framework for learning control policies in game environments (Silver et al., 2007; Amato and Shani, 2010; Branavan et al., 2011b; Szita, 2012). The game environment can be formulated as a sequence of state transitions (s, a, r, s&apos;) of a Markov Decision Process (MDP). The agent takes an action a in state s by consulting a state-action value function Q(s, a), which is a measure of the action’s expected long-term reward. Q-Learning (Watkins and Dayan, 1992) is a model-free technique which is used to learn an optimal Q(s, a) for the agent. Starting from a random Q-function, the agent continuously updates its Q-values by playing the game and obtaining rewards. The iterative updates are derived from the Bellman equation (Sutton and Barto, 1998): Qi+1(s, a) = E[r + -y max Qi(s&apos;, a&apos;) |s, a] (1) a where -y is a discount factor for future rewards and the expectation is over all game transitions that involved the agent taking action a in state s. Using these evolving Q-values, the agent chooses the action with the highest Q(s, a) to maximize its expecte</context>
</contexts>
<marker>Watkins, Dayan, 1992</marker>
<rawString>Christopher JCH Watkins and Peter Dayan. 1992. Qlearning. Machine learning, 8(3-4):279–292.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
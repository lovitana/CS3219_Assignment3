<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.999035">
Extracting Condition-Opinion Relations Toward
Fine-grained Opinion Mining
</title>
<author confidence="0.999376">
Yuki Nakayama Atsushi Fujii
</author>
<affiliation confidence="0.999503">
Department of Computer Science
Graduate School of Information Science and Engineering
Tokyo Institute of Technology
</affiliation>
<email confidence="0.94996">
{nakayama.y.aj@m,fujii@cs}.titech.ac.jp
</email>
<sectionHeader confidence="0.996677" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999155">
A fundamental issue in opinion mining is
to search a corpus for opinion units, each
of which typically comprises the evalua-
tion by an author for a target object from
an aspect, such as “This hotel is in a
good location”. However, few attempts
have been made to address cases where
the validity of an evaluation is restricted
on a condition in the source text, such
as “for traveling with small kids”. In
this paper, we propose a method to ex-
tract condition-opinion relations from on-
line reviews, which enables fine-grained
analysis for the utility of target objects de-
pending the user attribute, purpose, and
situation. Our method uses supervised
machine learning to identify sequences of
words or phrases that comprise conditions
for opinions. We propose several features
associated with lexical and syntactic infor-
mation, and show their effectiveness ex-
perimentally.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995474928571428">
Reflecting the rapid growth in the use of opin-
ionated texts on the Web, such as comments on
news articles and customer reviews, opinion min-
ing has been explored to facilitate utilizing opin-
ions mainly for improving products and decision-
making purposes. While in a broad sense opin-
ion mining refers to a process to discover useful
knowledge latent in a corpus of opinionated texts,
fundamental issues involve modeling an unit of
opinions and searching the corpus for those units,
each of which typically comprises the evaluation
by an author for a target object from an aspect.
Other elements, such as when the opinion was sub-
mitted, can optionally be included in an opinion
unit. We take the following review sentence as an
example opinionated description.
(1) I think hotel A offers a reasonable price if you
take a family trip with small kids.
From the above example, existing methods (Pang
and Lee, 2008; Seki et al., 2009; Jin et al., 2009;
Zhao et al., 2010; He et al., 2011; Liu and Zhang,
2012; Liu et al., 2013; Yang and Cardie, 2013; Liu
et al., 2014) are intended to extract the following
quintuple as an opinion unit.
Target = “hotel A”, Aspect = “price”,
Evaluation (Polarity) = “reasonable”
(positive), Holder = “I (author)”, Time
= N/A
Depending on the application, “Evaluation” can be
any of a literal opinion word (e.g., “reasonable”),
a polarity (positive/negative), or a value for multi-
point scale rating.
Given those standardized units extracted from
a corpus, it is feasible to overview the distribu-
tion of values for each element or a combination
of elements. For example, those who intend to im-
prove the quality of hotel A may investigate repre-
sentative values for “Aspect” in the units satisfy-
ing “Target=hotel A &amp; Polarity=negative”, while
those who look for accommodation may collect
the opinion units for one or more candidate ho-
tels and investigate the distribution of values for
“Polarity” on an aspect-by-aspect basis.
However, in the above example (1), the evalua-
tion for hotel A (“a reasonable price”) is valid for
“if you take a family trip with small kids”, and thus
it is not clear whether this evaluation is valid irre-
spective of the condition. For example, the price
may not be reasonable for a single customer in-
tending for business purposes. In this paper, we
shall call such a condition “condition for opinion
(CFO)”. We define CFO as a condition for which
an opinion unit has a polarity.
The existing methods for opinion mining, which
do not consider whether a target opinion is con-
ditional, potentially overestimate or underestimate
</bodyText>
<page confidence="0.966043">
622
</page>
<note confidence="0.985171">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 622–631,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999310023809524">
the utility of hotel A and consequently decrease
the quality of opinion mining. We manually an-
alyzed the first 7 000 sentences in the Rakuten
Travel data, which consists of 348 564 Japanese
reviews for hotels in Japan (see Section 4 for de-
tails of this data) and found that 2 272 sentences
are opinions, of which 630 opinions are condi-
tional and thus the result for an existing method
includes up to 28% (630/2272) errors.
Motivated by the above discussion, in this pa-
per we propose a method to extract pairs of a CFO
and its corresponding opinion unit from online re-
views. This method provides two solutions to the
above problem. First, a passive solution is detect-
ing whether an opinion includes a CFO and, if any,
isolating that opinion from the target of opinion
mining. As a result, we can avoid potential errors
as much as possible but the coverage is decreased.
Second, an active solution is identifying the
span of each CFO in conditional opinions and
classify them according to semantic categories,
such as purpose, situation, and user attribute so
that finer-grained opinion mining can be realized.
For example, the distribution of positive and neg-
ative opinions can be available on a category-by-
category basis. However, in this paper we focus
only on the identification for CFOs and leave the
semantic classification future work.
To produce a practical model for CFOs, it is im-
portant to investigate them from a grammar point
of view. It can easily be predicted that a typical
grammatical unit for CFOs is a conditional clause
as in example (1). Additionally, restrictive mod-
ifiers in general can potentially be CFOs because
they restrict the validity of an opinion unit from a
specific perspective. A restrictive modifier com-
prises a word, phrase, or clause. The CFO in ex-
ample (1), which is a dependent clause functioning
as a condition, is also a restrictive modifier.
Example (2), which has the same meaning as
example (1), includes a CFO as a prepositional
phrase.
</bodyText>
<listItem confidence="0.896699">
(2) Hotel A offers a reasonable price for taking
a family trip with small kids.
</listItem>
<bodyText confidence="0.899874166666667">
We denote CFOs and opinion words in bold and
italic faces, respectively. Examples (3) and (4)
also include a CFO as a prepositional phrase. Un-
like example (2), the validity of “reasonable” is re-
stricted from time and comparison points of view,
respectively.
</bodyText>
<listItem confidence="0.9690345">
(3) Hotel A offers a reasonable price during this
holiday season.
(4) Hotel A offers a reasonable price for a four
star hotel.
</listItem>
<bodyText confidence="0.640834">
In example (5), which has a similar meaning to
example (1), the CFO is a dependent clause func-
tioning as a reason.
</bodyText>
<listItem confidence="0.982979">
(5) Hotel A offers a reasonable price because we
take a family trip with small kids.
</listItem>
<bodyText confidence="0.613338666666667">
Finally, as in example (6), an opinion holder can
also be a CFO because the evaluation is restricted
from a perspective of that specific person.
</bodyText>
<listItem confidence="0.8237395">
(6) My mother regarded hotel A as a reasonable
choice.
</listItem>
<bodyText confidence="0.996744823529412">
If the restriction by a CFO is associated with a
user-related perspective, we call such CFOs “user-
restrictive CFOs (U-CFOs)”. In other words, tar-
get users to whom an opinion unit is relevant are
restricted by its corresponding U-CFO, although
those users may agree or disagree with the opin-
ion. The CFOs in examples (1), (2), and (5) are
U-CFOs because the target users are mainly those
who intend to travel with their children.
The CFO in example (3) is also U-CFO be-
cause the target users are those who intend to
travel during a specific holiday season. The CFO
in example (6) is also U-CFO because the opin-
ion holder (“my mother”) implies the opinion is
relevant mainly to adult females. However, opin-
ion holders who do not represent user-related per-
spectives, such as “I” without any profile, are not
U-CFOs.
The CFO in example (4) is not a U-CFO be-
cause the relevance of the opinion is not restricted
to specific customers. It may be argued that in ex-
ample (4) the target users are restricted to those
who are interested in the price. However, in exam-
ple (4) the price restricts the aspect of the opinion
unit, and should not be confused with U-CFOs and
even CFOs, which restrict the validity of the opin-
ion unit.
If we fully utilize U-CFOs, as discussed for the
active solution above, we need to classify U-CFOs
into semantic categories so that users can selec-
tively read relevant opinions. In other words, the
identification for U-CFOs facilitates predicting the
review helpfulness (O’Mahony and Smyth, 2010;
Moghaddam et al., 2012). Candidate categories
</bodyText>
<page confidence="0.998775">
623
</page>
<bodyText confidence="0.9999834">
include demographic and psychographic attributes
for target users (e.g., age and hobby) and situations
of target users (e.g., purpose, time, and place).
However, we leave the classification for U-CFOs
future work.
</bodyText>
<sectionHeader confidence="0.999704" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999967230769231">
As described in Section 1, the fundamental meth-
ods for opinion mining include opinion extraction,
which identifies elements for opinion units (i.e.,
target, aspect, evaluation, holder, and time) (He
et al., 2011; Jin et al., 2009; Liu et al., 2013;
Seki et al., 2009; Yang and Cardie, 2013; Zhao
et al., 2010), and opinion classification, which de-
termines the non-literal evaluation of each opinion
unit based on bipolar categories (i.e., positive and
negative) (He et al., 2011; Meng et al., 2012) or
multipoint scale categories (Fu and Wang, 2010;
Moghaddam and Ester, 2013). However, none of
these methods intends to determine whether or not
an opinion is conditional and to extract their con-
dition.
Narayanan et al. (2009) proposed a method for
sentiment classification targeting conditional sen-
tences. Although a conditional opinion is a kind
of conditional sentence, their research is funda-
mentally different from our research. Narayanan
et al. (2009) targeted such a conditional sentence
that comprises a single opinion as a whole, and
intended to categorize its polarity into any of posi-
tive, negative, or neutral. Examples (7) and (8) are
such conditional sentences associated with neutral
and positive categories, respectively.
</bodyText>
<listItem confidence="0.98168525">
(7) Hotel A would not have survived if the price
was not reasonable.
(8) If you are looking for a hotel with a reason-
able price, stay at hotel A.
</listItem>
<bodyText confidence="0.997401891891892">
In example (7), although the subordinate clause in-
cludes the opinion word “reasonable”, none of the
subordinate clause, main clause, or entire sentence
is an opinion. In example (8), the entire sentence
is an unconditional opinion about the price for ho-
tel A, but the main and subordinate clauses are not
opinions independently. In contrast, the purpose
of our research is to identify conditional opinions,
in which the main and subordinate clauses are an
opinion and its condition, respectively.
Kim and Hovy (2006) proposed a method to
identify a reason for the evaluation in an opinion,
such as “the service was terrible because the staff
was rude”. Although as discussed in Section 1
reasons can be CFOs, their purpose is to identify
grounds that justify the evaluation and thus is dif-
ferent from our purpose.
As discussed in Section 1, our research
is related to predicting the review helpful-
ness (O’Mahony and Smyth, 2010; Moghaddam
et al., 2012). The method proposed by O’Mahony
and Smyth (2010) determines the helpfulness of a
product review independent of the user profile and
thus cannot recommend reviews based on user-
related attributes.
Moghaddam et al. (2012) used collaborative fil-
tering to predict the review helpfulness. The eval-
uation by a target user for past reviews is used
to model the user and predict the helpfulness for
unread reviews, which results in different predic-
tions depending on the user. An advantage of
collaborative filtering is its applicability to items
whose content is usually difficult to analyze, such
as videos. However, this advantage is diluted in
recommending review text, from which effective
features for user modeling, such as U-CFOs, can
be obtained by opinion mining.
</bodyText>
<sectionHeader confidence="0.994398" genericHeader="method">
3 Proposed method
</sectionHeader>
<bodyText confidence="0.999918833333333">
The task in this paper is to extract condition-
opinion relations from reviews in Japanese. Cur-
rently, we assume that an opinion unit and its cor-
responding CFO are in the same sentence, and thus
perform the extraction on a sentence-by-sentence
basis. Given a sentence in reviews, we first search
for an opinion unit, and if found, we also search
for its corresponding CFO. Because in the first
process we rely on an existing method for the
opinion extraction, in this paper we focus only on
the extraction for CFOs.
As discussed in Section 1, because CFOs can be
different grammatical units, their length and struc-
ture are not standardized. We model the extraction
for CFOs as the BIO chunking, which labels each
token in a sentence as being the beginning (B), in-
side (I), or outside (O) of a span of interest. We
use “Other” to refer to “O” to avoid confusion be-
tween “O” and “0” (zero). To subdivide “B” and
“I” into U-CFOs and other CFOs, we use suffixes
“U” and “C”, respectively, such as “BU” denoting
the beginning of a U-CFO. We use “Cond” to refer
to any of BU, IU, BC, or IC.
Because we use the same method for both U-
</bodyText>
<page confidence="0.994866">
624
</page>
<bodyText confidence="0.996154839285715">
CFOs and other CFOs, the above distinction only
increases the number of categories to which each
token is classified. If the distinction of U-CFOs is
not important, the above suffixes can be omitted.
We regard Japanese bunsetsu phrases, which
consist of a content word and one or more postpo-
sitional particles, as tokens, and extract a sequence
of a BU-phrase and one or more IU-phrases, or
an independent BU-phrase as a condition. The
same method is used for BC/IC-phrases. However,
words and phrases in an opinion unit are classified
into its corresponding element. For example, an
aspect phrase is classified into the aspect category.
Given an input sequence of bunsetsu phrases,
x = x1 ... xn, our task is to predict a se-
quence of labels, y = y1 ... yn, where yi E
{BU, IU, BC, IC, Other, Target, Aspect,
OpinionWord}. However, because an opinion
unit in an input sentence has been identified in
advance, the task is a quinary classification with
respect to yi E {BU, IU, BC, IC, Other}. We
use Conditional Random Fields (CRF) (Lafferty
et al., 2001) to train a classifier for categorizing
each bunsetsu phrase into any of the aforemen-
tioned five categories. We use a combination of
unigram and bigram models and calculate the con-
ditional probability, p(y|x), for linear-chain CRF
by Equation (1).
Here, Z,, denotes a normalization factor, and fk
and gk denote feature functions for unigram and
bigram models, respectively. Let xi,v denote a
feature value for xi. While in the unigram model
yi depends on either xi−1,v or xi,v, in the bigram
model yi depends on either a combination of xi,v
and yi−1 or that of xi−1,v and yi−1. Feature func-
tions are produced for any possible combinations
of the values for the variables used (xi,v, yi−1, and
yi in fk), and take 1 if the corresponding combi-
nation appears and 0 otherwise. We use the four
combinations “unigram xi,v”, “unigram xi−1,v”,
“bigram yi−1 xi,v”, and “bigram yi−1 xi−1,v” for
feature functions.
The question here is how CFOs and U-CFOs
can be modeled and what kind of features are
needed. We assume characteristics of CFOs, and
U-CFOs and partially exemplify their validity us-
ing Figure 1, which depicts an example input sen-
tence and information related to its constituent
bunsetsu phrases. In the upper part of Figure 1, a
rectangle and an arrow denote a bunsetstu phrase
and a syntactic dependency between two phrases,
respectively, and in each phrase we show Japanese
words based on the Hepburn system and their En-
glish translations in parentheses.
CFOs are associated with the following charac-
teristics.
</bodyText>
<listItem confidence="0.981906666666667">
(a) By definition, CFOs determine the validity of
the evaluation in an opinion unit, and thus
syntactically modify an opinion word. Con-
sequently, CFOs usually do not modify other
elements in an opinion unit, such as an as-
pect.
(b) Like a conjunction in a conditional clause in
English, such as “if”, a CFO in Japanese also
includes a clue expression, which is usually
a functional expression (Matsuyoshi et al.,
2006) in the tail phrase, such as “ni wa (“for”
in English)”.
(c) The distribution for parts of speech as the
head of CFOs is skewed and heads of CFOs
are usually a noun or verb.
</listItem>
<bodyText confidence="0.718518">
Additionally, U-CFOs are associated with the
following characteristics.
</bodyText>
<listItem confidence="0.904536857142857">
(d) If a CFO is an opinion holder as in example
(6) in Section 1, it is usually a U-CFO, which
is the subject appearing at the beginning of a
target sentence.
(e) By definition, U-CFOs include expressions
related to user attributes, such as “nervosity”
in Figure 1.
</listItem>
<bodyText confidence="0.999119076923077">
In this paper, we propose thirteen features to
model CFOs and U-CFOs. In the bottom part of
Figure 1, for each phrase we show the values of the
thirteen features F1–F13 described below. These
features were developed for the above five charac-
teristics. F1–F5, F7–F10 and F13 are associated
with (a), (b) and (c), respectively, while F6 and
F11–F12 are associated with (d) and (e).
F1: Dependency distance to opinion word
CFOs, which affect the evaluation in that opinion,
usually syntactically modifies the opinion word.
Thus, there should be a pass of dependencies be-
tween a Cond-phrase and the opinion word, and a
</bodyText>
<equation confidence="0.992700833333333">
∑ �k·gk(yi−1, yi, x) ) (1)
i,k
1
p(y|x) =
Z exp(∑ Ak· fk(yi, x)+
i. ,k
</equation>
<page confidence="0.988363">
625
</page>
<table confidence="0.998890846153846">
F1 1 2 aspect 3 3 2 1 1 opinion word -1
F2 8 7 aspect 5 4 1 2 1 opinion word -1
F3 1 0 aspect 1 1 1 1 1 opinion word 1
F4 2 1 aspect -1 -1 -1 -1 -1 opinion word -1
F5 7 5 aspect 2 1 1 1 0 opinion word 0
F6 0 0 aspect 0 0 0 0 0 opinion word 0
F7 nothing nothing aspect nothing nothing nothing ni wa nothing opinion word nothing
F8 nothing nothing aspect nothing nothing nothing topic nothing opinion word nothing
F9 nothing nothing aspect ni wa ni wa ni wa ni wa nothing opinion word nothing
F10 nothing nothing aspect topic topic topic topic nothing opinion word nothing
F11 nothing densha aspect shinkeishitsu nothing nothing kata nothing opinion word nothing
F12 0 1 aspect 1 0 0 1 0 opinion word 0
F13 conjunction noun aspect noun noun verb noun adverb opinion word verb
</table>
<figureCaption confidence="0.998814">
Figure 1: Example of Japanese sentence and the feature value for each constituent bunsetsu phrase
</figureCaption>
<figure confidence="0.565425">
0 shikashi,
(However,)
</figure>
<bodyText confidence="0.997680782608696">
phrase that leads to the opinion word via a smaller
number of dependency arrows is more likely to
be a Cond-phrase. We use the dependency dis-
tance (i.e., the number of dependencies) between
a phrase in question and the opinion word as the
value for feature F1. The value for a phrase is
−1 if there is no pass between that phrase and the
opinion word. We use “CaboCha” (Kudo and Mat-
sumoto, 2002) for dependency analysis purposes.
F2: Phrase distance to opinion word F1 is not
robust against errors of the dependency analysis.
To alleviate this problem, we approximate the de-
pendency distance by a phrase distance. In prac-
tice, we subtract the ID for a phrase in question
from that for the opinion word as the value for fea-
ture F2. If the opinion word consists of more than
one phrase, we take the minimum difference. Be-
cause in Japanese a modifier is usually followed
by its modifying object, a phrase with a negative
value for feature F2 is usually an Other-phrase.
For example, in the last phrase in Figure 1, which
cannot be a modifier for the opinion word, is an
Other-phrase.
</bodyText>
<listItem confidence="0.93576975862069">
F3: Dependency pass to aspect Because a CFO
rarely modifies an aspect, for the value of feature
F3 we take 0 if there is a pass of dependencies
between a phrase in question and an aspect and 1
otherwise.
F4: Phrase distance to aspect Similar to F1,
F3 is not robust against errors of the dependency
analysis. As in F2, we approximate the value of F4
by a phrase distance between a phrase including an
aspect and a phrase in question.
F5: Difference between values for F2 and
F1 A CFO usually consists of a sequence of
Cond-phrases where each phrase modifies the next
phrase, as in Figure 1. There is a tendency that as
the difference of values of F1 and F2 for a phrase
becomes smaller, that phrase is more likely to be
a Cond-phrase. In Figure 1, the values for Cond-
phrases #3–#6 are smaller than those for Other-
phrases #0–#1.
F6: Beginning of sentence The subject of an
opinion sentence is often its U-CFO because the
evaluation is valid only from the perspective of
that specific subject. For example, in “my daugh-
ter was pleased with toys in the room” the positive
evaluation is restricted by the daughter’s perspec-
tive. Thus, the value of feature F6 takes 1 for the
first phrase in a sentence excluding a conjunction,
and 0 otherwise.
F7: Clue expression Because a CFO often ends
</listItem>
<bodyText confidence="0.9780662">
with one or more specific particles or auxiliary
verbs, we use the existence of those clue expres-
sions in a phrase as the value for feature F7. We
use words in a dictionary of Japanese functional
expressions “Tsutsuji” (Matsuyoshi et al., 2006)
</bodyText>
<page confidence="0.996067">
626
</page>
<bodyText confidence="0.986260632653061">
as the clue expressions. Table 1 shows exam-
ples of entries for Tsutsuji. Each entry is repre-
sented in a hierarchy structure with nine abstrac-
tion levels. We firstly collected “Head words” in
the nineteen categories (e.g., resultative condition
and purpose in L2) associated with our purpose,
consulting “Meaning categories”. Then we col-
lected “Surface forms” corresponding to the col-
lected head words and identified their correspond-
ing surface forms to standardize different forms.
For example, for ID 1 and ID 3 in Table 1, “to sure
ba” and “nde” are regarded as identical to “to suru
to” and “node”, respectively. As a result, we col-
lected 388 words, such as “ba (if)” and “ni (for)”
and used their existence in a phrase in question as
value for F7.
F8: Semantic categories for clue expression
Because the data sparseness is a crucial problem
for F7, we use the existence of semantic categories
in Tsutsuji as the values of F8 for smoothing pur-
poses. For example, in Table 1, “to suru to” and
“ba” have the same feature values “resultative con-
dition”. If a clue expression belongs to more than
one semantic category as in “ni” of Table 1, the
feature value is a set of these categories.
F9: Dependency pass to phrase including clue
expression (Surface form) As described in F7
above, the last phrase in a CFO often includes one
or more clue expressions. In addition, a CFO often
consists of more than one phrase. Given those con-
ditions, a phrase that modifies a phrase contain-
ing a clue expression is also likely to be a Cond-
phrase. We use the existence of a dependency pass
between a phrase in question and a phrase contain-
ing a clue expression as the values of feature F9.
F10: Dependency pass to phrase including clue
expression (Category) As with F8, we use the
existence of semantic categories of Tsutsuji as the
values of feature F10.
F11: Restrictive words We use the existence
of words that are strongly associated with U-CFO
as the value for F11. We call such words re-
strictive words. We automatically produced a dic-
tionary of restrictive words from advertising slo-
gans for hotels, which often include descriptions
for target users, such as “*joshikai ya kappuru ni
osusume!!* (Recommended to girls get-together
and couples)”. First, we extracted words in the ad-
vertising slogan based on the following steps.
</bodyText>
<table confidence="0.9978698">
Entry Abstraction levels
ID
L1: L2: ... L9:
Head word Meaning categories Surface forms
1 to suru to resultative ... to sure ba
condition ...
2 ba ba
3 node reason ... nde
4 ni purpose ... ni
5 target ...
</table>
<tableCaption confidence="0.64174">
Table 1: Example entries for Tsutsuji
Step 1: Extracting sentences that match to a
regular expression “( I hito I mono I kata) ni ( I wa
 |mo) osusume” (i.e., “recommended to” or “rec-
ommend to those who”).
</tableCaption>
<bodyText confidence="0.9900924">
Step 2: Collecting a sequence of content words
for each bunsetsu-phrase in the extracted sen-
tences.
For the above advertising slogan, we can col-
lect two restrictive words “joshikai (girls get-
together)” and “kappuru (couple)” by performing
those 2 steps.
Second, we collected a sequence of independent
words for bunsetsu phrases which comprises U-
CFO in an annotated corpus. We combined the ex-
tracted words from the advertising slogans an an-
notated corpora, discarded redundancy, and stan-
dardized similar words, such as “kanko suru (do
sightseeing) and “kanko (sightseeing)”. As a re-
sult, we collected 934 words.
Finally, we calculated a mutual information like
score, 5core(r, u), between a restrictive word
r and labels u, Cond-phrases for U-CFOs (i.e.,
phrases labeled with either of BU or IU), by Equa-
tion 2.
</bodyText>
<equation confidence="0.992477333333333">
5core(r, u) = P(r, u) log P(r,
)u (2)
P(r)P(u)
</equation>
<bodyText confidence="0.995303357142857">
P(r, u) denotes the probability that a phrase in-
cluding r is labeled with BU or IU in the annotated
corpus. P(r) denotes the probability that a phrase
including r appears in the annotated corpus while
P(u) denotes the probability that a phrase labeled
with BU or IU in the annotated corpus. If a phrase
includes a restrictive word r and 5core(r, u) is
greater than threshold 0, the feature value is r, and
“nothing” otherwise.
F12: Existence of restrictive word Because the
data sparseness is a crucial problem for F11, we
integrate all the restrictive words for F11 into a
single category for smoothing purposes. The value
for F12 is the existence (1/0) of restrictive words.
</bodyText>
<page confidence="0.996068">
627
</page>
<bodyText confidence="0.9421345">
F13: Part of speech for head The likelihood
that a phrase in question is a Cond-phrases par-
tially depends on the part of speech for the head
in that phrase. For example, in Figure 1, a phrase
whose head is a noun or verb tends to be a Cond-
phrase
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.990667574468085">
To evaluate the effectiveness of our method, we
used the Rakuten Travel data1, which consists
of 348 564 Japanese reviews for hotels in Japan.
From this dataset, we selected 580 reviews and
manually identified elements for opinion units.
We removed sentences consisting only of opin-
ion unit such as “The location is good” from the
evaluation. As a result, 3155 sentences remained,
which comprise our corpus. To evaluate the effec-
tiveness for identifying CFOs, we used the man-
ually annotated opinion elements as output of a
pseudo automatic method.
Given the above corpus, two annotators inde-
pendently identified U-CFOs or CFOs, if any,
for each opinion unit. For both annotations of
CFOs and U-CFOs, the Kappa value for the inter-
annotator agreement was 0.87, indicating strong
agreement. We show the details of our corpus in
Table 2. Using this corpus, we performed 10-fold
cross-validation and compared different methods
from different perspectives. Also, we determined
the threshold for Score (see Eq 2) by a develop-
ment set for each fold.
To evaluate the effectiveness of extracting U-
CFOs and CFOs independently, we first classified
bunsetsu phrases into any of BU, IU, BC, IC, or
Other. Then, for the U-CFO extraction we re-
garded phrases for BU and IU as the Cond-phrases
while for the CFO extraction we regarded phrases
for BU, IU, BC, and IC as the Cond-phrases.
We used “Partial match” and “Exact match”,
which denote different criteria for the correctness
of methods under evaluation. While in the partial
match each method was requested to only detect
whether or not a test sentence includes CFO, in
the exact match each method was also requested to
identify the span of each CFO. Also, we used dif-
ferent evaluation measures, namely precision (P),
recall (R), F-measure (F), and accuracy (A).
Rule-based method and SVM-based method
are used for comparison purposes. Rule-based
&apos;https://alaginrc.nict.go.jp/resources/rakuten-
dataset/rakuten-outline.html
method first identifies a bunsetsu phrase whose de-
pendency distance to the opinion word is 1 and in-
cluding a clue expression (see Section 3), and also
identifies a sequence of the phrases from which
there is a dependency path to the above phrase as a
CFO. For example, in Figure 1 because phrase #6
includes a clue expression, a sequence of phrases
#3–#6 is extracted as a CFO. These rules are based
on features F1, F7 and F9. For the U-CFO extrac-
tion task, we regarded a sequence of Cond-phrases
extracted by the above method as U-CFO if that
sequence includes a restrictive word. For SVM,
the thirteen features F1–F13 proposed in Section 3
was used. We used LIBSVM (Chang and Lin,
2011) to train a classifier. Our method used CRF to
train a classifier with the thirteen features and four
patterns for feature functions. We used CRF++2
to train a classifier for each phrase and regularized
the parameters using L2-norm.
Figure 2 shows the relationship between val-
ues of regularization parameter and F-measure for
exact match. In Figure 2, “Rule”, “SVM”, and
“CRF” denote a rule-based method, SVM-based
method, and our method, respectively. The F-
measure for Rule, independent of the regulariza-
tion parameter, is a constant. While the F-measure
for SVM substantially varied depending on the pa-
rameter value, that for CRF did not vary that much.
Additionally, the F-measure for CRF was larger
than that for SVM irrespective of the parameter
value and matching criterion.
Table 3 shows results obtained with the optimal
value for the regularization parameter. Looking at
Table 3, one can see that CRF outperformed the
other methods in terms of F-measure and accu-
racy for both partial and exact matches. We used
the two-tailed paired t-test for statistical testing
and found that the differences of CRF and each of
the other methods in F-measure and accuracy were
statistically significant at the 1% level irrespective
of the configuration.
Figure 3 shows the effectiveness of the pro-
posed features for exact match. The horizontal
axis “w/o X” denotes a method without feature X.
The vertical axis denotes a ratio of each method to
our method. If a method without feature X takes
less than 1 for value of vertical axis, the feature X
is effective for extracting CFOs. Looking at Fig-
ure 3, one can see that our complete method out-
performed any variation of our method in terms of
zhttp://crfpp.googlecode.com/svn/trunk/doc/index.html
</bodyText>
<page confidence="0.992034">
628
</page>
<table confidence="0.9993610625">
Opinion sentence # w/ CFO 799
# w/ U-CFO 526
# w/o CFO 1257
# Non opinion sentence 1099
# Total 3155
(a) Sentence unit
# BU-phrase 571
# IU-phrase 741
# BC-phrase 307
# IC-phrase 632
# Other-phrase 16 584
Opinion # Opinion word 3 764
# Aspect 3 406
unit
# Target 132
# Total 26137
</table>
<figure confidence="0.955997096774193">
(b) Phrase unit
Figure 2: Relationship between values for regularization parameter and F-measure in exact match
Table 2: Details of our corpus
CRF
SVM
Rule
0.01 0.1 1 10 100 1000
Regularization parameter
(a) CFO extraction
0.01 0.1 1 10 100 1000
Regularization parameter
(b) U-CFO extraction
0.7
F-measure
0.6
0.5
0.4
0.3
0.2
0.1
0
F-measure
0.5
0.4
0.3
0.2
0.1
0
Rule
SVM
CRF
</figure>
<table confidence="0.991038090909091">
Partial match Exact match
P R F A P R F A
Rule .576 .790 .664 .771 .294 .319 .300 .649
SVM .779 .842 .809 .885 .490 .500 .488 .789
CRF .889 .758 .818 .915 .592 .580 .583 .865
(a) CFO extraction
Partial match Exact match
P R F A P R F A
Rule .395 .692 .502 .740 .174 .187 .176 .663
SVM .622 .703 .659 .863 .319 .323 .315 .800
CRF .818 .643 .720 .917 .472 .462 .464 .892
</table>
<tableCaption confidence="0.688329">
(b) U-CFO extraction
Table 3: Results for different configurations (P: precision, R: recall, F: F-measure, A: accuracy)
</tableCaption>
<table confidence="0.871002857142857">
Ratio to method with 1 Ratio to method with 1
all features in F-m easu re 0.98 all features in F-measure 0.96
0.96 0.92
0.94 0.88
0.92 0.84
0.9 0.8
0.88
</table>
<figure confidence="0.786245">
1 2 3 4 5 6 7 8 9 10 11 12 13 1 2 3 4 5 6 7 8 9 10 11 12 13
Methods w/o any of proposed features Methods w/o any of proposed features
(a) CFO extraction (b) U-CFO extraction
</figure>
<figureCaption confidence="0.999076">
Figure 3: Effectiveness of proposed features for exact match
</figureCaption>
<page confidence="0.996265">
629
</page>
<bodyText confidence="0.999910181818182">
F-measure. Thus, we conclude that each of our
thirteen features was independently effective for
extracting CFO and U-CFO in review sentences
and that when used together the improvement was
even greater.
For the U-CFO extraction, we analyzed the er-
rors by our method. The total number of errors
was 363 by condition unit. We describe causes of
the errors with example sentences, translated into
English by the authors. In those examples, dou-
ble and single underlines denote false positive and
false negative, respectively. For each cause, we
show the number of errors in parentheses.
E1 (124) Errors were due to F11 and F12 with
insufficient dictionary for restrictive words. Typi-
cally, low frequency words (e.g., pilgrimage) and
words related to miscellaneous activities during a
travel (e.g., charging a battery of a mobile phone)
were not included in our dictionary. While it is im-
portant to increase the vocabulary size of our dic-
tionary, identifying synonymous expressions with
partial matching (e.g., go to sleep / go to bed) is
also important.
E2 (53) Errors were due to dependency anal-
ysis, which often mistakenly recognizes sen-
tence boundaries in an informal writing style
and dependency relations in a sentence com-
prising a phrase, such as “the best location
for fully enjoying Asakusa”. In this example,
CaboCha mistakenly associated the adnominal
modifier “for fully enjoying Asakusa” with “loca-
tion (aspect)” instead of “best (opinion word)”. As
a result, F1 and F3 did not regard this modifier as
a U-CFO.
E3 (40) Restrictive modifiers that modify a non-
opinion segment were mistakenly extracted as
U-CFOs. For example, in “I used this hotel
for business and the meal was good”, “for busi-
ness” includes the clue expression “for” but does
not modifies the opinion unit.
E4 (39) Similar to E3 but errors were due to
restrictive words instead of clue expressions. In
the example for E3, the restrict word “business”
caused the error.
E5 (26) U-CFOs that consist of a large num-
ber of phrases were often not extracted due
to F5, such as “This hotel is acceptable
for one night to take the train at the Chuo station
next morning”.
E6 (25) Errors were due to irrelevant entries in
our restrictive word dictionary.
E7 (11) Due to the sparseness problem for re-
strictive words in the training data, U-CFOs and
CFOs were not correctly distinguished.
E8 (9) Errors were due to part-of-speech tag-
ging.
E9 (6) Errors were due to extracting modifiers
consisting of a personal pronoun without addi-
tional user-related attributes, such as “enough
for me” , as U-CFOs. We need to identify whether
an expression for a person is associated with user-
related attributes, such as “the bed is small for a
person who is tall”, which indicates a physical at-
tribute of a user.
Additionally, there are 65 errors for which we
have not found a reason.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999825111111111">
Although a number of methods have been pro-
posed to search an opinionated corpus for opin-
ion units, few attempts have so far been made at
addressing cases where the validity of an evalua-
tion is restricted on a condition in the source text.
We proposed a method to identify such condi-
tions from sentences including opinion units. Our
method performs sequence labeling to determine
whether each phrase is a constituent of an condi-
tion for opinion. We proposed thirteen features as-
sociated with lexical and syntactic information of
Japanese, and showed their effectiveness using re-
views for hotels. The contributions of this paper
are introducing the notion of conditions for opin-
ions, which is language-independent, proposing a
method to extract condition-opinion relations from
opinionated corpora, and giving an insight into its
potential applications in opinion mining.
</bodyText>
<sectionHeader confidence="0.998965" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998476">
We would like to thank Professor Takenobu Toku-
naga (Tokyo Institute of Technology) for his valu-
able comments. This research was supported
in part by Grant-in-Aid for Scientific Research
(Grant No. 15H02747).
</bodyText>
<page confidence="0.996608">
630
</page>
<sectionHeader confidence="0.99832" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999860553191489">
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2(3):27.
Guohong Fu and Xin Wang. 2010. Chinese sentence-
level sentiment classification based on fuzzy sets. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 312–319.
Yulan He, Chenghua Lin, and Harith Alani. 2011.
Automatically extracting polarity-bearing topics for
cross-domain sentiment classification. In Proceed-
ings of the 49th Annual Meeting of the Association
for Computational Linguistics, pages 123–131.
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
Opinionminer: A novel machine learning system for
web opinion mining and extraction. In Proceedings
of the 15th ACM SIGKDD, pages 1195–1204.
Soo-Min Kim and Eduard Hovy. 2006. Automatic
identification of pro and con reasons in online re-
views. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 483–490.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the 6th Conference on Natural Language
Learning, pages 1–7.
John Lafferty, Andrew McCallum, and Fernando C.N.
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning, pages 282–
289.
Bing Liu and Lei Zhang. 2012. A survey of opinion
mining and sentiment analysis. In C.C. Aggarwal
and C.X.Zhai, editors, Mining Text Data, pages 415–
463. Springer.
Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic
patterns versus word alignment: Extracting opinion
targets from online reviews. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, pages 1754–1763.
Kang Liu, Liheng Xu, and Jun Zhao. 2014. Extracting
opinion targets and opinion words from online re-
views with graph co-ranking. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics, pages 314–324.
Suguru Matsuyoshi, Satoshi Sato, and Takehito Ut-
suro. 2006. Compilation of a dictionary of Japanese
functional expressions with hierarchical organiza-
tion. In Yuji Matsumoto, Richard Sproat, Kam-Fai
Wong, and Min Zhang, editors, Computer Process-
ing of Oriental Languages. Beyond the Orient: The
Research Challenges Ahead, volume 4285 of Lec-
ture Notes in Computer Science, pages 395–402.
Springer Berlin Heidelberg.
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-lingual
mixture model for sentiment classification. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 572–581.
Samaneh Moghaddam and Martin Ester. 2013. The
FLDA model for aspect-based opinion mining: Ad-
dressing the cold start problem. In Proceedings of
the 22nd International Conference on World Wide
Web, pages 909–918.
Samaneh Moghaddam, Mohsen Jamali, and Martin
Ester. 2012. ETF: Extended tensor factorization
model for personalizing prediction of review help-
fulness. In Proceedings of the Fifth ACM Interna-
tional Conference on Web Search and Data Mining,
pages 163–172.
Ramanathan Narayanan, Bing Liu, and Alok Choud-
hary. 2009. Sentiment analysis of conditional sen-
tences. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 180–189.
M.P. O’Mahony and B. Smyth. 2010. A classification-
based review recommender. Knowledge-Based Sys-
tems, 23(4):323–329.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1–2):1–135.
Yohei Seki, Noriko Kando, and Masaki Aono. 2009.
Multilingual opinion holder identification using au-
thor and authority viewpoints. Information Process-
ing and Management, 45(2):189–199.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proceedings
of the 51st Annual Meeting of the Association for
Computational Linguistics, pages 1640–1649.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opinions
with a MaxEnt-LDA hybrid. In Proceedings of the
2010 Conference on Empirical Methods in Natural
Language Processing, pages 56–65.
</reference>
<page confidence="0.998297">
631
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.640014">
<title confidence="0.990575">Extracting Condition-Opinion Relations Fine-grained Opinion Mining</title>
<author confidence="0.999873">Yuki Nakayama Atsushi Fujii</author>
<affiliation confidence="0.981419">Department of Computer Graduate School of Information Science and Tokyo Institute of</affiliation>
<abstract confidence="0.985257260869565">A fundamental issue in opinion mining is to search a corpus for opinion units, each of which typically comprises the evaluation by an author for a target object from an aspect, such as “This hotel is in a good location”. However, few attempts have been made to address cases where the validity of an evaluation is restricted on a condition in the source text, such as “for traveling with small kids”. In this paper, we propose a method to extract condition-opinion relations from online reviews, which enables fine-grained analysis for the utility of target objects depending the user attribute, purpose, and situation. Our method uses supervised machine learning to identify sequences of words or phrases that comprise conditions for opinions. We propose several features associated with lexical and syntactic information, and show their effectiveness experimentally.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<issue>3</issue>
<contexts>
<context position="27689" citStr="Chang and Lin, 2011" startWordPosition="4728" endWordPosition="4731">to the opinion word is 1 and including a clue expression (see Section 3), and also identifies a sequence of the phrases from which there is a dependency path to the above phrase as a CFO. For example, in Figure 1 because phrase #6 includes a clue expression, a sequence of phrases #3–#6 is extracted as a CFO. These rules are based on features F1, F7 and F9. For the U-CFO extraction task, we regarded a sequence of Cond-phrases extracted by the above method as U-CFO if that sequence includes a restrictive word. For SVM, the thirteen features F1–F13 proposed in Section 3 was used. We used LIBSVM (Chang and Lin, 2011) to train a classifier. Our method used CRF to train a classifier with the thirteen features and four patterns for feature functions. We used CRF++2 to train a classifier for each phrase and regularized the parameters using L2-norm. Figure 2 shows the relationship between values of regularization parameter and F-measure for exact match. In Figure 2, “Rule”, “SVM”, and “CRF” denote a rule-based method, SVM-based method, and our method, respectively. The Fmeasure for Rule, independent of the regularization parameter, is a constant. While the F-measure for SVM substantially varied depending on th</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guohong Fu</author>
<author>Xin Wang</author>
</authors>
<title>Chinese sentencelevel sentiment classification based on fuzzy sets.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>312--319</pages>
<contexts>
<context position="9111" citStr="Fu and Wang, 2010" startWordPosition="1513" endWordPosition="1516">, we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind of conditional sentence, their research is fundamentally different from our research. Narayanan et al. (2009) targeted such a conditional sentence that comprises a single opinion as a whole, and intended to categorize its polarity into any of positive, negative, or neutral. Examples (7) and (8) are </context>
</contexts>
<marker>Fu, Wang, 2010</marker>
<rawString>Guohong Fu and Xin Wang. 2010. Chinese sentencelevel sentiment classification based on fuzzy sets. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 312–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulan He</author>
<author>Chenghua Lin</author>
<author>Harith Alani</author>
</authors>
<title>Automatically extracting polarity-bearing topics for cross-domain sentiment classification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>123--131</pages>
<contexts>
<context position="2130" citStr="He et al., 2011" startWordPosition="337" endWordPosition="340">inionated texts, fundamental issues involve modeling an unit of opinions and searching the corpus for those units, each of which typically comprises the evaluation by an author for a target object from an aspect. Other elements, such as when the opinion was submitted, can optionally be included in an opinion unit. We take the following review sentence as an example opinionated description. (1) I think hotel A offers a reasonable price if you take a family trip with small kids. From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combination of elements</context>
<context position="8774" citStr="He et al., 2011" startWordPosition="1456" endWordPosition="1459">ions. In other words, the identification for U-CFOs facilitates predicting the review helpfulness (O’Mahony and Smyth, 2010; Moghaddam et al., 2012). Candidate categories 623 include demographic and psychographic attributes for target users (e.g., age and hobby) and situations of target users (e.g., purpose, time, and place). However, we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Alth</context>
</contexts>
<marker>He, Lin, Alani, 2011</marker>
<rawString>Yulan He, Chenghua Lin, and Harith Alani. 2011. Automatically extracting polarity-bearing topics for cross-domain sentiment classification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 123–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Jin</author>
<author>Hung Hay Ho</author>
<author>Rohini K Srihari</author>
</authors>
<title>Opinionminer: A novel machine learning system for web opinion mining and extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD,</booktitle>
<pages>1195--1204</pages>
<contexts>
<context position="2094" citStr="Jin et al., 2009" startWordPosition="329" endWordPosition="332">ul knowledge latent in a corpus of opinionated texts, fundamental issues involve modeling an unit of opinions and searching the corpus for those units, each of which typically comprises the evaluation by an author for a target object from an aspect. Other elements, such as when the opinion was submitted, can optionally be included in an opinion unit. We take the following review sentence as an example opinionated description. (1) I think hotel A offers a reasonable price if you take a family trip with small kids. From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each </context>
<context position="8792" citStr="Jin et al., 2009" startWordPosition="1460" endWordPosition="1463">rds, the identification for U-CFOs facilitates predicting the review helpfulness (O’Mahony and Smyth, 2010; Moghaddam et al., 2012). Candidate categories 623 include demographic and psychographic attributes for target users (e.g., age and hobby) and situations of target users (e.g., purpose, time, and place). However, we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional</context>
</contexts>
<marker>Jin, Ho, Srihari, 2009</marker>
<rawString>Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009. Opinionminer: A novel machine learning system for web opinion mining and extraction. In Proceedings of the 15th ACM SIGKDD, pages 1195–1204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-Min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Automatic identification of pro and con reasons in online reviews.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions,</booktitle>
<pages>483--490</pages>
<contexts>
<context position="10463" citStr="Kim and Hovy (2006)" startWordPosition="1730" endWordPosition="1733">price was not reasonable. (8) If you are looking for a hotel with a reasonable price, stay at hotel A. In example (7), although the subordinate clause includes the opinion word “reasonable”, none of the subordinate clause, main clause, or entire sentence is an opinion. In example (8), the entire sentence is an unconditional opinion about the price for hotel A, but the main and subordinate clauses are not opinions independently. In contrast, the purpose of our research is to identify conditional opinions, in which the main and subordinate clauses are an opinion and its condition, respectively. Kim and Hovy (2006) proposed a method to identify a reason for the evaluation in an opinion, such as “the service was terrible because the staff was rude”. Although as discussed in Section 1 reasons can be CFOs, their purpose is to identify grounds that justify the evaluation and thus is different from our purpose. As discussed in Section 1, our research is related to predicting the review helpfulness (O’Mahony and Smyth, 2010; Moghaddam et al., 2012). The method proposed by O’Mahony and Smyth (2010) determines the helpfulness of a product review independent of the user profile and thus cannot recommend reviews </context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-Min Kim and Eduard Hovy. 2006. Automatic identification of pro and con reasons in online reviews. In Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 483–490.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="18292" citStr="Kudo and Matsumoto, 2002" startWordPosition="3106" endWordPosition="3110">0 1 aspect 1 0 0 1 0 opinion word 0 F13 conjunction noun aspect noun noun verb noun adverb opinion word verb Figure 1: Example of Japanese sentence and the feature value for each constituent bunsetsu phrase 0 shikashi, (However,) phrase that leads to the opinion word via a smaller number of dependency arrows is more likely to be a Cond-phrase. We use the dependency distance (i.e., the number of dependencies) between a phrase in question and the opinion word as the value for feature F1. The value for a phrase is −1 if there is no pass between that phrase and the opinion word. We use “CaboCha” (Kudo and Matsumoto, 2002) for dependency analysis purposes. F2: Phrase distance to opinion word F1 is not robust against errors of the dependency analysis. To alleviate this problem, we approximate the dependency distance by a phrase distance. In practice, we subtract the ID for a phrase in question from that for the opinion word as the value for feature F2. If the opinion word consists of more than one phrase, we take the minimum difference. Because in Japanese a modifier is usually followed by its modifying object, a phrase with a negative value for feature F2 is usually an Other-phrase. For example, in the last phr</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proceedings of the 6th Conference on Natural Language Learning, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="13866" citStr="Lafferty et al., 2001" startWordPosition="2319" endWordPosition="2322">The same method is used for BC/IC-phrases. However, words and phrases in an opinion unit are classified into its corresponding element. For example, an aspect phrase is classified into the aspect category. Given an input sequence of bunsetsu phrases, x = x1 ... xn, our task is to predict a sequence of labels, y = y1 ... yn, where yi E {BU, IU, BC, IC, Other, Target, Aspect, OpinionWord}. However, because an opinion unit in an input sentence has been identified in advance, the task is a quinary classification with respect to yi E {BU, IU, BC, IC, Other}. We use Conditional Random Fields (CRF) (Lafferty et al., 2001) to train a classifier for categorizing each bunsetsu phrase into any of the aforementioned five categories. We use a combination of unigram and bigram models and calculate the conditional probability, p(y|x), for linear-chain CRF by Equation (1). Here, Z,, denotes a normalization factor, and fk and gk denote feature functions for unigram and bigram models, respectively. Let xi,v denote a feature value for xi. While in the unigram model yi depends on either xi−1,v or xi,v, in the bigram model yi depends on either a combination of xi,v and yi−1 or that of xi−1,v and yi−1. Feature functions are </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning, pages 282– 289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Lei Zhang</author>
</authors>
<title>A survey of opinion mining and sentiment analysis.</title>
<date>2012</date>
<booktitle>Mining Text Data,</booktitle>
<pages>415--463</pages>
<editor>In C.C. Aggarwal and C.X.Zhai, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="2151" citStr="Liu and Zhang, 2012" startWordPosition="341" endWordPosition="344">fundamental issues involve modeling an unit of opinions and searching the corpus for those units, each of which typically comprises the evaluation by an author for a target object from an aspect. Other elements, such as when the opinion was submitted, can optionally be included in an opinion unit. We take the following review sentence as an example opinionated description. (1) I think hotel A offers a reasonable price if you take a family trip with small kids. From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combination of elements. For example, those </context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>Bing Liu and Lei Zhang. 2012. A survey of opinion mining and sentiment analysis. In C.C. Aggarwal and C.X.Zhai, editors, Mining Text Data, pages 415– 463. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kang Liu</author>
<author>Liheng Xu</author>
<author>Jun Zhao</author>
</authors>
<title>Syntactic patterns versus word alignment: Extracting opinion targets from online reviews.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1754--1763</pages>
<contexts>
<context position="2169" citStr="Liu et al., 2013" startWordPosition="345" endWordPosition="348">volve modeling an unit of opinions and searching the corpus for those units, each of which typically comprises the evaluation by an author for a target object from an aspect. Other elements, such as when the opinion was submitted, can optionally be included in an opinion unit. We take the following review sentence as an example opinionated description. (1) I think hotel A offers a reasonable price if you take a family trip with small kids. From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combination of elements. For example, those who intend to impr</context>
<context position="8810" citStr="Liu et al., 2013" startWordPosition="1464" endWordPosition="1467">ation for U-CFOs facilitates predicting the review helpfulness (O’Mahony and Smyth, 2010; Moghaddam et al., 2012). Candidate categories 623 include demographic and psychographic attributes for target users (e.g., age and hobby) and situations of target users (e.g., purpose, time, and place). However, we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind</context>
</contexts>
<marker>Liu, Xu, Zhao, 2013</marker>
<rawString>Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic patterns versus word alignment: Extracting opinion targets from online reviews. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1754–1763.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kang Liu</author>
<author>Liheng Xu</author>
<author>Jun Zhao</author>
</authors>
<title>Extracting opinion targets and opinion words from online reviews with graph co-ranking.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>314--324</pages>
<contexts>
<context position="2211" citStr="Liu et al., 2014" startWordPosition="353" endWordPosition="356">arching the corpus for those units, each of which typically comprises the evaluation by an author for a target object from an aspect. Other elements, such as when the opinion was submitted, can optionally be included in an opinion unit. We take the following review sentence as an example opinionated description. (1) I think hotel A offers a reasonable price if you take a family trip with small kids. From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combination of elements. For example, those who intend to improve the quality of hotel A may investigate</context>
</contexts>
<marker>Liu, Xu, Zhao, 2014</marker>
<rawString>Kang Liu, Liheng Xu, and Jun Zhao. 2014. Extracting opinion targets and opinion words from online reviews with graph co-ranking. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 314–324.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Suguru Matsuyoshi</author>
<author>Satoshi Sato</author>
<author>Takehito Utsuro</author>
</authors>
<title>Compilation of a dictionary of Japanese functional expressions with hierarchical organization.</title>
<date>2006</date>
<booktitle>Computer Processing of Oriental Languages. Beyond the Orient: The Research Challenges Ahead,</booktitle>
<volume>4285</volume>
<pages>395--402</pages>
<editor>In Yuji Matsumoto, Richard Sproat, Kam-Fai Wong, and Min Zhang, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="15800" citStr="Matsuyoshi et al., 2006" startWordPosition="2642" endWordPosition="2645">etween two phrases, respectively, and in each phrase we show Japanese words based on the Hepburn system and their English translations in parentheses. CFOs are associated with the following characteristics. (a) By definition, CFOs determine the validity of the evaluation in an opinion unit, and thus syntactically modify an opinion word. Consequently, CFOs usually do not modify other elements in an opinion unit, such as an aspect. (b) Like a conjunction in a conditional clause in English, such as “if”, a CFO in Japanese also includes a clue expression, which is usually a functional expression (Matsuyoshi et al., 2006) in the tail phrase, such as “ni wa (“for” in English)”. (c) The distribution for parts of speech as the head of CFOs is skewed and heads of CFOs are usually a noun or verb. Additionally, U-CFOs are associated with the following characteristics. (d) If a CFO is an opinion holder as in example (6) in Section 1, it is usually a U-CFO, which is the subject appearing at the beginning of a target sentence. (e) By definition, U-CFOs include expressions related to user attributes, such as “nervosity” in Figure 1. In this paper, we propose thirteen features to model CFOs and U-CFOs. In the bottom part</context>
<context position="20507" citStr="Matsuyoshi et al., 2006" startWordPosition="3503" endWordPosition="3506">FO because the evaluation is valid only from the perspective of that specific subject. For example, in “my daughter was pleased with toys in the room” the positive evaluation is restricted by the daughter’s perspective. Thus, the value of feature F6 takes 1 for the first phrase in a sentence excluding a conjunction, and 0 otherwise. F7: Clue expression Because a CFO often ends with one or more specific particles or auxiliary verbs, we use the existence of those clue expressions in a phrase as the value for feature F7. We use words in a dictionary of Japanese functional expressions “Tsutsuji” (Matsuyoshi et al., 2006) 626 as the clue expressions. Table 1 shows examples of entries for Tsutsuji. Each entry is represented in a hierarchy structure with nine abstraction levels. We firstly collected “Head words” in the nineteen categories (e.g., resultative condition and purpose in L2) associated with our purpose, consulting “Meaning categories”. Then we collected “Surface forms” corresponding to the collected head words and identified their corresponding surface forms to standardize different forms. For example, for ID 1 and ID 3 in Table 1, “to sure ba” and “nde” are regarded as identical to “to suru to” and “</context>
</contexts>
<marker>Matsuyoshi, Sato, Utsuro, 2006</marker>
<rawString>Suguru Matsuyoshi, Satoshi Sato, and Takehito Utsuro. 2006. Compilation of a dictionary of Japanese functional expressions with hierarchical organization. In Yuji Matsumoto, Richard Sproat, Kam-Fai Wong, and Min Zhang, editors, Computer Processing of Oriental Languages. Beyond the Orient: The Research Challenges Ahead, volume 4285 of Lecture Notes in Computer Science, pages 395–402. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinfan Meng</author>
<author>Furu Wei</author>
<author>Xiaohua Liu</author>
<author>Ming Zhou</author>
<author>Ge Xu</author>
<author>Houfeng Wang</author>
</authors>
<title>Cross-lingual mixture model for sentiment classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>572--581</pages>
<contexts>
<context position="9061" citStr="Meng et al., 2012" startWordPosition="1505" endWordPosition="1508">get users (e.g., purpose, time, and place). However, we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind of conditional sentence, their research is fundamentally different from our research. Narayanan et al. (2009) targeted such a conditional sentence that comprises a single opinion as a whole, and intended to categorize its polarity into any of positiv</context>
</contexts>
<marker>Meng, Wei, Liu, Zhou, Xu, Wang, 2012</marker>
<rawString>Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou, Ge Xu, and Houfeng Wang. 2012. Cross-lingual mixture model for sentiment classification. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 572–581.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samaneh Moghaddam</author>
<author>Martin Ester</author>
</authors>
<title>The FLDA model for aspect-based opinion mining: Addressing the cold start problem.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd International Conference on World Wide Web,</booktitle>
<pages>909--918</pages>
<contexts>
<context position="9139" citStr="Moghaddam and Ester, 2013" startWordPosition="1517" endWordPosition="1520">sification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind of conditional sentence, their research is fundamentally different from our research. Narayanan et al. (2009) targeted such a conditional sentence that comprises a single opinion as a whole, and intended to categorize its polarity into any of positive, negative, or neutral. Examples (7) and (8) are such conditional sentences a</context>
</contexts>
<marker>Moghaddam, Ester, 2013</marker>
<rawString>Samaneh Moghaddam and Martin Ester. 2013. The FLDA model for aspect-based opinion mining: Addressing the cold start problem. In Proceedings of the 22nd International Conference on World Wide Web, pages 909–918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samaneh Moghaddam</author>
<author>Mohsen Jamali</author>
<author>Martin Ester</author>
</authors>
<title>ETF: Extended tensor factorization model for personalizing prediction of review helpfulness.</title>
<date>2012</date>
<booktitle>In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining,</booktitle>
<pages>163--172</pages>
<contexts>
<context position="8307" citStr="Moghaddam et al., 2012" startWordPosition="1387" endWordPosition="1390">c customers. It may be argued that in example (4) the target users are restricted to those who are interested in the price. However, in example (4) the price restricts the aspect of the opinion unit, and should not be confused with U-CFOs and even CFOs, which restrict the validity of the opinion unit. If we fully utilize U-CFOs, as discussed for the active solution above, we need to classify U-CFOs into semantic categories so that users can selectively read relevant opinions. In other words, the identification for U-CFOs facilitates predicting the review helpfulness (O’Mahony and Smyth, 2010; Moghaddam et al., 2012). Candidate categories 623 include demographic and psychographic attributes for target users (e.g., age and hobby) and situations of target users (e.g., purpose, time, and place). However, we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which</context>
<context position="10899" citStr="Moghaddam et al., 2012" startWordPosition="1805" endWordPosition="1808">In contrast, the purpose of our research is to identify conditional opinions, in which the main and subordinate clauses are an opinion and its condition, respectively. Kim and Hovy (2006) proposed a method to identify a reason for the evaluation in an opinion, such as “the service was terrible because the staff was rude”. Although as discussed in Section 1 reasons can be CFOs, their purpose is to identify grounds that justify the evaluation and thus is different from our purpose. As discussed in Section 1, our research is related to predicting the review helpfulness (O’Mahony and Smyth, 2010; Moghaddam et al., 2012). The method proposed by O’Mahony and Smyth (2010) determines the helpfulness of a product review independent of the user profile and thus cannot recommend reviews based on userrelated attributes. Moghaddam et al. (2012) used collaborative filtering to predict the review helpfulness. The evaluation by a target user for past reviews is used to model the user and predict the helpfulness for unread reviews, which results in different predictions depending on the user. An advantage of collaborative filtering is its applicability to items whose content is usually difficult to analyze, such as video</context>
</contexts>
<marker>Moghaddam, Jamali, Ester, 2012</marker>
<rawString>Samaneh Moghaddam, Mohsen Jamali, and Martin Ester. 2012. ETF: Extended tensor factorization model for personalizing prediction of review helpfulness. In Proceedings of the Fifth ACM International Conference on Web Search and Data Mining, pages 163–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramanathan Narayanan</author>
<author>Bing Liu</author>
<author>Alok Choudhary</author>
</authors>
<title>Sentiment analysis of conditional sentences.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>180--189</pages>
<contexts>
<context position="9289" citStr="Narayanan et al. (2009)" startWordPosition="1542" endWordPosition="1545"> identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind of conditional sentence, their research is fundamentally different from our research. Narayanan et al. (2009) targeted such a conditional sentence that comprises a single opinion as a whole, and intended to categorize its polarity into any of positive, negative, or neutral. Examples (7) and (8) are such conditional sentences associated with neutral and positive categories, respectively. (7) Hotel A would not have survived if the price was not reasonable. (8) If you are look</context>
</contexts>
<marker>Narayanan, Liu, Choudhary, 2009</marker>
<rawString>Ramanathan Narayanan, Bing Liu, and Alok Choudhary. 2009. Sentiment analysis of conditional sentences. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 180–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P O’Mahony</author>
<author>B Smyth</author>
</authors>
<title>A classificationbased review recommender.</title>
<date>2010</date>
<journal>Knowledge-Based Systems,</journal>
<volume>23</volume>
<issue>4</issue>
<marker>O’Mahony, Smyth, 2010</marker>
<rawString>M.P. O’Mahony and B. Smyth. 2010. A classificationbased review recommender. Knowledge-Based Systems, 23(4):323–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="2057" citStr="Pang and Lee, 2008" startWordPosition="321" endWordPosition="324">ng refers to a process to discover useful knowledge latent in a corpus of opinionated texts, fundamental issues involve modeling an unit of opinions and searching the corpus for those units, each of which typically comprises the evaluation by an author for a target object from an aspect. Other elements, such as when the opinion was submitted, can optionally be included in an opinion unit. We take the following review sentence as an example opinionated description. (1) I think hotel A offers a reasonable price if you take a family trip with small kids. From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1–2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
<author>Noriko Kando</author>
<author>Masaki Aono</author>
</authors>
<title>Multilingual opinion holder identification using author and authority viewpoints.</title>
<date>2009</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>45</volume>
<issue>2</issue>
<contexts>
<context position="2076" citStr="Seki et al., 2009" startWordPosition="325" endWordPosition="328">ss to discover useful knowledge latent in a corpus of opinionated texts, fundamental issues involve modeling an unit of opinions and searching the corpus for those units, each of which typically comprises the evaluation by an author for a target object from an aspect. Other elements, such as when the opinion was submitted, can optionally be included in an opinion unit. We take the following review sentence as an example opinionated description. (1) I think hotel A offers a reasonable price if you take a family trip with small kids. From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution o</context>
<context position="8829" citStr="Seki et al., 2009" startWordPosition="1468" endWordPosition="1471">acilitates predicting the review helpfulness (O’Mahony and Smyth, 2010; Moghaddam et al., 2012). Candidate categories 623 include demographic and psychographic attributes for target users (e.g., age and hobby) and situations of target users (e.g., purpose, time, and place). However, we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind of conditional sen</context>
</contexts>
<marker>Seki, Kando, Aono, 2009</marker>
<rawString>Yohei Seki, Noriko Kando, and Masaki Aono. 2009. Multilingual opinion holder identification using author and authority viewpoints. Information Processing and Management, 45(2):189–199.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1640--1649</pages>
<contexts>
<context position="2192" citStr="Yang and Cardie, 2013" startWordPosition="349" endWordPosition="352">unit of opinions and searching the corpus for those units, each of which typically comprises the evaluation by an author for a target object from an aspect. Other elements, such as when the opinion was submitted, can optionally be included in an opinion unit. We take the following review sentence as an example opinionated description. (1) I think hotel A offers a reasonable price if you take a family trip with small kids. From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combination of elements. For example, those who intend to improve the quality of hote</context>
<context position="8852" citStr="Yang and Cardie, 2013" startWordPosition="1472" endWordPosition="1475">ng the review helpfulness (O’Mahony and Smyth, 2010; Moghaddam et al., 2012). Candidate categories 623 include demographic and psychographic attributes for target users (e.g., age and hobby) and situations of target users (e.g., purpose, time, and place). However, we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind of conditional sentence, their research i</context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1640–1649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne Xin Zhao</author>
<author>Jing Jiang</author>
<author>Hongfei Yan</author>
<author>Xiaoming Li</author>
</authors>
<title>Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>56--65</pages>
<contexts>
<context position="2113" citStr="Zhao et al., 2010" startWordPosition="333" endWordPosition="336">t in a corpus of opinionated texts, fundamental issues involve modeling an unit of opinions and searching the corpus for those units, each of which typically comprises the evaluation by an author for a target object from an aspect. Other elements, such as when the opinion was submitted, can optionally be included in an opinion unit. We take the following review sentence as an example opinionated description. (1) I think hotel A offers a reasonable price if you take a family trip with small kids. From the above example, existing methods (Pang and Lee, 2008; Seki et al., 2009; Jin et al., 2009; Zhao et al., 2010; He et al., 2011; Liu and Zhang, 2012; Liu et al., 2013; Yang and Cardie, 2013; Liu et al., 2014) are intended to extract the following quintuple as an opinion unit. Target = “hotel A”, Aspect = “price”, Evaluation (Polarity) = “reasonable” (positive), Holder = “I (author)”, Time = N/A Depending on the application, “Evaluation” can be any of a literal opinion word (e.g., “reasonable”), a polarity (positive/negative), or a value for multipoint scale rating. Given those standardized units extracted from a corpus, it is feasible to overview the distribution of values for each element or a combin</context>
<context position="8872" citStr="Zhao et al., 2010" startWordPosition="1476" endWordPosition="1479">ss (O’Mahony and Smyth, 2010; Moghaddam et al., 2012). Candidate categories 623 include demographic and psychographic attributes for target users (e.g., age and hobby) and situations of target users (e.g., purpose, time, and place). However, we leave the classification for U-CFOs future work. 2 Related work As described in Section 1, the fundamental methods for opinion mining include opinion extraction, which identifies elements for opinion units (i.e., target, aspect, evaluation, holder, and time) (He et al., 2011; Jin et al., 2009; Liu et al., 2013; Seki et al., 2009; Yang and Cardie, 2013; Zhao et al., 2010), and opinion classification, which determines the non-literal evaluation of each opinion unit based on bipolar categories (i.e., positive and negative) (He et al., 2011; Meng et al., 2012) or multipoint scale categories (Fu and Wang, 2010; Moghaddam and Ester, 2013). However, none of these methods intends to determine whether or not an opinion is conditional and to extract their condition. Narayanan et al. (2009) proposed a method for sentiment classification targeting conditional sentences. Although a conditional opinion is a kind of conditional sentence, their research is fundamentally diff</context>
</contexts>
<marker>Zhao, Jiang, Yan, Li, 2010</marker>
<rawString>Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaoming Li. 2010. Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 56–65.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
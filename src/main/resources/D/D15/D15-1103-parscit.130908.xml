<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000049">
<title confidence="0.892142">
FINET: Context-Aware Fine-Grained Named Entity Typing
</title>
<author confidence="0.739607">
Luciano Del Corro* Abdalghani Abujabal* Rainer Gemulla† Gerhard Weikum*
</author>
<bodyText confidence="0.35216125">
* Max-Planck-Institut f¨ur Informatik, Saarbr¨ucken, Germany
{delcorro, abujabal, weikum}@mpi-inf.mpg.de
† Universit¨at Mannheim, Mannheim, Germany
rgemulla@uni-mannheim.de
</bodyText>
<sectionHeader confidence="0.974732" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999922142857143">
We propose FINET, a system for detect-
ing the types of named entities in short
inputs—such as sentences or tweets—with
respect to WordNet’s super fine-grained
type system. FINET generates candidate
types using a sequence of multiple extrac-
tors, ranging from explicitly mentioned
types to implicit types, and subsequently
selects the most appropriate using ideas
from word-sense disambiguation. FINET
combats data scarcity and noise from ex-
isting systems: It does not rely on supervi-
sion in its extractors and generates training
data for type selection from WordNet and
other resources. FINET supports the most
fine-grained type system so far, includ-
ing types with no annotated training data.
Our experiments indicate that FINET out-
performs state-of-the-art methods in terms
of recall, precision, and granularity of ex-
tracted types.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999542653061225">
Named entity typing (NET) is the task of detecting
the type(s) of a named entity in context. For in-
stance, given “John plays guitar on the stage”, our
goal is to infer that “John” is a guitarist or a mu-
sician and a person. We propose FINET, a system
for detecting the types of named entities in short
inputs—such as sentences or tweets—with respect
to WordNet’s super fine-grained type system (16k
types of organizations, persons and locations).
Named entity typing is a fundamental building
block for many natural-language processing tasks.
NET is at the heart of information extraction meth-
ods for finding types for entities in a knowledge
base1 (KB) (Mitchell et al., 2015). Likewise, NET
aids named entity disambiguation by reducing the
candidate space for a given entity mention. Entity
types are an important resource for entity-based
retrieval or aggregation tasks, such as semantic
search (Hoffart et al., 2014) or question answer-
ing (Yahya et al., 2013). Finally, type information
helps to increase the semantic content of syntactic
patterns (Nakashole et al., 2012) or in open infor-
mation extraction (Lin et al., 2012).
The extraction of explicit types has been studied
in the literature, most prominently in the context
of taxonomy induction (Snow et al., 2006). Ex-
plicit types occur, for example, in phrases such as
“Steinmeier, the German Foreign Minister, [...]”
or “Foreign Minister Steinmeier.” These explicit
types are often extracted via patterns, such as the
well-known Hearst patterns (Hearst, 1992), and
subsequently integrated into a taxonomy. Pattern-
based methods often have high precision but low
recall: Types are usually mentioned when a named
entity is introduced or expected to be unknown to
readers, but often are not explicitly stated. The
NET problem differs from taxonomy induction in
that (1) the type system is prespecified, (2) types
are disambiguated, and (3) types are associated
with each occurrence of named entity in context.
Our FINET system makes use of explicit type
extractions whenever possible. But even when
types are not explicitly mentioned, sentences may
give clues to the correct type. These clues range
from almost explicit to highly implicit. For exam-
ple, in “John plays soccer”, the type soccer player
is almost explicit. The sentence “Pavano never
even made it to the mound,” however, only implic-
itly indicates that “Pavano” is a baseball player. A
</bodyText>
<footnote confidence="0.992727">
1In this paper, we refer to WordNet as a type system and
to a collection of entities and their types as a KB.
</footnote>
<page confidence="0.867727">
868
</page>
<note confidence="0.985781">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 868–878,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.995595326923077">
key challenge in NET is to extract such implicit, Extractor Stopping Condition
context-aware types to improve recall.
One way to extract implicit types is by training
a supervised extractor on labeled data, in which
each entity is annotated with appropriate types.
The key problem of this approach is that training
data is scarce; this scarcity is amplified for fine-
grained type systems. To address this problem,
many existing systems generate training data by
exploiting KBs as a resource (Yosef et al., 2012).
A popular approach is to train an extractor on a
corpus of sentences (e.g., on Wikipedia), in which
each named entity is associated with all its types in
a KB. The key problem with such an approach is
that the so-obtained type information is oblivious
to the context in which the entity was mentioned.
For example, in the sentences “Klitschko is known
for his powerful punches” and “Klitschko is the
Mayor of Kiew,” “Klitschko” will be associated
with all its types, e.g., boxer, politician and mayor.
As a consequence, the labels in the training data
can be misleading and negatively affect the ex-
tractors. Moreover, such learned extractors are of-
ten biased towards prominent types but perform
poorly on infrequent types, and they are generally
problematic when types are correlated (e.g., most
presidents are also graduates and authors).
FINET addresses the above problems by first
generating a set of type candidates using multiple
extractors and then selecting the most appropriate
type(s). To generate candidates, we make use of a
sequence of extractors that range from explicit to
highly implicit. Implicit extractors are only used
when more explicit extractors fail to produce a
good type. Our extractors are based on patterns,
mention text, and verbal phrases. To additionally
extract highly implicit types, we use word vec-
tors (Mikolov et al., 2013) trained on a large unla-
beled corpus to determine the types of similar enti-
ties that appear in similar contexts. This extractor
is comparable to KB methods discussed above, but
is unsupervised, and takes as candidates the types
frequent within the related entities and contexts.
After type candidates have been generated, the
final step of FINET selects the types that best fit
the context. In this step, we leverage previous
work on word sense disambiguation (WSD) and
resources such as WordNet glosses, and, if avail-
able, manually annotated training data.
FINET leverages ideas from existing systems
and extends them by (1) handling short inputs (2)
supporting a very fine-grained type hierarchy, and
</bodyText>
<table confidence="0.760643">
Pattern-based (final) Always stop
Pattern-based (non-final) KB-lookup
Mention-based KB-lookup
Verb-based KB-lookup
</table>
<tableCaption confidence="0.816403">
Corpus-based 50% of score in 5 10 types
Table 1: Extractors and their stopping conditions
</tableCaption>
<bodyText confidence="0.974902606557377">
(3) producing types that focus on the entity con-
text. Existing systems are unable to extract more
than a couple of hundred types. Hyena (Yosef et
al., 2012), the system with the most fine-grained
type system so far, focuses only on a subset of
505 types from WordNet. Hyena lacks important
types such as president or businessman, and in-
cludes soccer player but not tennis player. Instead
of restricting types, FINET operates on the the en-
tire WordNet hierarchy with more than 16k types
for persons, organizations, and locations.
We evaluated FINET on a number of real-world
datasets. Our results indicate that FINET signifi-
cantly outperforms previous methods.
2 Candidate Generation
869 In this phase, we collect candidate types for each
entity. We first preprocess the input (Sec. 2.1) and
then apply a (i) pattern-based extractor (Sec. 2.2),
(ii) a mention-based extractor (Sec. 2.4), (iii) a
verb-based extractor (Sec. 2.5), and (iv) a corpus-
based extractor (Sec. 2.6). The extractors are or-
dered by decreasing degree of explicitness.
Each extractor has a stopping condition, which
we check whenever the extractor produced at least
one type. When the stopping condition is met, we
directly proceed to the type selection phase. The
reasoning behind this approach is to bias FINET
towards the most explicit types. When the con-
dition is not met, we enrich the set of candidate
types of the extractor with their hypernyms; we
expect types to be overly specific and want to al-
low the selection phase to be able to select a more
general type. We then also run subsequent extrac-
tors. Tab. 1 displays a summary of the extractors
and stopping conditions.
All so-found type candidates are passed to the
candidate selection phase (Sec. 3).
2.1 Preprocessing
Preprocessing consists of 5 steps: (i) dependency
parsing (Socher et al., 2013); (ii) co-reference (Re-
casens et al., 2013); (iii) named entity recogni-
tion (NER) (Finkel et al., 2005) with the detection
of coarse-grained types (i.e., person, organization,
location); (iv) clause identification (Del Corro and
Gemulla, 2013); (v) word and multi-word expres-
sion recognition (Del Corro et al., 2014).
FINET restricts its candidates to the hyponmys
of the coarse-grained (CG) type of the NER sys-
tem. Named entities with the same CG type in
a coordinating relation (e.g., “Messi and Ronaldo
are soccer players”) and identical mentions share
the candidate set; the latter is reasonable in short
input.
FINET extractors operate either on the sentence
or the clause level. A clause is a part of a sen-
tence that expresses a statement and is thus a suit-
able unit for automatic text processing (Del Corro
and Gemulla, 2013). Finally, we identify multi-
word explicit type mentions such as Prime Minis-
ter or Secretary of Housing and Urban Develop-
ment (Del Corro et al., 2014).
</bodyText>
<subsectionHeader confidence="0.997913">
2.2 Pattern-based extractor
</subsectionHeader>
<bodyText confidence="0.999858366666667">
Our pattern-based extractor targets explicit type
mentions. These are commonly used to intro-
duce entities when they first appear (“US Presi-
dent Barack Obama”) or when their mention does
not refer to the most prominent entity (“Barack
Obama, father of the US President”). Following
previous work (Hearst, 1992), we use a set of pat-
terns to look for expressions that may refer named
entity types. We refer to those expressions as lex-
ical types (e.g., “father”). Once lexical types have
been identified, we collect as candidate types the
WordNet synsets to which they refer (e.g., (father-
1), ..., (father-8), the eight senses of “father”).
Our extractor makes use of both syntactic pat-
terns, which operate on the dependency parse, and
regular expression patterns, which operate on the
text. Syntactic patterns are preferable in that they
do not rely on continuous chunks of text and can
skip non-relevant information. However, mistakes
in the dependency parse may lower recall. To cope
with this, we additionally include regular expres-
sions for some syntactic patterns. Fig. 1 shows
an example of a syntactic pattern and a related
regular-expression pattern. Both produce lexical
type “president” from “Barack Obama, president
of the US,” but only the syntactic pattern applies
to “Barack Obama, the current US president.”
Tab. 2 gives an overview of our patterns. Most
of them also have a symmetric version (e.g., “The
president, Barack Obama” and “Barack Obama,
</bodyText>
<figure confidence="0.999214">
(a) A syntactic pattern
NAMED ENTITY, NOUN
(b) A regular expression pattern
</figure>
<figureCaption confidence="0.998583">
Figure 1: Patterns capturing appositions
</figureCaption>
<table confidence="0.8882031875">
Pattern Example
Final patterns
Hearst I {Presidents} such as [Obama] (and) [Bush]
Hearst II {Presidents} like [Obama] (and) [Bush]
Hearst III Obama (and) other {presidents}
Hearst IV {Presidents} including [Obama] (and) [Bush]
Apposition [Obama], (the) {president}
Copular [Obama] is (the) {president}
Noun modifier {President} [Barack Obama]
Among [Joe Biden] among (other) {vice presidents}
Enough [Messi] is enough (of) a {player}
As [Messi] as {player}
Non-final patterns
Location {City} of [London]
Poss. + transf. [Shakespeare]’s {productions}
by-prep + transf. {productions} by [Shakespeare]
</table>
<tableCaption confidence="0.99229">
Table 2: Patterns for explicit type extraction
</tableCaption>
<bodyText confidence="0.949688233333333">
the president”), which is not displayed. We divide
our patterns into final and non-final. Final patterns
generally have high precision and extract the lexi-
cal type exactly as it occurs. When a final pattern
produces a lexical type, we add the correspond-
ing types to the candidate set and go directly to
the type selection phase, i.e., we do not consider
any other extractor. For non-final patterns, how-
ever, we expect erroneous extractions and proceed
differently; we perform a KB lookup for all lex-
ical types. The KB lookup (described in the next
section) both prunes and expands the candidate set
using a KB, and acts as a stopping condition. (FI-
NET can also be run without using KB lookups.)
We treat a pattern as non-final if it may or may
not denote a lexical type (e.g. “the president of
Argentina” vs. “the city of Buenos Aires”) or if
a verb-noun transformation is required to obtain
the lexical type (e.g. “Shakespeare’s productions”
to “producer”). To perform the transformations,
we use WordNet’s derivationally related forms,
which connect semantically related morphologi-
cal variations of verbs and nouns. For instance,
the noun “production” is connected to the verb
“produce,” which in turn is connected to the noun
“producer”. These variations can be exploited to
extract explicit types. We treat such transforma-
NAMED ENTITY, (modifier) NOUN (modifier) ,
appos
mod mod
</bodyText>
<page confidence="0.973492">
870
</page>
<bodyText confidence="0.999992444444444">
tions as non-final because we may make mistakes.
Moreover, WordNet is highly incomplete in terms
of derivational forms. For instance, we may not
be able to reach all the possible senses for “pro-
ducer” from “production,” and from “works” in
“works by Schumann”, we cannot reach “musi-
cian” or “artist,” but we reach “worker” and there
is no synset of “worker” that specifically denotes
an artist.
</bodyText>
<subsectionHeader confidence="0.999629">
2.3 Exploiting a knowledge base
</subsectionHeader>
<bodyText confidence="0.999987945945946">
Most of our extractors (optionally) leverage a
knowledge base to (1) prune candidate types, (2)
find additional candidates, and (3) decide whether
to consider subsequent extractors. To do so, we
extract from a KB a repository of (entity men-
tion, type)-pairs.2 We use the KB conservatively,
i.e., we consider KB evidence only if the extracted
types match the ones in the KB.
The KB is leveraged via a KB lookup. Each KB
lookup takes as input an entity mention e and a set
T of candidate types found by the extractor (lexi-
cal or disambiguated). We first replace each lexi-
cal type in T by the set of the corresponding types
from WordNet. Afterwards, for each type t E T,
we check whether there is one or more matching
types (e, tKB) in the KB for the given mention.
Type tKB matches t if it is either identical, a hy-
pernym, or a hyponym of t. For each match, the
KB lookup outputs t. If tKB is a hyponym of t, we
additionally output tKB, that is, a type more spe-
cific than the one found by the extractor. We leave
the decision of whether t or tKB is a more suit-
able type to the type selection phase. For example,
for e = “Messi” and t = (player-1), we output
(player-1) and (soccer player-1) (ahyponym).
The KB lookup is successful if it outputs at least
one type. Then we add the resulting types to the
candidate set and go directly to the type selection
phase. A KB lookup fails if no matching type
was found. We then proceed differently: if a KB
lookup fails, we add the complete set T to the can-
didate set and continue to the next extractor.
As mentioned above, FINET can also be run
without any KB lookups; the corresponding ex-
tractors then do not have a stopping condition. In
Sec. 4, we experimented with both variants and
found that KB lookups generally helped.
</bodyText>
<footnote confidence="0.64514">
2We used Yago2 (Hoffart et al., 2013). Our repository
contained roughly 9M entity mentions and 20M pairs.
</footnote>
<subsectionHeader confidence="0.985124">
2.4 Mention-based extractor
</subsectionHeader>
<bodyText confidence="0.999970409090909">
Our second extractor aims to extract type candi-
dates from the entity mention. This is particu-
larly effective for organizations, which often con-
tain the lexical type in their name (e.g., “Johnson
&amp; Wales University” or “Republican House”).
Given an entity mention, we check if any of the
words or expressions in the name corresponds to
a lexical type in WordNet. If so, we consider the
corresponding types as potential candidates. For
instance, for “Imperial College London”, we ex-
tract “college” and obtain types (college-1) and
(college-2) (both matching the CG type) from
WordNet. We then perform a KB lookup.
We extend the above procedure for entities
tagged as location. Since the set of (named-entity)
locations is quite static and known, we assume that
the KB contains all locations and their possible
types (our experiments strengthened this assump-
tion). If a mention of a location (e.g., “Berlin”) oc-
curs in the repository, we add all the corresponding
types from the repository to the candidate set (e.g.,
(city- 1)) and move to the type selection phase.
</bodyText>
<subsectionHeader confidence="0.968303">
2.5 Verb-based extractor
</subsectionHeader>
<bodyText confidence="0.999980925925926">
Verbs have been widely exploited to determine
the types or roles of its arguments: A verb sense
imposes a restriction on the type of its argu-
ments (Quirk et al., 1985; Levin, 1993; Hanks,
1996; Baker et al., 1998; Palmer et al., 2005;
Kipper et al., 2008). For instance, from “Ted
Kennedy was elected to Congress,” we infer that
“Ted Kennedy” is a person who can be elected.
Corresponding types include (representative-1),
(representative-2), or (politician-1). Our verb-
based extractor leverages this insight to extract
types. The extractor operates at the clause level.
A simple way to infer lexical types for entities
acting as subjects or objects of a clause is nomi-
nalization, i.e., the transformation of the verb into
deverbal nouns (e.g., “play” into “player”). To
exploit it, we apply a set of morphological trans-
formations to the verb (Quirk et al., 1985), which
depend on the grammatical function of the entity,
i.e., subject or object. If the entity mention acts as
a subject, we try adding the suffixes “-er,”, “-or,”
and “-ant” to the verb’s lemma. If the mention acts
as an object, we use suffixes “-ee” and “-ed”. To
obtain candidate types, we again use derivation-
ally related forms (DER). We consider as poten-
tial candidates all types referred to by one of the
deverbal nouns and connected to a sense of the
</bodyText>
<page confidence="0.988612">
871
</page>
<bodyText confidence="0.99890146875">
verb via DER. For instance, given “Messi plays in
Barcelona,” we collect for “Messi” all the senses
of “player” that are connected to some sense of
“play” ((player-1), (musician-1) and (actor-1)).
We also explore WordNet in a way that is not
restricted to morphological variations of the verb.
For instance, in “John committed a crime,” “com-
mit” is a synonym of “perpetrate,” which in turn
can be varied to “perpetrator”. We consider the
morphological variations of all synonyms of the
verb. Moreover, if the named entity is the subject
of the clause, and if the clause contains a direct ob-
ject, we form a new lexical type by adding the di-
rect object as a noun modifier of the deverbal noun
(e.g., From “Messi plays soccer”, we form “soccer
player”). If it exists in WordNet, we consider the
respective types as potential candidates as well.
A more indirect way of exploiting the verb-type
semantic concordance is via a corpus of frequent
(verb, type)-pairs, where the type refers to possi-
ble types of the verb’s subject or object. As stated
above, the set of argument types compatible with a
verb is limited. For instance, “treat” is usually fol-
lowed by (condition-1), (disease-1), or (patient-
1). FINET, uses the corpora of Flati and Nav-
igli (2013) and Del Corro et al. (2014). Given a
verb and an entity, we search for frequent candi-
date types (depending on whether the entity acts
as a subject or object). For example, from “Messi
was treated in the hospital,” we obtain (patient-1).
Once potential candidates have been collected,
we perform a KB lookup to decide how to proceed.
</bodyText>
<subsectionHeader confidence="0.974576">
2.6 Corpus-based extractor
</subsectionHeader>
<bodyText confidence="0.999893485714285">
Our final extractor leverages a large unlabeled cor-
pus to find entities that co-occur in similar con-
texts. It is based on the distributional hypothe-
sis (Sahlgren, 2008): similar entities tend to oc-
cur in similar contexts. For example, “Messi” and
“Cristiano Ronaldo” may both be mentioned in
the context of soccer. Thus entity mentions sim-
ilar to “Messi” in a sport context are likely to in-
clude soccer players. Our extractor is related to
semi-supervised KB methods in that it propagates
types of named entity mentions that may appear in
a similar context. It differs in that it is unsuper-
vised, does not require manually or automatically
generated training data, and in the way context is
modeled and candidates are generated.
Our corpus-based extractor makes use of word
vectors (Rumelhart et al., 1988) trained on a large
unlabeled corpus. A word vector is a semantic rep-
resentation of a phrase and represents the semantic
context in which the phrase occurs. Phrases that
are semantically related, and thus appear in similar
contexts, are close to each other in the word vec-
tor space. For instance, if “Messi” and “Cristiano
Ronaldo” tend to co-occur with a similar sets of
words, their word vectors are close. We also may
expect “Arnold Schwarzenegger” to be close to
both actors and politicians, since it occurs in both
contexts. In our work, we use word2vec (Mikolov
et al., 2013), which provides a model trained on
Google News to predict related words or phrases
for a query specified as a set of phrases. Given an
integer k, word2vec outputs the set of k phrases
that are most similar to the query.
Our corpus-based extractor uses (1) the input
sentence to construct a set of relevant queries and
(2) the word2vec results and a KB. To construct a
query for a given entity mention, we focus on the
part of the sentence directly related to the entity.
This relevant part consists of the clause in which
the entity occurs and the subordinate clauses that
do not contain another entity. Since word2vec is
most effective when queries are short, we con-
struct a set of small queries, each consisting of
the named entity mention and some context in-
formation. We construct a query for each noun
phrase (of length at most 2) and for each other en-
tity mention. If the named entity occurs as sub-
ject or object, we also take the corresponding verb
and the head of the object or subject. For exam-
ple, the queries for “Maradona expects to win in
South Africa” are {“Maradona”, “South Africa”J
and {“Maradona”, “expect”, “win”J.
For each query, we retrieve the 100 most re-
lated phrases with their similarity score and union
the results. We filter them using our KB and re-
tain only those phrases that correspond to entity
mentions (with the correct CG types). We then
enrich each mention by the set of their possible
types from the KB. Here we exclude widespread
but irrelevant implicit types such as (male-1),
(female-1), (adult-1), (commoner-1), (friend-1),
or (alumnus-1). We also include the types cor-
responding to the entity mention (with score 1).
If there is sufficient evidence that some of the
so-obtained types are most prominent, we take
these types as candidates. In our example, query
{“Maradona” “South Africa”J, all of the top-15
persons (e.g., “Diego Maradona”, “Carlos Alberto
Parreira”, “Dunga”, “Beckenbauer”) share type
(coach-1); a strong indication that Maradona may
</bodyText>
<page confidence="0.988966">
872
</page>
<bodyText confidence="0.999925">
also be of type (coach-1). To select prominent
types we traverse the results until we collect 50%
of the total score. We take all so-collected types
as candidates. If no more than 10 different types
were added this way, we directly go to the type
selection phase.
</bodyText>
<sectionHeader confidence="0.994458" genericHeader="method">
3 Type Selection
</sectionHeader>
<bodyText confidence="0.9999565">
The type selection phase selects the most appro-
priate type from the set of candidates of a given
named entity. We use techniques from WSD, but
adapt them to our setting. WSD aims to disam-
biguate a word or phrase (e.g., a noun) with re-
spect to a type system as WordNet; e.g., from
“player” to (player-1). The main difference be-
tween WSD and our type selection is that our goal
is to decide between a set of types for an entity
mention; e.g., from “Messi” to (soccer player-1).
Our type selection step can be used as-is; it is not
trained on any domain- or corpus-specific data.
</bodyText>
<subsectionHeader confidence="0.999197">
3.1 Obtaining context
</subsectionHeader>
<bodyText confidence="0.999993038461539">
All WSD systems take a set of candidate types and
contextual information as input. The key chal-
lenge lies in the construction of candidate types
(Sec. 2) as well as context. For each entity, we
consider entity-oblivious context (from the input
sentence) as well as entity-specific context (using
lexical expansions).
We take all words in the sentence as entity-
oblivious context. To construct entity-specific
context, we make use of lexical expansions, which
have been successfully applied in WSD (Miller et
al., 2012). Its goal is to enrich contextual informa-
tion to boost disambiguation. In our case, it also
helps to differentiate between multiple entities in
a sentence. We build the entity-specific context
using word vectors. As in the corpus-based ex-
tractor, we construct a set of queries for the entity
but in this case we take as context all so-obtained
words that do not correspond to a named entity.
For instance, the entity-specific context for the
entity mention “Maradona” for query “Maradona
South Africa” is: “coach”, “cup”, “striker”, “mid-
fielder”, and “captain”. The full context for
“Maradona” in “Maradona expects to win in South
Africa” additionally includes the entity-oblivious
context “expects”, “win”, “South Africa”.
</bodyText>
<subsectionHeader confidence="0.999737">
3.2 Selecting types
</subsectionHeader>
<bodyText confidence="0.999989642857143">
WSD systems fall into two classes: unsupervised,
which rely on background knowledge such as
WordNet (Ponzetto and Navigli, 2010), and super-
vised, which require training data (Zhong and Ng,
2010). Here we take a combination, i.e., we lever-
age WordNet and manually annotated data.
We train a Naive Bayes classifier to select the
most appropriate type given its context. We repre-
sent context by a bag of lemmatized words. This
allows us to automatically generate training data
from WordNet (and use manually labeled data).
Since WordNet provides information for each of
the relevant types, this approach combats the data
sparsity that arises with supervised systems. The
context for each individual WordNet type consists
of all words appearing in the type’s gloss and the
glosses of its neighbors (Banerjee and Pedersen,
2003). We also include for each type the neigh-
bors from Ponzetto and Navigli (2010) and the
corresponding verbs from Del Corro and Gemulla
(2013). Finally, we add all words in sentences con-
taining the type in SemCor3 (Landes et al., 1998)
and Ontonotes 5.0 (Hovy et al., 2006).
We trained one classifier per CG type. To
train the classifier, we create a single training
point for each corresponding WordNet type and
use the type’s context as features. To map the
CG types from our NER system to WordNet, we
considered as persons all descendants of (person-
1), (imaginary being-1), (characterization-3),
and (operator-2) (10584 in total); as locations
all descendants of (location-1), (way-1), and
(landmass-1) (3681 in total); and as organizations
all descendants of (organization-1) and (social
group-1) (1968 in total). This approach of han-
dling CG types suffers to some extent from Word-
Net’s incompleteness, esp. with respect to persons
and organizations. For instance, phrase “spon-
sored by Coca-Cola” implies that “Coca-Cola” is
a “sponsor,” but in WordNet, only persons can be
sponsors. Nevertheless, this approach worked well
in our experiments.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.9999825">
We conducted an experimental study on multiple
real-word datasets to compare FINET with two
state-of-the-art approaches. FINET is used as-is;
it does not require training or tuning for any spe-
cific dataset. All datasets, detected types, labels,
and our source code are publicly available.4
</bodyText>
<footnote confidence="0.9972685">
3http://web.eecs.umich.edu/˜mihalcea/
downloads.html
4http://dws.informatik.uni-mannheim.
de/en/resources/software/finet/
</footnote>
<page confidence="0.997986">
873
</page>
<subsectionHeader confidence="0.951776">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999933394736842">
Methods. Hyena (Yosef et al., 2012) is a repre-
sentative supervised method that uses a hierarchi-
cal classifier. Its features include the words in the
named entity mention, in sentence and paragraph,
and POS tags. It performs basic co-reference res-
olution and marks entity mentions connected to
a type in the KB using a binary feature. Sim-
ilar to Ling and Weld (2012), Hyena is trained
on Wikipedia entities, each being annotated with
its corresponding WordNet types from YAGO.
Hyena’s type system is restricted to 505 Word-
Net types with top categories (artifact-1), (event-
1), (person-1), (location-1), and (organization-
1). Hyena outperformed a number of previous sys-
tems (Fleischman and Hovy, 2002; Rahman and
Ng, 2010; Ling and Weld, 2012). We used Hyena
via its web service (Yosef et al., 2013).
Pearl (Nakashole et al., 2013) is a semi-
supervised system that leverages a repository of
300k relational patterns (Nakashole et al., 2012).
Subjects and objects of each pattern carry type
information. Pearl types named entity mentions
by the most likely type according to its pattern
database. Pearl’s type system is based on around
200 “interesting” WordNet types. We ran Pearl in
its hard setting, which performed best.
FINET. We ran FINET in two configurations:
(1) with KB lookup, (2) without the KB lookup.
This allows us to estimate the extent to which re-
ferring to a KB helps. Note that the corpus-based
extractor makes use of the KB in both settings.
Datasets. We used three different datasets rep-
resenting real-world use cases. We created two
datasets, New York Times and Twitter, and sam-
pled a subset of the CoNLL data, which provides
gold annotations for CG types. We did not con-
sider datasets such as FIGER (Ling and Weld,
2012) or BBN (Weischedel and Brunstein, 2005)
because they are not suitable for very fine-grained
typing.
New York Times consists of 500 random sen-
tences from the New York Times corpus (Sand-
haus, 2008), year 2007; we selected only sen-
tences that contained at least one named entity ac-
cording to the Stanford CoreNLP 4.4.1 tool.
CoNLL. We sampled 500 sentences from
CoNLL (Tjong Kim Sang and De Meulder, 2003),
a collection of newswires with manually annotated
entities with CG types labels. We directly used the
annotations in our evaluation. The sentences tend
to be short and sometimes non-verbal (e.g., “Jim
Grabb ( U.S. ) vs. Sandon Stolle ( Australia )”).
Most entities are prominent and likely to be found
in our KB (and the one of existing methods).
Twitter. We collected the first 100 tweets with
named entities retrieved by the Twitter API.
Type system. FINET’s type system consists of
more than 16k types with top categories persons,
locations and organizations. We used the map-
ping between these top categories and WordNet
described in Sec. 3.2. Hyena and Pearl use 505
and 200 WordNet types, resp., which is signif-
icantly smaller. To compare the performance
across different granularities, we classified each
type as coarse-grained (CG), fine-grained (FG)
or super fine-grained (SFG). The CG types were
(artifact-1), (event-1), (person-1), (location-1)
and (organization-1). The FG types were those
included in Pearl. All remaining types were con-
sidered SFG.
Labeling. All extractions by all systems were
independently evaluated by two labelers. We
adopted a pessimistic view, i.e., we treat an ex-
traction as correct only if it was labeled correct by
both labelers. The Cohen’s kappa measure ranged
0.54–0.86, indicating a substantial agreement.
</bodyText>
<sectionHeader confidence="0.62335" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999434">
Description of Tab. 3. Our results are summa-
rized in Tab. 3. When a method did not produce
a type of the considered granularity but a more
fine-grained type, we selected its closest hyper-
nym. For each configuration, the table shows the
number of named entities for which types have
been extracted, the total number of extracted types
(more than one distinct type per named entity for
some methods), the total number of correct types,
and the precision (P). The number of named en-
tities for which types have been found and the to-
tal number of correct extractions can be seen as a
loose measure of recall. It is difficult to estimate
recall directly for FG and SFG types since some
entities may be associated with either no or mul-
tiple such types. To gain more insight, we show
the number of correct distinct types, and the aver-
age depth (shortest path from (entity-1) in Word-
Net) for both correct FG and correct SFG types.
Finally, we list the Cohen’s kappa inter-annotator
agreement measure for each method.
</bodyText>
<page confidence="0.995852">
874
</page>
<table confidence="0.999201947368421">
System Coarse-Grained (CG) Fine-Grained (FG) Super Fine-Grained (SFG) Distinct Avg. Depth Cohen’s
types kappa
Enti- Total Correct Enti- Total Correct Enti- Total Correct FG SFG
ties types types (P) ties types types (P) ties types types (P)
New York Times (500 sentences)
FINET 992 992 872 (87.90) 616 631 457 (72.42) 319 329 233 (70.82) 191 5.96 7.25 0.60
FINET (w/o KB l.) 992 992 872 (87.90) 598 613 436 (71.13) 294 304 204 (67.11) 174 5.98 7.18 0.58
Hyena 895 1076 779 (72.40) 770 1847 522 (28.26) 518 775 160 (20.65) 127 5.79 6.98 0.74
Pearl (hard) 15 15 5 (33.33) 2 2 0 – – – (–) 1 – – 0.54
CoNLL (500 sentences)
FINET 1355 1355 1355 (1.0) 1074 1086 876 (80.66) 668 679 510 (75.11) 136 6.09 7.38 0.62
FINET (w/o KB l.) 1355 1355 1355 (1.0) 1075 1087 869 (79.94) 661 672 498 (66.13) 134 6.06 7.35 0.62
Hyena 1162 1172 1172 (1.0) 1064 2218 1329 (59.92) 719 944 268 (28.39) 103 5.89 6.57 0.69
Pearl (hard) 18 18 18 (1.0) 8 11 5 (45.45) – – – (–) 7 5.6 – 0.74
Twitter (100 tweets)
FINET 135 135 123 (91.11) 103 104 69 (66.35) 54 54 33 (61.11) 40 6.25 7.64 0.58
FINET (w/o KB l.) 135 135 123 (91.11) 104 105 65 (61.90) 56 56 30 (53.57) 40 6.14 7.6 0.55
Hyena 125 146 105 (71.91) 117 280 75 (26.79) 91 129 21 (16.28) 42 6.11 6.19 0.67
Pearl (hard) 10 10 5 (50.00) 3 4 1 (25.00) – – – (–) 3 6 – 0.86
</table>
<tableCaption confidence="0.999743">
Table 3: Summary of results
</tableCaption>
<bodyText confidence="0.999744462686567">
Discussion. First note that Pearl extracted sig-
nificantly fewer types than any other system. Pearl
does not support SFG. For CG and FG, we conjec-
ture that its database, which was generated from
Wikipedia, did not reflect the syntactic structure
of the sentences in our datasets. This finding
strengthens the case for the use of heterogeneous
sources in semi-supervised methods.
Hyena performed better than Pearl and in many
cases extracted the largest number of types. Hyena
tended to extract multiple types per named entity
and mostly at least one FG type. This more recall-
oriented approach, and its context-unaware use of
supervision, significantly reduced its precision.
FINET had significantly higher precision across
all settings, especially for SFG types (almost three
times more than Hyena). One reason for this is that
FINET is conservative: We provide more than one
type per named entity only if the types were ex-
plicit. In all other cases, our type selection phase
produced only a single type. FINET extracted
the largest number of correct SFG types on each
dataset. Hyena extracted more FG types, but with
a significantly lower precision. The average depth
of correct FG and SFG types in FINET was higher
than that of Pearl and Hyena. FINET also tended
to use more distinct correct types (191 in NYT
vs. 127 for Hyena). Again, this more fine-grained
typing stemmed from FINET’s use of multiple ex-
tractors, many of which do not rely on supervision.
Note that FINET also has higher precision for
CG. As stated, FINET makes use of the Stanford
NER to extract CG types (except in CoNLL, were
we used the manual labels), and respects these
types for its FG and SFG extractions. Hyena has
lower precision for CG types because it sometimes
outputs multiple CG types for a single named en-
tity. To ensure a fair comparison, for CoNLL we
indirectly used the gold labels for Pearl and Hyena
by discarding all types with an incorrect CG type.
FINET’s extractors. Tab. 4 shows individual
influence of each of FINET’s extractors in the
NYT dataset with KB lookups. The table shows
the number of entities typed by each extractor and
the precision of the resulting types after type selec-
tion. The mention-based extractor was the most
precise and also fired most often, mainly due to
locations. The pattern-based extractor also had
a good precision and tended to fire often. The
first three extractors, the more explicit ones, gen-
erated more than half of the extracted types; this
indicates that explicit type extractors are impor-
tant. There were also a substantial fraction of im-
plicit types, covered by the corpus-based extrac-
tor. The verb-based extractor had the lowest preci-
sion, mostly because of the noisiness and incom-
pleteness of its underlying resources (such as the
(verb,type)-repository). We expect overall preci-
sion to increase if this extractor is removed. How-
ever, this would hinder FINET to infer types from
verbs. Instead, we believe a better direction is to
improve the underlying resources.
Error analysis. One source of error for FINET
were incorrect CG labels. When CG labels were
correct (by Stanford NER), the precision of FI-
NET for FG types increased to more than 70% for
all datasets. When FG labels were correct, the pre-
</bodyText>
<page confidence="0.969628">
875
</page>
<bodyText confidence="0.985689725490196">
Last used extractor Entities P individually using an existing type system. FI-
NET draws from ideas used in taxonomy induc-
tion or KB construction. Existing systems are ei-
ther based on patterns or the distributional hypoth-
esis; these two approaches are discussed and com-
pared in (Shi et al., 2010). In FINET, we make
use of patterns (such as the ones of Hearst (1992))
in most of our extractors and of the distributional
hypothesis in our corpus-based extractor.
Yahya et al. (2014) developed a semi-
supervised method to extract facts such as “presi-
dent”(“Barack Obama”, “US”), in which the rela-
tion acts as a type. FINET differs in that it supports
implicit types and produces disambiguated types.
A number of NET systems have been proposed
which make use of a predefined type hierarchy.
Lin et al. (2012) proposes a semi-supervised sys-
tem that uses relational patterns to propagate type
information from a KB to entity mentions. Sim-
ilarly, the subsequent Pearl system (Nakashole et
al., 2013) is based on a corpus of typed relation
patterns. An alternative approach is taken by su-
pervised methods, which train classifiers based on
linguistic features (Fleischman and Hovy, 2002;
Rahman and Ng, 2010; Ling and Weld, 2012).
Both Yosef et al. (2013) and Ling and Weld (2012)
use Wikipedia and a KB to generate automatic
training data. FINET is less reliant on a KB or
training data than the above methods, which im-
proves both precision (no bias against KB types)
and recall (more fine-grained types supported).
Our type selection phase is based on
WSD (Navigli, 2012), a classification task
where words or phrases are disambiguated against
senses from some external resource such as
WordNet. Supervised WSD systems (Dang and
Palmer, 2005; Dligach and Palmer, 2008; Chen
and Palmer, 2009; Zhong and Ng, 2010) use a
classifier to assign such senses, mostly relying on
manually annotated data. KB methods (Agirre
and Soroa, 2009; Ponzetto and Navigli, 2010;
Miller et al., 2012; Agirre et al., 2014; Del Corro
et al., 2014) use of a background KB instead.
6 Conclusion
We presented FINET, a system for fine-grained
typing of named entities in context. FINET gener-
ates candidates using multiple extractors, ranging
from explicitly mentioned to implicit types, and
subsequently selects the most appropriate. Our ex-
perimental study indicates that FINET has signifi-
cantly better performance than previous methods.
</bodyText>
<table confidence="0.731157222222222">
Pattern-based 180 71.11
Mention-based 219 82.65
Verb-based 47 48.94
Corpus-based 205 64.39
Table 4: Per-extractor performance NYT
cision of SFG labels exceeded 90%.
Incompleteness of and noise in our underlying
resources also affected precision. For example,
some types in WordNet have missing hypernyms,
</table>
<bodyText confidence="0.986293282051282">
which reduced recall; e.g., sponsor in WordNet is a
person but cannot be an organization. WordNet is
also biased towards US types (e.g., supreme court
only refers to the US institution). Our repositories
of verbs and their argument types are incomplete
and noisy as well. Finally, errors in the KB af-
fected both KB lookups and our corpus-based ex-
tractor. One example of such errors are tempo-
ral discrepancies; e.g., a person who used to be a
(player-1) may now be a (coach-1). The KB types
are also noisy, e.g., many soccer players in Yago2
are typed as (football player-1) and the United
Nations is typed as a (nation-1).
Finally, the type selection phase of FINET in-
troduced mistakes (i.e., even when the correct type
was a candidate, type selection failed to select it).
This is especially visible in the verb-based extrac-
tor, which may produce a large number of candi-
dates and thus makes type selection difficult.
Hyena mainly suffered from the general prob-
lems of supervised systems. For instance, since
(graduate-1) or (region-1) are highly frequent in
the KB, many persons (locations) were incorrectly
typed as (graduate-1) ((region-1)). Errors in the
KB also propagate in supervised system, which
may lead to “contradictory” types (i.e., an entity
being typed as both (person-1) and (location-1)).
5 Related Work
The NET problem is related to taxonomy induc- 876
tion (Snow et al., 2006; Wu et al., 2012; Shi et
al., 2010; Velardi et al., 2013) and KB construc-
tion (Lee et al., 2013; Paulheim and Bizer, 2014;
Mitchell et al., 2015), although the goals are dif-
ferent. Taxonomy induction aims to produce or
extend a taxonomy of types, whereas KB con-
struction methods aim to find new types for the
entities present in a KB. In both cases, this is done
by reasoning over a large corpus. In contrast, we
are interested in typing each named entity mention
</bodyText>
<note confidence="0.93729975">
Johannes Hoffart, Fabian M. Suchanek, Klaus
Berberich, and Gerhard Weikum. 2013. Yago2: A
spatially and temporally enhanced knowledge base
from wikipedia. Artif. Intell., 194:28–61.
</note>
<sectionHeader confidence="0.570519" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.849369333333333">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
pagerank for word sense disambiguation. In Pro-
ceedings of EACL, pages 33–41.
</bodyText>
<reference confidence="0.998389104166666">
Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.
2014. Random walks for knowledge-based word
sense disambiguation. Computational Linguistics,
40(1):57–84.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The berkeley framenet project. In Proceed-
ings of ACL, pages 86–90.
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of IJCAI, pages 805–810.
Jinying Chen and Martha Palmer. 2009. Improving
english verb sense disambiguation performance with
linguistically motivated features and clear sense dis-
tinction boundaries. Language Resources and Eval-
uation, 43(2):181–208.
Hoa Trang Dang and Martha Palmer. 2005. The role
of semantic roles in disambiguating verb senses. In
Proceedings of ACL, pages 42–49.
Luciano Del Corro and Rainer Gemulla. 2013.
Clausie: clause-based open information extraction.
In Proceedings of WWW, pages 355–366.
Luciano Del Corro, Rainer Gemulla, and Gerhard
Weikum. 2014. Werdy: Recognition and disam-
biguation of verbs and verb phrases with syntactic
and semantic pruning. In Proceedings of EMNLP,
pages 374–385.
Dmitriy Dligach and Martha Palmer. 2008. Improv-
ing verb sense disambiguation with automatically
retrieved semantic knowledge. In Proceedings of
ICSC, pages 182–189.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of ACL, pages 363–370.
Tiziano Flati and Roberto Navigli. 2013. Spred:
Large-scale harvesting of semantic predicates. In
Proceedings of ACL, pages 1222–1232.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of COLING, pages 1–7.
Patrick Hanks. 1996. Contextual dependency and lex-
ical sets. International Journal of Corpus Linguis-
tics, 1(1):75–98.
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
COLING, pages 539–545.
Johannes Hoffart, Dragan Milchevski, and Gerhard
Weikum. 2014. Stics: Searching with strings,
things, and cats. In Proceedings of SIGIR (demo),
SIGIR ’14, pages 1247–1248.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90 In Proceedings of HLT-NAACL (Companion
Volume), pages 57–60.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Evalua-
tion, 42(1):21–40.
Shari Landes, Claudia Leacock, and Randee I. Tengi,
1998. Building Semantic Concordances. MIT
Press.
Taesung Lee, Zhongyuan Wang, Haixun Wang, and
Seung-won Hwang. 2013. Attribute extraction and
scoring: A probabilistic approach. In Proceedings
of ICDE, pages 194–205.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A Preliminary Investigation. University of
Chicago Press.
Thomas Lin, Mausam, and Oren Etzioni. 2012. No
noun phrase left behind: Detecting and typing un-
linkable entities. In Proceedings of EMNLP, pages
893–903.
Xiao Ling and Daniel S. Weld. 2012. Fine-grained
entity recognition. In In Proceedings of AAAI.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
Tristan Miller, Chris Biemann, Torsten Zesch, and
Iryna Gurevych. 2012. Using distributional similar-
ity for lexical expansion in knowledge-based word
sense disambiguation. In Proceedings of COLING,
pages 1781–1796.
Tom Mitchell, William Cohen, Estevam Hruscha,
Partha Talukdar, Justin Betteridge, Andrew Carlson,
Bhavana Dalvi, Matthew Gardner, Bryan Kisiel,
Jayant Krishnamurthy, Ni Lao, Kathryn Mazaitis,
Thahir Mohammad, Ndapa Nakashole, Emmanouil
Platanios, Alan Ritter, Mehdi Samadi, Burr Settles,
Richard Wang, Derry Wijaya, Abhinav Gupta, Xin-
lei Chen, Abulhair Saparov, Malcolm Greaves, and
Joel Welling. 2015. Never-ending learning. In Pro-
ceedings of AAAI, pages 2302–2310.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. Patty: A taxonomy of relational
patterns with semantic types. In Proceedings of
EMNLP, pages 1135–1145.
</reference>
<page confidence="0.978507">
877
</page>
<reference confidence="0.996791275862069">
Ndapandula Nakashole, Tomasz Tylenda, and Gerhard
Weikum. 2013. Fine-grained semantic typing of
emerging entities. In Proceedings of ACL, pages
1488–1497.
Roberto Navigli. 2012. A quick tour of word sense dis-
ambiguation, induction and related approaches. In
Proceedings of SOFSEM, pages 115–129.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Heiko Paulheim and Christian Bizer. 2014. Improving
the quality of linked data using statistical distribu-
tions. IJSWIS, 10(2):63–86.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich word sense disambiguation rivaling
supervised systems. In Proceedings of ACL, pages
1522–1531.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman.
Altaf Rahman and Vincent Ng. 2010. Inducing fine-
grained semantic classes via hierarchical and col-
lective classification. In Proceedings of COLING,
pages 931–939.
Marta Recasens, Marie C. de Marneffe, and Christo-
pher Potts. 2013. The Life and Death of Discourse
Entities: Identifying Singleton Mentions. In Pro-
ceedings of HLT-NAACL, pages 627–633.
David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.
Williams. 1988. Neurocomputing: Foundations
of research. chapter Learning Representations by
Back-propagating Errors, pages 696–699.
Magnus Sahlgren. 2008. The distributional hypothe-
sis. Italian Journal of Linguistics, 20(1):33–54.
Evan Sandhaus. 2008. The New York Times
Annotated Corpus. Linguistic Data Consortium,
Philadelphia, 6(12).
Shuming Shi, Huibin Zhang, Xiaojie Yuan, and Ji-
Rong Wen. 2010. Corpus-based semantic class
mining: Distributional vs. pattern-based approaches.
In Proceedings of COLING, pages 993–1001.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous
evidence. In Proceedings of COLING-ACL, pages
801–808.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compo-
sitional vector grammars. In Proceedings of ACL,
pages 455–465.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the conll-2003 shared task:
Language-independent named entity recognition. In
Proceedings of HLT-NAACL, pages 142–147.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2013. Ontolearn reloaded: A graph-based algorithm
for taxonomy induction. Computational Linguistics,
39(3):665–707.
Ralph Weischedel and Ada Brunstein. 2005. BBN
Pronoun Coreference and Entity Type Corpus.
Technical report, Linguistic Data Consortium.
Wentao Wu, Hongsong Li, Haixun Wang, and
Kenny Q. Zhu. 2012. Probase: A probabilistic tax-
onomy for text understanding. In Proceedings of
SIGMOD, pages 481–492.
Mohamed Yahya, Klaus Berberich, Shady Elbassuoni,
and Gerhard Weikum. 2013. Robust question an-
swering over the web of linked data. In Proceedings
of CIKM, pages 1107–1116.
Mohamed Yahya, Steven Euijong Whang, Rahul
Gupta, and Alon Halevy. 2014. Renoun: Fact ex-
traction for nominal attributes. In Proceedings of
EMNLP, pages 325–335.
Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-
fart, Marc Spaniol, and Gerhard Weikum. 2012.
HYENA: Hierarchical Type Classification for Entity
Names. In Proceedings of COLING, pages 1361–
1370.
Mohamed Amir Yosef, Sandro Bauer, Johannes Hof-
fart, Marc Spaniol, and Gerhard Weikum. 2013.
HYENA-live: Fine-Grained Online Entity Type
Classification from Natural-language Text. In Pro-
ceedings ofACL, pages 133–138.
Zhi Zhong and Hwee Tou Ng. 2010. It makes sense:
A wide-coverage word sense disambiguation sys-
tem for free text. In Proceedings of ACL: System
Demonstrations, pages 78–83.
</reference>
<page confidence="0.997609">
878
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.324280">
<title confidence="0.999793">FINET: Context-Aware Fine-Grained Named Entity Typing</title>
<author confidence="0.99965">Del Abdalghani Rainer</author>
<affiliation confidence="0.583357">f¨ur Informatik, Saarbr¨ucken, abujabal,</affiliation>
<address confidence="0.964734">Mannheim, Mannheim,</address>
<email confidence="0.99932">rgemulla@uni-mannheim.de</email>
<abstract confidence="0.998047727272727">We propose FINET, a system for detecting the types of named entities in short inputs—such as sentences or tweets—with respect to WordNet’s super fine-grained type system. FINET generates candidate types using a sequence of multiple extractors, ranging from explicitly mentioned types to implicit types, and subsequently selects the most appropriate using ideas from word-sense disambiguation. FINET combats data scarcity and noise from existing systems: It does not rely on supervision in its extractors and generates training data for type selection from WordNet and other resources. FINET supports the most fine-grained type system so far, including types with no annotated training data. Our experiments indicate that FINET outperforms state-of-the-art methods in terms of recall, precision, and granularity of extracted types.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
</authors>
<title>Oier Lopez de Lacalle, and Aitor Soroa.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<marker>Agirre, 2014</marker>
<rawString>Eneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa. 2014. Random walks for knowledge-based word sense disambiguation. Computational Linguistics, 40(1):57–84.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>86--90</pages>
<contexts>
<context position="16723" citStr="Baker et al., 1998" startWordPosition="2701" endWordPosition="2704">e set of (named-entity) locations is quite static and known, we assume that the KB contains all locations and their possible types (our experiments strengthened this assumption). If a mention of a location (e.g., “Berlin”) occurs in the repository, we add all the corresponding types from the repository to the candidate set (e.g., (city- 1)) and move to the type selection phase. 2.5 Verb-based extractor Verbs have been widely exploited to determine the types or roles of its arguments: A verb sense imposes a restriction on the type of its arguments (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). For instance, from “Ted Kennedy was elected to Congress,” we infer that “Ted Kennedy” is a person who can be elected. Corresponding types include (representative-1), (representative-2), or (politician-1). Our verbbased extractor leverages this insight to extract types. The extractor operates at the clause level. A simple way to infer lexical types for entities acting as subjects or objects of a clause is nominalization, i.e., the transformation of the verb into deverbal nouns (e.g., “play” into “player”). To exploit it, we apply a set of morphologic</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley framenet project. In Proceedings of ACL, pages 86–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of IJCAI,</booktitle>
<pages>805--810</pages>
<contexts>
<context position="25798" citStr="Banerjee and Pedersen, 2003" startWordPosition="4224" endWordPosition="4227"> Here we take a combination, i.e., we leverage WordNet and manually annotated data. We train a Naive Bayes classifier to select the most appropriate type given its context. We represent context by a bag of lemmatized words. This allows us to automatically generate training data from WordNet (and use manually labeled data). Since WordNet provides information for each of the relevant types, this approach combats the data sparsity that arises with supervised systems. The context for each individual WordNet type consists of all words appearing in the type’s gloss and the glosses of its neighbors (Banerjee and Pedersen, 2003). We also include for each type the neighbors from Ponzetto and Navigli (2010) and the corresponding verbs from Del Corro and Gemulla (2013). Finally, we add all words in sentences containing the type in SemCor3 (Landes et al., 1998) and Ontonotes 5.0 (Hovy et al., 2006). We trained one classifier per CG type. To train the classifier, we create a single training point for each corresponding WordNet type and use the type’s context as features. To map the CG types from our NER system to WordNet, we considered as persons all descendants of (person1), (imaginary being-1), (characterization-3), and</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of IJCAI, pages 805–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinying Chen</author>
<author>Martha Palmer</author>
</authors>
<title>Improving english verb sense disambiguation performance with linguistically motivated features and clear sense distinction boundaries.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="38262" citStr="Chen and Palmer, 2009" startWordPosition="6318" endWordPosition="6321"> and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign such senses, mostly relying on manually annotated data. KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Del Corro et al., 2014) use of a background KB instead. 6 Conclusion We presented FINET, a system for fine-grained typing of named entities in context. FINET generates candidates using multiple extractors, ranging from explicitly mentioned to implicit types, and subsequently selects the most appropriate. Our experimental study indicates that FINET has significantly better performance th</context>
</contexts>
<marker>Chen, Palmer, 2009</marker>
<rawString>Jinying Chen and Martha Palmer. 2009. Improving english verb sense disambiguation performance with linguistically motivated features and clear sense distinction boundaries. Language Resources and Evaluation, 43(2):181–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hoa Trang Dang</author>
<author>Martha Palmer</author>
</authors>
<title>The role of semantic roles in disambiguating verb senses.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="38213" citStr="Dang and Palmer, 2005" startWordPosition="6310" endWordPosition="6313">ssifiers based on linguistic features (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign such senses, mostly relying on manually annotated data. KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Del Corro et al., 2014) use of a background KB instead. 6 Conclusion We presented FINET, a system for fine-grained typing of named entities in context. FINET generates candidates using multiple extractors, ranging from explicitly mentioned to implicit types, and subsequently selects the most appropriate. Our experimental study indicates t</context>
</contexts>
<marker>Dang, Palmer, 2005</marker>
<rawString>Hoa Trang Dang and Martha Palmer. 2005. The role of semantic roles in disambiguating verb senses. In Proceedings of ACL, pages 42–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Del Corro</author>
<author>Rainer Gemulla</author>
</authors>
<title>Clausie: clause-based open information extraction.</title>
<date>2013</date>
<booktitle>In Proceedings of WWW,</booktitle>
<pages>355--366</pages>
<contexts>
<context position="8715" citStr="Corro and Gemulla, 2013" startWordPosition="1366" endWordPosition="1369">ly specific and want to allow the selection phase to be able to select a more general type. We then also run subsequent extractors. Tab. 1 displays a summary of the extractors and stopping conditions. All so-found type candidates are passed to the candidate selection phase (Sec. 3). 2.1 Preprocessing Preprocessing consists of 5 steps: (i) dependency parsing (Socher et al., 2013); (ii) co-reference (Recasens et al., 2013); (iii) named entity recognition (NER) (Finkel et al., 2005) with the detection of coarse-grained types (i.e., person, organization, location); (iv) clause identification (Del Corro and Gemulla, 2013); (v) word and multi-word expression recognition (Del Corro et al., 2014). FINET restricts its candidates to the hyponmys of the coarse-grained (CG) type of the NER system. Named entities with the same CG type in a coordinating relation (e.g., “Messi and Ronaldo are soccer players”) and identical mentions share the candidate set; the latter is reasonable in short input. FINET extractors operate either on the sentence or the clause level. A clause is a part of a sentence that expresses a statement and is thus a suitable unit for automatic text processing (Del Corro and Gemulla, 2013). Finally, </context>
<context position="25938" citStr="Corro and Gemulla (2013)" startWordPosition="4248" endWordPosition="4251">iate type given its context. We represent context by a bag of lemmatized words. This allows us to automatically generate training data from WordNet (and use manually labeled data). Since WordNet provides information for each of the relevant types, this approach combats the data sparsity that arises with supervised systems. The context for each individual WordNet type consists of all words appearing in the type’s gloss and the glosses of its neighbors (Banerjee and Pedersen, 2003). We also include for each type the neighbors from Ponzetto and Navigli (2010) and the corresponding verbs from Del Corro and Gemulla (2013). Finally, we add all words in sentences containing the type in SemCor3 (Landes et al., 1998) and Ontonotes 5.0 (Hovy et al., 2006). We trained one classifier per CG type. To train the classifier, we create a single training point for each corresponding WordNet type and use the type’s context as features. To map the CG types from our NER system to WordNet, we considered as persons all descendants of (person1), (imaginary being-1), (characterization-3), and (operator-2) (10584 in total); as locations all descendants of (location-1), (way-1), and (landmass-1) (3681 in total); and as organization</context>
</contexts>
<marker>Corro, Gemulla, 2013</marker>
<rawString>Luciano Del Corro and Rainer Gemulla. 2013. Clausie: clause-based open information extraction. In Proceedings of WWW, pages 355–366.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luciano Del Corro</author>
<author>Rainer Gemulla</author>
<author>Gerhard Weikum</author>
</authors>
<title>Werdy: Recognition and disambiguation of verbs and verb phrases with syntactic and semantic pruning.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>374--385</pages>
<contexts>
<context position="8788" citStr="Corro et al., 2014" startWordPosition="1378" endWordPosition="1381">general type. We then also run subsequent extractors. Tab. 1 displays a summary of the extractors and stopping conditions. All so-found type candidates are passed to the candidate selection phase (Sec. 3). 2.1 Preprocessing Preprocessing consists of 5 steps: (i) dependency parsing (Socher et al., 2013); (ii) co-reference (Recasens et al., 2013); (iii) named entity recognition (NER) (Finkel et al., 2005) with the detection of coarse-grained types (i.e., person, organization, location); (iv) clause identification (Del Corro and Gemulla, 2013); (v) word and multi-word expression recognition (Del Corro et al., 2014). FINET restricts its candidates to the hyponmys of the coarse-grained (CG) type of the NER system. Named entities with the same CG type in a coordinating relation (e.g., “Messi and Ronaldo are soccer players”) and identical mentions share the candidate set; the latter is reasonable in short input. FINET extractors operate either on the sentence or the clause level. A clause is a part of a sentence that expresses a statement and is thus a suitable unit for automatic text processing (Del Corro and Gemulla, 2013). Finally, we identify multiword explicit type mentions such as Prime Minister or Se</context>
<context position="19115" citStr="Corro et al. (2014)" startWordPosition="3108" endWordPosition="3111">un modifier of the deverbal noun (e.g., From “Messi plays soccer”, we form “soccer player”). If it exists in WordNet, we consider the respective types as potential candidates as well. A more indirect way of exploiting the verb-type semantic concordance is via a corpus of frequent (verb, type)-pairs, where the type refers to possible types of the verb’s subject or object. As stated above, the set of argument types compatible with a verb is limited. For instance, “treat” is usually followed by (condition-1), (disease-1), or (patient1). FINET, uses the corpora of Flati and Navigli (2013) and Del Corro et al. (2014). Given a verb and an entity, we search for frequent candidate types (depending on whether the entity acts as a subject or object). For example, from “Messi was treated in the hospital,” we obtain (patient-1). Once potential candidates have been collected, we perform a KB lookup to decide how to proceed. 2.6 Corpus-based extractor Our final extractor leverages a large unlabeled corpus to find entities that co-occur in similar contexts. It is based on the distributional hypothesis (Sahlgren, 2008): similar entities tend to occur in similar contexts. For example, “Messi” and “Cristiano Ronaldo” </context>
<context position="38496" citStr="Corro et al., 2014" startWordPosition="6358" endWordPosition="6361">ods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign such senses, mostly relying on manually annotated data. KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Del Corro et al., 2014) use of a background KB instead. 6 Conclusion We presented FINET, a system for fine-grained typing of named entities in context. FINET generates candidates using multiple extractors, ranging from explicitly mentioned to implicit types, and subsequently selects the most appropriate. Our experimental study indicates that FINET has significantly better performance than previous methods. Pattern-based 180 71.11 Mention-based 219 82.65 Verb-based 47 48.94 Corpus-based 205 64.39 Table 4: Per-extractor performance NYT cision of SFG labels exceeded 90%. Incompleteness of and noise in our underlying re</context>
</contexts>
<marker>Corro, Gemulla, Weikum, 2014</marker>
<rawString>Luciano Del Corro, Rainer Gemulla, and Gerhard Weikum. 2014. Werdy: Recognition and disambiguation of verbs and verb phrases with syntactic and semantic pruning. In Proceedings of EMNLP, pages 374–385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>Improving verb sense disambiguation with automatically retrieved semantic knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of ICSC,</booktitle>
<pages>182--189</pages>
<contexts>
<context position="38239" citStr="Dligach and Palmer, 2008" startWordPosition="6314" endWordPosition="6317">istic features (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign such senses, mostly relying on manually annotated data. KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Del Corro et al., 2014) use of a background KB instead. 6 Conclusion We presented FINET, a system for fine-grained typing of named entities in context. FINET generates candidates using multiple extractors, ranging from explicitly mentioned to implicit types, and subsequently selects the most appropriate. Our experimental study indicates that FINET has significantl</context>
</contexts>
<marker>Dligach, Palmer, 2008</marker>
<rawString>Dmitriy Dligach and Martha Palmer. 2008. Improving verb sense disambiguation with automatically retrieved semantic knowledge. In Proceedings of ICSC, pages 182–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="8575" citStr="Finkel et al., 2005" startWordPosition="1348" endWordPosition="1351">s. When the condition is not met, we enrich the set of candidate types of the extractor with their hypernyms; we expect types to be overly specific and want to allow the selection phase to be able to select a more general type. We then also run subsequent extractors. Tab. 1 displays a summary of the extractors and stopping conditions. All so-found type candidates are passed to the candidate selection phase (Sec. 3). 2.1 Preprocessing Preprocessing consists of 5 steps: (i) dependency parsing (Socher et al., 2013); (ii) co-reference (Recasens et al., 2013); (iii) named entity recognition (NER) (Finkel et al., 2005) with the detection of coarse-grained types (i.e., person, organization, location); (iv) clause identification (Del Corro and Gemulla, 2013); (v) word and multi-word expression recognition (Del Corro et al., 2014). FINET restricts its candidates to the hyponmys of the coarse-grained (CG) type of the NER system. Named entities with the same CG type in a coordinating relation (e.g., “Messi and Ronaldo are soccer players”) and identical mentions share the candidate set; the latter is reasonable in short input. FINET extractors operate either on the sentence or the clause level. A clause is a part</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of ACL, pages 363–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiziano Flati</author>
<author>Roberto Navigli</author>
</authors>
<title>Spred: Large-scale harvesting of semantic predicates.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1222--1232</pages>
<contexts>
<context position="19087" citStr="Flati and Navigli (2013)" startWordPosition="3101" endWordPosition="3105"> adding the direct object as a noun modifier of the deverbal noun (e.g., From “Messi plays soccer”, we form “soccer player”). If it exists in WordNet, we consider the respective types as potential candidates as well. A more indirect way of exploiting the verb-type semantic concordance is via a corpus of frequent (verb, type)-pairs, where the type refers to possible types of the verb’s subject or object. As stated above, the set of argument types compatible with a verb is limited. For instance, “treat” is usually followed by (condition-1), (disease-1), or (patient1). FINET, uses the corpora of Flati and Navigli (2013) and Del Corro et al. (2014). Given a verb and an entity, we search for frequent candidate types (depending on whether the entity acts as a subject or object). For example, from “Messi was treated in the hospital,” we obtain (patient-1). Once potential candidates have been collected, we perform a KB lookup to decide how to proceed. 2.6 Corpus-based extractor Our final extractor leverages a large unlabeled corpus to find entities that co-occur in similar contexts. It is based on the distributional hypothesis (Sahlgren, 2008): similar entities tend to occur in similar contexts. For example, “Mes</context>
</contexts>
<marker>Flati, Navigli, 2013</marker>
<rawString>Tiziano Flati and Roberto Navigli. 2013. Spred: Large-scale harvesting of semantic predicates. In Proceedings of ACL, pages 1222–1232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Eduard Hovy</author>
</authors>
<title>Fine grained classification of named entities.</title>
<date>2002</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="28093" citStr="Fleischman and Hovy, 2002" startWordPosition="4571" endWordPosition="4574">od that uses a hierarchical classifier. Its features include the words in the named entity mention, in sentence and paragraph, and POS tags. It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories (artifact-1), (event1), (person-1), (location-1), and (organization1). Hyena outperformed a number of previous systems (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). We used Hyena via its web service (Yosef et al., 2013). Pearl (Nakashole et al., 2013) is a semisupervised system that leverages a repository of 300k relational patterns (Nakashole et al., 2012). Subjects and objects of each pattern carry type information. Pearl types named entity mentions by the most likely type according to its pattern database. Pearl’s type system is based on around 200 “interesting” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without th</context>
<context position="37656" citStr="Fleischman and Hovy, 2002" startWordPosition="6217" endWordPosition="6220">(“Barack Obama”, “US”), in which the relation acts as a type. FINET differs in that it supports implicit types and produces disambiguated types. A number of NET systems have been proposed which make use of a predefined type hierarchy. Lin et al. (2012) proposes a semi-supervised system that uses relational patterns to propagate type information from a KB to entity mentions. Similarly, the subsequent Pearl system (Nakashole et al., 2013) is based on a corpus of typed relation patterns. An alternative approach is taken by supervised methods, which train classifiers based on linguistic features (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer</context>
</contexts>
<marker>Fleischman, Hovy, 2002</marker>
<rawString>Michael Fleischman and Eduard Hovy. 2002. Fine grained classification of named entities. In Proceedings of COLING, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Hanks</author>
</authors>
<title>Contextual dependency and lexical sets.</title>
<date>1996</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="16703" citStr="Hanks, 1996" startWordPosition="2699" endWordPosition="2700">ion. Since the set of (named-entity) locations is quite static and known, we assume that the KB contains all locations and their possible types (our experiments strengthened this assumption). If a mention of a location (e.g., “Berlin”) occurs in the repository, we add all the corresponding types from the repository to the candidate set (e.g., (city- 1)) and move to the type selection phase. 2.5 Verb-based extractor Verbs have been widely exploited to determine the types or roles of its arguments: A verb sense imposes a restriction on the type of its arguments (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). For instance, from “Ted Kennedy was elected to Congress,” we infer that “Ted Kennedy” is a person who can be elected. Corresponding types include (representative-1), (representative-2), or (politician-1). Our verbbased extractor leverages this insight to extract types. The extractor operates at the clause level. A simple way to infer lexical types for entities acting as subjects or objects of a clause is nominalization, i.e., the transformation of the verb into deverbal nouns (e.g., “play” into “player”). To exploit it, we apply </context>
</contexts>
<marker>Hanks, 1996</marker>
<rawString>Patrick Hanks. 1996. Contextual dependency and lexical sets. International Journal of Corpus Linguistics, 1(1):75–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>539--545</pages>
<contexts>
<context position="2673" citStr="Hearst, 1992" startWordPosition="400" endWordPosition="401">et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Explicit types occur, for example, in phrases such as “Steinmeier, the German Foreign Minister, [...]” or “Foreign Minister Steinmeier.” These explicit types are often extracted via patterns, such as the well-known Hearst patterns (Hearst, 1992), and subsequently integrated into a taxonomy. Patternbased methods often have high precision but low recall: Types are usually mentioned when a named entity is introduced or expected to be unknown to readers, but often are not explicitly stated. The NET problem differs from taxonomy induction in that (1) the type system is prespecified, (2) types are disambiguated, and (3) types are associated with each occurrence of named entity in context. Our FINET system makes use of explicit type extractions whenever possible. But even when types are not explicitly mentioned, sentences may give clues to </context>
<context position="9792" citStr="Hearst, 1992" startWordPosition="1546" endWordPosition="1547">a sentence that expresses a statement and is thus a suitable unit for automatic text processing (Del Corro and Gemulla, 2013). Finally, we identify multiword explicit type mentions such as Prime Minister or Secretary of Housing and Urban Development (Del Corro et al., 2014). 2.2 Pattern-based extractor Our pattern-based extractor targets explicit type mentions. These are commonly used to introduce entities when they first appear (“US President Barack Obama”) or when their mention does not refer to the most prominent entity (“Barack Obama, father of the US President”). Following previous work (Hearst, 1992), we use a set of patterns to look for expressions that may refer named entity types. We refer to those expressions as lexical types (e.g., “father”). Once lexical types have been identified, we collect as candidate types the WordNet synsets to which they refer (e.g., (father1), ..., (father-8), the eight senses of “father”). Our extractor makes use of both syntactic patterns, which operate on the dependency parse, and regular expression patterns, which operate on the text. Syntactic patterns are preferable in that they do not rely on continuous chunks of text and can skip non-relevant informa</context>
<context position="36845" citStr="Hearst (1992)" startWordPosition="6088" endWordPosition="6089">sources. Error analysis. One source of error for FINET were incorrect CG labels. When CG labels were correct (by Stanford NER), the precision of FINET for FG types increased to more than 70% for all datasets. When FG labels were correct, the pre875 Last used extractor Entities P individually using an existing type system. FINET draws from ideas used in taxonomy induction or KB construction. Existing systems are either based on patterns or the distributional hypothesis; these two approaches are discussed and compared in (Shi et al., 2010). In FINET, we make use of patterns (such as the ones of Hearst (1992)) in most of our extractors and of the distributional hypothesis in our corpus-based extractor. Yahya et al. (2014) developed a semisupervised method to extract facts such as “president”(“Barack Obama”, “US”), in which the relation acts as a type. FINET differs in that it supports implicit types and produces disambiguated types. A number of NET systems have been proposed which make use of a predefined type hierarchy. Lin et al. (2012) proposes a semi-supervised system that uses relational patterns to propagate type information from a KB to entity mentions. Similarly, the subsequent Pearl syste</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of COLING, pages 539–545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Dragan Milchevski</author>
<author>Gerhard Weikum</author>
</authors>
<title>Stics: Searching with strings, things, and cats.</title>
<date>2014</date>
<booktitle>In Proceedings of SIGIR (demo), SIGIR ’14,</booktitle>
<pages>1247--1248</pages>
<contexts>
<context position="2073" citStr="Hoffart et al., 2014" startWordPosition="305" endWordPosition="308">rt inputs—such as sentences or tweets—with respect to WordNet’s super fine-grained type system (16k types of organizations, persons and locations). Named entity typing is a fundamental building block for many natural-language processing tasks. NET is at the heart of information extraction methods for finding types for entities in a knowledge base1 (KB) (Mitchell et al., 2015). Likewise, NET aids named entity disambiguation by reducing the candidate space for a given entity mention. Entity types are an important resource for entity-based retrieval or aggregation tasks, such as semantic search (Hoffart et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Explicit types occur, for example, in phrases such as “Steinmeier, the German Foreign Minister, [...]” or “Foreign Minister Steinmeier.” These explicit types are often extracted via patterns, such as the well-known Hearst patterns (Hearst, 1992)</context>
</contexts>
<marker>Hoffart, Milchevski, Weikum, 2014</marker>
<rawString>Johannes Hoffart, Dragan Milchevski, and Gerhard Weikum. 2014. Stics: Searching with strings, things, and cats. In Proceedings of SIGIR (demo), SIGIR ’14, pages 1247–1248.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<date>2006</date>
<booktitle>Ontonotes: The 90 In Proceedings of HLT-NAACL (Companion Volume),</booktitle>
<pages>57--60</pages>
<contexts>
<context position="26069" citStr="Hovy et al., 2006" startWordPosition="4272" endWordPosition="4275">om WordNet (and use manually labeled data). Since WordNet provides information for each of the relevant types, this approach combats the data sparsity that arises with supervised systems. The context for each individual WordNet type consists of all words appearing in the type’s gloss and the glosses of its neighbors (Banerjee and Pedersen, 2003). We also include for each type the neighbors from Ponzetto and Navigli (2010) and the corresponding verbs from Del Corro and Gemulla (2013). Finally, we add all words in sentences containing the type in SemCor3 (Landes et al., 1998) and Ontonotes 5.0 (Hovy et al., 2006). We trained one classifier per CG type. To train the classifier, we create a single training point for each corresponding WordNet type and use the type’s context as features. To map the CG types from our NER system to WordNet, we considered as persons all descendants of (person1), (imaginary being-1), (characterization-3), and (operator-2) (10584 in total); as locations all descendants of (location-1), (way-1), and (landmass-1) (3681 in total); and as organizations all descendants of (organization-1) and (social group-1) (1968 in total). This approach of handling CG types suffers to some exte</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90 In Proceedings of HLT-NAACL (Companion Volume), pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karin Kipper</author>
<author>Anna Korhonen</author>
<author>Neville Ryant</author>
<author>Martha Palmer</author>
</authors>
<title>A large-scale classification of English verbs.</title>
<date>2008</date>
<journal>Language Resources and Evaluation,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="16766" citStr="Kipper et al., 2008" startWordPosition="2709" endWordPosition="2712">e static and known, we assume that the KB contains all locations and their possible types (our experiments strengthened this assumption). If a mention of a location (e.g., “Berlin”) occurs in the repository, we add all the corresponding types from the repository to the candidate set (e.g., (city- 1)) and move to the type selection phase. 2.5 Verb-based extractor Verbs have been widely exploited to determine the types or roles of its arguments: A verb sense imposes a restriction on the type of its arguments (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). For instance, from “Ted Kennedy was elected to Congress,” we infer that “Ted Kennedy” is a person who can be elected. Corresponding types include (representative-1), (representative-2), or (politician-1). Our verbbased extractor leverages this insight to extract types. The extractor operates at the clause level. A simple way to infer lexical types for entities acting as subjects or objects of a clause is nominalization, i.e., the transformation of the verb into deverbal nouns (e.g., “play” into “player”). To exploit it, we apply a set of morphological transformations to the verb (Quirk et al</context>
</contexts>
<marker>Kipper, Korhonen, Ryant, Palmer, 2008</marker>
<rawString>Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A large-scale classification of English verbs. Language Resources and Evaluation, 42(1):21–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shari Landes</author>
<author>Claudia Leacock</author>
<author>Randee I Tengi</author>
</authors>
<title>Building Semantic Concordances.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="26031" citStr="Landes et al., 1998" startWordPosition="4265" endWordPosition="4268"> automatically generate training data from WordNet (and use manually labeled data). Since WordNet provides information for each of the relevant types, this approach combats the data sparsity that arises with supervised systems. The context for each individual WordNet type consists of all words appearing in the type’s gloss and the glosses of its neighbors (Banerjee and Pedersen, 2003). We also include for each type the neighbors from Ponzetto and Navigli (2010) and the corresponding verbs from Del Corro and Gemulla (2013). Finally, we add all words in sentences containing the type in SemCor3 (Landes et al., 1998) and Ontonotes 5.0 (Hovy et al., 2006). We trained one classifier per CG type. To train the classifier, we create a single training point for each corresponding WordNet type and use the type’s context as features. To map the CG types from our NER system to WordNet, we considered as persons all descendants of (person1), (imaginary being-1), (characterization-3), and (operator-2) (10584 in total); as locations all descendants of (location-1), (way-1), and (landmass-1) (3681 in total); and as organizations all descendants of (organization-1) and (social group-1) (1968 in total). This approach of </context>
</contexts>
<marker>Landes, Leacock, Tengi, 1998</marker>
<rawString>Shari Landes, Claudia Leacock, and Randee I. Tengi, 1998. Building Semantic Concordances. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taesung Lee</author>
<author>Zhongyuan Wang</author>
<author>Haixun Wang</author>
<author>Seung-won Hwang</author>
</authors>
<title>Attribute extraction and scoring: A probabilistic approach.</title>
<date>2013</date>
<booktitle>In Proceedings of ICDE,</booktitle>
<pages>194--205</pages>
<contexts>
<context position="40678" citStr="Lee et al., 2013" startWordPosition="6709" endWordPosition="6712">es and thus makes type selection difficult. Hyena mainly suffered from the general problems of supervised systems. For instance, since (graduate-1) or (region-1) are highly frequent in the KB, many persons (locations) were incorrectly typed as (graduate-1) ((region-1)). Errors in the KB also propagate in supervised system, which may lead to “contradictory” types (i.e., an entity being typed as both (person-1) and (location-1)). 5 Related Work The NET problem is related to taxonomy induc- 876 tion (Snow et al., 2006; Wu et al., 2012; Shi et al., 2010; Velardi et al., 2013) and KB construction (Lee et al., 2013; Paulheim and Bizer, 2014; Mitchell et al., 2015), although the goals are different. Taxonomy induction aims to produce or extend a taxonomy of types, whereas KB construction methods aim to find new types for the entities present in a KB. In both cases, this is done by reasoning over a large corpus. In contrast, we are interested in typing each named entity mention Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: A spatially and temporally enhanced knowledge base from wikipedia. Artif. Intell., 194:28–61. References Eneko Agirre and Aitor Soroa. 2009. Pe</context>
</contexts>
<marker>Lee, Wang, Wang, Hwang, 2013</marker>
<rawString>Taesung Lee, Zhongyuan Wang, Haixun Wang, and Seung-won Hwang. 2013. Attribute extraction and scoring: A probabilistic approach. In Proceedings of ICDE, pages 194–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="16690" citStr="Levin, 1993" startWordPosition="2697" endWordPosition="2698">gged as location. Since the set of (named-entity) locations is quite static and known, we assume that the KB contains all locations and their possible types (our experiments strengthened this assumption). If a mention of a location (e.g., “Berlin”) occurs in the repository, we add all the corresponding types from the repository to the candidate set (e.g., (city- 1)) and move to the type selection phase. 2.5 Verb-based extractor Verbs have been widely exploited to determine the types or roles of its arguments: A verb sense imposes a restriction on the type of its arguments (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). For instance, from “Ted Kennedy was elected to Congress,” we infer that “Ted Kennedy” is a person who can be elected. Corresponding types include (representative-1), (representative-2), or (politician-1). Our verbbased extractor leverages this insight to extract types. The extractor operates at the clause level. A simple way to infer lexical types for entities acting as subjects or objects of a clause is nominalization, i.e., the transformation of the verb into deverbal nouns (e.g., “play” into “player”). To exploit </context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>Beth Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>No noun phrase left behind: Detecting and typing unlinkable entities.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>893--903</pages>
<contexts>
<context position="2282" citStr="Lin et al., 2012" startWordPosition="339" endWordPosition="342">ral-language processing tasks. NET is at the heart of information extraction methods for finding types for entities in a knowledge base1 (KB) (Mitchell et al., 2015). Likewise, NET aids named entity disambiguation by reducing the candidate space for a given entity mention. Entity types are an important resource for entity-based retrieval or aggregation tasks, such as semantic search (Hoffart et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Explicit types occur, for example, in phrases such as “Steinmeier, the German Foreign Minister, [...]” or “Foreign Minister Steinmeier.” These explicit types are often extracted via patterns, such as the well-known Hearst patterns (Hearst, 1992), and subsequently integrated into a taxonomy. Patternbased methods often have high precision but low recall: Types are usually mentioned when a named entity is introduced or expected to be unknown to readers,</context>
<context position="37283" citStr="Lin et al. (2012)" startWordPosition="6159" endWordPosition="6162">terns or the distributional hypothesis; these two approaches are discussed and compared in (Shi et al., 2010). In FINET, we make use of patterns (such as the ones of Hearst (1992)) in most of our extractors and of the distributional hypothesis in our corpus-based extractor. Yahya et al. (2014) developed a semisupervised method to extract facts such as “president”(“Barack Obama”, “US”), in which the relation acts as a type. FINET differs in that it supports implicit types and produces disambiguated types. A number of NET systems have been proposed which make use of a predefined type hierarchy. Lin et al. (2012) proposes a semi-supervised system that uses relational patterns to propagate type information from a KB to entity mentions. Similarly, the subsequent Pearl system (Nakashole et al., 2013) is based on a corpus of typed relation patterns. An alternative approach is taken by supervised methods, which train classifiers based on linguistic features (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, w</context>
</contexts>
<marker>Lin, Mausam, Etzioni, 2012</marker>
<rawString>Thomas Lin, Mausam, and Oren Etzioni. 2012. No noun phrase left behind: Detecting and typing unlinkable entities. In Proceedings of EMNLP, pages 893–903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Ling</author>
<author>Daniel S Weld</author>
</authors>
<title>Fine-grained entity recognition. In</title>
<date>2012</date>
<booktitle>In Proceedings of AAAI.</booktitle>
<contexts>
<context position="27762" citStr="Ling and Weld (2012)" startWordPosition="4522" endWordPosition="4525"> any specific dataset. All datasets, detected types, labels, and our source code are publicly available.4 3http://web.eecs.umich.edu/˜mihalcea/ downloads.html 4http://dws.informatik.uni-mannheim. de/en/resources/software/finet/ 873 4.1 Experimental Setup Methods. Hyena (Yosef et al., 2012) is a representative supervised method that uses a hierarchical classifier. Its features include the words in the named entity mention, in sentence and paragraph, and POS tags. It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories (artifact-1), (event1), (person-1), (location-1), and (organization1). Hyena outperformed a number of previous systems (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). We used Hyena via its web service (Yosef et al., 2013). Pearl (Nakashole et al., 2013) is a semisupervised system that leverages a repository of 300k relational patterns (Nakashole et al., 2012). Subjects and objects of each</context>
<context position="29134" citStr="Ling and Weld, 2012" startWordPosition="4750" endWordPosition="4753">ed on around 200 “interesting” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without the KB lookup. This allows us to estimate the extent to which referring to a KB helps. Note that the corpus-based extractor makes use of the KB in both settings. Datasets. We used three different datasets representing real-world use cases. We created two datasets, New York Times and Twitter, and sampled a subset of the CoNLL data, which provides gold annotations for CG types. We did not consider datasets such as FIGER (Ling and Weld, 2012) or BBN (Weischedel and Brunstein, 2005) because they are not suitable for very fine-grained typing. New York Times consists of 500 random sentences from the New York Times corpus (Sandhaus, 2008), year 2007; we selected only sentences that contained at least one named entity according to the Stanford CoreNLP 4.4.1 tool. CoNLL. We sampled 500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003), a collection of newswires with manually annotated entities with CG types labels. We directly used the annotations in our evaluation. The sentences tend to be short and sometimes non-verbal (e.g.,</context>
<context position="37699" citStr="Ling and Weld, 2012" startWordPosition="6225" endWordPosition="6228">ts as a type. FINET differs in that it supports implicit types and produces disambiguated types. A number of NET systems have been proposed which make use of a predefined type hierarchy. Lin et al. (2012) proposes a semi-supervised system that uses relational patterns to propagate type information from a KB to entity mentions. Similarly, the subsequent Pearl system (Nakashole et al., 2013) is based on a corpus of typed relation patterns. An alternative approach is taken by supervised methods, which train classifiers based on linguistic features (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifie</context>
</contexts>
<marker>Ling, Weld, 2012</marker>
<rawString>Xiao Ling and Daniel S. Weld. 2012. Fine-grained entity recognition. In In Proceedings of AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="5735" citStr="Mikolov et al., 2013" startWordPosition="891" endWordPosition="894">enerally problematic when types are correlated (e.g., most presidents are also graduates and authors). FINET addresses the above problems by first generating a set of type candidates using multiple extractors and then selecting the most appropriate type(s). To generate candidates, we make use of a sequence of extractors that range from explicit to highly implicit. Implicit extractors are only used when more explicit extractors fail to produce a good type. Our extractors are based on patterns, mention text, and verbal phrases. To additionally extract highly implicit types, we use word vectors (Mikolov et al., 2013) trained on a large unlabeled corpus to determine the types of similar entities that appear in similar contexts. This extractor is comparable to KB methods discussed above, but is unsupervised, and takes as candidates the types frequent within the related entities and contexts. After type candidates have been generated, the final step of FINET selects the types that best fit the context. In this step, we leverage previous work on word sense disambiguation (WSD) and resources such as WordNet glosses, and, if available, manually annotated training data. FINET leverages ideas from existing system</context>
<context position="20835" citStr="Mikolov et al., 2013" startWordPosition="3397" endWordPosition="3400">of word vectors (Rumelhart et al., 1988) trained on a large unlabeled corpus. A word vector is a semantic representation of a phrase and represents the semantic context in which the phrase occurs. Phrases that are semantically related, and thus appear in similar contexts, are close to each other in the word vector space. For instance, if “Messi” and “Cristiano Ronaldo” tend to co-occur with a similar sets of words, their word vectors are close. We also may expect “Arnold Schwarzenegger” to be close to both actors and politicians, since it occurs in both contexts. In our work, we use word2vec (Mikolov et al., 2013), which provides a model trained on Google News to predict related words or phrases for a query specified as a set of phrases. Given an integer k, word2vec outputs the set of k phrases that are most similar to the query. Our corpus-based extractor uses (1) the input sentence to construct a set of relevant queries and (2) the word2vec results and a KB. To construct a query for a given entity mention, we focus on the part of the sentence directly related to the entity. This relevant part consists of the clause in which the entity occurs and the subordinate clauses that do not contain another ent</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tristan Miller</author>
<author>Chris Biemann</author>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1781--1796</pages>
<contexts>
<context position="24230" citStr="Miller et al., 2012" startWordPosition="3978" endWordPosition="3981">tep can be used as-is; it is not trained on any domain- or corpus-specific data. 3.1 Obtaining context All WSD systems take a set of candidate types and contextual information as input. The key challenge lies in the construction of candidate types (Sec. 2) as well as context. For each entity, we consider entity-oblivious context (from the input sentence) as well as entity-specific context (using lexical expansions). We take all words in the sentence as entityoblivious context. To construct entity-specific context, we make use of lexical expansions, which have been successfully applied in WSD (Miller et al., 2012). Its goal is to enrich contextual information to boost disambiguation. In our case, it also helps to differentiate between multiple entities in a sentence. We build the entity-specific context using word vectors. As in the corpus-based extractor, we construct a set of queries for the entity but in this case we take as context all so-obtained words that do not correspond to a named entity. For instance, the entity-specific context for the entity mention “Maradona” for query “Maradona South Africa” is: “coach”, “cup”, “striker”, “midfielder”, and “captain”. The full context for “Maradona” in “M</context>
<context position="38450" citStr="Miller et al., 2012" startWordPosition="6349" endWordPosition="6352">t on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign such senses, mostly relying on manually annotated data. KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Del Corro et al., 2014) use of a background KB instead. 6 Conclusion We presented FINET, a system for fine-grained typing of named entities in context. FINET generates candidates using multiple extractors, ranging from explicitly mentioned to implicit types, and subsequently selects the most appropriate. Our experimental study indicates that FINET has significantly better performance than previous methods. Pattern-based 180 71.11 Mention-based 219 82.65 Verb-based 47 48.94 Corpus-based 205 64.39 Table 4: Per-extractor performance NYT cision of SFG labels exceeded 90%. In</context>
</contexts>
<marker>Miller, Biemann, Zesch, Gurevych, 2012</marker>
<rawString>Tristan Miller, Chris Biemann, Torsten Zesch, and Iryna Gurevych. 2012. Using distributional similarity for lexical expansion in knowledge-based word sense disambiguation. In Proceedings of COLING, pages 1781–1796.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tom Mitchell</author>
<author>William Cohen</author>
<author>Estevam Hruscha</author>
<author>Partha Talukdar</author>
<author>Justin Betteridge</author>
<author>Andrew Carlson</author>
<author>Bhavana Dalvi</author>
<author>Matthew Gardner</author>
<author>Bryan Kisiel</author>
<author>Jayant Krishnamurthy</author>
<author>Ni Lao</author>
</authors>
<title>Kathryn Mazaitis, Thahir Mohammad, Ndapa Nakashole, Emmanouil Platanios,</title>
<date>2015</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<pages>2302--2310</pages>
<location>Alan Ritter, Mehdi Samadi, Burr Settles, Richard Wang, Derry Wijaya, Abhinav Gupta, Xinlei</location>
<contexts>
<context position="1830" citStr="Mitchell et al., 2015" startWordPosition="269" endWordPosition="272"> type(s) of a named entity in context. For instance, given “John plays guitar on the stage”, our goal is to infer that “John” is a guitarist or a musician and a person. We propose FINET, a system for detecting the types of named entities in short inputs—such as sentences or tweets—with respect to WordNet’s super fine-grained type system (16k types of organizations, persons and locations). Named entity typing is a fundamental building block for many natural-language processing tasks. NET is at the heart of information extraction methods for finding types for entities in a knowledge base1 (KB) (Mitchell et al., 2015). Likewise, NET aids named entity disambiguation by reducing the candidate space for a given entity mention. Entity types are an important resource for entity-based retrieval or aggregation tasks, such as semantic search (Hoffart et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Ex</context>
<context position="40728" citStr="Mitchell et al., 2015" startWordPosition="6717" endWordPosition="6720">Hyena mainly suffered from the general problems of supervised systems. For instance, since (graduate-1) or (region-1) are highly frequent in the KB, many persons (locations) were incorrectly typed as (graduate-1) ((region-1)). Errors in the KB also propagate in supervised system, which may lead to “contradictory” types (i.e., an entity being typed as both (person-1) and (location-1)). 5 Related Work The NET problem is related to taxonomy induc- 876 tion (Snow et al., 2006; Wu et al., 2012; Shi et al., 2010; Velardi et al., 2013) and KB construction (Lee et al., 2013; Paulheim and Bizer, 2014; Mitchell et al., 2015), although the goals are different. Taxonomy induction aims to produce or extend a taxonomy of types, whereas KB construction methods aim to find new types for the entities present in a KB. In both cases, this is done by reasoning over a large corpus. In contrast, we are interested in typing each named entity mention Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: A spatially and temporally enhanced knowledge base from wikipedia. Artif. Intell., 194:28–61. References Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for word sense disambiguation</context>
</contexts>
<marker>Mitchell, Cohen, Hruscha, Talukdar, Betteridge, Carlson, Dalvi, Gardner, Kisiel, Krishnamurthy, Lao, 2015</marker>
<rawString>Tom Mitchell, William Cohen, Estevam Hruscha, Partha Talukdar, Justin Betteridge, Andrew Carlson, Bhavana Dalvi, Matthew Gardner, Bryan Kisiel, Jayant Krishnamurthy, Ni Lao, Kathryn Mazaitis, Thahir Mohammad, Ndapa Nakashole, Emmanouil Platanios, Alan Ritter, Mehdi Samadi, Burr Settles, Richard Wang, Derry Wijaya, Abhinav Gupta, Xinlei Chen, Abulhair Saparov, Malcolm Greaves, and Joel Welling. 2015. Never-ending learning. In Proceedings of AAAI, pages 2302–2310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Gerhard Weikum</author>
<author>Fabian Suchanek</author>
</authors>
<title>Patty: A taxonomy of relational patterns with semantic types.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1135--1145</pages>
<contexts>
<context position="2229" citStr="Nakashole et al., 2012" startWordPosition="329" endWordPosition="332">entity typing is a fundamental building block for many natural-language processing tasks. NET is at the heart of information extraction methods for finding types for entities in a knowledge base1 (KB) (Mitchell et al., 2015). Likewise, NET aids named entity disambiguation by reducing the candidate space for a given entity mention. Entity types are an important resource for entity-based retrieval or aggregation tasks, such as semantic search (Hoffart et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Explicit types occur, for example, in phrases such as “Steinmeier, the German Foreign Minister, [...]” or “Foreign Minister Steinmeier.” These explicit types are often extracted via patterns, such as the well-known Hearst patterns (Hearst, 1992), and subsequently integrated into a taxonomy. Patternbased methods often have high precision but low recall: Types are usually mentioned when a named entit</context>
<context position="28332" citStr="Nakashole et al., 2012" startWordPosition="4612" endWordPosition="4615">ing a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories (artifact-1), (event1), (person-1), (location-1), and (organization1). Hyena outperformed a number of previous systems (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). We used Hyena via its web service (Yosef et al., 2013). Pearl (Nakashole et al., 2013) is a semisupervised system that leverages a repository of 300k relational patterns (Nakashole et al., 2012). Subjects and objects of each pattern carry type information. Pearl types named entity mentions by the most likely type according to its pattern database. Pearl’s type system is based on around 200 “interesting” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without the KB lookup. This allows us to estimate the extent to which referring to a KB helps. Note that the corpus-based extractor makes use of the KB in both settings. Datasets. We used three different datasets representing real-world use cases. W</context>
</contexts>
<marker>Nakashole, Weikum, Suchanek, 2012</marker>
<rawString>Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. Patty: A taxonomy of relational patterns with semantic types. In Proceedings of EMNLP, pages 1135–1145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ndapandula Nakashole</author>
<author>Tomasz Tylenda</author>
<author>Gerhard Weikum</author>
</authors>
<title>Fine-grained semantic typing of emerging entities.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1488--1497</pages>
<contexts>
<context position="28224" citStr="Nakashole et al., 2013" startWordPosition="4595" endWordPosition="4598">S tags. It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories (artifact-1), (event1), (person-1), (location-1), and (organization1). Hyena outperformed a number of previous systems (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). We used Hyena via its web service (Yosef et al., 2013). Pearl (Nakashole et al., 2013) is a semisupervised system that leverages a repository of 300k relational patterns (Nakashole et al., 2012). Subjects and objects of each pattern carry type information. Pearl types named entity mentions by the most likely type according to its pattern database. Pearl’s type system is based on around 200 “interesting” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without the KB lookup. This allows us to estimate the extent to which referring to a KB helps. Note that the corpus-based extractor makes use</context>
<context position="37471" citStr="Nakashole et al., 2013" startWordPosition="6188" endWordPosition="6191">n most of our extractors and of the distributional hypothesis in our corpus-based extractor. Yahya et al. (2014) developed a semisupervised method to extract facts such as “president”(“Barack Obama”, “US”), in which the relation acts as a type. FINET differs in that it supports implicit types and produces disambiguated types. A number of NET systems have been proposed which make use of a predefined type hierarchy. Lin et al. (2012) proposes a semi-supervised system that uses relational patterns to propagate type information from a KB to entity mentions. Similarly, the subsequent Pearl system (Nakashole et al., 2013) is based on a corpus of typed relation patterns. An alternative approach is taken by supervised methods, which train classifiers based on linguistic features (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task wher</context>
</contexts>
<marker>Nakashole, Tylenda, Weikum, 2013</marker>
<rawString>Ndapandula Nakashole, Tomasz Tylenda, and Gerhard Weikum. 2013. Fine-grained semantic typing of emerging entities. In Proceedings of ACL, pages 1488–1497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>A quick tour of word sense disambiguation, induction and related approaches.</title>
<date>2012</date>
<booktitle>In Proceedings of SOFSEM,</booktitle>
<pages>115--129</pages>
<contexts>
<context position="38043" citStr="Navigli, 2012" startWordPosition="6287" endWordPosition="6288">uent Pearl system (Nakashole et al., 2013) is based on a corpus of typed relation patterns. An alternative approach is taken by supervised methods, which train classifiers based on linguistic features (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign such senses, mostly relying on manually annotated data. KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Del Corro et al., 2014) use of a background KB instead. 6 Conclusion We presented FINET, a system for fine-grained typing of named entities in context. FINET generates ca</context>
</contexts>
<marker>Navigli, 2012</marker>
<rawString>Roberto Navigli. 2012. A quick tour of word sense disambiguation, induction and related approaches. In Proceedings of SOFSEM, pages 115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="16744" citStr="Palmer et al., 2005" startWordPosition="2705" endWordPosition="2708">ty) locations is quite static and known, we assume that the KB contains all locations and their possible types (our experiments strengthened this assumption). If a mention of a location (e.g., “Berlin”) occurs in the repository, we add all the corresponding types from the repository to the candidate set (e.g., (city- 1)) and move to the type selection phase. 2.5 Verb-based extractor Verbs have been widely exploited to determine the types or roles of its arguments: A verb sense imposes a restriction on the type of its arguments (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). For instance, from “Ted Kennedy was elected to Congress,” we infer that “Ted Kennedy” is a person who can be elected. Corresponding types include (representative-1), (representative-2), or (politician-1). Our verbbased extractor leverages this insight to extract types. The extractor operates at the clause level. A simple way to infer lexical types for entities acting as subjects or objects of a clause is nominalization, i.e., the transformation of the verb into deverbal nouns (e.g., “play” into “player”). To exploit it, we apply a set of morphological transformations to</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heiko Paulheim</author>
<author>Christian Bizer</author>
</authors>
<title>Improving the quality of linked data using statistical distributions.</title>
<date>2014</date>
<journal>IJSWIS,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="40704" citStr="Paulheim and Bizer, 2014" startWordPosition="6713" endWordPosition="6716">type selection difficult. Hyena mainly suffered from the general problems of supervised systems. For instance, since (graduate-1) or (region-1) are highly frequent in the KB, many persons (locations) were incorrectly typed as (graduate-1) ((region-1)). Errors in the KB also propagate in supervised system, which may lead to “contradictory” types (i.e., an entity being typed as both (person-1) and (location-1)). 5 Related Work The NET problem is related to taxonomy induc- 876 tion (Snow et al., 2006; Wu et al., 2012; Shi et al., 2010; Velardi et al., 2013) and KB construction (Lee et al., 2013; Paulheim and Bizer, 2014; Mitchell et al., 2015), although the goals are different. Taxonomy induction aims to produce or extend a taxonomy of types, whereas KB construction methods aim to find new types for the entities present in a KB. In both cases, this is done by reasoning over a large corpus. In contrast, we are interested in typing each named entity mention Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: A spatially and temporally enhanced knowledge base from wikipedia. Artif. Intell., 194:28–61. References Eneko Agirre and Aitor Soroa. 2009. Personalizing pagerank for w</context>
</contexts>
<marker>Paulheim, Bizer, 2014</marker>
<rawString>Heiko Paulheim and Christian Bizer. 2014. Improving the quality of linked data using statistical distributions. IJSWIS, 10(2):63–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simone Paolo Ponzetto</author>
<author>Roberto Navigli</author>
</authors>
<title>Knowledge-rich word sense disambiguation rivaling supervised systems.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1522--1531</pages>
<contexts>
<context position="25103" citStr="Ponzetto and Navigli, 2010" startWordPosition="4112" endWordPosition="4115">, we construct a set of queries for the entity but in this case we take as context all so-obtained words that do not correspond to a named entity. For instance, the entity-specific context for the entity mention “Maradona” for query “Maradona South Africa” is: “coach”, “cup”, “striker”, “midfielder”, and “captain”. The full context for “Maradona” in “Maradona expects to win in South Africa” additionally includes the entity-oblivious context “expects”, “win”, “South Africa”. 3.2 Selecting types WSD systems fall into two classes: unsupervised, which rely on background knowledge such as WordNet (Ponzetto and Navigli, 2010), and supervised, which require training data (Zhong and Ng, 2010). Here we take a combination, i.e., we leverage WordNet and manually annotated data. We train a Naive Bayes classifier to select the most appropriate type given its context. We represent context by a bag of lemmatized words. This allows us to automatically generate training data from WordNet (and use manually labeled data). Since WordNet provides information for each of the relevant types, this approach combats the data sparsity that arises with supervised systems. The context for each individual WordNet type consists of all wor</context>
<context position="38429" citStr="Ponzetto and Navigli, 2010" startWordPosition="6345" endWordPosition="6348">g data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign such senses, mostly relying on manually annotated data. KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Del Corro et al., 2014) use of a background KB instead. 6 Conclusion We presented FINET, a system for fine-grained typing of named entities in context. FINET generates candidates using multiple extractors, ranging from explicitly mentioned to implicit types, and subsequently selects the most appropriate. Our experimental study indicates that FINET has significantly better performance than previous methods. Pattern-based 180 71.11 Mention-based 219 82.65 Verb-based 47 48.94 Corpus-based 205 64.39 Table 4: Per-extractor performance NYT cision of SFG la</context>
</contexts>
<marker>Ponzetto, Navigli, 2010</marker>
<rawString>Simone Paolo Ponzetto and Roberto Navigli. 2010. Knowledge-rich word sense disambiguation rivaling supervised systems. In Proceedings of ACL, pages 1522–1531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<date>1985</date>
<journal>A Comprehensive Grammar of the English Language. Longman.</journal>
<contexts>
<context position="16677" citStr="Quirk et al., 1985" startWordPosition="2693" endWordPosition="2696">dure for entities tagged as location. Since the set of (named-entity) locations is quite static and known, we assume that the KB contains all locations and their possible types (our experiments strengthened this assumption). If a mention of a location (e.g., “Berlin”) occurs in the repository, we add all the corresponding types from the repository to the candidate set (e.g., (city- 1)) and move to the type selection phase. 2.5 Verb-based extractor Verbs have been widely exploited to determine the types or roles of its arguments: A verb sense imposes a restriction on the type of its arguments (Quirk et al., 1985; Levin, 1993; Hanks, 1996; Baker et al., 1998; Palmer et al., 2005; Kipper et al., 2008). For instance, from “Ted Kennedy was elected to Congress,” we infer that “Ted Kennedy” is a person who can be elected. Corresponding types include (representative-1), (representative-2), or (politician-1). Our verbbased extractor leverages this insight to extract types. The extractor operates at the clause level. A simple way to infer lexical types for entities acting as subjects or objects of a clause is nominalization, i.e., the transformation of the verb into deverbal nouns (e.g., “play” into “player”)</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A Comprehensive Grammar of the English Language. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Altaf Rahman</author>
<author>Vincent Ng</author>
</authors>
<title>Inducing finegrained semantic classes via hierarchical and collective classification.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>931--939</pages>
<contexts>
<context position="28114" citStr="Rahman and Ng, 2010" startWordPosition="4575" endWordPosition="4578"> classifier. Its features include the words in the named entity mention, in sentence and paragraph, and POS tags. It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories (artifact-1), (event1), (person-1), (location-1), and (organization1). Hyena outperformed a number of previous systems (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). We used Hyena via its web service (Yosef et al., 2013). Pearl (Nakashole et al., 2013) is a semisupervised system that leverages a repository of 300k relational patterns (Nakashole et al., 2012). Subjects and objects of each pattern carry type information. Pearl types named entity mentions by the most likely type according to its pattern database. Pearl’s type system is based on around 200 “interesting” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without the KB lookup. This all</context>
<context position="37677" citStr="Rahman and Ng, 2010" startWordPosition="6221" endWordPosition="6224">which the relation acts as a type. FINET differs in that it supports implicit types and produces disambiguated types. A number of NET systems have been proposed which make use of a predefined type hierarchy. Lin et al. (2012) proposes a semi-supervised system that uses relational patterns to propagate type information from a KB to entity mentions. Similarly, the subsequent Pearl system (Nakashole et al., 2013) is based on a corpus of typed relation patterns. An alternative approach is taken by supervised methods, which train classifiers based on linguistic features (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng,</context>
</contexts>
<marker>Rahman, Ng, 2010</marker>
<rawString>Altaf Rahman and Vincent Ng. 2010. Inducing finegrained semantic classes via hierarchical and collective classification. In Proceedings of COLING, pages 931–939.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie C de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The Life and Death of Discourse Entities: Identifying Singleton Mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>627--633</pages>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie C. de Marneffe, and Christopher Potts. 2013. The Life and Death of Discourse Entities: Identifying Singleton Mentions. In Proceedings of HLT-NAACL, pages 627–633.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Rumelhart</author>
<author>Geoffrey E Hinton</author>
<author>Ronald J Williams</author>
</authors>
<title>Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors,</title>
<date>1988</date>
<pages>696--699</pages>
<contexts>
<context position="20254" citStr="Rumelhart et al., 1988" startWordPosition="3297" endWordPosition="3300">ies tend to occur in similar contexts. For example, “Messi” and “Cristiano Ronaldo” may both be mentioned in the context of soccer. Thus entity mentions similar to “Messi” in a sport context are likely to include soccer players. Our extractor is related to semi-supervised KB methods in that it propagates types of named entity mentions that may appear in a similar context. It differs in that it is unsupervised, does not require manually or automatically generated training data, and in the way context is modeled and candidates are generated. Our corpus-based extractor makes use of word vectors (Rumelhart et al., 1988) trained on a large unlabeled corpus. A word vector is a semantic representation of a phrase and represents the semantic context in which the phrase occurs. Phrases that are semantically related, and thus appear in similar contexts, are close to each other in the word vector space. For instance, if “Messi” and “Cristiano Ronaldo” tend to co-occur with a similar sets of words, their word vectors are close. We also may expect “Arnold Schwarzenegger” to be close to both actors and politicians, since it occurs in both contexts. In our work, we use word2vec (Mikolov et al., 2013), which provides a </context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1988</marker>
<rawString>David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. 1988. Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696–699.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The distributional hypothesis.</title>
<date>2008</date>
<journal>Italian Journal of Linguistics,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="19616" citStr="Sahlgren, 2008" startWordPosition="3193" endWordPosition="3194">on-1), (disease-1), or (patient1). FINET, uses the corpora of Flati and Navigli (2013) and Del Corro et al. (2014). Given a verb and an entity, we search for frequent candidate types (depending on whether the entity acts as a subject or object). For example, from “Messi was treated in the hospital,” we obtain (patient-1). Once potential candidates have been collected, we perform a KB lookup to decide how to proceed. 2.6 Corpus-based extractor Our final extractor leverages a large unlabeled corpus to find entities that co-occur in similar contexts. It is based on the distributional hypothesis (Sahlgren, 2008): similar entities tend to occur in similar contexts. For example, “Messi” and “Cristiano Ronaldo” may both be mentioned in the context of soccer. Thus entity mentions similar to “Messi” in a sport context are likely to include soccer players. Our extractor is related to semi-supervised KB methods in that it propagates types of named entity mentions that may appear in a similar context. It differs in that it is unsupervised, does not require manually or automatically generated training data, and in the way context is modeled and candidates are generated. Our corpus-based extractor makes use of</context>
</contexts>
<marker>Sahlgren, 2008</marker>
<rawString>Magnus Sahlgren. 2008. The distributional hypothesis. Italian Journal of Linguistics, 20(1):33–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times Annotated Corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia, 6(12).</location>
<contexts>
<context position="29330" citStr="Sandhaus, 2008" startWordPosition="4784" endWordPosition="4786">ws us to estimate the extent to which referring to a KB helps. Note that the corpus-based extractor makes use of the KB in both settings. Datasets. We used three different datasets representing real-world use cases. We created two datasets, New York Times and Twitter, and sampled a subset of the CoNLL data, which provides gold annotations for CG types. We did not consider datasets such as FIGER (Ling and Weld, 2012) or BBN (Weischedel and Brunstein, 2005) because they are not suitable for very fine-grained typing. New York Times consists of 500 random sentences from the New York Times corpus (Sandhaus, 2008), year 2007; we selected only sentences that contained at least one named entity according to the Stanford CoreNLP 4.4.1 tool. CoNLL. We sampled 500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003), a collection of newswires with manually annotated entities with CG types labels. We directly used the annotations in our evaluation. The sentences tend to be short and sometimes non-verbal (e.g., “Jim Grabb ( U.S. ) vs. Sandon Stolle ( Australia )”). Most entities are prominent and likely to be found in our KB (and the one of existing methods). Twitter. We collected the first 100 tweets w</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The New York Times Annotated Corpus. Linguistic Data Consortium, Philadelphia, 6(12).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shuming Shi</author>
<author>Huibin Zhang</author>
<author>Xiaojie Yuan</author>
<author>JiRong Wen</author>
</authors>
<title>Corpus-based semantic class mining: Distributional vs. pattern-based approaches.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>993--1001</pages>
<contexts>
<context position="36775" citStr="Shi et al., 2010" startWordPosition="6072" endWordPosition="6075">bs. Instead, we believe a better direction is to improve the underlying resources. Error analysis. One source of error for FINET were incorrect CG labels. When CG labels were correct (by Stanford NER), the precision of FINET for FG types increased to more than 70% for all datasets. When FG labels were correct, the pre875 Last used extractor Entities P individually using an existing type system. FINET draws from ideas used in taxonomy induction or KB construction. Existing systems are either based on patterns or the distributional hypothesis; these two approaches are discussed and compared in (Shi et al., 2010). In FINET, we make use of patterns (such as the ones of Hearst (1992)) in most of our extractors and of the distributional hypothesis in our corpus-based extractor. Yahya et al. (2014) developed a semisupervised method to extract facts such as “president”(“Barack Obama”, “US”), in which the relation acts as a type. FINET differs in that it supports implicit types and produces disambiguated types. A number of NET systems have been proposed which make use of a predefined type hierarchy. Lin et al. (2012) proposes a semi-supervised system that uses relational patterns to propagate type informati</context>
<context position="40617" citStr="Shi et al., 2010" startWordPosition="6697" endWordPosition="6700">based extractor, which may produce a large number of candidates and thus makes type selection difficult. Hyena mainly suffered from the general problems of supervised systems. For instance, since (graduate-1) or (region-1) are highly frequent in the KB, many persons (locations) were incorrectly typed as (graduate-1) ((region-1)). Errors in the KB also propagate in supervised system, which may lead to “contradictory” types (i.e., an entity being typed as both (person-1) and (location-1)). 5 Related Work The NET problem is related to taxonomy induc- 876 tion (Snow et al., 2006; Wu et al., 2012; Shi et al., 2010; Velardi et al., 2013) and KB construction (Lee et al., 2013; Paulheim and Bizer, 2014; Mitchell et al., 2015), although the goals are different. Taxonomy induction aims to produce or extend a taxonomy of types, whereas KB construction methods aim to find new types for the entities present in a KB. In both cases, this is done by reasoning over a large corpus. In contrast, we are interested in typing each named entity mention Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: A spatially and temporally enhanced knowledge base from wikipedia. Artif. Intell.,</context>
</contexts>
<marker>Shi, Zhang, Yuan, Wen, 2010</marker>
<rawString>Shuming Shi, Huibin Zhang, Xiaojie Yuan, and JiRong Wen. 2010. Corpus-based semantic class mining: Distributional vs. pattern-based approaches. In Proceedings of COLING, pages 993–1001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogenous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL,</booktitle>
<pages>801--808</pages>
<contexts>
<context position="2426" citStr="Snow et al., 2006" startWordPosition="362" endWordPosition="365">Mitchell et al., 2015). Likewise, NET aids named entity disambiguation by reducing the candidate space for a given entity mention. Entity types are an important resource for entity-based retrieval or aggregation tasks, such as semantic search (Hoffart et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Explicit types occur, for example, in phrases such as “Steinmeier, the German Foreign Minister, [...]” or “Foreign Minister Steinmeier.” These explicit types are often extracted via patterns, such as the well-known Hearst patterns (Hearst, 1992), and subsequently integrated into a taxonomy. Patternbased methods often have high precision but low recall: Types are usually mentioned when a named entity is introduced or expected to be unknown to readers, but often are not explicitly stated. The NET problem differs from taxonomy induction in that (1) the type system is prespecified, (2) types are</context>
<context position="40582" citStr="Snow et al., 2006" startWordPosition="6689" endWordPosition="6692">s is especially visible in the verb-based extractor, which may produce a large number of candidates and thus makes type selection difficult. Hyena mainly suffered from the general problems of supervised systems. For instance, since (graduate-1) or (region-1) are highly frequent in the KB, many persons (locations) were incorrectly typed as (graduate-1) ((region-1)). Errors in the KB also propagate in supervised system, which may lead to “contradictory” types (i.e., an entity being typed as both (person-1) and (location-1)). 5 Related Work The NET problem is related to taxonomy induc- 876 tion (Snow et al., 2006; Wu et al., 2012; Shi et al., 2010; Velardi et al., 2013) and KB construction (Lee et al., 2013; Paulheim and Bizer, 2014; Mitchell et al., 2015), although the goals are different. Taxonomy induction aims to produce or extend a taxonomy of types, whereas KB construction methods aim to find new types for the entities present in a KB. In both cases, this is done by reasoning over a large corpus. In contrast, we are interested in typing each named entity mention Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: A spatially and temporally enhanced knowledge b</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006. Semantic taxonomy induction from heterogenous evidence. In Proceedings of COLING-ACL, pages 801–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>455--465</pages>
<contexts>
<context position="8472" citStr="Socher et al., 2013" startWordPosition="1331" endWordPosition="1334">ype selection phase. The reasoning behind this approach is to bias FINET towards the most explicit types. When the condition is not met, we enrich the set of candidate types of the extractor with their hypernyms; we expect types to be overly specific and want to allow the selection phase to be able to select a more general type. We then also run subsequent extractors. Tab. 1 displays a summary of the extractors and stopping conditions. All so-found type candidates are passed to the candidate selection phase (Sec. 3). 2.1 Preprocessing Preprocessing consists of 5 steps: (i) dependency parsing (Socher et al., 2013); (ii) co-reference (Recasens et al., 2013); (iii) named entity recognition (NER) (Finkel et al., 2005) with the detection of coarse-grained types (i.e., person, organization, location); (iv) clause identification (Del Corro and Gemulla, 2013); (v) word and multi-word expression recognition (Del Corro et al., 2014). FINET restricts its candidates to the hyponmys of the coarse-grained (CG) type of the NER system. Named entities with the same CG type in a coordinating relation (e.g., “Messi and Ronaldo are soccer players”) and identical mentions share the candidate set; the latter is reasonable </context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013. Parsing with compositional vector grammars. In Proceedings of ACL, pages 455–465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>142--147</pages>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In Proceedings of HLT-NAACL, pages 142–147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>Ontolearn reloaded: A graph-based algorithm for taxonomy induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="40640" citStr="Velardi et al., 2013" startWordPosition="6701" endWordPosition="6704">hich may produce a large number of candidates and thus makes type selection difficult. Hyena mainly suffered from the general problems of supervised systems. For instance, since (graduate-1) or (region-1) are highly frequent in the KB, many persons (locations) were incorrectly typed as (graduate-1) ((region-1)). Errors in the KB also propagate in supervised system, which may lead to “contradictory” types (i.e., an entity being typed as both (person-1) and (location-1)). 5 Related Work The NET problem is related to taxonomy induc- 876 tion (Snow et al., 2006; Wu et al., 2012; Shi et al., 2010; Velardi et al., 2013) and KB construction (Lee et al., 2013; Paulheim and Bizer, 2014; Mitchell et al., 2015), although the goals are different. Taxonomy induction aims to produce or extend a taxonomy of types, whereas KB construction methods aim to find new types for the entities present in a KB. In both cases, this is done by reasoning over a large corpus. In contrast, we are interested in typing each named entity mention Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: A spatially and temporally enhanced knowledge base from wikipedia. Artif. Intell., 194:28–61. References </context>
</contexts>
<marker>Velardi, Faralli, Navigli, 2013</marker>
<rawString>Paola Velardi, Stefano Faralli, and Roberto Navigli. 2013. Ontolearn reloaded: A graph-based algorithm for taxonomy induction. Computational Linguistics, 39(3):665–707.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Ada Brunstein</author>
</authors>
<title>BBN Pronoun Coreference and Entity Type Corpus.</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Linguistic Data Consortium.</institution>
<contexts>
<context position="29174" citStr="Weischedel and Brunstein, 2005" startWordPosition="4756" endWordPosition="4759">” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without the KB lookup. This allows us to estimate the extent to which referring to a KB helps. Note that the corpus-based extractor makes use of the KB in both settings. Datasets. We used three different datasets representing real-world use cases. We created two datasets, New York Times and Twitter, and sampled a subset of the CoNLL data, which provides gold annotations for CG types. We did not consider datasets such as FIGER (Ling and Weld, 2012) or BBN (Weischedel and Brunstein, 2005) because they are not suitable for very fine-grained typing. New York Times consists of 500 random sentences from the New York Times corpus (Sandhaus, 2008), year 2007; we selected only sentences that contained at least one named entity according to the Stanford CoreNLP 4.4.1 tool. CoNLL. We sampled 500 sentences from CoNLL (Tjong Kim Sang and De Meulder, 2003), a collection of newswires with manually annotated entities with CG types labels. We directly used the annotations in our evaluation. The sentences tend to be short and sometimes non-verbal (e.g., “Jim Grabb ( U.S. ) vs. Sandon Stolle (</context>
</contexts>
<marker>Weischedel, Brunstein, 2005</marker>
<rawString>Ralph Weischedel and Ada Brunstein. 2005. BBN Pronoun Coreference and Entity Type Corpus. Technical report, Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wentao Wu</author>
<author>Hongsong Li</author>
<author>Haixun Wang</author>
<author>Kenny Q Zhu</author>
</authors>
<title>Probase: A probabilistic taxonomy for text understanding.</title>
<date>2012</date>
<booktitle>In Proceedings of SIGMOD,</booktitle>
<pages>481--492</pages>
<contexts>
<context position="40599" citStr="Wu et al., 2012" startWordPosition="6693" endWordPosition="6696">ible in the verb-based extractor, which may produce a large number of candidates and thus makes type selection difficult. Hyena mainly suffered from the general problems of supervised systems. For instance, since (graduate-1) or (region-1) are highly frequent in the KB, many persons (locations) were incorrectly typed as (graduate-1) ((region-1)). Errors in the KB also propagate in supervised system, which may lead to “contradictory” types (i.e., an entity being typed as both (person-1) and (location-1)). 5 Related Work The NET problem is related to taxonomy induc- 876 tion (Snow et al., 2006; Wu et al., 2012; Shi et al., 2010; Velardi et al., 2013) and KB construction (Lee et al., 2013; Paulheim and Bizer, 2014; Mitchell et al., 2015), although the goals are different. Taxonomy induction aims to produce or extend a taxonomy of types, whereas KB construction methods aim to find new types for the entities present in a KB. In both cases, this is done by reasoning over a large corpus. In contrast, we are interested in typing each named entity mention Johannes Hoffart, Fabian M. Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: A spatially and temporally enhanced knowledge base from wikipedi</context>
</contexts>
<marker>Wu, Li, Wang, Zhu, 2012</marker>
<rawString>Wentao Wu, Hongsong Li, Haixun Wang, and Kenny Q. Zhu. 2012. Probase: A probabilistic taxonomy for text understanding. In Proceedings of SIGMOD, pages 481–492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Klaus Berberich</author>
<author>Shady Elbassuoni</author>
<author>Gerhard Weikum</author>
</authors>
<title>Robust question answering over the web of linked data.</title>
<date>2013</date>
<booktitle>In Proceedings of CIKM,</booktitle>
<pages>1107--1116</pages>
<contexts>
<context position="2116" citStr="Yahya et al., 2013" startWordPosition="313" endWordPosition="316">spect to WordNet’s super fine-grained type system (16k types of organizations, persons and locations). Named entity typing is a fundamental building block for many natural-language processing tasks. NET is at the heart of information extraction methods for finding types for entities in a knowledge base1 (KB) (Mitchell et al., 2015). Likewise, NET aids named entity disambiguation by reducing the candidate space for a given entity mention. Entity types are an important resource for entity-based retrieval or aggregation tasks, such as semantic search (Hoffart et al., 2014) or question answering (Yahya et al., 2013). Finally, type information helps to increase the semantic content of syntactic patterns (Nakashole et al., 2012) or in open information extraction (Lin et al., 2012). The extraction of explicit types has been studied in the literature, most prominently in the context of taxonomy induction (Snow et al., 2006). Explicit types occur, for example, in phrases such as “Steinmeier, the German Foreign Minister, [...]” or “Foreign Minister Steinmeier.” These explicit types are often extracted via patterns, such as the well-known Hearst patterns (Hearst, 1992), and subsequently integrated into a taxono</context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Weikum, 2013</marker>
<rawString>Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, and Gerhard Weikum. 2013. Robust question answering over the web of linked data. In Proceedings of CIKM, pages 1107–1116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Steven Euijong Whang</author>
<author>Rahul Gupta</author>
<author>Alon Halevy</author>
</authors>
<title>Renoun: Fact extraction for nominal attributes.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>325--335</pages>
<contexts>
<context position="36960" citStr="Yahya et al. (2014)" startWordPosition="6104" endWordPosition="6107">(by Stanford NER), the precision of FINET for FG types increased to more than 70% for all datasets. When FG labels were correct, the pre875 Last used extractor Entities P individually using an existing type system. FINET draws from ideas used in taxonomy induction or KB construction. Existing systems are either based on patterns or the distributional hypothesis; these two approaches are discussed and compared in (Shi et al., 2010). In FINET, we make use of patterns (such as the ones of Hearst (1992)) in most of our extractors and of the distributional hypothesis in our corpus-based extractor. Yahya et al. (2014) developed a semisupervised method to extract facts such as “president”(“Barack Obama”, “US”), in which the relation acts as a type. FINET differs in that it supports implicit types and produces disambiguated types. A number of NET systems have been proposed which make use of a predefined type hierarchy. Lin et al. (2012) proposes a semi-supervised system that uses relational patterns to propagate type information from a KB to entity mentions. Similarly, the subsequent Pearl system (Nakashole et al., 2013) is based on a corpus of typed relation patterns. An alternative approach is taken by sup</context>
</contexts>
<marker>Yahya, Whang, Gupta, Halevy, 2014</marker>
<rawString>Mohamed Yahya, Steven Euijong Whang, Rahul Gupta, and Alon Halevy. 2014. Renoun: Fact extraction for nominal attributes. In Proceedings of EMNLP, pages 325–335.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Amir Yosef</author>
<author>Sandro Bauer</author>
<author>Johannes Hoffart</author>
<author>Marc Spaniol</author>
<author>Gerhard Weikum</author>
</authors>
<title>HYENA: Hierarchical Type Classification for Entity Names.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1361--1370</pages>
<contexts>
<context position="4375" citStr="Yosef et al., 2012" startWordPosition="671" endWordPosition="674">ng, pages 868–878, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. key challenge in NET is to extract such implicit, Extractor Stopping Condition context-aware types to improve recall. One way to extract implicit types is by training a supervised extractor on labeled data, in which each entity is annotated with appropriate types. The key problem of this approach is that training data is scarce; this scarcity is amplified for finegrained type systems. To address this problem, many existing systems generate training data by exploiting KBs as a resource (Yosef et al., 2012). A popular approach is to train an extractor on a corpus of sentences (e.g., on Wikipedia), in which each named entity is associated with all its types in a KB. The key problem with such an approach is that the so-obtained type information is oblivious to the context in which the entity was mentioned. For example, in the sentences “Klitschko is known for his powerful punches” and “Klitschko is the Mayor of Kiew,” “Klitschko” will be associated with all its types, e.g., boxer, politician and mayor. As a consequence, the labels in the training data can be misleading and negatively affect the ex</context>
<context position="6799" citStr="Yosef et al., 2012" startWordPosition="1057" endWordPosition="1060">disambiguation (WSD) and resources such as WordNet glosses, and, if available, manually annotated training data. FINET leverages ideas from existing systems and extends them by (1) handling short inputs (2) supporting a very fine-grained type hierarchy, and Pattern-based (final) Always stop Pattern-based (non-final) KB-lookup Mention-based KB-lookup Verb-based KB-lookup Corpus-based 50% of score in 5 10 types Table 1: Extractors and their stopping conditions (3) producing types that focus on the entity context. Existing systems are unable to extract more than a couple of hundred types. Hyena (Yosef et al., 2012), the system with the most fine-grained type system so far, focuses only on a subset of 505 types from WordNet. Hyena lacks important types such as president or businessman, and includes soccer player but not tennis player. Instead of restricting types, FINET operates on the the entire WordNet hierarchy with more than 16k types for persons, organizations, and locations. We evaluated FINET on a number of real-world datasets. Our results indicate that FINET significantly outperforms previous methods. 2 Candidate Generation 869 In this phase, we collect candidate types for each entity. We first p</context>
<context position="27432" citStr="Yosef et al., 2012" startWordPosition="4465" endWordPosition="4468">ca-Cola” is a “sponsor,” but in WordNet, only persons can be sponsors. Nevertheless, this approach worked well in our experiments. 4 Experiments We conducted an experimental study on multiple real-word datasets to compare FINET with two state-of-the-art approaches. FINET is used as-is; it does not require training or tuning for any specific dataset. All datasets, detected types, labels, and our source code are publicly available.4 3http://web.eecs.umich.edu/˜mihalcea/ downloads.html 4http://dws.informatik.uni-mannheim. de/en/resources/software/finet/ 873 4.1 Experimental Setup Methods. Hyena (Yosef et al., 2012) is a representative supervised method that uses a hierarchical classifier. Its features include the words in the named entity mention, in sentence and paragraph, and POS tags. It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories (artifact-1), (event1), (person-1), (location-1), and (organization1). Hyena outperf</context>
</contexts>
<marker>Yosef, Bauer, Hoffart, Spaniol, Weikum, 2012</marker>
<rawString>Mohamed Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum. 2012. HYENA: Hierarchical Type Classification for Entity Names. In Proceedings of COLING, pages 1361– 1370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Amir Yosef</author>
<author>Sandro Bauer</author>
<author>Johannes Hoffart</author>
<author>Marc Spaniol</author>
<author>Gerhard Weikum</author>
</authors>
<title>HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>133--138</pages>
<contexts>
<context position="28192" citStr="Yosef et al., 2013" startWordPosition="4590" endWordPosition="4593">ntence and paragraph, and POS tags. It performs basic co-reference resolution and marks entity mentions connected to a type in the KB using a binary feature. Similar to Ling and Weld (2012), Hyena is trained on Wikipedia entities, each being annotated with its corresponding WordNet types from YAGO. Hyena’s type system is restricted to 505 WordNet types with top categories (artifact-1), (event1), (person-1), (location-1), and (organization1). Hyena outperformed a number of previous systems (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). We used Hyena via its web service (Yosef et al., 2013). Pearl (Nakashole et al., 2013) is a semisupervised system that leverages a repository of 300k relational patterns (Nakashole et al., 2012). Subjects and objects of each pattern carry type information. Pearl types named entity mentions by the most likely type according to its pattern database. Pearl’s type system is based on around 200 “interesting” WordNet types. We ran Pearl in its hard setting, which performed best. FINET. We ran FINET in two configurations: (1) with KB lookup, (2) without the KB lookup. This allows us to estimate the extent to which referring to a KB helps. Note that the </context>
<context position="37725" citStr="Yosef et al. (2013)" startWordPosition="6230" endWordPosition="6233"> in that it supports implicit types and produces disambiguated types. A number of NET systems have been proposed which make use of a predefined type hierarchy. Lin et al. (2012) proposes a semi-supervised system that uses relational patterns to propagate type information from a KB to entity mentions. Similarly, the subsequent Pearl system (Nakashole et al., 2013) is based on a corpus of typed relation patterns. An alternative approach is taken by supervised methods, which train classifiers based on linguistic features (Fleischman and Hovy, 2002; Rahman and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign such senses, m</context>
</contexts>
<marker>Yosef, Bauer, Hoffart, Spaniol, Weikum, 2013</marker>
<rawString>Mohamed Amir Yosef, Sandro Bauer, Johannes Hoffart, Marc Spaniol, and Gerhard Weikum. 2013. HYENA-live: Fine-Grained Online Entity Type Classification from Natural-language Text. In Proceedings ofACL, pages 133–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhi Zhong</author>
<author>Hwee Tou Ng</author>
</authors>
<title>It makes sense: A wide-coverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL: System Demonstrations,</booktitle>
<pages>78--83</pages>
<contexts>
<context position="25169" citStr="Zhong and Ng, 2010" startWordPosition="4123" endWordPosition="4126"> context all so-obtained words that do not correspond to a named entity. For instance, the entity-specific context for the entity mention “Maradona” for query “Maradona South Africa” is: “coach”, “cup”, “striker”, “midfielder”, and “captain”. The full context for “Maradona” in “Maradona expects to win in South Africa” additionally includes the entity-oblivious context “expects”, “win”, “South Africa”. 3.2 Selecting types WSD systems fall into two classes: unsupervised, which rely on background knowledge such as WordNet (Ponzetto and Navigli, 2010), and supervised, which require training data (Zhong and Ng, 2010). Here we take a combination, i.e., we leverage WordNet and manually annotated data. We train a Naive Bayes classifier to select the most appropriate type given its context. We represent context by a bag of lemmatized words. This allows us to automatically generate training data from WordNet (and use manually labeled data). Since WordNet provides information for each of the relevant types, this approach combats the data sparsity that arises with supervised systems. The context for each individual WordNet type consists of all words appearing in the type’s gloss and the glosses of its neighbors </context>
<context position="38283" citStr="Zhong and Ng, 2010" startWordPosition="6322" endWordPosition="6325"> and Ng, 2010; Ling and Weld, 2012). Both Yosef et al. (2013) and Ling and Weld (2012) use Wikipedia and a KB to generate automatic training data. FINET is less reliant on a KB or training data than the above methods, which improves both precision (no bias against KB types) and recall (more fine-grained types supported). Our type selection phase is based on WSD (Navigli, 2012), a classification task where words or phrases are disambiguated against senses from some external resource such as WordNet. Supervised WSD systems (Dang and Palmer, 2005; Dligach and Palmer, 2008; Chen and Palmer, 2009; Zhong and Ng, 2010) use a classifier to assign such senses, mostly relying on manually annotated data. KB methods (Agirre and Soroa, 2009; Ponzetto and Navigli, 2010; Miller et al., 2012; Agirre et al., 2014; Del Corro et al., 2014) use of a background KB instead. 6 Conclusion We presented FINET, a system for fine-grained typing of named entities in context. FINET generates candidates using multiple extractors, ranging from explicitly mentioned to implicit types, and subsequently selects the most appropriate. Our experimental study indicates that FINET has significantly better performance than previous methods. </context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Zhi Zhong and Hwee Tou Ng. 2010. It makes sense: A wide-coverage word sense disambiguation system for free text. In Proceedings of ACL: System Demonstrations, pages 78–83.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
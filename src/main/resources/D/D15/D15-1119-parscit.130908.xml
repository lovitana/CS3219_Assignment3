<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.029284">
<title confidence="0.974072">
Improving fast align by Reordering
</title>
<author confidence="0.994047">
Chenchen Ding, Masao Utiyama, Eiichiro Sumita
</author>
<affiliation confidence="0.991534">
Multilingual Translation Laboratory
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.937356">
3-5 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0289, Japan
</address>
<email confidence="0.994039">
{chenchen.ding, mutiyama, eiichiro.sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.993895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999895235294118">
fast align is a simple, fast, and effi-
cient approach for word alignment based
on the IBM model 2. fast align per-
forms well for language pairs with rel-
atively similar word orders; however, it
does not perform well for language pairs
with drastically different word orders. We
propose a segmenting-reversing reorder-
ing process to solve this problem by al-
ternately applying fast align and re-
ordering source sentences during train-
ing. Experimental results with Japanese-
English translation demonstrate that the
proposed approach improves the per-
formance of fast align significantly
without the loss of efficiency. Experiments
using other languages are also reported.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999842133333333">
Aligning words in a parallel corpus is a basic task
for almost all state-of-the-art statistical machine
translation (SMT) systems. Word alignment is
used to extract translation rules in various way,
such as the phrase pairs used in a phrase-based
(PB) SMT system (Koehn et al., 2003), the hier-
archical rules used in a HIERO system (Chiang,
2007), and the sophisticated translation templates
used in tree-based SMT systems (Liu et al., 2006).
Among different approaches, GIZA++1 (Och
and Ney, 2003), which is based on the IBM
translation models, is the most widely used word
alignment tool. Other well-known tools are
the BerkeleyAligner2, Nile3 (Riesa et al.,
2011), and pialign4 (Neubig et al., 2011).
</bodyText>
<footnote confidence="0.997907333333333">
1http://www.statmt.org/moses/giza/
GIZA++.html
2https://code.google.com/p/
berkeleyaligner/
3http://jasonriesa.github.io/nile/
4http://www.phontron.com/pialign/
</footnote>
<bodyText confidence="0.998590794871795">
fast align5 (Dyer et al., 2013) is a recently
proposed word alignment approach based on the
reparameterization of the IBM model 2, which
is usually referred to as a zero-order alignment
model (Och and Ney, 2003). Taking advantage of
the simplicity of the IBM model 2, fast align
introduces a “tension” parameter to model the
overall accordance of word orders and an effi-
cient parameter re-estimation algorithm is devised.
It has been reported that the fast align ap-
proach is more than 10 times faster than baseline
GIZA++, with comparable results in end-to-end
French-, Chinese-, and Arabic-to-English transla-
tion experiments.
However, the simplicity of the IBM model 2
also leads to a limitation. As demonstrated in
this study, fast align does not perform well
when applied to language pairs with drastically
different word orders, e.g., Japanese and English.
The problem is because of the IBM model 2’s in-
trinsic inability to handle complex distortions. In
this study, we propose a simple and efficient re-
ordering approach to improve the fast align’s
performance in such situations, referred to as
segmenting-reversing (seg rev). Our motivation
is to apply a rough but robust reordering to make
the source and target sentences have more simi-
lar word orders, where fast align can show
its power. Specifically, seg rev first segments
a source-target sentence pair into a sequence of
minimal monotone chunk pairs6 based on the au-
tomatically generated word alignment. Within the
chunk pairs, source word sequences are exam-
ined to determine whether they should be com-
pletely reversed or the original order should be
retained. The objective of this step is to con-
vert the source sentence to a roughly target-like
word order. The seg rev process is applied re-
cursively but not deeply (only twice in our ex-
</bodyText>
<footnote confidence="0.9999795">
5https://github.com/clab/fast_align
6same as the “tuple” used in Mari˜no et al. (2006)
</footnote>
<page confidence="0.883011">
1034
</page>
<note confidence="0.864425">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1034–1039,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.999823725490196">
F reverse 4
the
.
this
signal
by
cutting
unit
8
is
activated
the
.
この
信号
により
切断
部
8
が
作動
する
。
the
cutting
unit
8
is
activated
by
this
signal
.
この
信号
により
切断
部
8
が
作動
する F reverse 4
。
signal by
this activated
is
8
unit
cutting
F 4
rev.
</figure>
<figureCaption confidence="0.983172">
Figure 1: Example of seg rev applied to a word-aligned English-Japanese sentence pair. Based on the
word alignment, the source sentence is reordered in a target-like order after applying seg rev twice.
</figureCaption>
<bodyText confidence="0.999903066666667">
periments) for each source sentence in the train-
ing data. Consequently, the seg rev process is
lightweight and shallow. Local word sequences,
except those at chunk boundaries, are not scram-
bled, while global word orders are re-arranged if
there are large chunks.
Our primary experimental results for Japanese-
English translation show that applying seg rev
significantly improves fast align’s perfor-
mance to a level comparable to GIZA++. The
training time becomes 2–4 times that of a baseline
fast align, which is still at least 2 – 4 times
faster than the training time required by base-
line GIZA++. Results for German-, French-, and
Chinese-English translations are also reported.
</bodyText>
<sectionHeader confidence="0.924541" genericHeader="method">
2 Segmenting-Reversing Reordering
</sectionHeader>
<bodyText confidence="0.999944156862745">
The seg rev is inspired by the “REV preorder”
(Katz-Brown and Collins, 2008), which is a sim-
ple pre-reordering approach originally designed
for the Japanese-to-English translation task. More
efficient pre-reordering approaches usually require
trained parsers and sophisticated machine learning
frameworks (de Gispert et al., 2015; Hoshino et
al., 2015). We adopt the REV method in Katz-
Brown and Collins (2008) considering it is the
simplest and lightest pre-reordering approach (to
our knowledge), which may bring a minimal ef-
fect on the efficiency of fast align.
An example seg rev process, where the word
alignment is generated by fast align, is illus-
trated in Fig. 1. The example we selected has rel-
atively correct word alignment and seg rev per-
forms well. In general cases, the alignment has
significant noise and the reordering is rougher.
Algorithm 1 describes the repeated (6 times)
application of the seg rev process, and Algo-
rithm 2 describes a single application. Specifi-
cally, Algorithm 1 applies Algorithm 2 6 times.
For each application of Algorithm 2, source sen-
tence 5 and source indices in alignment A are
reordered, and the overall permutation RI is up-
dated and recorded. In Fig. 1, the original En-
glish sentence had 10 words (including the pe-
riod), being indexed as [0, 1, 2, 3, 4, 5, 6, 7, 8, 9].
After the first application of seg rev, RI was
[8,7,6,5,4,3,2,1,0, 9], and after the second appli-
cation, RI was [7,8 ,6, 1,2,3,4,5 ,0, 9] (reversed
parts are boxed).
In Algorithm 2, the main for loop (line 3) scans
the source sentence from the beginning to the end
to obtain monotone segmentation. The foreach
(line 5) and if (line 11) are general phrase pair ex-
traction process. The if (line 13) guarantees that
the chunk is monotone on the target side. The
rev function (line 16), which is described in Al-
gorithm 3, determines whether the sub-sequence
from sstart to send should be reversed by exam-
ining the related alignment Asub. For example,
in the first application shown in Fig.1, two sub-
sequences [0 : 8] and [9 : 9] are processed by rev
and [0 : 8] is reversed. Four sub-sequences [0 : 1],
[2 : 2], [3 : 7], and [9 : 9] are processed in the sec-
ond application and [0:1] and [3:7] are reversed.7
Finally, source sentence 5 and source indices in
alignment A are reordered (lines 19 – 20).8
Algorithm 3 performs the reversal. We count
the concordant and discordant pairs9 and reverse
</bodyText>
<footnote confidence="0.9946338">
7The sub-sequences are based on the input, not the origi-
nal sentence, e.g., sub-sequence[0:1]contains the 8th and 7th
word of the original source sentence in the 2nd application.
8Unaligned words between chunks on the source side are
problematic. They are not touched by line 18. Although they
can be attached to preceding or succeeding chunks, we do
not use further heuristics to handle them. An example is the
drifting “the” in the English sentence in Fig. 1, which our
approach cannot handle properly.
9As used in Kendall’s τ or Goodman and Kruskal’s γ.
</footnote>
<page confidence="0.994336">
1035
</page>
<bodyText confidence="0.999964973684211">
the sub-sequence if and only if there are more dis-
cordant pairs than the concordant pairs. In Fig.1,
the sub-sequence [0: 8] in the first application has
C28 = 28 pairs of aligned word pair (i.e., 28 gray
block pairs for eight gray blocks); however, only
11 pairs are concordant (C25 =10 pairs in [1:5] and
one pair in [7:8]), Consequently, the sub-sequence
[0 : 8] is reversed because there are more discor-
dant pairs (17 = 28−11). The two reversed sub-
sequences in the second application are obvious.
Algorithm 4 describes the training frame-
work, where fast align and seg rev are ap-
plied alternately. To generate word alignment,
fast align is run bi-directionally and sym-
metrization heuristics are applied to reduce noise
(line 11). In each iteration, the source sen-
tences for seg rev are the original sentences,
and fast align uses the reordered sentences
with the exception of the first iteration. The word
alignment generated is thus based on the reordered
source sentences; consequently, the recorded per-
mutation (line 14) is used to recover word align-
ment before the next iteration. The permutation is
a one-to-one mapping; therefore, recovering is re-
alized by the inverse mapping of the permutation,
which transfers the source-side word alignment in-
dices to match the original source sentences.
The time complexity of Algorithm 3 is O(l2),
where l is the size of Asub that is related to the
chunk size. If the average chunk size is a constant
C depending on languages pairs or data sets, then
the time complexity of Algorithm 2 is O(C · I2)
assuming J and the size of A are both linear
against I. The average chunk size will be reduced
when seg rev is applied successively; there-
fore, the time required for subsequent seg rev
processes will decrease. In practice, compared
with the training time required by fast align,
</bodyText>
<figure confidence="0.984944685185185">
Algorithm 1: seg reva
input : same as Algorithm 2 and a depth δ;
output : same as Algorithm 2 except RA;
1 RI ← [0,···, I]; RS ← S; RA ← A;
2 for i ← 1 to δ do
3 RS, RA, RI ← seg rev (RS, T, RA);
4 permute RI with RI,;
5 return RS, RI;
Algorithm 2: seg rev
input : source sentence S = s0, · · · ,sI;
target sentence T = t0 · · · , tJ;
word alignment A = {(i, j)|
si, tj are aligned, i ∈ [0, I], j ∈ [0, J]};
output : reordered source sentence RS;
source index reordered alignmentRA;
reordered indices RI = ir(0, · · · , I),
which is a permutation from 0 to I;
1 RI ← [0, ··· , I];
2 sstart ← 0; tpre end ← −1;
3 for send ← sstart to I do
tstart ← J + 1; tend ← −1;
foreach (i, j) ∈ A do
if i ∈ [sstart, send] then
if j &lt; tstart then
tstart ← j;
if j &gt; tend then
tend ← j;
if ∃(i, j) ∈ A :
j ∈ [tstart, tend] ∧ i ∈/ [sstart, send] then
continue;
if ∃(i, j) ∈ A : j ∈ (tpre end, tstart) then
continue;
Asub ← {(i, j)|(i, j) ∈ A,
i ∈ [sstart, send] ∧ j ∈ [tstart, tend]};
rev (RI[sstart : send], Asub);
tpre end ← tend;
sstart ← min({i|i&gt;send ∧ ∃(i, j) ∈ A});
19 RS ← permute S according to RI;
20 RA ← permute i in A according to RI;
21 return RS, RA, RI;
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
3
4
5
6
7
8
9
Algorithm 3: rev
input : index sequence Isub; word alignment Asub;
1 con ← 0; dis ← 0;
2 foreach unordered tuple ((i0, j0), (i1, j1)) :
(i0, j0) ∈ Asub ∧ (i1, j1) ∈ Asub do
x ← (i1 − i0) × (j1 − j0);
if x &gt; 0 then
con ← con + 1;
else if x &lt; 0 then
dis ← dis + 1;
continue;
10 if dis &gt; con then
11 Isub ← reverse Isub;
Algorithm 4: fast align with seg rev
input : parallel corpus C with N sentence pairs
C = {(S1, T1) ··· , (SN, T N)};
maximum iteration M; depth δ for seg rev;
output : word alignment A = {A1, · · · , AN};
1 A ← 0;
2 for iter ← 1 to M do
I ← 0; C&apos; ← 0;
if A =6 0 then
for n ← 1 to N do
RS,RI ←seg revδ(Sn, Tn, An);
append (RS, Tn) to Cl;
append RI to I;
else
C&apos; ← C;
A ← sym ◦ fast align (C&apos;);
if I =6 0 then
for n ← 1 to N do
recover An by I [n];
15 return A;
3
4
5
6
7
8
9
10
11
12
13
14
else
</figure>
<page confidence="0.990282">
1036
</page>
<bodyText confidence="0.986770333333333">
seg rev processing time is negligible. Note that
seg rev processes are accelerated easily by par-
allel processing.
</bodyText>
<sectionHeader confidence="0.993708" genericHeader="evaluation">
3 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.999924575">
We applied the proposed approach to Japanese-
English translation, a language pair with dramat-
ically different word orders. In addition, we ap-
plied the approach to German-English translation,
a language pair with relatively different word or-
ders among European languages.
For Japanese-English translation, we used
NTCIR-7 PAT-MT data (Fujii et al., 2008). For
German-English translation, we used the Eu-
roparl v7 corpus10 (Koehn, 2005) for training, the
WMT 0811 / WMT 0912 test sets for development
/ testing, respectively. Default settings for the PB
SMT in MOSES13 (Koehn et al., 2007) were used,
except for Japanese-English translations where the
distortion-limit was set to 12 to reach a recently
reported baseline (Isozaki et al., 2012). MERT
(Och, 2003) was used to tune development set pa-
rameter weights and BLEU (Papineni et al., 2002)
was used on test sets to evaluate the translation
performance. Bootstrap sampling (Koehn, 2004)
was employed to test statistical significance using
bleu kit14.
We compared GIZA++ and fast align with
default settings. GIZA++ was used as a mod-
ule of MOSES. The bi-directional outputs of
fast align were symmetrized by atools in
cdec15 (Dyer et al., 2010), and further training
steps were conducted using MOSES. grow-diag-
final-and symmetrization was used consistently in
the experiments. For the the proposed approach,
we set S = 2 and M = 4 in Algorithm 4. Note that
S can be set to a larger value and seg rev could
be applied repeatedly until no additional reorder-
ing is possible. As mentioned, the word alignment
is noisy and our intention is a robust and rough
process; therefore, we restricted seg rev to two
applications and did not consider the difference
in sentence lengths or different languages during
training. Within each iteration, fast align was
run with default settings, except initial diagonal-
</bodyText>
<footnote confidence="0.999851571428572">
10http://www.statmt.org/europarl/
11http://www.statmt.org/wmt08/
12http://www.statmt.org/wmt09/
13http://www.statmt.org/moses/
14http://www.nlp.mibel.cs.tsukuba.ac.
jp/bleu_kit/
15http://www.cdec-decoder.org/
</footnote>
<table confidence="0.999754142857143">
ja-en en-ja de-en en-de
GIZA++ 28.8 30.8 18.2 12.9
FAλini=4.0 28.1‡ 29.5‡ 18.0† 12.7†
FAλini=0.1 28.0‡ 29.8‡ 17.5‡ 12.5‡
iteration 2 28.3† 30.9 17.9‡ 12.8
iteration 3 28.4† 30.1‡ 18.1 12.7†
iteration 4 28.8 30.7 18.1 12.7†
</table>
<tableCaption confidence="0.99984">
Table 1: Test set BLEU scores for Japanese-
</tableCaption>
<bodyText confidence="0.941510780487805">
English and German-English translations. (‡, sta-
tistical significance at p&lt;0.01; †, at p&lt;0.05; bold-
face, no significance; all compared with GIZA++)
tension (Aini) was set to 0.1 in the first iteration,
to avoid overly strong monotone preference at the
beginning of training.
Experimental results for Japanese-English and
German-English translations in both directions
are listed in Table 1. The first two rows show
the baseline performance. fast align (using
a default Aini = 4.0) performance was statisti-
cally significantly lower than GIZA++, particu-
larly for Japanese-English translation. The fol-
lowing four rows show the results of the pro-
posed approach. For the first iteration, Aini was
set to 0.1, and the performance did not change
significantly. The translations from English im-
proved (equal to GIZA++) at the second itera-
tion. However, translations to English improved
more slowly. We attribute the difference in im-
provement rates between translation to and from
English to the relatively fixed word order of En-
glish, whereby the reordering process is easier
and more consistent. Note that once transla-
tions from English improved in the second iter-
ation, performance decreased in the following it-
erations. The results in Table 1 were obtained us-
ing predictable-seed for tuning, which generated
determinate results. Another attempt using ran-
dom seeds to tune returned test set BLEU scores
of 30.5, 30.4 on en-ja and 12.8, 12.8 on en-
de, for iterations 3 and 4, respectively. These
four scores had no statistical significance against
GIZA++. The instability is largely due to the
alignment of function words, which affects trans-
lation performance (Riesa et al., 2011). The align-
ment does not change significantly after the sec-
ond iteration; however, it is unstable around func-
tion words,16 because seg rev does not process
16Specifically, to, articles, and prepositions in English-
Japanese; of, have, and relative pronouns in English-German.
</bodyText>
<page confidence="0.950373">
1037
</page>
<table confidence="0.99830125">
fr-en zh-en
GIZA++ 23.3 31.4
FAλini=4.0 23.1 31.7
iteration 2 23.1 31.7
</table>
<tableCaption confidence="0.994737">
Table 2: Test set BLEU scores for French-to-
</tableCaption>
<bodyText confidence="0.993118413793103">
English and Chinese-to-English translations. For
fr-en, the data sets were the same as for de-en.
For zh-en, NIST 2006 OpenMT data were used
for training and test; test data from 2002 to 2005
OpenMT were used for tuning.
unaligned function words between chunks. Our
approach is too rough to handle function words
precisely. We plan to address this in future.
We also tested our approach on French- and
Chinese-to-English translations. The results are
listed in Table 2. GIZA++ and fast align
showed no statistically significant difference in
performance, which is consistent with Dyer et al.
(2013). The proposed approach did not affect
performance for French- and Chinese-to-English
translations. These results are expected as these
language pairs have similar word orders.
With regard to processing time, a naive, single-
thread implementation of seg rev in C++ took
approximately 60s / 40s in the first / second ap-
plication on the entire Japanese-English corpus17.
The recover process took less than 30s in each it-
eration. In contrast, fast align, although very
fast, took approximately one hour for one round
of training (using five iterations for its log-linear
model) on the same corpus. Therefore, the addi-
tional time required in our approach is quite small
and can be ignored compared with the training
time of fast align.18
</bodyText>
<sectionHeader confidence="0.996724" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9534516">
We have proposed a simple and efficient approach
to improve the performance of fast align on
language pairs with drastically different word or-
ders. With the proposed approach, fast align
obtained results comparable with GIZA++, and its
efficiency is retained. We are investigating further
properties of seg rev and plan to extend it to
achieve greater stability and efficiency.19
171.8 M sentence pairs with an average length of 35 words.
18GIZA++ actually took around 18 hours to align the
Japanese-English corpus (parallel processes for two direc-
tions, including mkcls, five iterations for the model 1 and
the HMM model, three iteration for the model 3 and 4).
19Ongoing experiments using seg rev with GIZA++ re-
turned negative results. BLEU decreases by approx. 1 point.
</bodyText>
<sectionHeader confidence="0.994942" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997056">
We thank Dr. Atsushi Fujita for his helpful discus-
sions on this work.
</bodyText>
<sectionHeader confidence="0.990192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999849979166667">
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201–228.
Adri`a de Gispert, Gonzalo Iglesias, and Bill Byrne.
2015. Fast and accurate preordering for SMT us-
ing neural networks. In Proc. of NAACL-HLT, pages
1012–1017.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proc. of ACL (System Demonstrations), pages 7–
12.
Chris Dyer, Victor Chahuneau, and Noah A Smith.
2013. A simple, fast, and effective reparameteri-
zation of IBM model 2. In Proc. of NAACL-HLT,
pages 644–648.
Atsushi Fujii, Masao Utiyama, Mikio Yamamoto,
Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-
ya, and Sayori Shimohata. 2008. Overview of the
patent translation task at the NTCIR-7 workshop. In
Proc. of NTCIR, pages 389–400.
Sho Hoshino, Yusuke Miyao, Katsuhito Sudoh, Kat-
suhiko Hayashi, and Masaaki Nagata. 2015. Dis-
criminative preordering meets Kendall’s Tau max-
imization. In Proc. of ACL (Short Papers), pages
139–144.
Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and
Kevin Duh. 2012. HPSG-based preprocessing
for English-to-Japanese translation. ACM Transac-
tions on Asian Language Information Processing,
11(3):8.
Jason Katz-Brown and Michael Collins. 2008. Syn-
tactic reordering in preprocessing for Japanese →
English translation: MIT system description for
NTCIR-7 patent translation task. In Proc. of NTCIR,
pages 409–414.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL, pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proc. of ACL (Demo and Poster Sessions),
pages 177–180.
</reference>
<page confidence="0.807368">
1038
</page>
<reference confidence="0.999879875">
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP,
pages 388–395.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proc. of MT sum-
mit, pages 79–86.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. of ACL, pages 609–616.
Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrik Lambert, Jos´e A. R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527–549.
Graham Neubig, Taro Watanabe, Eiichiro Sumita,
Shinsuke Mori, and Tatsuya Kawahara. 2011. An
unsupervised model for joint phrase alignment and
extraction. In Proc. of ACL-HLT, pages 632–641.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311–318.
Jason Riesa, Ann Irvine, and Daniel Marcu. 2011.
Feature-rich language-independent syntax-based
alignment for statistical machine translation. In
Proc. of EMNLP, pages 497–507.
</reference>
<page confidence="0.995967">
1039
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.428739">
<title confidence="0.998946">align Reordering</title>
<author confidence="0.51615">Chenchen Ding</author>
<author confidence="0.51615">Masao Utiyama</author>
<author confidence="0.51615">Eiichiro</author>
<affiliation confidence="0.6165325">Multilingual Translation National Institute of Information and Communications</affiliation>
<address confidence="0.874816">3-5 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0289,</address>
<email confidence="0.989513">mutiyama,</email>
<abstract confidence="0.9988445">align a simple, fast, and efficient approach for word alignment based the IBM model align performs well for language pairs with relatively similar word orders; however, it does not perform well for language pairs with drastically different word orders. We a reorderto solve this problem by alapplying align reordering source sentences during training. Experimental results with Japanese- English translation demonstrate that the proposed approach improves the perof align without the loss of efficiency. Experiments using other languages are also reported.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1325" citStr="Chiang, 2007" startWordPosition="192" endWordPosition="193">nces during training. Experimental results with JapaneseEnglish translation demonstrate that the proposed approach improves the performance of fast align significantly without the loss of efficiency. Experiments using other languages are also reported. 1 Introduction Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2, Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1http://www.statmt.org/moses/giza/ GIZA++.html 2https://code.google.com/p/ berkeleyaligner/ 3http://jasonriesa.github.io/nile/ 4http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach ba</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
<author>Gonzalo Iglesias</author>
<author>Bill Byrne</author>
</authors>
<title>Fast and accurate preordering for SMT using neural networks.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>1012--1017</pages>
<marker>de Gispert, Iglesias, Byrne, 2015</marker>
<rawString>Adri`a de Gispert, Gonzalo Iglesias, and Bill Byrne. 2015. Fast and accurate preordering for SMT using neural networks. In Proc. of NAACL-HLT, pages 1012–1017.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In Proc. of ACL (System Demonstrations),</booktitle>
<pages>7--12</pages>
<contexts>
<context position="13236" citStr="Dyer et al., 2010" startWordPosition="2331" endWordPosition="2334">l., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14. We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach, we set S = 2 and M = 4 in Algorithm 4. Note that S can be set to a larger value and seg rev could be applied repeatedly until no additional reordering is possible. As mentioned, the word alignment is noisy and our intention is a robust and rough process; therefore, we restricted seg rev to two applications and did not consider the difference in sentence lengths or different languages during training. Within each iteration, fast alig</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proc. of ACL (System Demonstrations), pages 7– 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Victor Chahuneau</author>
<author>Noah A Smith</author>
</authors>
<title>A simple, fast, and effective reparameterization of IBM model 2. In</title>
<date>2013</date>
<booktitle>Proc. of NAACL-HLT,</booktitle>
<pages>644--648</pages>
<contexts>
<context position="1875" citStr="Dyer et al., 2013" startWordPosition="257" endWordPosition="260"> 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2, Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1http://www.statmt.org/moses/giza/ GIZA++.html 2https://code.google.com/p/ berkeleyaligner/ 3http://jasonriesa.github.io/nile/ 4http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantage of the simplicity of the IBM model 2, fast align introduces a “tension” parameter to model the overall accordance of word orders and an efficient parameter re-estimation algorithm is devised. It has been reported that the fast align approach is more than 10 times faster than baseline GIZA++, with comparable results in end-to-end French-, Chinese-, and Arabic-to-English translation experiments. Howe</context>
<context position="17031" citStr="Dyer et al. (2013)" startWordPosition="2913" endWordPosition="2916">scores for French-toEnglish and Chinese-to-English translations. For fr-en, the data sets were the same as for de-en. For zh-en, NIST 2006 OpenMT data were used for training and test; test data from 2002 to 2005 OpenMT were used for tuning. unaligned function words between chunks. Our approach is too rough to handle function words precisely. We plan to address this in future. We also tested our approach on French- and Chinese-to-English translations. The results are listed in Table 2. GIZA++ and fast align showed no statistically significant difference in performance, which is consistent with Dyer et al. (2013). The proposed approach did not affect performance for French- and Chinese-to-English translations. These results are expected as these language pairs have similar word orders. With regard to processing time, a naive, singlethread implementation of seg rev in C++ took approximately 60s / 40s in the first / second application on the entire Japanese-English corpus17. The recover process took less than 30s in each iteration. In contrast, fast align, although very fast, took approximately one hour for one round of training (using five iterations for its log-linear model) on the same corpus. Theref</context>
</contexts>
<marker>Dyer, Chahuneau, Smith, 2013</marker>
<rawString>Chris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. In Proc. of NAACL-HLT, pages 644–648.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Atsushi Fujii</author>
</authors>
<title>Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, Terumasa Ehara, Hiroshi Echizenya, and Sayori Shimohata.</title>
<date>2008</date>
<booktitle>In Proc. of NTCIR,</booktitle>
<pages>389--400</pages>
<marker>Fujii, 2008</marker>
<rawString>Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, Takehito Utsuro, Terumasa Ehara, Hiroshi Echizenya, and Sayori Shimohata. 2008. Overview of the patent translation task at the NTCIR-7 workshop. In Proc. of NTCIR, pages 389–400.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sho Hoshino</author>
<author>Yusuke Miyao</author>
<author>Katsuhito Sudoh</author>
<author>Katsuhiko Hayashi</author>
<author>Masaaki Nagata</author>
</authors>
<title>Discriminative preordering meets Kendall’s Tau maximization.</title>
<date>2015</date>
<booktitle>In Proc. of ACL (Short Papers),</booktitle>
<pages>139--144</pages>
<contexts>
<context position="5433" citStr="Hoshino et al., 2015" startWordPosition="834" endWordPosition="837">ing time becomes 2–4 times that of a baseline fast align, which is still at least 2 – 4 times faster than the training time required by baseline GIZA++. Results for German-, French-, and Chinese-English translations are also reported. 2 Segmenting-Reversing Reordering The seg rev is inspired by the “REV preorder” (Katz-Brown and Collins, 2008), which is a simple pre-reordering approach originally designed for the Japanese-to-English translation task. More efficient pre-reordering approaches usually require trained parsers and sophisticated machine learning frameworks (de Gispert et al., 2015; Hoshino et al., 2015). We adopt the REV method in KatzBrown and Collins (2008) considering it is the simplest and lightest pre-reordering approach (to our knowledge), which may bring a minimal effect on the efficiency of fast align. An example seg rev process, where the word alignment is generated by fast align, is illustrated in Fig. 1. The example we selected has relatively correct word alignment and seg rev performs well. In general cases, the alignment has significant noise and the reordering is rougher. Algorithm 1 describes the repeated (6 times) application of the seg rev process, and Algorithm 2 describes </context>
</contexts>
<marker>Hoshino, Miyao, Sudoh, Hayashi, Nagata, 2015</marker>
<rawString>Sho Hoshino, Yusuke Miyao, Katsuhito Sudoh, Katsuhiko Hayashi, and Masaaki Nagata. 2015. Discriminative preordering meets Kendall’s Tau maximization. In Proc. of ACL (Short Papers), pages 139–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Isozaki</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
<author>Kevin Duh</author>
</authors>
<title>HPSG-based preprocessing for English-to-Japanese translation.</title>
<date>2012</date>
<journal>ACM Transactions on Asian Language Information Processing,</journal>
<volume>11</volume>
<issue>3</issue>
<contexts>
<context position="12781" citStr="Isozaki et al., 2012" startWordPosition="2256" endWordPosition="2259">rs. In addition, we applied the approach to German-English translation, a language pair with relatively different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14. We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the </context>
</contexts>
<marker>Isozaki, Sudoh, Tsukada, Duh, 2012</marker>
<rawString>Hideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, and Kevin Duh. 2012. HPSG-based preprocessing for English-to-Japanese translation. ACM Transactions on Asian Language Information Processing, 11(3):8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Katz-Brown</author>
<author>Michael Collins</author>
</authors>
<title>Syntactic reordering in preprocessing for Japanese → English translation: MIT system description for NTCIR-7 patent translation task.</title>
<date>2008</date>
<booktitle>In Proc. of NTCIR,</booktitle>
<pages>409--414</pages>
<contexts>
<context position="5157" citStr="Katz-Brown and Collins, 2008" startWordPosition="798" endWordPosition="801">nk boundaries, are not scrambled, while global word orders are re-arranged if there are large chunks. Our primary experimental results for JapaneseEnglish translation show that applying seg rev significantly improves fast align’s performance to a level comparable to GIZA++. The training time becomes 2–4 times that of a baseline fast align, which is still at least 2 – 4 times faster than the training time required by baseline GIZA++. Results for German-, French-, and Chinese-English translations are also reported. 2 Segmenting-Reversing Reordering The seg rev is inspired by the “REV preorder” (Katz-Brown and Collins, 2008), which is a simple pre-reordering approach originally designed for the Japanese-to-English translation task. More efficient pre-reordering approaches usually require trained parsers and sophisticated machine learning frameworks (de Gispert et al., 2015; Hoshino et al., 2015). We adopt the REV method in KatzBrown and Collins (2008) considering it is the simplest and lightest pre-reordering approach (to our knowledge), which may bring a minimal effect on the efficiency of fast align. An example seg rev process, where the word alignment is generated by fast align, is illustrated in Fig. 1. The e</context>
</contexts>
<marker>Katz-Brown, Collins, 2008</marker>
<rawString>Jason Katz-Brown and Michael Collins. 2008. Syntactic reordering in preprocessing for Japanese → English translation: MIT system description for NTCIR-7 patent translation task. In Proc. of NTCIR, pages 409–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1263" citStr="Koehn et al., 2003" startWordPosition="179" endWordPosition="182">oblem by alternately applying fast align and reordering source sentences during training. Experimental results with JapaneseEnglish translation demonstrate that the proposed approach improves the performance of fast align significantly without the loss of efficiency. Experiments using other languages are also reported. 1 Introduction Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2, Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1http://www.statmt.org/moses/giza/ GIZA++.html 2https://code.google.com/p/ berkeleyaligner/ 3http://jasonriesa.github.io/nile/ 4http://www.phontron.com/pialign/ fast align5 (Dyer e</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. of HLT-NAACL, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL (Demo and Poster Sessions),</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="12627" citStr="Koehn et al., 2007" startWordPosition="2233" endWordPosition="2236">ing. 3 Experiments and Discussion We applied the proposed approach to JapaneseEnglish translation, a language pair with dramatically different word orders. In addition, we applied the approach to German-English translation, a language pair with relatively different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14. We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et a</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of ACL (Demo and Poster Sessions), pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="12979" citStr="Koehn, 2004" startWordPosition="2290" endWordPosition="2291">T data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14. We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach, we set S = 2 and M = 4 in Algorithm 4. Note that S can be set to a larger value and seg rev could be applied repeatedly until no additional reordering is possible. As mentioned, t</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proc. of EMNLP, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of MT summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="12474" citStr="Koehn, 2005" startWordPosition="2208" endWordPosition="2209">4 5 6 7 8 9 10 11 12 13 14 else 1036 seg rev processing time is negligible. Note that seg rev processes are accelerated easily by parallel processing. 3 Experiments and Discussion We applied the proposed approach to JapaneseEnglish translation, a language pair with dramatically different word orders. In addition, we applied the approach to German-English translation, a language pair with relatively different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14. We compared GIZA++ and fast al</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proc. of MT summit, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1420" citStr="Liu et al., 2006" startWordPosition="204" endWordPosition="207">t the proposed approach improves the performance of fast align significantly without the loss of efficiency. Experiments using other languages are also reported. 1 Introduction Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2, Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1http://www.statmt.org/moses/giza/ GIZA++.html 2https://code.google.com/p/ berkeleyaligner/ 3http://jasonriesa.github.io/nile/ 4http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order </context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proc. of ACL, pages 609–616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e B Mari˜no</author>
<author>Rafael E Banchs</author>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrik Lambert</author>
<author>Jos´e A R Fonollosa</author>
<author>Marta R Costa-Juss`a</author>
</authors>
<title>N-grambased machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-Juss`a, 2006</marker>
<rawString>Jos´e B. Mari˜no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos´e A. R. Fonollosa, and Marta R. Costa-Juss`a. 2006. N-grambased machine translation. Computational Linguistics, 32(4):527–549.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Shinsuke Mori</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>An unsupervised model for joint phrase alignment and extraction.</title>
<date>2011</date>
<booktitle>In Proc. of ACL-HLT,</booktitle>
<pages>632--641</pages>
<contexts>
<context position="1681" citStr="Neubig et al., 2011" startWordPosition="245" endWordPosition="248">rt statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2, Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1http://www.statmt.org/moses/giza/ GIZA++.html 2https://code.google.com/p/ berkeleyaligner/ 3http://jasonriesa.github.io/nile/ 4http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantage of the simplicity of the IBM model 2, fast align introduces a “tension” parameter to model the overall accordance of word orders and an efficient parameter re-estimation algorithm is devised. It has been rep</context>
</contexts>
<marker>Neubig, Watanabe, Sumita, Mori, Kawahara, 2011</marker>
<rawString>Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shinsuke Mori, and Tatsuya Kawahara. 2011. An unsupervised model for joint phrase alignment and extraction. In Proc. of ACL-HLT, pages 632–641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1477" citStr="Och and Ney, 2003" startWordPosition="212" endWordPosition="215"> align significantly without the loss of efficiency. Experiments using other languages are also reported. 1 Introduction Aligning words in a parallel corpus is a basic task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2, Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1http://www.statmt.org/moses/giza/ GIZA++.html 2https://code.google.com/p/ berkeleyaligner/ 3http://jasonriesa.github.io/nile/ 4http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantage of </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="12799" citStr="Och, 2003" startWordPosition="2261" endWordPosition="2262">he approach to German-English translation, a language pair with relatively different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14. We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach,</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="12883" citStr="Papineni et al., 2002" startWordPosition="2274" endWordPosition="2277">ly different word orders among European languages. For Japanese-English translation, we used NTCIR-7 PAT-MT data (Fujii et al., 2008). For German-English translation, we used the Europarl v7 corpus10 (Koehn, 2005) for training, the WMT 0811 / WMT 0912 test sets for development / testing, respectively. Default settings for the PB SMT in MOSES13 (Koehn et al., 2007) were used, except for Japanese-English translations where the distortion-limit was set to 12 to reach a recently reported baseline (Isozaki et al., 2012). MERT (Och, 2003) was used to tune development set parameter weights and BLEU (Papineni et al., 2002) was used on test sets to evaluate the translation performance. Bootstrap sampling (Koehn, 2004) was employed to test statistical significance using bleu kit14. We compared GIZA++ and fast align with default settings. GIZA++ was used as a module of MOSES. The bi-directional outputs of fast align were symmetrized by atools in cdec15 (Dyer et al., 2010), and further training steps were conducted using MOSES. grow-diagfinal-and symmetrization was used consistently in the experiments. For the the proposed approach, we set S = 2 and M = 4 in Algorithm 4. Note that S can be set to a larger value and</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Riesa</author>
<author>Ann Irvine</author>
<author>Daniel Marcu</author>
</authors>
<title>Feature-rich language-independent syntax-based alignment for statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>497--507</pages>
<contexts>
<context position="1645" citStr="Riesa et al., 2011" startWordPosition="239" endWordPosition="242"> task for almost all state-of-the-art statistical machine translation (SMT) systems. Word alignment is used to extract translation rules in various way, such as the phrase pairs used in a phrase-based (PB) SMT system (Koehn et al., 2003), the hierarchical rules used in a HIERO system (Chiang, 2007), and the sophisticated translation templates used in tree-based SMT systems (Liu et al., 2006). Among different approaches, GIZA++1 (Och and Ney, 2003), which is based on the IBM translation models, is the most widely used word alignment tool. Other well-known tools are the BerkeleyAligner2, Nile3 (Riesa et al., 2011), and pialign4 (Neubig et al., 2011). 1http://www.statmt.org/moses/giza/ GIZA++.html 2https://code.google.com/p/ berkeleyaligner/ 3http://jasonriesa.github.io/nile/ 4http://www.phontron.com/pialign/ fast align5 (Dyer et al., 2013) is a recently proposed word alignment approach based on the reparameterization of the IBM model 2, which is usually referred to as a zero-order alignment model (Och and Ney, 2003). Taking advantage of the simplicity of the IBM model 2, fast align introduces a “tension” parameter to model the overall accordance of word orders and an efficient parameter re-estimation a</context>
<context position="16039" citStr="Riesa et al., 2011" startWordPosition="2755" endWordPosition="2758">ess is easier and more consistent. Note that once translations from English improved in the second iteration, performance decreased in the following iterations. The results in Table 1 were obtained using predictable-seed for tuning, which generated determinate results. Another attempt using random seeds to tune returned test set BLEU scores of 30.5, 30.4 on en-ja and 12.8, 12.8 on ende, for iterations 3 and 4, respectively. These four scores had no statistical significance against GIZA++. The instability is largely due to the alignment of function words, which affects translation performance (Riesa et al., 2011). The alignment does not change significantly after the second iteration; however, it is unstable around function words,16 because seg rev does not process 16Specifically, to, articles, and prepositions in EnglishJapanese; of, have, and relative pronouns in English-German. 1037 fr-en zh-en GIZA++ 23.3 31.4 FAλini=4.0 23.1 31.7 iteration 2 23.1 31.7 Table 2: Test set BLEU scores for French-toEnglish and Chinese-to-English translations. For fr-en, the data sets were the same as for de-en. For zh-en, NIST 2006 OpenMT data were used for training and test; test data from 2002 to 2005 OpenMT were us</context>
</contexts>
<marker>Riesa, Irvine, Marcu, 2011</marker>
<rawString>Jason Riesa, Ann Irvine, and Daniel Marcu. 2011. Feature-rich language-independent syntax-based alignment for statistical machine translation. In Proc. of EMNLP, pages 497–507.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
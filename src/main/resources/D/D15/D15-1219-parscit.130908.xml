<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000100">
<title confidence="0.972075">
Abstractive Multi-document Summarization with Semantic Infor-
mation Extraction
</title>
<author confidence="0.994354">
Wei Li
</author>
<affiliation confidence="0.9812725">
Key Lab of Intelligent Info. Processing,
Institute of Computing Technology, CAS
</affiliation>
<address confidence="0.650262">
Beijing, 100190, China
</address>
<email confidence="0.997814">
weili@kg.ict.ac.cn
</email>
<sectionHeader confidence="0.998593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892785714286">
This paper proposes a novel approach to
generate abstractive summary for multi-
ple documents by extracting semantic in-
formation from texts. The concept of
Basic Semantic Unit (BSU) is defined to
describe the semantics of an event or ac-
tion. A semantic link network on BSUs is
constructed to capture the semantic in-
formation of texts. Summary structure is
planned with sentences generated based
on the semantic link network. Experi-
ments demonstrate that the approach is
effective in generating informative, co-
herent and compact summary.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918826923077">
Most automatic summarization approaches are
extractive which leverage only literal or syntactic
information in documents. Sentences are extract-
ed from the original documents directly by rank-
ing or scoring and only little post-editing is made
(Yih et al., 2007; Wan et al., 2007; Wang et al.,
2008; Wan and Xiao, 2009). Pure extraction has
intrinsic limits compared to abstraction (Carenini
and Cheung, 2008).
Abstractive summarization requires semantic
analysis and abstract representation of texts,
which need knowledge on and beyond the texts
(Zhuge, 2015a). There are some abstractive ap-
proaches in recent years: sentence compression
(Knight and Marcu, 2000; Knight and Marcu,
2002; Cohn and Lapata, 2009), sentence fusion
(Barzilay and McKeown, 2005; Filippova and
Strube, 2008), and sentence revision (Tanaka et
al., 2009). However, these approaches are sen-
tence rewriting techniques based on syntactical
analysis without semantic analysis and abstract
representation.
Fully abstractive summarization approach re-
quires a separate process for the analysis of texts
that serves as an intermediate step before the
generation of sentences (Genest and Lapalme,
2011). Statistics of words or phrases and syntac-
tical analysis that have been widely used in exist-
ing summarization approaches are all shallow
processing of text. It is necessary to explore
summarization methods based on deeper seman-
tic analysis.
We define the concept of Basic Semantic Unit
(BSU) to express the semantics of texts. A BSU
is an action indicator with its obligatory argu-
ments which contain actor and receiver of the
action. BSU is the most basic element of coher-
ent information in texts, which can describe the
semantics of an event or action. The semantic
information of texts is represented by extracting
BSUs and constructing BSU semantic link net-
work (Zhuge, 2009). Semantic Link Network
consists of semantic nodes, semantic links and
reasoning rules (Zhuge, 2010; 2011; 2012;
2015b). The semantic nodes can be any resources.
In this work, the semantic nodes are BSUs ex-
tracted from texts. We use semantic relatedness
between BSUs as semantic links. Then summary
can be generated based on the semantic link net-
work through summary structure planning.
The characteristics of our approaches are as
follows:
</bodyText>
<listItem confidence="0.998130875">
• Each BSU describes the semantics of an
event or action. The semantic relatedness be-
tween BSUs can capture the context seman-
tic relations of texts.
• The BSU semantic link network is an ab-
stract representation of texts. Reduction on
the network can obtain important infor-
mation of texts with no redundancy.
</listItem>
<page confidence="0.933681">
1908
</page>
<note confidence="0.6427925">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1908–1913,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<listItem confidence="0.98221275">
• Summary is built from sentence to sentence
to a coherent body of information based on
the BSU semantic link network by summary
structure planning.
</listItem>
<sectionHeader confidence="0.999099" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999982137254902">
There are some abstractive summarization ap-
proaches in recent years. An approach TTG at-
tempts to generate abstractive summary by using
text-to-text generation to generate sentence for
each subject-verb-object triple (Genest and
Lapalme, 2011). A system that attempts to gen-
erate abstractive summaries for spoken meetings
was proposed (Wang and Cardie, 2013). It iden-
tifies relation instances that are represented by a
lexical indicator with an argument constituent
from texts. Then the relation instances are filled
into templates which are extracted by applying
multiple sequence alignment. Both of these sys-
tems need to select a subset of the large volumes
of generated sentences. However, our system
generates summary directly by summary struc-
ture planning. It can generate well-organized and
coherent summary more effectively.
A recent work aims to generate abstractive
summary based on Abstract Meaning Represen-
tation (AMR) (Liu et al., 2015). It first parses the
source text into AMR graphs, and then trans-
forms them into a summary graph and plans to
generate text from it. This work only focuses on
the graph-to-graph transformation. The module
of text generation from AMR has not been de-
veloped. The nodes and edges of AMR graph are
entities and relations between entities respective-
ly, which are sufficiently different from the BSUs
semantic link network. Moreover, texts can be
generated efficiently from the BSUs network.
Another recent abstractive summarization meth-
od generates new sentences by selecting and
merging phrases from the input documents (Bing
et al., 2015). It first extracts noun phrases and
verb-object phrases from the input documents,
and then calculates saliency scores for them. An
ILP optimization framework is used to simulta-
neously select and merge informative phrases to
maximize the salience of phrases and meanwhile
satisfy the sentence construction constraints. As
the results show that the method is difficult to
generate new informative sentences really differ-
ent from the original sentences and may generate
some none factual sentences since phrases from
different sentences are merged.
Open information extraction has been pro-
posed by (Banko et al., 2007; Etzioni et al.,
2011). They extract binary relations from the
web, which is different from our approach that
extracts events or actions expressed in texts.
</bodyText>
<sectionHeader confidence="0.989254" genericHeader="method">
3 The Summarization Framework
</sectionHeader>
<bodyText confidence="0.99986275">
Our system produces an abstractive summary for
a set of topic related documents. It consists of
two major components: Information extraction
and summary generation.
</bodyText>
<subsectionHeader confidence="0.99648">
3.1 Information Extraction
</subsectionHeader>
<bodyText confidence="0.911244418604651">
The semantic information of texts is obtained by
extracting BSUs and constructing BSU semantic
link network. A BSU is represented as an actor-
action-receiver triple, which can both detects
the crucial content and incorporates enough syn-
tactic information to facilitate the downstream
sentence generation. Some actions may not have
the receiver argument. For example, “Flight
MH370 – disappear” and “Flight MH370 - leave
- Kuala Lumpur” are two BSUs.
BSU Extraction. BSUs are extracted from the
sentences of the documents. The texts are pre-
processed by name entity recognition (Finkel et
al., 2005) and co-reference resolution (Lee et al.,
2011). Constituent and dependency parses are
obtained by Stanford parser (Klein and Manning,
2003). The eligible action indicator is restricted
to be a predicate verb; the eligible actor and re-
ceiver arguments are noun phrase. Both the actor
and receiver arguments take the form of constit-
uents in the parse tree. A valid BSU should have
one action indicator and at least one actor argu-
ment, and satisfy the following constraints:
• The actor argument is the nominal subject or
external subject or the complement of a pas-
sive verb which is introduced by the preposi-
tion “by” and does the action.
• The receiver argument is the direct object or
the passive nominal subject or the object of
a preposition following the action verb.
We create some manual rules and syntactic
constraints to identify all BSUs based on the syn-
tactic structure of sentences in the input texts.
Constructing BSU Semantic Link Network.
The semantic relatedness between BSUs contains
three parts: Arguments Semantic Relatedness
(ASR), Action-Verbs Semantic Relatedness
(VSR) and Co-occurrence in the Same Sentence
(CSS). Arguments of BSUs include actors and
receivers, which both are noun phrases and indi-
cate concepts or entities in the text. When com-
puting ASR, the semantic relatedness between
concepts must be measured. We use the explicit
</bodyText>
<page confidence="0.984591">
1909
</page>
<bodyText confidence="0.99993640625">
semantic analysis based on Wikipedia to com-
pute semantic relatedness between concepts (Ga-
brilovich and Markovitch, 2007). When compu-
ting VSR, WordNet-based measure is used to
calculate the semantic relatedness between action
verbs (Mihalcea et al., 2006). CSS is measured
whether two different BSUs co-occur in the same
sentence. Semantic relations between BSUs are
computed by linearly combining these three parts.
Then BSUs that are extracted from the texts form
a semantic link network.
Semantic Link Network Reduction. A dis-
criminative ranker based on Support Vector Re-
gression (SVR) (Smola and Scholkopf, 2004) is
utilized to assign each BSU a summary-worthy
score. Training data was constructed from the
DUC 2005 datasets which contain both the
source documents and human generated refer-
ence summaries. BSUs are extracted from these
datasets. For each BSU in the source documents,
if it has occurred in the corresponding human
generated summaries or the semantic relatedness
between the BSU and one BSU in the corre-
sponding human generated summaries is above a
threshold δ , then it is considered to be a positive
sample and be assigned 1 to its summary-worthy
score. Otherwise, the BSU is considered to be a
negative sample and be assigned 0 to its sum-
mary-worthy score. Table 1 displays the features
of BSU used in the SVR model. Then the salien-
cy score of each BSU in the semantic link net-
work is calculated by the following equation:
</bodyText>
<equation confidence="0.994785">
Sal BSU =SW ∑ R (1)
( i ) i * j ij
</equation>
<bodyText confidence="0.967602846153846">
Where SWi is the summary-worthy score of
BSUi ; Rij is the semantic relatedness between
BSUi and BSUj.
BSUs in the semantic link network are clus-
tered by hierarchical complete-link clustering
methods. BSUs in each cluster are semantically
similar. For example, Malaysia Airlines plane -
vanish and Flight MH370 – disappear. Only the
most important one with the largest saliency
score is reserved in the network. These less im-
portant BSUs are eliminated. The remaining BSU
semantic link network represents the important
information of the texts with no redundancy.
</bodyText>
<subsectionHeader confidence="0.999658">
3.2 Summary Generation
</subsectionHeader>
<bodyText confidence="0.9998434">
The summary for the documents is generated
directly based on the BSU semantic link network.
The summary should be well-structured and
well-organized. It should not just be a heap of
related information, but should build from sen-
</bodyText>
<subsectionHeader confidence="0.77639">
Basic Features
</subsectionHeader>
<bodyText confidence="0.551970166666667">
Number of words in actor/receiver
Number of nouns in actor/receiver
Number of new nouns in actor/receiver
Actor/receiver has capitalized word?
Actor/receiver has stopword?
Action is a phrasal verb?
</bodyText>
<table confidence="0.8896714">
Content Features
Actor/receiver has name entity?
TF/IDF/TF-IDF of action
TF/IDF/TF-IDF min max average of actor/receiver
Syntax Features
</table>
<tableCaption confidence="0.8844595">
Constituent tag of actor/action/receiver
Dependency relation of action with actor
Dependency relation of action with receiver
Table 1. Features for BSU summary-worthy
</tableCaption>
<bodyText confidence="0.978447842105263">
scoring. We use SVM-light with RBF kernel
by default parameters (Joachims, 1999).
tence to sentence to a coherent body of infor-
mation about a topic.
The summary structure is planned based on
the BSU semantic link network. An optimal path
which covers all the nodes in the network is
found. The following two factors are considered
when finding the optimal path: (1) Context Se-
mantic Coherent. To make the summary seman-
tic coherent, all adjacent sentences should be se-
mantically related. We need to find an optimal
path, in which every two adjacent nodes are
strong semantically related. The optimal path is
denoted as P = [ pr , p, , ..., prn ] and maximize
the theme of generated summary clear-cut, the
important content should be put in prior position.
The order of the ith node in the path is denoted
as ui and its weight is denoted as
</bodyText>
<equation confidence="0.8918225">
wi =1/ Sal (BSUi) and maximize∑n 1 =
iui wi .
</equation>
<bodyText confidence="0.963108833333333">
To combine the above two factors, we need to
find an optimal path which covers each node on-
ly once and has the longest distance. The biased-
sum weight of all nodes in the path should be
maximized. The problem can be proved to be
NP-hard by reduction to TSP problem. It can be
formalized as an integer linear programming
(ILP) as follow. xij is defined to indicate wheth-
er the optimal path goes from node i to node j.
xij
Since each node can be traversed only once,
the following constraints must be satisfied.
</bodyText>
<equation confidence="0.976479818181818">
∑ n x = =
1 j n
1,...
i j i ij
= 1, ≠
n
∑
x = =
1 i 1,...n
= 1, ≠
j j i ij
</equation>
<bodyText confidence="0.96866">
The nodes in the path are sequentially ordered.
If the edge between two nodes is in the path, then
</bodyText>
<equation confidence="0.90829875">
n −
n ∑ = R +
i 1 r r
i i
</equation>
<figure confidence="0.968162222222222">
1 . (2) Clear-cut Theme. To make
1
1
(2)
0 otherwise

=  
1 if the path goes from node i to node j
(3)
</figure>
<page confidence="0.857763">
1910
</page>
<table confidence="0.999290285714286">
System ROUGE-1 ROUGE-2 ROUGE-SU4
OurSystem 0.42145 0.11016 0.15632
MultiMR 0.41967 0.10302 0.15385
RankBSU 0.39123 0.08742 0.14381
TTG 0.39268 0.09645 0.14553
AveDUC 0.39684 0.09495 0.14671
NIST Baseline 0.33126 0.06425 0.11114
</table>
<tableCaption confidence="0.8262495">
Table 2. Comparison results (F-measure) on
DUC 2007 under ROUGE evaluation.
</tableCaption>
<table confidence="0.999602666666667">
System OurSystem MultiMR RankBSU TTG
Pyr (Th:0.6) 0.858 0.845 0.832 0.834
Pyr (Th:0.65) 0.743 0.731 0.718 0.721
</table>
<tableCaption confidence="0.988112">
Table 3. Comparison results on DUC 2007 un-
</tableCaption>
<bodyText confidence="0.96255">
der the automated pyramid evaluation with two
threshold value 0.6 and 0.65.
the order of the two nodes is sequentially close to
each other, which can be formulated as follow:
At last, we can formulate the objective func-
tion as follow:
</bodyText>
<equation confidence="0.9823616">
n n n
max 1 n ∑ ∑ R x λ w u
i = 1 j j i ij ij + ∑
= ≠
1, i=1 i i
</equation>
<bodyText confidence="0.999946777777778">
where parameter λ tunes the effect of the two
parts and n is the quantity of BSUs in the final
BSU semantic link network (after reduction).
Sentence Generation. After the summary
structure has been planned, sentences are gener-
ated for each node in the BSU semantic link net-
work. As the BSU contains enough semantic and
syntactic information, sentence can be generated
efficiently according to the following rules:
</bodyText>
<listItem confidence="0.82503375">
• Generate a Noun Phrase (NP) based on the
actor argument to represent the subject, a NP
based on the receiver argument to represent
the object if present.
• Generate a Verb Phrase (VP) based on the
action verb to link the components above.
The tense of the verb is set to the same as in
the original sentence, and most modifiers like
auxiliaries and negation are conserved.
• Generate complements for the VP when the
BSU has no receiver. The verb modifiers fol-
lowing the action verb such as prepositional
phrases and infinitive phrases can be used as
the complement, in case that the verb would
have no interesting meaning without a com-
plement.
</listItem>
<bodyText confidence="0.999942846153846">
The process of sentence generation for each
node is based on the syntactic structure of the
source sentence where the BSU is extracted from.
The time and location preposition phrases which
are important information of new events are kept.
The generated sentences are organized according
to the summary structure. If some adjacent sen-
tences in the summary have the same subject, the
subject of the latter can be substituted by a pro-
noun (such as it or they) to avoid repetition of
noun phrases. One sample summary generated
by our system for “Malaysia MH370 Disappear”
news is shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="method">
4 Evaluation Results
</sectionHeader>
<subsectionHeader confidence="0.997215">
4.1 Dataset and Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9999905">
In order to evaluate the performance of our sys-
tem, we use two datasets that have been used in
recent multi-document summarization shared
tasks: DUC2005 and DUC2007. Each task has a
gold standard dataset consisting of document
clusters and reference summaries. In our experi-
ments, DUC2005 was used for training and pa-
rameter tuning, and DUC2007 was used for test-
ing. Based on the tuning set, the parameter λ is
set as 10 and δ is set as 0.7 after tuning.
Our system is compared with one state-of-the-
art graph-based extractive approach MultiMR
(Wan and Xiao, 2009) and one abstractive ap-
proach TTG (Genest and Lapalme, 2011). In ad-
dition, we have implemented another baseline
RankBSU which uses the graph-based ranking
methods on the BSUs network to rank BSUs and
select the top ranked BSUs to generate sentences.
</bodyText>
<sectionHeader confidence="0.699119" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.9999565">
ROUGE-1.5.5 toolkit was used to evaluate the
quality of summary on DUC 2007 dataset (Lin
and Hovy, 2003). The ROUGE scores of the
NIST Baseline system (i.e. NIST Baseline) and
average ROUGE scores of all the participating
systems (i.e. AveDUC) for DUC 2007 main task
were also listed. According to the results in Ta-
ble 2, our system much outperforms the NIST
Baseline and AveDUC, and achieves higher
ROUGE scores than the abstractive approach
TTG. So the abstract representation of texts and
the information extraction process in our system
are effective for multi-document summarization.
Our system also achieves better performance
than the baseline RankBSU, which demonstrates
that the network reduction method is more effi-
cient than the popular graph-based ranking
methods. As compared with the state-of-art
graph-based extractive method MultiMR, our
system also achieves better performance. Fur-
thermore, our system is abstractive with abstract
representation and sentence generation. Incorrect
</bodyText>
<equation confidence="0.944817857142857">
u u nx n
− + ≤ − 1 ≤ ≠ ≤
1 i j n
i j ij
ui ≤n i=n
u ∈ i=
i
</equation>
<figure confidence="0.9037048">
1≤
1,...
1,...n
(4)
(5)
</figure>
<page confidence="0.983847">
1911
</page>
<bodyText confidence="0.999985875">
parser and co-reference resolution will lead to
wrong extraction of BSU. If with more accurate
parser and co-reference resolution, our system
will be expected to achieve better performance.
Since ROUGE metric evaluates summaries
only from word overlapping perspective, we also
use the pyramid evaluation metric (Nenkova and
Passonneau, 2004) which can measure the sum-
mary quality beyond simply string matching. The
pyramid evaluation metric involves semantic
matching of summary content units (SCUs) so as
to recognize alternate realizations of the same
meaning, which is a better metric for the abstrac-
tive summary evaluation. Since the manual pyr-
amid evaluation is time-consuming and the eval-
uation results can’t be reproducible with different
groups of assessors, we use the automated ver-
sion of pyramid proposed in (Passonneau et al.,
2013) and adopt the same setting as in (Bing et
al., 2015). Table 3 shows the evaluation results
of our system and the three baseline systems on
DUC 2007. The results show that the perfor-
mance of our system is significantly better than
the three baseline systems, which demonstrates
that the summaries of our system contain more
SCUs than summaries of other systems. So our
system can generate more informative summary.
In addition, large volumes of news texts for
popular news events are crawled from the news
websites. Figure 1 and 2 show the summaries for
the “Malaysia MH370 Disappear” news event
generated by our system and MultiMR respec-
tively. The summary by MultiMR contains some
repetition of facts obviously. And it is just a heap
of information about MH370. The summary by
our system doesn’t contain much repetition of
facts, so it can contain more useful information.
And it is built from sentence to sentence to a co-
herent body. Obviously, the summary by our sys-
tem is more coherent and compact.
</bodyText>
<sectionHeader confidence="0.991994" genericHeader="conclusions">
5 Conclusions and Future Works
</sectionHeader>
<bodyText confidence="0.999984833333333">
The proposed summarization approach is effec-
tive in information extraction and achieves good
performance on DUC datasets. Through the
sample summary, we can find that the approach
is very effective for summarizing texts that main-
ly describe facts and actions of news event.
Summaries generated by our system are informa-
tive, coherent and compact.
But for texts expressing opinions, the ap-
proach can’t settle it appropriately. For example,
when the verbs of BSUs are not meaningful ac-
tions, like “be”, the semantic relations between
</bodyText>
<figureCaption confidence="0.98284575">
Figure 1. The summary of “Malaysia MH370 Dis-
appear” news event generated by our system.
Figure 2. The summary of “Malaysia MH370 Dis-
appear” news event generated by MultiMR.
</figureCaption>
<bodyText confidence="0.995473">
them can’t be appropriately computed by the
methods described in the paper. More efficient
methods to computer semantic relations between
BSUs should be developed in the following work.
The sentence generation process described in
the paper is just a preliminary scheme. It should
be developed to generate sentence relying less on
the original sentence structure and aggregating
information from several different BSUs.
</bodyText>
<sectionHeader confidence="0.995656" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.997725666666667">
Barzilay, R., and McKeown, K. R. 2005. Sentence
fusion for multidocument news summarization.
Computational Linguistics, 31(3): 297-328.
Banko, M., Cafarella, M. J., Soderland, S., et al. 2007.
Open information extraction for the web. In IJCAI
2007, 7: 2670-2676.
Bing, L., Li, P., Liao, Y., Lam, W., et al., 2015. Ab-
stractive Multi-Document Summarization via
Phrase Selection and Merging. In ACL 2015, 1587-
</reference>
<page confidence="0.515975">
1597
</page>
<bodyText confidence="0.999886097560976">
Flight MH370 disappeared after leaving Kuala Lumpur. It had been expected to
land in Beijing at 06:30. It took off at 00:41 MYT from runway 32R. It ended in
the southern Indian Ocean. The aircraft, a Boeing 777-200ER made a sharp turn
westwards. It passed into Vietnamese airspace. The captain of another aircraft
attempted to reach the crew of Malaysia Airlines flight MH370. Malaysia
Airlines flight 386 was requested to attempt to contact Malaysia Airlines flight
MH370 on the Lumpur Radar frequency. Malaysia Airlines flight MH17,
another Boeing 777-200ER, was surpassed Malaysia Airlines flight MH370.
Malaysia Airlines assumes beyond reasonable doubt there are no survivors.
They reported the Malaysia Airlines flight MH370 missing. They releases
passenger manifest of flight MH370. They will give US$ 5000 to the relatives
of each passenger. Malaysia released preliminary report. It set up a Joint Inves-
tigation Team. Southeast Asian states have joined forces to search waters
between Malaysia and Vietnam. Chinese government criticizes Malaysia for
inadequate answers regarding Malaysia Airlines flight MH370. Malaysia will
be deploying more ships and equipment to assist in the search. It ends hunt in
South China Sea. Continued refinement of analysis of flight MH370&apos;s satellite
communications identified a wide area search. Australia and Malaysia are
working on a Memorandum of Understanding to cover financial and co-
operation arrangements for search and recovery activities.
Malaysia Airlines said in a statement that flight MH370 had disappeared at
02:40 local time on Saturday after leaving Kuala Lumpur. Southeast Asian states
have joined forces to search waters between Malaysia and Vietnam after a
Malaysia Airlines plane vanished on a flight to Beijing, with 239 people on
board. Flight MH370 had been expected to land in Beijing at 06:30. If Malaysia
Airlines flight MH370 had impacted the ocean hard, resulting underwater sounds
could have been detected by hydrophones, given favorable circumstances.
Scientists from the CTBTO analyzed their recordings soon after flight MH370
disappeared, finding nothing of interest. The CMST researchers believe that the
most likely explanation of the hydroacoustic data is that they come from the
same event, but unrelated to Malaysia Airlines flight MH370. The lead research-
er of the CMST team, Dr.Alec Duncan, believes there&apos;s a slim chance that the
acoustic event is related to Malaysia Airlines flight MH370. Several IMOS
recorders deployed in the Indian Ocean off northwestern Australia by CMST
may have recorded data related to Malaysia Airlines flight MH370. Malaysia
Airlines released the names and nationalities of the 227 passengers and 12 crew
members, based on the flight manifest, later modified to include two Iranian
passengers travelling on stolen passports. If the data relates to the same event,
related to flight MH370, but the arc derived from analysis of the aircraft&apos;s satel-
lite transmission is incorrect, then the most likely place to look for the aircraft
would be along a line from HA01.
</bodyText>
<page confidence="0.995335">
1912
</page>
<reference confidence="0.996110979591837">
Carenini, G., and Cheung, J. C. K. 2008. Extractive vs.
NLG-based abstractive summarization of evalua-
tive text: The effect of corpus controversiality.
Proceedings of the Fifth International Natural Lan-
guage Generation Conference. Association for
Computational Linguistics, 33-41.
Cohn, T., and Lapata, M. 2009. Sentence compres-
sion as tree transduction. Journal of Artificial Intel-
ligence Research, 637-674.
Etzioni, O., Fader, A., Christensen, J., et al. 2011.
Open Information Extraction: The Second Genera-
tion. In IJCAI 2011, 11: 3-10.
Finkel, J. R., Grenager, T., and Manning, C. 2005.
Incorporating Non-local Information into Infor-
mation Extraction Systems by Gibbs Sampling. In
ACL 2005, 363-370.
Filippova, K., and Strube, M. Sentence fusion via
dependency graph compression. In EMNLP 2008,
177-185.
Gabrilovich, E., and Markovitch, S. 2007. Computing
Semantic Relatedness Using Wikipedia-based Ex-
plicit Semantic Analysis. In IJCAI 2007, 7: 1606-
1611.
Genest, P. E., and Lapalme, G. 2011. Framework for
abstractive summarization using text-to-text gener-
ation. In Proceedings of the Workshop on Mono-
lingual Text-To-Text Generation, 64-73.
Joachims, T. 1999. Svmlight: Support vector machine.
SVM-Light Support Vector Machine
http://svmlight. joachims. org/, University of Dort-
mund, 19(4).
Knight, K., and Marcu, D. 2000. Statistics-based
summarization-step one: Sentence compression. In
AAAI/IAAI 2000, 703-710.
Knight, K., and Marcu, D. 2002. Summarization be-
yond sentence extraction: A probabilistic approach
to sentence compression. In Artificial Intelligence.
139(1): 91-107
Klein, D., and Manning, C. D. 2003. Accurate unlexi-
calized parsing. In ACL 2003, 423-430.
Liu, F., Flanigan, J., et al. 2015. Toward Abstractive
Summarization Using Semantic Representations.
In HLT-NAACL 2015.
Lin, C. Y., and Hovy, E. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In HLT-NAACL 2003, 71-78.
Lee, H., Peirsman, Y., Chang, A., et al. 2011. Stan-
ford&apos;s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In ACL 2011,
28-34.
Mihalcea, R., Corley, C., and Strapparava, C. 2006.
Corpus-based and knowledge-based measures of
text semantic similarity. In AAAI 2006, 6: 775-780.
Nenkova, A., and Passonneau, R. 2004. Evaluating
content selection in summarization: The pyramid
method. In HLT-NAACL, pages 145-152.
Passonneau, R. J., Chen, E., Guo, W., and Perin, D.
2013. Automated Pyramid Scoring of Summaries
using Distributional Semantics. In ACL(2), pages:
143-147.
Smola, A. J., and Schölkopf, B. 2004. A tutorial on
support vector regression. Statistics and computing,
14(3): 199-222.
Tanaka, H., Kinoshita, A., Kobayakawa, T., Kumano,
T., and Kato, N. 2009. Syntax-driven sentence re-
vision for broadcast news summarization.
In Proceedings of the 2009 Workshop on Language
Generation and Summarisation, 39-47.
Wan, X., Yang, J., and Xiao, J. 2007. Manifold-
Ranking Based Topic-Focused Multi-Document
Summarization. In IJCAI 2007, 7:2903-2908.
Wang, D., Li, T., Zhu, S., and Chris, D. 2008. Multi-
document summarization via sentence-level se-
mantic analysis and symmetric matrix factoriza-
tion. In SIGIR 2008, 307-314.
Wan, X., and Xiao, J. 2009. Graph-Based Multi-
Modality Learning for Topic-Focused Multi-
Document Summarization. In IJCAI 2009, 1586-
1591.
Wang, L., and Cardie, C. 2013. Domain-Independent
Abstract Generation for Focused Meeting Summa-
rization. In ACL 2013, 1395-1405.
Zhuge, H. 2009. Communities and Emerging Seman-
tics in Semantic Link Network: Discovery and
Learning, IEEE Transactions on Knowledge and
Data Engineering, vol.21, no.6, 2009, pp. 785-799.
Zhuge, H. 2010. Interactive Semantics, Artificial In-
telligence, 174(2010)190-204.
Zhuge, H. 2011. Semantic linking through spaces for
cyber-physical-socio intelligence: A methodology,
Artificial Intelligence, 175(2011)988-1019.
Zhuge, H. 2012. Chapter 2 in The Knowledge Grid:
Toward Cyber-Physical Society. World Scientific.
Zhuge, H. 2015a. Dimensionality on Summarization,
arXiv:1507.00209 [cs.CL], 2 July 2015.
Zhuge, H. 2015b. Mapping Big Data into Knowledge
Space with Cognitive Cyber-Infrastructure,
arXiv:1507.06500, 24 July 2015.
</reference>
<page confidence="0.984039">
1913
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880348">
<title confidence="0.9990175">Multi-document Summarization with Semantic mation Extraction</title>
<author confidence="0.988529">Wei</author>
<affiliation confidence="0.9622665">Key Lab of Intelligent Info. Institute of Computing Technology,</affiliation>
<address confidence="0.999976">Beijing, 100190, China</address>
<email confidence="0.992276">weili@kg.ict.ac.cn</email>
<abstract confidence="0.997609466666667">This paper proposes a novel approach to generate abstractive summary for multiple documents by extracting semantic information from texts. The concept of Semantic Unit is defined to describe the semantics of an event or ac- A semantic link network on constructed to capture the semantic information of texts. Summary structure is planned with sentences generated based on the semantic link network. Experiments demonstrate that the approach is effective in generating informative, coherent and compact summary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>K R McKeown</author>
</authors>
<title>Sentence fusion for multidocument news summarization.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>3</issue>
<pages>297--328</pages>
<contexts>
<context position="1519" citStr="Barzilay and McKeown, 2005" startWordPosition="221" endWordPosition="224">nces are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existing summarization approaches are all shallow processing of text. It is necessary to explore su</context>
</contexts>
<marker>Barzilay, McKeown, 2005</marker>
<rawString>Barzilay, R., and McKeown, K. R. 2005. Sentence fusion for multidocument news summarization. Computational Linguistics, 31(3): 297-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M J Cafarella</author>
<author>S Soderland</author>
</authors>
<title>Open information extraction for the web. In IJCAI</title>
<date>2007</date>
<volume>7</volume>
<pages>2670--2676</pages>
<contexts>
<context position="5921" citStr="Banko et al., 2007" startWordPosition="904" endWordPosition="907">15). It first extracts noun phrases and verb-object phrases from the input documents, and then calculates saliency scores for them. An ILP optimization framework is used to simultaneously select and merge informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. As the results show that the method is difficult to generate new informative sentences really different from the original sentences and may generate some none factual sentences since phrases from different sentences are merged. Open information extraction has been proposed by (Banko et al., 2007; Etzioni et al., 2011). They extract binary relations from the web, which is different from our approach that extracts events or actions expressed in texts. 3 The Summarization Framework Our system produces an abstractive summary for a set of topic related documents. It consists of two major components: Information extraction and summary generation. 3.1 Information Extraction The semantic information of texts is obtained by extracting BSUs and constructing BSU semantic link network. A BSU is represented as an actoraction-receiver triple, which can both detects the crucial content and incorpor</context>
</contexts>
<marker>Banko, Cafarella, Soderland, 2007</marker>
<rawString>Banko, M., Cafarella, M. J., Soderland, S., et al. 2007. Open information extraction for the web. In IJCAI 2007, 7: 2670-2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bing</author>
<author>P Li</author>
<author>Y Liao</author>
<author>W Lam</author>
</authors>
<title>Abstractive Multi-Document Summarization via Phrase Selection and Merging.</title>
<date>2015</date>
<booktitle>In ACL 2015,</booktitle>
<pages>1587</pages>
<contexts>
<context position="5306" citStr="Bing et al., 2015" startWordPosition="811" endWordPosition="814"> the source text into AMR graphs, and then transforms them into a summary graph and plans to generate text from it. This work only focuses on the graph-to-graph transformation. The module of text generation from AMR has not been developed. The nodes and edges of AMR graph are entities and relations between entities respectively, which are sufficiently different from the BSUs semantic link network. Moreover, texts can be generated efficiently from the BSUs network. Another recent abstractive summarization method generates new sentences by selecting and merging phrases from the input documents (Bing et al., 2015). It first extracts noun phrases and verb-object phrases from the input documents, and then calculates saliency scores for them. An ILP optimization framework is used to simultaneously select and merge informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. As the results show that the method is difficult to generate new informative sentences really different from the original sentences and may generate some none factual sentences since phrases from different sentences are merged. Open information extraction has been proposed by (Ban</context>
<context position="18059" citStr="Bing et al., 2015" startWordPosition="2957" endWordPosition="2960">e the pyramid evaluation metric (Nenkova and Passonneau, 2004) which can measure the summary quality beyond simply string matching. The pyramid evaluation metric involves semantic matching of summary content units (SCUs) so as to recognize alternate realizations of the same meaning, which is a better metric for the abstractive summary evaluation. Since the manual pyramid evaluation is time-consuming and the evaluation results can’t be reproducible with different groups of assessors, we use the automated version of pyramid proposed in (Passonneau et al., 2013) and adopt the same setting as in (Bing et al., 2015). Table 3 shows the evaluation results of our system and the three baseline systems on DUC 2007. The results show that the performance of our system is significantly better than the three baseline systems, which demonstrates that the summaries of our system contain more SCUs than summaries of other systems. So our system can generate more informative summary. In addition, large volumes of news texts for popular news events are crawled from the news websites. Figure 1 and 2 show the summaries for the “Malaysia MH370 Disappear” news event generated by our system and MultiMR respectively. The sum</context>
</contexts>
<marker>Bing, Li, Liao, Lam, 2015</marker>
<rawString>Bing, L., Li, P., Liao, Y., Lam, W., et al., 2015. Abstractive Multi-Document Summarization via Phrase Selection and Merging. In ACL 2015, 1587-</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Carenini</author>
<author>J C K Cheung</author>
</authors>
<title>Extractive vs. NLG-based abstractive summarization of evaluative text: The effect of corpus controversiality.</title>
<date>2008</date>
<booktitle>Proceedings of the Fifth International Natural Language Generation Conference. Association for Computational Linguistics,</booktitle>
<pages>33--41</pages>
<contexts>
<context position="1173" citStr="Carenini and Cheung, 2008" startWordPosition="172" endWordPosition="175">. Summary structure is planned with sentences generated based on the semantic link network. Experiments demonstrate that the approach is effective in generating informative, coherent and compact summary. 1 Introduction Most automatic summarization approaches are extractive which leverage only literal or syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization a</context>
</contexts>
<marker>Carenini, Cheung, 2008</marker>
<rawString>Carenini, G., and Cheung, J. C. K. 2008. Extractive vs. NLG-based abstractive summarization of evaluative text: The effect of corpus controversiality. Proceedings of the Fifth International Natural Language Generation Conference. Association for Computational Linguistics, 33-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Cohn</author>
<author>M Lapata</author>
</authors>
<title>Sentence compression as tree transduction.</title>
<date>2009</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>637--674</pages>
<contexts>
<context position="1474" citStr="Cohn and Lapata, 2009" startWordPosition="215" endWordPosition="218">syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existing summarization approaches are all shallow proc</context>
</contexts>
<marker>Cohn, Lapata, 2009</marker>
<rawString>Cohn, T., and Lapata, M. 2009. Sentence compression as tree transduction. Journal of Artificial Intelligence Research, 637-674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>A Fader</author>
<author>J Christensen</author>
</authors>
<title>Open Information Extraction: The Second Generation.</title>
<date>2011</date>
<booktitle>In IJCAI 2011,</booktitle>
<volume>11</volume>
<pages>3--10</pages>
<contexts>
<context position="5944" citStr="Etzioni et al., 2011" startWordPosition="908" endWordPosition="911">ts noun phrases and verb-object phrases from the input documents, and then calculates saliency scores for them. An ILP optimization framework is used to simultaneously select and merge informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. As the results show that the method is difficult to generate new informative sentences really different from the original sentences and may generate some none factual sentences since phrases from different sentences are merged. Open information extraction has been proposed by (Banko et al., 2007; Etzioni et al., 2011). They extract binary relations from the web, which is different from our approach that extracts events or actions expressed in texts. 3 The Summarization Framework Our system produces an abstractive summary for a set of topic related documents. It consists of two major components: Information extraction and summary generation. 3.1 Information Extraction The semantic information of texts is obtained by extracting BSUs and constructing BSU semantic link network. A BSU is represented as an actoraction-receiver triple, which can both detects the crucial content and incorporates enough syntactic i</context>
</contexts>
<marker>Etzioni, Fader, Christensen, 2011</marker>
<rawString>Etzioni, O., Fader, A., Christensen, J., et al. 2011. Open Information Extraction: The Second Generation. In IJCAI 2011, 11: 3-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>T Grenager</author>
<author>C Manning</author>
</authors>
<title>Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling.</title>
<date>2005</date>
<booktitle>In ACL 2005,</booktitle>
<pages>363--370</pages>
<contexts>
<context position="6897" citStr="Finkel et al., 2005" startWordPosition="1053" endWordPosition="1056"> Information Extraction The semantic information of texts is obtained by extracting BSUs and constructing BSU semantic link network. A BSU is represented as an actoraction-receiver triple, which can both detects the crucial content and incorporates enough syntactic information to facilitate the downstream sentence generation. Some actions may not have the receiver argument. For example, “Flight MH370 – disappear” and “Flight MH370 - leave - Kuala Lumpur” are two BSUs. BSU Extraction. BSUs are extracted from the sentences of the documents. The texts are preprocessed by name entity recognition (Finkel et al., 2005) and co-reference resolution (Lee et al., 2011). Constituent and dependency parses are obtained by Stanford parser (Klein and Manning, 2003). The eligible action indicator is restricted to be a predicate verb; the eligible actor and receiver arguments are noun phrase. Both the actor and receiver arguments take the form of constituents in the parse tree. A valid BSU should have one action indicator and at least one actor argument, and satisfy the following constraints: • The actor argument is the nominal subject or external subject or the complement of a passive verb which is introduced by the </context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Finkel, J. R., Grenager, T., and Manning, C. 2005. Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. In ACL 2005, 363-370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Filippova</author>
<author>M Strube</author>
</authors>
<title>Sentence fusion via dependency graph compression.</title>
<date>2008</date>
<booktitle>In EMNLP</booktitle>
<pages>177--185</pages>
<contexts>
<context position="1548" citStr="Filippova and Strube, 2008" startWordPosition="225" endWordPosition="228">original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existing summarization approaches are all shallow processing of text. It is necessary to explore summarization methods based on </context>
</contexts>
<marker>Filippova, Strube, 2008</marker>
<rawString>Filippova, K., and Strube, M. Sentence fusion via dependency graph compression. In EMNLP 2008, 177-185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In IJCAI</booktitle>
<volume>7</volume>
<pages>1606--1611</pages>
<contexts>
<context position="8382" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="1290" endWordPosition="1294"> on the syntactic structure of sentences in the input texts. Constructing BSU Semantic Link Network. The semantic relatedness between BSUs contains three parts: Arguments Semantic Relatedness (ASR), Action-Verbs Semantic Relatedness (VSR) and Co-occurrence in the Same Sentence (CSS). Arguments of BSUs include actors and receivers, which both are noun phrases and indicate concepts or entities in the text. When computing ASR, the semantic relatedness between concepts must be measured. We use the explicit 1909 semantic analysis based on Wikipedia to compute semantic relatedness between concepts (Gabrilovich and Markovitch, 2007). When computing VSR, WordNet-based measure is used to calculate the semantic relatedness between action verbs (Mihalcea et al., 2006). CSS is measured whether two different BSUs co-occur in the same sentence. Semantic relations between BSUs are computed by linearly combining these three parts. Then BSUs that are extracted from the texts form a semantic link network. Semantic Link Network Reduction. A discriminative ranker based on Support Vector Regression (SVR) (Smola and Scholkopf, 2004) is utilized to assign each BSU a summary-worthy score. Training data was constructed from the DUC 2005 d</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Gabrilovich, E., and Markovitch, S. 2007. Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis. In IJCAI 2007, 7: 1606-1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P E Genest</author>
<author>G Lapalme</author>
</authors>
<title>Framework for abstractive summarization using text-to-text generation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Monolingual Text-To-Text Generation,</booktitle>
<pages>64--73</pages>
<contexts>
<context position="1932" citStr="Genest and Lapalme, 2011" startWordPosition="279" endWordPosition="282">xts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existing summarization approaches are all shallow processing of text. It is necessary to explore summarization methods based on deeper semantic analysis. We define the concept of Basic Semantic Unit (BSU) to express the semantics of texts. A BSU is an action indicator with its obligatory arguments which contain actor and receiver of the action. BSU is the most basic element of coherent information in texts, which can describe the semantics of an event or action. The semantic information of texts is represen</context>
<context position="3966" citStr="Genest and Lapalme, 2011" startWordPosition="601" endWordPosition="604">ith no redundancy. 1908 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1908–1913, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. • Summary is built from sentence to sentence to a coherent body of information based on the BSU semantic link network by summary structure planning. 2 Related Work There are some abstractive summarization approaches in recent years. An approach TTG attempts to generate abstractive summary by using text-to-text generation to generate sentence for each subject-verb-object triple (Genest and Lapalme, 2011). A system that attempts to generate abstractive summaries for spoken meetings was proposed (Wang and Cardie, 2013). It identifies relation instances that are represented by a lexical indicator with an argument constituent from texts. Then the relation instances are filled into templates which are extracted by applying multiple sequence alignment. Both of these systems need to select a subset of the large volumes of generated sentences. However, our system generates summary directly by summary structure planning. It can generate well-organized and coherent summary more effectively. A recent wo</context>
<context position="15873" citStr="Genest and Lapalme, 2011" startWordPosition="2604" endWordPosition="2607">s In order to evaluate the performance of our system, we use two datasets that have been used in recent multi-document summarization shared tasks: DUC2005 and DUC2007. Each task has a gold standard dataset consisting of document clusters and reference summaries. In our experiments, DUC2005 was used for training and parameter tuning, and DUC2007 was used for testing. Based on the tuning set, the parameter λ is set as 10 and δ is set as 0.7 after tuning. Our system is compared with one state-of-theart graph-based extractive approach MultiMR (Wan and Xiao, 2009) and one abstractive approach TTG (Genest and Lapalme, 2011). In addition, we have implemented another baseline RankBSU which uses the graph-based ranking methods on the BSUs network to rank BSUs and select the top ranked BSUs to generate sentences. 4.2 Results ROUGE-1.5.5 toolkit was used to evaluate the quality of summary on DUC 2007 dataset (Lin and Hovy, 2003). The ROUGE scores of the NIST Baseline system (i.e. NIST Baseline) and average ROUGE scores of all the participating systems (i.e. AveDUC) for DUC 2007 main task were also listed. According to the results in Table 2, our system much outperforms the NIST Baseline and AveDUC, and achieves highe</context>
</contexts>
<marker>Genest, Lapalme, 2011</marker>
<rawString>Genest, P. E., and Lapalme, G. 2011. Framework for abstractive summarization using text-to-text generation. In Proceedings of the Workshop on Monolingual Text-To-Text Generation, 64-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Svmlight: Support vector machine. SVM-Light Support Vector Machine http://svmlight. joachims. org/,</title>
<date>1999</date>
<institution>University of Dortmund,</institution>
<contexts>
<context position="11153" citStr="Joachims, 1999" startWordPosition="1736" endWordPosition="1737">uld build from senBasic Features Number of words in actor/receiver Number of nouns in actor/receiver Number of new nouns in actor/receiver Actor/receiver has capitalized word? Actor/receiver has stopword? Action is a phrasal verb? Content Features Actor/receiver has name entity? TF/IDF/TF-IDF of action TF/IDF/TF-IDF min max average of actor/receiver Syntax Features Constituent tag of actor/action/receiver Dependency relation of action with actor Dependency relation of action with receiver Table 1. Features for BSU summary-worthy scoring. We use SVM-light with RBF kernel by default parameters (Joachims, 1999). tence to sentence to a coherent body of information about a topic. The summary structure is planned based on the BSU semantic link network. An optimal path which covers all the nodes in the network is found. The following two factors are considered when finding the optimal path: (1) Context Semantic Coherent. To make the summary semantic coherent, all adjacent sentences should be semantically related. We need to find an optimal path, in which every two adjacent nodes are strong semantically related. The optimal path is denoted as P = [ pr , p, , ..., prn ] and maximize the theme of generated</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Joachims, T. 1999. Svmlight: Support vector machine. SVM-Light Support Vector Machine http://svmlight. joachims. org/, University of Dortmund, 19(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Statistics-based summarization-step one: Sentence compression.</title>
<date>2000</date>
<booktitle>In AAAI/IAAI</booktitle>
<pages>703--710</pages>
<contexts>
<context position="1426" citStr="Knight and Marcu, 2000" startWordPosition="207" endWordPosition="210">s are extractive which leverage only literal or syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existi</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Knight, K., and Marcu, D. 2000. Statistics-based summarization-step one: Sentence compression. In AAAI/IAAI 2000, 703-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>Summarization beyond sentence extraction: A probabilistic approach to sentence compression.</title>
<date>2002</date>
<journal>In Artificial Intelligence.</journal>
<volume>139</volume>
<issue>1</issue>
<pages>91--107</pages>
<contexts>
<context position="1450" citStr="Knight and Marcu, 2002" startWordPosition="211" endWordPosition="214">everage only literal or syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existing summarization approac</context>
</contexts>
<marker>Knight, Marcu, 2002</marker>
<rawString>Knight, K., and Marcu, D. 2002. Summarization beyond sentence extraction: A probabilistic approach to sentence compression. In Artificial Intelligence. 139(1): 91-107</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL 2003,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="7037" citStr="Klein and Manning, 2003" startWordPosition="1073" endWordPosition="1076">SU is represented as an actoraction-receiver triple, which can both detects the crucial content and incorporates enough syntactic information to facilitate the downstream sentence generation. Some actions may not have the receiver argument. For example, “Flight MH370 – disappear” and “Flight MH370 - leave - Kuala Lumpur” are two BSUs. BSU Extraction. BSUs are extracted from the sentences of the documents. The texts are preprocessed by name entity recognition (Finkel et al., 2005) and co-reference resolution (Lee et al., 2011). Constituent and dependency parses are obtained by Stanford parser (Klein and Manning, 2003). The eligible action indicator is restricted to be a predicate verb; the eligible actor and receiver arguments are noun phrase. Both the actor and receiver arguments take the form of constituents in the parse tree. A valid BSU should have one action indicator and at least one actor argument, and satisfy the following constraints: • The actor argument is the nominal subject or external subject or the complement of a passive verb which is introduced by the preposition “by” and does the action. • The receiver argument is the direct object or the passive nominal subject or the object of a preposi</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D., and Manning, C. D. 2003. Accurate unlexicalized parsing. In ACL 2003, 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Liu</author>
<author>J Flanigan</author>
</authors>
<title>Toward Abstractive Summarization Using Semantic Representations.</title>
<date>2015</date>
<booktitle>In HLT-NAACL</booktitle>
<marker>Liu, Flanigan, 2015</marker>
<rawString>Liu, F., Flanigan, J., et al. 2015. Toward Abstractive Summarization Using Semantic Representations. In HLT-NAACL 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>E Hovy</author>
</authors>
<title>Automatic evaluation of summaries using n-gram co-occurrence statistics.</title>
<date>2003</date>
<booktitle>In HLT-NAACL 2003,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="16179" citStr="Lin and Hovy, 2003" startWordPosition="2655" endWordPosition="2658"> and parameter tuning, and DUC2007 was used for testing. Based on the tuning set, the parameter λ is set as 10 and δ is set as 0.7 after tuning. Our system is compared with one state-of-theart graph-based extractive approach MultiMR (Wan and Xiao, 2009) and one abstractive approach TTG (Genest and Lapalme, 2011). In addition, we have implemented another baseline RankBSU which uses the graph-based ranking methods on the BSUs network to rank BSUs and select the top ranked BSUs to generate sentences. 4.2 Results ROUGE-1.5.5 toolkit was used to evaluate the quality of summary on DUC 2007 dataset (Lin and Hovy, 2003). The ROUGE scores of the NIST Baseline system (i.e. NIST Baseline) and average ROUGE scores of all the participating systems (i.e. AveDUC) for DUC 2007 main task were also listed. According to the results in Table 2, our system much outperforms the NIST Baseline and AveDUC, and achieves higher ROUGE scores than the abstractive approach TTG. So the abstract representation of texts and the information extraction process in our system are effective for multi-document summarization. Our system also achieves better performance than the baseline RankBSU, which demonstrates that the network reductio</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Lin, C. Y., and Hovy, E. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In HLT-NAACL 2003, 71-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Lee</author>
<author>Y Peirsman</author>
<author>A Chang</author>
</authors>
<title>Stanford&apos;s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task.</title>
<date>2011</date>
<booktitle>In ACL 2011,</booktitle>
<pages>28--34</pages>
<contexts>
<context position="6944" citStr="Lee et al., 2011" startWordPosition="1060" endWordPosition="1063">f texts is obtained by extracting BSUs and constructing BSU semantic link network. A BSU is represented as an actoraction-receiver triple, which can both detects the crucial content and incorporates enough syntactic information to facilitate the downstream sentence generation. Some actions may not have the receiver argument. For example, “Flight MH370 – disappear” and “Flight MH370 - leave - Kuala Lumpur” are two BSUs. BSU Extraction. BSUs are extracted from the sentences of the documents. The texts are preprocessed by name entity recognition (Finkel et al., 2005) and co-reference resolution (Lee et al., 2011). Constituent and dependency parses are obtained by Stanford parser (Klein and Manning, 2003). The eligible action indicator is restricted to be a predicate verb; the eligible actor and receiver arguments are noun phrase. Both the actor and receiver arguments take the form of constituents in the parse tree. A valid BSU should have one action indicator and at least one actor argument, and satisfy the following constraints: • The actor argument is the nominal subject or external subject or the complement of a passive verb which is introduced by the preposition “by” and does the action. • The rec</context>
</contexts>
<marker>Lee, Peirsman, Chang, 2011</marker>
<rawString>Lee, H., Peirsman, Y., Chang, A., et al. 2011. Stanford&apos;s multi-pass sieve coreference resolution system at the CoNLL-2011 shared task. In ACL 2011, 28-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>C Corley</author>
<author>C Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In AAAI</booktitle>
<volume>6</volume>
<pages>775--780</pages>
<contexts>
<context position="8516" citStr="Mihalcea et al., 2006" startWordPosition="1311" endWordPosition="1314">ns three parts: Arguments Semantic Relatedness (ASR), Action-Verbs Semantic Relatedness (VSR) and Co-occurrence in the Same Sentence (CSS). Arguments of BSUs include actors and receivers, which both are noun phrases and indicate concepts or entities in the text. When computing ASR, the semantic relatedness between concepts must be measured. We use the explicit 1909 semantic analysis based on Wikipedia to compute semantic relatedness between concepts (Gabrilovich and Markovitch, 2007). When computing VSR, WordNet-based measure is used to calculate the semantic relatedness between action verbs (Mihalcea et al., 2006). CSS is measured whether two different BSUs co-occur in the same sentence. Semantic relations between BSUs are computed by linearly combining these three parts. Then BSUs that are extracted from the texts form a semantic link network. Semantic Link Network Reduction. A discriminative ranker based on Support Vector Regression (SVR) (Smola and Scholkopf, 2004) is utilized to assign each BSU a summary-worthy score. Training data was constructed from the DUC 2005 datasets which contain both the source documents and human generated reference summaries. BSUs are extracted from these datasets. For e</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Mihalcea, R., Corley, C., and Strapparava, C. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In AAAI 2006, 6: 775-780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nenkova</author>
<author>R Passonneau</author>
</authors>
<title>Evaluating content selection in summarization: The pyramid method.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="17503" citStr="Nenkova and Passonneau, 2004" startWordPosition="2867" endWordPosition="2870">tate-of-art graph-based extractive method MultiMR, our system also achieves better performance. Furthermore, our system is abstractive with abstract representation and sentence generation. Incorrect u u nx n − + ≤ − 1 ≤ ≠ ≤ 1 i j n i j ij ui ≤n i=n u ∈ i= i 1≤ 1,... 1,...n (4) (5) 1911 parser and co-reference resolution will lead to wrong extraction of BSU. If with more accurate parser and co-reference resolution, our system will be expected to achieve better performance. Since ROUGE metric evaluates summaries only from word overlapping perspective, we also use the pyramid evaluation metric (Nenkova and Passonneau, 2004) which can measure the summary quality beyond simply string matching. The pyramid evaluation metric involves semantic matching of summary content units (SCUs) so as to recognize alternate realizations of the same meaning, which is a better metric for the abstractive summary evaluation. Since the manual pyramid evaluation is time-consuming and the evaluation results can’t be reproducible with different groups of assessors, we use the automated version of pyramid proposed in (Passonneau et al., 2013) and adopt the same setting as in (Bing et al., 2015). Table 3 shows the evaluation results of ou</context>
</contexts>
<marker>Nenkova, Passonneau, 2004</marker>
<rawString>Nenkova, A., and Passonneau, R. 2004. Evaluating content selection in summarization: The pyramid method. In HLT-NAACL, pages 145-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R J Passonneau</author>
<author>E Chen</author>
<author>W Guo</author>
<author>D Perin</author>
</authors>
<title>Automated Pyramid Scoring of Summaries using Distributional Semantics. In</title>
<date>2013</date>
<booktitle>ACL(2),</booktitle>
<pages>143--147</pages>
<contexts>
<context position="18006" citStr="Passonneau et al., 2013" startWordPosition="2946" endWordPosition="2949">ummaries only from word overlapping perspective, we also use the pyramid evaluation metric (Nenkova and Passonneau, 2004) which can measure the summary quality beyond simply string matching. The pyramid evaluation metric involves semantic matching of summary content units (SCUs) so as to recognize alternate realizations of the same meaning, which is a better metric for the abstractive summary evaluation. Since the manual pyramid evaluation is time-consuming and the evaluation results can’t be reproducible with different groups of assessors, we use the automated version of pyramid proposed in (Passonneau et al., 2013) and adopt the same setting as in (Bing et al., 2015). Table 3 shows the evaluation results of our system and the three baseline systems on DUC 2007. The results show that the performance of our system is significantly better than the three baseline systems, which demonstrates that the summaries of our system contain more SCUs than summaries of other systems. So our system can generate more informative summary. In addition, large volumes of news texts for popular news events are crawled from the news websites. Figure 1 and 2 show the summaries for the “Malaysia MH370 Disappear” news event gene</context>
</contexts>
<marker>Passonneau, Chen, Guo, Perin, 2013</marker>
<rawString>Passonneau, R. J., Chen, E., Guo, W., and Perin, D. 2013. Automated Pyramid Scoring of Summaries using Distributional Semantics. In ACL(2), pages: 143-147.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Smola</author>
<author>B Schölkopf</author>
</authors>
<title>A tutorial on support vector regression. Statistics and computing,</title>
<date>2004</date>
<volume>14</volume>
<issue>3</issue>
<pages>199--222</pages>
<marker>Smola, Schölkopf, 2004</marker>
<rawString>Smola, A. J., and Schölkopf, B. 2004. A tutorial on support vector regression. Statistics and computing, 14(3): 199-222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tanaka</author>
<author>A Kinoshita</author>
<author>T Kobayakawa</author>
<author>T Kumano</author>
<author>N Kato</author>
</authors>
<title>Syntax-driven sentence revision for broadcast news summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Language Generation and Summarisation,</booktitle>
<pages>39--47</pages>
<contexts>
<context position="1593" citStr="Tanaka et al., 2009" startWordPosition="232" endWordPosition="235">d only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existing summarization approaches are all shallow processing of text. It is necessary to explore summarization methods based on deeper semantic analysis. We define the conce</context>
</contexts>
<marker>Tanaka, Kinoshita, Kobayakawa, Kumano, Kato, 2009</marker>
<rawString>Tanaka, H., Kinoshita, A., Kobayakawa, T., Kumano, T., and Kato, N. 2009. Syntax-driven sentence revision for broadcast news summarization. In Proceedings of the 2009 Workshop on Language Generation and Summarisation, 39-47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Yang</author>
<author>J Xiao</author>
</authors>
<title>ManifoldRanking Based Topic-Focused Multi-Document Summarization. In IJCAI</title>
<date>2007</date>
<pages>7--2903</pages>
<contexts>
<context position="1043" citStr="Wan et al., 2007" startWordPosition="152" endWordPosition="155">ntics of an event or action. A semantic link network on BSUs is constructed to capture the semantic information of texts. Summary structure is planned with sentences generated based on the semantic link network. Experiments demonstrate that the approach is effective in generating informative, coherent and compact summary. 1 Introduction Most automatic summarization approaches are extractive which leverage only literal or syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting</context>
</contexts>
<marker>Wan, Yang, Xiao, 2007</marker>
<rawString>Wan, X., Yang, J., and Xiao, J. 2007. ManifoldRanking Based Topic-Focused Multi-Document Summarization. In IJCAI 2007, 7:2903-2908.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wang</author>
<author>T Li</author>
<author>S Zhu</author>
<author>D Chris</author>
</authors>
<title>Multidocument summarization via sentence-level semantic analysis and symmetric matrix factorization.</title>
<date>2008</date>
<booktitle>In SIGIR</booktitle>
<pages>307--314</pages>
<contexts>
<context position="1062" citStr="Wang et al., 2008" startWordPosition="156" endWordPosition="159">or action. A semantic link network on BSUs is constructed to capture the semantic information of texts. Summary structure is planned with sentences generated based on the semantic link network. Experiments demonstrate that the approach is effective in generating informative, coherent and compact summary. 1 Introduction Most automatic summarization approaches are extractive which leverage only literal or syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based o</context>
</contexts>
<marker>Wang, Li, Zhu, Chris, 2008</marker>
<rawString>Wang, D., Li, T., Zhu, S., and Chris, D. 2008. Multidocument summarization via sentence-level semantic analysis and symmetric matrix factorization. In SIGIR 2008, 307-314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wan</author>
<author>J Xiao</author>
</authors>
<title>Graph-Based MultiModality Learning for Topic-Focused MultiDocument Summarization.</title>
<date>2009</date>
<booktitle>In IJCAI 2009,</booktitle>
<pages>1586--1591</pages>
<contexts>
<context position="1083" citStr="Wan and Xiao, 2009" startWordPosition="160" endWordPosition="163">ic link network on BSUs is constructed to capture the semantic information of texts. Summary structure is planned with sentences generated based on the semantic link network. Experiments demonstrate that the approach is effective in generating informative, coherent and compact summary. 1 Introduction Most automatic summarization approaches are extractive which leverage only literal or syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysi</context>
<context position="15813" citStr="Wan and Xiao, 2009" startWordPosition="2594" endWordPosition="2597">valuation Results 4.1 Dataset and Experimental Settings In order to evaluate the performance of our system, we use two datasets that have been used in recent multi-document summarization shared tasks: DUC2005 and DUC2007. Each task has a gold standard dataset consisting of document clusters and reference summaries. In our experiments, DUC2005 was used for training and parameter tuning, and DUC2007 was used for testing. Based on the tuning set, the parameter λ is set as 10 and δ is set as 0.7 after tuning. Our system is compared with one state-of-theart graph-based extractive approach MultiMR (Wan and Xiao, 2009) and one abstractive approach TTG (Genest and Lapalme, 2011). In addition, we have implemented another baseline RankBSU which uses the graph-based ranking methods on the BSUs network to rank BSUs and select the top ranked BSUs to generate sentences. 4.2 Results ROUGE-1.5.5 toolkit was used to evaluate the quality of summary on DUC 2007 dataset (Lin and Hovy, 2003). The ROUGE scores of the NIST Baseline system (i.e. NIST Baseline) and average ROUGE scores of all the participating systems (i.e. AveDUC) for DUC 2007 main task were also listed. According to the results in Table 2, our system much </context>
</contexts>
<marker>Wan, Xiao, 2009</marker>
<rawString>Wan, X., and Xiao, J. 2009. Graph-Based MultiModality Learning for Topic-Focused MultiDocument Summarization. In IJCAI 2009, 1586-1591.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wang</author>
<author>C Cardie</author>
</authors>
<title>Domain-Independent Abstract Generation for Focused Meeting Summarization.</title>
<date>2013</date>
<booktitle>In ACL 2013,</booktitle>
<pages>1395--1405</pages>
<contexts>
<context position="4081" citStr="Wang and Cardie, 2013" startWordPosition="619" endWordPosition="622">1908–1913, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. • Summary is built from sentence to sentence to a coherent body of information based on the BSU semantic link network by summary structure planning. 2 Related Work There are some abstractive summarization approaches in recent years. An approach TTG attempts to generate abstractive summary by using text-to-text generation to generate sentence for each subject-verb-object triple (Genest and Lapalme, 2011). A system that attempts to generate abstractive summaries for spoken meetings was proposed (Wang and Cardie, 2013). It identifies relation instances that are represented by a lexical indicator with an argument constituent from texts. Then the relation instances are filled into templates which are extracted by applying multiple sequence alignment. Both of these systems need to select a subset of the large volumes of generated sentences. However, our system generates summary directly by summary structure planning. It can generate well-organized and coherent summary more effectively. A recent work aims to generate abstractive summary based on Abstract Meaning Representation (AMR) (Liu et al., 2015). It first</context>
</contexts>
<marker>Wang, Cardie, 2013</marker>
<rawString>Wang, L., and Cardie, C. 2013. Domain-Independent Abstract Generation for Focused Meeting Summarization. In ACL 2013, 1395-1405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhuge</author>
</authors>
<title>Communities and Emerging Semantics in Semantic Link Network: Discovery and Learning,</title>
<date>2009</date>
<journal>IEEE Transactions on Knowledge and Data Engineering,</journal>
<volume>21</volume>
<pages>785--799</pages>
<contexts>
<context position="2611" citStr="Zhuge, 2009" startWordPosition="394" endWordPosition="395"> been widely used in existing summarization approaches are all shallow processing of text. It is necessary to explore summarization methods based on deeper semantic analysis. We define the concept of Basic Semantic Unit (BSU) to express the semantics of texts. A BSU is an action indicator with its obligatory arguments which contain actor and receiver of the action. BSU is the most basic element of coherent information in texts, which can describe the semantics of an event or action. The semantic information of texts is represented by extracting BSUs and constructing BSU semantic link network (Zhuge, 2009). Semantic Link Network consists of semantic nodes, semantic links and reasoning rules (Zhuge, 2010; 2011; 2012; 2015b). The semantic nodes can be any resources. In this work, the semantic nodes are BSUs extracted from texts. We use semantic relatedness between BSUs as semantic links. Then summary can be generated based on the semantic link network through summary structure planning. The characteristics of our approaches are as follows: • Each BSU describes the semantics of an event or action. The semantic relatedness between BSUs can capture the context semantic relations of texts. • The BSU </context>
</contexts>
<marker>Zhuge, 2009</marker>
<rawString>Zhuge, H. 2009. Communities and Emerging Semantics in Semantic Link Network: Discovery and Learning, IEEE Transactions on Knowledge and Data Engineering, vol.21, no.6, 2009, pp. 785-799.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhuge</author>
</authors>
<title>Interactive Semantics,</title>
<date>2010</date>
<journal>Artificial Intelligence,</journal>
<pages>174--2010</pages>
<contexts>
<context position="2710" citStr="Zhuge, 2010" startWordPosition="408" endWordPosition="409">essary to explore summarization methods based on deeper semantic analysis. We define the concept of Basic Semantic Unit (BSU) to express the semantics of texts. A BSU is an action indicator with its obligatory arguments which contain actor and receiver of the action. BSU is the most basic element of coherent information in texts, which can describe the semantics of an event or action. The semantic information of texts is represented by extracting BSUs and constructing BSU semantic link network (Zhuge, 2009). Semantic Link Network consists of semantic nodes, semantic links and reasoning rules (Zhuge, 2010; 2011; 2012; 2015b). The semantic nodes can be any resources. In this work, the semantic nodes are BSUs extracted from texts. We use semantic relatedness between BSUs as semantic links. Then summary can be generated based on the semantic link network through summary structure planning. The characteristics of our approaches are as follows: • Each BSU describes the semantics of an event or action. The semantic relatedness between BSUs can capture the context semantic relations of texts. • The BSU semantic link network is an abstract representation of texts. Reduction on the network can obtain i</context>
</contexts>
<marker>Zhuge, 2010</marker>
<rawString>Zhuge, H. 2010. Interactive Semantics, Artificial Intelligence, 174(2010)190-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhuge</author>
</authors>
<title>Semantic linking through spaces for cyber-physical-socio intelligence: A methodology,</title>
<date>2011</date>
<journal>Artificial Intelligence,</journal>
<pages>175--2011</pages>
<marker>Zhuge, 2011</marker>
<rawString>Zhuge, H. 2011. Semantic linking through spaces for cyber-physical-socio intelligence: A methodology, Artificial Intelligence, 175(2011)988-1019.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhuge</author>
</authors>
<title>Chapter 2 in The Knowledge Grid: Toward Cyber-Physical Society. World Scientific.</title>
<date>2012</date>
<marker>Zhuge, 2012</marker>
<rawString>Zhuge, H. 2012. Chapter 2 in The Knowledge Grid: Toward Cyber-Physical Society. World Scientific.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhuge</author>
</authors>
<date>2015</date>
<booktitle>Dimensionality on Summarization, arXiv:1507.00209 [cs.CL],</booktitle>
<volume>2</volume>
<contexts>
<context position="1323" citStr="Zhuge, 2015" startWordPosition="194" endWordPosition="195">rmative, coherent and compact summary. 1 Introduction Most automatic summarization approaches are extractive which leverage only literal or syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapal</context>
</contexts>
<marker>Zhuge, 2015</marker>
<rawString>Zhuge, H. 2015a. Dimensionality on Summarization, arXiv:1507.00209 [cs.CL], 2 July 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhuge</author>
</authors>
<title>Mapping Big Data into Knowledge Space with Cognitive Cyber-Infrastructure,</title>
<date>2015</date>
<contexts>
<context position="1323" citStr="Zhuge, 2015" startWordPosition="194" endWordPosition="195">rmative, coherent and compact summary. 1 Introduction Most automatic summarization approaches are extractive which leverage only literal or syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapal</context>
</contexts>
<marker>Zhuge, 2015</marker>
<rawString>Zhuge, H. 2015b. Mapping Big Data into Knowledge Space with Cognitive Cyber-Infrastructure, arXiv:1507.06500, 24 July 2015.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
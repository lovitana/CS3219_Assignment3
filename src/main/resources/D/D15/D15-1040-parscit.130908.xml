<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000139">
<title confidence="0.9993515">
A Neural Network Model for Low-Resource
Universal Dependency Parsing
</title>
<author confidence="0.998649">
Long Duong,12 Trevor Cohn,1 Steven Bird,1 and Paul Cook3
</author>
<affiliation confidence="0.999216">
1Department of Computing and Information Systems, University of Melbourne
2National ICT Australia, Victoria Research Laboratory
3Faculty of Computer Science, University of New Brunswick
</affiliation>
<email confidence="0.99516">
lduong@student.unimelb.edu.au {t.cohn,sbird}@unimelb.edu.au paul.cook@unb.ca
</email>
<sectionHeader confidence="0.994712" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999742214285714">
Accurate dependency parsing requires
large treebanks, which are only available
for a few languages. We propose a method
that takes advantage of shared structure
across languages to build a mature parser
using less training data. We propose
a model for learning a shared “univer-
sal” parser that operates over an inter-
lingual continuous representation of lan-
guage, along with language-specific map-
ping components. Compared with super-
vised learning, our methods give a con-
sistent 8-10% improvement across several
treebanks in low-resource simulations.
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999456875">
Dependency parsing is an important task for Nat-
ural Language Processing (NLP) with application
to text classification (¨Ozg¨ur and G¨ung¨or, 2010),
relation extraction (Bunescu and Mooney, 2005),
question answering (Cui et al., 2005), statistical
machine translation (Xu et al., 2009), and sen-
timent analysis (Socher et al., 2013). A mature
parser normally requires a large treebank for train-
ing, yet such resources are rarely available and
are costly to build. Ideally, we would be able to
construct a high quality parser with less training
data, thereby enabling accurate parsing for low-
resource languages.
In this paper we formalize the dependency pars-
ing task for a low-resource language as a domain
adaptation task, in which a target resource-poor
language treebank is treated as in-domain, while
a much larger treebank in a high-resource lan-
guage forms the out-of-domain data. In this way,
we can apply well-understood domain adaptation
techniques to the dependency parsing task. How-
ever, a crucial requirement for domain adaptation
is that the in-domain and out-of-domain data have
compatible representations. In applying our ap-
proach to data from several languages, we must
learn such a cross-lingual representation. Here
we frame this representation learning as part of a
neural network training. The underlying hypoth-
esis for the joint learning is that there are some
shared-structures across languages that we can ex-
ploit. This hypothesis is motivated by the excellent
results of the cross-lingual application of unlexi-
calised parsing (McDonald et al., 2011), whereby
a delexicalized parser constructed on one language
is applied directly to another language.
Our approach works by jointly training a neu-
ral network dependency parser to model the syn-
tax in both a source and target language. Many of
the parameters of the source and target language
parsers are shared, except for a small handful of
language-specific parameters. In this way, the in-
formation can flow back and forth between lan-
guages, allowing for the learning of a compatible
cross-lingual syntactic representation, while also
allowing the parsers to mutually correct one an-
other’s errors. We include some language-specific
components, in order to better model the lexicon
of each language and allow learning of the syntac-
tic idiosyncrasies of each language. Our experi-
ments show that this outperforms a purely super-
vised setting, on both small and large data condi-
tions, with a gain as high as 10% for small train-
ing sets. Our proposed joint training method also
out-performs the conventional cascade approach
where the parameters between source and target
languages are related together through a regular-
ization term (Duong et al., 2015).
Our model is flexible, allowing easy incorpora-
tion of peripheral information. For example, as-
suming the presence of a small bilingual dictio-
nary is befitting of a low-resource setting, as this
is prototypically one of the first artifacts gener-
ated by field linguists. We incorporate a bilin-
gual dictionary as a set of soft constraints on the
</bodyText>
<page confidence="0.986623">
339
</page>
<note confidence="0.985064">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 339–348,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999807769230769">
model, such that it learns similar representations
for each word and its translation(s). For example,
the representation of house in English should be
close to haus in German. We empirically show
that adding a bilingual dictionary improves parser
performance, particularly when target data is lim-
ited.
The final contribution of the paper concerns
the learned word embeddings. We demonstrate
that these encode meaningful syntactic phenom-
ena, both in terms of the observable clusters and
through a verb classification task. The code for
this paper is published as an open source project.1
</bodyText>
<sectionHeader confidence="0.999832" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999971794117647">
This work is motivated by the idea of delexical-
ized parsing, in which a parser is built without
any lexical features and trained on a treebank for
a resource-rich source language (Zeman et al.,
2008). It is then applied directly to parse sentences
in the target resource-poor languages. Delexical-
ized parsing relies on the fact that identical part-of-
speech (POS) inventories are highly informative of
dependency relations, and that there exists shared
dependency structures across languages.
Building a dependency parser for a resource-
poor language usually starts with the delexical-
ized parser and then uses other resources to refine
the model. McDonald et al. (2011) and Ma and
Xia (2014) exploited parallel data as the bridge
to transfer constraints from the source resource-
rich language to the target resource-poor lan-
guages. T¨ackstr¨om et al. (2012) also used par-
allel data to induce cross-lingual word clusters
which added as features for their delexicalized
parser. Durrett et al. (2012) constructed the set of
language-independent features and used a bilin-
gual dictionary as the bridge to transfer these fea-
tures from source to target language. T¨ackstr¨om
et al. (2013) additionally used high-level linguis-
tic features extracted from the World Atlas of Lan-
guage Structures (WALS) (Dryer and Haspelmath,
2013).
For low-resource languages, no large paral-
lel corpus is available. Some linguists are
dependency-annotating small amounts of field
data, e.g. for Karuk, a nearly-extinct language of
Northwest California (Garrett et al., 2013). Ac-
cordingly, we adopt a different resource require-
</bodyText>
<footnote confidence="0.968995">
1http://github.com/longdt219/
universal_dependency_parser
</footnote>
<bodyText confidence="0.998661952380952">
ment: a small treebank in the target low-resource
language.
Domain adaptation or joint-training is a differ-
ent branch of research, and falls outside the scope
of this paper. Nevertheless, we would like to con-
trast our work with Senna (Collobert et al., 2011),
a neural network framework to perform a vari-
ety of NLP tasks such as part-of-speech (POS)
tagging, named entity recognition (NER), chunk-
ing, and so forth. Both approaches exploit com-
mon linguistic properties of the data through joint
learning. However, Collobert et al’s goal is to find
a single input representation that can work well
for many tasks. Our goal is different: we allow
the joint-training inputs to be different but con-
strain the parameter weights in the upper layer
to be identical. Consequently, our method ap-
plies to the task where inputs are different, pos-
sibly from different languages or domains. Their
method applies for different tasks in the same lan-
guage/domain where the inputs are fairly similar.
</bodyText>
<subsectionHeader confidence="0.995226">
2.1 Supervised Neural Network Parser
</subsectionHeader>
<bodyText confidence="0.999935875">
This section describes the monolingual neural net-
work dependency parser structure of Chen and
Manning (2014). This parser achieves excellent
performance, and has a highly flexible formula-
tion allowing auxilliary inputs. The model is based
on a transition-based dependency parser (Nivre,
2006) formulated as a neural-network classifier to
decide which transition to apply to each parsing
state configuration.2 That is, for each configura-
tion, the selected list of words, POS tags and la-
bels from the Stack, Queue and Arcs are extracted.
Each word, POS and label is mapped into a low-
dimension vector representation using an embed-
ding matrix, which is then fed into a two-layer
neural network classifier to predict the next pars-
ing action. The set of parameters for the model is
</bodyText>
<equation confidence="0.525842">
E = {Eword, Epos, Earc} for the embedding layer,
</equation>
<bodyText confidence="0.989149">
W1 for the fully connected cubic hidden layer and
W2 for the softmax output layer. The model pre-
diction function is
</bodyText>
<equation confidence="0.997869333333333">
P(Y |X = x,W1,W2,E) =
( 1
softmax W2 x cube(W1 x Φ [�x, E]) (1)
</equation>
<footnote confidence="0.908111">
2Our approach is focused on a technique for transfer
learning which can be more widely applied to other types
of dependency parser (and models, generally) regardless of
whether they are transition-based or graph-based.
</footnote>
<page confidence="0.997807">
340
</page>
<bodyText confidence="0.9999812">
where cube is a non-linear activation function, Φ is
the embedding function that returns a vector rep-
resentation of parsing state x using an embedding
matrix E. We refer the reader to Chen and Man-
ning (2014) for a more detailed description.
</bodyText>
<sectionHeader confidence="0.98182" genericHeader="method">
3 A Joint Interlingual Model
</sectionHeader>
<bodyText confidence="0.999929266666667">
We assume a small treebank in a target resource-
poor language, as well as a larger treebank in the
source language. Our objective is to learn a model
of both languages, subject to the constraint that
both models are similar overall, while allowing for
some limited language variability. Instead of just
training two different parsers on source and then
on target, we train them jointly, in order to learn
an interlingual parser. This allows the method to
take maximum advantage of the limited treebank
data available, resulting in highly accurate pre-
dicted parses.
Training a monolingual parser as de-
scribed in section 2.1 requires optimizing
the simple cross-entropy learning objec-
</bodyText>
<equation confidence="0.976612">
tive, L = − �|D|
i=1 log P(Y = ~y(i)|X = ~x(i)),
</equation>
<bodyText confidence="0.9327765">
where P (Y |X) is given by equation 1 and
D = {~x(i), ~y(i)}ni=1 is the training data. Joint
training of a parser over the source and target
languages can be achieved by simply adding two
such cross-entropy objectives, i.e.,
log P(Ys = ~y(i)
</bodyText>
<equation confidence="0.9178954">
s |Xs = ~x(i)
s )
log P(Yt = ~y(i)
t |Xt = ~x(i)
t ) , (2)
</equation>
<bodyText confidence="0.983668461538462">
where the training data, D = Ds ∪ Dt, comprises
data in both the source and target language. How-
ever training the model according to equation 2
will result in two independent parsers. To enforce
similarity between the two parsers, we adopt pa-
rameter sharing: the neural network parameters,
W1 and W2, are identical in both parsers. Thereby
P(Yα|Xα = ~x) = P(Y |X = ~x, W1, W2, Eα),
where the subscript α ∈ {s, t} denotes the source
or target language. We allow the embedding
matrix Eα to differ in order to accommodate
language-specific features, in terms of the repre-
sentations of lexical types, Eword
</bodyText>
<equation confidence="0.61165475">
s , part-of-speech,
Epos
s and dependency arc labels Earc
s . This reflects
</equation>
<bodyText confidence="0.999958823529412">
the fact that different languages have different lex-
icon, parts-of-speech often exhibit different roles,
and dependency edges serve different functions,
e.g. in Korean a static verb can serve as an adjec-
tive (Kim, 2001). During training, the language-
specific errors are back propagated through dif-
ferent branches according to the language, guid-
ing learning towards an interlingual representa-
tion that informs parsing decisions in both lan-
guages. The set of parameters for the model is
W1, W2, Es, Et where Es, Et are the embedding
matrices for the source and target languages.
Generally speaking, we can understand the
model as building the universal dependency parser
that parses the universal language. Specifically,
the model is the combination of two parts: the
universal part (W1, W2) that is shared between the
languages, and the conversion part (Es, Et) that
maps a language-specific representation into the
universal language. Naturally, we could stack sev-
eral non-linear layers in the conversion compo-
nents such that the model can better transform the
input into the universal representation; we leave
this exploration for future work. Currently, our
cross-lingual word embeddings are meaningful for
a pair of source and target languages. However,
our model can easily be used for joint training over
k &gt; 2 languages. We also leave this avenue of en-
quiry for future work
One concern from equation 2 is that when the
source language treebank Ds is much bigger than
the target language treebank Dt, it is likely to
dominate, and consequently, learning will mainly
focus on optimizing the source language parser.
We adjust for this disparity by balancing the two
datasets, Ds and Dt, during training. When select-
ing mini-batches for online gradient updates, we
select an equal number of classification instances
from the source and target languages. Thus, for
each step |Ds |= |Dt|, effectively reweighting the
cross-entropy components in (2) to ensure parity
between the languages.
The other concern is over-fitting, especially
when we only have a small treebank in the tar-
get language. As suggested by Chen and Man-
ning (2014), we apply drop-out, a form of reg-
ularization for both source and target language.
That is, we randomly drop some of the activa-
tion units from both hidden layer and input layer.
Following Srivastava et al. (2014), we randomly
dropout 20% of the input layer and 50% of the hid-
</bodyText>
<equation confidence="0.992751">
|Ds|
Ljoint = −
i=1
−
� |Dt|
i=1
</equation>
<page confidence="0.975361">
341
</page>
<bodyText confidence="0.999413333333333">
den layer. Empirically, we observe a substantial
improvement applying dropout to the model over
MLE or l2 regularization.
</bodyText>
<subsectionHeader confidence="0.999218">
3.1 Incorporating a Dictionary
</subsectionHeader>
<bodyText confidence="0.998112782608696">
Our model is flexible, enabling us to freely add
additional components. In this section, we assume
the presence of a bilingual dictionary between the
source and target language. We seek to incorpo-
rate this dictionary as a part of model learning, to
encode the intuition that if two lexical items are
translations of one another, the parser should treat
them similarly.3 Recall that the mapping layer
is the combination of word, pos and arc embed-
dings, i.e., Eα = {Eword
α , Epos
α , Earc
α }. We can
easily add bilingual dictionary constraints to the
model in the form of regularization to minimize
the l2 distance between word representations, i.e.,
� hi,ji∈D 11E3 ord(i) − Et ord(j) 11F, where D com-
prises translation pairs, word(i) and word(j).
When the languages share the same POS tagset
and arc set,4 we can also add further constraints
such as their language-specific embeddings be
close together. This results a regularised training
objective,
</bodyText>
<equation confidence="0.971217333333333">
( E word(i)_ word(j) 2
Ldict = Ljoint−A ��Es Et I F
hi,ji∈D
+ I I Ers − Eios 112 + I I Earc
s − Earc
t IIF) , (3)
</equation>
<bodyText confidence="0.9999258">
where A E [0, oc] controls to what degree we
bind these words or pos tags or arc labels to-
gether, with high A tying the parameters and small
A allowing independent learning. We expect the
best value of A to fall somewhere between these
extremes. Finally, we use a mini-batch size of
1000 instance pairs and adaptive learning rate
trainer, adagrad (Duchi et al., 2011) to build our
two separate models corresponding to equations 2
and 3.
</bodyText>
<sectionHeader confidence="0.999374" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.912229375">
In this section, we compare our joint training ap-
proach with baseline methods of supervised learn-
ing in the target language, and cascaded learning
of source and target parsers.
3However, this is not always the case. For example, modal
or auxiliary verbs in English often have no translations in
different languages or map to words with different syntactic
functions.
</bodyText>
<footnote confidence="0.598475">
4As was the case for our experiments.
</footnote>
<subsectionHeader confidence="0.971984">
4.1 Dataset
</subsectionHeader>
<bodyText confidence="0.9999554">
We experiment with the Universal Dependency
Treebank (UDT) V1.0 (Nivre et al., 2015), sim-
ulating low resource settings.5 This treebank has
many desirable properties for our model: the de-
pendency types (arc labels set) and coarse POS
tagset are the same across languages. This re-
moves the need for mapping the source and target
language tagsets to a common tagset. Moreover,
the dependency types are also common across
languages allowing evaluation of the labelled at-
tachment score (LAS). The treebank covers 10
languages,6 with some languages very highly
resourced—Czech, French and Spanish have 400k
tokens—and only modest amounts of data for
other languages—Hungarian and Irish have only
around 25k tokens. Cross-lingual models assume
English as the source language, for which we have
a large treebank, and only a small treebank of 3k
tokens exists in each target language, simulated by
subsampling the corpus.
</bodyText>
<subsectionHeader confidence="0.886911">
4.2 Baseline Cascade Model
</subsectionHeader>
<bodyText confidence="0.999986571428571">
We compare our approach to a baseline inter-
lingual model based on the same parsing algo-
rithm as presented in section 2.1, but with cas-
caded training (Duong et al., 2015). This works
by first learning the source language parser, and
then training the target language parser using a
regularization term to minimise the distance be-
tween the parameters of the target parser and the
source parser (which is fixed). In this way, some
structural information from the source parser can
be used in the target parser, however it is likely
that the representation will be overly biased to-
wards the source language and consequently may
not prove as useful for modelling the target.
</bodyText>
<subsectionHeader confidence="0.997853">
4.3 Monolingual Word Embeddings
</subsectionHeader>
<bodyText confidence="0.952822153846154">
While the Epos and Earc are randomly initialized,
we initialize both the source and target language
word embeddings Eword
s , Eword
t of our neural net-
work models with pre-trained embeddings. This is
an advantage since we can incorporate the mono-
lingual data which is often available, even for
5Evaluating on truly resource-poor languages would be
preferable to simulation. However for ease of training and
evaluation, which requires a small treebank in the target lan-
guage, we simulate the low-resource setting using a small part
of the UDT.
</bodyText>
<footnote confidence="0.973710333333333">
6Czech (cs), English (en), Finnish (fi), French (fr), Ger-
man (de), Hungarian (hu), Irish (ga), Italian (it), Spanish (es),
Swedish (sv).
</footnote>
<page confidence="0.987831">
342
</page>
<figure confidence="0.980873">
54 55 56 57 58
LAS (%)
Lambda
</figure>
<figureCaption confidence="0.981068333333333">
Figure 1: Sensitivity of regularization parameter A
against the LAS measured on the Swedish devel-
opment set trained on 1000 (tokens).
</figureCaption>
<bodyText confidence="0.9928472">
resource-poor languages. We collect monolingual
data for each language from the Machine Trans-
lation Workshop (WMT) data,7 Europarl (Koehn,
2005) and EU Bookshop Corpus (Skadin¸ˇs et al.,
2014). The size of monolingual data also varies
significantly, with as much as 400 million tokens
for English and German, and as few as 4 mil-
lion tokens for Irish. We use the skip-gram model
(Mikolov et al., 2013b) to induce 50-dimensional
word embeddings.
</bodyText>
<subsectionHeader confidence="0.997136">
4.4 Bilingual Dictionary
</subsectionHeader>
<bodyText confidence="0.999996166666667">
For the extended model as described in section 3.1,
we also need a bilingual dictionary. We extract
dictionaries from PanLex (Kamholz et al., 2014)
which currently covers around 1300 language va-
rieties and about 12 million expressions. This
dataset is growing and aims at covering all lan-
guages in the world and up to 350 million expres-
sions. The translations in PanLex come from var-
ious sources such as glossaries, dictionaries, au-
tomatic inference from other languages, etc. Nat-
urally, the bilingual dictionary size varies greatly
among resource-poor and resource-rich languages.
</bodyText>
<subsectionHeader confidence="0.987516">
4.5 Regularization Parameter Tuning
</subsectionHeader>
<bodyText confidence="0.999905714285714">
Joint training with a dictionary (see equation 3)
includes a regularization sensitivity parameter A.
This parameter controls to what extent we should
bind the source words and their target translation,
common POS tags and arcs together. In this sec-
tion we measure the sensitivity of our approach
with respect to this parameter. In a real world sce-
</bodyText>
<footnote confidence="0.840305">
7http://www.statmt.org/wmt14/
</footnote>
<bodyText confidence="0.999969769230769">
nario, getting development data to tune this param-
eter is difficult. Thus, we want a parameter that
can work well cross-lingually. To simulate this,
we only tune the parameter on one language and
apply it directly to different languages. We trained
on a small Swedish treebank with 1k tokens, test-
ing several different values of A. We evaluated on
the Swedish development dataset. Figure 1 shows
the labelled attachment score (LAS) for different
A. It’s clearly visible that A = 0.0001 gives the
maximum LAS on the development set. Thus, we
use this value for all the experiments involving a
dictionary hereafter.
</bodyText>
<sectionHeader confidence="0.846028" genericHeader="method">
4.6 Results
</sectionHeader>
<bodyText confidence="0.999956724137931">
For our initial experiments we assume that we
have only a small target treebank with 3000 to-
kens (around 200 sentences). Ideally the much
larger source language (English) treebank should
be able to improve parser performance versus sim-
ple supervised learning on such a small collection.
We apply the joint model (equation 2) and joint
model with the dictionary constraints (equation 3)
for each target language,
The results are reported in Table 1. The su-
pervised neural network dependency parser per-
formed worst, as expected, and the baseline cas-
cade model consistently outperformed the super-
vised model on all languages by an average mar-
gin of 5.6% (absolute).8 The joint model also
consistently out-performed both baselines giving
a further 1.9% average improvement over the cas-
cade. This was despite the fact that the cascaded
model had the benefit of tuning for the regulariza-
tion parameters on a development corpus, while
the joint model had no parameter tuning. Note that
the improvement varies substantially across lan-
guages, and is largest for Czech but is only minor
for Swedish. The joint model with the bilingual
dictionary outperforms the joint model, however,
the improvement is modest (0.7%). Nevertheless,
this model gives substantial improvements com-
pared with the cascaded and the supervised model
(2.6% and 8.2%).
</bodyText>
<sectionHeader confidence="0.998706" genericHeader="method">
5 Analysis
</sectionHeader>
<subsectionHeader confidence="0.999668">
5.1 Learning Curve
</subsectionHeader>
<bodyText confidence="0.809027">
In section 4.6, we used a 3k token treebank in the
target language. What if we have more or less
8We use absolute percentage comparisons herein.
</bodyText>
<figure confidence="0.9966444">
●
●
●
●
●
●
●
● Swedish
●
1e−07 1e−06 1e−05 1e−04 0.001 0.01 0.1 1
</figure>
<page confidence="0.995264">
343
</page>
<table confidence="0.9988766">
cs de es fi fr ga hu it sv µ
Supervised 43.1 47.3 60.3 46.4 56.2 59.4 48.4 65.4 52.6 53.2
Baseline Cascaded 49.6 59.2 66.4 49.5 63.2 59.5 50.5 69.9 61.4 58.8
Joint 55.2 61.2 69.1 51.4 65.3 60.6 51.2 71.2 61.4 60.7
Joint + Dict 55.7 61.8 70.5 51.5 67.2 61.1 51.0 71.3 62.5 61.4
</table>
<tableCaption confidence="0.9812855">
Table 1: Labelled attachment score (LAS) for each model type trained on 3000 tokens for each target
language (columns). All bar the supervised model also use a large English treebank.
</tableCaption>
<figure confidence="0.997630090909091">
45 55 65 75
LAS (%)
●
●
●
●
● Joint + Dict Model
Joint Model
Cascade Model
● ● Supervised Model
Data Size (tokens)
</figure>
<figureCaption confidence="0.999012">
Figure 2: Learning curve for Joint model, Joint
</figureCaption>
<bodyText confidence="0.969715946428572">
+ Dict model, Baseline cascaded and Supervised
model: the x-axis is the size of data (number of
tokens); the y-axis is the average LAS measured
on 9 languages (except English).
target language data? Figure 2 shows the learn-
ing curve with respect to various models on differ-
ent data sizes averaged over all target languages.
For small datasets of 1k training tokens, the cas-
caded model, joint model and joint + dict model
performed similarly well, out-performing the su-
pervised model by about 10% (absolute). With
more training data, we see interesting changes
to the relative performance of the different mod-
els. While the baseline cascade model still out-
performs the supervised model, the improvement
is diminishing, and by 15k, the difference is only
2.9%. On the other hand, compared with the su-
pervised model, the joint and joint + dict models
perform consistently well at all sizes, maintaining
an 8% lead at 15k. This shows the superiority of
joint training compared with single language train-
ing.
To understand this pattern of performance dif-
ferences for the cascade versus the joint model,
one needs to consider the cascade model formu-
lation. In this approach, the target language pa-
rameters are tied (softly) with the source language
parameters through regularization. This is a bene-
fit for small datasets, providing a smoothing func-
tion to limit overtraining. However, when we
have more training data, these constraints limit
the capacity of the model to describe the target
data. This is compounded by the problem that
the source representation may not be appropriate
for modelling the target language, and there is no
way to correct for this. In contrast the joint model
learns a mutually compatible representation auto-
matically during joint training.
The performance results for the joint model
with and without the dictionary are similar over-
all. Only on small datasets (1k, 3k), is the dif-
ference notable. From 5k tokens, the bilingual
dictionary doesn’t confer additional information,
presumably as there is sufficient data for learning
syntactic word representations. Moreover, trans-
lation entries exist between syntactically related
word types as well as semantically related pairs,
with the latter potentially limiting the beneficial
effect of the dictionary.
When training on all the target language data,
the supervised model does well, surpassing the
cascade model. Surprisingly, the joint models out-
perform slightly, yielding a 0.4% improvement.
This is an interesting observation suggesting that
our method has potential for use not only for low
resource problems, but also high resource settings.
</bodyText>
<subsectionHeader confidence="0.998607">
5.2 Different Tagsets
</subsectionHeader>
<bodyText confidence="0.999993777777778">
In the above experiments, we used the universal
POS tagset for all the languages in the corpus.
However, for some languages,9 the UDT also pro-
vides language specific POS tags. We use this data
to test the relative performance of the model using
a universal tagset cf. language specific tagsets. In
this experiment, we applied the same joint model
(see §3) but with a language specific tagset instead
of UPOS for these languages. We expect the joint
</bodyText>
<equation confidence="0.611585">
9en, cs, fi, ga, it and sv.
1k 3k 5k 10k 15k All
</equation>
<page confidence="0.85693">
344
</page>
<figure confidence="0.984559666666667">
45 55 65 75
LAS (%)
Data Size (tokens)
</figure>
<figureCaption confidence="0.9497574">
Figure 3: Learning curve for joint model using the
UPOS tagset or language specific POS tagset: the
x-axis is the size of data (number of tokens); the y-
axis is the average LAS measured on 5 languages
(except English).
</figureCaption>
<bodyText confidence="0.99978435">
model to automatically learn to project the differ-
ent tagsets into a common space, i.e., implicitly
learn a tagset mapping between languages. Fig-
ure 3 shows the learning curve comparing the joint
model with the two types of POS tagsets. For the
small dataset, it is clear that the data is insuffi-
cient for the model to learn a good tagset map-
ping, especially for a morphologically rich lan-
guage like Czech. However, with more data, the
model is better able to learn the tagset mapping as
part of joint training. Beyond 15k tokens, the joint
model using the language specific POS tagset out-
performs UPOS. Clearly there is some information
lost in the UPOS tagset, although the UPOS map-
ping simultanously provides implicit linguistic su-
pervision. This explains why the UPOS might be
useful in small data scenarios, but detrimental at
scale. Using all the target data (“All”) the language
specific POS provides a 1% (absolute) gain over
UPOS.
</bodyText>
<subsectionHeader confidence="0.999192">
5.3 Universal Representation
</subsectionHeader>
<bodyText confidence="0.97466135">
As described in section 3, we can consider our
joint model as the combination of two parts: a uni-
versal parser and a language-specific embedding
Es or Et that converts the source and target lan-
guage into the universal representation. We now
seek to analyse qualitatively this universal repre-
sentation through visualization. For this purpose
we use a joint model of English and French, using
all the available French treebank (more than 350k
Figure 4: Universal Language visualization ac-
cording to language and POS. (This should be
viewed in colour.)
tokens) as well as a bilingual dictionary.10 Fig-
ure 4 shows the t-SNE (Van Der Maaten, 2014)
projection of the 50 dimensional word embed-
dings in both languages. We can see that English
and French are mixed nicely together. The colour-
ing denotes the POS tag, showing clearly that the
words with similar POS tags are grouped together
regardless of languages. This is partially under-
standable since word embeddings for dependency
parsing need to convey the dependency context
rather than surrounding words, as in most distri-
butional embedding models. Words having similar
dependency relation should be grouped together as
they are treated similarly by the parser.
Some of the learned cross-lingual word-
embeddings are shown in Table 2, which includes
the five nearest neighbours to selected English
words according to the monolingual word embed-
ding (section 4.3) and our cross-lingual depen-
dency word embeddings, trained using PanLex.
The monolingual sets appear to be strongly char-
acterised by distributional similarity. The cross-
lingual embeddings display greater semantic sim-
ilarity, while being more variable morphosyntacti-
cally. In many cases, the top five words of English
and French are translations of each other, but with
varying inflectional endings in the French forms.
For example, “buy” vs “vendez” or “invest” vs “in-
</bodyText>
<footnote confidence="0.843079428571428">
10We also visualized the cross-lingual word embeddings
without the dictionary, however the results were rather odd.
Although we saw coherent POS clusters, the two languages
were largely disjoint. We speculate that many components of
the embeddings are use for only one language, and these out-
number the shared components, and thus more careful pro-
jection is needed for meaningful visualisation.
</footnote>
<figure confidence="0.99368947368421">
●
●
●
●
●
●
UPOS
Lang POS
●
ADP
NUM
VERB
DET
NOUN
ADJ
PROPN ADV
English
French
1k 3k 5k 10k 15k All
</figure>
<page confidence="0.973752">
345
</page>
<table confidence="0.990429666666667">
Cross lingual embedding
Words Mono
En Fr
buy buy revendre
eat invest vendez
sell produce integrate acheter
compete guide ach`etent
burn eat investir
serving sailing jouait
acting play navigue
playing paying moving jouent
pursuing faces pi`ece
running ran jouer
difficult crazy dur
harder strange dures
hard easy beautiful hard
magnificent friendly fou
painful difficult folles
originally originally r´eellement
previously previously d´ej`a
initially officially officially r´ecemment
basically actually derni`erement
already already surroˆıt
teachers school universitaire
student education universit´e
university teacher student ´ecole
student medicine scolaire
training participant school
wireless computers mobile
goods Web mobiles
mobile online Internet ordinateurs
freight computer Web
broadband web internet
</table>
<tableCaption confidence="0.984366">
Table 2: Examples of 5 nearest neighbours with
</tableCaption>
<bodyText confidence="0.973802052631579">
the target English word using the original mono-
lingual word embedding and our cross-lingual de-
pendency based word embedding.
vestir”. This is a direct consequence of incorpo-
rating the bilingual lexicon. Moreover, the top five
closest words of both English and French mostly
have the same part of speech. This is consistent
with the finding in Figure 4.
Levin (1993) has shown that there is a strong
connection between a verb’s meaning and its syn-
tactic behaviour. We compare the English side
of our cross-lingual dependency based word em-
beddings with various other pre-trained monolin-
gual English word embeddings and our mono-
lingual embedding (section 4.3) on Verb-143
dataset (Baker et al., 2014). This dataset con-
tains 143 pairs of verbs that are manually given
score from 1 to 10 according to the meaning sim-
ilarity. Table 3 shows the Pearson correlation
</bodyText>
<table confidence="0.929363833333333">
Correlation
Senna (Collobert et al., 2011) 0.36
Skip-gram (Mikolov et al., 2013a) 0.27
RNN (Mikolov et al., 2011) 0.31
Our monolingual embedding 0.39
Our crosslingual embedding 0.44
</table>
<tableCaption confidence="0.981702">
Table 3: Compare the English side of our cross-
</tableCaption>
<bodyText confidence="0.97108694117647">
lingual embeddings with various other embed-
dings evaluated on Verb-143 dataset (Baker et al.,
2014). We directly use the pre-trained models
from corresponding papers.
with human judgment for our embeddings and
other pre-trained embeddings. As expected, our
cross-lingual embeddings out-perform others em-
beddings on this dataset. This is partly because the
syntactic behaviour is well encoded in our word
embeddings through dependency relation.
Our embeddings encode not just cross-lingual
correspondences, but also capture dependency re-
lations which we expect might be beneficial for
other NLP tasks based on dependency parsing,
e.g., cross-lingual semantic role labelling where
long-distance relationship can be captured by
word embedding.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999915045454546">
In this paper, we present a training method for
building a dependency parser for a resource-
poor language using a larger treebank in a high-
resource language. Our approach takes advantage
of the shared structure among languages to learn
a universal parser and language-specific mappings
to the lexicon, parts of speech and dependency
arcs. Compared with supervised learning, our
joint model gives a consistent 8-10% improvement
over several different datasets in simulation low-
resource scenarios. Interestingly, some small but
consistent gains are still realised by joint cross-
lingual training even on large complete treebanks.
This suggests that our approach has utility not just
in low resource settings. Our joint model is flexi-
ble, allowing the incorporation of a bilingual dic-
tionary, which results in small improvements par-
ticularly for tiny training scenarios.
As the side-effect of training our joint model,
we obtain cross-lingual word embeddings special-
ized for dependency parsing. We expect these em-
beddings to be beneficial to other syntatic and se-
</bodyText>
<page confidence="0.996662">
346
</page>
<bodyText confidence="0.9997785">
mantic tasks. In future work, we plan to extend
joint training to several languages, and further ex-
plore the idea of learning and exploiting cross-
lingual embeddings.
</bodyText>
<sectionHeader confidence="0.99547" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999039">
This work was supported by the University of
Melbourne and National ICT Australia (NICTA).
Trevor Cohn is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105).
</bodyText>
<sectionHeader confidence="0.997793" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999827457446809">
Simon Baker, Roi Reichart, and Anna Korhonen. 2014.
An unsupervised model for instance level subcate-
gorization acquisition. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 278–289.
Razvan C. Bunescu and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the Conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, HLT ’05, pages
724–731, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Danqi Chen and Christopher Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Process-
ing (EMNLP), pages 740–750, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 12:2493–2537,
November.
Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and
Tat-Seng Chua. 2005. Question answering passage
retrieval using dependency relations. In Proceed-
ings of the 28th Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, SIGIR ’05, pages 400–407, New
York, NY, USA. ACM.
Matthew S. Dryer and Martin Haspelmath, editors.
2013. WALS Online. Max Planck Institute for Evo-
lutionary Anthropology, Leipzig.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121–2159, July.
Long Duong, Trevor Cohn, Steven Bird, and Paul
Cook. 2015. Low resource dependency parsing:
Cross-lingual parameter sharing in a neural network
parser. In Proceedings of the 53rd Annual Meet-
ing of the Association for Computational Linguistics
and the 7th International Joint Conference on Natu-
ral Language Processing (Volume 2: Short Papers),
pages 845–850, Beijing, China, July. Association for
Computational Linguistics.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’12, pages 1–11, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Andrew Garrett, Clare Sandy, Erik Maier, Line
Mikkelsen, and Patrick Davidson. 2013. Develop-
ing the Karuk Treebank. Fieldwork Forum, Depart-
ment of Linguistics, UC Berkeley.
David Kamholz, Jonathan Pool, and Susan Colowick.
2014. Panlex: Building a resource for panlingual
lexical translation. In Proceedings of the Ninth In-
ternational Conference on Language Resources and
Evaluation (LREC’14), pages 3145–50, Reykjavik,
Iceland. European Language Resources Association
(ELRA).
Min-joo Kim. 2001. Does korean have adjectives.
In MIT Working Papers 43. Proceedings of HUMIT
2001, pages 71–89. MIT Working Papers.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of
the Tenth Machine Translation Summit (MT Summit
X), pages 79–86, Phuket, Thailand.
B. Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University of
Chicago Press.
Xuezhe Ma and Fei Xia. 2014. Unsupervised depen-
dency parsing with transferring distribution via par-
allel guidance and entropy regularization. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1337–1348. Association for Compu-
tational Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ’11, pages 62–72.
Tomas Mikolov, Stefan Kombrink, Anoop Deoras,
Lukar Burget, and Jan Honza Cernocky. 2011.
Rnnlm – recurrent neural network language model-
ing toolkit. In Proc. IEEE Automatic Speech Recog-
nition and Understanding Workshop, December.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
</reference>
<page confidence="0.983143">
347
</page>
<reference confidence="0.99981197260274">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems 26, pages 3111–3119.
Joakim Nivre, Cristina Bosco, Jinho Choi, Marie-
Catherine de Marneffe, Timothy Dozat, Rich´ard
Farkas, Jennifer Foster, Filip Ginter, Yoav Gold-
berg, Jan Hajiˇc, Jenna Kanerva, Veronika Laippala,
Alessandro Lenci, Teresa Lynn, Christopher Man-
ning, Ryan McDonald, Anna Missil¨a, Simonetta
Montemagni, Slav Petrov, Sampo Pyysalo, Natalia
Silveira, Maria Simi, Aaron Smith, Reut Tsarfaty,
Veronika Vincze, and Daniel Zeman. 2015. Univer-
sal dependencies 1.0.
Joakim Nivre. 2006. Inductive Dependency Parsing
(Text, Speech and Language Technology). Springer-
Verlag New York, Inc., Secaucus, NJ, USA.
Levent ¨Ozg¨ur and Tunga G¨ung¨or. 2010. Text clas-
sification with the support of pruned dependency
patterns. Pattern Recogn. Lett., 31(12):1598–1607,
September.
Raivis Skadin¸ˇs, J¨org Tiedemann, Roberts Rozis, and
Daiga Deksne. 2014. Billions of parallel words for
free: Building and using the eu bookshop corpus. In
Proceedings of the 9th International Conference on
Language Resources and Evaluation (LREC-2014),
Reykjavik, Iceland, May. European Language Re-
sources Association (ELRA).
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Ng, and
Christopher Potts. 2013. Recursive deep models
for semantic compositionality over a sentiment tree-
bank. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1631–1642, Seattle, Washington, USA,
October. Association for Computational Linguistics.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,
Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
Dropout: A simple way to prevent neural networks
from overfitting. Journal of Machine Learning Re-
search, 15:1929–1958.
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ’12,
pages 477–487. Association for Computational Lin-
guistics.
Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre.
2013. Target language adaptation of discrimina-
tive transfer parsers. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 1061–1071, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Laurens Van Der Maaten. 2014. Accelerating t-sne
using tree-based algorithms. J. Mach. Learn. Res.,
15(1):3221–3245, January.
Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 245–253, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Daniel Zeman, Univerzita Karlova, and Philip Resnik.
2008. Cross-language parser adaptation between re-
lated languages. In In IJCNLP-08 Workshop on NLP
for Less Privileged Languages, pages 35–42.
</reference>
<page confidence="0.998058">
348
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.474793">
<title confidence="0.9996805">A Neural Network Model for Universal Dependency Parsing</title>
<author confidence="0.998846">Trevor Steven</author>
<affiliation confidence="0.9568205">of Computing and Information Systems, University of ICT Australia, Victoria Research</affiliation>
<address confidence="0.502901">of Computer Science, University of New Brunswick</address>
<email confidence="0.99171">paul.cook@unb.ca</email>
<abstract confidence="0.999649533333333">Accurate dependency parsing requires large treebanks, which are only available for a few languages. We propose a method that takes advantage of shared structure across languages to build a mature parser using less training data. We propose a model for learning a shared “universal” parser that operates over an interlingual continuous representation of language, along with language-specific mapping components. Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Simon Baker</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>An unsupervised model for instance level subcategorization acquisition.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>278--289</pages>
<contexts>
<context position="30322" citStr="Baker et al., 2014" startWordPosition="4907" endWordPosition="4910">cross-lingual dependency based word embedding. vestir”. This is a direct consequence of incorporating the bilingual lexicon. Moreover, the top five closest words of both English and French mostly have the same part of speech. This is consistent with the finding in Figure 4. Levin (1993) has shown that there is a strong connection between a verb’s meaning and its syntactic behaviour. We compare the English side of our cross-lingual dependency based word embeddings with various other pre-trained monolingual English word embeddings and our monolingual embedding (section 4.3) on Verb-143 dataset (Baker et al., 2014). This dataset contains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning similarity. Table 3 shows the Pearson correlation Correlation Senna (Collobert et al., 2011) 0.36 Skip-gram (Mikolov et al., 2013a) 0.27 RNN (Mikolov et al., 2011) 0.31 Our monolingual embedding 0.39 Our crosslingual embedding 0.44 Table 3: Compare the English side of our crosslingual embeddings with various other embeddings evaluated on Verb-143 dataset (Baker et al., 2014). We directly use the pre-trained models from corresponding papers. with human judgment for our embeddings and o</context>
</contexts>
<marker>Baker, Reichart, Korhonen, 2014</marker>
<rawString>Simon Baker, Roi Reichart, and Anna Korhonen. 2014. An unsupervised model for instance level subcategorization acquisition. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, pages 278–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Razvan C Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>724--731</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="1156" citStr="Bunescu and Mooney, 2005" startWordPosition="152" endWordPosition="155">vantage of shared structure across languages to build a mature parser using less training data. We propose a model for learning a shared “universal” parser that operates over an interlingual continuous representation of language, along with language-specific mapping components. Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations. 1 Introduction Dependency parsing is an important task for Natural Language Processing (NLP) with application to text classification (¨Ozg¨ur and G¨ung¨or, 2010), relation extraction (Bunescu and Mooney, 2005), question answering (Cui et al., 2005), statistical machine translation (Xu et al., 2009), and sentiment analysis (Socher et al., 2013). A mature parser normally requires a large treebank for training, yet such resources are rarely available and are costly to build. Ideally, we would be able to construct a high quality parser with less training data, thereby enabling accurate parsing for lowresource languages. In this paper we formalize the dependency parsing task for a low-resource language as a domain adaptation task, in which a target resource-poor language treebank is treated as in-domain</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan C. Bunescu and Raymond J. Mooney. 2005. A shortest path dependency kernel for relation extraction. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 724–731, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="7604" citStr="Chen and Manning (2014)" startWordPosition="1157" endWordPosition="1160">owever, Collobert et al’s goal is to find a single input representation that can work well for many tasks. Our goal is different: we allow the joint-training inputs to be different but constrain the parameter weights in the upper layer to be identical. Consequently, our method applies to the task where inputs are different, possibly from different languages or domains. Their method applies for different tasks in the same language/domain where the inputs are fairly similar. 2.1 Supervised Neural Network Parser This section describes the monolingual neural network dependency parser structure of Chen and Manning (2014). This parser achieves excellent performance, and has a highly flexible formulation allowing auxilliary inputs. The model is based on a transition-based dependency parser (Nivre, 2006) formulated as a neural-network classifier to decide which transition to apply to each parsing state configuration.2 That is, for each configuration, the selected list of words, POS tags and labels from the Stack, Queue and Arcs are extracted. Each word, POS and label is mapped into a lowdimension vector representation using an embedding matrix, which is then fed into a two-layer neural network classifier to pred</context>
<context position="8931" citStr="Chen and Manning (2014)" startWordPosition="1382" endWordPosition="1386">dding layer, W1 for the fully connected cubic hidden layer and W2 for the softmax output layer. The model prediction function is P(Y |X = x,W1,W2,E) = ( 1 softmax W2 x cube(W1 x Φ [�x, E]) (1) 2Our approach is focused on a technique for transfer learning which can be more widely applied to other types of dependency parser (and models, generally) regardless of whether they are transition-based or graph-based. 340 where cube is a non-linear activation function, Φ is the embedding function that returns a vector representation of parsing state x using an embedding matrix E. We refer the reader to Chen and Manning (2014) for a more detailed description. 3 A Joint Interlingual Model We assume a small treebank in a target resourcepoor language, as well as a larger treebank in the source language. Our objective is to learn a model of both languages, subject to the constraint that both models are similar overall, while allowing for some limited language variability. Instead of just training two different parsers on source and then on target, we train them jointly, in order to learn an interlingual parser. This allows the method to take maximum advantage of the limited treebank data available, resulting in highly </context>
<context position="12837" citStr="Chen and Manning (2014)" startWordPosition="2037" endWordPosition="2041">s likely to dominate, and consequently, learning will mainly focus on optimizing the source language parser. We adjust for this disparity by balancing the two datasets, Ds and Dt, during training. When selecting mini-batches for online gradient updates, we select an equal number of classification instances from the source and target languages. Thus, for each step |Ds |= |Dt|, effectively reweighting the cross-entropy components in (2) to ensure parity between the languages. The other concern is over-fitting, especially when we only have a small treebank in the target language. As suggested by Chen and Manning (2014), we apply drop-out, a form of regularization for both source and target language. That is, we randomly drop some of the activation units from both hidden layer and input layer. Following Srivastava et al. (2014), we randomly dropout 20% of the input layer and 50% of the hid|Ds| Ljoint = − i=1 − � |Dt| i=1 341 den layer. Empirically, we observe a substantial improvement applying dropout to the model over MLE or l2 regularization. 3.1 Incorporating a Dictionary Our model is flexible, enabling us to freely add additional components. In this section, we assume the presence of a bilingual dictiona</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2493</pages>
<contexts>
<context position="6734" citStr="Collobert et al., 2011" startWordPosition="1017" endWordPosition="1020">(Dryer and Haspelmath, 2013). For low-resource languages, no large parallel corpus is available. Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013). Accordingly, we adopt a different resource require1http://github.com/longdt219/ universal_dependency_parser ment: a small treebank in the target low-resource language. Domain adaptation or joint-training is a different branch of research, and falls outside the scope of this paper. Nevertheless, we would like to contrast our work with Senna (Collobert et al., 2011), a neural network framework to perform a variety of NLP tasks such as part-of-speech (POS) tagging, named entity recognition (NER), chunking, and so forth. Both approaches exploit common linguistic properties of the data through joint learning. However, Collobert et al’s goal is to find a single input representation that can work well for many tasks. Our goal is different: we allow the joint-training inputs to be different but constrain the parameter weights in the upper layer to be identical. Consequently, our method applies to the task where inputs are different, possibly from different lan</context>
<context position="30525" citStr="Collobert et al., 2011" startWordPosition="4942" endWordPosition="4945">e the same part of speech. This is consistent with the finding in Figure 4. Levin (1993) has shown that there is a strong connection between a verb’s meaning and its syntactic behaviour. We compare the English side of our cross-lingual dependency based word embeddings with various other pre-trained monolingual English word embeddings and our monolingual embedding (section 4.3) on Verb-143 dataset (Baker et al., 2014). This dataset contains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning similarity. Table 3 shows the Pearson correlation Correlation Senna (Collobert et al., 2011) 0.36 Skip-gram (Mikolov et al., 2013a) 0.27 RNN (Mikolov et al., 2011) 0.31 Our monolingual embedding 0.39 Our crosslingual embedding 0.44 Table 3: Compare the English side of our crosslingual embeddings with various other embeddings evaluated on Verb-143 dataset (Baker et al., 2014). We directly use the pre-trained models from corresponding papers. with human judgment for our embeddings and other pre-trained embeddings. As expected, our cross-lingual embeddings out-perform others embeddings on this dataset. This is partly because the syntactic behaviour is well encoded in our word embeddings</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493–2537, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Renxu Sun</author>
<author>Keya Li</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Question answering passage retrieval using dependency relations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’05,</booktitle>
<pages>400--407</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1195" citStr="Cui et al., 2005" startWordPosition="158" endWordPosition="161"> build a mature parser using less training data. We propose a model for learning a shared “universal” parser that operates over an interlingual continuous representation of language, along with language-specific mapping components. Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations. 1 Introduction Dependency parsing is an important task for Natural Language Processing (NLP) with application to text classification (¨Ozg¨ur and G¨ung¨or, 2010), relation extraction (Bunescu and Mooney, 2005), question answering (Cui et al., 2005), statistical machine translation (Xu et al., 2009), and sentiment analysis (Socher et al., 2013). A mature parser normally requires a large treebank for training, yet such resources are rarely available and are costly to build. Ideally, we would be able to construct a high quality parser with less training data, thereby enabling accurate parsing for lowresource languages. In this paper we formalize the dependency parsing task for a low-resource language as a domain adaptation task, in which a target resource-poor language treebank is treated as in-domain, while a much larger treebank in a hig</context>
</contexts>
<marker>Cui, Sun, Li, Kan, Chua, 2005</marker>
<rawString>Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua. 2005. Question answering passage retrieval using dependency relations. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’05, pages 400–407, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="false">
<editor>Matthew S. Dryer and Martin Haspelmath, editors. 2013. WALS Online. Max</editor>
<institution>Planck Institute for Evolutionary Anthropology,</institution>
<location>Leipzig.</location>
<marker></marker>
<rawString>Matthew S. Dryer and Martin Haspelmath, editors. 2013. WALS Online. Max Planck Institute for Evolutionary Anthropology, Leipzig.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>12--2121</pages>
<contexts>
<context position="14735" citStr="Duchi et al., 2011" startWordPosition="2377" endWordPosition="2380">also add further constraints such as their language-specific embeddings be close together. This results a regularised training objective, ( E word(i)_ word(j) 2 Ldict = Ljoint−A ��Es Et I F hi,ji∈D + I I Ers − Eios 112 + I I Earc s − Earc t IIF) , (3) where A E [0, oc] controls to what degree we bind these words or pos tags or arc labels together, with high A tying the parameters and small A allowing independent learning. We expect the best value of A to fall somewhere between these extremes. Finally, we use a mini-batch size of 1000 instance pairs and adaptive learning rate trainer, adagrad (Duchi et al., 2011) to build our two separate models corresponding to equations 2 and 3. 4 Experiments In this section, we compare our joint training approach with baseline methods of supervised learning in the target language, and cascaded learning of source and target parsers. 3However, this is not always the case. For example, modal or auxiliary verbs in English often have no translations in different languages or map to words with different syntactic functions. 4As was the case for our experiments. 4.1 Dataset We experiment with the Universal Dependency Treebank (UDT) V1.0 (Nivre et al., 2015), simulating lo</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. J. Mach. Learn. Res., 12:2121–2159, July.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Long Duong</author>
<author>Trevor Cohn</author>
<author>Steven Bird</author>
<author>Paul Cook</author>
</authors>
<title>Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),</booktitle>
<pages>845--850</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="3679" citStr="Duong et al., 2015" startWordPosition="550" endWordPosition="553">e also allowing the parsers to mutually correct one another’s errors. We include some language-specific components, in order to better model the lexicon of each language and allow learning of the syntactic idiosyncrasies of each language. Our experiments show that this outperforms a purely supervised setting, on both small and large data conditions, with a gain as high as 10% for small training sets. Our proposed joint training method also out-performs the conventional cascade approach where the parameters between source and target languages are related together through a regularization term (Duong et al., 2015). Our model is flexible, allowing easy incorporation of peripheral information. For example, assuming the presence of a small bilingual dictionary is befitting of a low-resource setting, as this is prototypically one of the first artifacts generated by field linguists. We incorporate a bilingual dictionary as a set of soft constraints on the 339 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 339–348, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. model, such that it learns similar representations for each</context>
<context position="16345" citStr="Duong et al., 2015" startWordPosition="2637" endWordPosition="2640">e treebank covers 10 languages,6 with some languages very highly resourced—Czech, French and Spanish have 400k tokens—and only modest amounts of data for other languages—Hungarian and Irish have only around 25k tokens. Cross-lingual models assume English as the source language, for which we have a large treebank, and only a small treebank of 3k tokens exists in each target language, simulated by subsampling the corpus. 4.2 Baseline Cascade Model We compare our approach to a baseline interlingual model based on the same parsing algorithm as presented in section 2.1, but with cascaded training (Duong et al., 2015). This works by first learning the source language parser, and then training the target language parser using a regularization term to minimise the distance between the parameters of the target parser and the source parser (which is fixed). In this way, some structural information from the source parser can be used in the target parser, however it is likely that the representation will be overly biased towards the source language and consequently may not prove as useful for modelling the target. 4.3 Monolingual Word Embeddings While the Epos and Earc are randomly initialized, we initialize bot</context>
</contexts>
<marker>Duong, Cohn, Bird, Cook, 2015</marker>
<rawString>Long Duong, Trevor Cohn, Steven Bird, and Paul Cook. 2015. Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 845–850, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Syntactic transfer using a bilingual lexicon.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5816" citStr="Durrett et al. (2012)" startWordPosition="883" endWordPosition="886">s are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). For low-resource languages, no large parallel corpus is available. Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013). Accordingly, we adopt a different resource requi</context>
</contexts>
<marker>Durrett, Pauls, Klein, 2012</marker>
<rawString>Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntactic transfer using a bilingual lexicon. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12, pages 1–11, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Garrett</author>
<author>Clare Sandy</author>
<author>Erik Maier</author>
<author>Line Mikkelsen</author>
<author>Patrick Davidson</author>
</authors>
<title>Developing the Karuk Treebank. Fieldwork Forum, Department of Linguistics,</title>
<date>2013</date>
<location>UC Berkeley.</location>
<contexts>
<context position="6366" citStr="Garrett et al., 2013" startWordPosition="964" endWordPosition="967">added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). For low-resource languages, no large parallel corpus is available. Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013). Accordingly, we adopt a different resource require1http://github.com/longdt219/ universal_dependency_parser ment: a small treebank in the target low-resource language. Domain adaptation or joint-training is a different branch of research, and falls outside the scope of this paper. Nevertheless, we would like to contrast our work with Senna (Collobert et al., 2011), a neural network framework to perform a variety of NLP tasks such as part-of-speech (POS) tagging, named entity recognition (NER), chunking, and so forth. Both approaches exploit common linguistic properties of the data through jo</context>
</contexts>
<marker>Garrett, Sandy, Maier, Mikkelsen, Davidson, 2013</marker>
<rawString>Andrew Garrett, Clare Sandy, Erik Maier, Line Mikkelsen, and Patrick Davidson. 2013. Developing the Karuk Treebank. Fieldwork Forum, Department of Linguistics, UC Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kamholz</author>
<author>Jonathan Pool</author>
<author>Susan Colowick</author>
</authors>
<title>Panlex: Building a resource for panlingual lexical translation.</title>
<date>2014</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<pages>3145--50</pages>
<location>Reykjavik,</location>
<contexts>
<context position="18342" citStr="Kamholz et al., 2014" startWordPosition="2962" endWordPosition="2965">ns). resource-poor languages. We collect monolingual data for each language from the Machine Translation Workshop (WMT) data,7 Europarl (Koehn, 2005) and EU Bookshop Corpus (Skadin¸ˇs et al., 2014). The size of monolingual data also varies significantly, with as much as 400 million tokens for English and German, and as few as 4 million tokens for Irish. We use the skip-gram model (Mikolov et al., 2013b) to induce 50-dimensional word embeddings. 4.4 Bilingual Dictionary For the extended model as described in section 3.1, we also need a bilingual dictionary. We extract dictionaries from PanLex (Kamholz et al., 2014) which currently covers around 1300 language varieties and about 12 million expressions. This dataset is growing and aims at covering all languages in the world and up to 350 million expressions. The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc. Naturally, the bilingual dictionary size varies greatly among resource-poor and resource-rich languages. 4.5 Regularization Parameter Tuning Joint training with a dictionary (see equation 3) includes a regularization sensitivity parameter A. This parameter controls to w</context>
</contexts>
<marker>Kamholz, Pool, Colowick, 2014</marker>
<rawString>David Kamholz, Jonathan Pool, and Susan Colowick. 2014. Panlex: Building a resource for panlingual lexical translation. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), pages 3145–50, Reykjavik, Iceland. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-joo Kim</author>
</authors>
<title>Does korean have adjectives.</title>
<date>2001</date>
<booktitle>In MIT Working Papers 43. Proceedings of HUMIT 2001,</booktitle>
<pages>71--89</pages>
<publisher>MIT Working Papers.</publisher>
<contexts>
<context position="10923" citStr="Kim, 2001" startWordPosition="1733" endWordPosition="1734">, W1 and W2, are identical in both parsers. Thereby P(Yα|Xα = ~x) = P(Y |X = ~x, W1, W2, Eα), where the subscript α ∈ {s, t} denotes the source or target language. We allow the embedding matrix Eα to differ in order to accommodate language-specific features, in terms of the representations of lexical types, Eword s , part-of-speech, Epos s and dependency arc labels Earc s . This reflects the fact that different languages have different lexicon, parts-of-speech often exhibit different roles, and dependency edges serve different functions, e.g. in Korean a static verb can serve as an adjective (Kim, 2001). During training, the languagespecific errors are back propagated through different branches according to the language, guiding learning towards an interlingual representation that informs parsing decisions in both languages. The set of parameters for the model is W1, W2, Es, Et where Es, Et are the embedding matrices for the source and target languages. Generally speaking, we can understand the model as building the universal dependency parser that parses the universal language. Specifically, the model is the combination of two parts: the universal part (W1, W2) that is shared between the la</context>
</contexts>
<marker>Kim, 2001</marker>
<rawString>Min-joo Kim. 2001. Does korean have adjectives. In MIT Working Papers 43. Proceedings of HUMIT 2001, pages 71–89. MIT Working Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Tenth Machine Translation Summit (MT Summit X),</booktitle>
<pages>79--86</pages>
<location>Phuket, Thailand.</location>
<contexts>
<context position="17870" citStr="Koehn, 2005" startWordPosition="2886" endWordPosition="2887">r for ease of training and evaluation, which requires a small treebank in the target language, we simulate the low-resource setting using a small part of the UDT. 6Czech (cs), English (en), Finnish (fi), French (fr), German (de), Hungarian (hu), Irish (ga), Italian (it), Spanish (es), Swedish (sv). 342 54 55 56 57 58 LAS (%) Lambda Figure 1: Sensitivity of regularization parameter A against the LAS measured on the Swedish development set trained on 1000 (tokens). resource-poor languages. We collect monolingual data for each language from the Machine Translation Workshop (WMT) data,7 Europarl (Koehn, 2005) and EU Bookshop Corpus (Skadin¸ˇs et al., 2014). The size of monolingual data also varies significantly, with as much as 400 million tokens for English and German, and as few as 4 million tokens for Irish. We use the skip-gram model (Mikolov et al., 2013b) to induce 50-dimensional word embeddings. 4.4 Bilingual Dictionary For the extended model as described in section 3.1, we also need a bilingual dictionary. We extract dictionaries from PanLex (Kamholz et al., 2014) which currently covers around 1300 language varieties and about 12 million expressions. This dataset is growing and aims at cov</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of the Tenth Machine Translation Summit (MT Summit X), pages 79–86, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="29990" citStr="Levin (1993)" startWordPosition="4856" endWordPosition="4857">teacher student ´ecole student medicine scolaire training participant school wireless computers mobile goods Web mobiles mobile online Internet ordinateurs freight computer Web broadband web internet Table 2: Examples of 5 nearest neighbours with the target English word using the original monolingual word embedding and our cross-lingual dependency based word embedding. vestir”. This is a direct consequence of incorporating the bilingual lexicon. Moreover, the top five closest words of both English and French mostly have the same part of speech. This is consistent with the finding in Figure 4. Levin (1993) has shown that there is a strong connection between a verb’s meaning and its syntactic behaviour. We compare the English side of our cross-lingual dependency based word embeddings with various other pre-trained monolingual English word embeddings and our monolingual embedding (section 4.3) on Verb-143 dataset (Baker et al., 2014). This dataset contains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning similarity. Table 3 shows the Pearson correlation Correlation Senna (Collobert et al., 2011) 0.36 Skip-gram (Mikolov et al., 2013a) 0.27 RNN (Mikolov et al.,</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuezhe Ma</author>
<author>Fei Xia</author>
</authors>
<title>Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1337--1348</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5511" citStr="Ma and Xia (2014)" startWordPosition="836" endWordPosition="839">parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008). It is then applied directly to parse sentences in the target resource-poor languages. Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) </context>
</contexts>
<marker>Ma, Xia, 2014</marker>
<rawString>Xuezhe Ma and Fei Xia. 2014. Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1337–1348. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>62--72</pages>
<contexts>
<context position="2531" citStr="McDonald et al., 2011" startWordPosition="366" endWordPosition="369">niques to the dependency parsing task. However, a crucial requirement for domain adaptation is that the in-domain and out-of-domain data have compatible representations. In applying our approach to data from several languages, we must learn such a cross-lingual representation. Here we frame this representation learning as part of a neural network training. The underlying hypothesis for the joint learning is that there are some shared-structures across languages that we can exploit. This hypothesis is motivated by the excellent results of the cross-lingual application of unlexicalised parsing (McDonald et al., 2011), whereby a delexicalized parser constructed on one language is applied directly to another language. Our approach works by jointly training a neural network dependency parser to model the syntax in both a source and target language. Many of the parameters of the source and target language parsers are shared, except for a small handful of language-specific parameters. In this way, the information can flow back and forth between languages, allowing for the learning of a compatible cross-lingual syntactic representation, while also allowing the parsers to mutually correct one another’s errors. W</context>
<context position="5489" citStr="McDonald et al. (2011)" startWordPosition="831" endWordPosition="834">alized parsing, in which a parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008). It is then applied directly to parse sentences in the target resource-poor languages. Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Langu</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 62–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Stefan Kombrink</author>
<author>Anoop Deoras</author>
<author>Lukar Burget</author>
<author>Jan Honza Cernocky</author>
</authors>
<title>Rnnlm – recurrent neural network language modeling toolkit.</title>
<date>2011</date>
<booktitle>In Proc. IEEE Automatic Speech Recognition and Understanding Workshop,</booktitle>
<contexts>
<context position="30596" citStr="Mikolov et al., 2011" startWordPosition="4954" endWordPosition="4957">4. Levin (1993) has shown that there is a strong connection between a verb’s meaning and its syntactic behaviour. We compare the English side of our cross-lingual dependency based word embeddings with various other pre-trained monolingual English word embeddings and our monolingual embedding (section 4.3) on Verb-143 dataset (Baker et al., 2014). This dataset contains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning similarity. Table 3 shows the Pearson correlation Correlation Senna (Collobert et al., 2011) 0.36 Skip-gram (Mikolov et al., 2013a) 0.27 RNN (Mikolov et al., 2011) 0.31 Our monolingual embedding 0.39 Our crosslingual embedding 0.44 Table 3: Compare the English side of our crosslingual embeddings with various other embeddings evaluated on Verb-143 dataset (Baker et al., 2014). We directly use the pre-trained models from corresponding papers. with human judgment for our embeddings and other pre-trained embeddings. As expected, our cross-lingual embeddings out-perform others embeddings on this dataset. This is partly because the syntactic behaviour is well encoded in our word embeddings through dependency relation. Our embeddings encode not just cross-ling</context>
</contexts>
<marker>Mikolov, Kombrink, Deoras, Burget, Cernocky, 2011</marker>
<rawString>Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar Burget, and Jan Honza Cernocky. 2011. Rnnlm – recurrent neural network language modeling toolkit. In Proc. IEEE Automatic Speech Recognition and Understanding Workshop, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="18125" citStr="Mikolov et al., 2013" startWordPosition="2930" endWordPosition="2933">rish (ga), Italian (it), Spanish (es), Swedish (sv). 342 54 55 56 57 58 LAS (%) Lambda Figure 1: Sensitivity of regularization parameter A against the LAS measured on the Swedish development set trained on 1000 (tokens). resource-poor languages. We collect monolingual data for each language from the Machine Translation Workshop (WMT) data,7 Europarl (Koehn, 2005) and EU Bookshop Corpus (Skadin¸ˇs et al., 2014). The size of monolingual data also varies significantly, with as much as 400 million tokens for English and German, and as few as 4 million tokens for Irish. We use the skip-gram model (Mikolov et al., 2013b) to induce 50-dimensional word embeddings. 4.4 Bilingual Dictionary For the extended model as described in section 3.1, we also need a bilingual dictionary. We extract dictionaries from PanLex (Kamholz et al., 2014) which currently covers around 1300 language varieties and about 12 million expressions. This dataset is growing and aims at covering all languages in the world and up to 350 million expressions. The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc. Naturally, the bilingual dictionary size varies great</context>
<context position="30562" citStr="Mikolov et al., 2013" startWordPosition="4948" endWordPosition="4951">stent with the finding in Figure 4. Levin (1993) has shown that there is a strong connection between a verb’s meaning and its syntactic behaviour. We compare the English side of our cross-lingual dependency based word embeddings with various other pre-trained monolingual English word embeddings and our monolingual embedding (section 4.3) on Verb-143 dataset (Baker et al., 2014). This dataset contains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning similarity. Table 3 shows the Pearson correlation Correlation Senna (Collobert et al., 2011) 0.36 Skip-gram (Mikolov et al., 2013a) 0.27 RNN (Mikolov et al., 2011) 0.31 Our monolingual embedding 0.39 Our crosslingual embedding 0.44 Table 3: Compare the English side of our crosslingual embeddings with various other embeddings evaluated on Verb-143 dataset (Baker et al., 2014). We directly use the pre-trained models from corresponding papers. with human judgment for our embeddings and other pre-trained embeddings. As expected, our cross-lingual embeddings out-perform others embeddings on this dataset. This is partly because the syntactic behaviour is well encoded in our word embeddings through dependency relation. Our emb</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems 26,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="18125" citStr="Mikolov et al., 2013" startWordPosition="2930" endWordPosition="2933">rish (ga), Italian (it), Spanish (es), Swedish (sv). 342 54 55 56 57 58 LAS (%) Lambda Figure 1: Sensitivity of regularization parameter A against the LAS measured on the Swedish development set trained on 1000 (tokens). resource-poor languages. We collect monolingual data for each language from the Machine Translation Workshop (WMT) data,7 Europarl (Koehn, 2005) and EU Bookshop Corpus (Skadin¸ˇs et al., 2014). The size of monolingual data also varies significantly, with as much as 400 million tokens for English and German, and as few as 4 million tokens for Irish. We use the skip-gram model (Mikolov et al., 2013b) to induce 50-dimensional word embeddings. 4.4 Bilingual Dictionary For the extended model as described in section 3.1, we also need a bilingual dictionary. We extract dictionaries from PanLex (Kamholz et al., 2014) which currently covers around 1300 language varieties and about 12 million expressions. This dataset is growing and aims at covering all languages in the world and up to 350 million expressions. The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc. Naturally, the bilingual dictionary size varies great</context>
<context position="30562" citStr="Mikolov et al., 2013" startWordPosition="4948" endWordPosition="4951">stent with the finding in Figure 4. Levin (1993) has shown that there is a strong connection between a verb’s meaning and its syntactic behaviour. We compare the English side of our cross-lingual dependency based word embeddings with various other pre-trained monolingual English word embeddings and our monolingual embedding (section 4.3) on Verb-143 dataset (Baker et al., 2014). This dataset contains 143 pairs of verbs that are manually given score from 1 to 10 according to the meaning similarity. Table 3 shows the Pearson correlation Correlation Senna (Collobert et al., 2011) 0.36 Skip-gram (Mikolov et al., 2013a) 0.27 RNN (Mikolov et al., 2011) 0.31 Our monolingual embedding 0.39 Our crosslingual embedding 0.44 Table 3: Compare the English side of our crosslingual embeddings with various other embeddings evaluated on Verb-143 dataset (Baker et al., 2014). We directly use the pre-trained models from corresponding papers. with human judgment for our embeddings and other pre-trained embeddings. As expected, our cross-lingual embeddings out-perform others embeddings on this dataset. This is partly because the syntactic behaviour is well encoded in our word embeddings through dependency relation. Our emb</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26, pages 3111–3119.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Joakim Nivre</author>
<author>Cristina Bosco</author>
<author>Jinho Choi</author>
<author>MarieCatherine de Marneffe</author>
<author>Timothy Dozat</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
<author>Filip Ginter</author>
<author>Yoav Goldberg</author>
<author>Jan Hajiˇc</author>
<author>Jenna Kanerva</author>
</authors>
<date>2015</date>
<location>Veronika Laippala, Alessandro Lenci, Teresa Lynn, Christopher Manning, Ryan McDonald, Anna Missil¨a, Simonetta Montemagni, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Maria</location>
<note>Universal dependencies 1.0.</note>
<marker>Nivre, Bosco, Choi, de Marneffe, Dozat, Farkas, Foster, Ginter, Goldberg, Hajiˇc, Kanerva, 2015</marker>
<rawString>Joakim Nivre, Cristina Bosco, Jinho Choi, MarieCatherine de Marneffe, Timothy Dozat, Rich´ard Farkas, Jennifer Foster, Filip Ginter, Yoav Goldberg, Jan Hajiˇc, Jenna Kanerva, Veronika Laippala, Alessandro Lenci, Teresa Lynn, Christopher Manning, Ryan McDonald, Anna Missil¨a, Simonetta Montemagni, Slav Petrov, Sampo Pyysalo, Natalia Silveira, Maria Simi, Aaron Smith, Reut Tsarfaty, Veronika Vincze, and Daniel Zeman. 2015. Universal dependencies 1.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Inductive Dependency Parsing (Text, Speech and Language Technology).</title>
<date>2006</date>
<publisher>SpringerVerlag</publisher>
<location>New York, Inc., Secaucus, NJ, USA.</location>
<contexts>
<context position="7788" citStr="Nivre, 2006" startWordPosition="1185" endWordPosition="1186">the parameter weights in the upper layer to be identical. Consequently, our method applies to the task where inputs are different, possibly from different languages or domains. Their method applies for different tasks in the same language/domain where the inputs are fairly similar. 2.1 Supervised Neural Network Parser This section describes the monolingual neural network dependency parser structure of Chen and Manning (2014). This parser achieves excellent performance, and has a highly flexible formulation allowing auxilliary inputs. The model is based on a transition-based dependency parser (Nivre, 2006) formulated as a neural-network classifier to decide which transition to apply to each parsing state configuration.2 That is, for each configuration, the selected list of words, POS tags and labels from the Stack, Queue and Arcs are extracted. Each word, POS and label is mapped into a lowdimension vector representation using an embedding matrix, which is then fed into a two-layer neural network classifier to predict the next parsing action. The set of parameters for the model is E = {Eword, Epos, Earc} for the embedding layer, W1 for the fully connected cubic hidden layer and W2 for the softma</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>Joakim Nivre. 2006. Inductive Dependency Parsing (Text, Speech and Language Technology). SpringerVerlag New York, Inc., Secaucus, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levent ¨Ozg¨ur</author>
<author>Tunga G¨ung¨or</author>
</authors>
<title>Text classification with the support of pruned dependency patterns.</title>
<date>2010</date>
<journal>Pattern Recogn. Lett.,</journal>
<volume>31</volume>
<issue>12</issue>
<marker>¨Ozg¨ur, G¨ung¨or, 2010</marker>
<rawString>Levent ¨Ozg¨ur and Tunga G¨ung¨or. 2010. Text classification with the support of pruned dependency patterns. Pattern Recogn. Lett., 31(12):1598–1607, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Raivis Skadin¸ˇs</author>
<author>J¨org Tiedemann</author>
<author>Roberts Rozis</author>
<author>Daiga Deksne</author>
</authors>
<title>Billions of parallel words for free: Building and using the eu bookshop corpus.</title>
<date>2014</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC-2014),</booktitle>
<location>Reykjavik, Iceland,</location>
<marker>Skadin¸ˇs, Tiedemann, Rozis, Deksne, 2014</marker>
<rawString>Raivis Skadin¸ˇs, J¨org Tiedemann, Roberts Rozis, and Daiga Deksne. 2014. Billions of parallel words for free: Building and using the eu bookshop corpus. In Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC-2014), Reykjavik, Iceland, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1631--1642</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle, Washington, USA,</location>
<contexts>
<context position="1292" citStr="Socher et al., 2013" startWordPosition="173" endWordPosition="176">versal” parser that operates over an interlingual continuous representation of language, along with language-specific mapping components. Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations. 1 Introduction Dependency parsing is an important task for Natural Language Processing (NLP) with application to text classification (¨Ozg¨ur and G¨ung¨or, 2010), relation extraction (Bunescu and Mooney, 2005), question answering (Cui et al., 2005), statistical machine translation (Xu et al., 2009), and sentiment analysis (Socher et al., 2013). A mature parser normally requires a large treebank for training, yet such resources are rarely available and are costly to build. Ideally, we would be able to construct a high quality parser with less training data, thereby enabling accurate parsing for lowresource languages. In this paper we formalize the dependency parsing task for a low-resource language as a domain adaptation task, in which a target resource-poor language treebank is treated as in-domain, while a much larger treebank in a high-resource language forms the out-of-domain data. In this way, we can apply well-understood domai</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1631–1642, Seattle, Washington, USA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>15--1929</pages>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929–1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12,</booktitle>
<pages>477--487</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT ’12, pages 477–487. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Target language adaptation of discriminative transfer parsers.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1061--1071</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>T¨ackstr¨om, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Joakim Nivre. 2013. Target language adaptation of discriminative transfer parsers. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1061–1071, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurens Van Der Maaten</author>
</authors>
<title>Accelerating t-sne using tree-based algorithms.</title>
<date>2014</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Van Der Maaten, 2014</marker>
<rawString>Laurens Van Der Maaten. 2014. Accelerating t-sne using tree-based algorithms. J. Mach. Learn. Res., 15(1):3221–3245, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Xu</author>
<author>Jaeho Kang</author>
<author>Michael Ringgaard</author>
<author>Franz Och</author>
</authors>
<title>Using a dependency parser to improve smt for subject-object-verb languages.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>245--253</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1246" citStr="Xu et al., 2009" startWordPosition="165" endWordPosition="168">propose a model for learning a shared “universal” parser that operates over an interlingual continuous representation of language, along with language-specific mapping components. Compared with supervised learning, our methods give a consistent 8-10% improvement across several treebanks in low-resource simulations. 1 Introduction Dependency parsing is an important task for Natural Language Processing (NLP) with application to text classification (¨Ozg¨ur and G¨ung¨or, 2010), relation extraction (Bunescu and Mooney, 2005), question answering (Cui et al., 2005), statistical machine translation (Xu et al., 2009), and sentiment analysis (Socher et al., 2013). A mature parser normally requires a large treebank for training, yet such resources are rarely available and are costly to build. Ideally, we would be able to construct a high quality parser with less training data, thereby enabling accurate parsing for lowresource languages. In this paper we formalize the dependency parsing task for a low-resource language as a domain adaptation task, in which a target resource-poor language treebank is treated as in-domain, while a much larger treebank in a high-resource language forms the out-of-domain data. I</context>
</contexts>
<marker>Xu, Kang, Ringgaard, Och, 2009</marker>
<rawString>Peng Xu, Jaeho Kang, Michael Ringgaard, and Franz Och. 2009. Using a dependency parser to improve smt for subject-object-verb languages. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 245–253, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
<author>Univerzita Karlova</author>
<author>Philip Resnik</author>
</authors>
<title>Cross-language parser adaptation between related languages. In</title>
<date>2008</date>
<booktitle>In IJCNLP-08 Workshop on NLP for Less Privileged Languages,</booktitle>
<pages>35--42</pages>
<contexts>
<context position="5021" citStr="Zeman et al., 2008" startWordPosition="761" endWordPosition="764">irically show that adding a bilingual dictionary improves parser performance, particularly when target data is limited. The final contribution of the paper concerns the learned word embeddings. We demonstrate that these encode meaningful syntactic phenomena, both in terms of the observable clusters and through a verb classification task. The code for this paper is published as an open source project.1 2 Related Work This work is motivated by the idea of delexicalized parsing, in which a parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008). It is then applied directly to parse sentences in the target resource-poor languages. Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the ta</context>
</contexts>
<marker>Zeman, Karlova, Resnik, 2008</marker>
<rawString>Daniel Zeman, Univerzita Karlova, and Philip Resnik. 2008. Cross-language parser adaptation between related languages. In In IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 35–42.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
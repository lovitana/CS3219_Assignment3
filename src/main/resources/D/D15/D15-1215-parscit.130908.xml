<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9868725">
Transition-based Dependency Parsing
Using Two Heterogeneous Gated Recursive Neural Networks
</title>
<author confidence="0.993612">
Xinchi Chen, Yaqian Zhou, Chenxi Zhu, Xipeng Qiu, Xuanjing Huang
</author>
<affiliation confidence="0.9989995">
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
</affiliation>
<address confidence="0.967244">
825 Zhangheng Road, Shanghai, China
</address>
<email confidence="0.997833">
{xinchichen13,zhouyaqian,czhu13,xpqiu,xjhuang}@fudan.edu.cn
</email>
<sectionHeader confidence="0.984815" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999944785714286">
Recently, neural network based depen-
dency parsing has attracted much interest,
which can effectively alleviate the prob-
lems of data sparsity and feature engineer-
ing by using the dense features. How-
ever, it is still a challenge problem to
sufficiently model the complicated syn-
tactic and semantic compositions of the
dense features in neural network based
methods. In this paper, we propose
two heterogeneous gated recursive neu-
ral networks: tree structured gated re-
cursive neural network (Tree-GRNN) and
directed acyclic graph structured gated
recursive neural network (DAG-GRNN).
Then we integrate them to automati-
cally learn the compositions of the dense
features for transition-based dependency
parsing. Specifically, Tree-GRNN mod-
els the feature combinations for the trees
in stack, which already have partial depen-
dency structures. DAG-GRNN models the
feature combinations of the nodes whose
dependency relations have not been built
yet. Experiment results on two prevalent
benchmark datasets (PTB3 and CTB5)
show the effectiveness of our proposed
model.
</bodyText>
<sectionHeader confidence="0.992537" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.944540428571428">
Transition-based dependency parsing is a core task
in natural language processing, which has been
studied with considerable efforts in the NLP com-
munity. The traditional discriminative dependency
parsing methods have achieved great success (Koo
et al., 2008; He et al., 2013; Bohnet, 2010; Huang
and Sagae, 2010; Zhang and Nivre, 2011; Mar-
</bodyText>
<note confidence="0.683186">
tins et al., 2009; McDonald et al., 2005; Nivre et
al., 2006; K¨ubler et al., 2009; Goldberg and Nivre,
</note>
<figure confidence="0.972948">
(a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN
</figure>
<figureCaption confidence="0.999088">
Figure 1: Sketch of three recursive neural net-
</figureCaption>
<bodyText confidence="0.980718818181818">
works (RNN). (a) is the standard RNN for con-
stituent tree; (b) is Tree-GRNN for dependency
tree, in which the dashed arcs indicate the depen-
dency relations between the nodes; (c) is DAG-
GRNN for the nodes without given topological
structure.
2013; Choi and McCallum, 2013; Ballesteros and
Bohnet, 2014). However, these methods are based
on discrete features and suffer from the problems
of data sparsity and feature engineering (Chen and
Manning, 2014).
Recently, distributed representations have been
widely used in a variety of natural language pro-
cessing (NLP) tasks (Collobert et al., 2011; De-
vlin et al., 2014; Socher et al., 2013; Turian et al.,
2010; Mikolov et al., 2013b; Bengio et al., 2003).
Specific to the transition-based parsing, the neu-
ral network based methods have also been increas-
ingly focused on due to their ability to minimize
the efforts in feature engineering and the boosted
performance (Le and Zuidema, 2014; Stenetorp,
2013; Bansal et al., 2014; Chen and Manning,
2014; Zhu et al., 2015).
However, most of the existing neural network
based methods still need some efforts in feature
engineering. For example, most methods often se-
lect the first and second leftmost/rightmost chil-
dren of the top nodes in stack, which could miss
some valuable information hidden in the unchosen
nodes. Besides, the features of the selected nodes
are just simply concatenated and then fed into neu-
ral network. Since the concatenation operation is
relatively simple, it is difficult to model the com-
</bodyText>
<page confidence="0.372235">
1879
</page>
<note confidence="0.996305">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1879–1889,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999892789473684">
plicated feature combinations which can be man-
ually designed in the traditional discrete feature
based methods.
To tackle these problems, we use two het-
erogeneous gated recursive neural networks, tree
structured gated recursive neural network (Tree-
GRNN) and directed acyclic graph gated struc-
tured recursive neural network (DAG-GRNN), to
model each configuration during transition based
dependency parsing. The two proposed GRNNs
introduce the gate mechanism (Chung et al., 2014)
to improve the standard recursive neural network
(RNN) (Socher et al., 2013; Socher et al., 2014),
and can model the syntactic and semantic compo-
sitions of the nodes during parsing.
Figure 1 gives a rough sketch for the standard
RNN, Tree-GRNN and DAG-GRNN. Tree-GRNN
is applied to the partial-constructed trees in stack,
which have already been constructed according to
the previous transition actions. DAG-GRNN is ap-
plied to model the feature composition of nodes in
stack and buffer which have not been labeled their
dependency relations yet. Intuitively, Tree-GRNN
selects and merges features recursively from chil-
dren nodes into their parent according to their de-
pendency structures, while DAG-GRNN further
models the complicated combinations of extracted
features and explicitly exploits features in differ-
ent levels of granularity.
To evaluate our approach, we experiment on
two prevalent benchmark datasets: English Penn
Treebank 3 (PTB3) and Chinese Penn Treebank 5
(CTB5) datasets. Experiment results show the ef-
fectiveness of our proposed method. Compared to
the parser of Chen and Manning (2014), we re-
ceive 0.6% (UAS) and 0.9% (LAS) improvement
on PTB3 test set, while we receive 0.8% (UAS)
and 1.3% (LAS) improvement on CTB5 test set.
</bodyText>
<sectionHeader confidence="0.9615285" genericHeader="introduction">
2 Neural Network Based Transition
Dependency Parsing
</sectionHeader>
<subsectionHeader confidence="0.996163">
2.1 Transition Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.996055423076923">
In this paper, we employ the arc-standard tran-
sition systems (Nivre, 2004) and examine only
greedy parsing for its efficiency. Figure 2 gives
an example of arc-standard transition dependency
parsing.
In transition-based dependency parsing, the
consecutive configurations of parsing process can
be defined as c(i) = (s(i), b(i), A(i)) which con-
sists of a stack s, a buffer b, and a set of
dependency arcs A. Then, the greedy pars-
ing process consecutively predicts the actions
based on the features extracted from the corre-
sponding configurations. For a given sentence
w1, ... , wn, parsing process starts from a initial
configuration c(0) = ([ROOT], [w1, ... , wn], 0),
and terminates at some configuration c(2n) =
([ROOT], 0, A(2n)], where n is the length of the
given sentence w1:n. As a result, we derive the
parse tree of the sentence w1:n according to the
arcs set A(2n).
In arc-standard system, there are three types of
actions: Left-Arc, Right-Arc and Shift. Denot-
ing sj(j = 1, 2,... ) as the jth top element of the
stack, and bj(j = 1, 2,... ) as the jth front ele-
ment of the buffer, we can formalize the three ac-
tions of arc-standard system as:
</bodyText>
<listItem confidence="0.999324111111111">
• Left-Arc(l) adds an arc s2 +— s1 with label
l and removes s2 from the stack, resulting a
new arc l(s1, s2). Precondition: |s |&gt; 3 (The
ROOT node cannot be child node).
• Right-Arc(l) adds an arc s2 —* s1 with label
l and removes s1 from the stack, resulting a
new arc l(s2, s1). Precondition: |s |&gt; 2.
• Shift removes b1 from the buffer, and adds it
to the stack. Precondition: |b |&gt; 1.
</listItem>
<bodyText confidence="0.999810714285714">
The greedy parser aims to predict the correct
transition action for a given configuration. There
are two versions of parsing: unlabeled and labeled
versions. The set of possible action candidates
T = 2nl + 1 in the labeled version of parsing,
and T = 3 in the unlabeled version, where nl is
number of different types of arc labels.
</bodyText>
<subsectionHeader confidence="0.992031">
2.2 Neural Network Based Parser
</subsectionHeader>
<bodyText confidence="0.99706375">
In neural network architecture, the words, POS
tags and arc labels are mapped into distributed
vectors (embeddings). Specifically, given the
word embedding matrix Ew E Rde×nw, each word
wi is mapped into its corresponding column ewwi E
Rde of Ew according to its index in the dictionary,
where de is the dimensionality of embeddings and
nw is the dictionary size. Likewise, The POS and
arc labels are also mapped into embeddings by the
POS embedding matrix Et E Rde×nt and arc la-
bel embedding matrix El E Rde×nl respectively,
where nt and nl are the numbers of distinct POS
tags and arc labels respectively. Correspondingly,
embeddings of each POS tag ti and each arc label
li are etti E Rde and elli E Rde extracted from Et
and El respectively.
</bodyText>
<table confidence="0.961934642857143">
1880
Configurations Gold Actions
ID Stack Buffer A
0 [ROOT] [He likes story books.] 0
1 [ROOT He] [likes story books .] Shift
2 [ROOT He likes] [story books .] Shift
3 [ROOT likes] [story books .] A U SUB(likes,He) Left Arc(SUB)
4 [ROOT likes story] [books.] Shift
5 [ROOT likes story books] [.] Shift
6 [ROOT likes books] [.] AUNMOD(books,story) Left Arc(NMOD)
7 [ROOT likes] [.] AUOBJ(likes,books) Right Arc(OBJ)
8 [ROOT likes.] 0 Shift
9 [ROOT likes] 0 AUP(likes,.) Right Arc(P)
10 [ROOT] 0 AUROOT(ROOT,likes) Right Arc(ROOT)
</table>
<figureCaption confidence="0.935308">
Figure 2: An example of arc-standard transition dependency parsing.
Figure 3: Architecture of neural network based
transition dependency parsing.
Figure 3 gives the architecture of neu-
</figureCaption>
<bodyText confidence="0.978791571428571">
ral network based parser. Following
Chen and Manning (2014), a set of el-
ements 5 from stack and buffer (e.g.
5 = {s2.lc2.rc1, s2.lc1, s1, b2, s2.rc2.rc1, ... })
is chosen as input. Specifically, the information
(word, POS or label) of each element in the set 5
(e.g. {s2.lc2.rc1.t, s2.lc1.l, s1.w, s1.t, b2.w, ... })
are extracted and mapped into their corresponding
embeddings. Then these embeddings are concate-
nated as the input vector x E Rˆd. A special token
NULL is used to represent a non-existent element.
We perform a standard neural network using
one hidden layer with dh hidden units followed by
a softmax layer as:
</bodyText>
<equation confidence="0.99998">
h = g(W1x + b1), (1)
p = softmax(W2h + b2), (2)
</equation>
<bodyText confidence="0.882577">
where W1 E Rdhxˆd, b1 E Rdh, W2 E R|T |xdh,
b2 E R|T |.Here, g is a non-linear function which
can be hyperbolic tangent, sigmoid, cube (Chen
and Manning, 2014), etc.
</bodyText>
<sectionHeader confidence="0.979127" genericHeader="method">
3 Recursive Neural Network
</sectionHeader>
<bodyText confidence="0.99734575">
Recursive neural network (RNN) is one of classi-
cal neural networks, which performs the same set
of parameters recursively on a given structure (e.g.
syntactic tree) in topological order (Pollack, 1990;
Socher et al., 2013).
In the simplest case, children nodes are com-
bined into their parent node using a weight matrix
W which is shared across the whole network, fol-
lowed by a non-linear function g(·). Specifically,
given the left child node vector hL E Rd and right
child node vector hR E Rd, their parent node vec-
tor hP E Rd will be formalized as:
</bodyText>
<equation confidence="0.927853">
hP = g (W L hR J) , (3)
</equation>
<bodyText confidence="0.999865">
where W E Rdx2d and g is a non-linear function
as mentioned above.
</bodyText>
<subsectionHeader confidence="0.929414333333333">
4 Architecture of Two Heterogeneous
Gated Recursive Neural Networks for
Transition-based Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.999957384615385">
In this paper, we apply the idea of recursive neu-
ral network (RNN) to dependency parsing task.
RNN needs a pre-defined topological structure.
However, in each configuration during parsing,
just partial dependency relations have been con-
structed, while the remains are still unknown. Be-
sides, the standard RNN can just deal with the bi-
nary tree. Therefore we cannot apply the standard
RNN directly.
Here, we propose two heterogeneous recursive
neural networks: tree structured gated recursive
neural network (Tree-GRNN) and directed acyclic
graph structured gated recursive neural network
</bodyText>
<figure confidence="0.99903996">
Stack
s2.lc1
Hidden units h
Input x
Probability of each action p
s3
s2.lc2
s2.rc2.lc1
s2.lc2.rc1
... ...
s2.rc2.lc2
s2.lc1
s2
... ...
s1
s2.rc2
S
s1
...
...
L
R
s2.rc1
p=softmax(W2h+b2)
h=tanh(W1x+b1)
s2.rc2.rc2
Concatenate
b1
b2
s2.rc2.rc1
b2
s2.rc2.rc1
Sub tree
b3
Buffer
1881
Stack
s3
s2
Softmax
s1
S
...
L
R
b1
b2
DAG-GRNN
b3
Buffer
</figure>
<figureCaption confidence="0.8200735">
Figure 5: Minimal structure of tree structured
gated recursive neural network (Tree-GRNN). The
solid arrow denotes that there is a weight matrix on
the link, while the dashed one denotes none.
</figureCaption>
<figure confidence="0.999165">
vp.lc1 vp.lc1 vp.lc1
l n r
Child node: p.lc1
0 Reset gate Element-wise multiplication operator AVG Average operator
0 0 0 0
AVG Parent node: p AVG
vp.lc 2 vp.lc 2 vp.lc 2
l n r
Child node: p.lc2
vpl vp n vp r
Child node: p.rc2 Child node: p.rc1
vp.rc2 vp.rc2 vp.rc2 vp.rc1 vp.rc1 vp.rc1
l n r l n r
s2.lc1
s2.rc2.lc1
Tree-GRNN
</figure>
<figureCaption confidence="0.996888">
Figure 4: Architecture of our proposed depen-
</figureCaption>
<bodyText confidence="0.988443214285714">
dency parser using two heterogeneous gated recur-
sive neural networks.
(DAG-GRNN). Tree-GRNN is applied to the sub-
trees with partial dependency relations in stack
which have already been constructed according
to the previous transition actions. DAG-GRNN
is employed to model the feature composition of
nodes in stack and buffer which have not been la-
beled their dependency relations yet.
Figure 4 shows the whole architecture of our
model, which integrates two different GRNNs to
predict the action for each parsing configuration.
The detailed descriptions of two GRNNs will be
discussed in the following two subsections.
</bodyText>
<figure confidence="0.988440222222222">
s2.lc2
... ...
s2.rc2
s2.rc1
Sub tree
s2.rc2.lc2
... ...
s2.rc2.rc2
s2.rc2.rc1
</figure>
<bodyText confidence="0.9885181">
crucial features according to the gate state. Fig-
ure 5 shows the minimal structure of Tree-GRNN
model.
In Tree-GRNN, each node p of trees in stack is
composed of three components: state vector of left
children nodes vpl E Rdc, state vector of current
node vpn E Rdn and state vector of right children
nodes vpr E Rdc, where dn and dc indicate the cor-
responding vector dimensionalities. Particularly,
we represent information of node p as a vector
</bodyText>
<figure confidence="0.881506">
⎡ vp ⎤
vp = ⎣ l ⎦ ,(4)
vpn
vp
r
</figure>
<bodyText confidence="0.9985446">
where vp E Rq and q = 2dc+dn. Specifically, vpn
contains the information of current node including
its word form p.w, pos tag p.t and label type p.l
as shown in Eq. 5, and vpl and vpr are initialized
by zero vectors 0 E Rdc, then update as Eq. 6.
</bodyText>
<subsectionHeader confidence="0.706307">
4.1 Tree Structured Gated Recursive Neural
Network
</subsectionHeader>
<bodyText confidence="0.999970866666667">
It is a natural way to merge the information from
children nodes into their parent node recursively
according to the given tree structures in stack. Al-
though the dependency relations have been built,
it is still hard to apply the recursive neural net-
work (as Eq. 3) directly for the uncertain num-
ber of children of each node in stack. By aver-
aging operation on children nodes (Socher et al.,
2014), the parent node cannot well capture the
crucial features from the mixed information of its
children nodes. Here, we propose tree structured
gated recursive neural network (Tree-GRNN) in-
corporating the gate mechanism (Cho et al., 2014;
Chung et al., 2014; Chen et al., 2015a; Chen
et al., 2015b), which can selectively choose the
</bodyText>
<equation confidence="0.963052166666667">
⎛ ⎡ ⎤ ⎞
ew
p.w
vpn = tanh ⎝ ⎣et ⎦ ⎠ , (5)
p.t
el p.l
</equation>
<bodyText confidence="0.999963538461539">
where word embedding ewp.w E Rde, pos embed-
ding et t E Rde and label embedding elp l E Rde
p.are extracted from embedding matrices Ew, Et
and El according to the indices of the correspond-
ing word p.w, pos p.t and label p.l respectively.
Specifically, in the case of unlabeled attachment
parsing, we ignore the last term elp.l in Eq. 5.
Thus, the dimensionality dn of vpn varies. In la-
beled attachment parsing case, we set a special to-
ken NULL to represent label p.l if not available
(e.g. p is the node in stack or buffer).
By given node p and its left children nodes p.lci
and right children nodes p.rci, we update the left
</bodyText>
<page confidence="0.416725">
1882
</page>
<bodyText confidence="0.986687">
children information vpl and right children infor-
mation vpr as
</bodyText>
<equation confidence="0.996486375">
1 �
vl = tanh(Wl
p
NL(p)
i (6)
1 vr = tanh(Wr
p NR (p) op.rci O vp.rci + br),
i
</equation>
<bodyText confidence="0.9996388">
where op.lci and op.rci are the reset gates of the
nodes p.lci and p.rci respectively as shown in Eq.
7. In addition, functions NL(p) and NR(p) re-
sult the numbers of left and right children nodes
of node p respectively. The operator O indicates
element multiplication here. Wl E Rdc×q and
Wr E Rdc×q are weight matrices. bl E Rdc and
br E Rdc are bias terms.
The reset gates op.lci and op.rci can be formal-
ized as
</bodyText>
<equation confidence="0.969556">
r vp.rci 1 (7)
op.rci = Q(Wo L p J + bo),
Vn
</equation>
<bodyText confidence="0.9998634">
where Q indicates the sigmoid function, Wo E
Rq×(q+dn) and bo E Rq.
By the mechanism above, we can summarize
the whole information into the stack recursively
from children nodes to their parent using the
partial-built tree structure. Intuitively, the gate
mechanism can selectively choose the crucial fea-
tures of a child node according to the gate state
which is derived from the current child node and
its parent.
</bodyText>
<subsectionHeader confidence="0.940092">
4.2 Directed Acyclic Graph Structured
Gated Recursive Neural Network
</subsectionHeader>
<bodyText confidence="0.999723666666667">
Previous neural based parsing works feed the ex-
tracted features into a standard neural network
with one hidden layer. Then, the hidden units are
fed into a softmax layer, outputting the probability
vector of available actions. Actually, it cannot well
model the complicated combinations of extracted
features. As for the nodes, whose dependency
relations are still unknown, we propose another
recursive neural network namely directed acyclic
graph structured gated recursive neural network
(DAG-GRNN) to better model the interactions of
features.
Intuitively, the DAG structure models the com-
binations of features by recursively mixing the in-
formation from the bottom layer to the top layer
</bodyText>
<figure confidence="0.548887">
Parent node P
</figure>
<figureCaption confidence="0.706184">
Figure 6: Minimal structure of directed acyclic
</figureCaption>
<bodyText confidence="0.99909908">
graph structured gated recursive neural network
(DAG-GRNN). The solid arrow denotes that there
is a weight matrix on the link, while the dashed
one denotes none.
as shown in Figure 4. The concatenation opera-
tion can be regraded as a mix of features in differ-
ent levels of granularity. Each node in the directed
acyclic graph can be seen as a complicated feature
composition of its governed nodes.
Moreover, we also use the gate mechanism to
better model the feature combinations by introduc-
ing two kinds of gates, namely “reset gate” and
“update gate”. Intuitively, each node in the net-
work seems to preserve all the information of its
governed notes without gates, and the gate mech-
anism similarly plays a role of filter which de-
cides how to selectively exploit the information of
its children nodes, discovering and preserving the
crucial features.
DAG-GRNN structure consists of minimal
structures as shown in Figure 6. Vectors hP, hL,
hR and h Pˆ E Rq denote the value of the parent
node P, left child node L, right child node R and
new activation node Pˆ respectively. The value of
parent node hP E Rq is computed as:
</bodyText>
<equation confidence="0.996994">
hP = z PˆO hPˆ+ zL O hL + zR O hR, (8)
</equation>
<bodyText confidence="0.99815775">
where z Pˆ, zL and zR E Rq are update gates for
new activation node Pˆ, left child node L and right
child node R respectively. Operator O indicates
element-wise multiplication.
</bodyText>
<figure confidence="0.9735785625">
6
6 Reset gate
Child node L Child node R
Element-wise multiplication operator
New activation node P
Φ
Φ
Φ
Φ
Update gate
6
op.lci O vp.lci + bl),
r 1
op.lci = Q(Wo L p Vp.lc.� J + bo),
Vn
1883
</figure>
<bodyText confidence="0.450722">
The update gates z can be formalized as:
</bodyText>
<equation confidence="0.53913025">
⎤ ⎡ ⎤
hPˆ
⎦ a exp(Wz ⎣hL ⎦), (9)
hR
</equation>
<table confidence="0.9083842">
which are constrained by:
⎧ [Z Pˆ]k + [ZL]k + [ZR]k = 1, 1 ≤ k ≤ q, (10)
⎨⎪ [Z Pˆ]k ≥ 0, 1 ≤ k ≤ q,
⎪⎩ [ZL]k ≥ 0, 1 ≤ k ≤ q,
[ZR]k ≥ 0, 1 ≤ k ≤ q,
</table>
<bodyText confidence="0.9946705">
where Wz E R3q×3q is the coefficient of update
gates.
The value of new activation node hPˆ is com-
puted as:
</bodyText>
<equation confidence="0.9944525">
~ h ˆ = tanh(W ˆ rL O hL ) (11)
P P rR O hR ,
</equation>
<bodyText confidence="0.99874575">
where W Pˆ E Rq×2q, rL E Rq, rR E Rq. rL
and rR are the reset gates for left child node L
and right child node R respectively, which can be
formalized as:
</bodyText>
<equation confidence="0.99271">
r = [ rL ] = σ(Wr [ hL ] ), (12)
</equation>
<bodyText confidence="0.999963428571429">
where Wr E R2q×2q is the coefficient of two reset
gates and σ indicates the sigmoid function.
Intuitively, the reset gates r partially read the
information from the left and right children, out-
putting a new activation node hPˆ, while the up-
date gates z selectively choosing the information
among the the new activation node Pˆ, the left child
node L and the right child node R. This gate
mechanism is effective to model the combinations
of features.
Finally, we concatenate all the nodes in the
DAG-GRNN structure as input x of the architec-
ture described in Section 2.2, resulting the proba-
bility vector for all available actions.
</bodyText>
<subsectionHeader confidence="0.93107">
4.3 Inference
</subsectionHeader>
<bodyText confidence="0.999969538461538">
We use greedy decoding in parsing. At each step,
we apply our two GRNNs on the current config-
uration to extract the features. After softmax op-
eration, we choose the feasible transition with the
highest possibility, and perform the chosen tran-
sition on the current configuration to get the next
configuration state.
In practice, we do not need calculate the Tree-
GRNN over the all trees in the stack on the current
configuration. Instead, we preserve the represen-
tations of trees in the stack. When we need apply a
new transition on the configuration, we update the
relative representations using Tree-GRNN.
</bodyText>
<sectionHeader confidence="0.989026" genericHeader="method">
5 Training
</sectionHeader>
<bodyText confidence="0.999950833333333">
We use the maximum likelihood (ML) criterion to
train our model. By extracting training set (xi, yi)
from gold parse trees using a shortest stack oracle
which always prefers Left-Arc(l) or Right-Arc(l)
action over Shift, the goal of our model is to mini-
mize the loss function with the parameter set θ:
</bodyText>
<equation confidence="0.989773">
log p(yi|xi; θ)+ 2m11θ112
λ 2, (13)
</equation>
<bodyText confidence="0.99990275">
where m is number of extracted training examples
which is as same as the number of all configura-
tions.
Following Socher et al. (2013), we use the diag-
onal variant of AdaGrad (Duchi et al., 2011) with
minibatch strategy to minimize the objective. We
also employ dropout strategy to avoid overfitting.
In practice, we perform DAG-GRNN with
two hidden layers, which gets the best perfor-
mance. We use the approximated gradient for
Tree-GRNN, which only performs gradient back
propagation on the first two layers.
</bodyText>
<sectionHeader confidence="0.998138" genericHeader="method">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.940679">
6.1 Datasets
</subsectionHeader>
<bodyText confidence="0.99232525">
To evaluate our proposed model, we experiment
on two prevalent datasets: English Penn Treebank
3 (PTB3) and Chinese Penn Treebank 5 (CTB5)
datasets.
</bodyText>
<listItem confidence="0.994693071428571">
• English For English Penn Treebank 3
(PTB3) dataset, we use sections 2-21 for
training, section 22 and section 23 as de-
velopment set and test set respectively. We
adopt CoNLL Syntactic Dependencies (CD)
(Johansson and Nugues, 2007) using the
LTH Constituent-to-Dependency Conversion
Tool.
• Chinese For Chinese Penn Treebank 5
(CTB5) dataset, we follow the same split as
described in (Zhang and Clark, 2008). De-
pendencies are converted by the Penn2Malt
tool with the head-finding rules of (Zhang
and Clark, 2008).
</listItem>
<equation confidence="0.738971266666667">
⎡
z = ⎣
z Pˆ
zL
zR
1
J(θ) = � m
Xm
i=1
1884
Embedding size de = 50
Dimensionality of child node vector dc = 50
Initial learning rate α = 0.05
Regularization A = 10−8
Dropout rate p = 20%
</equation>
<tableCaption confidence="0.991118">
Table 1: Hyper-parameter settings.
</tableCaption>
<subsectionHeader confidence="0.996659">
6.2 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.9999856">
For parameter initialization, we use random ini-
tialization within (-0.01, 0.01) for all parameters
except the word embedding matrix E&apos;. Specifi-
cally, we adopt pre-trained English word embed-
dings from (Collobert et al., 2011). And we pre-
train the Chinese word embeddings on a huge un-
labeled data, the Chinese Wikipedia corpus, with
word2vec toolkit (Mikolov et al., 2013a).
Table 1 gives the details of hyper-parameter set-
tings of our approach. In addition, we set mini-
batch size to 20. In all experiments, we only take
s1, s2, s3 nodes in stack and b1, b2, b3 nodes in
buffer into account. We also apply dropout strat-
egy here, and only dropout at the nodes in stack
and buffer with probability p = 20%.
</bodyText>
<subsectionHeader confidence="0.75624">
6.3 Results
</subsectionHeader>
<bodyText confidence="0.999977115384616">
The experiment results on PTB3 and CTB5
datasets are list in Table 2 and 3 respectively. On
all datasets, we report unlabeled attachment scores
(UAS) and labeled attachment scores (LAS). Con-
ventionally, punctuations are excluded in all eval-
uation metrics.
To evaluate the effectiveness of our approach,
we compare our parsers with feature-based parser
and neural-based parser. For feature-based parser,
we compare our models with two prevalent
parsers: MaltParser (Nivre et al., 2006) and
MSTParser (McDonald and Pereira, 2006). For
neural-based parser, we compare our results with
parser of Chen and Manning (2014). Compared
with parser of Chen and Manning (2014), our
parser with two heterogeneous gated recursive
neural networks (Tree-GRNN+DAG-GRNN) re-
ceives 0.6% (UAS) and 0.9% (LAS) improvement
on PTB3 test set, and receives 0.8% (UAS) and
1.3% (LAS) improvement on CTB5 test set.
Since that speed of algorithm is not the focus of
our paper, we do not optimize the speed a lot. On
CTB (UAS), it takes about 2 days to train Tree-
GRNN+DAG-GRNN model with CPU only. The
testing speed is about 2.7 sentences per second.
All implementation is based on Python.
</bodyText>
<subsectionHeader confidence="0.99827">
6.4 Effects of Gate Mechanisms
</subsectionHeader>
<bodyText confidence="0.99879762">
We adopt five different models: plain
parser, Tree-RNN parser, Tree-GRNN parser,
Tree-RNN+DAG-GRNN parser, and Tree-
GRNN+DAG-GRNN parser. The experiment
results show the effectiveness of our proposed two
heterogeneous gated recursive neural networks.
Specifically, plain parser is as same as parser
of Chen and Manning (2014). The difference
between them is that plain parser only takes the
nodes in stack and buffer into account, which
uses a simpler feature template than parser of
Chen and Manning (2014). As plain parser
omits all children nodes of trees in stack, it
performs poorly compared with parser of Chen
and Manning (2014). In addition, we find
plain parser outperforms MaltParser (standard) on
PTB3 dataset making about 1% progress, while
it performs poorer than MaltParser (standard) on
CTB5 dataset. It shows that the children nodes
of trees in stack is of great importance, especially
for Chinese. Moreover, it also shows the effective-
ness of neural network based model which could
represent complicated features as compacted em-
beddings. Tree-RNN parser additionally exploits
all the children nodes of trees in stack, which is
a simplified version of Tree-GRNN without incor-
porating the gate mechanism described in Section
4.1. In anther word, Tree-RNN omits the gate
terms op-li and op-&amp;quot;i in Eq. 6. As we can see,
the results are significantly boosted by utilizing
the all information in stack, which again shows
the importance of children nodes of trees in stack.
Although the results of Tree-RNN are compara-
ble to results of Chen and Manning (2014), it not
outperforms parser of Chen and Manning (2014)
in all cases (e.g. UAS on CTB5), which implies
that exploiting all information without selection
might lead to incorporate noise features. More-
over, Tree-GRNN parser further boosts the perfor-
mance by incorporating the gate mechanism. In-
tuitively, Tree-RNN who exploits all the informa-
tion of stack without selection cannot well capture
the crucial features, while Tree-GRNN with gate
mechanism could selectively choose and preserve
the effective features by adapting the current gate
state.
We also experiment on parsers using two
heterogeneous gated recursive neural networks:
Tree-RNN+DAG-GRNN parser and Tree-
GRNN+DAG-GRNN parser. The similarity of
</bodyText>
<figure confidence="0.975817916666667">
1885
UAS(%)
2 4 6 8 10
0.92
0.88
0.86
0.9
Plain
Tree-RNN
Tree-GRNN
Tree-RNN+DAG-GRNN
Tree-GRNN+DAG-GRNN
</figure>
<table confidence="0.992008545454546">
models Dev Test
UAS LAS UAS LAS
Malt:standard 90.0 88.8 89.9 88.5
Malt:eager 90.1 88.9 90.1 88.7
MSTParser 92.1 90.8 92.0 90.5
Chen’s Parser 92.2 91.0 92.0 90.7
Plain 91.1 90.0 91.2 89.7
Tree-RNN 92.4 91.0 92.1 90.8
Tree-GRNN 92.6 91.1 92.4 91.0
Tree-RNN+DAG-GRNN 92.8 91.9 92.4 91.5
Tree-GRNN+DAG-GRNN 92.6 91.9 92.6 91.6
</table>
<tableCaption confidence="0.895874666666667">
Table 2: Performance of different models on PTB3
dataset. UAS: unlabeled attachment score. LAS:
labeled attachment score.
</tableCaption>
<table confidence="0.990775333333333">
epoches
models Dev Test
UAS LAS UAS LAS
Malt:standard 82.4 80.5 82.4 80.6
Malt:eager 91.2 79.3 80.2 78.4
MSTParser 84.0 82.1 83.0 81.2
Chen’s Parser 84.0 82.4 83.9 82.4
Plain 81.6 79.3 81.1 78.8
Tree-RNN 83.5 82.5 83.8 82.7
Tree-GRNN 84.2 82.5 84.3 83.1
Tree-RNN+DAG-GRNN 84.5 83.3 84.5 83.1
Tree-GRNN+DAG-GRNN 84.6 83.6 84.7 83.7
</table>
<tableCaption confidence="0.953401666666667">
Table 3: Performance of different models on
CTB5 dataset. UAS: unlabeled attachment score.
LAS: labeled attachment score.
</tableCaption>
<bodyText confidence="0.999010833333333">
two parsers is that they all employ the DAG
structured recursive neural network with gate
mechanism to model the combination of features
extracted from stack and buffer. The difference
between them is the former one employs the
Tree-RNN without gate mechanism to model the
features of stack, while the later one employs the
gated version (Tree-GRNN). Again, the perfor-
mance of these two parsers is further boosted,
which shows DAG-GRNN can well model the
combinations of features which is summarized by
Tree-(G)RNN structure. In addition, we find the
performance does not drop a lot in almost cases by
turning off the gate mechanism of Tree-GRNN,
which implies that the DAG-GRNN can help
selecting the information from trees in stack, even
it has not been selected by gate mechanism of
Tree-GRNN yet.
</bodyText>
<subsectionHeader confidence="0.998937">
6.5 Convergency Speed
</subsectionHeader>
<bodyText confidence="0.9997168">
To further analyze the convergency speed of our
approach, we compare the UAS results on devel-
opment sets of two datasets for first ten epoches
as shown in Figure 7 and 8. As plain parser
only take the nodes in stack and buffer into ac-
</bodyText>
<figureCaption confidence="0.916862">
Figure 7: Performance of different models on
PTB3 development set. UAS: unlabeled attach-
ment score.
</figureCaption>
<figure confidence="0.589831">
epoches
</figure>
<figureCaption confidence="0.736096">
Figure 8: Performance of different models on
CTB5 development set. UAS: unlabeled attach-
ment score.
</figureCaption>
<bodyText confidence="0.9999395">
count, the performance is much poorer than the
rest parsers. Moreover, Tree-GRNN converges
slower than Tree-RNN, which shows that it might
be more difficult to learn this gate mechanism. By
introducing the DAG-GRNN, both Tree-RNN and
Tree-GRNN parsers become faster to converge,
which shows that the DAG-GRNN is of great help
in boosting the convergency speed.
</bodyText>
<sectionHeader confidence="0.999051" genericHeader="method">
7 Related Work
</sectionHeader>
<bodyText confidence="0.9974282">
Many neural network based methods have been
used for transition based dependency parsing.
Chen et al. (2014) and Bansal et al. (2014) used
the dense vectors (embeddings) to represent words
or features and found these representations are
</bodyText>
<figure confidence="0.976076857142857">
2 4 6 8 10
UAS(%)
0.84
0.82
0.78
0.76
0.74
0.8
Plain
Tree-RNN
Tree-GRNN
Tree-RNN+DAG-GRNN
Tree-GRNN+DAG-GRNN
1886
</figure>
<bodyText confidence="0.999951454545455">
complementary to the traditional discrete feature
representation. However, these two methods only
focus on the dense representations (embeddings)
of words or features.
Stenetorp (2013) first used RNN for transition
based dependency parsing. He followed the stan-
dard RNN and used the binary combination to
model the representation of two linked words. But
his model does not achieve the performance of the
traditional method.
Le and Zuidema (2014) proposed a genera-
tive re-ranking model with Inside-Outside Recur-
sive Neural Network (IORNN), which can pro-
cess trees both bottom-up and top-down. How-
ever, IORNN works in generative way and just es-
timates the probability of a given tree, so IORNN
cannot fully utilize the incorrect trees in k-best
candidate results. Besides, IORNN treats depen-
dency tree as a sequence, which can be regarded
as a generalization of simple recurrent neural net-
work (SRNN) (Elman, 1990).
Although the two methods also used RNN, they
just deal with the binary combination, which is un-
natural for dependency tree.
Zhu et al. (2015) proposed a recursive convolu-
tional neural network (RCNN) architecture to cap-
ture syntactic and compositional-semantic repre-
sentations of phrases and words in a dependency
tree. Different with the original recursive neu-
ral network, they introduced the convolution and
pooling layers, which can model a variety of com-
positions by the feature maps and choose the most
informative compositions by the pooling layers.
Chen and Manning (2014) improved the
transition-based dependency parsing by represent-
ing all words, POS tags and arc labels as dense
vectors, and modeled their interactions with neu-
ral network to make predictions of actions. Their
method only relies on dense features, and is not
able to automatically learn the most useful feature
conjunctions to predict the transition action.
Compared with (Chen and Manning, 2014), our
method can fully exploit the information of all the
descendants of a node in stack with Tree-GRNN.
Then DAG-GRNN automatically learns the com-
plicated combination of all the features, while the
traditional discrete feature based methods need
manually design them.
Dyer et al. (2015) improved the transition-based
dependency parsing using stack long short term
memory neural network and received significant
improvement on performance. They focused on
exploiting the long distance dependencies and in-
formation, while we aims to automatically model
the complicated feature combination.
</bodyText>
<sectionHeader confidence="0.994653" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999988">
In this paper, we pay attention to the syntac-
tic and semantic composition of the dense fea-
tures for transition-based dependency parsing. We
propose two heterogeneous gated recursive neu-
ral networks, Tree-GRNN and DAG-GRNN. Each
hidden neuron in two proposed GRNNs can be re-
garded as a different combination of the input fea-
tures. Thus, the whole model has an ability to sim-
ulate the design of the sophisticated feature com-
binations in the traditional discrete feature based
methods.
Although the two proposed GRNNs are only
used for the greedy parsing based on arc-standard
transition system in this paper, it is easy to gen-
eralize them to other transition systems and graph
based parsing. In future work, we would also like
to extend our GRNNs for the other NLP tasks.
</bodyText>
<sectionHeader confidence="0.995469" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999940625">
We would like to thank the anonymous review-
ers for their valuable comments. This work was
partially funded by the National Natural Science
Foundation of China (61472088, 61473092), Na-
tional High Technology Research and Develop-
ment Program of China (2015AA015408), Shang-
hai Science and Technology Development Funds
(14ZR1403200).
</bodyText>
<sectionHeader confidence="0.983149" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996553771929825">
Miguel Ballesteros and Bernd Bohnet. 2014. Au-
tomatic feature selection for agenda-based depen-
dency parsing. In Proceedings of the 25th Interna-
tional Conference on Computational Linguistics.
Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014. Tailoring continuous word representations for
dependency parsing. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. The Journal of Machine Learning Re-
search, 3:1137–1155.
Bernd Bohnet. 2010. Very high accuracy and fast de-
pendency parsing is not a contradiction. In Proceed-
ings of the 23rd International Conference on Com-
1887
putational Linguistics, pages 89–97. Association for
Computational Linguistics.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 740–750.
Wenliang Chen, Yue Zhang, and Min Zhang. 2014.
Feature embedding for dependency parsing. In Pro-
ceedings of COLING 2014, the 25th International
Conference on Computational Linguistics: Techni-
cal Papers, pages 816–826, Dublin, Ireland, August.
Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing
Huang. 2015a. Gated recursive neural network for
Chinese word segmentation. In Proceedings of An-
nual Meeting of the Association for Computational
Linguistics.
Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Shiyu Wu, and
Xuanjing Huang. 2015b. Sentence modeling with
gated recursive neural network. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using rnn encoder-decoder for statistical machine
translation. In Proceedings of EMNLP.
Jinho D Choi and Andrew McCallum. 2013.
Transition-based dependency parsing with selec-
tional branching. In ACL (1), pages 1052–1062.
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. arXiv preprint arXiv:1412.3555.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas
Lamar, Richard Schwartz, and John Makhoul. 2014.
Fast and robust neural network joint models for sta-
tistical machine translation. In 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, Baltimore, MD, USA, June.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Chris Dyer, Miguel Ballesteros, Wang Ling, Austin
Matthews, and Noah A Smith. 2015. Transition-
based dependency parsing with stack long short-
term memory. arXiv preprint arXiv:1505.08075.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.
Yoav Goldberg and Joakim Nivre. 2013. Training
deterministic parsers with non-deterministic oracles.
TACL, 1:403–414.
He He, Hal Daum´e III, and Jason Eisner. 2013. Dy-
namic feature selection for dependency parsing. In
EMNLP, pages 1455–1464.
Liang Huang and Kenji Sagae. 2010. Dynamic pro-
gramming for linear-time incremental parsing. In
Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1077–
1086.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In 16th Nordic Conference of Computational
Linguistics, pages 105–112. University of Tartu.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL.
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency parsing. Synthesis Lectures on
Human Language Technologies, 1(1):1–127.
Phong Le and Willem Zuidema. 2014. The inside-
outside recursive neural network model for depen-
dency parsing. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 729–739, Doha, Qatar,
October. Association for Computational Linguistics.
Andr´e FT Martins, Noah A Smith, and Eric P Xing.
2009. Concise integer linear programming formula-
tions for dependency parsing. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 1-Volume 1, pages 342–350. Association for
Computational Linguistics.
Ryan T McDonald and Fernando CN Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In EACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’05, pages 91–98.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In NIPS, pages 3111–3119.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of LREC, vol-
ume 6, pages 2216–2219.
1888
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50–57. Association
for Computational Linguistics.
Jordan B Pollack. 1990. Recursive distributed repre-
sentations. Artificial Intelligence, 46(1):77–105.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013. Parsing with composi-
tional vector grammars. In In Proceedings of the
ACL conference. Citeseer.
Richard Socher, Andrej Karpathy, Quoc V Le, Christo-
pher D Manning, and Andrew Y Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions
of the Association for Computational Linguistics,
2:207–218.
Pontus Stenetorp. 2013. Transition-based dependency
parsing using recursive neural networks. In NIPS
Workshop on Deep Learning.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for compu-
tational linguistics, pages 384–394. Association for
Computational Linguistics.
Yue Zhang and Stephen Clark. 2008. A tale of
two parsers: investigating and combining graph-
based and transition-based dependency parsing us-
ing beam-search. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 562–571.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
188–193.
Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and Xuanjing
Huang. 2015. A re-ranking model for dependency
parser with recursive convolutional neural network.
In Proceedings ofAnnual Meeting of the Association
for Computational Linguistics.
</reference>
<page confidence="0.858561">
1889
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.741506">
<title confidence="0.9996805">Transition-based Dependency Parsing Using Two Heterogeneous Gated Recursive Neural Networks</title>
<author confidence="0.8907915">Xinchi Chen</author>
<author confidence="0.8907915">Yaqian Zhou</author>
<author confidence="0.8907915">Chenxi Zhu</author>
<author confidence="0.8907915">Xipeng Qiu</author>
<author confidence="0.8907915">Xuanjing Shanghai Key Laboratory of Intelligent Information Processing</author>
<author confidence="0.8907915">Fudan</author>
<affiliation confidence="0.996625">School of Computer Science, Fudan</affiliation>
<address confidence="0.995503">825 Zhangheng Road, Shanghai,</address>
<abstract confidence="0.998385103448276">Recently, neural network based dependency parsing has attracted much interest, which can effectively alleviate the problems of data sparsity and feature engineering by using the dense features. However, it is still a challenge problem to sufficiently model the complicated syntactic and semantic compositions of the dense features in neural network based methods. In this paper, we propose two heterogeneous gated recursive neural networks: tree structured gated recursive neural network (Tree-GRNN) and directed acyclic graph structured gated recursive neural network (DAG-GRNN). Then we integrate them to automatically learn the compositions of the dense features for transition-based dependency parsing. Specifically, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Miguel Ballesteros</author>
<author>Bernd Bohnet</author>
</authors>
<title>Automatic feature selection for agenda-based dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="2296" citStr="Ballesteros and Bohnet, 2014" startWordPosition="333" endWordPosition="336">ethods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted p</context>
</contexts>
<marker>Ballesteros, Bohnet, 2014</marker>
<rawString>Miguel Ballesteros and Bernd Bohnet. 2014. Automatic feature selection for agenda-based dependency parsing. In Proceedings of the 25th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Bansal</author>
<author>Kevin Gimpel</author>
<author>Karen Livescu</author>
</authors>
<title>Tailoring continuous word representations for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2966" citStr="Bansal et al., 2014" startWordPosition="441" endWordPosition="444">ures and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes. Besides, the features of the selected nodes are just simply concatenated and then fed into neural network. Since the concatenation operation is relatively simple, it is difficult to model the com1879 Proceedings of the 2015 Conference on Empirical Methods in N</context>
<context position="28814" citStr="Bansal et al. (2014)" startWordPosition="4844" endWordPosition="4847">e. epoches Figure 8: Performance of different models on CTB5 development set. UAS: unlabeled attachment score. count, the performance is much poorer than the rest parsers. Moreover, Tree-GRNN converges slower than Tree-RNN, which shows that it might be more difficult to learn this gate mechanism. By introducing the DAG-GRNN, both Tree-RNN and Tree-GRNN parsers become faster to converge, which shows that the DAG-GRNN is of great help in boosting the convergency speed. 7 Related Work Many neural network based methods have been used for transition based dependency parsing. Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are 2 4 6 8 10 UAS(%) 0.84 0.82 0.78 0.76 0.74 0.8 Plain Tree-RNN Tree-GRNN Tree-RNN+DAG-GRNN Tree-GRNN+DAG-GRNN 1886 complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. Stenetorp (2013) first used RNN for transition based dependency parsing. He followed the standard RNN and used the binary combination to model the representation of two linked words. But his model does not achieve </context>
</contexts>
<marker>Bansal, Gimpel, Livescu, 2014</marker>
<rawString>Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2014. Tailoring continuous word representations for dependency parsing. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2695" citStr="Bengio et al., 2003" startWordPosition="398" endWordPosition="401">dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable information hidden in the uncho</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A neural probabilistic language model. The Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Very high accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Com1887</booktitle>
<contexts>
<context position="1750" citStr="Bohnet, 2010" startWordPosition="242" endWordPosition="243">ombinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model. 1 Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete feature</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Very high accuracy and fast dependency parsing is not a contradiction. In Proceedings of the 23rd International Conference on Com1887</rawString>
</citation>
<citation valid="false">
<authors>
<author>putational Linguistics</author>
</authors>
<pages>89--97</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Linguistics, </marker>
<rawString>putational Linguistics, pages 89–97. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>740--750</pages>
<contexts>
<context position="2446" citStr="Chen and Manning, 2014" startWordPosition="356" endWordPosition="359">onald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neura</context>
<context position="5288" citStr="Chen and Manning (2014)" startWordPosition="791" endWordPosition="794">r which have not been labeled their dependency relations yet. Intuitively, Tree-GRNN selects and merges features recursively from children nodes into their parent according to their dependency structures, while DAG-GRNN further models the complicated combinations of extracted features and explicitly exploits features in different levels of granularity. To evaluate our approach, we experiment on two prevalent benchmark datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. Experiment results show the effectiveness of our proposed method. Compared to the parser of Chen and Manning (2014), we receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set. 2 Neural Network Based Transition Dependency Parsing 2.1 Transition Dependency Parsing In this paper, we employ the arc-standard transition systems (Nivre, 2004) and examine only greedy parsing for its efficiency. Figure 2 gives an example of arc-standard transition dependency parsing. In transition-based dependency parsing, the consecutive configurations of parsing process can be defined as c(i) = (s(i), b(i), A(i)) which consists of a stack s, a buffer</context>
<context position="8941" citStr="Chen and Manning (2014)" startWordPosition="1424" endWordPosition="1427">y books .] Shift 3 [ROOT likes] [story books .] A U SUB(likes,He) Left Arc(SUB) 4 [ROOT likes story] [books.] Shift 5 [ROOT likes story books] [.] Shift 6 [ROOT likes books] [.] AUNMOD(books,story) Left Arc(NMOD) 7 [ROOT likes] [.] AUOBJ(likes,books) Right Arc(OBJ) 8 [ROOT likes.] 0 Shift 9 [ROOT likes] 0 AUP(likes,.) Right Arc(P) 10 [ROOT] 0 AUROOT(ROOT,likes) Right Arc(ROOT) Figure 2: An example of arc-standard transition dependency parsing. Figure 3: Architecture of neural network based transition dependency parsing. Figure 3 gives the architecture of neural network based parser. Following Chen and Manning (2014), a set of elements 5 from stack and buffer (e.g. 5 = {s2.lc2.rc1, s2.lc1, s1, b2, s2.rc2.rc1, ... }) is chosen as input. Specifically, the information (word, POS or label) of each element in the set 5 (e.g. {s2.lc2.rc1.t, s2.lc1.l, s1.w, s1.t, b2.w, ... }) are extracted and mapped into their corresponding embeddings. Then these embeddings are concatenated as the input vector x E Rˆd. A special token NULL is used to represent a non-existent element. We perform a standard neural network using one hidden layer with dh hidden units followed by a softmax layer as: h = g(W1x + b1), (1) p = softmax(</context>
<context position="23189" citStr="Chen and Manning (2014)" startWordPosition="3947" endWordPosition="3950">Results The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS). Conventionally, punctuations are excluded in all evaluation metrics. To evaluate the effectiveness of our approach, we compare our parsers with feature-based parser and neural-based parser. For feature-based parser, we compare our models with two prevalent parsers: MaltParser (Nivre et al., 2006) and MSTParser (McDonald and Pereira, 2006). For neural-based parser, we compare our results with parser of Chen and Manning (2014). Compared with parser of Chen and Manning (2014), our parser with two heterogeneous gated recursive neural networks (Tree-GRNN+DAG-GRNN) receives 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, and receives 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set. Since that speed of algorithm is not the focus of our paper, we do not optimize the speed a lot. On CTB (UAS), it takes about 2 days to train TreeGRNN+DAG-GRNN model with CPU only. The testing speed is about 2.7 sentences per second. All implementation is based on Python. 6.4 Effects of Gate Mechanisms We adopt five different </context>
<context position="25332" citStr="Chen and Manning (2014)" startWordPosition="4293" endWordPosition="4296">fectiveness of neural network based model which could represent complicated features as compacted embeddings. Tree-RNN parser additionally exploits all the children nodes of trees in stack, which is a simplified version of Tree-GRNN without incorporating the gate mechanism described in Section 4.1. In anther word, Tree-RNN omits the gate terms op-li and op-&amp;quot;i in Eq. 6. As we can see, the results are significantly boosted by utilizing the all information in stack, which again shows the importance of children nodes of trees in stack. Although the results of Tree-RNN are comparable to results of Chen and Manning (2014), it not outperforms parser of Chen and Manning (2014) in all cases (e.g. UAS on CTB5), which implies that exploiting all information without selection might lead to incorporate noise features. Moreover, Tree-GRNN parser further boosts the performance by incorporating the gate mechanism. Intuitively, Tree-RNN who exploits all the information of stack without selection cannot well capture the crucial features, while Tree-GRNN with gate mechanism could selectively choose and preserve the effective features by adapting the current gate state. We also experiment on parsers using two heterogeneous </context>
<context position="30525" citStr="Chen and Manning (2014)" startWordPosition="5111" endWordPosition="5114">current neural network (SRNN) (Elman, 1990). Although the two methods also used RNN, they just deal with the binary combination, which is unnatural for dependency tree. Zhu et al. (2015) proposed a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, they introduced the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Chen and Manning (2014) improved the transition-based dependency parsing by representing all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural network to make predictions of actions. Their method only relies on dense features, and is not able to automatically learn the most useful feature conjunctions to predict the transition action. Compared with (Chen and Manning, 2014), our method can fully exploit the information of all the descendants of a node in stack with Tree-GRNN. Then DAG-GRNN automatically learns the complicated combination of all the features, while the traditi</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Yue Zhang</author>
<author>Min Zhang</author>
</authors>
<title>Feature embedding for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>816--826</pages>
<location>Dublin, Ireland,</location>
<contexts>
<context position="28789" citStr="Chen et al. (2014)" startWordPosition="4839" endWordPosition="4842">labeled attachment score. epoches Figure 8: Performance of different models on CTB5 development set. UAS: unlabeled attachment score. count, the performance is much poorer than the rest parsers. Moreover, Tree-GRNN converges slower than Tree-RNN, which shows that it might be more difficult to learn this gate mechanism. By introducing the DAG-GRNN, both Tree-RNN and Tree-GRNN parsers become faster to converge, which shows that the DAG-GRNN is of great help in boosting the convergency speed. 7 Related Work Many neural network based methods have been used for transition based dependency parsing. Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are 2 4 6 8 10 UAS(%) 0.84 0.82 0.78 0.76 0.74 0.8 Plain Tree-RNN Tree-GRNN Tree-RNN+DAG-GRNN Tree-GRNN+DAG-GRNN 1886 complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. Stenetorp (2013) first used RNN for transition based dependency parsing. He followed the standard RNN and used the binary combination to model the representation of two linked words. But hi</context>
</contexts>
<marker>Chen, Zhang, Zhang, 2014</marker>
<rawString>Wenliang Chen, Yue Zhang, and Min Zhang. 2014. Feature embedding for dependency parsing. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 816–826, Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinchi Chen</author>
<author>Xipeng Qiu</author>
<author>Chenxi Zhu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Gated recursive neural network for Chinese word segmentation.</title>
<date>2015</date>
<booktitle>In Proceedings of Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14136" citStr="Chen et al., 2015" startWordPosition="2322" endWordPosition="2325">dren nodes into their parent node recursively according to the given tree structures in stack. Although the dependency relations have been built, it is still hard to apply the recursive neural network (as Eq. 3) directly for the uncertain number of children of each node in stack. By averaging operation on children nodes (Socher et al., 2014), the parent node cannot well capture the crucial features from the mixed information of its children nodes. Here, we propose tree structured gated recursive neural network (Tree-GRNN) incorporating the gate mechanism (Cho et al., 2014; Chung et al., 2014; Chen et al., 2015a; Chen et al., 2015b), which can selectively choose the ⎛ ⎡ ⎤ ⎞ ew p.w vpn = tanh ⎝ ⎣et ⎦ ⎠ , (5) p.t el p.l where word embedding ewp.w E Rde, pos embedding et t E Rde and label embedding elp l E Rde p.are extracted from embedding matrices Ew, Et and El according to the indices of the corresponding word p.w, pos p.t and label p.l respectively. Specifically, in the case of unlabeled attachment parsing, we ignore the last term elp.l in Eq. 5. Thus, the dimensionality dn of vpn varies. In labeled attachment parsing case, we set a special token NULL to represent label p.l if not available (e.g. p</context>
</contexts>
<marker>Chen, Qiu, Zhu, Huang, 2015</marker>
<rawString>Xinchi Chen, Xipeng Qiu, Chenxi Zhu, and Xuanjing Huang. 2015a. Gated recursive neural network for Chinese word segmentation. In Proceedings of Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinchi Chen</author>
<author>Xipeng Qiu</author>
<author>Chenxi Zhu</author>
<author>Shiyu Wu</author>
<author>Xuanjing Huang</author>
</authors>
<title>Sentence modeling with gated recursive neural network.</title>
<date>2015</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="14136" citStr="Chen et al., 2015" startWordPosition="2322" endWordPosition="2325">dren nodes into their parent node recursively according to the given tree structures in stack. Although the dependency relations have been built, it is still hard to apply the recursive neural network (as Eq. 3) directly for the uncertain number of children of each node in stack. By averaging operation on children nodes (Socher et al., 2014), the parent node cannot well capture the crucial features from the mixed information of its children nodes. Here, we propose tree structured gated recursive neural network (Tree-GRNN) incorporating the gate mechanism (Cho et al., 2014; Chung et al., 2014; Chen et al., 2015a; Chen et al., 2015b), which can selectively choose the ⎛ ⎡ ⎤ ⎞ ew p.w vpn = tanh ⎝ ⎣et ⎦ ⎠ , (5) p.t el p.l where word embedding ewp.w E Rde, pos embedding et t E Rde and label embedding elp l E Rde p.are extracted from embedding matrices Ew, Et and El according to the indices of the corresponding word p.w, pos p.t and label p.l respectively. Specifically, in the case of unlabeled attachment parsing, we ignore the last term elp.l in Eq. 5. Thus, the dimensionality dn of vpn varies. In labeled attachment parsing case, we set a special token NULL to represent label p.l if not available (e.g. p</context>
</contexts>
<marker>Chen, Qiu, Zhu, Wu, Huang, 2015</marker>
<rawString>Xinchi Chen, Xipeng Qiu, Chenxi Zhu, Shiyu Wu, and Xuanjing Huang. 2015b. Sentence modeling with gated recursive neural network. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Andrew McCallum</author>
</authors>
<title>Transition-based dependency parsing with selectional branching.</title>
<date>2013</date>
<booktitle>In ACL (1),</booktitle>
<pages>1052--1062</pages>
<contexts>
<context position="2265" citStr="Choi and McCallum, 2013" startWordPosition="329" endWordPosition="332">tive dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in featur</context>
</contexts>
<marker>Choi, McCallum, 2013</marker>
<rawString>Jinho D Choi and Andrew McCallum. 2013. Transition-based dependency parsing with selectional branching. In ACL (1), pages 1052–1062.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyoung Chung</author>
<author>Caglar Gulcehre</author>
<author>KyungHyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.</title>
<date>2014</date>
<contexts>
<context position="4180" citStr="Chung et al., 2014" startWordPosition="622" endWordPosition="625"> in Natural Language Processing, pages 1879–1889, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. plicated feature combinations which can be manually designed in the traditional discrete feature based methods. To tackle these problems, we use two heterogeneous gated recursive neural networks, tree structured gated recursive neural network (TreeGRNN) and directed acyclic graph gated structured recursive neural network (DAG-GRNN), to model each configuration during transition based dependency parsing. The two proposed GRNNs introduce the gate mechanism (Chung et al., 2014) to improve the standard recursive neural network (RNN) (Socher et al., 2013; Socher et al., 2014), and can model the syntactic and semantic compositions of the nodes during parsing. Figure 1 gives a rough sketch for the standard RNN, Tree-GRNN and DAG-GRNN. Tree-GRNN is applied to the partial-constructed trees in stack, which have already been constructed according to the previous transition actions. DAG-GRNN is applied to model the feature composition of nodes in stack and buffer which have not been labeled their dependency relations yet. Intuitively, Tree-GRNN selects and merges features re</context>
<context position="14117" citStr="Chung et al., 2014" startWordPosition="2318" endWordPosition="2321">nformation from children nodes into their parent node recursively according to the given tree structures in stack. Although the dependency relations have been built, it is still hard to apply the recursive neural network (as Eq. 3) directly for the uncertain number of children of each node in stack. By averaging operation on children nodes (Socher et al., 2014), the parent node cannot well capture the crucial features from the mixed information of its children nodes. Here, we propose tree structured gated recursive neural network (Tree-GRNN) incorporating the gate mechanism (Cho et al., 2014; Chung et al., 2014; Chen et al., 2015a; Chen et al., 2015b), which can selectively choose the ⎛ ⎡ ⎤ ⎞ ew p.w vpn = tanh ⎝ ⎣et ⎦ ⎠ , (5) p.t el p.l where word embedding ewp.w E Rde, pos embedding et t E Rde and label embedding elp l E Rde p.are extracted from embedding matrices Ew, Et and El according to the indices of the corresponding word p.w, pos p.t and label p.l respectively. Specifically, in the case of unlabeled attachment parsing, we ignore the last term elp.l in Eq. 5. Thus, the dimensionality dn of vpn varies. In labeled attachment parsing case, we set a special token NULL to represent label p.l if no</context>
</contexts>
<marker>Chung, Gulcehre, Cho, Bengio, 2014</marker>
<rawString>Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="2587" citStr="Collobert et al., 2011" startWordPosition="377" endWordPosition="380"> of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/</context>
<context position="22083" citStr="Collobert et al., 2011" startWordPosition="3763" endWordPosition="3766">plit as described in (Zhang and Clark, 2008). Dependencies are converted by the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008). ⎡ z = ⎣ z Pˆ zL zR 1 J(θ) = � m Xm i=1 1884 Embedding size de = 50 Dimensionality of child node vector dc = 50 Initial learning rate α = 0.05 Regularization A = 10−8 Dropout rate p = 20% Table 1: Hyper-parameter settings. 6.2 Experimental Settings For parameter initialization, we use random initialization within (-0.01, 0.01) for all parameters except the word embedding matrix E&apos;. Specifically, we adopt pre-trained English word embeddings from (Collobert et al., 2011). And we pretrain the Chinese word embeddings on a huge unlabeled data, the Chinese Wikipedia corpus, with word2vec toolkit (Mikolov et al., 2013a). Table 1 gives the details of hyper-parameter settings of our approach. In addition, we set minibatch size to 20. In all experiments, we only take s1, s2, s3 nodes in stack and b1, b2, b3 nodes in buffer into account. We also apply dropout strategy here, and only dropout at the nodes in stack and buffer with probability p = 20%. 6.3 Results The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Rabih Zbib</author>
<author>Zhongqiang Huang</author>
<author>Thomas Lamar</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
</authors>
<title>Fast and robust neural network joint models for statistical machine translation.</title>
<date>2014</date>
<booktitle>In 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Baltimore, MD, USA,</location>
<contexts>
<context position="2608" citStr="Devlin et al., 2014" startWordPosition="381" endWordPosition="385">al networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of</context>
</contexts>
<marker>Devlin, Zbib, Huang, Lamar, Schwartz, Makhoul, 2014</marker>
<rawString>Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John Makhoul. 2014. Fast and robust neural network joint models for statistical machine translation. In 52nd Annual Meeting of the Association for Computational Linguistics, Baltimore, MD, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="20605" citStr="Duchi et al., 2011" startWordPosition="3521" endWordPosition="3524"> the configuration, we update the relative representations using Tree-GRNN. 5 Training We use the maximum likelihood (ML) criterion to train our model. By extracting training set (xi, yi) from gold parse trees using a shortest stack oracle which always prefers Left-Arc(l) or Right-Arc(l) action over Shift, the goal of our model is to minimize the loss function with the parameter set θ: log p(yi|xi; θ)+ 2m11θ112 λ 2, (13) where m is number of extracted training examples which is as same as the number of all configurations. Following Socher et al. (2013), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatch strategy to minimize the objective. We also employ dropout strategy to avoid overfitting. In practice, we perform DAG-GRNN with two hidden layers, which gets the best performance. We use the approximated gradient for Tree-GRNN, which only performs gradient back propagation on the first two layers. 6 Experiments 6.1 Datasets To evaluate our proposed model, we experiment on two prevalent datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. • English For English Penn Treebank 3 (PTB3) dataset, we use sections 2-21 for training, section 22 and secti</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Miguel Ballesteros</author>
<author>Wang Ling</author>
<author>Austin Matthews</author>
<author>Noah A Smith</author>
</authors>
<title>Transitionbased dependency parsing with stack long shortterm memory. arXiv preprint arXiv:1505.08075.</title>
<date>2015</date>
<contexts>
<context position="31206" citStr="Dyer et al. (2015)" startWordPosition="5217" endWordPosition="5220">ng all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural network to make predictions of actions. Their method only relies on dense features, and is not able to automatically learn the most useful feature conjunctions to predict the transition action. Compared with (Chen and Manning, 2014), our method can fully exploit the information of all the descendants of a node in stack with Tree-GRNN. Then DAG-GRNN automatically learns the complicated combination of all the features, while the traditional discrete feature based methods need manually design them. Dyer et al. (2015) improved the transition-based dependency parsing using stack long short term memory neural network and received significant improvement on performance. They focused on exploiting the long distance dependencies and information, while we aims to automatically model the complicated feature combination. 8 Conclusion In this paper, we pay attention to the syntactic and semantic composition of the dense features for transition-based dependency parsing. We propose two heterogeneous gated recursive neural networks, Tree-GRNN and DAG-GRNN. Each hidden neuron in two proposed GRNNs can be regarded as a </context>
</contexts>
<marker>Dyer, Ballesteros, Ling, Matthews, Smith, 2015</marker>
<rawString>Chris Dyer, Miguel Ballesteros, Wang Ling, Austin Matthews, and Noah A Smith. 2015. Transitionbased dependency parsing with stack long shortterm memory. arXiv preprint arXiv:1505.08075.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="29945" citStr="Elman, 1990" startWordPosition="5022" endWordPosition="5023"> to model the representation of two linked words. But his model does not achieve the performance of the traditional method. Le and Zuidema (2014) proposed a generative re-ranking model with Inside-Outside Recursive Neural Network (IORNN), which can process trees both bottom-up and top-down. However, IORNN works in generative way and just estimates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate results. Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generalization of simple recurrent neural network (SRNN) (Elman, 1990). Although the two methods also used RNN, they just deal with the binary combination, which is unnatural for dependency tree. Zhu et al. (2015) proposed a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, they introduced the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Chen and Manning (2014) improved the transi</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Goldberg</author>
<author>Joakim Nivre</author>
</authors>
<title>Training deterministic parsers with non-deterministic oracles.</title>
<date>2013</date>
<tech>TACL,</tech>
<pages>1--403</pages>
<marker>Goldberg, Nivre, 2013</marker>
<rawString>Yoav Goldberg and Joakim Nivre. 2013. Training deterministic parsers with non-deterministic oracles. TACL, 1:403–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>He He</author>
<author>Hal Daum´e</author>
<author>Jason Eisner</author>
</authors>
<title>Dynamic feature selection for dependency parsing. In</title>
<date>2013</date>
<booktitle>EMNLP,</booktitle>
<pages>1455--1464</pages>
<marker>He, Daum´e, Eisner, 2013</marker>
<rawString>He He, Hal Daum´e III, and Jason Eisner. 2013. Dynamic feature selection for dependency parsing. In EMNLP, pages 1455–1464.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kenji Sagae</author>
</authors>
<title>Dynamic programming for linear-time incremental parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1077--1086</pages>
<contexts>
<context position="1773" citStr="Huang and Sagae, 2010" startWordPosition="244" endWordPosition="247">r the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model. 1 Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the p</context>
</contexts>
<marker>Huang, Sagae, 2010</marker>
<rawString>Liang Huang and Kenji Sagae. 2010. Dynamic programming for linear-time incremental parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1077– 1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In 16th Nordic Conference of Computational Linguistics,</booktitle>
<pages>105--112</pages>
<institution>University of Tartu.</institution>
<contexts>
<context position="21328" citStr="Johansson and Nugues, 2007" startWordPosition="3634" endWordPosition="3637">rfitting. In practice, we perform DAG-GRNN with two hidden layers, which gets the best performance. We use the approximated gradient for Tree-GRNN, which only performs gradient back propagation on the first two layers. 6 Experiments 6.1 Datasets To evaluate our proposed model, we experiment on two prevalent datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. • English For English Penn Treebank 3 (PTB3) dataset, we use sections 2-21 for training, section 22 and section 23 as development set and test set respectively. We adopt CoNLL Syntactic Dependencies (CD) (Johansson and Nugues, 2007) using the LTH Constituent-to-Dependency Conversion Tool. • Chinese For Chinese Penn Treebank 5 (CTB5) dataset, we follow the same split as described in (Zhang and Clark, 2008). Dependencies are converted by the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008). ⎡ z = ⎣ z Pˆ zL zR 1 J(θ) = � m Xm i=1 1884 Embedding size de = 50 Dimensionality of child node vector dc = 50 Initial learning rate α = 0.05 Regularization A = 10−8 Dropout rate p = 20% Table 1: Hyper-parameter settings. 6.2 Experimental Settings For parameter initialization, we use random initialization within (-0</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In 16th Nordic Conference of Computational Linguistics, pages 105–112. University of Tartu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1719" citStr="Koo et al., 2008" startWordPosition="234" endWordPosition="237">lly, Tree-GRNN models the feature combinations for the trees in stack, which already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model. 1 Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these method</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<date>2009</date>
<booktitle>Dependency parsing. Synthesis Lectures on Human Language Technologies,</booktitle>
<volume>1</volume>
<issue>1</issue>
<marker>K¨ubler, McDonald, Nivre, 2009</marker>
<rawString>Sandra K¨ubler, Ryan McDonald, and Joakim Nivre. 2009. Dependency parsing. Synthesis Lectures on Human Language Technologies, 1(1):1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phong Le and Willem Zuidema</author>
</authors>
<title>The insideoutside recursive neural network model for dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>729--739</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Doha, Qatar,</location>
<contexts>
<context position="2928" citStr="Zuidema, 2014" startWordPosition="437" endWordPosition="438">thods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes. Besides, the features of the selected nodes are just simply concatenated and then fed into neural network. Since the concatenation operation is relatively simple, it is difficult to model the com1879 Proceedings of the 201</context>
<context position="29478" citStr="Zuidema (2014)" startWordPosition="4946" endWordPosition="4947">words or features and found these representations are 2 4 6 8 10 UAS(%) 0.84 0.82 0.78 0.76 0.74 0.8 Plain Tree-RNN Tree-GRNN Tree-RNN+DAG-GRNN Tree-GRNN+DAG-GRNN 1886 complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. Stenetorp (2013) first used RNN for transition based dependency parsing. He followed the standard RNN and used the binary combination to model the representation of two linked words. But his model does not achieve the performance of the traditional method. Le and Zuidema (2014) proposed a generative re-ranking model with Inside-Outside Recursive Neural Network (IORNN), which can process trees both bottom-up and top-down. However, IORNN works in generative way and just estimates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate results. Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generalization of simple recurrent neural network (SRNN) (Elman, 1990). Although the two methods also used RNN, they just deal with the binary combination, which is unnatural for dependency tree. Zhu et </context>
</contexts>
<marker>Zuidema, 2014</marker>
<rawString>Phong Le and Willem Zuidema. 2014. The insideoutside recursive neural network model for dependency parsing. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 729–739, Doha, Qatar, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e FT Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume</booktitle>
<volume>1</volume>
<pages>342--350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1818" citStr="Martins et al., 2009" startWordPosition="252" endWordPosition="256">al dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model. 1 Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineer</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andr´e FT Martins, Noah A Smith, and Eric P Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1, pages 342–350. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In EACL.</booktitle>
<contexts>
<context position="23101" citStr="McDonald and Pereira, 2006" startWordPosition="3933" endWordPosition="3936">ategy here, and only dropout at the nodes in stack and buffer with probability p = 20%. 6.3 Results The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS). Conventionally, punctuations are excluded in all evaluation metrics. To evaluate the effectiveness of our approach, we compare our parsers with feature-based parser and neural-based parser. For feature-based parser, we compare our models with two prevalent parsers: MaltParser (Nivre et al., 2006) and MSTParser (McDonald and Pereira, 2006). For neural-based parser, we compare our results with parser of Chen and Manning (2014). Compared with parser of Chen and Manning (2014), our parser with two heterogeneous gated recursive neural networks (Tree-GRNN+DAG-GRNN) receives 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, and receives 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set. Since that speed of algorithm is not the focus of our paper, we do not optimize the speed a lot. On CTB (UAS), it takes about 2 days to train TreeGRNN+DAG-GRNN model with CPU only. The testing speed is about 2.7 sentences per second. All im</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan T McDonald and Fernando CN Pereira. 2006. Online learning of approximate dependency parsing algorithms. In EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="1841" citStr="McDonald et al., 2005" startWordPosition="257" endWordPosition="260">es. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model. 1 Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, </context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="2672" citStr="Mikolov et al., 2013" startWordPosition="394" endWordPosition="397">; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable informat</context>
<context position="22228" citStr="Mikolov et al., 2013" startWordPosition="3788" endWordPosition="3791">). ⎡ z = ⎣ z Pˆ zL zR 1 J(θ) = � m Xm i=1 1884 Embedding size de = 50 Dimensionality of child node vector dc = 50 Initial learning rate α = 0.05 Regularization A = 10−8 Dropout rate p = 20% Table 1: Hyper-parameter settings. 6.2 Experimental Settings For parameter initialization, we use random initialization within (-0.01, 0.01) for all parameters except the word embedding matrix E&apos;. Specifically, we adopt pre-trained English word embeddings from (Collobert et al., 2011). And we pretrain the Chinese word embeddings on a huge unlabeled data, the Chinese Wikipedia corpus, with word2vec toolkit (Mikolov et al., 2013a). Table 1 gives the details of hyper-parameter settings of our approach. In addition, we set minibatch size to 20. In all experiments, we only take s1, s2, s3 nodes in stack and b1, b2, b3 nodes in buffer into account. We also apply dropout strategy here, and only dropout at the nodes in stack and buffer with probability p = 20%. 6.3 Results The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS). Conventionally, punctuations are excluded in all evaluation metrics</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="2672" citStr="Mikolov et al., 2013" startWordPosition="394" endWordPosition="397">; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable informat</context>
<context position="22228" citStr="Mikolov et al., 2013" startWordPosition="3788" endWordPosition="3791">). ⎡ z = ⎣ z Pˆ zL zR 1 J(θ) = � m Xm i=1 1884 Embedding size de = 50 Dimensionality of child node vector dc = 50 Initial learning rate α = 0.05 Regularization A = 10−8 Dropout rate p = 20% Table 1: Hyper-parameter settings. 6.2 Experimental Settings For parameter initialization, we use random initialization within (-0.01, 0.01) for all parameters except the word embedding matrix E&apos;. Specifically, we adopt pre-trained English word embeddings from (Collobert et al., 2011). And we pretrain the Chinese word embeddings on a huge unlabeled data, the Chinese Wikipedia corpus, with word2vec toolkit (Mikolov et al., 2013a). Table 1 gives the details of hyper-parameter settings of our approach. In addition, we set minibatch size to 20. In all experiments, we only take s1, s2, s3 nodes in stack and b1, b2, b3 nodes in buffer into account. We also apply dropout strategy here, and only dropout at the nodes in stack and buffer with probability p = 20%. 6.3 Results The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS). Conventionally, punctuations are excluded in all evaluation metrics</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In NIPS, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
</authors>
<title>Maltparser: A data-driven parser-generator for dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>2216--2219</pages>
<contexts>
<context position="1861" citStr="Nivre et al., 2006" startWordPosition="261" endWordPosition="264"> feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model. 1 Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, dis</context>
<context position="23058" citStr="Nivre et al., 2006" startWordPosition="3927" endWordPosition="3930"> account. We also apply dropout strategy here, and only dropout at the nodes in stack and buffer with probability p = 20%. 6.3 Results The experiment results on PTB3 and CTB5 datasets are list in Table 2 and 3 respectively. On all datasets, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS). Conventionally, punctuations are excluded in all evaluation metrics. To evaluate the effectiveness of our approach, we compare our parsers with feature-based parser and neural-based parser. For feature-based parser, we compare our models with two prevalent parsers: MaltParser (Nivre et al., 2006) and MSTParser (McDonald and Pereira, 2006). For neural-based parser, we compare our results with parser of Chen and Manning (2014). Compared with parser of Chen and Manning (2014), our parser with two heterogeneous gated recursive neural networks (Tree-GRNN+DAG-GRNN) receives 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, and receives 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set. Since that speed of algorithm is not the focus of our paper, we do not optimize the speed a lot. On CTB (UAS), it takes about 2 days to train TreeGRNN+DAG-GRNN model with CPU only. The testing spee</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Maltparser: A data-driven parser-generator for dependency parsing. In Proceedings of LREC, volume 6, pages 2216–2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5591" citStr="Nivre, 2004" startWordPosition="842" endWordPosition="843">ures in different levels of granularity. To evaluate our approach, we experiment on two prevalent benchmark datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. Experiment results show the effectiveness of our proposed method. Compared to the parser of Chen and Manning (2014), we receive 0.6% (UAS) and 0.9% (LAS) improvement on PTB3 test set, while we receive 0.8% (UAS) and 1.3% (LAS) improvement on CTB5 test set. 2 Neural Network Based Transition Dependency Parsing 2.1 Transition Dependency Parsing In this paper, we employ the arc-standard transition systems (Nivre, 2004) and examine only greedy parsing for its efficiency. Figure 2 gives an example of arc-standard transition dependency parsing. In transition-based dependency parsing, the consecutive configurations of parsing process can be defined as c(i) = (s(i), b(i), A(i)) which consists of a stack s, a buffer b, and a set of dependency arcs A. Then, the greedy parsing process consecutively predicts the actions based on the features extracted from the corresponding configurations. For a given sentence w1, ... , wn, parsing process starts from a initial configuration c(0) = ([ROOT], [w1, ... , wn], 0), and t</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan B Pollack</author>
</authors>
<title>Recursive distributed representations.</title>
<date>1990</date>
<journal>Artificial Intelligence,</journal>
<volume>46</volume>
<issue>1</issue>
<contexts>
<context position="9949" citStr="Pollack, 1990" startWordPosition="1603" endWordPosition="1604">al token NULL is used to represent a non-existent element. We perform a standard neural network using one hidden layer with dh hidden units followed by a softmax layer as: h = g(W1x + b1), (1) p = softmax(W2h + b2), (2) where W1 E Rdhxˆd, b1 E Rdh, W2 E R|T |xdh, b2 E R|T |.Here, g is a non-linear function which can be hyperbolic tangent, sigmoid, cube (Chen and Manning, 2014), etc. 3 Recursive Neural Network Recursive neural network (RNN) is one of classical neural networks, which performs the same set of parameters recursively on a given structure (e.g. syntactic tree) in topological order (Pollack, 1990; Socher et al., 2013). In the simplest case, children nodes are combined into their parent node using a weight matrix W which is shared across the whole network, followed by a non-linear function g(·). Specifically, given the left child node vector hL E Rd and right child node vector hR E Rd, their parent node vector hP E Rd will be formalized as: hP = g (W L hR J) , (3) where W E Rdx2d and g is a non-linear function as mentioned above. 4 Architecture of Two Heterogeneous Gated Recursive Neural Networks for Transition-based Dependency Parsing In this paper, we apply the idea of recursive neur</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>Jordan B Pollack. 1990. Recursive distributed representations. Artificial Intelligence, 46(1):77–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars. In</title>
<date>2013</date>
<booktitle>In Proceedings of the ACL conference. Citeseer.</booktitle>
<contexts>
<context position="2629" citStr="Socher et al., 2013" startWordPosition="386" endWordPosition="389">) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in sta</context>
<context position="4256" citStr="Socher et al., 2013" startWordPosition="634" endWordPosition="637">ptember 2015. c�2015 Association for Computational Linguistics. plicated feature combinations which can be manually designed in the traditional discrete feature based methods. To tackle these problems, we use two heterogeneous gated recursive neural networks, tree structured gated recursive neural network (TreeGRNN) and directed acyclic graph gated structured recursive neural network (DAG-GRNN), to model each configuration during transition based dependency parsing. The two proposed GRNNs introduce the gate mechanism (Chung et al., 2014) to improve the standard recursive neural network (RNN) (Socher et al., 2013; Socher et al., 2014), and can model the syntactic and semantic compositions of the nodes during parsing. Figure 1 gives a rough sketch for the standard RNN, Tree-GRNN and DAG-GRNN. Tree-GRNN is applied to the partial-constructed trees in stack, which have already been constructed according to the previous transition actions. DAG-GRNN is applied to model the feature composition of nodes in stack and buffer which have not been labeled their dependency relations yet. Intuitively, Tree-GRNN selects and merges features recursively from children nodes into their parent according to their dependenc</context>
<context position="9971" citStr="Socher et al., 2013" startWordPosition="1605" endWordPosition="1608">s used to represent a non-existent element. We perform a standard neural network using one hidden layer with dh hidden units followed by a softmax layer as: h = g(W1x + b1), (1) p = softmax(W2h + b2), (2) where W1 E Rdhxˆd, b1 E Rdh, W2 E R|T |xdh, b2 E R|T |.Here, g is a non-linear function which can be hyperbolic tangent, sigmoid, cube (Chen and Manning, 2014), etc. 3 Recursive Neural Network Recursive neural network (RNN) is one of classical neural networks, which performs the same set of parameters recursively on a given structure (e.g. syntactic tree) in topological order (Pollack, 1990; Socher et al., 2013). In the simplest case, children nodes are combined into their parent node using a weight matrix W which is shared across the whole network, followed by a non-linear function g(·). Specifically, given the left child node vector hL E Rd and right child node vector hR E Rd, their parent node vector hP E Rd will be formalized as: hP = g (W L hR J) , (3) where W E Rdx2d and g is a non-linear function as mentioned above. 4 Architecture of Two Heterogeneous Gated Recursive Neural Networks for Transition-based Dependency Parsing In this paper, we apply the idea of recursive neural network (RNN) to de</context>
<context position="20544" citStr="Socher et al. (2013)" startWordPosition="3509" endWordPosition="3512"> of trees in the stack. When we need apply a new transition on the configuration, we update the relative representations using Tree-GRNN. 5 Training We use the maximum likelihood (ML) criterion to train our model. By extracting training set (xi, yi) from gold parse trees using a shortest stack oracle which always prefers Left-Arc(l) or Right-Arc(l) action over Shift, the goal of our model is to minimize the loss function with the parameter set θ: log p(yi|xi; θ)+ 2m11θ112 λ 2, (13) where m is number of extracted training examples which is as same as the number of all configurations. Following Socher et al. (2013), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatch strategy to minimize the objective. We also employ dropout strategy to avoid overfitting. In practice, we perform DAG-GRNN with two hidden layers, which gets the best performance. We use the approximated gradient for Tree-GRNN, which only performs gradient back propagation on the first two layers. 6 Experiments 6.1 Datasets To evaluate our proposed model, we experiment on two prevalent datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. • English For English Penn Treebank 3 (PTB3) dat</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013. Parsing with compositional vector grammars. In In Proceedings of the ACL conference. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--207</pages>
<contexts>
<context position="4278" citStr="Socher et al., 2014" startWordPosition="638" endWordPosition="641">Association for Computational Linguistics. plicated feature combinations which can be manually designed in the traditional discrete feature based methods. To tackle these problems, we use two heterogeneous gated recursive neural networks, tree structured gated recursive neural network (TreeGRNN) and directed acyclic graph gated structured recursive neural network (DAG-GRNN), to model each configuration during transition based dependency parsing. The two proposed GRNNs introduce the gate mechanism (Chung et al., 2014) to improve the standard recursive neural network (RNN) (Socher et al., 2013; Socher et al., 2014), and can model the syntactic and semantic compositions of the nodes during parsing. Figure 1 gives a rough sketch for the standard RNN, Tree-GRNN and DAG-GRNN. Tree-GRNN is applied to the partial-constructed trees in stack, which have already been constructed according to the previous transition actions. DAG-GRNN is applied to model the feature composition of nodes in stack and buffer which have not been labeled their dependency relations yet. Intuitively, Tree-GRNN selects and merges features recursively from children nodes into their parent according to their dependency structures, while DA</context>
<context position="13862" citStr="Socher et al., 2014" startWordPosition="2278" endWordPosition="2281"> current node including its word form p.w, pos tag p.t and label type p.l as shown in Eq. 5, and vpl and vpr are initialized by zero vectors 0 E Rdc, then update as Eq. 6. 4.1 Tree Structured Gated Recursive Neural Network It is a natural way to merge the information from children nodes into their parent node recursively according to the given tree structures in stack. Although the dependency relations have been built, it is still hard to apply the recursive neural network (as Eq. 3) directly for the uncertain number of children of each node in stack. By averaging operation on children nodes (Socher et al., 2014), the parent node cannot well capture the crucial features from the mixed information of its children nodes. Here, we propose tree structured gated recursive neural network (Tree-GRNN) incorporating the gate mechanism (Cho et al., 2014; Chung et al., 2014; Chen et al., 2015a; Chen et al., 2015b), which can selectively choose the ⎛ ⎡ ⎤ ⎞ ew p.w vpn = tanh ⎝ ⎣et ⎦ ⎠ , (5) p.t el p.l where word embedding ewp.w E Rde, pos embedding et t E Rde and label embedding elp l E Rde p.are extracted from embedding matrices Ew, Et and El according to the indices of the corresponding word p.w, pos p.t and lab</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pontus Stenetorp</author>
</authors>
<title>Transition-based dependency parsing using recursive neural networks.</title>
<date>2013</date>
<booktitle>In NIPS Workshop on Deep Learning.</booktitle>
<contexts>
<context position="2945" citStr="Stenetorp, 2013" startWordPosition="439" endWordPosition="440"> on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes. Besides, the features of the selected nodes are just simply concatenated and then fed into neural network. Since the concatenation operation is relatively simple, it is difficult to model the com1879 Proceedings of the 2015 Conference on E</context>
<context position="29216" citStr="Stenetorp (2013)" startWordPosition="4903" endWordPosition="4904"> the DAG-GRNN is of great help in boosting the convergency speed. 7 Related Work Many neural network based methods have been used for transition based dependency parsing. Chen et al. (2014) and Bansal et al. (2014) used the dense vectors (embeddings) to represent words or features and found these representations are 2 4 6 8 10 UAS(%) 0.84 0.82 0.78 0.76 0.74 0.8 Plain Tree-RNN Tree-GRNN Tree-RNN+DAG-GRNN Tree-GRNN+DAG-GRNN 1886 complementary to the traditional discrete feature representation. However, these two methods only focus on the dense representations (embeddings) of words or features. Stenetorp (2013) first used RNN for transition based dependency parsing. He followed the standard RNN and used the binary combination to model the representation of two linked words. But his model does not achieve the performance of the traditional method. Le and Zuidema (2014) proposed a generative re-ranking model with Inside-Outside Recursive Neural Network (IORNN), which can process trees both bottom-up and top-down. However, IORNN works in generative way and just estimates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate results. Besides, IORNN treats</context>
</contexts>
<marker>Stenetorp, 2013</marker>
<rawString>Pontus Stenetorp. 2013. Transition-based dependency parsing using recursive neural networks. In NIPS Workshop on Deep Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics,</booktitle>
<pages>384--394</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2650" citStr="Turian et al., 2010" startWordPosition="390" endWordPosition="393"> for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss </context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, pages 384–394. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>562--571</pages>
<contexts>
<context position="21504" citStr="Zhang and Clark, 2008" startWordPosition="3661" endWordPosition="3664"> propagation on the first two layers. 6 Experiments 6.1 Datasets To evaluate our proposed model, we experiment on two prevalent datasets: English Penn Treebank 3 (PTB3) and Chinese Penn Treebank 5 (CTB5) datasets. • English For English Penn Treebank 3 (PTB3) dataset, we use sections 2-21 for training, section 22 and section 23 as development set and test set respectively. We adopt CoNLL Syntactic Dependencies (CD) (Johansson and Nugues, 2007) using the LTH Constituent-to-Dependency Conversion Tool. • Chinese For Chinese Penn Treebank 5 (CTB5) dataset, we follow the same split as described in (Zhang and Clark, 2008). Dependencies are converted by the Penn2Malt tool with the head-finding rules of (Zhang and Clark, 2008). ⎡ z = ⎣ z Pˆ zL zR 1 J(θ) = � m Xm i=1 1884 Embedding size de = 50 Dimensionality of child node vector dc = 50 Initial learning rate α = 0.05 Regularization A = 10−8 Dropout rate p = 20% Table 1: Hyper-parameter settings. 6.2 Experimental Settings For parameter initialization, we use random initialization within (-0.01, 0.01) for all parameters except the word embedding matrix E&apos;. Specifically, we adopt pre-trained English word embeddings from (Collobert et al., 2011). And we pretrain the</context>
</contexts>
<marker>Zhang, Clark, 2008</marker>
<rawString>Yue Zhang and Stephen Clark. 2008. A tale of two parsers: investigating and combining graphbased and transition-based dependency parsing using beam-search. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 562–571.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<pages>188--193</pages>
<contexts>
<context position="1796" citStr="Zhang and Nivre, 2011" startWordPosition="248" endWordPosition="251">hich already have partial dependency structures. DAG-GRNN models the feature combinations of the nodes whose dependency relations have not been built yet. Experiment results on two prevalent benchmark datasets (PTB3 and CTB5) show the effectiveness of our proposed model. 1 Introduction Transition-based dependency parsing is a core task in natural language processing, which has been studied with considerable efforts in the NLP community. The traditional discriminative dependency parsing methods have achieved great success (Koo et al., 2008; He et al., 2013; Bohnet, 2010; Huang and Sagae, 2010; Zhang and Nivre, 2011; Martins et al., 2009; McDonald et al., 2005; Nivre et al., 2006; K¨ubler et al., 2009; Goldberg and Nivre, (a) Standard RNN (b) Tree-GRNN (c) DAG-GRNN Figure 1: Sketch of three recursive neural networks (RNN). (a) is the standard RNN for constituent tree; (b) is Tree-GRNN for dependency tree, in which the dashed arcs indicate the dependency relations between the nodes; (c) is DAGGRNN for the nodes without given topological structure. 2013; Choi and McCallum, 2013; Ballesteros and Bohnet, 2014). However, these methods are based on discrete features and suffer from the problems of data sparsit</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 188–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chenxi Zhu</author>
<author>Xipeng Qiu</author>
<author>Xinchi Chen</author>
<author>Xuanjing Huang</author>
</authors>
<title>A re-ranking model for dependency parser with recursive convolutional neural network.</title>
<date>2015</date>
<booktitle>In Proceedings ofAnnual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3009" citStr="Zhu et al., 2015" startWordPosition="449" endWordPosition="452">rsity and feature engineering (Chen and Manning, 2014). Recently, distributed representations have been widely used in a variety of natural language processing (NLP) tasks (Collobert et al., 2011; Devlin et al., 2014; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Specific to the transition-based parsing, the neural network based methods have also been increasingly focused on due to their ability to minimize the efforts in feature engineering and the boosted performance (Le and Zuidema, 2014; Stenetorp, 2013; Bansal et al., 2014; Chen and Manning, 2014; Zhu et al., 2015). However, most of the existing neural network based methods still need some efforts in feature engineering. For example, most methods often select the first and second leftmost/rightmost children of the top nodes in stack, which could miss some valuable information hidden in the unchosen nodes. Besides, the features of the selected nodes are just simply concatenated and then fed into neural network. Since the concatenation operation is relatively simple, it is difficult to model the com1879 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1879–1889</context>
<context position="30088" citStr="Zhu et al. (2015)" startWordPosition="5045" endWordPosition="5048"> (2014) proposed a generative re-ranking model with Inside-Outside Recursive Neural Network (IORNN), which can process trees both bottom-up and top-down. However, IORNN works in generative way and just estimates the probability of a given tree, so IORNN cannot fully utilize the incorrect trees in k-best candidate results. Besides, IORNN treats dependency tree as a sequence, which can be regarded as a generalization of simple recurrent neural network (SRNN) (Elman, 1990). Although the two methods also used RNN, they just deal with the binary combination, which is unnatural for dependency tree. Zhu et al. (2015) proposed a recursive convolutional neural network (RCNN) architecture to capture syntactic and compositional-semantic representations of phrases and words in a dependency tree. Different with the original recursive neural network, they introduced the convolution and pooling layers, which can model a variety of compositions by the feature maps and choose the most informative compositions by the pooling layers. Chen and Manning (2014) improved the transition-based dependency parsing by representing all words, POS tags and arc labels as dense vectors, and modeled their interactions with neural n</context>
</contexts>
<marker>Zhu, Qiu, Chen, Huang, 2015</marker>
<rawString>Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and Xuanjing Huang. 2015. A re-ranking model for dependency parser with recursive convolutional neural network. In Proceedings ofAnnual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
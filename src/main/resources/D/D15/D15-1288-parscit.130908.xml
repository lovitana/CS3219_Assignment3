<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99388">
Compact, Efficient and Unlimited Capacity: Language Modeling with
Compressed Suffix Trees
</title>
<author confidence="0.953807">
Ehsan Shareghi,[ Matthias Petri,\ Gholamreza Haffari[ and Trevor Cohn\
</author>
<affiliation confidence="0.6144075">
[ Faculty of Information Technology, Monash University
\ Computing and Information Systems, The University of Melbourne
</affiliation>
<email confidence="0.59376">
first.last@{monash.edu,unimelb.edu.au}
</email>
<sectionHeader confidence="0.980205" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999531411764706">
Efficient methods for storing and querying
language models are critical for scaling to
large corpora and high Markov orders. In
this paper we propose methods for mod-
eling extremely large corpora without im-
posing a Markov condition. At its core,
our approach uses a succinct index – a
compressed suffix tree – which provides
near optimal compression while support-
ing efficient search. We present algorithms
for on-the-fly computation of probabilities
under a Kneser-Ney language model. Our
technique is exact and although slower
than leading LM toolkits, it shows promis-
ing scaling properties, which we demon-
strate through oc-order modeling over the
full Wikipedia collection.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993886440678">
Language models (LMs) are critical components
in many modern NLP systems, including machine
translation (Koehn, 2010) and automatic speech
recognition (Rabiner and Juang, 1993). The most
widely used LMs are mgram models (Chen and
Goodman, 1996), based on explicit storage of
mgrams and their counts, which have proved
highly accurate when trained on large datasets. To
be useful, LMs need to be not only accurate but
also fast and compact.
Depending on the order and the training corpus
size, a typical mgram LM may contain as many
as several hundred billions of mgrams (Brants
et al., 2007), raising challenges of efficient stor-
age and retrieval. As always, there is a trade-off
between accuracy, space, and time, with recent
papers considering small but approximate lossy
LMs (Chazelle et al., 2004; Talbot and Osborne,
2007; Guthrie and Hepple, 2010), or loss-less
LMs backed by tries (Stolcke et al., 2011), or re-
lated compressed structures (Germann et al., 2009;
Heafield, 2011; Pauls and Klein, 2011; Sorensen
and Allauzen, 2011; Watanabe et al., 2009). How-
ever, none of these approaches scale well to very
high-order m or very large corpora, due to their
high memory and time requirements. An impor-
tant exception is Kennington et al. (2012), who
also propose a language model based on a suffix
tree which scales well with m but poorly with the
corpus size (requiring memory of about 20× the
training corpus).
In contrast, we1 make use of recent advances in
compressed suffix trees (CSTs) (Sadakane, 2007)
to build compact indices with much more mod-
est memory requirements (≈ the size of the cor-
pus). We present methods for extracting frequency
and unique context count statistics for mgram
queries from CSTs, and two algorithms for com-
puting Kneser-Ney LM probabilities on the fly us-
ing these statistics. The first method uses two
CSTs (over the corpus and the reversed corpus),
which allow for efficient computation of the num-
ber of unique contexts to the left and right of an
mgram, but is inefficient in several ways, most
notably when computing the number of unique
contexts to both sides. Our second method ad-
dresses this problem using a single CST backed by
a wavelet tree based FM-index (Ferragina et al.,
2007), which results in better time complexity and
considerably faster runtime performance.
Our experiments show that our method is prac-
tical for large-scale language modelling, although
querying is substantially slower than a SRILM
benchmark. However our technique scales much
more gracefully with Markov order m, allowing
unbounded ‘non-Markov’ application, and enables
training on large corpora as we demonstrate on the
complete Wikipedia dump. Overall this paper il-
lustrates the vast potential succinct indexes have
</bodyText>
<footnote confidence="0.927877">
1For the implementation see: https://github.com/eehsan/
lm-sdsl.
</footnote>
<page confidence="0.927602">
2409
</page>
<note confidence="0.986199">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2409–2418,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.95512325">
much larger than T and prohibit the use of these
structures for all but small data sets.
for language modelling and other ‘big data’ prob-
lems in language processing.
</bodyText>
<sectionHeader confidence="0.957387" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999355083333334">
Suffix Arrays and Suffix Trees Let T be a
string of size n drawn from an alphabet E of size
σ. Let T [i..n − 1] be a suffix of T . The suffix
tree (Weiner, 1973) of T is the compact labeled
tree of n + 1 leaves where the root to leaf paths
correspond to all suffixes of T$, where $ is a ter-
minating symbol not in E. The path-label of each
node v corresponds to the concatenation of edge
labels from the root node to v. The node depth
of v corresponds to the number of ancestors in the
tree, whereas the string depth corresponds to the
length of the path-label. Searching for a pattern α
of size m in T translates to finding the locus node
v closest to the root such that α is a prefix of the
path-label of v in O(m) time. We refer to this ap-
proach as forward search. Figure 1a shows a suffix
tree over a sample text. A suffix tree requires O(n)
space and can be constructed in O(n) time (Ukko-
nen, 1995). The children of each node in the suffix
tree are lexicographically ordered by their edge la-
bels. The i-th smallest suffix in T corresponds to
the path-label of the i-th leaf. The starting position
of the suffix can be associated its corresponding
leaf in the tree as shown in Figure 1a. All occur-
rences of α in T can be retrieved by visiting all
leaves in the subtree of the locus of α. For exam-
ple, pattern “the night” occurs at positions 12 and
19 in the sample text. We further refer the number
of children of a node v as its degree and the num-
ber of leaves in the subtree rooted at v as the size
of v.
The suffix array (Manber and Myers, 1993) of
T is an array SA[0 ... n − 1] such that SA[i] corre-
sponds to the starting position of the i-th smallest
suffix in T or the i-th leaf in the suffix tree of T .
The suffix array requires n log n bits of space and
can also be constructed in O(n) time (K¨arkk¨ainen
et al., 2006). Using only the suffix array and the
text, pattern search can be performed using bi-
nary search in O(m log n) time. For example, the
pattern “the night” is found by performing binary
search using SA and T to determine SA[18,19], the
interval in SA corresponding the the suffixes in T
prefixed by the pattern. In practice, suffix arrays
use 4 − 8n bytes of space whereas the most ef-
ficient suffix tree implementations require at least
20n bytes of space (Kurtz, 1999) which are both
Compressed Suffix Structures Reducing the
space usage of suffix based index structure has
recently become an active area of research. The
space usage of a suffix array can be reduced sig-
nificantly by utilizing the compressibility of text
combined with succinct data structures. A suc-
cinct data structure provides the same function-
ality as an equivalent uncompressed data struc-
ture, but requires only space equivalent to the
information-theoretic lower bound of the underly-
ing data. For simplicity, we focus on the FM-Index
which emulates the functionality of a suffix array
over T using nHk(T) + o(n log σ) bits of space
where Hk refers to the k-th order entropy of the
text (Ferragina et al., 2007). In practice, the FM-
Index of T uses roughly space equivalent to the
compressed representation of T using a standard
compressor such as bzip2. For a more compre-
hensive overview on succinct text indexes, see the
excellent survey of Ferragina et al. (2008).
The FM-Index relies on the duality between
the suffix array and the BWT (Burrows and
Wheeler, 1994), a permutation of the text such that
Tbwt[i] = T[SA[i] − 1] (see Figure 1). Search-
ing for a pattern using the FM-Index is performed
in reverse order by performing RANK(Tbwt, i, c)
operations O(m) times. Here, RANK(Tbwt, i, c)
counts the number of times symbol c occurs in
Tbwt[0 ... i − 1]. This process is usually referred
to as backward search. Let SA[li, ri] be the in-
terval corresponding to the suffixes in T match-
ing α[i ... m − 1]. By definition of the BWT,
Tbwt[li, ri] corresponds to the symbols in T pre-
ceding α[i ... m − 1] in T . Due to the lexico-
graphical ordering of all suffixes in SA, the interval
SA[li−1, ri−1] corresponding to all occurrences of
α[i − 1... m − 1] can be determined by comput-
ing the rank of all occurrences of c = α[i − 1] in
Tbwt[li, ri]. Thus, we compute RANK(Tbwt, li, c),
the number of times symbol c occurs before li and
RANK(Tbwt, ri + 1, c), the number of occurrences
of c in Tbwt[0, ri]. To determine SA[li−1, ri−1],
we additionally store the starting positions Cs of
all suffixes for each symbol s in E at a negligi-
ble cost of σ log n bits. Thus, the new interval
is computed as li−1 = Cc+RANK(Tbwt, li, c) and
ri−1 = Cc+RANK(Tbwt, ri + 1, c).
The time and space complexity of the FM-
index thus depends on the cost of storing and pre-
</bodyText>
<page confidence="0.977785">
2410
</page>
<figure confidence="0.99881284057971">
(a) Word-based Suffix Tree.
#nT$ppttnnrrttotssi##it
01100011110011111100001
(b) Wavelet tree and RANK(Tbwt,17, ‘t’) = 5.
22
21
11
0
18
8
17
7
14
4
15
5 3 6
20
13
keeper..
2
16
19
12
1
9
10
0 1
#$pprri##i
0011110000
nTttnnttotsst
0111001101001
0
#$##
1011
pprr
0011
nnnoss
000100
0
Ttttttt
1000000
1
0
0 1
0 1
nnnss
11100
1 0 1 1
i
p
r
o
t
T
1
$
#
1
0
s n
#$i##i
001001
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22
T [SA[i]]
SA
Tbwt
$ # # # i i p p r r s s n n n o t t t t t t T
22 21 11 0 18 8 17 7 14 4 15 5 20 13 3 2 16 6 19 12 1 9 10
# n T $ p p t t n n r r t t o t s s i # # i t
</figure>
<figureCaption confidence="0.994211">
Figure 1: Data structures for the sample text T=“#the old night keeper keeps the keep in the town#
the night keeper keeps the keep in the night#$” with alphabet E={the, old, night, keeper, keeps, keep,
in, town, #} and code words $=0000, #=0001, i=in=001, p=keep=010, r=keeper=011, s=keeps=1000,
o=old=101, t=the=110, n=night=1001 and T=town=111.
</figureCaption>
<bodyText confidence="0.967238298245614">
processing Tbwt to answer RANK efficiently. A
wavelet tree can be used to answer RANK over
Tbwt in O(log σ) time. The wavelet tree re-
duces RANK over an alphabet E into multiple
RANK operations over a binary alphabet which
can be answered in O(1) time and o(n) bits
extra space by periodically storing absolute and
relative RANK counts (Munro, 1996). The al-
phabet is reduced by recursively splitting sym-
bols based on their code words into subgroups
to form a binary tree as shown in Figure 1b
for Tbwt. To answer RANK(Tbwt, i, c), the tree
is traversed based on the code word of c, per-
forming binary RANK at each level. For exam-
ple, RANK(Tbwt, 17, ‘t’) translates to performing
RANK(WTroot, 17,1) = 12 on the top level of
the wavelet tree, as t=the=110. We recurse
to the right subtree of the root node and com-
pute RANK(WT1,12,1) as there were 12 ones
in the root node and the next bit in the code-
word of ‘the’ is also one. This process contin-
ues until the correct leaf node is reached to answer
RANK(Tbwt, 17,‘t’) = 5 in O(log σ) time. The
space usage of a regular wavelet tree is n log σ +
o(n log σ) bits which roughly matches the size of
the text.2 If locations of matches are required, ad-
2However, if code-words for each symbol are chosen
based on their Huffman-codes the size of the wavelet tree
ditional space is needed to access SA[i] or the in-
verse suffix array SA−1[SA[i]] = i. In the sim-
plest scheme, both values are periodically sam-
pled using a given sample rate SAS (e.g. 32) such
that SA[i] mod SAS = 0. Then, for any SA[i]
or SA−1[i], at most O(SAS) RANK operations on
Tbwt are required to access the value. Differ-
ent sample rates, bitvector implementations and
wavelet tree types result in a wide variety of time-
space tradeoffs which can be explored in prac-
tice (Gog et al., 2014).
In the same way the FM-index emulates the
functionality of the suffix array in little space,
compressed suffix trees (CST) provide the func-
tionality of suffix trees while requiring signifi-
cantly less space than their uncompressed coun-
terparts (Sadakane, 2007). A CST uses a com-
pressed suffix array (CSA) such as the FM-Index
but stores additional information to represent the
shape of the suffix tree as well as information
about path-labels. Again a variety of different stor-
age schemes exist, however for simplicity we fo-
cus on the CST of Ohlebusch et al. (2010) which
we use in our experiments. Here, the shape of the
tree is stored using a balanced-parenthesis (BP) se-
quence which for a tree of p nodes requires ≈ 2p
reduces to nH0(T)(1 + o(1)) bits which can be further be
reduced to to nHk(T) + o(n log v) bits by using entropy
compressed bitvectors.
</bodyText>
<page confidence="0.95018">
2411
</page>
<bodyText confidence="0.999851695652174">
bits. Using little extra space and advanced bit-
operations, the BP-sequence can be used to per-
form operations such as string-depth(v), parent(v)
or accessing the i-th leaf can be answered in con-
stant time. To support more advanced operations
such as accessing path-labels, the underlying CSA
or a compressed version of the LCP array are re-
quired which can be more expensive.3 In practice,
a CST requires roughly 4 − 6n bits in addition to
the cost of storing the CSA. For a more extensive
overview of CSTs see Russo et al. (2011).
Kneser Ney Language Modelling Recall our
problem of efficient mgram language modeling
backed by a corpus encoded in a succinct index.
Although our method is generally applicable to
many LM variants, we focus on the Kneser-Ney
LM (Kneser and Ney, 1995), specifically the inter-
polated variant described in Chen and Goodman
(1996), which has been shown to outperform other
ngram LMs and has become the de-facto standard.
Interpolated Kneser-Ney describes the condi-
tional probability of a word wi conditioned on the
context of m − 1 preceding words, wi−1
</bodyText>
<equation confidence="0.928477">
i−m+1, as
P(wi |wi−m+1) = max [c(wi−mil) − Dm, 01
c(wi−m+1)
DmN1+(wi−1 · )
i−m−1
c(i−1
wi−m+1)
</equation>
<bodyText confidence="0.999964">
where lower-order smoothed probabilities are de-
fined recursively (for 1 &lt; k &lt; m) as
</bodyText>
<equation confidence="0.937827">
P¯(wi|wi−1
i−k+1) = N1+(·wi−1 ·
i−k+1 )
</equation>
<bodyText confidence="0.9991885">
In the above formula, Dk is the kgram-specific
discount parameter, and the occurrence count
N1+(α·) = |{w : c(αw) &gt; 0} |is the number of
observed word types following the pattern α; the
occurrence counts N1+(·α) and N1+(·α·) are
defined accordingly. The recursion stops at uni-
gram level where the unigram probabilities are de-
fined as P¯(wi) = N1+(·wi)/N1+(··).4
</bodyText>
<footnote confidence="0.9967755">
3See Supplementary Materials Table 1 for an overview of
the complexities of the functionality of the CST that is used
in our experiments.
4Modified Kneser-Ney, proposed by Chen and Good-
man (1996), typically outperforms interpolated Kneser-Ney
through its use of context-specific discount parameters. The
</footnote>
<sectionHeader confidence="0.577738" genericHeader="method">
3 Using CSTs for KN Computation
</sectionHeader>
<bodyText confidence="0.999722652173913">
The key requirements for computing probability
under a Kneser-Ney language model are two types
of counts: raw frequencies of mgrams and occur-
rence counts, quantifying how many different con-
texts the mgram has occurred in. Figure 2 (right)
illustrates the requisite counts for calculating the
probability of an example 4-gram. In electing to
store the corpus directly in a suffix tree, we need to
provide mechanisms for computing these counts
based on queries into the suffix tree.
The raw frequency counts are the simplest to
compute. First we identify the locus node v in
the suffix tree for the query mgram; the frequency
corresponds to the node’s size, an O(1) operation
which returns the number of leaves below v. To il-
lustrate, consider searching for c(the night) in Fig-
ure 1a, which matches a node with two leaves (la-
belled 19 and 12), and thus c = 2.
More problematic are the occurrence counts,
which come in several flavours: right contexts,
N1+(α·), left contexts, N1+(·α), and contexts
to both sides of the pattern, N1+(·α·). The first
of these can be handled easily, as
</bodyText>
<equation confidence="0.950199">
N1+(α·) = degree(v), if α = label(v)
1, otherwise
</equation>
<bodyText confidence="0.999936722222222">
where v is the node matching α, and label(v) de-
notes the path-label of v.5 For example, keep
in has two child nodes in Figure 1a, and thus
there are two unique contexts in which it can oc-
cur, N1+(keep in ·) = 2, while the keep par-
tially matches an edge in the forward suffix tree
in Figure 1a as it can only be followed by in,
N1+(the keep ·) = 1. A similar line of reason-
ing applies to computing N1+(·α). Assuming
we also have a second suffix tree representing the
reversed corpus, we first identify the reversed pat-
tern (e.g., in keepR) and then use above method to
compute the occurrence count (denoted hereafter
N1P(t, v, α)6, where t is the CST.).
implementation of this with our data structures is straight-
forward in principle, but brings a few added complexities
in terms of dynamic computing other types of occurrence
counts, which we leave for future work.
</bodyText>
<footnote confidence="0.49679975">
5See the Supplementary Materials for the explicit algo-
rithm, but note there are some corner cases involving sen-
tinels # and $, which must be excluded when computing oc-
currence counts. Such tests have been omitted from the pre-
sentation for clarity.
6In the presented algorithms, we overload the pattern ar-
gument in function calls for readability, and use · to denote
the query context.
</footnote>
<equation confidence="0.96503225">
+
P¯(wi|wi−1
i−m+2), (1)
max [N1+(·wii−k+1) − Dk, 01
+ DkN1+(wi−1 P¯(wi|wi−1
i−k+1·) i−k+2) . (2)
N1+(·wi−1 · )
i−k+1
</equation>
<page confidence="0.983828">
2412
</page>
<figureCaption confidence="0.9161295">
Figure 2: Counts required for computing P(town|keep in the) (right) and the suffix tree nodes required
for computing each value (left). The two left-most columns correspond to vall
</figureCaption>
<bodyText confidence="0.416781">
R and vR and are updated
using forward-search in the reverse CST, while the righter-most column correspond to vF and is updated
using backward-search in the forward CST. See Algorithm 2 for details.
</bodyText>
<figure confidence="0.935722210526316">
N1+(• the town) N1+(• the •) N1+(the •)
in the town,
in the2
in the,
c(keep the) N1+(keep
in the town) c(keep in in the •)
keep in the town,
keep in the2
(in the •)
1+ 1+
N (• town) N
the town,
the,
the2
(••)
town,
root: [0,n] root: [0,n] root: [0,n]
N1+(• in the town) N1+
(• in the •) N1+
</figure>
<bodyText confidence="0.983902909090909">
The final component of the Kneser-Ney LM
computation is N1+(·α·), the number of unique
contexts considering symbols on both sides of the
pattern. Unfortunately this does not map to a sim-
ple suffix tree operation, but instead requires enu-
meration, N1+(·α·) = Ks∈F(α) N1+(·αs),
where F(α) is the set of symbols that can follow
α. Algorithm 1 shows how this is computed, with
lines 7 and 8 enumerating s ∈ F(α) using the
edge labels of the children of v. For each symbol,
line 9 searches for an extended pattern incorporat-
ing the new symbol s in the reverse CSA (part of
the reverse CST), by refining the existing match
vR using a single backward search operation af-
ter which we can compute N1+(·αs).7 Line 5
deals with the special case where the pattern does
not match a complete edge, in which case there
is only only unique right context and therefore
N1+(·α·) = N1+(·α).
N1P and N1PFRONTBACK can compute the
requisite occurence counts for mgram language
modelling, however at considerable cost in terms
of space and time. The need for twin reverse and
forward CSTs incurs a significant storage over-
head, as well as the search time to match the pat-
tern in both CSTs. We show in Section 5 how
we can avoid the need for the reversed suffix tree,
giving rise to lower memory requirements and
faster runtime. Beyond the need for twin suf-
fix trees, the highest time complexity calls are
string-depth, edge and backward-search. Calling
string-depth is constant time for internal nodes,
but O(SAS log σ) for leaf nodes; fortunately we
</bodyText>
<footnote confidence="0.9447415">
7Backward search in the reverse tree corresponds to
searching for the reversed pattern appended with one symbol.
</footnote>
<table confidence="0.739395666666667">
Algorithm 1 Two-sided occ., N1+(·α·)
Precondition: vF in forward CST tF matches α
Precondition: vR in reverse CST tR matches α
</table>
<listItem confidence="0.978845090909091">
1: function N1PFRONTBACK(tF, vF, tR, vR, α)
2: o ← 0
3: d ← string-depth(vF)
4: if d &gt; |α |then
5: o ← N1P(tR, vR,·α)
6: else
7: for uF ← children(vF) do
8: s ← edge(uF, d + 1)
9: uR ← back-search(vR, s)
10: o ← o + N1P(tR, uR,·αs)
11: return o
</listItem>
<bodyText confidence="0.999667333333333">
can avoid this call for leaves, which by definition
extend to the end of the corpus and consequently
extend further than our pattern.8 The costly calls
to edge and backward-search however cannot be
avoided. This leads to an overall time complex-
ity of O(1) for N1P and O(F(α) × SAS × log σ)
for N1PFRONTBACK, where F(α) is the number
of following symbols and SAS is the suffix array
value sample rate described in Section 2.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="method">
4 Dual CST Algorithm
</sectionHeader>
<bodyText confidence="0.999825">
The methods above for computing the frequency
and occurrence counts provide the ingredients
necessary for computing mgram language model
probabilities. This leaves the algorithmic problem
</bodyText>
<footnote confidence="0.645436">
8We assume search patterns do not extend beyond a single
sentence, and thus will always be shorter than the edge labels.
</footnote>
<page confidence="0.913537">
2413
</page>
<bodyText confidence="0.563225">
Algorithm 2 KN probability P (wk  |wk−1
</bodyText>
<equation confidence="0.700878">
k−(m− 1) )
</equation>
<listItem confidence="0.833768333333333">
1: function PROBKNESERNEY(tF, tR, w, m)
2: vF root(tF) &gt; match for suffix of wk−1
k−(m−1)
3: vR root(tR) &gt; match for suffix of wk−1
k−(m−1)
4: vallR +-root(tR) &gt; match for suffix of wkk−(m−1)
5: p + -1
6: fori+- 1 to m do
7: vallR +- forw-search(vall
</listItem>
<figure confidence="0.926991625">
R , wk−i+1)
8: if i &gt; 1 then
9: vF +- back-search(vF, wk−i+1)
10: if i &lt; m then
11: vR +- forw-search(vR, wk−i+1)
12: Di lookup discount for igram
13: if i = m then
14: c size(vall
R )
15: d +- size(vF)
16: else
17: c N1P(tR, vall
R ,·wk k−i+1)
18: d +-
N1PFRONTBACK(tF, vF, tR, vR,·wk−1
k−i+1·)
19: if i &gt; 1 then
20: if vF is valid then
21: q +- N1P(tF, vF, wk−1
k−i+1·)
22: p +-d1 (max(c − Di, 0) + Diqp)
23: else if i = 1 then
p +- c/N1+(··)
25: return p
</figure>
<bodyText confidence="0.977415965517241">
of efficiently ordering the search operations in for-
ward and reverse CST structures.
This paper considers an interpolated LM for-
mulation, in which probabilities from higher or-
der contexts are interpolated with lower order es-
timates. This iterative process is apparent in Fig-
ure 2 (right) which shows the quantities required
for probability scoring for an example mgram.
Equivalently, the iteration can be considered in re-
verse, starting from unigram estimates and suc-
cessively growing to large mgrams, in each stage
adding a single new symbol to left of the pattern.
This suits incremental search in a CST in which
search bounds are iteratively refined, which has a
substantially lower time complexity compared to
searching over the full index in each step.
Algorithm 2 presents an outline of the approach.
This uses a forward CST, tF, and a reverse CST,
tR, with three CST nodes (lines 2–4) tracking the
match progress for the full igram (vall
R ) and the
(i − 1)gram context (vF, vR), i = 1... m. The
need to maintain three concurrent searches arises
from the calls to size, N1+(·α), N1+(α·) and
N1+(·α·) (lines 14, 15; 17; 21; and 18, respec-
tively). These calls impose conditions on the di-
rection of the suffix tree, e.g., such that the edge
labels and node degree can be used to compute
Algorithm 3 Precompute KN discounts
</bodyText>
<listItem confidence="0.841000444444444">
1: function PRECOMPUTEDISCOUNTS(tR, m)
2: ck f +- 0 `dk E [1, m], f E [1, 2]
3: 1&lt;g +- 0 `dk E [1, m], g E [1, 2]
4: N1+(··) +- 0
5: for vR +- descendents(root(tR)) do &gt; depth-first
6: dP +- string-depth(parent(vR))
7: d +- string-depth(vR)
8: fork + -dP + 1 to min(d,dP+m)do
9: s +- edge(vR, k)
</listItem>
<figure confidence="0.965100666666667">
10: if s is the end of sentence sentinel then
11: skip all children of vR
12: else
13: ifk=2then
N1+(··) +- N1+(··) + 1
15: f +- size(vR)
16: if 1 &lt; f &lt; 2 then
17: ck,f +- ck,f + 1
18: if k &lt; d then
19: g 1
20: else
21: g degree(vR)
22: if 1 &lt; g &lt; 2 then
23: N1k,g N1k,g + 1
24: return c, N1, N1+(··)
</figure>
<bodyText confidence="0.997019379310345">
the number of left or right contexts in which a
pattern appears. The matching process is illus-
trated in Figure 2 where the three search nodes are
shown on the left, considered bottom to top, and
their corresponding count operations are shown to
the right. The N1+(·α) calls require a match
in the reverse CST (left-most column, vall
R ), while
the N1+(α·) require a match in the forward CST
(right-most column, vF, matching the (i − 1)gram
context). The N1+(·α·) computation reuses the
forward match while also requiring a match for the
(i−1)gram context in the reversed CST, as tracked
by the middle column (vR). Because of the mix
of forward and reverse CSTs, coupled with search
patterns that are revealed right-to-left, incremen-
tal search in each of the CSTs needs to be han-
dled differently (lines 7–11). In the forward CST,
we perform backward search to extend the search
pattern to the left, which can be computed very ef-
ficiently from the BWT in the CSA.9 Conversely
in the reverse CST, we must use forward search as
we are effectively extending the reversed pattern
to the right; this operation is considerably more
costly.
The discounts D on line 12 of Algorithm 2 and
N1+(··) (a special case of line 18) are precom-
puted directly from the CSTs thus avoiding several
costly computations at runtime. The precomputa-
</bodyText>
<footnote confidence="0.972147">
9See Supplementary Materials Table 1 for the time com-
plexities of this and other CSA and CST methods.
</footnote>
<page confidence="0.995599">
2414
</page>
<bodyText confidence="0.999962">
tion algorithm is provided in Algorithm 3 which
operates by traversing the nodes of the reverse
CST and at each stage computing the number of
mgrams that occur 1–2 times (used for computing
Dm in eq. 1), or with N1+(·α) E [1 − 2] (used
for computing Dk in eq. 2), for various lengths
of mgrams. These quantities are used to compute
the discount parameters, which are then stored for
later use in inference.10 Note that the PRECOM-
PUTEDISCOUNTS algorithm can be slow, although
it is significantly faster if we remove the edge calls
and simply include in our counts all mgrams fin-
ishing a sentence or spanning more than one sen-
tence. This has a negligible (often beneficial) ef-
fect on perplexity.
</bodyText>
<sectionHeader confidence="0.994527" genericHeader="method">
5 Improved Single CST Approach
</sectionHeader>
<bodyText confidence="0.997002705882353">
The above dual CST algorithm provides an el-
egant means of computing LM probabilities of
arbitrary order and with a limited space com-
plexity (O(n), or roughly n in practice). How-
ever the time complexity is problematic, stem-
ming from the expensive method for computing
N1PFRONTBACK and repeated searches over the
CST, particularly forward-search. Now we out-
line a method for speeding up the algorithm by
doing away with the reverse CST. Instead the crit-
ical counts, N1+(·α) and N1+(·α·) are com-
puted directly from a single forward CST. This
confers the benefit of using only backward search
and avoiding redundant searches for the same pat-
tern (cf. lines 9 and 11 in Algorithm 2).
The full algorithm for computing LM prob-
abilities is given in Algorithm 4, however for
space reasons we will not describe this in de-
tail. Instead we will focus on the method’s most
critical component, the algorithm for computing
N1+(·α·) from the forward CST, presented in
Algorithm 5. The key difference from Algorithm 1
is the loop from lines 6–9, which uses the interval-
symbols (Schnattinger et al., 2010) method. This
method assumes a wavelet tree representation of
the SA component of the CST, an efficient encod-
ing of the BWT as describes in section 2. The
interval-symbols method uses RANK operations
to efficiently identify for a given pattern the set of
preceding symbols P(α) and the ranges SA[ls, rs]
corresponding to the patterns sα for all s E P(α)
10Discounts are computed up to a limit on mgram size,
here set to 10. The highest order values are used for comput-
ing the discount of mgrams above the limit at runtime.
</bodyText>
<figure confidence="0.978770212121212">
Algorithm 4 KN probability P(wk|wk−(m−1))
using a single CST
2: vF ← root(tF) . match for context wk−1
k−i
3: vall
F ← root(tF) . match for wk k−i
4: p ← 1
5: for i ← 1 to m do
6: vall
F ← back-search([lb(vall
F ), rb(vall
F )], wk−i+1)
7: if i &gt; 1 then
8: vF ← back-search([lb(vF), rb(vF)], wk−i+1)
9: Di ← discount parameter for igram
10: if i = m then
11: c ← size(vall
F )
12:
F ,·wk−1
14: c ← N1PBACK1(tF, vall k−i+1)
d ← N1PFRONTBACK1(tF, vF,·wk−1
15: k−i+1·)
16: if i &gt; 1 then
17: if vF is valid then
18: q ← N1P(tF, vF, wk−1
k−i+1·)
19: p ←1d (max(c − Di, 0) + Diqp)
20: else
p ← c/N1+(··)
22: return p
Algorithm 5 N1+(·α·), using forward CST
Precondition: vF in forward CST tF matches α
</figure>
<listItem confidence="0.8646094">
1: function N1PFRONTBACK1(tF, vF, α)
2: o ← 0
3: if string-depth(vF) &gt; |α |then
4: o ← N1PBACK1(tF, vF,·α)
5: else
6: for hl, r, si ← int-syms(tF, [lb(vF), rb(vF)]) do
7: l0 ← Cs + l
8: r0 ← Cs + r
o ← o + N1P(tF, node(l0, r0), sα·)
10: return o
</listItem>
<bodyText confidence="0.977921473684211">
by visiting all leaves of the wavelet tree of sym-
bols occurring in Tbwt[l, r] (corresponding to α)
in O(|P(α) |log σ) time (lines 6-8). These ranges
SA[l&apos;, r&apos;] can be used to find the corresponding suf-
fix tree node for each sα in O(1) time. To illus-
trate, consider the pattern α = “night” in Fig-
ure 1a. From Tbwt we can see that this is pre-
ceeded by s =“old” (1st occurrence in Tbwt) and
s =“the” (3rd and 4th); from which we can com-
pute the suffix tree nodes, namely [15,15] and
[16 + (3 − 1),16 + (4 − 1)] = [18,19] for “old”
and “the” respectively.11
N1PBACK1 is computed in a similar way, us-
ing the interval-symbols method to compute the
number of unique preceeding symbols (see Sup-
plementary Materials, Algorithm 7). Overall the
time complexity of inference for both N1PBACK1
11Using the offsets into the SA for each symbol, Cold = 15
and Cthe = 16, while −1 adjusts for counting from 1.
</bodyText>
<figure confidence="0.667410333333333">
1: function PROBKNESERNEY1(tF, w, m)
d ← size(vF)
13: else
</figure>
<page confidence="0.690089">
2415
</page>
<table confidence="0.999371545454545">
Language Size(MiB) Tokens(M) Word Types Sentences(K)
BG 36.11 8.53 114930 329
CS 53.48 12.25 174592 535
DE 171.80 44.07 399354 1785
EN 179.15 49.32 124233 1815
FI 145.32 32.85 721389 1737
FR 197.68 53.82 147058 1792
HU 52.53 12.02 318882 527
IT 186.67 48.08 178259 1703
PT 187.20 49.03 183633 1737
Wikipedia 8637 9057 196 87835
</table>
<tableCaption confidence="0.971722">
Table 1: Dataset statistics, showing total un-
</tableCaption>
<figureCaption confidence="0.524158428571429">
compressed size; and tokens, types and sentence
counts for the training partition. For Wikipedia
the Word Types, and Tokens are computed based
on characters.
Figure 3: Perplexity results on several Europarl
languages for different mgram sizes, m =
2 ... 10,15, 20, oo.
</figureCaption>
<figure confidence="0.9846154">
2 3 4 5 6 7 8 9 10 15 20 oo
mgram size
Perplexity [Relative to ∞]
200%
150%
120%
110%
105%
102%
100%
</figure>
<table confidence="0.9552283">
Language (2-gram pplx oo-gram pplx)
BG (2-gram 117.39 oo-gram 73.01)
CS (2-gram 232.36 oo-gram 161.15)
DE (2-gram 178.11 oo-gram 108.27)
EN (2-gram 67.14 oo-gram 59.92)
FI (2-gram 446.29 oo-gram 314.22)
FR (2-gram 89.95 oo-gram 47.91)
HU (2-gram 251.49 oo-gram 182.18)
IT (2-gram 132.26 oo-gram 77.80)
PT (2-gram 121.42 oo-gram 68.58)
</table>
<bodyText confidence="0.962605416666667">
and N1PFRONTBACK1 is O(P(α) logo) where
P(α) is the number of preceeding symbols of α, a
considerable improvement over N1PFRONTBACK
using the forward and reverse CSTs. Overall
this leads to considerably faster computation of
mgram probabilities compared to the two CST ap-
proach, and although still slower than highly opti-
mised LM toolkits like SRILM, it is fast enough to
support large scale experiments, and has consider-
ably better scaling performance with the Markov
order m (even allowing unlimited order), as we
will now demonstrate.
</bodyText>
<sectionHeader confidence="0.999827" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.998510839285714">
We used Europarl dataset and the data was num-
berized after tokenizing, splitting, and excluding
XML markup. The first 10k sentences were used
as the test data, and the last 80% as the train-
ing data, giving rise to training corpora of be-
tween 8M and 50M tokens and uncompressed size
of up to 200 MiB (see Table 1 for detailed cor-
pus statistics). We also processed the full 52 GiB
uncompressed “20150205” English Wikipedia ar-
ticles dump to create a character level language
model consisting of 72M sentences. We excluded
10k random sentences from the collection as test
data. We use the SDSL library (Gog et al., 2014) to
implement all our structures and compare our in-
dexes to SRILM (Stolcke, 2002). We refer to our
dual-CST approach as D-CST, and the single-CST
as S-CST.
We evaluated the perplexity across different lan-
guages and using mgrams of varying order from
m = 2 to oo (unbounded), as shown on Figure 3.
Our results matched the perplexity results from
SRILM (for smaller values of m in which SRILM
training was feasible, m &lt; 10). Note that perplex-
ity drops dramatically from m = 2 ... 5 however
the gains thereafter are modest for most languages.
Despite this, several large mgram matches were
found ranging in size up to a 34-gram match. We
speculate that the perplexity plateau is due to the
simplistic Kneser-Ney discounting formula which
is not designed for higher order mgram LMs and
appear to discount large mgrams too aggressively.
We leave further exploration of richer discounting
techniques such as Modified Kneser-Ney (Chen
and Goodman, 1996) or the Sequence Memoizer
(Wood et al., 2011) to our future work.
Figure 4 compares space and time of our in-
dexes with SRILM on the German part of Eu-
roparl. The construction cost of our indexes in
terms of both space and time is comparable to
that of a 3/4-gram SRILM index. The space us-
age of D-CST index is comparable to a compact
3-gram SRILM index. Our S-CST index uses only
177 MiB RAM at query time, which is compara-
ble to the size of the collection (172 MiB). How-
ever, query processing is significantly slower for
both our structures. For 2-grams, D-CST is 3 times
slower than a 2-gram SRILM index as the expen-
sive N1+(·α·) is not computed. However, for
large mgrams, our indexes are much slower than
SRILM. For m &gt; 2, the D-CST index is roughly
six times slower than S-CST. Our fastest index, is
10 times slower than the slowest SRILM 10-gram
index. However, our run-time is independent of
m. Thus, as m increases, our index will become
more competitive to SRILM while using a constant
amount of space.
</bodyText>
<page confidence="0.975984">
2416
</page>
<figure confidence="0.996928891891892">
Construction Cost Query Cost
2
2
2 − ∞
3
2 − ∞
3
6 7 8910
5 789
4 5 6
4
10
2
2
2
3 − ∞
3
2
3 − 5
3
4
4
6 7
5
6
910
7 8
910
8
Time [sec]
10 k
1 k
100
10
100 M 1 G 10 G 100 M 1 G 10 G
Space Usage [bytes]
D-CST S-CST srilm-compact srilm-default
</figure>
<figureCaption confidence="0.983716666666667">
Figure 4: Time versus space tradeoffs measured on
Europarl German (de) dataset, showing memory
and time requirements.
Figure 5: Runtime breakdown of a single pattern
averaged over all patterns for both methods over
the Wikipedia collection.
</figureCaption>
<bodyText confidence="0.999972590909091">
Next we analyze the performance of our in-
dex on the large Wikipedia dataset. The S-CST,
character level index for the data set requires
22 GiB RAM at query time whereas the D-CST re-
quires 43 GiB. Figure 5 shows the run-time per-
formance of both indexes for different mgrams,
broken down by the different components of the
computation. As discussed above, 2-gram per-
formance is much faster. For both indexes, most
time is spent computing N1PFRONTBACK (i.e.,
N1+(·α·)) for all m &gt; 2. However, the wavelet
tree traversal used in S-CST roughly reduces the
running time by a factor of three. The complex-
ity of N1PFRONTBACK depends on the number of
contexts, which is likely small for larger mgrams,
but can be large for small mgrams, which sug-
gest partial precomputation could significantly in-
crease the query performance of our indexes. Ex-
ploring the myraid of different CST and CSA con-
figurations available could also lead to significant
improvements in runtime and space usage also re-
mains future work.
</bodyText>
<sectionHeader confidence="0.998728" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999993777777778">
This paper has demonstrated the massive poten-
tial that succinct indexes have for language mod-
elling, by developing efficient algorithms for on-
the-fly computing of mgram counts and language
model probabilities. Although we only consid-
ered a Kneser-Ney LM, our approach is portable to
the many other LM smoothing method formulated
around similar count statistics. Our complexity
analysis and experimental results show favourable
scaling properties with corpus size and Markov or-
der, albeit running between 1-2 orders of magni-
tude slower than a leading count-based LM. Our
ongoing work seeks to close this gap: preliminary
experiments suggest that with careful tuning of the
succinct index parameters and caching expensive
computations, query time can be competitive with
state-of-the-art toolkits, while using less memory
and allowing the use of unlimited context.
</bodyText>
<sectionHeader confidence="0.998209" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995525444444445">
Ehsan Shareghi and Gholamreza Haffari are grate-
ful to National ICT Australia (NICTA) for gen-
erous funding, as part of collaborative machine
learning research projects. Matthias Petri is the
recipient of an Australian Research Councils Dis-
covery Project scheme (project DP140103256).
Trevor Cohn is the recipient of an Australian Re-
search Council Future Fellowship (project number
FT130101105).
</bodyText>
<sectionHeader confidence="0.99905" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999893285714286">
Thorsten Brants, Ashok C Popat, Peng Xu, Franz J
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proc. EMNLP-
CoNLL.
M. Burrows and D. Wheeler. 1994. A block sorting
lossless data compression algorithm. Technical Re-
port 124, DEC.
Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and
Ayellet Tal. 2004. The bloomier filter: An efficient
data structure for static support lookup tables. In
Proc. SODA, pages 30–39.
Stanley F Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proc. ACL, pages 310–318.
</reference>
<figure confidence="0.997965705882353">
2 3 4 5 6 8 10 ∞
mgram size
Time per Sentence [msec]
1500
1000
500
300
200
100
0
0
N1PFRONTBACK
fw-search
back-search
N1PBACK
N1PFRONT
D-CST S-CST
</figure>
<page confidence="0.967649">
2417
</page>
<reference confidence="0.999942468354431">
P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro.
2007. Compressed representations of sequences
and full-text indexes. ACM Trans. on Algorithms,
3(2):article 20.
Paolo Ferragina, Rodrigo Gonz´alez, Gonzalo Navarro,
and Rossano Venturini. 2008. Compressed text in-
dexes: From theory to practice. ACM J. of Exp. Al-
gorithmics, 13.
Ulrich Germann, Eric Joanis, and Samuel Larkin.
2009. Tightly packed tries: How to fit large models
into memory, and make them load fast, too. In Proc.
of the Workshop on Software Engineering, Testing,
and Quality Assurance for Natural Language Pro-
cessing, pages 31–39.
Simon Gog, Timo Beller, Alistair Moffat, and Matthias
Petri. 2014. From theory to practice: Plug and play
with succinct data structures. In Proc. SEA, pages
326–337.
David Guthrie and Mark Hepple. 2010. Storing the
web in memory: Space efficient language models
with constant time retrieval. In Proc. EMNLP, pages
262–272.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proc. WMT.
Juha K¨arkk¨ainen, Peter Sanders, and Stefan Burkhardt.
2006. Linear work suffix array construction. J.
ACM, 53(6):918–936.
Casey Redd Kennington, Martin Kay, and Annemarie
Friedrich. 2012. Suffix trees as language models.
In Proc. LREC, pages 446–453.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In
Proc. ICASSP, volume 1, pages 181–184.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Stefan Kurtz. 1999. Reducing the space requirement
of suffix trees. Softw., Pract. Exper., 29(13):1149–
1171.
Udi Manber and Eugene W. Myers. 1993. Suffix
arrays: A new method for on-line string searches.
SIAM J. Comput., 22(5):935–948.
Ian Munro. 1996. Tables. In Proc. FSTTCS, pages
37–42.
Enno Ohlebusch, Johannes Fischer, and Simon Gog.
2010. CST++. In Proc. SPIRE, pages 322–333.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proc. ACL-HLT.
Lawrence Rabiner and Biing-Hwang Juang. 1993.
Fundamentals of speech recognition. Prentice-Hall.
L. Russo, G. Navarro, and A. Oliveira. 2011. Fully-
compressed suffix trees. ACM Trans. Algorithms,
7(4):article 53.
Kunihiko Sadakane. 2007. Compressed suffix trees
with full functionality. Theory Comput. Syst.,
41(4):589–607.
Thomas Schnattinger, Enno Ohlebusch, and Simon
Gog. 2010. Bidirectional search in a string with
wavelet trees. In Proc. CPM, pages 40–50.
Jeffrey Sorensen and Cyril Allauzen. 2011. Unary
data structures for language models. In Proc. IN-
TERSPEECH, pages 1425–1428.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. Srilm at sixteen: Update and out-
look. In Proc. ASRU, page 5.
Andreas Stolcke. 2002. SRILM–an extensible lan-
guage modeling toolkit. In Proc. INTERSPEECH.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proc. ACL.
Esko Ukkonen. 1995. On-line construction of suffix
trees. Algorithmica, 14(3):249–260.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2009. A succinct n-gram language model. In Proc.
ACL Short Papers, pages 341–344.
Peter Weiner. 1973. Linear pattern matching algo-
rithms. In Proc. SWAT, pages 1–11.
Frank Wood, Jan Gasthaus, C´edric Archambeau,
Lancelot James, and Yee Whye Teh. 2011. The se-
quence memoizer. CACM, 54(2):91–98.
</reference>
<page confidence="0.990656">
2418
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.696149">
<title confidence="0.997992">Compact, Efficient and Unlimited Capacity: Language Modeling Compressed Suffix Trees</title>
<author confidence="0.970261">Matthias</author>
<author confidence="0.970261">Trevor</author>
<affiliation confidence="0.8449195">of Information Technology, Monash and Information Systems, The University of</affiliation>
<abstract confidence="0.996375833333333">Efficient methods for storing and querying language models are critical for scaling to large corpora and high Markov orders. In this paper we propose methods for modeling extremely large corpora without imposing a Markov condition. At its core, our approach uses a succinct index – a suffix tree which provides near optimal compression while supporting efficient search. We present algorithms for on-the-fly computation of probabilities under a Kneser-Ney language model. Our is although slower than leading LM toolkits, it shows promising scaling properties, which we demonthrough modeling over the full Wikipedia collection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proc. EMNLPCoNLL.</booktitle>
<contexts>
<context position="1611" citStr="Brants et al., 2007" startWordPosition="237" endWordPosition="240">a collection. 1 Introduction Language models (LMs) are critical components in many modern NLP systems, including machine translation (Koehn, 2010) and automatic speech recognition (Rabiner and Juang, 1993). The most widely used LMs are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirem</context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C Popat, Peng Xu, Franz J Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proc. EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Burrows</author>
<author>D Wheeler</author>
</authors>
<title>A block sorting lossless data compression algorithm.</title>
<date>1994</date>
<tech>Technical Report 124,</tech>
<contexts>
<context position="7570" citStr="Burrows and Wheeler, 1994" startWordPosition="1279" endWordPosition="1282">rmation-theoretic lower bound of the underlying data. For simplicity, we focus on the FM-Index which emulates the functionality of a suffix array over T using nHk(T) + o(n log σ) bits of space where Hk refers to the k-th order entropy of the text (Ferragina et al., 2007). In practice, the FMIndex of T uses roughly space equivalent to the compressed representation of T using a standard compressor such as bzip2. For a more comprehensive overview on succinct text indexes, see the excellent survey of Ferragina et al. (2008). The FM-Index relies on the duality between the suffix array and the BWT (Burrows and Wheeler, 1994), a permutation of the text such that Tbwt[i] = T[SA[i] − 1] (see Figure 1). Searching for a pattern using the FM-Index is performed in reverse order by performing RANK(Tbwt, i, c) operations O(m) times. Here, RANK(Tbwt, i, c) counts the number of times symbol c occurs in Tbwt[0 ... i − 1]. This process is usually referred to as backward search. Let SA[li, ri] be the interval corresponding to the suffixes in T matching α[i ... m − 1]. By definition of the BWT, Tbwt[li, ri] corresponds to the symbols in T preceding α[i ... m − 1] in T . Due to the lexicographical ordering of all suffixes in SA,</context>
</contexts>
<marker>Burrows, Wheeler, 1994</marker>
<rawString>M. Burrows and D. Wheeler. 1994. A block sorting lossless data compression algorithm. Technical Report 124, DEC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Chazelle</author>
<author>Joe Kilian</author>
<author>Ronitt Rubinfeld</author>
<author>Ayellet Tal</author>
</authors>
<title>The bloomier filter: An efficient data structure for static support lookup tables.</title>
<date>2004</date>
<booktitle>In Proc. SODA,</booktitle>
<pages>30--39</pages>
<contexts>
<context position="1820" citStr="Chazelle et al., 2004" startWordPosition="269" endWordPosition="272">he most widely used LMs are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the tr</context>
</contexts>
<marker>Chazelle, Kilian, Rubinfeld, Tal, 2004</marker>
<rawString>Bernard Chazelle, Joe Kilian, Ronitt Rubinfeld, and Ayellet Tal. 2004. The bloomier filter: An efficient data structure for static support lookup tables. In Proc. SODA, pages 30–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="1264" citStr="Chen and Goodman, 1996" startWordPosition="176" endWordPosition="179">ch provides near optimal compression while supporting efficient search. We present algorithms for on-the-fly computation of probabilities under a Kneser-Ney language model. Our technique is exact and although slower than leading LM toolkits, it shows promising scaling properties, which we demonstrate through oc-order modeling over the full Wikipedia collection. 1 Introduction Language models (LMs) are critical components in many modern NLP systems, including machine translation (Koehn, 2010) and automatic speech recognition (Rabiner and Juang, 1993). The most widely used LMs are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepp</context>
<context position="13311" citStr="Chen and Goodman (1996)" startWordPosition="2388" endWordPosition="2391">ations such as accessing path-labels, the underlying CSA or a compressed version of the LCP array are required which can be more expensive.3 In practice, a CST requires roughly 4 − 6n bits in addition to the cost of storing the CSA. For a more extensive overview of CSTs see Russo et al. (2011). Kneser Ney Language Modelling Recall our problem of efficient mgram language modeling backed by a corpus encoded in a succinct index. Although our method is generally applicable to many LM variants, we focus on the Kneser-Ney LM (Kneser and Ney, 1995), specifically the interpolated variant described in Chen and Goodman (1996), which has been shown to outperform other ngram LMs and has become the de-facto standard. Interpolated Kneser-Ney describes the conditional probability of a word wi conditioned on the context of m − 1 preceding words, wi−1 i−m+1, as P(wi |wi−m+1) = max [c(wi−mil) − Dm, 01 c(wi−m+1) DmN1+(wi−1 · ) i−m−1 c(i−1 wi−m+1) where lower-order smoothed probabilities are defined recursively (for 1 &lt; k &lt; m) as P¯(wi|wi−1 i−k+1) = N1+(·wi−1 · i−k+1 ) In the above formula, Dk is the kgram-specific discount parameter, and the occurrence count N1+(α·) = |{w : c(αw) &gt; 0} |is the number of observed word types </context>
<context position="31937" citStr="Chen and Goodman, 1996" startWordPosition="5720" endWordPosition="5723">rplexity results from SRILM (for smaller values of m in which SRILM training was feasible, m &lt; 10). Note that perplexity drops dramatically from m = 2 ... 5 however the gains thereafter are modest for most languages. Despite this, several large mgram matches were found ranging in size up to a 34-gram match. We speculate that the perplexity plateau is due to the simplistic Kneser-Ney discounting formula which is not designed for higher order mgram LMs and appear to discount large mgrams too aggressively. We leave further exploration of richer discounting techniques such as Modified Kneser-Ney (Chen and Goodman, 1996) or the Sequence Memoizer (Wood et al., 2011) to our future work. Figure 4 compares space and time of our indexes with SRILM on the German part of Europarl. The construction cost of our indexes in terms of both space and time is comparable to that of a 3/4-gram SRILM index. The space usage of D-CST index is comparable to a compact 3-gram SRILM index. Our S-CST index uses only 177 MiB RAM at query time, which is comparable to the size of the collection (172 MiB). However, query processing is significantly slower for both our structures. For 2-grams, D-CST is 3 times slower than a 2-gram SRILM i</context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F Chen and Joshua Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. ACL, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ferragina</author>
<author>G Manzini</author>
<author>V M¨akinen</author>
<author>G Navarro</author>
</authors>
<title>Compressed representations of sequences and full-text indexes.</title>
<date>2007</date>
<journal>ACM Trans. on Algorithms,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>20</pages>
<marker>Ferragina, Manzini, M¨akinen, Navarro, 2007</marker>
<rawString>P. Ferragina, G. Manzini, V. M¨akinen, and G. Navarro. 2007. Compressed representations of sequences and full-text indexes. ACM Trans. on Algorithms, 3(2):article 20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paolo Ferragina</author>
<author>Rodrigo Gonz´alez</author>
<author>Gonzalo Navarro</author>
<author>Rossano Venturini</author>
</authors>
<title>Compressed text indexes: From theory to practice.</title>
<date>2008</date>
<journal>ACM J. of Exp. Algorithmics,</journal>
<volume>13</volume>
<marker>Ferragina, Gonz´alez, Navarro, Venturini, 2008</marker>
<rawString>Paolo Ferragina, Rodrigo Gonz´alez, Gonzalo Navarro, and Rossano Venturini. 2008. Compressed text indexes: From theory to practice. ACM J. of Exp. Algorithmics, 13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Eric Joanis</author>
<author>Samuel Larkin</author>
</authors>
<title>Tightly packed tries: How to fit large models into memory, and make them load fast, too.</title>
<date>2009</date>
<booktitle>In Proc. of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</booktitle>
<pages>31--39</pages>
<contexts>
<context position="1986" citStr="Germann et al., 2009" startWordPosition="296" endWordPosition="299"> on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus). In contrast, we1 make use of recent advances in compressed suffix trees (CSTs) (Sadakane, 2007) to build compact indices with much more modest memory </context>
</contexts>
<marker>Germann, Joanis, Larkin, 2009</marker>
<rawString>Ulrich Germann, Eric Joanis, and Samuel Larkin. 2009. Tightly packed tries: How to fit large models into memory, and make them load fast, too. In Proc. of the Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 31–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Gog</author>
<author>Timo Beller</author>
<author>Alistair Moffat</author>
<author>Matthias Petri</author>
</authors>
<title>From theory to practice: Plug and play with succinct data structures.</title>
<date>2014</date>
<booktitle>In Proc. SEA,</booktitle>
<pages>326--337</pages>
<contexts>
<context position="11590" citStr="Gog et al., 2014" startWordPosition="2093" endWordPosition="2096">e required, ad2However, if code-words for each symbol are chosen based on their Huffman-codes the size of the wavelet tree ditional space is needed to access SA[i] or the inverse suffix array SA−1[SA[i]] = i. In the simplest scheme, both values are periodically sampled using a given sample rate SAS (e.g. 32) such that SA[i] mod SAS = 0. Then, for any SA[i] or SA−1[i], at most O(SAS) RANK operations on Tbwt are required to access the value. Different sample rates, bitvector implementations and wavelet tree types result in a wide variety of timespace tradeoffs which can be explored in practice (Gog et al., 2014). In the same way the FM-index emulates the functionality of the suffix array in little space, compressed suffix trees (CST) provide the functionality of suffix trees while requiring significantly less space than their uncompressed counterparts (Sadakane, 2007). A CST uses a compressed suffix array (CSA) such as the FM-Index but stores additional information to represent the shape of the suffix tree as well as information about path-labels. Again a variety of different storage schemes exist, however for simplicity we focus on the CST of Ohlebusch et al. (2010) which we use in our experiments. </context>
<context position="30991" citStr="Gog et al., 2014" startWordPosition="5561" endWordPosition="5564"> We used Europarl dataset and the data was numberized after tokenizing, splitting, and excluding XML markup. The first 10k sentences were used as the test data, and the last 80% as the training data, giving rise to training corpora of between 8M and 50M tokens and uncompressed size of up to 200 MiB (see Table 1 for detailed corpus statistics). We also processed the full 52 GiB uncompressed “20150205” English Wikipedia articles dump to create a character level language model consisting of 72M sentences. We excluded 10k random sentences from the collection as test data. We use the SDSL library (Gog et al., 2014) to implement all our structures and compare our indexes to SRILM (Stolcke, 2002). We refer to our dual-CST approach as D-CST, and the single-CST as S-CST. We evaluated the perplexity across different languages and using mgrams of varying order from m = 2 to oo (unbounded), as shown on Figure 3. Our results matched the perplexity results from SRILM (for smaller values of m in which SRILM training was feasible, m &lt; 10). Note that perplexity drops dramatically from m = 2 ... 5 however the gains thereafter are modest for most languages. Despite this, several large mgram matches were found ranging</context>
</contexts>
<marker>Gog, Beller, Moffat, Petri, 2014</marker>
<rawString>Simon Gog, Timo Beller, Alistair Moffat, and Matthias Petri. 2014. From theory to practice: Plug and play with succinct data structures. In Proc. SEA, pages 326–337.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Guthrie</author>
<author>Mark Hepple</author>
</authors>
<title>Storing the web in memory: Space efficient language models with constant time retrieval.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>262--272</pages>
<contexts>
<context position="1873" citStr="Guthrie and Hepple, 2010" startWordPosition="277" endWordPosition="280">d Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus). In contrast, we1 make use of recent a</context>
</contexts>
<marker>Guthrie, Hepple, 2010</marker>
<rawString>David Guthrie and Mark Hepple. 2010. Storing the web in memory: Space efficient language models with constant time retrieval. In Proc. EMNLP, pages 262–272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: Faster and smaller language model queries. In</title>
<date>2011</date>
<booktitle>Proc. WMT.</booktitle>
<contexts>
<context position="2002" citStr="Heafield, 2011" startWordPosition="300" endWordPosition="301"> be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus). In contrast, we1 make use of recent advances in compressed suffix trees (CSTs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ </context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proc. WMT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juha K¨arkk¨ainen</author>
<author>Peter Sanders</author>
<author>Stefan Burkhardt</author>
</authors>
<title>Linear work suffix array construction.</title>
<date>2006</date>
<journal>J. ACM,</journal>
<volume>53</volume>
<issue>6</issue>
<marker>K¨arkk¨ainen, Sanders, Burkhardt, 2006</marker>
<rawString>Juha K¨arkk¨ainen, Peter Sanders, and Stefan Burkhardt. 2006. Linear work suffix array construction. J. ACM, 53(6):918–936.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Redd Kennington</author>
<author>Martin Kay</author>
<author>Annemarie Friedrich</author>
</authors>
<title>Suffix trees as language models.</title>
<date>2012</date>
<booktitle>In Proc. LREC,</booktitle>
<pages>446--453</pages>
<contexts>
<context position="2267" citStr="Kennington et al. (2012)" startWordPosition="343" endWordPosition="346">t storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus). In contrast, we1 make use of recent advances in compressed suffix trees (CSTs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracting frequency and unique context count statistics for mgram queries from CSTs, and two algorithms for computing Kneser-Ney LM probabilities on the fly using these statistics. The first method uses two CSTs (ove</context>
</contexts>
<marker>Kennington, Kay, Friedrich, 2012</marker>
<rawString>Casey Redd Kennington, Martin Kay, and Annemarie Friedrich. 2012. Suffix trees as language models. In Proc. LREC, pages 446–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved backing-off for m-gram language modeling.</title>
<date>1995</date>
<booktitle>In Proc. ICASSP,</booktitle>
<volume>1</volume>
<pages>181--184</pages>
<contexts>
<context position="13235" citStr="Kneser and Ney, 1995" startWordPosition="2377" endWordPosition="2380"> i-th leaf can be answered in constant time. To support more advanced operations such as accessing path-labels, the underlying CSA or a compressed version of the LCP array are required which can be more expensive.3 In practice, a CST requires roughly 4 − 6n bits in addition to the cost of storing the CSA. For a more extensive overview of CSTs see Russo et al. (2011). Kneser Ney Language Modelling Recall our problem of efficient mgram language modeling backed by a corpus encoded in a succinct index. Although our method is generally applicable to many LM variants, we focus on the Kneser-Ney LM (Kneser and Ney, 1995), specifically the interpolated variant described in Chen and Goodman (1996), which has been shown to outperform other ngram LMs and has become the de-facto standard. Interpolated Kneser-Ney describes the conditional probability of a word wi conditioned on the context of m − 1 preceding words, wi−1 i−m+1, as P(wi |wi−m+1) = max [c(wi−mil) − Dm, 01 c(wi−m+1) DmN1+(wi−1 · ) i−m−1 c(i−1 wi−m+1) where lower-order smoothed probabilities are defined recursively (for 1 &lt; k &lt; m) as P¯(wi|wi−1 i−k+1) = N1+(·wi−1 · i−k+1 ) In the above formula, Dk is the kgram-specific discount parameter, and the occurr</context>
</contexts>
<marker>Kneser, Ney, 1995</marker>
<rawString>Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Proc. ICASSP, volume 1, pages 181–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1137" citStr="Koehn, 2010" startWordPosition="158" endWordPosition="159">ithout imposing a Markov condition. At its core, our approach uses a succinct index – a compressed suffix tree – which provides near optimal compression while supporting efficient search. We present algorithms for on-the-fly computation of probabilities under a Kneser-Ney language model. Our technique is exact and although slower than leading LM toolkits, it shows promising scaling properties, which we demonstrate through oc-order modeling over the full Wikipedia collection. 1 Introduction Language models (LMs) are critical components in many modern NLP systems, including machine translation (Koehn, 2010) and automatic speech recognition (Rabiner and Juang, 1993). The most widely used LMs are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, wi</context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Kurtz</author>
</authors>
<title>Reducing the space requirement of suffix trees.</title>
<date>1999</date>
<journal>Softw., Pract. Exper.,</journal>
<volume>29</volume>
<issue>13</issue>
<pages>1171</pages>
<contexts>
<context position="6500" citStr="Kurtz, 1999" startWordPosition="1102" endWordPosition="1103">eaf in the suffix tree of T . The suffix array requires n log n bits of space and can also be constructed in O(n) time (K¨arkk¨ainen et al., 2006). Using only the suffix array and the text, pattern search can be performed using binary search in O(m log n) time. For example, the pattern “the night” is found by performing binary search using SA and T to determine SA[18,19], the interval in SA corresponding the the suffixes in T prefixed by the pattern. In practice, suffix arrays use 4 − 8n bytes of space whereas the most efficient suffix tree implementations require at least 20n bytes of space (Kurtz, 1999) which are both Compressed Suffix Structures Reducing the space usage of suffix based index structure has recently become an active area of research. The space usage of a suffix array can be reduced significantly by utilizing the compressibility of text combined with succinct data structures. A succinct data structure provides the same functionality as an equivalent uncompressed data structure, but requires only space equivalent to the information-theoretic lower bound of the underlying data. For simplicity, we focus on the FM-Index which emulates the functionality of a suffix array over T usi</context>
</contexts>
<marker>Kurtz, 1999</marker>
<rawString>Stefan Kurtz. 1999. Reducing the space requirement of suffix trees. Softw., Pract. Exper., 29(13):1149– 1171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udi Manber</author>
<author>Eugene W Myers</author>
</authors>
<title>Suffix arrays: A new method for on-line string searches.</title>
<date>1993</date>
<journal>SIAM J. Comput.,</journal>
<volume>22</volume>
<issue>5</issue>
<contexts>
<context position="5755" citStr="Manber and Myers, 1993" startWordPosition="958" endWordPosition="961">e in the suffix tree are lexicographically ordered by their edge labels. The i-th smallest suffix in T corresponds to the path-label of the i-th leaf. The starting position of the suffix can be associated its corresponding leaf in the tree as shown in Figure 1a. All occurrences of α in T can be retrieved by visiting all leaves in the subtree of the locus of α. For example, pattern “the night” occurs at positions 12 and 19 in the sample text. We further refer the number of children of a node v as its degree and the number of leaves in the subtree rooted at v as the size of v. The suffix array (Manber and Myers, 1993) of T is an array SA[0 ... n − 1] such that SA[i] corresponds to the starting position of the i-th smallest suffix in T or the i-th leaf in the suffix tree of T . The suffix array requires n log n bits of space and can also be constructed in O(n) time (K¨arkk¨ainen et al., 2006). Using only the suffix array and the text, pattern search can be performed using binary search in O(m log n) time. For example, the pattern “the night” is found by performing binary search using SA and T to determine SA[18,19], the interval in SA corresponding the the suffixes in T prefixed by the pattern. In practice,</context>
</contexts>
<marker>Manber, Myers, 1993</marker>
<rawString>Udi Manber and Eugene W. Myers. 1993. Suffix arrays: A new method for on-line string searches. SIAM J. Comput., 22(5):935–948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian Munro</author>
</authors>
<title>Tables. In</title>
<date>1996</date>
<booktitle>Proc. FSTTCS,</booktitle>
<pages>37--42</pages>
<contexts>
<context position="10143" citStr="Munro, 1996" startWordPosition="1822" endWordPosition="1823">ep in the town# the night keeper keeps the keep in the night#$” with alphabet E={the, old, night, keeper, keeps, keep, in, town, #} and code words $=0000, #=0001, i=in=001, p=keep=010, r=keeper=011, s=keeps=1000, o=old=101, t=the=110, n=night=1001 and T=town=111. processing Tbwt to answer RANK efficiently. A wavelet tree can be used to answer RANK over Tbwt in O(log σ) time. The wavelet tree reduces RANK over an alphabet E into multiple RANK operations over a binary alphabet which can be answered in O(1) time and o(n) bits extra space by periodically storing absolute and relative RANK counts (Munro, 1996). The alphabet is reduced by recursively splitting symbols based on their code words into subgroups to form a binary tree as shown in Figure 1b for Tbwt. To answer RANK(Tbwt, i, c), the tree is traversed based on the code word of c, performing binary RANK at each level. For example, RANK(Tbwt, 17, ‘t’) translates to performing RANK(WTroot, 17,1) = 12 on the top level of the wavelet tree, as t=the=110. We recurse to the right subtree of the root node and compute RANK(WT1,12,1) as there were 12 ones in the root node and the next bit in the codeword of ‘the’ is also one. This process continues un</context>
</contexts>
<marker>Munro, 1996</marker>
<rawString>Ian Munro. 1996. Tables. In Proc. FSTTCS, pages 37–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Enno Ohlebusch</author>
<author>Johannes Fischer</author>
<author>Simon Gog</author>
</authors>
<date>2010</date>
<booktitle>CST++. In Proc. SPIRE,</booktitle>
<pages>322--333</pages>
<contexts>
<context position="12156" citStr="Ohlebusch et al. (2010)" startWordPosition="2187" endWordPosition="2190">offs which can be explored in practice (Gog et al., 2014). In the same way the FM-index emulates the functionality of the suffix array in little space, compressed suffix trees (CST) provide the functionality of suffix trees while requiring significantly less space than their uncompressed counterparts (Sadakane, 2007). A CST uses a compressed suffix array (CSA) such as the FM-Index but stores additional information to represent the shape of the suffix tree as well as information about path-labels. Again a variety of different storage schemes exist, however for simplicity we focus on the CST of Ohlebusch et al. (2010) which we use in our experiments. Here, the shape of the tree is stored using a balanced-parenthesis (BP) sequence which for a tree of p nodes requires ≈ 2p reduces to nH0(T)(1 + o(1)) bits which can be further be reduced to to nHk(T) + o(n log v) bits by using entropy compressed bitvectors. 2411 bits. Using little extra space and advanced bitoperations, the BP-sequence can be used to perform operations such as string-depth(v), parent(v) or accessing the i-th leaf can be answered in constant time. To support more advanced operations such as accessing path-labels, the underlying CSA or a compre</context>
</contexts>
<marker>Ohlebusch, Fischer, Gog, 2010</marker>
<rawString>Enno Ohlebusch, Johannes Fischer, and Simon Gog. 2010. CST++. In Proc. SPIRE, pages 322–333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Faster and smaller n-gram language models.</title>
<date>2011</date>
<booktitle>In Proc. ACL-HLT.</booktitle>
<contexts>
<context position="2025" citStr="Pauls and Klein, 2011" startWordPosition="302" endWordPosition="305">need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus). In contrast, we1 make use of recent advances in compressed suffix trees (CSTs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus)</context>
</contexts>
<marker>Pauls, Klein, 2011</marker>
<rawString>Adam Pauls and Dan Klein. 2011. Faster and smaller n-gram language models. In Proc. ACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Rabiner</author>
<author>Biing-Hwang Juang</author>
</authors>
<title>Fundamentals of speech recognition.</title>
<date>1993</date>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="1196" citStr="Rabiner and Juang, 1993" startWordPosition="164" endWordPosition="167">, our approach uses a succinct index – a compressed suffix tree – which provides near optimal compression while supporting efficient search. We present algorithms for on-the-fly computation of probabilities under a Kneser-Ney language model. Our technique is exact and although slower than leading LM toolkits, it shows promising scaling properties, which we demonstrate through oc-order modeling over the full Wikipedia collection. 1 Introduction Language models (LMs) are critical components in many modern NLP systems, including machine translation (Koehn, 2010) and automatic speech recognition (Rabiner and Juang, 1993). The most widely used LMs are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LM</context>
</contexts>
<marker>Rabiner, Juang, 1993</marker>
<rawString>Lawrence Rabiner and Biing-Hwang Juang. 1993. Fundamentals of speech recognition. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Russo</author>
<author>G Navarro</author>
<author>A Oliveira</author>
</authors>
<title>Fullycompressed suffix trees.</title>
<date>2011</date>
<journal>ACM Trans. Algorithms,</journal>
<volume>7</volume>
<issue>4</issue>
<pages>53</pages>
<contexts>
<context position="12982" citStr="Russo et al. (2011)" startWordPosition="2336" endWordPosition="2339">her be reduced to to nHk(T) + o(n log v) bits by using entropy compressed bitvectors. 2411 bits. Using little extra space and advanced bitoperations, the BP-sequence can be used to perform operations such as string-depth(v), parent(v) or accessing the i-th leaf can be answered in constant time. To support more advanced operations such as accessing path-labels, the underlying CSA or a compressed version of the LCP array are required which can be more expensive.3 In practice, a CST requires roughly 4 − 6n bits in addition to the cost of storing the CSA. For a more extensive overview of CSTs see Russo et al. (2011). Kneser Ney Language Modelling Recall our problem of efficient mgram language modeling backed by a corpus encoded in a succinct index. Although our method is generally applicable to many LM variants, we focus on the Kneser-Ney LM (Kneser and Ney, 1995), specifically the interpolated variant described in Chen and Goodman (1996), which has been shown to outperform other ngram LMs and has become the de-facto standard. Interpolated Kneser-Ney describes the conditional probability of a word wi conditioned on the context of m − 1 preceding words, wi−1 i−m+1, as P(wi |wi−m+1) = max [c(wi−mil) − Dm, </context>
</contexts>
<marker>Russo, Navarro, Oliveira, 2011</marker>
<rawString>L. Russo, G. Navarro, and A. Oliveira. 2011. Fullycompressed suffix trees. ACM Trans. Algorithms, 7(4):article 53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kunihiko Sadakane</author>
</authors>
<title>Compressed suffix trees with full functionality.</title>
<date>2007</date>
<journal>Theory Comput. Syst.,</journal>
<volume>41</volume>
<issue>4</issue>
<contexts>
<context position="2531" citStr="Sadakane, 2007" startWordPosition="390" endWordPosition="391"> et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus). In contrast, we1 make use of recent advances in compressed suffix trees (CSTs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracting frequency and unique context count statistics for mgram queries from CSTs, and two algorithms for computing Kneser-Ney LM probabilities on the fly using these statistics. The first method uses two CSTs (over the corpus and the reversed corpus), which allow for efficient computation of the number of unique contexts to the left and right of an mgram, but is inefficient in several ways, most notably when computing the number of unique contexts to both sides. Our second</context>
<context position="11851" citStr="Sadakane, 2007" startWordPosition="2135" endWordPosition="2136">ampled using a given sample rate SAS (e.g. 32) such that SA[i] mod SAS = 0. Then, for any SA[i] or SA−1[i], at most O(SAS) RANK operations on Tbwt are required to access the value. Different sample rates, bitvector implementations and wavelet tree types result in a wide variety of timespace tradeoffs which can be explored in practice (Gog et al., 2014). In the same way the FM-index emulates the functionality of the suffix array in little space, compressed suffix trees (CST) provide the functionality of suffix trees while requiring significantly less space than their uncompressed counterparts (Sadakane, 2007). A CST uses a compressed suffix array (CSA) such as the FM-Index but stores additional information to represent the shape of the suffix tree as well as information about path-labels. Again a variety of different storage schemes exist, however for simplicity we focus on the CST of Ohlebusch et al. (2010) which we use in our experiments. Here, the shape of the tree is stored using a balanced-parenthesis (BP) sequence which for a tree of p nodes requires ≈ 2p reduces to nH0(T)(1 + o(1)) bits which can be further be reduced to to nHk(T) + o(n log v) bits by using entropy compressed bitvectors. 24</context>
</contexts>
<marker>Sadakane, 2007</marker>
<rawString>Kunihiko Sadakane. 2007. Compressed suffix trees with full functionality. Theory Comput. Syst., 41(4):589–607.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Schnattinger</author>
<author>Enno Ohlebusch</author>
<author>Simon Gog</author>
</authors>
<title>Bidirectional search in a string with wavelet trees.</title>
<date>2010</date>
<booktitle>In Proc. CPM,</booktitle>
<pages>40--50</pages>
<contexts>
<context position="26323" citStr="Schnattinger et al., 2010" startWordPosition="4708" endWordPosition="4711">s, N1+(·α) and N1+(·α·) are computed directly from a single forward CST. This confers the benefit of using only backward search and avoiding redundant searches for the same pattern (cf. lines 9 and 11 in Algorithm 2). The full algorithm for computing LM probabilities is given in Algorithm 4, however for space reasons we will not describe this in detail. Instead we will focus on the method’s most critical component, the algorithm for computing N1+(·α·) from the forward CST, presented in Algorithm 5. The key difference from Algorithm 1 is the loop from lines 6–9, which uses the intervalsymbols (Schnattinger et al., 2010) method. This method assumes a wavelet tree representation of the SA component of the CST, an efficient encoding of the BWT as describes in section 2. The interval-symbols method uses RANK operations to efficiently identify for a given pattern the set of preceding symbols P(α) and the ranges SA[ls, rs] corresponding to the patterns sα for all s E P(α) 10Discounts are computed up to a limit on mgram size, here set to 10. The highest order values are used for computing the discount of mgrams above the limit at runtime. Algorithm 4 KN probability P(wk|wk−(m−1)) using a single CST 2: vF ← root(tF)</context>
</contexts>
<marker>Schnattinger, Ohlebusch, Gog, 2010</marker>
<rawString>Thomas Schnattinger, Enno Ohlebusch, and Simon Gog. 2010. Bidirectional search in a string with wavelet trees. In Proc. CPM, pages 40–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Sorensen</author>
<author>Cyril Allauzen</author>
</authors>
<title>Unary data structures for language models.</title>
<date>2011</date>
<booktitle>In Proc. INTERSPEECH,</booktitle>
<pages>1425--1428</pages>
<contexts>
<context position="2054" citStr="Sorensen and Allauzen, 2011" startWordPosition="306" endWordPosition="309">urate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus). In contrast, we1 make use of recent advances in compressed suffix trees (CSTs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extr</context>
</contexts>
<marker>Sorensen, Allauzen, 2011</marker>
<rawString>Jeffrey Sorensen and Cyril Allauzen. 2011. Unary data structures for language models. In Proc. INTERSPEECH, pages 1425–1428.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Jing Zheng</author>
<author>Wen Wang</author>
<author>Victor Abrash</author>
</authors>
<title>Srilm at sixteen: Update and outlook.</title>
<date>2011</date>
<booktitle>In Proc. ASRU,</booktitle>
<pages>5</pages>
<contexts>
<context position="1930" citStr="Stolcke et al., 2011" startWordPosition="287" endWordPosition="290">ir counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus). In contrast, we1 make use of recent advances in compressed suffix trees (CSTs) (Sadakane, 2007</context>
</contexts>
<marker>Stolcke, Zheng, Wang, Abrash, 2011</marker>
<rawString>Andreas Stolcke, Jing Zheng, Wen Wang, and Victor Abrash. 2011. Srilm at sixteen: Update and outlook. In Proc. ASRU, page 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM–an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. INTERSPEECH.</booktitle>
<contexts>
<context position="31072" citStr="Stolcke, 2002" startWordPosition="5577" endWordPosition="5578">nd excluding XML markup. The first 10k sentences were used as the test data, and the last 80% as the training data, giving rise to training corpora of between 8M and 50M tokens and uncompressed size of up to 200 MiB (see Table 1 for detailed corpus statistics). We also processed the full 52 GiB uncompressed “20150205” English Wikipedia articles dump to create a character level language model consisting of 72M sentences. We excluded 10k random sentences from the collection as test data. We use the SDSL library (Gog et al., 2014) to implement all our structures and compare our indexes to SRILM (Stolcke, 2002). We refer to our dual-CST approach as D-CST, and the single-CST as S-CST. We evaluated the perplexity across different languages and using mgrams of varying order from m = 2 to oo (unbounded), as shown on Figure 3. Our results matched the perplexity results from SRILM (for smaller values of m in which SRILM training was feasible, m &lt; 10). Note that perplexity drops dramatically from m = 2 ... 5 however the gains thereafter are modest for most languages. Despite this, several large mgram matches were found ranging in size up to a 34-gram match. We speculate that the perplexity plateau is due t</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM–an extensible language modeling toolkit. In Proc. INTERSPEECH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Talbot</author>
<author>Miles Osborne</author>
</authors>
<title>Randomised language modelling for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1846" citStr="Talbot and Osborne, 2007" startWordPosition="273" endWordPosition="276"> are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus). In contras</context>
</contexts>
<marker>Talbot, Osborne, 2007</marker>
<rawString>David Talbot and Miles Osborne. 2007. Randomised language modelling for statistical machine translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Esko Ukkonen</author>
</authors>
<title>On-line construction of suffix trees.</title>
<date>1995</date>
<journal>Algorithmica,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="5106" citStr="Ukkonen, 1995" startWordPosition="832" endWordPosition="834">erminating symbol not in E. The path-label of each node v corresponds to the concatenation of edge labels from the root node to v. The node depth of v corresponds to the number of ancestors in the tree, whereas the string depth corresponds to the length of the path-label. Searching for a pattern α of size m in T translates to finding the locus node v closest to the root such that α is a prefix of the path-label of v in O(m) time. We refer to this approach as forward search. Figure 1a shows a suffix tree over a sample text. A suffix tree requires O(n) space and can be constructed in O(n) time (Ukkonen, 1995). The children of each node in the suffix tree are lexicographically ordered by their edge labels. The i-th smallest suffix in T corresponds to the path-label of the i-th leaf. The starting position of the suffix can be associated its corresponding leaf in the tree as shown in Figure 1a. All occurrences of α in T can be retrieved by visiting all leaves in the subtree of the locus of α. For example, pattern “the night” occurs at positions 12 and 19 in the sample text. We further refer the number of children of a node v as its degree and the number of leaves in the subtree rooted at v as the siz</context>
</contexts>
<marker>Ukkonen, 1995</marker>
<rawString>Esko Ukkonen. 1995. On-line construction of suffix trees. Algorithmica, 14(3):249–260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>A succinct n-gram language model.</title>
<date>2009</date>
<booktitle>In Proc. ACL Short Papers,</booktitle>
<pages>341--344</pages>
<contexts>
<context position="2078" citStr="Watanabe et al., 2009" startWordPosition="310" endWordPosition="313">ct. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; Heafield, 2011; Pauls and Klein, 2011; Sorensen and Allauzen, 2011; Watanabe et al., 2009). However, none of these approaches scale well to very high-order m or very large corpora, due to their high memory and time requirements. An important exception is Kennington et al. (2012), who also propose a language model based on a suffix tree which scales well with m but poorly with the corpus size (requiring memory of about 20× the training corpus). In contrast, we1 make use of recent advances in compressed suffix trees (CSTs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracting frequency and uni</context>
</contexts>
<marker>Watanabe, Tsukada, Isozaki, 2009</marker>
<rawString>Taro Watanabe, Hajime Tsukada, and Hideki Isozaki. 2009. A succinct n-gram language model. In Proc. ACL Short Papers, pages 341–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Weiner</author>
</authors>
<title>Linear pattern matching algorithms.</title>
<date>1973</date>
<booktitle>In Proc. SWAT,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="4365" citStr="Weiner, 1973" startWordPosition="685" endWordPosition="686">es have 1For the implementation see: https://github.com/eehsan/ lm-sdsl. 2409 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2409–2418, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. much larger than T and prohibit the use of these structures for all but small data sets. for language modelling and other ‘big data’ problems in language processing. 2 Background Suffix Arrays and Suffix Trees Let T be a string of size n drawn from an alphabet E of size σ. Let T [i..n − 1] be a suffix of T . The suffix tree (Weiner, 1973) of T is the compact labeled tree of n + 1 leaves where the root to leaf paths correspond to all suffixes of T$, where $ is a terminating symbol not in E. The path-label of each node v corresponds to the concatenation of edge labels from the root node to v. The node depth of v corresponds to the number of ancestors in the tree, whereas the string depth corresponds to the length of the path-label. Searching for a pattern α of size m in T translates to finding the locus node v closest to the root such that α is a prefix of the path-label of v in O(m) time. We refer to this approach as forward se</context>
</contexts>
<marker>Weiner, 1973</marker>
<rawString>Peter Weiner. 1973. Linear pattern matching algorithms. In Proc. SWAT, pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Wood</author>
<author>Jan Gasthaus</author>
<author>C´edric Archambeau</author>
<author>Lancelot James</author>
<author>Yee Whye Teh</author>
</authors>
<title>The sequence memoizer.</title>
<date>2011</date>
<journal>CACM,</journal>
<volume>54</volume>
<issue>2</issue>
<contexts>
<context position="31982" citStr="Wood et al., 2011" startWordPosition="5728" endWordPosition="5731"> m in which SRILM training was feasible, m &lt; 10). Note that perplexity drops dramatically from m = 2 ... 5 however the gains thereafter are modest for most languages. Despite this, several large mgram matches were found ranging in size up to a 34-gram match. We speculate that the perplexity plateau is due to the simplistic Kneser-Ney discounting formula which is not designed for higher order mgram LMs and appear to discount large mgrams too aggressively. We leave further exploration of richer discounting techniques such as Modified Kneser-Ney (Chen and Goodman, 1996) or the Sequence Memoizer (Wood et al., 2011) to our future work. Figure 4 compares space and time of our indexes with SRILM on the German part of Europarl. The construction cost of our indexes in terms of both space and time is comparable to that of a 3/4-gram SRILM index. The space usage of D-CST index is comparable to a compact 3-gram SRILM index. Our S-CST index uses only 177 MiB RAM at query time, which is comparable to the size of the collection (172 MiB). However, query processing is significantly slower for both our structures. For 2-grams, D-CST is 3 times slower than a 2-gram SRILM index as the expensive N1+(·α·) is not compute</context>
</contexts>
<marker>Wood, Gasthaus, Archambeau, James, Teh, 2011</marker>
<rawString>Frank Wood, Jan Gasthaus, C´edric Archambeau, Lancelot James, and Yee Whye Teh. 2011. The sequence memoizer. CACM, 54(2):91–98.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
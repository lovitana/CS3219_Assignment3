<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000024">
<title confidence="0.9924315">
When Are Tree Structures Necessary for Deep Learning of
Representations?
</title>
<author confidence="0.999512">
Jiwei Li&apos;, Minh-Thang Luong&apos;, Dan Jurafsky&apos; and Eduard Hovy2
</author>
<affiliation confidence="0.999318">
&apos;Computer Science Department, Stanford University, Stanford, CA 94305
2Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213
</affiliation>
<email confidence="0.998596">
jiweil,lmthang,jurafsky@stanford.edu ehovy@andrew.cmu.edu
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919733333333">
Recursive neural models, which use syn-
tactic parse trees to recursively generate
representations bottom-up, are a popular
architecture. However there have not been
rigorous evaluations showing for exactly
which tasks this syntax-based method is
appropriate. In this paper, we benchmark
recursive neural models against sequential
recurrent neural models, enforcing apples-
to-apples comparison as much as possible.
We investigate 4 tasks: (1) sentiment clas-
sification at the sentence level and phrase
level; (2) matching questions to answer-
phrases; (3) discourse parsing; (4) seman-
tic relation extraction.
Our goal is to understand better when,
and why, recursive models can outperform
simpler models. We find that recursive
models help mainly on tasks (like seman-
tic relation extraction) that require long-
distance connection modeling, particularly
on very long sequences. We then intro-
duce a method for allowing recurrent mod-
els to achieve similar performance: break-
ing long sentences into clause-like units
at punctuation and processing them sepa-
rately before combining. Our results thus
help understand the limitations of both
classes of models, and suggest directions
for improving recurrent models.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998933234042553">
Deep learning based methods learn low-
dimensional, real-valued vectors for word
tokens, mostly from large-scale data corpus (e.g.,
(Mikolov et al., 2013; Le and Mikolov, 2014;
Collobert et al., 2011)), successfully capturing
syntactic and semantic aspects of text.
For tasks where the inputs are larger text units
(e.g., phrases, sentences or documents), a compo-
sitional model is first needed to aggregate tokens
into a vector with fixed dimensionality that can be
used as a feature for other NLP tasks. Models for
achieving this usually fall into two categories: re-
current models and recursive models:
Recurrent models (also referred to as sequence
models) deal successfully with time-series data
(Pearlmutter, 1989; Dorffner, 1996) like speech
(Robinson et al., 1996; Lippmann, 1989; Graves et
al., 2013) or handwriting recognition (Graves and
Schmidhuber, 2009; Graves, 2012). They were ap-
plied early on to NLP (Elman, 1990), by modeling
a sentence as tokens processed sequentially and at
each step combining the current token with pre-
viously built embeddings. Recurrent models can
be extended to bidirectional ones from both left-
to-right and right-to-left. These models generally
consider no linguistic structure aside from word
order.
Recursive neural models (also referred to as tree
models), by contrast, are structured by syntactic
parse trees. Instead of considering tokens sequen-
tially, recursive models combine neighbors based
on the recursive structure of parse trees, starting
from the leaves and proceeding recursively in a
bottom-up fashion until the root of the parse tree
is reached. For example, for the phrase the food
is delicious, following the operation sequence (
(the food) (is delicious) ) rather than the sequen-
tial order (((the food) is) delicious). Many recur-
sive models have been proposed (e.g., (Paulus et
al., 2014; Irsoy and Cardie, 2014)), and applied to
various NLP tasks, among them entailment (Bow-
man, 2013; Bowman et al., 2014), sentiment anal-
ysis (Socher et al., 2013; Irsoy and Cardie, 2013;
Dong et al., 2014), question-answering (Iyyer et
al., 2014), relation classification (Socher et al.,
2012; Hashimoto et al., 2013), and discourse (Li
and Hovy, 2014).
</bodyText>
<page confidence="0.951159">
2304
</page>
<note confidence="0.9847485">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9999624">
One possible advantage of recursive models is
their potential for capturing long-distance depen-
dencies: two tokens may be structurally close to
each other, even though they are far away in word
sequence. For example, a verb and its correspond-
ing direct object can be far away in terms of to-
kens if many adjectives lies in between, but they
are adjacent in the parse tree (Irsoy and Cardie,
2013). However we do not know if this advan-
tage is truly important, and if so for which tasks,
or whether other issues are at play. Indeed, the
reliance of recursive models on parsing is also a
potential disadvantage, given that parsing is rela-
tively slow, domain-dependent, and can be error-
ful.
On the other hand, recent progress in multi-
ple subfields of neural NLP has suggested that re-
current nets may be sufficient to deal with many
of the tasks for which recursive models have
been proposed. Recurrent models without parse
structures have shown good results in sequence-
to-sequence generation (Sutskever et al., 2014)
for machine translation (e.g., (Kalchbrenner and
Blunsom, 2013; 3; Luong et al., 2014)), pars-
ing (Vinyals et al., 2014), and sentiment, where
for example recurrent-based paragraph vectors (Le
and Mikolov, 2014) outperform recursive models
(Socher et al., 2013) on the Stanford sentiment-
bank dataset.
Our goal in this paper is thus to investigate a
number of tasks with the goal of understanding
for which kinds of problems recurrent models may
be sufficient, and for which kinds recursive mod-
els offer specific advantages. We investigate four
tasks with different properties.
</bodyText>
<listItem confidence="0.98150959375">
• Binary sentiment classification at the sen-
tence level (Pang et al., 2002) and phrase
level (Socher et al., 2013) that focus on
understanding the role of recursive models
in dealing with semantic compositionally in
various scenarios such as different lengths of
inputs and whether or not supervision is com-
prehensive.
• Phrase Matching on the UMD-QA dataset
(Iyyer et al., 2014) can help see the difference
between outputs from intermediate compo-
nents from different models, i.e., representa-
tions for intermediate parse tree nodes and
outputs from recurrent models at different
time steps. It also helps see whether pars-
ing is useful for finding similarities between
question sentences and target phrases.
• Semantic Relation Classification on the
SemEval-2010 (Hendrickx et al., 2009) data
can help understand whether parsing is help-
ful in dealing with long-term dependencies,
such as relations between two words that are
far apart in the sequence.
• Discourse parsing (RST dataset) is useful
for measuring the extent to which parsing im-
proves discourse tasks that need to combine
meanings of larger text units. Discourse pars-
ing treats elementary discourse units (EDUs)
as basic units to operate on, which are usually
short clauses. The task also sheds light on
the extent to which syntactic structures help
acquire shot text representations.
</listItem>
<bodyText confidence="0.999903933333333">
The principal motivation for this paper is to un-
derstand better when, and why, recursive models
are needed to outperform simpler models by en-
forcing apples-to-apples comparison as much as
possible. This paper applies existing models to
existing tasks, barely offering novel algorithms or
tasks. Our goal is rather an analytic one, to inves-
tigate different versions of recursive and recurrent
models. This work helps understand the limita-
tions of both classes of models, and suggest direc-
tions for improving recurrent models.
The rest of this paper organized as follows: We
detail versions of recursive/recurrent models in
Section 2, present the tasks and results in Section
3, and conclude with discussions in Section 4.
</bodyText>
<sectionHeader confidence="0.987915" genericHeader="introduction">
2 Recursive and Recurrent Models
</sectionHeader>
<subsectionHeader confidence="0.905515">
2.1 Notations
</subsectionHeader>
<bodyText confidence="0.967747857142857">
We assume that the text unit S, which could
be a phrase, a sentence or a document, is com-
prised of a sequence of tokens/words: S =
{w1, w2, ..., wNS}, where Ns denotes the num-
ber of tokens in S. Each word w is associated
with a K-dimensional vector embedding ew =
{e1 w, e2 w, ..., eK w }. The goal of recursive and re-
current models is to map the sequence to a K-
dimensional eS, based on its tokens and their cor-
respondent embeddings.
Standard Recurrent/Sequence Models suc-
cessively take word wt at step t, combines its vec-
tor representation et with the previously built hid-
den vector ht−1 from time t − 1, calculates the re-
</bodyText>
<page confidence="0.74884">
2305
</page>
<bodyText confidence="0.997538647058823">
sulting current embedding ht, and passes it to the ⎡ it 1[ σ ⎤
next step. The embedding ht for the current time t ⎢ ⎢ ⎣ ft σ W - r ht−1 1 (5)
is thus: ot σ L et J
ht = f(W - ht−1 + V - et) (1) lt tanh
where W and V denote compositional matrices. If
Ns denotes the length of the sequence, hNs repre-
sents the whole sequence S.
Standard recursive/Tree models work in a
similar way, but processing neighboring words by
parse tree order rather than sequence order. It
computes a representation for each parent node
based on its immediate children recursively in a
bottom-up fashion until reaching the root of the
tree. For a given node η in the tree and its left child
ηleft (with representation eleft) and right child ηright
(with representation eright), the standard recursive
network calculates eη as follows:
</bodyText>
<equation confidence="0.994035">
eη = f(W - eηleft + V - eηright) (2)
</equation>
<bodyText confidence="0.997198888888889">
Bidirectional Models (Schuster and Paliwal,
1997) add bidirectionality to the recurrent frame-
work where embeddings for each time are calcu-
lated both forwardly and backwardly:
Normally, final representations for sentences can
be achieved either by concatenating vectors calcu-
lated from both directions [e←1 , e→NS] or using fur-
ther compositional operation to preserve vector di-
mensionality
</bodyText>
<equation confidence="0.957804">
ht = f(WL - [h←t , h→t ]) (4)
</equation>
<bodyText confidence="0.991816846153846">
where WL denotes a K x 2K dimensional matrix.
Long Short Term Memory (LSTM) LSTM
models (Hochreiter and Schmidhuber, 1997) are
defined as follows: given a sequence of inputs
X = {x1, x2, ..., xnX }, an LSTM associates each
timestep with an input, memory and output gate,
respectively denoted as it, ft and ot. We notation-
ally disambiguate e and h: et denotes the vector
for individual text units (e.g., word or sentence) at
time step t, while ht denotes the vector computed
by the LSTM model at time t by combining et and
ht−1. σ denotes the sigmoid function. The vector
representation ht for each time-step t is given by:
</bodyText>
<equation confidence="0.997136">
ct = ft - ct−1 + it - lt (6)
hst = ot - ct (7)
</equation>
<bodyText confidence="0.993488777777778">
where W E R4K×2K. Labels at the
phrase/sentence level are predicted representations
outputted from the last time step.
Tree LSTMs Recent research has extended the
LSTM idea to tree-based structures (Zhu et al.,
2015; Tai et al., 2015) that associate memory and
forget gates to nodes of the parse trees.
Bi-directional LSTMs These combine bi-
directional models and LSTMs.
</bodyText>
<sectionHeader confidence="0.999696" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.963685">
In this section, we detail our experimental settings
and results. We consider the following tasks, each
representative of a different class of NLP tasks.
</bodyText>
<listItem confidence="0.994753565217391">
• Binary sentiment classification on the Pang
et al. (2002) dataset. This addresses the is-
sues where supervision only appears globally
after a long sequence of operations.
• Sentiment Classification on the Stanford
Sentiment Treebank (Socher et al., 2013):
comprehensive labels are found for words
and phrases where local compositionally
(such as from negation, mood, or others cued
by phrase-structure) is to be learned.
• Sentence-Target Matching on the UMD-
QA dataset (Iyyer et al., 2014): Learns
matches between target and components in
the source sentences, which are parse tree
nodes for recursive models and different
time-steps for recurrent models.
• Semantic Relation Classification on the
SemEval-2010 task (Hendrickx et al., 2009).
Learns long-distance relationships between
two words that may be far apart sequentially.
• Discourse Parsing (Li et al., 2014; Hernault
et al., 2010): Learns sentence-to-sentence re-
lations based on calculated representations.
</listItem>
<equation confidence="0.994283">
h← t = f(W← - h← t+1 + V ← - et) (3)
h→t = f(W→ - h→ t−1 + V → - et)
</equation>
<page confidence="0.911968">
2306
</page>
<bodyText confidence="0.993617333333333">
In each case we followed the protocols de-
scribed in the original papers. We first group the
algorithm variants into two groups as follows:
</bodyText>
<listItem confidence="0.9996184">
• Standard tree models vs standard sequence
models vs standard bi-directional sequence
models
• LSTM tree models, LSTM sequence models
vs LSTM bi-directional sequence models.
</listItem>
<bodyText confidence="0.999970052631579">
We employed standard training frameworks for
neural models: for each task, we used stochas-
tic gradient decent using AdaGrad (Duchi et al.,
2011) with minibatches (Cotter et al., 2011). Pa-
rameters are tuned using the development dataset
if available in the original datasets or from cross-
validation if not. Derivatives are calculated from
standard back-propagation (Goller and Kuchler,
1996). Parameters to tune include size of mini
batches, learning rate, and parameters for L2 pe-
nalizations. The number of running iterations
is treated as a parameter to tune and the model
achieving best performance on the development
set is used as the final model to be evaluated.
For settings where no repeated experiments are
performed, the bootstrap test is adopted for sta-
tistical significance testing (Efron and Tibshirani,
1994). Test scores that achieve significance level
of 0.05 are marked by an asterisk (*).
</bodyText>
<subsectionHeader confidence="0.999455">
3.1 Stanford Sentiment TreeBank
</subsectionHeader>
<bodyText confidence="0.999454923076923">
Task Description We start with the Stanford
Sentiment TreeBank (Socher et al., 2013). This
dataset contains gold-standard labels for every
parse tree constituent, from the sentence to phrases
to individual words.
Of course, any conclusions drawn from imple-
menting sequence models on a dataset that was
based on parse trees may have to be weakened,
since sequence models may still benefit from the
way that the dataset was collected. Nevertheless
we add an evaluation on this dataset because it has
been a widely used benchmark dataset for neural
model evaluations.
For recursive models, we followed the proto-
cols in Socher et al. (2013) where node embed-
dings in the parse trees are obtained from recur-
sive models and then fed to a softmax classifier.
We transformed the dataset for recurrent model
use as illustrated in Figure 1. Each phrase is recon-
structed from parse tree nodes and treated as a sep-
arate data point. As the treebank contains 11,855
sentences with 215,154 phrases, the reconstructed
dataset for recurrent models comprises 215,154
examples. Models are evaluated at both the phrase
level (82,600 instances) and the sentence root level
(2,210 instances).
</bodyText>
<table confidence="0.9927695">
Fine-Grained Binary
Tree 0.433 0.815
Sequence 0.420 (-0.013) 0.807 (-0.007)
P-value 0.042* 0.098
Bi-Sequence 0.435 (+0.08) 0.816 (+0.002)
P-value 0.078 0.210
</table>
<tableCaption confidence="0.9918605">
Table 1: Test set accuracies on the Stanford Senti-
ment Treebank at root level.
</tableCaption>
<table confidence="0.9967985">
Fine-Grained Binary
Tree 0.820 0.860
Sequence 0.818 (-0.002) 0.864 (+0.004)
P-value 0.486 0.305
Bi-Sequence 0.826 (+0.06) 0.862 (+0.002)
P-value 0.148 0.450
</table>
<tableCaption confidence="0.7563835">
Table 2: Test set accuracies on the Stanford Senti-
ment Treebank at phrase level.
</tableCaption>
<bodyText confidence="0.9980725">
Results are shown in Table 1 and 21. When
comparing the standard version of tree models
to sequence models, we find it helps a bit at
root level identification (for sequences but not bi-
sequences), but yields no significant improvement
at the phrase level.
LSTM Tai et al. (2015) discovered that LSTM
tree models generate better performances in terms
of sentence root level evaluation than sequence
models. We explore this task a bit more by training
deeper and more sophisticated models. We exam-
ine the following three models:
</bodyText>
<listItem confidence="0.9933798">
1. Tree-structured LSTM models (Tai et al.,
2015)2.
2. Deep Bi-LSTM sequence models (denoted as
Sequence) that treat the whole sentence as
just one sequence.
3. Deep Bi-LSTM hierarchical sequence mod-
els (denoted as Hierarchical Sequence) that
first slice the sentence into a sequence of sub-
sentences by using a look-up table of punc-
tuations (i.e., comma, period, question mark
</listItem>
<footnote confidence="0.986455166666667">
1The performance of our implementations of recursive
models is not exactly identical to that reported in Socher et
al. (2013), but the relative difference is around 1% to 2%.
2Tai et al.. achieved 0.510 accuracy in terms of fine-
grained evaluation at the root level as reported in (Tai et al.,
2015), similar to results from our implementations (0.504).
</footnote>
<page confidence="0.99037">
2307
</page>
<figureCaption confidence="0.92637775">
Figure 1: Transforming Stanford Sentiment Treebank to Sequences for Sequence Models.
Figure 2: Illustration of two sequence models. A,
B, C, D denote clauses or sub sentences separated
by punctuation.
</figureCaption>
<bodyText confidence="0.999329576923077">
and exclamation mark). The representation
for each sub-sentence is first computed sep-
arately, and another level of sequence LSTM
(one-directional) is then used to join the sub-
sentences. Illustrations are shown in Figure2.
We consider the third model because the dataset
used in Tai et al. (2015) contains long sentences
and the evaluation is performed only at the sen-
tence root level. Since a parsing algorithm will
naturally break long sentences into sub-sentences,
we would like to know whether any performance
boost is introduced by the intra-clause parse tree
structure or just by this broader segmentation of a
sentence into clause-like units; this latter advan-
tage could be approximated by using punctuation-
based approximations to clause boundaries.
We run 15 iterations for each algorithm. Pa-
rameters are harvested at the end of each itera-
tion; those performing best on the development
set are used on the test set. The whole process
takes roughly 15-20 minutes on a single GPU ma-
chine3. For a more convincing comparison, we
did not use the bootstrap test where parallel ex-
amples are generated from one same dataset. In-
stead, we repeated the aforementioned procedure
for each algorithm 20 times and report accuracies
</bodyText>
<footnote confidence="0.684551">
3Tesla K40m, 2880 Cuda cores.
</footnote>
<table confidence="0.8995454">
with standard deviation in Table 3.
Model all-fine root-fine root-coarse
Tree LSTM 83.4 (0.3) 50.4 (0.9) 86.7 (0.5)
Bi-Sequence 83.3 (0.4) 49.8 (0.9) 86.7 (0.5)
Hier-Sequence 82.9 (0.3) 50.7 (0.8) 86.9 (0.6)
</table>
<tableCaption confidence="0.999114">
Table 3: Test set accuracies on the Stanford Sen-
</tableCaption>
<bodyText confidence="0.992619947368421">
timent Treebank with deviations. For our exper-
iments, we report accuracies over 20 runs with
standard deviation.
Tree LSTMs are equivalent or marginally bet-
ter than standard bi-directional sequence model
(two-tailed p-value equals 0.041*, and only at the
root level, with p-value for the phrase level at
0.376). The hierarchical sequence model achieves
the same performance with a p-value of 0.198.
Discussion The results above suggest that
clausal segmentation of long sentences offers a
slight performance boost, a result also supported
by the fact that very little difference exists between
the three models for phrase-level sentiment eval-
uation. Clausal segmentation of long sentences
thus provides a simple approximation to parse-tree
based models.
We suggest a few reasons for this slightly better
performances introduced by clausal segmentation:
</bodyText>
<listItem confidence="0.977975181818182">
1. Treating clauses as basic units (to the extent
that punctuation approximates clauses) pre-
serves the semantic structure of text.
2. Semantic compositions such as negations or
conjunctions usually appear at the clause
level. Working on clauses individually
and then combining them model inter-clause
compositions.
3. Errors are back-propagated to individual to-
kens using fewer steps in hierarchical models
than in standard models. Consider a movie
</listItem>
<page confidence="0.987582">
2308
</page>
<figureCaption confidence="0.978052">
Figure 3: Sentiment prediction using a one-
</figureCaption>
<bodyText confidence="0.925845">
directional (left to right) LSTM. Decisions at each
time step are made by feeding embeddings calcu-
lated from the LSTM into a softmax classifier.
review “simple as the plot was , i still like it a
lot”. With standard recurrent models it takes
12 steps before the prediction error gets back
to the first token “simple”:
error—*lot—*a—*it—*like—*still—*i—*,—*was
—*plot—* the—*as—*simple
In a hierarchical model, the second clause is
compacted into one component, and the error
propagation is thus given by:
error—* second-clause —* first-clause —*
was—*plot—*the—*as—*simple.
Propagation with clause segmentation con-
sists of only 8 operations. Such a procedure
thus tends to attenuate the gradient vanish-
ing problem, potentially yielding better per-
formance.
</bodyText>
<subsectionHeader confidence="0.998116">
3.2 Binary Sentiment Classification (Pang)
</subsectionHeader>
<bodyText confidence="0.9918029375">
Task Description: The sentiment dataset
of Pang et al. (2002) consists of sentences
with a sentiment label for each sentence.
We divide the original dataset into train-
ing(8101)/dev(500)/testing(2000). No pre-
training procedure as described in Socher et al.
(2011b) is employed. Word embeddings are
initialized using skip-grams and kept fixed in
the learning procedure. We trained skip-gram
embeddings on the Wikipedia+Gigaword dataset
using the word2vec package4. Sentence level
embeddings are fed into a sigmoid classifier.
Performances for 50 dimensional vectors are
given in the table below:
Discussion Why don’t parse trees help on this
task? One possible explanation is the distance
</bodyText>
<footnote confidence="0.960906">
4https://code.google.com/p/word2vec/
</footnote>
<table confidence="0.999350833333333">
Standard LSTM
Tree 0.745 0.774
Sequence 0.733 (-0.012) 0.783 (+0.008)
P-value 0.060 0.136
Bi-Sequence 0.754 (+0.09) 0.790 (+0.016)
P-value 0.058 0.024*
</table>
<tableCaption confidence="0.924446">
Table 4: Test set accuracies on the Pang’s senti-
ment dataset using Standard model settings.
</tableCaption>
<bodyText confidence="0.998227272727273">
of the supervision signal from the local composi-
tional structure. The Pang et al. dataset has an av-
erage sentence length of 22.5 words, which means
it takes multiple steps before sentiment related ev-
idence comes up to the surface. It is therefore un-
clear whether local compositional operators (such
as negation) can be learned; there is only a small
amount of training data (around 8,000 examples)
and the sentiment supervision only at the level of
the sentence may not be easy to propagate down to
deeply buried local phrases.
</bodyText>
<subsectionHeader confidence="0.967555">
3.3 Question-Answer Matching
</subsectionHeader>
<bodyText confidence="0.99846372">
Task Description: In the question-answering
dataset QANTA5, each answer is a token or short
phrase. The task is different from standard gener-
ation focused QA task but formalized as a multi-
class classification task that matches a source
question with a candidates phrase from a prede-
fined pool of candidate phrases We give an illus-
trative example here:
Question: He left unfinished a novel whose title
character forges his father’s signature to get out
of school and avoids the draft by feigning desire
to join. Name this German author of The Magic
Mountain and Death in Venice.
Answer: Thomas Mann from the pool of
phrases. Other candidates might include George
Washington, Charlie Chaplin, etc.
The model of Iyyer et al. (2014) minimizes the
distances between answer embeddings and node
embeddings along the parse tree of the question.
Concretely, let c denote the correct answer to ques-
tion S, with embedding c, and z denoting any ran-
dom wrong answer. The objective function sums
over the dot product between representation for
every node q along the question parse trees and
the answer representations:
</bodyText>
<equation confidence="0.9469825">
�L = � max(0,1−c·en+z·en) (8)
n∈[parse tree] z
</equation>
<footnote confidence="0.9982275">
5http://cs.umd.edu/˜miyyer/qblearn/. Be-
cause the publicly released dataset is smaller than the version
used in (Iyyer et al., 2014) due to privacy issues, our numbers
are not comparable to those in (Iyyer et al., 2014).
</footnote>
<page confidence="0.99717">
2309
</page>
<bodyText confidence="0.999976625">
where eη denotes the embedding for parse tree
node calculated from the recursive neural model.
Here the parse trees are dependency parses follow-
ing (Iyyer et al., 2014).
By adjusting the framework to recurrent mod-
els, we minimize the distance between the answer
embedding and the embeddings calculated from
each timestep t of the sequence:
</bodyText>
<equation confidence="0.9991935">
�L = � max(0,1 − c� · et + z� · et) (9)
t∈[1,N3] z
</equation>
<bodyText confidence="0.979976428571429">
At test time, the model chooses the answer (from
the set of candidates) that gives the lowest loss
score. As can be seen from results presented in
Table 5, the difference is only significant for the
LSTM setting between the tree model and the
sequence model; no significant difference is ob-
served for other settings.
</bodyText>
<table confidence="0.990153333333333">
Standard LSTM
Tree 0.523 0.558
Sequence 0.525 (+0.002) 0.546 (-0.012)
P-value 0.490 0.046*
Bi-Sequence 0.530 (+0.007) 0.564 (+0.006)
P-value 0.075 0.120
</table>
<tableCaption confidence="0.998903">
Table 5: Test set accuracies for UMD-QA dataset.
</tableCaption>
<bodyText confidence="0.995439631578948">
Discussion The UMD-QA task represents a
group of situations where because we have in-
sufficient supervision about matching (it’s hard
to know which node in the parse tree or which
timestep provides the most direct evidence for the
answer), decisions have to be made by looking at
and iterating over all subunits (all nodes in parse
trees or timesteps). Similar ideas can be found in
pooling structures (e.g. Socher et al. (2011a)).
The results above illustrate that for tasks where
we try to align the target with different source
components (i.e., parse tree nodes for tree mod-
els and different time steps for sequence models),
components from sequence models are able to em-
bed important information, despite the fact that se-
quence model components are just sentence frag-
ments and hence usually not linguistically mean-
ingful components in the way that parse tree con-
stituents are.
</bodyText>
<subsectionHeader confidence="0.982446">
3.4 Semantic Relationship Classification
</subsectionHeader>
<bodyText confidence="0.992903647058824">
Task Description: SemEval-2010 Task 8 (Hen-
drickx et al., 2009) is to find semantic rela-
tionships between pairs of nominals, e.g., in
“My [apartment]e1 has a pretty large [kitchen]e2”
classifying the relation between [apartment] and
[kitchen] as component-whole. The dataset con-
tains 9 ordered relationships, so the task is formal-
ized as a 19-class classification problem, with di-
rected relations treated as separate labels; see Hen-
drickx et al. (2009; Socher et al. (2012) for details.
For the recursive implementations, we follow
the neural framework defined in Socher et al.
(2012). The path in the parse tree between the two
nominals is retrieved, and the embedding is calcu-
lated based on recursive models and fed to a soft-
max classifier6. Retrieved paths are transformed
for the recurrent models as shown in Figure 5.
</bodyText>
<figureCaption confidence="0.983794">
Figure 4: Illustration of Models for Semantic Re-
lationship Classification.
</figureCaption>
<bodyText confidence="0.999574083333333">
Discussion Unlike for earlier tasks, here recur-
sive models yield much better performance than
the corresponding recurrent versions for all ver-
sions (e.g., standard tree vs. standard sequence,
p = 0.004). These results suggest that it is the
need to integrate structures far apart in the sen-
tence that characterizes the tasks where recursive
models surpass recurrent models. In parse-based
models, the two target words are drawn together
much earlier in the decision process than in recur-
rent models, which must remember one target un-
til the other one appears.
</bodyText>
<subsectionHeader confidence="0.928935">
3.5 Discourse Parsing
</subsectionHeader>
<bodyText confidence="0.939153272727273">
Task Description: Our final task, discourse
parsing based on the RST-DT corpus (Carlson et
6(Socher et al., 2012) achieve state-of-art performance
by combining a sophisticated model, MV-RNN, in which
each word is presented with both a matrix and a vector with
human-feature engineering. Again, because MV-RNN is dif-
ficult to adapt to a recurrent version, we do not employ this
state-of-the-art model, adhering only to the general versions
of recursive models described in Section 2, since our main
goal is to compare equivalent recursive and recurrent models
rather than implement the state of the art.
</bodyText>
<page confidence="0.959577">
2310
</page>
<table confidence="0.999044166666667">
Standard LSTM
Tree 0.748 0.767
Sequence 0.712 (-0.036) 0.740 (-0.027)
P-value 0.004* 0.020*
Bi-Sequence 0.730 (-0.018) 0.752 (-0.014)
P-value 0.017* 0.041*
</table>
<tableCaption confidence="0.975558">
Table 6: Test set accuracies on the SemEval-2010
Semantic Relationship Classification task.
</tableCaption>
<figureCaption confidence="0.797361">
Figure 5: An illustration of discourse parsing.
</figureCaption>
<bodyText confidence="0.989797638297873">
[e1, e2,...] denote EDUs (elementary discourse
units), each consisting of a sequence of tokens.
[r12, r34, r56] denote relationships to be classified.
A binary classification model is first used to decide
whether two EDUs should be merged and a multi-
class classifier is then used to decide the relation
type.
al., 2003), is to build a discourse tree for a doc-
ument, based on assigning Rhetorical Structure
Theory (RST) relations between elementary dis-
course units (EDUs). Because discourse relations
express the coherence structure of discourse, they
presumably express different aspects of compo-
sitional meaning than sentiment or nominal rela-
tions. See Hernault et al. (2010) for more details
on discourse parsing and the RST-DT corpus.
Representations for adjacent EDUs are fed into
binary classification (whether two EDUs are re-
lated) and multi-class relation classification mod-
els, as defined in Li et al. (2014). Related EDUs
are then merged into a new EDU, the representa-
tion of which is obtained through an operation of
neural composition based on the previous two re-
lated EDUs. This step is repeated until all units
are merged.
Discourse parsing takes EDUs as the basic units
to operate on; EDUs are short clauses, not full sen-
tences, with an average length of 7.2 words. Re-
cursive and recurrent models are applied on EDUs
to create embeddings to be used as inputs for dis-
course parsing. We use this task for two rea-
sons: (1) to illustrate whether syntactic parse trees
are useful for acquiring representations for short
clauses. (2) to measure the extent to which pars-
ing improves discourse tasks that need to combine
the meanings of larger text units.
Models are traditionally evaluated in terms of
three metrics, i.e., spans7, nuclearity8, and identi-
fying the rhetorical relation between two clauses.
Due to space limits, we only focus the last one,
rhetorical relation identification, because (1) rela-
tion labels are treated as correct only if spans and
nuclearity are correctly labeled (2) relation identi-
fication between clauses offer more insights about
model’s abilities to represent sentence semantics.
In order to perform a plain comparison, no addi-
tional human-developed features are added.
</bodyText>
<table confidence="0.9983835">
Standard LSTM
Tree 0.568 0.564
Sequence 0.572 (+0.004) 0.563 (-0.002)
P-value 0.160 0.422
Bi-Sequence 0.578 (+0.01) 0.575 (+0.012)
P-value 0.054 0.040*
</table>
<tableCaption confidence="0.9382205">
Table 7: Test set accuracies for relation identifica-
tion on RST discourse parsing data set.
</tableCaption>
<bodyText confidence="0.9992656875">
Discussion We see no large differences between
equivalent recurrent and recursive models. We
suggest two possible explanations. (1) EDUs tend
to be short; thus for some clauses, parsing might
not change the order of operations on words. Even
for those whose orders are changed by parse trees,
the influence of short phrases on the final represen-
tation may not be great enough. (2) Unlike earlier
tasks, where text representations are immediately
used as inputs into classifiers, the algorithm pre-
sented here adopts additional levels of neural com-
position during the process of EDU merging. We
suspect that neural layers may act as information
filters, separating the informational chaff from the
wheat, which in turn makes the model a bit more
immune to the initial inputs.
</bodyText>
<sectionHeader confidence="0.984406" genericHeader="evaluation">
4 Discussions and Conclusions
</sectionHeader>
<bodyText confidence="0.994844111111111">
We compared recursive and recurrent neural mod-
els for representation learning on 5 distinct NLP
tasks in 4 areas for which recursive neural models
are known to achieve good performance (Socher
et al., 2012; Socher et al., 2013; Li et al., 2014;
Iyyer et al., 2014).
As with any comparison between models, our
results come with some caveats: First, we ex-
plore the most general or basic forms of recur-
</bodyText>
<footnote confidence="0.9981755">
7on blank tree structures.
8on tree structures with nuclearity indication.
</footnote>
<page confidence="0.992568">
2311
</page>
<bodyText confidence="0.999074173913044">
sive/recurrent models rather than various sophis-
ticated algorithm variants. This is because fair
comparison becomes more and more difficult as
models get complex (e.g., the number of lay-
ers, number of hidden units within each layer,
etc.). Thus most neural models employed in this
work are comprised of only one layer of neural
compositions—despite the fact that deep neural
models with multiple layers give better results.
Our conclusions might thus be limited to the al-
gorithms employed in this paper, and it is unclear
whether they can be extended to other variants or
to the latest state-of-the-art. Second, in order to
compare models “fairly”, we force every model to
be trained exactly in the same way: AdaGrad with
minibatches, same set of initializations, etc. How-
ever, this may not necessarily be the optimal way
to train every model; different training strategies
tailored for specific models may improve their per-
formances. In that sense, our attempts to be “fair”
in this paper may nevertheless be unfair.
Pace these caveats, our conclusions can be sum-
marized as follows:
</bodyText>
<listItem confidence="0.634392846153846">
• In tasks like semantic relation extraction, in
which single headwords need to be associ-
ated across a long distance, recursive models
shine. This suggests that for the many other
kinds of tasks in which long-distance seman-
tic dependencies play a role (e.g., translation
between languages with significant reorder-
ing like Chinese-English translation), syntac-
tic structures from recursive models may of-
fer useful power.
• Tree models tend to help more on long se-
quences than shorter ones with sufficient su-
pervision: tree models slightly help root
</listItem>
<bodyText confidence="0.996432333333333">
level identification on the Stanford Sentiment
Treebank, but do not help much at the phrase
level. Adopting bi-directional versions of re-
current models seem to largely bridge this
gap, producing equivalent or sometimes bet-
ter results.
</bodyText>
<listItem confidence="0.764434714285714">
• On long sequences where supervision is not
sufficient, e.g., in Pang at al.,’s dataset (super-
vision only exists on top of long sequences),
no significant difference is observed between
tree based and sequence based models.
• In cases where tree-based models do well, a
simple approximation to tree-based models
</listItem>
<bodyText confidence="0.958557142857143">
seems to improve recurrent models to equiv-
alent or almost equivalent performance: (1)
break long sentences (on punctuation) into a
series of clause-like units, (2) work on these
clauses separately, and (3) join them together.
This model sometimes works as well as tree
models for the sentiment task, suggesting
that one of the reasons tree models help is
by breaking down long sentences into more
manageable units.
• Despite that the fact that components (out-
puts from different time steps) in recur-
rent models are not linguistically meaningful,
they may do as well as linguistically mean-
ingful phrases (represented by parse tree
nodes) in embedding informative evidence,
as demonstrated in UMD-QA task. Indeed,
recent work in parallel with ours (Bowman
et al., 2015) has shown that recurrent models
like LSTMs can discover implicit recursive
compositional structure.
</bodyText>
<sectionHeader confidence="0.9994" genericHeader="conclusions">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.998585136363636">
We would especially like to thank Richard Socher
and Kai-Sheng Tai for insightful comments, ad-
vice, and suggestions. We would also like to thank
Sam Bowman, Ignacio Cases, Jon Gauthier, Kevin
Gu, Gabor Angeli, Sida Wang, Percy Liang and
other members of the Stanford NLP group, as well
as the anonymous reviewers for their helpful ad-
vice on various aspects of this work. We acknowl-
edge the support of NVIDIA Corporation with the
donation of Tesla K40 GPUs We gratefully ac-
knowledge support from an Enlight Foundation
Graduate Fellowship, a gift from Bloomberg L.P.,
the Defense Advanced Research Projects Agency
(DARPA) Deep Exploration and Filtering of Text
(DEFT) Program under Air Force Research Lab-
oratory (AFRL) contract no. FA8750-13-2-0040,
and the NSF via award IIS-1514268. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily reflect the views of
Bloomberg L.P., DARPA, AFRL, NSF, or the US
government.
</bodyText>
<sectionHeader confidence="0.997007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.7737535">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
</reference>
<page confidence="0.962491">
2312
</page>
<note confidence="0.5378025">
learning to align and translate. arXiv preprint
arXiv:1409.0473.
</note>
<reference confidence="0.996991608695652">
Samuel R Bowman, Christopher Potts, and Christo-
pher D Manning. 2014. Recursive neural net-
works for learning logical semantics. arXiv preprint
arXiv:1406.1827.
Samuel R Bowman, Christopher D Manning, and
Christopher Potts. 2015. Tree-structured compo-
sition in neural networks without tree-structured ar-
chitectures. arXiv preprint arXiv:1506.04834.
Samuel R Bowman. 2013. Can recursive neural tensor
networks learn logical reasoning? arXiv preprint
arXiv:1312.6192.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
In Current and New Directions in Discourse and Di-
alogue Text, Speech and Language Technology. vol-
ume 22. Springer.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik
Sridharan. 2011. Better mini-batch algorithms via
accelerated gradient methods. In Advances in Neu-
ral Information Processing Systems, pages 1647–
1655.
Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming
Zhou, and Ke Xu. 2014. Adaptive recursive neural
network for target-dependent twitter sentiment clas-
sification. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 49–54.
Georg Dorffner. 1996. Neural networks for time series
processing. In Neural Network World.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121–2159.
Bradley Efron and Robert J Tibshirani. 1994. An in-
troduction to the bootstrap. CRC press.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179–211.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347–352. IEEE.
Alex Graves and Juergen Schmidhuber. 2009. Offline
handwriting recognition with multidimensional re-
current neural networks. In Advances in Neural In-
formation Processing Systems, pages 545–552.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey
Hinton. 2013. Speech recognition with deep recur-
rent neural networks. In Acoustics, Speech and Sig-
nal Processing (ICASSP), 2013 IEEE International
Conference on, pages 6645–6649. IEEE.
Alex Graves. 2012. Supervised sequence labeling
with recurrent neural networks, In Studies in Com-
putational Intelligence. volume 385. Springer.
Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-
ruoka, and Takashi Chikayama. 2013. Simple cus-
tomization of recursive neural networks for seman-
tic relation classification. In EMNLP, pages 1372–
1376.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian
Pad´o, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009. Semeval-2010 task
8: Multi-way classification of semantic relations
between pairs of nominals. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions, pages 94–99.
Association for Computational Linguistics.
Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka.
2010. Hilda: a discourse parser using support vector
machine classification. Dialogue &amp; Discourse, 1(3).
Sepp Hochreiter and J¨urgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735–1780.
Ozan Irsoy and Claire Cardie. 2013. Bidirectional re-
cursive neural networks for token-level labeling with
structure. arXiv preprint arXiv:1312.0493.
Ozan Irsoy and Claire Cardie. 2014. Deep recursive
neural networks for compositionality in language.
In Advances in Neural Information Processing Sys-
tems, pages 2096–2104.
Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daum´e III. 2014. A neural
network for factoid question answering over para-
graphs. In Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 633–644.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In EMNLP, pages
1700–1709.
Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. arXiv
preprint arXiv:1405.4053.
Jiwei Li and Eduard Hovy. 2014. A model of coher-
ence based on distributed sentence representation.
In Proceedings of the 2014 Conference on Empirical
Methods in Natural Language Processing (EMNLP)
Jiwei Li, Rumeng Li, and Eduard Hovy. 2014. Recur-
sive deep models for discourse parsing. In Proceed-
ings of the 2014 Conference on Empirical Methods
2313
in Natural Language Processing (EMNLP), pages
2061–2069.
Richard P Lippmann. 1989. Review of neural net-
works for speech recognition. Neural computation,
1(1):1–38.
Thang Luong, Ilya Sutskever, Quoc V Le, Oriol
Vinyals, and Wojciech Zaremba. 2014. Addressing
the rare word problem in neural machine translation.
Proceedings of ACL. 2015.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space
word representations. In HLT-NAACL, pages 746–
751.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In Proceedings of the
ACL-02 conference on Empirical methods in natural
language processing-Volume 10, pages 79–86. As-
sociation for Computational Linguistics.
Romain Paulus, Richard Socher, and Christopher D
Manning. 2014. Global belief recursive neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 2888–2896.
Barak A Pearlmutter. 1989. Learning state space tra-
jectories in recurrent neural networks. Neural Com-
putation, 1(2):263–269.
Tony Robinson, Mike Hochberg, and Steve Renals.
1996. The use of recurrent neural networks in con-
tinuous speech recognition. In Automatic speech
and speaker recognition, pages 233–258. Springer.
Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673–2681.
Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-
pher D Manning, and Andrew Y Ng. 2011a. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. In Advances in Neural In-
formation Processing Systems, pages 801–809.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011b.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161. Association for
Computational Linguistics.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201–1211. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In Advances in Neural Information Process-
ing Systems, pages 3104–3112.
Kai Sheng Tai, Richard Socher, and Christopher D
Manning. Improved semantic representations from
tree-structured long short-term memory networks.
ACL. 2015.
Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. 2014.
Grammar as a foreign language. arXiv preprint
arXiv:1412.7449.
Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo.
2015. Long short-term memory over recursive
structures. In Proceedings of the 32nd International
Conference on Machine Learning (ICML-15), pages
1604–1612.
</reference>
<page confidence="0.992205">
2314
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.766560">
<title confidence="0.9846">When Are Tree Structures Necessary for Deep Learning Representations?</title>
<author confidence="0.99861">Minh-Thang Dan Eduard</author>
<affiliation confidence="0.867021">Science Department, Stanford University, Stanford, CA</affiliation>
<address confidence="0.896009">Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213</address>
<email confidence="0.992818">jiweil,lmthang,jurafsky@stanford.eduehovy@andrew.cmu.edu</email>
<abstract confidence="0.999195193548387">Recursive neural models, which use syntactic parse trees to recursively generate representations bottom-up, are a popular architecture. However there have not been rigorous evaluations showing for exactly which tasks this syntax-based method is appropriate. In this paper, we benchmark models against sequential models, enforcing applesto-apples comparison as much as possible. We investigate 4 tasks: (1) sentiment classification at the sentence level and phrase level; (2) matching questions to answerphrases; (3) discourse parsing; (4) semantic relation extraction. Our goal is to understand better when, and why, recursive models can outperform simpler models. We find that recursive models help mainly on tasks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly</title>
<date>2014</date>
<marker>Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel R Bowman</author>
<author>Christopher Potts</author>
<author>Christopher D Manning</author>
</authors>
<title>Recursive neural networks for learning logical semantics. arXiv preprint arXiv:1406.1827.</title>
<date>2014</date>
<contexts>
<context position="3531" citStr="Bowman et al., 2014" startWordPosition="516" endWordPosition="519">e trees. Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even thoug</context>
</contexts>
<marker>Bowman, Potts, Manning, 2014</marker>
<rawString>Samuel R Bowman, Christopher Potts, and Christopher D Manning. 2014. Recursive neural networks for learning logical semantics. arXiv preprint arXiv:1406.1827.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel R Bowman</author>
<author>Christopher D Manning</author>
<author>Christopher Potts</author>
</authors>
<title>Tree-structured composition in neural networks without tree-structured architectures. arXiv preprint arXiv:1506.04834.</title>
<date>2015</date>
<contexts>
<context position="33629" citStr="Bowman et al., 2015" startWordPosition="5360" endWordPosition="5363">-like units, (2) work on these clauses separately, and (3) join them together. This model sometimes works as well as tree models for the sentiment task, suggesting that one of the reasons tree models help is by breaking down long sentences into more manageable units. • Despite that the fact that components (outputs from different time steps) in recurrent models are not linguistically meaningful, they may do as well as linguistically meaningful phrases (represented by parse tree nodes) in embedding informative evidence, as demonstrated in UMD-QA task. Indeed, recent work in parallel with ours (Bowman et al., 2015) has shown that recurrent models like LSTMs can discover implicit recursive compositional structure. 5 Acknowledgments We would especially like to thank Richard Socher and Kai-Sheng Tai for insightful comments, advice, and suggestions. We would also like to thank Sam Bowman, Ignacio Cases, Jon Gauthier, Kevin Gu, Gabor Angeli, Sida Wang, Percy Liang and other members of the Stanford NLP group, as well as the anonymous reviewers for their helpful advice on various aspects of this work. We acknowledge the support of NVIDIA Corporation with the donation of Tesla K40 GPUs We gratefully acknowledge</context>
</contexts>
<marker>Bowman, Manning, Potts, 2015</marker>
<rawString>Samuel R Bowman, Christopher D Manning, and Christopher Potts. 2015. Tree-structured composition in neural networks without tree-structured architectures. arXiv preprint arXiv:1506.04834.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel R Bowman</author>
</authors>
<title>Can recursive neural tensor networks learn logical reasoning? arXiv preprint arXiv:1312.6192.</title>
<date>2013</date>
<contexts>
<context position="3509" citStr="Bowman, 2013" startWordPosition="513" endWordPosition="515">syntactic parse trees. Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to </context>
</contexts>
<marker>Bowman, 2013</marker>
<rawString>Samuel R Bowman. 2013. Can recursive neural tensor networks learn logical reasoning? arXiv preprint arXiv:1312.6192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2003</date>
<booktitle>In Current and New Directions in Discourse and Dialogue Text, Speech and Language Technology.</booktitle>
<volume>22</volume>
<publisher>Springer.</publisher>
<marker>Carlson, Marcu, Okurowski, 2003</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2003. Building a discourse-tagged corpus in the framework of rhetorical structure theory. In Current and New Directions in Discourse and Dialogue Text, Speech and Language Technology. volume 22. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="1766" citStr="Collobert et al., 2011" startWordPosition="242" endWordPosition="245">require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models. 1 Introduction Deep learning based methods learn lowdimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al.</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Cotter</author>
<author>Ohad Shamir</author>
<author>Nati Srebro</author>
<author>Karthik Sridharan</author>
</authors>
<title>Better mini-batch algorithms via accelerated gradient methods.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>1647--1655</pages>
<contexts>
<context position="12307" citStr="Cotter et al., 2011" startWordPosition="1980" endWordPosition="1983">e relations based on calculated representations. h← t = f(W← - h← t+1 + V ← - et) (3) h→t = f(W→ - h→ t−1 + V → - et) 2306 In each case we followed the protocols described in the original papers. We first group the algorithm variants into two groups as follows: • Standard tree models vs standard sequence models vs standard bi-directional sequence models • LSTM tree models, LSTM sequence models vs LSTM bi-directional sequence models. We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad (Duchi et al., 2011) with minibatches (Cotter et al., 2011). Parameters are tuned using the development dataset if available in the original datasets or from crossvalidation if not. Derivatives are calculated from standard back-propagation (Goller and Kuchler, 1996). Parameters to tune include size of mini batches, learning rate, and parameters for L2 penalizations. The number of running iterations is treated as a parameter to tune and the model achieving best performance on the development set is used as the final model to be evaluated. For settings where no repeated experiments are performed, the bootstrap test is adopted for statistical significanc</context>
</contexts>
<marker>Cotter, Shamir, Srebro, Sridharan, 2011</marker>
<rawString>Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan. 2011. Better mini-batch algorithms via accelerated gradient methods. In Advances in Neural Information Processing Systems, pages 1647– 1655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Li Dong</author>
<author>Furu Wei</author>
<author>Chuanqi Tan</author>
<author>Duyu Tang</author>
<author>Ming Zhou</author>
<author>Ke Xu</author>
</authors>
<title>Adaptive recursive neural network for target-dependent twitter sentiment classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>49--54</pages>
<contexts>
<context position="3616" citStr="Dong et al., 2014" startWordPosition="531" endWordPosition="534"> based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direc</context>
</contexts>
<marker>Dong, Wei, Tan, Tang, Zhou, Xu, 2014</marker>
<rawString>Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent twitter sentiment classification. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 49–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georg Dorffner</author>
</authors>
<title>Neural networks for time series processing.</title>
<date>1996</date>
<booktitle>In Neural Network World.</booktitle>
<contexts>
<context position="2300" citStr="Dorffner, 1996" startWordPosition="325" endWordPosition="326">orpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012). They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings. Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left. These models generally consider no linguistic structure aside from word order. Recursive neural models (also referred to as tree models), by contrast, are structured by syn</context>
</contexts>
<marker>Dorffner, 1996</marker>
<rawString>Georg Dorffner. 1996. Neural networks for time series processing. In Neural Network World.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2121</pages>
<contexts>
<context position="12268" citStr="Duchi et al., 2011" startWordPosition="1974" endWordPosition="1977">al., 2010): Learns sentence-to-sentence relations based on calculated representations. h← t = f(W← - h← t+1 + V ← - et) (3) h→t = f(W→ - h→ t−1 + V → - et) 2306 In each case we followed the protocols described in the original papers. We first group the algorithm variants into two groups as follows: • Standard tree models vs standard sequence models vs standard bi-directional sequence models • LSTM tree models, LSTM sequence models vs LSTM bi-directional sequence models. We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad (Duchi et al., 2011) with minibatches (Cotter et al., 2011). Parameters are tuned using the development dataset if available in the original datasets or from crossvalidation if not. Derivatives are calculated from standard back-propagation (Goller and Kuchler, 1996). Parameters to tune include size of mini batches, learning rate, and parameters for L2 penalizations. The number of running iterations is treated as a parameter to tune and the model achieving best performance on the development set is used as the final model to be evaluated. For settings where no repeated experiments are performed, the bootstrap test</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An introduction to the bootstrap.</title>
<date>1994</date>
<publisher>CRC press.</publisher>
<contexts>
<context position="12945" citStr="Efron and Tibshirani, 1994" startWordPosition="2079" endWordPosition="2082">s are tuned using the development dataset if available in the original datasets or from crossvalidation if not. Derivatives are calculated from standard back-propagation (Goller and Kuchler, 1996). Parameters to tune include size of mini batches, learning rate, and parameters for L2 penalizations. The number of running iterations is treated as a parameter to tune and the model achieving best performance on the development set is used as the final model to be evaluated. For settings where no repeated experiments are performed, the bootstrap test is adopted for statistical significance testing (Efron and Tibshirani, 1994). Test scores that achieve significance level of 0.05 are marked by an asterisk (*). 3.1 Stanford Sentiment TreeBank Task Description We start with the Stanford Sentiment TreeBank (Socher et al., 2013). This dataset contains gold-standard labels for every parse tree constituent, from the sentence to phrases to individual words. Of course, any conclusions drawn from implementing sequence models on a dataset that was based on parse trees may have to be weakened, since sequence models may still benefit from the way that the dataset was collected. Nevertheless we add an evaluation on this dataset </context>
</contexts>
<marker>Efron, Tibshirani, 1994</marker>
<rawString>Bradley Efron and Robert J Tibshirani. 1994. An introduction to the bootstrap. CRC press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey L Elman</author>
</authors>
<title>Finding structure in time.</title>
<date>1990</date>
<journal>Cognitive science,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="2494" citStr="Elman, 1990" startWordPosition="356" endWordPosition="357">e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012). They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings. Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left. These models generally consider no linguistic structure aside from word order. Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recurs</context>
</contexts>
<marker>Elman, 1990</marker>
<rawString>Jeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas Kuchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In Neural Networks, 1996., IEEE International Conference on,</booktitle>
<volume>1</volume>
<pages>347--352</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="12514" citStr="Goller and Kuchler, 1996" startWordPosition="2010" endWordPosition="2013">st group the algorithm variants into two groups as follows: • Standard tree models vs standard sequence models vs standard bi-directional sequence models • LSTM tree models, LSTM sequence models vs LSTM bi-directional sequence models. We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent using AdaGrad (Duchi et al., 2011) with minibatches (Cotter et al., 2011). Parameters are tuned using the development dataset if available in the original datasets or from crossvalidation if not. Derivatives are calculated from standard back-propagation (Goller and Kuchler, 1996). Parameters to tune include size of mini batches, learning rate, and parameters for L2 penalizations. The number of running iterations is treated as a parameter to tune and the model achieving best performance on the development set is used as the final model to be evaluated. For settings where no repeated experiments are performed, the bootstrap test is adopted for statistical significance testing (Efron and Tibshirani, 1994). Test scores that achieve significance level of 0.05 are marked by an asterisk (*). 3.1 Stanford Sentiment TreeBank Task Description We start with the Stanford Sentimen</context>
</contexts>
<marker>Goller, Kuchler, 1996</marker>
<rawString>Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In Neural Networks, 1996., IEEE International Conference on, volume 1, pages 347–352. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Juergen Schmidhuber</author>
</authors>
<title>Offline handwriting recognition with multidimensional recurrent neural networks.</title>
<date>2009</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>545--552</pages>
<contexts>
<context position="2430" citStr="Graves and Schmidhuber, 2009" startWordPosition="342" endWordPosition="345">and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012). They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings. Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left. These models generally consider no linguistic structure aside from word order. Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structur</context>
</contexts>
<marker>Graves, Schmidhuber, 2009</marker>
<rawString>Alex Graves and Juergen Schmidhuber. 2009. Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in Neural Information Processing Systems, pages 545–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Abdel-rahman Mohamed</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Speech recognition with deep recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,</booktitle>
<pages>6645--6649</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="2373" citStr="Graves et al., 2013" startWordPosition="335" endWordPosition="338">et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012). They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings. Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left. These models generally consider no linguistic structure aside from word order. Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of considering tokens sequentially, recursive</context>
</contexts>
<marker>Graves, Mohamed, Hinton, 2013</marker>
<rawString>Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013. Speech recognition with deep recurrent neural networks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on, pages 6645–6649. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
</authors>
<title>Supervised sequence labeling with recurrent neural networks,</title>
<date>2012</date>
<booktitle>In Studies in Computational Intelligence.</booktitle>
<volume>385</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="2445" citStr="Graves, 2012" startWordPosition="346" endWordPosition="347">For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012). They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings. Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left. These models generally consider no linguistic structure aside from word order. Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structure of parse tree</context>
</contexts>
<marker>Graves, 2012</marker>
<rawString>Alex Graves. 2012. Supervised sequence labeling with recurrent neural networks, In Studies in Computational Intelligence. volume 385. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazuma Hashimoto</author>
<author>Makoto Miwa</author>
<author>Yoshimasa Tsuruoka</author>
<author>Takashi Chikayama</author>
</authors>
<title>Simple customization of recursive neural networks for semantic relation classification.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1372--1376</pages>
<contexts>
<context position="3728" citStr="Hashimoto et al., 2013" startWordPosition="546" endWordPosition="549">bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the par</context>
</contexts>
<marker>Hashimoto, Miwa, Tsuruoka, Chikayama, 2013</marker>
<rawString>Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsuruoka, and Takashi Chikayama. 2013. Simple customization of recursive neural networks for semantic relation classification. In EMNLP, pages 1372– 1376.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Iris Hendrickx</author>
<author>Su Nam Kim</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Diarmuid O´ S´eaghdha</author>
<author>Sebastian Pad´o</author>
<author>Marco Pennacchiotti</author>
<author>Lorenza Romano</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions,</booktitle>
<pages>94--99</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hendrickx, Kim, Kozareva, Nakov, S´eaghdha, Pad´o, Pennacchiotti, Romano, Szpakowicz, 2009</marker>
<rawString>Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid O´ S´eaghdha, Sebastian Pad´o, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2009. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94–99. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
</authors>
<title>Helmut Prendinger, Mitsuru Ishizuka.</title>
<date>2010</date>
<journal>Dialogue &amp; Discourse,</journal>
<volume>1</volume>
<issue>3</issue>
<marker>Hernault, 2010</marker>
<rawString>Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka. 2010. Hilda: a discourse parser using support vector machine classification. Dialogue &amp; Discourse, 1(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long short-term memory.</title>
<date>1997</date>
<booktitle>Neural computation,</booktitle>
<pages>9--8</pages>
<contexts>
<context position="9681" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="1541" endWordPosition="1544">e standard recursive network calculates eη as follows: eη = f(W - eηleft + V - eηright) (2) Bidirectional Models (Schuster and Paliwal, 1997) add bidirectionality to the recurrent framework where embeddings for each time are calculated both forwardly and backwardly: Normally, final representations for sentences can be achieved either by concatenating vectors calculated from both directions [e←1 , e→NS] or using further compositional operation to preserve vector dimensionality ht = f(WL - [h←t , h→t ]) (4) where WL denotes a K x 2K dimensional matrix. Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, ..., xnX }, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as it, ft and ot. We notationally disambiguate e and h: et denotes the vector for individual text units (e.g., word or sentence) at time step t, while ht denotes the vector computed by the LSTM model at time t by combining et and ht−1. σ denotes the sigmoid function. The vector representation ht for each time-step t is given by: ct = ft - ct−1 + it - lt (6) hst = ot - ct (7) where W E R4K×2K. Labels at the phrase/sentence leve</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Bidirectional recursive neural networks for token-level labeling with structure. arXiv preprint arXiv:1312.0493.</title>
<date>2013</date>
<contexts>
<context position="3596" citStr="Irsoy and Cardie, 2013" startWordPosition="527" endWordPosition="530">models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its</context>
</contexts>
<marker>Irsoy, Cardie, 2013</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2013. Bidirectional recursive neural networks for token-level labeling with structure. arXiv preprint arXiv:1312.0493.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2096--2104</pages>
<contexts>
<context position="3437" citStr="Irsoy and Cardie, 2014" startWordPosition="500" endWordPosition="503">ve neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for captur</context>
</contexts>
<marker>Irsoy, Cardie, 2014</marker>
<rawString>Ozan Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Systems, pages 2096–2104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Iyyer</author>
<author>Jordan Boyd-Graber</author>
<author>Leonardo Claudino</author>
<author>Richard Socher</author>
<author>Hal Daum´e</author>
</authors>
<title>A neural network for factoid question answering over paragraphs.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>633--644</pages>
<marker>Iyyer, Boyd-Graber, Claudino, Socher, Daum´e, 2014</marker>
<rawString>Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III. 2014. A neural network for factoid question answering over paragraphs. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 633–644.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="5039" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="758" endWordPosition="761">ly important, and if so for which tasks, or whether other issues are at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is relatively slow, domain-dependent, and can be errorful. On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In EMNLP, pages 1700–1709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.</title>
<date>2014</date>
<contexts>
<context position="1741" citStr="Mikolov, 2014" startWordPosition="240" endWordPosition="241">traction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models. 1 Introduction Deep learning based methods learn lowdimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lipp</context>
<context position="5187" citStr="Mikolov, 2014" startWordPosition="783" endWordPosition="784">given that parsing is relatively slow, domain-dependent, and can be errorful. On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2013) that focus on understanding the role of recursive models in dealing with semantic compositionally in various scenarios </context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Eduard Hovy</author>
</authors>
<title>A model of coherence based on distributed sentence representation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</booktitle>
<contexts>
<context position="3763" citStr="Li and Hovy, 2014" startWordPosition="552" endWordPosition="555">parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). H</context>
</contexts>
<marker>Li, Hovy, 2014</marker>
<rawString>Jiwei Li and Eduard Hovy. 2014. A model of coherence based on distributed sentence representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Rumeng Li</author>
<author>Eduard Hovy</author>
</authors>
<title>Recursive deep models for discourse parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods</booktitle>
<pages>2313</pages>
<contexts>
<context position="11635" citStr="Li et al., 2014" startWordPosition="1865" endWordPosition="1868">3): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned. • Sentence-Target Matching on the UMDQA dataset (Iyyer et al., 2014): Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different time-steps for recurrent models. • Semantic Relation Classification on the SemEval-2010 task (Hendrickx et al., 2009). Learns long-distance relationships between two words that may be far apart sequentially. • Discourse Parsing (Li et al., 2014; Hernault et al., 2010): Learns sentence-to-sentence relations based on calculated representations. h← t = f(W← - h← t+1 + V ← - et) (3) h→t = f(W→ - h→ t−1 + V → - et) 2306 In each case we followed the protocols described in the original papers. We first group the algorithm variants into two groups as follows: • Standard tree models vs standard sequence models vs standard bi-directional sequence models • LSTM tree models, LSTM sequence models vs LSTM bi-directional sequence models. We employed standard training frameworks for neural models: for each task, we used stochastic gradient decent u</context>
<context position="27862" citStr="Li et al. (2014)" startWordPosition="4430" endWordPosition="4433">on type. al., 2003), is to build a discourse tree for a document, based on assigning Rhetorical Structure Theory (RST) relations between elementary discourse units (EDUs). Because discourse relations express the coherence structure of discourse, they presumably express different aspects of compositional meaning than sentiment or nominal relations. See Hernault et al. (2010) for more details on discourse parsing and the RST-DT corpus. Representations for adjacent EDUs are fed into binary classification (whether two EDUs are related) and multi-class relation classification models, as defined in Li et al. (2014). Related EDUs are then merged into a new EDU, the representation of which is obtained through an operation of neural composition based on the previous two related EDUs. This step is repeated until all units are merged. Discourse parsing takes EDUs as the basic units to operate on; EDUs are short clauses, not full sentences, with an average length of 7.2 words. Recursive and recurrent models are applied on EDUs to create embeddings to be used as inputs for discourse parsing. We use this task for two reasons: (1) to illustrate whether syntactic parse trees are useful for acquiring representatio</context>
<context position="30447" citStr="Li et al., 2014" startWordPosition="4849" endWordPosition="4852">e immediately used as inputs into classifiers, the algorithm presented here adopts additional levels of neural composition during the process of EDU merging. We suspect that neural layers may act as information filters, separating the informational chaff from the wheat, which in turn makes the model a bit more immune to the initial inputs. 4 Discussions and Conclusions We compared recursive and recurrent neural models for representation learning on 5 distinct NLP tasks in 4 areas for which recursive neural models are known to achieve good performance (Socher et al., 2012; Socher et al., 2013; Li et al., 2014; Iyyer et al., 2014). As with any comparison between models, our results come with some caveats: First, we explore the most general or basic forms of recur7on blank tree structures. 8on tree structures with nuclearity indication. 2311 sive/recurrent models rather than various sophisticated algorithm variants. This is because fair comparison becomes more and more difficult as models get complex (e.g., the number of layers, number of hidden units within each layer, etc.). Thus most neural models employed in this work are comprised of only one layer of neural compositions—despite the fact that d</context>
</contexts>
<marker>Li, Li, Hovy, 2014</marker>
<rawString>Jiwei Li, Rumeng Li, and Eduard Hovy. 2014. Recursive deep models for discourse parsing. In Proceedings of the 2014 Conference on Empirical Methods 2313</rawString>
</citation>
<citation valid="false">
<booktitle>in Natural Language Processing (EMNLP),</booktitle>
<pages>2061--2069</pages>
<marker></marker>
<rawString>in Natural Language Processing (EMNLP), pages 2061–2069.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard P Lippmann</author>
</authors>
<title>Review of neural networks for speech recognition.</title>
<date>1989</date>
<booktitle>Neural computation,</booktitle>
<pages>1--1</pages>
<contexts>
<context position="2351" citStr="Lippmann, 1989" startWordPosition="333" endWordPosition="334">2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012). They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings. Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left. These models generally consider no linguistic structure aside from word order. Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of considering tokens s</context>
</contexts>
<marker>Lippmann, 1989</marker>
<rawString>Richard P Lippmann. 1989. Review of neural networks for speech recognition. Neural computation, 1(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Luong</author>
<author>Ilya Sutskever</author>
<author>Quoc V Le</author>
<author>Oriol Vinyals</author>
<author>Wojciech Zaremba</author>
</authors>
<title>Addressing the rare word problem in neural machine translation.</title>
<date>2014</date>
<booktitle>Proceedings of ACL.</booktitle>
<contexts>
<context position="5063" citStr="Luong et al., 2014" startWordPosition="763" endWordPosition="766">asks, or whether other issues are at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is relatively slow, domain-dependent, and can be errorful. On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2</context>
</contexts>
<marker>Luong, Sutskever, Le, Vinyals, Zaremba, 2014</marker>
<rawString>Thang Luong, Ilya Sutskever, Quoc V Le, Oriol Vinyals, and Wojciech Zaremba. 2014. Addressing the rare word problem in neural machine translation. Proceedings of ACL. 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Wen-tau Yih</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Linguistic regularities in continuous space word representations.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>746--751</pages>
<contexts>
<context position="1719" citStr="Mikolov et al., 2013" startWordPosition="234" endWordPosition="237">ks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models. 1 Introduction Deep learning based methods learn lowdimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robin</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word representations. In HLT-NAACL, pages 746– 751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5628" citStr="Pang et al., 2002" startWordPosition="852" endWordPosition="855">alchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2013) that focus on understanding the role of recursive models in dealing with semantic compositionally in various scenarios such as different lengths of inputs and whether or not supervision is comprehensive. • Phrase Matching on the UMD-QA dataset (Iyyer et al., 2014) can help see the difference between outputs from intermediate components from different models, i.e., representations for intermediate parse tree nodes and outputs from recurrent models at different time steps. It also helps see whether parsing is useful for finding similarities between questio</context>
<context position="10826" citStr="Pang et al. (2002)" startWordPosition="1744" endWordPosition="1747">6) hst = ot - ct (7) where W E R4K×2K. Labels at the phrase/sentence level are predicted representations outputted from the last time step. Tree LSTMs Recent research has extended the LSTM idea to tree-based structures (Zhu et al., 2015; Tai et al., 2015) that associate memory and forget gates to nodes of the parse trees. Bi-directional LSTMs These combine bidirectional models and LSTMs. 3 Experiments In this section, we detail our experimental settings and results. We consider the following tasks, each representative of a different class of NLP tasks. • Binary sentiment classification on the Pang et al. (2002) dataset. This addresses the issues where supervision only appears globally after a long sequence of operations. • Sentiment Classification on the Stanford Sentiment Treebank (Socher et al., 2013): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned. • Sentence-Target Matching on the UMDQA dataset (Iyyer et al., 2014): Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different time-steps for recurrent models. • </context>
<context position="19889" citStr="Pang et al. (2002)" startWordPosition="3165" endWordPosition="3168">12 steps before the prediction error gets back to the first token “simple”: error—*lot—*a—*it—*like—*still—*i—*,—*was —*plot—* the—*as—*simple In a hierarchical model, the second clause is compacted into one component, and the error propagation is thus given by: error—* second-clause —* first-clause —* was—*plot—*the—*as—*simple. Propagation with clause segmentation consists of only 8 operations. Such a procedure thus tends to attenuate the gradient vanishing problem, potentially yielding better performance. 3.2 Binary Sentiment Classification (Pang) Task Description: The sentiment dataset of Pang et al. (2002) consists of sentences with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). No pretraining procedure as described in Socher et al. (2011b) is employed. Word embeddings are initialized using skip-grams and kept fixed in the learning procedure. We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package4. Sentence level embeddings are fed into a sigmoid classifier. Performances for 50 dimensional vectors are given in the table below: Discussion Why don’t parse trees help on this task? One possible ex</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Romain Paulus</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Global belief recursive neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>2888--2896</pages>
<contexts>
<context position="3412" citStr="Paulus et al., 2014" startWordPosition="496" endWordPosition="499">m word order. Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of considering tokens sequentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. One possible advantage of recursive models is t</context>
</contexts>
<marker>Paulus, Socher, Manning, 2014</marker>
<rawString>Romain Paulus, Richard Socher, and Christopher D Manning. 2014. Global belief recursive neural networks. In Advances in Neural Information Processing Systems, pages 2888–2896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barak A Pearlmutter</author>
</authors>
<title>Learning state space trajectories in recurrent neural networks.</title>
<date>1989</date>
<journal>Neural Computation,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="2283" citStr="Pearlmutter, 1989" startWordPosition="323" endWordPosition="324"> large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012). They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings. Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left. These models generally consider no linguistic structure aside from word order. Recursive neural models (also referred to as tree models), by contrast, are </context>
</contexts>
<marker>Pearlmutter, 1989</marker>
<rawString>Barak A Pearlmutter. 1989. Learning state space trajectories in recurrent neural networks. Neural Computation, 1(2):263–269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Robinson</author>
<author>Mike Hochberg</author>
<author>Steve Renals</author>
</authors>
<title>The use of recurrent neural networks in continuous speech recognition. In Automatic speech and speaker recognition,</title>
<date>1996</date>
<pages>233--258</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2335" citStr="Robinson et al., 1996" startWordPosition="329" endWordPosition="332"> 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robinson et al., 1996; Lippmann, 1989; Graves et al., 2013) or handwriting recognition (Graves and Schmidhuber, 2009; Graves, 2012). They were applied early on to NLP (Elman, 1990), by modeling a sentence as tokens processed sequentially and at each step combining the current token with previously built embeddings. Recurrent models can be extended to bidirectional ones from both leftto-right and right-to-left. These models generally consider no linguistic structure aside from word order. Recursive neural models (also referred to as tree models), by contrast, are structured by syntactic parse trees. Instead of cons</context>
</contexts>
<marker>Robinson, Hochberg, Renals, 1996</marker>
<rawString>Tony Robinson, Mike Hochberg, and Steve Renals. 1996. The use of recurrent neural networks in continuous speech recognition. In Automatic speech and speaker recognition, pages 233–258. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Schuster</author>
<author>Kuldip K Paliwal</author>
</authors>
<title>Bidirectional recurrent neural networks.</title>
<date>1997</date>
<journal>Signal Processing, IEEE Transactions on,</journal>
<volume>45</volume>
<issue>11</issue>
<contexts>
<context position="9189" citStr="Schuster and Paliwal, 1997" startWordPosition="1462" endWordPosition="1465">gth of the sequence, hNs represents the whole sequence S. Standard recursive/Tree models work in a similar way, but processing neighboring words by parse tree order rather than sequence order. It computes a representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree. For a given node η in the tree and its left child ηleft (with representation eleft) and right child ηright (with representation eright), the standard recursive network calculates eη as follows: eη = f(W - eηleft + V - eηright) (2) Bidirectional Models (Schuster and Paliwal, 1997) add bidirectionality to the recurrent framework where embeddings for each time are calculated both forwardly and backwardly: Normally, final representations for sentences can be achieved either by concatenating vectors calculated from both directions [e←1 , e→NS] or using further compositional operation to preserve vector dimensionality ht = f(WL - [h←t , h→t ]) (4) where WL denotes a K x 2K dimensional matrix. Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, ..., xnX }, an LSTM associates each timeste</context>
</contexts>
<marker>Schuster, Paliwal, 1997</marker>
<rawString>Mike Schuster and Kuldip K Paliwal. 1997. Bidirectional recurrent neural networks. Signal Processing, IEEE Transactions on, 45(11):2673–2681.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennin</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>801--809</pages>
<contexts>
<context position="20089" citStr="Socher et al. (2011" startWordPosition="3194" endWordPosition="3197">into one component, and the error propagation is thus given by: error—* second-clause —* first-clause —* was—*plot—*the—*as—*simple. Propagation with clause segmentation consists of only 8 operations. Such a procedure thus tends to attenuate the gradient vanishing problem, potentially yielding better performance. 3.2 Binary Sentiment Classification (Pang) Task Description: The sentiment dataset of Pang et al. (2002) consists of sentences with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). No pretraining procedure as described in Socher et al. (2011b) is employed. Word embeddings are initialized using skip-grams and kept fixed in the learning procedure. We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package4. Sentence level embeddings are fed into a sigmoid classifier. Performances for 50 dimensional vectors are given in the table below: Discussion Why don’t parse trees help on this task? One possible explanation is the distance 4https://code.google.com/p/word2vec/ Standard LSTM Tree 0.745 0.774 Sequence 0.733 (-0.012) 0.783 (+0.008) P-value 0.060 0.136 Bi-Sequence 0.754 (+0.09) 0.790 (+0.016) P-valu</context>
<context position="24067" citStr="Socher et al. (2011" startWordPosition="3837" endWordPosition="3840">LSTM Tree 0.523 0.558 Sequence 0.525 (+0.002) 0.546 (-0.012) P-value 0.490 0.046* Bi-Sequence 0.530 (+0.007) 0.564 (+0.006) P-value 0.075 0.120 Table 5: Test set accuracies for UMD-QA dataset. Discussion The UMD-QA task represents a group of situations where because we have insufficient supervision about matching (it’s hard to know which node in the parse tree or which timestep provides the most direct evidence for the answer), decisions have to be made by looking at and iterating over all subunits (all nodes in parse trees or timesteps). Similar ideas can be found in pooling structures (e.g. Socher et al. (2011a)). The results above illustrate that for tasks where we try to align the target with different source components (i.e., parse tree nodes for tree models and different time steps for sequence models), components from sequence models are able to embed important information, despite the fact that sequence model components are just sentence fragments and hence usually not linguistically meaningful components in the way that parse tree constituents are. 3.4 Semantic Relationship Classification Task Description: SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between</context>
</contexts>
<marker>Socher, Huang, Pennin, Manning, Ng, 2011</marker>
<rawString>Richard Socher, Eric H Huang, Jeffrey Pennin, Christopher D Manning, and Andrew Y Ng. 2011a. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems, pages 801–809.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20089" citStr="Socher et al. (2011" startWordPosition="3194" endWordPosition="3197">into one component, and the error propagation is thus given by: error—* second-clause —* first-clause —* was—*plot—*the—*as—*simple. Propagation with clause segmentation consists of only 8 operations. Such a procedure thus tends to attenuate the gradient vanishing problem, potentially yielding better performance. 3.2 Binary Sentiment Classification (Pang) Task Description: The sentiment dataset of Pang et al. (2002) consists of sentences with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). No pretraining procedure as described in Socher et al. (2011b) is employed. Word embeddings are initialized using skip-grams and kept fixed in the learning procedure. We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package4. Sentence level embeddings are fed into a sigmoid classifier. Performances for 50 dimensional vectors are given in the table below: Discussion Why don’t parse trees help on this task? One possible explanation is the distance 4https://code.google.com/p/word2vec/ Standard LSTM Tree 0.745 0.774 Sequence 0.733 (-0.012) 0.783 (+0.008) P-value 0.060 0.136 Bi-Sequence 0.754 (+0.09) 0.790 (+0.016) P-valu</context>
<context position="24067" citStr="Socher et al. (2011" startWordPosition="3837" endWordPosition="3840">LSTM Tree 0.523 0.558 Sequence 0.525 (+0.002) 0.546 (-0.012) P-value 0.490 0.046* Bi-Sequence 0.530 (+0.007) 0.564 (+0.006) P-value 0.075 0.120 Table 5: Test set accuracies for UMD-QA dataset. Discussion The UMD-QA task represents a group of situations where because we have insufficient supervision about matching (it’s hard to know which node in the parse tree or which timestep provides the most direct evidence for the answer), decisions have to be made by looking at and iterating over all subunits (all nodes in parse trees or timesteps). Similar ideas can be found in pooling structures (e.g. Socher et al. (2011a)). The results above illustrate that for tasks where we try to align the target with different source components (i.e., parse tree nodes for tree models and different time steps for sequence models), components from sequence models are able to embed important information, despite the fact that sequence model components are just sentence fragments and hence usually not linguistically meaningful components in the way that parse tree constituents are. 3.4 Semantic Relationship Classification Task Description: SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011b. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1201--1211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3703" citStr="Socher et al., 2012" startWordPosition="542" endWordPosition="545">ing recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but the</context>
<context position="25035" citStr="Socher et al. (2012)" startWordPosition="3989" endWordPosition="3992">ts and hence usually not linguistically meaningful components in the way that parse tree constituents are. 3.4 Semantic Relationship Classification Task Description: SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of nominals, e.g., in “My [apartment]e1 has a pretty large [kitchen]e2” classifying the relation between [apartment] and [kitchen] as component-whole. The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009; Socher et al. (2012) for details. For the recursive implementations, we follow the neural framework defined in Socher et al. (2012). The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier6. Retrieved paths are transformed for the recurrent models as shown in Figure 5. Figure 4: Illustration of Models for Semantic Relationship Classification. Discussion Unlike for earlier tasks, here recursive models yield much better performance than the corresponding recurrent versions for all versions (e.g., standard tree vs. st</context>
<context position="30409" citStr="Socher et al., 2012" startWordPosition="4841" endWordPosition="4844">rlier tasks, where text representations are immediately used as inputs into classifiers, the algorithm presented here adopts additional levels of neural composition during the process of EDU merging. We suspect that neural layers may act as information filters, separating the informational chaff from the wheat, which in turn makes the model a bit more immune to the initial inputs. 4 Discussions and Conclusions We compared recursive and recurrent neural models for representation learning on 5 distinct NLP tasks in 4 areas for which recursive neural models are known to achieve good performance (Socher et al., 2012; Socher et al., 2013; Li et al., 2014; Iyyer et al., 2014). As with any comparison between models, our results come with some caveats: First, we explore the most general or basic forms of recur7on blank tree structures. 8on tree structures with nuclearity indication. 2311 sive/recurrent models rather than various sophisticated algorithm variants. This is because fair comparison becomes more and more difficult as models get complex (e.g., the number of layers, number of hidden units within each layer, etc.). Thus most neural models employed in this work are comprised of only one layer of neura</context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1201–1211. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<contexts>
<context position="3572" citStr="Socher et al., 2013" startWordPosition="523" endWordPosition="526">uentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For</context>
<context position="5237" citStr="Socher et al., 2013" startWordPosition="788" endWordPosition="791">n-dependent, and can be errorful. On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2013) that focus on understanding the role of recursive models in dealing with semantic compositionally in various scenarios such as different lengths of inputs and whether or</context>
<context position="11022" citStr="Socher et al., 2013" startWordPosition="1773" endWordPosition="1776">to tree-based structures (Zhu et al., 2015; Tai et al., 2015) that associate memory and forget gates to nodes of the parse trees. Bi-directional LSTMs These combine bidirectional models and LSTMs. 3 Experiments In this section, we detail our experimental settings and results. We consider the following tasks, each representative of a different class of NLP tasks. • Binary sentiment classification on the Pang et al. (2002) dataset. This addresses the issues where supervision only appears globally after a long sequence of operations. • Sentiment Classification on the Stanford Sentiment Treebank (Socher et al., 2013): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned. • Sentence-Target Matching on the UMDQA dataset (Iyyer et al., 2014): Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different time-steps for recurrent models. • Semantic Relation Classification on the SemEval-2010 task (Hendrickx et al., 2009). Learns long-distance relationships between two words that may be far apart sequentially. • Discourse Parsing (Li</context>
<context position="13146" citStr="Socher et al., 2013" startWordPosition="2110" endWordPosition="2113"> to tune include size of mini batches, learning rate, and parameters for L2 penalizations. The number of running iterations is treated as a parameter to tune and the model achieving best performance on the development set is used as the final model to be evaluated. For settings where no repeated experiments are performed, the bootstrap test is adopted for statistical significance testing (Efron and Tibshirani, 1994). Test scores that achieve significance level of 0.05 are marked by an asterisk (*). 3.1 Stanford Sentiment TreeBank Task Description We start with the Stanford Sentiment TreeBank (Socher et al., 2013). This dataset contains gold-standard labels for every parse tree constituent, from the sentence to phrases to individual words. Of course, any conclusions drawn from implementing sequence models on a dataset that was based on parse trees may have to be weakened, since sequence models may still benefit from the way that the dataset was collected. Nevertheless we add an evaluation on this dataset because it has been a widely used benchmark dataset for neural model evaluations. For recursive models, we followed the protocols in Socher et al. (2013) where node embeddings in the parse trees are ob</context>
<context position="15736" citStr="Socher et al. (2013)" startWordPosition="2524" endWordPosition="2527">explore this task a bit more by training deeper and more sophisticated models. We examine the following three models: 1. Tree-structured LSTM models (Tai et al., 2015)2. 2. Deep Bi-LSTM sequence models (denoted as Sequence) that treat the whole sentence as just one sequence. 3. Deep Bi-LSTM hierarchical sequence models (denoted as Hierarchical Sequence) that first slice the sentence into a sequence of subsentences by using a look-up table of punctuations (i.e., comma, period, question mark 1The performance of our implementations of recursive models is not exactly identical to that reported in Socher et al. (2013), but the relative difference is around 1% to 2%. 2Tai et al.. achieved 0.510 accuracy in terms of finegrained evaluation at the root level as reported in (Tai et al., 2015), similar to results from our implementations (0.504). 2307 Figure 1: Transforming Stanford Sentiment Treebank to Sequences for Sequence Models. Figure 2: Illustration of two sequence models. A, B, C, D denote clauses or sub sentences separated by punctuation. and exclamation mark). The representation for each sub-sentence is first computed separately, and another level of sequence LSTM (one-directional) is then used to joi</context>
<context position="30430" citStr="Socher et al., 2013" startWordPosition="4845" endWordPosition="4848">xt representations are immediately used as inputs into classifiers, the algorithm presented here adopts additional levels of neural composition during the process of EDU merging. We suspect that neural layers may act as information filters, separating the informational chaff from the wheat, which in turn makes the model a bit more immune to the initial inputs. 4 Discussions and Conclusions We compared recursive and recurrent neural models for representation learning on 5 distinct NLP tasks in 4 areas for which recursive neural models are known to achieve good performance (Socher et al., 2012; Socher et al., 2013; Li et al., 2014; Iyyer et al., 2014). As with any comparison between models, our results come with some caveats: First, we explore the most general or basic forms of recur7on blank tree structures. 8on tree structures with nuclearity indication. 2311 sive/recurrent models rather than various sophisticated algorithm variants. This is because fair comparison becomes more and more difficult as models get complex (e.g., the number of layers, number of hidden units within each layer, etc.). Thus most neural models employed in this work are comprised of only one layer of neural compositions—despit</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="4976" citStr="Sutskever et al., 2014" startWordPosition="750" endWordPosition="753">, 2013). However we do not know if this advantage is truly important, and if so for which tasks, or whether other issues are at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is relatively slow, domain-dependent, and can be errorful. On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment clas</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from tree-structured long short-term memory networks.</title>
<date>2015</date>
<publisher>ACL.</publisher>
<contexts>
<context position="10463" citStr="Tai et al., 2015" startWordPosition="1687" endWordPosition="1690">oted as it, ft and ot. We notationally disambiguate e and h: et denotes the vector for individual text units (e.g., word or sentence) at time step t, while ht denotes the vector computed by the LSTM model at time t by combining et and ht−1. σ denotes the sigmoid function. The vector representation ht for each time-step t is given by: ct = ft - ct−1 + it - lt (6) hst = ot - ct (7) where W E R4K×2K. Labels at the phrase/sentence level are predicted representations outputted from the last time step. Tree LSTMs Recent research has extended the LSTM idea to tree-based structures (Zhu et al., 2015; Tai et al., 2015) that associate memory and forget gates to nodes of the parse trees. Bi-directional LSTMs These combine bidirectional models and LSTMs. 3 Experiments In this section, we detail our experimental settings and results. We consider the following tasks, each representative of a different class of NLP tasks. • Binary sentiment classification on the Pang et al. (2002) dataset. This addresses the issues where supervision only appears globally after a long sequence of operations. • Sentiment Classification on the Stanford Sentiment Treebank (Socher et al., 2013): comprehensive labels are found for word</context>
<context position="14985" citStr="Tai et al. (2015)" startWordPosition="2405" endWordPosition="2408">) P-value 0.078 0.210 Table 1: Test set accuracies on the Stanford Sentiment Treebank at root level. Fine-Grained Binary Tree 0.820 0.860 Sequence 0.818 (-0.002) 0.864 (+0.004) P-value 0.486 0.305 Bi-Sequence 0.826 (+0.06) 0.862 (+0.002) P-value 0.148 0.450 Table 2: Test set accuracies on the Stanford Sentiment Treebank at phrase level. Results are shown in Table 1 and 21. When comparing the standard version of tree models to sequence models, we find it helps a bit at root level identification (for sequences but not bisequences), but yields no significant improvement at the phrase level. LSTM Tai et al. (2015) discovered that LSTM tree models generate better performances in terms of sentence root level evaluation than sequence models. We explore this task a bit more by training deeper and more sophisticated models. We examine the following three models: 1. Tree-structured LSTM models (Tai et al., 2015)2. 2. Deep Bi-LSTM sequence models (denoted as Sequence) that treat the whole sentence as just one sequence. 3. Deep Bi-LSTM hierarchical sequence models (denoted as Hierarchical Sequence) that first slice the sentence into a sequence of subsentences by using a look-up table of punctuations (i.e., com</context>
<context position="16465" citStr="Tai et al. (2015)" startWordPosition="2641" endWordPosition="2644">valuation at the root level as reported in (Tai et al., 2015), similar to results from our implementations (0.504). 2307 Figure 1: Transforming Stanford Sentiment Treebank to Sequences for Sequence Models. Figure 2: Illustration of two sequence models. A, B, C, D denote clauses or sub sentences separated by punctuation. and exclamation mark). The representation for each sub-sentence is first computed separately, and another level of sequence LSTM (one-directional) is then used to join the subsentences. Illustrations are shown in Figure2. We consider the third model because the dataset used in Tai et al. (2015) contains long sentences and the evaluation is performed only at the sentence root level. Since a parsing algorithm will naturally break long sentences into sub-sentences, we would like to know whether any performance boost is introduced by the intra-clause parse tree structure or just by this broader segmentation of a sentence into clause-like units; this latter advantage could be approximated by using punctuationbased approximations to clause boundaries. We run 15 iterations for each algorithm. Parameters are harvested at the end of each iteration; those performing best on the development se</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D Manning. Improved semantic representations from tree-structured long short-term memory networks. ACL. 2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Lukasz Kaiser</author>
<author>Terry Koo</author>
<author>Slav Petrov</author>
<author>Ilya Sutskever</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Grammar as a foreign language. arXiv preprint arXiv:1412.7449.</title>
<date>2014</date>
<contexts>
<context position="5096" citStr="Vinyals et al., 2014" startWordPosition="769" endWordPosition="772">re at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is relatively slow, domain-dependent, and can be errorful. On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2013) that focus on understanding </context>
</contexts>
<marker>Vinyals, Kaiser, Koo, Petrov, Sutskever, Hinton, 2014</marker>
<rawString>Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2014. Grammar as a foreign language. arXiv preprint arXiv:1412.7449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Parinaz Sobihani</author>
<author>Hongyu Guo</author>
</authors>
<title>Long short-term memory over recursive structures.</title>
<date>2015</date>
<booktitle>In Proceedings of the 32nd International Conference on Machine Learning (ICML-15),</booktitle>
<pages>1604--1612</pages>
<contexts>
<context position="10444" citStr="Zhu et al., 2015" startWordPosition="1683" endWordPosition="1686">, respectively denoted as it, ft and ot. We notationally disambiguate e and h: et denotes the vector for individual text units (e.g., word or sentence) at time step t, while ht denotes the vector computed by the LSTM model at time t by combining et and ht−1. σ denotes the sigmoid function. The vector representation ht for each time-step t is given by: ct = ft - ct−1 + it - lt (6) hst = ot - ct (7) where W E R4K×2K. Labels at the phrase/sentence level are predicted representations outputted from the last time step. Tree LSTMs Recent research has extended the LSTM idea to tree-based structures (Zhu et al., 2015; Tai et al., 2015) that associate memory and forget gates to nodes of the parse trees. Bi-directional LSTMs These combine bidirectional models and LSTMs. 3 Experiments In this section, we detail our experimental settings and results. We consider the following tasks, each representative of a different class of NLP tasks. • Binary sentiment classification on the Pang et al. (2002) dataset. This addresses the issues where supervision only appears globally after a long sequence of operations. • Sentiment Classification on the Stanford Sentiment Treebank (Socher et al., 2013): comprehensive labels</context>
</contexts>
<marker>Zhu, Sobihani, Guo, 2015</marker>
<rawString>Xiaodan Zhu, Parinaz Sobihani, and Hongyu Guo. 2015. Long short-term memory over recursive structures. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1604–1612.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
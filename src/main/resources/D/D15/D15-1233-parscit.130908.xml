<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.061094">
<title confidence="0.9945365">
Reversibility reconsidered: finite-state factors for efficient probabilistic
sampling in parsing and generation
</title>
<author confidence="0.990243">
Marc Dymetmant, Sriram Venkatapathyt, Chunyang Xiaot
</author>
<affiliation confidence="0.9478275">
tXerox Research Centre Europe, Grenoble, France
$Amazon, Machine Learning Team, Bangalore, India*
</affiliation>
<email confidence="0.995807">
t{first.last}@xrce.xerox.com,tvesriram@amazon.com
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.961496888888889">
We restate the classical logical notion of
generation/parsing reversibility in terms of
feasible probabilistic sampling, and argue
for an implementation based on finite-state
factors. We propose a modular decompo-
sition that reconciles generation accuracy
with parsing robustness and allows the in-
troduction of dynamic contextual factors.
(Opinion Piece)
</bodyText>
<sectionHeader confidence="0.997845" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98993464">
The objective of Natural Language Understanding
(NLU) is to map linguistic utterances to semantic
representations, that of Natural Language Genera-
tion (NLG) to map semantic representations to lin-
guistic utterances. In most of NLP practice, these
two objectives are handled by different processes,
and computational linguists rarely operate at the
intersection of the two subdomains.
For a few years around the early nineties, based
both on cognitive, linguistic, and engineering con-
siderations, there was a surge of interest in so
called reversible grammar approaches to NLP,
where one and the same grammatical specification
could serve both for parsing utterance x into logi-
cal form z, but also for generating x from z (Strza-
lkowski, 1994).
We start by a brief review of this historical non-
probabilistic notion of reversibility and point out
certain of its weaknesses, in particular regarding
robustness; we then give in section 3 a new proba-
bilistic definition of reversibility; then, in section 4
we argue for a reversibility model based on modu-
lar weighted finite-state transducers. We end with
a discussion of recent related work.
∗ Work done while at XRCE.
</bodyText>
<sectionHeader confidence="0.969929" genericHeader="method">
2 Classical reversibility
</sectionHeader>
<bodyText confidence="0.999983">
The most direct approaches to NLU attempt to de-
sign procedures for semantic parsing that, given
an input utterance x, produce a semantic repre-
sentation z, by following a number of interme-
diate steps where the surface form is gradually
transformed into semantic structure. Such “pro-
cedural” approaches to semantic parsing are typ-
ically very hard or impossible to invert: start-
ing from a semantic representation z, there is no
simple process that is able to find an x which,
when given to the parser, would produce z. For-
mally, a Boolean relation r(x, z) can be such that
the question ?]z r(x, z) is decidable for all x’s,
while the reciprocal question ?]x r(x, z) is unde-
cidable for some z’s (Dymetman, 1991).1 One of
the motivations for the emerging paradigm of uni-
fication grammars at the end of the eighties was
the clean separation they promised between spec-
ifying well-formed linguistic structures, both on
the syntactic and semantic levels, through a for-
mal description of the relation r(x, z), and pro-
ducing efficient implementations of the specifi-
cation; in particular, there was much hope that
such formalisms would be conductive to effec-
tive reversibility (by contrast to variable assign-
ment, variable unification is inherently symmetri-
cal), that is, to feasible (and if possible efficient)
implementations of the parsing problem r(x, ?)
and of the generation problem r(?, z).
To some extent, this hope was validated through
a number of works at the time, mostly involving
machine translation applications, and constrain-
ing in more or less explicit ways the specifica-
tion of r (van Noord, 1990). However, for the
non-statistical approaches to parsing then strongly
dominant, robustness was an issue: a parser had to
</bodyText>
<footnote confidence="0.985638">
1Some intuition into the issue may be gained by consider-
ing typical techniques of public key cryptography, which rely
on the difficulty of inverting some simple arithmetic compu-
tations.
</footnote>
<page confidence="0.824821">
1990
</page>
<note confidence="0.6883925">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1990–1995,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999971">
either accept or reject a given input x, with no in-
termediary options, and in order to be able to parse
actual utterances, with all their empirical diversity,
parsers had to be rather tolerant. In the procedural
view of parsing, such robustness issues could of-
ten be mitigated through engineering tricks such as
ordering the rules from strict to lax, where gram-
matical constructions were given preference over
less conventional ones; however, when trying to
move to reversible grammars, these tricks could
not be reproduced: if the grammar was able to
parse an x into z, then, by design, it was also able
to generate x from z, and there was no obvious
way, in these non-probabilistic approaches, to dis-
tinguish between producing a linguistically correct
x or producing a deviant or incorrect one.
</bodyText>
<sectionHeader confidence="0.975124" genericHeader="method">
3 Probabilistic reversibility
</sectionHeader>
<bodyText confidence="0.942468416666667">
In the classical non-probabilistic case, a (relative)
consensus existed around the fact that a reversible
grammar should be, as we indicated above, a for-
mal specification of the relation r(x, z) such that
the problems r(x, ?) and r(?, z) were effectively
solvable.
Transposing this to the probabilistic world, we
propose the following semi-formal Definition:
A probabilistic reversible grammar is a for-
mal specification of a joint probability distribu-
tion p(x, z) over logical forms z and utterance
strings x such that the conditional distributions
</bodyText>
<equation confidence="0.9581208">
p(z|x) def = p(x,z)
� z/ p(x,z/) (parsing) and p(x|z) def =
p(x,z) (generation) can be efficiently sampled
Ex/ p(x ,z)
from.2
</equation>
<bodyText confidence="0.99851305">
Why such focus on sampling? We could have
chosen other definitions of parsing (and similarly
for generation), for instance the ability to re-
turn the most probable z given x, i.e. to return
argmaxz p(z|x); however sampling is the most di-
rect way of providing a concrete view of the un-
derlying probabilistic distribution, and has many
applications to learning, so we think the definition
above is reasonable (see also footnote4).
2We note the “semi-formal” aspect of this definition: con-
trarily to the classical case, which has a formal notion of ef-
fective computation, there is no universally accepted notion
of effective sampling from a probability distribution. For
many probability distributions, the only feasible sampling
approaches are the MCMC techniques (Robert and Casella,
2004), which typically do not come with convergence guar-
antees; in some situations, exact sampling techniques are ap-
plicable, which come with much better guarantees. We will
see that the approach proposed in section 4 allows such exact
sampling to take place.
</bodyText>
<sectionHeader confidence="0.939072" genericHeader="method">
4 Finite-state models for reversibility
</sectionHeader>
<bodyText confidence="0.99807316981132">
Finite-state transducers have properties which
make them uniquely suited to implementing re-
versible linguistic specifications in the above
sense. Consider a simple weighted string-to-
string transducer T(s, t), where s, t are strings, and
where the underlying semiring is the “probabilis-
tic semiring” over the nonnegative reals, addition
and multiplication having their usual interpreta-
tions. Such a transducer preserves regularity, both
in the forward (resp. reverse) directions, meaning
that the image through T of any weighted regular
language over s (resp. over t) is again a weighted
regular language over t (resp. over s). In partic-
ular the forward (resp. reverse) image of a fixed
string s0 (resp t0) can be computed in a compact
form as a weighted finite-state automaton (FSA)
over t (resp. s), which we can denote by T(s0, ·)
(resp. T(·, t0)). A weighted FSA can be easily nor-
malized into a probabilistic FSA3 and, from this
probabilitic FSA exact samplers for the “parser”
T(s0, ·) and for the “generator” T(·, t0)) are di-
rectly obtained.4
In general, some of the properties that make
weighted FSAs and FSTs — over strings or trees
— specially relevant for probabilistic models of
language are the following: (i) they allow com-
pact representations of complex probability distri-
butions over linguistic objects (automata) or pairs
of linguistic objects (transducers), (ii) they permit
efficient exact sampling (and efficient optimiza-
tion over derivations (but not always over strings)),
(iii) they support modularity: intersection of au-
tomata, composition of transducers, projections of
an automaton through a transducer.5
Conceptual architecture Armed with these
general considerations, let us now propose a con-
ceptual architecture based on a small number of
3That is, into a weighted FSA such the weights of the tran-
sitions from each state sum to 1.
4 While sampling strings from a weighted finite-state au-
tomaton is simple, finding the most probable string (not path)
in a probabilistic FSA is an NP-hard problem (Casacuberta
and de la Higuera, 2000), and one has to resort to the so-
called Viterbi approximation (assuming that the most prob-
able path projects into the most probable string). Contrary
to popular belief, sampling can sometimes be simpler than
optimization.
5Outside of the realm of finite-state machines, this modu-
larity is typically impossible to obtain. Thus, in general, the
availability of a sampler for a distribution p(x) (resp. a dis-
tribution q(x)) does not imply that we can efficiently sample
from the product (i.e. intersection) p(x).q(x), but we can in
case p and q are both represented by weighted FSAs.
</bodyText>
<page confidence="0.962397">
1991
</page>
<bodyText confidence="0.99978575">
finite-state modules, which attempts to satisfy the
definition given above for probabilistic reversibil-
ity, to address the problem of robustness that we
described earlier, and can also support contex-
tual preferences. We illustrate the approach with
some simple examples of human-machine dia-
logues (between a customer and a virtual agent), a
domain for which reversibility has high relevance,
due to effects such as self-monitoring (Neumann,
1998; Levelt, 1983), interleaving of understand-
ing and generation (Otsuka and Purver, 2003), and
lexical entrainment (Brennan, 1996).
</bodyText>
<equation confidence="0.9094565">
w C µ A
z _ y _ x
</equation>
<figureCaption confidence="0.9854895">
Figure 1: Reversible specification through finite-
state factors.
</figureCaption>
<bodyText confidence="0.999972892857143">
The conceptual architecture is shown in Figure 1.
Formally, the figure represents a probabilistic
graphical model in so-called factor form, where
the factors are w, n, a, A (we have also indicated
for future reference the “contextual” factors C, µ,
that we ignore for now). The factors take as argu-
ments three types of objects: z is a logical form,
that is, a structured object which can be naturally
represented as a tree, x is a surface string, and y
is a latent “underlying” string that corresponds to
one of a small collection of “canonical” texts for
realizing the logical form z (more about that later).
Each factor is realized through a weighted
finite-state machine (acceptor or transducer) over
strings or trees (Mohri, 2009; F¨ul¨op and Vogler,
2009; Maletti, 2010; Graehl et al., 2008).
The A factor is a string automaton that repre-
sents a standard ngram language model (typically
specific to domain), in other words a probability
distribution over utterances x. Symmetrically, the
regular tree automaton w represents a distribution
over logical forms z, which can be seen as play-
ing a similar role to the language model, but at the
semantic level, namely telling us what are the pos-
sible/likely logical forms in a certain domain.6
The “canonical factor” n is a weighted tree-
to-string transducer (Graehl et al., 2008), which
implements a relation between logical forms z
</bodyText>
<footnote confidence="0.988529333333333">
6In particular, the ω factor makes explicit the notion of a
well-formed semantic representation, a notion often left im-
plicit in semantic parsing.
</footnote>
<bodyText confidence="0.999704653061225">
and a small number of latent “canonical” texts
y realizing these logical forms. For example, n
may associate the logical form (dialog act) z =
wad(batLife, iphone6) — with wad an abbrevi-
ation for “what is the value of this attribute on this
device?”, and batLife an abbreviation for “bat-
tery life” —, with such a canonical text (among
a few others) as: What is the battery life of the
Iphone 6?.
The “similarity factor” a is a weighted string-
to-string finite state transducer which gives scores
to x, y according to a notion of similarity. It has
the role of “bridging” the gap between the actual
utterances x and the latent canonical utterances y.
The intention behind the similarity factor is to “de-
couple” the task of modeling some possible real-
izations of a given logical form from the task of
recognizing that a given more or less well-formed
input is a variant of such a realization. This fac-
tor relates the two strings y and x, where y is a
possible canonical utterance in the limited reper-
tory produced by n, and x is an actual utterance,
in particular any utterance that could be produced
by a human speaker. So for instance suppose that
the user’s utterance is x = What about battery du-
ration on this Iphone 6?, we would like this x to
have a significant similarity with the canonical ut-
terance y = What is the battery life of the Iphone
6? but a negligible similarity with another canon-
ical utterance such as y&apos; = What is the screen size
of the Galaxy Trend?.
Overall, the canonical factor n(z, y) concen-
trates more on a core “generation model”, namely
on producing some well-formed output y from a
logical form z, while the similarity factor a(y, x)
allows relating an actual user input x to a possi-
ble output y of the n model. The main import of a
is then to allow to use the core generation model
defined by n to be exploited for robust semantic
parsing.
Different instantiations of this scheme can be
employed. In some preliminary experiments that
we have performed,7 a is a simple edit-distance
transducer (Mohri, 2003) which penalizes differ-
ently the discrepancies between x and y: strongly
for some salient content words or named entities of
the domain, weakly for less relevant content words
and for non-content words, with limited use of lo-
cal paraphrases (which can also be implemented
</bodyText>
<footnote confidence="0.849447">
7In these experiments, we used string-based approxima-
tions of the logical forms, and only employed string-based
transducers from the OpenFST library.
</footnote>
<figure confidence="0.9953786">
n a
a
a
a
a
</figure>
<page confidence="0.988618">
1992
</page>
<bodyText confidence="0.999706311111111">
through Q). This strategy seems to work reason-
ably well when the semantical repertory of the do-
main is restricted, because a large number of pos-
sible variants for x are “attracted” to the same un-
derlying semantics. In domains where small nu-
ances of expression may result in distinct seman-
tics, the division of work between K and Q may be
different.
Parsing and Generation To understand the re-
versibility properties of the model of Figure 1,
let us first simplify the description by assuming
that z, instead of being a tree, is actually a string.
Then both w and A are string automata, and both
K and Q string-to-string transducers. Such a spec-
ification satisfies our definition of probabilistic re-
versibility, exploiting well-known compositional-
ity properties of weighted finite-state machines
over strings (Mohri, 2009). For parsing, we start
from a fixed xo, and can project it through Q into
a weighted FSA over y; in turn we can project
this automaton onto an FSA over z, and finally
intersect this automaton with w, obtaining a fi-
nal weighted “x0-parser” automaton over z, rep-
resenting a probability distribution from which we
can draw exact samples as explained above.8 Gen-
eration works in exactly the reverse way, starting
from a zo and eventually building a “z0-generator”
automaton over x.
In the actual proposal, z is a tree, meaning that
w is a tree automaton, and K a tree-to-string trans-
ducer. While finite-state tree automata correspond
to a single concept, and share all the nice proper-
ties of string automata (Comon et al., 2007), the
situation with tree-to-tree or tree-to-string trans-
ducers is more complicated (Maletti, 2010; Graehl
et al., 2008): several variants exist, only some of
which support the operations that our conceptual
model requires (composition with the string trans-
ducer Q and intersection with the tree automaton
w). In particular, the “linear non-deleting top-
down tree transducers” defined in (Maletti, 2010)9
have the requisite properties.
Contextual factors We now briefly come back
to the factors ( (tree automaton) and µ (string
automaton) of Figure 1, which highlight the use-
</bodyText>
<footnote confidence="0.995581666666667">
8We could also have precompiled a generic parser for all
x’s by first marginalizing the latent variable y through a com-
position of the transducers κ and v, and then intersecting the
resulting transducer with the automaton w.
9The paper only defines tree-to-tree transducers, but tree-
to-string variants can be derived easily.
</footnote>
<bodyText confidence="0.9999572">
fulness of our modular finite-state architecture.
These factors play similar roles to w and A, but
they evolve dynamically with the context. In dia-
logue applications, utterances can often only be in-
terpreted by reference to the current dialogue state
(e.g. “ten hours” in the context of a question about
battery life), and the ( factor can be used as a com-
pact representation of the current expectations of
the dialogue manager about the next logical form,
to be combined with the actual customer’s utter-
ance. Symmetrically, the µ factor can be used
to represent such phenomena as lexical entrain-
ment (Brennan, 1996), where the agent’s utterance
is oriented towards using similar wordings to the
customer’s.
</bodyText>
<sectionHeader confidence="0.999857" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.998839411764706">
The unique formal properties of finite-state ma-
chines, which favor modular decompositions of
complex tasks, have long been exploited in Com-
putational Linguistics. Tree transducers in partic-
ular have gained popularity in Statistical Machine
Translation, starting with (Yamada and Knight,
2001), as described in the surveys (Maletti, 2010;
Razmara, 2011).
The reversibility properties of finite-state trans-
ducers have been exploited to a more limited ex-
tent, starting with applications of non-weighted
string-to-string transducers to morphological anal-
ysis and generation (Beesley, 1996).
Concerning the application of weighted finite-
state tree machines to NLU/NLG reversibility, our
proposal is strongly related on the one hand to
the approach of (Jones et al., 2012), who ex-
plicitely proposes tree-to-string transducers as a
tool for modelling semantic parsing and for train-
ing on semantically annotated data, and on the
other hand to (Wong, 2007; Wong and Mooney,
2007), who focus more directly on the problem of
inverting a semantic parser into a generator. Wong
et al. do not explicitely use tree-based transducers,
but rather a formalism inspired by SCFGs (syn-
chronous context-free grammars), which essen-
tially corresponds to a form of tree-to-string trans-
ducer. In relation to reversibility considerations,
presentations in terms of synchronous formalisms
have the interest that they are intrinsically sym-
metrical. Such formalisms have tight relations to
tree-transducers (Shieber, 2004); one recently pro-
posed generalization, “Interpreted Regular Tree
Grammars” (Koller and Kuhlmann, 2011), allows
</bodyText>
<page confidence="0.975682">
1993
</page>
<bodyText confidence="0.99999276">
multiple (possibly more than two) synchronized
views of an underlying abstract derivation tree,
and has the advantage of permitting a uniform
treatment of strings and trees.
One important aspect in which our proposal dif-
fers from these previous approaches is in propos-
ing to decouple the “core” task of mapping logical
forms to well-formed latent canonical realizations
from the task of relating these realizations to ac-
tual utterances, through an additional “similarity”
transducer acting as a bridge.
This idea of a bridge is however close to another
line of work in semantic parsing, not transducer
based, namely (Berant and Liang, 2014; Wang
et al., 2015). There, a simple generic grammar
is used to generate canonical realizations from a
repertory of possible logical forms (expressed in
a variant of lambda calculus). Given an input to
parse, simple heuristics are used to select a fi-
nite list of potential logical forms which are then
ranked according to the (paraphrase-based) simi-
larity of their associated canonical realization with
the input. Thus in this approach, a form of gener-
ation plays an important role, not for its own sake,
but as a tool for semantic parsing.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999786363636364">
Because of their unique compositional properties,
finite-state modules are a natural choice for imple-
menting our definition of reversibility as efficient
bidirectional sampling from a common specifica-
tion. In this piece we have argued in favor of an
architecture realizing this definition and display-
ing robustness and contextuality.
Acknowledgments We thank the anonymous
reviewers for their detailed comments and for
pointing us to some relevant literature that we had
overlooked.
</bodyText>
<sectionHeader confidence="0.999003" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999794890625">
Kenneth Beesley. 1996. Arabic Finite-State Morpho-
logical Analysis and Generation. In Coling, pages
89–94.
Jonathan Berant and Percy Liang. 2014. Seman-
tic parsing via paraphrasing. In Proceedings of the
52nd Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), pages
1415–1425, Baltimore, Maryland, June. Association
for Computational Linguistics.
S.E. Brennan. 1996. Lexical entrainment in sponta-
neous dialog. In Proceedings of International Sym-
posium on Spoken Dialogue (ISSD-96).
Francisco Casacuberta and Colin de la Higuera. 2000.
Computational complexity of problems on proba-
bilistic grammars and transducers. In ICGI, pages
15–24.
H. Comon, M. Dauchet, R. Gilleron, C. L¨oding,
F. Jacquemard, D. Lugiez, S. Tison, and M. Tom-
masi. 2007. Tree automata techniques and appli-
cations. Available on: http://www.grappa.
univ-lille3.fr/tata. release October, 12th
2007.
Marc Dymetman. 1991. Inherently reversible gram-
mars, logic programming and computability. In
In Proceedings of the ACL Workshop: Reversible
Grammar in Natural Language Processing.
Zolt´an F¨ul¨op and Heiko Vogler. 2009. Weighted tree
automata and tree transducers. In Manfred Droste,
Werner Kuich, and Heiko Vogler, editors, Handbook
of Weighted Automata, Monographs in Theoretical
Computer Science. An EATCS Series, pages 313–
403. Springer Berlin Heidelberg.
Jonathan Graehl, Kevin Knight, and Jonathan May.
2008. Training tree transducers. Comput. Linguist.,
34(3):391–427, September.
Bevan Keeley Jones, Mark Johnson, and Sharon Gold-
water. 2012. Semantic parsing with bayesian tree
transducers. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, ACL ’12, pages
488–496, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Alexander Koller and Marco Kuhlmann. 2011. A
generalized view on parsing and translation. In
Proceedings of the 12th International Conference
on Parsing Technologies, IWPT ’11, pages 2–13,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
W.J.M Levelt. 1983. Monitoring and self-repair in
speech. Cognition, 14(1):41–104.
Andreas Maletti. 2010. Survey: Tree transducers in
machine translation. Technical report, Universitat
Rovira i Virgili.
Mehryar Mohri. 2003. Edit-Distance of Weighted Au-
tomata: General Definitions and Algorithms. Inter-
national Journal of Foundations of Computer Sci-
ence, 14:957–982.
Mehryar Mohri. 2009. Weighted automata algo-
rithms. In Manfred Droste, Werner Kuich, and
Heiko Vogler, editors, Handbook of Weighted Au-
tomata, pages 213–254. Springer.
G¨unter Neumann. 1998. Interleaving natural language
parsing and generation through uniform processing.
Artificial Intelligence, 99(1):121–163.
</reference>
<page confidence="0.896654">
1994
</page>
<reference confidence="0.999885395348837">
M. Otsuka and M. Purver. 2003. Incremental gener-
ation by incremental parsing. In Proceedings 6th
CLUK Colloquium.
Majid Razmara. 2011. Applications of tree transduc-
ers in statistical machine translation. Technical re-
port, Simon Fraser University.
Christian P. Robert and George Casella. 2004. Monte
Carlo Statistical Methods (Springer Texts in Statis-
tics). Springer-Verlag New York, Inc., Secaucus,
NJ, USA.
Stuart M. Shieber. 2004. Synchronous grammars
as tree transducers. In Proceedings of the Seventh
International Workshop on Tree Adjoining Gram-
mar and Related Formalisms (TAG+ 7), Vancouver,
Canada, 20–22 May.
Tomek Strzalkowski, editor. 1994. Reversible Gram-
mar in Natural Language Processing. Springer.
Gertjan van Noord. 1990. Reversible unification based
machine translation. In Proceedings of the 13th
Conference on Computational Linguistics - Volume
2, COLING ’90, pages 299–304, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Y. Wang, J. Berant, and P. Liang. 2015. Building a
semantic parser overnight. In Association for Com-
putational Linguistics (ACL).
Yuk Wah Wong and Raymond J Mooney. 2007. Gen-
eration by inverting a semantic parser that uses sta-
tistical machine translation. In HLT-NAACL, pages
172–179.
Yuk Wah Wong. 2007. Learning for Semantic Pars-
ing and Natural Language Generation Using Statis-
tical Machine Translation Techniques. Ph.D. the-
sis, Department of Computer Sciences, University
of Texas at Austin, Austin, TX, August. Also ap-
pears as Technical Report AI07-343, Artificial Intel-
ligence Lab, University of Texas at Austin, August
2007.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of the 39th Annual Meeting on Association for Com-
putational Linguistics, ACL ’01, pages 523–530,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
</reference>
<page confidence="0.992909">
1995
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.619938">
<title confidence="0.9850945">Reversibility reconsidered: finite-state factors for efficient sampling in parsing and generation</title>
<author confidence="0.970398">Sriram Chunyang</author>
<affiliation confidence="0.842582">Research Centre Europe, Grenoble,</affiliation>
<address confidence="0.848402">Machine Learning Team, Bangalore,</address>
<abstract confidence="0.999794222222222">We restate the classical logical notion of generation/parsing reversibility in terms of feasible probabilistic sampling, and argue for an implementation based on finite-state factors. We propose a modular decomposition that reconciles generation accuracy with parsing robustness and allows the introduction of dynamic contextual factors.</abstract>
<intro confidence="0.894004">(Opinion Piece)</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kenneth Beesley</author>
</authors>
<title>Arabic Finite-State Morphological Analysis and Generation. In Coling,</title>
<date>1996</date>
<pages>89--94</pages>
<contexts>
<context position="17632" citStr="Beesley, 1996" startWordPosition="2838" endWordPosition="2839">e customer’s. 5 Related work The unique formal properties of finite-state machines, which favor modular decompositions of complex tasks, have long been exploited in Computational Linguistics. Tree transducers in particular have gained popularity in Statistical Machine Translation, starting with (Yamada and Knight, 2001), as described in the surveys (Maletti, 2010; Razmara, 2011). The reversibility properties of finite-state transducers have been exploited to a more limited extent, starting with applications of non-weighted string-to-string transducers to morphological analysis and generation (Beesley, 1996). Concerning the application of weighted finitestate tree machines to NLU/NLG reversibility, our proposal is strongly related on the one hand to the approach of (Jones et al., 2012), who explicitely proposes tree-to-string transducers as a tool for modelling semantic parsing and for training on semantically annotated data, and on the other hand to (Wong, 2007; Wong and Mooney, 2007), who focus more directly on the problem of inverting a semantic parser into a generator. Wong et al. do not explicitely use tree-based transducers, but rather a formalism inspired by SCFGs (synchronous context-free</context>
</contexts>
<marker>Beesley, 1996</marker>
<rawString>Kenneth Beesley. 1996. Arabic Finite-State Morphological Analysis and Generation. In Coling, pages 89–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing via paraphrasing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>1415--1425</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="19295" citStr="Berant and Liang, 2014" startWordPosition="3089" endWordPosition="3092">y more than two) synchronized views of an underlying abstract derivation tree, and has the advantage of permitting a uniform treatment of strings and trees. One important aspect in which our proposal differs from these previous approaches is in proposing to decouple the “core” task of mapping logical forms to well-formed latent canonical realizations from the task of relating these realizations to actual utterances, through an additional “similarity” transducer acting as a bridge. This idea of a bridge is however close to another line of work in semantic parsing, not transducer based, namely (Berant and Liang, 2014; Wang et al., 2015). There, a simple generic grammar is used to generate canonical realizations from a repertory of possible logical forms (expressed in a variant of lambda calculus). Given an input to parse, simple heuristics are used to select a finite list of potential logical forms which are then ranked according to the (paraphrase-based) similarity of their associated canonical realization with the input. Thus in this approach, a form of generation plays an important role, not for its own sake, but as a tool for semantic parsing. 6 Conclusion Because of their unique compositional propert</context>
</contexts>
<marker>Berant, Liang, 2014</marker>
<rawString>Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1415–1425, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Brennan</author>
</authors>
<title>Lexical entrainment in spontaneous dialog.</title>
<date>1996</date>
<booktitle>In Proceedings of International Symposium on Spoken Dialogue (ISSD-96).</booktitle>
<contexts>
<context position="9793" citStr="Brennan, 1996" startWordPosition="1527" endWordPosition="1528"> both represented by weighted FSAs. 1991 finite-state modules, which attempts to satisfy the definition given above for probabilistic reversibility, to address the problem of robustness that we described earlier, and can also support contextual preferences. We illustrate the approach with some simple examples of human-machine dialogues (between a customer and a virtual agent), a domain for which reversibility has high relevance, due to effects such as self-monitoring (Neumann, 1998; Levelt, 1983), interleaving of understanding and generation (Otsuka and Purver, 2003), and lexical entrainment (Brennan, 1996). w C µ A z _ y _ x Figure 1: Reversible specification through finitestate factors. The conceptual architecture is shown in Figure 1. Formally, the figure represents a probabilistic graphical model in so-called factor form, where the factors are w, n, a, A (we have also indicated for future reference the “contextual” factors C, µ, that we ignore for now). The factors take as arguments three types of objects: z is a logical form, that is, a structured object which can be naturally represented as a tree, x is a surface string, and y is a latent “underlying” string that corresponds to one of a sm</context>
<context position="16940" citStr="Brennan, 1996" startWordPosition="2741" endWordPosition="2742">lness of our modular finite-state architecture. These factors play similar roles to w and A, but they evolve dynamically with the context. In dialogue applications, utterances can often only be interpreted by reference to the current dialogue state (e.g. “ten hours” in the context of a question about battery life), and the ( factor can be used as a compact representation of the current expectations of the dialogue manager about the next logical form, to be combined with the actual customer’s utterance. Symmetrically, the µ factor can be used to represent such phenomena as lexical entrainment (Brennan, 1996), where the agent’s utterance is oriented towards using similar wordings to the customer’s. 5 Related work The unique formal properties of finite-state machines, which favor modular decompositions of complex tasks, have long been exploited in Computational Linguistics. Tree transducers in particular have gained popularity in Statistical Machine Translation, starting with (Yamada and Knight, 2001), as described in the surveys (Maletti, 2010; Razmara, 2011). The reversibility properties of finite-state transducers have been exploited to a more limited extent, starting with applications of non-we</context>
</contexts>
<marker>Brennan, 1996</marker>
<rawString>S.E. Brennan. 1996. Lexical entrainment in spontaneous dialog. In Proceedings of International Symposium on Spoken Dialogue (ISSD-96).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
<author>Colin de la Higuera</author>
</authors>
<title>Computational complexity of problems on probabilistic grammars and transducers.</title>
<date>2000</date>
<booktitle>In ICGI,</booktitle>
<pages>15--24</pages>
<marker>Casacuberta, Higuera, 2000</marker>
<rawString>Francisco Casacuberta and Colin de la Higuera. 2000. Computational complexity of problems on probabilistic grammars and transducers. In ICGI, pages 15–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Comon</author>
<author>M Dauchet</author>
<author>R Gilleron</author>
<author>C L¨oding</author>
<author>F Jacquemard</author>
<author>D Lugiez</author>
<author>S Tison</author>
<author>M Tommasi</author>
</authors>
<title>Tree automata techniques and applications. Available on: http://www.grappa. univ-lille3.fr/tata. release October,</title>
<date>2007</date>
<marker>Comon, Dauchet, Gilleron, L¨oding, Jacquemard, Lugiez, Tison, Tommasi, 2007</marker>
<rawString>H. Comon, M. Dauchet, R. Gilleron, C. L¨oding, F. Jacquemard, D. Lugiez, S. Tison, and M. Tommasi. 2007. Tree automata techniques and applications. Available on: http://www.grappa. univ-lille3.fr/tata. release October, 12th 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Dymetman</author>
</authors>
<title>Inherently reversible grammars, logic programming and computability. In</title>
<date>1991</date>
<booktitle>In Proceedings of the ACL Workshop: Reversible Grammar in Natural Language Processing.</booktitle>
<contexts>
<context position="2587" citStr="Dymetman, 1991" startWordPosition="392" endWordPosition="393">ven an input utterance x, produce a semantic representation z, by following a number of intermediate steps where the surface form is gradually transformed into semantic structure. Such “procedural” approaches to semantic parsing are typically very hard or impossible to invert: starting from a semantic representation z, there is no simple process that is able to find an x which, when given to the parser, would produce z. Formally, a Boolean relation r(x, z) can be such that the question ?]z r(x, z) is decidable for all x’s, while the reciprocal question ?]x r(x, z) is undecidable for some z’s (Dymetman, 1991).1 One of the motivations for the emerging paradigm of unification grammars at the end of the eighties was the clean separation they promised between specifying well-formed linguistic structures, both on the syntactic and semantic levels, through a formal description of the relation r(x, z), and producing efficient implementations of the specification; in particular, there was much hope that such formalisms would be conductive to effective reversibility (by contrast to variable assignment, variable unification is inherently symmetrical), that is, to feasible (and if possible efficient) impleme</context>
</contexts>
<marker>Dymetman, 1991</marker>
<rawString>Marc Dymetman. 1991. Inherently reversible grammars, logic programming and computability. In In Proceedings of the ACL Workshop: Reversible Grammar in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zolt´an F¨ul¨op</author>
<author>Heiko Vogler</author>
</authors>
<title>Weighted tree automata and tree transducers.</title>
<date>2009</date>
<booktitle>Handbook of Weighted Automata, Monographs in Theoretical Computer Science. An EATCS Series,</booktitle>
<pages>313--403</pages>
<editor>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>F¨ul¨op, Vogler, 2009</marker>
<rawString>Zolt´an F¨ul¨op and Heiko Vogler. 2009. Weighted tree automata and tree transducers. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, Monographs in Theoretical Computer Science. An EATCS Series, pages 313– 403. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Jonathan May</author>
</authors>
<title>Training tree transducers.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>3</issue>
<contexts>
<context position="10673" citStr="Graehl et al., 2008" startWordPosition="1676" endWordPosition="1679"> have also indicated for future reference the “contextual” factors C, µ, that we ignore for now). The factors take as arguments three types of objects: z is a logical form, that is, a structured object which can be naturally represented as a tree, x is a surface string, and y is a latent “underlying” string that corresponds to one of a small collection of “canonical” texts for realizing the logical form z (more about that later). Each factor is realized through a weighted finite-state machine (acceptor or transducer) over strings or trees (Mohri, 2009; F¨ul¨op and Vogler, 2009; Maletti, 2010; Graehl et al., 2008). The A factor is a string automaton that represents a standard ngram language model (typically specific to domain), in other words a probability distribution over utterances x. Symmetrically, the regular tree automaton w represents a distribution over logical forms z, which can be seen as playing a similar role to the language model, but at the semantic level, namely telling us what are the possible/likely logical forms in a certain domain.6 The “canonical factor” n is a weighted treeto-string transducer (Graehl et al., 2008), which implements a relation between logical forms z 6In particular</context>
<context position="15545" citStr="Graehl et al., 2008" startWordPosition="2514" endWordPosition="2517">ed “x0-parser” automaton over z, representing a probability distribution from which we can draw exact samples as explained above.8 Generation works in exactly the reverse way, starting from a zo and eventually building a “z0-generator” automaton over x. In the actual proposal, z is a tree, meaning that w is a tree automaton, and K a tree-to-string transducer. While finite-state tree automata correspond to a single concept, and share all the nice properties of string automata (Comon et al., 2007), the situation with tree-to-tree or tree-to-string transducers is more complicated (Maletti, 2010; Graehl et al., 2008): several variants exist, only some of which support the operations that our conceptual model requires (composition with the string transducer Q and intersection with the tree automaton w). In particular, the “linear non-deleting topdown tree transducers” defined in (Maletti, 2010)9 have the requisite properties. Contextual factors We now briefly come back to the factors ( (tree automaton) and µ (string automaton) of Figure 1, which highlight the use8We could also have precompiled a generic parser for all x’s by first marginalizing the latent variable y through a composition of the transducers</context>
</contexts>
<marker>Graehl, Knight, May, 2008</marker>
<rawString>Jonathan Graehl, Kevin Knight, and Jonathan May. 2008. Training tree transducers. Comput. Linguist., 34(3):391–427, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Keeley Jones</author>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Semantic parsing with bayesian tree transducers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12,</booktitle>
<pages>488--496</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17813" citStr="Jones et al., 2012" startWordPosition="2865" endWordPosition="2868">al Linguistics. Tree transducers in particular have gained popularity in Statistical Machine Translation, starting with (Yamada and Knight, 2001), as described in the surveys (Maletti, 2010; Razmara, 2011). The reversibility properties of finite-state transducers have been exploited to a more limited extent, starting with applications of non-weighted string-to-string transducers to morphological analysis and generation (Beesley, 1996). Concerning the application of weighted finitestate tree machines to NLU/NLG reversibility, our proposal is strongly related on the one hand to the approach of (Jones et al., 2012), who explicitely proposes tree-to-string transducers as a tool for modelling semantic parsing and for training on semantically annotated data, and on the other hand to (Wong, 2007; Wong and Mooney, 2007), who focus more directly on the problem of inverting a semantic parser into a generator. Wong et al. do not explicitely use tree-based transducers, but rather a formalism inspired by SCFGs (synchronous context-free grammars), which essentially corresponds to a form of tree-to-string transducer. In relation to reversibility considerations, presentations in terms of synchronous formalisms have </context>
</contexts>
<marker>Jones, Johnson, Goldwater, 2012</marker>
<rawString>Bevan Keeley Jones, Mark Johnson, and Sharon Goldwater. 2012. Semantic parsing with bayesian tree transducers. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1, ACL ’12, pages 488–496, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Marco Kuhlmann</author>
</authors>
<title>A generalized view on parsing and translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 12th International Conference on Parsing Technologies, IWPT ’11,</booktitle>
<pages>2--13</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18642" citStr="Koller and Kuhlmann, 2011" startWordPosition="2986" endWordPosition="2989">), who focus more directly on the problem of inverting a semantic parser into a generator. Wong et al. do not explicitely use tree-based transducers, but rather a formalism inspired by SCFGs (synchronous context-free grammars), which essentially corresponds to a form of tree-to-string transducer. In relation to reversibility considerations, presentations in terms of synchronous formalisms have the interest that they are intrinsically symmetrical. Such formalisms have tight relations to tree-transducers (Shieber, 2004); one recently proposed generalization, “Interpreted Regular Tree Grammars” (Koller and Kuhlmann, 2011), allows 1993 multiple (possibly more than two) synchronized views of an underlying abstract derivation tree, and has the advantage of permitting a uniform treatment of strings and trees. One important aspect in which our proposal differs from these previous approaches is in proposing to decouple the “core” task of mapping logical forms to well-formed latent canonical realizations from the task of relating these realizations to actual utterances, through an additional “similarity” transducer acting as a bridge. This idea of a bridge is however close to another line of work in semantic parsing,</context>
</contexts>
<marker>Koller, Kuhlmann, 2011</marker>
<rawString>Alexander Koller and Marco Kuhlmann. 2011. A generalized view on parsing and translation. In Proceedings of the 12th International Conference on Parsing Technologies, IWPT ’11, pages 2–13, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W J M Levelt</author>
</authors>
<title>Monitoring and self-repair in speech.</title>
<date>1983</date>
<journal>Cognition,</journal>
<volume>14</volume>
<issue>1</issue>
<contexts>
<context position="9680" citStr="Levelt, 1983" startWordPosition="1512" endWordPosition="1513">ly that we can efficiently sample from the product (i.e. intersection) p(x).q(x), but we can in case p and q are both represented by weighted FSAs. 1991 finite-state modules, which attempts to satisfy the definition given above for probabilistic reversibility, to address the problem of robustness that we described earlier, and can also support contextual preferences. We illustrate the approach with some simple examples of human-machine dialogues (between a customer and a virtual agent), a domain for which reversibility has high relevance, due to effects such as self-monitoring (Neumann, 1998; Levelt, 1983), interleaving of understanding and generation (Otsuka and Purver, 2003), and lexical entrainment (Brennan, 1996). w C µ A z _ y _ x Figure 1: Reversible specification through finitestate factors. The conceptual architecture is shown in Figure 1. Formally, the figure represents a probabilistic graphical model in so-called factor form, where the factors are w, n, a, A (we have also indicated for future reference the “contextual” factors C, µ, that we ignore for now). The factors take as arguments three types of objects: z is a logical form, that is, a structured object which can be naturally re</context>
</contexts>
<marker>Levelt, 1983</marker>
<rawString>W.J.M Levelt. 1983. Monitoring and self-repair in speech. Cognition, 14(1):41–104.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Survey: Tree transducers in machine translation.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>Universitat Rovira i Virgili.</institution>
<contexts>
<context position="10651" citStr="Maletti, 2010" startWordPosition="1674" endWordPosition="1675"> w, n, a, A (we have also indicated for future reference the “contextual” factors C, µ, that we ignore for now). The factors take as arguments three types of objects: z is a logical form, that is, a structured object which can be naturally represented as a tree, x is a surface string, and y is a latent “underlying” string that corresponds to one of a small collection of “canonical” texts for realizing the logical form z (more about that later). Each factor is realized through a weighted finite-state machine (acceptor or transducer) over strings or trees (Mohri, 2009; F¨ul¨op and Vogler, 2009; Maletti, 2010; Graehl et al., 2008). The A factor is a string automaton that represents a standard ngram language model (typically specific to domain), in other words a probability distribution over utterances x. Symmetrically, the regular tree automaton w represents a distribution over logical forms z, which can be seen as playing a similar role to the language model, but at the semantic level, namely telling us what are the possible/likely logical forms in a certain domain.6 The “canonical factor” n is a weighted treeto-string transducer (Graehl et al., 2008), which implements a relation between logical </context>
<context position="15523" citStr="Maletti, 2010" startWordPosition="2512" endWordPosition="2513"> a final weighted “x0-parser” automaton over z, representing a probability distribution from which we can draw exact samples as explained above.8 Generation works in exactly the reverse way, starting from a zo and eventually building a “z0-generator” automaton over x. In the actual proposal, z is a tree, meaning that w is a tree automaton, and K a tree-to-string transducer. While finite-state tree automata correspond to a single concept, and share all the nice properties of string automata (Comon et al., 2007), the situation with tree-to-tree or tree-to-string transducers is more complicated (Maletti, 2010; Graehl et al., 2008): several variants exist, only some of which support the operations that our conceptual model requires (composition with the string transducer Q and intersection with the tree automaton w). In particular, the “linear non-deleting topdown tree transducers” defined in (Maletti, 2010)9 have the requisite properties. Contextual factors We now briefly come back to the factors ( (tree automaton) and µ (string automaton) of Figure 1, which highlight the use8We could also have precompiled a generic parser for all x’s by first marginalizing the latent variable y through a composit</context>
<context position="17383" citStr="Maletti, 2010" startWordPosition="2805" endWordPosition="2806">ical form, to be combined with the actual customer’s utterance. Symmetrically, the µ factor can be used to represent such phenomena as lexical entrainment (Brennan, 1996), where the agent’s utterance is oriented towards using similar wordings to the customer’s. 5 Related work The unique formal properties of finite-state machines, which favor modular decompositions of complex tasks, have long been exploited in Computational Linguistics. Tree transducers in particular have gained popularity in Statistical Machine Translation, starting with (Yamada and Knight, 2001), as described in the surveys (Maletti, 2010; Razmara, 2011). The reversibility properties of finite-state transducers have been exploited to a more limited extent, starting with applications of non-weighted string-to-string transducers to morphological analysis and generation (Beesley, 1996). Concerning the application of weighted finitestate tree machines to NLU/NLG reversibility, our proposal is strongly related on the one hand to the approach of (Jones et al., 2012), who explicitely proposes tree-to-string transducers as a tool for modelling semantic parsing and for training on semantically annotated data, and on the other hand to (</context>
</contexts>
<marker>Maletti, 2010</marker>
<rawString>Andreas Maletti. 2010. Survey: Tree transducers in machine translation. Technical report, Universitat Rovira i Virgili.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Edit-Distance of Weighted Automata: General Definitions and Algorithms.</title>
<date>2003</date>
<journal>International Journal of Foundations of Computer Science,</journal>
<pages>14--957</pages>
<contexts>
<context position="13441" citStr="Mohri, 2003" startWordPosition="2164" endWordPosition="2165">= What is the screen size of the Galaxy Trend?. Overall, the canonical factor n(z, y) concentrates more on a core “generation model”, namely on producing some well-formed output y from a logical form z, while the similarity factor a(y, x) allows relating an actual user input x to a possible output y of the n model. The main import of a is then to allow to use the core generation model defined by n to be exploited for robust semantic parsing. Different instantiations of this scheme can be employed. In some preliminary experiments that we have performed,7 a is a simple edit-distance transducer (Mohri, 2003) which penalizes differently the discrepancies between x and y: strongly for some salient content words or named entities of the domain, weakly for less relevant content words and for non-content words, with limited use of local paraphrases (which can also be implemented 7In these experiments, we used string-based approximations of the logical forms, and only employed string-based transducers from the OpenFST library. n a a a a a 1992 through Q). This strategy seems to work reasonably well when the semantical repertory of the domain is restricted, because a large number of possible variants fo</context>
</contexts>
<marker>Mohri, 2003</marker>
<rawString>Mehryar Mohri. 2003. Edit-Distance of Weighted Automata: General Definitions and Algorithms. International Journal of Foundations of Computer Science, 14:957–982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
</authors>
<title>Weighted automata algorithms.</title>
<date>2009</date>
<booktitle>Handbook of Weighted Automata,</booktitle>
<pages>213--254</pages>
<editor>In Manfred Droste, Werner Kuich, and Heiko Vogler, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="10610" citStr="Mohri, 2009" startWordPosition="1668" endWordPosition="1669">lled factor form, where the factors are w, n, a, A (we have also indicated for future reference the “contextual” factors C, µ, that we ignore for now). The factors take as arguments three types of objects: z is a logical form, that is, a structured object which can be naturally represented as a tree, x is a surface string, and y is a latent “underlying” string that corresponds to one of a small collection of “canonical” texts for realizing the logical form z (more about that later). Each factor is realized through a weighted finite-state machine (acceptor or transducer) over strings or trees (Mohri, 2009; F¨ul¨op and Vogler, 2009; Maletti, 2010; Graehl et al., 2008). The A factor is a string automaton that represents a standard ngram language model (typically specific to domain), in other words a probability distribution over utterances x. Symmetrically, the regular tree automaton w represents a distribution over logical forms z, which can be seen as playing a similar role to the language model, but at the semantic level, namely telling us what are the possible/likely logical forms in a certain domain.6 The “canonical factor” n is a weighted treeto-string transducer (Graehl et al., 2008), whi</context>
<context position="14700" citStr="Mohri, 2009" startWordPosition="2372" endWordPosition="2373">ntics. In domains where small nuances of expression may result in distinct semantics, the division of work between K and Q may be different. Parsing and Generation To understand the reversibility properties of the model of Figure 1, let us first simplify the description by assuming that z, instead of being a tree, is actually a string. Then both w and A are string automata, and both K and Q string-to-string transducers. Such a specification satisfies our definition of probabilistic reversibility, exploiting well-known compositionality properties of weighted finite-state machines over strings (Mohri, 2009). For parsing, we start from a fixed xo, and can project it through Q into a weighted FSA over y; in turn we can project this automaton onto an FSA over z, and finally intersect this automaton with w, obtaining a final weighted “x0-parser” automaton over z, representing a probability distribution from which we can draw exact samples as explained above.8 Generation works in exactly the reverse way, starting from a zo and eventually building a “z0-generator” automaton over x. In the actual proposal, z is a tree, meaning that w is a tree automaton, and K a tree-to-string transducer. While finite-</context>
</contexts>
<marker>Mohri, 2009</marker>
<rawString>Mehryar Mohri. 2009. Weighted automata algorithms. In Manfred Droste, Werner Kuich, and Heiko Vogler, editors, Handbook of Weighted Automata, pages 213–254. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨unter Neumann</author>
</authors>
<title>Interleaving natural language parsing and generation through uniform processing.</title>
<date>1998</date>
<journal>Artificial Intelligence,</journal>
<volume>99</volume>
<issue>1</issue>
<contexts>
<context position="9665" citStr="Neumann, 1998" startWordPosition="1510" endWordPosition="1511">)) does not imply that we can efficiently sample from the product (i.e. intersection) p(x).q(x), but we can in case p and q are both represented by weighted FSAs. 1991 finite-state modules, which attempts to satisfy the definition given above for probabilistic reversibility, to address the problem of robustness that we described earlier, and can also support contextual preferences. We illustrate the approach with some simple examples of human-machine dialogues (between a customer and a virtual agent), a domain for which reversibility has high relevance, due to effects such as self-monitoring (Neumann, 1998; Levelt, 1983), interleaving of understanding and generation (Otsuka and Purver, 2003), and lexical entrainment (Brennan, 1996). w C µ A z _ y _ x Figure 1: Reversible specification through finitestate factors. The conceptual architecture is shown in Figure 1. Formally, the figure represents a probabilistic graphical model in so-called factor form, where the factors are w, n, a, A (we have also indicated for future reference the “contextual” factors C, µ, that we ignore for now). The factors take as arguments three types of objects: z is a logical form, that is, a structured object which can </context>
</contexts>
<marker>Neumann, 1998</marker>
<rawString>G¨unter Neumann. 1998. Interleaving natural language parsing and generation through uniform processing. Artificial Intelligence, 99(1):121–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Otsuka</author>
<author>M Purver</author>
</authors>
<title>Incremental generation by incremental parsing.</title>
<date>2003</date>
<booktitle>In Proceedings 6th CLUK Colloquium.</booktitle>
<contexts>
<context position="9752" citStr="Otsuka and Purver, 2003" startWordPosition="1520" endWordPosition="1523">rsection) p(x).q(x), but we can in case p and q are both represented by weighted FSAs. 1991 finite-state modules, which attempts to satisfy the definition given above for probabilistic reversibility, to address the problem of robustness that we described earlier, and can also support contextual preferences. We illustrate the approach with some simple examples of human-machine dialogues (between a customer and a virtual agent), a domain for which reversibility has high relevance, due to effects such as self-monitoring (Neumann, 1998; Levelt, 1983), interleaving of understanding and generation (Otsuka and Purver, 2003), and lexical entrainment (Brennan, 1996). w C µ A z _ y _ x Figure 1: Reversible specification through finitestate factors. The conceptual architecture is shown in Figure 1. Formally, the figure represents a probabilistic graphical model in so-called factor form, where the factors are w, n, a, A (we have also indicated for future reference the “contextual” factors C, µ, that we ignore for now). The factors take as arguments three types of objects: z is a logical form, that is, a structured object which can be naturally represented as a tree, x is a surface string, and y is a latent “underlyin</context>
</contexts>
<marker>Otsuka, Purver, 2003</marker>
<rawString>M. Otsuka and M. Purver. 2003. Incremental generation by incremental parsing. In Proceedings 6th CLUK Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Majid Razmara</author>
</authors>
<title>Applications of tree transducers in statistical machine translation.</title>
<date>2011</date>
<tech>Technical report,</tech>
<institution>Simon Fraser University.</institution>
<contexts>
<context position="17399" citStr="Razmara, 2011" startWordPosition="2807" endWordPosition="2808">e combined with the actual customer’s utterance. Symmetrically, the µ factor can be used to represent such phenomena as lexical entrainment (Brennan, 1996), where the agent’s utterance is oriented towards using similar wordings to the customer’s. 5 Related work The unique formal properties of finite-state machines, which favor modular decompositions of complex tasks, have long been exploited in Computational Linguistics. Tree transducers in particular have gained popularity in Statistical Machine Translation, starting with (Yamada and Knight, 2001), as described in the surveys (Maletti, 2010; Razmara, 2011). The reversibility properties of finite-state transducers have been exploited to a more limited extent, starting with applications of non-weighted string-to-string transducers to morphological analysis and generation (Beesley, 1996). Concerning the application of weighted finitestate tree machines to NLU/NLG reversibility, our proposal is strongly related on the one hand to the approach of (Jones et al., 2012), who explicitely proposes tree-to-string transducers as a tool for modelling semantic parsing and for training on semantically annotated data, and on the other hand to (Wong, 2007; Wong</context>
</contexts>
<marker>Razmara, 2011</marker>
<rawString>Majid Razmara. 2011. Applications of tree transducers in statistical machine translation. Technical report, Simon Fraser University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian P Robert</author>
<author>George Casella</author>
</authors>
<title>Monte Carlo Statistical Methods (Springer Texts in Statistics).</title>
<date>2004</date>
<publisher>Springer-Verlag</publisher>
<location>New York, Inc., Secaucus, NJ, USA.</location>
<contexts>
<context position="6280" citStr="Robert and Casella, 2004" startWordPosition="973" endWordPosition="976"> z given x, i.e. to return argmaxz p(z|x); however sampling is the most direct way of providing a concrete view of the underlying probabilistic distribution, and has many applications to learning, so we think the definition above is reasonable (see also footnote4). 2We note the “semi-formal” aspect of this definition: contrarily to the classical case, which has a formal notion of effective computation, there is no universally accepted notion of effective sampling from a probability distribution. For many probability distributions, the only feasible sampling approaches are the MCMC techniques (Robert and Casella, 2004), which typically do not come with convergence guarantees; in some situations, exact sampling techniques are applicable, which come with much better guarantees. We will see that the approach proposed in section 4 allows such exact sampling to take place. 4 Finite-state models for reversibility Finite-state transducers have properties which make them uniquely suited to implementing reversible linguistic specifications in the above sense. Consider a simple weighted string-tostring transducer T(s, t), where s, t are strings, and where the underlying semiring is the “probabilistic semiring” over t</context>
</contexts>
<marker>Robert, Casella, 2004</marker>
<rawString>Christian P. Robert and George Casella. 2004. Monte Carlo Statistical Methods (Springer Texts in Statistics). Springer-Verlag New York, Inc., Secaucus, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Synchronous grammars as tree transducers.</title>
<date>2004</date>
<booktitle>In Proceedings of the Seventh International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+ 7),</booktitle>
<location>Vancouver, Canada,</location>
<contexts>
<context position="18539" citStr="Shieber, 2004" startWordPosition="2975" endWordPosition="2976">on semantically annotated data, and on the other hand to (Wong, 2007; Wong and Mooney, 2007), who focus more directly on the problem of inverting a semantic parser into a generator. Wong et al. do not explicitely use tree-based transducers, but rather a formalism inspired by SCFGs (synchronous context-free grammars), which essentially corresponds to a form of tree-to-string transducer. In relation to reversibility considerations, presentations in terms of synchronous formalisms have the interest that they are intrinsically symmetrical. Such formalisms have tight relations to tree-transducers (Shieber, 2004); one recently proposed generalization, “Interpreted Regular Tree Grammars” (Koller and Kuhlmann, 2011), allows 1993 multiple (possibly more than two) synchronized views of an underlying abstract derivation tree, and has the advantage of permitting a uniform treatment of strings and trees. One important aspect in which our proposal differs from these previous approaches is in proposing to decouple the “core” task of mapping logical forms to well-formed latent canonical realizations from the task of relating these realizations to actual utterances, through an additional “similarity” transducer </context>
</contexts>
<marker>Shieber, 2004</marker>
<rawString>Stuart M. Shieber. 2004. Synchronous grammars as tree transducers. In Proceedings of the Seventh International Workshop on Tree Adjoining Grammar and Related Formalisms (TAG+ 7), Vancouver, Canada, 20–22 May.</rawString>
</citation>
<citation valid="true">
<date>1994</date>
<booktitle>Reversible Grammar in Natural Language Processing.</booktitle>
<editor>Tomek Strzalkowski, editor.</editor>
<publisher>Springer.</publisher>
<marker>1994</marker>
<rawString>Tomek Strzalkowski, editor. 1994. Reversible Grammar in Natural Language Processing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Reversible unification based machine translation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th Conference on Computational Linguistics - Volume 2, COLING ’90,</booktitle>
<pages>299--304</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>van Noord, 1990</marker>
<rawString>Gertjan van Noord. 1990. Reversible unification based machine translation. In Proceedings of the 13th Conference on Computational Linguistics - Volume 2, COLING ’90, pages 299–304, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>J Berant</author>
<author>P Liang</author>
</authors>
<title>Building a semantic parser overnight.</title>
<date>2015</date>
<booktitle>In Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="19315" citStr="Wang et al., 2015" startWordPosition="3093" endWordPosition="3096">nized views of an underlying abstract derivation tree, and has the advantage of permitting a uniform treatment of strings and trees. One important aspect in which our proposal differs from these previous approaches is in proposing to decouple the “core” task of mapping logical forms to well-formed latent canonical realizations from the task of relating these realizations to actual utterances, through an additional “similarity” transducer acting as a bridge. This idea of a bridge is however close to another line of work in semantic parsing, not transducer based, namely (Berant and Liang, 2014; Wang et al., 2015). There, a simple generic grammar is used to generate canonical realizations from a repertory of possible logical forms (expressed in a variant of lambda calculus). Given an input to parse, simple heuristics are used to select a finite list of potential logical forms which are then ranked according to the (paraphrase-based) similarity of their associated canonical realization with the input. Thus in this approach, a form of generation plays an important role, not for its own sake, but as a tool for semantic parsing. 6 Conclusion Because of their unique compositional properties, finite-state mo</context>
</contexts>
<marker>Wang, Berant, Liang, 2015</marker>
<rawString>Y. Wang, J. Berant, and P. Liang. 2015. Building a semantic parser overnight. In Association for Computational Linguistics (ACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Generation by inverting a semantic parser that uses statistical machine translation.</title>
<date>2007</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>172--179</pages>
<contexts>
<context position="18017" citStr="Wong and Mooney, 2007" startWordPosition="2898" endWordPosition="2901">011). The reversibility properties of finite-state transducers have been exploited to a more limited extent, starting with applications of non-weighted string-to-string transducers to morphological analysis and generation (Beesley, 1996). Concerning the application of weighted finitestate tree machines to NLU/NLG reversibility, our proposal is strongly related on the one hand to the approach of (Jones et al., 2012), who explicitely proposes tree-to-string transducers as a tool for modelling semantic parsing and for training on semantically annotated data, and on the other hand to (Wong, 2007; Wong and Mooney, 2007), who focus more directly on the problem of inverting a semantic parser into a generator. Wong et al. do not explicitely use tree-based transducers, but rather a formalism inspired by SCFGs (synchronous context-free grammars), which essentially corresponds to a form of tree-to-string transducer. In relation to reversibility considerations, presentations in terms of synchronous formalisms have the interest that they are intrinsically symmetrical. Such formalisms have tight relations to tree-transducers (Shieber, 2004); one recently proposed generalization, “Interpreted Regular Tree Grammars” (K</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J Mooney. 2007. Generation by inverting a semantic parser that uses statistical machine translation. In HLT-NAACL, pages 172–179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
</authors>
<title>Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Sciences, University of Texas at Austin,</institution>
<location>Austin, TX,</location>
<contexts>
<context position="17993" citStr="Wong, 2007" startWordPosition="2896" endWordPosition="2897">; Razmara, 2011). The reversibility properties of finite-state transducers have been exploited to a more limited extent, starting with applications of non-weighted string-to-string transducers to morphological analysis and generation (Beesley, 1996). Concerning the application of weighted finitestate tree machines to NLU/NLG reversibility, our proposal is strongly related on the one hand to the approach of (Jones et al., 2012), who explicitely proposes tree-to-string transducers as a tool for modelling semantic parsing and for training on semantically annotated data, and on the other hand to (Wong, 2007; Wong and Mooney, 2007), who focus more directly on the problem of inverting a semantic parser into a generator. Wong et al. do not explicitely use tree-based transducers, but rather a formalism inspired by SCFGs (synchronous context-free grammars), which essentially corresponds to a form of tree-to-string transducer. In relation to reversibility considerations, presentations in terms of synchronous formalisms have the interest that they are intrinsically symmetrical. Such formalisms have tight relations to tree-transducers (Shieber, 2004); one recently proposed generalization, “Interpreted R</context>
</contexts>
<marker>Wong, 2007</marker>
<rawString>Yuk Wah Wong. 2007. Learning for Semantic Parsing and Natural Language Generation Using Statistical Machine Translation Techniques. Ph.D. thesis, Department of Computer Sciences, University of Texas at Austin, Austin, TX, August. Also appears as Technical Report AI07-343, Artificial Intelligence Lab, University of Texas at Austin, August 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntaxbased statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL ’01,</booktitle>
<pages>523--530</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="17339" citStr="Yamada and Knight, 2001" startWordPosition="2796" endWordPosition="2799">expectations of the dialogue manager about the next logical form, to be combined with the actual customer’s utterance. Symmetrically, the µ factor can be used to represent such phenomena as lexical entrainment (Brennan, 1996), where the agent’s utterance is oriented towards using similar wordings to the customer’s. 5 Related work The unique formal properties of finite-state machines, which favor modular decompositions of complex tasks, have long been exploited in Computational Linguistics. Tree transducers in particular have gained popularity in Statistical Machine Translation, starting with (Yamada and Knight, 2001), as described in the surveys (Maletti, 2010; Razmara, 2011). The reversibility properties of finite-state transducers have been exploited to a more limited extent, starting with applications of non-weighted string-to-string transducers to morphological analysis and generation (Beesley, 1996). Concerning the application of weighted finitestate tree machines to NLU/NLG reversibility, our proposal is strongly related on the one hand to the approach of (Jones et al., 2012), who explicitely proposes tree-to-string transducers as a tool for modelling semantic parsing and for training on semanticall</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntaxbased statistical translation model. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL ’01, pages 523–530, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.988161">
Effective Approaches to Attention-based Neural Machine Translation
</title>
<author confidence="0.994024">
Minh-Thang Luong Hieu Pham Christopher D. Manning
</author>
<affiliation confidence="0.993854">
Computer Science Department, Stanford University, Stanford, CA 94305
</affiliation>
<email confidence="0.997496">
{lmthang,hyhieu,manning}@stanford.edu
</email>
<sectionHeader confidence="0.994768" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972269230769">
An attentional mechanism has lately been
used to improve neural machine transla-
tion (NMT) by selectively focusing on
parts of the source sentence during trans-
lation. However, there has been little
work exploring useful architectures for
attention-based NMT. This paper exam-
ines two simple and effective classes of at-
tentional mechanism: a global approach
which always attends to all source words
and a local one that only looks at a subset
of source words at a time. We demonstrate
the effectiveness of both approaches on the
WMT translation tasks between English
and German in both directions. With local
attention, we achieve a significant gain of
5.0 BLEU points over non-attentional sys-
tems that already incorporate known tech-
niques such as dropout. Our ensemble
model using different attention architec-
tures yields a new state-of-the-art result in
the WMT’15 English to German transla-
tion task with 25.9 BLEU points, an im-
provement of 1.0 BLEU points over the
existing best system backed by NMT and
an n-gram reranker.1
</bodyText>
<sectionHeader confidence="0.998873" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999193">
Neural Machine Translation (NMT) achieved
state-of-the-art performances in large-scale trans-
lation tasks such as from English to French (Luong
et al., 2015) and English to German (Jean et al.,
2015). NMT is appealing since it requires minimal
domain knowledge and is conceptually simple.
The model by Luong et al. (2015) reads through all
the source words until the end-of-sentence symbol
&lt;eos&gt; is reached. It then starts emitting one tar-
get word at a time, as illustrated in Figure 1. NMT
</bodyText>
<footnote confidence="0.816122">
1All our code and models are publicly available at http:
//nlp.stanford.edu/projects/nmt.
</footnote>
<figure confidence="0.666636">
X Y Z &lt;eos&gt;
A B C D &lt;eos&gt; X Y Z
</figure>
<figureCaption confidence="0.898143">
Figure 1: Neural machine translation – a stack-
</figureCaption>
<bodyText confidence="0.970104931034483">
ing recurrent architecture for translating a source
sequence A B C D into a target sequence X Y
Z. Here, &lt;eos&gt; marks the end of a sentence.
is often a large neural network that is trained in an
end-to-end fashion and has the ability to general-
ize well to very long word sequences. This means
the model does not have to explicitly store gigantic
phrase tables and language models as in the case
of standard MT; hence, NMT has a small memory
footprint. Lastly, implementing NMT decoders is
easy unlike the highly intricate decoders in stan-
dard MT (Koehn et al., 2003).
In parallel, the concept of “attention” has gained
popularity recently in training neural networks, al-
lowing models to learn alignments between dif-
ferent modalities, e.g., between image objects
and agent actions in the dynamic control problem
(Mnih et al., 2014), between speech frames and
text in the speech recognition task (Chorowski et
al., 2014), or between visual features of a picture
and its text description in the image caption gen-
eration task (Xu et al., 2015). In the context of
NMT, Bahdanau et al. (2015) has successfully ap-
plied such attentional mechanism to jointly trans-
late and align words. To the best of our knowl-
edge, there has not been any other work exploring
the use of attention-based architectures for NMT.
In this work, we design, with simplicity and ef-
fectiveness in mind, two novel types of attention-
</bodyText>
<page confidence="0.952743">
1412
</page>
<note confidence="0.9849115">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.999888931034483">
based models: a global approach in which all
source words are attended and a local one whereby
only a subset of source words are considered at a
time. The former approach resembles the model
of (Bahdanau et al., 2015) but is simpler architec-
turally. The latter can be viewed as an interesting
blend between the hard and soft attention models
proposed in (Xu et al., 2015): it is computation-
ally less expensive than the global model or the
soft attention; at the same time, unlike the hard at-
tention, the local attention is differentiable, mak-
ing it easier to implement and train.2 Besides, we
also examine various alignment functions for our
attention-based models.
Experimentally, we demonstrate that both of
our approaches are effective in the WMT trans-
lation tasks between English and German in both
directions. Our attentional models yield a boost
of up to 5.0 BLEU over non-attentional systems
which already incorporate known techniques such
as dropout. For English to German translation,
we achieve new state-of-the-art (SOTA) results
for both WMT’14 and WMT’15, outperforming
previous SOTA systems, backed by NMT mod-
els and n-gram LM rerankers, by more than 1.0
BLEU. We conduct extensive analysis to evaluate
our models in terms of learning, the ability to han-
dle long sentences, choices of attentional architec-
tures, alignment quality, and translation outputs.
</bodyText>
<sectionHeader confidence="0.992064" genericHeader="method">
2 Neural Machine Translation
</sectionHeader>
<bodyText confidence="0.999767444444445">
A neural machine translation system is a neural
network that directly models the conditional prob-
ability p(y|x) of translating a source sentence,
x1, ... , xn, to a target sentence, y1, ... , ym.3 A
basic form of NMT consists of two components:
(a) an encoder which computes a representation s
for each source sentence and (b) a decoder which
generates one target word at a time and hence de-
composes the conditional probability as:
</bodyText>
<equation confidence="0.97115">
log p(y|x) = 1:m1 log p(yj|y&lt;j,s) (1)
=
</equation>
<bodyText confidence="0.994911666666667">
A natural choice to model such a decomposi-
tion in the decoder is to use a recurrent neural net-
work (RNN) architecture, which most of the re-
</bodyText>
<footnote confidence="0.9809225">
2There is a recent work by Gregor et al. (2015), which is
very similar to our local attention and applied to the image
generation task. However, as we detail later, our model is
much simpler and can achieve good performance for NMT.
3All sentences are assumed to terminate with a special
“end-of-sentence” token &lt;eos&gt;.
</footnote>
<bodyText confidence="0.998825857142857">
cent NMT work such as (Kalchbrenner and Blun-
som, 2013; Sutskever et al., 2014; Cho et al., 2014;
Bahdanau et al., 2015; Luong et al., 2015; Jean et
al., 2015) have in common. They, however, dif-
fer in terms of which RNN architectures are used
for the decoder and how the encoder computes the
source sentence representation s.
Kalchbrenner and Blunsom (2013) used an
RNN with the standard hidden unit for the decoder
and a convolutional neural network for encoding
the source sentence representation. On the other
hand, both Sutskever et al. (2014) and Luong et
al. (2015) stacked multiple layers of an RNN with
a Long Short-Term Memory (LSTM) hidden unit
for both the encoder and the decoder. Cho et al.
(2014), Bahdanau et al. (2015), and Jean et al.
(2015) all adopted a different version of the RNN
with an LSTM-inspired hidden unit, the gated re-
current unit (GRU), for both components.4
In more detail, one can parameterize the proba-
bility of decoding each word yj as:
</bodyText>
<equation confidence="0.967469">
p (yj|y&lt;j, s) = softmax (g (hj)) (2)
</equation>
<bodyText confidence="0.984064666666666">
with g being the transformation function that out-
puts a vocabulary-sized vector.5 Here, hj is the
RNN hidden unit, abstractly computed as:
</bodyText>
<equation confidence="0.991019">
hj = f(hj−1, s), (3)
</equation>
<bodyText confidence="0.99286452631579">
where f computes the current hidden state given
the previous hidden state and can be either a
vanilla RNN unit, a GRU, or an LSTM unit. In
(Kalchbrenner and Blunsom, 2013; Sutskever et
al., 2014; Cho et al., 2014; Luong et al., 2015),
the source representation s is only used once to
initialize the decoder hidden state. On the other
hand, in (Bahdanau et al., 2015; Jean et al., 2015)
and this work, s, in fact, implies a set of source
hidden states which are consulted throughout the
entire course of the translation process. Such an
approach is referred to as an attention mechanism,
which we will discuss next.
In this work, following (Sutskever et al., 2014;
Luong et al., 2015), we use the stacking LSTM
architecture for our NMT systems, as illustrated
in Figure 1. We use the LSTM unit defined in
(Zaremba et al., 2015). Our training objective is
formulated as follows:
</bodyText>
<equation confidence="0.9664765">
1:
Jt = (x,y)∈D − log p(y|x) (4)
</equation>
<footnote confidence="0.9988665">
4They all used a single RNN layer except for the latter two
works which utilized a bidirectional RNN for the encoder.
5One can provide g with other inputs such as the currently
predicted word yj as in (Bahdanau et al., 2015).
</footnote>
<page confidence="0.984575">
1413
</page>
<bodyText confidence="0.972658">
with D being our parallel training corpus.
</bodyText>
<sectionHeader confidence="0.99882" genericHeader="method">
3 Attention-based Models
</sectionHeader>
<bodyText confidence="0.9999432">
Our various attention-based models are classifed
into two broad categories, global and local. These
classes differ in terms of whether the “attention”
is placed on all source positions or on only a few
source positions. We illustrate these two model
types in Figure 2 and 3 respectively.
Common to these two types of models is the fact
that at each time step t in the decoding phase, both
approaches first take as input the hidden state ht
at the top layer of a stacking LSTM. The goal is
then to derive a context vector ct that captures rel-
evant source-side information to help predict the
current target word yt. While these models differ
in how the context vector ct is derived, they share
the same subsequent steps.
Specifically, given the target hidden state ht and
the source-side context vector ct, we employ a
simple concatenation layer to combine the infor-
mation from both vectors to produce an attentional
hidden state as follows:
</bodyText>
<equation confidence="0.984566">
˜ht = tanh(Wc[ct; ht]) (5)
</equation>
<bodyText confidence="0.999848">
The attentional vector ˜ht is then fed through the
softmax layer to produce the predictive distribu-
tion formulated as:
</bodyText>
<equation confidence="0.992192">
p(yt|y&lt;t,x) = softmax(Ws ˜ht) (6)
</equation>
<bodyText confidence="0.999783">
We now detail how each model type computes
the source-side context vector ct.
</bodyText>
<subsectionHeader confidence="0.997661">
3.1 Global Attention
</subsectionHeader>
<bodyText confidence="0.999986285714286">
The idea of a global attentional model is to con-
sider all the hidden states of the encoder when de-
riving the context vector ct. In this model type,
a variable-length alignment vector at, whose size
equals the number of time steps on the source side,
is derived by comparing the current target hidden
state ht with each source hidden state ¯hs:
</bodyText>
<equation confidence="0.897653666666667">
at(s) = align(ht, ¯hs) (7)
exp (score(ht, ¯hs))
= Es′ exp (score(ht, ¯hs′))
</equation>
<bodyText confidence="0.999348">
Here, score is referred as a content-based function
for which we consider three different alternatives:
</bodyText>
<footnote confidence="0.902217666666667">
h⊤t ¯hs dot
h⊤t Wa¯hs general (8)
Wa[ht; ¯hs] concat
</footnote>
<figureCaption confidence="0.855984">
Figure 2: Global attentional model – at each time
</figureCaption>
<bodyText confidence="0.966304205882353">
step t, the model infers a variable-length align-
ment weight vector at based on the current target
state ht and all source states ¯hs. A global context
vector ct is then computed as the weighted aver-
age, according to at, over all the source states.
Besides, in our early attempts to build attention-
based models, we use a location-based function
in which the alignment scores are computed from
solely the target hidden state ht as follows:
at = softmax(Waht) location (9)
Given the alignment vector as weights, the context
vector ct is computed as the weighted average over
all the source hidden states.6
Comparison to (Bahdanau et al., 2015) – While
our global attention approach is similar in spirit
to the model proposed by Bahdanau et al. (2015),
there are several key differences which reflect how
we have both simplified and generalized from the
original model. First, we simply use hidden states
at the top LSTM layers in both the encoder and
decoder as illustrated in Figure 2. Bahdanau et
al. (2015), on the other hand, use the concatena-
tion of the forward and backward source hidden
states in the bi-directional encoder and target hid-
den states in their non-stacking uni-directional de-
coder. Second, our computation path is simpler;
we go from ht —* at —* ct —* ˜ht then make
a prediction as detailed in Eq. (5), Eq. (6), and
Figure 2. On the other hand, at any time t, Bah-
danau et al. (2015) build from the previous hidden
state ht−1 —* at —* ct —* ht, which, in turn,
6Eq. (9) implies that all alignment vectors at are of the
same length. For short sentences, we only use the top part of
at and for long sentences, we ignore words near the end.
</bodyText>
<figure confidence="0.9515759">
hs
ct
Attention Layer
Context vector
Global align weights
at
yt
˜ht
ht
score(ht, hs) = {
</figure>
<page confidence="0.808768">
1414
</page>
<figureCaption confidence="0.907038">
Figure 3: Local attention model – the model first
</figureCaption>
<bodyText confidence="0.8109005">
predicts a single aligned position pt for the current
target word. A window centered around the source
position pt is then used to compute a context vec-
tor ct, a weighted average of the source hidden
states in the window. The weights at are inferred
from the current target state ht and those source
states ¯hs in the window.
goes through a deep-output and a maxout layer
before making predictions.7 Lastly, Bahdanau et
al. (2015) only experimented with one alignment
function, the concat product; whereas we show
later that the other alternatives are better.
</bodyText>
<subsectionHeader confidence="0.999658">
3.2 Local Attention
</subsectionHeader>
<bodyText confidence="0.99997845">
The global attention has a drawback that it has to
attend to all words on the source side for each tar-
get word, which is expensive and can potentially
render it impractical to translate longer sequences,
e.g., paragraphs or documents. To address this
deficiency, we propose a local attentional mech-
anism that chooses to focus only on a small subset
of the source positions per target word.
This model takes inspiration from the tradeoff
between the soft and hard attentional models pro-
posed by Xu et al. (2015) to tackle the image cap-
tion generation task. In their work, soft attention
refers to the global attention approach in which
weights are placed “softly” over all patches in the
source image. The hard attention, on the other
hand, selects one patch of the image to attend to at
a time. While less expensive at inference time, the
hard attention model is non-differentiable and re-
quires more complicated techniques such as vari-
ance reduction or reinforcement learning to train.
</bodyText>
<footnote confidence="0.525042">
7We will refer to this difference again in Section 3.3.
</footnote>
<bodyText confidence="0.999932952380953">
Our local attention mechanism selectively fo-
cuses on a small window of context and is differ-
entiable. This approach has an advantage of avoid-
ing the expensive computation incurred in the soft
attention and at the same time, is easier to train
than the hard attention approach. In concrete de-
tails, the model first generates an aligned position
pt for each target word at time t. The context vec-
tor ct is then derived as a weighted average over
the set of source hidden states within the window
[pt−D, pt+D]; D is empirically selected.$ Unlike
the global approach, the local alignment vector at
is now fixed-dimensional, i.e., E R2D+1. We con-
sider two variants of the model as below.
Monotonic alignment (local-m) – we simply set
pt = t assuming that source and target sequences
are roughly monotonically aligned. The alignment
vector at is defined according to Eq. (7).9
Predictive alignment (local-p) – instead of as-
suming monotonic alignments, our model predicts
an aligned position as follows:
</bodyText>
<equation confidence="0.99691">
pt = 5 · sigmoid(v⊤p tanh(Wpht)), (10)
</equation>
<bodyText confidence="0.998760333333333">
Wp and vp are the model parameters which will
be learned to predict positions. 5 is the source sen-
tence length. As a result of sigmoid, pt E [0, 5].
To favor alignment points near pt, we place a
Gaussian distribution centered around pt . Specif-
ically, our alignment weights are now defined as:
</bodyText>
<equation confidence="0.946003">
2
at (s) = align (ht, hs) exp (− (s 2 − a2 t) 1 (11)
</equation>
<bodyText confidence="0.842525217391304">
We use the same align function as in Eq. J(7) and
the standard deviation is empirically set as a = D2 .
It is important to note that pt is a real nummber;
whereas s is an integer within the window cen-
tered at pt.10
Comparison to (Gregor et al., 2015) – have pro-
posed a selective attention mechanism, very simi-
lar to our local attention, for the image generation
task. Their approach allows the model to select an
image patch of varying location and zoom. We,
instead, use the same “zoom” for all target posi-
tions, which greatly simplifies the formulation and
still achieves good performance.
$If the window crosses the sentence boundaries, we sim-
ply ignore the outside part and consider words in the window.
9local-m is the same as the global model except that the
vector at is fixed-length and shorter.
10local-p is similar to the local-m model except that we
dynamically compute pt and use a Gaussian distribution
to modify the original alignment weights align(ht,
¯hs)
shown in Eq. (11). By utilizing pt to derive at, we can com-
pute backprop gradients for WT, and vp.
</bodyText>
<figure confidence="0.969994214285714">
hs
ct
Attention Layer
at
Context vector
Local weights
Aligned position
pt
yt
˜ht
ht
as
1415
X Y Z &lt;eos&gt;
</figure>
<figureCaption confidence="0.967255666666667">
Figure 4: Input-feeding approach – Attentional
vectors ˜ht are fed as inputs to the next time steps to
inform the model about past alignment decisions.
</figureCaption>
<subsectionHeader confidence="0.990419">
3.3 Input-feeding Approach
</subsectionHeader>
<bodyText confidence="0.990583648648648">
In our proposed global and local approaches,
the attentional decisions are made independently,
which is suboptimal. Whereas, in standard MT,
a coverage set is often maintained during the
translation process to keep track of which source
words have been translated. Likewise, in atten-
tional NMTs, alignment decisions should be made
jointly taking into account past alignment infor-
mation. To address that, we propose an input-
feeding approach in which attentional vectors ˜ht
are concatenated with inputs at the next time steps
as illustrated in Figure 4.11 The effects of hav-
ing such connections are two-fold: (a) we hope
to make the model fully aware of previous align-
ment choices and (b) we create a very deep net-
work spanning both horizontally and vertically.
Comparison to other work – Bahdanau et al.
(2015) use context vectors, similar to our ct, in
building subsequent hidden states, which can also
achieve the “coverage” effect. However, there has
not been any analysis of whether such connections
are useful as done in this work. Also, our approach
is more general; as illustrated in Figure 4, it can be
applied to general stacking recurrent architectures,
including non-attentional models.
Xu et al. (2015) propose a doubly attentional
approach with an additional constraint added to
the training objective to make sure the model pays
equal attention to all parts of the image during the
caption generation process. Such a constraint can
11If n is the number of LSTM cells, the input size of the
first LSTM layer is 2n; those of subsequent layers are n.
also be useful to capture the coverage set effect
in NMT that we mentioned earlier. However, we
chose to use the input-feeding approach since it
provides flexibility for the model to decide on any
attentional constraints it deems suitable.
</bodyText>
<sectionHeader confidence="0.999228" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999911666666667">
We evaluate the effectiveness of our models on the
WMT translation tasks between English and Ger-
man in both directions. newstest2013 (3000 sen-
tences) is used as a development set to select our
hyperparameters. Translation performances are
reported in case-sensitive BLEU (Papineni et al.,
2002) on newstest2014 (2737 sentences) and new-
stest2015 (2169 sentences). Following (Luong et
al., 2015), we report translation quality using two
types of BLEU: (a) tokenized12 BLEU to be com-
parable with existing NMT work and (b) NIST13
BLEU to be comparable with WMT results.
</bodyText>
<subsectionHeader confidence="0.99443">
4.1 Training Details
</subsectionHeader>
<bodyText confidence="0.99954162962963">
All our models are trained on the WMT’14 train-
ing data consisting of 4.5M sentences pairs (116M
English words, 110M German words). Similar to
(Jean et al., 2015), we limit our vocabularies to
be the top 50K most frequent words for both lan-
guages. Words not in these shortlisted vocabular-
ies are converted into a universal token &lt;unk&gt;.
When training our NMT systems, following
(Bahdanau et al., 2015; Jean et al., 2015), we fil-
ter out sentence pairs whose lengths exceed 50
words and shuffle mini-batches as we proceed.
Our stacking LSTM models have 4 layers, each
with 1000 cells, and 1000-dimensional embed-
dings. We follow (Sutskever et al., 2014; Luong
et al., 2015) in training NMT with similar set-
tings: (a) our parameters are uniformly initialized
in [−0.1,0.1], (b) we train for 10 epochs using
plain SGD, (c) a simple learning rate schedule is
employed – we start with a learning rate of 1; after
5 epochs, we begin to halve the learning rate ev-
ery epoch, (d) our mini-batch size is 128, and (e)
the normalized gradient is rescaled whenever its
norm exceeds 5. Additionally, we also use dropout
for our LSTMs as suggested by (Zaremba et al.,
2015). For dropout models, we train for 12 epochs
and start halving the learning rate after 8 epochs.
Our code is implemented in MATLAB. When
</bodyText>
<footnote confidence="0.694935666666667">
12All texts are tokenized with tokenizer.perl and
BLEU scores are computed with multi-bleu.perl.
13With the mteval-v13a script as per WMT guideline.
</footnote>
<figure confidence="0.969459823529412">
˜ht
Attention Layer
A B C D &lt;eos&gt; X Y Z
1416
Ppl
BLEU
20.7
16.5
19.0
21.6
System
Winning WMT’14 system – phrase-based + large LM (Buck et al., 2014)
Existing NMT systems
RNNsearch (Jean et al., 2015)
RNNsearch + unk replace (Jean et al., 2015)
RNNsearch + unk replace + large vocab + ensemble 8 models (Jean et al., 2015)
Our NMT systems
</figure>
<tableCaption confidence="0.616073">
Table 1: WMT’14 English-German results – shown are the perplexities (ppl) and the tokenized BLEU
scores of various systems on newstest2014. We highlight the best system in bold and give progressive
improvements in italic between consecutive systems. local-p referes to the local attention with predictive
alignments. We indicate for each attention model the alignment score function used in pararentheses.
</tableCaption>
<figure confidence="0.999436954545455">
Ensemble 8 models + unk replace
23.0 (+2.1)
16.8 (+2.8)
7.3
Base + reverse + dropout + global attention (location)
6.4
18.1 (+1.3)
Base + reverse + dropout + global attention (location) + feed input
19.0 (+0.9)
5.9
20.9 (+1.9)
Base + reverse + dropout + local-p attention (general) + feed input
Base + reverse + dropout + local-p attention (general) + feed input + unk replace
10.6
9.9
8.1
Base
Base + reverse
Base + reverse + dropout
11.3
12.6 (+1.3)
14.0 (+1.4)
</figure>
<bodyText confidence="0.979165666666667">
running on a single GPU device Tesla K40, we
achieve a speed of 1K target words per second.
It takes 7–10 days to completely train a model.
</bodyText>
<subsectionHeader confidence="0.934452">
4.2 English-German Results
</subsectionHeader>
<bodyText confidence="0.999979930232558">
We compare our NMT systems in the English-
German task with various other systems. These
include the winning system in WMT’14 (Buck et
al., 2014), a phrase-based system whose language
models were trained on a huge monolingual text,
the Common Crawl corpus. For end-to-end neu-
ral machine translation systems, to the best of our
knowledge, (Jean et al., 2015) is the only work ex-
perimenting with this language pair and currently
the SOTA system. We only present results for
some of our attention models and will later ana-
lyze the rest in Section 5.
As shown in Table 1, we achieve progressive
improvements when (a) reversing the source sen-
tence, +1.3 BLEU, as proposed in (Sutskever et
al., 2014) and (b) using dropout, +1.4 BLEU. On
top of that, (c) the global attention approach gives
a significant boost of +2.8 BLEU, making our
model slightly better than the base attentional sys-
tem of Bahdanau et al. (2015) (row RNNSearch).
When (d) using the input-feeding approach, we
seize another notable gain of +1.3 BLEU and out-
perform their system. The local attention model
with predictive alignments (row local-p) proves
to be even better, giving us a further improve-
ment of +0.9 BLEU on top of the global attention
model. It is interesting to observe the trend pre-
viously reported in (Luong et al., 2015) that per-
plexity strongly correlates with translation quality.
In total, we achieve a significant gain of 5.0 BLEU
points over the non-attentional baseline, which al-
ready includes known techniques such as source
reversing and dropout.
The unknown replacement technique proposed
in (Luong et al., 2015; Jean et al., 2015) yields
another nice gain of +1.9 BLEU, demonstrating
that our attentional models do learn useful align-
ments for unknown works. Finally, by ensembling
8 different models of various settings, e.g., using
different attention approaches, with and without
dropout etc., we were able to achieve a new SOTA
result of 23.0 BLEU, outperforming the existing
best system (Jean et al., 2015) by +1.4 BLEU.
</bodyText>
<subsectionHeader confidence="0.543793">
System BLEU
</subsectionHeader>
<bodyText confidence="0.8420264">
SOTA – NMT + 5-gram rerank (MILA) 24.9
Our ensemble 8 models + unk replace 25.9
Table 2: WMT’15 English-German results –
NIST BLEU scores of the existing WMT’15 SOTA
system and our best one on newstest2015.
Latest results in WMT’15 – despite the fact that
our models were trained on WMT’14 with slightly
less data, we test them on newstest2015 to demon-
strate that they can generalize well to different test
sets. As shown in Table 2, our best system es-
</bodyText>
<page confidence="0.979964">
1417
</page>
<figure confidence="0.99827">
NMT + 5-gram rerank (MILA)
Base (reverse)
System
SOTA – phrase-based (Edinburgh)
+ global (location)
+ global (location) + feed
+ global (dot) + drop + feed
+ global (dot) + drop + feed + unk
WMT’15 systems
Our NMT systems
Ppl.
14.3
12.7
10.9
9.7
20.1 (+1.0)
22.8 (+2.7)
24.9 (+2.1)
19.1 (+2.2)
BLEU
29.2
27.6
16.9
Test cost
6
4
2
5
3
0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8
Mini−batches
basic
basic+reverse
basic+reverse+dropout
basic+reverse+dropout+globalAttn
basic+reverse+dropout+globalAttn+feedInput
basic+reverse+dropout+pLocalAttn+feedInput
x 105
</figure>
<tableCaption confidence="0.944499">
Table 3: WMT’15 German-English results –
</tableCaption>
<bodyText confidence="0.794678142857143">
performances of various systems (similar to Ta-
ble 1). The base system already includes source
reversing on which we add global attention,
dropout, input feeding, and unk replacement.
tablishes a new SOTA performance of 25.9 BLEU,
outperforming the existing best system backed by
NMT and a 5-gram LM reranker by +1.0 BLEU.
</bodyText>
<subsectionHeader confidence="0.99683">
4.3 German-English Results
</subsectionHeader>
<bodyText confidence="0.9999888125">
We carry out a similar set of experiments for the
WMT’15 translation task from German to En-
glish. While our systems have not yet matched
the performance of the SOTA system, we never-
theless show the effectiveness of our approaches
with large and progressive gains in terms of BLEU
as illustrated in Table 3. The attentional mech-
anism gives us +2.2 BLEU gain and on top of
that, we obtain another boost of up to +1.0 BLEU
from the input-feeding approach. Using a better
alignment function, the content-based dot product
one, together with dropout yields another gain of
+2.7 BLEU. Lastly, when applying the unknown
word replacement technique, we seize an addi-
tional +2.1 BLEU, demonstrating the usefulness
of attention in aligning rare words.
</bodyText>
<sectionHeader confidence="0.96039" genericHeader="method">
5 Analysis
</sectionHeader>
<bodyText confidence="0.999963166666667">
We conduct extensive analysis to better understand
our models in terms of learning, the ability to han-
dle long sentences, choices of attentional archi-
tectures, and alignment quality. All models con-
sidered here are English-German NMT systems
tested on newstest2014.
</bodyText>
<subsectionHeader confidence="0.999274">
5.1 Learning curves
</subsectionHeader>
<bodyText confidence="0.916480923076923">
We compare models built on top of one another as
listed in Table 1. It is pleasant to observe in Fig-
ure 5 a clear separation between non-attentional
and attentional models. The input-feeding ap-
Figure 5: Learning curves – test cost (ln perplex-
ity) on newstest2014 for English-German NMTs
as training progresses.
proach and the local attention model also demon-
strate their abilities in driving the test costs lower.
The non-attentional model with dropout (the blue
+ curve) learns slower than other non-dropout
models, but as time goes by, it becomes more ro-
bust in terms of minimizing test errors.
</bodyText>
<subsectionHeader confidence="0.999606">
5.2 Effects of Translating Long Sentences
</subsectionHeader>
<bodyText confidence="0.999608777777778">
We follow (Bahdanau et al., 2015) to group sen-
tences of similar lengths together and compute a
BLEU score per group. As demonstrated in Fig-
ure 6, our attentional models are more effective
than the other non-attentional model in handling
long sentences: the translation quality does not de-
grade as sentences become longer. Our best model
(the blue + curve) outperforms all other systems in
all length buckets.
</bodyText>
<subsectionHeader confidence="0.965907">
Sent Lengths
</subsectionHeader>
<bodyText confidence="0.7380775">
Figure 6: Length Analysis – translation qualities
of different systems as sentences become longer.
</bodyText>
<subsectionHeader confidence="0.999834">
5.3 Choices of Attentional Architectures
</subsectionHeader>
<bodyText confidence="0.9997364">
We examine different attention models (global,
local-m, local-p) and different alignment func-
tions (location, dot, general, concat) as described
in Section 3. Due to limited resources, we can-
not run all the possible combinations. However,
</bodyText>
<figure confidence="0.94518975">
25
20
BLEU
15
</figure>
<figureCaption confidence="0.7890396">
ours, no attn (BLEU 13.9)
ours, local−p attn (BLEU 20.9)
ours, best system (BLEU 23.0)
WMT’14 best (BLEU 20.7)
Jeans et al., 2015 (BLEU 21.6)
</figureCaption>
<figure confidence="0.988555083333333">
10
10 20 30 40 50 60 70
1418
Before
18.1
18.6
17.3
x
18.6
18.0
19
System
Ppl
BLEU
global (location)
global (dot)
global (general)
local-m (dot)
local-m (general)
local-p (dot)
6.6
local-p (general)
5.9
6.4
6.1
6.1
&gt;7.0
6.2
After unk
19.3 (+1.2)
20.5 (+1.9)
19.1 (+1.8)
x
20.4 (+1.8)
19.6 (+1.9)
20.9 (+1.9)
</figure>
<table confidence="0.9666675">
Method AER
global (location) 0.39
local-m (general) 0.34
local-p (general) 0.36
ensemble 0.34
Berkeley Aligner 0.32
</table>
<tableCaption confidence="0.8703144">
Table 6: AER scores – results of various models
on the RWTH English-German alignment data.
Table 4: Attentional Architectures – perfor-
mances of different attentional models. We trained
two local-m (dot) models; both have ppl &gt; 7.0.
</tableCaption>
<bodyText confidence="0.997024071428571">
results in Table 4 do give us some idea about dif-
ferent choices. The location-based function does
not learn good alignments: the global (location)
model can only obtain a small gain when perform-
ing unknown word replacement compared to using
other alignment functions.14 For content-based
functions, our implementation of concat does not
yield good performances and more analysis should
be done to understand the reason.15 It is interest-
ing to observe that dot works well for the global
attention and general is better for the local atten-
tion. Among the different models, the local atten-
tion model with predictive alignments (local-p) is
best, both in terms of perplexities and BLEU.
</bodyText>
<subsectionHeader confidence="0.995603">
5.4 Alignment Quality
</subsectionHeader>
<bodyText confidence="0.998655361111111">
A by-product of attentional models are word align-
ments. While (Bahdanau et al., 2015) visualized
alignments for some sample sentences and ob-
served gains in translation quality as an indica-
tion of a working attention model, no work has as-
sessed the alignments learned as a whole. In con-
trast, we set out to evaluate the alignment quality
using the alignment error rate (AER) metric.
Given the gold alignment data provided by
RWTH for 508 English-German Europarl sen-
tences, we “force” decode our attentional models
to produce translations that match the references.
We extract only one-to-one alignments by select-
ing the source word with the highest alignment
14There is a subtle difference in how we retrieve align-
ments for the different alignment functions. At time step t in
which we receive yt−1 as input and then compute ht, at, ct,
and ˜ht before predicting yt, the alignment vector at is used
as alignment weights for (a) the predicted word yt in the
location-based alignment functions and (b) the input word
yt−1 in the content-based functions.
15With concat, the perplexities achieved by different mod-
els are 6.7 (global), 7.1 (local-m), and 7.1 (local-p).
weight per target word. Nevertheless, as shown in
Table 6, we were able to achieve AER scores com-
parable to the one-to-many alignments obtained
by the Berkeley aligner (Liang et al., 2006).16
We also found that the alignments produced by
local attention models achieve lower AERs than
those of the global one. The AER obtained by
the ensemble, while good, is not better than the
local-m AER, suggesting the well-known observa-
tion that AER and translation scores are not well
correlated (Fraser and Marcu, 2007). Due to space
constraint, we can only show alignment visualiza-
tions in the arXiv version of our paper.17
</bodyText>
<subsectionHeader confidence="0.987841">
5.5 Sample Translations
</subsectionHeader>
<bodyText confidence="0.999996470588235">
We show in Table 5 sample translations in both
directions. It it appealing to observe the ef-
fect of attentional models in correctly translat-
ing names such as “Miranda Kerr” and “Roger
Dow”. Non-attentional models, while producing
sensible names from a language model perspec-
tive, lack the direct connections from the source
side to make correct translations.
We also observed an interesting case in the
second English-German example, which requires
translating the doubly-negated phrase, “not in-
compatible”. The attentional model correctly
produces “nicht ... unvereinbar”; whereas the
non-attentional model generates “nicht vereinbar”,
meaning “not compatible”.18 The attentional
model also demonstrates its superiority in trans-
lating long sentences as in the last example.
</bodyText>
<sectionHeader confidence="0.999322" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.87565025">
In this paper, we propose two simple and effec-
tive attentional mechanisms for neural machine
16We concatenate the 508 sentence pairs with 1M sentence
pairs from WMT and run the Berkeley aligner.
</bodyText>
<footnote confidence="0.9654705">
17http://arxiv.org/abs/1508.04025
18The reference uses a more fancy translation of “incom-
patible”, which is “im Widerspruch zu etwas stehen”. Both
models, however, failed to translate “passenger experience”.
</footnote>
<page confidence="0.990838">
1419
</page>
<subsectionHeader confidence="0.696133">
English-German translations
</subsectionHeader>
<bodyText confidence="0.965384416666667">
src Orlando Bloom and Miranda Kerr still love each other
ref Orlando Bloom und Miranda Kerr lieben sich noch immer
best Orlando Bloom und Miranda Kerr lieben einander noch immer .
base Orlando Bloom und Lucas Miranda lieben einander noch immer .
src ″We ′ re pleased the FAA recognizes that an enjoyable passenger experience is not incompatible
with safety and security , ″said Roger Dow , CEO of the U.S. Travel Association .
ref “ Wir freuen uns , dass die FAA erkennt , dass ein angenehmes Passagiererlebnis nicht im Wider-
spruch zur Sicherheit steht ” , sagte Roger Dow, CEO der U.S. Travel Association.
best ″Wir freuen uns , dass die FAA anerkennt , dass ein angenehmes ist nicht mit Sicherheit und
Sicherheit unvereinbar ist ″, sagte Roger Dow , CEO der US - die .
base ″Wir freuen uns ¨uber die &lt;unk&gt; , dass ein &lt;unk&gt; &lt;unk&gt; mit Sicherheit nicht vereinbar ist mit
Sicherheit und Sicherheit ″, sagte Roger Cameron , CEO der US - &lt;unk&gt; .
</bodyText>
<subsectionHeader confidence="0.919762">
German-English translations
</subsectionHeader>
<bodyText confidence="0.999019868421053">
src In einem Interview sagte Bloom jedoch , dass er und Kerr sich noch immer lieben .
ref However, in an interview , Bloom has said that he and Kerr still love each other.
best In an interview , however , Bloom said that he and Kerr still love .
base However, in an interview, Bloom said that he and Tina were still &lt;unk&gt; .
src Wegen der von Berlin und der Europ¨aischen Zentralbank verh¨angten strengen Sparpolitik in
Verbindung mit der Zwangsjacke , in die die jeweilige nationale Wirtschaft durch das Festhal-
ten an der gemeinsamen W¨ahrung gen¨otigt wird, sind viele Menschen der Ansicht , das Projekt
Europa sei zu weit gegangen
ref The austerity imposed by Berlin and the European Central Bank, coupled with the straitjacket
imposed on national economies through adherence to the common currency, has led many people
to think Project Europe has gone too far.
best Because of the strict austerity measures imposed by Berlin and the European Central Bank in
connection with the straitjacket in which the respective national economy is forced to adhere to
the common currency, many people believe that the European project has gone too far.
base Because of the pressure imposed by the European Central Bank and the Federal Central Bank
with the strict austerity imposed on the national economy in the face of the single currency ,
many people believe that the European project has gone too far.
Table 5: Sample translations – for each example, we show the source (src), the human translation (ref),
the translation from our best model (best), and the translation of a non-attentional model (base). We
italicize some correct translation segments and highlight a few wrong ones in bold.
translation: the global approach which always
looks at all source positions and the local one
that only attends to a subset of source positions
at a time. We test the effectiveness of our mod-
els in the WMT translation tasks between En-
glish and German in both directions. Our local
attention yields large gains of up to 5.0 BLEU
over non-attentional models that already incorpo-
rate known techniques such as dropout. For the
English to German translation direction, our en-
semble model has established new state-of-the-art
results for both WMT’14 and WMT’15.
We have compared various alignment functions
and shed light on which functions are best for
which attentional models. Our analysis shows that
attention-based NMT models are superior to non-
attentional ones in many cases, for example in
translating names and handling long sentences.
</bodyText>
<sectionHeader confidence="0.964428" genericHeader="conclusions">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.99570825">
We gratefully acknowledge support from a gift
from Bloomberg L.P. and the support of NVIDIA
Corporation with the donation of Tesla K40 GPUs.
We thank Andrew Ng and his group as well as
the Stanford Research Computing for letting us
use their computing resources. We thank Rus-
sell Stewart for helpful discussions on the models.
Lastly, we thank Quoc Le, Ilya Sutskever, Oriol
Vinyals, Richard Socher, Michael Kayser, Jiwei
Li, Panupong Pasupat, Kelvin Gu, members of the
Stanford NLP Group and the annonymous review-
ers for their valuable comments and feedback.
</bodyText>
<page confidence="0.985839">
1420
</page>
<sectionHeader confidence="0.993808" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999852666666667">
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015. Neural machine translation by jointly
learning to align and translate. In ICLR.
Christian Buck, Kenneth Heafield, and Bas van Ooyen.
2014. N-gram counts and language models from the
common crawl. In LREC.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using RNN encoder-decoder for statistical machine
translation. In EMNLP.
Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho,
and Yoshua Bengio. 2014. End-to-end continuous
speech recognition using attention-based recurrent
NN: first results. CoRR, abs/1412.1602.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine trans-
lation. Computational Linguistics, 33(3):293–303.
Karol Gregor, Ivo Danihelka, Alex Graves,
Danilo Jimenez Rezende, and Daan Wierstra.
2015. DRAW: A recurrent neural network for
image generation. In ICML.
S´ebastien Jean, Kyunghyun Cho, Roland Memisevic,
and Yoshua Bengio. 2015. On using very large tar-
get vocabulary for neural machine translation. In
ACL.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In NAACL.
Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2015. Addressing
the rare word problem in neural machine translation.
In ACL.
Volodymyr Mnih, Nicolas Heess, Alex Graves, and Ko-
ray Kavukcuoglu. 2014. Recurrent models of visual
attention. In NIPS.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun
Cho, Aaron C. Courville, Ruslan Salakhutdinov,
Richard S. Zemel, and Yoshua Bengio. 2015. Show,
attend and tell: Neural image caption generation
with visual attention. In ICML.
Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.
2015. Recurrent neural network regularization. In
ICLR.
</reference>
<page confidence="0.992729">
1421
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.559263">
<title confidence="0.999165">Effective Approaches to Attention-based Neural Machine Translation</title>
<author confidence="0.984883">Minh-Thang Luong Hieu Pham Christopher D</author>
<affiliation confidence="0.700161">Computer Science Department, Stanford University, Stanford, CA</affiliation>
<abstract confidence="0.992002192307692">An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of atmechanism: a which always attends to all source words a that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Neural machine translation by jointly learning to align and translate.</title>
<date>2015</date>
<booktitle>In ICLR.</booktitle>
<contexts>
<context position="3016" citStr="Bahdanau et al. (2015)" startWordPosition="486" endWordPosition="489">ementing NMT decoders is easy unlike the highly intricate decoders in standard MT (Koehn et al., 2003). In parallel, the concept of “attention” has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (Chorowski et al., 2014), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015). In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words. To the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for NMT. In this work, we design, with simplicity and effectiveness in mind, two novel types of attention1412 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. based models: a global approach in which all source words are attended and a local one wh</context>
<context position="5974" citStr="Bahdanau et al., 2015" startWordPosition="975" endWordPosition="978"> as: log p(y|x) = 1:m1 log p(yj|y&lt;j,s) (1) = A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the re2There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task. However, as we detail later, our model is much simpler and can achieve good performance for NMT. 3All sentences are assumed to terminate with a special “end-of-sentence” token &lt;eos&gt;. cent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdana</context>
<context position="7390" citStr="Bahdanau et al., 2015" startWordPosition="1223" endWordPosition="1226"> parameterize the probability of decoding each word yj as: p (yj|y&lt;j, s) = softmax (g (hj)) (2) with g being the transformation function that outputs a vocabulary-sized vector.5 Here, hj is the RNN hidden unit, abstractly computed as: hj = f(hj−1, s), (3) where f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit. In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state. On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next. In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated in Figure 1. We use the LSTM unit defined in (Zaremba et al., 2015). Our training objective is formulated as follows: 1: Jt = (x,y)∈D − log p(y|x) (4) 4They all used a single RNN layer except for the latter</context>
<context position="10721" citStr="Bahdanau et al., 2015" startWordPosition="1796" endWordPosition="1799">infers a variable-length alignment weight vector at based on the current target state ht and all source states ¯hs. A global context vector ct is then computed as the weighted average, according to at, over all the source states. Besides, in our early attempts to build attentionbased models, we use a location-based function in which the alignment scores are computed from solely the target hidden state ht as follows: at = softmax(Waht) location (9) Given the alignment vector as weights, the context vector ct is computed as the weighted average over all the source hidden states.6 Comparison to (Bahdanau et al., 2015) – While our global attention approach is similar in spirit to the model proposed by Bahdanau et al. (2015), there are several key differences which reflect how we have both simplified and generalized from the original model. First, we simply use hidden states at the top LSTM layers in both the encoder and decoder as illustrated in Figure 2. Bahdanau et al. (2015), on the other hand, use the concatenation of the forward and backward source hidden states in the bi-directional encoder and target hidden states in their non-stacking uni-directional decoder. Second, our computation path is simpler;</context>
<context position="12310" citStr="Bahdanau et al. (2015)" startWordPosition="2085" endWordPosition="2088">r long sentences, we ignore words near the end. hs ct Attention Layer Context vector Global align weights at yt ˜ht ht score(ht, hs) = { 1414 Figure 3: Local attention model – the model first predicts a single aligned position pt for the current target word. A window centered around the source position pt is then used to compute a context vector ct, a weighted average of the source hidden states in the window. The weights at are inferred from the current target state ht and those source states ¯hs in the window. goes through a deep-output and a maxout layer before making predictions.7 Lastly, Bahdanau et al. (2015) only experimented with one alignment function, the concat product; whereas we show later that the other alternatives are better. 3.2 Local Attention The global attention has a drawback that it has to attend to all words on the source side for each target word, which is expensive and can potentially render it impractical to translate longer sequences, e.g., paragraphs or documents. To address this deficiency, we propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word. This model takes inspiration from the tradeoff between the s</context>
<context position="17049" citStr="Bahdanau et al. (2015)" startWordPosition="2899" endWordPosition="2902">ranslation process to keep track of which source words have been translated. Likewise, in attentional NMTs, alignment decisions should be made jointly taking into account past alignment information. To address that, we propose an inputfeeding approach in which attentional vectors ˜ht are concatenated with inputs at the next time steps as illustrated in Figure 4.11 The effects of having such connections are two-fold: (a) we hope to make the model fully aware of previous alignment choices and (b) we create a very deep network spanning both horizontally and vertically. Comparison to other work – Bahdanau et al. (2015) use context vectors, similar to our ct, in building subsequent hidden states, which can also achieve the “coverage” effect. However, there has not been any analysis of whether such connections are useful as done in this work. Also, our approach is more general; as illustrated in Figure 4, it can be applied to general stacking recurrent architectures, including non-attentional models. Xu et al. (2015) propose a doubly attentional approach with an additional constraint added to the training objective to make sure the model pays equal attention to all parts of the image during the caption genera</context>
<context position="19040" citStr="Bahdanau et al., 2015" startWordPosition="3225" endWordPosition="3228">ollowing (Luong et al., 2015), we report translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work and (b) NIST13 BLEU to be comparable with WMT results. 4.1 Training Details All our models are trained on the WMT’14 training data consisting of 4.5M sentences pairs (116M English words, 110M German words). Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages. Words not in these shortlisted vocabularies are converted into a universal token &lt;unk&gt;. When training our NMT systems, following (Bahdanau et al., 2015; Jean et al., 2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed. Our stacking LSTM models have 4 layers, each with 1000 cells, and 1000-dimensional embeddings. We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters are uniformly initialized in [−0.1,0.1], (b) we train for 10 epochs using plain SGD, (c) a simple learning rate schedule is employed – we start with a learning rate of 1; after 5 epochs, we begin to halve the learning rate every epoch, (d) our mini-batch size is 128, and</context>
<context position="22365" citStr="Bahdanau et al. (2015)" startWordPosition="3798" endWordPosition="3801">ranslation systems, to the best of our knowledge, (Jean et al., 2015) is the only work experimenting with this language pair and currently the SOTA system. We only present results for some of our attention models and will later analyze the rest in Section 5. As shown in Table 1, we achieve progressive improvements when (a) reversing the source sentence, +1.3 BLEU, as proposed in (Sutskever et al., 2014) and (b) using dropout, +1.4 BLEU. On top of that, (c) the global attention approach gives a significant boost of +2.8 BLEU, making our model slightly better than the base attentional system of Bahdanau et al. (2015) (row RNNSearch). When (d) using the input-feeding approach, we seize another notable gain of +1.3 BLEU and outperform their system. The local attention model with predictive alignments (row local-p) proves to be even better, giving us a further improvement of +0.9 BLEU on top of the global attention model. It is interesting to observe the trend previously reported in (Luong et al., 2015) that perplexity strongly correlates with translation quality. In total, we achieve a significant gain of 5.0 BLEU points over the non-attentional baseline, which already includes known techniques such as sour</context>
<context position="26588" citStr="Bahdanau et al., 2015" startWordPosition="4486" endWordPosition="4489">isted in Table 1. It is pleasant to observe in Figure 5 a clear separation between non-attentional and attentional models. The input-feeding apFigure 5: Learning curves – test cost (ln perplexity) on newstest2014 for English-German NMTs as training progresses. proach and the local attention model also demonstrate their abilities in driving the test costs lower. The non-attentional model with dropout (the blue + curve) learns slower than other non-dropout models, but as time goes by, it becomes more robust in terms of minimizing test errors. 5.2 Effects of Translating Long Sentences We follow (Bahdanau et al., 2015) to group sentences of similar lengths together and compute a BLEU score per group. As demonstrated in Figure 6, our attentional models are more effective than the other non-attentional model in handling long sentences: the translation quality does not degrade as sentences become longer. Our best model (the blue + curve) outperforms all other systems in all length buckets. Sent Lengths Figure 6: Length Analysis – translation qualities of different systems as sentences become longer. 5.3 Choices of Attentional Architectures We examine different attention models (global, local-m, local-p) and di</context>
<context position="28957" citStr="Bahdanau et al., 2015" startWordPosition="4869" endWordPosition="4872"> a small gain when performing unknown word replacement compared to using other alignment functions.14 For content-based functions, our implementation of concat does not yield good performances and more analysis should be done to understand the reason.15 It is interesting to observe that dot works well for the global attention and general is better for the local attention. Among the different models, the local attention model with predictive alignments (local-p) is best, both in terms of perplexities and BLEU. 5.4 Alignment Quality A by-product of attentional models are word alignments. While (Bahdanau et al., 2015) visualized alignments for some sample sentences and observed gains in translation quality as an indication of a working attention model, no work has assessed the alignments learned as a whole. In contrast, we set out to evaluate the alignment quality using the alignment error rate (AER) metric. Given the gold alignment data provided by RWTH for 508 English-German Europarl sentences, we “force” decode our attentional models to produce translations that match the references. We extract only one-to-one alignments by selecting the source word with the highest alignment 14There is a subtle differe</context>
</contexts>
<marker>Bahdanau, Cho, Bengio, 2015</marker>
<rawString>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Buck</author>
<author>Kenneth Heafield</author>
<author>Bas van Ooyen</author>
</authors>
<title>N-gram counts and language models from the common crawl.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<marker>Buck, Heafield, van Ooyen, 2014</marker>
<rawString>Christian Buck, Kenneth Heafield, and Bas van Ooyen. 2014. N-gram counts and language models from the common crawl. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Chorowski</author>
<author>Dzmitry Bahdanau</author>
<author>Kyunghyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>End-to-end continuous speech recognition using attention-based recurrent NN: first results.</title>
<date>2014</date>
<location>CoRR, abs/1412.1602.</location>
<contexts>
<context position="2848" citStr="Chorowski et al., 2014" startWordPosition="455" endWordPosition="458">e model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT (Koehn et al., 2003). In parallel, the concept of “attention” has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (Chorowski et al., 2014), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015). In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words. To the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for NMT. In this work, we design, with simplicity and effectiveness in mind, two novel types of attention1412 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon, Por</context>
</contexts>
<marker>Chorowski, Bahdanau, Cho, Bengio, 2014</marker>
<rawString>Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. End-to-end continuous speech recognition using attention-based recurrent NN: first results. CoRR, abs/1412.1602.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Fraser</author>
<author>Daniel Marcu</author>
</authors>
<title>Measuring word alignment quality for statistical machine translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="30546" citStr="Fraser and Marcu, 2007" startWordPosition="5132" endWordPosition="5135">15With concat, the perplexities achieved by different models are 6.7 (global), 7.1 (local-m), and 7.1 (local-p). weight per target word. Nevertheless, as shown in Table 6, we were able to achieve AER scores comparable to the one-to-many alignments obtained by the Berkeley aligner (Liang et al., 2006).16 We also found that the alignments produced by local attention models achieve lower AERs than those of the global one. The AER obtained by the ensemble, while good, is not better than the local-m AER, suggesting the well-known observation that AER and translation scores are not well correlated (Fraser and Marcu, 2007). Due to space constraint, we can only show alignment visualizations in the arXiv version of our paper.17 5.5 Sample Translations We show in Table 5 sample translations in both directions. It it appealing to observe the effect of attentional models in correctly translating names such as “Miranda Kerr” and “Roger Dow”. Non-attentional models, while producing sensible names from a language model perspective, lack the direct connections from the source side to make correct translations. We also observed an interesting case in the second English-German example, which requires translating the doubl</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>Alexander Fraser and Daniel Marcu. 2007. Measuring word alignment quality for statistical machine translation. Computational Linguistics, 33(3):293–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karol Gregor</author>
</authors>
<title>Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra.</title>
<date>2015</date>
<booktitle>In ICML.</booktitle>
<marker>Gregor, 2015</marker>
<rawString>Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. 2015. DRAW: A recurrent neural network for image generation. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S´ebastien Jean</author>
<author>Kyunghyun Cho</author>
<author>Roland Memisevic</author>
<author>Yoshua Bengio</author>
</authors>
<title>On using very large target vocabulary for neural machine translation.</title>
<date>2015</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1471" citStr="Jean et al., 2015" startWordPosition="218" endWordPosition="221">achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.1 1 Introduction Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-of-sentence symbol &lt;eos&gt; is reached. It then starts emitting one target word at a time, as illustrated in Figure 1. NMT 1All our code and models are publicly available at http: //nlp.stanford.edu/projects/nmt. X Y Z &lt;eos&gt; A B C D &lt;eos&gt; X Y Z Figure 1: Neural machine translation – a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z. Here, &lt;eos&gt; marks the end of a sentence. </context>
<context position="6014" citStr="Jean et al., 2015" startWordPosition="983" endWordPosition="986">= A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the re2There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task. However, as we detail later, our model is much simpler and can achieve good performance for NMT. 3All sentences are assumed to terminate with a special “end-of-sentence” token &lt;eos&gt;. cent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdanau et al. (2015), and Jean et al. (2015) </context>
<context position="7410" citStr="Jean et al., 2015" startWordPosition="1227" endWordPosition="1230">bility of decoding each word yj as: p (yj|y&lt;j, s) = softmax (g (hj)) (2) with g being the transformation function that outputs a vocabulary-sized vector.5 Here, hj is the RNN hidden unit, abstractly computed as: hj = f(hj−1, s), (3) where f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit. In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state. On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next. In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated in Figure 1. We use the LSTM unit defined in (Zaremba et al., 2015). Our training objective is formulated as follows: 1: Jt = (x,y)∈D − log p(y|x) (4) 4They all used a single RNN layer except for the latter two works which uti</context>
<context position="18803" citStr="Jean et al., 2015" startWordPosition="3185" endWordPosition="3188">13 (3000 sentences) is used as a development set to select our hyperparameters. Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences) and newstest2015 (2169 sentences). Following (Luong et al., 2015), we report translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work and (b) NIST13 BLEU to be comparable with WMT results. 4.1 Training Details All our models are trained on the WMT’14 training data consisting of 4.5M sentences pairs (116M English words, 110M German words). Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages. Words not in these shortlisted vocabularies are converted into a universal token &lt;unk&gt;. When training our NMT systems, following (Bahdanau et al., 2015; Jean et al., 2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed. Our stacking LSTM models have 4 layers, each with 1000 cells, and 1000-dimensional embeddings. We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters are uniformly initialized in [−0.1</context>
<context position="20281" citStr="Jean et al., 2015" startWordPosition="3442" endWordPosition="3445">dient is rescaled whenever its norm exceeds 5. Additionally, we also use dropout for our LSTMs as suggested by (Zaremba et al., 2015). For dropout models, we train for 12 epochs and start halving the learning rate after 8 epochs. Our code is implemented in MATLAB. When 12All texts are tokenized with tokenizer.perl and BLEU scores are computed with multi-bleu.perl. 13With the mteval-v13a script as per WMT guideline. ˜ht Attention Layer A B C D &lt;eos&gt; X Y Z 1416 Ppl BLEU 20.7 16.5 19.0 21.6 System Winning WMT’14 system – phrase-based + large LM (Buck et al., 2014) Existing NMT systems RNNsearch (Jean et al., 2015) RNNsearch + unk replace (Jean et al., 2015) RNNsearch + unk replace + large vocab + ensemble 8 models (Jean et al., 2015) Our NMT systems Table 1: WMT’14 English-German results – shown are the perplexities (ppl) and the tokenized BLEU scores of various systems on newstest2014. We highlight the best system in bold and give progressive improvements in italic between consecutive systems. local-p referes to the local attention with predictive alignments. We indicate for each attention model the alignment score function used in pararentheses. Ensemble 8 models + unk replace 23.0 (+2.1) 16.8 (+2.8)</context>
<context position="21812" citStr="Jean et al., 2015" startWordPosition="3700" endWordPosition="3703"> replace 10.6 9.9 8.1 Base Base + reverse Base + reverse + dropout 11.3 12.6 (+1.3) 14.0 (+1.4) running on a single GPU device Tesla K40, we achieve a speed of 1K target words per second. It takes 7–10 days to completely train a model. 4.2 English-German Results We compare our NMT systems in the EnglishGerman task with various other systems. These include the winning system in WMT’14 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus. For end-to-end neural machine translation systems, to the best of our knowledge, (Jean et al., 2015) is the only work experimenting with this language pair and currently the SOTA system. We only present results for some of our attention models and will later analyze the rest in Section 5. As shown in Table 1, we achieve progressive improvements when (a) reversing the source sentence, +1.3 BLEU, as proposed in (Sutskever et al., 2014) and (b) using dropout, +1.4 BLEU. On top of that, (c) the global attention approach gives a significant boost of +2.8 BLEU, making our model slightly better than the base attentional system of Bahdanau et al. (2015) (row RNNSearch). When (d) using the input-feed</context>
<context position="23076" citStr="Jean et al., 2015" startWordPosition="3913" endWordPosition="3916"> +1.3 BLEU and outperform their system. The local attention model with predictive alignments (row local-p) proves to be even better, giving us a further improvement of +0.9 BLEU on top of the global attention model. It is interesting to observe the trend previously reported in (Luong et al., 2015) that perplexity strongly correlates with translation quality. In total, we achieve a significant gain of 5.0 BLEU points over the non-attentional baseline, which already includes known techniques such as source reversing and dropout. The unknown replacement technique proposed in (Luong et al., 2015; Jean et al., 2015) yields another nice gain of +1.9 BLEU, demonstrating that our attentional models do learn useful alignments for unknown works. Finally, by ensembling 8 different models of various settings, e.g., using different attention approaches, with and without dropout etc., we were able to achieve a new SOTA result of 23.0 BLEU, outperforming the existing best system (Jean et al., 2015) by +1.4 BLEU. System BLEU SOTA – NMT + 5-gram rerank (MILA) 24.9 Our ensemble 8 models + unk replace 25.9 Table 2: WMT’15 English-German results – NIST BLEU scores of the existing WMT’15 SOTA system and our best one on </context>
</contexts>
<marker>Jean, Cho, Memisevic, Bengio, 2015</marker>
<rawString>S´ebastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2015. On using very large target vocabulary for neural machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models. In</title>
<date>2013</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="5909" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="962" endWordPosition="966">one target word at a time and hence decomposes the conditional probability as: log p(y|x) = 1:m1 log p(yj|y&lt;j,s) (1) = A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the re2There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task. However, as we detail later, our model is much simpler and can achieve good performance for NMT. 3All sentences are assumed to terminate with a special “end-of-sentence” token &lt;eos&gt;. cent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit</context>
<context position="7195" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="1188" endWordPosition="1191">, Bahdanau et al. (2015), and Jean et al. (2015) all adopted a different version of the RNN with an LSTM-inspired hidden unit, the gated recurrent unit (GRU), for both components.4 In more detail, one can parameterize the probability of decoding each word yj as: p (yj|y&lt;j, s) = softmax (g (hj)) (2) with g being the transformation function that outputs a vocabulary-sized vector.5 Here, hj is the RNN hidden unit, abstractly computed as: hj = f(hj−1, s), (3) where f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit. In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state. On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next. In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated in Figure 1</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In EMNLP. Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by agreement.</title>
<date>2006</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="30224" citStr="Liang et al., 2006" startWordPosition="5079" endWordPosition="5082">ent alignment functions. At time step t in which we receive yt−1 as input and then compute ht, at, ct, and ˜ht before predicting yt, the alignment vector at is used as alignment weights for (a) the predicted word yt in the location-based alignment functions and (b) the input word yt−1 in the content-based functions. 15With concat, the perplexities achieved by different models are 6.7 (global), 7.1 (local-m), and 7.1 (local-p). weight per target word. Nevertheless, as shown in Table 6, we were able to achieve AER scores comparable to the one-to-many alignments obtained by the Berkeley aligner (Liang et al., 2006).16 We also found that the alignments produced by local attention models achieve lower AERs than those of the global one. The AER obtained by the ensemble, while good, is not better than the local-m AER, suggesting the well-known observation that AER and translation scores are not well correlated (Fraser and Marcu, 2007). Due to space constraint, we can only show alignment visualizations in the arXiv version of our paper.17 5.5 Sample Translations We show in Table 5 sample translations in both directions. It it appealing to observe the effect of attentional models in correctly translating name</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by agreement. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Ilya Sutskever</author>
<author>Quoc V Le</author>
<author>Oriol Vinyals</author>
<author>Wojciech Zaremba</author>
</authors>
<title>Addressing the rare word problem in neural machine translation.</title>
<date>2015</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="1429" citStr="Luong et al., 2015" startWordPosition="210" endWordPosition="213"> both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.1 1 Introduction Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015). NMT is appealing since it requires minimal domain knowledge and is conceptually simple. The model by Luong et al. (2015) reads through all the source words until the end-of-sentence symbol &lt;eos&gt; is reached. It then starts emitting one target word at a time, as illustrated in Figure 1. NMT 1All our code and models are publicly available at http: //nlp.stanford.edu/projects/nmt. X Y Z &lt;eos&gt; A B C D &lt;eos&gt; X Y Z Figure 1: Neural machine translation – a stacking recurrent architecture for translating a source sequence A B C D into a target sequence X Y Z.</context>
<context position="5994" citStr="Luong et al., 2015" startWordPosition="979" endWordPosition="982">log p(yj|y&lt;j,s) (1) = A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the re2There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task. However, as we detail later, our model is much simpler and can achieve good performance for NMT. 3All sentences are assumed to terminate with a special “end-of-sentence” token &lt;eos&gt;. cent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder. Cho et al. (2014), Bahdanau et al. (2015), and</context>
<context position="7258" citStr="Luong et al., 2015" startWordPosition="1200" endWordPosition="1203">ersion of the RNN with an LSTM-inspired hidden unit, the gated recurrent unit (GRU), for both components.4 In more detail, one can parameterize the probability of decoding each word yj as: p (yj|y&lt;j, s) = softmax (g (hj)) (2) with g being the transformation function that outputs a vocabulary-sized vector.5 Here, hj is the RNN hidden unit, abstractly computed as: hj = f(hj−1, s), (3) where f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit. In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state. On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next. In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated in Figure 1. We use the LSTM unit defined in (Zaremba et al., 2015). Our t</context>
<context position="18448" citStr="Luong et al., 2015" startWordPosition="3124" endWordPosition="3127">e coverage set effect in NMT that we mentioned earlier. However, we chose to use the input-feeding approach since it provides flexibility for the model to decide on any attentional constraints it deems suitable. 4 Experiments We evaluate the effectiveness of our models on the WMT translation tasks between English and German in both directions. newstest2013 (3000 sentences) is used as a development set to select our hyperparameters. Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences) and newstest2015 (2169 sentences). Following (Luong et al., 2015), we report translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work and (b) NIST13 BLEU to be comparable with WMT results. 4.1 Training Details All our models are trained on the WMT’14 training data consisting of 4.5M sentences pairs (116M English words, 110M German words). Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages. Words not in these shortlisted vocabularies are converted into a universal token &lt;unk&gt;. When training our NMT systems, following (Bahdanau et al., 2015; Jean e</context>
<context position="22756" citStr="Luong et al., 2015" startWordPosition="3864" endWordPosition="3867">t al., 2014) and (b) using dropout, +1.4 BLEU. On top of that, (c) the global attention approach gives a significant boost of +2.8 BLEU, making our model slightly better than the base attentional system of Bahdanau et al. (2015) (row RNNSearch). When (d) using the input-feeding approach, we seize another notable gain of +1.3 BLEU and outperform their system. The local attention model with predictive alignments (row local-p) proves to be even better, giving us a further improvement of +0.9 BLEU on top of the global attention model. It is interesting to observe the trend previously reported in (Luong et al., 2015) that perplexity strongly correlates with translation quality. In total, we achieve a significant gain of 5.0 BLEU points over the non-attentional baseline, which already includes known techniques such as source reversing and dropout. The unknown replacement technique proposed in (Luong et al., 2015; Jean et al., 2015) yields another nice gain of +1.9 BLEU, demonstrating that our attentional models do learn useful alignments for unknown works. Finally, by ensembling 8 different models of various settings, e.g., using different attention approaches, with and without dropout etc., we were able t</context>
</contexts>
<marker>Luong, Sutskever, Le, Vinyals, Zaremba, 2015</marker>
<rawString>Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2015. Addressing the rare word problem in neural machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Volodymyr Mnih</author>
<author>Nicolas Heess</author>
<author>Alex Graves</author>
<author>Koray Kavukcuoglu</author>
</authors>
<title>Recurrent models of visual attention.</title>
<date>2014</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="2760" citStr="Mnih et al., 2014" startWordPosition="441" endWordPosition="444">n and has the ability to generalize well to very long word sequences. This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT (Koehn et al., 2003). In parallel, the concept of “attention” has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (Chorowski et al., 2014), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015). In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words. To the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for NMT. In this work, we design, with simplicity and effectiveness in mind, two novel types of attention1412 Proceedings of the 2015 Conf</context>
</contexts>
<marker>Mnih, Heess, Graves, Kavukcuoglu, 2014</marker>
<rawString>Volodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. 2014. Recurrent models of visual attention. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei jing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="18349" citStr="Papineni et al., 2002" startWordPosition="3110" endWordPosition="3113">put size of the first LSTM layer is 2n; those of subsequent layers are n. also be useful to capture the coverage set effect in NMT that we mentioned earlier. However, we chose to use the input-feeding approach since it provides flexibility for the model to decide on any attentional constraints it deems suitable. 4 Experiments We evaluate the effectiveness of our models on the WMT translation tasks between English and German in both directions. newstest2013 (3000 sentences) is used as a development set to select our hyperparameters. Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences) and newstest2015 (2169 sentences). Following (Luong et al., 2015), we report translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work and (b) NIST13 BLEU to be comparable with WMT results. 4.1 Training Details All our models are trained on the WMT’14 training data consisting of 4.5M sentences pairs (116M English words, 110M German words). Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages. Words not in these shortlisted vocabularies are converted in</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="5933" citStr="Sutskever et al., 2014" startWordPosition="967" endWordPosition="970">nce decomposes the conditional probability as: log p(y|x) = 1:m1 log p(yj|y&lt;j,s) (1) = A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the re2There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task. However, as we detail later, our model is much simpler and can achieve good performance for NMT. 3All sentences are assumed to terminate with a special “end-of-sentence” token &lt;eos&gt;. cent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common. They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s. Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation. On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder an</context>
<context position="7219" citStr="Sutskever et al., 2014" startWordPosition="1192" endWordPosition="1195">an et al. (2015) all adopted a different version of the RNN with an LSTM-inspired hidden unit, the gated recurrent unit (GRU), for both components.4 In more detail, one can parameterize the probability of decoding each word yj as: p (yj|y&lt;j, s) = softmax (g (hj)) (2) with g being the transformation function that outputs a vocabulary-sized vector.5 Here, hj is the RNN hidden unit, abstractly computed as: hj = f(hj−1, s), (3) where f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit. In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state. On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process. Such an approach is referred to as an attention mechanism, which we will discuss next. In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated in Figure 1. We use the LSTM unit d</context>
<context position="19289" citStr="Sutskever et al., 2014" startWordPosition="3267" endWordPosition="3270">d on the WMT’14 training data consisting of 4.5M sentences pairs (116M English words, 110M German words). Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages. Words not in these shortlisted vocabularies are converted into a universal token &lt;unk&gt;. When training our NMT systems, following (Bahdanau et al., 2015; Jean et al., 2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed. Our stacking LSTM models have 4 layers, each with 1000 cells, and 1000-dimensional embeddings. We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters are uniformly initialized in [−0.1,0.1], (b) we train for 10 epochs using plain SGD, (c) a simple learning rate schedule is employed – we start with a learning rate of 1; after 5 epochs, we begin to halve the learning rate every epoch, (d) our mini-batch size is 128, and (e) the normalized gradient is rescaled whenever its norm exceeds 5. Additionally, we also use dropout for our LSTMs as suggested by (Zaremba et al., 2015). For dropout models, we train for 12 epochs and start halving the learning rate after 8 epoc</context>
<context position="22149" citStr="Sutskever et al., 2014" startWordPosition="3760" endWordPosition="3763">er systems. These include the winning system in WMT’14 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus. For end-to-end neural machine translation systems, to the best of our knowledge, (Jean et al., 2015) is the only work experimenting with this language pair and currently the SOTA system. We only present results for some of our attention models and will later analyze the rest in Section 5. As shown in Table 1, we achieve progressive improvements when (a) reversing the source sentence, +1.3 BLEU, as proposed in (Sutskever et al., 2014) and (b) using dropout, +1.4 BLEU. On top of that, (c) the global attention approach gives a significant boost of +2.8 BLEU, making our model slightly better than the base attentional system of Bahdanau et al. (2015) (row RNNSearch). When (d) using the input-feeding approach, we seize another notable gain of +1.3 BLEU and outperform their system. The local attention model with predictive alignments (row local-p) proves to be even better, giving us a further improvement of +0.9 BLEU on top of the global attention model. It is interesting to observe the trend previously reported in (Luong et al.</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kelvin Xu</author>
<author>Jimmy Ba</author>
<author>Ryan Kiros</author>
<author>Kyunghyun Cho</author>
<author>Aaron C Courville</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard S Zemel</author>
<author>Yoshua Bengio</author>
</authors>
<title>Show, attend and tell: Neural image caption generation with visual attention.</title>
<date>2015</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="2969" citStr="Xu et al., 2015" startWordPosition="477" endWordPosition="480">as a small memory footprint. Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT (Koehn et al., 2003). In parallel, the concept of “attention” has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (Chorowski et al., 2014), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015). In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words. To the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for NMT. In this work, we design, with simplicity and effectiveness in mind, two novel types of attention1412 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. based models: a global approach in which a</context>
<context position="12970" citStr="Xu et al. (2015)" startWordPosition="2195" endWordPosition="2198">on, the concat product; whereas we show later that the other alternatives are better. 3.2 Local Attention The global attention has a drawback that it has to attend to all words on the source side for each target word, which is expensive and can potentially render it impractical to translate longer sequences, e.g., paragraphs or documents. To address this deficiency, we propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word. This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al. (2015) to tackle the image caption generation task. In their work, soft attention refers to the global attention approach in which weights are placed “softly” over all patches in the source image. The hard attention, on the other hand, selects one patch of the image to attend to at a time. While less expensive at inference time, the hard attention model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. 7We will refer to this difference again in Section 3.3. Our local attention mechanism selectively focuses on a small window </context>
<context position="17453" citStr="Xu et al. (2015)" startWordPosition="2963" endWordPosition="2966">two-fold: (a) we hope to make the model fully aware of previous alignment choices and (b) we create a very deep network spanning both horizontally and vertically. Comparison to other work – Bahdanau et al. (2015) use context vectors, similar to our ct, in building subsequent hidden states, which can also achieve the “coverage” effect. However, there has not been any analysis of whether such connections are useful as done in this work. Also, our approach is more general; as illustrated in Figure 4, it can be applied to general stacking recurrent architectures, including non-attentional models. Xu et al. (2015) propose a doubly attentional approach with an additional constraint added to the training objective to make sure the model pays equal attention to all parts of the image during the caption generation process. Such a constraint can 11If n is the number of LSTM cells, the input size of the first LSTM layer is 2n; those of subsequent layers are n. also be useful to capture the coverage set effect in NMT that we mentioned earlier. However, we chose to use the input-feeding approach since it provides flexibility for the model to decide on any attentional constraints it deems suitable. 4 Experiment</context>
</contexts>
<marker>Xu, Ba, Kiros, Cho, Courville, Salakhutdinov, Zemel, Bengio, 2015</marker>
<rawString>Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron C. Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wojciech Zaremba</author>
</authors>
<title>Ilya Sutskever, and Oriol Vinyals.</title>
<date>2015</date>
<booktitle>In ICLR.</booktitle>
<marker>Zaremba, 2015</marker>
<rawString>Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2015. Recurrent neural network regularization. In ICLR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
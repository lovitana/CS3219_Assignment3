<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000755">
<title confidence="0.986402">
Visual Bilingual Lexicon Induction with Transferred ConvNet Features
</title>
<author confidence="0.984329">
Douwe Kiela
</author>
<affiliation confidence="0.987142">
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.977886">
douwe.kiela@cl.cam.ac.uk
</email>
<author confidence="0.99178">
Ivan Vuli´c
</author>
<affiliation confidence="0.999061">
Department of Computer Science
</affiliation>
<address confidence="0.707219">
KU Leuven
</address>
<email confidence="0.977423">
ivan.vulic@cs.kuleuven.be
</email>
<author confidence="0.99103">
Stephen Clark
</author>
<affiliation confidence="0.98938">
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.997011">
stephen.clark@cl.cam.ac.uk
</email>
<sectionHeader confidence="0.993855" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999984631578948">
This paper is concerned with the task of
bilingual lexicon induction using image-
based features. By applying features from
a convolutional neural network (CNN), we
obtain state-of-the-art performance on a
standard dataset, obtaining a 79% relative
improvement over previous work which
uses bags of visual words based on SIFT
features. The CNN image-based approach
is also compared with state-of-the-art lin-
guistic approaches to bilingual lexicon in-
duction, even outperforming these for one
of three language pairs on another stan-
dard dataset. Furthermore, we shed new
light on the type of visual similarity met-
ric to use for genuine similarity versus re-
latedness tasks, and experiment with using
multiple layers from the same network in
an attempt to improve performance.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999791857142857">
Bilingual lexicon induction is the task of finding
words that share a common meaning across differ-
ent languages. It plays an important role in a va-
riety of tasks in information retrieval and natural
language processing, including cross-lingual in-
formation retrieval (Lavrenko et al., 2002; Levow
et al., 2005) and statistical machine translation
(Och and Ney, 2003). Although parallel corpora
have been used successfully for inducing bilin-
gual lexicons for some languages (Och and Ney,
2003), these corpora are either too small or un-
available for many language pairs. Consequently,
mono-lingual approaches that rely on compara-
ble instead of parallel corpora have been devel-
oped (Fung and Yee, 1998; Koehn and Knight,
2002). These approaches work by mapping lan-
guage pairs to a shared bilingual space and ex-
tracting lexical items from that space. Bergsma
and Van Durme (2011) showed that this bilingual
space need not be linguistic in nature: they used
labeled images from the Web to obtain bilingual
lexical translation pairs based on the visual fea-
tures of corresponding images. Local features are
computed using SIFT (Lowe, 2004) and color his-
tograms (Deselaers et al., 2008) and aggregated as
bags of visual words (BOVW) (Sivic and Zisser-
man, 2003) to get bilingual representations in a
shared visual space. Their highest performance is
obtained by combining these visual features with
normalized edit distance, an orthographic similar-
ity metric (Navarro, 2001).
There are several advantages to having a vi-
sual rather than a linguistic intermediate bilin-
gual space: First, while labeled images are readily
available for many languages through resources
such as Google Images, language pairs that have
sizeable comparable, let alone parallel, corpora
are relatively scarce. Second, it has been found
that meaning is often grounded in the perceptual
system, and that the quality of semantic repre-
sentations improves significantly when they are
grounded in the visual modality (Silberer and La-
pata, 2012; Bruni et al., 2014). Having an inter-
mediate visual space means that words in differ-
ent languages can be grounded in the same space.
Third, it is natural to use vision as an intermediate:
when we communicate with someone who does
not speak our language, we often communicate
by directly referring to our surroundings. Lan-
guages that are linguistically far apart will, by cog-
nitive necessity, still refer to objects in the same
visual space. While some approaches to bilingual
lexicon induction rely on orthographic properties
(Haghighi et al., 2008; Koehn and Knight, 2002)
or properties of frequency distributions (Schafer
and Yarowsky, 2002) that will work only for
</bodyText>
<page confidence="0.9623">
148
</page>
<note confidence="0.985122">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.997949526315789">
closely related languages, a visual space can work
for any language, whether it’s English or Chinese,
Arabic or Icelandic, or all Greek to you.
It has recently been shown, however, that much
better performance can be achieved on seman-
tic similarity and relatedness tasks by using vi-
sual representations from deep convolutional neu-
ral networks (CNNs) instead of BOVW features
(Kiela and Bottou, 2014). In this paper we ap-
ply such CNN-derived visual features to the task
of bilingual lexicon induction. To obtain a trans-
lation of a word in a source language, we find
the nearest neighbours from words in the target
language, where words in both languages reside
in a shared visual space made up of CNN-based
features. Nearest neighbours are found by apply-
ing similarity metrics from both Kiela and Bottou
(2014) and Bergsma and Van Durme (2011). In
summary, the contributions of this paper are:
</bodyText>
<listItem confidence="0.889833190476191">
• We obtain a relative improvement of 79%
over Bergsma and Van Durme (2011) on a
standard dataset based on fifteen language
pairs.
• We shed new light on the question of whether
genuine similarity versus semantic related-
ness tasks require different similarity metrics
for optimal performance (Kiela and Bottou,
2014).
• We experiment with using different layers of
the CNN and find that performance is not af-
fected significantly in either case, obtaining
a slight improvement for the relatedness task
but no improvement for genuine similarity.
• Finally, we show that the visual approach out-
performs the linguistic approaches on one of
the three language pairs on a standard dataset.
To our knowledge this is the first work to pro-
vide a comparison of visual and state-of-the-
art linguistic approaches to bilingual lexicon
induction.
</listItem>
<sectionHeader confidence="0.999744" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999387">
2.1 Bilingual Lexicon Learning
</subsectionHeader>
<bodyText confidence="0.999983512195122">
Bilingual lexicon learning is the task of auto-
matically inducing word translations from raw
data, and is an attractive alternative to the time-
consuming and expensive process of manually
building high-quality resources for a wide vari-
ety of language pairs and domains. Early ap-
proaches relied on limited and domain-restricted
parallel data, and the induced lexicons were typi-
cally a by-product of word alignment models (Och
and Ney, 2003). To alleviate the issue of low cov-
erage, a large body of work has been dedicated
to lexicon learning from more abundant and less
restricted comparable data, e.g., (Fung and Yee,
1998; Rapp, 1999; Gaussier et al., 2004; Shezaf
and Rappoport, 2010; Tamura et al., 2012). How-
ever, these models typically rely on the availabil-
ity of bilingual seed lexicons to produce shared
bilingual spaces, as well as large repositories of
comparable data. Therefore, several approaches
attempt to learn lexicons from large monolingual
data sets in two languages (Koehn and Knight,
2002; Haghighi et al., 2008), but their perfor-
mance again relies on language pair-dependent
clues such as orthographic similarity. A further
approach removed the requirement of seed lexi-
cons, and induced lexicons using bilingual spaces
spanned by multilingual probabilistic topic mod-
els (Vuli´c et al., 2011; Liu et al., 2013; Vuli´c and
Moens, 2013b). However, these models require
document alignments as initial bilingual signals.
In this work, following recent research in
multi-modal semantics and image representation
learning—in particular deep learning and con-
volutional neural networks—we test the ability
of purely visual data to induce shared bilingual
spaces and to consequently learn bilingual word
correspondences in these spaces. By compiling
images related to linguistic concepts given in dif-
ferent languages, the potentially prohibitive data
requirements and language pair-dependence from
prior work is removed.
</bodyText>
<subsectionHeader confidence="0.99822">
2.2 Deep Convolutional Neural Networks
</subsectionHeader>
<bodyText confidence="0.9996732">
Deep convolutional neural networks (CNNs) have
become extremely popular in the computer vi-
sion community. These networks currently pro-
vide state-of-the-art performance for a variety of
key computer vision tasks such as object recogni-
tion (Razavian et al., 2014). They tend to be rel-
atively deep, consisting of a number of rectified
linear unit layers (Nair and Hinton, 2010) and a
series of convolutional layers (Krizhevsky et al.,
2012). Recently, such layers have been used in
transfer learning techniques, where they are used
as mid-level features in other computer vision
tasks (Oquab et al., 2014). Although the idea of
transferring CNN features is not new (Driancourt
and Bottou, 1990), the simultaneous availability of
</bodyText>
<page confidence="0.999027">
149
</page>
<figureCaption confidence="0.9788445">
Figure 1: Illustration of calculating similarity be-
tween images from different languages.
</figureCaption>
<bodyText confidence="0.9982455">
massive amounts of data and cheap GPUs has led
to considerable advances in computer vision, simi-
lar in scale to those witnessed with SIFT and HOG
descriptors a decade ago (Razavian et al., 2014).
</bodyText>
<subsectionHeader confidence="0.994726">
2.3 Multi-Modal Semantics
</subsectionHeader>
<bodyText confidence="0.998038225806452">
Multi-modal semantics is motivated by parallels
with human concept acquisition. It has been found
that semantic knowledge, from a very early age,
relies heavily on perceptual information (Louw-
erse, 2008), and there exists substantial evidence
that many concepts are grounded in the percep-
tual system (Barsalou, 2008). One way to accom-
plish such grounding is by combining linguistic
representations with information from a percep-
tual modality, obtained from, e.g., property norm-
ing experiments (Silberer and Lapata, 2012; Sil-
berer et al., 2013; Roller and Schulte im Walde,
2013; Hill and Korhonen, 2014) or extracting
features from raw image data (Feng and Lapata,
2010; Leong and Mihalcea, 2011; Bruni et al.,
2014; Kiela et al., 2014). Such multi-modal vi-
sual approaches often rely on local descriptors,
such as SIFT (Lowe, 2004), SURF (Bay et al.,
2008), or HOG (Dalal and Triggs, 2005), as well
as pyramidal variants of these descriptors such as
PHOW (Bosch et al., 2007). However, deep CNN
features have recently been successfully trans-
ferred to multi-modal semantics (Kiela and Bot-
tou, 2014; Shen et al., 2014). Deep learning tech-
niques have also been successfully employed in
cross-modal tasks (Frome et al., 2013; Socher et
al., 2014; Lazaridou et al., 2014; Kiros et al.,
2014). Other examples of multi-modal deep learn-
ing use restricted Boltzmann machines (Srivastava
and Salakhutdinov, 2014) or auto-encoders (Wu et
al., 2013; Silberer and Lapata, 2014).
</bodyText>
<sectionHeader confidence="0.934208" genericHeader="method">
3 A Purely Visual Approach to Bilingual
</sectionHeader>
<subsectionHeader confidence="0.704703">
Lexicon Learning
</subsectionHeader>
<bodyText confidence="0.999985578947369">
We assume that the best translation, or match-
ing lexical item, of a word ws (in the source lan-
guage) is the word wt (in the target language)
that is the nearest cross-lingual neighbour to ws
in the bilingual visual space. Hence a similarity
(or distance) score between lexical items from dif-
ferent languages is required. In this section, we
describe: one, how to build image representations
from sets of images associated with each lexical
item, i.e. how to induce a shared bilingual visual
space in which all lexical items are represented;
and two, how to compute the similarity between
lexical items using their visual representations in
the shared bilingual space. We also describe the
evaluation datasets and metrics we use.
To facilitate further research, we will make our
code and data publicly available. Please see the
following webpage: http://www.cl.cam.
ac.uk/˜dk427/bli.html.
</bodyText>
<subsectionHeader confidence="0.999308">
3.1 Image Representations
</subsectionHeader>
<bodyText confidence="0.999877208333333">
We use Google Images to extract the top n ranked
images for each lexical item in the evaluation
datasets. It has been shown that images from
Google yield higher quality representations than
comparable sources such as Flickr (Bergsma and
Goebel, 2011) and that Google-derived datasets
are competitive with “hand prepared datasets”
(Fergus et al., 2005). Google Images also has
the advantage that it has full coverage and is
multi-lingual, as opposed to other potential im-
age sources such as ImageNet (Deng et al., 2009)
or the ESP Game Dataset (von Ahn and Dabbish,
2004). For each Google search we specify the tar-
get language corresponding to the lexical item’s
language. Figure 2 gives some example images
retrieved using the same query terms in different
languages. For each image, we extract the pre-
softmax layer of an AlexNet (Krizhevsky et al.,
2012). The network contains a number of lay-
ers, starting with five convolutional layers, two
fully connected layers and finally a softmax, and
has been pre-trained on the ImageNet classifica-
tion task using Caffe (Jia et al., 2014). See Figure
1 for a simple diagram illustrating the approach.
</bodyText>
<subsectionHeader confidence="0.999448">
3.2 Visual Similarity
</subsectionHeader>
<bodyText confidence="0.9987445">
Suppose that, as part of the evaluation, the similar-
ity between bicycle and fiets is required. Each of
</bodyText>
<page confidence="0.987607">
150
</page>
<bodyText confidence="0.9999915">
the two words has n images associated with it – the
top n as returned by Google image search, using
bicycle and fiets as separate query terms. Hence
to calculate the similarity, a measure is required
which takes two sets of images as input. The stan-
dard approach in multi-modal semantics is to de-
rive a single image representation for each word,
e.g., by averaging the n images. An alternative is
to take the pointwise maximum across the n im-
age vector representations, also producing a sin-
gle vector (Kiela and Bottou, 2014). Kiela and
Bottou call these combined representations CNN-
MEAN and CNN-MAX, respectively. Cosine is
then used to calculate the similarity between the
resulting pair of image vectors.
An alternative strategy, however, is to consider
the similarities between individual images instead
of their aggregated representations. Bergsma and
Van Durme (2011) propose two similarity met-
rics based on this principle: taking the average
of the maximum similarity scores (AVGMAX), or
the maximum of the maximum similarity scores
(MAXMAX) between associated images. Contin-
uing with our example, for each of the n images
for bicycle, the maximum similarity is found by
searching over the n images for fiets. AVGMAX
then takes the average of those n maximum simi-
larites; MAXMAX takes the maximum. To avoid
confusion, we will refer to the CNN-based mod-
els that use these metrics as CNN-AVGMAX and
CNN-MAXMAX. Formally, these metrics are de-
fined as in Table 1. We experiment with both kinds
of MAX and find that they optimize for different
kinds of similarity.
</bodyText>
<subsectionHeader confidence="0.997532">
3.3 Evaluations
</subsectionHeader>
<bodyText confidence="0.999699666666667">
Test Sets. Bergsma and Van Durme’s primary
evaluation dataset consists of a set of five hundred
matching lexical items for fifteen language pairs,
based on six languages. (The fifteen pairs results
from all ways of pairing six languages). The data
is publicly available online.1 In order to get the
five hundred lexical items, they first rank nouns
by the conditional probability of them occurring
in the pattern “{image,photo,photograph,picture}
of {a,an} ” in the web-scale Google N-gram
corpus (Lin et al., 2010), and take the top five hun-
dred words as their English lexicon. For each item
</bodyText>
<footnote confidence="0.651049">
1http://www.clsp.jhu.edu/˜sbergsma/LexImg/
</footnote>
<figure confidence="0.796287">
� max sim(is, it)
1 itET(wt)
n
i3ET(w3)
max max sim(is, it)
i3ET(w3) itET(wt)
�sim(n1 � it)
i3ET(w3) 1
is, n
itET(wt)
sim(max&apos;Z(ws), max&apos;Z(wt))
</figure>
<tableCaption confidence="0.932366">
Table 1: Visual similarity metrics between two
</tableCaption>
<bodyText confidence="0.981533777777778">
sets of n images. Z(ws) represents the set of im-
ages for a given source word ws, Z(wt) the set of
images for a given target word wt; max&apos; takes a
set of vectors and returns the single element-wise
maximum vector.
in the English lexicon, they obtain correspond-
ing items in the other languages—Spanish, Ital-
ian, French, German and Dutch—through Google
Translate. We call this dataset BERGSMA500.
In addition to that dataset, we evaluate on a
dataset constructed to measure the general perfor-
mance of bilingual lexicon learning models from
comparable Wikipedia data (Vuli´c and Moens,
2013a). The dataset comprises 1, 000 nouns in
three languages: Spanish (ES), Italian (IT), and
Dutch (NL), along with their one-to-one gold-
standard word translations in English (EN) com-
piled semi-automatically using Google Translate
and manual annotators for each language. We call
this dataset VULIC10002. The test set is accom-
panied with comparable data for training, for the
three language pairs ES/IT/NL-EN on which text-
based models for bilingual lexicon induction were
trained (Vuli´c and Moens, 2013a).
Given the way that the BERGSMA500 dataset
was created, in particular the use of the pattern
described above, it contains largely concrete lin-
guistic concepts (since, eg, image of a democracy
is unlikely to have a high corpus frequency). In
contrast, VULIC1000 was designed to capture
general bilingual word correspondences, and con-
tains several highly abstract test examples, such as
entendimiento (understanding) and desigualdad
(inequality) in Spanish, or scoperta (discovery)
and cambiamento (change) in Italian. Using the
two evaluation datasets can potentially provide
</bodyText>
<footnote confidence="0.683184">
2http://people.cs.kuleuven.be/˜ivan.vulic/software/
</footnote>
<figure confidence="0.841796">
AVGMAX
MAXMAX
CNN-MEAN
CNN-MAX
</figure>
<page confidence="0.620688">
151
</page>
<figureCaption confidence="0.980583">
Figure 2: Example images for the languages in the Bergsma and Van Durme dataset.
</figureCaption>
<table confidence="0.999922714285714">
Method P@1 P@5 P@20 MRR
B&amp;VD Visual-Only 31.1 41.4 53.7 0.367
B&amp;VD Visual + NED 48.0 59.5 68.7 0.536
CNN-AVGMAX 56.7 69.2 77.4 0.658
CNN-MAXMAX 42.8 60.0 64.5 0.529
CNN-MEAN 50.5 62.7 71.1 0.586
CNN-MAX 51.4 64.9 74.8 0.608
</table>
<tableCaption confidence="0.763188">
Table 2: Performance on BERGSMA500 com-
pared to Bergsma and Van Durme (B&amp;VD).
</tableCaption>
<bodyText confidence="0.9933048">
some insight into how purely visual models for
bilingual lexicon induction behave with respect to
both abstract and concrete concepts.
Evaluation Metrics. We measure performance in
a standard way using mean-reciprocal rank:
</bodyText>
<equation confidence="0.999005">
1 M 1
MRR = M Z=1 rank(ws, wt) (1)
</equation>
<bodyText confidence="0.999966888888889">
where rank(ws, wt) denotes the rank of the cor-
rect translation wt (as provided in the gold stan-
dard) in the ranked list of translation candidates
for ws, and M is the number of test cases. We also
use precision at N (P@N) (Gaussier et al., 2004;
Tamura et al., 2012; Vuli´c and Moens, 2013a),
which measures the proportion of test instances
where the correct translation is within the top N
highest ranked translations.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.9998786">
We evaluate the four similarity metrics on the
BERGSMA500 dataset and compare the results to
the systems of Bergsma and Van Durme, who
report results for the AVGMAX function, hav-
ing concluded that it performs better than MAX-
MAX on English-Spanish translations. We report
their best-performing visual-only system, which
combines SIFT-based descriptors with color his-
tograms, as well as their best-performing overall
system, which combines the visual approach with
normalized edit distance (NED). Results are aver-
aged over fifteen language pairs.
The results can be seen in Table 2. Each of the
CNN-based methods outperforms the B&amp;VD sys-
tems. The best performing method overall, CNN-
AVGMAX, provides a 79% relative improvement
over the B&amp;VD visual-only system on the MRR
measure, and a 23% relative improvement over
their best-performing approach, which includes
non-visual information in the form of orthographic
similarity. Moreover, their methods include a tun-
ing parameter A that governs the contributions
of SIFT-based, color histogram and normalized
edit distance similarity scores, whilst our approach
does not require any parameter tuning.
</bodyText>
<subsectionHeader confidence="0.978036">
4.1 Similarity and Relatedness
</subsectionHeader>
<bodyText confidence="0.9872235">
The results in Table 2 indicate that the per-
image CNN-AVGMAX metric outperforms the
</bodyText>
<page confidence="0.975531">
152
</page>
<table confidence="0.999975125">
Language Pair Method P@1 P@5 P@10 P@20 MRR
BOOTSTRAP 57.7 74.7 80.9 84.8 0.652
CNN-AVGMAX 41.9 54.6 59.1 65.6 0.485
ES ⇒ EN CNN-MAXMAX 34.9 47.4 53.7 58.5 0.414
CNN-MEAN 35.4 48.5 51.7 55.8 0.416
CNN-MAX 33.3 46.3 50.3 54.5 0.395
BOOTSTRAP 64.7 80.6 85.6 89.7 0.716
CNN-AVGMAX 28.3 40.6 44.8 50.9 0.343
IT ⇒ EN CNN-MAXMAX 22.6 33.5 38.6 44.4 0.282
CNN-MEAN 22.7 33.2 37.9 42.6 0.281
CNN-MAX 21.3 32.7 36.8 41.5 0.269
BOOTSTRAP 20.6 35.7 43.4 51.3 0.277
CNN-AVGMAX 38.4 48.5 53.7 58.6 0.435
NL ⇒ EN CNN-MAXMAX 30.8 42.6 47.8 52.9 0.367
CNN-MEAN 32.3 42.3 46.5 50.1 0.373
CNN-MAX 30.4 41.0 44.3 49.3 0.356
</table>
<tableCaption confidence="0.975613">
Table 4: Performance on VULIC1000 compared to the linguistic bootstrapping method of Vuli´c and
Moens (2013b).
</tableCaption>
<table confidence="0.999952">
Method MEN SimLex-999
CNN-AVGMAX 0.56 0.34
CNN-MAXMAX 0.55 0.36
CNN-MEAN 0.61 0.32
CNN-MAX 0.60 0.27
</table>
<tableCaption confidence="0.842201">
Table 3: Spearman ρs correlation for the visual
similarity metrics on a relatedness (MEN) and a
genuine similarity (SimLex-999) dataset.
</tableCaption>
<bodyText confidence="0.98269042">
aggregated visual representation-based metrics of
CNN-MEAN and CNN-MAX, despite the fact
that Kiela and Bottou (2014) achieved optimal per-
formance using the latter metrics on a well-known
conceptual relatedness dataset. It has been noted
before that there is a clear distinction between sim-
ilarity and relatedness. This is one of the reasons
that, for example, WordSim353 (Finkelstein et al.,
2002) has been criticized: it gives high similarity
scores to cases of genuine similarity as well as re-
latedness (Agirre et al., 2009; Hill et al., 2014).
The MEN dataset (Bruni et al., 2014) that Kiela
and Bottou (2014) evaluate on explicitly measures
word relatedness. In contrast, the current lexicon
learning task seems to require something else than
relatedness: whilst a chair and table are semanti-
cally related, a translation for chair is not a good
translation for table. For example, we want to
make sure we translate chair to stuhl in German,
and not to tisch. In other words, what we are inter-
ested in for this particular task is genuine similar-
ity, rather than relatedness.
Thus, we can evaluate the quality of our simi-
larity metrics by comparing their performance on
similarity and relatedness tasks: if a metric per-
forms well at measuring genuine similarity, this is
indicative of its performance in the bilingual lexi-
con induction task. In order to examine this ques-
tion further, we evaluate performance on the MEN
dataset, which measures relatedness (Bruni et al.,
2014), and the nouns-subset of the SimLex-999
dataset, which measures genuine similarity (Hill
et al., 2014). For each pair in the dataset, we cal-
culate the similarity score and report the Spearman
ρs correlation, which measures how well the rank-
ing of pairs given by the automatic system matches
that according to the gold-standard human similar-
ity scores. The results are reported in Table 3.
It is clear that the per-image similarity met-
rics perform better on genuine similarity, as mea-
sured by SimLex-999, than on relatedness, as mea-
sured by MEN. In fact, the “aggressive” CNN-
MAXMAX method, which picks out a single pair
of images to represent a linguistic pair, works best
for SimLex-999, indicating how stringently it fo-
cuses on genuine similarity. For the aggregated vi-
sual representation-based metrics, we see the op-
posite effect: they perform better on the related-
ness task. This sheds light on a question raised
by Kiela and Bottou (2014), where they speculate
</bodyText>
<page confidence="0.998333">
153
</page>
<bodyText confidence="0.999851571428572">
that certain errors are a result of whether their vi-
sual similarity metric measures genuine similar-
ity on the one hand or relatedness on the other:
we are better off using per-image visual metrics
for genuine similarity, while aggregated visual
representation-based metrics yield better perfor-
mance on relatedness tasks.
</bodyText>
<subsectionHeader confidence="0.805747">
4.2 Results on VULIC1000
</subsectionHeader>
<bodyText confidence="0.9997025">
This section compares our visual-only approach
to linguistic approaches for bilingual lexicon in-
duction. Since BERGSMA500 has not been eval-
uated with such methods, we evaluate on the
VULIC1000 dataset (Vuli´c and Moens, 2013a).
This dataset has been used to test the ability of
bilingual lexicon induction models to learn trans-
lations from comparable data (see sect. 3.3). We
do not necessarily expect visual methods to out-
perform linguistic ones, but it is instructive to see
the comparison.
We compare our visual models against the cur-
rent state-of-the-art lexicon induction model us-
ing comparable data (Vuli´c and Moens, 2013b).
This model induces translations from compara-
ble Wikipedia data in two steps: (1) It learns a
set of highly reliable one-to-one translation pairs
using a shared bilingual space obtained by ap-
plying the multilingual probabilistic topic model-
ing (MuPTM) framework (Mimno et al., 2009).
(2) These highly reliable one-to-one translation
pairs serve as dimensions of a word-based bilin-
gual semantic space (Gaussier et al., 2004; Tamura
et al., 2012). The model then bootstraps from
the high-precision seed lexicon of translations and
learns new dimensions of the bilingual space until
convergence. This model, which we call BOOT-
STRAP, obtains the current best results on the eval-
uation dataset. For more details about the boot-
strapping model and its comparison against other
approaches, we refer to Vuli´c and Moens (2013b).
Table 4 shows the results for the language pairs
in the VULIC1000 dataset. Of the four similar-
ity metrics, CNN-AVGMAX again performs best,
as it did for BERGSMA500. The linguistic BOOT-
STRAP method outperforms our visual approach
for two of the three language pairs, but, for the
NL-EN language pair, the visual methods in fact
perform better. This can be explained by the ob-
servation that Vuli´c and Moens’s NL-EN training
data for the BOOTSTRAP model is less abundant
(2-3 times fewer Wikipedia articles) and of lower
</bodyText>
<table confidence="0.99941025">
Method FC7 FC6+FC7 POOL5+
FC6+FC7
MEN
CNN-AVGMAX 0.56 0.57 0.57
CNN-MAXMAX 0.55 0.55 0.56
CNN-MEAN 0.61 0.61 0.61
CNN-MAX 0.60 0.62 0.61
SimLex-999
CNN-AVGMAX 0.34 0.33 0.31
CNN-MAXMAX 0.36 0.35 0.34
CNN-MEAN 0.32 0.32 0.31
CNN-MAX 0.27 0.26 0.26
</table>
<tableCaption confidence="0.994912">
Table 5: Spearman ρs correlation for the visual
</tableCaption>
<bodyText confidence="0.9445305">
similarity metrics on a relatedness (MEN) and
a genuine similarity (SimLex-999) dataset using
more than one layer from the CNN.
quality than the data for their ES-EN and IT-EN
models. We view these results as highly encourag-
ing: while purely visual methods cannot yet reach
the peak performance of linguistic approaches that
are trained on sufficient amounts of high-quality
text data, they outperform linguistic state-of-the-
art methods when there is less or lower quality text
data available —which one might reasonably ex-
pect to be the default scenario.
</bodyText>
<subsectionHeader confidence="0.999415">
4.3 Adding CNN Layers
</subsectionHeader>
<bodyText confidence="0.999954090909091">
The AlexNet (Krizhevsky et al., 2012) from which
our image representations are extracted contains
a number of layers. Kiela and Bottou (2014)
only use the fully connected pre-softmax layer
(which we call FC7) for their image representa-
tions. It has been found, however, that other layers
in the network, especially the preceding fully con-
nected (FC6) and fifth convolutional max pooling
(POOL5) layers, also have good properties for
usage in transfer learning (Girshick et al., 2014;
Yosinski et al., 2014). Hence we performed a
(very) preliminary investigation of whether perfor-
mance increases with the use of additional layers.
In light of our findings concerning the differ-
ence between genuine similarity and relatedness,
this also gives rise to the question of whether the
additional layers might be useful for similarity
or relatedness, or both. We hypothesize that the
nature of the task matters here: if we are only
concerned with genuine similarity, layer FC7 is
likely to contain all the necessary information to
judge whether two images are similar or not, since
</bodyText>
<page confidence="0.999442">
154
</page>
<table confidence="0.995570727272727">
Dataset Language Image dispersion
BERGSMA500 EN 0.640 (v=0.074)
ES 0.639 (v=0.072)
IT 0.646 (v=0.071)
FR 0.647 (v=0.072)
DE 0.642 (v=0.072)
NL 0.645 (v=0.074)
0.705 (v=0.095)
0.694 (v=0.092)
0.725 (v=0.078)
0.716 (v=0.080)
</table>
<tableCaption confidence="0.85008">
Table 6: Average image dispersion for the
datasets, by language.
</tableCaption>
<bodyText confidence="0.999871615384615">
the network has been trained for object recogni-
tion. If, however, we are interested in related-
ness, related properties may just as well be en-
coded deeper in the network, so in the layers pre-
ceding FC7 rather than in FC7 itself.
We combined CNN layers with each other by
concatenating the normalized layers. For the bilin-
gual lexicon induction tasks, we found that perfor-
mance did not signficantly increase, which is con-
sistent with our hypothesis (since bilingual lexicon
induction requires genuine similarity rather than
relatedness, and so only requires FC7). We then
tested on the MEN dataset (Bruni et al., 2014) for
relatedness and the nouns subset of the SimLex-
999 dataset (Hill et al., 2014) for genuine similar-
ity. The results can be found in Table 5.
The results appear to indicate that adding such
additional information does not have a clear effect
for genuine similarity, but may lead to a small per-
formance increase for relatedness. This could ex-
plain why we did not see increased performance
on the bilingual lexicon induction task with ad-
ditional layers. However, the increase in perfor-
mance on the relatedness task is relatively minor,
and further investigation is required into the utility
of the additional layers for relatedness tasks.
</bodyText>
<sectionHeader confidence="0.998712" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999968277777778">
A possible explanation for the difference in per-
formance between languages and datasets is that
some words are more concrete than others: a vi-
sual representation for elephant is likely to be
of higher quality than one for happiness. Visual
representations in multi-modal models have been
found to perform much better for concrete than ab-
stract concepts (Kiela et al., 2014).
Although concreteness ratings are available for
(some) English words, this is not the case for other
languages, so in order to examine the concreteness
of the datasets we use a substitute method that has
been shown to closely mirror how abstract a con-
cept is: image dispersion (Kiela et al., 2014). The
image dispersion d of a concept word w is defined
as the average pairwise cosine distance between
all the image representations {ii ... in} in the set
of images for a given word:
</bodyText>
<equation confidence="0.997452">
2 1 −ij · ik
d(w) = n(n − 1) i&lt;j≤n |ij||ik |(2)
</equation>
<bodyText confidence="0.999969617647059">
The average image dispersions for the two
datasets, broken down by language, are shown in
Table 6. BERGSMA500 has a lower average im-
age dispersion score in general, and thus is more
concrete than VULIC1000. It also has less vari-
ance. This may explain why we score higher, in
absolute terms, on that dataset than on the more
abstract one.
When examining individual languages in the
datasets, we note that the worst performing lan-
guage on VULIC1000 is Italian, which is also the
most abstract dataset, with the highest average im-
age dispersion score and the lowest variance.
There is some evidence that abstract concepts
are also perceptually grounded (Lakoff and John-
son, 1999), but in a more complex way, since
abstract concepts express more varied situations
(Barsalou and Wiemer-Hastings, 2005). Using an
image resource like Google Images that has full
coverage for almost any word, means that we can
retrieve what we might call “associated” images
(such as images of voters for words like democ-
racy) as opposed to “extensional” images (such
as images of cats for cat). This explains why we
still obtain good performance on the more abstract
VULIC1000 dataset, in some cases outperform-
ing linguistic methods: even abstract concepts can
have a clear visual representation, albeit of the as-
sociated rather than extensional kind.
However, abstract concepts are overall more
likely to yield noisier image sets. Thus, one way to
improve results would be to take a multi-modal ap-
proach, where we also include linguistic informa-
tion, if available, especially for abstract concepts.
</bodyText>
<sectionHeader confidence="0.993676" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9983965">
We have presented a novel approach to bilingual
lexicon induction that uses convolutional neural
</bodyText>
<figure confidence="0.9832962">
VULIC1000
EN
ES
IT
NL
</figure>
<page confidence="0.996464">
155
</page>
<bodyText confidence="0.997620333333333">
network-derived visual features. Using only such
visual features, we outperform existing visual and
orthographic systems, and even a state-of-the-art
linguistic approach for one language, on standard
bilingual lexicon induction tasks. In doing so,
we have shed new light on which visual similar-
ity metric to use for similarity or relatedness tasks,
and have experimented with using multiple layers
from a CNN. The beauty of the current approach is
that it is completely language agnostic and closely
mirrors how humans would perform bilingual lex-
icon induction: by referring to the external world.
</bodyText>
<sectionHeader confidence="0.997386" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99979475">
DK is supported by EPSRC grant EP/I037512/1.
IV is supported by the PARIS project (IWT-SBO
110067) and the PDM Kort postdoctoral fellow-
ship from KU Leuven. SC is supported by ERC
Starting Grant DisCoTex (306920) and EPSRC
grant EP/I037512/1. We thank Marco Baroni for
useful feedback and the anonymous reviewers for
their helpful comments.
</bodyText>
<sectionHeader confidence="0.997942" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998117538461538">
Eneko Agirre, Enrique Alfonseca, Keith B. Hall, Jana
Kravalova, Marius Pasca, and Aitor Soroa. 2009.
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In NAACL,
pages 19–27.
Lawrence W. Barsalou and Katja Wiemer-Hastings.
2005. Situating abstract concepts. In Grounding
cognition: The role ofperception and action in mem-
ory, language, and thought, pages 129–163.
Lawrence W. Barsalou. 2008. Grounded cognition.
Annual Review of Psychology, 59(1):617–645.
Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc
J. Van Gool. 2008. Speeded-up robust features
(SURF). Computer Vision and Image Understand-
ing, 110(3):346–359.
Shane Bergsma and Randy Goebel. 2011. Using vi-
sual information to predict lexical preference. In
RANLP, pages 399–405.
Shane Bergsma and Benjamin Van Durme. 2011.
Learning bilingual lexicons using the visual similar-
ity of labeled web images. In IJCAI, pages 1764–
1769.
Anna Bosch, Andrew Zisserman, and Xavier Mu˜noz.
2007. Image classification using random forests and
ferns. In ICCV, pages 1–8.
Elia Bruni, Nam-Khanh Tran, and Marco Baroni.
2014. Multimodal distributional semantics. Journal
of Artifical Intelligence Research, 49:1–47.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detection. In CVPR,
pages 886–893.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Fei-Fei Li. 2009. ImageNet: A large-scale
hierarchical image database. In CVPR, pages 248–
255.
Thomas Deselaers, Daniel Keysers, and Hermann Ney.
2008. Features for image retrieval: An experimental
comparison. Information Retrieval, 11(2):77–107.
Xavier Driancourt and L´eon Bottou. 1990. TDNN-
extracted features. In Neuro Nimes 90.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In NAACL, pages
91–99.
Robert Fergus, Fei-Fei Li, Pietro Perona, and Andrew
Zisserman. 2005. Learning object categories from
Google’s image search. In ICCV, pages 1816–1823.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing Search in Context: The
Concept Revisited. ACM Transactions on Informa-
tion Systems, 20(1):116—131.
Andrea Frome, Gregory S. Corrado, Jonathon Shlens,
Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato,
and Tomas Mikolov. 2013. Devise: A deep visual-
semantic embedding model. In NIPS, pages 2121–
2129.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compa-
rable texts. In ACL, pages 414–420.
´Eric Gaussier, Jean-Michel Renders, Irina Matveeva,
Cyril Goutte, and Herv´e D´ejean. 2004. A geometric
view on bilingual lexicon extraction from compara-
ble corpora. In ACL, pages 526–533.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jiten-
dra Malik. 2014. Rich feature hierarchies for accu-
rate object detection and semantic segmentation. In
CVPR.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, pages 771–779.
Felix Hill and Anna Korhonen. 2014. Learning ab-
stract concept embeddings from multi-modal data:
Since you probably can’t see what I mean. In
EMNLP, pages 255–265.
Felix Hill, Roi Reichart, and Anna Korhonen.
2014. SimLex-999: Evaluating semantic mod-
els with (genuine) similarity estimation. CoRR,
abs/1408.3456.
</reference>
<page confidence="0.994054">
156
</page>
<reference confidence="0.998635615384615">
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey
Karayev, Jonathan Long, Ross B. Girshick, Sergio
Guadarrama, and Trevor Darrell. 2014. Caffe: Con-
volutional architecture for fast feature embedding.
In ACM Multimedia, pages 675–678.
Douwe Kiela and L´eon Bottou. 2014. Learning image
embeddings using convolutional neural networks for
improved multi-modal semantics. In EMNLP, pages
36–45.
Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen
Clark. 2014. Improving multi-modal representa-
tions using image dispersion: Why less is sometimes
more. In ACL, pages 835–841.
Ryan Kiros, Ruslan Salakhutdinov, and Richard S.
Zemel. 2014. Multimodal neural language models.
In ICML, pages 595–603.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
ULA’02 Workshop, pages 9–16.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. 2012. ImageNet classification with deep con-
volutional neural networks. In NIPS, pages 1106–
1114.
George Lakoff and Mark Johnson. 1999. Philosophy
in the flesh: The embodied mind and its challenge to
Western thought.
Victor Lavrenko, Martin Choquette, and W. Bruce
Croft. 2002. Cross-lingual relevance models. In
SIGIR, pages 175–182.
Angeliki Lazaridou, Elia Bruni, and Marco Baroni.
2014. Is this a wampimuk? Cross-modal map-
ping between distributional semantics and the visual
world. In ACL, pages 1403–1414.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In IJCNLP, pages 1403–
1407.
Gina-Anne Levow, Douglas Oard, and Philip Resnik.
2005. Dictionary-based techniques for cross-
language information retrieval. Information Pro-
cessing &amp; Management, 41:523 – 547, 2005/05//.
Dekang Lin, Kenneth Ward Church, Heng Ji, Satoshi
Sekine, David Yarowsky, Shane Bergsma, Kailash
Patil, Emily Pitler, Rachel Lathbury, Vikram Rao,
Kapil Dalwani, and Sushant Narsale. 2010. New
tools for Web-scale N-grams. In LREC, pages
2221–2227.
Xiaodong Liu, Kevin Duh, and Yuji Matsumoto. 2013.
Topic models + word alignment = A flexible frame-
work for extracting bilingual dictionary from com-
parable corpus. In CoNLL, pages 212–221.
Max M. Louwerse. 2008. Symbol interdependency in
symbolic and embodied cognition. Topics in Cogni-
tive Science, 59(1):617–645.
David G. Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2):91–110.
David M. Mimno, Hanna M. Wallach, Jason Narad-
owsky, David A. Smith, and Andrew McCallum.
2009. Polylingual topic models. In EMNLP, pages
880–889.
Vinod Nair and Geoffrey E Hinton. 2010. Rectified
linear units improve restricted boltzmann machines.
In ICML, pages 807–814.
Gonzalo Navarro. 2001. A guided tour to approx-
imate string matching. ACM Computing Surveys,
33(1):31–88.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Maxime Oquab, L´eon Bottou, Ivan Laptev, and Josef
Sivic. 2014. Learning and transferring mid-level
image representations using convolutional neural
networks. In CVPR, pages 1717–1724.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated English and Ger-
man corpora. In ACL.
Ali Sharif Razavian, Hossein Azizpour, Josephine Sul-
livan, and Stefan Carlsson. 2014. CNN features off-
the-shelf: an astounding baseline for recognition.
CoRR, abs/1403.6382.
Stephen Roller and Sabine Schulte im Walde. 2013. A
multimodal LDA model integrating textual, cogni-
tive and visual modalities. In EMNLP, pages 1146–
1157.
Charles Schafer and David Yarowsky. 2002. Inducing
translation lexicons via diverse similarity measures
and bridge languages. In CoNLL, pages 1–7.
Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng,
and Gr´egoire Mesnil. 2014. Learning semantic rep-
resentations using convolutional neural networks for
Web search. In WWW, pages 373–374.
Daphna Shezaf and Ari Rappoport. 2010. Bilingual
lexicon generation using non-aligned signatures. In
ACL, pages 98–107.
Carina Silberer and Mirella Lapata. 2012. Grounded
models of semantic representation. In EMNLP,
pages 1423–1433.
Carina Silberer and Mirella Lapata. 2014. Learn-
ing grounded meaning representations with autoen-
coders. In ACL, pages 721–732.
Carina Silberer, Vittorio Ferrari, and Mirella Lapata.
2013. Models of semantic representation with visual
attributes. In ACL, pages 572–582.
</reference>
<page confidence="0.975258">
157
</page>
<reference confidence="0.999823921052632">
Josef Sivic and Andrew Zisserman. 2003. Video
google: A text retrieval approach to object match-
ing in videos. In ICCV, pages 1470–1477.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-
pher D. Manning, and Andrew Y. Ng. 2014.
Grounded compositional semantics for finding and
describing images with sentences. Transactions of
ACL, 2:207–218.
Nitish Srivastava and Ruslan Salakhutdinov. 2014.
Multimodal learning with deep Boltzmann ma-
chines. Journal of Machine Learning Research,
15(1):2949–2980.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from comparable
corpora using label propagation. In EMNLP, pages
24–36.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In CHI, pages 319–326.
Ivan Vuli´c and Marie-Francine Moens. 2013a. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In NAACL, pages
106–116.
Ivan Vuli´c and Marie-Francine Moens. 2013b. A study
on bootstrapping bilingual vector spaces from non-
parallel data (and nothing else). In EMNLP, pages
1613–1624.
Ivan Vuli´c, Wim De Smet, and Marie-Francine Moens.
2011. Identifying word translations from compara-
ble corpora using latent topic models. In ACL, pages
479–484.
Pengcheng Wu, Steven C. H. Hoi, Hao Xia, Peilin
Zhao, Dayong Wang, and Chunyan Miao. 2013.
Online multimodal deep similarity learning with ap-
plication to image retrieval. In ACM Multimedia,
pages 153–162.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod
Lipson. 2014. How transferable are features in deep
neural networks? In NIPS, pages 3320–3328.
</reference>
<page confidence="0.99709">
158
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.279102">
<title confidence="0.994238">Visual Bilingual Lexicon Induction with Transferred ConvNet Features</title>
<author confidence="0.76533">Douwe</author>
<affiliation confidence="0.977052">Computer University of</affiliation>
<email confidence="0.960012">douwe.kiela@cl.cam.ac.uk</email>
<author confidence="0.982529">Ivan</author>
<affiliation confidence="0.99849">Department of Computer</affiliation>
<address confidence="0.439481">KU</address>
<email confidence="0.821392">ivan.vulic@cs.kuleuven.be</email>
<author confidence="0.987426">Stephen</author>
<affiliation confidence="0.9955295">Computer University of</affiliation>
<email confidence="0.98918">stephen.clark@cl.cam.ac.uk</email>
<abstract confidence="0.99932465">This paper is concerned with the task of bilingual lexicon induction using imagebased features. By applying features from a convolutional neural network (CNN), we obtain state-of-the-art performance on a standard dataset, obtaining a 79% relative improvement over previous work which uses bags of visual words based on SIFT features. The CNN image-based approach is also compared with state-of-the-art linguistic approaches to bilingual lexicon induction, even outperforming these for one of three language pairs on another standard dataset. Furthermore, we shed new light on the type of visual similarity metric to use for genuine similarity versus relatedness tasks, and experiment with using multiple layers from the same network in an attempt to improve performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Enrique Alfonseca</author>
<author>Keith B Hall</author>
<author>Jana Kravalova</author>
<author>Marius Pasca</author>
<author>Aitor Soroa</author>
</authors>
<title>A study on similarity and relatedness using distributional and WordNet-based approaches.</title>
<date>2009</date>
<booktitle>In NAACL,</booktitle>
<pages>pages</pages>
<contexts>
<context position="20445" citStr="Agirre et al., 2009" startWordPosition="3208" endWordPosition="3211">ual similarity metrics on a relatedness (MEN) and a genuine similarity (SimLex-999) dataset. aggregated visual representation-based metrics of CNN-MEAN and CNN-MAX, despite the fact that Kiela and Bottou (2014) achieved optimal performance using the latter metrics on a well-known conceptual relatedness dataset. It has been noted before that there is a clear distinction between similarity and relatedness. This is one of the reasons that, for example, WordSim353 (Finkelstein et al., 2002) has been criticized: it gives high similarity scores to cases of genuine similarity as well as relatedness (Agirre et al., 2009; Hill et al., 2014). The MEN dataset (Bruni et al., 2014) that Kiela and Bottou (2014) evaluate on explicitly measures word relatedness. In contrast, the current lexicon learning task seems to require something else than relatedness: whilst a chair and table are semantically related, a translation for chair is not a good translation for table. For example, we want to make sure we translate chair to stuhl in German, and not to tisch. In other words, what we are interested in for this particular task is genuine similarity, rather than relatedness. Thus, we can evaluate the quality of our simila</context>
</contexts>
<marker>Agirre, Alfonseca, Hall, Kravalova, Pasca, Soroa, 2009</marker>
<rawString>Eneko Agirre, Enrique Alfonseca, Keith B. Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. A study on similarity and relatedness using distributional and WordNet-based approaches. In NAACL, pages 19–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
<author>Katja Wiemer-Hastings</author>
</authors>
<title>Situating abstract concepts. In Grounding cognition: The role ofperception and action in memory, language, and thought,</title>
<date>2005</date>
<pages>129--163</pages>
<contexts>
<context position="29891" citStr="Barsalou and Wiemer-Hastings, 2005" startWordPosition="4748" endWordPosition="4751">al, and thus is more concrete than VULIC1000. It also has less variance. This may explain why we score higher, in absolute terms, on that dataset than on the more abstract one. When examining individual languages in the datasets, we note that the worst performing language on VULIC1000 is Italian, which is also the most abstract dataset, with the highest average image dispersion score and the lowest variance. There is some evidence that abstract concepts are also perceptually grounded (Lakoff and Johnson, 1999), but in a more complex way, since abstract concepts express more varied situations (Barsalou and Wiemer-Hastings, 2005). Using an image resource like Google Images that has full coverage for almost any word, means that we can retrieve what we might call “associated” images (such as images of voters for words like democracy) as opposed to “extensional” images (such as images of cats for cat). This explains why we still obtain good performance on the more abstract VULIC1000 dataset, in some cases outperforming linguistic methods: even abstract concepts can have a clear visual representation, albeit of the associated rather than extensional kind. However, abstract concepts are overall more likely to yield noisier</context>
</contexts>
<marker>Barsalou, Wiemer-Hastings, 2005</marker>
<rawString>Lawrence W. Barsalou and Katja Wiemer-Hastings. 2005. Situating abstract concepts. In Grounding cognition: The role ofperception and action in memory, language, and thought, pages 129–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence W Barsalou</author>
</authors>
<title>Grounded cognition.</title>
<date>2008</date>
<journal>Annual Review of Psychology,</journal>
<volume>59</volume>
<issue>1</issue>
<contexts>
<context position="9076" citStr="Barsalou, 2008" startWordPosition="1398" endWordPosition="1399">ustration of calculating similarity between images from different languages. massive amounts of data and cheap GPUs has led to considerable advances in computer vision, similar in scale to those witnessed with SIFT and HOG descriptors a decade ago (Razavian et al., 2014). 2.3 Multi-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal vari</context>
</contexts>
<marker>Barsalou, 2008</marker>
<rawString>Lawrence W. Barsalou. 2008. Grounded cognition. Annual Review of Psychology, 59(1):617–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Bay</author>
</authors>
<title>Andreas Ess, Tinne Tuytelaars, and</title>
<date>2008</date>
<booktitle>Speeded-up robust features (SURF). Computer Vision and Image Understanding,</booktitle>
<volume>110</volume>
<issue>3</issue>
<marker>Bay, 2008</marker>
<rawString>Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc J. Van Gool. 2008. Speeded-up robust features (SURF). Computer Vision and Image Understanding, 110(3):346–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Randy Goebel</author>
</authors>
<title>Using visual information to predict lexical preference.</title>
<date>2011</date>
<booktitle>In RANLP,</booktitle>
<pages>399--405</pages>
<contexts>
<context position="11447" citStr="Bergsma and Goebel, 2011" startWordPosition="1774" endWordPosition="1777">; and two, how to compute the similarity between lexical items using their visual representations in the shared bilingual space. We also describe the evaluation datasets and metrics we use. To facilitate further research, we will make our code and data publicly available. Please see the following webpage: http://www.cl.cam. ac.uk/˜dk427/bli.html. 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also has the advantage that it has full coverage and is multi-lingual, as opposed to other potential image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004). For each Google search we specify the target language corresponding to the lexical item’s language. Figure 2 gives some example images retrieved using the same query terms in different languages. For each image, we extract the presoftmax layer of an AlexNet (Krizhevsky et al., </context>
</contexts>
<marker>Bergsma, Goebel, 2011</marker>
<rawString>Shane Bergsma and Randy Goebel. 2011. Using visual information to predict lexical preference. In RANLP, pages 399–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Learning bilingual lexicons using the visual similarity of labeled web images.</title>
<date>2011</date>
<booktitle>In IJCAI,</booktitle>
<pages>1764--1769</pages>
<marker>Bergsma, Van Durme, 2011</marker>
<rawString>Shane Bergsma and Benjamin Van Durme. 2011. Learning bilingual lexicons using the visual similarity of labeled web images. In IJCAI, pages 1764– 1769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Bosch</author>
<author>Andrew Zisserman</author>
<author>Xavier Mu˜noz</author>
</authors>
<title>Image classification using random forests and ferns.</title>
<date>2007</date>
<booktitle>In ICCV,</booktitle>
<pages>1--8</pages>
<marker>Bosch, Zisserman, Mu˜noz, 2007</marker>
<rawString>Anna Bosch, Andrew Zisserman, and Xavier Mu˜noz. 2007. Image classification using random forests and ferns. In ICCV, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Nam-Khanh Tran</author>
<author>Marco Baroni</author>
</authors>
<title>Multimodal distributional semantics.</title>
<date>2014</date>
<journal>Journal of Artifical Intelligence Research,</journal>
<pages>49--1</pages>
<contexts>
<context position="3133" citStr="Bruni et al., 2014" startWordPosition="469" endWordPosition="472">distance, an orthographic similarity metric (Navarro, 2001). There are several advantages to having a visual rather than a linguistic intermediate bilingual space: First, while labeled images are readily available for many languages through resources such as Google Images, language pairs that have sizeable comparable, let alone parallel, corpora are relatively scarce. Second, it has been found that meaning is often grounded in the perceptual system, and that the quality of semantic representations improves significantly when they are grounded in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Scha</context>
<context position="9475" citStr="Bruni et al., 2014" startWordPosition="1460" endWordPosition="1463"> that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-mod</context>
<context position="20503" citStr="Bruni et al., 2014" startWordPosition="3219" endWordPosition="3222">e similarity (SimLex-999) dataset. aggregated visual representation-based metrics of CNN-MEAN and CNN-MAX, despite the fact that Kiela and Bottou (2014) achieved optimal performance using the latter metrics on a well-known conceptual relatedness dataset. It has been noted before that there is a clear distinction between similarity and relatedness. This is one of the reasons that, for example, WordSim353 (Finkelstein et al., 2002) has been criticized: it gives high similarity scores to cases of genuine similarity as well as relatedness (Agirre et al., 2009; Hill et al., 2014). The MEN dataset (Bruni et al., 2014) that Kiela and Bottou (2014) evaluate on explicitly measures word relatedness. In contrast, the current lexicon learning task seems to require something else than relatedness: whilst a chair and table are semantically related, a translation for chair is not a good translation for table. For example, we want to make sure we translate chair to stuhl in German, and not to tisch. In other words, what we are interested in for this particular task is genuine similarity, rather than relatedness. Thus, we can evaluate the quality of our similarity metrics by comparing their performance on similarity </context>
<context position="27538" citStr="Bruni et al., 2014" startWordPosition="4350" endWordPosition="4353">e. the network has been trained for object recognition. If, however, we are interested in relatedness, related properties may just as well be encoded deeper in the network, so in the layers preceding FC7 rather than in FC7 itself. We combined CNN layers with each other by concatenating the normalized layers. For the bilingual lexicon induction tasks, we found that performance did not signficantly increase, which is consistent with our hypothesis (since bilingual lexicon induction requires genuine similarity rather than relatedness, and so only requires FC7). We then tested on the MEN dataset (Bruni et al., 2014) for relatedness and the nouns subset of the SimLex999 dataset (Hill et al., 2014) for genuine similarity. The results can be found in Table 5. The results appear to indicate that adding such additional information does not have a clear effect for genuine similarity, but may lead to a small performance increase for relatedness. This could explain why we did not see increased performance on the bilingual lexicon induction task with additional layers. However, the increase in performance on the relatedness task is relatively minor, and further investigation is required into the utility of the ad</context>
</contexts>
<marker>Bruni, Tran, Baroni, 2014</marker>
<rawString>Elia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014. Multimodal distributional semantics. Journal of Artifical Intelligence Research, 49:1–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Navneet Dalal</author>
<author>Bill Triggs</author>
</authors>
<title>Histograms of oriented gradients for human detection.</title>
<date>2005</date>
<booktitle>In CVPR,</booktitle>
<pages>886--893</pages>
<contexts>
<context position="9649" citStr="Dalal and Triggs, 2005" startWordPosition="1490" endWordPosition="1493">grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3 A Purely Visual Appro</context>
</contexts>
<marker>Dalal, Triggs, 2005</marker>
<rawString>Navneet Dalal and Bill Triggs. 2005. Histograms of oriented gradients for human detection. In CVPR, pages 886–893.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Fei-Fei Li</author>
</authors>
<title>ImageNet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In CVPR,</booktitle>
<pages>248--255</pages>
<contexts>
<context position="11715" citStr="Deng et al., 2009" startWordPosition="1817" endWordPosition="1820">ease see the following webpage: http://www.cl.cam. ac.uk/˜dk427/bli.html. 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also has the advantage that it has full coverage and is multi-lingual, as opposed to other potential image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004). For each Google search we specify the target language corresponding to the lexical item’s language. Figure 2 gives some example images retrieved using the same query terms in different languages. For each image, we extract the presoftmax layer of an AlexNet (Krizhevsky et al., 2012). The network contains a number of layers, starting with five convolutional layers, two fully connected layers and finally a softmax, and has been pre-trained on the ImageNet classification task using Caffe (Jia et al., 2014). See Figure 1 for a simple diagram il</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Li, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. 2009. ImageNet: A large-scale hierarchical image database. In CVPR, pages 248– 255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Deselaers</author>
<author>Daniel Keysers</author>
<author>Hermann Ney</author>
</authors>
<title>Features for image retrieval: An experimental comparison.</title>
<date>2008</date>
<journal>Information Retrieval,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="2286" citStr="Deselaers et al., 2008" startWordPosition="339" endWordPosition="342">nguage pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and extracting lexical items from that space. Bergsma and Van Durme (2011) showed that this bilingual space need not be linguistic in nature: they used labeled images from the Web to obtain bilingual lexical translation pairs based on the visual features of corresponding images. Local features are computed using SIFT (Lowe, 2004) and color histograms (Deselaers et al., 2008) and aggregated as bags of visual words (BOVW) (Sivic and Zisserman, 2003) to get bilingual representations in a shared visual space. Their highest performance is obtained by combining these visual features with normalized edit distance, an orthographic similarity metric (Navarro, 2001). There are several advantages to having a visual rather than a linguistic intermediate bilingual space: First, while labeled images are readily available for many languages through resources such as Google Images, language pairs that have sizeable comparable, let alone parallel, corpora are relatively scarce. S</context>
</contexts>
<marker>Deselaers, Keysers, Ney, 2008</marker>
<rawString>Thomas Deselaers, Daniel Keysers, and Hermann Ney. 2008. Features for image retrieval: An experimental comparison. Information Retrieval, 11(2):77–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Driancourt</author>
<author>L´eon Bottou</author>
</authors>
<title>TDNNextracted features.</title>
<date>1990</date>
<booktitle>In Neuro Nimes 90.</booktitle>
<contexts>
<context position="8409" citStr="Driancourt and Bottou, 1990" startWordPosition="1295" endWordPosition="1298"> popular in the computer vision community. These networks currently provide state-of-the-art performance for a variety of key computer vision tasks such as object recognition (Razavian et al., 2014). They tend to be relatively deep, consisting of a number of rectified linear unit layers (Nair and Hinton, 2010) and a series of convolutional layers (Krizhevsky et al., 2012). Recently, such layers have been used in transfer learning techniques, where they are used as mid-level features in other computer vision tasks (Oquab et al., 2014). Although the idea of transferring CNN features is not new (Driancourt and Bottou, 1990), the simultaneous availability of 149 Figure 1: Illustration of calculating similarity between images from different languages. massive amounts of data and cheap GPUs has led to considerable advances in computer vision, similar in scale to those witnessed with SIFT and HOG descriptors a decade ago (Razavian et al., 2014). 2.3 Multi-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that m</context>
</contexts>
<marker>Driancourt, Bottou, 1990</marker>
<rawString>Xavier Driancourt and L´eon Bottou. 1990. TDNNextracted features. In Neuro Nimes 90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Visual information in semantic representation.</title>
<date>2010</date>
<booktitle>In NAACL,</booktitle>
<pages>91--99</pages>
<contexts>
<context position="9429" citStr="Feng and Lapata, 2010" startWordPosition="1452" endWordPosition="1455">with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Ki</context>
</contexts>
<marker>Feng, Lapata, 2010</marker>
<rawString>Yansong Feng and Mirella Lapata. 2010. Visual information in semantic representation. In NAACL, pages 91–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Fergus</author>
<author>Fei-Fei Li</author>
<author>Pietro Perona</author>
<author>Andrew Zisserman</author>
</authors>
<title>Learning object categories from Google’s image search.</title>
<date>2005</date>
<booktitle>In ICCV,</booktitle>
<pages>1816--1823</pages>
<contexts>
<context position="11548" citStr="Fergus et al., 2005" startWordPosition="1788" endWordPosition="1791">hared bilingual space. We also describe the evaluation datasets and metrics we use. To facilitate further research, we will make our code and data publicly available. Please see the following webpage: http://www.cl.cam. ac.uk/˜dk427/bli.html. 3.1 Image Representations We use Google Images to extract the top n ranked images for each lexical item in the evaluation datasets. It has been shown that images from Google yield higher quality representations than comparable sources such as Flickr (Bergsma and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also has the advantage that it has full coverage and is multi-lingual, as opposed to other potential image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004). For each Google search we specify the target language corresponding to the lexical item’s language. Figure 2 gives some example images retrieved using the same query terms in different languages. For each image, we extract the presoftmax layer of an AlexNet (Krizhevsky et al., 2012). The network contains a number of layers, starting with five convolutional layers, two fully co</context>
</contexts>
<marker>Fergus, Li, Perona, Zisserman, 2005</marker>
<rawString>Robert Fergus, Fei-Fei Li, Pietro Perona, and Andrew Zisserman. 2005. Learning object categories from Google’s image search. In ICCV, pages 1816–1823.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing Search in Context: The Concept Revisited.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="20317" citStr="Finkelstein et al., 2002" startWordPosition="3186" endWordPosition="3189">SimLex-999 CNN-AVGMAX 0.56 0.34 CNN-MAXMAX 0.55 0.36 CNN-MEAN 0.61 0.32 CNN-MAX 0.60 0.27 Table 3: Spearman ρs correlation for the visual similarity metrics on a relatedness (MEN) and a genuine similarity (SimLex-999) dataset. aggregated visual representation-based metrics of CNN-MEAN and CNN-MAX, despite the fact that Kiela and Bottou (2014) achieved optimal performance using the latter metrics on a well-known conceptual relatedness dataset. It has been noted before that there is a clear distinction between similarity and relatedness. This is one of the reasons that, for example, WordSim353 (Finkelstein et al., 2002) has been criticized: it gives high similarity scores to cases of genuine similarity as well as relatedness (Agirre et al., 2009; Hill et al., 2014). The MEN dataset (Bruni et al., 2014) that Kiela and Bottou (2014) evaluate on explicitly measures word relatedness. In contrast, the current lexicon learning task seems to require something else than relatedness: whilst a chair and table are semantically related, a translation for chair is not a good translation for table. For example, we want to make sure we translate chair to stuhl in German, and not to tisch. In other words, what we are intere</context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2002</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing Search in Context: The Concept Revisited. ACM Transactions on Information Systems, 20(1):116—131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Gregory S Corrado</author>
<author>Jonathon Shlens</author>
<author>Samy Bengio</author>
<author>Jeffrey Dean</author>
<author>Marc’Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
</authors>
<title>Devise: A deep visualsemantic embedding model.</title>
<date>2013</date>
<booktitle>In NIPS,</booktitle>
<pages>2121--2129</pages>
<contexts>
<context position="9980" citStr="Frome et al., 2013" startWordPosition="1543" endWordPosition="1546">) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3 A Purely Visual Approach to Bilingual Lexicon Learning We assume that the best translation, or matching lexical item, of a word ws (in the source language) is the word wt (in the target language) that is the nearest cross-lingual neighbour to ws in the bilingual visual space. Hence a similarity (or distance) score between lexical items from different</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Ranzato, Mikolov, 2013</marker>
<rawString>Andrea Frome, Gregory S. Corrado, Jonathon Shlens, Samy Bengio, Jeffrey Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. Devise: A deep visualsemantic embedding model. In NIPS, pages 2121– 2129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascale Fung</author>
<author>Lo Yuen Yee</author>
</authors>
<title>An IR approach for translating new words from nonparallel, comparable texts.</title>
<date>1998</date>
<booktitle>In ACL,</booktitle>
<pages>414--420</pages>
<contexts>
<context position="1806" citStr="Fung and Yee, 1998" startWordPosition="260" endWordPosition="263">aning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and extracting lexical items from that space. Bergsma and Van Durme (2011) showed that this bilingual space need not be linguistic in nature: they used labeled images from the Web to obtain bilingual lexical translation pairs based on the visual features of corresponding images. Local features are computed using SIFT (Lowe, 2004) and color histograms (Deselaers et al., 2008) and aggregated as bags of visual words (BOVW) (Sivic and Zisserman, 2003) to get bilingual representations in a shared </context>
<context position="6369" citStr="Fung and Yee, 1998" startWordPosition="990" endWordPosition="993">lingual lexicon learning is the task of automatically inducing word translations from raw data, and is an attractive alternative to the timeconsuming and expensive process of manually building high-quality resources for a wide variety of language pairs and domains. Early approaches relied on limited and domain-restricted parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using</context>
</contexts>
<marker>Fung, Yee, 1998</marker>
<rawString>Pascale Fung and Lo Yuen Yee. 1998. An IR approach for translating new words from nonparallel, comparable texts. In ACL, pages 414–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>´Eric Gaussier</author>
<author>Jean-Michel Renders</author>
<author>Irina Matveeva</author>
<author>Cyril Goutte</author>
<author>Herv´e D´ejean</author>
</authors>
<title>A geometric view on bilingual lexicon extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>526--533</pages>
<marker>Gaussier, Renders, Matveeva, Goutte, D´ejean, 2004</marker>
<rawString>´Eric Gaussier, Jean-Michel Renders, Irina Matveeva, Cyril Goutte, and Herv´e D´ejean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In ACL, pages 526–533.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Girshick</author>
<author>Jeff Donahue</author>
<author>Trevor Darrell</author>
<author>Jitendra Malik</author>
</authors>
<title>Rich feature hierarchies for accurate object detection and semantic segmentation.</title>
<date>2014</date>
<booktitle>In CVPR.</booktitle>
<contexts>
<context position="26039" citStr="Girshick et al., 2014" startWordPosition="4110" endWordPosition="4113">en there is less or lower quality text data available —which one might reasonably expect to be the default scenario. 4.3 Adding CNN Layers The AlexNet (Krizhevsky et al., 2012) from which our image representations are extracted contains a number of layers. Kiela and Bottou (2014) only use the fully connected pre-softmax layer (which we call FC7) for their image representations. It has been found, however, that other layers in the network, especially the preceding fully connected (FC6) and fifth convolutional max pooling (POOL5) layers, also have good properties for usage in transfer learning (Girshick et al., 2014; Yosinski et al., 2014). Hence we performed a (very) preliminary investigation of whether performance increases with the use of additional layers. In light of our findings concerning the difference between genuine similarity and relatedness, this also gives rise to the question of whether the additional layers might be useful for similarity or relatedness, or both. We hypothesize that the nature of the task matters here: if we are only concerned with genuine similarity, layer FC7 is likely to contain all the necessary information to judge whether two images are similar or not, since 154 Datas</context>
</contexts>
<marker>Girshick, Donahue, Darrell, Malik, 2014</marker>
<rawString>Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>771--779</pages>
<contexts>
<context position="3661" citStr="Haghighi et al., 2008" startWordPosition="555" endWordPosition="558">when they are grounded in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for 148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. closely related languages, a visual space can work for any language, whether it’s English or Chinese, Arabic or Icelandic, or all Greek to you. It has recently been shown, however, that much better performance can be achieved on semantic similarity and relatedness tasks by using vis</context>
<context position="6779" citStr="Haghighi et al., 2008" startWordPosition="1054" endWordPosition="1057">nt models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic models (Vuli´c et al., 2011; Liu et al., 2013; Vuli´c and Moens, 2013b). However, these models require document alignments as initial bilingual signals. In this work, following recent research in multi-modal semantics and image representation learning—in particular deep learning and convolutional neural networks—we test the ability of purely visu</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In ACL, pages 771–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
</authors>
<title>Learning abstract concept embeddings from multi-modal data: Since you probably can’t see what I mean.</title>
<date>2014</date>
<booktitle>In EMNLP,</booktitle>
<pages>255--265</pages>
<contexts>
<context position="9363" citStr="Hill and Korhonen, 2014" startWordPosition="1441" endWordPosition="1444">ulti-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (F</context>
</contexts>
<marker>Hill, Korhonen, 2014</marker>
<rawString>Felix Hill and Anna Korhonen. 2014. Learning abstract concept embeddings from multi-modal data: Since you probably can’t see what I mean. In EMNLP, pages 255–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>SimLex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<location>CoRR, abs/1408.3456.</location>
<contexts>
<context position="20465" citStr="Hill et al., 2014" startWordPosition="3212" endWordPosition="3215">s on a relatedness (MEN) and a genuine similarity (SimLex-999) dataset. aggregated visual representation-based metrics of CNN-MEAN and CNN-MAX, despite the fact that Kiela and Bottou (2014) achieved optimal performance using the latter metrics on a well-known conceptual relatedness dataset. It has been noted before that there is a clear distinction between similarity and relatedness. This is one of the reasons that, for example, WordSim353 (Finkelstein et al., 2002) has been criticized: it gives high similarity scores to cases of genuine similarity as well as relatedness (Agirre et al., 2009; Hill et al., 2014). The MEN dataset (Bruni et al., 2014) that Kiela and Bottou (2014) evaluate on explicitly measures word relatedness. In contrast, the current lexicon learning task seems to require something else than relatedness: whilst a chair and table are semantically related, a translation for chair is not a good translation for table. For example, we want to make sure we translate chair to stuhl in German, and not to tisch. In other words, what we are interested in for this particular task is genuine similarity, rather than relatedness. Thus, we can evaluate the quality of our similarity metrics by comp</context>
<context position="27620" citStr="Hill et al., 2014" startWordPosition="4365" endWordPosition="4368">ted in relatedness, related properties may just as well be encoded deeper in the network, so in the layers preceding FC7 rather than in FC7 itself. We combined CNN layers with each other by concatenating the normalized layers. For the bilingual lexicon induction tasks, we found that performance did not signficantly increase, which is consistent with our hypothesis (since bilingual lexicon induction requires genuine similarity rather than relatedness, and so only requires FC7). We then tested on the MEN dataset (Bruni et al., 2014) for relatedness and the nouns subset of the SimLex999 dataset (Hill et al., 2014) for genuine similarity. The results can be found in Table 5. The results appear to indicate that adding such additional information does not have a clear effect for genuine similarity, but may lead to a small performance increase for relatedness. This could explain why we did not see increased performance on the bilingual lexicon induction task with additional layers. However, the increase in performance on the relatedness task is relatively minor, and further investigation is required into the utility of the additional layers for relatedness tasks. 5 Discussion A possible explanation for the</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. SimLex-999: Evaluating semantic models with (genuine) similarity estimation. CoRR, abs/1408.3456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangqing Jia</author>
<author>Evan Shelhamer</author>
<author>Jeff Donahue</author>
<author>Sergey Karayev</author>
<author>Jonathan Long</author>
<author>Ross B Girshick</author>
<author>Sergio Guadarrama</author>
<author>Trevor Darrell</author>
</authors>
<title>Caffe: Convolutional architecture for fast feature embedding.</title>
<date>2014</date>
<booktitle>In ACM Multimedia,</booktitle>
<pages>675--678</pages>
<contexts>
<context position="12277" citStr="Jia et al., 2014" startWordPosition="1911" endWordPosition="1914">tial image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004). For each Google search we specify the target language corresponding to the lexical item’s language. Figure 2 gives some example images retrieved using the same query terms in different languages. For each image, we extract the presoftmax layer of an AlexNet (Krizhevsky et al., 2012). The network contains a number of layers, starting with five convolutional layers, two fully connected layers and finally a softmax, and has been pre-trained on the ImageNet classification task using Caffe (Jia et al., 2014). See Figure 1 for a simple diagram illustrating the approach. 3.2 Visual Similarity Suppose that, as part of the evaluation, the similarity between bicycle and fiets is required. Each of 150 the two words has n images associated with it – the top n as returned by Google image search, using bicycle and fiets as separate query terms. Hence to calculate the similarity, a measure is required which takes two sets of images as input. The standard approach in multi-modal semantics is to derive a single image representation for each word, e.g., by averaging the n images. An alternative is to take the</context>
</contexts>
<marker>Jia, Shelhamer, Donahue, Karayev, Long, Girshick, Guadarrama, Darrell, 2014</marker>
<rawString>Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross B. Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional architecture for fast feature embedding. In ACM Multimedia, pages 675–678.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>L´eon Bottou</author>
</authors>
<title>Learning image embeddings using convolutional neural networks for improved multi-modal semantics.</title>
<date>2014</date>
<booktitle>In EMNLP,</booktitle>
<pages>36--45</pages>
<contexts>
<context position="4377" citStr="Kiela and Bottou, 2014" startWordPosition="662" endWordPosition="665">2) that will work only for 148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. closely related languages, a visual space can work for any language, whether it’s English or Chinese, Arabic or Icelandic, or all Greek to you. It has recently been shown, however, that much better performance can be achieved on semantic similarity and relatedness tasks by using visual representations from deep convolutional neural networks (CNNs) instead of BOVW features (Kiela and Bottou, 2014). In this paper we apply such CNN-derived visual features to the task of bilingual lexicon induction. To obtain a translation of a word in a source language, we find the nearest neighbours from words in the target language, where words in both languages reside in a shared visual space made up of CNN-based features. Nearest neighbours are found by applying similarity metrics from both Kiela and Bottou (2014) and Bergsma and Van Durme (2011). In summary, the contributions of this paper are: • We obtain a relative improvement of 79% over Bergsma and Van Durme (2011) on a standard dataset based on</context>
<context position="9856" citStr="Kiela and Bottou, 2014" startWordPosition="1522" endWordPosition="1526">orming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3 A Purely Visual Approach to Bilingual Lexicon Learning We assume that the best translation, or matching lexical item, of a word ws (in the source language) is the word wt (in the target language) that is the nearest cross-lingua</context>
<context position="12994" citStr="Kiela and Bottou, 2014" startWordPosition="2036" endWordPosition="2039">hat, as part of the evaluation, the similarity between bicycle and fiets is required. Each of 150 the two words has n images associated with it – the top n as returned by Google image search, using bicycle and fiets as separate query terms. Hence to calculate the similarity, a measure is required which takes two sets of images as input. The standard approach in multi-modal semantics is to derive a single image representation for each word, e.g., by averaging the n images. An alternative is to take the pointwise maximum across the n image vector representations, also producing a single vector (Kiela and Bottou, 2014). Kiela and Bottou call these combined representations CNNMEAN and CNN-MAX, respectively. Cosine is then used to calculate the similarity between the resulting pair of image vectors. An alternative strategy, however, is to consider the similarities between individual images instead of their aggregated representations. Bergsma and Van Durme (2011) propose two similarity metrics based on this principle: taking the average of the maximum similarity scores (AVGMAX), or the maximum of the maximum similarity scores (MAXMAX) between associated images. Continuing with our example, for each of the n im</context>
<context position="20036" citStr="Kiela and Bottou (2014)" startWordPosition="3142" endWordPosition="3145">0.277 CNN-AVGMAX 38.4 48.5 53.7 58.6 0.435 NL ⇒ EN CNN-MAXMAX 30.8 42.6 47.8 52.9 0.367 CNN-MEAN 32.3 42.3 46.5 50.1 0.373 CNN-MAX 30.4 41.0 44.3 49.3 0.356 Table 4: Performance on VULIC1000 compared to the linguistic bootstrapping method of Vuli´c and Moens (2013b). Method MEN SimLex-999 CNN-AVGMAX 0.56 0.34 CNN-MAXMAX 0.55 0.36 CNN-MEAN 0.61 0.32 CNN-MAX 0.60 0.27 Table 3: Spearman ρs correlation for the visual similarity metrics on a relatedness (MEN) and a genuine similarity (SimLex-999) dataset. aggregated visual representation-based metrics of CNN-MEAN and CNN-MAX, despite the fact that Kiela and Bottou (2014) achieved optimal performance using the latter metrics on a well-known conceptual relatedness dataset. It has been noted before that there is a clear distinction between similarity and relatedness. This is one of the reasons that, for example, WordSim353 (Finkelstein et al., 2002) has been criticized: it gives high similarity scores to cases of genuine similarity as well as relatedness (Agirre et al., 2009; Hill et al., 2014). The MEN dataset (Bruni et al., 2014) that Kiela and Bottou (2014) evaluate on explicitly measures word relatedness. In contrast, the current lexicon learning task seems </context>
<context position="22341" citStr="Kiela and Bottou (2014)" startWordPosition="3524" endWordPosition="3527">tandard human similarity scores. The results are reported in Table 3. It is clear that the per-image similarity metrics perform better on genuine similarity, as measured by SimLex-999, than on relatedness, as measured by MEN. In fact, the “aggressive” CNNMAXMAX method, which picks out a single pair of images to represent a linguistic pair, works best for SimLex-999, indicating how stringently it focuses on genuine similarity. For the aggregated visual representation-based metrics, we see the opposite effect: they perform better on the relatedness task. This sheds light on a question raised by Kiela and Bottou (2014), where they speculate 153 that certain errors are a result of whether their visual similarity metric measures genuine similarity on the one hand or relatedness on the other: we are better off using per-image visual metrics for genuine similarity, while aggregated visual representation-based metrics yield better performance on relatedness tasks. 4.2 Results on VULIC1000 This section compares our visual-only approach to linguistic approaches for bilingual lexicon induction. Since BERGSMA500 has not been evaluated with such methods, we evaluate on the VULIC1000 dataset (Vuli´c and Moens, 2013a).</context>
<context position="25698" citStr="Kiela and Bottou (2014)" startWordPosition="4056" endWordPosition="4059">re than one layer from the CNN. quality than the data for their ES-EN and IT-EN models. We view these results as highly encouraging: while purely visual methods cannot yet reach the peak performance of linguistic approaches that are trained on sufficient amounts of high-quality text data, they outperform linguistic state-of-theart methods when there is less or lower quality text data available —which one might reasonably expect to be the default scenario. 4.3 Adding CNN Layers The AlexNet (Krizhevsky et al., 2012) from which our image representations are extracted contains a number of layers. Kiela and Bottou (2014) only use the fully connected pre-softmax layer (which we call FC7) for their image representations. It has been found, however, that other layers in the network, especially the preceding fully connected (FC6) and fifth convolutional max pooling (POOL5) layers, also have good properties for usage in transfer learning (Girshick et al., 2014; Yosinski et al., 2014). Hence we performed a (very) preliminary investigation of whether performance increases with the use of additional layers. In light of our findings concerning the difference between genuine similarity and relatedness, this also gives </context>
</contexts>
<marker>Kiela, Bottou, 2014</marker>
<rawString>Douwe Kiela and L´eon Bottou. 2014. Learning image embeddings using convolutional neural networks for improved multi-modal semantics. In EMNLP, pages 36–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Felix Hill</author>
<author>Anna Korhonen</author>
<author>Stephen Clark</author>
</authors>
<title>Improving multi-modal representations using image dispersion: Why less is sometimes more.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>835--841</pages>
<contexts>
<context position="9496" citStr="Kiela et al., 2014" startWordPosition="1464" endWordPosition="1467">edge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use </context>
<context position="28563" citStr="Kiela et al., 2014" startWordPosition="4519" endWordPosition="4522"> lexicon induction task with additional layers. However, the increase in performance on the relatedness task is relatively minor, and further investigation is required into the utility of the additional layers for relatedness tasks. 5 Discussion A possible explanation for the difference in performance between languages and datasets is that some words are more concrete than others: a visual representation for elephant is likely to be of higher quality than one for happiness. Visual representations in multi-modal models have been found to perform much better for concrete than abstract concepts (Kiela et al., 2014). Although concreteness ratings are available for (some) English words, this is not the case for other languages, so in order to examine the concreteness of the datasets we use a substitute method that has been shown to closely mirror how abstract a concept is: image dispersion (Kiela et al., 2014). The image dispersion d of a concept word w is defined as the average pairwise cosine distance between all the image representations {ii ... in} in the set of images for a given word: 2 1 −ij · ik d(w) = n(n − 1) i&lt;j≤n |ij||ik |(2) The average image dispersions for the two datasets, broken down by l</context>
</contexts>
<marker>Kiela, Hill, Korhonen, Clark, 2014</marker>
<rawString>Douwe Kiela, Felix Hill, Anna Korhonen, and Stephen Clark. 2014. Improving multi-modal representations using image dispersion: Why less is sometimes more. In ACL, pages 835–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Kiros</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard S Zemel</author>
</authors>
<title>Multimodal neural language models.</title>
<date>2014</date>
<booktitle>In ICML,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="10046" citStr="Kiros et al., 2014" startWordPosition="1555" endWordPosition="1558">10; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3 A Purely Visual Approach to Bilingual Lexicon Learning We assume that the best translation, or matching lexical item, of a word ws (in the source language) is the word wt (in the target language) that is the nearest cross-lingual neighbour to ws in the bilingual visual space. Hence a similarity (or distance) score between lexical items from different languages is required. In this section, we describe: one, how to </context>
</contexts>
<marker>Kiros, Salakhutdinov, Zemel, 2014</marker>
<rawString>Ryan Kiros, Ruslan Salakhutdinov, and Richard S. Zemel. 2014. Multimodal neural language models. In ICML, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In ULA’02 Workshop,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="1831" citStr="Koehn and Knight, 2002" startWordPosition="264" endWordPosition="267">nt languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and extracting lexical items from that space. Bergsma and Van Durme (2011) showed that this bilingual space need not be linguistic in nature: they used labeled images from the Web to obtain bilingual lexical translation pairs based on the visual features of corresponding images. Local features are computed using SIFT (Lowe, 2004) and color histograms (Deselaers et al., 2008) and aggregated as bags of visual words (BOVW) (Sivic and Zisserman, 2003) to get bilingual representations in a shared visual space. Their highe</context>
<context position="3686" citStr="Koehn and Knight, 2002" startWordPosition="559" endWordPosition="562">in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for 148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. closely related languages, a visual space can work for any language, whether it’s English or Chinese, Arabic or Icelandic, or all Greek to you. It has recently been shown, however, that much better performance can be achieved on semantic similarity and relatedness tasks by using visual representations from </context>
<context position="6755" citStr="Koehn and Knight, 2002" startWordPosition="1050" endWordPosition="1053">-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic models (Vuli´c et al., 2011; Liu et al., 2013; Vuli´c and Moens, 2013b). However, these models require document alignments as initial bilingual signals. In this work, following recent research in multi-modal semantics and image representation learning—in particular deep learning and convolutional neural networks—we test th</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In ULA’02 Workshop, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>ImageNet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In NIPS,</booktitle>
<pages>1106--1114</pages>
<contexts>
<context position="8155" citStr="Krizhevsky et al., 2012" startWordPosition="1255" endWordPosition="1258">istic concepts given in different languages, the potentially prohibitive data requirements and language pair-dependence from prior work is removed. 2.2 Deep Convolutional Neural Networks Deep convolutional neural networks (CNNs) have become extremely popular in the computer vision community. These networks currently provide state-of-the-art performance for a variety of key computer vision tasks such as object recognition (Razavian et al., 2014). They tend to be relatively deep, consisting of a number of rectified linear unit layers (Nair and Hinton, 2010) and a series of convolutional layers (Krizhevsky et al., 2012). Recently, such layers have been used in transfer learning techniques, where they are used as mid-level features in other computer vision tasks (Oquab et al., 2014). Although the idea of transferring CNN features is not new (Driancourt and Bottou, 1990), the simultaneous availability of 149 Figure 1: Illustration of calculating similarity between images from different languages. massive amounts of data and cheap GPUs has led to considerable advances in computer vision, similar in scale to those witnessed with SIFT and HOG descriptors a decade ago (Razavian et al., 2014). 2.3 Multi-Modal Seman</context>
<context position="12052" citStr="Krizhevsky et al., 2012" startWordPosition="1874" endWordPosition="1877">a and Goebel, 2011) and that Google-derived datasets are competitive with “hand prepared datasets” (Fergus et al., 2005). Google Images also has the advantage that it has full coverage and is multi-lingual, as opposed to other potential image sources such as ImageNet (Deng et al., 2009) or the ESP Game Dataset (von Ahn and Dabbish, 2004). For each Google search we specify the target language corresponding to the lexical item’s language. Figure 2 gives some example images retrieved using the same query terms in different languages. For each image, we extract the presoftmax layer of an AlexNet (Krizhevsky et al., 2012). The network contains a number of layers, starting with five convolutional layers, two fully connected layers and finally a softmax, and has been pre-trained on the ImageNet classification task using Caffe (Jia et al., 2014). See Figure 1 for a simple diagram illustrating the approach. 3.2 Visual Similarity Suppose that, as part of the evaluation, the similarity between bicycle and fiets is required. Each of 150 the two words has n images associated with it – the top n as returned by Google image search, using bicycle and fiets as separate query terms. Hence to calculate the similarity, a mea</context>
<context position="25594" citStr="Krizhevsky et al., 2012" startWordPosition="4040" endWordPosition="4043">e visual similarity metrics on a relatedness (MEN) and a genuine similarity (SimLex-999) dataset using more than one layer from the CNN. quality than the data for their ES-EN and IT-EN models. We view these results as highly encouraging: while purely visual methods cannot yet reach the peak performance of linguistic approaches that are trained on sufficient amounts of high-quality text data, they outperform linguistic state-of-theart methods when there is less or lower quality text data available —which one might reasonably expect to be the default scenario. 4.3 Adding CNN Layers The AlexNet (Krizhevsky et al., 2012) from which our image representations are extracted contains a number of layers. Kiela and Bottou (2014) only use the fully connected pre-softmax layer (which we call FC7) for their image representations. It has been found, however, that other layers in the network, especially the preceding fully connected (FC6) and fifth convolutional max pooling (POOL5) layers, also have good properties for usage in transfer learning (Girshick et al., 2014; Yosinski et al., 2014). Hence we performed a (very) preliminary investigation of whether performance increases with the use of additional layers. In ligh</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. 2012. ImageNet classification with deep convolutional neural networks. In NIPS, pages 1106– 1114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Lakoff</author>
<author>Mark Johnson</author>
</authors>
<title>Philosophy in the flesh: The embodied mind and its challenge to Western thought.</title>
<date>1999</date>
<contexts>
<context position="29771" citStr="Lakoff and Johnson, 1999" startWordPosition="4730" endWordPosition="4734"> broken down by language, are shown in Table 6. BERGSMA500 has a lower average image dispersion score in general, and thus is more concrete than VULIC1000. It also has less variance. This may explain why we score higher, in absolute terms, on that dataset than on the more abstract one. When examining individual languages in the datasets, we note that the worst performing language on VULIC1000 is Italian, which is also the most abstract dataset, with the highest average image dispersion score and the lowest variance. There is some evidence that abstract concepts are also perceptually grounded (Lakoff and Johnson, 1999), but in a more complex way, since abstract concepts express more varied situations (Barsalou and Wiemer-Hastings, 2005). Using an image resource like Google Images that has full coverage for almost any word, means that we can retrieve what we might call “associated” images (such as images of voters for words like democracy) as opposed to “extensional” images (such as images of cats for cat). This explains why we still obtain good performance on the more abstract VULIC1000 dataset, in some cases outperforming linguistic methods: even abstract concepts can have a clear visual representation, al</context>
</contexts>
<marker>Lakoff, Johnson, 1999</marker>
<rawString>George Lakoff and Mark Johnson. 1999. Philosophy in the flesh: The embodied mind and its challenge to Western thought.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Lavrenko</author>
<author>Martin Choquette</author>
<author>W Bruce Croft</author>
</authors>
<title>Cross-lingual relevance models.</title>
<date>2002</date>
<booktitle>In SIGIR,</booktitle>
<pages>175--182</pages>
<contexts>
<context position="1397" citStr="Lavrenko et al., 2002" startWordPosition="196" endWordPosition="199">n, even outperforming these for one of three language pairs on another standard dataset. Furthermore, we shed new light on the type of visual similarity metric to use for genuine similarity versus relatedness tasks, and experiment with using multiple layers from the same network in an attempt to improve performance. 1 Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and extracting lexical items from that space. Bergsma and Van Durme (2011) showed that t</context>
</contexts>
<marker>Lavrenko, Choquette, Croft, 2002</marker>
<rawString>Victor Lavrenko, Martin Choquette, and W. Bruce Croft. 2002. Cross-lingual relevance models. In SIGIR, pages 175–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Elia Bruni</author>
<author>Marco Baroni</author>
</authors>
<title>Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world. In</title>
<date>2014</date>
<booktitle>ACL,</booktitle>
<pages>1403--1414</pages>
<contexts>
<context position="10025" citStr="Lazaridou et al., 2014" startWordPosition="1551" endWordPosition="1554">ata (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3 A Purely Visual Approach to Bilingual Lexicon Learning We assume that the best translation, or matching lexical item, of a word ws (in the source language) is the word wt (in the target language) that is the nearest cross-lingual neighbour to ws in the bilingual visual space. Hence a similarity (or distance) score between lexical items from different languages is required. In this section, we d</context>
</contexts>
<marker>Lazaridou, Bruni, Baroni, 2014</marker>
<rawString>Angeliki Lazaridou, Elia Bruni, and Marco Baroni. 2014. Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world. In ACL, pages 1403–1414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chee Wee Leong</author>
<author>Rada Mihalcea</author>
</authors>
<title>Going beyond text: A hybrid image-text approach for measuring word relatedness.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>1403--1407</pages>
<contexts>
<context position="9455" citStr="Leong and Mihalcea, 2011" startWordPosition="1456" endWordPosition="1459">isition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other e</context>
</contexts>
<marker>Leong, Mihalcea, 2011</marker>
<rawString>Chee Wee Leong and Rada Mihalcea. 2011. Going beyond text: A hybrid image-text approach for measuring word relatedness. In IJCNLP, pages 1403– 1407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gina-Anne Levow</author>
<author>Douglas Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Dictionary-based techniques for crosslanguage information retrieval.</title>
<date>2005</date>
<booktitle>Information Processing &amp; Management, 41:523 – 547,</booktitle>
<pages>2005--05</pages>
<contexts>
<context position="1418" citStr="Levow et al., 2005" startWordPosition="200" endWordPosition="203">hese for one of three language pairs on another standard dataset. Furthermore, we shed new light on the type of visual similarity metric to use for genuine similarity versus relatedness tasks, and experiment with using multiple layers from the same network in an attempt to improve performance. 1 Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and extracting lexical items from that space. Bergsma and Van Durme (2011) showed that this bilingual space n</context>
</contexts>
<marker>Levow, Oard, Resnik, 2005</marker>
<rawString>Gina-Anne Levow, Douglas Oard, and Philip Resnik. 2005. Dictionary-based techniques for crosslanguage information retrieval. Information Processing &amp; Management, 41:523 – 547, 2005/05//.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Kenneth Ward Church</author>
<author>Heng Ji</author>
<author>Satoshi Sekine</author>
<author>David Yarowsky</author>
<author>Shane Bergsma</author>
</authors>
<title>New tools for Web-scale N-grams. In</title>
<date>2010</date>
<booktitle>LREC,</booktitle>
<pages>2221--2227</pages>
<institution>Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant</institution>
<contexts>
<context position="14567" citStr="Lin et al., 2010" startWordPosition="2285" endWordPosition="2288">with both kinds of MAX and find that they optimize for different kinds of similarity. 3.3 Evaluations Test Sets. Bergsma and Van Durme’s primary evaluation dataset consists of a set of five hundred matching lexical items for fifteen language pairs, based on six languages. (The fifteen pairs results from all ways of pairing six languages). The data is publicly available online.1 In order to get the five hundred lexical items, they first rank nouns by the conditional probability of them occurring in the pattern “{image,photo,photograph,picture} of {a,an} ” in the web-scale Google N-gram corpus (Lin et al., 2010), and take the top five hundred words as their English lexicon. For each item 1http://www.clsp.jhu.edu/˜sbergsma/LexImg/ � max sim(is, it) 1 itET(wt) n i3ET(w3) max max sim(is, it) i3ET(w3) itET(wt) �sim(n1 � it) i3ET(w3) 1 is, n itET(wt) sim(max&apos;Z(ws), max&apos;Z(wt)) Table 1: Visual similarity metrics between two sets of n images. Z(ws) represents the set of images for a given source word ws, Z(wt) the set of images for a given target word wt; max&apos; takes a set of vectors and returns the single element-wise maximum vector. in the English lexicon, they obtain corresponding items in the other langua</context>
</contexts>
<marker>Lin, Church, Ji, Sekine, Yarowsky, Bergsma, 2010</marker>
<rawString>Dekang Lin, Kenneth Ward Church, Heng Ji, Satoshi Sekine, David Yarowsky, Shane Bergsma, Kailash Patil, Emily Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani, and Sushant Narsale. 2010. New tools for Web-scale N-grams. In LREC, pages 2221–2227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Liu</author>
<author>Kevin Duh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Topic models + word alignment = A flexible framework for extracting bilingual dictionary from comparable corpus. In CoNLL,</title>
<date>2013</date>
<pages>212--221</pages>
<contexts>
<context position="7076" citStr="Liu et al., 2013" startWordPosition="1099" endWordPosition="1102">ver, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic models (Vuli´c et al., 2011; Liu et al., 2013; Vuli´c and Moens, 2013b). However, these models require document alignments as initial bilingual signals. In this work, following recent research in multi-modal semantics and image representation learning—in particular deep learning and convolutional neural networks—we test the ability of purely visual data to induce shared bilingual spaces and to consequently learn bilingual word correspondences in these spaces. By compiling images related to linguistic concepts given in different languages, the potentially prohibitive data requirements and language pair-dependence from prior work is remove</context>
</contexts>
<marker>Liu, Duh, Matsumoto, 2013</marker>
<rawString>Xiaodong Liu, Kevin Duh, and Yuji Matsumoto. 2013. Topic models + word alignment = A flexible framework for extracting bilingual dictionary from comparable corpus. In CoNLL, pages 212–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Max M Louwerse</author>
</authors>
<title>Symbol interdependency in symbolic and embodied cognition.</title>
<date>2008</date>
<journal>Topics in Cognitive Science,</journal>
<volume>59</volume>
<issue>1</issue>
<contexts>
<context position="8963" citStr="Louwerse, 2008" startWordPosition="1380" endWordPosition="1382">ferring CNN features is not new (Driancourt and Bottou, 1990), the simultaneous availability of 149 Figure 1: Illustration of calculating similarity between images from different languages. massive amounts of data and cheap GPUs has led to considerable advances in computer vision, similar in scale to those witnessed with SIFT and HOG descriptors a decade ago (Razavian et al., 2014). 2.3 Multi-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptor</context>
</contexts>
<marker>Louwerse, 2008</marker>
<rawString>Max M. Louwerse. 2008. Symbol interdependency in symbolic and embodied cognition. Topics in Cognitive Science, 59(1):617–645.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Lowe</author>
</authors>
<title>Distinctive image features from scale-invariant keypoints.</title>
<date>2004</date>
<journal>International Journal of Computer Vision,</journal>
<volume>60</volume>
<issue>2</issue>
<contexts>
<context position="2240" citStr="Lowe, 2004" startWordPosition="333" endWordPosition="334">o small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and extracting lexical items from that space. Bergsma and Van Durme (2011) showed that this bilingual space need not be linguistic in nature: they used labeled images from the Web to obtain bilingual lexical translation pairs based on the visual features of corresponding images. Local features are computed using SIFT (Lowe, 2004) and color histograms (Deselaers et al., 2008) and aggregated as bags of visual words (BOVW) (Sivic and Zisserman, 2003) to get bilingual representations in a shared visual space. Their highest performance is obtained by combining these visual features with normalized edit distance, an orthographic similarity metric (Navarro, 2001). There are several advantages to having a visual rather than a linguistic intermediate bilingual space: First, while labeled images are readily available for many languages through resources such as Google Images, language pairs that have sizeable comparable, let al</context>
<context position="9591" citStr="Lowe, 2004" startWordPosition="1481" endWordPosition="1482">s substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al.,</context>
</contexts>
<marker>Lowe, 2004</marker>
<rawString>David G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60(2):91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Mimno</author>
<author>Hanna M Wallach</author>
<author>Jason Naradowsky</author>
<author>David A Smith</author>
<author>Andrew McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>880--889</pages>
<contexts>
<context position="23627" citStr="Mimno et al., 2009" startWordPosition="3724" endWordPosition="3727">n induction models to learn translations from comparable data (see sect. 3.3). We do not necessarily expect visual methods to outperform linguistic ones, but it is instructive to see the comparison. We compare our visual models against the current state-of-the-art lexicon induction model using comparable data (Vuli´c and Moens, 2013b). This model induces translations from comparable Wikipedia data in two steps: (1) It learns a set of highly reliable one-to-one translation pairs using a shared bilingual space obtained by applying the multilingual probabilistic topic modeling (MuPTM) framework (Mimno et al., 2009). (2) These highly reliable one-to-one translation pairs serve as dimensions of a word-based bilingual semantic space (Gaussier et al., 2004; Tamura et al., 2012). The model then bootstraps from the high-precision seed lexicon of translations and learns new dimensions of the bilingual space until convergence. This model, which we call BOOTSTRAP, obtains the current best results on the evaluation dataset. For more details about the bootstrapping model and its comparison against other approaches, we refer to Vuli´c and Moens (2013b). Table 4 shows the results for the language pairs in the VULIC1</context>
</contexts>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>David M. Mimno, Hanna M. Wallach, Jason Naradowsky, David A. Smith, and Andrew McCallum. 2009. Polylingual topic models. In EMNLP, pages 880–889.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vinod Nair</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Rectified linear units improve restricted boltzmann machines.</title>
<date>2010</date>
<booktitle>In ICML,</booktitle>
<pages>807--814</pages>
<contexts>
<context position="8092" citStr="Nair and Hinton, 2010" startWordPosition="1245" endWordPosition="1248">ndences in these spaces. By compiling images related to linguistic concepts given in different languages, the potentially prohibitive data requirements and language pair-dependence from prior work is removed. 2.2 Deep Convolutional Neural Networks Deep convolutional neural networks (CNNs) have become extremely popular in the computer vision community. These networks currently provide state-of-the-art performance for a variety of key computer vision tasks such as object recognition (Razavian et al., 2014). They tend to be relatively deep, consisting of a number of rectified linear unit layers (Nair and Hinton, 2010) and a series of convolutional layers (Krizhevsky et al., 2012). Recently, such layers have been used in transfer learning techniques, where they are used as mid-level features in other computer vision tasks (Oquab et al., 2014). Although the idea of transferring CNN features is not new (Driancourt and Bottou, 1990), the simultaneous availability of 149 Figure 1: Illustration of calculating similarity between images from different languages. massive amounts of data and cheap GPUs has led to considerable advances in computer vision, similar in scale to those witnessed with SIFT and HOG descript</context>
</contexts>
<marker>Nair, Hinton, 2010</marker>
<rawString>Vinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve restricted boltzmann machines. In ICML, pages 807–814.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gonzalo Navarro</author>
</authors>
<title>A guided tour to approximate string matching.</title>
<date>2001</date>
<journal>ACM Computing Surveys,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="2573" citStr="Navarro, 2001" startWordPosition="384" endWordPosition="385">sma and Van Durme (2011) showed that this bilingual space need not be linguistic in nature: they used labeled images from the Web to obtain bilingual lexical translation pairs based on the visual features of corresponding images. Local features are computed using SIFT (Lowe, 2004) and color histograms (Deselaers et al., 2008) and aggregated as bags of visual words (BOVW) (Sivic and Zisserman, 2003) to get bilingual representations in a shared visual space. Their highest performance is obtained by combining these visual features with normalized edit distance, an orthographic similarity metric (Navarro, 2001). There are several advantages to having a visual rather than a linguistic intermediate bilingual space: First, while labeled images are readily available for many languages through resources such as Google Images, language pairs that have sizeable comparable, let alone parallel, corpora are relatively scarce. Second, it has been found that meaning is often grounded in the perceptual system, and that the quality of semantic representations improves significantly when they are grounded in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space me</context>
</contexts>
<marker>Navarro, 2001</marker>
<rawString>Gonzalo Navarro. 2001. A guided tour to approximate string matching. ACM Computing Surveys, 33(1):31–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="1474" citStr="Och and Ney, 2003" startWordPosition="208" endWordPosition="211">dataset. Furthermore, we shed new light on the type of visual similarity metric to use for genuine similarity versus relatedness tasks, and experiment with using multiple layers from the same network in an attempt to improve performance. 1 Introduction Bilingual lexicon induction is the task of finding words that share a common meaning across different languages. It plays an important role in a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). Although parallel corpora have been used successfully for inducing bilingual lexicons for some languages (Och and Ney, 2003), these corpora are either too small or unavailable for many language pairs. Consequently, mono-lingual approaches that rely on comparable instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and extracting lexical items from that space. Bergsma and Van Durme (2011) showed that this bilingual space need not be linguistic in nature: they used labeled image</context>
<context position="6186" citStr="Och and Ney, 2003" startWordPosition="958" endWordPosition="961">dge this is the first work to provide a comparison of visual and state-of-theart linguistic approaches to bilingual lexicon induction. 2 Related Work 2.1 Bilingual Lexicon Learning Bilingual lexicon learning is the task of automatically inducing word translations from raw data, and is an attractive alternative to the timeconsuming and expensive process of manually building high-quality resources for a wide variety of language pairs and domains. Early approaches relied on limited and domain-restricted parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but t</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxime Oquab</author>
<author>L´eon Bottou</author>
<author>Ivan Laptev</author>
<author>Josef Sivic</author>
</authors>
<title>Learning and transferring mid-level image representations using convolutional neural networks.</title>
<date>2014</date>
<booktitle>In CVPR,</booktitle>
<pages>1717--1724</pages>
<contexts>
<context position="8320" citStr="Oquab et al., 2014" startWordPosition="1281" endWordPosition="1284"> Neural Networks Deep convolutional neural networks (CNNs) have become extremely popular in the computer vision community. These networks currently provide state-of-the-art performance for a variety of key computer vision tasks such as object recognition (Razavian et al., 2014). They tend to be relatively deep, consisting of a number of rectified linear unit layers (Nair and Hinton, 2010) and a series of convolutional layers (Krizhevsky et al., 2012). Recently, such layers have been used in transfer learning techniques, where they are used as mid-level features in other computer vision tasks (Oquab et al., 2014). Although the idea of transferring CNN features is not new (Driancourt and Bottou, 1990), the simultaneous availability of 149 Figure 1: Illustration of calculating similarity between images from different languages. massive amounts of data and cheap GPUs has led to considerable advances in computer vision, similar in scale to those witnessed with SIFT and HOG descriptors a decade ago (Razavian et al., 2014). 2.3 Multi-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily</context>
</contexts>
<marker>Oquab, Bottou, Laptev, Sivic, 2014</marker>
<rawString>Maxime Oquab, L´eon Bottou, Ivan Laptev, and Josef Sivic. 2014. Learning and transferring mid-level image representations using convolutional neural networks. In CVPR, pages 1717–1724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated English and German corpora.</title>
<date>1999</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="6381" citStr="Rapp, 1999" startWordPosition="994" endWordPosition="995">ning is the task of automatically inducing word translations from raw data, and is an attractive alternative to the timeconsuming and expensive process of manually building high-quality resources for a wide variety of language pairs and domains. Early approaches relied on limited and domain-restricted parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual s</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated English and German corpora. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Sharif Razavian</author>
<author>Hossein Azizpour</author>
<author>Josephine Sullivan</author>
<author>Stefan Carlsson</author>
</authors>
<title>CNN features offthe-shelf: an astounding baseline for recognition.</title>
<date>2014</date>
<location>CoRR, abs/1403.6382.</location>
<contexts>
<context position="7979" citStr="Razavian et al., 2014" startWordPosition="1225" endWordPosition="1228">ability of purely visual data to induce shared bilingual spaces and to consequently learn bilingual word correspondences in these spaces. By compiling images related to linguistic concepts given in different languages, the potentially prohibitive data requirements and language pair-dependence from prior work is removed. 2.2 Deep Convolutional Neural Networks Deep convolutional neural networks (CNNs) have become extremely popular in the computer vision community. These networks currently provide state-of-the-art performance for a variety of key computer vision tasks such as object recognition (Razavian et al., 2014). They tend to be relatively deep, consisting of a number of rectified linear unit layers (Nair and Hinton, 2010) and a series of convolutional layers (Krizhevsky et al., 2012). Recently, such layers have been used in transfer learning techniques, where they are used as mid-level features in other computer vision tasks (Oquab et al., 2014). Although the idea of transferring CNN features is not new (Driancourt and Bottou, 1990), the simultaneous availability of 149 Figure 1: Illustration of calculating similarity between images from different languages. massive amounts of data and cheap GPUs ha</context>
</contexts>
<marker>Razavian, Azizpour, Sullivan, Carlsson, 2014</marker>
<rawString>Ali Sharif Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. 2014. CNN features offthe-shelf: an astounding baseline for recognition. CoRR, abs/1403.6382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Roller</author>
<author>Sabine Schulte im Walde</author>
</authors>
<title>A multimodal LDA model integrating textual, cognitive and visual modalities.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1146--1157</pages>
<marker>Roller, Walde, 2013</marker>
<rawString>Stephen Roller and Sabine Schulte im Walde. 2013. A multimodal LDA model integrating textual, cognitive and visual modalities. In EMNLP, pages 1146– 1157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Schafer</author>
<author>David Yarowsky</author>
</authors>
<title>Inducing translation lexicons via diverse similarity measures and bridge languages. In CoNLL,</title>
<date>2002</date>
<pages>1--7</pages>
<contexts>
<context position="3756" citStr="Schafer and Yarowsky, 2002" startWordPosition="568" endWordPosition="571">014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequency distributions (Schafer and Yarowsky, 2002) that will work only for 148 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 148–158, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. closely related languages, a visual space can work for any language, whether it’s English or Chinese, Arabic or Icelandic, or all Greek to you. It has recently been shown, however, that much better performance can be achieved on semantic similarity and relatedness tasks by using visual representations from deep convolutional neural networks (CNNs) instead of BOVW features (Ki</context>
</contexts>
<marker>Schafer, Yarowsky, 2002</marker>
<rawString>Charles Schafer and David Yarowsky. 2002. Inducing translation lexicons via diverse similarity measures and bridge languages. In CoNLL, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yelong Shen</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
<author>Gr´egoire Mesnil</author>
</authors>
<title>Learning semantic representations using convolutional neural networks for Web search. In</title>
<date>2014</date>
<booktitle>WWW,</booktitle>
<pages>373--374</pages>
<contexts>
<context position="9876" citStr="Shen et al., 2014" startWordPosition="1527" endWordPosition="1530">erer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3 A Purely Visual Approach to Bilingual Lexicon Learning We assume that the best translation, or matching lexical item, of a word ws (in the source language) is the word wt (in the target language) that is the nearest cross-lingual neighbour to ws in</context>
</contexts>
<marker>Shen, He, Gao, Deng, Mesnil, 2014</marker>
<rawString>Yelong Shen, Xiaodong He, Jianfeng Gao, Li Deng, and Gr´egoire Mesnil. 2014. Learning semantic representations using convolutional neural networks for Web search. In WWW, pages 373–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphna Shezaf</author>
<author>Ari Rappoport</author>
</authors>
<title>Bilingual lexicon generation using non-aligned signatures.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>98--107</pages>
<contexts>
<context position="6432" citStr="Shezaf and Rappoport, 2010" startWordPosition="1000" endWordPosition="1003">nducing word translations from raw data, and is an attractive alternative to the timeconsuming and expensive process of manually building high-quality resources for a wide variety of language pairs and domains. Early approaches relied on limited and domain-restricted parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic m</context>
</contexts>
<marker>Shezaf, Rappoport, 2010</marker>
<rawString>Daphna Shezaf and Ari Rappoport. 2010. Bilingual lexicon generation using non-aligned signatures. In ACL, pages 98–107.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Grounded models of semantic representation.</title>
<date>2012</date>
<booktitle>In EMNLP,</booktitle>
<pages>1423--1433</pages>
<contexts>
<context position="3112" citStr="Silberer and Lapata, 2012" startWordPosition="464" endWordPosition="468">tures with normalized edit distance, an orthographic similarity metric (Navarro, 2001). There are several advantages to having a visual rather than a linguistic intermediate bilingual space: First, while labeled images are readily available for many languages through resources such as Google Images, language pairs that have sizeable comparable, let alone parallel, corpora are relatively scarce. Second, it has been found that meaning is often grounded in the perceptual system, and that the quality of semantic representations improves significantly when they are grounded in the visual modality (Silberer and Lapata, 2012; Bruni et al., 2014). Having an intermediate visual space means that words in different languages can be grounded in the same space. Third, it is natural to use vision as an intermediate: when we communicate with someone who does not speak our language, we often communicate by directly referring to our surroundings. Languages that are linguistically far apart will, by cognitive necessity, still refer to objects in the same visual space. While some approaches to bilingual lexicon induction rely on orthographic properties (Haghighi et al., 2008; Koehn and Knight, 2002) or properties of frequenc</context>
<context position="9279" citStr="Silberer and Lapata, 2012" startWordPosition="1426" endWordPosition="1429">e witnessed with SIFT and HOG descriptors a decade ago (Razavian et al., 2014). 2.3 Multi-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). D</context>
</contexts>
<marker>Silberer, Lapata, 2012</marker>
<rawString>Carina Silberer and Mirella Lapata. 2012. Grounded models of semantic representation. In EMNLP, pages 1423–1433.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Mirella Lapata</author>
</authors>
<title>Learning grounded meaning representations with autoencoders.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>721--732</pages>
<contexts>
<context position="10224" citStr="Silberer and Lapata, 2014" startWordPosition="1580" endWordPosition="1583">(Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3 A Purely Visual Approach to Bilingual Lexicon Learning We assume that the best translation, or matching lexical item, of a word ws (in the source language) is the word wt (in the target language) that is the nearest cross-lingual neighbour to ws in the bilingual visual space. Hence a similarity (or distance) score between lexical items from different languages is required. In this section, we describe: one, how to build image representations from sets of images associated with each lexical item, i.e. how to induce a shared bilingual visual space in which all lexical items are represented; </context>
</contexts>
<marker>Silberer, Lapata, 2014</marker>
<rawString>Carina Silberer and Mirella Lapata. 2014. Learning grounded meaning representations with autoencoders. In ACL, pages 721–732.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carina Silberer</author>
<author>Vittorio Ferrari</author>
<author>Mirella Lapata</author>
</authors>
<title>Models of semantic representation with visual attributes.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>572--582</pages>
<contexts>
<context position="9302" citStr="Silberer et al., 2013" startWordPosition="1430" endWordPosition="1434">OG descriptors a decade ago (Razavian et al., 2014). 2.3 Multi-Modal Semantics Multi-modal semantics is motivated by parallels with human concept acquisition. It has been found that semantic knowledge, from a very early age, relies heavily on perceptual information (Louwerse, 2008), and there exists substantial evidence that many concepts are grounded in the perceptual system (Barsalou, 2008). One way to accomplish such grounding is by combining linguistic representations with information from a perceptual modality, obtained from, e.g., property norming experiments (Silberer and Lapata, 2012; Silberer et al., 2013; Roller and Schulte im Walde, 2013; Hill and Korhonen, 2014) or extracting features from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques</context>
</contexts>
<marker>Silberer, Ferrari, Lapata, 2013</marker>
<rawString>Carina Silberer, Vittorio Ferrari, and Mirella Lapata. 2013. Models of semantic representation with visual attributes. In ACL, pages 572–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josef Sivic</author>
<author>Andrew Zisserman</author>
</authors>
<title>Video google: A text retrieval approach to object matching in videos.</title>
<date>2003</date>
<booktitle>In ICCV,</booktitle>
<pages>1470--1477</pages>
<contexts>
<context position="2360" citStr="Sivic and Zisserman, 2003" startWordPosition="351" endWordPosition="355">able instead of parallel corpora have been developed (Fung and Yee, 1998; Koehn and Knight, 2002). These approaches work by mapping language pairs to a shared bilingual space and extracting lexical items from that space. Bergsma and Van Durme (2011) showed that this bilingual space need not be linguistic in nature: they used labeled images from the Web to obtain bilingual lexical translation pairs based on the visual features of corresponding images. Local features are computed using SIFT (Lowe, 2004) and color histograms (Deselaers et al., 2008) and aggregated as bags of visual words (BOVW) (Sivic and Zisserman, 2003) to get bilingual representations in a shared visual space. Their highest performance is obtained by combining these visual features with normalized edit distance, an orthographic similarity metric (Navarro, 2001). There are several advantages to having a visual rather than a linguistic intermediate bilingual space: First, while labeled images are readily available for many languages through resources such as Google Images, language pairs that have sizeable comparable, let alone parallel, corpora are relatively scarce. Second, it has been found that meaning is often grounded in the perceptual </context>
</contexts>
<marker>Sivic, Zisserman, 2003</marker>
<rawString>Josef Sivic and Andrew Zisserman. 2003. Video google: A text retrieval approach to object matching in videos. In ICCV, pages 1470–1477.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Andrej Karpathy</author>
<author>Quoc V Le</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences.</title>
<date>2014</date>
<journal>Transactions of ACL,</journal>
<pages>2--207</pages>
<contexts>
<context position="10001" citStr="Socher et al., 2014" startWordPosition="1547" endWordPosition="1550">ures from raw image data (Feng and Lapata, 2010; Leong and Mihalcea, 2011; Bruni et al., 2014; Kiela et al., 2014). Such multi-modal visual approaches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3 A Purely Visual Approach to Bilingual Lexicon Learning We assume that the best translation, or matching lexical item, of a word ws (in the source language) is the word wt (in the target language) that is the nearest cross-lingual neighbour to ws in the bilingual visual space. Hence a similarity (or distance) score between lexical items from different languages is require</context>
</contexts>
<marker>Socher, Karpathy, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of ACL, 2:207–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Ruslan Salakhutdinov</author>
</authors>
<title>Multimodal learning with deep Boltzmann machines.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="10162" citStr="Srivastava and Salakhutdinov, 2014" startWordPosition="1570" endWordPosition="1573">aches often rely on local descriptors, such as SIFT (Lowe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3 A Purely Visual Approach to Bilingual Lexicon Learning We assume that the best translation, or matching lexical item, of a word ws (in the source language) is the word wt (in the target language) that is the nearest cross-lingual neighbour to ws in the bilingual visual space. Hence a similarity (or distance) score between lexical items from different languages is required. In this section, we describe: one, how to build image representations from sets of images associated with each lexical item, i.e. how to induce a shared bilin</context>
</contexts>
<marker>Srivastava, Salakhutdinov, 2014</marker>
<rawString>Nitish Srivastava and Ruslan Salakhutdinov. 2014. Multimodal learning with deep Boltzmann machines. Journal of Machine Learning Research, 15(1):2949–2980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akihiro Tamura</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Bilingual lexicon extraction from comparable corpora using label propagation.</title>
<date>2012</date>
<booktitle>In EMNLP,</booktitle>
<pages>24--36</pages>
<contexts>
<context position="6454" citStr="Tamura et al., 2012" startWordPosition="1004" endWordPosition="1007">om raw data, and is an attractive alternative to the timeconsuming and expensive process of manually building high-quality resources for a wide variety of language pairs and domains. Early approaches relied on limited and domain-restricted parallel data, and the induced lexicons were typically a by-product of word alignment models (Och and Ney, 2003). To alleviate the issue of low coverage, a large body of work has been dedicated to lexicon learning from more abundant and less restricted comparable data, e.g., (Fung and Yee, 1998; Rapp, 1999; Gaussier et al., 2004; Shezaf and Rappoport, 2010; Tamura et al., 2012). However, these models typically rely on the availability of bilingual seed lexicons to produce shared bilingual spaces, as well as large repositories of comparable data. Therefore, several approaches attempt to learn lexicons from large monolingual data sets in two languages (Koehn and Knight, 2002; Haghighi et al., 2008), but their performance again relies on language pair-dependent clues such as orthographic similarity. A further approach removed the requirement of seed lexicons, and induced lexicons using bilingual spaces spanned by multilingual probabilistic topic models (Vuli´c et al., </context>
<context position="17537" citStr="Tamura et al., 2012" startWordPosition="2753" endWordPosition="2756">4 64.9 74.8 0.608 Table 2: Performance on BERGSMA500 compared to Bergsma and Van Durme (B&amp;VD). some insight into how purely visual models for bilingual lexicon induction behave with respect to both abstract and concrete concepts. Evaluation Metrics. We measure performance in a standard way using mean-reciprocal rank: 1 M 1 MRR = M Z=1 rank(ws, wt) (1) where rank(ws, wt) denotes the rank of the correct translation wt (as provided in the gold standard) in the ranked list of translation candidates for ws, and M is the number of test cases. We also use precision at N (P@N) (Gaussier et al., 2004; Tamura et al., 2012; Vuli´c and Moens, 2013a), which measures the proportion of test instances where the correct translation is within the top N highest ranked translations. 4 Results We evaluate the four similarity metrics on the BERGSMA500 dataset and compare the results to the systems of Bergsma and Van Durme, who report results for the AVGMAX function, having concluded that it performs better than MAXMAX on English-Spanish translations. We report their best-performing visual-only system, which combines SIFT-based descriptors with color histograms, as well as their best-performing overall system, which combin</context>
<context position="23789" citStr="Tamura et al., 2012" startWordPosition="3749" endWordPosition="3752">is instructive to see the comparison. We compare our visual models against the current state-of-the-art lexicon induction model using comparable data (Vuli´c and Moens, 2013b). This model induces translations from comparable Wikipedia data in two steps: (1) It learns a set of highly reliable one-to-one translation pairs using a shared bilingual space obtained by applying the multilingual probabilistic topic modeling (MuPTM) framework (Mimno et al., 2009). (2) These highly reliable one-to-one translation pairs serve as dimensions of a word-based bilingual semantic space (Gaussier et al., 2004; Tamura et al., 2012). The model then bootstraps from the high-precision seed lexicon of translations and learns new dimensions of the bilingual space until convergence. This model, which we call BOOTSTRAP, obtains the current best results on the evaluation dataset. For more details about the bootstrapping model and its comparison against other approaches, we refer to Vuli´c and Moens (2013b). Table 4 shows the results for the language pairs in the VULIC1000 dataset. Of the four similarity metrics, CNN-AVGMAX again performs best, as it did for BERGSMA500. The linguistic BOOTSTRAP method outperforms our visual appr</context>
</contexts>
<marker>Tamura, Watanabe, Sumita, 2012</marker>
<rawString>Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita. 2012. Bilingual lexicon extraction from comparable corpora using label propagation. In EMNLP, pages 24–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In CHI,</booktitle>
<pages>319--326</pages>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In CHI, pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Crosslingual semantic similarity of words as the similarity of their semantic word responses.</title>
<date>2013</date>
<booktitle>In NAACL,</booktitle>
<pages>106--116</pages>
<marker>Vuli´c, Moens, 2013</marker>
<rawString>Ivan Vuli´c and Marie-Francine Moens. 2013a. Crosslingual semantic similarity of words as the similarity of their semantic word responses. In NAACL, pages 106–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Marie-Francine Moens</author>
</authors>
<title>A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else).</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1613--1624</pages>
<marker>Vuli´c, Moens, 2013</marker>
<rawString>Ivan Vuli´c and Marie-Francine Moens. 2013b. A study on bootstrapping bilingual vector spaces from nonparallel data (and nothing else). In EMNLP, pages 1613–1624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Vuli´c</author>
<author>Wim De Smet</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Identifying word translations from comparable corpora using latent topic models.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>479--484</pages>
<marker>Vuli´c, De Smet, Moens, 2011</marker>
<rawString>Ivan Vuli´c, Wim De Smet, and Marie-Francine Moens. 2011. Identifying word translations from comparable corpora using latent topic models. In ACL, pages 479–484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pengcheng Wu</author>
<author>Steven C H Hoi</author>
<author>Hao Xia</author>
<author>Peilin Zhao</author>
<author>Dayong Wang</author>
<author>Chunyan Miao</author>
</authors>
<title>Online multimodal deep similarity learning with application to image retrieval.</title>
<date>2013</date>
<booktitle>In ACM Multimedia,</booktitle>
<pages>153--162</pages>
<contexts>
<context position="10196" citStr="Wu et al., 2013" startWordPosition="1576" endWordPosition="1579">owe, 2004), SURF (Bay et al., 2008), or HOG (Dalal and Triggs, 2005), as well as pyramidal variants of these descriptors such as PHOW (Bosch et al., 2007). However, deep CNN features have recently been successfully transferred to multi-modal semantics (Kiela and Bottou, 2014; Shen et al., 2014). Deep learning techniques have also been successfully employed in cross-modal tasks (Frome et al., 2013; Socher et al., 2014; Lazaridou et al., 2014; Kiros et al., 2014). Other examples of multi-modal deep learning use restricted Boltzmann machines (Srivastava and Salakhutdinov, 2014) or auto-encoders (Wu et al., 2013; Silberer and Lapata, 2014). 3 A Purely Visual Approach to Bilingual Lexicon Learning We assume that the best translation, or matching lexical item, of a word ws (in the source language) is the word wt (in the target language) that is the nearest cross-lingual neighbour to ws in the bilingual visual space. Hence a similarity (or distance) score between lexical items from different languages is required. In this section, we describe: one, how to build image representations from sets of images associated with each lexical item, i.e. how to induce a shared bilingual visual space in which all lex</context>
</contexts>
<marker>Wu, Hoi, Xia, Zhao, Wang, Miao, 2013</marker>
<rawString>Pengcheng Wu, Steven C. H. Hoi, Hao Xia, Peilin Zhao, Dayong Wang, and Chunyan Miao. 2013. Online multimodal deep similarity learning with application to image retrieval. In ACM Multimedia, pages 153–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Yosinski</author>
<author>Jeff Clune</author>
<author>Yoshua Bengio</author>
<author>Hod Lipson</author>
</authors>
<title>How transferable are features in deep neural networks?</title>
<date>2014</date>
<booktitle>In NIPS,</booktitle>
<pages>3320--3328</pages>
<contexts>
<context position="26063" citStr="Yosinski et al., 2014" startWordPosition="4114" endWordPosition="4117">er quality text data available —which one might reasonably expect to be the default scenario. 4.3 Adding CNN Layers The AlexNet (Krizhevsky et al., 2012) from which our image representations are extracted contains a number of layers. Kiela and Bottou (2014) only use the fully connected pre-softmax layer (which we call FC7) for their image representations. It has been found, however, that other layers in the network, especially the preceding fully connected (FC6) and fifth convolutional max pooling (POOL5) layers, also have good properties for usage in transfer learning (Girshick et al., 2014; Yosinski et al., 2014). Hence we performed a (very) preliminary investigation of whether performance increases with the use of additional layers. In light of our findings concerning the difference between genuine similarity and relatedness, this also gives rise to the question of whether the additional layers might be useful for similarity or relatedness, or both. We hypothesize that the nature of the task matters here: if we are only concerned with genuine similarity, layer FC7 is likely to contain all the necessary information to judge whether two images are similar or not, since 154 Dataset Language Image disper</context>
</contexts>
<marker>Yosinski, Clune, Bengio, Lipson, 2014</marker>
<rawString>Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In NIPS, pages 3320–3328.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
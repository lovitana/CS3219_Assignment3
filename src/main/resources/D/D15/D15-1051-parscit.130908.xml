<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.965224">
Spelling Correction of User Search Queries through Statistical Machine
Translation
</title>
<author confidence="0.886931">
Saša Hasan∗ Carmen Heger† Saab Mansour
</author>
<email confidence="0.73744">
sasa.hasan@gmail.com heger.carmen@gmail.com saamansour@ebay.com
</email>
<author confidence="0.310205">
eBay Inc.
</author>
<affiliation confidence="0.275468">
2065 Hamilton Ave
</affiliation>
<address confidence="0.90435">
San Jose, CA 95125, USA
</address>
<sectionHeader confidence="0.970995" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933388888889">
We use character-based statistical machine
translation in order to correct user search
queries in the e-commerce domain. The
training data is automatically extracted
from event logs where users re-issue their
search queries with potentially corrected
spelling within the same session. We show
results on a test set which was annotated
by humans and compare against online
autocorrection capabilities of three addi-
tional web sites. Overall, the methods
presented in this paper outperform fully
productized spellchecking and autocorrec-
tion services in terms of accuracy and F1
score. We also propose novel evaluation
steps based on retrieved search results of
the corrected queries in terms of quantity
and relevance.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997995836734694">
Spelling correction is an important feature for any
interactive service that takes input generated by
users, e.g. an e-commerce web site that allows
searching for goods and products. Misspellings
are very common with user-generated input and
the reason why many web sites offer spelling cor-
rection in the form of “Did you mean?” sug-
gestions or automatic corrections. Autocorrec-
tion increases user satisfaction by correcting ob-
vious errors, whereas suggestions make it con-
venient for users to accept a proposed correction
without retyping or correcting the query manually.
Spelling correction is not a trivial task, as search
∗ The author is now affiliated with Lilt Inc., Stanford,
CA, USA.
† The author is now affiliated with Stylight GmbH, Mu-
nich, Germany.
queries are often short and lack context. Mis-
spelled queries might be considered correct by a
statistical spelling correction system as there is ev-
idence in the data through frequent occurrences.
While common successful methods (cf. Sec-
tion 1.1) rely on either human-annotated data or
the entire web, we wanted to use easily accessi-
ble in-domain data and on top of that technology
that is already available. In this work, we use user
event logs from an e-commerce web site to fetch
similar search query pairs within an active session.
The main idea is that users issue a search query but
alter it into something similar within a given time
window which might be the correction of a poten-
tial typo. To the best of our knowledge, the idea
of collecting query corrections using user session
and time information is novel. Previous work sug-
gested collecting queries using information that a
user clicked on a proposed correction. Our pro-
posed method for collecting training data has sev-
eral advantages. First, we do not rely on a pre-
vious spelling correction system, but on user for-
mulations. Second, for many search queries, es-
pecially from the tail where search recall is gen-
erally low, these misspellings yield few results,
and thus, users looking for certain products are in-
clined to correct the query themselves in order to
find what they are looking for. We use Damerau-
Levenshtein distance (Damerau, 1964) on charac-
ter level as similarity criterion, i.e. queries within a
specific edit distance are considered to be related.
The steps proposed in this work are:
</bodyText>
<listItem confidence="0.9989576">
1. Extraction of similar user queries from search
logs for bootstrapping training data (Sec-
tion 2),
2. classification and filtering of data to remove
noisy entries (Section 3), and
</listItem>
<page confidence="0.982152">
451
</page>
<note confidence="0.9855215">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 451–460,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.982383947368421">
3. spelling correction cast into a statistical ma-
chine translation framework based on charac-
ter bigram sequences (Section 4).
We also evaluate the work thoroughly in Sec-
tion 5 where we compare our method to three other
online sites, two of them from the e-commerce
domain, and present a novel approach that deter-
mines quality based on retrieved search results.
We show examples indicating that our method
can handle both corrections of misspelled queries
and queries with segmentation issues (i.e. missing
whitespace delimiters). A summary can be found
in Section 6.
To our knowledge, this is the first work that
uses character-based machine translation technol-
ogy on user-generated data for spelling correction.
Moreover, it is the first to evaluate the performance
in an e-commerce setting with there relevant mea-
sures.
</bodyText>
<sectionHeader confidence="0.836881" genericHeader="related work">
1.1 Related Work
</sectionHeader>
<bodyText confidence="0.999987259259259">
One of the more prominent papers on autocorrec-
tion of misspelled input is (Whitelaw et al., 2009).
The three-step approach incorporates a classifica-
tion step that determines whether a word is mis-
spelled, computes the most likely correction can-
didate and then, again, classifies whether this can-
didate is likely to be correct. An error model based
after (Brill and Moore, 2000) is trained on sim-
ilar word pairs extracted from large amounts of
web sites, and a language model is used to dis-
ambiguate correction candidates based on left and
right context around the current position. Our
method differs in several ways. First, we consider
full query pairs as training data, and do not use
single words as primary mode of operation. Sec-
ond, we do not train explicit error models P(wIs)
for words w and observed corrections s, but use
standard phrase-based machine translation model-
ing to derive phrase and lexical translation mod-
els. Although our level of context is shorter, es-
pecially for long words, the system automatically
uses cross-word level context.
The idea of using consecutive user queries from
a stream of events to improve ranking of web
search results was described in (Radlinski and
Joachims, 2005). The authors introduce the no-
tion of query chains that take advantage of users
reformulating their queries as a means to learn bet-
ter ranking functions. The classification of query
chains is performed by support vector machines,
and its training data is generated in a supervised
fashion by manual inspection and annotation. In
contrast, we do not manually annotate any of our
training data. Since our initial sets are quite noisy,
we apply a couple of heuristics that try to produce
a cleaner subset of the data that contains mostly
misspelled queries and their potential correction
candidates.
Another focus of researchers was specifically
to tackle misspelled web search queries and use
search engine logs for training and evaluation data
(Gao et al., 2010), which differs from our work
by collecting data using “click-through” enforce-
ment. The user is presented with a spelling cor-
rection, and if she clicks on it, they learn that the
correction is valid. Our method does not need a
previous spelling correction to work. In addition,
our proposed method has the potential to learn cor-
rections of new and rare terms that will not be pro-
duced by an automatic spelling correction.
In (Zhang et al., 2006), the authors use a
conventional spellchecker to correct web queries
through additional reranking of its output by a
ranking SVM. The training data is in part auto-
matically extracted, but also contains manually an-
notated pairs and, thus, is a semi-supervised ap-
proach. In this paper, we use an unsupervised ap-
proach to generate training data. Query spelling
correction that is based on click-through data and
uses a phrase-based error model is reported in
(Sun et al., 2010). Our models operate on char-
acter sequences instead of words, and we do not
observe issues with identity transformations (i.e.
non-corrections for correctly spelled input).
In (Cucerzan and Brill, 2004), the authors inves-
tigate a transformation method that corrects un-
likely queries into more likely variants based on
web query logs. The iterative approach transforms
a search query based on word uni- and bigram
decompositions, and the authors evaluate on both
a large set that contains around 17% misspelled
queries and a smaller set that is based on succes-
sive user-reformulated similar queries, a similar
setup that we use to extract our training data. They
stress the importance of a good language model,
as performance drops drastically going from a bi-
gram to a unigram LM.
The use of character-based models in combi-
nation with statistical machine translation is not
novel and was proposed for spelling correction,
e.g., in (Formiga and Fonollosa, 2012), (Liu et al.,
</bodyText>
<page confidence="0.997897">
452
</page>
<bodyText confidence="0.999772052631579">
2013) or (Chiu et al., 2013). The authors compare
a distance-based approach including a language
model, a confusion network-based approach, a
translation approach through a heuristically de-
fined phrase table coding all character transfor-
mations, and a character-based machine transla-
tion approach using standard procedures (auto-
matic word alignment and phrase extraction). The
training data is manually created in contrast to
our work where we automatically bootstrap train-
ing data from query logs. Research has also been
done for translation of closely related languages
(e.g. (Vilar et al., 2007) and (Nakov and Tiede-
mann, 2012)) and transliteration (e.g. (Deselaers et
al., 2009)). An early summary paper with various
spelling-related problems (non-word error detec-
tion, isolated-word error correction, and context-
dependent word correction) can be found in (Ku-
kich, 1992).
</bodyText>
<sectionHeader confidence="0.863605" genericHeader="method">
2 Extraction of training data
</sectionHeader>
<bodyText confidence="0.999519034482758">
We use event logs that track user interactions on
an e-commerce web site. Each action of a user
visiting the site is stored in a data warehouse and
HDFS (Hadoop Distributed File System). We
store several billion of these user records on a daily
basis. The setup allows us to efficiently process
large amounts of data points using a Map-Reduce
framework via Hadoop1. As part of user event
tracking, all interactions on the site are stored in
the database, e.g. which search terms were en-
tered, which links were clicked, and what actions
resulted in this (i.e. buying an item, adding it to a
like or watch list, or simply advancing to the next
page of search results, and so on). We focus on
search terms that users enter within a given period
of time.
Our hypothesis is that users that enter consecu-
tive search terms are not satisfied with the results
and try to modify the query until the results are ac-
ceptable. We then analyze the sequence of search
queries as part of each user session. We only ex-
tract consecutive search queries that are similar
in terms of character-level Damerau-Levenshtein
edit distance which is the minimum number of
character insertions, deletions, substitutions and
transpositions (i.e. the swapping of two charac-
ters). Note that spelling correction in this kind of
environment also needs to address segmentation of
queries in case of missing whitespace. It is quite
</bodyText>
<footnote confidence="0.979673">
1http://hadoop.apache.org
</footnote>
<bodyText confidence="0.999893411764706">
common to find query terms being concatenated
without the use of a space character, e.g. calvin-
klein, ipadair or xboxone. Also, search queries are
short in nature and often lack context, and particu-
larly for the e-commerce domain largely consist of
brand and product names and associated attributes
(e.g. size of clothing).
Our method extracts similar search query pairs
that will be used in our statistical machine trans-
lation setup as training data. Table 1 shows ex-
amples that we extract from the event logs. We
use edit distance thresholds of 3 and 5 characters,
where the latter is generally noisier. Noise in this
context is everything that is not related to mis-
spelled queries. After a closer look, we observe
that many queries are simply rewrites, i.e., users
either make refinements to their original query
by adding more words to narrow down results,
e.g., leather wallet —* leather wallet men, delet-
ing words to decrease specificity, e.g., gucci belt
—* gucci, or simply going through various related
products or attributes, e.g., iphone 5 —* iphone 5s
—* iphone 6 where they progress through different
products or nike air 9 —* nike air 9.5 —* nike air
10 where they iterate through different sizes.
The logs on HDFS are organized as sessions
where each session contains a stream of user
events up to a specific time of inactivity. We use
event timestamps to determine how long the users
need between consecutive queries, and discard
similar query pairs if they are above a threshold
of 20 seconds. We use Hadoop-based mapper and
reducer steps for the data extraction procedure. In
the mappers, pairs of similar user search queries
get emitted as per above edit distance criterion on
character level, whereas the reducer simply accu-
mulates all counts for identical search query pairs.
Due to the size of the data, we run the Hadoop
extraction jobs on 24-hour log portions, thus ob-
taining separate data sets for each day. Overall,
we can extract several hundred thousand to sev-
eral million similar query pairs on a daily basis for
edit distance thresholds of 3 and 5, respectively.
We pull several months of data from the Hadoop
logs and accumulate each daily pull with unique
entries for training our spelling correction system.
As mentioned above, search queries that are simi-
lar but where the original query is not misspelled
make up a big portion of the extracted data. The
following section focuses on how to filter the data
to result in containing mostly query pairs that fit
</bodyText>
<page confidence="0.996565">
453
</page>
<bodyText confidence="0.821816625">
Search query Similar consecutive query Edit distance User correction?
nike air hurache nike air huarache 1 Yes
jordan size 9 jordan size 9.5 2 No
galaxy s4 galaxy s5 1 No
pawer cord forplaystation 3 power cord for playstation 3 2 Yes
iphine 6 iphone 6 1 Yes
iphone 6 plus iphone 6 plus case 5 No
micheal korrs wstches michael kors watches 3 Yes
calvin klien men boit calvin klein men boot 2 Yes
boots boots men 4 No
sueter sweater 2 Yes
Table 1: Extracted query pairs found in user event logs. Labels in column 4 indicate whether user-
initiated spelling correction (Yes) has taken place vs. a search reformulation that entails the source query
not being misspelled (No).
our use case, i.e. mappings from misspelled to cor-
rected ones.
</bodyText>
<sectionHeader confidence="0.917498" genericHeader="method">
3 Filtering non-misspelled input
</sectionHeader>
<bodyText confidence="0.999171857142857">
We tested a heuristic approach to filtering search
query pairs: a combination of regular expressions
and thresholded feature scores that detect search
progressions and refinements which should be re-
moved from the training data as they do not repre-
sent valid user corrections.
The manual filtering heuristic calculates a se-
quence of features for each search query pair, and
as soon as a feature fires, the entry is removed.
In the following, we will use the notation (x, y)
for a search query pair that is extracted from the
logs explained by our method in the previous sec-
tion. Query x is a user search query, and query y
is a similar query issued by the user in the same
session within a specific time window. Exam-
ple query pairs are (babydoll, baby dolls), (bike
wherl, bike wheel) or (size 12 yellow dress, size
14 yellow dress). We use the following features as
part of this process:
Regular expressions. We remove search query
pairs (x, y) if y is a rewrite of x using search-
related operators, e.g. y = &amp;quot;x&amp;quot; which adds quo-
tation marks around the query (and, thus, has an
edit distance of 2). Example: (bluetooth speakers,
&amp;quot;bluetooth speakers&amp;quot;).
LM ratio. We use an English language model
trained on publicly available corpora (e.g. Eu-
roparl), frequent queries and web content from the
e-commerce domain to calculate log-likelihoods
for each query and filter entries if the ratio is above
zero, i.e. log p(x/y) = log p(x) − log p(y) &gt; 0.
This step essentially removes query pairs (x, y)
if the log-likelihood of query y is smaller than
x which usually indicates that the correction is
more perplexing than the original query. Exam-
ple: (bluetooth earphones, bluetooth ear hpones)
with a log-likelihood ratio of -7.31 + 16.43 &gt; 0 is
removed as a typo actually appears on the “cor-
rected” side.
Edit operations. We use a simple heuristic that
detects search refinements in terms of word inser-
tions and deletions: a word-level edit distance cri-
terion is used to remove entries where edit opera-
tions indicate insertions or deletions of full words.
We also detect substitutions on number tokens
which are also excluded from training data. Ex-
amples: (polo shirt, polo shirt xl), (nikon d700,
nikon d7100).
Frequent terms. We look at queries (x, y) and
use a vocabulary with relative frequencies based
on the query data y to determine whether sub-
stitutions on word level change a frequent token
into another frequent token. Examples: (snake
bat wooden, snake bat wood), (hd dvds, hd dvd)
where wood/wooden and dvds/dvd are both fre-
quent tokens and, thus, most likely rewrites and
not corrections.
Language ID. The primary search language on
US sites is English, but there is also a non-
negligible mix of other languages, Spanish in par-
ticular. We do not remove all queries that are not
identified as English because language identifica-
tion on (often short) search queries is a non-trivial
</bodyText>
<page confidence="0.9932">
454
</page>
<table confidence="0.989457666666667">
Method #queries #tokens MT Acc
all data 80.5M 235.6M 62.0%
heuristic filter 12.6M 40.1M 65.5%
</table>
<tableCaption confidence="0.722545666666667">
Table 2: Filtering training data with a heuristic set
of features. Accuracies are given on DEV for MT
baselines with differences only in the data setup.
</tableCaption>
<bodyText confidence="0.999657708333333">
task with a high error rate and filtering for only En-
glish would remove a lot of valid entries. We sim-
ply remove query pairs where x is identified as ei-
ther Spanish or Unknown based on Google’s Com-
pact Language Detector2. Example: (accesorios
cuarto, accesorios de cuarto).
These heuristics help us to reduce the training
data size from 80 million noisy search query pairs
with around 235 million tokens to 12.5 million
query pairs with roughly 40 million tokens that are
of higher quality and most likely spelling correc-
tions.
Table 2 shows results of this filtering step in
terms of data sizes and the accuracy on the dev
set (cf. Section 4). We observe that the filter-
ing scheme reduces overall training size by almost
85% and increases accuracy on the development
set by 3.5% absolute. The removal of query pairs
is very aggressive at this point, and overall quality
of the spelling correction framework might benefit
from a more careful selection. We will look into
improved variants of filtering through a maximum
entropy classifier trained on actual search results
in the future.
</bodyText>
<sectionHeader confidence="0.998231" genericHeader="method">
4 Autocorrection framework
</sectionHeader>
<bodyText confidence="0.9999378">
We cast the autocorrection task into character-
based statistical machine translation. For this, we
prepare the data by splitting words into sequences
of lowercased characters and use a special charac-
ter to mark whitespace that indicates word bound-
aries. Once these sequences of characters are cor-
rected, i.e. translated, they are merged back to the
full word forms. Table 3 shows an example search
query being preprocessed, translated and postpro-
cessed. We use bigram characters instead of single
characters, as suggested in (Tiedemann, 2012), in
order to improve the statistical alignment models
and make them more expressive.
For training the autocorrection system we use
basic methods and open-source tools for statistical
</bodyText>
<footnote confidence="0.541326">
2https://code.google.com/p/cld2
</footnote>
<bodyText confidence="0.999949941176471">
machine translation. The character alignment is
obtained by using GIZA++ (Och and Ney, 2003)
for 4, 3 and 2 iterations of IBM Model 1, HMM,
and IBM Model 3, respectively. As opposed to
the standard machine translation task, we did not
observe improvements from IBM Model 4 and do
not use it as part of the alignment process.
We use Moses (Koehn et al., 2007) for standard
phrase extraction, building KenLM language mod-
els (Heafield, 2011) and tuning. The standard set
of features is used, including a phrase model, word
lexicon model, length penalty, jump penalty and a
language model. The model weights are optimized
using MERT (Och, 2003). The Moses framework
allows us to easily conduct experiments with sev-
eral settings and find the optimal one for our task
at hand. We experiment with varying context sizes
for phrase table and language model, additional
features and different scoring methods, e.g. BLEU
(Papineni et al., 2002) in comparison to directly
maximizing accuracy on the dev set. A more de-
tailed description of those experiments including
results will follow in Section 5.
Evaluation data. In order to evaluate our pro-
posed framework, we extracted 10,000 query pairs
from a one week period not part of the training
data. The initial size of 3.5M query pairs was re-
duced to 10k by exponential reservoir sampling
(Osborne et al., 2014) after sorting by frequency.
The result is a set of representative query pairs that
focuses more on frequent misspellings, but also
contains rare queries from the tail of the distribu-
tion.
We asked humans to create a gold reference an-
notation that we can use for tuning and evaluation
purposes. The guidelines were to check whether
for a search query pair (x, y), the left query x is
misspelled, and if so, whether the similar candi-
date y is a correct correction or else provide the
most likely correction. If x was not misspelled,
the guidelines instructed to propagate query x to
the gold reference y, i.e. for those cases, we have
identity or a true negative. If query x is not En-
glish, the annotators had to mark those entries and
we removed them from the set. This step affected
798 queries (i.e. around 8%, mostly Spanish), and
the final set contains 9202 query pairs. We split
those into half to produce 4,600 queries for dev
and 4,602 for test. The true negatives in those sets
(i.e. entries that do not need to be corrected) are
~15%. We are aware that this approach focuses on
</bodyText>
<page confidence="0.99623">
455
</page>
<bodyText confidence="0.7555258">
characters character bigrams
Original: hollowin custome
Preprocessing: h o l l o w i n S c u s t o m e ho ol ll lo ow wi in nS Sc cu st to om me
Translation: h a l l o w e e n S costume ha al ll lo ow we ee en nS Sc co st tu um me
Postprocessing: halloween costume
</bodyText>
<tableCaption confidence="0.961461">
Table 3: Preprocessing of input data as single character or character bigram sequences. “S” denotes
whitespace. After translation, postprocessing transforms back to word-level surface forms.
</tableCaption>
<table confidence="0.99820225">
correction
Speller produces
identity
Eval data DEV TEST
#queries 4,600 4,602
#tokens 15,593 15,557
CER[%] 6.1 6.0
SER[%] 84.2 84.3
</table>
<tableCaption confidence="0.994034">
Table 4: Statistics on dev and test portions of
</tableCaption>
<bodyText confidence="0.85732775">
the post-edited evaluation data. CER is the char-
acter error rate of the misspelled queries against
the gold reference, SER is the sentence (i.e. here
query-level) error rate of the sets.
</bodyText>
<figure confidence="0.949208375">
Gold reference demands
correction identity
correctly
autocorrected
TP
TN
correctly
non−corrected non−corrected
</figure>
<figureCaption confidence="0.999587">
Figure 1: Scoring schema.
</figureCaption>
<figure confidence="0.9446638">
FN
wrongly
wrongly
autocorrected
FP
</figure>
<bodyText confidence="0.999630139534884">
recall over precision, as the majority of queries is
usually not misspelled. Nevertheless, exponential
reservoir sampling helps us to focus on the head of
that distribution, and our main goal is to evaluate
the capabilities of the spelling correction frame-
work, not the overall system integration. A final
investigation in combination with query expansion
that will be evaluated in the context of the live site
is left for future work. Detailed statistics on the
two sets can be found in Table 4.
Additional training data. In addition to the par-
allel data pull from the query logs as described in
Section 2, we also take the top queries from those
logs where we are sure they are most likely cor-
rect and can be used as gold reference (e.g. search
queries like handbags or wedding dress appear
thousands of times), and generate artificially nois-
ified variants based on an English keyboard layout
and statistics derived from our development set.
On the dev set, we calculated a character error rate
of roughly 6%, and this rate is used as a muta-
tion rate for the artificially introduced noise. We
also determine the following edit operation rates
based on the dev set: 6% character transpositions,
18% for deletions, 33% for insertions, and 43%
for substitutions. The target character for substi-
tutions and insertions is based on a random Gaus-
sian distribution with a distance mean 1 and stan-
dard deviation of 1 around the selected character,
i.e. we target neighboring keys on the keyboard.
For the query wedding dress, e.g., this method in-
troduces misspelled variants such as weddig dress,
weedimg dress, or weddinb dreess. We run this
method 10 times on a set of 688k queries and re-
move all duplicates, resulting in additional 4.6M
search query pairs with around 13M tokens for the
training data.
As a final step, we add frequent queries from
user event logs but also product titles of the live in-
ventory to the language model. In total, the mono-
lingual portion of the data contains roughly 31M
entries with 319M tokens which are added to the
target side of the bilingual training portion.
</bodyText>
<sectionHeader confidence="0.999646" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999984692307692">
In this section, we report on a series of hillclimb-
ing experiments used to optimize performance. In
the following, we use standard information re-
trieval criteria for evaluation, namely accuracy,
precision, recall, and F1 score. Figure 1 depicts
a natural way of how the scoring is performed.
TP denotes true positives, FN false negatives,
FP false positives, and TN true negatives, respec-
tively. Note that for the case where the gold ref-
erence demands a correction, and the speller pro-
duces a wrong correction different from the source
query, we have to increase both false positives FP
and false negatives FN. With this, we can do stan-
</bodyText>
<page confidence="0.998543">
456
</page>
<table confidence="0.987077666666667">
TEST [%] Acc Prec Rec F1
68.8 84.0 64.2 72.8
63.0 77.4 60.2 67.7
56.3 58.6 60.7 59.7
70.6 74.0 72.3 73.1
70.2 77.1 69.3 73.0
</table>
<tableCaption confidence="0.75376125">
Table 5: Comparison of accuracy, precision, recall
and F1 score against other online sites on the test
set.
dard calculation of accuracy as (TP + TN)/(P +
</tableCaption>
<equation confidence="0.702197">
N) with P = TP +FN and N = TN +FP, pre-
cision as TP/(TP + FP), recall as TP/P, and
F1 score as 2TP/(2TP + FP + FN).
</equation>
<subsectionHeader confidence="0.995733">
5.1 Hillclimbing and performance
</subsectionHeader>
<bodyText confidence="0.999997129411765">
We compare the performance of our autocorrec-
tion system with that of three other online search
engines, both from the e-commerce as well as web
search domain. For this, we automated entering
our search terms on the corresponding sites and
extracted candidates from “Did you mean...” sug-
gestions or “Showing results for...” autocorrec-
tions. Table 5 shows that our method outperforms
all other systems in accuracy, recall and F1 score.
We argue that recall in our setup is more impor-
tant than precision because the goal is to correct
as much as possible as autocorrection is usually
invoked by the search backend as a result of low
or null recall size.
Gradual improvements were made to the system
setup and we track those on the development set.
Table 6 gives a couple of major waypoints in the
hillclimbing process. The baseline incorporates
all extracted training data and uses phrases up to
length 3 (i.e. up to three character bigrams for both
source and target side). The baseline language
model uses 6-grams. This setup is trained on very
noisy data that contains a lot of search refinements
that are not actual misspellings. The filtered data
improves results. In general, we observe a 3-4%
relative improvement across all scores when us-
ing bigrams instead of single characters. We ex-
periment with additional phrase features as part
of the phrase table and add 5 features based on
phrase pair-specific edit operations, i.e. the num-
ber of insertions, deletions, substitutions, transpo-
sitions and final overall edit distance, which helps
to increase precision. The artificially noisified data
gives additional small gains, as well as direct op-
timization of accuracy instead of BLEU or WER.
We did not observe significant differences when
tuning on BLEU versus WER. Most of the im-
provement though comes from increasing context
size, i.e., 10-gram language models and lengths
up to 5 bigram character spans for both source
and target phrases. We also observe that iterative
correction, i.e. running the speller a second time
over already corrected data, further improves per-
formance slightly which is in-line with findings in
(Cucerzan and Brill, 2004) and (Gubanov et al.,
2014).
Increasing precision. We also investigated a
system setup that focuses on precision over re-
call in order to be more in sync with the online
systems that have been most likely optimized to a
more cautious correction mode. Our previous ex-
periments prefer recall which is due to the 85:15
split of misspelled vs. correct queries in the dev
set. For a more conservative mode that focuses
on precision, we updated the dev set by automat-
ically adding “mostly-correct” queries with iden-
tity as correction candidate. For this, we extracted
the most frequent queries with a high number of
search results which can be deemed to be “al-
most” correct. The dev size increased to roughly
22k queries with an approximate split of 85:15 for
“correct” vs. misspelled. This is a more realistic
setting if the correction is applied to all incom-
ing queries, irrespective of the number of corre-
sponding search results (note also that this mode is
different from our initial one where we apply cor-
rections only to queries with low or null results).
We also found that tuning on Matthews Correla-
tion Coefficient (Matthews, 1975) balances better
precision versus recall, especially for unbalanced
classes which is the case here (i.e. 85:15 split).
In the last line of Table 6 we added more data
extracted over several additional months. The ad-
ditional data amounts to 60M queries, therefore
increasing the total training size to 72M queries.
This final setup, as described, improves precision
but hurts recall slightly. Overall, the F1 and ac-
curacy measures are still improved which is most
likely due to the additional training data.
Finally, we ran a large-scale experiment and
extracted the 100k most frequent unique queries
(which account for 7.8M queries based on their
actual frequencies). Since they represent the most
common queries, some of them typed thousands
of times by the users, they are deemed to be
</bodyText>
<figure confidence="0.9435404">
online A
online B
online C
MT (this work)
MT (alt. setup)
457
TEST [%]
misspelled
gold corrections
online A
online B
online C
MT (this work)
null results KL div. &lt; 1
81.6 40.9
27.1 100.0
36.5 84.4
36.0 82.1
33.0 76.4
29.8 86.0
</figure>
<table confidence="0.993121">
DEV [%] Acc Prec Rec F1
baseline 62.0 66.2 61.0 63.5
+filtered data 65.5 68.8 67.6 68.2
+phrase length 5 67.2 70.0 69.6 69.8
+phrase features 67.3 71.2 68.3 69.8
+artificial noise 67.8 71.4 68.3 69.8
+10-gram LM 68.7 71.9 70.5 71.2
+tune on Acc 68.9 72.1 70.3 71.2
+iterative 2nd run 69.0 72.2 70.5 71.4
+alt. setup (Prec) 69.5 75.4 68.5 71.8
</table>
<tableCaption confidence="0.97791325">
Table 6: Hillclimbing on the dev set.
Query Category distribution
moto x 50% cell phones, 30% auto-
motive, 5% clothing,...
tory burch fitbit 75% jewelry, 20% sports, ...
madden 15 65% games, 18% toys, 7%
collectibles, ...
Table 7: Item category distributions for queries.
</tableCaption>
<bodyText confidence="0.999881125">
“mostly” correct. We autocorrect the set through
our best system which results in changed queries
in &lt;0.5% of the cases after manual filtering of
non-errors (e.g. segmentation differences that are
handled separately by our query expansion sys-
tem, e.g. rayban -* ray ban which are equivalent
in the search backend) and when compared against
the original input set. This means that most of
the queries are left unchanged and that the decod-
ing process of the SMT system does a good job
in classifying whether a correction is needed or
not. Manual inspection of the most frequent differ-
ences shows that changes actually happen on fre-
quently misspelled words which are part of those
top 100k queries, e.g. micheal -* michael, infared
-* infrared, or accesories -* accessories.
</bodyText>
<subsectionHeader confidence="0.999937">
5.2 Search evaluation
</subsectionHeader>
<bodyText confidence="0.999392764705882">
An interesting way to evaluate the performance of
the autocorrection system is to look at it from a
search perspective. If the goal is to improve user
experience in search and retrieve more relevant re-
sults, we need to compare the search result sets
in terms of quantity and quality. For quantity we
are mainly interested in how many queries lead
to 0 search results before and after autocorrection.
Table 8 shows these numbers for our test set. We
improve the rate of null results by 51.8% absolute.
In a deeper analysis we see that out of the 81.6% in
Table 8: Search results evaluation on the test set.
For null results, lower rates are better. For KL div.
evaluation, higher rates are better, as they indicate
a closer match with the category distribution from
the gold corrections.
the source set we improve the query behavior for
63.8% queries after autocorrection while we only
decrease from non-null to null for 1.6%. A manual
inspection of the 1.6% shows that those queries al-
ready led to very low search results (&lt;10) even for
the source.
For quality, we compare the search result sets
for the autocorrected queries with those of the ref-
erence and get an estimate of how similar each
autocorrected query behaves to the expected be-
havior given by its gold correction. In particular,
we extract the categories from all items which are
returned as part of the search API calls (see Ta-
ble 7). We use Kullback-Leibler divergence (Kull-
back and Leibler, 1951) to compute the difference
between the category distributions. The KL diver-
gence defines the information lost when using one
distribution to approximate another:
</bodyText>
<equation confidence="0.9963435">
�P (i) )
P(i) ln Q(i)
</equation>
<bodyText confidence="0.999668375">
In our case, we use the category distribution of the
results of the reference query as the observed data
P which we want to model with the category dis-
tribution Q from the results of the autocorrected
query. This gives us a more realistic evaluation of
the method, as it is more relevant to the retrieval
problem that autocorrection is trying to address.
The queries perfumes for women and perfume for
women, e.g., retrieve similar amounts and types of
items although their strings are different (note the
plural s in the first one) which is penalized when
computing accuracy on string level.
In order to deal with zero-probability events, we
smooth both distributions such that they contain
exactly the same categories. For this we add cat-
egories that are present in one distribution but not
</bodyText>
<equation confidence="0.596134">
�
D(P||Q) =
Z
</equation>
<page confidence="0.989305">
458
</page>
<bodyText confidence="0.975238">
Misspelled query Correction
adidda whatch blooto adidas watch bluetooth
awrppstale buttin shirt aeropostale button shirt
bangen olafsn headset bang olufsen headset
camra exesers camera accessory
crystal and righston cuf ring crystal and rhinestone cuff ring
fauxfurmidcalfwesternboots faux fur mid calf western boots
fotboolchus football shoes
otherbooxiphon5 otterbox iphone 5
womens realhairsaltandpeper womens real hair salt and pepper
</bodyText>
<tableCaption confidence="0.967413">
Table 9: Examples of misspelled user search queries that the presented system is able to correct.
</tableCaption>
<bodyText confidence="0.999959476190476">
the other with a very small probability which we
subtract from the highest probability in the distri-
bution so that all probabilities still sum up to 1. In
the case of getting 0 search results for either the
autocorrected or reference query we cannot com-
pute the KL divergence and we simply mark those
cases with a high number. If both queries return 0
search results we return a distance of 0. Note
that with this we do not penalize queries that are
misspelled and should have been corrected even if
they still retrieve 0 items in search after correction.
However, in this test we are only interested in the
search results we get and not in the correction it-
self.
Table 8 shows that the presented method is clos-
est in terms of KL divergence to category distri-
butions of search results based on the gold refer-
ences. Even though this is a nice and easy way to
indicate quality of search results, we do not claim
this method to be an in-depth analysis of relevance
of search results and leave this for future work.
</bodyText>
<subsectionHeader confidence="0.970694">
5.3 Examples
</subsectionHeader>
<bodyText confidence="0.999979625">
The examples in Table 9 demonstrate the abil-
ities of the presented spelling correction frame-
work. We are able to handle extremely misspelled
queries, e.g. due to phonetic spelling by possibly
non-native speakers (camra —* camera, chus —*
shoes), words being glued together due to missing
whitespace (fauxfurmid... —* faux fur mid...), or
brand name confusions (righston —* rhinestone).
</bodyText>
<sectionHeader confidence="0.999595" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99994195">
In this paper, we presented a powerful spelling
correction system for the e-commerce domain us-
ing statistical phrase-based machine translation
based on character bigram sequences. We ex-
tracted training data from event logs where users
issue similar search queries within a certain pe-
riod of time. Filtering through various heuristics
is used to clean the initially noisy data set and re-
move entries not related to spelling errors. We
evaluated our system against established online
search sites, both in the general and e-commerce
domain, and showed favorable results in terms of
recall and retrieval rates.
We plan to further invest in improving preci-
sion. For this, we feel that adding full word-
level models will help to overcome the somewhat
limited context present in our character sequence-
based models. We also plan to investigate the pro-
totype directly in query expansion as part of the
search backend.
</bodyText>
<sectionHeader confidence="0.998631" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999325">
The authors would like to thank the Localization
team at eBay for creating gold corrections for
the evaluation sets, and members of the HLT and
Search teams for fruitful discussions as well as
reading early drafts of this work and giving valu-
able feedback.
</bodyText>
<sectionHeader confidence="0.998104" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.979345333333333">
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction.
In Proc. of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 286–293,
Hong Kong.
Hsun-wen Chiu, Jian-cheng Wu, and Jason S. Chang.
2013. Chinese spelling checker based on statisti-
cal machine translation. In Proc. of the Seventh
SIGHAN Workshop on Chinese Language Process-
ing, pages 49–53, Nagoya, Japan.
Silviu Cucerzan and Eric Brill. 2004. Spelling correc-
tion as an iterative process that exploits the collective
</reference>
<page confidence="0.989506">
459
</page>
<reference confidence="0.999867603603603">
knowledge of web users. In Proc. of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing, pages 293–300, Barcelona, Spain.
Frederick J. Damerau. 1964. A technique for computer
detection and correction of spelling errors. Commu-
nication ofACM, 7(3):171–176.
Thomas Deselaers, Saša Hasan, Oliver Bender, and
Hermann Ney. 2009. A deep learning approach to
machine transliteration. In Fourth EACL Workshop
on Statistical Machine Translation, pages 233–241,
Athens, Greece.
Lluís Formiga and José A. R. Fonollosa. 2012. Deal-
ing with input noise in statistical machine transla-
tion. In Proc. of the 24th International Confer-
ence on Computational Linguistics (Coling 2012):
Posters, pages 319–328, Mumbai, India.
Jianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk,
and Xu Sun. 2010. A large scale ranker-based sys-
tem for search query spelling correction. In Proc.
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 358–366,
Beijing, China.
Sergey Gubanov, Irina Galinskaya, and Alexey Baytin.
2014. Improved iterative correction for distant
spelling errors. In Proc. of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics: Short Papers, pages 168–173, Baltimore, MD.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proc. of the EMNLP
2011 Sixth Workshop on Statistical Machine Trans-
lation, pages 187–197, Edinburgh, UK.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics: Demos and Posters,
pages 177–180, Prague, Czech Republic.
Karen Kukich. 1992. Techniques for automatically
correcting words in text. ACM Computing Surveys,
24(4):377–439.
Solomon Kullback and Richard Leibler. 1951. On in-
formation and sufficiency. Annals of Mathematical
Statistics, 22(1):79–86.
Xiaodong Liu, Kevin Cheng, Yanyan Luo, Kevin Duh,
and Yuji Matsumoto. 2013. A hybrid Chinese
spelling correction using language model and statis-
tical machine translation with reranking. In Proc.
of the Seventh SIGHAN Workshop on Chinese Lan-
guage Processing, pages 54–58, Nagoya, Japan.
Brian W. Matthews. 1975. Comparison of the pre-
dicted and observed secondary structure of T4 phage
lysozyme. Biochimica et Biophysica Acta (BBA) -
Protein Structure, 405(2):442–451.
Preslav Nakov and Jörg Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages.
In Proc. of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Short Papers,
pages 301–305, Jeju Island, Korea.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41st
Annual Meeting of the Association for Computa-
tional Linguistics, pages 160–167, Sapporo, Japan.
Miles Osborne, Ashwin Lall, and Benjamin
Van Durme. 2014. Exponential reservoir sampling
for streaming language models. In Proc. of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 2: Short Papers), pages
687–692, Baltimore, Maryland.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proc. of the
40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA.
Filip Radlinski and Thorsten Joachims. 2005. Query
chains: Learning to rank from implicit feedback. In
Proc. of the Eleventh ACM SIGKDD International
Conference on Knowledge Discovery in Data Min-
ing, pages 239–248, Chicago, IL.
Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk.
2010. Learning phrase-based spelling error models
from clickthrough data. In Proc. of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 266–274, Uppsala, Sweden.
Jörg Tiedemann. 2012. Character-based pivot transla-
tion for under-resourced languages and domains. In
Proc. of the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 141–151, Avignon, France.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Second Work-
shop on Statistical Machine Translation, pages 33–
39, Prague, Czech Republic.
Casey Whitelaw, Ben Hutchinson, Grace Y Chung, and
Ged Ellis. 2009. Using the Web for language inde-
pendent spellchecking and autocorrection. In Proc.
of the 2009 Conference on Empirical Methods in
Natural Language Processing, pages 890–899, Sin-
gapore.
Yang Zhang, Pilian He, Wei Xiang, and Mu Li. 2006.
Discriminative reranking for spelling correction. In
Proc. of the 20th Pacific Asia Conference on Lan-
guage, Information and Computation, pages 64–71,
Wuhan, China.
</reference>
<page confidence="0.999212">
460
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915884">
<title confidence="0.9991105">Spelling Correction of User Search Queries through Statistical Machine Translation</title>
<author confidence="0.96391">Mansour</author>
<email confidence="0.977432">sasa.hasan@gmail.comheger.carmen@gmail.comsaamansour@ebay.com</email>
<affiliation confidence="0.998633">eBay Inc.</affiliation>
<address confidence="0.999376">2065 Hamilton Ave San Jose, CA 95125, USA</address>
<abstract confidence="0.998572842105263">We use character-based statistical machine translation in order to correct user search queries in the e-commerce domain. The training data is automatically extracted from event logs where users re-issue their search queries with potentially corrected spelling within the same session. We show results on a test set which was annotated by humans and compare against online autocorrection capabilities of three additional web sites. Overall, the methods presented in this paper outperform fully productized spellchecking and autocorrection services in terms of accuracy and F1 score. We also propose novel evaluation steps based on retrieved search results of the corrected queries in terms of quantity and relevance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Robert C Moore</author>
</authors>
<title>An improved error model for noisy channel spelling correction.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>286--293</pages>
<location>Hong Kong.</location>
<contexts>
<context position="4894" citStr="Brill and Moore, 2000" startWordPosition="770" endWordPosition="773">the first work that uses character-based machine translation technology on user-generated data for spelling correction. Moreover, it is the first to evaluate the performance in an e-commerce setting with there relevant measures. 1.1 Related Work One of the more prominent papers on autocorrection of misspelled input is (Whitelaw et al., 2009). The three-step approach incorporates a classification step that determines whether a word is misspelled, computes the most likely correction candidate and then, again, classifies whether this candidate is likely to be correct. An error model based after (Brill and Moore, 2000) is trained on similar word pairs extracted from large amounts of web sites, and a language model is used to disambiguate correction candidates based on left and right context around the current position. Our method differs in several ways. First, we consider full query pairs as training data, and do not use single words as primary mode of operation. Second, we do not train explicit error models P(wIs) for words w and observed corrections s, but use standard phrase-based machine translation modeling to derive phrase and lexical translation models. Although our level of context is shorter, espe</context>
</contexts>
<marker>Brill, Moore, 2000</marker>
<rawString>Eric Brill and Robert C. Moore. 2000. An improved error model for noisy channel spelling correction. In Proc. of the 38th Annual Meeting of the Association for Computational Linguistics, pages 286–293, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hsun-wen Chiu</author>
<author>Jian-cheng Wu</author>
<author>Jason S Chang</author>
</authors>
<title>Chinese spelling checker based on statistical machine translation.</title>
<date>2013</date>
<booktitle>In Proc. of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>49--53</pages>
<location>Nagoya, Japan.</location>
<contexts>
<context position="8414" citStr="Chiu et al., 2013" startWordPosition="1351" endWordPosition="1354"> on word uni- and bigram decompositions, and the authors evaluate on both a large set that contains around 17% misspelled queries and a smaller set that is based on successive user-reformulated similar queries, a similar setup that we use to extract our training data. They stress the importance of a good language model, as performance drops drastically going from a bigram to a unigram LM. The use of character-based models in combination with statistical machine translation is not novel and was proposed for spelling correction, e.g., in (Formiga and Fonollosa, 2012), (Liu et al., 452 2013) or (Chiu et al., 2013). The authors compare a distance-based approach including a language model, a confusion network-based approach, a translation approach through a heuristically defined phrase table coding all character transformations, and a character-based machine translation approach using standard procedures (automatic word alignment and phrase extraction). The training data is manually created in contrast to our work where we automatically bootstrap training data from query logs. Research has also been done for translation of closely related languages (e.g. (Vilar et al., 2007) and (Nakov and Tiedemann, 201</context>
</contexts>
<marker>Chiu, Wu, Chang, 2013</marker>
<rawString>Hsun-wen Chiu, Jian-cheng Wu, and Jason S. Chang. 2013. Chinese spelling checker based on statistical machine translation. In Proc. of the Seventh SIGHAN Workshop on Chinese Language Processing, pages 49–53, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Silviu Cucerzan</author>
<author>Eric Brill</author>
</authors>
<title>Spelling correction as an iterative process that exploits the collective knowledge of web users.</title>
<date>2004</date>
<booktitle>In Proc. of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>293--300</pages>
<location>Barcelona,</location>
<contexts>
<context position="7610" citStr="Cucerzan and Brill, 2004" startWordPosition="1218" endWordPosition="1221">rrect web queries through additional reranking of its output by a ranking SVM. The training data is in part automatically extracted, but also contains manually annotated pairs and, thus, is a semi-supervised approach. In this paper, we use an unsupervised approach to generate training data. Query spelling correction that is based on click-through data and uses a phrase-based error model is reported in (Sun et al., 2010). Our models operate on character sequences instead of words, and we do not observe issues with identity transformations (i.e. non-corrections for correctly spelled input). In (Cucerzan and Brill, 2004), the authors investigate a transformation method that corrects unlikely queries into more likely variants based on web query logs. The iterative approach transforms a search query based on word uni- and bigram decompositions, and the authors evaluate on both a large set that contains around 17% misspelled queries and a smaller set that is based on successive user-reformulated similar queries, a similar setup that we use to extract our training data. They stress the importance of a good language model, as performance drops drastically going from a bigram to a unigram LM. The use of character-b</context>
<context position="27742" citStr="Cucerzan and Brill, 2004" startWordPosition="4617" endWordPosition="4620"> which helps to increase precision. The artificially noisified data gives additional small gains, as well as direct optimization of accuracy instead of BLEU or WER. We did not observe significant differences when tuning on BLEU versus WER. Most of the improvement though comes from increasing context size, i.e., 10-gram language models and lengths up to 5 bigram character spans for both source and target phrases. We also observe that iterative correction, i.e. running the speller a second time over already corrected data, further improves performance slightly which is in-line with findings in (Cucerzan and Brill, 2004) and (Gubanov et al., 2014). Increasing precision. We also investigated a system setup that focuses on precision over recall in order to be more in sync with the online systems that have been most likely optimized to a more cautious correction mode. Our previous experiments prefer recall which is due to the 85:15 split of misspelled vs. correct queries in the dev set. For a more conservative mode that focuses on precision, we updated the dev set by automatically adding “mostly-correct” queries with identity as correction candidate. For this, we extracted the most frequent queries with a high n</context>
</contexts>
<marker>Cucerzan, Brill, 2004</marker>
<rawString>Silviu Cucerzan and Eric Brill. 2004. Spelling correction as an iterative process that exploits the collective knowledge of web users. In Proc. of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 293–300, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick J Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Communication ofACM,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="3144" citStr="Damerau, 1964" startWordPosition="497" endWordPosition="498">ime information is novel. Previous work suggested collecting queries using information that a user clicked on a proposed correction. Our proposed method for collecting training data has several advantages. First, we do not rely on a previous spelling correction system, but on user formulations. Second, for many search queries, especially from the tail where search recall is generally low, these misspellings yield few results, and thus, users looking for certain products are inclined to correct the query themselves in order to find what they are looking for. We use DamerauLevenshtein distance (Damerau, 1964) on character level as similarity criterion, i.e. queries within a specific edit distance are considered to be related. The steps proposed in this work are: 1. Extraction of similar user queries from search logs for bootstrapping training data (Section 2), 2. classification and filtering of data to remove noisy entries (Section 3), and 451 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 451–460, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. 3. spelling correction cast into a statistical machine translatio</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>Frederick J. Damerau. 1964. A technique for computer detection and correction of spelling errors. Communication ofACM, 7(3):171–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Deselaers</author>
<author>Saša Hasan</author>
<author>Oliver Bender</author>
<author>Hermann Ney</author>
</authors>
<title>A deep learning approach to machine transliteration.</title>
<date>2009</date>
<booktitle>In Fourth EACL Workshop on Statistical Machine Translation,</booktitle>
<pages>233--241</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="9068" citStr="Deselaers et al., 2009" startWordPosition="1447" endWordPosition="1450">e-based approach including a language model, a confusion network-based approach, a translation approach through a heuristically defined phrase table coding all character transformations, and a character-based machine translation approach using standard procedures (automatic word alignment and phrase extraction). The training data is manually created in contrast to our work where we automatically bootstrap training data from query logs. Research has also been done for translation of closely related languages (e.g. (Vilar et al., 2007) and (Nakov and Tiedemann, 2012)) and transliteration (e.g. (Deselaers et al., 2009)). An early summary paper with various spelling-related problems (non-word error detection, isolated-word error correction, and contextdependent word correction) can be found in (Kukich, 1992). 2 Extraction of training data We use event logs that track user interactions on an e-commerce web site. Each action of a user visiting the site is stored in a data warehouse and HDFS (Hadoop Distributed File System). We store several billion of these user records on a daily basis. The setup allows us to efficiently process large amounts of data points using a Map-Reduce framework via Hadoop1. As part of</context>
</contexts>
<marker>Deselaers, Hasan, Bender, Ney, 2009</marker>
<rawString>Thomas Deselaers, Saša Hasan, Oliver Bender, and Hermann Ney. 2009. A deep learning approach to machine transliteration. In Fourth EACL Workshop on Statistical Machine Translation, pages 233–241, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluís Formiga</author>
<author>José A R Fonollosa</author>
</authors>
<title>Dealing with input noise in statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proc. of the 24th International Conference on Computational Linguistics (Coling 2012): Posters,</booktitle>
<pages>319--328</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="8367" citStr="Formiga and Fonollosa, 2012" startWordPosition="1341" endWordPosition="1344">s. The iterative approach transforms a search query based on word uni- and bigram decompositions, and the authors evaluate on both a large set that contains around 17% misspelled queries and a smaller set that is based on successive user-reformulated similar queries, a similar setup that we use to extract our training data. They stress the importance of a good language model, as performance drops drastically going from a bigram to a unigram LM. The use of character-based models in combination with statistical machine translation is not novel and was proposed for spelling correction, e.g., in (Formiga and Fonollosa, 2012), (Liu et al., 452 2013) or (Chiu et al., 2013). The authors compare a distance-based approach including a language model, a confusion network-based approach, a translation approach through a heuristically defined phrase table coding all character transformations, and a character-based machine translation approach using standard procedures (automatic word alignment and phrase extraction). The training data is manually created in contrast to our work where we automatically bootstrap training data from query logs. Research has also been done for translation of closely related languages (e.g. (Vi</context>
</contexts>
<marker>Formiga, Fonollosa, 2012</marker>
<rawString>Lluís Formiga and José A. R. Fonollosa. 2012. Dealing with input noise in statistical machine translation. In Proc. of the 24th International Conference on Computational Linguistics (Coling 2012): Posters, pages 319–328, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaolong Li</author>
<author>Daniel Micol</author>
<author>Chris Quirk</author>
<author>Xu Sun</author>
</authors>
<title>A large scale ranker-based system for search query spelling correction.</title>
<date>2010</date>
<booktitle>In Proc. of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>358--366</pages>
<location>Beijing, China.</location>
<contexts>
<context position="6489" citStr="Gao et al., 2010" startWordPosition="1032" endWordPosition="1035">unctions. The classification of query chains is performed by support vector machines, and its training data is generated in a supervised fashion by manual inspection and annotation. In contrast, we do not manually annotate any of our training data. Since our initial sets are quite noisy, we apply a couple of heuristics that try to produce a cleaner subset of the data that contains mostly misspelled queries and their potential correction candidates. Another focus of researchers was specifically to tackle misspelled web search queries and use search engine logs for training and evaluation data (Gao et al., 2010), which differs from our work by collecting data using “click-through” enforcement. The user is presented with a spelling correction, and if she clicks on it, they learn that the correction is valid. Our method does not need a previous spelling correction to work. In addition, our proposed method has the potential to learn corrections of new and rare terms that will not be produced by an automatic spelling correction. In (Zhang et al., 2006), the authors use a conventional spellchecker to correct web queries through additional reranking of its output by a ranking SVM. The training data is in p</context>
</contexts>
<marker>Gao, Li, Micol, Quirk, Sun, 2010</marker>
<rawString>Jianfeng Gao, Xiaolong Li, Daniel Micol, Chris Quirk, and Xu Sun. 2010. A large scale ranker-based system for search query spelling correction. In Proc. of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Gubanov</author>
<author>Irina Galinskaya</author>
<author>Alexey Baytin</author>
</authors>
<title>Improved iterative correction for distant spelling errors.</title>
<date>2014</date>
<booktitle>In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics: Short Papers,</booktitle>
<pages>168--173</pages>
<location>Baltimore, MD.</location>
<contexts>
<context position="27769" citStr="Gubanov et al., 2014" startWordPosition="4622" endWordPosition="4625">ion. The artificially noisified data gives additional small gains, as well as direct optimization of accuracy instead of BLEU or WER. We did not observe significant differences when tuning on BLEU versus WER. Most of the improvement though comes from increasing context size, i.e., 10-gram language models and lengths up to 5 bigram character spans for both source and target phrases. We also observe that iterative correction, i.e. running the speller a second time over already corrected data, further improves performance slightly which is in-line with findings in (Cucerzan and Brill, 2004) and (Gubanov et al., 2014). Increasing precision. We also investigated a system setup that focuses on precision over recall in order to be more in sync with the online systems that have been most likely optimized to a more cautious correction mode. Our previous experiments prefer recall which is due to the 85:15 split of misspelled vs. correct queries in the dev set. For a more conservative mode that focuses on precision, we updated the dev set by automatically adding “mostly-correct” queries with identity as correction candidate. For this, we extracted the most frequent queries with a high number of search results whi</context>
</contexts>
<marker>Gubanov, Galinskaya, Baytin, 2014</marker>
<rawString>Sergey Gubanov, Irina Galinskaya, and Alexey Baytin. 2014. Improved iterative correction for distant spelling errors. In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics: Short Papers, pages 168–173, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
</authors>
<title>KenLM: faster and smaller language model queries.</title>
<date>2011</date>
<booktitle>In Proc. of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>187--197</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="19418" citStr="Heafield, 2011" startWordPosition="3192" endWordPosition="3193">lignment models and make them more expressive. For training the autocorrection system we use basic methods and open-source tools for statistical 2https://code.google.com/p/cld2 machine translation. The character alignment is obtained by using GIZA++ (Och and Ney, 2003) for 4, 3 and 2 iterations of IBM Model 1, HMM, and IBM Model 3, respectively. As opposed to the standard machine translation task, we did not observe improvements from IBM Model 4 and do not use it as part of the alignment process. We use Moses (Koehn et al., 2007) for standard phrase extraction, building KenLM language models (Heafield, 2011) and tuning. The standard set of features is used, including a phrase model, word lexicon model, length penalty, jump penalty and a language model. The model weights are optimized using MERT (Och, 2003). The Moses framework allows us to easily conduct experiments with several settings and find the optimal one for our task at hand. We experiment with varying context sizes for phrase table and language model, additional features and different scoring methods, e.g. BLEU (Papineni et al., 2002) in comparison to directly maximizing accuracy on the dev set. A more detailed description of those exper</context>
</contexts>
<marker>Heafield, 2011</marker>
<rawString>Kenneth Heafield. 2011. KenLM: faster and smaller language model queries. In Proc. of the EMNLP 2011 Sixth Workshop on Statistical Machine Translation, pages 187–197, Edinburgh, UK.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics: Demos and Posters,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="19338" citStr="Koehn et al., 2007" startWordPosition="3179" endWordPosition="3182">characters, as suggested in (Tiedemann, 2012), in order to improve the statistical alignment models and make them more expressive. For training the autocorrection system we use basic methods and open-source tools for statistical 2https://code.google.com/p/cld2 machine translation. The character alignment is obtained by using GIZA++ (Och and Ney, 2003) for 4, 3 and 2 iterations of IBM Model 1, HMM, and IBM Model 3, respectively. As opposed to the standard machine translation task, we did not observe improvements from IBM Model 4 and do not use it as part of the alignment process. We use Moses (Koehn et al., 2007) for standard phrase extraction, building KenLM language models (Heafield, 2011) and tuning. The standard set of features is used, including a phrase model, word lexicon model, length penalty, jump penalty and a language model. The model weights are optimized using MERT (Och, 2003). The Moses framework allows us to easily conduct experiments with several settings and find the optimal one for our task at hand. We experiment with varying context sizes for phrase table and language model, additional features and different scoring methods, e.g. BLEU (Papineni et al., 2002) in comparison to directl</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. of the 45th Annual Meeting of the Association for Computational Linguistics: Demos and Posters, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="9260" citStr="Kukich, 1992" startWordPosition="1475" endWordPosition="1477">-based machine translation approach using standard procedures (automatic word alignment and phrase extraction). The training data is manually created in contrast to our work where we automatically bootstrap training data from query logs. Research has also been done for translation of closely related languages (e.g. (Vilar et al., 2007) and (Nakov and Tiedemann, 2012)) and transliteration (e.g. (Deselaers et al., 2009)). An early summary paper with various spelling-related problems (non-word error detection, isolated-word error correction, and contextdependent word correction) can be found in (Kukich, 1992). 2 Extraction of training data We use event logs that track user interactions on an e-commerce web site. Each action of a user visiting the site is stored in a data warehouse and HDFS (Hadoop Distributed File System). We store several billion of these user records on a daily basis. The setup allows us to efficiently process large amounts of data points using a Map-Reduce framework via Hadoop1. As part of user event tracking, all interactions on the site are stored in the database, e.g. which search terms were entered, which links were clicked, and what actions resulted in this (i.e. buying an</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys, 24(4):377–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Solomon Kullback</author>
<author>Richard Leibler</author>
</authors>
<title>On information and sufficiency.</title>
<date>1951</date>
<journal>Annals of Mathematical Statistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="32759" citStr="Kullback and Leibler, 1951" startWordPosition="5475" endWordPosition="5479"> 63.8% queries after autocorrection while we only decrease from non-null to null for 1.6%. A manual inspection of the 1.6% shows that those queries already led to very low search results (&lt;10) even for the source. For quality, we compare the search result sets for the autocorrected queries with those of the reference and get an estimate of how similar each autocorrected query behaves to the expected behavior given by its gold correction. In particular, we extract the categories from all items which are returned as part of the search API calls (see Table 7). We use Kullback-Leibler divergence (Kullback and Leibler, 1951) to compute the difference between the category distributions. The KL divergence defines the information lost when using one distribution to approximate another: �P (i) ) P(i) ln Q(i) In our case, we use the category distribution of the results of the reference query as the observed data P which we want to model with the category distribution Q from the results of the autocorrected query. This gives us a more realistic evaluation of the method, as it is more relevant to the retrieval problem that autocorrection is trying to address. The queries perfumes for women and perfume for women, e.g., r</context>
</contexts>
<marker>Kullback, Leibler, 1951</marker>
<rawString>Solomon Kullback and Richard Leibler. 1951. On information and sufficiency. Annals of Mathematical Statistics, 22(1):79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong Liu</author>
<author>Kevin Cheng</author>
<author>Yanyan Luo</author>
<author>Kevin Duh</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A hybrid Chinese spelling correction using language model and statistical machine translation with reranking.</title>
<date>2013</date>
<booktitle>In Proc. of the Seventh SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>54--58</pages>
<location>Nagoya, Japan.</location>
<marker>Liu, Cheng, Luo, Duh, Matsumoto, 2013</marker>
<rawString>Xiaodong Liu, Kevin Cheng, Yanyan Luo, Kevin Duh, and Yuji Matsumoto. 2013. A hybrid Chinese spelling correction using language model and statistical machine translation with reranking. In Proc. of the Seventh SIGHAN Workshop on Chinese Language Processing, pages 54–58, Nagoya, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian W Matthews</author>
</authors>
<title>Comparison of the predicted and observed secondary structure of T4 phage lysozyme.</title>
<date>1975</date>
<journal>Biochimica et Biophysica Acta (BBA) -Protein Structure,</journal>
<volume>405</volume>
<issue>2</issue>
<contexts>
<context position="28875" citStr="Matthews, 1975" startWordPosition="4813" endWordPosition="4814">rrection candidate. For this, we extracted the most frequent queries with a high number of search results which can be deemed to be “almost” correct. The dev size increased to roughly 22k queries with an approximate split of 85:15 for “correct” vs. misspelled. This is a more realistic setting if the correction is applied to all incoming queries, irrespective of the number of corresponding search results (note also that this mode is different from our initial one where we apply corrections only to queries with low or null results). We also found that tuning on Matthews Correlation Coefficient (Matthews, 1975) balances better precision versus recall, especially for unbalanced classes which is the case here (i.e. 85:15 split). In the last line of Table 6 we added more data extracted over several additional months. The additional data amounts to 60M queries, therefore increasing the total training size to 72M queries. This final setup, as described, improves precision but hurts recall slightly. Overall, the F1 and accuracy measures are still improved which is most likely due to the additional training data. Finally, we ran a large-scale experiment and extracted the 100k most frequent unique queries (</context>
</contexts>
<marker>Matthews, 1975</marker>
<rawString>Brian W. Matthews. 1975. Comparison of the predicted and observed secondary structure of T4 phage lysozyme. Biochimica et Biophysica Acta (BBA) -Protein Structure, 405(2):442–451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Jörg Tiedemann</author>
</authors>
<title>Combining word-level and character-level models for machine translation between closely-related languages.</title>
<date>2012</date>
<booktitle>In Proc. of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers,</booktitle>
<pages>301--305</pages>
<location>Jeju Island,</location>
<contexts>
<context position="9016" citStr="Nakov and Tiedemann, 2012" startWordPosition="1439" endWordPosition="1443">) or (Chiu et al., 2013). The authors compare a distance-based approach including a language model, a confusion network-based approach, a translation approach through a heuristically defined phrase table coding all character transformations, and a character-based machine translation approach using standard procedures (automatic word alignment and phrase extraction). The training data is manually created in contrast to our work where we automatically bootstrap training data from query logs. Research has also been done for translation of closely related languages (e.g. (Vilar et al., 2007) and (Nakov and Tiedemann, 2012)) and transliteration (e.g. (Deselaers et al., 2009)). An early summary paper with various spelling-related problems (non-word error detection, isolated-word error correction, and contextdependent word correction) can be found in (Kukich, 1992). 2 Extraction of training data We use event logs that track user interactions on an e-commerce web site. Each action of a user visiting the site is stored in a data warehouse and HDFS (Hadoop Distributed File System). We store several billion of these user records on a daily basis. The setup allows us to efficiently process large amounts of data points </context>
</contexts>
<marker>Nakov, Tiedemann, 2012</marker>
<rawString>Preslav Nakov and Jörg Tiedemann. 2012. Combining word-level and character-level models for machine translation between closely-related languages. In Proc. of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers, pages 301–305, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="19072" citStr="Och and Ney, 2003" startWordPosition="3128" endWordPosition="3131">ates word boundaries. Once these sequences of characters are corrected, i.e. translated, they are merged back to the full word forms. Table 3 shows an example search query being preprocessed, translated and postprocessed. We use bigram characters instead of single characters, as suggested in (Tiedemann, 2012), in order to improve the statistical alignment models and make them more expressive. For training the autocorrection system we use basic methods and open-source tools for statistical 2https://code.google.com/p/cld2 machine translation. The character alignment is obtained by using GIZA++ (Och and Ney, 2003) for 4, 3 and 2 iterations of IBM Model 1, HMM, and IBM Model 3, respectively. As opposed to the standard machine translation task, we did not observe improvements from IBM Model 4 and do not use it as part of the alignment process. We use Moses (Koehn et al., 2007) for standard phrase extraction, building KenLM language models (Heafield, 2011) and tuning. The standard set of features is used, including a phrase model, word lexicon model, length penalty, jump penalty and a language model. The model weights are optimized using MERT (Och, 2003). The Moses framework allows us to easily conduct ex</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="19620" citStr="Och, 2003" startWordPosition="3225" endWordPosition="3226">cter alignment is obtained by using GIZA++ (Och and Ney, 2003) for 4, 3 and 2 iterations of IBM Model 1, HMM, and IBM Model 3, respectively. As opposed to the standard machine translation task, we did not observe improvements from IBM Model 4 and do not use it as part of the alignment process. We use Moses (Koehn et al., 2007) for standard phrase extraction, building KenLM language models (Heafield, 2011) and tuning. The standard set of features is used, including a phrase model, word lexicon model, length penalty, jump penalty and a language model. The model weights are optimized using MERT (Och, 2003). The Moses framework allows us to easily conduct experiments with several settings and find the optimal one for our task at hand. We experiment with varying context sizes for phrase table and language model, additional features and different scoring methods, e.g. BLEU (Papineni et al., 2002) in comparison to directly maximizing accuracy on the dev set. A more detailed description of those experiments including results will follow in Section 5. Evaluation data. In order to evaluate our proposed framework, we extracted 10,000 query pairs from a one week period not part of the training data. The</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miles Osborne</author>
<author>Ashwin Lall</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Exponential reservoir sampling for streaming language models.</title>
<date>2014</date>
<booktitle>In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>687--692</pages>
<location>Baltimore, Maryland.</location>
<marker>Osborne, Lall, Van Durme, 2014</marker>
<rawString>Miles Osborne, Ashwin Lall, and Benjamin Van Durme. 2014. Exponential reservoir sampling for streaming language models. In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 687–692, Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="19913" citStr="Papineni et al., 2002" startWordPosition="3270" endWordPosition="3273">ment process. We use Moses (Koehn et al., 2007) for standard phrase extraction, building KenLM language models (Heafield, 2011) and tuning. The standard set of features is used, including a phrase model, word lexicon model, length penalty, jump penalty and a language model. The model weights are optimized using MERT (Och, 2003). The Moses framework allows us to easily conduct experiments with several settings and find the optimal one for our task at hand. We experiment with varying context sizes for phrase table and language model, additional features and different scoring methods, e.g. BLEU (Papineni et al., 2002) in comparison to directly maximizing accuracy on the dev set. A more detailed description of those experiments including results will follow in Section 5. Evaluation data. In order to evaluate our proposed framework, we extracted 10,000 query pairs from a one week period not part of the training data. The initial size of 3.5M query pairs was reduced to 10k by exponential reservoir sampling (Osborne et al., 2014) after sorting by frequency. The result is a set of representative query pairs that focuses more on frequent misspellings, but also contains rare queries from the tail of the distribut</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Filip Radlinski</author>
<author>Thorsten Joachims</author>
</authors>
<title>Query chains: Learning to rank from implicit feedback.</title>
<date>2005</date>
<booktitle>In Proc. of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining,</booktitle>
<pages>239--248</pages>
<location>Chicago, IL.</location>
<contexts>
<context position="5728" citStr="Radlinski and Joachims, 2005" startWordPosition="909" endWordPosition="912">Our method differs in several ways. First, we consider full query pairs as training data, and do not use single words as primary mode of operation. Second, we do not train explicit error models P(wIs) for words w and observed corrections s, but use standard phrase-based machine translation modeling to derive phrase and lexical translation models. Although our level of context is shorter, especially for long words, the system automatically uses cross-word level context. The idea of using consecutive user queries from a stream of events to improve ranking of web search results was described in (Radlinski and Joachims, 2005). The authors introduce the notion of query chains that take advantage of users reformulating their queries as a means to learn better ranking functions. The classification of query chains is performed by support vector machines, and its training data is generated in a supervised fashion by manual inspection and annotation. In contrast, we do not manually annotate any of our training data. Since our initial sets are quite noisy, we apply a couple of heuristics that try to produce a cleaner subset of the data that contains mostly misspelled queries and their potential correction candidates. Ano</context>
</contexts>
<marker>Radlinski, Joachims, 2005</marker>
<rawString>Filip Radlinski and Thorsten Joachims. 2005. Query chains: Learning to rank from implicit feedback. In Proc. of the Eleventh ACM SIGKDD International Conference on Knowledge Discovery in Data Mining, pages 239–248, Chicago, IL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Jianfeng Gao</author>
<author>Daniel Micol</author>
<author>Chris Quirk</author>
</authors>
<title>Learning phrase-based spelling error models from clickthrough data.</title>
<date>2010</date>
<booktitle>In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>266--274</pages>
<location>Uppsala,</location>
<contexts>
<context position="7408" citStr="Sun et al., 2010" startWordPosition="1188" endWordPosition="1191">the potential to learn corrections of new and rare terms that will not be produced by an automatic spelling correction. In (Zhang et al., 2006), the authors use a conventional spellchecker to correct web queries through additional reranking of its output by a ranking SVM. The training data is in part automatically extracted, but also contains manually annotated pairs and, thus, is a semi-supervised approach. In this paper, we use an unsupervised approach to generate training data. Query spelling correction that is based on click-through data and uses a phrase-based error model is reported in (Sun et al., 2010). Our models operate on character sequences instead of words, and we do not observe issues with identity transformations (i.e. non-corrections for correctly spelled input). In (Cucerzan and Brill, 2004), the authors investigate a transformation method that corrects unlikely queries into more likely variants based on web query logs. The iterative approach transforms a search query based on word uni- and bigram decompositions, and the authors evaluate on both a large set that contains around 17% misspelled queries and a smaller set that is based on successive user-reformulated similar queries, a</context>
</contexts>
<marker>Sun, Gao, Micol, Quirk, 2010</marker>
<rawString>Xu Sun, Jianfeng Gao, Daniel Micol, and Chris Quirk. 2010. Learning phrase-based spelling error models from clickthrough data. In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics, pages 266–274, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jörg Tiedemann</author>
</authors>
<title>Character-based pivot translation for under-resourced languages and domains.</title>
<date>2012</date>
<booktitle>In Proc. of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>141--151</pages>
<location>Avignon, France.</location>
<contexts>
<context position="9016" citStr="Tiedemann, 2012" startWordPosition="1441" endWordPosition="1443"> et al., 2013). The authors compare a distance-based approach including a language model, a confusion network-based approach, a translation approach through a heuristically defined phrase table coding all character transformations, and a character-based machine translation approach using standard procedures (automatic word alignment and phrase extraction). The training data is manually created in contrast to our work where we automatically bootstrap training data from query logs. Research has also been done for translation of closely related languages (e.g. (Vilar et al., 2007) and (Nakov and Tiedemann, 2012)) and transliteration (e.g. (Deselaers et al., 2009)). An early summary paper with various spelling-related problems (non-word error detection, isolated-word error correction, and contextdependent word correction) can be found in (Kukich, 1992). 2 Extraction of training data We use event logs that track user interactions on an e-commerce web site. Each action of a user visiting the site is stored in a data warehouse and HDFS (Hadoop Distributed File System). We store several billion of these user records on a daily basis. The setup allows us to efficiently process large amounts of data points </context>
<context position="18764" citStr="Tiedemann, 2012" startWordPosition="3088" endWordPosition="3089">trained on actual search results in the future. 4 Autocorrection framework We cast the autocorrection task into characterbased statistical machine translation. For this, we prepare the data by splitting words into sequences of lowercased characters and use a special character to mark whitespace that indicates word boundaries. Once these sequences of characters are corrected, i.e. translated, they are merged back to the full word forms. Table 3 shows an example search query being preprocessed, translated and postprocessed. We use bigram characters instead of single characters, as suggested in (Tiedemann, 2012), in order to improve the statistical alignment models and make them more expressive. For training the autocorrection system we use basic methods and open-source tools for statistical 2https://code.google.com/p/cld2 machine translation. The character alignment is obtained by using GIZA++ (Och and Ney, 2003) for 4, 3 and 2 iterations of IBM Model 1, HMM, and IBM Model 3, respectively. As opposed to the standard machine translation task, we did not observe improvements from IBM Model 4 and do not use it as part of the alignment process. We use Moses (Koehn et al., 2007) for standard phrase extra</context>
</contexts>
<marker>Tiedemann, 2012</marker>
<rawString>Jörg Tiedemann. 2012. Character-based pivot translation for under-resourced languages and domains. In Proc. of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 141–151, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jan-Thorsten Peter</author>
<author>Hermann Ney</author>
</authors>
<title>Can we translate letters?</title>
<date>2007</date>
<booktitle>In Second Workshop on Statistical Machine Translation,</booktitle>
<pages>33--39</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="8984" citStr="Vilar et al., 2007" startWordPosition="1434" endWordPosition="1437">2), (Liu et al., 452 2013) or (Chiu et al., 2013). The authors compare a distance-based approach including a language model, a confusion network-based approach, a translation approach through a heuristically defined phrase table coding all character transformations, and a character-based machine translation approach using standard procedures (automatic word alignment and phrase extraction). The training data is manually created in contrast to our work where we automatically bootstrap training data from query logs. Research has also been done for translation of closely related languages (e.g. (Vilar et al., 2007) and (Nakov and Tiedemann, 2012)) and transliteration (e.g. (Deselaers et al., 2009)). An early summary paper with various spelling-related problems (non-word error detection, isolated-word error correction, and contextdependent word correction) can be found in (Kukich, 1992). 2 Extraction of training data We use event logs that track user interactions on an e-commerce web site. Each action of a user visiting the site is stored in a data warehouse and HDFS (Hadoop Distributed File System). We store several billion of these user records on a daily basis. The setup allows us to efficiently proce</context>
</contexts>
<marker>Vilar, Peter, Ney, 2007</marker>
<rawString>David Vilar, Jan-Thorsten Peter, and Hermann Ney. 2007. Can we translate letters? In Second Workshop on Statistical Machine Translation, pages 33– 39, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
<author>Ben Hutchinson</author>
<author>Grace Y Chung</author>
<author>Ged Ellis</author>
</authors>
<title>Using the Web for language independent spellchecking and autocorrection.</title>
<date>2009</date>
<booktitle>In Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>890--899</pages>
<contexts>
<context position="4615" citStr="Whitelaw et al., 2009" startWordPosition="725" endWordPosition="728">nes quality based on retrieved search results. We show examples indicating that our method can handle both corrections of misspelled queries and queries with segmentation issues (i.e. missing whitespace delimiters). A summary can be found in Section 6. To our knowledge, this is the first work that uses character-based machine translation technology on user-generated data for spelling correction. Moreover, it is the first to evaluate the performance in an e-commerce setting with there relevant measures. 1.1 Related Work One of the more prominent papers on autocorrection of misspelled input is (Whitelaw et al., 2009). The three-step approach incorporates a classification step that determines whether a word is misspelled, computes the most likely correction candidate and then, again, classifies whether this candidate is likely to be correct. An error model based after (Brill and Moore, 2000) is trained on similar word pairs extracted from large amounts of web sites, and a language model is used to disambiguate correction candidates based on left and right context around the current position. Our method differs in several ways. First, we consider full query pairs as training data, and do not use single word</context>
</contexts>
<marker>Whitelaw, Hutchinson, Chung, Ellis, 2009</marker>
<rawString>Casey Whitelaw, Ben Hutchinson, Grace Y Chung, and Ged Ellis. 2009. Using the Web for language independent spellchecking and autocorrection. In Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 890–899, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Zhang</author>
<author>Pilian He</author>
<author>Wei Xiang</author>
<author>Mu Li</author>
</authors>
<title>Discriminative reranking for spelling correction.</title>
<date>2006</date>
<booktitle>In Proc. of the 20th Pacific Asia Conference on Language, Information and Computation,</booktitle>
<pages>64--71</pages>
<location>Wuhan, China.</location>
<contexts>
<context position="6934" citStr="Zhang et al., 2006" startWordPosition="1110" endWordPosition="1113">ndidates. Another focus of researchers was specifically to tackle misspelled web search queries and use search engine logs for training and evaluation data (Gao et al., 2010), which differs from our work by collecting data using “click-through” enforcement. The user is presented with a spelling correction, and if she clicks on it, they learn that the correction is valid. Our method does not need a previous spelling correction to work. In addition, our proposed method has the potential to learn corrections of new and rare terms that will not be produced by an automatic spelling correction. In (Zhang et al., 2006), the authors use a conventional spellchecker to correct web queries through additional reranking of its output by a ranking SVM. The training data is in part automatically extracted, but also contains manually annotated pairs and, thus, is a semi-supervised approach. In this paper, we use an unsupervised approach to generate training data. Query spelling correction that is based on click-through data and uses a phrase-based error model is reported in (Sun et al., 2010). Our models operate on character sequences instead of words, and we do not observe issues with identity transformations (i.e.</context>
</contexts>
<marker>Zhang, He, Xiang, Li, 2006</marker>
<rawString>Yang Zhang, Pilian He, Wei Xiang, and Mu Li. 2006. Discriminative reranking for spelling correction. In Proc. of the 20th Pacific Asia Conference on Language, Information and Computation, pages 64–71, Wuhan, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000026">
<title confidence="0.931051">
LCSTS: A Large Scale Chinese Short Text Summarization Dataset
</title>
<author confidence="0.990341">
Baotian Hu Qingcai Chen Fangze Zhu
</author>
<affiliation confidence="0.9686265">
Intelligent Computing Research Center
Harbin Institute of Technology, Shenzhen Graduate School
</affiliation>
<email confidence="0.998645">
{baotianchina,qingcai.chen, zhufangze123}@gmail.com
</email>
<sectionHeader confidence="0.993895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948521739131">
Automatic text summarization is widely
regarded as the highly difficult problem,
partially because of the lack of large
text summarization data set. Due to the
great challenge of constructing the large
scale summaries for full text, in this pa-
per, we introduce a large corpus of Chi-
nese short text summarization dataset con-
structed from the Chinese microblogging
website Sina Weibo, which is released to
the public1. This corpus consists of over
2 million real Chinese short texts with
short summaries given by the author of
each text. We also manually tagged the
relevance of 10,666 short summaries with
their corresponding short texts. Based on
the corpus, we introduce recurrent neural
network for the summary generation and
achieve promising results, which not only
shows the usefulness of the proposed cor-
pus for short text summarization research,
but also provides a baseline for further re-
search on this topic.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998777714285714">
Nowadays, individuals or organizations can eas-
ily share or post information to the public on the
social network. Take the popular Chinese mi-
croblogging website (Sina Weibo) as an example,
the People’s Daily, one of the media in China,
posts more than tens of weibos (analogous to
tweets) each day. Most of these weibos are well-
written and highly informative because of the text
length limitation (less than140 Chinese charac-
ters). Such data is regarded as naturally annotated
web resources (Sun, 2011). If we can mine these
high-quality data from these naturally annotated
web resources, it will be beneficial to the research
that has been hampered by the lack of data.
</bodyText>
<footnote confidence="0.956347">
1http://icrc.hitsz.edu.cn/Article/show/139.html
</footnote>
<figureCaption confidence="0.999829">
Figure 1: A Weibo Posted by People’s Daily.
</figureCaption>
<bodyText confidence="0.999948181818182">
In the Natural Language Processing (NLP)
community, automatic text summarization is a hot
and difficult task. A good summarization system
should understand the whole text and re-organize
the information to generate coherent, informative,
and significantly short summaries which convey
important information of the original text (Hovy
and Lin, 1998), (Martins, 2007). Most of tradi-
tional abstractive summarization methods divide
the process into two phrases (Bing et al., 2015).
First, key textual elements are extracted from the
original text by using unsupervised methods or lin-
guistic knowledge. And then, unclear extracted
components are rewritten or paraphrased to pro-
duce a concise summary of the original text by
using linguistic rules or language generation tech-
niques. Although extensive researches have been
done, the linguistic quality of abstractive sum-
mary is still far from satisfactory. Recently, deep
learning methods have shown potential abilities
to learn representation (Hu et al., 2014; Zhou et
al., 2015) and generate language (Bahdanau et al.,
2014; Sutskever et al., 2014) from large scale data
by utilizing GPUs. Many researchers realize that
we are closer to generate abstractive summariza-
tions by using the deep learning methods. How-
ever, the publicly available and high-quality large
scale summarization data set is still very rare and
not easy to be constructed manually. For exam-
ple, the popular document summarization dataset
DUC2, TAC3 and TREC4 have only hundreds of
human written English text summarizations. The
problem is even worse for Chinese. In this pa-
</bodyText>
<footnote confidence="0.999978666666667">
2http://duc.nist.gov/data.html
3http://www.nist.gov/tac/2015/KBP/
4http://trec.nist.gov/
</footnote>
<page confidence="0.871445">
1967
</page>
<note confidence="0.866278">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1967–1972,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<subsectionHeader confidence="0.37254">
Social Media
</subsectionHeader>
<figureCaption confidence="0.996693">
Figure 2: Diagram of the process for creating the dataset.
</figureCaption>
<figure confidence="0.982082461538462">
Users Collection
Seeds
User
crawler
selecting
Chosen Users
Text
crawler
Raw Text
filtering,
cleaning and
extracting
Data set
</figure>
<bodyText confidence="0.999900083333333">
per, we take one step back and focus on construct-
ing LCSTS, the Large-scale Chinese Short Text
Summarization dataset by utilizing the naturally
annotated web resources on Sina Weibo. Figure 1
shows one weibo posted by the People’s Daily. In
order to convey the import information to the pub-
lic quickly, it also writes a very informative and
short summary (in the blue circle) of the news.
Our goal is to mine a large scale, high-quality short
text summarization dataset from these texts.
This paper makes the following contributions:
(1) We introduce a large scale Chinese short text
summarization dataset. To our knowledge, it is
the largest one to date; (2) We provide standard
splits for the dataset into large scale training set
and human labeled test set which will be easier for
benchmarking the related methods; (3) We explore
the properties of the dataset and sample 10,666
instances for manually checking and scoring the
quality of the dataset; (4) We perform recurrent
neural network based encoder-decoder method on
the dataset to generate summary and get promis-
ing results, which can be used as one baseline of
the task.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999887375">
Our work is related to recent works on automatic
text summarization and natural language process-
ing based on naturally annotated web resources,
which are briefly introduced as follows.
Automatic Text Summarization in some form
has been studied since 1950. Since then, most re-
searches are related to extractive summarizations
by analyzing the organization of the words in the
document (Nenkova and McKeown, 2011) (Luhn,
1998); Since it needs labeled data sets for su-
pervised machine learning methods and labeling
dataset is very intensive, some researches focused
on the unsupervised methods (Mihalcea, 2004).
The scale of existing data sets are usually very
small (most of them are less than 1000). For
example, DUC2002 dataset contains 567 docu-
ments and each document is provided with two
100-words human summaries. Our work is also
related to the headline generation, which is a task
to generate one sentence of the text it entitles.
Colmenares et.al construct a 1.3 million financial
news headline dataset written in English for head-
line generation (Colmenares et al., 2015). How-
ever, the data set is not publicly available.
</bodyText>
<subsectionHeader confidence="0.568173">
Naturally Annotated Web Resources based
</subsectionHeader>
<bodyText confidence="0.999880823529412">
Natural Language Processing is proposed by
Sun (Sun, 2011). Naturally Annotated Web Re-
sources is the data generated by users for commu-
nicative purposes such as web pages, blogs and
microblogs. We can mine knowledge or useful
data from these raw data by using marks generated
by users unintentionally. Jure et.al track 1.6 mil-
lion mainstream media sites and blogs and mine a
set of novel and persistent temporal patterns in the
news cycle (Leskovec et al., 2009). Sepandar et.al
use the users’ naturally annotated pattern ‘we feel’
and ‘i feel’ to extract the ‘Feeling’ sentence collec-
tion which is used to collect the world’s emotions.
In this work, we use the naturally annotated re-
sources to construct the large scale Chinese short
text summarization data to facilitate the research
on text summarization.
</bodyText>
<sectionHeader confidence="0.988373" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.999977">
A lot of popular Chinese media and organizations
have created accounts on the Sina Weibo. They
use their accounts to post news and information.
These accounts are verified on the Weibo and la-
beled by a blue ‘V’. In order to guarantee the qual-
ity of the crawled text, we only crawl the verified
organizations’ weibos which are more likely to be
clean, formal and informative. There are a lot of
human intervention required in each step. The pro-
cess of the data collection is shown as Figure 2 and
</bodyText>
<page confidence="0.973656">
1968
</page>
<bodyText confidence="0.999815944444445">
summarized as follows:
1) We first collect 50 very popular organiza-
tion users as seeds. They come from the domains
of politic, economic, military, movies, game and
etc, such as People’s Daily, the Economic Observe
press, the Ministry of National Defense and etc. 2)
We then crawl fusers followed by these seed users
and filter them by using human written rules such
as the user must be blue verified, the number of
followers is more than 1 million and etc. 3) We
use the chosen users and text crawler to crawl their
weibos. 4) we filter, clean and extract (short text,
summary) pairs. About 100 rules are used to ex-
tract high quality pairs. These rules are concluded
by 5 peoples via carefully investigating of the raw
text. We also remove those paris, whose short text
length is too short (less than 80 characters) and
length of summaries is out of [10,30].
</bodyText>
<sectionHeader confidence="0.994402" genericHeader="method">
4 Data Properties
</sectionHeader>
<bodyText confidence="0.956957954545455">
The dataset consists of three parts shown as Ta-
ble 1 and the length distributions of texts are
shown as Figure 3.
Part I is the main content of LCSTS that con-
tains 2,400,591 (short text, summary) pairs. These
pairs can be used to train supervised learning
model for summary generation.
Part II contains the 10,666 human labeled
(short text, summary) pairs with the score ranges
from 1 to 5 that indicates the relevance between
the short text and the corresponding summary. ‘1’
denotes “ the least relevant ” and ‘5’ denotes “the
most relevant”. For annotating this part, we recruit
5 volunteers, each pair is only labeled by one an-
notator. These pairs are randomly sampled from
Part I and are used to analysize the distribution of
pairs in the Part I. Figure 4 illustrates examples of
different scores. From the examples we can see
that pairs scored by 3, 4 or 5 are very relevant to
the corresponding summaries. These summaries
are highly informative, concise and significantly
short compared to original text. We can also see
that many words in the summary do not appear
in the original text, which indicates the significant
difference of our dataset from sentence compres-
sion datasets. The summaries of pairs scored by
1 or 2 are highly abstractive and relatively hard to
conclude the summaries from the short text. They
are more likely to be headlines or comments in-
stead of summaries. The statistics show that the
percent of score 1 and 2 is less than 20% of the
Figure 3: Box plot of lengths for short text(ST),
segmented short text(Segmented ST), sum-
mary(SUM) and segmented summary(Segmented
SUM). The red line denotes the median, and the
edges of the box the quartiles.
data, which can be filtered by using trained classi-
fier.
Part III contains 1,106 pairs. For this part, 3
annotators label the same 2000 texts and we ex-
tract the text with common scores. This part is
independent from Part I and Part II. In this work,
we use pairs scored by 3, 4 and 5 of this part as the
test set for short text summary generation task.
</bodyText>
<table confidence="0.999872076923077">
Part I 2,400,591
Part II Number of Pairs 10,666
Human Score 1 942
Human Score 2 1,039
Human Score 3 2,019
Human Score 4 3,128
Human Score 5 3,538
Part III Number of Pairs 1,106
Human Score 1 165
Human Score 2 216
Human Score 3 227
Human Score 4 301
Human Score 5 197
</table>
<tableCaption confidence="0.995226">
Table 1: Data Statistics
</tableCaption>
<sectionHeader confidence="0.996483" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<bodyText confidence="0.999328777777778">
Recently, recurrent neural network (RNN) have
shown powerful abilities on speech recogni-
tion (Graves et al., 2013), machine transla-
tion (Sutskever et al., 2014) and automatic dialog
response (Shang et al., 2015). However, there is
rare research on the automatic text summarization
by using deep models. In this section, we use RNN
as encoder and decoder to generate the summary
of short text. We use the Part I as the training set
</bodyText>
<page confidence="0.961432">
1969
</page>
<table confidence="0.999921620689655">
Short Text: -i&apos;jC- =.lalDF&apos;,t+&amp;A$E����BH�+&amp;quot;*I
~~~1-=.4/��17*�(C�3#@�561!)�(C�3
&gt;?561!)a./s—�&gt;?561J�%,�IfrAt—��0-I2A8[�J�c
1G ,�,J�A8-=.;&lt;�&gt;FA-:&amp;quot;f1 %tta
Mingzhong Chen, the Chief Secretary of the Water Devision of the Ministry of
Water Resources, revealed today at a press conference, according to the just&lt;
completed assessment of water resources management system, some
provinces are closed to the red line indicator, some provinces are over the red
line indicator. In some places over the red line,It will enforce regional
approval restrictions on some water projects, implement strictly water
resources assessmentandthe approval of water licensing.
Summarization: CST3&gt;?*AL0-56!) A-I2~9G
Some provinces exceeds the red line indicator of annual water using, some
water project will be. limited approved
Human Score: 5
Short Text: 1$SM215C ,� 76&amp;P&apos;J@&amp;quot;K�]=#)FQ16.95%H
78.1�D��&amp;quot;A7�9�L8%4(&amp;quot;K��B&apos;R�!3J:7��
&gt;i�-Oq*0dt,••TG—ri(&amp;quot;KB�!3J:/N,-EfNc��?7;
1BF,��
According to China’s Ministry of Commerce, China’s actually utilized foreign
capital in Julyfell sharply about 16.95% to 7.81 billion dollars, comparing to last
year. Analysis of the outside world believe that it is related to the recent official
intensive antitrust investigation. DanyangShen responded, “It can not be linked
to the antitrust investigation of foreign investment, or do other unfounded
association”
Summarization: ��MI+�!3���&lt;$��.&amp;quot;����
China‘s Ministry of Commerce respond to antitrust investigation: Several cases
will not scare foreign investors away.
Human Score: 1
</table>
<figureCaption confidence="0.993429">
Figure 4: Five examples of different scores.
</figureCaption>
<bodyText confidence="0.974180166666667">
and the subset of Part III, which is scored by 3, 4
and 5, as test set.
Two approaches are used to preprocess the data:
1) character-based method, we take the Chinese
character as input, which will reduce the vocab-
ulary size to 4,000. 2) word-based method, the
</bodyText>
<figureCaption confidence="0.9380705">
Figure 5: The graphical depiction of RNN encoder
and decoder framework without context.
Figure 6: The graphical depiction of the RNN en-
coder and decoder framework with context.
</figureCaption>
<bodyText confidence="0.9997766">
text is segmented into Chinese words by using
jieba5. The vocabulary is limited to 50,000. We
adopt two deep architectures, 1) The local con-
text is not used during decoding. We use the
RNN as encoder and it’s last hidden state as the
input of decoder, as shown in Figure 5; 2) The
context is used during decoding, following (Bah-
danau et al., 2014), we use the combination of
all the hidden states of encoder as input of the
decoder, as shown in Figure 6. For the RNN,
we adopt the gated recurrent unit (GRU) which is
proposed by (Chung et al., 2015) and has been
proved comparable to LSTM (Chung et al., 2014).
All the parameters (including the embeddings) of
the two architectures are randomly initialized and
ADADELTA (Zeiler, 2012) is used to update the
learning rate. After the model is trained, the beam
search is used to generate the best summaries in
the process of decoding and the size of beam is set
to 10 in our experiment.
</bodyText>
<footnote confidence="0.621296">
5https://pypi.python.org/pypi/jieba/
</footnote>
<figure confidence="0.9214296">
Short Text: �&amp;RI&lt;:75�8@�E�.AO-30%V4T,0(?&gt;PC8&lt;/
7%4*B�]Ccc= o+-gpEQFGO2O#J*,5-48!&amp;%iFGO2O3
&apos;2$A&amp;quot;)fJ,�!19�H�=��9�H����9�#D�
A
6�,=V9JA#O2O#J*; ,*.
</figure>
<figureCaption confidence="0.669793">
Groupons’ sales on mobile terminals are below 30 percent. User’s preference of
shopping through PCs can not be changed in the short term. In the future
Chinese O2O catering market, mobile terminals will become the strategic
development direction. And also, it will become offDline driving from onDline
driving. The first and second tier cities are facing growth difficulties. However,
O2O market in the third and fourth tier cities contains opportunities.
</figureCaption>
<figure confidence="0.791700428571429">
Summarization: 5.48&amp;FGO2O3&apos;2$A&amp;quot;)V7
The mobile terminals will become catering’s strategic development direction.
Human Score: 4
Short Text: 7,4��t)%�$&amp;quot;A�.AI10347it/&amp;quot;*5,3/-L0
0.87%,9~#6,~-?714�,3/�0�$`p,$M,4h-4r,1JIT],r#J
�0!�&lt;&gt;10%���&amp;��:����6� ��A8�&gt;�;2�&apos;
~+0@=~
</figure>
<bodyText confidence="0.5879336">
In July, 1002cities’ average newly2built house prices is 10347 yuan per square,
which rose 0.87%. It rises for the 14th consecutive month. Among them,
Guangzhou, Beijing, Shenzhen, Nanjing rise more than 10%. Dawei Zhang, from
Centaline Property Agency, said that because the first and second2tier city
gatherstoo many resources, the price of house is likelyto rise and hard to fall.
</bodyText>
<table confidence="0.750715375">
Summarization: 4J&gt;&amp;&apos;*3/&amp;quot;14?0&amp;quot;��6� 0!(�
1002cities’ house prices gain “14th consecutive rising”, the first and second2tier
cities rise more.
Human Score: 3
Short Text: A:/3k1,2009*;g`,8B6&amp;quot;!A DV�,;� D
2+),2014*5+6(72014050- D!-€C4,j,).8B *t6&amp;quot;th#
AfA1&apos;3B9!hTQA,34,E@&amp;H�*I�&lt;5&apos;%�� D�A5
B�.?F���
</table>
<bodyText confidence="0.9261455">
Reporters combed the information and found, from 2009 to now, there are at
least 8 lottery delayed events and the delayed time are more than 2 hours. On
May 6, 2014, the No. 2014050 delay more than 4 hours. The center of welfare
lottery only respond to 3 of the 8 event. Their explanations are that a
communications breakdown and heavy rain led to a data upload extension.
There are no explanations for other 5 delay events.
</bodyText>
<figure confidence="0.919243625">
Summarization:=GX=2!A D:!AYJf-T&gt;8&apos;%0$?
Ask about the lottery delaythird times: why lottery should wait data collection?
Human Score: 2
(y1, y2, ..., yn)
x1 x2 xt71 xt
h0
GRU GRU
h1 h2
...
GRU GRU
ht71
RNN Decoder
ht
(y1, y2, ..., yn)
RNN Decoder
Context Generator
h1 h2
ht-1
ht
h0
GRU GRU
�
GRU GRU
x1 x2 xt-1 xt
</figure>
<page confidence="0.90322">
1970
</page>
<table confidence="0.9976912">
model data R-1 R-2 R-L
RNN Word 0.043 0.025 0.043
Char 0.061 0.028 0.057
RNN context Word 0.087 0.054 0.085
Char 0.108 0.073 0.107
</table>
<tableCaption confidence="0.950676666666667">
Table 2: The experiment result: “Word” and
“Char” denote the word-based and character-
based input respectively.
</tableCaption>
<figure confidence="0.904632142857143">
Short Text: 9230,+Hat0ltJ9;iH:?5F!fc&apos;T(2014&apos;2015)
~7=~~- ���8#GA�)�&amp;3J�,��71�=��;�%�
/~&amp;quot;~B~~~&gt;
~~&gt;~~~~~.~&gt;28 �� I&lt;~%~.~
1K.
On September 3, the Geneva&apos;based World Economic Forum released “ The
Global Competitiveness Report 2014&apos;2015 ”. Switzerland topped the list for six
consecutive years , becoming the world‘s most competitive country. Singapore
and the United States are in the second and third place respectively. China is in
the 28th place, ranking highest among the BRICcountries.
Human: k7=+b.D3 tCR&amp;2 8 a &amp;I&lt;EQ%Ja
the Global competitiveness ranking list, China is in the 28th place, the highest
among BRIC countries.
RNN+Char:8#+@92300 ip:+@J4 �C*(8#+E2012)
&gt;~~&gt;~$C~~8#-~~7&gt;~$~%5000~B~
It is not a fluent sentence and can not be translated. There are only some
segments related to original text.
RNN+Word: 7=+l.�:8#&amp;�7&gt;3��CN&gt;1a&gt;1a&gt;1atM&gt;1
~8#&gt;1~&gt;1
Global competitiveness ranking: Switzerland ranks 3rd place, China 1st place, 1st
place, China 1st place, Switzerland 1st place, first
RNN+Context+Char: 47=�jl.���.�&gt;28�I&lt;�1K
In the Global competitiveness ranking list, China is in the 28th place which is
highestamong the BRICcountries.
RNN+Context+Word: 47=$jj-adt6��. 28~I&lt;~%~.�
&gt;— ( &gt;)L
“The Global Competitiveness Report” is released. China is in the 28th place,
rank highest among the BRICcountries (ninth
</figure>
<figureCaption confidence="0.999788">
Figure 7: An example of the generated summaries.
</figureCaption>
<bodyText confidence="0.938336363636364">
For evaluation, we adopt the ROUGE met-
rics (Lin, 2004) proposed by (Lin and Hovy,
2003), which have been proved strongly correlated
with human evaluations. ROUGE-1, ROUGE-
2 and ROUGE-L are used. All the models are
trained on the GPUs tesla M2090 for about one
week.Table 2 lists the experiment results. As we
can see in Figure 7, the summaries generated by
RNN with context are very close to human written
summaries, which indicates that if we feed enough
data to the RNN encoder and decoder, it may gen-
erate summary almost from scratch.
The results also show that the RNN with con-
text outperforms RNN without context on both
character and word based input. This result indi-
cates that the internal hidden states of the RNN
encoder can be combined to represent the context
of words in summary. And also the performances
of the character-based input outperform the word-
based input. As shown in Figure 8, the summary
generated by RNN with context by inputing the
character-based short text is relatively good, while
the the summary generated by RNN with context
Short Text: $T, R&lt;Q,=20 $/.) is1DF,&amp;quot;&amp;in#+,M
$���LP;$H������GN��.4*���F@(K4�U�
�56I�94�%-�8&amp;quot;/T���2!�E�B�%-~8&amp;quot;~S
T, A�O3T:&apos;Ja
On The factory’s door is locked. About 20 works are scattered to sit under the
shade. “We are ordinary workers, we are waiting for our salary here.” one of
them said. On the morning of July 4th, reporters arrived at Shenzhen
Yuanjing Photoelectron Corporation,located on Qinghu Road, Longhua
District, Shenzhen. Just as the rumor, Yuanjing Photoelectron Corporation is
closed down and the large shareholder Xing Yi is missing.
</bodyText>
<figure confidence="0.761995428571429">
Human: 4���&gt;LED���S7*F$/•C;?0
Hundred-million-yuan-class LED enterprise is closed down and workers wait for
the boss under the under the scorching sun.
RNN+Context+Char: 4J)11%-k8&amp;quot;*1ST
Shenzhen Yuanjing PhotoElectron Corporation is closed down.
RNN+Context+Word: »g3nl UNK UNK UNK UNK UNK UNK UNK UNK UNK UNK
UNK UNKShenzhen UNK UNKUNK UNKUNK UNK UNK UNKUNK UNKUNK
</figure>
<figureCaption confidence="0.983257">
Figure 8: An example of the generated summaries
with UNKs.
</figureCaption>
<bodyText confidence="0.999921625">
on word-based input contains many UNKs. This
may attribute to that the segmentation may lead to
many UNKs in the vocabulary and text such as the
person name and organization name. For exam-
ple, “愿景光电子” is a company name which
is not in the vocabulary of word-based RNN, the
RNN summarizer has to use the UNKs to replace
the “愿景光电子” in the process of decoding.
</bodyText>
<sectionHeader confidence="0.996168" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999981882352941">
We constructed a large-scale Chinese short text
summarization dataset and performed RNN-based
methods on it, which achieved some promising re-
sults. This is just a start of deep models on this
task and there is much room for improvement. We
take the whole short text as one sequence, this may
not be very reasonable, because most of short texts
contain several sentences. A hierarchical RNN (Li
et al., 2015) is one possible direction. The rare
word problem is also very important for the gener-
ation of the summaries, especially when the input
is word-based instead of character-based. It is also
a hot topic in the neural generative models such
as neural translation machine(NTM) (Luong et al.,
2014), which can benefit to this task. We also plan
to construct a large document summarization data
set by using naturally annotated web resources.
</bodyText>
<sectionHeader confidence="0.998364" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.946779666666667">
This work is supported by National Natu-
ral Science Foundation of China: 61473101,
61173075 and 61272383, Strategic Emerg-
ing Industry Development Special Funds
of Shenzhen: JCYJ20140417172417105,
JCYJ20140508161040764 and
JCYJ20140627163809422. We thank to Baolin
Peng, Lin Ma, Li Yu and the anonymous reviewers
for their insightful comments.
</bodyText>
<page confidence="0.973377">
1971
</page>
<figure confidence="0.5011728">
Chin-Yew Lin and E.H. Hovy. 2003. Automatic eval-
uation of summaries using n-gram co-occurrence
statistics. In Proceedings of 2003 Language Tech-
nology Conference (HLT-NAACL 2003), Edmonton,
Canada.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
Bengio. 2014. Neural machine translation by
jointly learning to align and translate. CoRR,
abs/1409.0473.
</figure>
<reference confidence="0.999160698795181">
Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo,
and Rebecca Passonneau. 2015. Abstractive multi-
document summarization via phrase selection and
merging. In Proceedings of the ACL-IJCNLP, pages
1587–1597, Beijing, China, July. Association for
Computational Linguistics.
Junyoung Chung, C¸aglar G¨ulc¸ehre, KyungHyun Cho,
and Yoshua Bengio. 2014. Empirical evaluation of
gated recurrent neural networks on sequence model-
ing. CoRR, abs/1412.3555.
Junyoung Chung, C¸aglar G¨ulc¸ehre, KyungHyun Cho,
and Yoshua Bengio. 2015. Gated feedback recur-
rent neural networks. CoRR, abs/1502.02367.
Carlos A. Colmenares, Marina Litvak, Amin Mantrach,
and Fabrizio Silvestri. 2015. Heads: Headline
generation as sequence prediction using an abstract
feature-rich space. In Proceddings of 2015 Confer-
ence of the North American Chapter of the Asso-
ciation for Computational Linguistics–Human Lan-
guage Technologies (NAACL HLT 2015).
Alex Graves, Abdel-rahman Mohamed, and Geof-
frey E. Hinton. 2013. Speech recognition with deep
recurrent neural networks. CoRR, abs/1303.5778.
Eduard Hovy and Chin-Yew Lin. 1998. Automated
text summarization and the summarist system. In
Proceedings of a Workshop on Held at Baltimore,
Maryland: October 13-15, 1998, TIPSTER ’98,
pages 197–214, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network archi-
tectures for matching natural language sentences. In
Advances in Neural Information Processing Systems
27, pages 2042–2050. Curran Associates, Inc.
Jure Leskovec, Lars Backstrom, and Jon Kleinberg.
2009. Meme-tracking and the dynamics of the news
cycle. In Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge Discovery
and Data Mining, KDD ’09, pages 497–506.
Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015.
A hierarchical neural autoencoder for paragraphs
and documents. In Proceedings of ACL.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In In Proceedings
of Workshop on Text Summarization Branches Out,
Post-Conference Workshop ofACL 2004, Barcelona,
Spain.
H. P. Luhn. 1998. The automatic creation of literature
abstracts. IBM Journal of Research and Develop-
ment, 2(2):159–165.
Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol
Vinyals, and Wojciech Zaremba. 2014. Addressing
the rare word problem in neural machine translation.
CoRR, abs/1410.8206.
Dipanjan Das Andr F.T. Martins. 2007. A survey
on automatic text summarization. Technical report,
CMU.
Rada Mihalcea. 2004. Graph-based ranking algo-
rithms for sentence extraction, applied to text sum-
marization. In Proceedings of the 42nd Annual
Meeting of the Association for Computational Lin-
guistics, companion volume, Spain.
Ani Nenkova and Kathleen McKeown. 2011. Auto-
matic summarization. Foundations and Trend in In-
formation Retrieval, 5(2-3):103–233.
Lifeng Shang, Zhengdong Lu, and Hang Li. 2015.
Neural responding machine for short-text conversa-
tion. CoRR, abs/1503.02364.
Mao Song Sun. 2011. Natural language procesing
based on naturaly annotated web resources. Journal
of Chinese Information Processing, 25(6):26–32.
Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le.
2014. Sequence to sequence learning with neural
networks. In Advances in Neural Information Pro-
cessing Systems 27, pages 3104–3112.
Matthew D. Zeiler. 2012. ADADELTA: an adaptive
learning rate method. CoRR, abs/1212.5701.
Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou
Tang, and Xiaolong Wang. 2015. Answer sequence
learning with neural networks for answer selection
in community question answering. In Proceedings
of theACL-IJCNLP, pages 713–718, Beijing, China,
July. Association for Computational Linguistics.
</reference>
<page confidence="0.995869">
1972
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.947602">
<title confidence="0.999754">LCSTS: A Large Scale Chinese Short Text Summarization Dataset</title>
<author confidence="0.994288">Baotian Hu Qingcai Chen Fangze Zhu</author>
<affiliation confidence="0.9941385">Intelligent Computing Research Center Harbin Institute of Technology, Shenzhen Graduate</affiliation>
<abstract confidence="0.998492041666667">Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
</authors>
<title>Abstractive multidocument summarization via phrase selection and merging.</title>
<date>2015</date>
<booktitle>In Proceedings of the ACL-IJCNLP,</booktitle>
<pages>1587--1597</pages>
<location>Beijing, China,</location>
<marker>Passonneau, 2015</marker>
<rawString>and Rebecca Passonneau. 2015. Abstractive multidocument summarization via phrase selection and merging. In Proceedings of the ACL-IJCNLP, pages 1587–1597, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyoung Chung</author>
<author>C¸aglar G¨ulc¸ehre</author>
<author>KyungHyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Empirical evaluation of gated recurrent neural networks on sequence modeling.</title>
<date>2014</date>
<location>CoRR, abs/1412.3555.</location>
<marker>Chung, G¨ulc¸ehre, Cho, Bengio, 2014</marker>
<rawString>Junyoung Chung, C¸aglar G¨ulc¸ehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junyoung Chung</author>
<author>C¸aglar G¨ulc¸ehre</author>
<author>KyungHyun Cho</author>
<author>Yoshua Bengio</author>
</authors>
<title>Gated feedback recurrent neural networks.</title>
<date>2015</date>
<tech>CoRR, abs/1502.02367.</tech>
<marker>Chung, G¨ulc¸ehre, Cho, Bengio, 2015</marker>
<rawString>Junyoung Chung, C¸aglar G¨ulc¸ehre, KyungHyun Cho, and Yoshua Bengio. 2015. Gated feedback recurrent neural networks. CoRR, abs/1502.02367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos A Colmenares</author>
<author>Marina Litvak</author>
<author>Amin Mantrach</author>
<author>Fabrizio Silvestri</author>
</authors>
<title>Heads: Headline generation as sequence prediction using an abstract feature-rich space.</title>
<date>2015</date>
<booktitle>In Proceddings of 2015 Conference of the North American Chapter of the Association for Computational Linguistics–Human Language Technologies (NAACL HLT</booktitle>
<contexts>
<context position="6248" citStr="Colmenares et al., 2015" startWordPosition="956" endWordPosition="959"> sets for supervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods (Mihalcea, 2004). The scale of existing data sets are usually very small (most of them are less than 1000). For example, DUC2002 dataset contains 567 documents and each document is provided with two 100-words human summaries. Our work is also related to the headline generation, which is a task to generate one sentence of the text it entitles. Colmenares et.al construct a 1.3 million financial news headline dataset written in English for headline generation (Colmenares et al., 2015). However, the data set is not publicly available. Naturally Annotated Web Resources based Natural Language Processing is proposed by Sun (Sun, 2011). Naturally Annotated Web Resources is the data generated by users for communicative purposes such as web pages, blogs and microblogs. We can mine knowledge or useful data from these raw data by using marks generated by users unintentionally. Jure et.al track 1.6 million mainstream media sites and blogs and mine a set of novel and persistent temporal patterns in the news cycle (Leskovec et al., 2009). Sepandar et.al use the users’ naturally annota</context>
</contexts>
<marker>Colmenares, Litvak, Mantrach, Silvestri, 2015</marker>
<rawString>Carlos A. Colmenares, Marina Litvak, Amin Mantrach, and Fabrizio Silvestri. 2015. Heads: Headline generation as sequence prediction using an abstract feature-rich space. In Proceddings of 2015 Conference of the North American Chapter of the Association for Computational Linguistics–Human Language Technologies (NAACL HLT 2015).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Graves</author>
<author>Abdel-rahman Mohamed</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>Speech recognition with deep recurrent neural networks.</title>
<date>2013</date>
<tech>CoRR, abs/1303.5778.</tech>
<contexts>
<context position="10993" citStr="Graves et al., 2013" startWordPosition="1789" endWordPosition="1792">ract the text with common scores. This part is independent from Part I and Part II. In this work, we use pairs scored by 3, 4 and 5 of this part as the test set for short text summary generation task. Part I 2,400,591 Part II Number of Pairs 10,666 Human Score 1 942 Human Score 2 1,039 Human Score 3 2,019 Human Score 4 3,128 Human Score 5 3,538 Part III Number of Pairs 1,106 Human Score 1 165 Human Score 2 216 Human Score 3 227 Human Score 4 301 Human Score 5 197 Table 1: Data Statistics 5 Experiment Recently, recurrent neural network (RNN) have shown powerful abilities on speech recognition (Graves et al., 2013), machine translation (Sutskever et al., 2014) and automatic dialog response (Shang et al., 2015). However, there is rare research on the automatic text summarization by using deep models. In this section, we use RNN as encoder and decoder to generate the summary of short text. We use the Part I as the training set 1969 Short Text: -i&apos;jC- =.lalDF&apos;,t+&amp;A$E����BH�+&amp;quot;*I ~~~1-=.4/��17*�(C�3#@�561!)�(C�3 &gt;?561!)a./s—�&gt;?561J�%,�IfrAt—��0-I2A8[�J�c 1G ,�,J�A8-=.;&lt;�&gt;FA-:&amp;quot;f1 %tta Mingzhong Chen, the Chief Secretary of the Water Devision of the Ministry of Water Resources, revealed today at a press confer</context>
</contexts>
<marker>Graves, Mohamed, Hinton, 2013</marker>
<rawString>Alex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. 2013. Speech recognition with deep recurrent neural networks. CoRR, abs/1303.5778.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Chin-Yew Lin</author>
</authors>
<title>Automated text summarization and the summarist system.</title>
<date>1998</date>
<journal>TIPSTER</journal>
<booktitle>In Proceedings of a Workshop on Held at</booktitle>
<volume>98</volume>
<pages>197--214</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland:</location>
<contexts>
<context position="2296" citStr="Hovy and Lin, 1998" startWordPosition="344" endWordPosition="347">Sun, 2011). If we can mine these high-quality data from these naturally annotated web resources, it will be beneficial to the research that has been hampered by the lack of data. 1http://icrc.hitsz.edu.cn/Article/show/139.html Figure 1: A Weibo Posted by People’s Daily. In the Natural Language Processing (NLP) community, automatic text summarization is a hot and difficult task. A good summarization system should understand the whole text and re-organize the information to generate coherent, informative, and significantly short summaries which convey important information of the original text (Hovy and Lin, 1998), (Martins, 2007). Most of traditional abstractive summarization methods divide the process into two phrases (Bing et al., 2015). First, key textual elements are extracted from the original text by using unsupervised methods or linguistic knowledge. And then, unclear extracted components are rewritten or paraphrased to produce a concise summary of the original text by using linguistic rules or language generation techniques. Although extensive researches have been done, the linguistic quality of abstractive summary is still far from satisfactory. Recently, deep learning methods have shown pote</context>
</contexts>
<marker>Hovy, Lin, 1998</marker>
<rawString>Eduard Hovy and Chin-Yew Lin. 1998. Automated text summarization and the summarist system. In Proceedings of a Workshop on Held at Baltimore, Maryland: October 13-15, 1998, TIPSTER ’98, pages 197–214, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Baotian Hu</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
<author>Qingcai Chen</author>
</authors>
<title>Convolutional neural network architectures for matching natural language sentences.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27,</booktitle>
<pages>2042--2050</pages>
<publisher>Curran Associates, Inc.</publisher>
<contexts>
<context position="2952" citStr="Hu et al., 2014" startWordPosition="442" endWordPosition="445">stractive summarization methods divide the process into two phrases (Bing et al., 2015). First, key textual elements are extracted from the original text by using unsupervised methods or linguistic knowledge. And then, unclear extracted components are rewritten or paraphrased to produce a concise summary of the original text by using linguistic rules or language generation techniques. Although extensive researches have been done, the linguistic quality of abstractive summary is still far from satisfactory. Recently, deep learning methods have shown potential abilities to learn representation (Hu et al., 2014; Zhou et al., 2015) and generate language (Bahdanau et al., 2014; Sutskever et al., 2014) from large scale data by utilizing GPUs. Many researchers realize that we are closer to generate abstractive summarizations by using the deep learning methods. However, the publicly available and high-quality large scale summarization data set is still very rare and not easy to be constructed manually. For example, the popular document summarization dataset DUC2, TAC3 and TREC4 have only hundreds of human written English text summarizations. The problem is even worse for Chinese. In this pa2http://duc.ni</context>
</contexts>
<marker>Hu, Lu, Li, Chen, 2014</marker>
<rawString>Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai Chen. 2014. Convolutional neural network architectures for matching natural language sentences. In Advances in Neural Information Processing Systems 27, pages 2042–2050. Curran Associates, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jure Leskovec</author>
<author>Lars Backstrom</author>
<author>Jon Kleinberg</author>
</authors>
<title>Meme-tracking and the dynamics of the news cycle.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09,</booktitle>
<pages>497--506</pages>
<contexts>
<context position="6800" citStr="Leskovec et al., 2009" startWordPosition="1048" endWordPosition="1051">t written in English for headline generation (Colmenares et al., 2015). However, the data set is not publicly available. Naturally Annotated Web Resources based Natural Language Processing is proposed by Sun (Sun, 2011). Naturally Annotated Web Resources is the data generated by users for communicative purposes such as web pages, blogs and microblogs. We can mine knowledge or useful data from these raw data by using marks generated by users unintentionally. Jure et.al track 1.6 million mainstream media sites and blogs and mine a set of novel and persistent temporal patterns in the news cycle (Leskovec et al., 2009). Sepandar et.al use the users’ naturally annotated pattern ‘we feel’ and ‘i feel’ to extract the ‘Feeling’ sentence collection which is used to collect the world’s emotions. In this work, we use the naturally annotated resources to construct the large scale Chinese short text summarization data to facilitate the research on text summarization. 3 Data Collection A lot of popular Chinese media and organizations have created accounts on the Sina Weibo. They use their accounts to post news and information. These accounts are verified on the Weibo and labeled by a blue ‘V’. In order to guarantee t</context>
</contexts>
<marker>Leskovec, Backstrom, Kleinberg, 2009</marker>
<rawString>Jure Leskovec, Lars Backstrom, and Jon Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’09, pages 497–506.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
<author>Minh-Thang Luong</author>
<author>Dan Jurafsky</author>
</authors>
<title>A hierarchical neural autoencoder for paragraphs and documents.</title>
<date>2015</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="21023" citStr="Li et al., 2015" startWordPosition="3306" endWordPosition="3309"> For example, “愿景光电子” is a company name which is not in the vocabulary of word-based RNN, the RNN summarizer has to use the UNKs to replace the “愿景光电子” in the process of decoding. 6 Conclusion and Future Work We constructed a large-scale Chinese short text summarization dataset and performed RNN-based methods on it, which achieved some promising results. This is just a start of deep models on this task and there is much room for improvement. We take the whole short text as one sequence, this may not be very reasonable, because most of short texts contain several sentences. A hierarchical RNN (Li et al., 2015) is one possible direction. The rare word problem is also very important for the generation of the summaries, especially when the input is word-based instead of character-based. It is also a hot topic in the neural generative models such as neural translation machine(NTM) (Luong et al., 2014), which can benefit to this task. We also plan to construct a large document summarization data set by using naturally annotated web resources. Acknowledgments This work is supported by National Natural Science Foundation of China: 61473101, 61173075 and 61272383, Strategic Emerging Industry Development Sp</context>
</contexts>
<marker>Li, Luong, Jurafsky, 2015</marker>
<rawString>Jiwei Li, Minh-Thang Luong, and Dan Jurafsky. 2015. A hierarchical neural autoencoder for paragraphs and documents. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A package for automatic evaluation of summaries. In</title>
<date>2004</date>
<booktitle>In Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop ofACL 2004,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="18249" citStr="Lin, 2004" startWordPosition="2859" endWordPosition="2860">ord: 7=+l.�:8#&amp;�7&gt;3��CN&gt;1a&gt;1a&gt;1atM&gt;1 ~8#&gt;1~&gt;1 Global competitiveness ranking: Switzerland ranks 3rd place, China 1st place, 1st place, China 1st place, Switzerland 1st place, first RNN+Context+Char: 47=�jl.���.�&gt;28�I&lt;�1K In the Global competitiveness ranking list, China is in the 28th place which is highestamong the BRICcountries. RNN+Context+Word: 47=$jj-adt6��. 28~I&lt;~%~.� &gt;— ( &gt;)L “The Global Competitiveness Report” is released. China is in the 28th place, rank highest among the BRICcountries (ninth Figure 7: An example of the generated summaries. For evaluation, we adopt the ROUGE metrics (Lin, 2004) proposed by (Lin and Hovy, 2003), which have been proved strongly correlated with human evaluations. ROUGE-1, ROUGE2 and ROUGE-L are used. All the models are trained on the GPUs tesla M2090 for about one week.Table 2 lists the experiment results. As we can see in Figure 7, the summaries generated by RNN with context are very close to human written summaries, which indicates that if we feed enough data to the RNN encoder and decoder, it may generate summary almost from scratch. The results also show that the RNN with context outperforms RNN without context on both character and word based inpu</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In In Proceedings of Workshop on Text Summarization Branches Out, Post-Conference Workshop ofACL 2004, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The automatic creation of literature abstracts.</title>
<date>1998</date>
<journal>IBM Journal of Research and Development,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="5595" citStr="Luhn, 1998" startWordPosition="853" endWordPosition="854"> perform recurrent neural network based encoder-decoder method on the dataset to generate summary and get promising results, which can be used as one baseline of the task. 2 Related Work Our work is related to recent works on automatic text summarization and natural language processing based on naturally annotated web resources, which are briefly introduced as follows. Automatic Text Summarization in some form has been studied since 1950. Since then, most researches are related to extractive summarizations by analyzing the organization of the words in the document (Nenkova and McKeown, 2011) (Luhn, 1998); Since it needs labeled data sets for supervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods (Mihalcea, 2004). The scale of existing data sets are usually very small (most of them are less than 1000). For example, DUC2002 dataset contains 567 documents and each document is provided with two 100-words human summaries. Our work is also related to the headline generation, which is a task to generate one sentence of the text it entitles. Colmenares et.al construct a 1.3 million financial news headline dataset written in Engl</context>
</contexts>
<marker>Luhn, 1998</marker>
<rawString>H. P. Luhn. 1998. The automatic creation of literature abstracts. IBM Journal of Research and Development, 2(2):159–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thang Luong</author>
<author>Ilya Sutskever</author>
<author>Quoc V Le</author>
<author>Oriol Vinyals</author>
<author>Wojciech Zaremba</author>
</authors>
<title>Addressing the rare word problem in neural machine translation.</title>
<date>2014</date>
<location>CoRR, abs/1410.8206.</location>
<contexts>
<context position="21316" citStr="Luong et al., 2014" startWordPosition="3354" endWordPosition="3357">ed RNN-based methods on it, which achieved some promising results. This is just a start of deep models on this task and there is much room for improvement. We take the whole short text as one sequence, this may not be very reasonable, because most of short texts contain several sentences. A hierarchical RNN (Li et al., 2015) is one possible direction. The rare word problem is also very important for the generation of the summaries, especially when the input is word-based instead of character-based. It is also a hot topic in the neural generative models such as neural translation machine(NTM) (Luong et al., 2014), which can benefit to this task. We also plan to construct a large document summarization data set by using naturally annotated web resources. Acknowledgments This work is supported by National Natural Science Foundation of China: 61473101, 61173075 and 61272383, Strategic Emerging Industry Development Special Funds of Shenzhen: JCYJ20140417172417105, JCYJ20140508161040764 and JCYJ20140627163809422. We thank to Baolin Peng, Lin Ma, Li Yu and the anonymous reviewers for their insightful comments. 1971 Chin-Yew Lin and E.H. Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrenc</context>
</contexts>
<marker>Luong, Sutskever, Le, Vinyals, Zaremba, 2014</marker>
<rawString>Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. 2014. Addressing the rare word problem in neural machine translation. CoRR, abs/1410.8206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das Andr F T Martins</author>
</authors>
<title>A survey on automatic text summarization.</title>
<date>2007</date>
<tech>Technical report, CMU.</tech>
<contexts>
<context position="2313" citStr="Martins, 2007" startWordPosition="348" endWordPosition="349">mine these high-quality data from these naturally annotated web resources, it will be beneficial to the research that has been hampered by the lack of data. 1http://icrc.hitsz.edu.cn/Article/show/139.html Figure 1: A Weibo Posted by People’s Daily. In the Natural Language Processing (NLP) community, automatic text summarization is a hot and difficult task. A good summarization system should understand the whole text and re-organize the information to generate coherent, informative, and significantly short summaries which convey important information of the original text (Hovy and Lin, 1998), (Martins, 2007). Most of traditional abstractive summarization methods divide the process into two phrases (Bing et al., 2015). First, key textual elements are extracted from the original text by using unsupervised methods or linguistic knowledge. And then, unclear extracted components are rewritten or paraphrased to produce a concise summary of the original text by using linguistic rules or language generation techniques. Although extensive researches have been done, the linguistic quality of abstractive summary is still far from satisfactory. Recently, deep learning methods have shown potential abilities t</context>
</contexts>
<marker>Martins, 2007</marker>
<rawString>Dipanjan Das Andr F.T. Martins. 2007. A survey on automatic text summarization. Technical report, CMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
</authors>
<title>Graph-based ranking algorithms for sentence extraction, applied to text summarization.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, companion volume,</booktitle>
<contexts>
<context position="5778" citStr="Mihalcea, 2004" startWordPosition="880" endWordPosition="881">ed Work Our work is related to recent works on automatic text summarization and natural language processing based on naturally annotated web resources, which are briefly introduced as follows. Automatic Text Summarization in some form has been studied since 1950. Since then, most researches are related to extractive summarizations by analyzing the organization of the words in the document (Nenkova and McKeown, 2011) (Luhn, 1998); Since it needs labeled data sets for supervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods (Mihalcea, 2004). The scale of existing data sets are usually very small (most of them are less than 1000). For example, DUC2002 dataset contains 567 documents and each document is provided with two 100-words human summaries. Our work is also related to the headline generation, which is a task to generate one sentence of the text it entitles. Colmenares et.al construct a 1.3 million financial news headline dataset written in English for headline generation (Colmenares et al., 2015). However, the data set is not publicly available. Naturally Annotated Web Resources based Natural Language Processing is proposed</context>
</contexts>
<marker>Mihalcea, 2004</marker>
<rawString>Rada Mihalcea. 2004. Graph-based ranking algorithms for sentence extraction, applied to text summarization. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, companion volume, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Kathleen McKeown</author>
</authors>
<date>2011</date>
<booktitle>Automatic summarization. Foundations and Trend in Information Retrieval,</booktitle>
<pages>5--2</pages>
<contexts>
<context position="5582" citStr="Nenkova and McKeown, 2011" startWordPosition="849" endWordPosition="852">ality of the dataset; (4) We perform recurrent neural network based encoder-decoder method on the dataset to generate summary and get promising results, which can be used as one baseline of the task. 2 Related Work Our work is related to recent works on automatic text summarization and natural language processing based on naturally annotated web resources, which are briefly introduced as follows. Automatic Text Summarization in some form has been studied since 1950. Since then, most researches are related to extractive summarizations by analyzing the organization of the words in the document (Nenkova and McKeown, 2011) (Luhn, 1998); Since it needs labeled data sets for supervised machine learning methods and labeling dataset is very intensive, some researches focused on the unsupervised methods (Mihalcea, 2004). The scale of existing data sets are usually very small (most of them are less than 1000). For example, DUC2002 dataset contains 567 documents and each document is provided with two 100-words human summaries. Our work is also related to the headline generation, which is a task to generate one sentence of the text it entitles. Colmenares et.al construct a 1.3 million financial news headline dataset wr</context>
</contexts>
<marker>Nenkova, McKeown, 2011</marker>
<rawString>Ani Nenkova and Kathleen McKeown. 2011. Automatic summarization. Foundations and Trend in Information Retrieval, 5(2-3):103–233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lifeng Shang</author>
<author>Zhengdong Lu</author>
<author>Hang Li</author>
</authors>
<title>Neural responding machine for short-text conversation.</title>
<date>2015</date>
<location>CoRR, abs/1503.02364.</location>
<contexts>
<context position="11090" citStr="Shang et al., 2015" startWordPosition="1804" endWordPosition="1807">we use pairs scored by 3, 4 and 5 of this part as the test set for short text summary generation task. Part I 2,400,591 Part II Number of Pairs 10,666 Human Score 1 942 Human Score 2 1,039 Human Score 3 2,019 Human Score 4 3,128 Human Score 5 3,538 Part III Number of Pairs 1,106 Human Score 1 165 Human Score 2 216 Human Score 3 227 Human Score 4 301 Human Score 5 197 Table 1: Data Statistics 5 Experiment Recently, recurrent neural network (RNN) have shown powerful abilities on speech recognition (Graves et al., 2013), machine translation (Sutskever et al., 2014) and automatic dialog response (Shang et al., 2015). However, there is rare research on the automatic text summarization by using deep models. In this section, we use RNN as encoder and decoder to generate the summary of short text. We use the Part I as the training set 1969 Short Text: -i&apos;jC- =.lalDF&apos;,t+&amp;A$E����BH�+&amp;quot;*I ~~~1-=.4/��17*�(C�3#@�561!)�(C�3 &gt;?561!)a./s—�&gt;?561J�%,�IfrAt—��0-I2A8[�J�c 1G ,�,J�A8-=.;&lt;�&gt;FA-:&amp;quot;f1 %tta Mingzhong Chen, the Chief Secretary of the Water Devision of the Ministry of Water Resources, revealed today at a press conference, according to the just&lt; completed assessment of water resources management system, some prov</context>
</contexts>
<marker>Shang, Lu, Li, 2015</marker>
<rawString>Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neural responding machine for short-text conversation. CoRR, abs/1503.02364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mao Song Sun</author>
</authors>
<title>Natural language procesing based on naturaly annotated web resources.</title>
<date>2011</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>25</volume>
<issue>6</issue>
<contexts>
<context position="1687" citStr="Sun, 2011" startWordPosition="259" endWordPosition="260">text summarization research, but also provides a baseline for further research on this topic. 1 Introduction Nowadays, individuals or organizations can easily share or post information to the public on the social network. Take the popular Chinese microblogging website (Sina Weibo) as an example, the People’s Daily, one of the media in China, posts more than tens of weibos (analogous to tweets) each day. Most of these weibos are wellwritten and highly informative because of the text length limitation (less than140 Chinese characters). Such data is regarded as naturally annotated web resources (Sun, 2011). If we can mine these high-quality data from these naturally annotated web resources, it will be beneficial to the research that has been hampered by the lack of data. 1http://icrc.hitsz.edu.cn/Article/show/139.html Figure 1: A Weibo Posted by People’s Daily. In the Natural Language Processing (NLP) community, automatic text summarization is a hot and difficult task. A good summarization system should understand the whole text and re-organize the information to generate coherent, informative, and significantly short summaries which convey important information of the original text (Hovy and L</context>
<context position="6397" citStr="Sun, 2011" startWordPosition="981" endWordPosition="982">le of existing data sets are usually very small (most of them are less than 1000). For example, DUC2002 dataset contains 567 documents and each document is provided with two 100-words human summaries. Our work is also related to the headline generation, which is a task to generate one sentence of the text it entitles. Colmenares et.al construct a 1.3 million financial news headline dataset written in English for headline generation (Colmenares et al., 2015). However, the data set is not publicly available. Naturally Annotated Web Resources based Natural Language Processing is proposed by Sun (Sun, 2011). Naturally Annotated Web Resources is the data generated by users for communicative purposes such as web pages, blogs and microblogs. We can mine knowledge or useful data from these raw data by using marks generated by users unintentionally. Jure et.al track 1.6 million mainstream media sites and blogs and mine a set of novel and persistent temporal patterns in the news cycle (Leskovec et al., 2009). Sepandar et.al use the users’ naturally annotated pattern ‘we feel’ and ‘i feel’ to extract the ‘Feeling’ sentence collection which is used to collect the world’s emotions. In this work, we use t</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Mao Song Sun. 2011. Natural language procesing based on naturaly annotated web resources. Journal of Chinese Information Processing, 25(6):26–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc V V Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Advances in Neural Information Processing Systems 27,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="3042" citStr="Sutskever et al., 2014" startWordPosition="457" endWordPosition="460">2015). First, key textual elements are extracted from the original text by using unsupervised methods or linguistic knowledge. And then, unclear extracted components are rewritten or paraphrased to produce a concise summary of the original text by using linguistic rules or language generation techniques. Although extensive researches have been done, the linguistic quality of abstractive summary is still far from satisfactory. Recently, deep learning methods have shown potential abilities to learn representation (Hu et al., 2014; Zhou et al., 2015) and generate language (Bahdanau et al., 2014; Sutskever et al., 2014) from large scale data by utilizing GPUs. Many researchers realize that we are closer to generate abstractive summarizations by using the deep learning methods. However, the publicly available and high-quality large scale summarization data set is still very rare and not easy to be constructed manually. For example, the popular document summarization dataset DUC2, TAC3 and TREC4 have only hundreds of human written English text summarizations. The problem is even worse for Chinese. In this pa2http://duc.nist.gov/data.html 3http://www.nist.gov/tac/2015/KBP/ 4http://trec.nist.gov/ 1967 Proceeding</context>
<context position="11039" citStr="Sutskever et al., 2014" startWordPosition="1796" endWordPosition="1799"> is independent from Part I and Part II. In this work, we use pairs scored by 3, 4 and 5 of this part as the test set for short text summary generation task. Part I 2,400,591 Part II Number of Pairs 10,666 Human Score 1 942 Human Score 2 1,039 Human Score 3 2,019 Human Score 4 3,128 Human Score 5 3,538 Part III Number of Pairs 1,106 Human Score 1 165 Human Score 2 216 Human Score 3 227 Human Score 4 301 Human Score 5 197 Table 1: Data Statistics 5 Experiment Recently, recurrent neural network (RNN) have shown powerful abilities on speech recognition (Graves et al., 2013), machine translation (Sutskever et al., 2014) and automatic dialog response (Shang et al., 2015). However, there is rare research on the automatic text summarization by using deep models. In this section, we use RNN as encoder and decoder to generate the summary of short text. We use the Part I as the training set 1969 Short Text: -i&apos;jC- =.lalDF&apos;,t+&amp;A$E����BH�+&amp;quot;*I ~~~1-=.4/��17*�(C�3#@�561!)�(C�3 &gt;?561!)a./s—�&gt;?561J�%,�IfrAt—��0-I2A8[�J�c 1G ,�,J�A8-=.;&lt;�&gt;FA-:&amp;quot;f1 %tta Mingzhong Chen, the Chief Secretary of the Water Devision of the Ministry of Water Resources, revealed today at a press conference, according to the just&lt; completed assessm</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc V. V Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems 27, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew D Zeiler</author>
</authors>
<title>ADADELTA: an adaptive learning rate method.</title>
<date>2012</date>
<location>CoRR, abs/1212.5701.</location>
<contexts>
<context position="14069" citStr="Zeiler, 2012" startWordPosition="2257" endWordPosition="2258">s, 1) The local context is not used during decoding. We use the RNN as encoder and it’s last hidden state as the input of decoder, as shown in Figure 5; 2) The context is used during decoding, following (Bahdanau et al., 2014), we use the combination of all the hidden states of encoder as input of the decoder, as shown in Figure 6. For the RNN, we adopt the gated recurrent unit (GRU) which is proposed by (Chung et al., 2015) and has been proved comparable to LSTM (Chung et al., 2014). All the parameters (including the embeddings) of the two architectures are randomly initialized and ADADELTA (Zeiler, 2012) is used to update the learning rate. After the model is trained, the beam search is used to generate the best summaries in the process of decoding and the size of beam is set to 10 in our experiment. 5https://pypi.python.org/pypi/jieba/ Short Text: �&amp;RI&lt;:75�8@�E�.AO-30%V4T,0(?&gt;PC8&lt;/ 7%4*B�]Ccc= o+-gpEQFGO2O#J*,5-48!&amp;%iFGO2O3 &apos;2$A&amp;quot;)fJ,�!19�H�=��9�H����9�#D� A 6�,=V9JA#O2O#J*; ,*. Groupons’ sales on mobile terminals are below 30 percent. User’s preference of shopping through PCs can not be changed in the short term. In the future Chinese O2O catering market, mobile terminals will become the str</context>
</contexts>
<marker>Zeiler, 2012</marker>
<rawString>Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. CoRR, abs/1212.5701.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Zhou</author>
<author>Baotian Hu</author>
<author>Qingcai Chen</author>
<author>Buzhou Tang</author>
<author>Xiaolong Wang</author>
</authors>
<title>Answer sequence learning with neural networks for answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of theACL-IJCNLP,</booktitle>
<pages>713--718</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="2972" citStr="Zhou et al., 2015" startWordPosition="446" endWordPosition="449">zation methods divide the process into two phrases (Bing et al., 2015). First, key textual elements are extracted from the original text by using unsupervised methods or linguistic knowledge. And then, unclear extracted components are rewritten or paraphrased to produce a concise summary of the original text by using linguistic rules or language generation techniques. Although extensive researches have been done, the linguistic quality of abstractive summary is still far from satisfactory. Recently, deep learning methods have shown potential abilities to learn representation (Hu et al., 2014; Zhou et al., 2015) and generate language (Bahdanau et al., 2014; Sutskever et al., 2014) from large scale data by utilizing GPUs. Many researchers realize that we are closer to generate abstractive summarizations by using the deep learning methods. However, the publicly available and high-quality large scale summarization data set is still very rare and not easy to be constructed manually. For example, the popular document summarization dataset DUC2, TAC3 and TREC4 have only hundreds of human written English text summarizations. The problem is even worse for Chinese. In this pa2http://duc.nist.gov/data.html 3ht</context>
</contexts>
<marker>Zhou, Hu, Chen, Tang, Wang, 2015</marker>
<rawString>Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou Tang, and Xiaolong Wang. 2015. Answer sequence learning with neural networks for answer selection in community question answering. In Proceedings of theACL-IJCNLP, pages 713–718, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
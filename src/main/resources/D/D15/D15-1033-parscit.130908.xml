<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010002">
<title confidence="0.999374">
Learning Better Embeddings for Rare Words
Using Distributional Representations
</title>
<author confidence="0.988753">
Irina Sergienya and Hinrich Sch¨utze
</author>
<affiliation confidence="0.9980575">
Center for Information and Language Processing
University of Munich, Germany
</affiliation>
<email confidence="0.995427">
sergienya@cis.lmu.de
</email>
<sectionHeader confidence="0.997342" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999386">
There are two main types of word repre-
sentations: low-dimensional embeddings
and high-dimensional distributional vec-
tors, in which each dimension corresponds
to a context word. In this paper, we ini-
tialize an embedding-learning model with
distributional vectors. Evaluation on word
similarity shows that this initialization sig-
nificantly increases the quality of embed-
dings for rare words.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946318181818">
Standard neural network (NN) architectures for in-
ducing embeddings have an input layer that repre-
sents each word as a one-hot vector (e.g., Turian
et al. (2010), Collobert et al. (2011), Mikolov et
al. (2013)). There is no usable information avail-
able in this input-layer representation except for
the identity of the word. We call this standard ini-
tialization method one-hot initialization.
Distributional representations (e.g., Sch¨utze
(1992), Lund and Burgess (1996), Sahlgren
(2008), Turney and Pantel (2010), Baroni and
Lenci (2010)) represent a word as a high-
dimensional vector in which each dimension cor-
responds to a context word. They have been suc-
cessfully used for a wide variety of tasks in natu-
ral language processing such as phrase similarity
(Mitchell and Lapata, 2010) and sentiment analy-
sis (Turney and Littman, 2003).
In this paper, we investigate distributional ini-
tialization: the use of distributional vectors as rep-
resentations of words at the input layer of NN ar-
chitectures for embedding learning to improve the
embeddings of rare words. It is difficult for one-
hot initialization to learn good embeddings from
only a few examples. In contrast, distributional
initialization provides an additional source of in-
formation – the global distribution of the word in
the corpus – that improves embeddings learned for
rare words. We will demonstrate this type of im-
provement in the experiments reported below.
In summary, we introduce the idea of dis-
tributional initialization for embedding learn-
ing, an alternative to one-hot initialization that
combines distributed representations (or embed-
dings) with distributional representations (or high-
dimensional vectors). We show that distributional
initialization significantly improves the quality of
embeddings learned for rare words.
We will first describe our methods in Section 2
and the experimental setup in Section 3. Section 4
presents and discusses experimental results. We
summarize related work in Section 5 and finish
with conclusion in Section 6 and discussion of fu-
ture work in Section 7.
</bodyText>
<sectionHeader confidence="0.991276" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999307">
Weighting. We use two different weighting
schemes for distributional vectors. Let vi, ... , vn
be the vocabulary of context words. In BINARY
weighting, entry 1 G i G n in the distributional
vector of target word w is set to 1 iff vi and w
cooccur at a distance of at most ten words in the
corpus and to 0 otherwise.
In PPMI weighting, entry 1 G i G n in the
distributional vector of target word w is set to the
PPMI (positive pointwise mutual information, in-
troduced by Niwa and Nitta (1994)) of w and vi.
We divide PPMI values by their maximum to en-
sure they are in [0, 1] because we will combine
one-hot vectors (whose values are 0/1) with PPMI
weights and it is important that they are on the
same scale.
We use two different distributional initializa-
tions, shown in Figure 1: separate (left) and mixed
(right). Combinations of these two initializations
with both BINARY and PPMI weighting will be
investigated in the experiments.
Recall that n is the dimensionality of the distri-
</bodyText>
<page confidence="0.939198">
280
</page>
<note confidence="0.7239">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 280–285,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.914937333333333">
k n k n − k
z } |{z } |{
z } |{z } |{
w1
w2
1
1
1
w3
...
wk
...
1
...
...
...
0 0 1 ··· 1 1 0 ··· 1
wn
w1
w2
w3
freq.
words
...
⎧
⎨⎪⎪⎪⎪⎪⎪ ⎪
⎪⎪⎪⎪⎪⎪⎪⎩
wk
wk+1
rare
words
...
⎧
⎨⎪
⎪⎩
wn
⎧
⎨⎪⎪⎪⎪⎪⎪⎪
⎪⎪⎪⎪⎪⎪⎪⎩
freq.
words
⎧
⎨⎪
⎪⎩
rare
words
0
...
0
1
...
1
1 1 0 ··· 0 1
0 1 0 ··· 1 1
1
1
0
wk+1 0 1 1 ··· 0 1 1 ··· 0
 |{z }  |{z }
k + n n
</figure>
<figureCaption confidence="0.998141">
Figure 1: One-hot vectors of frequent words and distributional vectors of rare words are separate in separate initialization (left)
and overlap in mixed initialization (right). This example is for BINARY weighting.
</figureCaption>
<bodyText confidence="0.999263409090909">
butional vectors. Let k be the number of words
with frequency &gt; 0, where the frequency thresh-
old 0 is a parameter.
In separate initialization, the input represen-
tation for a word is the concatenation of a k-
dimensional vector and an n-dimensional vec-
tor. For a word with frequency &gt; 0, the k-
dimensional vector is a one-hot vector and the n-
dimensional vector is zero. For a word with fre-
quency &lt; 0, the k-dimensional vector is zero and
the n-dimensional vector is its distributional vec-
tor.
In mixed initialization, the input representation
for a word is an n-dimensional vector: a one-hot
vector for a word with frequency &gt; 0 and a distri-
butional vector for a word with frequency &lt; 0.
In summary, separate initialization uses sepa-
rate representation spaces for frequent words (one-
hot space) and rare words (distributional space).
Mixed initialization uses the same representation
space for all words; and rare words share weights
with the frequent words that they cooccur with.
</bodyText>
<sectionHeader confidence="0.995899" genericHeader="method">
3 Experimental setup
</sectionHeader>
<bodyText confidence="0.999970217391304">
We use ukWaC+WaCkypedia (Baroni et al.,
2009), a corpus of 2.4 billion tokens and 6 million
word types. Based on (Turian et al., 2010), we
preprocess the corpus by removing sentences that
are less than 90% lowercase; lowercasing; replac-
ing URLs, email addresses and digits by special
tokens; tokenization (Schmid, 2000); replacing
words with frequency 1 with &lt;unk&gt;; and adding
end-of-sentence tokens. After preprocessing, the
size n of the context word vocabulary is 2.7 mil-
lion.
We evaluate on six word similarity judgment
data sets (number of pairs in parentheses): RG
(Rubenstein and Goodenough (1965), 65) MC
(Miller and Charles (1991), 30), MEN1 (Bruni et
al. (2012), 3000), WordSim3532 (Finkelstein et al.
(2001), 353), Stanford Rare Word3 (Luong et al.
(2013), 2034) and SimLex-9994 (Hill et al. (2014),
999). We exclude from the evaluation the 16 pairs
in RW that contain a word that does not occur in
our corpus.
Our goal in this paper is to investigate the ef-
fect of using distributional initialization vs. one-
hot initialization on the quality of embeddings of
rare words.
However, except for RW, the six data sets con-
tain only a single word with frequency &lt;100, all
other words are more frequent.
To address this issue, we artificially make all
words in the six data sets rare. We do this by
keeping only 0 randomly chosen occurrences in
the corpus (for words with frequency &gt;0) and re-
placing all other occurrences with a different to-
ken (e.g., “fire” is replaced with “*fire*”). This
procedure – corpus downsampling – ensures that
all words in the six data sets are rare in the corpus
and that our setup directly evaluates the impact of
distributional initialization on rare words.
Note that we use 0 for two different purposes:
(i) 0 is the frequency threshold that determines
which words are classified as rare and which as
frequent in Figure 1 – changing 0 corresponds to
moving the horizontal dashed line in separate and
mixed initialization up and down; (ii) 0 is the pa-
rameter that determines how many occurrences of
a word are left in the corpus when we remove oc-
</bodyText>
<footnote confidence="0.99813775">
1clic.cimec.unitn.it/˜elia.bruni/MEN
2alfonseca.org/eng/research/wordsim353.html
3www-nlp.stanford.edu/˜lmthang/morphoNLM/
4cl.cam.ac.uk/˜fh295/simlex.html
</footnote>
<page confidence="0.985549">
281
</page>
<table confidence="0.997208647058824">
0 A B C D E F G H I J K L
RG MC MEN WS RW SL
mixed sep mixed sep mixed sep mixed sep mixed sep mixed sep
1 10 *56.54 47.06 35.96 32.10 *43.76*45.56 34.21*40.93 *24.81 20.85 *18.30*13.76
2 20 *59.08 45.31 *46.66 35.22 52.05*52.38 41.44 47.53 *29.48 26.93 *20.85*16.86
3 50 *63.20 51.07 *52.35 37.45 58.21 53.80 43.14 44.88 31.32 29.16 *24.19*22.45
4 BINARY 68.33 52.50 61.70 35.94 61.69 55.23 48.25 44.89 33.29 30.22 *26.74 24.66
100
5 10 *56.87*51.94 *37.31*46.52 *48.05*50.49 38.41*47.54 *25.53 23.12 *19.70*15.59
6 20 *59.08*50.32 *47.51*45.17 *54.88*56.42 43.31*53.19 *29.78*28.51 *21.84*19.23
PPMI
7 50 *64.90*64.36 *55.27*56.75 60.51 61.04 45.76 55.55 32.05 30.25 *25.11*21.60
8 100 71.08 58.37 68.14 52.33 63.05 60.74 48.66 55.49 33.25 30.49 *27.13 22.60
9 10 38.93 16.67 40.70 35.17 20.69 8.97
10 hteoon- 20 42.17 25.21 50.21 43.74 26.58 13.62
11 50 56.01 42.35 60.22 54.10 32.16 20.01
12 100 67.47 61.33 65.14 59.87 35.19 24.06
</table>
<tableCaption confidence="0.998730666666667">
Table 1: Spearman correlation coefficients ×100 between human and embedding-based similarity judgments, averaged over 5
runs. Distributional initialization correlations that are higher (resp. significantly higher) than corresponding one-hot correlations
are set in bold (resp. marked *).
</tableCaption>
<bodyText confidence="0.965672046511628">
currences to ensure that words from the evaluation
data sets are rare in the corpus.
We covary these two parameters in the experi-
ments below; e.g., we apply distributional initial-
ization with 0 = 20 to a corpus constructed to have
0 = 20 occurrences of words from similarity data
sets. We do this to ensure that all evaluation words
are rare words for the purpose of distributional ini-
tialization and so we can exploit all pairs in the
evaluation data sets for evaluating the efficacy of
our method for rare words.
We modified word2vec5 (Mikolov et al., 2013)
to accommodate distributional initialization; to
support distributional vectors at the input layer,
we changed the implementation of activation func-
tions and backpropagation. We use the skipgram
model, hierarchical softmax, set the size of the
context window to 10 (10 words to the left and 10
to the right), min-count to 1 (train on all tokens),
embedding size to 100, sampling rate to 10−3 and
train models for one epoch.
For four values of the frequency threshold,
0 E 110, 20, 50, 1001,6 we train word2vec models
5code.google.com/p/word2vec
6A reviewer asks whether the value of θ should depend on
the size of the training corpus. Our intuition is that it is in-
dependent of corpus size. If a certain amount of information
– corresponding to a certain number of contexts – is required
to learn a meaningful representation of a word, then it should
not matter whether that given number of contexts occurs in a
small corpus or in a large corpus. However, if the contexts
themselves contain many rare words (which is more likely in
a small corpus), then corpus size could be an important vari-
with one-hot initialization and with the four com-
binations of weighting (BINARY, PPMI) and dis-
tributional initialization (mixed, separate), a total
of 4 x (1 + 2 x 2) = 20 models. For each train-
ing run, we perform corpus downsampling and ini-
tialize the parameters of the models randomly. To
get a reliable assessment of performance, we train
5 instances of each model and report averages of
the 5 runs. One model takes -3 hours to train on
23 CPU cores, 2.30GHz.
</bodyText>
<sectionHeader confidence="0.971208" genericHeader="method">
4 Experimental results and discussion
</sectionHeader>
<bodyText confidence="0.968958176470588">
Table 1 shows experimental results, averaged over
5 runs. The evaluation measure is Spearman
correlation x100 between human and machine-
generated pair similarity judgments.
Frequency threshold 0. The main result is that
for 0 E 110, 201 distributional initialization is
better than one-hot initialization (see bold num-
bers): compare lines 1&amp;5 with line 9; and lines
2&amp;6 with line 10. This is true for both mixed and
separate initialization, with the exception of WS,
for which mixed (column G) is better in only 1
(line 5) of 4 cases.
Looking only at results for 0 E 110, 201, 18 of
24 improvements are significant7 for mixed initial-
ization and 16 of 24 improvements are significant
for separate initialization (lines 1&amp;5 vs 9 and lines
able to take into account.
</bodyText>
<footnote confidence="0.9856795">
7Two-sample t-test, two-tailed, assuming equal variance,
p &lt; .05
</footnote>
<page confidence="0.993168">
282
</page>
<bodyText confidence="0.99592955882353">
2&amp;6 vs 10).
For 0 E 150, 1001, mixed initialization does
well for RG, MC and SL, but the gap between
mixed and one-hot initializations is generally
smaller for these larger values of 0; e.g., the dif-
ference is larger than 9 for 0 = 10 (A1&amp;A5 vs
A/B9, C1&amp;C5 vs C/D9, K1&amp;K5 vs K/L9) and less
than 9 for 0 = 100 (A4&amp;A8 vs A/B12, C4&amp;C8
vs C/D12, K4&amp;K8 vs K/L12) for these three data
sets.
Recall that each value of 0 effectively results in
a different training corpus – a training corpus in
which the number of occurrences of the words in
the evaluation data sets has been reduced to &lt; 0
(cf. Section 3).
Our results indicate that distributional initializa-
tion is beneficial for very rare words – those that
occur no more than 20 times in the corpus. Our
results for medium rare words – those that occur
between 50 and 100 times – are less clear: either
there are no improvements or improvements are
small.
Thus, our recommendation is to use 0 = 20.
Scalability. The time complexity of the ba-
sic version of word2vec is O(ECW D log V )
(Mikolov et al., 2013) where E is the number
of epochs, C is the corpus size, W is the con-
text window size, D is the number of dimensions
of the embedding space, and V is the vocabu-
lary size. Distributional initialization adds a term
I, the average number of entries in the distribu-
tional vectors, so that time complexity increases to
O(IECWD log V ). For rare words, I is small, so
that there is no big difference in efficiency between
one-hot initialization and distributional initializa-
tion of word2vec. However, for frequent words
I would be large, so that distributional initializa-
tion may not be scalable in that case. So even if
our experiments had shown that distributional ini-
tialization helps for both rare and frequent words,
scalability would be an argument for only using it
for rare words.
Binary vs. PPMI. PPMI weighting is almost al-
ways better than BINARY, with three exceptions
(I8, L7, L8) where the difference between the two
is small and not significant. The probable explana-
tion is that the PPMI weights in [0, 1] convey de-
tailed, graded information about the strength of as-
sociation between two words, taking into account
their base frequencies. In contrast, the BINARY
weights in 10, 11 only indicate if there was any in-
stance of cooccurrence at all – without considering
frequency of cooccurrence and without normaliz-
ing for base frequencies.
Mixed vs. Separate. Mixed initialization is less
variable and more predictable than separate initial-
ization: performance for mixed initialization al-
ways goes up as 0 increases, e.g., 56.54 —* 59.08
—* 63.20 —* 68.33 (column A, lines 1–4). In con-
trast, separate initialization performance often de-
creases, e.g., from 47.06 to 45.31 (column B, lines
1–2) when 0 is increased. Since more informa-
tion (more occurrences of the words that simi-
larity judgments are computed for) should gener-
ally not have a negative effect on performance, the
only explanation is that separate is more variable
than mixed and that this variability sometimes re-
sults in decreased performance. Figure 1 explains
this difference between the two initializations: in
mixed initialization (right panel), rare words are
tied to frequent words, so their representations are
smoothed by representations learned for frequent
words. In separate initialization (left panel), no
such links to frequent words exist, resulting in
higher variability.
Because of its lower variability, our experiments
suggest that mixed initialiation is a better choice
than separate initialization.
One-hot vs. Distributional initialization. Our
experiments show that distributional representa-
tion is helpful for rare words. It is difficult for
one-hot initialization to learn good embeddings
for such words, based on only a small number of
contexts in the corpus. In such cases, distribu-
tional initialization makes the learning task easier
since in addition to the contexts of the rare word,
the learner now also has access to the global dis-
tribution of the rare word and can take advantage
of weight sharing with other words that have sim-
ilar distributional representations to smooth em-
beddings systematically.
Thus, distributional initialization is a form of
smoothing: the embedding of a rare word is tied to
the embeddings of other words via the links shown
in Figure 1: the 1s in the lower “rare words” part
of the illustrations for separate and mixed initial-
ization. As is true for smoothing in general, pa-
rameter estimates for frequent events benefit less
from smoothing or can even deteriorate. In con-
trast, smoothing is essential for rare events. Where
the boundary lies between rare and frequent events
depends on the specifics of the problem and the
</bodyText>
<page confidence="0.995937">
283
</page>
<bodyText confidence="0.9999702">
smoothing method used and is usually an empiri-
cal question. Our results indicate that that bound-
ary lies somewhere between 20 and 50 in our set-
ting.8
Variance of results. Table 1 shows averages of
five runs. The variance of results was quite high
for low-performing models. For higher perform-
ing models – those with values ≥ 40 – the ra-
tio of standard deviation divided by mean ranged
from .005 to .29. The median was .044. While
the variance from run to run is quite high for low-
performing models and for a few high-performing
models, the significance test takes this into ac-
count, so that the relatively high variability does
not undermine our results.
In summary, we have shown that distributional
initialization improves the quality of word embed-
dings for rare words. Our recommendation is to
use mixed initialization with PPMI weighting and
the value 0 = 20 of the frequency threshold.
</bodyText>
<sectionHeader confidence="0.999936" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.996621742857143">
An alternative to using distributional information
for initialization is to use syntactic and semantic
information for initialization. Approaches along
these lines include Botha and Blunsom (2014)
who represent a word as a sum of embedding vec-
tors of its morphemes. Cui et al. (2014) use a
weighted average of vectors of morphologically
similar words. Bian et al. (2014) extend a word’s
vector with vectors of entity categories and POS
tags. This line of work also is partially motivated
by improving the embeddings of rare words. Dis-
tributional information on the one hand and syn-
tactic/semantic information on the other hand are
likely to be complementary, so that a combination
of our approach with this prior work is promising.
Le et al. (2010) propose three schemes to ad-
dress word embedding initialization. Reinitializa-
tion and iterative reinitialization use vectors from
prediction space to initialize the context space dur-
ing training. This approach is both more complex
and less efficient than ours. One-vector initializa-
tion initializes all word embeddings with the same
8A reviewer asks: “If a word is rare, its distributional vec-
tor should also be sparse and less informative, which does not
guarantee to be a good starting point.” This is true and it sug-
gests that it may not be possible to learn a very high-quality
representation for a rare word. But this it not our goal. Our
goal is simply to learn a better representation than the one
that is learned by standard word2vec. Our explanation for
our positive experimental results is that distributional initial-
ization implements a form of smoothing.
random vector to keep rare words close to each
other. This approach is also less efficient than ours
since the initial embedding is much denser than in
our approach.
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99991475">
We have introduced distributional initialization of
neural network architectures for learning better
embeddings for rare words. Experimental results
on a word similarity judgment task demonstrate
that embeddings of rare words learned with dis-
tributional initialization perform better than em-
beddings learned with traditional one-hot initial-
ization.
</bodyText>
<sectionHeader confidence="0.999887" genericHeader="discussions">
7 Future work
</sectionHeader>
<bodyText confidence="0.976207685714286">
Our work is the first exploration of the utility
of distributional representations as initialization
for embedding learning algorithms like word2vec.
There are a number of research questions we
would like to investigate in the future.
First, we showed that distributional represen-
tation is beneficial for words with very low fre-
quency. It was not beneficial in our experiments
for more frequent words. A more extensive analy-
sis of the factors that are responsible for the posi-
tive effect of distributional representation is in or-
der.
Second, to simplify our experimental setup and
make the number of runs mangeable, we used the
parameter 0 both for corpus processing (only 0 oc-
currences of a particular word were left in the cor-
pus) and as the separator between rare words that
are distributionally initialized and frequent words
that are not. It remains to be investigated whether
there are interactions between these two properties
of our model, e.g., a high rare-frequent separator
may work well for words whose corpus frequency
is much smaller than the separator.
Third, while we have shown that distributional
initialization improves the quality of representa-
tions of rare words, we did not investigate whether
distributional initialization for rare words has any
adverse effect on the quality of representations of
frequent words for which one-hot initialization is
applied. Since rare and frequent words are linked
in the mixed model, this possibility cannot be dis-
missed and we plan to investigate it in future work.
Acknowledgments. This work was supported
by Deutsche Forschungsgemeinschaft (grant DFG
SCHU 2246/10-1, FADeBaC).
</bodyText>
<page confidence="0.996612">
284
</page>
<sectionHeader confidence="0.996354" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999709112359551">
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673–721.
Marco Baroni, Silvia Bernardini, Adriano Ferraresi,
and Eros Zanchetta. 2009. The WaCky Wide Web:
A Collection of Very Large Linguistically Processed
Web-Crawled Corpora. Language Resources and
Evaluation, 43(3):209–226.
Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014.
Knowledge-powered deep learning for word embed-
ding. In Machine Learning and Knowledge Dis-
covery in Databases - European Conference, ECML
PKDD, pages 132–148.
Jan A. Botha and Phil Blunsom. 2014. Compositional
morphology for word representations and language
modelling. In Proceedings of the 31st International
Conference on Machine Learning (ICML), Beijing,
China, June.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in technicolor. In ACL, pages 136–145.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493–2537.
Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, and Tie-Yan
Liu. 2014. Knet: A general framework for learning
word embedding using morphological knowledge.
Preprint pubslished on arXiv arXiv:1407.1687
[cs.CL].
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2001. Placing search in context: The
concept revisited. In WWW, pages 406–414.
Felix Hill, Roi Reichart, and Anna Korhonen. 2014.
Simlex-999: Evaluating semantic models with (gen-
uine) similarity estimation. Preprint pubslished on
arXivarXiv:1408:3456 [cs.CL].
Hai Son Le, Alexandre Allauzen, Guillaume Wis-
niewski, and Franc¸ois Yvon. 2010. Training contin-
uous space language models: Some practical issues.
In EMNLP, pages 778–788.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, &amp; Computers, 28(2):203–208.
Minh-Thang Luong, Richard Socher, and Christo-
pher D. Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In CoNLL.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Workshop at ICLR.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
&amp; Cognitive Processes, 6(1):1–28.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1439.
Yoshiki Niwa and Yoshihiko Nitta. 1994. Co-
occurrence vectors from corpora vs. distance vec-
tors from dictionaries. In COLING, volume 1, pages
304–309.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Commun. ACM,
8(10):627–633, October.
Magnus Sahlgren. 2008. The distributional hypothe-
sis. Rivista di Linguistica (Italian Journal of Lin-
guistics), 20(1):33–53.
Helmut Schmid. 2000. Unsupervised Learning of Pe-
riod Disambiguation for Tokenisation. Technical re-
port, IMS, University of Stuttgart.
Hinrich Sch¨utze. 1992. Dimensions of Meaning. In
ACM/IEEE Conference on Supercomputing, pages
787–796.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In ACL, pages
384–394.
Peter D. Turney and Michael L. Littman. 2003.
Measuring praise and criticism: Inference of se-
mantic orientation from association. ACM TOIS,
21(4):315–346.
Peter D. Turney and Patrick Pantel. 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research
(JAIR), 37:141–188.
</reference>
<page confidence="0.998504">
285
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.865073">
<title confidence="0.9994075">Learning Better Embeddings for Rare Using Distributional Representations</title>
<author confidence="0.915454">Sergienya</author>
<affiliation confidence="0.9982745">Center for Information and Language University of Munich,</affiliation>
<email confidence="0.997132">sergienya@cis.lmu.de</email>
<abstract confidence="0.995236818181818">There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors, in which each dimension corresponds to a context word. In this paper, we initialize an embedding-learning model with distributional vectors. Evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Alessandro Lenci</author>
</authors>
<title>Distributional memory: A general framework for corpus-based semantics.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<issue>4</issue>
<contexts>
<context position="1166" citStr="Baroni and Lenci (2010)" startWordPosition="161" endWordPosition="164">ion significantly increases the quality of embeddings for rare words. 1 Introduction Standard neural network (NN) architectures for inducing embeddings have an input layer that represents each word as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013)). There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (Mitchell and Lapata, 2010) and sentiment analysis (Turney and Littman, 2003). In this paper, we investigate distributional initialization: the use of distributional vectors as representations of words at the input layer of NN architectures for embedding learning to improve the embeddings of rare words. It is difficult for onehot initialization to learn good embeddings from onl</context>
</contexts>
<marker>Baroni, Lenci, 2010</marker>
<rawString>Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based semantics. Computational Linguistics, 36(4):673–721.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
</authors>
<title>The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--3</pages>
<contexts>
<context position="5472" citStr="Baroni et al., 2009" startWordPosition="922" endWordPosition="925">s zero and the n-dimensional vector is its distributional vector. In mixed initialization, the input representation for a word is an n-dimensional vector: a one-hot vector for a word with frequency &gt; 0 and a distributional vector for a word with frequency &lt; 0. In summary, separate initialization uses separate representation spaces for frequent words (onehot space) and rare words (distributional space). Mixed initialization uses the same representation space for all words; and rare words share weights with the frequent words that they cooccur with. 3 Experimental setup We use ukWaC+WaCkypedia (Baroni et al., 2009), a corpus of 2.4 billion tokens and 6 million word types. Based on (Turian et al., 2010), we preprocess the corpus by removing sentences that are less than 90% lowercase; lowercasing; replacing URLs, email addresses and digits by special tokens; tokenization (Schmid, 2000); replacing words with frequency 1 with &lt;unk&gt;; and adding end-of-sentence tokens. After preprocessing, the size n of the context word vocabulary is 2.7 million. We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30),</context>
</contexts>
<marker>Baroni, Bernardini, Ferraresi, Zanchetta, 2009</marker>
<rawString>Marco Baroni, Silvia Bernardini, Adriano Ferraresi, and Eros Zanchetta. 2009. The WaCky Wide Web: A Collection of Very Large Linguistically Processed Web-Crawled Corpora. Language Resources and Evaluation, 43(3):209–226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Knowledge-powered deep learning for word embedding.</title>
<date>2014</date>
<booktitle>In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD,</booktitle>
<pages>132--148</pages>
<contexts>
<context position="17862" citStr="Bian et al. (2014)" startWordPosition="2991" endWordPosition="2994">summary, we have shown that distributional initialization improves the quality of word embeddings for rare words. Our recommendation is to use mixed initialization with PPMI weighting and the value 0 = 20 of the frequency threshold. 5 Related work An alternative to using distributional information for initialization is to use syntactic and semantic information for initialization. Approaches along these lines include Botha and Blunsom (2014) who represent a word as a sum of embedding vectors of its morphemes. Cui et al. (2014) use a weighted average of vectors of morphologically similar words. Bian et al. (2014) extend a word’s vector with vectors of entity categories and POS tags. This line of work also is partially motivated by improving the embeddings of rare words. Distributional information on the one hand and syntactic/semantic information on the other hand are likely to be complementary, so that a combination of our approach with this prior work is promising. Le et al. (2010) propose three schemes to address word embedding initialization. Reinitialization and iterative reinitialization use vectors from prediction space to initialize the context space during training. This approach is both more</context>
</contexts>
<marker>Bian, Gao, Liu, 2014</marker>
<rawString>Jiang Bian, Bin Gao, and Tie-Yan Liu. 2014. Knowledge-powered deep learning for word embedding. In Machine Learning and Knowledge Discovery in Databases - European Conference, ECML PKDD, pages 132–148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan A Botha</author>
<author>Phil Blunsom</author>
</authors>
<title>Compositional morphology for word representations and language modelling.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31st International Conference on Machine Learning (ICML),</booktitle>
<location>Beijing, China,</location>
<contexts>
<context position="17688" citStr="Botha and Blunsom (2014)" startWordPosition="2959" endWordPosition="2962">owperforming models and for a few high-performing models, the significance test takes this into account, so that the relatively high variability does not undermine our results. In summary, we have shown that distributional initialization improves the quality of word embeddings for rare words. Our recommendation is to use mixed initialization with PPMI weighting and the value 0 = 20 of the frequency threshold. 5 Related work An alternative to using distributional information for initialization is to use syntactic and semantic information for initialization. Approaches along these lines include Botha and Blunsom (2014) who represent a word as a sum of embedding vectors of its morphemes. Cui et al. (2014) use a weighted average of vectors of morphologically similar words. Bian et al. (2014) extend a word’s vector with vectors of entity categories and POS tags. This line of work also is partially motivated by improving the embeddings of rare words. Distributional information on the one hand and syntactic/semantic information on the other hand are likely to be complementary, so that a combination of our approach with this prior work is promising. Le et al. (2010) propose three schemes to address word embedding</context>
</contexts>
<marker>Botha, Blunsom, 2014</marker>
<rawString>Jan A. Botha and Phil Blunsom. 2014. Compositional morphology for word representations and language modelling. In Proceedings of the 31st International Conference on Machine Learning (ICML), Beijing, China, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Nam Khanh Tran</author>
</authors>
<title>Distributional semantics in technicolor.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>136--145</pages>
<contexts>
<context position="6098" citStr="Bruni et al. (2012)" startWordPosition="1022" endWordPosition="1025">pus of 2.4 billion tokens and 6 million word types. Based on (Turian et al., 2010), we preprocess the corpus by removing sentences that are less than 90% lowercase; lowercasing; replacing URLs, email addresses and digits by special tokens; tokenization (Schmid, 2000); replacing words with frequency 1 with &lt;unk&gt;; and adding end-of-sentence tokens. After preprocessing, the size n of the context word vocabulary is 2.7 million. We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN1 (Bruni et al. (2012), 3000), WordSim3532 (Finkelstein et al. (2001), 353), Stanford Rare Word3 (Luong et al. (2013), 2034) and SimLex-9994 (Hill et al. (2014), 999). We exclude from the evaluation the 16 pairs in RW that contain a word that does not occur in our corpus. Our goal in this paper is to investigate the effect of using distributional initialization vs. onehot initialization on the quality of embeddings of rare words. However, except for RW, the six data sets contain only a single word with frequency &lt;100, all other words are more frequent. To address this issue, we artificially make all words in the si</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. 2012. Distributional semantics in technicolor. In ACL, pages 136–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="813" citStr="Collobert et al. (2011)" startWordPosition="111" endWordPosition="114">ny sergienya@cis.lmu.de Abstract There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors, in which each dimension corresponds to a context word. In this paper, we initialize an embedding-learning model with distributional vectors. Evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words. 1 Introduction Standard neural network (NN) architectures for inducing embeddings have an input layer that represents each word as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013)). There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (Mitchell and Lapata, 2010)</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qing Cui</author>
<author>Bin Gao</author>
<author>Jiang Bian</author>
<author>Siyu Qiu</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Knet: A general framework for learning word embedding using morphological knowledge. Preprint pubslished on arXiv arXiv:1407.1687 [cs.CL].</title>
<date>2014</date>
<contexts>
<context position="17775" citStr="Cui et al. (2014)" startWordPosition="2977" endWordPosition="2980">o account, so that the relatively high variability does not undermine our results. In summary, we have shown that distributional initialization improves the quality of word embeddings for rare words. Our recommendation is to use mixed initialization with PPMI weighting and the value 0 = 20 of the frequency threshold. 5 Related work An alternative to using distributional information for initialization is to use syntactic and semantic information for initialization. Approaches along these lines include Botha and Blunsom (2014) who represent a word as a sum of embedding vectors of its morphemes. Cui et al. (2014) use a weighted average of vectors of morphologically similar words. Bian et al. (2014) extend a word’s vector with vectors of entity categories and POS tags. This line of work also is partially motivated by improving the embeddings of rare words. Distributional information on the one hand and syntactic/semantic information on the other hand are likely to be complementary, so that a combination of our approach with this prior work is promising. Le et al. (2010) propose three schemes to address word embedding initialization. Reinitialization and iterative reinitialization use vectors from predi</context>
</contexts>
<marker>Cui, Gao, Bian, Qiu, Liu, 2014</marker>
<rawString>Qing Cui, Bin Gao, Jiang Bian, Siyu Qiu, and Tie-Yan Liu. 2014. Knet: A general framework for learning word embedding using morphological knowledge. Preprint pubslished on arXiv arXiv:1407.1687 [cs.CL].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Finkelstein</author>
<author>Evgeniy Gabrilovich</author>
<author>Yossi Matias</author>
<author>Ehud Rivlin</author>
<author>Zach Solan</author>
<author>Gadi Wolfman</author>
<author>Eytan Ruppin</author>
</authors>
<title>Placing search in context: The concept revisited.</title>
<date>2001</date>
<booktitle>In WWW,</booktitle>
<pages>406--414</pages>
<contexts>
<context position="6145" citStr="Finkelstein et al. (2001)" startWordPosition="1028" endWordPosition="1031">ord types. Based on (Turian et al., 2010), we preprocess the corpus by removing sentences that are less than 90% lowercase; lowercasing; replacing URLs, email addresses and digits by special tokens; tokenization (Schmid, 2000); replacing words with frequency 1 with &lt;unk&gt;; and adding end-of-sentence tokens. After preprocessing, the size n of the context word vocabulary is 2.7 million. We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN1 (Bruni et al. (2012), 3000), WordSim3532 (Finkelstein et al. (2001), 353), Stanford Rare Word3 (Luong et al. (2013), 2034) and SimLex-9994 (Hill et al. (2014), 999). We exclude from the evaluation the 16 pairs in RW that contain a word that does not occur in our corpus. Our goal in this paper is to investigate the effect of using distributional initialization vs. onehot initialization on the quality of embeddings of rare words. However, except for RW, the six data sets contain only a single word with frequency &lt;100, all other words are more frequent. To address this issue, we artificially make all words in the six data sets rare. We do this by keeping only 0 </context>
</contexts>
<marker>Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, Ruppin, 2001</marker>
<rawString>Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In WWW, pages 406–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Felix Hill</author>
<author>Roi Reichart</author>
<author>Anna Korhonen</author>
</authors>
<title>Simlex-999: Evaluating semantic models with (genuine) similarity estimation.</title>
<date>2014</date>
<note>Preprint pubslished on arXivarXiv:1408:3456 [cs.CL].</note>
<contexts>
<context position="6236" citStr="Hill et al. (2014)" startWordPosition="1043" endWordPosition="1046"> less than 90% lowercase; lowercasing; replacing URLs, email addresses and digits by special tokens; tokenization (Schmid, 2000); replacing words with frequency 1 with &lt;unk&gt;; and adding end-of-sentence tokens. After preprocessing, the size n of the context word vocabulary is 2.7 million. We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN1 (Bruni et al. (2012), 3000), WordSim3532 (Finkelstein et al. (2001), 353), Stanford Rare Word3 (Luong et al. (2013), 2034) and SimLex-9994 (Hill et al. (2014), 999). We exclude from the evaluation the 16 pairs in RW that contain a word that does not occur in our corpus. Our goal in this paper is to investigate the effect of using distributional initialization vs. onehot initialization on the quality of embeddings of rare words. However, except for RW, the six data sets contain only a single word with frequency &lt;100, all other words are more frequent. To address this issue, we artificially make all words in the six data sets rare. We do this by keeping only 0 randomly chosen occurrences in the corpus (for words with frequency &gt;0) and replacing all o</context>
</contexts>
<marker>Hill, Reichart, Korhonen, 2014</marker>
<rawString>Felix Hill, Roi Reichart, and Anna Korhonen. 2014. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Preprint pubslished on arXivarXiv:1408:3456 [cs.CL].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Son Le</author>
<author>Alexandre Allauzen</author>
<author>Guillaume Wisniewski</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Training continuous space language models: Some practical issues.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>778--788</pages>
<contexts>
<context position="18240" citStr="Le et al. (2010)" startWordPosition="3055" endWordPosition="3058">n. Approaches along these lines include Botha and Blunsom (2014) who represent a word as a sum of embedding vectors of its morphemes. Cui et al. (2014) use a weighted average of vectors of morphologically similar words. Bian et al. (2014) extend a word’s vector with vectors of entity categories and POS tags. This line of work also is partially motivated by improving the embeddings of rare words. Distributional information on the one hand and syntactic/semantic information on the other hand are likely to be complementary, so that a combination of our approach with this prior work is promising. Le et al. (2010) propose three schemes to address word embedding initialization. Reinitialization and iterative reinitialization use vectors from prediction space to initialize the context space during training. This approach is both more complex and less efficient than ours. One-vector initialization initializes all word embeddings with the same 8A reviewer asks: “If a word is rare, its distributional vector should also be sparse and less informative, which does not guarantee to be a good starting point.” This is true and it suggests that it may not be possible to learn a very high-quality representation for</context>
</contexts>
<marker>Le, Allauzen, Wisniewski, Yvon, 2010</marker>
<rawString>Hai Son Le, Alexandre Allauzen, Guillaume Wisniewski, and Franc¸ois Yvon. 2010. Training continuous space language models: Some practical issues. In EMNLP, pages 778–788.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Lund</author>
<author>Curt Burgess</author>
</authors>
<title>Producing high-dimensional semantic spaces from lexical cooccurrence.</title>
<date>1996</date>
<journal>Behavior Research Methods, Instruments, &amp; Computers,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="1098" citStr="Lund and Burgess (1996)" startWordPosition="151" endWordPosition="154">l vectors. Evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words. 1 Introduction Standard neural network (NN) architectures for inducing embeddings have an input layer that represents each word as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013)). There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (Mitchell and Lapata, 2010) and sentiment analysis (Turney and Littman, 2003). In this paper, we investigate distributional initialization: the use of distributional vectors as representations of words at the input layer of NN architectures for embedding learning to improve the embeddings of rare words. It is d</context>
</contexts>
<marker>Lund, Burgess, 1996</marker>
<rawString>Kevin Lund and Curt Burgess. 1996. Producing high-dimensional semantic spaces from lexical cooccurrence. Behavior Research Methods, Instruments, &amp; Computers, 28(2):203–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minh-Thang Luong</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Better word representations with recursive neural networks for morphology. In CoNLL.</title>
<date>2013</date>
<contexts>
<context position="6193" citStr="Luong et al. (2013)" startWordPosition="1036" endWordPosition="1039">ss the corpus by removing sentences that are less than 90% lowercase; lowercasing; replacing URLs, email addresses and digits by special tokens; tokenization (Schmid, 2000); replacing words with frequency 1 with &lt;unk&gt;; and adding end-of-sentence tokens. After preprocessing, the size n of the context word vocabulary is 2.7 million. We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN1 (Bruni et al. (2012), 3000), WordSim3532 (Finkelstein et al. (2001), 353), Stanford Rare Word3 (Luong et al. (2013), 2034) and SimLex-9994 (Hill et al. (2014), 999). We exclude from the evaluation the 16 pairs in RW that contain a word that does not occur in our corpus. Our goal in this paper is to investigate the effect of using distributional initialization vs. onehot initialization on the quality of embeddings of rare words. However, except for RW, the six data sets contain only a single word with frequency &lt;100, all other words are more frequent. To address this issue, we artificially make all words in the six data sets rare. We do this by keeping only 0 randomly chosen occurrences in the corpus (for w</context>
</contexts>
<marker>Luong, Socher, Manning, 2013</marker>
<rawString>Minh-Thang Luong, Richard Socher, and Christopher D. Manning. 2013. Better word representations with recursive neural networks for morphology. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Workshop at ICLR.</booktitle>
<contexts>
<context position="836" citStr="Mikolov et al. (2013)" startWordPosition="115" endWordPosition="118">bstract There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors, in which each dimension corresponds to a context word. In this paper, we initialize an embedding-learning model with distributional vectors. Evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words. 1 Introduction Standard neural network (NN) architectures for inducing embeddings have an input layer that represents each word as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013)). There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (Mitchell and Lapata, 2010) and sentiment analysis</context>
<context position="9452" citStr="Mikolov et al., 2013" startWordPosition="1568" endWordPosition="1571">onding one-hot correlations are set in bold (resp. marked *). currences to ensure that words from the evaluation data sets are rare in the corpus. We covary these two parameters in the experiments below; e.g., we apply distributional initialization with 0 = 20 to a corpus constructed to have 0 = 20 occurrences of words from similarity data sets. We do this to ensure that all evaluation words are rare words for the purpose of distributional initialization and so we can exploit all pairs in the evaluation data sets for evaluating the efficacy of our method for rare words. We modified word2vec5 (Mikolov et al., 2013) to accommodate distributional initialization; to support distributional vectors at the input layer, we changed the implementation of activation functions and backpropagation. We use the skipgram model, hierarchical softmax, set the size of the context window to 10 (10 words to the left and 10 to the right), min-count to 1 (train on all tokens), embedding size to 100, sampling rate to 10−3 and train models for one epoch. For four values of the frequency threshold, 0 E 110, 20, 50, 1001,6 we train word2vec models 5code.google.com/p/word2vec 6A reviewer asks whether the value of θ should depend </context>
<context position="12931" citStr="Mikolov et al., 2013" startWordPosition="2181" endWordPosition="2184"> in a different training corpus – a training corpus in which the number of occurrences of the words in the evaluation data sets has been reduced to &lt; 0 (cf. Section 3). Our results indicate that distributional initialization is beneficial for very rare words – those that occur no more than 20 times in the corpus. Our results for medium rare words – those that occur between 50 and 100 times – are less clear: either there are no improvements or improvements are small. Thus, our recommendation is to use 0 = 20. Scalability. The time complexity of the basic version of word2vec is O(ECW D log V ) (Mikolov et al., 2013) where E is the number of epochs, C is the corpus size, W is the context window size, D is the number of dimensions of the embedding space, and V is the vocabulary size. Distributional initialization adds a term I, the average number of entries in the distributional vectors, so that time complexity increases to O(IECWD log V ). For rare words, I is small, so that there is no big difference in efficiency between one-hot initialization and distributional initialization of word2vec. However, for frequent words I would be large, so that distributional initialization may not be scalable in that cas</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Workshop at ICLR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>Walter G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<journal>Language &amp; Cognitive Processes,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="6066" citStr="Miller and Charles (1991)" startWordPosition="1016" endWordPosition="1019">aCkypedia (Baroni et al., 2009), a corpus of 2.4 billion tokens and 6 million word types. Based on (Turian et al., 2010), we preprocess the corpus by removing sentences that are less than 90% lowercase; lowercasing; replacing URLs, email addresses and digits by special tokens; tokenization (Schmid, 2000); replacing words with frequency 1 with &lt;unk&gt;; and adding end-of-sentence tokens. After preprocessing, the size n of the context word vocabulary is 2.7 million. We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN1 (Bruni et al. (2012), 3000), WordSim3532 (Finkelstein et al. (2001), 353), Stanford Rare Word3 (Luong et al. (2013), 2034) and SimLex-9994 (Hill et al. (2014), 999). We exclude from the evaluation the 16 pairs in RW that contain a word that does not occur in our corpus. Our goal in this paper is to investigate the effect of using distributional initialization vs. onehot initialization on the quality of embeddings of rare words. However, except for RW, the six data sets contain only a single word with frequency &lt;100, all other words are more frequent. To address this issue, we artif</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>George A. Miller and Walter G. Charles. 1991. Contextual correlates of semantic similarity. Language &amp; Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="1413" citStr="Mitchell and Lapata, 2010" startWordPosition="204" endWordPosition="207">), Collobert et al. (2011), Mikolov et al. (2013)). There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (Mitchell and Lapata, 2010) and sentiment analysis (Turney and Littman, 2003). In this paper, we investigate distributional initialization: the use of distributional vectors as representations of words at the input layer of NN architectures for embedding learning to improve the embeddings of rare words. It is difficult for onehot initialization to learn good embeddings from only a few examples. In contrast, distributional initialization provides an additional source of information – the global distribution of the word in the corpus – that improves embeddings learned for rare words. We will demonstrate this type of impro</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshiki Niwa</author>
<author>Yoshihiko Nitta</author>
</authors>
<title>Cooccurrence vectors from corpora vs. distance vectors from dictionaries.</title>
<date>1994</date>
<booktitle>In COLING,</booktitle>
<volume>1</volume>
<pages>304--309</pages>
<contexts>
<context position="3194" citStr="Niwa and Nitta (1994)" startWordPosition="497" endWordPosition="500"> summarize related work in Section 5 and finish with conclusion in Section 6 and discussion of future work in Section 7. 2 Method Weighting. We use two different weighting schemes for distributional vectors. Let vi, ... , vn be the vocabulary of context words. In BINARY weighting, entry 1 G i G n in the distributional vector of target word w is set to 1 iff vi and w cooccur at a distance of at most ten words in the corpus and to 0 otherwise. In PPMI weighting, entry 1 G i G n in the distributional vector of target word w is set to the PPMI (positive pointwise mutual information, introduced by Niwa and Nitta (1994)) of w and vi. We divide PPMI values by their maximum to ensure they are in [0, 1] because we will combine one-hot vectors (whose values are 0/1) with PPMI weights and it is important that they are on the same scale. We use two different distributional initializations, shown in Figure 1: separate (left) and mixed (right). Combinations of these two initializations with both BINARY and PPMI weighting will be investigated in the experiments. Recall that n is the dimensionality of the distri280 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 280–285, L</context>
</contexts>
<marker>Niwa, Nitta, 1994</marker>
<rawString>Yoshiki Niwa and Yoshihiko Nitta. 1994. Cooccurrence vectors from corpora vs. distance vectors from dictionaries. In COLING, volume 1, pages 304–309.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert Rubenstein</author>
<author>John B Goodenough</author>
</authors>
<title>Contextual correlates of synonymy.</title>
<date>1965</date>
<journal>Commun. ACM,</journal>
<volume>8</volume>
<issue>10</issue>
<contexts>
<context position="6031" citStr="Rubenstein and Goodenough (1965)" startWordPosition="1010" endWordPosition="1013"> with. 3 Experimental setup We use ukWaC+WaCkypedia (Baroni et al., 2009), a corpus of 2.4 billion tokens and 6 million word types. Based on (Turian et al., 2010), we preprocess the corpus by removing sentences that are less than 90% lowercase; lowercasing; replacing URLs, email addresses and digits by special tokens; tokenization (Schmid, 2000); replacing words with frequency 1 with &lt;unk&gt;; and adding end-of-sentence tokens. After preprocessing, the size n of the context word vocabulary is 2.7 million. We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN1 (Bruni et al. (2012), 3000), WordSim3532 (Finkelstein et al. (2001), 353), Stanford Rare Word3 (Luong et al. (2013), 2034) and SimLex-9994 (Hill et al. (2014), 999). We exclude from the evaluation the 16 pairs in RW that contain a word that does not occur in our corpus. Our goal in this paper is to investigate the effect of using distributional initialization vs. onehot initialization on the quality of embeddings of rare words. However, except for RW, the six data sets contain only a single word with frequency &lt;100, all other words are more freque</context>
</contexts>
<marker>Rubenstein, Goodenough, 1965</marker>
<rawString>Herbert Rubenstein and John B. Goodenough. 1965. Contextual correlates of synonymy. Commun. ACM, 8(10):627–633, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The distributional hypothesis. Rivista di Linguistica (Italian</title>
<date>2008</date>
<journal>Journal of Linguistics),</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1115" citStr="Sahlgren (2008)" startWordPosition="155" endWordPosition="156">word similarity shows that this initialization significantly increases the quality of embeddings for rare words. 1 Introduction Standard neural network (NN) architectures for inducing embeddings have an input layer that represents each word as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013)). There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (Mitchell and Lapata, 2010) and sentiment analysis (Turney and Littman, 2003). In this paper, we investigate distributional initialization: the use of distributional vectors as representations of words at the input layer of NN architectures for embedding learning to improve the embeddings of rare words. It is difficult for oneh</context>
</contexts>
<marker>Sahlgren, 2008</marker>
<rawString>Magnus Sahlgren. 2008. The distributional hypothesis. Rivista di Linguistica (Italian Journal of Linguistics), 20(1):33–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Unsupervised Learning of Period Disambiguation for Tokenisation.</title>
<date>2000</date>
<tech>Technical report, IMS,</tech>
<institution>University of Stuttgart.</institution>
<contexts>
<context position="5746" citStr="Schmid, 2000" startWordPosition="968" endWordPosition="969"> initialization uses separate representation spaces for frequent words (onehot space) and rare words (distributional space). Mixed initialization uses the same representation space for all words; and rare words share weights with the frequent words that they cooccur with. 3 Experimental setup We use ukWaC+WaCkypedia (Baroni et al., 2009), a corpus of 2.4 billion tokens and 6 million word types. Based on (Turian et al., 2010), we preprocess the corpus by removing sentences that are less than 90% lowercase; lowercasing; replacing URLs, email addresses and digits by special tokens; tokenization (Schmid, 2000); replacing words with frequency 1 with &lt;unk&gt;; and adding end-of-sentence tokens. After preprocessing, the size n of the context word vocabulary is 2.7 million. We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN1 (Bruni et al. (2012), 3000), WordSim3532 (Finkelstein et al. (2001), 353), Stanford Rare Word3 (Luong et al. (2013), 2034) and SimLex-9994 (Hill et al. (2014), 999). We exclude from the evaluation the 16 pairs in RW that contain a word that does not occur in our corpu</context>
</contexts>
<marker>Schmid, 2000</marker>
<rawString>Helmut Schmid. 2000. Unsupervised Learning of Period Disambiguation for Tokenisation. Technical report, IMS, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Dimensions of Meaning.</title>
<date>1992</date>
<booktitle>In ACM/IEEE Conference on Supercomputing,</booktitle>
<pages>787--796</pages>
<marker>Sch¨utze, 1992</marker>
<rawString>Hinrich Sch¨utze. 1992. Dimensions of Meaning. In ACM/IEEE Conference on Supercomputing, pages 787–796.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev-Arie Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="788" citStr="Turian et al. (2010)" startWordPosition="107" endWordPosition="110">rsity of Munich, Germany sergienya@cis.lmu.de Abstract There are two main types of word representations: low-dimensional embeddings and high-dimensional distributional vectors, in which each dimension corresponds to a context word. In this paper, we initialize an embedding-learning model with distributional vectors. Evaluation on word similarity shows that this initialization significantly increases the quality of embeddings for rare words. 1 Introduction Standard neural network (NN) architectures for inducing embeddings have an input layer that represents each word as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013)). There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (M</context>
<context position="5561" citStr="Turian et al., 2010" startWordPosition="939" endWordPosition="942">, the input representation for a word is an n-dimensional vector: a one-hot vector for a word with frequency &gt; 0 and a distributional vector for a word with frequency &lt; 0. In summary, separate initialization uses separate representation spaces for frequent words (onehot space) and rare words (distributional space). Mixed initialization uses the same representation space for all words; and rare words share weights with the frequent words that they cooccur with. 3 Experimental setup We use ukWaC+WaCkypedia (Baroni et al., 2009), a corpus of 2.4 billion tokens and 6 million word types. Based on (Turian et al., 2010), we preprocess the corpus by removing sentences that are less than 90% lowercase; lowercasing; replacing URLs, email addresses and digits by special tokens; tokenization (Schmid, 2000); replacing words with frequency 1 with &lt;unk&gt;; and adding end-of-sentence tokens. After preprocessing, the size n of the context word vocabulary is 2.7 million. We evaluate on six word similarity judgment data sets (number of pairs in parentheses): RG (Rubenstein and Goodenough (1965), 65) MC (Miller and Charles (1991), 30), MEN1 (Bruni et al. (2012), 3000), WordSim3532 (Finkelstein et al. (2001), 353), Stanford</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In ACL, pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Measuring praise and criticism: Inference of semantic orientation from association.</title>
<date>2003</date>
<journal>ACM TOIS,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="1463" citStr="Turney and Littman, 2003" startWordPosition="212" endWordPosition="215"> There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (Mitchell and Lapata, 2010) and sentiment analysis (Turney and Littman, 2003). In this paper, we investigate distributional initialization: the use of distributional vectors as representations of words at the input layer of NN architectures for embedding learning to improve the embeddings of rare words. It is difficult for onehot initialization to learn good embeddings from only a few examples. In contrast, distributional initialization provides an additional source of information – the global distribution of the word in the corpus – that improves embeddings learned for rare words. We will demonstrate this type of improvement in the experiments reported below. In summa</context>
</contexts>
<marker>Turney, Littman, 2003</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM TOIS, 21(4):315–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research (JAIR),</journal>
<pages>37--141</pages>
<contexts>
<context position="1141" citStr="Turney and Pantel (2010)" startWordPosition="157" endWordPosition="160">hows that this initialization significantly increases the quality of embeddings for rare words. 1 Introduction Standard neural network (NN) architectures for inducing embeddings have an input layer that represents each word as a one-hot vector (e.g., Turian et al. (2010), Collobert et al. (2011), Mikolov et al. (2013)). There is no usable information available in this input-layer representation except for the identity of the word. We call this standard initialization method one-hot initialization. Distributional representations (e.g., Sch¨utze (1992), Lund and Burgess (1996), Sahlgren (2008), Turney and Pantel (2010), Baroni and Lenci (2010)) represent a word as a highdimensional vector in which each dimension corresponds to a context word. They have been successfully used for a wide variety of tasks in natural language processing such as phrase similarity (Mitchell and Lapata, 2010) and sentiment analysis (Turney and Littman, 2003). In this paper, we investigate distributional initialization: the use of distributional vectors as representations of words at the input layer of NN architectures for embedding learning to improve the embeddings of rare words. It is difficult for onehot initialization to learn</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research (JAIR), 37:141–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>
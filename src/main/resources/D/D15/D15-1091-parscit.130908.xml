<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000009">
<title confidence="0.994708">
TSDPMM: Incorporating Prior Topic Knowledge into Dirichlet Process
Mixture Models for Text Clustering
</title>
<author confidence="0.667921">
Linmei Hu†, Juanzi Li†, Xiaoli Li�, Chao Shao†, Xuzhong Wang§
</author>
<affiliation confidence="0.347765">
† Dept. of Computer Sci. and Tech., Tsinghua University, China
$ Institute for Infocomm Research(I2R), A*STAR, Singapore
§ State Key Laboratory of Math. Eng. and Advanced Computing, China
</affiliation>
<email confidence="0.797591">
{hulinmei1991, lijuanzi2008}@gmail.com
xlli@i2r.a-star.edu.sg, {birdlinux, koodoneko}@gmail.com
</email>
<sectionHeader confidence="0.994506" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999959578947369">
Dirichlet process mixture model (DPM-
M) has great potential for detecting the
underlying structure of data. Extensive
studies have applied it for text cluster-
ing in terms of topics. However, due to
the unsupervised nature, the topic cluster-
s are always less satisfactory. Considering
that people often have some prior knowl-
edge about which potential topics should
exist in given data, we aim to incorpo-
rate such knowledge into the DPMM to
improve text clustering. We propose a
novel model TSDPMM based on a new
seeded P´olya urn scheme. Experimen-
tal results on document clustering across
three datasets demonstrate our proposed
TSDPMM significantly outperforms state-
of-the-art DPMM model and can be ap-
plied in a lifelong learning framework.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929298245614">
Dirichlet process mixture model (DPMM) (Neal,
2000) has been used in detecting the underlying
structure in data. For example, (Vlachos et al.,
2008; Vlachos et al., 2009) applied it to lexical-
semantic verb clustering. (Wang et al., 2011;
Huang et al., 2013; Yin and Wang, 2014) applied
it for text clustering in terms of their topics. While
DPMM achieved some promising results, it can
still sometimes produce unsatisfactory topic clus-
ters due to its unsupervised nature.
On the other hand, people often have prior
knowledge about what potential topics should ex-
ist in a given text corpus. Take an earthquake event
corpus as an example. The topics, such as “ca-
sualties and damages”, “rescue” and “government
reaction”, called prior topics, are expected to oc-
cur in the corpus according to our common knowl-
edge (e.g., the topics automatically learned from
previous events using topic modeling (Ahmed and
Xing, 2008)) or external resources (e.g., table of
contents at Wikipedia event pages 1). Similarly, in
academic fields, “call for papers (CFP)” of confer-
ences 2 lists main topics that conference organizers
would like to focus on. Clearly, these prior topic-
s can be represented as sets of words, which are
available in many real-world applications. They
can serve as weakly supervised information to en-
hance the unsupervised DPMM for text clustering.
Standard DPMM (Neal, 2000; Ranganathan,
2006) lacks a mechanism for incorporating pri-
or knowledge. Some existing work (Vlachos et
al., 2008; Vlachos et al., 2009) added knowledge
of observed instance-level constraints (must-links
and cannot-links between documents) to DPMM.
(Ahmed and Xing, 2008) proposed recurrent Chi-
nese Restaurant Process to incorporate previous
documents with known topic clusters. We focus
on incorporating topic-level knowledge, which is
more challenging, as seed/prior topics could be la-
tent rather than observable.
Particularly, we construct our novel TSDPM-
M (Topic Seeded DPMM) based on a principled
seeded P´olya urn (sPU) scheme. Our model inher-
its the nonparametric property of DPMM and has
additional technical merits. Importantly, our mod-
el is encouraged but not forced to find evidences of
seed topics. Therefore, it has freedom to discover
new topics beyond prior topics, as well as to detect
which prior topics are not covered by current da-
ta. It is thus convenient to observe topic variations
between prior topics and newly mined topics. Ex-
perimental results on document clustering across
three corpora demonstrate that our model effec-
tively incorporates prior topics, and significantly
outperforms state-of-the-art DPMM model. Par-
ticularly, our TSDPMM can be applied in a life-
long learning framework which enables the prior
</bodyText>
<footnote confidence="0.886979">
&apos;e.g., http://en.wikipedia.org/wiki/2010 Chile earthquake
ze.g., https://nips.cc/Conferences/2014/CallForPapers
</footnote>
<page confidence="0.901844">
787
</page>
<note confidence="0.6820515">
Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 787–792,
Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics.
</note>
<bodyText confidence="0.9662215">
topic knowledge to evolve as more and more data
are observed.
</bodyText>
<sectionHeader confidence="0.955104" genericHeader="method">
2 Topic Seeded DPMM
</sectionHeader>
<bodyText confidence="0.999930714285714">
In this section, we first introduce the standard
DPMM model for document clustering in terms
of topics. Then we describe how to incorporate
seed/prior topics into the model using a seeded
P´olya urn (sPU) scheme, which gives us our novel
TSDPMM model (Topic Seeded DPMM). Finally,
we present the model inference.
</bodyText>
<subsectionHeader confidence="0.810742">
2.1 DPMM
</subsectionHeader>
<bodyText confidence="0.999355666666666">
The DPMM (Antoniak, 1974) as a non-parametric
model assumes the given data is governed by an
infinite number of components where only a frac-
tion of these components are activated by the da-
ta. Figure 1 illustrates the DPMM graphical model
and its generative process of a document xi. First,
we sample a topic Bi = {Bij}j=|V |
j=1 (a multinomial
distribution over words belonging to the vocabu-
lary V ) for the document xi according to a Dirich-
let Process (DP) G — DP(α, G0), where α &gt; 0
is a concentration parameter and the base measure
G0 = Dir(⃗β) can be considered as a prior distri-
bution for B. Consider the document xi as a bag of
words, given the topic Bi, the generative distribu-
tion F is a given likelihood function parameterized
by B. We define F as p(xi|Bi) = ∏|x�|
j=1 p(xij|Bi),
where xij is the jth word in xi. Note that the DP-
MM assumes each document can be assigned to
one topic cluster only.
</bodyText>
<figureCaption confidence="0.992554">
Figure 1: Graphical Representation of DPMM.
</figureCaption>
<bodyText confidence="0.999562">
The DP process of DPMM, according to which
topic Bi for a document xi is drawn, can be ex-
plained by the popular metaphor of P´olya urn (PU)
scheme (Blackwell and MacQueen, 1973), equiv-
alent to the Chinese Restaurant Process (Ahmed
and Xing, 2008). The PU scheme works on balls
(documents) and colors (topics). It starts with an
empty urn. With probability proportional to α, we
draw Bi — G0, and add a ball of this color to the
urn. With probability proportional to i — 1 (i.e.,
the current number of balls in the urn), we draw
a ball at random from the urn, observe its color Bi
and replace the ball with two balls of the same col-
or. In this way, we draw topic Bi for document xi.
As shown in the process, the prior probability of
assigning a document to a topic is proportional to
the number of documents already assigned to the
topic. As a result, the DPMM exhibits the “rich
get richer” property.
</bodyText>
<subsectionHeader confidence="0.995602">
2.2 TSDPMM: Incorporating Seed Topics
</subsectionHeader>
<bodyText confidence="0.8937188">
In this section, we describe our proposed algorith-
m to incorporate prior seed topics into the DPM-
M. A prior/seed topic k is represented by a vec-
tor ⃗N(0)
k (word frequencies under the topic). We
</bodyText>
<equation confidence="0.864095">
⃗N(0)
k
</equation>
<bodyText confidence="0.88070075">
from past learning of topic models or external re-
sources such as Wikipedia and “CFP”. Assuming
we have K(0) prior topics, we use the parame-
ter ⃗α(0) = {α(0)
</bodyText>
<equation confidence="0.984064">
k }K(0)
</equation>
<bodyText confidence="0.986757714285714">
k=1 to control our confidence
about how likely each prior topic exists. Let us
go back to P´olya urn (PU) scheme, where a prior
topic can be taken as a known color. We extend
the PU scheme to incorporate prior topics, which
gives the sPU (seeded P´olya Urn) scheme. The
sPU scheme can be described as follows:
</bodyText>
<listItem confidence="0.960324666666667">
• We start with an urn with α(0)
k balls of each
known color k E {1, ..., K(0)}.
• With a probability proportional to α, we draw
Bi — G0 and add a ball of this color to the urn.
• With probability proportional to i — 1 +
</listItem>
<equation confidence="0.9977675">
∑K(0)
k=1 α(0)
</equation>
<bodyText confidence="0.979066275862069">
k , we draw a random ball from the
urn, and replace the ball with two balls of the
same color.
As shown in the above process, instead of start-
ing with an empty urn in DPMM, we assume that
the urn already has certain balls of known colors.
In this way, we incorporate the prior seed topic-
s. The number of initial balls (documents) α(0)
k
controls how likely the topic k exists. We can use
different values of α(0)
k for prior topics with dif-
ferent confidence levels. This sPU scheme gives
our novel model TSDPMM (Topic Seeded DPM-
M) incorporating prior topics. The TSDPMM has
can obtain the prior topics represented by
788
k,
similar graphical representation as DPMM (Fig-
ure 1), except the introduction of hyper-parameter
⃗α(0). We then present a collapsed gibbs sampling
algorithm for model inference as follows.
TSDPMM Inference. The model inference is
described in detail in Algorithm 1. It first ini-
tializes all documents with random topic clusters.
Then it iteratively updates the topic cluster assign-
ments of documents according to the conditional
probabilities (Eq.1) until convergence. Eq.1 can
be derived as:
</bodyText>
<equation confidence="0.94641">
p(zi|⃗Z−i, ⃗X) ∝ p(zi|⃗Z−i, α, ⃗α(0))p(xi|⃗X−i, ⃗Z, ⃗β)
(1)
</equation>
<bodyText confidence="0.99942875">
where zi is the topic assignment of observation xi,
X⃗ is the given document corpus, and ⃗Z−i are ⃗X−i
are the set of topic assignments and the corpus ex-
cluding the ith observation xi, respectively.
</bodyText>
<sectionHeader confidence="0.350719" genericHeader="method">
Algorithm 1: Collapsed Gibbs Sampling
</sectionHeader>
<bodyText confidence="0.985693352941176">
In Eq.1, the first item p(zi=k|⃗Z−i, α, ⃗α(0)) de-
notes a prior probability of zi=k, which is pro-
portional to the number of documents already as-
signed to it. If k is a prior topic, it is propor-
tional to nk,−i + α(0)
k , where nk,−i is the num-
ber of documents of topic k excluding the cur-
rent document xi. If k is an existing (not pri-
or) topic, it is proportional to nk,−i. If k is a
new topic, the probability is proportional to α.
The second item p(xi |⃗X−i, ⃗Z−i, zi = k, ⃗β) is
the likelihood of xi given ⃗X−i, ⃗Z−i and zi=k.
They can be derived as p(xi |⃗X−i, ⃗Z−i, zi =
−i |⃗Z−i,⃗β)where p( ⃗X|⃗Z, ⃗β) =
f p( ⃗X|⃗Z, Θ)p(Θ|⃗β)dΘ. As p(Θ|⃗β) is a Dirich-
let distribution and p( ⃗X|⃗Z, Θ) is a multinomial
distribution, we can get p( ⃗X |⃗Z)=HK ∆( ⃗Nk+⃗β)
</bodyText>
<equation confidence="0.888634">
k=1 ∆(⃗β) ,
</equation>
<bodyText confidence="0.9927605">
of occurrences of word w in the kth topic. Here,
we adopt the function ∆ in (Heinrich, 2009), and
</bodyText>
<equation confidence="0.997579">
∏V
⃗β) = w=1 Γ(βR and ∆( ⃗Nk + ⃗β) =
Γ(∑Vw=1)h&apos;
∏Vw v1 Γ(Nk,w+β) Finally, we can derive:
Γ(∑w=1(Nk,w+β))
</equation>
<bodyText confidence="0.99835">
where ⃗Nk,−i is a vector with the word counts for
all the documents assigned to topic k excluding xi,
</bodyText>
<equation confidence="0.975853">
⃗N(0)
</equation>
<bodyText confidence="0.969311538461538">
k are vectors with word counts in doc-
ument xi and in all the documents assigned to k
in prior knowledge respectively. According to this
equation, documents are likely to go into clusters
which are bigger and give higher likelihood of the
documents. When the Gibbs sampler converges,
we obtain topic cluster assignments of all the doc-
uments. Different from DPMM inference process
in which topics are removed when no documents is
assigned to them, TSDPMM inference can retain
prior topics all the time due to the initial number
of documents ⃗α(0), making it able to track prior
topics, as well as to detect new topics.
</bodyText>
<sectionHeader confidence="0.999689" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999916833333333">
We evaluate our proposed TSDPMM model for
document clustering on 3 datasets where each
cluster corresponds to a topic. We implement both
DPMM and TSDPMM models — their source
codes are available at https://github.com/
newsminer/DPMM_and_TSDPMM.
</bodyText>
<subsectionHeader confidence="0.975235">
3.1 Datasets
</subsectionHeader>
<bodyText confidence="0.95239825">
We collect machine learning conference NIPS
datasets composed of paper titles and abstracts
from 2012 to 2014 – each year includes 342, 360
and 411 documents respectively. They are named
as NIPS-12, NIPS-13 and NIPS-14.
We also employ the standard benchmark news
datasets, including 20 Newsgroups 3 and Reuters-
21578. As news is often timely reported, we
choose three continuous days with the largest
number of documents in 20 Newsgroups (i.e. 11,
12 and 13 May) and Reuters-21578 (i.e. 3, 4 and
5 March) for our experiments. These datasets are
</bodyText>
<figure confidence="0.877639816326531">
Input: Document dataset X⃗ = {xi}mi=1, prior
topics { ⃗N(0)
k }K(0)
k=1 , parameter ⃗α(0)
Output: Topic assignments Z⃗ of all
documents
Initialize the topic assignments Z⃗ based on
prior topics randomly;
repeat
Select a document xi ∈
X⃗ randomly
Fix the other topic assignments ⃗Z−i
Assign a new value to
zi:zi ∼ p(zi|⃗Z−i, ⃗X)(Eq. 1)
until Convergence;
p(zi = k|
⃗Z−i,
⃗X)
⃗N.,i+⃗Nk,−i+
(nk,−i + α(0)) · ∆(
{
∝
∆(
nk,−i ·
existing
∆( ⃗Nk,−i+⃗β)
⃗N.,i+⃗β)
∆(
∆(
α ·
new ,
⃗β)
⃗Nk,−i+
⃗N.,i+⃗Nk,−i+⃗β)
∆(
⃗N(0)
k +⃗β) prior
⃗N(0)
k +⃗β)
⃗β) ∝ p(
⃗X
p
(
⃗Z,
⃗X|
⃗β)
we have ∆(
⃗N.,i and
where ⃗Nk = {Nk,w}Vw=1 and Nk,w is the number 3http://people.csail.mit.edu/jrennie/20Newsgroups/
</figure>
<page confidence="0.981069">
789
</page>
<bodyText confidence="0.9888244">
denoted as 20N-1, 20N-2, 20N-3 (including 103,
96, 106 documents) and Reu-1, Reu-2, Reu-3 ( in-
cluding 282, 249, 207 documents), respectively.
For all the datasets, we conduct the following
preprocessing: (1) Convert letters into lowercase;
</bodyText>
<listItem confidence="0.9988585">
(2) Remove non-Latin characters and stop words;
(3) Remove words with document frequency &lt; 2.
</listItem>
<subsectionHeader confidence="0.998736">
3.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999981871794872">
We take the standard DPMM as our baseline
method and compare it with our proposed TSDP-
MM model using different prior knowledge ob-
tained with different manners.
For NIPS datasets, we use two kinds of prior
knowledge: one is the topics learned by DPMM
from previous year’s dataset; the other one is from
an external resource “CFP” 4 (10 topics, same for
each year). We name them as TSDPMM-P and
TSDPMM-E respectively. As the topic descrip-
tions in “CFP” are sparse, we repeat each topic
description by ten times and then represent a topic
with the words with word frequencies in its de-
scription text.
For both 20 Newsgroups and Reuters datasets,
we use prior knowledge learned by DPMM from
the previous day’s dataset. Furthermore, to test
if we can improve the results continuously by ap-
plying TSDPMM, every time when we model a
new dataset, we incorporate prior topics learned
by TSDPMM from previous day’s dataset, similar
to lifelong learning (Chen and Liu, 2014; Thrun,
1998). We call this model as TSDPMM-L.
Parameter Setting. Following a previous work
(Vlachos et al., 2009), we set the hyper-parameters
α=1, ⃗α(0)={1.0}, β⃗ ={1.0}. We run Gibbs sam-
pler for 100 iterations and stop the iteration once
the log-likelihood of the training data converges.
Evaluation. The widely used NMI (normal-
ized mutual information) measure (Dom, 2002),
has been employed to evaluate document cluster-
ing results. The higher a value of NMI, the better
a clustering result is. However, NMI needs true
class labels for documents, and can only be ap-
plied to our benchmark news datasets. For NIPS
datasets without true labels, we use the measure of
perplexity, as defined in (Blei et al., 2003), to test
per-word likelihood of the datasets. The lower the
perplexity, the better a model fits the data.
</bodyText>
<footnote confidence="0.872742">
4https://nips.cc/Conferences/2014/CallForPapers
</footnote>
<subsectionHeader confidence="0.958632">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.99967945">
Table 1 shows the average perplexity values of
five runs of 3 models on NIPS datasets. It shows
that both TSDPMM-P and TSDPMM-E, lever-
aging prior topics from previous learning and
“CFP” significantly outperform DPMM. In ad-
dition, TSDPMM-E achieves lower performance
than TSDPMM-P due to its lower quality of prior
topics directly obtained from “CFP”, compared to
higher quality topics from past learning. We may
improve “CFP” knowledge by extending it with
related texts from search engines or Wikipedia us-
ing keywords in “CFP” in future work.
An insight of our clustering results on NIPS-14
dataset suggests that most prior topics in 2013 are
covered again in 2014 (consistent topics), except
a few missing topics such as “lasso for Bayesian
networks”. Additionally, some newly evolved top-
ics in 2014, e.g. “monte carlo particle filtering”
and “nash games”, are successfully discovered by
our proposed model.
</bodyText>
<table confidence="0.9996255">
Models NIPS-12 NIPS-13 NIPS-14
DPMM 321.7 317.1 362.9
TSDPMM-P 290.1 298.7 346.8
TSDPMM-E 307.5 315.6 360.1
</table>
<tableCaption confidence="0.9086845">
Table 1: Average perplexity of different models on
NIPS.
</tableCaption>
<bodyText confidence="0.961451153846154">
Table 2 illustrates the average NMI values of
five runs of DPMM, TSDPMM and TSDPMM-L
on news datasets. The results show that TSDP-
MM using prior topics learnt by DPMM outper-
forms DPMM (on average +5.8%; p &lt;0.025 with
t-test). Additionally, TSDPMM-L, which continu-
ously uses prior topics learnt by TSDPMM from
previous dataset, further outperforms TSDPMM
(on average +3.2%; p &lt;0.025 with t-test). Note
TSDPMM-L uses TSDPMM results of 20N-1 and
Reu-1 as prior knowledge for the first time, so
there are no TSDPMM-L results for the first days
in Table 2 for 20N-1 and Reu-1 respectively.
</bodyText>
<subsectionHeader confidence="0.936049">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.999955571428571">
The experimental results across 3 datasets have
demonstrated that our proposed models can im-
prove DPMM model by incorporating prior top-
ic knowledge, and the higher-quality knowledge
will lead to better results. By applying our TS-
DPMM in a lifelong continuous learning frame-
work, namely TSDPMM-L, can further improve
</bodyText>
<page confidence="0.986992">
790
</page>
<table confidence="0.9992065">
Models 20N-1 20N-2 20N-3 Reu-1 Reu-2 Reus-3
DPMM 0.610 0.537 0.590 0.509 0.647 0.653
TSDPMM 0.645 0.610 0.681 0.648 0.654 0.655
TSDPMM-L ― 0.681 0.697 ― 0.689 0.656
</table>
<tableCaption confidence="0.999117">
Table 2: Average NMI of different models on news datasets.
</tableCaption>
<bodyText confidence="0.996169">
text clustering due to the better prior topic knowl-
edge obtained in the evolving environment.
</bodyText>
<sectionHeader confidence="0.999962" genericHeader="evaluation">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999965">
Our work is related to papers (Vlachos et al.,
2008; Vlachos et al., 2009), which added supervi-
sion (instance-level must-links or cannot-links be-
tween documents) to the DPMM. (Ahmed and X-
ing, 2008) proposed recurrent Chinese Restauran-
t Process to incorporate previous documents with
known topic clusters. However, our work is very
different as we focus on how to incorporate latent
topic-level prior knowledge. We model prior top-
ics as known colors that have a certain probability
proportional to α(0)
k to be assigned to a document.
In addition, our inference mechanism subsequent-
ly takes the prior knowledge into consideration for
automatically assigning topics to documents.
Some existing studies such as (Ramage et al.,
2009; Andrzejewski et al., 2009; Jagarlamudi et
al., 2012; Andrzejewski et al., 2011) worked on
incorporating prior lexical or domain knowledge
into LDA. Different from all these work, we focus
on the nonparametric model DPMM and propose
to incorporate the prior topic knowledge obtained
in multiple ways.
</bodyText>
<sectionHeader confidence="0.998925" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999974133333333">
In this paper, we propose a novel problem of in-
corporating prior topics into DPMM model and
address it through a simple yet principled seeded
P´olya urn scheme. We show that the topic knowl-
edge can be obtained in multiple ways. Exper-
iments on document clustering across 3 dataset-
s demonstrate our proposed model can effectively
incorporate the prior topic knowledge and signifi-
cantly enhance the standard DPMM for text clus-
tering. In future work, we will study how to dis-
cover overlapping clusters, i.e., allowing one doc-
ument to be grouped into multiple topic cluster-
s. We will also explore how to incorporate prior
knowledge about topic relations (such as causation
and correlation) into topic modeling.
</bodyText>
<sectionHeader confidence="0.998114" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.7164575">
The work is supported by 973 Program
(No. 2014CB340504), NSFC-ANR (No.
61261130588), Tsinghua University Initiative
Scientific Research Program (No. 20131089256),
Science and Technology Support Program (No.
2014BAK04B00), and THU-NUS NExT Co-Lab.
</reference>
<sectionHeader confidence="0.964334" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997344514285714">
Amr Ahmed and Eric P Xing. 2008. Dynamic non-
parametric mixture models and the recurrent chinese
restaurant process: with applications to evolutionary
clustering. In SDM, pages 219–230. SIAM.
David Andrzejewski, Xiaojin Zhu, and Mark Craven.
2009. Incorporating domain knowledge into topic
modeling via dirichlet forest priors. In Proceedings
of the 26th Annual ICML, pages 25–32. ACM.
David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorpo-
rating general domain knowledge into latent dirich-
let allocation using first-order logic. In Proceedings-
IJCAI, volume 22, page 1171.
Charles E Antoniak. 1974. Mixtures of dirichlet pro-
cesses with applications to bayesian nonparametric
problems. The annals of statistics, pages 1152–
1174.
David Blackwell and James B MacQueen. 1973. Fer-
guson distributions via p´olya urn schemes. The an-
nals of statistics, pages 353–355.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. Journal of ma-
chine Learning research, 3:993–1022.
Zhiyuan Chen and Bing Liu. 2014. Mining topics in
documents: standing on the shoulders of big data. In
Proceedings of the 20th ACM SIGKDD internation-
al conference, pages 1116–1125. ACM.
Byron E Dom. 2002. An information-theoretic ex-
ternal cluster-validity measure. In Proceedings of
the Eighteenth conference on Uncertainty in artifi-
cial intelligence, pages 137–145. Morgan Kaufmann
Publishers Inc.
Gregor Heinrich. 2009. Parameter estimation for text
analysis. Technical report, vsonix GmbH and Uni-
versity of Leipzig.
</reference>
<page confidence="0.976164">
791
</page>
<reference confidence="0.993209658536585">
Ruizhang Huang, Guan Yu, Zhaojun Wang, Jun Zhang,
and Liangxing Shi. 2013. Dirichlet process
mixture model for document clustering with fea-
ture partition. Knowledge and Data Engineering,,
25(8):1748–1759.
Jagadeesh Jagarlamudi, Hal Daum´e III, and Raghaven-
dra Udupa. 2012. Incorporating lexical priors into
topic models. In Proceedings of the 13th Conference
of the European Chapter of the ACL, pages 204–213.
Association for Computational Linguistics.
Radford M Neal. 2000. Markov chain sampling meth-
ods for dirichlet process mixture models. Journal
of computational and graphical statistics, 9(2):249–
265.
Daniel Ramage, David Leo Wright Hall, Ramesh Nal-
lapati, and Christopher D. Manning. 2009. Labeled
LDA: A supervised topic model for credit attribution
in multi-labeled corpora. In Proceedings of the 2009
Conference on EMNLP, pages 248–256.
Ananth Ranganathan. 2006. The dirichlet process
mixture (dpm) model. Technical report, Citeseer.
Sebastian Thrun. 1998. Lifelong learning algorithms.
In Learning to learn, pages 181–209. Springer.
Andreas Vlachos, Zoubin Ghahramani, and Anna Ko-
rhonen. 2008. Dirichlet process mixture models for
verb clustering. In Proceedings of the ICML work-
shop on Prior Knowledge for Text and Language.
Andreas Vlachos, Anna Korhonen, and Zoubin
Ghahramani. 2009. Unsupervised and constrained
dirichlet process mixture models for verb cluster-
ing. In Proceedings of the workshop on geometrical
models of natural language semantics, pages 74–82.
Association for Computational Linguistics.
Chan Wang, Caixia Yuan, Xiaojie Wang, and Wenwei
Xue. 2011. Dirichlet process mixture models based
topic identification for short text streams. In NLP-
KE, pages 80–87. IEEE.
Jianhua Yin and Jianyong Wang. 2014. A dirich-
let multinomial mixture model-based approach for
short text clustering. In Proceedings of the 20th
ACM SIGKDD, pages 233–242. ACM.
</reference>
<page confidence="0.997383">
792
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.517139">
<title confidence="0.9976645">TSDPMM: Incorporating Prior Topic Knowledge into Dirichlet Mixture Models for Text Clustering</title>
<author confidence="0.997688">Juanzi Xiaoli Chao Xuzhong</author>
<affiliation confidence="0.999464">of Computer Sci. and Tech., Tsinghua University,</affiliation>
<address confidence="0.637646">for Infocomm Research(I2R), A*STAR,</address>
<keyword confidence="0.875097">Key Laboratory of Math. Eng. and Advanced Computing,</keyword>
<abstract confidence="0.9936142">Dirichlet process mixture model (DPM- M) has great potential for detecting the underlying structure of data. Extensive studies have applied it for text clustering in terms of topics. However, due to the unsupervised nature, the topic clusters are always less satisfactory. Considering that people often have some prior knowledge about which potential topics should exist in given data, we aim to incorporate such knowledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded P´olya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM significantly outperforms stateof-the-art DPMM model and can be applied in a lifelong learning framework.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>The work is supported by 973 Program (No.</title>
<booktitle>2014CB340504), NSFC-ANR (No. 61261130588), Tsinghua University Initiative Scientific Research Program (No. 20131089256), Science and Technology Support Program (No. 2014BAK04B00), and THU-NUS NExT Co-Lab.</booktitle>
<marker></marker>
<rawString>The work is supported by 973 Program (No. 2014CB340504), NSFC-ANR (No. 61261130588), Tsinghua University Initiative Scientific Research Program (No. 20131089256), Science and Technology Support Program (No. 2014BAK04B00), and THU-NUS NExT Co-Lab.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amr Ahmed</author>
<author>Eric P Xing</author>
</authors>
<title>Dynamic nonparametric mixture models and the recurrent chinese restaurant process: with applications to evolutionary clustering.</title>
<date>2008</date>
<booktitle>In SDM,</booktitle>
<pages>219--230</pages>
<publisher>SIAM.</publisher>
<contexts>
<context position="2126" citStr="Ahmed and Xing, 2008" startWordPosition="322" endWordPosition="325">text clustering in terms of their topics. While DPMM achieved some promising results, it can still sometimes produce unsatisfactory topic clusters due to its unsupervised nature. On the other hand, people often have prior knowledge about what potential topics should exist in a given text corpus. Take an earthquake event corpus as an example. The topics, such as “casualties and damages”, “rescue” and “government reaction”, called prior topics, are expected to occur in the corpus according to our common knowledge (e.g., the topics automatically learned from previous events using topic modeling (Ahmed and Xing, 2008)) or external resources (e.g., table of contents at Wikipedia event pages 1). Similarly, in academic fields, “call for papers (CFP)” of conferences 2 lists main topics that conference organizers would like to focus on. Clearly, these prior topics can be represented as sets of words, which are available in many real-world applications. They can serve as weakly supervised information to enhance the unsupervised DPMM for text clustering. Standard DPMM (Neal, 2000; Ranganathan, 2006) lacks a mechanism for incorporating prior knowledge. Some existing work (Vlachos et al., 2008; Vlachos et al., 2009</context>
<context position="5836" citStr="Ahmed and Xing, 2008" startWordPosition="917" endWordPosition="920">r distribution for B. Consider the document xi as a bag of words, given the topic Bi, the generative distribution F is a given likelihood function parameterized by B. We define F as p(xi|Bi) = ∏|x�| j=1 p(xij|Bi), where xij is the jth word in xi. Note that the DPMM assumes each document can be assigned to one topic cluster only. Figure 1: Graphical Representation of DPMM. The DP process of DPMM, according to which topic Bi for a document xi is drawn, can be explained by the popular metaphor of P´olya urn (PU) scheme (Blackwell and MacQueen, 1973), equivalent to the Chinese Restaurant Process (Ahmed and Xing, 2008). The PU scheme works on balls (documents) and colors (topics). It starts with an empty urn. With probability proportional to α, we draw Bi — G0, and add a ball of this color to the urn. With probability proportional to i — 1 (i.e., the current number of balls in the urn), we draw a ball at random from the urn, observe its color Bi and replace the ball with two balls of the same color. In this way, we draw topic Bi for document xi. As shown in the process, the prior probability of assigning a document to a topic is proportional to the number of documents already assigned to the topic. As a res</context>
<context position="16777" citStr="Ahmed and Xing, 2008" startWordPosition="2799" endWordPosition="2803">DPMM in a lifelong continuous learning framework, namely TSDPMM-L, can further improve 790 Models 20N-1 20N-2 20N-3 Reu-1 Reu-2 Reus-3 DPMM 0.610 0.537 0.590 0.509 0.647 0.653 TSDPMM 0.645 0.610 0.681 0.648 0.654 0.655 TSDPMM-L ― 0.681 0.697 ― 0.689 0.656 Table 2: Average NMI of different models on news datasets. text clustering due to the better prior topic knowledge obtained in the evolving environment. 4 Related Work Our work is related to papers (Vlachos et al., 2008; Vlachos et al., 2009), which added supervision (instance-level must-links or cannot-links between documents) to the DPMM. (Ahmed and Xing, 2008) proposed recurrent Chinese Restaurant Process to incorporate previous documents with known topic clusters. However, our work is very different as we focus on how to incorporate latent topic-level prior knowledge. We model prior topics as known colors that have a certain probability proportional to α(0) k to be assigned to a document. In addition, our inference mechanism subsequently takes the prior knowledge into consideration for automatically assigning topics to documents. Some existing studies such as (Ramage et al., 2009; Andrzejewski et al., 2009; Jagarlamudi et al., 2012; Andrzejewski e</context>
</contexts>
<marker>Ahmed, Xing, 2008</marker>
<rawString>Amr Ahmed and Eric P Xing. 2008. Dynamic nonparametric mixture models and the recurrent chinese restaurant process: with applications to evolutionary clustering. In SDM, pages 219–230. SIAM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
<author>Mark Craven</author>
</authors>
<title>Incorporating domain knowledge into topic modeling via dirichlet forest priors.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual ICML,</booktitle>
<pages>25--32</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="17335" citStr="Andrzejewski et al., 2009" startWordPosition="2886" endWordPosition="2889">cannot-links between documents) to the DPMM. (Ahmed and Xing, 2008) proposed recurrent Chinese Restaurant Process to incorporate previous documents with known topic clusters. However, our work is very different as we focus on how to incorporate latent topic-level prior knowledge. We model prior topics as known colors that have a certain probability proportional to α(0) k to be assigned to a document. In addition, our inference mechanism subsequently takes the prior knowledge into consideration for automatically assigning topics to documents. Some existing studies such as (Ramage et al., 2009; Andrzejewski et al., 2009; Jagarlamudi et al., 2012; Andrzejewski et al., 2011) worked on incorporating prior lexical or domain knowledge into LDA. Different from all these work, we focus on the nonparametric model DPMM and propose to incorporate the prior topic knowledge obtained in multiple ways. 5 Conclusion In this paper, we propose a novel problem of incorporating prior topics into DPMM model and address it through a simple yet principled seeded P´olya urn scheme. We show that the topic knowledge can be obtained in multiple ways. Experiments on document clustering across 3 datasets demonstrate our proposed model </context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, 2009</marker>
<rawString>David Andrzejewski, Xiaojin Zhu, and Mark Craven. 2009. Incorporating domain knowledge into topic modeling via dirichlet forest priors. In Proceedings of the 26th Annual ICML, pages 25–32. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
<author>Mark Craven</author>
<author>Benjamin Recht</author>
</authors>
<title>A framework for incorporating general domain knowledge into latent dirichlet allocation using first-order logic.</title>
<date>2011</date>
<booktitle>In ProceedingsIJCAI,</booktitle>
<volume>22</volume>
<pages>1171</pages>
<contexts>
<context position="17389" citStr="Andrzejewski et al., 2011" startWordPosition="2894" endWordPosition="2897">nd Xing, 2008) proposed recurrent Chinese Restaurant Process to incorporate previous documents with known topic clusters. However, our work is very different as we focus on how to incorporate latent topic-level prior knowledge. We model prior topics as known colors that have a certain probability proportional to α(0) k to be assigned to a document. In addition, our inference mechanism subsequently takes the prior knowledge into consideration for automatically assigning topics to documents. Some existing studies such as (Ramage et al., 2009; Andrzejewski et al., 2009; Jagarlamudi et al., 2012; Andrzejewski et al., 2011) worked on incorporating prior lexical or domain knowledge into LDA. Different from all these work, we focus on the nonparametric model DPMM and propose to incorporate the prior topic knowledge obtained in multiple ways. 5 Conclusion In this paper, we propose a novel problem of incorporating prior topics into DPMM model and address it through a simple yet principled seeded P´olya urn scheme. We show that the topic knowledge can be obtained in multiple ways. Experiments on document clustering across 3 datasets demonstrate our proposed model can effectively incorporate the prior topic knowledge </context>
</contexts>
<marker>Andrzejewski, Zhu, Craven, Recht, 2011</marker>
<rawString>David Andrzejewski, Xiaojin Zhu, Mark Craven, and Benjamin Recht. 2011. A framework for incorporating general domain knowledge into latent dirichlet allocation using first-order logic. In ProceedingsIJCAI, volume 22, page 1171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles E Antoniak</author>
</authors>
<title>Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The annals of statistics,</title>
<date>1974</date>
<pages>1152--1174</pages>
<contexts>
<context position="4665" citStr="Antoniak, 1974" startWordPosition="703" endWordPosition="704">ings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 787–792, Lisbon, Portugal, 17-21 September 2015. c�2015 Association for Computational Linguistics. topic knowledge to evolve as more and more data are observed. 2 Topic Seeded DPMM In this section, we first introduce the standard DPMM model for document clustering in terms of topics. Then we describe how to incorporate seed/prior topics into the model using a seeded P´olya urn (sPU) scheme, which gives us our novel TSDPMM model (Topic Seeded DPMM). Finally, we present the model inference. 2.1 DPMM The DPMM (Antoniak, 1974) as a non-parametric model assumes the given data is governed by an infinite number of components where only a fraction of these components are activated by the data. Figure 1 illustrates the DPMM graphical model and its generative process of a document xi. First, we sample a topic Bi = {Bij}j=|V | j=1 (a multinomial distribution over words belonging to the vocabulary V ) for the document xi according to a Dirichlet Process (DP) G — DP(α, G0), where α &gt; 0 is a concentration parameter and the base measure G0 = Dir(⃗β) can be considered as a prior distribution for B. Consider the document xi as </context>
</contexts>
<marker>Antoniak, 1974</marker>
<rawString>Charles E Antoniak. 1974. Mixtures of dirichlet processes with applications to bayesian nonparametric problems. The annals of statistics, pages 1152– 1174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Blackwell</author>
<author>James B MacQueen</author>
</authors>
<title>Ferguson distributions via p´olya urn schemes. The annals of statistics,</title>
<date>1973</date>
<pages>353--355</pages>
<contexts>
<context position="5767" citStr="Blackwell and MacQueen, 1973" startWordPosition="906" endWordPosition="909">ation parameter and the base measure G0 = Dir(⃗β) can be considered as a prior distribution for B. Consider the document xi as a bag of words, given the topic Bi, the generative distribution F is a given likelihood function parameterized by B. We define F as p(xi|Bi) = ∏|x�| j=1 p(xij|Bi), where xij is the jth word in xi. Note that the DPMM assumes each document can be assigned to one topic cluster only. Figure 1: Graphical Representation of DPMM. The DP process of DPMM, according to which topic Bi for a document xi is drawn, can be explained by the popular metaphor of P´olya urn (PU) scheme (Blackwell and MacQueen, 1973), equivalent to the Chinese Restaurant Process (Ahmed and Xing, 2008). The PU scheme works on balls (documents) and colors (topics). It starts with an empty urn. With probability proportional to α, we draw Bi — G0, and add a ball of this color to the urn. With probability proportional to i — 1 (i.e., the current number of balls in the urn), we draw a ball at random from the urn, observe its color Bi and replace the ball with two balls of the same color. In this way, we draw topic Bi for document xi. As shown in the process, the prior probability of assigning a document to a topic is proportion</context>
</contexts>
<marker>Blackwell, MacQueen, 1973</marker>
<rawString>David Blackwell and James B MacQueen. 1973. Ferguson distributions via p´olya urn schemes. The annals of statistics, pages 353–355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of machine Learning research,</journal>
<pages>3--993</pages>
<contexts>
<context position="14086" citStr="Blei et al., 2003" startWordPosition="2370" endWordPosition="2373">lachos et al., 2009), we set the hyper-parameters α=1, ⃗α(0)={1.0}, β⃗ ={1.0}. We run Gibbs sampler for 100 iterations and stop the iteration once the log-likelihood of the training data converges. Evaluation. The widely used NMI (normalized mutual information) measure (Dom, 2002), has been employed to evaluate document clustering results. The higher a value of NMI, the better a clustering result is. However, NMI needs true class labels for documents, and can only be applied to our benchmark news datasets. For NIPS datasets without true labels, we use the measure of perplexity, as defined in (Blei et al., 2003), to test per-word likelihood of the datasets. The lower the perplexity, the better a model fits the data. 4https://nips.cc/Conferences/2014/CallForPapers 3.3 Results Table 1 shows the average perplexity values of five runs of 3 models on NIPS datasets. It shows that both TSDPMM-P and TSDPMM-E, leveraging prior topics from previous learning and “CFP” significantly outperform DPMM. In addition, TSDPMM-E achieves lower performance than TSDPMM-P due to its lower quality of prior topics directly obtained from “CFP”, compared to higher quality topics from past learning. We may improve “CFP” knowled</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhiyuan Chen</author>
<author>Bing Liu</author>
</authors>
<title>Mining topics in documents: standing on the shoulders of big data.</title>
<date>2014</date>
<booktitle>In Proceedings of the 20th ACM SIGKDD international conference,</booktitle>
<pages>1116--1125</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13373" citStr="Chen and Liu, 2014" startWordPosition="2253" endWordPosition="2256">ame for each year). We name them as TSDPMM-P and TSDPMM-E respectively. As the topic descriptions in “CFP” are sparse, we repeat each topic description by ten times and then represent a topic with the words with word frequencies in its description text. For both 20 Newsgroups and Reuters datasets, we use prior knowledge learned by DPMM from the previous day’s dataset. Furthermore, to test if we can improve the results continuously by applying TSDPMM, every time when we model a new dataset, we incorporate prior topics learned by TSDPMM from previous day’s dataset, similar to lifelong learning (Chen and Liu, 2014; Thrun, 1998). We call this model as TSDPMM-L. Parameter Setting. Following a previous work (Vlachos et al., 2009), we set the hyper-parameters α=1, ⃗α(0)={1.0}, β⃗ ={1.0}. We run Gibbs sampler for 100 iterations and stop the iteration once the log-likelihood of the training data converges. Evaluation. The widely used NMI (normalized mutual information) measure (Dom, 2002), has been employed to evaluate document clustering results. The higher a value of NMI, the better a clustering result is. However, NMI needs true class labels for documents, and can only be applied to our benchmark news dat</context>
</contexts>
<marker>Chen, Liu, 2014</marker>
<rawString>Zhiyuan Chen and Bing Liu. 2014. Mining topics in documents: standing on the shoulders of big data. In Proceedings of the 20th ACM SIGKDD international conference, pages 1116–1125. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byron E Dom</author>
</authors>
<title>An information-theoretic external cluster-validity measure.</title>
<date>2002</date>
<booktitle>In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,</booktitle>
<pages>137--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="13749" citStr="Dom, 2002" startWordPosition="2313" endWordPosition="2314">to test if we can improve the results continuously by applying TSDPMM, every time when we model a new dataset, we incorporate prior topics learned by TSDPMM from previous day’s dataset, similar to lifelong learning (Chen and Liu, 2014; Thrun, 1998). We call this model as TSDPMM-L. Parameter Setting. Following a previous work (Vlachos et al., 2009), we set the hyper-parameters α=1, ⃗α(0)={1.0}, β⃗ ={1.0}. We run Gibbs sampler for 100 iterations and stop the iteration once the log-likelihood of the training data converges. Evaluation. The widely used NMI (normalized mutual information) measure (Dom, 2002), has been employed to evaluate document clustering results. The higher a value of NMI, the better a clustering result is. However, NMI needs true class labels for documents, and can only be applied to our benchmark news datasets. For NIPS datasets without true labels, we use the measure of perplexity, as defined in (Blei et al., 2003), to test per-word likelihood of the datasets. The lower the perplexity, the better a model fits the data. 4https://nips.cc/Conferences/2014/CallForPapers 3.3 Results Table 1 shows the average perplexity values of five runs of 3 models on NIPS datasets. It shows </context>
</contexts>
<marker>Dom, 2002</marker>
<rawString>Byron E Dom. 2002. An information-theoretic external cluster-validity measure. In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence, pages 137–145. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Heinrich</author>
</authors>
<title>Parameter estimation for text analysis.</title>
<date>2009</date>
<tech>Technical report, vsonix</tech>
<institution>GmbH and University of Leipzig.</institution>
<contexts>
<context position="9716" citStr="Heinrich, 2009" startWordPosition="1639" endWordPosition="1640">of documents of topic k excluding the current document xi. If k is an existing (not prior) topic, it is proportional to nk,−i. If k is a new topic, the probability is proportional to α. The second item p(xi |⃗X−i, ⃗Z−i, zi = k, ⃗β) is the likelihood of xi given ⃗X−i, ⃗Z−i and zi=k. They can be derived as p(xi |⃗X−i, ⃗Z−i, zi = −i |⃗Z−i,⃗β)where p( ⃗X|⃗Z, ⃗β) = f p( ⃗X|⃗Z, Θ)p(Θ|⃗β)dΘ. As p(Θ|⃗β) is a Dirichlet distribution and p( ⃗X|⃗Z, Θ) is a multinomial distribution, we can get p( ⃗X |⃗Z)=HK ∆( ⃗Nk+⃗β) k=1 ∆(⃗β) , of occurrences of word w in the kth topic. Here, we adopt the function ∆ in (Heinrich, 2009), and ∏V ⃗β) = w=1 Γ(βR and ∆( ⃗Nk + ⃗β) = Γ(∑Vw=1)h&apos; ∏Vw v1 Γ(Nk,w+β) Finally, we can derive: Γ(∑w=1(Nk,w+β)) where ⃗Nk,−i is a vector with the word counts for all the documents assigned to topic k excluding xi, ⃗N(0) k are vectors with word counts in document xi and in all the documents assigned to k in prior knowledge respectively. According to this equation, documents are likely to go into clusters which are bigger and give higher likelihood of the documents. When the Gibbs sampler converges, we obtain topic cluster assignments of all the documents. Different from DPMM inference process in</context>
</contexts>
<marker>Heinrich, 2009</marker>
<rawString>Gregor Heinrich. 2009. Parameter estimation for text analysis. Technical report, vsonix GmbH and University of Leipzig.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruizhang Huang</author>
<author>Guan Yu</author>
<author>Zhaojun Wang</author>
<author>Jun Zhang</author>
<author>Liangxing Shi</author>
</authors>
<title>Dirichlet process mixture model for document clustering with feature partition.</title>
<date>2013</date>
<journal>Knowledge and Data Engineering,,</journal>
<volume>25</volume>
<issue>8</issue>
<contexts>
<context position="1468" citStr="Huang et al., 2013" startWordPosition="215" endWordPosition="218"> incorporate such knowledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded P´olya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM significantly outperforms stateof-the-art DPMM model and can be applied in a lifelong learning framework. 1 Introduction Dirichlet process mixture model (DPMM) (Neal, 2000) has been used in detecting the underlying structure in data. For example, (Vlachos et al., 2008; Vlachos et al., 2009) applied it to lexicalsemantic verb clustering. (Wang et al., 2011; Huang et al., 2013; Yin and Wang, 2014) applied it for text clustering in terms of their topics. While DPMM achieved some promising results, it can still sometimes produce unsatisfactory topic clusters due to its unsupervised nature. On the other hand, people often have prior knowledge about what potential topics should exist in a given text corpus. Take an earthquake event corpus as an example. The topics, such as “casualties and damages”, “rescue” and “government reaction”, called prior topics, are expected to occur in the corpus according to our common knowledge (e.g., the topics automatically learned from p</context>
</contexts>
<marker>Huang, Yu, Wang, Zhang, Shi, 2013</marker>
<rawString>Ruizhang Huang, Guan Yu, Zhaojun Wang, Jun Zhang, and Liangxing Shi. 2013. Dirichlet process mixture model for document clustering with feature partition. Knowledge and Data Engineering,, 25(8):1748–1759.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jagadeesh Jagarlamudi</author>
<author>Hal Daum´e</author>
<author>Raghavendra Udupa</author>
</authors>
<title>Incorporating lexical priors into topic models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the ACL,</booktitle>
<pages>204--213</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Jagarlamudi, Daum´e, Udupa, 2012</marker>
<rawString>Jagadeesh Jagarlamudi, Hal Daum´e III, and Raghavendra Udupa. 2012. Incorporating lexical priors into topic models. In Proceedings of the 13th Conference of the European Chapter of the ACL, pages 204–213. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Markov chain sampling methods for dirichlet process mixture models.</title>
<date>2000</date>
<journal>Journal of computational and graphical statistics,</journal>
<volume>9</volume>
<issue>2</issue>
<pages>265</pages>
<contexts>
<context position="1263" citStr="Neal, 2000" startWordPosition="182" endWordPosition="183">o the unsupervised nature, the topic clusters are always less satisfactory. Considering that people often have some prior knowledge about which potential topics should exist in given data, we aim to incorporate such knowledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded P´olya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM significantly outperforms stateof-the-art DPMM model and can be applied in a lifelong learning framework. 1 Introduction Dirichlet process mixture model (DPMM) (Neal, 2000) has been used in detecting the underlying structure in data. For example, (Vlachos et al., 2008; Vlachos et al., 2009) applied it to lexicalsemantic verb clustering. (Wang et al., 2011; Huang et al., 2013; Yin and Wang, 2014) applied it for text clustering in terms of their topics. While DPMM achieved some promising results, it can still sometimes produce unsatisfactory topic clusters due to its unsupervised nature. On the other hand, people often have prior knowledge about what potential topics should exist in a given text corpus. Take an earthquake event corpus as an example. The topics, su</context>
<context position="2590" citStr="Neal, 2000" startWordPosition="398" endWordPosition="399"> corpus according to our common knowledge (e.g., the topics automatically learned from previous events using topic modeling (Ahmed and Xing, 2008)) or external resources (e.g., table of contents at Wikipedia event pages 1). Similarly, in academic fields, “call for papers (CFP)” of conferences 2 lists main topics that conference organizers would like to focus on. Clearly, these prior topics can be represented as sets of words, which are available in many real-world applications. They can serve as weakly supervised information to enhance the unsupervised DPMM for text clustering. Standard DPMM (Neal, 2000; Ranganathan, 2006) lacks a mechanism for incorporating prior knowledge. Some existing work (Vlachos et al., 2008; Vlachos et al., 2009) added knowledge of observed instance-level constraints (must-links and cannot-links between documents) to DPMM. (Ahmed and Xing, 2008) proposed recurrent Chinese Restaurant Process to incorporate previous documents with known topic clusters. We focus on incorporating topic-level knowledge, which is more challenging, as seed/prior topics could be latent rather than observable. Particularly, we construct our novel TSDPMM (Topic Seeded DPMM) based on a principl</context>
</contexts>
<marker>Neal, 2000</marker>
<rawString>Radford M Neal. 2000. Markov chain sampling methods for dirichlet process mixture models. Journal of computational and graphical statistics, 9(2):249– 265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>David Leo Wright Hall</author>
<author>Ramesh Nallapati</author>
<author>Christopher D Manning</author>
</authors>
<title>Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on EMNLP,</booktitle>
<pages>248--256</pages>
<contexts>
<context position="17308" citStr="Ramage et al., 2009" startWordPosition="2882" endWordPosition="2885">-level must-links or cannot-links between documents) to the DPMM. (Ahmed and Xing, 2008) proposed recurrent Chinese Restaurant Process to incorporate previous documents with known topic clusters. However, our work is very different as we focus on how to incorporate latent topic-level prior knowledge. We model prior topics as known colors that have a certain probability proportional to α(0) k to be assigned to a document. In addition, our inference mechanism subsequently takes the prior knowledge into consideration for automatically assigning topics to documents. Some existing studies such as (Ramage et al., 2009; Andrzejewski et al., 2009; Jagarlamudi et al., 2012; Andrzejewski et al., 2011) worked on incorporating prior lexical or domain knowledge into LDA. Different from all these work, we focus on the nonparametric model DPMM and propose to incorporate the prior topic knowledge obtained in multiple ways. 5 Conclusion In this paper, we propose a novel problem of incorporating prior topics into DPMM model and address it through a simple yet principled seeded P´olya urn scheme. We show that the topic knowledge can be obtained in multiple ways. Experiments on document clustering across 3 datasets demo</context>
</contexts>
<marker>Ramage, Hall, Nallapati, Manning, 2009</marker>
<rawString>Daniel Ramage, David Leo Wright Hall, Ramesh Nallapati, and Christopher D. Manning. 2009. Labeled LDA: A supervised topic model for credit attribution in multi-labeled corpora. In Proceedings of the 2009 Conference on EMNLP, pages 248–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananth Ranganathan</author>
</authors>
<title>The dirichlet process mixture (dpm) model.</title>
<date>2006</date>
<tech>Technical report, Citeseer.</tech>
<contexts>
<context position="2610" citStr="Ranganathan, 2006" startWordPosition="400" endWordPosition="401">rding to our common knowledge (e.g., the topics automatically learned from previous events using topic modeling (Ahmed and Xing, 2008)) or external resources (e.g., table of contents at Wikipedia event pages 1). Similarly, in academic fields, “call for papers (CFP)” of conferences 2 lists main topics that conference organizers would like to focus on. Clearly, these prior topics can be represented as sets of words, which are available in many real-world applications. They can serve as weakly supervised information to enhance the unsupervised DPMM for text clustering. Standard DPMM (Neal, 2000; Ranganathan, 2006) lacks a mechanism for incorporating prior knowledge. Some existing work (Vlachos et al., 2008; Vlachos et al., 2009) added knowledge of observed instance-level constraints (must-links and cannot-links between documents) to DPMM. (Ahmed and Xing, 2008) proposed recurrent Chinese Restaurant Process to incorporate previous documents with known topic clusters. We focus on incorporating topic-level knowledge, which is more challenging, as seed/prior topics could be latent rather than observable. Particularly, we construct our novel TSDPMM (Topic Seeded DPMM) based on a principled seeded P´olya urn</context>
</contexts>
<marker>Ranganathan, 2006</marker>
<rawString>Ananth Ranganathan. 2006. The dirichlet process mixture (dpm) model. Technical report, Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Thrun</author>
</authors>
<title>Lifelong learning algorithms.</title>
<date>1998</date>
<booktitle>In Learning to learn,</booktitle>
<pages>181--209</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="13387" citStr="Thrun, 1998" startWordPosition="2257" endWordPosition="2258">We name them as TSDPMM-P and TSDPMM-E respectively. As the topic descriptions in “CFP” are sparse, we repeat each topic description by ten times and then represent a topic with the words with word frequencies in its description text. For both 20 Newsgroups and Reuters datasets, we use prior knowledge learned by DPMM from the previous day’s dataset. Furthermore, to test if we can improve the results continuously by applying TSDPMM, every time when we model a new dataset, we incorporate prior topics learned by TSDPMM from previous day’s dataset, similar to lifelong learning (Chen and Liu, 2014; Thrun, 1998). We call this model as TSDPMM-L. Parameter Setting. Following a previous work (Vlachos et al., 2009), we set the hyper-parameters α=1, ⃗α(0)={1.0}, β⃗ ={1.0}. We run Gibbs sampler for 100 iterations and stop the iteration once the log-likelihood of the training data converges. Evaluation. The widely used NMI (normalized mutual information) measure (Dom, 2002), has been employed to evaluate document clustering results. The higher a value of NMI, the better a clustering result is. However, NMI needs true class labels for documents, and can only be applied to our benchmark news datasets. For NIP</context>
</contexts>
<marker>Thrun, 1998</marker>
<rawString>Sebastian Thrun. 1998. Lifelong learning algorithms. In Learning to learn, pages 181–209. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Zoubin Ghahramani</author>
<author>Anna Korhonen</author>
</authors>
<title>Dirichlet process mixture models for verb clustering.</title>
<date>2008</date>
<booktitle>In Proceedings of the ICML workshop on Prior Knowledge for Text and Language.</booktitle>
<contexts>
<context position="1359" citStr="Vlachos et al., 2008" startWordPosition="196" endWordPosition="199"> that people often have some prior knowledge about which potential topics should exist in given data, we aim to incorporate such knowledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded P´olya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM significantly outperforms stateof-the-art DPMM model and can be applied in a lifelong learning framework. 1 Introduction Dirichlet process mixture model (DPMM) (Neal, 2000) has been used in detecting the underlying structure in data. For example, (Vlachos et al., 2008; Vlachos et al., 2009) applied it to lexicalsemantic verb clustering. (Wang et al., 2011; Huang et al., 2013; Yin and Wang, 2014) applied it for text clustering in terms of their topics. While DPMM achieved some promising results, it can still sometimes produce unsatisfactory topic clusters due to its unsupervised nature. On the other hand, people often have prior knowledge about what potential topics should exist in a given text corpus. Take an earthquake event corpus as an example. The topics, such as “casualties and damages”, “rescue” and “government reaction”, called prior topics, are exp</context>
<context position="2704" citStr="Vlachos et al., 2008" startWordPosition="413" endWordPosition="416">sing topic modeling (Ahmed and Xing, 2008)) or external resources (e.g., table of contents at Wikipedia event pages 1). Similarly, in academic fields, “call for papers (CFP)” of conferences 2 lists main topics that conference organizers would like to focus on. Clearly, these prior topics can be represented as sets of words, which are available in many real-world applications. They can serve as weakly supervised information to enhance the unsupervised DPMM for text clustering. Standard DPMM (Neal, 2000; Ranganathan, 2006) lacks a mechanism for incorporating prior knowledge. Some existing work (Vlachos et al., 2008; Vlachos et al., 2009) added knowledge of observed instance-level constraints (must-links and cannot-links between documents) to DPMM. (Ahmed and Xing, 2008) proposed recurrent Chinese Restaurant Process to incorporate previous documents with known topic clusters. We focus on incorporating topic-level knowledge, which is more challenging, as seed/prior topics could be latent rather than observable. Particularly, we construct our novel TSDPMM (Topic Seeded DPMM) based on a principled seeded P´olya urn (sPU) scheme. Our model inherits the nonparametric property of DPMM and has additional techni</context>
<context position="16631" citStr="Vlachos et al., 2008" startWordPosition="2777" endWordPosition="2780">s can improve DPMM model by incorporating prior topic knowledge, and the higher-quality knowledge will lead to better results. By applying our TSDPMM in a lifelong continuous learning framework, namely TSDPMM-L, can further improve 790 Models 20N-1 20N-2 20N-3 Reu-1 Reu-2 Reus-3 DPMM 0.610 0.537 0.590 0.509 0.647 0.653 TSDPMM 0.645 0.610 0.681 0.648 0.654 0.655 TSDPMM-L ― 0.681 0.697 ― 0.689 0.656 Table 2: Average NMI of different models on news datasets. text clustering due to the better prior topic knowledge obtained in the evolving environment. 4 Related Work Our work is related to papers (Vlachos et al., 2008; Vlachos et al., 2009), which added supervision (instance-level must-links or cannot-links between documents) to the DPMM. (Ahmed and Xing, 2008) proposed recurrent Chinese Restaurant Process to incorporate previous documents with known topic clusters. However, our work is very different as we focus on how to incorporate latent topic-level prior knowledge. We model prior topics as known colors that have a certain probability proportional to α(0) k to be assigned to a document. In addition, our inference mechanism subsequently takes the prior knowledge into consideration for automatically assi</context>
</contexts>
<marker>Vlachos, Ghahramani, Korhonen, 2008</marker>
<rawString>Andreas Vlachos, Zoubin Ghahramani, and Anna Korhonen. 2008. Dirichlet process mixture models for verb clustering. In Proceedings of the ICML workshop on Prior Knowledge for Text and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Vlachos</author>
<author>Anna Korhonen</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>Unsupervised and constrained dirichlet process mixture models for verb clustering.</title>
<date>2009</date>
<booktitle>In Proceedings of the workshop on geometrical models of natural language semantics,</booktitle>
<pages>74--82</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1382" citStr="Vlachos et al., 2009" startWordPosition="200" endWordPosition="203">e some prior knowledge about which potential topics should exist in given data, we aim to incorporate such knowledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded P´olya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM significantly outperforms stateof-the-art DPMM model and can be applied in a lifelong learning framework. 1 Introduction Dirichlet process mixture model (DPMM) (Neal, 2000) has been used in detecting the underlying structure in data. For example, (Vlachos et al., 2008; Vlachos et al., 2009) applied it to lexicalsemantic verb clustering. (Wang et al., 2011; Huang et al., 2013; Yin and Wang, 2014) applied it for text clustering in terms of their topics. While DPMM achieved some promising results, it can still sometimes produce unsatisfactory topic clusters due to its unsupervised nature. On the other hand, people often have prior knowledge about what potential topics should exist in a given text corpus. Take an earthquake event corpus as an example. The topics, such as “casualties and damages”, “rescue” and “government reaction”, called prior topics, are expected to occur in the c</context>
<context position="2727" citStr="Vlachos et al., 2009" startWordPosition="417" endWordPosition="420">hmed and Xing, 2008)) or external resources (e.g., table of contents at Wikipedia event pages 1). Similarly, in academic fields, “call for papers (CFP)” of conferences 2 lists main topics that conference organizers would like to focus on. Clearly, these prior topics can be represented as sets of words, which are available in many real-world applications. They can serve as weakly supervised information to enhance the unsupervised DPMM for text clustering. Standard DPMM (Neal, 2000; Ranganathan, 2006) lacks a mechanism for incorporating prior knowledge. Some existing work (Vlachos et al., 2008; Vlachos et al., 2009) added knowledge of observed instance-level constraints (must-links and cannot-links between documents) to DPMM. (Ahmed and Xing, 2008) proposed recurrent Chinese Restaurant Process to incorporate previous documents with known topic clusters. We focus on incorporating topic-level knowledge, which is more challenging, as seed/prior topics could be latent rather than observable. Particularly, we construct our novel TSDPMM (Topic Seeded DPMM) based on a principled seeded P´olya urn (sPU) scheme. Our model inherits the nonparametric property of DPMM and has additional technical merits. Importantly</context>
<context position="13488" citStr="Vlachos et al., 2009" startWordPosition="2271" endWordPosition="2274">parse, we repeat each topic description by ten times and then represent a topic with the words with word frequencies in its description text. For both 20 Newsgroups and Reuters datasets, we use prior knowledge learned by DPMM from the previous day’s dataset. Furthermore, to test if we can improve the results continuously by applying TSDPMM, every time when we model a new dataset, we incorporate prior topics learned by TSDPMM from previous day’s dataset, similar to lifelong learning (Chen and Liu, 2014; Thrun, 1998). We call this model as TSDPMM-L. Parameter Setting. Following a previous work (Vlachos et al., 2009), we set the hyper-parameters α=1, ⃗α(0)={1.0}, β⃗ ={1.0}. We run Gibbs sampler for 100 iterations and stop the iteration once the log-likelihood of the training data converges. Evaluation. The widely used NMI (normalized mutual information) measure (Dom, 2002), has been employed to evaluate document clustering results. The higher a value of NMI, the better a clustering result is. However, NMI needs true class labels for documents, and can only be applied to our benchmark news datasets. For NIPS datasets without true labels, we use the measure of perplexity, as defined in (Blei et al., 2003), </context>
<context position="16654" citStr="Vlachos et al., 2009" startWordPosition="2781" endWordPosition="2784">el by incorporating prior topic knowledge, and the higher-quality knowledge will lead to better results. By applying our TSDPMM in a lifelong continuous learning framework, namely TSDPMM-L, can further improve 790 Models 20N-1 20N-2 20N-3 Reu-1 Reu-2 Reus-3 DPMM 0.610 0.537 0.590 0.509 0.647 0.653 TSDPMM 0.645 0.610 0.681 0.648 0.654 0.655 TSDPMM-L ― 0.681 0.697 ― 0.689 0.656 Table 2: Average NMI of different models on news datasets. text clustering due to the better prior topic knowledge obtained in the evolving environment. 4 Related Work Our work is related to papers (Vlachos et al., 2008; Vlachos et al., 2009), which added supervision (instance-level must-links or cannot-links between documents) to the DPMM. (Ahmed and Xing, 2008) proposed recurrent Chinese Restaurant Process to incorporate previous documents with known topic clusters. However, our work is very different as we focus on how to incorporate latent topic-level prior knowledge. We model prior topics as known colors that have a certain probability proportional to α(0) k to be assigned to a document. In addition, our inference mechanism subsequently takes the prior knowledge into consideration for automatically assigning topics to documen</context>
</contexts>
<marker>Vlachos, Korhonen, Ghahramani, 2009</marker>
<rawString>Andreas Vlachos, Anna Korhonen, and Zoubin Ghahramani. 2009. Unsupervised and constrained dirichlet process mixture models for verb clustering. In Proceedings of the workshop on geometrical models of natural language semantics, pages 74–82. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chan Wang</author>
<author>Caixia Yuan</author>
<author>Xiaojie Wang</author>
<author>Wenwei Xue</author>
</authors>
<title>Dirichlet process mixture models based topic identification for short text streams.</title>
<date>2011</date>
<booktitle>In NLPKE,</booktitle>
<pages>80--87</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1448" citStr="Wang et al., 2011" startWordPosition="211" endWordPosition="214">ven data, we aim to incorporate such knowledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded P´olya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM significantly outperforms stateof-the-art DPMM model and can be applied in a lifelong learning framework. 1 Introduction Dirichlet process mixture model (DPMM) (Neal, 2000) has been used in detecting the underlying structure in data. For example, (Vlachos et al., 2008; Vlachos et al., 2009) applied it to lexicalsemantic verb clustering. (Wang et al., 2011; Huang et al., 2013; Yin and Wang, 2014) applied it for text clustering in terms of their topics. While DPMM achieved some promising results, it can still sometimes produce unsatisfactory topic clusters due to its unsupervised nature. On the other hand, people often have prior knowledge about what potential topics should exist in a given text corpus. Take an earthquake event corpus as an example. The topics, such as “casualties and damages”, “rescue” and “government reaction”, called prior topics, are expected to occur in the corpus according to our common knowledge (e.g., the topics automati</context>
</contexts>
<marker>Wang, Yuan, Wang, Xue, 2011</marker>
<rawString>Chan Wang, Caixia Yuan, Xiaojie Wang, and Wenwei Xue. 2011. Dirichlet process mixture models based topic identification for short text streams. In NLPKE, pages 80–87. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Yin</author>
<author>Jianyong Wang</author>
</authors>
<title>A dirichlet multinomial mixture model-based approach for short text clustering.</title>
<date>2014</date>
<booktitle>In Proceedings of the 20th ACM SIGKDD,</booktitle>
<pages>233--242</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1489" citStr="Yin and Wang, 2014" startWordPosition="219" endWordPosition="222">owledge into the DPMM to improve text clustering. We propose a novel model TSDPMM based on a new seeded P´olya urn scheme. Experimental results on document clustering across three datasets demonstrate our proposed TSDPMM significantly outperforms stateof-the-art DPMM model and can be applied in a lifelong learning framework. 1 Introduction Dirichlet process mixture model (DPMM) (Neal, 2000) has been used in detecting the underlying structure in data. For example, (Vlachos et al., 2008; Vlachos et al., 2009) applied it to lexicalsemantic verb clustering. (Wang et al., 2011; Huang et al., 2013; Yin and Wang, 2014) applied it for text clustering in terms of their topics. While DPMM achieved some promising results, it can still sometimes produce unsatisfactory topic clusters due to its unsupervised nature. On the other hand, people often have prior knowledge about what potential topics should exist in a given text corpus. Take an earthquake event corpus as an example. The topics, such as “casualties and damages”, “rescue” and “government reaction”, called prior topics, are expected to occur in the corpus according to our common knowledge (e.g., the topics automatically learned from previous events using </context>
</contexts>
<marker>Yin, Wang, 2014</marker>
<rawString>Jianhua Yin and Jianyong Wang. 2014. A dirichlet multinomial mixture model-based approach for short text clustering. In Proceedings of the 20th ACM SIGKDD, pages 233–242. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>